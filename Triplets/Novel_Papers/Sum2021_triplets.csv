topic,paper_ID,sentence_ID,info-unit,sub,pred,obj,triplets,pred_weights
translation,0,66,baselines,random,extracts,random sentences,random extracts random sentences,0.6996886730194092
translation,0,66,baselines,random sentences,until,summary,random sentences until summary,0.7189894318580627
translation,0,66,baselines,summary,reaches,target word count ( average summary length ),summary reaches target word count ( average summary length ),0.6968466639518738
translation,0,66,baselines,lexrank,selects,top-k sentences,lexrank selects top-k sentences,0.6655735373497009
translation,0,66,baselines,top-k sentences,with,"largest lexrank ( erkan and radev , 2004 ) score","top-k sentences with largest lexrank ( erkan and radev , 2004 ) score",0.6122522354125977
translation,0,66,baselines,target word count,is,reached,target word count is reached,0.5975184440612793
translation,0,66,baselines,baselines,has,random,baselines has random,0.6280671954154968
translation,0,67,baselines,supervised baseline,present,clinneusum,supervised baseline present clinneusum,0.7400453686714172
translation,0,67,baselines,supervised baseline,present,clinneusum,supervised baseline present clinneusum,0.7400453686714172
translation,0,67,baselines,clinneusum,is,hierarchical lstm network,clinneusum is hierarchical lstm network,0.5895667672157288
translation,0,67,baselines,variant of the neusum model,adapted to,clinical genre,variant of the neusum model adapted to clinical genre,0.6821069717407227
translation,0,67,baselines,clinneusum,is,hierarchical lstm network,clinneusum is hierarchical lstm network,0.5895667672157288
translation,0,67,baselines,hierarchical lstm network,trained on,ground -truth labels,hierarchical lstm network trained on ground -truth labels,0.7067385911941528
translation,0,67,baselines,ground -truth labels,derived from,ora - cle gain,ground -truth labels derived from ora - cle gain,0.5841142535209656
translation,0,67,baselines,clinneusum,has,variant of the neusum model,clinneusum has variant of the neusum model,0.5837224721908569
translation,1,27,experimental-setup,transformer architecture,with,12 hidden layers,transformer architecture with 12 hidden layers,0.6222838759422302
translation,1,27,experimental-setup,transformer architecture,with,12 attention heads,transformer architecture with 12 attention heads,0.6552371978759766
translation,1,27,experimental-setup,transformer architecture,total of,223 m parameters,transformer architecture total of 223 m parameters,0.6585829257965088
translation,1,27,experimental-setup,hidden size,of,768,hidden size of 768,0.6723939776420593
translation,1,27,experimental-setup,transformer architecture,has,hidden size,transformer architecture has hidden size,0.5714672207832336
translation,1,27,experimental-setup,filter size,has,3072,filter size has 3072,0.6134928464889526
translation,1,27,experimental-setup,experimental setup,use,transformer architecture,experimental setup use transformer architecture,0.5702999830245972
translation,1,28,experimental-setup,1.5 million steps,on,"c4 corpus ( raffel et al. , 2019 )","1.5 million steps on c4 corpus ( raffel et al. , 2019 )",0.5103748440742493
translation,1,28,experimental-setup,batch size,of,16,batch size of 16,0.6842944622039795
translation,1,28,experimental-setup,learning rate,of,0.01,learning rate of 0.01,0.6152973175048828
translation,1,28,experimental-setup,maximum input-output lengths,of,512 and 256,maximum input-output lengths of 512 and 256,0.6381828784942627
translation,1,28,experimental-setup,experimental setup,pretrained for,1.5 million steps,experimental setup pretrained for 1.5 million steps,0.7042998671531677
translation,1,35,experimental-setup,15 %,of,input text,15 % of input text,0.5988770723342896
translation,1,63,results,performance,of,span prediction objective,performance of span prediction objective,0.5397035479545593
translation,1,63,results,performance,of,salient sentence prediction objective,performance of salient sentence prediction objective,0.5178585648536682
translation,1,63,results,salient sentence prediction objective,for,all three datasets,salient sentence prediction objective for all three datasets,0.5242556929588318
translation,1,63,results,salient sentence prediction objective,when using,whole training set,salient sentence prediction objective when using whole training set,0.6165773272514343
translation,1,63,results,results,found that,performance,results found that performance,0.7056670188903809
translation,1,71,results,lead baseline and mleads,are,on par,lead baseline and mleads are on par,0.6271767616271973
translation,1,71,results,mleads,does not use,any training data,mleads does not use any training data,0.7524858713150024
translation,1,71,results,any training data,when,1000 examples or less,any training data when 1000 examples or less,0.6301698684692383
translation,1,71,results,outperform,has,other methods,outperform has other methods,0.5804759860038757
translation,1,71,results,results,has,lead baseline and mleads,results has lead baseline and mleads,0.5203626155853271
translation,1,75,results,sentence - level pretraining,in,"mrnds , mleads and pegasus","sentence - level pretraining in mrnds , mleads and pegasus",0.49810129404067993
translation,1,75,results,"mrnds , mleads and pegasus",better than,t5,"mrnds , mleads and pegasus better than t5",0.7180495262145996
translation,1,75,results,t5,in producing,well - formed summaries,t5 in producing well - formed summaries,0.7429976463317871
translation,1,80,results,outperforms,on,all dataset sizes,outperforms on all dataset sizes,0.5130265355110168
translation,1,80,results,t5,on,all dataset sizes,t5 on all dataset sizes,0.5792683362960815
translation,1,80,results,textcor,has,outperforms,textcor has outperforms,0.6313523054122925
translation,1,80,results,outperforms,has,t5,outperforms has t5,0.6486032009124756
translation,1,81,results,unrelated task -specific pretraining objective,has,hurts,unrelated task -specific pretraining objective has hurts,0.5531905889511108
translation,1,81,results,hurts,has,performance,hurts has performance,0.6043882966041565
translation,1,81,results,results,show,unrelated task -specific pretraining objective,results show unrelated task -specific pretraining objective,0.5386560559272766
translation,1,86,results,pretrained textcor model,achieves,f 0.5 score,pretrained textcor model achieves f 0.5 score,0.6285765767097473
translation,1,86,results,pretrained textcor model,achieves,precision,pretrained textcor model achieves precision,0.6297683715820312
translation,1,86,results,pretrained textcor model,achieves,recall,pretrained textcor model achieves recall,0.650330126285553
translation,1,86,results,f 0.5 score,of,18.64,f 0.5 score of 18.64,0.5750303864479065
translation,1,86,results,precision,has,40.94,precision has 40.94,0.55828458070755
translation,1,86,results,recall,has,5.87,recall has 5.87,0.5924963355064392
translation,1,86,results,results,has,pretrained textcor model,results has pretrained textcor model,0.5118261575698853
translation,2,7,baselines,wikitransfer,fine-tunes,pretrained models,wikitransfer fine-tunes pretrained models,0.685437023639679
translation,2,7,baselines,pretrained models,on,pseudo-summaries,pretrained models on pseudo-summaries,0.4980853497982025
translation,2,7,baselines,baselines,has,wikitransfer,baselines has wikitransfer,0.581860363483429
translation,2,6,model,novel and generalizable method,called,wikitransfer,novel and generalizable method called wikitransfer,0.6700897812843323
translation,2,6,model,novel and generalizable method,for fine-tuning,pretrained models,novel and generalizable method for fine-tuning pretrained models,0.7397302389144897
translation,2,6,model,pretrained models,for,summarization,pretrained models for summarization,0.5589563250541687
translation,2,6,model,summarization,in,"unsupervised , dataset -specific manner","summarization in unsupervised , dataset -specific manner",0.5019083619117737
translation,2,6,model,model,introduce,novel and generalizable method,model introduce novel and generalizable method,0.6731829047203064
translation,2,27,model,knowledge,into,wikipedia article data,knowledge into wikipedia article data,0.5208615660667419
translation,2,27,model,wikipedia article data,by extracting,summaries,wikipedia article data by extracting summaries,0.7136968970298767
translation,2,27,model,summaries,of,desired output length,summaries of desired output length,0.6261389851570129
translation,2,27,model,examples,based on,desired level of abstraction,examples based on desired level of abstraction,0.6812852025032043
translation,2,27,model,model,encode,knowledge,model encode knowledge,0.787060558795929
translation,2,28,model,novel method,called,wikitransfer,novel method called wikitransfer,0.668368399143219
translation,2,28,model,novel method,creates,pseudo-summaries,novel method creates pseudo-summaries,0.6646875739097595
translation,2,28,model,pseudo-summaries,with,subaspects,pseudo-summaries with subaspects,0.6357275247573853
translation,2,28,model,subaspects,of,target dataset,subaspects of target dataset,0.5757784247398376
translation,2,28,model,subaspects,used as,unlabeled data,subaspects used as unlabeled data,0.5915693640708923
translation,2,28,model,unlabeled data,for,intermediate fine-tuning,unlabeled data for intermediate fine-tuning,0.5835570693016052
translation,2,28,model,model,introduce,novel method,model introduce novel method,0.6845430135726929
translation,3,173,ablation-analysis,ablation analysis,has,global information modeling,ablation analysis has global information modeling,0.5236870646476746
translation,3,194,ablation-analysis,window length,set to,368 - 512,window length set to 368 - 512,0.715558648109436
translation,3,194,ablation-analysis,performance,shows,stable trend,performance shows stable trend,0.7040448784828186
translation,3,194,ablation-analysis,window length,has,performance,window length has performance,0.58903568983078
translation,3,194,ablation-analysis,368 - 512,has,performance,368 - 512 has performance,0.5912848711013794
translation,3,194,ablation-analysis,ablation analysis,when,window length,ablation analysis when window length,0.6455308794975281
translation,3,236,ablation-analysis,declines dramatically,on,both datasets,declines dramatically on both datasets,0.5427358746528625
translation,3,236,ablation-analysis,declines dramatically,when,memory module,declines dramatically when memory module,0.6530120968818665
translation,3,236,ablation-analysis,memory module,is,removed,memory module is removed,0.5897093415260315
translation,3,236,ablation-analysis,performance,has,declines dramatically,performance has declines dramatically,0.5904573798179626
translation,3,251,ablation-analysis,model,achieves,better performance,model achieves better performance,0.6787512898445129
translation,3,251,ablation-analysis,better performance,in,three indicators,better performance in three indicators,0.5244069695472717
translation,3,251,ablation-analysis,better performance,when combined with,memory mechanism,better performance when combined with memory mechanism,0.6038609743118286
translation,3,251,ablation-analysis,three indicators,when combined with,memory mechanism,three indicators when combined with memory mechanism,0.6726917028427124
translation,3,251,ablation-analysis,ablation analysis,has,model,ablation analysis has model,0.4912438988685608
translation,3,147,baselines,"match - sum ( zhong et al. , 2020 )",is,state- of- theart bert - based summarization model,"match - sum ( zhong et al. , 2020 ) is state- of- theart bert - based summarization model",0.5320518612861633
translation,3,147,baselines,baselines,has,"match - sum ( zhong et al. , 2020 )","baselines has match - sum ( zhong et al. , 2020 )",0.5312862992286682
translation,3,160,baselines,earlystop strategy,applied when,valid loss,earlystop strategy applied when valid loss,0.6862590312957764
translation,3,160,baselines,valid loss,is,no longer decent,valid loss is no longer decent,0.5723716616630554
translation,3,160,baselines,baselines,has,earlystop strategy,baselines has earlystop strategy,0.5728113651275635
translation,3,153,experimental-setup,maximum length,of,window,maximum length of window,0.590071976184845
translation,3,153,experimental-setup,window,set to,512,window set to 512,0.7181904911994934
translation,3,153,experimental-setup,documents,with,sentence,documents with sentence,0.624058187007904
translation,3,153,experimental-setup,sentence,as,smallest unit,sentence as smallest unit,0.5422272682189941
translation,3,153,experimental-setup,experimental setup,segment,documents,experimental setup segment documents,0.6535866260528564
translation,3,153,experimental-setup,experimental setup,has,maximum length,experimental setup has maximum length,0.4922432005405426
translation,3,154,experimental-setup,memory module,set,number of slots,memory module set number of slots,0.6583795547485352
translation,3,154,experimental-setup,memory module,dimension of,memory vector,memory module dimension of memory vector,0.6952628493309021
translation,3,154,experimental-setup,number of slots,to,50,number of slots to 50,0.6073609590530396
translation,3,154,experimental-setup,memory vector,to,768,memory vector to 768,0.6167912483215332
translation,3,154,experimental-setup,experimental setup,For,memory module,experimental setup For memory module,0.5459499359130859
translation,3,155,experimental-setup,iteration number,of,gat,iteration number of gat,0.6442285180091858
translation,3,155,experimental-setup,iteration number,set to,2,iteration number set to 2,0.7619167566299438
translation,3,155,experimental-setup,gat,set to,2,gat set to 2,0.7394549250602722
translation,3,155,experimental-setup,experimental setup,has,iteration number,experimental setup has iteration number,0.5213159918785095
translation,3,156,experimental-setup,"rouge ( lin , 2004 )",as,evaluation metric,"rouge ( lin , 2004 ) as evaluation metric",0.4853402376174927
translation,3,156,experimental-setup,"rouge ( lin , 2004 )",select,hyperparameters,"rouge ( lin , 2004 ) select hyperparameters",0.6267780065536499
translation,3,156,experimental-setup,hyperparameters,by,grid search,hyperparameters by grid search,0.5530908107757568
translation,3,156,experimental-setup,grid search,based on,  rouge - 2   performance on validation sets,grid search based on   rouge - 2   performance on validation sets,0.6067153811454773
translation,3,156,experimental-setup,experimental setup,use,"rouge ( lin , 2004 )","experimental setup use rouge ( lin , 2004 )",0.6175395250320435
translation,3,156,experimental-setup,experimental setup,select,hyperparameters,experimental setup select hyperparameters,0.5912401676177979
translation,3,158,experimental-setup,model,with,2 nvidia v100 cards,model with 2 nvidia v100 cards,0.6176756024360657
translation,3,158,experimental-setup,2 nvidia v100 cards,with,small batch size,2 nvidia v100 cards with small batch size,0.6071759462356567
translation,3,158,experimental-setup,small batch size,of,16,small batch size of 16,0.6674927473068237
translation,3,158,experimental-setup,experimental setup,train,model,experimental setup train model,0.6514950394630432
translation,3,159,experimental-setup,training,use,"adam ( kingma and ba , 2015 )","training use adam ( kingma and ba , 2015 )",0.6230131983757019
translation,3,159,experimental-setup,"adam ( kingma and ba , 2015 )",to optimize,parameters,"adam ( kingma and ba , 2015 ) to optimize parameters",0.6919412016868591
translation,3,159,experimental-setup,parameters,with,learning rate,parameters with learning rate,0.5885912179946899
translation,3,159,experimental-setup,learning rate,of,5e - 4,learning rate of 5e - 4,0.6392122507095337
translation,3,159,experimental-setup,experimental setup,During,training,experimental setup During training,0.6835477948188782
translation,3,152,experiments,sliding encoder,use,  bert- baseuncased   version,sliding encoder use   bert- baseuncased   version,0.6170487403869629
translation,3,152,experiments,  bert- baseuncased   version,with,hidden size,  bert- baseuncased   version with hidden size,0.6444762945175171
translation,3,152,experiments,hidden size,of,768,hidden size of 768,0.6723939776420593
translation,3,6,model,sliding selector network,with,dynamic memory,sliding selector network with dynamic memory,0.6107451319694519
translation,3,6,model,sliding selector network,employs,sliding window,sliding selector network employs sliding window,0.5375488996505737
translation,3,6,model,dynamic memory,for,extractive summarization,dynamic memory for extractive summarization,0.5550331473350525
translation,3,6,model,extractive summarization,of,long-form documents,extractive summarization of long-form documents,0.5519500970840454
translation,3,6,model,sliding window,to extract,summary sentences,sliding window to extract summary sentences,0.7090283632278442
translation,3,6,model,summary sentences,has,segment by segment,summary sentences has segment by segment,0.5962355732917786
translation,3,6,model,model,propose,sliding selector network,model propose sliding selector network,0.6022902131080627
translation,3,7,model,memory mechanism,to preserve and update,history information,memory mechanism to preserve and update history information,0.7129053473472595
translation,3,7,model,history information,allowing,semantic flow,history information allowing semantic flow,0.6982669830322266
translation,3,7,model,semantic flow,across,different windows,semantic flow across different windows,0.7301557064056396
translation,3,7,model,history information,has,dynamically,history information has dynamically,0.6015236377716064
translation,3,7,model,model,adopt,memory mechanism,model adopt memory mechanism,0.6769400238990784
translation,3,33,model,novel extractive summarization,for,long -form documents,novel extractive summarization for long -form documents,0.5808442234992981
translation,3,33,model,model,propose,novel extractive summarization,model propose novel extractive summarization,0.6705418825149536
translation,3,34,model,input document,into,multiple windows,input document into multiple windows,0.5808058977127075
translation,3,34,model,model,split,input document,model split input document,0.7025478482246399
translation,3,35,model,memory,to preserve,salient information,memory to preserve salient information,0.6637890338897705
translation,3,35,model,salient information,learned from,previous windows,salient information learned from previous windows,0.6898255944252014
translation,3,35,model,salient information,used to,complete and enrich,salient information used to complete and enrich,0.6370230317115784
translation,3,35,model,complete and enrich,has,local texts,complete and enrich has local texts,0.5570492148399353
translation,3,35,model,model,introduce,memory,model introduce memory,0.6673679351806641
translation,3,66,model,beyond - window,when performing,segmentlevel summarization,beyond - window when performing segmentlevel summarization,0.6675998568534851
translation,3,89,model,memory layer,infuses,history information,memory layer infuses history information,0.730963408946991
translation,3,89,model,history information,into,sentence representations,history information into sentence representations,0.5565688610076904
translation,3,89,model,sentence representations,via,graph neural networks,sentence representations via graph neural networks,0.5332456827163696
translation,3,89,model,model,has,memory layer,model has memory layer,0.5436630845069885
translation,3,143,model,pointer generator network ( pgn,extends,standard seq2seq framework,pointer generator network ( pgn extends standard seq2seq framework,0.7225736379623413
translation,3,143,model,standard seq2seq framework,with,"attention , coverage , and copy mechanism","standard seq2seq framework with attention , coverage , and copy mechanism",0.5758532881736755
translation,3,143,model,model,has,pointer generator network ( pgn,model has pointer generator network ( pgn,0.5929471254348755
translation,3,171,results,vanilla seq2seq with attention model,perform,rather poorly,vanilla seq2seq with attention model perform rather poorly,0.5829979777336121
translation,3,171,results,rather poorly,on,two datasets,rather poorly on two datasets,0.4891817271709442
translation,3,171,results,results,has,vanilla seq2seq with attention model,results has vanilla seq2seq with attention model,0.5287020802497864
translation,3,174,results,seq2seq-local&global and topic- graphsum,show,promising results,seq2seq-local&global and topic- graphsum show promising results,0.6412748098373413
translation,3,174,results,promising results,on,two datasets,promising results on two datasets,0.49200472235679626
translation,3,174,results,results,observe,seq2seq-local&global and topic- graphsum,results observe seq2seq-local&global and topic- graphsum,0.6031613349914551
translation,3,177,results,all the baselines,on,two datasets,all the baselines on two datasets,0.48764243721961975
translation,3,177,results,our two models,has,substantially outperform,our two models has substantially outperform,0.5695672035217285
translation,3,177,results,substantially outperform,has,all the baselines,substantially outperform has all the baselines,0.5840393304824829
translation,3,177,results,results,has,our two models,results has our two models,0.5413253903388977
translation,3,182,results,incorporation,of,discourse information,incorporation of discourse information,0.604419469833374
translation,3,182,results,discourse information,brings,no substantial performance gain,discourse information brings no substantial performance gain,0.5851117968559265
translation,3,182,results,no substantial performance gain,for,our model,no substantial performance gain for our model,0.6184155344963074
translation,3,182,results,results,shows that,incorporation,results shows that incorporation,0.5705166459083557
translation,4,162,ablation-analysis,low mse,with,full,low mse with full,0.6734412908554077
translation,4,162,ablation-analysis,first component,has,decision faithfulness,first component has decision faithfulness,0.5859485864639282
translation,4,162,ablation-analysis,ablation analysis,has,first component,ablation analysis has first component,0.5184145569801331
translation,4,162,ablation-analysis,ablation analysis,has,decision faithfulness,ablation analysis has decision faithfulness,0.5476465821266174
translation,4,103,baselines,baselines,has,text -only summarization baselines,baselines has text -only summarization baselines,0.5564929842948914
translation,4,105,baselines,extractive summarization method,with,hierarchical encoders,extractive summarization method with hierarchical encoders,0.6221625208854675
translation,4,107,baselines,bart,is,seq2seq model,bart is seq2seq model,0.6191695928573608
translation,4,107,baselines,seq2seq model,trained with,denoising objective,seq2seq model trained with denoising objective,0.727066159248352
translation,4,107,baselines,denoising objective,has,"lewis et al. , 2020","denoising objective has lewis et al. , 2020",0.5699503421783447
translation,4,107,baselines,baselines,has,bart,baselines has bart,0.5961952805519104
translation,4,108,baselines,bartlarge- cnn model,fine-tuned on,cnn / dm,bartlarge- cnn model fine-tuned on cnn / dm,0.7158180475234985
translation,4,108,baselines,random sentences,from,input reviews,random sentences from input reviews,0.5504430532455444
translation,4,148,experimental-setup,decsum,set,k = 15,decsum set k = 15,0.7229236364364624
translation,4,148,experimental-setup,k = 15,in,beam search,k = 15 in beam search,0.5497692227363586
translation,4,148,experimental-setup,experimental setup,For,decsum,experimental setup For decsum,0.6160829067230225
translation,4,7,model,predictive model,makes,decision,predictive model makes decision,0.673696756362915
translation,4,7,model,decision,based on,full text,decision based on full text,0.5723475813865662
translation,4,7,model,decision,based on,text,decision based on text,0.7437611222267151
translation,4,7,model,decision,to provide,valuable insights,decision to provide valuable insights,0.6676899194717407
translation,4,7,model,decision,be inferred from,text,decision be inferred from text,0.7189095616340637
translation,4,7,model,model,leverage,predictive model,model leverage predictive model,0.7724384665489197
translation,4,10,results,model- based explanation methods,in,decision faithfulness and representativeness,model- based explanation methods in decision faithfulness and representativeness,0.4964247941970825
translation,4,10,results,decsum,has,substantially outperforms,decsum has substantially outperforms,0.6272405385971069
translation,4,10,results,substantially outperforms,has,text-only summarization methods,substantially outperforms has text-only summarization methods,0.5776442885398865
translation,4,10,results,results,has,decsum,results has decsum,0.5870840549468994
translation,4,34,results,automatic metrics,demonstrate,our method ( decsum ),automatic metrics demonstrate our method ( decsum ),0.6098316311836243
translation,4,34,results,model - based explanation methods,in,decision faithfulness,model - based explanation methods in decision faithfulness,0.5201510787010193
translation,4,34,results,model - based explanation methods,in,decision representativeness,model - based explanation methods in decision representativeness,0.48217594623565674
translation,4,34,results,our method ( decsum ),has,outperforms,our method ( decsum ) has outperforms,0.624053418636322
translation,4,34,results,outperforms,has,text-only summarization methods,outperforms has text-only summarization methods,0.575436532497406
translation,4,34,results,results,has,automatic metrics,results has automatic metrics,0.5785090327262878
translation,4,35,results,improves,at the cost of,grammaticality,improves at the cost of grammaticality,0.6245959997177124
translation,4,35,results,textual nonredundancy,over,baselines,textual nonredundancy over baselines,0.7144938707351685
translation,4,35,results,textual nonredundancy,at the cost of,grammaticality,textual nonredundancy at the cost of grammaticality,0.6602903604507446
translation,4,35,results,improves,has,textual nonredundancy,improves has textual nonredundancy,0.5727661848068237
translation,4,35,results,results,has,decsum,results has decsum,0.5870840549468994
translation,4,152,results,automatic metrics,show,decsum,automatic metrics show decsum,0.6670042276382446
translation,4,152,results,decsum,provides,better decision faithfulness,decsum provides better decision faithfulness,0.6647907495498657
translation,4,152,results,decsum,provides,decision representativeness,decsum provides decision representativeness,0.6332105994224548
translation,4,152,results,decsum,provides,textual non-redundancy,decsum provides textual non-redundancy,0.6613560318946838
translation,4,152,results,decsum,sacrifices,other text-only qualities,decsum sacrifices other text-only qualities,0.7169639468193054
translation,4,152,results,better decision faithfulness,sacrifices,other text-only qualities,better decision faithfulness sacrifices other text-only qualities,0.7019153237342834
translation,4,152,results,textual non-redundancy,than,other baselines,textual non-redundancy than other baselines,0.5333072543144226
translation,4,152,results,results,has,automatic metrics,results has automatic metrics,0.5785090327262878
translation,4,158,results,decsum,with,all components,decsum with all components,0.6940704584121704
translation,4,158,results,much better faithfulness,than,any of other baselines,much better faithfulness than any of other baselines,0.5997002124786377
translation,4,158,results,much better faithfulness,close to,0,much better faithfulness close to 0,0.782953679561615
translation,4,158,results,results,shows,decsum,results shows decsum,0.717586100101471
translation,4,159,results,text-only summarization methods,have,mse,text-only summarization methods have mse,0.5586479306221008
translation,4,159,results,mse,with,full,mse with full,0.6955181360244751
translation,4,159,results,mse,with,of about 0.34,mse with of about 0.34,0.6497535109519958
translation,4,159,results,of about 0.34,more than 100 times as much as,decsum,of about 0.34 more than 100 times as much as decsum,0.7156140208244324
translation,4,159,results,full,has,of about 0.34,full has of about 0.34,0.6115736961364746
translation,4,159,results,results,has,text-only summarization methods,results has text-only summarization methods,0.5049077868461609
translation,4,160,results,model - based explanation methods,lead to,even poorer faithfulness,model - based explanation methods lead to even poorer faithfulness,0.6930100917816162
translation,4,160,results,even poorer faithfulness,than,text-only methods,even poorer faithfulness than text-only methods,0.5787426829338074
translation,4,160,results,results,has,model - based explanation methods,results has model - based explanation methods,0.48715850710868835
translation,4,163,results,textual nonredundancy,improves,mse,textual nonredundancy improves mse,0.7013840079307556
translation,4,163,results,mse,with,full,mse with full,0.6955181360244751
translation,4,163,results,full,over,optimizing decision faithfulness alone,full over optimizing decision faithfulness alone,0.736237645149231
translation,4,163,results,results,has,textual nonredundancy,results has textual nonredundancy,0.5802367925643921
translation,4,171,results,full text,of,all ten reviews,full text of all ten reviews,0.5406894087791443
translation,4,171,results,full text,achieves,best mse,full text achieves best mse,0.6601194143295288
translation,4,171,results,best mse,compared to,summarization methods,best mse compared to summarization methods,0.6306828856468201
translation,4,171,results,results,using,full text,results using full text,0.55547034740448
translation,4,173,results,decsum,leads to,best performance,decsum leads to best performance,0.6843364834785461
translation,4,173,results,best performance,compared to,baseline models,best performance compared to baseline models,0.6292000412940979
translation,4,173,results,results,has,decsum,results has decsum,0.5870840549468994
translation,4,174,results,text-only summarization ( presumm and bart ),provides,similar performance,text-only summarization ( presumm and bart ) provides similar performance,0.6062572002410889
translation,4,174,results,similar performance,as,random,similar performance as random,0.6543458104133606
translation,4,174,results,outperforms,has,explanation methods ( ig and attention ),outperforms has explanation methods ( ig and attention ),0.5906460881233215
translation,4,174,results,results,has,text-only summarization ( presumm and bart ),results has text-only summarization ( presumm and bart ),0.5264645218849182
translation,4,177,results,decsum,is,significantly better,decsum is significantly better,0.5969606041908264
translation,4,177,results,significantly better,than,random,significantly better than random,0.6052526235580444
translation,4,177,results,significantly better,than,textonly summarization,significantly better than textonly summarization,0.5818946361541748
translation,4,177,results,significantly better,than,model - based explanation,significantly better than model - based explanation,0.5768453478813171
translation,4,177,results,results,shows,decsum,results shows decsum,0.717586100101471
translation,4,186,results,presumm and bart,select,positive sentences,presumm and bart select positive sentences,0.7059711813926697
translation,4,186,results,positive sentences,over,negative sentences,positive sentences over negative sentences,0.6234570741653442
translation,4,186,results,negative sentences,results in,less representative summary,negative sentences results in less representative summary,0.574602484703064
translation,4,186,results,potentially mislead,has,human decision making,potentially mislead has human decision making,0.5506369471549988
translation,4,186,results,results,has,presumm and bart,results has presumm and bart,0.5531893968582153
translation,4,189,results,decsum,achieves,strong textual non-redundancy,decsum achieves strong textual non-redundancy,0.6653780937194824
translation,4,189,results,strong textual non-redundancy,has,0.760,strong textual non-redundancy has 0.760,0.561164915561676
translation,4,190,results,presumm,achieves,worst non-redundancy,presumm achieves worst non-redundancy,0.7069358229637146
translation,4,190,results,worst non-redundancy,among,baselines,worst non-redundancy among baselines,0.6155918836593628
translation,4,190,results,results,has,presumm,results has presumm,0.5946120619773865
translation,4,191,results,explanation methods ( ig and attention ),provide,worse non-redundancy,explanation methods ( ig and attention ) provide worse non-redundancy,0.5929635167121887
translation,4,191,results,worse non-redundancy,than,decsum,worse non-redundancy than decsum,0.6256250143051147
translation,4,191,results,results,has,explanation methods ( ig and attention ),results has explanation methods ( ig and attention ),0.5042638182640076
translation,4,192,results,decsum,leads to,inferior performance,decsum leads to inferior performance,0.7067424058914185
translation,4,192,results,inferior performance,based on,other text-only evaluation metrics,inferior performance based on other text-only evaluation metrics,0.604911208152771
translation,4,192,results,other text-only evaluation metrics,such as,grammaticality and coherence,other text-only evaluation metrics such as grammaticality and coherence,0.5710880160331726
translation,4,192,results,results,has,decsum,results has decsum,0.5870840549468994
translation,4,193,results,textual non-redundancy,improves,grammaticality and coherence,textual non-redundancy improves grammaticality and coherence,0.6717702746391296
translation,4,193,results,grammaticality and coherence,compared to,"( 1 , 1 , 0 )","grammaticality and coherence compared to ( 1 , 1 , 0 )",0.6400131583213806
translation,4,193,results,results,has,textual non-redundancy,results has textual non-redundancy,0.5821229815483093
translation,4,194,results,coherence,into,account,coherence into account,0.6127086877822876
translation,4,194,results,coherence,than,text-only summarization methods,coherence than text-only summarization methods,0.5347800254821777
translation,4,194,results,better coherence,than,text-only summarization methods,better coherence than text-only summarization methods,0.5712071061134338
translation,4,198,results,decsum,is,best summarization method,decsum is best summarization method,0.570055365562439
translation,4,198,results,best summarization method,with,accuracy,best summarization method with accuracy,0.5889449119567871
translation,4,198,results,accuracy,of,76.1 %,accuracy of 76.1 %,0.551650881767273
translation,4,198,results,results,has,decsum,results has decsum,0.5870840549468994
translation,4,199,results,presumm,achieves,above 60 %,presumm achieves above 60 %,0.6947222352027893
translation,4,199,results,above 60 %,in,simplified task,above 60 % in simplified task,0.506996750831604
translation,4,199,results,our baselines,has,presumm,our baselines has presumm,0.6502454876899719
translation,4,199,results,results,Among,our baselines,results Among our baselines,0.5942517518997192
translation,5,14,baselines,documents and summaries,from,systematic literature reviews,documents and summaries from systematic literature reviews,0.5150074362754822
translation,5,6,experiments,20 k summaries,derived from,scientific literature,20 k summaries derived from scientific literature,0.6411691308021545
translation,5,20,experiments,multi-document summarization dataset,in,biomedical domain,multi-document summarization dataset in biomedical domain,0.475447416305542
translation,5,20,experiments,ms? 2,has,multi-document summarization dataset,ms? 2 has multi-document summarization dataset,0.5686963796615601
translation,5,9,model,summarization inputs and targets,in,free text and structured forms,summarization inputs and targets in free text and structured forms,0.4780968725681305
translation,5,9,model,summarization inputs and targets,modify,recently proposed metric,summarization inputs and targets modify recently proposed metric,0.6294551491737366
translation,5,9,model,recently proposed metric,to assess,quality,recently proposed metric to assess quality,0.6955046057701111
translation,5,15,model,ms?2,expand,mds,ms?2 expand mds,0.6860030889511108
translation,5,15,model,mds,to,biomedical domain,mds to biomedical domain,0.560534656047821
translation,5,15,model,model,expand,mds,model expand mds,0.7508953213691711
translation,6,131,ablation-analysis,transformer,receives,more votes,transformer receives more votes,0.7598435282707214
translation,6,131,ablation-analysis,more votes,regarding,saliency,more votes regarding saliency,0.6003265380859375
translation,6,131,ablation-analysis,wikihow dataset,has,transformer,wikihow dataset has transformer,0.6133559346199036
translation,6,131,ablation-analysis,ablation analysis,On,wikihow dataset,ablation analysis On wikihow dataset,0.5468220114707947
translation,6,181,ablation-analysis,tp-transformer 's advantage,becomes,larger,tp-transformer 's advantage becomes larger,0.6899504661560059
translation,6,181,ablation-analysis,larger,in,last three layers,larger in last three layers,0.5807709097862244
translation,6,181,ablation-analysis,last three layers,except for,final layer,last three layers except for final layer,0.6519694924354553
translation,6,181,ablation-analysis,final layer,in,srl task,final layer in srl task,0.5285536646842957
translation,6,181,ablation-analysis,ablation analysis,observe,tp-transformer 's advantage,ablation analysis observe tp-transformer 's advantage,0.6204404830932617
translation,6,190,ablation-analysis,different intermediate representations,of,tp-transformer and the transformer,different intermediate representations of tp-transformer and the transformer,0.5942522883415222
translation,6,190,ablation-analysis,different intermediate representations,show,compositional tpr,different intermediate representations show compositional tpr,0.6493263244628906
translation,6,190,ablation-analysis,different intermediate representations,having,compositional tpr,different intermediate representations having compositional tpr,0.6740865111351013
translation,6,190,ablation-analysis,compositional tpr,results in,more interpretable final representations,compositional tpr results in more interpretable final representations,0.5916100740432739
translation,6,190,ablation-analysis,more interpretable final representations,at,every layer,more interpretable final representations at every layer,0.5396271347999573
translation,6,190,ablation-analysis,every layer,regarding,syntactic features,every layer regarding syntactic features,0.5974260568618774
translation,6,190,ablation-analysis,syntactic features,of,next word to be generated,syntactic features of next word to be generated,0.5544776916503906
translation,6,190,ablation-analysis,ablation analysis,probing,different intermediate representations,ablation analysis probing different intermediate representations,0.7994404435157776
translation,6,120,experiments,cnn / daily mail,is,least abstractive one,cnn / daily mail is least abstractive one,0.621207594871521
translation,6,120,experiments,least abstractive one,among,four datasets,least abstractive one among four datasets,0.5922834277153015
translation,6,110,hyperparameters,tp-transformers,have,6 layers,tp-transformers have 6 layers,0.613625705242157
translation,6,110,hyperparameters,6 layers,dimension per head,d k = 64,6 layers dimension per head d k = 64,0.6955153346061707
translation,6,110,hyperparameters,d f = 2048,for,encoder and decoder,d f = 2048 for encoder and decoder,0.6457849144935608
translation,6,110,hyperparameters,feedforward dimension,has,d f = 2048,feedforward dimension has d f = 2048,0.6035020351409912
translation,6,111,hyperparameters,tp-transformer,with,discrete roles,tp-transformer with discrete roles,0.6614450216293335
translation,6,111,hyperparameters,n r =50 role embeddings,of,dimension d r = 64,n r =50 role embeddings of dimension d r = 64,0.6202762126922607
translation,6,111,hyperparameters,dimension d r = 64,at,every layer,dimension d r = 64 at every layer,0.5558546185493469
translation,6,111,hyperparameters,discrete roles,has,n r =50 role embeddings,discrete roles has n r =50 role embeddings,0.5638039708137512
translation,6,111,hyperparameters,hyperparameters,has,tp-transformer,hyperparameters has tp-transformer,0.57990962266922
translation,6,112,hyperparameters,all three models from scratch,using,adafactor optimizer,all three models from scratch using adafactor optimizer,0.6553466320037842
translation,6,112,hyperparameters,adafactor optimizer,with,square root learning rate decay,adafactor optimizer with square root learning rate decay,0.6077343225479126
translation,6,112,hyperparameters,adafactor optimizer,with,dropout rate,adafactor optimizer with dropout rate,0.58604896068573
translation,6,112,hyperparameters,dropout rate,of,0.1,dropout rate of 0.1,0.5755324363708496
translation,6,112,hyperparameters,hyperparameters,train,all three models from scratch,hyperparameters train all three models from scratch,0.6543642282485962
translation,6,5,model,model,adapt,"tp-transformer ( schlag et al. , 2019 )","model adapt tp-transformer ( schlag et al. , 2019 )",0.8000944256782532
translation,6,6,model,key feature,is,structural bias,key feature is structural bias,0.5363231301307678
translation,6,6,model,structural bias,by encoding,two separate representations,structural bias by encoding two separate representations,0.7404170632362366
translation,6,6,model,two separate representations,for,each token,two separate representations for each token,0.6544166803359985
translation,6,6,model,each token,to represent,syntactic structure,each token to represent syntactic structure,0.6297868490219116
translation,6,6,model,syntactic structure,with,role vectors ),syntactic structure with role vectors ),0.635883629322052
translation,6,6,model,semantic content,with,filler vectors ),semantic content with filler vectors ),0.6738272309303284
translation,6,6,model,model,is,structural bias,model is structural bias,0.5511296391487122
translation,6,6,model,model,has,key feature,model has key feature,0.5533388257026672
translation,6,26,model,new types of computational primitives,for,transformers,new types of computational primitives for transformers,0.6150803565979004
translation,6,26,model,new types of computational primitives,are,explicitly -compositional vector embeddings of symbolic structures,new types of computational primitives are explicitly -compositional vector embeddings of symbolic structures,0.579370379447937
translation,6,26,model,model,investigate,new types of computational primitives,model investigate new types of computational primitives,0.6428452730178833
translation,6,27,model,tensor product representation,encodes,constituent,tensor product representation encodes constituent,0.7217137813568115
translation,6,27,model,tensor product representation,encodes,filler,tensor product representation encodes filler,0.6871230602264404
translation,6,27,model,constituent,in,symbolic structure,constituent in symbolic structure,0.5160226225852966
translation,6,27,model,constituent,as,composite,constituent as composite,0.5833832025527954
translation,6,27,model,composite,of,role,composite of role,0.6526533961296082
translation,6,27,model,filler,content of,"constituent ( e.g. , the meaning of a word","filler content of constituent ( e.g. , the meaning of a word",0.6599788665771484
translation,6,27,model,model,has,tensor product representation,model has tensor product representation,0.5146547555923462
translation,6,31,model,role vectors,by attending to,learned dictionary,role vectors by attending to learned dictionary,0.6197575330734253
translation,6,31,model,learned dictionary,of,role embeddings,learned dictionary of role embeddings,0.4937562942504883
translation,6,31,model,model,generates,role vectors,model generates role vectors,0.6772572994232178
translation,6,60,model,output vectors,as,fillers,output vectors as fillers,0.5538854002952576
translation,6,60,model,role vectors,to construct,tensor product representation,role vectors to construct tensor product representation,0.6710538268089294
translation,6,60,model,tensor product representation,passed through,feed -forward network,tensor product representation passed through feed -forward network,0.694726824760437
translation,6,60,model,feed -forward network,to yield,final states,feed -forward network to yield final states,0.6950920820236206
translation,6,35,results,tp-transformer,achieves,competitive performance,tp-transformer achieves competitive performance,0.6993061900138855
translation,6,35,results,significantly outperforms,with,continuous roles,significantly outperforms with continuous roles,0.7035446763038635
translation,6,35,results,tp-transformer,with,continuous roles,tp-transformer with continuous roles,0.6748427748680115
translation,6,35,results,continuous roles,on,"xsum ( narayan et al. , 2018 )","continuous roles on xsum ( narayan et al. , 2018 )",0.5289031863212585
translation,6,35,results,continuous roles,on,"wikihow ( koupaee and wang , 2018 )","continuous roles on wikihow ( koupaee and wang , 2018 )",0.5299362540245056
translation,6,35,results,continuous roles,on,"arxiv ( cohan et al. , 2018 ) datasets","continuous roles on arxiv ( cohan et al. , 2018 ) datasets",0.4901271462440491
translation,6,35,results,competitive performance,on,cnn / daily mail (,competitive performance on cnn / daily mail (,0.5300636291503906
translation,6,35,results,tp-transformer,has,significantly outperforms,tp-transformer has significantly outperforms,0.6220341920852661
translation,6,35,results,significantly outperforms,has,standard transformer,significantly outperforms has standard transformer,0.584766685962677
translation,6,35,results,results,has,tp-transformer,results has tp-transformer,0.5620753169059753
translation,6,41,results,compositional tpr,results in,more interpretable final representation,compositional tpr results in more interpretable final representation,0.5718905329704285
translation,6,41,results,more interpretable final representation,than,original transformer,more interpretable final representation than original transformer,0.6130233407020569
translation,6,41,results,original transformer,has,every layer,original transformer has every layer,0.593831479549408
translation,6,41,results,every layer,regarding,syntactic features,every layer regarding syntactic features,0.5974260568618774
translation,6,41,results,syntactic features,of,next word to be generated,syntactic features of next word to be generated,0.5544776916503906
translation,6,41,results,results,having,compositional tpr,results having compositional tpr,0.6279318332672119
translation,6,116,results,original transformer,on,all metrics,original transformer on all metrics,0.5112496018409729
translation,6,116,results,"xsum , arxiv , and wikihow datasets",has,our tp-transformer ( tpt -d ),"xsum , arxiv , and wikihow datasets has our tp-transformer ( tpt -d )",0.5806971788406372
translation,6,116,results,our tp-transformer ( tpt -d ),has,outperforms,our tp-transformer ( tpt -d ) has outperforms,0.6229627132415771
translation,6,116,results,outperforms,has,original transformer,outperforms has original transformer,0.6301623582839966
translation,6,117,results,both models,obtain,similar performance,both models obtain similar performance,0.5963948965072632
translation,6,117,results,similar performance,across,all metrics,similar performance across all metrics,0.6777200698852539
translation,6,117,results,cnn / daily mail dataset,has,both models,cnn / daily mail dataset has both models,0.5282748937606812
translation,6,117,results,results,On,cnn / daily mail dataset,results On cnn / daily mail dataset,0.4994055926799774
translation,6,118,results,tpt -c model,is,worst,tpt -c model is worst,0.6306116580963135
translation,6,118,results,excels,on,mathematics dataset,excels on mathematics dataset,0.5486623048782349
translation,6,118,results,worst,among,three models,worst among three models,0.6777657270431519
translation,6,118,results,every dataset,has,tpt -c model,every dataset has tpt -c model,0.5951233506202698
translation,6,118,results,results,On,every dataset,results On every dataset,0.5593622326850891
translation,6,129,results,summaries,generated by,tp-transformer,summaries generated by tp-transformer,0.7308077812194824
translation,6,129,results,summaries,significantly better in,grammar,summaries significantly better in grammar,0.6240431070327759
translation,6,129,results,xsum dataset,has,summaries,xsum dataset has summaries,0.5635121464729309
translation,6,129,results,results,on,xsum dataset,results on xsum dataset,0.569536030292511
translation,6,171,results,increasing scores,as,representations,increasing scores as representations,0.6135141253471375
translation,6,171,results,representations,closer to,final step,representations closer to final step,0.7350490689277649
translation,6,171,results,final step,of computing,distribution,final step of computing distribution,0.5530225038528442
translation,6,171,results,distribution,over,vocabulary,distribution over vocabulary,0.6993670463562012
translation,6,178,results,final representations,of,tp-transformer,final representations of tp-transformer,0.6257621645927429
translation,6,178,results,final representations,achieve,higher f1 scores,final representations achieve higher f1 scores,0.5779770612716675
translation,6,178,results,transformer,in,last three layers,transformer in last three layers,0.5620033144950867
translation,6,178,results,results,has,final representations,results has final representations,0.5558505654335022
translation,6,180,results,dependency labeling ( dep ) and semantic role labeling ( srl ) tasks,observe,our tp-transformer 's final representations,dependency labeling ( dep ) and semantic role labeling ( srl ) tasks observe our tp-transformer 's final representations,0.5980514287948608
translation,6,180,results,our tp-transformer 's final representations,consistently beat,transformer,our tp-transformer 's final representations consistently beat transformer,0.7809663414955139
translation,6,180,results,transformer,across,all layers,transformer across all layers,0.7607484459877014
translation,6,180,results,results,consider,dependency labeling ( dep ) and semantic role labeling ( srl ) tasks,results consider dependency labeling ( dep ) and semantic role labeling ( srl ) tasks,0.5704604387283325
translation,6,186,results,transformer 's filler-only representation,at,every layer,transformer 's filler-only representation at every layer,0.5583196878433228
translation,6,186,results,strongly outperforms,has,transformer 's filler-only representation,strongly outperforms has transformer 's filler-only representation,0.6039237380027771
translation,7,102,baselines,unsupervised neural machine translation,to train,qg model,unsupervised neural machine translation to train qg model,0.6724783778190613
translation,7,102,baselines,dependency trees,to generate,questions,dependency trees to generate questions,0.655238151550293
translation,7,102,baselines,cited documents,as,passages,cited documents as passages,0.548892617225647
translation,7,9,experimental-setup,freely available news summary data,transforming,declarative summary sentences,freely available news summary data transforming declarative summary sentences,0.6625398993492126
translation,7,9,experimental-setup,declarative summary sentences,into,appropriate questions,declarative summary sentences into appropriate questions,0.5687862038612366
translation,7,9,experimental-setup,appropriate questions,using,heuristics,appropriate questions using heuristics,0.7400257587432861
translation,7,9,experimental-setup,heuristics,informed by,dependency parsing,heuristics informed by dependency parsing,0.6851568818092346
translation,7,9,experimental-setup,heuristics,informed by,named entity recognition,heuristics informed by named entity recognition,0.6687888503074646
translation,7,9,experimental-setup,heuristics,informed by,semantic role labeling,heuristics informed by semantic role labeling,0.6842511296272278
translation,7,9,experimental-setup,experimental setup,use of,freely available news summary data,experimental setup use of freely available news summary data,0.6098079085350037
translation,7,32,experiments,qg model,to generate,synthetic qa data,qg model to generate synthetic qa data,0.6947221159934998
translation,7,32,experiments,synthetic qa data,to train,qa model,synthetic qa data to train qa model,0.6843710541725159
translation,7,32,experiments,qa model,in,unsupervised setting,qa model in unsupervised setting,0.5138639211654663
translation,7,87,hyperparameters,qg model,employ,bart - base,qg model employ bart - base,0.5511183738708496
translation,7,87,hyperparameters,qg model,is,bart - base,qg model is bart - base,0.5878439545631409
translation,7,87,hyperparameters,hyperparameters,has,qg model,hyperparameters has qg model,0.5291590690612793
translation,7,88,hyperparameters,qg model,on,qg data,qg model on qg data,0.5884768962860107
translation,7,88,hyperparameters,qg model,for,3 epochs,qg model for 3 epochs,0.6555938720703125
translation,7,88,hyperparameters,qg model,with,learning rate,qg model with learning rate,0.5997630953788757
translation,7,88,hyperparameters,qg model,using,adamw optimizer,qg model using adamw optimizer,0.6598852276802063
translation,7,88,hyperparameters,qg data,for,3 epochs,qg data for 3 epochs,0.6012498140335083
translation,7,88,hyperparameters,3 epochs,with,learning rate,3 epochs with learning rate,0.6580738425254822
translation,7,88,hyperparameters,learning rate,of,3 ? 10 ?5,learning rate of 3 ? 10 ?5,0.652306854724884
translation,7,88,hyperparameters,hyperparameters,train,qg model,hyperparameters train qg model,0.6870105266571045
translation,7,98,hyperparameters,document length and stride length,are,364 and 128,document length and stride length are 364 and 128,0.5905517339706421
translation,7,98,hyperparameters,learning rate,set to,1 ? 10 ?5,learning rate set to 1 ? 10 ?5,0.7357043623924255
translation,7,98,hyperparameters,unsupervised qa,are,exact match ( em ),unsupervised qa are exact match ( em ),0.5810995697975159
translation,7,98,hyperparameters,unsupervised qa,are,f - 1 score,unsupervised qa are f - 1 score,0.5865723490715027
translation,7,98,hyperparameters,hyperparameters,has,document length and stride length,hyperparameters has document length and stride length,0.5130524635314941
translation,7,98,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,7,8,model,unsupervised qg method,uses,questions,unsupervised qg method uses questions,0.6461511850357056
translation,7,8,model,questions,generated,heuristically,questions generated heuristically,0.720017671585083
translation,7,8,model,heuristically,from,summaries,heuristically from summaries,0.6641649007797241
translation,7,8,model,summaries,source of,training data,summaries source of training data,0.6436200141906738
translation,7,8,model,training data,for,qg system,training data for qg system,0.6470000743865967
translation,7,8,model,model,propose,unsupervised qg method,model propose unsupervised qg method,0.6688835024833679
translation,7,22,model,new unsupervised approach,frames,qg,new unsupervised approach frames qg,0.7442396283149719
translation,7,22,model,qg,as,summarizationquestioning process,qg as summarizationquestioning process,0.5424497127532959
translation,7,22,model,model,propose,new unsupervised approach,model propose new unsupervised approach,0.7225928902626038
translation,7,23,model,freely available summary data,apply,dependency parsing,freely available summary data apply dependency parsing,0.6294867396354675
translation,7,23,model,freely available summary data,apply,named entity recognition,freely available summary data apply named entity recognition,0.5740895867347717
translation,7,23,model,freely available summary data,apply,semantic role labeling,freely available summary data apply semantic role labeling,0.5996688604354858
translation,7,23,model,semantic role labeling,to,summaries,semantic role labeling to summaries,0.5450180768966675
translation,7,23,model,questions,based on,parsed summaries,questions based on parsed summaries,0.6509854793548584
translation,7,23,model,dependency parsing,has,named entity recognition,dependency parsing has named entity recognition,0.4794151186943054
translation,7,23,model,model,employing,freely available summary data,model employing freely available summary data,0.6335984468460083
translation,7,103,results,models,fine-tuned on,correspond - ing training sets,models fine-tuned on correspond - ing training sets,0.6845040917396545
translation,7,105,results,performance,of,one supervised model,performance of one supervised model,0.5884253978729248
translation,7,105,results,our proposed method,has,outperforms,our proposed method has outperforms,0.6310744285583496
translation,7,105,results,outperforms,has,all unsupervised baselines,outperforms has all unsupervised baselines,0.5897190570831299
translation,7,105,results,exceeds,has,performance,exceeds has performance,0.6372079253196716
translation,7,105,results,results,for,natural questions and triviaqa,results for natural questions and triviaqa,0.5823025703430176
translation,7,107,results,outperforms,obtaining,relative improvements,outperforms obtaining relative improvements,0.6708425283432007
translation,7,107,results,previous state - of - the - art unsupervised methods,by,substantial margin,previous state - of - the - art unsupervised methods by substantial margin,0.5643678307533264
translation,7,107,results,previous state - of - the - art unsupervised methods,obtaining,relative improvements,previous state - of - the - art unsupervised methods obtaining relative improvements,0.5792034268379211
translation,7,107,results,relative improvements,over,best unsupervised baseline model,relative improvements over best unsupervised baseline model,0.6431013941764832
translation,7,107,results,best unsupervised baseline model,of,47 %,best unsupervised baseline model of 47 %,0.5386427640914917
translation,7,107,results,best unsupervised baseline model,of,10 % f-1,best unsupervised baseline model of 10 % f-1,0.556998074054718
translation,7,107,results,best unsupervised baseline model,with respect to,10 % f-1,best unsupervised baseline model with respect to 10 % f-1,0.5990042686462402
translation,7,107,results,47 %,with respect to,em,47 % with respect to em,0.706517219543457
translation,7,107,results,10 % f-1,on,natural questions,10 % f-1 on natural questions,0.5203707218170166
translation,7,107,results,34 % em and 12 % f-1,on,triviaqa,34 % em and 12 % f-1 on triviaqa,0.5741708874702454
translation,7,107,results,our method,has,outperforms,our method has outperforms,0.6322360634803772
translation,7,107,results,outperforms,has,previous state - of - the - art unsupervised methods,outperforms has previous state - of - the - art unsupervised methods,0.5468583703041077
translation,7,107,results,results,has,our method,results has our method,0.5589964985847473
translation,7,108,results,our method,achieves,best performance,our method achieves best performance,0.6578624248504639
translation,7,108,results,best performance,both in terms of,em and f - 1 ),best performance both in terms of em and f - 1 ),0.7275009751319885
translation,7,108,results,best performance,both in terms of,three unsupervised models,best performance both in terms of three unsupervised models,0.6583202481269836
translation,7,108,results,best performance,out of,three unsupervised models,best performance out of three unsupervised models,0.6097716689109802
translation,7,108,results,em and f - 1 ),out of,three unsupervised models,em and f - 1 ) out of three unsupervised models,0.5959208011627197
translation,7,108,results,results,has,our method,results has our method,0.5589964985847473
translation,7,113,results,additionally outperforms,achieving,f1 improvements,additionally outperforms achieving f1 improvements,0.6490057110786438
translation,7,113,results,unsupervised baseline models,on,out-of- domain datasets,unsupervised baseline models on out-of- domain datasets,0.4925383925437927
translation,7,113,results,unsupervised baseline models,achieving,f1 improvements,unsupervised baseline models achieving f1 improvements,0.6177172064781189
translation,7,113,results,f1 improvements,over,previous state - of - the - art methods,f1 improvements over previous state - of - the - art methods,0.6083161234855652
translation,7,113,results,proposed method,has,additionally outperforms,proposed method has additionally outperforms,0.6220488548278809
translation,7,113,results,additionally outperforms,has,unsupervised baseline models,additionally outperforms has unsupervised baseline models,0.6025682687759399
translation,7,113,results,results,show,proposed method,results show proposed method,0.6722329258918762
translation,7,135,results,articles,as,passages,articles as passages,0.577997624874115
translation,7,135,results,lexical overlap,with,summarygenerated questions,lexical overlap with summarygenerated questions,0.6498166918754578
translation,7,135,results,summarygenerated questions,has,greatly improves,summarygenerated questions has greatly improves,0.6042221784591675
translation,7,135,results,greatly improves,has,qa performance,greatly improves has qa performance,0.5924198627471924
translation,7,135,results,results,using,articles,results using articles,0.5699619054794312
translation,7,136,results,outperforms,by,roughly 20 em points,outperforms by roughly 20 em points,0.6361545920372009
translation,7,136,results,outperforms,by,16 f - 1 points,outperforms by 16 f - 1 points,0.6444655060768127
translation,7,136,results,naive - qg,by,roughly 20 em points,naive - qg by roughly 20 em points,0.5998828411102295
translation,7,136,results,naive - qg,by,16 f - 1 points,naive - qg by 16 f - 1 points,0.6061923503875732
translation,7,136,results,summary - qg,has,outperforms,summary - qg has outperforms,0.613674521446228
translation,7,136,results,outperforms,has,naive - qg,outperforms has naive - qg,0.6106508374214172
translation,7,136,results,results,has,summary - qg,results has summary - qg,0.5836509466171265
translation,7,137,results,other heuristics,show,continuously improve,other heuristics show continuously improve,0.5979468822479248
translation,7,137,results,performance,especially,wh-movement,performance especially wh-movement,0.6447816491127014
translation,7,137,results,performance,especially,decomp - verb,performance especially decomp - verb,0.6890290975570679
translation,7,137,results,decomp - verb,make,questions,decomp - verb make questions,0.6881497502326965
translation,7,137,results,questions,in,qg data,questions in qg data,0.5212505459785461
translation,7,137,results,continuously improve,has,performance,continuously improve has performance,0.5828331708908081
translation,7,140,results,our synthetic data,allows,qa model,our synthetic data allows qa model,0.6980165243148804
translation,7,140,results,qa model,to achieve,competitive performance,qa model to achieve competitive performance,0.6971367597579956
translation,7,140,results,competitive performance,with,fewer than 20k examples,competitive performance with fewer than 20k examples,0.6397050619125366
translation,7,140,results,results,show,our synthetic data,results show our synthetic data,0.6290448904037476
translation,8,135,baselines,noteworthy utterances,use,two baselines,noteworthy utterances use two baselines,0.6381484270095825
translation,8,135,baselines,logistic regression,on,tf - idf utterance representations,logistic regression on tf - idf utterance representations,0.5531925559043884
translation,8,135,baselines,bidirectional lstm,to compute,token - averaged utterance representations,bidirectional lstm to compute token - averaged utterance representations,0.7169960737228394
translation,8,135,baselines,token - averaged utterance representations,followed by,linear classification layer,token - averaged utterance representations followed by linear classification layer,0.6397554874420166
translation,8,135,baselines,baselines,For predicting,noteworthy utterances,baselines For predicting noteworthy utterances,0.7206103205680847
translation,8,137,baselines,context,from,neighboring utterances,context from neighboring utterances,0.5761574506759644
translation,8,127,hyperparameters,randomnote,randomly and uniformly samples,soap note,randomnote randomly and uniformly samples soap note,0.7599825859069824
translation,8,127,hyperparameters,randomnote,outputs it as,summary,randomnote outputs it as summary,0.6287199258804321
translation,8,127,hyperparameters,soap note,from,training set,soap note from training set,0.5736632347106934
translation,8,127,hyperparameters,summary,for,any input conversation,summary for any input conversation,0.6256937384605408
translation,8,127,hyperparameters,hyperparameters,has,randomnote,hyperparameters has randomnote,0.5549558401107788
translation,8,6,model,first complete pipelines,to leverage,deep summarization models,first complete pipelines to leverage deep summarization models,0.6726340651512146
translation,8,6,model,deep summarization models,to generate,notes,deep summarization models to generate notes,0.6317831873893738
translation,8,6,model,notes,based on,transcripts of conversations,notes based on transcripts of conversations,0.6683590412139893
translation,8,6,model,transcripts of conversations,between,physicians and patients,transcripts of conversations between physicians and patients,0.6254639029502869
translation,8,6,model,model,introduce,first complete pipelines,model introduce first complete pipelines,0.6995400190353394
translation,8,7,model,important utterances,relevant to,each summary section,important utterances relevant to each summary section,0.7006939053535461
translation,8,7,model,one summary sentence,has,per cluster,one summary sentence has per cluster,0.6004244089126587
translation,8,7,model,model,propose,cluster2sent,model propose cluster2sent,0.6803352236747742
translation,8,17,model,first end-to - end methods,for generating,whole soap notes,first end-to - end methods for generating whole soap notes,0.6849677562713623
translation,8,17,model,whole soap notes,based on,clinical conversations,whole soap notes based on clinical conversations,0.6925798654556274
translation,8,17,model,model,introduce,first end-to - end methods,model introduce first end-to - end methods,0.6838994026184082
translation,8,18,model,unique corpus,consists of,thousands of,unique corpus consists of thousands of,0.6836509108543396
translation,8,18,model,transcripts,of,recorded clinical conversations,transcripts of recorded clinical conversations,0.5527871251106262
translation,8,18,model,recorded clinical conversations,together with,associated soap notes,recorded clinical conversations together with associated soap notes,0.6833115816116333
translation,8,18,model,associated soap notes,drafted by,work force,associated soap notes drafted by work force,0.5963093042373657
translation,8,18,model,work force,trained in,official style of soap note documentation,work force trained in official style of soap note documentation,0.6788643598556519
translation,8,18,model,thousands of,has,transcripts,thousands of has transcripts,0.5616406798362732
translation,8,18,model,model,builds on,unique corpus,model builds on unique corpus,0.6771591901779175
translation,8,23,model,progressively more work,from,abstractive,progressively more work from abstractive,0.5884696841239929
translation,8,23,model,abstractive,to,extractive component,abstractive to extractive component,0.5682359933853149
translation,8,23,model,summarization,on,end-to - end abstractive module,summarization on end-to - end abstractive module,0.5597584843635559
translation,8,104,model,bidirectional lstm - based encoderdecoder model,with,attention,bidirectional lstm - based encoderdecoder model with attention,0.6090034246444702
translation,8,104,model,model,is,bidirectional lstm - based encoderdecoder model,model is bidirectional lstm - based encoderdecoder model,0.5058935880661011
translation,8,105,model,pointer mechanism,to copy,tokens,pointer mechanism to copy tokens,0.5671055316925049
translation,8,105,model,tokens,directly from,input,tokens directly from input,0.5455116033554077
translation,8,105,model,model,employs,pointer mechanism,model employs pointer mechanism,0.6043812036514282
translation,8,8,results,cluster2sent,produces,significantly more factual and coherent sentences,cluster2sent produces significantly more factual and coherent sentences,0.623706579208374
translation,8,8,results,purely abstractive counterpart,by,8 rouge - 1 points,purely abstractive counterpart by 8 rouge - 1 points,0.580004096031189
translation,8,8,results,significantly more factual and coherent sentences,assessed by,expert human evaluators,significantly more factual and coherent sentences assessed by expert human evaluators,0.6753019690513611
translation,8,8,results,cluster2sent,has,outperforms,cluster2sent has outperforms,0.6523776054382324
translation,8,8,results,outperforms,has,purely abstractive counterpart,outperforms has purely abstractive counterpart,0.6189274191856384
translation,8,8,results,results,has,cluster2sent,results has cluster2sent,0.5458744168281555
translation,8,131,results,both baselines,give,similar performance,both baselines give similar performance,0.6078455448150635
translation,8,131,results,both baselines,outperformed by,simple conv2note approach,both baselines outperformed by simple conv2note approach,0.7365848422050476
translation,8,131,results,results,has,both baselines,results has both baselines,0.5172712802886963
translation,8,134,results,all algorithms,relying on,oracle noteworthy utterances,all algorithms relying on oracle noteworthy utterances,0.7008745670318604
translation,8,134,results,all algorithms,exhibit,monotonic and significant rise,all algorithms exhibit monotonic and significant rise,0.639133632183075
translation,8,134,results,monotonic and significant rise,in,rouge scores,monotonic and significant rise in rouge scores,0.5723257660865784
translation,8,134,results,monotonic and significant rise,move towards,extraction - heavy end,monotonic and significant rise move towards extraction - heavy end,0.7111194729804993
translation,8,134,results,extraction - heavy end,of,spectrum,extraction - heavy end of spectrum,0.6209511756896973
translation,8,134,results,oracle noteworthy utterances,has,outperform,oracle noteworthy utterances has outperform,0.6307518482208252
translation,8,134,results,outperform,has,conv2note,outperform has conv2note,0.6255813837051392
translation,8,134,results,results,has,all algorithms,results has all algorithms,0.5286640524864197
translation,8,138,results,latter two methods,perform,much better,latter two methods perform much better,0.5592606663703918
translation,8,138,results,much better,demonstrating,benefit,much better demonstrating benefit,0.6683920621871948
translation,8,138,results,benefit,of incorporating,neighboring context,benefit of incorporating neighboring context,0.6892853379249573
translation,8,138,results,neighboring context,with,bert - lstm,neighboring context with bert - lstm,0.6794646382331848
translation,8,138,results,bert - lstm,performing,best,bert - lstm performing best,0.6062764525413513
translation,8,138,results,results,has,latter two methods,results has latter two methods,0.5342047810554504
translation,8,139,results,predicted noteworthy utterances and clusters,instead of,oracle ones,predicted noteworthy utterances and clusters instead of oracle ones,0.6498861312866211
translation,8,139,results,predicted noteworthy utterances and clusters,leads to,drop,predicted noteworthy utterances and clusters leads to drop,0.7113029956817627
translation,8,139,results,drop,in,rouge scores,drop in rouge scores,0.5418292284011841
translation,8,139,results,performance,of,ext2sec and cluster2sent,performance of ext2sec and cluster2sent,0.6120505928993225
translation,8,139,results,ext2sec and cluster2sent,is,still better,ext2sec and cluster2sent is still better,0.6050370335578918
translation,8,139,results,still better,than,conv2note,still better than conv2note,0.628997266292572
translation,8,139,results,results,Using,predicted noteworthy utterances and clusters,results Using predicted noteworthy utterances and clusters,0.6540923118591309
translation,8,140,results,medical dataset,using,bert - lstm extractor,medical dataset using bert - lstm extractor,0.6532139778137207
translation,8,140,results,bert - lstm extractor,leads to,best performance,bert - lstm extractor leads to best performance,0.6370252370834351
translation,8,140,results,best performance,with,cluster2sent,best performance with cluster2sent,0.6528065800666809
translation,8,140,results,conv2note,by,about 8 points in rouge - 1,conv2note by about 8 points in rouge - 1,0.6380177140235901
translation,8,140,results,cluster2sent,has,outperforming,cluster2sent has outperforming,0.626916229724884
translation,8,140,results,outperforming,has,conv2note,outperforming has conv2note,0.5623785257339478
translation,8,140,results,results,For,medical dataset,results For medical dataset,0.6034293174743652
translation,8,141,results,t5 - small variant,achieves,similar performance,t5 - small variant achieves similar performance,0.6969411969184875
translation,8,141,results,similar performance,to,t5 - base,similar performance to t5 - base,0.571790874004364
translation,8,141,results,results,has,t5 - small variant,results has t5 - small variant,0.5494537949562073
translation,8,146,results,performance,of,bert - lstm,performance of bert - lstm,0.5810787081718445
translation,8,146,results,performance,is,better,performance is better,0.6231186985969543
translation,8,146,results,bert - lstm,is,better,bert - lstm is better,0.6138485670089722
translation,8,146,results,bert - lstm,when used in tandem with,abstractive module,bert - lstm when used in tandem with abstractive module,0.6496291160583496
translation,8,146,results,better,than,hlstm,better than hlstm,0.6535636186599731
translation,8,146,results,rouge scores,achieved by,overall pipeline,rouge scores achieved by overall pipeline,0.6213468909263611
translation,8,146,results,extractor,has,performance,extractor has performance,0.5590951442718506
translation,8,146,results,results,As,extractor,results As extractor,0.565392017364502
translation,8,147,results,results,observe,clustering heuristic,results observe clustering heuristic,0.6400427222251892
translation,8,155,results,models,trained on,clean dataset,models trained on clean dataset,0.714803159236908
translation,8,155,results,clean dataset,perform,worse,clean dataset perform worse,0.5677608847618103
translation,8,155,results,worse,on,10 % corrupted test dataset,worse on 10 % corrupted test dataset,0.5069238543510437
translation,8,155,results,results,has,models,results has models,0.5335168838500977
translation,8,157,results,our models,on,data,our models on data,0.6002437472343445
translation,8,157,results,our models,recover,much of the performance drop,our models recover much of the performance drop,0.6572992205619812
translation,8,157,results,data,corrupted at,10 % asr error rate,data corrupted at 10 % asr error rate,0.7566720843315125
translation,8,157,results,our models,recover,much of the performance drop,our models recover much of the performance drop,0.6572992205619812
translation,8,157,results,our models,has,our models,our models has our models,0.5481649041175842
translation,8,157,results,results,train,our models,results train our models,0.6721099019050598
translation,8,158,results,simulated asr errors,are,dialed,simulated asr errors are dialed,0.6406206488609314
translation,8,158,results,dialed,up to,30 % error rate,dialed up to 30 % error rate,0.6222751140594482
translation,8,158,results,dialed,see,smaller performance drop,dialed see smaller performance drop,0.6191397309303284
translation,8,158,results,smaller performance drop,for,cluster2sent,smaller performance drop for cluster2sent,0.6238345503807068
translation,8,158,results,cluster2sent,compared to,conv2note,cluster2sent compared to conv2note,0.6730369925498962
translation,8,195,results,human annotations,show,both cluster2sent - based methods,human annotations show both cluster2sent - based methods,0.5877113938331604
translation,8,195,results,both cluster2sent - based methods,produced,higher yield,both cluster2sent - based methods produced higher yield,0.6857209205627441
translation,8,195,results,higher yield,than,conv2note baseline,higher yield than conv2note baseline,0.6103315353393555
translation,8,195,results,results,has,human annotations,results has human annotations,0.507675051689148
translation,8,196,results,t5 - base,performs,better,t5 - base performs better,0.6739246249198914
translation,8,196,results,better,than,conditioned pointer -generator,better than conditioned pointer -generator,0.6136071085929871
translation,8,196,results,conditioned pointer -generator,as,abstractive module,conditioned pointer -generator as abstractive module,0.509934663772583
translation,8,196,results,abstractive module,in,cluster2sent setting,abstractive module in cluster2sent setting,0.5564305186271667
translation,8,196,results,results,has,t5 - base,results has t5 - base,0.5731919407844543
translation,9,141,ablation-analysis,pointer generator networks,effectively handle,named entities and outof-vocabulary words,pointer generator networks effectively handle named entities and outof-vocabulary words,0.7179846167564392
translation,9,141,ablation-analysis,coverage mechanism,avoid,repetitive generation,coverage mechanism avoid repetitive generation,0.7012261748313904
translation,9,141,ablation-analysis,ablation analysis,conclude that,pointer generator networks,ablation analysis conclude that pointer generator networks,0.62013840675354
translation,9,102,baselines,several modifications,of,seq2seq with attention,several modifications of seq2seq with attention,0.5459963083267212
translation,9,102,baselines,several modifications,of,seq2seq with attention,several modifications of seq2seq with attention,0.5459963083267212
translation,9,104,model,same sized hidden states,with,attention mechanism,same sized hidden states with attention mechanism,0.6390429139137268
translation,9,104,model,same sized hidden states,with,soft-max layer,same sized hidden states with soft-max layer,0.6371216773986816
translation,9,104,model,attention mechanism,over,source hidden states,attention mechanism over source hidden states,0.6417195796966553
translation,9,104,model,soft-max layer,over,vocabulary,soft-max layer over vocabulary,0.6744242906570435
translation,9,104,model,vocabulary,to generate,words,vocabulary to generate words,0.6403059959411621
translation,9,104,model,model,has,both the encoder and decoder,model has both the encoder and decoder,0.5658589005470276
translation,9,109,model,pointer - generator network,integrates,basic seq2seq model ( with attention ),pointer - generator network integrates basic seq2seq model ( with attention ),0.6693099141120911
translation,9,109,model,basic seq2seq model ( with attention ),with,copying mechanism,basic seq2seq model ( with attention ) with copying mechanism,0.6294796466827393
translation,9,109,model,model,use,pointer - generator network,model use pointer - generator network,0.6675338745117188
translation,9,10,results,seq2seq models,that adequately summarize,news articles,seq2seq models that adequately summarize news articles,0.6911131143569946
translation,9,10,results,results,observe,seq2seq models,results observe seq2seq models,0.5698755383491516
translation,9,129,results,beam search,shown to be,better,beam search shown to be better,0.6455022692680359
translation,9,129,results,better,generating,first sequence,better generating first sequence,0.7588692307472229
translation,9,129,results,results,has,beam search,results has beam search,0.5500618815422058
translation,9,138,results,non-covered words,in,output summary,non-covered words in output summary,0.5249845385551453
translation,9,138,results,all three approaches,has,abstractive,all three approaches has abstractive,0.6019176244735718
translation,9,139,results,coverage ( + cov ) mechanism,problem of,repetition,coverage ( + cov ) mechanism problem of repetition,0.7792860269546509
translation,9,139,results,repetition,in,summaries,repetition in summaries,0.5705008506774902
translation,9,139,results,results,by adding,coverage ( + cov ) mechanism,results by adding coverage ( + cov ) mechanism,0.7592329978942871
translation,9,140,results,rouge scores,show,improvement,rouge scores show improvement,0.6393654942512512
translation,9,140,results,improvement,after applying,coverage,improvement after applying coverage,0.7291444540023804
translation,9,140,results,coverage,to,pointer - generator networks,coverage to pointer - generator networks,0.5907654762268066
translation,9,140,results,results,has,rouge scores,results has rouge scores,0.541054368019104
translation,9,142,results,abstractive approach,shows,best results,abstractive approach shows best results,0.6770908236503601
translation,9,142,results,precision ( pr ),has,abstractive approach,precision ( pr ) has abstractive approach,0.5588440299034119
translation,9,142,results,results,in terms of,precision ( pr ),results in terms of precision ( pr ),0.6660932898521423
translation,9,143,results,results,has,recall ( re ),results has recall ( re ),0.5215449929237366
translation,10,167,ablation-analysis,semantic similarity,in,pre-training objectives,semantic similarity in pre-training objectives,0.47101670503616333
translation,10,167,ablation-analysis,semantic similarity,critical in,improving,semantic similarity critical in improving,0.7260704040527344
translation,10,167,ablation-analysis,improving,has,final summary,improving has final summary,0.5950397849082947
translation,10,167,ablation-analysis,ablation analysis,considering,semantic similarity,ablation analysis considering semantic similarity,0.6874343156814575
translation,10,48,baselines,arman,uses,sr,arman uses sr,0.567425012588501
translation,10,48,baselines,sr,as,pre-training methods,sr as pre-training methods,0.5508939027786255
translation,10,48,baselines,sr,change,order,sr change order,0.7991997599601746
translation,10,48,baselines,baselines,has,arman,baselines has arman,0.6195071339607239
translation,10,179,baselines,arman ( ss ) base,with,two different values,arman ( ss ) base with two different values,0.6856286525726318
translation,10,179,baselines,two different values,for,masking rate,two different values for masking rate,0.6465855240821838
translation,10,179,baselines,masking rate,in,tss objective,masking rate in tss objective,0.479917049407959
translation,10,179,baselines,arman ( ss - 80 ) base,masked,only 80 %,arman ( ss - 80 ) base masked only 80 %,0.8244726061820984
translation,10,179,baselines,arman ( ss - 80 ) base,left,other 20 %,arman ( ss - 80 ) base left other 20 %,0.7904599905014038
translation,10,179,baselines,only 80 %,of,important sentences,only 80 % of important sentences,0.5651702284812927
translation,10,179,baselines,other 20 %,in,input text,other 20 % in input text,0.5108950734138489
translation,10,179,baselines,unchanged,in,input text,unchanged in input text,0.5184937119483948
translation,10,179,baselines,arman ( ss - 100 ) base,masked,all of the important sentences,arman ( ss - 100 ) base masked all of the important sentences,0.7640530467033386
translation,10,179,baselines,all of the important sentences,without copying,sentences,all of the important sentences without copying sentences,0.5531262755393982
translation,10,179,baselines,sentences,from,input text,sentences from input text,0.541641116142273
translation,10,179,baselines,other 20 %,has,unchanged,other 20 % has unchanged,0.593354344367981
translation,10,157,experimental-setup,arman,contained,12 layer encoder,arman contained 12 layer encoder,0.559617817401886
translation,10,157,experimental-setup,arman,contained,12 layer decoder,arman contained 12 layer decoder,0.5930601358413696
translation,10,157,experimental-setup,arman,contained,12 self-attention heads,arman contained 12 self-attention heads,0.6166725754737854
translation,10,157,experimental-setup,12 layer decoder,with,768 embedding / hidden size,12 layer decoder with 768 embedding / hidden size,0.6466476917266846
translation,10,157,experimental-setup,12 layer decoder,with,3072 feed - forward filter size,12 layer decoder with 3072 feed - forward filter size,0.6322720050811768
translation,10,157,experimental-setup,12 layer decoder,with,12 self-attention heads,12 layer decoder with 12 self-attention heads,0.6451919674873352
translation,10,157,experimental-setup,experimental setup,pre-trained,arman,experimental setup pre-trained arman,0.7067707777023315
translation,10,158,experimental-setup,arman and pegasus,trained on,"( kudo , 2018 )","arman and pegasus trained on ( kudo , 2018 )",0.7781702876091003
translation,10,158,experimental-setup,arman and pegasus,to generate,vocabulary,arman and pegasus to generate vocabulary,0.7126307487487793
translation,10,158,experimental-setup,"( kudo , 2018 )",to generate,vocabulary,"( kudo , 2018 ) to generate vocabulary",0.7068463563919067
translation,10,158,experimental-setup,vocabulary,for,our models,vocabulary for our models,0.6821007132530212
translation,10,158,experimental-setup,experimental setup,has,arman and pegasus,experimental setup has arman and pegasus,0.5799646973609924
translation,10,162,experimental-setup,input length ( l input ),to,512,input length ( l input ) to 512,0.5648599863052368
translation,10,162,experimental-setup,output length,to,256,output length to 256,0.5454347133636475
translation,10,162,experimental-setup,experimental setup,set,input length ( l input ),experimental setup set input length ( l input ),0.6352999210357666
translation,10,162,experimental-setup,experimental setup,set,output length,experimental setup set output length,0.605212390422821
translation,10,163,experimental-setup,beam-search,as,wu et al . ( 2016 ),beam-search as wu et al . ( 2016 ),0.49335604906082153
translation,10,163,experimental-setup,beam-search,as,approach,beam-search as approach,0.594911515712738
translation,10,163,experimental-setup,beam-search,with,beam-size,beam-search with beam-size,0.6574042439460754
translation,10,163,experimental-setup,beam-search,with,length penalty,beam-search with length penalty,0.6431351900100708
translation,10,163,experimental-setup,approach,with,beam-size,approach with beam-size,0.6819643974304199
translation,10,163,experimental-setup,approach,with,length penalty,approach with length penalty,0.6554275155067444
translation,10,163,experimental-setup,beam-size,of,8,beam-size of 8,0.6946957111358643
translation,10,163,experimental-setup,length penalty,of,0.8,length penalty of 0.8,0.5940213799476624
translation,10,163,experimental-setup,wu et al . ( 2016 ),has,approach,wu et al . ( 2016 ) has approach,0.5507346987724304
translation,10,163,experimental-setup,experimental setup,used,beam-search,experimental setup used beam-search,0.5509057641029358
translation,10,290,experimental-setup,training steps,of,pre-training,training steps of pre-training,0.5675410032272339
translation,10,290,experimental-setup,training steps,set to,128 and 1 m,training steps set to 128 and 1 m,0.7565010786056519
translation,10,290,experimental-setup,pre-training,set to,128 and 1 m,pre-training set to 128 and 1 m,0.7206164598464966
translation,10,290,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,10,290,experimental-setup,experimental setup,has,training steps,experimental setup has training steps,0.5158189535140991
translation,10,291,experimental-setup,"adafactor ( shazeer and stern , 2018 )",with,square root learning rate decay,"adafactor ( shazeer and stern , 2018 ) with square root learning rate decay",0.597303032875061
translation,10,291,experimental-setup,"adafactor ( shazeer and stern , 2018 )",with,dropout rate,"adafactor ( shazeer and stern , 2018 ) with dropout rate",0.5930767059326172
translation,10,291,experimental-setup,dropout rate,of,0.1,dropout rate of 0.1,0.5755324363708496
translation,10,291,experimental-setup,dropout rate,used in,pre-training,dropout rate used in pre-training,0.620975136756897
translation,10,291,experimental-setup,0.1,used in,pre-training,0.1 used in pre-training,0.6085186004638672
translation,10,291,experimental-setup,0.1,used in,fine-tuning,0.1 used in fine-tuning,0.6030716896057129
translation,10,291,experimental-setup,experimental setup,has,"adafactor ( shazeer and stern , 2018 )","experimental setup has adafactor ( shazeer and stern , 2018 )",0.5129154920578003
translation,10,292,experimental-setup,pre-training experiments,carried out on,google colab platform,pre-training experiments carried out on google colab platform,0.668670117855072
translation,10,292,experimental-setup,google colab platform,with,tpu v2 -8,google colab platform with tpu v2 -8,0.6812644004821777
translation,10,292,experimental-setup,experimental setup,has,pre-training experiments,experimental setup has pre-training experiments,0.5359456539154053
translation,10,168,experiments,arman ( msr ) base,encouraged,model,arman ( msr ) base encouraged model,0.6699392199516296
translation,10,168,experiments,model,to learn,correct relative orders,model to learn correct relative orders,0.6418048143386841
translation,10,168,experiments,correct relative orders,between,sentences,correct relative orders between sentences,0.6288487315177917
translation,10,168,experiments,sentences,by,reordering,sentences by reordering,0.5814028978347778
translation,10,168,experiments,reordering,at,sentence level,reordering at sentence level,0.5357016324996948
translation,10,190,experiments,arman,uses,more novel words,arman uses more novel words,0.6563529372215271
translation,10,190,experiments,more novel words,compared to,pegasus,more novel words compared to pegasus,0.6958900690078735
translation,10,190,experiments,tebyan dataset,has,arman,tebyan dataset has arman,0.6054321527481079
translation,10,190,experiments,arman,has,higher density,arman has higher density,0.651059091091156
translation,10,213,experiments,our best- performing model,achieves,highest accuracy,our best- performing model achieves highest accuracy,0.6495936512947083
translation,10,213,experiments,highest accuracy,in,math and logic questions,highest accuracy in math and logic questions,0.45678064227104187
translation,10,213,experiments,multiple - choice qa,has,our best- performing model,multiple - choice qa has our best- performing model,0.5623912215232849
translation,10,6,model,arman,has,transformer - based encoderdecoder model,arman has transformer - based encoderdecoder model,0.5070163607597351
translation,10,6,model,model,propose,arman,model propose arman,0.6995408535003662
translation,10,24,model,summary,corresponding to,each document,summary corresponding to each document,0.6019799113273621
translation,10,24,model,summary,by selecting,important sentences,summary by selecting important sentences,0.6853808164596558
translation,10,24,model,each document,in,dataset,each document in dataset,0.5021604299545288
translation,10,24,model,important sentences,based on,semantic scores,important sentences based on semantic scores,0.5992656350135803
translation,10,24,model,semantic scores,in,self-supervised manner,semantic scores in self-supervised manner,0.5113968849182129
translation,10,24,model,model,prepare,summary,model prepare summary,0.6150245666503906
translation,10,25,model,three novel objectives,for,pre-training a seq2seq transformer,three novel objectives for pre-training a seq2seq transformer,0.5706619620323181
translation,10,25,model,model,propose,three novel objectives,model propose three novel objectives,0.7205378413200378
translation,10,26,model,arman,uses,transformer encoder-decoder structure,arman uses transformer encoder-decoder structure,0.6165223121643066
translation,10,26,model,arman,introduces,new combination of masking sentences,arman introduces new combination of masking sentences,0.6292789578437805
translation,10,26,model,new combination of masking sentences,with,sentence shuffling and reordering objectives,new combination of masking sentences with sentence shuffling and reordering objectives,0.6343849301338196
translation,10,26,model,model,introduces,new combination of masking sentences,model introduces new combination of masking sentences,0.6599558591842651
translation,10,26,model,model,has,arman,model has arman,0.5945211052894592
translation,10,58,model,new semantic - based approach,for selecting,important document sentences,new semantic - based approach for selecting important document sentences,0.6685474514961243
translation,10,58,model,important document sentences,to make,pseudo summary,important document sentences to make pseudo summary,0.5949050188064575
translation,10,58,model,model,introduce,new semantic - based approach,model introduce new semantic - based approach,0.6500794291496277
translation,10,84,model,transformer encoder-decoder structure,introduce,new combination of masking sentences,transformer encoder-decoder structure introduce new combination of masking sentences,0.6136025190353394
translation,10,84,model,new combination of masking sentences,plus,shuffling and reordering objectives,new combination of masking sentences plus shuffling and reordering objectives,0.6840407252311707
translation,10,84,model,model,use,transformer encoder-decoder structure,model use transformer encoder-decoder structure,0.6250584125518799
translation,10,156,model,model,based on,"transformer ( vaswani et al. , 2017 ) encoder-decoder structure","model based on transformer ( vaswani et al. , 2017 ) encoder-decoder structure",0.6425139307975769
translation,10,30,results,our models,generated,even better summaries,our models generated even better summaries,0.6758514046669006
translation,10,30,results,even better summaries,than,previous sota,even better summaries than previous sota,0.6307259798049927
translation,10,30,results,even better summaries,fine-tuned with,small number of document-summary pairs,even better summaries fine-tuned with small number of document-summary pairs,0.732206404209137
translation,10,30,results,results,has,our models,results has our models,0.5733726620674133
translation,10,166,results,pegasus base,in,all datasets,pegasus base in all datasets,0.5150707960128784
translation,10,166,results,even arman ( ss ) base,has,our basic proposed method,even arman ( ss ) base has our basic proposed method,0.6011103987693787
translation,10,166,results,even arman ( ss ) base,has,outperforms,even arman ( ss ) base has outperforms,0.6090893745422363
translation,10,166,results,our basic proposed method,has,outperforms,our basic proposed method has outperforms,0.6202875971794128
translation,10,166,results,outperforms,has,pegasus base,outperforms has pegasus base,0.6177080869674683
translation,10,169,results,reordering objective,gives,improvement,reordering objective gives improvement,0.6239661574363708
translation,10,169,results,improvement,in,summarization,improvement in summarization,0.5272501111030579
translation,10,169,results,results,show,reordering objective,results show reordering objective,0.6695547699928284
translation,10,170,results,arman ( sh ) base,does not help,improving,arman ( sh ) base does not help improving,0.660325288772583
translation,10,170,results,second model,has,arman ( sh ) base,second model has arman ( sh ) base,0.5717650055885315
translation,10,170,results,improving,has,quality of summaries,improving has quality of summaries,0.5448057055473328
translation,10,170,results,results,has,second model,results has second model,0.5045840740203857
translation,10,180,results,arman ( ss - 100 ) base,produces,higher rouge score,arman ( ss - 100 ) base produces higher rouge score,0.6331718564033508
translation,10,180,results,higher rouge score,do not consider,copying,higher rouge score do not consider copying,0.7574634552001953
translation,10,180,results,copying,in,pre-training objective,copying in pre-training objective,0.5234823226928711
translation,10,180,results,zero-shot setting,has,arman ( ss - 100 ) base,zero-shot setting has arman ( ss - 100 ) base,0.5955504179000854
translation,10,180,results,results,show,zero-shot setting,results show zero-shot setting,0.628065824508667
translation,10,180,results,results,in,zero-shot setting,results in zero-shot setting,0.5531973242759705
translation,10,182,results,arman ( ss - 100 ) base,performs,better,arman ( ss - 100 ) base performs better,0.6137655377388
translation,10,182,results,better,than,arman ( ss - 80 ),better than arman ( ss - 80 ),0.6117669343948364
translation,10,182,results,arman ( ss - 80 ),before and after,fine-tuning,arman ( ss - 80 ) before and after fine-tuning,0.544678807258606
translation,10,182,results,results,show,arman ( ss - 100 ) base,results show arman ( ss - 100 ) base,0.6531098484992981
translation,10,189,results,lower density and coverage,compared to,pe - gasus,lower density and coverage compared to pe - gasus,0.7475513815879822
translation,10,189,results,pe - gasus,in,4 out of 6 tasks,pe - gasus in 4 out of 6 tasks,0.5603819489479065
translation,10,189,results,arman,has,lower density and coverage,arman has lower density and coverage,0.6263683438301086
translation,10,189,results,results,show,arman,results show arman,0.6422526240348816
translation,10,195,results,higher f1 - target and precision - source score,than,pega - sus,higher f1 - target and precision - source score than pega - sus,0.6007340550422668
translation,10,195,results,pega - sus,in,5 out of 6 tasks,pega - sus in 5 out of 6 tasks,0.5320096015930176
translation,10,195,results,arman,has,higher f1 - target and precision - source score,arman has higher f1 - target and precision - source score,0.5712994933128357
translation,10,195,results,results,show,arman,results show arman,0.6422526240348816
translation,10,200,results,our models,has,outperformed,our models has outperformed,0.5781962871551514
translation,10,200,results,outperformed,has,pegasus,outperformed has pegasus,0.6265373229980469
translation,10,200,results,results,show,our models,results show our models,0.6820906400680542
translation,10,203,results,our model,beaten,state - of - the - art model,our model beaten state - of - the - art model,0.5740007162094116
translation,10,203,results,state - of - the - art model,with,only seeing 1 k samples,state - of - the - art model with only seeing 1 k samples,0.6337430477142334
translation,10,203,results,wiki summary and voa datasets,has,our model,wiki summary and voa datasets has our model,0.5753157734870911
translation,10,210,results,arman ( sh ) base,beaten,other models,arman ( sh ) base beaten other models,0.647546112537384
translation,10,210,results,other models,in,natural part,other models in natural part,0.516346275806427
translation,10,210,results,natural part,of,textual entailment,natural part of textual entailment,0.5369622707366943
translation,10,210,results,natural part,of,question paraphrasing,natural part of question paraphrasing,0.5767929553985596
translation,10,214,results,our proposed model,with,semantic similarity and mask-only approach,our proposed model with semantic similarity and mask-only approach,0.6443319320678711
translation,10,214,results,surpasses,others in,literature questions,surpasses others in literature questions,0.7359964847564697
translation,10,214,results,our proposed model,has,surpasses,our proposed model has surpasses,0.6001195907592773
translation,10,214,results,semantic similarity and mask-only approach,has,surpasses,semantic similarity and mask-only approach has surpasses,0.6005592346191406
translation,10,214,results,results,has,our proposed model,results has our proposed model,0.5871988534927368
translation,10,232,results,most recent sota models,in,all six downstream tasks,most recent sota models in all six downstream tasks,0.5042588114738464
translation,10,232,results,modified sentence selection and reordering model,has,outperforms,modified sentence selection and reordering model has outperforms,0.5996202826499939
translation,10,232,results,outperforms,has,most recent sota models,outperforms has most recent sota models,0.5991425514221191
translation,10,232,results,results,show,modified sentence selection and reordering model,results show modified sentence selection and reordering model,0.608532190322876
translation,10,233,results,our model,achieved,higher score,our model achieved higher score,0.728976309299469
translation,10,233,results,higher score,than,previous sota,higher score than previous sota,0.5935596823692322
translation,10,233,results,previous sota,with,only 1 k examples,previous sota with only 1 k examples,0.6287077069282532
translation,10,233,results,only 1 k examples,in the case of,low supervised sample sizes,only 1 k examples in the case of low supervised sample sizes,0.621353030204773
translation,10,233,results,results,has,our model,results has our model,0.5871725678443909
translation,10,308,results,significantly improves,in comparison with,arman,significantly improves in comparison with arman,0.7397827506065369
translation,10,308,results,significantly improves,in comparison with,pegasus,significantly improves in comparison with pegasus,0.6948129534721375
translation,10,308,results,results,in comparison with,arman,results in comparison with arman,0.7089453935623169
translation,10,308,results,ar - man,has,msr ),ar - man has msr ),0.6689146161079407
translation,10,308,results,ar - man,has,significantly improves,ar - man has significantly improves,0.632279098033905
translation,10,308,results,msr ),has,significantly improves,msr ) has significantly improves,0.6302136182785034
translation,10,308,results,significantly improves,has,results,significantly improves has results,0.5862932801246643
translation,10,308,results,arman,has,ss - 80 ),arman has ss - 80 ),0.6438928842544556
translation,10,308,results,results,has,ar - man,results has ar - man,0.5814328193664551
translation,11,5,model,contrast candidate generation and selection,as,model- agnostic post-processing technique,contrast candidate generation and selection as model- agnostic post-processing technique,0.5335216522216797
translation,11,5,model,model- agnostic post-processing technique,to correct,extrinsic hallucinations,model- agnostic post-processing technique to correct extrinsic hallucinations,0.6792219281196594
translation,11,5,model,extrinsic hallucinations,in,unfaithful summaries,extrinsic hallucinations in unfaithful summaries,0.5100020170211792
translation,11,5,model,model,study,contrast candidate generation and selection,model study contrast candidate generation and selection,0.6613985300064087
translation,11,6,model,discriminative correction model,by generating,alternative candidate summaries,discriminative correction model by generating alternative candidate summaries,0.7301120758056641
translation,11,6,model,alternative candidate summaries,where,named entities and quantities,alternative candidate summaries where named entities and quantities,0.5879239439964294
translation,11,6,model,named entities and quantities,in,generated summary,named entities and quantities in generated summary,0.5401563048362732
translation,11,6,model,named entities and quantities,replaced with,ones,named entities and quantities replaced with ones,0.7334610223770142
translation,11,6,model,ones,with,compatible semantic types,ones with compatible semantic types,0.6439911723136902
translation,11,6,model,compatible semantic types,from,source document,compatible semantic types from source document,0.5407291054725647
translation,11,6,model,model,learn,discriminative correction model,model learn discriminative correction model,0.6667797565460205
translation,11,24,model,selection step,rank,generated candidates,selection step rank generated candidates,0.7382033467292786
translation,11,24,model,generated candidates,with,discriminative model,generated candidates with discriminative model,0.612024188041687
translation,11,24,model,discriminative model,trained to distinguish,faithful summaries,discriminative model trained to distinguish faithful summaries,0.6910422444343567
translation,11,24,model,discriminative model,trained to distinguish,synthetic negative candidates,discriminative model trained to distinguish synthetic negative candidates,0.6612309813499451
translation,11,24,model,synthetic negative candidates,generated given,source,synthetic negative candidates generated given source,0.7461404204368591
translation,11,24,model,model,In,selection step,model In selection step,0.5255255103111267
translation,11,69,results,both the baseline and our method,do,well,both the baseline and our method do well,0.46125003695487976
translation,11,69,results,well,in,rouge and bertscore,well in rouge and bertscore,0.6154642105102539
translation,11,69,results,trails behind,in,both metrics,trails behind in both metrics,0.5588847994804382
translation,11,69,results,our method,has,trails behind,our method has trails behind,0.5974574089050293
translation,11,69,results,results,observe,both the baseline and our method,results observe both the baseline and our method,0.5463680624961853
translation,11,75,results,corrected summaries,present,statistically significant improvements,corrected summaries present statistically significant improvements,0.7254267334938049
translation,11,75,results,statistically significant improvements,over,original ones,statistically significant improvements over original ones,0.6085314750671387
translation,11,75,results,results,suggest,corrected summaries,results suggest corrected summaries,0.5867476463317871
translation,11,84,results,our system,achieves,consistently high level of precision,our system achieves consistently high level of precision,0.6702513098716736
translation,11,84,results,consistently high level of precision,across,models,consistently high level of precision across models,0.7072319388389587
translation,11,84,results,results,has,our system,results has our system,0.5954442024230957
translation,11,85,results,high relative recall,with respect to,%,high relative recall with respect to %,0.6975646018981934
translation,11,85,results,%,of,entity and quantity hallucinations,% of entity and quantity hallucinations,0.6113230586051941
translation,11,85,results,entity and quantity hallucinations,among,all hallucinations,entity and quantity hallucinations among all hallucinations,0.5844003558158875
translation,11,87,results,our method,achieves,high recall,our method achieves high recall,0.6633724570274353
translation,11,87,results,high recall,on,models,high recall on models,0.5638704895973206
translation,11,87,results,models,with,lower rouge and bertscore,models with lower rouge and bertscore,0.6828519105911255
translation,11,87,results,drops,on,pretrained models,drops on pretrained models,0.5603471994400024
translation,11,87,results,pretrained models,such as,berts2s,pretrained models such as berts2s,0.602408230304718
translation,11,87,results,recall,has,drops,recall has drops,0.6701967716217041
translation,11,87,results,results,observe,our method,results observe our method,0.5780253410339355
translation,12,106,baselines,chen and bansal ( 2018 ),is,hybrid model,chen and bansal ( 2018 ) is hybrid model,0.5653833746910095
translation,12,106,baselines,hybrid model,first extracts,sentences,hybrid model first extracts sentences,0.7989439964294434
translation,12,106,baselines,hybrid model,rewrites,extracted sentences,hybrid model rewrites extracted sentences,0.7271925210952759
translation,12,106,baselines,sentences,from,source document,sentences from source document,0.5012192726135254
translation,12,106,baselines,extracted sentences,by,abstractive rewriter,extracted sentences by abstractive rewriter,0.5303420424461365
translation,12,44,experiments,human-written short ( < 30 words ) and long ( < 100 words ) abstractive summaries,of,"2,549 email threads","human-written short ( < 30 words ) and long ( < 100 words ) abstractive summaries of 2,549 email threads",0.5551113486289978
translation,12,44,experiments,"2,549 email threads",constructed from,"avocado research email collection ( oard et al. , 2015 )","2,549 email threads constructed from avocado research email collection ( oard et al. , 2015 )",0.6230713129043579
translation,12,49,experiments,different summarization techniques,including,extractive and abstractive summarization methods,different summarization techniques including extractive and abstractive summarization methods,0.6272068619728088
translation,12,49,experiments,different summarization techniques,including,single-document and hierarchical models,different summarization techniques including single-document and hierarchical models,0.6520039439201355
translation,12,49,experiments,different summarization techniques,including,transfer learning,different summarization techniques including transfer learning,0.6339523792266846
translation,12,49,experiments,different summarization techniques,including,semi-supervised learning,different summarization techniques including semi-supervised learning,0.6418092846870422
translation,12,49,experiments,semi-supervised learning,for,short and long summary generation,semi-supervised learning for short and long summary generation,0.5765883922576904
translation,12,55,experiments,new email -sum dataset,provides,larger resource,new email -sum dataset provides larger resource,0.6399009823799133
translation,12,55,experiments,larger resource,for studying,email thread summarization task,larger resource for studying email thread summarization task,0.6692232489585876
translation,12,133,model,token - level and email- level encoders,sharing,weights,token - level and email- level encoders sharing weights,0.6885697841644287
translation,12,133,model,weights,of,t5 encoder,weights of t5 encoder,0.6039655804634094
translation,12,133,model,model,Both,token - level and email- level encoders,model Both token - level and email- level encoders,0.666940450668335
translation,12,133,model,model,has,token - level and email- level encoders,model has token - level and email- level encoders,0.5778797268867493
translation,12,115,results,t5 model,from,scratch,t5 model from scratch,0.6618645191192627
translation,12,115,results,t5 model,performs,poorly,t5 model performs poorly,0.7360241413116455
translation,12,115,results,results,Training,t5 model,results Training t5 model,0.7250394225120544
translation,12,144,results,barely improves,over,baseline,barely improves over baseline,0.7084334492683411
translation,12,144,results,transferring,by,pre-finetuning,transferring by pre-finetuning,0.5989448428153992
translation,12,144,results,works better,than,joint-training,works better than joint-training,0.6319023370742798
translation,12,144,results,transfer learning,has,barely improves,transfer learning has barely improves,0.6251432299613953
translation,12,144,results,pre-finetuning,has,works better,pre-finetuning has works better,0.5845972299575806
translation,12,148,results,number of emails,in,thread,number of emails in thread,0.510067880153656
translation,12,148,results,marginally outperforms,has,non-hierarchical,marginally outperforms has non-hierarchical,0.6261942982673645
translation,12,148,results,results,has,hierarchical t5 base model,results has hierarchical t5 base model,0.5564337372779846
translation,12,157,results,semi-supervised models ( r1 - best ),are,even more extractive,semi-supervised models ( r1 - best ) are even more extractive,0.5692222714424133
translation,12,157,results,even more extractive,than,baseline,even more extractive than baseline,0.5801759958267212
translation,12,157,results,results,has,semi-supervised models ( r1 - best ),results has semi-supervised models ( r1 - best ),0.5283472537994385
translation,12,158,results,model performance ( rouge - 1 ),decreases as,number of emails,model performance ( rouge - 1 ) decreases as number of emails,0.6020137071609497
translation,12,158,results,number of emails,in,thread,number of emails in thread,0.510067880153656
translation,12,158,results,base and best models,has,model performance ( rouge - 1 ),base and best models has model performance ( rouge - 1 ),0.5554826855659485
translation,12,158,results,short and long summaries,has,model performance ( rouge - 1 ),short and long summaries has model performance ( rouge - 1 ),0.5630990266799927
translation,12,158,results,results,for,base and best models,results for base and best models,0.620762288570404
translation,12,250,results,correlations,are,fairly poor,correlations are fairly poor,0.5752713084220886
translation,12,250,results,results,has,correlations,results has correlations,0.5100684762001038
translation,12,251,results,best correlation,between,rouge - 1 and human overall quality ranking,best correlation between rouge - 1 and human overall quality ranking,0.5838537812232971
translation,12,251,results,rouge - 1 and human overall quality ranking,for,short summary generation,rouge - 1 and human overall quality ranking for short summary generation,0.5675318837165833
translation,12,251,results,results,has,best correlation,results has best correlation,0.5544105172157288
translation,13,112,ablation-analysis,syntactic or discourse information,was,beneficial,syntactic or discourse information was beneficial,0.6305384635925293
translation,13,112,ablation-analysis,ablation analysis,has,synroberta,ablation analysis has synroberta,0.5707912445068359
translation,13,116,ablation-analysis,"synroberta ( n s = { 1 , 2 } )",explicitly incorporates,keywords information,"synroberta ( n s = { 1 , 2 } ) explicitly incorporates keywords information",0.741975724697113
translation,13,116,ablation-analysis,"synroberta ( n s = { 1 , 2 } )",can further improve,performance,"synroberta ( n s = { 1 , 2 } ) can further improve performance",0.7423434853553772
translation,13,116,ablation-analysis,keywords information,through,syntactic information,keywords information through syntactic information,0.5369255542755127
translation,13,116,ablation-analysis,syntactic information,can further improve,performance,syntactic information can further improve performance,0.683947741985321
translation,13,116,ablation-analysis,performance,of,roberta,performance of roberta,0.6133401989936829
translation,13,116,ablation-analysis,ablation analysis,has,"synroberta ( n s = { 1 , 2 } )","ablation analysis has synroberta ( n s = { 1 , 2 } )",0.5735436081886292
translation,13,96,baselines,trigram blocking,to reduce,redundancy,trigram blocking to reduce redundancy,0.6693727970123291
translation,13,96,baselines,trigram blocking,to improve,informativeness,trigram blocking to improve informativeness,0.6644781827926636
translation,13,96,baselines,informativeness,for,all models,informativeness for all models,0.5554906129837036
translation,13,96,baselines,baselines,has,trigram blocking,baselines has trigram blocking,0.5885849595069885
translation,13,98,baselines,neroberta,considers,nested tree structure,neroberta considers nested tree structure,0.6511975526809692
translation,13,98,baselines,nested tree structure,for,syntactic and discourse information,nested tree structure for syntactic and discourse information,0.5727354288101196
translation,13,100,baselines,bertsum,introduces,method,bertsum introduces method,0.6598495841026306
translation,13,100,baselines,method,for learning,sentence boundary,method for learning sentence boundary,0.7028530240058899
translation,13,100,baselines,sentence boundary,in,bert - based model,sentence boundary in bert - based model,0.5001925826072693
translation,13,100,baselines,sentence boundary,for,document summarization task,sentence boundary for document summarization task,0.5564178824424744
translation,13,100,baselines,bert - based model,for,document summarization task,bert - based model for document summarization task,0.5825466513633728
translation,13,101,baselines,discobert,constructs,summary,discobert constructs summary,0.6938417553901672
translation,13,101,baselines,summary,based on,edu - level extraction,summary based on edu - level extraction,0.6311609745025635
translation,13,101,baselines,summary,incorporating,discourse and coreference information,summary incorporating discourse and coreference information,0.6920053362846375
translation,13,101,baselines,discourse and coreference information,has,"xu et al. , 2020","discourse and coreference information has xu et al. , 2020",0.574299156665802
translation,13,101,baselines,baselines,has,discobert,baselines has discobert,0.6481128931045532
translation,13,102,baselines,baselines,has,matchsum,baselines has matchsum,0.5584253072738647
translation,13,93,hyperparameters,number of attention heads,set to,6,number of attention heads set to 6,0.7147284150123596
translation,13,93,hyperparameters,6,in,graph encoder,6 in graph encoder,0.5205747485160828
translation,13,93,hyperparameters,hyperparameters,has,number of attention heads,hyperparameters has number of attention heads,0.5335925221443176
translation,13,6,model,nested tree - based extractive summarization,on,roberta ( neroberta ),nested tree - based extractive summarization on roberta ( neroberta ),0.548760712146759
translation,13,6,model,nested tree structures,consist of,syntactic and discourse trees,nested tree structures consist of syntactic and discourse trees,0.6224833130836487
translation,13,6,model,model,propose,nested tree - based extractive summarization,model propose nested tree - based extractive summarization,0.6598131656646729
translation,13,23,model,nested tree - based extractive summarization model,on,roberta ( neroberta ),nested tree - based extractive summarization model on roberta ( neroberta ),0.5514702796936035
translation,13,23,model,model,propose,nested tree - based extractive summarization model,model propose nested tree - based extractive summarization model,0.6564103960990906
translation,13,103,model,roberta,encodes,input documents,roberta encodes input documents,0.728278398513794
translation,13,103,model,input documents,using,  roberta- based   model,input documents using   roberta- based   model,0.6686841249465942
translation,13,103,model,model,has,roberta,model has roberta,0.6099422574043274
translation,13,110,results,strong baseline roberta,has,outperformed,strong baseline roberta has outperformed,0.6028274297714233
translation,13,110,results,outperformed,has,bertsum,outperformed has bertsum,0.6478826999664307
translation,13,110,results,results,has,strong baseline roberta,results has strong baseline roberta,0.6020678281784058
translation,13,114,results,roberta,with,clear margin,roberta with clear margin,0.6988204717636108
translation,13,114,results,clear margin,specifically,0.31 points,clear margin specifically 0.31 points,0.6450886130332947
translation,13,114,results,0.31 points,in,r-1 - f score,0.31 points in r-1 - f score,0.5112230777740479
translation,13,114,results,outperformed,has,roberta,outperformed has roberta,0.6626365780830383
translation,13,114,results,results,has,outperformed,results has outperformed,0.636634886264801
translation,13,115,results,roberta,improve,prediction loss,roberta improve prediction loss,0.6945910453796387
translation,13,115,results,prediction loss,compared with,bert - sum,prediction loss compared with bert - sum,0.6483258008956909
translation,13,123,results,proposed neroberta,considers,nested tree structure,proposed neroberta considers nested tree structure,0.6645166873931885
translation,13,123,results,proposed neroberta,could capture,coherence,proposed neroberta could capture coherence,0.7386236786842346
translation,13,123,results,coherence,better than,"our strong baseline , roberta","coherence better than our strong baseline , roberta",0.745140790939331
translation,13,124,results,neroberta,comparable to,current state - of- the - art model,neroberta comparable to current state - of- the - art model,0.6643679738044739
translation,13,124,results,results,has,neroberta,results has neroberta,0.5666230320930481
translation,14,114,baselines,fame,to,two popular seq2seq architectures,fame to two popular seq2seq architectures,0.5482409596443176
translation,14,116,experiments,robertas2s - large,with,shared encoder and decoder,robertas2s - large with shared encoder and decoder,0.6807223558425903
translation,14,116,experiments,hidden size,of,1024,hidden size of 1024,0.6436027884483337
translation,14,116,experiments,filter size,of,4096,filter size of 4096,0.6193011999130249
translation,14,116,experiments,vocabulary,with,50 k sentence pieces,vocabulary with 50 k sentence pieces,0.6339030265808105
translation,14,112,hyperparameters,length,of,summaries,length of summaries,0.6203900575637817
translation,14,112,hyperparameters,summaries,limited to,64,summaries limited to 64,0.6091726422309875
translation,14,112,hyperparameters,hyperparameters,has,length,hyperparameters has length,0.5058407187461853
translation,14,6,model,simple yet effective method,to encourage,decoders,simple yet effective method to encourage decoders,0.7559569478034973
translation,14,6,model,decoders,to proactively generate,tokens,decoders to proactively generate tokens,0.7672752141952515
translation,14,6,model,tokens,that are,similar or topical,tokens that are similar or topical,0.622121274471283
translation,14,6,model,focus attention mechanism,has,simple yet effective method,focus attention mechanism has simple yet effective method,0.4970172047615051
translation,14,6,model,model,introduce,focus attention mechanism,model introduce focus attention mechanism,0.6078298687934875
translation,14,7,model,focus sampling method,generation of,diverse summaries,focus sampling method generation of diverse summaries,0.6765007376670837
translation,14,7,model,model,propose,focus sampling method,model propose focus sampling method,0.6444394588470459
translation,14,25,model,focus attention mechanism ( or fame ),to,transformer - based seq2seq architectures,focus attention mechanism ( or fame ) to transformer - based seq2seq architectures,0.5516769886016846
translation,14,25,model,model,introduce,focus attention mechanism ( or fame ),model introduce focus attention mechanism ( or fame ),0.6246491074562073
translation,14,27,model,fame,to perform,source-side planning,fame to perform source-side planning,0.7396879196166992
translation,14,27,model,source-side planning,to focus,summary,source-side planning to focus summary,0.7072177529335022
translation,14,27,model,summary,on,supported and topical content,summary on supported and topical content,0.5467312335968018
translation,14,27,model,model,has,fame,model has fame,0.6043808460235596
translation,14,28,model,fame,through,novel technique,fame through novel technique,0.7418982982635498
translation,14,28,model,novel technique,augments,standard contextual representations,novel technique augments standard contextual representations,0.7413826584815979
translation,14,28,model,standard contextual representations,with,dynamic source -conditioned vocabulary biasing layer,standard contextual representations with dynamic source -conditioned vocabulary biasing layer,0.6058343648910522
translation,14,28,model,model,has,fame,model has fame,0.6043808460235596
translation,14,29,results,fame,promotes,summaries,fame promotes summaries,0.7096884846687317
translation,14,29,results,faithful,to,source,faithful to source,0.6009606122970581
translation,14,29,results,summaries,has,faithful,summaries has faithful,0.5984981060028076
translation,14,172,results,div nucleus,tends to generate,"most distinct unigrams , bigrams , and trigrams","div nucleus tends to generate most distinct unigrams , bigrams , and trigrams",0.7441263794898987
translation,14,172,results,results,has,div nucleus,results has div nucleus,0.5493414402008057
translation,14,179,results,"focus sample , k",superior to,"div top , k","focus sample , k superior to div top , k",0.7210808396339417
translation,14,179,results,"focus sample , k",superior to,div nucleus,"focus sample , k superior to div nucleus",0.7712879776954651
translation,14,179,results,div nucleus,in gen-erating,better quality summaries,div nucleus in gen-erating better quality summaries,0.6829264760017395
translation,14,179,results,results,has,"focus sample , k","results has focus sample , k",0.5355852842330933
translation,14,201,results,without any topical supervision,is,not significantly better,without any topical supervision is not significantly better,0.5528202056884766
translation,14,201,results,not significantly better,than,baselines,not significantly better than baselines,0.6478003263473511
translation,14,201,results,focus attention,has,without any topical supervision,focus attention has without any topical supervision,0.5901632905006409
translation,14,201,results,results,has,focus attention,results has focus attention,0.5297524929046631
translation,14,202,results,robfame and peg - fame,has,significantly outperform,robfame and peg - fame has significantly outperform,0.5793929696083069
translation,14,202,results,significantly outperform,has,robertas2s and pega - sus,significantly outperform has robertas2s and pega - sus,0.5972650051116943
translation,14,202,results,results,has,robfame and peg - fame,results has robfame and peg - fame,0.4971773624420166
translation,14,203,results,our best model pegfame,performs,better,our best model pegfame performs better,0.6072660684585571
translation,14,203,results,our best model pegfame,performs,worse,our best model pegfame performs worse,0.6297903656959534
translation,14,203,results,better,than,ptgen,better than ptgen,0.6466163992881775
translation,14,203,results,better,than,"mass ( song et al. , 2019 )","better than mass ( song et al. , 2019 )",0.5867515802383423
translation,14,203,results,better,than,bart,better than bart,0.6386485695838928
translation,14,203,results,worse,when,original pegasus,worse when original pegasus,0.7261284589767456
translation,14,203,results,convs2s,has,"narayan et al. , 2018 )","convs2s has narayan et al. , 2018 )",0.560818612575531
translation,14,203,results,mmn,has,"kim et al. , 2019 )","mmn has kim et al. , 2019 )",0.5870904326438904
translation,14,203,results,results,has,our best model pegfame,results has our best model pegfame,0.5946620106697083
translation,15,169,ablation-analysis,vocabulary overlap,not,helping,vocabulary overlap not helping,0.7343826293945312
translation,15,169,ablation-analysis,helping,either of,models,helping either of models,0.5207620859146118
translation,15,169,ablation-analysis,models,in terms of,predictive performance,models in terms of predictive performance,0.6779448390007019
translation,15,169,ablation-analysis,ablation analysis,conclude that,vocabulary overlap,ablation analysis conclude that vocabulary overlap,0.5690854787826538
translation,15,5,experiments,checking factual correctness,of,textual summarization ( cfcs ),checking factual correctness of textual summarization ( cfcs ),0.5584978461265564
translation,15,171,experiments,race converted,on,converted forms,race converted on converted forms,0.586106538772583
translation,15,171,experiments,converted forms,of,mcrc datasets,converted forms of mcrc datasets,0.6132701635360718
translation,15,139,results,model,trained on,long- premise race converted dataset,model trained on long- premise race converted dataset,0.7300937175750732
translation,15,139,results,model,trained on,short- premise nli datasets,model trained on short- premise nli datasets,0.7187294363975525
translation,15,139,results,model,trained on,short- premise nli datasets,model trained on short- premise nli datasets,0.7187294363975525
translation,15,139,results,short- premise nli datasets,in,regular and segmented forms,short- premise nli datasets in regular and segmented forms,0.5072154402732849
translation,15,139,results,long- premise race converted dataset,has,outperforms,long- premise race converted dataset has outperforms,0.6029102206230164
translation,15,139,results,outperforms,has,model,outperforms has model,0.6832752227783203
translation,15,165,results,model,trained on,race converted,model trained on race converted,0.7553454041481018
translation,15,165,results,model,trained on,short- premise nli datasets,model trained on short- premise nli datasets,0.7187294363975525
translation,15,165,results,models,trained on,short- premise nli datasets,models trained on short- premise nli datasets,0.7033385634422302
translation,15,165,results,considerably outperforms,has,models,considerably outperforms has models,0.6634339094161987
translation,15,165,results,results,see that,model,results see that model,0.640961766242981
translation,15,168,results,performance,of,two models,performance of two models,0.6440985202789307
translation,15,168,results,two models,on,high vocabulary overlap subsets,two models on high vocabulary overlap subsets,0.529872715473175
translation,15,168,results,overall performances,on,respective datasets,overall performances on respective datasets,0.46470633149147034
translation,15,180,results,race converted model,performs,better,race converted model performs better,0.6330445408821106
translation,15,180,results,better,on,manually annotated subset,better on manually annotated subset,0.5731756687164307
translation,15,180,results,results,shows,race converted model,results shows race converted model,0.6634796857833862
translation,16,94,ablation-analysis,tri-gram blocking and contextual information,within,sentence selector,tri-gram blocking and contextual information within sentence selector,0.6692873239517212
translation,16,99,ablation-analysis,inner and outer skip connections,play,important role,inner and outer skip connections play important role,0.7222833037376404
translation,16,99,ablation-analysis,important role,in,multi-gcn,important role in multi-gcn,0.5507919192314148
translation,16,99,ablation-analysis,ablation analysis,indicates,inner and outer skip connections,ablation analysis indicates inner and outer skip connections,0.6455132365226746
translation,16,75,baselines,re,has,"-fresh ( narayan et al. , 2018 )","re has -fresh ( narayan et al. , 2018 )",0.5670058131217957
translation,16,77,experimental-setup,vocabulary size,fixed as,50,vocabulary size fixed as 50,0.7496793866157532
translation,16,77,experimental-setup,50,",",000,"50 , 000",0.6518385410308838
translation,16,77,experimental-setup,pre-trained glove embeddings,used for,input word embeddings,pre-trained glove embeddings used for input word embeddings,0.5653665065765381
translation,16,77,experimental-setup,experimental setup,has,vocabulary size,experimental setup has vocabulary size,0.5189570188522339
translation,16,77,experimental-setup,experimental setup,has,pre-trained glove embeddings,experimental setup has pre-trained glove embeddings,0.5117311477661133
translation,16,80,experimental-setup,all the hidden dimensions,as,300,all the hidden dimensions as 300,0.5730821490287781
translation,16,80,experimental-setup,experimental setup,fix,all the hidden dimensions,experimental setup fix all the hidden dimensions,0.6672786474227905
translation,16,81,experimental-setup,stanford corenlp natural connection graphs,filter out,stop words,stanford corenlp natural connection graphs filter out stop words,0.6620305180549622
translation,16,81,experimental-setup,stanford corenlp natural connection graphs,filter out,punctuation,stanford corenlp natural connection graphs filter out punctuation,0.6827859282493591
translation,16,81,experimental-setup,stanford corenlp natural connection graphs,filter out,words,stanford corenlp natural connection graphs filter out words,0.6935324668884277
translation,16,81,experimental-setup,words,whose,document frequency,words whose document frequency,0.5587896704673767
translation,16,81,experimental-setup,document frequency,less than,100,document frequency less than 100,0.6670684218406677
translation,16,81,experimental-setup,experimental setup,use,stanford corenlp natural connection graphs,experimental setup use stanford corenlp natural connection graphs,0.5540034770965576
translation,16,82,experimental-setup,training,use,adam optimizer,training use adam optimizer,0.6185989379882812
translation,16,82,experimental-setup,training,use,learning rates,training use learning rates,0.6818098425865173
translation,16,82,experimental-setup,learning rates,for,"cnn , dailymail , and cnn / dailymail datasets","learning rates for cnn , dailymail , and cnn / dailymail datasets",0.5475345849990845
translation,16,82,experimental-setup,"cnn , dailymail , and cnn / dailymail datasets",set to be,"0.0001 , 0.0005 , and 0.0005","cnn , dailymail , and cnn / dailymail datasets set to be 0.0001 , 0.0005 , and 0.0005",0.6493550539016724
translation,16,82,experimental-setup,experimental setup,During,training,experimental setup During training,0.6835477948188782
translation,16,8,model,novel multiplex graph convolutional network ( multi - gcn ),to jointly model,different types of relationships,novel multiplex graph convolutional network ( multi - gcn ) to jointly model different types of relationships,0.7512296438217163
translation,16,8,model,different types of relationships,among,sentences and words,different types of relationships among sentences and words,0.5845617651939392
translation,16,8,model,model,propose,novel multiplex graph convolutional network ( multi - gcn ),model propose novel multiplex graph convolutional network ( multi - gcn ),0.6418722867965698
translation,16,78,model,initialization modules,employ,two -layer bi-lstms,initialization modules employ two -layer bi-lstms,0.5496716499328613
translation,16,78,model,both of the word block and the sentence block,has,initialization modules,both of the word block and the sentence block has initialization modules,0.5622381567955017
translation,16,78,model,model,For,both of the word block and the sentence block,model For both of the word block and the sentence block,0.605570912361145
translation,16,79,model,multi-gcn modules,use,two -layer skip- gcns,multi-gcn modules use two -layer skip- gcns,0.614924430847168
translation,16,79,model,model,has,multi-gcn modules,model has multi-gcn modules,0.5869089365005493
translation,16,92,results,hsg,achieves,highest performance,hsg achieves highest performance,0.7139871716499329
translation,16,92,results,baseline methods,has,hsg,baseline methods has hsg,0.5409514904022217
translation,16,92,results,results,Within,baseline methods,results Within baseline methods,0.58750981092453
translation,16,93,results,multi-,achieves,0.21/0.38/0.26 performance increase,multi- achieves 0.21/0.38/0.26 performance increase,0.6584051847457886
translation,16,93,results,gras,achieves,0.21/0.38/0.26 performance increase,gras achieves 0.21/0.38/0.26 performance increase,0.6181111931800842
translation,16,93,results,0.21/0.38/0.26 performance increase,on,r-1/r-2/ r-l scores,0.21/0.38/0.26 performance increase on r-1/r-2/ r-l scores,0.5163557529449463
translation,16,93,results,multi-,has,gras,multi- has gras,0.6052138209342957
translation,16,93,results,gras,has,outperforms,gras has outperforms,0.6331567168235779
translation,16,93,results,outperforms,has,all of the comparison methods,outperforms has all of the comparison methods,0.5618786811828613
translation,16,93,results,results,observe,multi-,results observe multi-,0.5829969644546509
translation,16,97,results,"lstm , multi-gras word and multi- gras",observed that,multi-gcn,"lstm , multi-gras word and multi- gras observed that multi-gcn",0.6427414417266846
translation,16,97,results,multi-gcn,in,sentence and word blocks,multi-gcn in sentence and word blocks,0.5384263396263123
translation,16,97,results,multi-gcn,both,sentence and word blocks,multi-gcn both sentence and word blocks,0.6795489192008972
translation,16,97,results,sentence and word blocks,has,significantly improve,sentence and word blocks has significantly improve,0.5852512121200562
translation,16,97,results,significantly improve,has,performance,significantly improve has performance,0.5721634030342102
translation,16,97,results,results,comparing,"lstm , multi-gras word and multi- gras","results comparing lstm , multi-gras word and multi- gras",0.6357722878456116
translation,16,100,results,different relations,is,always better,different relations is always better,0.6162710785865784
translation,16,100,results,always better,than considering,one relation alone,always better than considering one relation alone,0.7496054768562317
translation,16,101,results,initialization module,in,word and sentence blocks,initialization module in word and sentence blocks,0.49960005283355713
translation,16,101,results,lstm,performs better,transformer,lstm performs better transformer,0.7112020254135132
translation,16,101,results,initialization module,has,lstm,initialization module has lstm,0.5381556749343872
translation,16,101,results,word and sentence blocks,has,lstm,word and sentence blocks has lstm,0.580054521560669
translation,16,101,results,transformer,has,"vaswani et al. , 2017 )","transformer has vaswani et al. , 2017 )",0.5807598829269409
translation,16,101,results,results,for,initialization module,results for initialization module,0.5934627652168274
translation,16,105,results,oracle,ranks,highest,oracle ranks highest,0.8196830153465271
translation,16,105,results,gras,ranks,higher,gras ranks higher,0.7937729358673096
translation,16,105,results,higher,than,hsg,higher than hsg,0.6050590872764587
translation,16,105,results,multi-,has,gras,multi- has gras,0.6052138209342957
translation,16,105,results,results,has,human evaluation,results has human evaluation,0.5143810510635376
translation,16,108,results,multi- gras,performs,best,multi- gras performs best,0.6536471247673035
translation,16,108,results,best,when,number of the selected sentences,best when number of the selected sentences,0.6031559109687805
translation,16,108,results,number of the selected sentences,is,2,number of the selected sentences is 2,0.5718388557434082
translation,16,108,results,number of the selected sentences,is,3,number of the selected sentences is 3,0.5883952975273132
translation,16,108,results,2,for,cnn dataset,2 for cnn dataset,0.4694550633430481
translation,16,108,results,3,for,dailymail dataset,3 for dailymail dataset,0.467416375875473
translation,16,108,results,results,show,multi- gras,results show multi- gras,0.6085737943649292
translation,17,177,baselines,baselines,has,local attention v.s. mcs,baselines has local attention v.s. mcs,0.5468685626983643
translation,17,61,experimental-setup,"publicly released bart model ( lewis et al. , 2020 )",fine-tuned on,"cn - ndm ( hermann et al. , 2015 )","publicly released bart model ( lewis et al. , 2020 ) fine-tuned on cn - ndm ( hermann et al. , 2015 )",0.6966017484664917
translation,17,61,experimental-setup,experimental setup,use,"publicly released bart model ( lewis et al. , 2020 )","experimental setup use publicly released bart model ( lewis et al. , 2020 )",0.6034073829650879
translation,17,8,model,long-span dependencies,in,abstractive summarization,long-span dependencies in abstractive summarization,0.474683940410614
translation,17,8,model,abstractive summarization,using,two methods,abstractive summarization using two methods,0.6236457824707031
translation,17,8,model,model,exploit,large pre-trained transformer - based models,model exploit large pre-trained transformer - based models,0.7602147459983826
translation,17,25,model,long-span dependencies,handled by,two complementary methods,long-span dependencies handled by two complementary methods,0.6564369201660156
translation,17,25,model,model,propose,long-span dependencies,model propose long-span dependencies,0.6313942074775696
translation,17,26,model,standard transformer models,by constraining,attention mechanism,standard transformer models by constraining attention mechanism,0.6780965328216553
translation,17,26,model,attention mechanism,to be,local,attention mechanism to be local,0.5447068810462952
translation,17,26,model,longer input spans,during,training,longer input spans during training,0.6876122951507568
translation,17,29,model,to select data,for training,fixed - span abstractive models,to select data for training fixed - span abstractive models,0.8086045384407043
translation,17,30,model,multitask content selection method,ranks,sentences,multitask content selection method ranks sentences,0.647605299949646
translation,17,30,model,sentences,through,extractive labelling based module,sentences through extractive labelling based module,0.6588711142539978
translation,17,30,model,sentences,through,attention based module,sentences through attention based module,0.6682755947113037
translation,17,30,model,model,propose,multitask content selection method,model propose multitask content selection method,0.6097090244293213
translation,17,31,model,combined approach,consisting of,local self-attention transformer,combined approach consisting of local self-attention transformer,0.7398377656936646
translation,17,31,model,combined approach,consisting of,content selection,combined approach consisting of content selection,0.7164223194122314
translation,17,31,model,content selection,for,long-document summarization,content selection for long-document summarization,0.5311301350593567
translation,17,31,model,model,explore,combined approach,model explore combined approach,0.7032114267349243
translation,17,66,model,linear layer,on top of,sentence - level gru,linear layer on top of sentence - level gru,0.6316572427749634
translation,17,66,model,model,add,linear layer,model add linear layer,0.671330988407135
translation,17,123,results,orc pad-rand,is,better,orc pad-rand is better,0.6141883134841919
translation,17,123,results,orc pad-rand,introduces,more diversity,orc pad-rand introduces more diversity,0.6653111577033997
translation,17,123,results,better,introduces,more diversity,better introduces more diversity,0.6694420576095581
translation,17,123,results,more diversity,to,abstractive model,more diversity to abstractive model,0.562671959400177
translation,17,123,results,orc pad-lead,has,orc pad-rand,orc pad-lead has orc pad-rand,0.6313093900680542
translation,17,123,results,results,Compared to,orc pad-lead,results Compared to orc pad-lead,0.6248175501823425
translation,17,123,results,results,Compared to,orc pad-rand,results Compared to orc pad-rand,0.6325168013572693
translation,17,125,results,content selection,at,test time,content selection at test time,0.5373066067695618
translation,17,125,results,content selection,benefit from,orc pad-rand,content selection benefit from orc pad-rand,0.6464632749557495
translation,17,125,results,lobart ( 4 k ) and lobart ( 8 k ),benefit from,orc pad-rand,lobart ( 4 k ) and lobart ( 8 k ) benefit from orc pad-rand,0.6461713314056396
translation,17,125,results,content selection,has,lobart ( 4 k ) and lobart ( 8 k ),content selection has lobart ( 4 k ) and lobart ( 8 k ),0.5911444425582886
translation,17,146,results,mcs,yields,better summarization performance,mcs yields better summarization performance,0.6783251166343689
translation,17,146,results,better summarization performance,than,attn-only and ext-only baselines,better summarization performance than attn-only and ext-only baselines,0.5390486121177673
translation,17,146,results,bart,has,mcs,bart has mcs,0.6354673504829407
translation,17,147,results,mcs,achieves,higher recall rate,mcs achieves higher recall rate,0.6837458610534668
translation,17,147,results,higher recall rate,of,sentences,higher recall rate of sentences,0.5803115963935852
translation,17,147,results,sentences,with,"d( x i , y ) > 0","sentences with d( x i , y ) > 0",0.6763262748718262
translation,17,147,results,"d( x i , y ) > 0",than,two baselines,"d( x i , y ) > 0 than two baselines",0.5660741925239563
translation,17,147,results,results,has,mcs,results has mcs,0.48432251811027527
translation,17,151,results,different configurations,with,mcs,different configurations with mcs,0.6889704465866089
translation,17,151,results,different configurations,with,mcs,different configurations with mcs,0.6889704465866089
translation,17,151,results,different configurations,seen that,gain,different configurations seen that gain,0.7671187520027161
translation,17,151,results,mcs,in,lobart ( 8 k ) system,mcs in lobart ( 8 k ) system,0.5762409567832947
translation,17,151,results,gain,from,mcs,gain from mcs,0.6570854783058167
translation,17,151,results,mcs,in,lobart ( 8 k ) system,mcs in lobart ( 8 k ) system,0.5762409567832947
translation,17,151,results,mcs,is,lowest,mcs is lowest,0.6386525630950928
translation,17,151,results,results,comparing,different configurations,results comparing different configurations,0.7081391215324402
translation,17,154,results,cued - filt,by,improved content selection,cued - filt by improved content selection,0.578872561454773
translation,17,154,results,improved content selection,at,training time and test time,improved content selection at training time and test time,0.5097998976707458
translation,17,154,results,outperforms,has,cued - filt,outperforms has cued - filt,0.6107499599456787
translation,17,156,results,lobart,requires,more resource,lobart requires more resource,0.7055701017379761
translation,17,156,results,lobart,does not perform,as well,lobart does not perform as well,0.7308136820793152
translation,17,156,results,lobart,due to,smaller attention window,lobart due to smaller attention window,0.687639594078064
translation,17,156,results,k ),requires,more resource,k ) requires more resource,0.685043215751648
translation,17,156,results,more resource,to,train,more resource to train,0.5875676274299622
translation,17,156,results,as well,as,lobart ( 4 k ),as well as lobart ( 4 k ),0.6125685572624207
translation,17,156,results,lobart ( 4 k ),due to,smaller attention window,lobart ( 4 k ) due to smaller attention window,0.6938351988792419
translation,17,156,results,lower improvement,when adding,mcs,lower improvement when adding mcs,0.7248188257217407
translation,17,156,results,lobart,has,k ),lobart has k ),0.6335488557815552
translation,17,163,results,lobart ( 4 k ) + mcs,has,outperform,lobart ( 4 k ) + mcs has outperform,0.6252451539039612
translation,17,163,results,outperform,has,all existing systems,outperform has all existing systems,0.6132820248603821
translation,17,173,results,mcs,to,bart ( 1 k ) or lobart ( 4 k ),mcs to bart ( 1 k ) or lobart ( 4 k ),0.6240165829658508
translation,17,173,results,mcs,yields,significant improvement,mcs yields significant improvement,0.7252479791641235
translation,17,173,results,significant improvement,resulting in,state - of - the - art results,significant improvement resulting in state - of - the - art results,0.6268618702888489
translation,17,173,results,results,adding,mcs,results adding mcs,0.5996802449226379
translation,17,175,results,lobart ( 4 k ) + mcs,achieves,state - of- the - art results,lobart ( 4 k ) + mcs achieves state - of- the - art results,0.6653608083724976
translation,17,175,results,results,has,pubmed,results has pubmed,0.45539310574531555
translation,17,175,results,results,has,lobart ( 4 k ) + mcs,results has lobart ( 4 k ) + mcs,0.5500847697257996
translation,17,176,results,gain,from,mcs,gain from mcs,0.6570854783058167
translation,17,176,results,not as high,in,k settings,not as high in k settings,0.5783535242080688
translation,17,176,results,bart ( 1 k ) + mcs,has,does not outperform,bart ( 1 k ) + mcs has does not outperform,0.6361387372016907
translation,17,176,results,does not outperform,has,lobart ( 4 k ),does not outperform has lobart ( 4 k ),0.6246827840805054
translation,17,178,results,local attention,yields,better performance,local attention yields better performance,0.6963356137275696
translation,17,178,results,better performance,on,pubmed,better performance on pubmed,0.552823007106781
translation,17,178,results,better performance,on,arxiv,better performance on arxiv,0.5520762801170349
translation,17,178,results,better performance,on,arxiv,better performance on arxiv,0.5520762801170349
translation,17,178,results,mcs,yields,better performance,mcs yields better performance,0.7243126034736633
translation,17,178,results,better performance,on,arxiv,better performance on arxiv,0.5520762801170349
translation,17,178,results,results,has,local attention,results has local attention,0.518004298210144
translation,18,6,model,abstractive summaries,enables,fine - grained analysis,abstractive summaries enables fine - grained analysis,0.6560239791870117
translation,18,6,model,fine - grained analysis,of,"models , data , and evaluation metrics","fine - grained analysis of models , data , and evaluation metrics",0.5322306752204895
translation,18,6,model,model,introduce,summvis,model introduce summvis,0.654254674911499
translation,18,25,model,open-source interactive visu-alization tool,for analyzing,text summarization,open-source interactive visu-alization tool for analyzing text summarization,0.7086089849472046
translation,18,25,model,summvis,has,open-source interactive visu-alization tool,summvis has open-source interactive visu-alization tool,0.550171971321106
translation,18,25,model,model,introduce,summvis,model introduce summvis,0.654254674911499
translation,18,27,results,summvis,scaffolds,human analysis,summvis scaffolds human analysis,0.7406038641929626
translation,18,27,results,human analysis,by offering,clear visual indicators,human analysis by offering clear visual indicators,0.7363612651824951
translation,18,27,results,clear visual indicators,of,semantic and lexical relationships,clear visual indicators of semantic and lexical relationships,0.571574330329895
translation,18,27,results,clear visual indicators,of,intelligent navigation,clear visual indicators of intelligent navigation,0.5739472508430481
translation,18,27,results,semantic and lexical relationships,between,texts,semantic and lexical relationships between texts,0.5909623503684998
translation,18,27,results,intelligent navigation,within,text,intelligent navigation within text,0.6261862516403198
translation,18,27,results,results,has,summvis,results has summvis,0.5723631381988525
translation,19,6,model,unlabeled conversations,combine,coda,unlabeled conversations combine coda,0.734089195728302
translation,19,6,model,unlabeled conversations,with,pseudo summaries,unlabeled conversations with pseudo summaries,0.6110904216766357
translation,19,6,model,coda,with,two -stage noisy selftraining,coda with two -stage noisy selftraining,0.6594259142875671
translation,19,6,model,summarization model,on,unlabeled conversations,summarization model on unlabeled conversations,0.481389582157135
translation,19,6,model,summarization model,on,labeled conversations,summarization model on labeled conversations,0.49946901202201843
translation,19,6,model,unlabeled conversations,with,pseudo summaries,unlabeled conversations with pseudo summaries,0.6110904216766357
translation,19,6,model,model,To further utilize,unlabeled conversations,model To further utilize unlabeled conversations,0.6639145016670227
translation,19,15,model,conversational data augmentation ( coda ),for,conversation summarization,conversational data augmentation ( coda ) for conversation summarization,0.5866430401802063
translation,19,15,model,conversation summarization,guided by,conversation structures and context,conversation summarization guided by conversation structures and context,0.6229646801948547
translation,19,15,model,randomly swap or delete utterances,in,conversations,randomly swap or delete utterances in conversations,0.5671581625938416
translation,19,15,model,conversations,to perturb,discourse relations,conversations to perturb discourse relations,0.6510878801345825
translation,19,15,model,randomly insert utterances,based on,dialogue acts,randomly insert utterances based on dialogue acts,0.6397028565406799
translation,19,15,model,dialogue acts,like,self-talk,dialogue acts like self-talk,0.636215090751648
translation,19,15,model,conditional - generation - based substitution,randomly substitute,utterances,conditional - generation - based substitution randomly substitute utterances,0.6689193248748779
translation,19,15,model,utterances,in,conversations,utterances in conversations,0.5681173205375671
translation,19,15,model,random swapping / deletion,has,randomly swap or delete utterances,random swapping / deletion has randomly swap or delete utterances,0.6303767561912537
translation,19,15,model,dialogue - acts - guided insertion,has,randomly insert utterances,dialogue - acts - guided insertion has randomly insert utterances,0.6158682703971863
translation,19,17,model,performance,when,labeled summaries,performance when labeled summaries,0.6733083724975586
translation,19,17,model,performance,extend,coda,performance extend coda,0.7164583802223206
translation,19,17,model,labeled summaries,extend,coda,labeled summaries extend coda,0.758137047290802
translation,19,17,model,coda,to,semisupervised settings,coda to semisupervised settings,0.5317879915237427
translation,19,17,model,coda,combine,coda,coda combine coda,0.7372279167175293
translation,19,17,model,coda,with,two -stage noisy self-training,coda with two -stage noisy self-training,0.6565250754356384
translation,19,17,model,semi-coda,combine,coda,semi-coda combine coda,0.7598025798797607
translation,19,17,model,coda,with,two -stage noisy self-training,coda with two -stage noisy self-training,0.6565250754356384
translation,19,17,model,two -stage noisy self-training,to utilize,conversations,two -stage noisy self-training to utilize conversations,0.6516345143318176
translation,19,17,model,conversations,has,without annotated summaries,conversations has without annotated summaries,0.5874492526054382
translation,19,17,model,model,To further enhance,performance,model To further enhance performance,0.7176869511604309
translation,19,17,model,model,extend,coda,model extend coda,0.7550089359283447
translation,19,17,model,model,combine,coda,model combine coda,0.7485147714614868
translation,20,36,experiments,abstractive model,on,several conversation datasets,abstractive model on several conversation datasets,0.49282968044281006
translation,20,36,experiments,dialogue summarization,from,"samsum ( gliwa et al. , 2019 b","dialogue summarization from samsum ( gliwa et al. , 2019 b",0.5285897254943848
translation,20,36,experiments,heuristic -generated community question answering,from,"cqasumm ( chowdhury and chakraborty , 2018 )","heuristic -generated community question answering from cqasumm ( chowdhury and chakraborty , 2018 )",0.5663018226623535
translation,20,36,experiments,meeting summarization data,from,ami and icsi,meeting summarization data from ami and icsi,0.5638335943222046
translation,20,36,experiments,smaller test sets,in,"news comments , discussion forum , and email domains","smaller test sets in news comments , discussion forum , and email domains",0.47289150953292847
translation,20,69,experiments,community question answering subdomain,use,stackexchange ( stack ),community question answering subdomain use stackexchange ( stack ),0.6635946035385132
translation,20,69,experiments,stackexchange ( stack ),provides access to,all forums,stackexchange ( stack ) provides access to all forums,0.7418624758720398
translation,20,179,experiments,base abstractive text summarization model,is,"bart - large ( lewis et al. , 2020 )","base abstractive text summarization model is bart - large ( lewis et al. , 2020 )",0.5418781042098999
translation,20,179,experiments,pretrained denoising autoencoder,with,336 m parameters,pretrained denoising autoencoder with 336 m parameters,0.5976725220680237
translation,20,179,experiments,pretrained denoising autoencoder,builds on,sequence -,pretrained denoising autoencoder builds on sequence -,0.6461870670318604
translation,20,179,experiments,"bart - large ( lewis et al. , 2020 )",has,pretrained denoising autoencoder,"bart - large ( lewis et al. , 2020 ) has pretrained denoising autoencoder",0.5427417755126953
translation,20,180,hyperparameters,bart,using,polynomial decay learning rate scheduler,bart using polynomial decay learning rate scheduler,0.6659263968467712
translation,20,180,hyperparameters,polynomial decay learning rate scheduler,with,adam optimizer,polynomial decay learning rate scheduler with adam optimizer,0.5891664028167725
translation,20,180,hyperparameters,hyperparameters,finetune,bart,hyperparameters finetune bart,0.6966188549995422
translation,20,181,hyperparameters,learning rate,of,3e - 5,learning rate of 3e - 5,0.6390295624732971
translation,20,181,hyperparameters,learning rate,of,warmup,learning rate of warmup,0.6157140731811523
translation,20,181,hyperparameters,total updates,of,20 and 200,total updates of 20 and 200,0.6485185623168945
translation,20,186,hyperparameters,longformer model,with,bart parameters,longformer model with bart parameters,0.6341959834098816
translation,20,186,hyperparameters,bart parameters,trained on,cnn - dailymail dataset,bart parameters trained on cnn - dailymail dataset,0.6878397464752197
translation,20,186,hyperparameters,hyperparameters,initialize,longformer model,hyperparameters initialize longformer model,0.7417515516281128
translation,20,6,model,annotation protocols,motivated by,issues-viewpoints - assertions framework,annotation protocols motivated by issues-viewpoints - assertions framework,0.6947619915008545
translation,20,6,model,issues-viewpoints - assertions framework,to crowdsource,four new datasets,issues-viewpoints - assertions framework to crowdsource four new datasets,0.743649959564209
translation,20,6,model,four new datasets,on,diverse online conversation forms,four new datasets on diverse online conversation forms,0.5004815459251404
translation,20,6,model,four new datasets,on,discussion forums,four new datasets on discussion forums,0.5295764207839966
translation,20,6,model,diverse online conversation forms,of,news comments,diverse online conversation forms of news comments,0.5417692065238953
translation,20,6,model,diverse online conversation forms,of,discussion forums,diverse online conversation forms of discussion forums,0.5376749038696289
translation,20,6,model,diverse online conversation forms,of,community question answering forums,diverse online conversation forms of community question answering forums,0.5245707631111145
translation,20,6,model,diverse online conversation forms,of,email threads,diverse online conversation forms of email threads,0.5324046015739441
translation,20,6,model,model,design,annotation protocols,model design annotation protocols,0.5762853026390076
translation,20,120,results,redundancy,see,reddit,redundancy see reddit,0.6882511973381042
translation,20,120,results,reddit,shows,most uniform distribution,reddit shows most uniform distribution,0.7086725234985352
translation,20,120,results,most uniform distribution,of,semantic units,most uniform distribution of semantic units,0.589381754398346
translation,20,120,results,results,For,redundancy,results For redundancy,0.5514267683029175
translation,20,191,results,unsupervised extractive models,perform,well below,unsupervised extractive models perform well below,0.5787054300308228
translation,20,191,results,well below,has,extractive oracle performance,well below has extractive oracle performance,0.5764821767807007
translation,20,191,results,results,has,unsupervised extractive models,results has unsupervised extractive models,0.5504536628723145
translation,20,195,results,abstractive model,performs,extractive oracle,abstractive model performs extractive oracle,0.6007533073425293
translation,20,195,results,abstractive model,at or above,extractive oracle,abstractive model at or above extractive oracle,0.46613115072250366
translation,20,195,results,results,has,abstractive model,results has abstractive model,0.5396823287010193
translation,20,196,results,results,train on,argument mining - based approaches,results train on argument mining - based approaches,0.7420101761817932
translation,20,197,results,rouge improvements,when applying,bart - arggraph,rouge improvements when applying bart - arggraph,0.7067720890045166
translation,20,197,results,rouge improvements,when applying,stack data,rouge improvements when applying stack data,0.7115992903709412
translation,20,197,results,bart - arggraph,for,reddit,bart - arggraph for reddit,0.6696990132331848
translation,20,197,results,bart - arggraph,for,stack data,bart - arggraph for stack data,0.6486015319824219
translation,20,197,results,results,see,rouge improvements,results see rouge improvements,0.5589802265167236
translation,20,198,results,- arg- filtered variation,is,less noisy version,- arg- filtered variation is less noisy version,0.6142236590385437
translation,20,198,results,less noisy version,of,input,less noisy version of input,0.596606433391571
translation,20,198,results,- arg - graph variation,on,email and nyt data,- arg - graph variation on email and nyt data,0.5986864566802979
translation,20,198,results,- arg- filtered variation,has,outperformed,- arg- filtered variation has outperformed,0.6433961391448975
translation,20,198,results,outperformed,has,- arg - graph variation,outperformed has - arg - graph variation,0.6407515406608582
translation,20,198,results,results,has,- arg- filtered variation,results has - arg- filtered variation,0.6061135530471802
translation,20,203,results,improved results,using,- arg - filtered,improved results using - arg - filtered,0.7115985751152039
translation,20,203,results,- arg - filtered,to filter,non-argumentative units,- arg - filtered to filter non-argumentative units,0.6869715452194214
translation,20,203,results,non-argumentative units,incorporating,graph structure,non-argumentative units incorporating graph structure,0.6976708769798279
translation,20,203,results,results,found,improved results,results found improved results,0.6949815154075623
translation,20,205,results,longformer model,performs,as well or better,longformer model performs as well or better,0.6494458317756653
translation,20,205,results,as well or better,than,previous state - of - the - art results,as well or better than previous state - of - the - art results,0.5856916308403015
translation,20,205,results,results,has,longformer model,results has longformer model,0.5375713109970093
translation,20,208,results,prior work,on,samsum,prior work on samsum,0.5646991729736328
translation,20,208,results,"chowdhury and chakraborty , 2018 )",with,bart and bart - arggraph models,"chowdhury and chakraborty , 2018 ) with bart and bart - arggraph models",0.6293259859085083
translation,20,208,results,outperform,has,prior work,outperform has prior work,0.5811013579368591
translation,20,208,results,results,has,outperform,results has outperform,0.642206609249115
translation,20,210,results,bart and bart - arg - graph models,from,our email and news -comment data,bart and bart - arg - graph models from our email and news -comment data,0.5651206970214844
translation,20,210,results,results,of,bart and bart - arg - graph models,results of bart and bart - arg - graph models,0.5077636241912842
translation,20,222,results,annotators,prefer,our argument mining - based approaches,annotators prefer our argument mining - based approaches,0.6773112416267395
translation,20,222,results,our argument mining - based approaches,in,both dimensions,our argument mining - based approaches in both dimensions,0.5493886470794678
translation,20,222,results,results,find that,annotators,results find that annotators,0.5359574556350708
translation,21,136,ablation-analysis,pgn ( d rd ),contributes,most,pgn ( d rd ) contributes most,0.6883311867713928
translation,21,136,ablation-analysis,ami,has,pgn ( d rd ),ami has pgn ( d rd ),0.6673777103424072
translation,21,136,ablation-analysis,ablation analysis,For,ami,ablation analysis For ami,0.6474417448043823
translation,21,159,ablation-analysis,entities,play,important role,entities play important role,0.7281543016433716
translation,21,159,ablation-analysis,important role,in,summary generation,important role in summary generation,0.5023067593574524
translation,21,159,ablation-analysis,ablation analysis,show,entities,ablation analysis show entities,0.6587666869163513
translation,21,107,baselines,pgn,is,lstm - based model,pgn is lstm - based model,0.5517714619636536
translation,21,107,baselines,baselines,has,other one,baselines has other one,0.6312651634216309
translation,21,109,baselines,"transformer ( vaswani et al. , 2017 )",as,backbone architecture,"transformer ( vaswani et al. , 2017 ) as backbone architecture",0.5199054479598999
translation,21,117,baselines,longest - 3,views,first three utterances,longest - 3 views first three utterances,0.6488093137741089
translation,21,117,baselines,first three utterances,as,summary,first three utterances as summary,0.564077615737915
translation,21,117,baselines,samsum,has,longest - 3,samsum has longest - 3,0.6870481967926025
translation,21,118,baselines,textrank,is,traditional graph - based method,textrank is traditional graph - based method,0.5405217409133911
translation,21,118,baselines,baselines,has,textrank,baselines has textrank,0.5301661491394043
translation,21,119,baselines,"transformer ( vaswani et al. , 2017 )",is,seq2seq method,"transformer ( vaswani et al. , 2017 ) is seq2seq method",0.5433552861213684
translation,21,119,baselines,seq2seq method,based on,full self-attention operations,seq2seq method based on full self-attention operations,0.6641507744789124
translation,21,119,baselines,baselines,has,"transformer ( vaswani et al. , 2017 )","baselines has transformer ( vaswani et al. , 2017 )",0.5352681279182434
translation,21,120,baselines,d-hgn,incorporates,commonsense knowledge,d-hgn incorporates commonsense knowledge,0.7392563819885254
translation,21,120,baselines,commonsense knowledge,to help,understand,commonsense knowledge to help understand,0.6763061285018921
translation,21,120,baselines,d-hgn,has,"feng et al. , 2020a","d-hgn has feng et al. , 2020a",0.5933523178100586
translation,21,120,baselines,understand,has,dialogues,understand has dialogues,0.6280289888381958
translation,21,120,baselines,baselines,has,d-hgn,baselines has d-hgn,0.5571632385253906
translation,21,121,baselines,"tgdga ( zhao et al. , 2020 )",uses,topic words,"tgdga ( zhao et al. , 2020 ) uses topic words",0.5817497372627258
translation,21,121,baselines,"tgdga ( zhao et al. , 2020 )",models,graph structures,"tgdga ( zhao et al. , 2020 ) models graph structures",0.7369193434715271
translation,21,121,baselines,graph structures,for,dialogues,graph structures for dialogues,0.608105480670929
translation,21,121,baselines,baselines,has,"tgdga ( zhao et al. , 2020 )","baselines has tgdga ( zhao et al. , 2020 )",0.5284493565559387
translation,21,122,baselines,dialogpt,means that,finetuning,dialogpt means that finetuning,0.6627928018569946
translation,21,122,baselines,dialogpt,on,samsum,dialogpt on samsum,0.6132363080978394
translation,21,122,baselines,dialogpt,on,samsum,dialogpt on samsum,0.6132363080978394
translation,21,122,baselines,dialogpt,has,"zhang et al. , 2020 b","dialogpt has zhang et al. , 2020 b",0.578278660774231
translation,21,122,baselines,finetuning,has,dialogpt,finetuning has dialogpt,0.5534536838531494
translation,21,122,baselines,baselines,has,dialogpt,baselines has dialogpt,0.6045893430709839
translation,21,124,baselines,summarunner,is,extractive method,summarunner is extractive method,0.6016300916671753
translation,21,124,baselines,extractive method,based on,hierarchical rnn network,extractive method based on hierarchical rnn network,0.6899286508560181
translation,21,124,baselines,ami,has,summarunner,ami has summarunner,0.6760910749435425
translation,21,125,baselines,"uns ( shang et al. , 2018 )",is,fully unsupervised and graph- based method,"uns ( shang et al. , 2018 ) is fully unsupervised and graph- based method",0.5377293229103088
translation,21,125,baselines,baselines,has,"uns ( shang et al. , 2018 )","baselines has uns ( shang et al. , 2018 )",0.5221564769744873
translation,21,126,baselines,"topicseg ( li et al. , 2019 )",incorporates,topics,"topicseg ( li et al. , 2019 ) incorporates topics",0.6973448991775513
translation,21,126,baselines,topics,to model,meeting,topics to model meeting,0.6848713159561157
translation,21,126,baselines,baselines,has,"topicseg ( li et al. , 2019 )","baselines has topicseg ( li et al. , 2019 )",0.5291662812232971
translation,21,127,baselines,"hmnet ( zhu et al. , 2020 )",is,transformer - based method,"hmnet ( zhu et al. , 2020 ) is transformer - based method",0.5777319073677063
translation,21,127,baselines,"hmnet ( zhu et al. , 2020 )",pre-trained on,news summarization dataset,"hmnet ( zhu et al. , 2020 ) pre-trained on news summarization dataset",0.7377849221229553
translation,21,127,baselines,transformer - based method,incorporates,pos and entity information,transformer - based method incorporates pos and entity information,0.6860430240631104
translation,21,127,baselines,transformer - based method,pre-trained on,news summarization dataset,transformer - based method pre-trained on news summarization dataset,0.7213055491447449
translation,21,127,baselines,baselines,has,"hmnet ( zhu et al. , 2020 )","baselines has hmnet ( zhu et al. , 2020 )",0.5214114785194397
translation,21,7,experiments,dialogpt,to label,three types of features,dialogpt to label three types of features,0.7057822942733765
translation,21,7,experiments,dialogpt,employ,pre-trained and non pre-trained models,dialogpt employ pre-trained and non pre-trained models,0.5528987646102905
translation,21,7,experiments,three types of features,on,two dialogue summarization datasets,three types of features on two dialogue summarization datasets,0.514053463935852
translation,21,7,experiments,pre-trained and non pre-trained models,as,our summarizers,pre-trained and non pre-trained models as our summarizers,0.4976382553577423
translation,21,33,hyperparameters,"non pre-trained pgn ( see et al. , 2017 )",as,our summarizers,"non pre-trained pgn ( see et al. , 2017 ) as our summarizers",0.4781555235385895
translation,21,33,hyperparameters,hyperparameters,employ,"pre-traind bart ( lewis et al. , 2020 )","hyperparameters employ pre-traind bart ( lewis et al. , 2020 )",0.509105384349823
translation,21,33,hyperparameters,hyperparameters,employ,"non pre-trained pgn ( see et al. , 2017 )","hyperparameters employ non pre-trained pgn ( see et al. , 2017 )",0.48414576053619385
translation,21,6,model,"dialogpt ( zhang et al. , 2020 b )",developed as,unsupervised dialogue annotator,"dialogpt ( zhang et al. , 2020 b ) developed as unsupervised dialogue annotator",0.6172537803649902
translation,21,6,model,unsupervised dialogue annotator,takes advantage of,dialogue background knowledge,unsupervised dialogue annotator takes advantage of dialogue background knowledge,0.5823336243629456
translation,21,6,model,dialogue background knowledge,encoded in,dialogpt,dialogue background knowledge encoded in dialogpt,0.7182513475418091
translation,21,6,model,model,show,"dialogpt ( zhang et al. , 2020 b )","model show dialogpt ( zhang et al. , 2020 b )",0.6629492044448853
translation,21,108,model,typical sequence - to-sequence framework,first encodes,source dialogue d,typical sequence - to-sequence framework first encodes source dialogue d,0.7063793540000916
translation,21,108,model,source dialogue d,to,distributed representations,source dialogue d to distributed representations,0.516116738319397
translation,21,108,model,target summary s,with,decoder,target summary s with decoder,0.6680151224136353
translation,21,108,model,model,inherit,typical sequence - to-sequence framework,model inherit typical sequence - to-sequence framework,0.5955088138580322
translation,21,132,results,bart and pgn,obtain,improvements,bart and pgn obtain improvements,0.6269180178642273
translation,21,132,results,"our annotated datasets d ke , d rd and d ts",has,bart and pgn,"our annotated datasets d ke , d rd and d ts has bart and pgn",0.6067095398902893
translation,21,132,results,results,see that,"our annotated datasets d ke , d rd and d ts","results see that our annotated datasets d ke , d rd and d ts",0.6262189745903015
translation,21,132,results,results,using,"our annotated datasets d ke , d rd and d ts","results using our annotated datasets d ke , d rd and d ts",0.6174526810646057
translation,21,133,results,bart ( d all ),achieves,sota performance,bart ( d all ) achieves sota performance,0.7217171788215637
translation,21,133,results,results,has,bart ( d all ),results has bart ( d all ),0.5870583057403564
translation,21,134,results,samsum,worth noting,bart ( d ke ),samsum worth noting bart ( d ke ),0.6792984008789062
translation,21,134,results,bart ( d ke ),performs,better,bart ( d ke ) performs better,0.6570988893508911
translation,21,134,results,better,compared with,bart ( d rd ),better compared with bart ( d rd ),0.6981693506240845
translation,21,134,results,better,compared with,bart ( d ts ),better compared with bart ( d ts ),0.6874991059303284
translation,21,134,results,results,For,samsum,results For samsum,0.6055014133453369
translation,21,147,results,our method,achieve,higher scores,our method achieve higher scores,0.5890278220176697
translation,21,147,results,higher scores,in,all three metrics,higher scores in all three metrics,0.4945247173309326
translation,21,147,results,results,see that,our method,results see that our method,0.631170928478241
translation,21,148,results,our model,get,best score,our model get best score,0.5948446393013
translation,21,148,results,best score,in,conciseness,best score in conciseness,0.47605428099632263
translation,21,148,results,d rd,has,our model,d rd has our model,0.6121900081634521
translation,21,148,results,results,combined with,d rd,results combined with d rd,0.7069578170776367
translation,21,149,results,our model,can perform,better,our model can perform better,0.7557834386825562
translation,21,149,results,better,in,coverage,better in coverage,0.5370565056800842
translation,21,149,results,d ts,has,our model,d ts has our model,0.6064794659614563
translation,21,150,results,hmnet,gets,best score,hmnet gets best score,0.5689134001731873
translation,21,150,results,best score,in,informativeness and coverage,best score in informativeness and coverage,0.5088738799095154
translation,21,150,results,results,has,hmnet,results has hmnet,0.575340986251831
translation,21,158,results,our method,achieves,higher scores,our method achieves higher scores,0.6174023747444153
translation,21,158,results,results,see that,our method,results see that our method,0.631170928478241
translation,21,160,results,keybert,get,better results,keybert get better results,0.649476170539856
translation,21,160,results,dialogpt embeddings,has,keybert,dialogpt embeddings has keybert,0.5795796513557434
translation,21,163,results,entities,as,keywords,entities as keywords,0.47742196917533875
translation,21,163,results,entities,get,best precision score,entities get best precision score,0.5800127387046814
translation,21,163,results,results,Directly using,entities,results Directly using entities,0.6108822226524353
translation,21,164,results,textrank and entities,perform,poorly,textrank and entities perform poorly,0.5856591463088989
translation,21,164,results,poorly,in,recall,poorly in recall,0.5551996231079102
translation,21,164,results,results,has,textrank and entities,results has textrank and entities,0.5619566440582275
translation,21,165,results,our method,gets,best score,our method gets best score,0.5734251141548157
translation,21,165,results,our method,extract,more diverse keywords,our method extract more diverse keywords,0.6505320072174072
translation,21,165,results,best score,in terms of,f 1,best score in terms of f 1,0.7136852741241455
translation,21,165,results,results,has,our method,results has our method,0.5589964985847473
translation,21,169,results,our method,performs,better,our method performs better,0.6336022615432739
translation,21,169,results,results,see that,our method,results see that our method,0.631170928478241
translation,21,176,results,our method,get,comparable results,our method get comparable results,0.541016697883606
translation,21,176,results,comparable results,with,strong baseline c99 ( w / dialogpt emb ),comparable results with strong baseline c99 ( w / dialogpt emb ),0.6620492935180664
translation,21,176,results,results,see that,our method,results see that our method,0.631170928478241
translation,21,177,results,pgn,achieve,best result,pgn achieve best result,0.6496161222457886
translation,21,177,results,ami,has,pgn,ami has pgn,0.7351860404014587
translation,21,177,results,golden topic annotation,has,pgn,golden topic annotation has pgn,0.5581324100494385
translation,21,177,results,results,For,ami,results For ami,0.554703950881958
translation,22,7,model,multi-source pretraining paradigm,to better leverage,external summary data,multi-source pretraining paradigm to better leverage external summary data,0.694794774055481
translation,22,7,model,model,propose,multi-source pretraining paradigm,model propose multi-source pretraining paradigm,0.6297709345817566
translation,22,8,model,large-scale in- domain nonsummary data,to separately pretrain,dialogue encoder and the summary decoder,large-scale in- domain nonsummary data to separately pretrain dialogue encoder and the summary decoder,0.681509792804718
translation,22,8,model,model,exploit,large-scale in- domain nonsummary data,model exploit large-scale in- domain nonsummary data,0.6897851824760437
translation,22,9,model,combined encoder-decoder model,pretrained on,out-of- domain summary data,combined encoder-decoder model pretrained on out-of- domain summary data,0.7687072157859802
translation,22,9,model,out-of- domain summary data,using,adversarial critics,out-of- domain summary data using adversarial critics,0.641096830368042
translation,22,9,model,adversarial critics,to facilitate,domain-agnostic summarization,adversarial critics to facilitate domain-agnostic summarization,0.6299073100090027
translation,22,9,model,model,has,combined encoder-decoder model,model has combined encoder-decoder model,0.5629696249961853
translation,22,25,model,novel pretraining paradigm,called,domain-agnostic multi-source pretraining ( dams ),novel pretraining paradigm called domain-agnostic multi-source pretraining ( dams ),0.6572556495666504
translation,22,25,model,novel pretraining paradigm,to summarize,dialogues,novel pretraining paradigm to summarize dialogues,0.7222744226455688
translation,22,25,model,dialogues,in,low-resource setting,dialogues in low-resource setting,0.5524715185165405
translation,22,25,model,model,introduce,novel pretraining paradigm,model introduce novel pretraining paradigm,0.6431609392166138
translation,22,26,model,pretraining,of,dialogue summarization,pretraining of dialogue summarization,0.5317040681838989
translation,22,28,model,summary decoder,pretrained on,large-scale summarylike short texts,summary decoder pretrained on large-scale summarylike short texts,0.7385729551315308
translation,22,28,model,large-scale summarylike short texts,to learn,language model,large-scale summarylike short texts to learn language model,0.5649698376655579
translation,22,28,model,language model,in the style of,dialogue summaries,language model in the style of dialogue summaries,0.6739495992660522
translation,22,28,model,model,has,summary decoder,model has summary decoder,0.5919745564460754
translation,22,29,model,encoder and decoder,combined and pretrained on,external summary data,encoder and decoder combined and pretrained on external summary data,0.7580891251564026
translation,22,29,model,external summary data,to go through,integral process,external summary data to go through integral process,0.6726195216178894
translation,22,29,model,integral process,of,summarization,integral process of summarization,0.6030057668685913
translation,22,29,model,model,has,encoder and decoder,model has encoder and decoder,0.5709518790245056
translation,22,32,model,adversarial critics,to capture,features,adversarial critics to capture features,0.6817454099655151
translation,22,32,model,features,shared between,dialogues and general documents,features shared between dialogues and general documents,0.7206833362579346
translation,22,32,model,model,has,adversarial critics,model has adversarial critics,0.5435144305229187
translation,23,197,ablation-analysis,mtls,underperforms,tls,mtls underperforms tls,0.754882276058197
translation,23,197,ablation-analysis,tls,by,"15.1 % , 4.8 %","tls by 15.1 % , 4.8 %",0.5738596320152283
translation,23,197,ablation-analysis,"15.1 % , 4.8 %",in terms of,align + m:1 rouge - 1,"15.1 % , 4.8 % in terms of align + m:1 rouge - 1",0.7165841460227966
translation,23,197,ablation-analysis,"l = 1 , 2",has,mtls,"l = 1 , 2 has mtls",0.6042149662971497
translation,23,197,ablation-analysis,ablation analysis,observe,"l = 1 , 2","ablation analysis observe l = 1 , 2",0.6286491751670837
translation,23,197,ablation-analysis,ablation analysis,observe,mtls,ablation analysis observe mtls,0.6161504983901978
translation,23,197,ablation-analysis,ablation analysis,when,"l = 1 , 2","ablation analysis when l = 1 , 2",0.6671160459518433
translation,23,217,ablation-analysis,without es,d-select and align + m:1 ROUGE - 2 scores,decrease,without es d-select and align + m:1 ROUGE - 2 scores decrease,0.7625724673271179
translation,23,217,ablation-analysis,14.6 % and 42.2 %,compared with,2saps,14.6 % and 42.2 % compared with 2saps,0.6954652070999146
translation,23,217,ablation-analysis,decrease,has,14.6 % and 42.2 %,decrease has 14.6 % and 42.2 %,0.5700075626373291
translation,23,217,ablation-analysis,ablation analysis,observe,without es,ablation analysis observe without es,0.6031621694564819
translation,23,219,ablation-analysis,generated timeline set,tends to contain,noisy timelines,generated timeline set tends to contain noisy timelines,0.6806015372276306
translation,23,219,ablation-analysis,noisy timelines,causing,low rouge - 1,noisy timelines causing low rouge - 1,0.687637448310852
translation,23,219,ablation-analysis,low rouge - 1,as,performance,low rouge - 1 as performance,0.5453838109970093
translation,23,219,ablation-analysis,drops,by,18.8 %,drops by 18.8 %,0.6354024410247803
translation,23,219,ablation-analysis,ts component,has,generated timeline set,ts component has generated timeline set,0.5625139474868774
translation,23,219,ablation-analysis,performance,has,drops,performance has drops,0.5993483662605286
translation,23,219,ablation-analysis,ablation analysis,without,ts component,ablation analysis without ts component,0.7044093012809753
translation,23,162,baselines,baselines,as,divide-and-summarize   approaches,baselines as divide-and-summarize   approaches,0.5135977268218994
translation,23,162,baselines,baselines,design,baselines,baselines design baselines,0.5706356167793274
translation,23,162,baselines,baselines,as,divide-and-summarize   approaches,baselines as divide-and-summarize   approaches,0.5135977268218994
translation,23,174,baselines,frequently used unsupervised tls baseline,selects,top-ranked sentences,frequently used unsupervised tls baseline selects top-ranked sentences,0.6193233132362366
translation,23,174,baselines,top-ranked sentences,based on,summed similaries,top-ranked sentences based on summed similaries,0.6273117661476135
translation,23,174,baselines,summed similaries,within,n-day window,summed similaries within n-day window,0.6937169432640076
translation,23,175,baselines,baselines,has,martschat2018,baselines has martschat2018,0.5705093741416931
translation,23,172,experimental-setup,k-means algorithm,in,scikit-learn,k-means algorithm in scikit-learn,0.5263255834579468
translation,23,172,experimental-setup,experimental setup,use,k-means algorithm,experimental setup use k-means algorithm,0.6241795420646667
translation,23,23,model,model,propose,two -stage affinity propagation summarization framework ( 2saps ),model propose two -stage affinity propagation summarization framework ( 2saps ),0.6651595830917358
translation,23,24,model,temporal information,embedded in,sentences,temporal information embedded in sentences,0.697449803352356
translation,23,24,model,sentences,to discover,important events,sentences to discover important events,0.6514962911605835
translation,23,24,model,linking information,latent in,news articles,linking information latent in news articles,0.6372577548027039
translation,23,24,model,model,uses,temporal information,model uses temporal information,0.6197836995124817
translation,23,17,results,mtls,automatically generates,set of timelines,mtls automatically generates set of timelines,0.746066153049469
translation,23,17,results,set of timelines,summarize,disparate yet important stories,set of timelines summarize disparate yet important stories,0.6718956232070923
translation,23,17,results,results,has,mtls,results has mtls,0.563865602016449
translation,23,198,results,tls,by,"150 % , 117.1 % , and 94.7 %","tls by 150 % , 117.1 % , and 94.7 %",0.5485313534736633
translation,23,198,results,"150 % , 117.1 % , and 94.7 %",when,"l equals 3,4,5","150 % , 117.1 % , and 94.7 % when l equals 3,4,5",0.6273670792579651
translation,23,198,results,outperforms,has,tls,outperforms has tls,0.6357085108757019
translation,23,207,results,2saps,achieves,best performance,2saps achieves best performance,0.7133826613426208
translation,23,207,results,best performance,in terms of,all rouge metrics,best performance in terms of all rouge metrics,0.6130699515342712
translation,23,207,results,results,observe,2saps,results observe 2saps,0.5715556740760803
translation,23,208,results,ghalandari2020,in terms of,concat rouge - 1 score,ghalandari2020 in terms of concat rouge - 1 score,0.6857220530509949
translation,23,208,results,outperforms,by,"52.9 % , 12.2 % , and 16.4 %","outperforms by 52.9 % , 12.2 % , and 16.4 %",0.6060534119606018
translation,23,208,results,chieu2004,has,martschat2018,chieu2004 has martschat2018,0.6271286606788635
translation,23,208,results,results,compared with,chieu2004,results compared with chieu2004,0.6618269681930542
translation,23,209,results,ghalandari2020 method,achieves,best performance,ghalandari2020 method achieves best performance,0.6495532989501953
translation,23,209,results,best performance,among,baselines,best performance among baselines,0.6057958602905273
translation,23,209,results,baselines,except for,concat rouge -1,baselines except for concat rouge -1,0.6412498354911804
translation,23,209,results,results,observe,ghalandari2020 method,results observe ghalandari2020 method,0.5896794199943542
translation,23,210,results,kmeans,works,best,kmeans works best,0.6426859498023987
translation,23,210,results,best,in,dividing datasets,best in dividing datasets,0.5208999514579773
translation,23,211,results,random and lda,by,15 % and 7.2 %,random and lda by 15 % and 7.2 %,0.5758245587348938
translation,23,211,results,random and lda,in terms of,concat rouge -1,random and lda in terms of concat rouge -1,0.6659016013145447
translation,23,211,results,k-means,has,outperforms,k-means has outperforms,0.6320335865020752
translation,23,211,results,outperforms,has,random and lda,outperforms has random and lda,0.6158967018127441
translation,23,212,results,our 2saps,outperforms it by,"9.9 % , 15.1 % , 0 % , 10 % , 4.7 % , 3.6 % , 19.1 %","our 2saps outperforms it by 9.9 % , 15.1 % , 0 % , 10 % , 4.7 % , 3.6 % , 19.1 %",0.725293755531311
translation,23,212,results,best-performing baseline,has,k-means - ghalandari2020,best-performing baseline has k-means - ghalandari2020,0.5667633414268494
translation,23,212,results,best-performing baseline,has,our 2saps,best-performing baseline has our 2saps,0.5585995316505432
translation,23,212,results,k-means - ghalandari2020,has,our 2saps,k-means - ghalandari2020 has our 2saps,0.6159916520118713
translation,23,212,results,results,compared with,best-performing baseline,results compared with best-performing baseline,0.7183142900466919
translation,24,134,baselines,match -sum,is,comparable competitor,match -sum is comparable competitor,0.5944640040397644
translation,24,134,baselines,match -sum,extract,thenmatch summary,match -sum extract thenmatch summary,0.6560978889465332
translation,24,134,baselines,comparable competitor,to,our differ -sum,comparable competitor to our differ -sum,0.6646512150764465
translation,24,134,baselines,comparable competitor,formulates,extractive summarization task,comparable competitor formulates extractive summarization task,0.6381739377975464
translation,24,134,baselines,extractive summarization task,as,two-step problem,extractive summarization task as two-step problem,0.5327151417732239
translation,24,134,baselines,thenmatch summary,based on,well - trained bert - sumext,thenmatch summary based on well - trained bert - sumext,0.66636723279953
translation,24,134,baselines,baselines,has,match -sum,baselines has match -sum,0.5928851366043091
translation,24,115,experimental-setup,code,based on,"pytorch ( paszke et al. , 2019 )","code based on pytorch ( paszke et al. , 2019 )",0.6539609432220459
translation,24,115,experimental-setup,pre-trained model,employed in,differsum,pre-trained model employed in differsum,0.6834653615951538
translation,24,115,experimental-setup,pre-trained model,employed in,differsum,pre-trained model employed in differsum,0.6834653615951538
translation,24,115,experimental-setup,pre-trained model,based on,huggingface,pre-trained model based on huggingface,0.6228824257850647
translation,24,115,experimental-setup,differsum,is,albert- xxlarge - v2,differsum is albert- xxlarge - v2,0.6377697587013245
translation,24,115,experimental-setup,differsum,train,two days,differsum train two days,0.6779245138168335
translation,24,115,experimental-setup,differsum,on,"2 gpus ( nvidia tesla v100 , 32gb )","differsum on 2 gpus ( nvidia tesla v100 , 32gb )",0.5439937114715576
translation,24,115,experimental-setup,two days,for,"100,000 steps","two days for 100,000 steps",0.6624603271484375
translation,24,115,experimental-setup,two days,on,"2 gpus ( nvidia tesla v100 , 32gb )","two days on 2 gpus ( nvidia tesla v100 , 32gb )",0.5330265164375305
translation,24,115,experimental-setup,"100,000 steps",on,"2 gpus ( nvidia tesla v100 , 32gb )","100,000 steps on 2 gpus ( nvidia tesla v100 , 32gb )",0.5216022729873657
translation,24,115,experimental-setup,"2 gpus ( nvidia tesla v100 , 32gb )",with,gradient accumulation,"2 gpus ( nvidia tesla v100 , 32gb ) with gradient accumulation",0.6051123738288879
translation,24,115,experimental-setup,gradient accumulation,every,two steps,gradient accumulation every two steps,0.6030693054199219
translation,24,115,experimental-setup,differsum,has,two days,differsum has two days,0.6419637799263
translation,24,115,experimental-setup,experimental setup,train,differsum,experimental setup train differsum,0.6558526158332825
translation,24,115,experimental-setup,experimental setup,train,two days,experimental setup train two days,0.6454251408576965
translation,24,115,experimental-setup,experimental setup,has,code,experimental setup has code,0.5089583992958069
translation,24,116,experimental-setup,adam,with,"? 1 = 0.9 , ? 2 = 0.999","adam with ? 1 = 0.9 , ? 2 = 0.999",0.6669601798057556
translation,24,116,experimental-setup,"? 1 = 0.9 , ? 2 = 0.999",used as,optimizer,"? 1 = 0.9 , ? 2 = 0.999 used as optimizer",0.6552394032478333
translation,24,116,experimental-setup,experimental setup,has,adam,experimental setup has adam,0.4964992105960846
translation,24,117,experimental-setup,learning rate schedule,follows,strategy,learning rate schedule follows strategy,0.7209322452545166
translation,24,117,experimental-setup,strategy,with,warming - up,strategy with warming - up,0.685965895652771
translation,24,117,experimental-setup,warming - up,on,"first 10,000 steps","warming - up on first 10,000 steps",0.5722484588623047
translation,24,117,experimental-setup,experimental setup,has,learning rate schedule,experimental setup has learning rate schedule,0.504158079624176
translation,24,118,experimental-setup,iteration steps,of,2/4/6/8,iteration steps of 2/4/6/8,0.6001441478729248
translation,24,118,experimental-setup,iteration steps,of,k = 4,iteration steps of k = 4,0.6182352304458618
translation,24,118,experimental-setup,2/4/6/8,for,iterative refinement,2/4/6/8 for iterative refinement,0.5891336798667908
translation,24,118,experimental-setup,best choice,based on,validation set,best choice based on validation set,0.6469578146934509
translation,24,118,experimental-setup,experimental setup,tried,iteration steps,experimental setup tried iteration steps,0.6786606907844543
translation,24,118,experimental-setup,experimental setup,tried,k = 4,experimental setup tried k = 4,0.6834715604782104
translation,24,6,model,single-document extractive summarization,present,deep differential amplifier framework,single-document extractive summarization present deep differential amplifier framework,0.6263822913169861
translation,24,6,model,deep differential amplifier framework,to enhance,features of summary sentences,deep differential amplifier framework to enhance features of summary sentences,0.6634117364883423
translation,24,6,model,model,consider,single-document extractive summarization,model consider single-document extractive summarization,0.6604874134063721
translation,24,6,model,model,present,deep differential amplifier framework,model present deep differential amplifier framework,0.6479440331459045
translation,24,7,model,semantic difference,between,each sentence and other sentences,semantic difference between each sentence and other sentences,0.6151624917984009
translation,24,7,model,semantic difference,apply,residual unit,semantic difference apply residual unit,0.5787061452865601
translation,24,7,model,residual unit,to deepen,differential amplifier architecture,residual unit to deepen differential amplifier architecture,0.5922755599021912
translation,24,7,model,model,calculate and amplify,semantic difference,model calculate and amplify semantic difference,0.6870619058609009
translation,24,8,model,corresponding objective loss,of,minority class,corresponding objective loss of minority class,0.553640604019165
translation,24,8,model,corresponding objective loss,boosted by,weighted cross-entropy,corresponding objective loss boosted by weighted cross-entropy,0.702895998954773
translation,24,8,model,model,has,corresponding objective loss,model has corresponding objective loss,0.528630793094635
translation,24,9,model,more attention,to,pivotal information,more attention to pivotal information,0.5901167392730713
translation,24,9,model,pivotal information,of,one sentence,pivotal information of one sentence,0.5803132653236389
translation,24,9,model,model,pays,more attention,model pays more attention,0.7802199721336365
translation,24,143,model,several strategies,to improve,performance,several strategies to improve performance,0.7717296481132507
translation,24,143,model,performance,of,extractive summarization,performance of extractive summarization,0.5829050540924072
translation,24,143,model,extractive summarization,including,differential amplifier ( vs. normal residual network ),extractive summarization including differential amplifier ( vs. normal residual network ),0.6908503174781799
translation,24,143,model,iterative refinement,has,vs. none ),iterative refinement has vs. none ),0.5611465573310852
translation,24,143,model,model,propose,several strategies,model propose several strategies,0.7299914956092834
translation,24,36,results,differsum,shows,superiority,differsum shows superiority,0.692957878112793
translation,24,36,results,superiority,over,other extractive methods,superiority over other extractive methods,0.6961408853530884
translation,24,36,results,other extractive methods,in,two aspects,other extractive methods in two aspects,0.5010345578193665
translation,24,36,results,representation,of,pivotal information,representation of pivotal information,0.6036432385444641
translation,24,36,results,enhancing,has,representation,enhancing has representation,0.6052634119987488
translation,24,36,results,results,has,differsum,results has differsum,0.5963534116744995
translation,24,125,results,results,on,cnn / dm,results on cnn / dm,0.548747181892395
translation,24,126,results,results,on,cnn / dailymail,results on cnn / dailymail,0.5091644525527954
translation,24,131,results,differsum,has,outperforms,differsum has outperforms,0.6740999817848206
translation,24,131,results,outperforms,has,all extractive baseline models,outperforms has all extractive baseline models,0.5666305422782898
translation,24,132,results,differsum,achieves,0.85/1.02/0.93 improvements,differsum achieves 0.85/1.02/0.93 improvements,0.6388841271400452
translation,24,132,results,0.85/1.02/0.93 improvements,on,"r -1 , r - 2 , and r -l","0.85/1.02/0.93 improvements on r -1 , r - 2 , and r -l",0.539970874786377
translation,24,132,results,large version bertsumext,has,differsum,large version bertsumext has differsum,0.599105179309845
translation,24,132,results,results,Compared with,large version bertsumext,results Compared with large version bertsumext,0.6771799325942993
translation,24,133,results,early approaches,observe,bert,early approaches observe bert,0.6311342120170593
translation,24,133,results,trigram - blocking,leads to,great improvement,trigram - blocking leads to great improvement,0.6865853667259216
translation,24,133,results,great improvement,on,all rouge metrics,great improvement on all rouge metrics,0.4940711557865143
translation,24,133,results,bert,has,outperforms,bert has outperforms,0.7181920409202576
translation,24,133,results,outperforms,has,all previous non-bert - based summarization systems,outperforms has all previous non-bert - based summarization systems,0.5661518573760986
translation,24,133,results,results,Compared with,early approaches,results Compared with early approaches,0.6811296343803406
translation,24,140,results,our differential amplifier modeling,performs,better,our differential amplifier modeling performs better,0.6448987722396851
translation,24,140,results,better,than,lstm and bert,better than lstm and bert,0.5858780145645142
translation,24,141,results,extractive approaches,show,superiority,extractive approaches show superiority,0.629276692867279
translation,24,141,results,superiority,over,abstractive models,superiority over abstractive models,0.7044422626495361
translation,24,141,results,rouge scores,higher than,cnn / dailymail,rouge scores higher than cnn / dailymail,0.663658618927002
translation,24,141,results,results,find that,extractive approaches,results find that extractive approaches,0.6205524206161499
translation,24,154,results,trigram blocking strategy,leads to,great improvement,trigram blocking strategy leads to great improvement,0.6937971711158752
translation,24,154,results,trigram blocking,leads to,great improvement,trigram blocking leads to great improvement,0.6905325651168823
translation,24,154,results,great improvement,on,all rouge metrics,great improvement on all rouge metrics,0.4940711557865143
translation,24,154,results,great improvement,for,many extractive approaches,great improvement for many extractive approaches,0.6054319739341736
translation,24,154,results,trigram blocking strategy,has,trigram blocking,trigram blocking strategy has trigram blocking,0.6148203015327454
translation,24,154,results,results,has,trigram blocking strategy,results has trigram blocking strategy,0.567201554775238
translation,24,183,results,differsum,is,more sensitive,differsum is more sensitive,0.5974084138870239
translation,24,183,results,more sensitive,to,word features,more sensitive to word features,0.5544799566268921
translation,24,183,results,results,has,differsum,results has differsum,0.5963534116744995
translation,26,188,ablation-analysis,temporal regularizer,results in,consistent performance drop,temporal regularizer results in consistent performance drop,0.5820472836494446
translation,26,188,ablation-analysis,consistent performance drop,on,date f1,consistent performance drop on date f1,0.5758169293403625
translation,26,188,ablation-analysis,consistent performance drop,showing,our time - aware ot,consistent performance drop showing our time - aware ot,0.7135854959487915
translation,26,188,ablation-analysis,ablation analysis,Removing,temporal regularizer,ablation analysis Removing temporal regularizer,0.7232887744903564
translation,26,171,baselines,typical extractive model,based on,sentence similarity,typical extractive model based on sentence similarity,0.5790709853172302
translation,26,171,baselines,state - of - the - art extractive timeline sumarization model,based on,submodular functions,state - of - the - art extractive timeline sumarization model based on submodular functions,0.6039538383483887
translation,26,172,baselines,unsupervised graph - based ranking summarization baseline,utilizes,bert,unsupervised graph - based ranking summarization baseline utilizes bert,0.5983895063400269
translation,26,172,baselines,bert,to encode,sentences,bert to encode sentences,0.7938413619995117
translation,26,172,baselines,sentences,for,sentence centrality ranking,sentences for sentence centrality ranking,0.5574765801429749
translation,26,172,baselines,sentence centrality ranking,in,sentence graph,sentence centrality ranking in sentence graph,0.4904473125934601
translation,26,172,baselines,"pac-sum ( zheng and lapata , 2019 )",has,state - of - the - art,"pac-sum ( zheng and lapata , 2019 ) has state - of - the - art",0.5595544576644897
translation,26,172,baselines,state - of - the - art,has,unsupervised graph - based ranking summarization baseline,state - of - the - art has unsupervised graph - based ranking summarization baseline,0.5042737722396851
translation,26,172,baselines,baselines,has,"pac-sum ( zheng and lapata , 2019 )","baselines has pac-sum ( zheng and lapata , 2019 )",0.5600749254226685
translation,26,174,baselines,unsupervised multi-document summarization baseline,constructs,sentence graph,unsupervised multi-document summarization baseline constructs sentence graph,0.579619824886322
translation,26,174,baselines,unsupervised multi-document summarization baseline,performs,spectral clustering,unsupervised multi-document summarization baseline performs spectral clustering,0.5154576301574707
translation,26,174,baselines,baselines,has,summpip,baselines has summpip,0.6064870953559875
translation,26,183,experimental-setup,graph compression model,trained on,one tesla v100 gpu,graph compression model trained on one tesla v100 gpu,0.676782488822937
translation,26,183,experimental-setup,one tesla v100 gpu,with,16gb dram,one tesla v100 gpu with 16gb dram,0.5651768445968628
translation,26,183,experimental-setup,experimental setup,has,graph compression model,experimental setup has graph compression model,0.5149171948432922
translation,26,7,model,articles,as,event-graph,articles as event-graph,0.5579253435134888
translation,26,9,model,time - aware optimal transport distance,for learning,compression model,time - aware optimal transport distance for learning compression model,0.6332809329032898
translation,26,9,model,compression model,in,unsupervised manner,compression model in unsupervised manner,0.530514657497406
translation,26,9,model,model,has,time - aware optimal transport distance,model has time - aware optimal transport distance,0.5151434540748596
translation,26,80,model,graph compression,to compress,g,graph compression to compress g,0.7777982354164124
translation,26,80,model,g,to,summary graph s.,g to summary graph s.,0.5906517505645752
translation,26,186,model,event graph,connects,events,event graph connects events,0.6809768080711365
translation,26,186,model,event graph,excludes,unrelated events,event graph excludes unrelated events,0.7045993208885193
translation,26,186,model,events,through,entities and temporal relations,events through entities and temporal relations,0.6071295738220215
translation,26,186,model,model,has,event graph,model has event graph,0.5483927130699158
translation,26,242,model,novel event graph compression framework,for,timeline summarization,novel event graph compression framework for timeline summarization,0.525981605052948
translation,26,242,model,model,propose,novel event graph compression framework,model propose novel event graph compression framework,0.648727297782898
translation,26,185,results,outperforms,on,all three datasets,outperforms on all three datasets,0.5061540603637695
translation,26,185,results,baselines,on,all three datasets,baselines on all three datasets,0.47616812586784363
translation,26,185,results,our method,has,outperforms,our method has outperforms,0.6322360634803772
translation,26,185,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
translation,26,185,results,results,has,our method,results has our method,0.5589964985847473
translation,26,187,results,results,has,gen-eral multi-document summarization,results has gen-eral multi-document summarization,0.5471858382225037
translation,26,189,results,larger gains,compared to,baselines,larger gains compared to baselines,0.6673413515090942
translation,26,189,results,baselines,on,crisis dataset,baselines on crisis dataset,0.5425149202346802
translation,26,189,results,results,achieve,larger gains,results achieve larger gains,0.6690330505371094
translation,26,191,results,performance gain,on,timeline 100,performance gain on timeline 100,0.6105915307998657
translation,26,191,results,timeline 100,is,larger,timeline 100 is larger,0.6361356377601624
translation,26,191,results,timeline 100,cover,more scenarios,timeline 100 cover more scenarios,0.7798820734024048
translation,26,191,results,larger,cover,more scenarios,larger cover more scenarios,0.7117481231689453
translation,26,191,results,timeline 17,has,performance gain,timeline 17 has performance gain,0.5758964419364929
translation,26,191,results,results,Compared to,timeline 17,results Compared to timeline 17,0.6670876145362854
translation,26,201,results,our model,successfully detect,salient events,our model successfully detect salient events,0.7489325404167175
translation,26,201,results,salient events,in,graph compression process,salient events in graph compression process,0.5000990033149719
translation,26,201,results,reference timeline,has,our model,reference timeline has our model,0.5264424085617065
translation,26,201,results,results,Compared to,reference timeline,results Compared to reference timeline,0.680672287940979
translation,26,215,results,our approach,gets,better results,our approach gets better results,0.6062663793563843
translation,26,215,results,better results,on,all four measures,better results on all four measures,0.4710691571235657
translation,26,215,results,our model,to find,"semantically relevant , structurally salient and temporally coherent events","our model to find semantically relevant , structurally salient and temporally coherent events",0.5590540170669556
translation,26,215,results,results,shows,our approach,results shows our approach,0.7114341259002686
translation,26,223,results,our model,achieves,larger gains,our model achieves larger gains,0.6555323600769043
translation,26,223,results,larger gains,compared to,baselines,larger gains compared to baselines,0.6673413515090942
translation,26,223,results,baselines,on,timeline,baselines on timeline,0.5583287477493286
translation,26,223,results,results,shows,our model,results shows our model,0.7287026643753052
translation,26,227,results,timelines,for,same complex event bp oil spill,timelines for same complex event bp oil spill,0.6611934304237366
translation,26,227,results,generally increasing,with respect to,input graph size,generally increasing with respect to input graph size,0.687376081943512
translation,26,227,results,timelines,has,performance gain,timelines has performance gain,0.5855953693389893
translation,26,227,results,results,When generating,timelines,results When generating timelines,0.7584888339042664
translation,27,291,ablation-analysis,more than 80 %,of,redundant qa pairs,more than 80 % of redundant qa pairs,0.6473221182823181
translation,27,291,ablation-analysis,redundant qa pairs,exist in,generated summary,redundant qa pairs exist in generated summary,0.6873939633369446
translation,27,291,ablation-analysis,ablation analysis,has,more than 80 %,ablation analysis has more than 80 %,0.5303409695625305
translation,27,201,baselines,baselines,has,"lexpagerank ( erkan and radev , 2004 )","baselines has lexpagerank ( erkan and radev , 2004 )",0.5089179277420044
translation,27,203,baselines,supervised extractive summarization,by scoring,each utterance,supervised extractive summarization by scoring each utterance,0.7238767743110657
translation,27,203,baselines,each utterance,using,rnn,each utterance using rnn,0.6494377851486206
translation,27,203,baselines,summarunner,has,),summarunner has ),0.6226474046707153
translation,27,203,baselines,summarunner,has,supervised extractive summarization,summarunner has supervised extractive summarization,0.5111163258552551
translation,27,203,baselines,),has,supervised extractive summarization,) has supervised extractive summarization,0.5758985280990601
translation,27,203,baselines,baselines,has,summarunner,baselines has summarunner,0.5826749801635742
translation,27,209,baselines,rnn - based seq2seq model,using,source word copy mechanism,rnn - based seq2seq model using source word copy mechanism,0.6405209302902222
translation,27,209,baselines,rnn - based seq2seq model,using,attention coverage mechanism,rnn - based seq2seq model using attention coverage mechanism,0.6365137696266174
translation,27,210,baselines,baselines,has,"fast - rl ( chen and bansal , 2018 )","baselines has fast - rl ( chen and bansal , 2018 )",0.5172646045684814
translation,27,213,baselines,baselines,has,"bertabs ( liu and lapata , 2019 )","baselines has bertabs ( liu and lapata , 2019 )",0.5716451406478882
translation,27,215,baselines,baselines,has,tds +satm,baselines has tds +satm,0.557595431804657
translation,27,229,baselines,baselines,has,"bertscore ( zhang et al. , 2020 )","baselines has bertscore ( zhang et al. , 2020 )",0.5344494581222534
translation,27,231,baselines,baselines,has,"moverscore ( zhao et al. , 2019 )","baselines has moverscore ( zhao et al. , 2019 )",0.5157960057258606
translation,27,202,model,dialogue utterances,by,pagerank algorithm,dialogue utterances by pagerank algorithm,0.5801615118980408
translation,27,202,model,dialogue utterances,in,order,dialogue utterances in order,0.5286978483200073
translation,27,202,model,utterances,in,order,utterances in order,0.5267197489738464
translation,27,202,model,utterances,length of,summary,utterances length of summary,0.6345527768135071
translation,27,202,model,summary,reaches,limit,summary reaches limit,0.7034710049629211
translation,27,214,model,pretrained bert,as,encoder,pretrained bert as encoder,0.5577908158302307
translation,27,214,model,transformerbased network,as,decoder,transformerbased network as decoder,0.5841867327690125
translation,27,214,model,model,uses,pretrained bert,model uses pretrained bert,0.6296400427818298
translation,27,214,model,model,uses,transformerbased network,model uses transformerbased network,0.5639694929122925
translation,27,8,results,csds,improves,abstractive summaries,csds improves abstractive summaries,0.6438702344894409
translation,27,8,results,results,has,csds,results has csds,0.5241585969924927
translation,27,248,results,abstractive methods,perform better than,extractive methods,abstractive methods perform better than extractive methods,0.6342930197715759
translation,27,248,results,extractive methods,with,large margin,extractive methods with large margin,0.6347478032112122
translation,27,248,results,results,observe that,abstractive methods,results observe that abstractive methods,0.45102548599243164
translation,27,249,results,summarunner,achieves,best results,summarunner achieves best results,0.6733815670013428
translation,27,249,results,extractive methods,has,summarunner,extractive methods has summarunner,0.5867274403572083
translation,27,249,results,results,Among,extractive methods,results Among extractive methods,0.5566853284835815
translation,27,250,results,fast-rl and fast - rl *,perform,best,fast-rl and fast - rl * perform best,0.6060338020324707
translation,27,250,results,best,on,almost all metrics,best on almost all metrics,0.4893215596675873
translation,27,250,results,almost all metrics,except for,rouge -l,almost all metrics except for rouge -l,0.6106302738189697
translation,27,250,results,abstractive methods,has,fast-rl and fast - rl *,abstractive methods has fast-rl and fast - rl *,0.5500026345252991
translation,27,252,results,"enhanced methods ( fast - rl * , tds + satm * )",usually better than,original version,"enhanced methods ( fast - rl * , tds + satm * ) usually better than original version",0.7655959129333496
translation,27,252,results,original version,on,overall summary,original version on overall summary,0.5122971534729004
translation,27,252,results,original version,on,user summary,original version on user summary,0.4988277852535248
translation,27,254,results,agent summary scores,are,much lower,agent summary scores are much lower,0.5953827500343323
translation,27,254,results,much lower,than,overall summary and user summary,much lower than overall summary and user summary,0.5723256468772888
translation,27,254,results,overall summary and user summary,in,most metrics,overall summary and user summary in most metrics,0.4494045078754425
translation,27,260,results,inter-annotator agreement study,on,three volunteers ' scores,inter-annotator agreement study on three volunteers ' scores,0.5153231024742126
translation,27,260,results,kappa score,is,0.52,kappa score is 0.52,0.5134148001670837
translation,27,260,results,results,run,inter-annotator agreement study,results run inter-annotator agreement study,0.6278761029243469
translation,27,262,results,all methods,perform,poorly,all methods perform poorly,0.6648768186569214
translation,27,262,results,poorly,on,nonredundancy,poorly on nonredundancy,0.5976601243019104
translation,27,262,results,results,has,all methods,results has all methods,0.48065561056137085
translation,27,265,results,nearly 30 percent,of,overall summaries,nearly 30 percent of overall summaries,0.6306896805763245
translation,27,265,results,overall summaries,have,unmatched questions and answers,overall summaries have unmatched questions and answers,0.547059953212738
translation,27,265,results,unmatched questions and answers,through,matching rate,unmatched questions and answers through matching rate,0.6397169232368469
translation,27,265,results,results,find that,nearly 30 percent,results find that nearly 30 percent,0.6413671374320984
translation,28,8,model,two novel rewards,obtained from,downstream tasks,two novel rewards obtained from downstream tasks,0.5784837603569031
translation,28,8,model,two novel rewards,to regularize,question generation model,two novel rewards to regularize question generation model,0.6659077405929565
translation,28,8,model,question - focus recognition,to regularize,question generation model,question - focus recognition to regularize question generation model,0.656629204750061
translation,28,8,model,model,propose,two novel rewards,model propose two novel rewards,0.6932446360588074
translation,28,21,model,framework,for,abstractive question summarization,framework for abstractive question summarization,0.5701790452003479
translation,28,22,model,model,propose,two novel question - aware semantic reward functions,model propose two novel question - aware semantic reward functions,0.6978337168693542
translation,28,103,results,benchmark model,on,meq - sum,benchmark model on meq - sum,0.537034809589386
translation,28,103,results,our proposed model,obtained,improvement,our proposed model obtained improvement,0.65309739112854
translation,28,103,results,improvement,of,9.63 %,improvement of 9.63 %,0.5650222897529602
translation,28,103,results,benchmark model,has,our proposed model,benchmark model has our proposed model,0.5867198705673218
translation,28,103,results,meq - sum,has,our proposed model,meq - sum has our proposed model,0.6298880577087402
translation,28,103,results,results,In comparison to,benchmark model,results In comparison to benchmark model,0.6795220375061035
translation,28,105,results,individual qtr and qfr rewards,improve over,prophetnet and rougebased rewards,individual qtr and qfr rewards improve over prophetnet and rougebased rewards,0.683123767375946
translation,28,105,results,results,show that,individual qtr and qfr rewards,results show that individual qtr and qfr rewards,0.4404984712600708
translation,28,106,results,question - type reward,assists,model,question - type reward assists model,0.655059814453125
translation,28,106,results,model,to capture,underlying question semantics,model to capture underlying question semantics,0.7109671235084534
translation,28,106,results,salient entities,learned from,question - focus reward,salient entities learned from question - focus reward,0.6463996171951294
translation,28,106,results,fewer incorrect summaries,unrelated to,question topic,fewer incorrect summaries unrelated to question topic,0.6717063188552856
translation,28,108,results,downstream tasks,of,question - type identification,downstream tasks of question - type identification,0.5642900466918945
translation,28,108,results,downstream tasks,of,questionfocus recognition,downstream tasks of questionfocus recognition,0.5883509516716003
translation,28,108,results,pre-trained bert model,achieves,f-score,pre-trained bert model achieves f-score,0.6506645679473877
translation,28,108,results,f-score,of,97.10 % and 77.24 %,f-score of 97.10 % and 77.24 %,0.5605337619781494
translation,28,108,results,97.10 % and 77.24 %,on,10 %,97.10 % and 77.24 % on 10 %,0.602226972579956
translation,28,108,results,10 %,of,manually labeled meq - sum pairs,10 % of manually labeled meq - sum pairs,0.5988548398017883
translation,28,108,results,downstream tasks,has,pre-trained bert model,downstream tasks has pre-trained bert model,0.574617326259613
translation,28,108,results,question - type identification,has,pre-trained bert model,question - type identification has pre-trained bert model,0.546405553817749
translation,28,108,results,questionfocus recognition,has,pre-trained bert model,questionfocus recognition has pre-trained bert model,0.5740859508514404
translation,28,108,results,results,On,downstream tasks,results On downstream tasks,0.5158278942108154
translation,28,116,results,our proposed rewards,enhance,model,our proposed rewards enhance model,0.6332380175590515
translation,28,116,results,model,by capturing,underlying semantics and facts,model by capturing underlying semantics and facts,0.7392847537994385
translation,28,116,results,higher proportions,of,perfect and acceptable summaries,higher proportions of perfect and acceptable summaries,0.556489884853363
translation,28,116,results,results,show,our proposed rewards,results show our proposed rewards,0.6733121871948242
translation,29,46,baselines,original bart large checkpoint model,on,samsum,original bart large checkpoint model on samsum,0.5632471442222595
translation,29,46,baselines,fine-tuning,has,original bart large checkpoint model,fine-tuning has original bart large checkpoint model,0.5691624283790588
translation,29,47,baselines,baselines,has,multi-view seq2seq,baselines has multi-view seq2seq,0.5426007509231567
translation,29,42,experimental-setup,experimental setup,use,"bart large architecture ( lewis et al. , 2020 )","experimental setup use bart large architecture ( lewis et al. , 2020 )",0.5900759696960449
translation,29,44,experiments,experiments,run using,"fairseq ( ott et al. , 2019 )","experiments run using fairseq ( ott et al. , 2019 )",0.7502274513244629
translation,29,58,results,rouge improvements,on,both validation and test sets,rouge improvements on both validation and test sets,0.544126033782959
translation,29,58,results,both validation and test sets,of,samsum,both validation and test sets of samsum,0.6360777020454407
translation,29,58,results,results,observe,rouge improvements,results observe rouge improvements,0.5646216869354248
translation,29,61,results,"more participants ( 7 , 8 , 12 )",exhibit,higher rouge boost,"more participants ( 7 , 8 , 12 ) exhibit higher rouge boost",0.6252287030220032
translation,29,91,results,mtl,gives,performance boost,mtl gives performance boost,0.640429675579071
translation,29,91,results,performance boost,in,almost all cases,performance boost in almost all cases,0.5477023720741272
translation,29,91,results,vanilla bart and the multi-view ss baseline,on,development and test sets,vanilla bart and the multi-view ss baseline on development and test sets,0.5590388178825378
translation,29,91,results,vanilla bart and the multi-view ss baseline,both,development and test sets,vanilla bart and the multi-view ss baseline both development and test sets,0.6615393757820129
translation,29,91,results,outperforming,has,vanilla bart and the multi-view ss baseline,outperforming has vanilla bart and the multi-view ss baseline,0.5737215280532837
translation,29,91,results,results,clear that,mtl,results clear that mtl,0.7224762439727783
translation,29,104,results,vanilla bart,on,all metrics,vanilla bart on all metrics,0.5664314031600952
translation,29,104,results,multiview ss baseline,on,test set rouge - 2 and rouge -l,multiview ss baseline on test set rouge - 2 and rouge -l,0.5302022099494934
translation,29,104,results,"best model ( personachat , word masking )",has,outperforms,"best model ( personachat , word masking ) has outperforms",0.5760400295257568
translation,29,104,results,outperforms,has,vanilla bart,outperforms has vanilla bart,0.6809602379798889
translation,29,104,results,results,has,"best model ( personachat , word masking )","results has best model ( personachat , word masking )",0.5185699462890625
translation,29,105,results,personachat,better than,pretraining,personachat better than pretraining,0.7232937216758728
translation,29,105,results,pretraining,on,personachat and reddit,pretraining on personachat and reddit,0.5990781188011169
translation,29,105,results,results,BART pretrained on,personachat,results BART pretrained on personachat,0.7074735164642334
translation,29,107,results,whole word masking,performs,slightly better,whole word masking performs slightly better,0.6157891154289246
translation,29,107,results,slightly better,than,span masking,slightly better than span masking,0.5900478363037109
translation,29,107,results,results,see that,whole word masking,results see that whole word masking,0.5364817380905151
translation,29,109,results,pretraining,using,dialoguespecific objectives,pretraining using dialoguespecific objectives,0.609320878982544
translation,29,109,results,dialoguespecific objectives,performing,well,dialoguespecific objectives performing well,0.6618483066558838
translation,29,109,results,random span masking,on,validation set,random span masking on validation set,0.5594385266304016
translation,29,109,results,even outperforming,has,random span masking,even outperforming has random span masking,0.6128261089324951
translation,29,109,results,results,see that,pretraining,results see that pretraining,0.650836706161499
translation,29,113,results,fine-tuning,on,samsum and roc,fine-tuning on samsum and roc,0.5412383675575256
translation,29,113,results,samsum and roc,gives,best performance,samsum and roc gives best performance,0.6257723569869995
translation,29,113,results,best performance,over,validation set,best performance over validation set,0.692641019821167
translation,29,113,results,outperforming,has,all other settings,outperforming has all other settings,0.5863034129142761
translation,29,113,results,results,pretraining on,personachat,results pretraining on personachat,0.7538394331932068
translation,29,114,results,test set,performing,very well,test set performing very well,0.5923434495925903
translation,29,114,results,slightly outperformed,by,multi-tasking,slightly outperformed by multi-tasking,0.5964035391807556
translation,29,114,results,multi-tasking,with,roc,multi-tasking with roc,0.6406440138816833
translation,29,114,results,roc,in,rouge - 2 and rouge -l.,roc in rouge - 2 and rouge -l.,0.5979043245315552
translation,29,114,results,results,On,test set,results On test set,0.582119882106781
translation,29,116,results,name substitution,does not give,performance boost,name substitution does not give performance boost,0.6860013604164124
translation,29,116,results,performance boost,when used in combination with,pretraining and mtl,performance boost when used in combination with pretraining and mtl,0.6193525195121765
translation,29,116,results,results,observe,name substitution,results observe name substitution,0.5996958613395691
translation,30,153,ablation-analysis,aggregation strategy,in,our proposed evaluation framework,aggregation strategy in our proposed evaluation framework,0.49752452969551086
translation,30,153,ablation-analysis,great influence,on,evaluation performance,great influence on evaluation performance,0.4460947513580322
translation,30,153,ablation-analysis,our proposed evaluation framework,has,great influence,our proposed evaluation framework has great influence,0.5695528388023376
translation,30,153,ablation-analysis,ablation analysis,find that,aggregation strategy,ablation analysis find that aggregation strategy,0.6424798965454102
translation,30,124,hyperparameters,max sequence length,of,sentence pairs,max sequence length of sentence pairs,0.5868560671806335
translation,30,124,hyperparameters,max sequence length,set to,150,max sequence length set to 150,0.7673394680023193
translation,30,124,hyperparameters,150,during,evidence reasoning,150 during evidence reasoning,0.6315779685974121
translation,30,124,hyperparameters,hyperparameters,has,max sequence length,hyperparameters has max sequence length,0.4825800061225891
translation,30,126,hyperparameters,generated data,for,four epochs,generated data for four epochs,0.6222102642059326
translation,30,126,hyperparameters,generated data,with,batch size,generated data with batch size,0.6244993805885315
translation,30,126,hyperparameters,generated data,takes,around 10 hours,generated data takes around 10 hours,0.7110532522201538
translation,30,126,hyperparameters,four epochs,with,batch size,four epochs with batch size,0.6376678347587585
translation,30,126,hyperparameters,batch size,set to,28,batch size set to 28,0.7368164658546448
translation,30,126,hyperparameters,hyperparameters,trained on,generated data,hyperparameters trained on generated data,0.7149443030357361
translation,30,127,hyperparameters,adamw optimizer,with,initial learning rate,adamw optimizer with initial learning rate,0.5909740328788757
translation,30,127,hyperparameters,initial learning rate,used for,training,initial learning rate used for training,0.6885052919387817
translation,30,127,hyperparameters,3e - 5,used for,training,3e - 5 used for training,0.7286993265151978
translation,30,127,hyperparameters,initial learning rate,has,3e - 5,initial learning rate has 3e - 5,0.5660555958747864
translation,30,127,hyperparameters,hyperparameters,has,adamw optimizer,hyperparameters has adamw optimizer,0.4970969557762146
translation,30,128,hyperparameters,weight,of,crossentropy loss,weight of crossentropy loss,0.5812909603118896
translation,30,128,hyperparameters,crossentropy loss,set to,0.5,crossentropy loss set to 0.5,0.6455380320549011
translation,30,128,hyperparameters,0.5,during,training,0.5 during training,0.7222657203674316
translation,30,128,hyperparameters,hyperparameters,has,weight,hyperparameters has weight,0.5009066462516785
translation,30,7,model,summary sentence,in,first stage,summary sentence in first stage,0.5239748954772949
translation,30,7,model,sumfc,selects,top -k most relevant sentences,sumfc selects top -k most relevant sentences,0.6735424399375916
translation,30,7,model,top -k most relevant sentences,with,summary sentence,top -k most relevant sentences with summary sentence,0.5762553811073303
translation,30,7,model,summary sentence,from,document,summary sentence from document,0.6005898714065552
translation,30,7,model,document,has,sumfc,document has sumfc,0.6480398178100586
translation,30,7,model,summary sentence,has,sumfc,summary sentence has sumfc,0.6169532537460327
translation,30,7,model,first stage,has,sumfc,first stage has sumfc,0.6302536725997925
translation,30,7,model,model,Given,document,model Given document,0.7779427170753479
translation,30,24,model,fact consistency assessment framework,for,summarization models,fact consistency assessment framework for summarization models,0.5956932902336121
translation,30,24,model,model,propose,fact consistency assessment framework,model propose fact consistency assessment framework,0.6533118486404419
translation,30,25,model,assessment process,into,two stages,assessment process into two stages,0.5898862481117249
translation,30,25,model,two stages,in,sentence selection stage,two stages in sentence selection stage,0.5064458847045898
translation,30,25,model,two stages,in,consistency checking stage,two stages in consistency checking stage,0.544683575630188
translation,30,25,model,top - k pieces of evidence,selected from,original document,top - k pieces of evidence selected from original document,0.6215910315513611
translation,30,25,model,each piece of evidence,reasoned with,summary sentence,each piece of evidence reasoned with summary sentence,0.7285872101783752
translation,30,25,model,sumfc,aggregates,results,sumfc aggregates results,0.7310951948165894
translation,30,25,model,results,of,top - k pieces of evidence,results of top - k pieces of evidence,0.5102975964546204
translation,30,25,model,sentence selection stage,has,top - k pieces of evidence,sentence selection stage has top - k pieces of evidence,0.5072113275527954
translation,30,25,model,consistency checking stage,has,each piece of evidence,consistency checking stage has each piece of evidence,0.5694748759269714
translation,30,25,model,model,split,assessment process,model split assessment process,0.7237513065338135
translation,30,125,model,training data,into,model,training data into model,0.6126469373703003
translation,30,125,model,model,in,pairs ( one consistent sample and corresponding inconsistent sample ),model in pairs ( one consistent sample and corresponding inconsistent sample ),0.539869487285614
translation,30,125,model,model,feed,training data,model feed training data,0.744066596031189
translation,30,230,model,clean and intuitive factual assessment framework,splits,assessment process,clean and intuitive factual assessment framework splits assessment process,0.6698684096336365
translation,30,230,model,assessment process,into,two stages,assessment process into two stages,0.5898862481117249
translation,30,230,model,two stages,including,sentence -selection stage,two stages including sentence -selection stage,0.6667703986167908
translation,30,230,model,two stages,including,consistencychecking stage,two stages including consistencychecking stage,0.6656450629234314
translation,30,230,model,model,propose,clean and intuitive factual assessment framework,model propose clean and intuitive factual assessment framework,0.6489577889442444
translation,30,140,results,nli datasets,transfer,poorly,nli datasets transfer poorly,0.7694246172904968
translation,30,140,results,poorly,to,factual consistency assessment,poorly to factual consistency assessment,0.5927386283874512
translation,30,141,results,methods,based on,weakly supervised dataset,methods based on weakly supervised dataset,0.5841822624206543
translation,30,141,results,rulebased transformation,has,strongly outperform,rulebased transformation has strongly outperform,0.5938999056816101
translation,30,141,results,strongly outperform,has,nli dataset,strongly outperform has nli dataset,0.5781351923942566
translation,30,141,results,results,has,methods,results has methods,0.44259312748908997
translation,30,148,results,best ability,to distinguish,differences in detail,best ability to distinguish differences in detail,0.7093492150306702
translation,30,148,results,sumfc,has,best ability,sumfc has best ability,0.5722477436065674
translation,30,149,results,sumfc,improves,performance,sumfc improves performance,0.6894405484199524
translation,30,149,results,performance,by,6.6 % higher,performance by 6.6 % higher,0.5699710845947266
translation,30,149,results,qags,has,sumfc,qags has sumfc,0.6217818856239319
translation,30,149,results,results,Compared with,qags,results Compared with qags,0.690434455871582
translation,30,154,results,considerable difference,among,results,considerable difference among results,0.5989994406700134
translation,30,154,results,results,obtained by,different aggregation strategies,results obtained by different aggregation strategies,0.6465933322906494
translation,30,154,results,max strategy and the avg strategy,obtain,very poor results,max strategy and the avg strategy obtain very poor results,0.5753284692764282
translation,30,154,results,min strategy and the wgt strategy,perform,much better,min strategy and the wgt strategy perform much better,0.5737518072128296
translation,30,154,results,different aggregation strategies,has,max strategy and the avg strategy,different aggregation strategies has max strategy and the avg strategy,0.5785843729972839
translation,30,182,results,our proposed framework,performs,better,our proposed framework performs better,0.6922876834869385
translation,30,182,results,better,on,ourdata,better on ourdata,0.5783065557479858
translation,30,182,results,better,on,all metrics,better on all metrics,0.524868369102478
translation,30,182,results,better,on,all metrics,better on all metrics,0.524868369102478
translation,30,182,results,ourdata,on,all metrics,ourdata on all metrics,0.5085508823394775
translation,30,182,results,results,has,our proposed framework,results has our proposed framework,0.5857126712799072
translation,30,183,results,sentence ranking score,is,quite sensitive,sentence ranking score is quite sensitive,0.5334907174110413
translation,30,183,results,quite sensitive,to,dataset,quite sensitive to dataset,0.5542860627174377
translation,30,183,results,ourdata,greatly improves,ranking accuracy,ourdata greatly improves ranking accuracy,0.754641056060791
translation,30,183,results,results,notice,sentence ranking score,results notice sentence ranking score,0.679284930229187
translation,31,120,ablation-analysis,window size,from,bottom to the top layer,window size from bottom to the top layer,0.5806884169578552
translation,31,120,ablation-analysis,window size,leads to,best performance,window size leads to best performance,0.64298415184021
translation,31,120,ablation-analysis,bottom to the top layer,leads to,best performance,bottom to the top layer leads to best performance,0.6559943556785583
translation,31,120,ablation-analysis,best performance,has,from 32 to 512 ),best performance has from 32 to 512 ),0.5636387467384338
translation,31,120,ablation-analysis,ablation analysis,increasing,window size,ablation analysis increasing window size,0.7053272128105164
translation,31,125,ablation-analysis,other tokens,could decrease,performance,other tokens could decrease performance,0.7198465466499329
translation,31,125,ablation-analysis,ablation analysis,without using,sentence node,ablation analysis without using sentence node,0.6758679747581482
translation,31,127,ablation-analysis,entity node,has,performance,entity node has performance,0.5666481852531433
translation,31,127,ablation-analysis,ablation analysis,without,entity node,ablation analysis without entity node,0.6554141640663147
translation,31,94,experimental-setup,dropout,with,probability 0.1,dropout with probability 0.1,0.6266955733299255
translation,31,94,experimental-setup,probability 0.1,before,all linear layers,probability 0.1 before all linear layers,0.6418308615684509
translation,31,94,experimental-setup,experimental setup,apply,dropout,experimental setup apply dropout,0.5288015604019165
translation,31,95,experimental-setup,longformer - base architecture,where,number of d model hidden units,longformer - base architecture where number of d model hidden units,0.622001588344574
translation,31,95,experimental-setup,longformer - base architecture,where,d h hidden size,longformer - base architecture where d h hidden size,0.6390141844749451
translation,31,95,experimental-setup,longformer - base architecture,where,layer number,longformer - base architecture where layer number,0.6428859829902649
translation,31,95,experimental-setup,longformer - base architecture,where,number of heads,longformer - base architecture where number of heads,0.6334316730499268
translation,31,95,experimental-setup,number of d model hidden units,in,our models,number of d model hidden units in our models,0.49547016620635986
translation,31,95,experimental-setup,number of d model hidden units,set as,768,number of d model hidden units set as 768,0.6695080995559692
translation,31,95,experimental-setup,d h hidden size,is,64,d h hidden size is 64,0.6153907179832458
translation,31,95,experimental-setup,layer number,is,12,layer number is 12,0.6138647794723511
translation,31,95,experimental-setup,layer number,is,12,layer number is 12,0.6138647794723511
translation,31,95,experimental-setup,number of heads,is,12,number of heads is 12,0.587431013584137
translation,31,96,experimental-setup,model,for,500k steps,model for 500k steps,0.65699303150177
translation,31,96,experimental-setup,500k steps,on,"titanrtx , 24g gpu","500k steps on titanrtx , 24g gpu",0.45934414863586426
translation,31,96,experimental-setup,"titanrtx , 24g gpu",with,gradient accumulation,"titanrtx , 24g gpu with gradient accumulation",0.6162652969360352
translation,31,96,experimental-setup,gradient accumulation,with,adam optimizers,gradient accumulation with adam optimizers,0.6077145338058472
translation,31,96,experimental-setup,experimental setup,train,model,experimental setup train model,0.6514950394630432
translation,31,6,model,model,with,multi-granularity sparse attentions,model with multi-granularity sparse attentions,0.6048445105552673
translation,31,6,model,multi-granularity sparse attentions,for,long -text extractive summarization,multi-granularity sparse attentions for long -text extractive summarization,0.5470924973487854
translation,31,6,model,model,proposes,het,model proposes het,0.632375955581665
translation,31,7,model,different types of semantic nodes,in,raw text,different types of semantic nodes in raw text,0.5004732012748718
translation,31,7,model,different types of semantic nodes,as,potential heterogeneous graph,different types of semantic nodes as potential heterogeneous graph,0.5347539782524109
translation,31,7,model,heterogeneous relationships ( edges ),among,nodes,heterogeneous relationships ( edges ) among nodes,0.5906032919883728
translation,31,7,model,nodes,by,transformer,nodes by transformer,0.6081165671348572
translation,31,7,model,model,model,different types of semantic nodes,model model different types of semantic nodes,0.7328917980194092
translation,31,7,model,model,directly learn,heterogeneous relationships ( edges ),model directly learn heterogeneous relationships ( edges ),0.6615233421325684
translation,31,20,model,model,propose,hetformer,model propose hetformer,0.7125060558319092
translation,31,21,model,"tokens , entities , sentences",as,different types of nodes,"tokens , entities , sentences as different types of nodes",0.48029521107673645
translation,31,21,model,multiple sparse masks,as,different types of edges,multiple sparse masks as different types of edges,0.5196821093559265
translation,31,21,model,different types of edges,to represent,"relations ( e.g. , token - to- token , token - to-sentence )","different types of edges to represent relations ( e.g. , token - to- token , token - to-sentence )",0.6675310730934143
translation,31,21,model,"relations ( e.g. , token - to- token , token - to-sentence )",preserve,graph structure,"relations ( e.g. , token - to- token , token - to-sentence ) preserve graph structure",0.7166984677314758
translation,31,21,model,model,treat,"tokens , entities , sentences","model treat tokens , entities , sentences",0.5613807439804077
translation,31,22,model,our approach,eschew,gnn,our approach eschew gnn,0.7104772925376892
translation,31,22,model,our approach,rely entirely on,sparse attention mechanism,our approach rely entirely on sparse attention mechanism,0.6846885681152344
translation,31,22,model,sparse attention mechanism,to draw,heterogeneous graph structural dependencies,sparse attention mechanism to draw heterogeneous graph structural dependencies,0.5823540091514587
translation,31,22,model,heterogeneous graph structural dependencies,between,input tokens,heterogeneous graph structural dependencies between input tokens,0.5674387812614441
translation,31,22,model,model,has,our approach,model has our approach,0.5610997676849365
translation,31,24,model,model,has,hetformer on summarization,model has hetformer on summarization,0.6117001175880432
translation,31,25,model,different types of semantic nodes,in,raw text,different types of semantic nodes in raw text,0.5004732012748718
translation,31,25,model,different types of semantic nodes,as,potential heterogeneous graph,different types of semantic nodes as potential heterogeneous graph,0.5347539782524109
translation,31,25,model,multi-granularity sparse attention patterns,to directly capture,heterogeneous relationships,multi-granularity sparse attention patterns to directly capture heterogeneous relationships,0.6647443175315857
translation,31,25,model,heterogeneous relationships,among,nodes,heterogeneous relationships among nodes,0.6165971755981445
translation,31,25,model,model,model,different types of semantic nodes,model model different types of semantic nodes,0.7328917980194092
translation,31,25,model,model,explore,multi-granularity sparse attention patterns,model explore multi-granularity sparse attention patterns,0.6164723634719849
translation,31,26,model,interactively updated,in,fine-tuned manner,interactively updated in fine-tuned manner,0.5492410659790039
translation,31,26,model,sentence node representations,to predict,labels,sentence node representations to predict labels,0.6650877594947815
translation,31,26,model,labels,for,extractive text summarization,labels for extractive text summarization,0.5460172295570374
translation,31,26,model,model,has,node representations,model has node representations,0.5675575733184814
translation,31,36,model,neuralcoref,to obtain,coreference resolution,neuralcoref to obtain coreference resolution,0.5184908509254456
translation,31,36,model,coreference resolution,of,each entity,coreference resolution of each entity,0.53585284948349
translation,31,36,model,model,utilize,neuralcoref,model utilize neuralcoref,0.6322578191757202
translation,31,39,model,multi-granularity sparse attention mechanisms,in,transformer,multi-granularity sparse attention mechanisms in transformer,0.503757119178772
translation,31,39,model,multi-granularity sparse attention mechanisms,considering,five attention patterns,multi-granularity sparse attention mechanisms considering five attention patterns,0.6678022742271423
translation,31,39,model,model,leverage,multi-granularity sparse attention mechanisms,model leverage multi-granularity sparse attention mechanisms,0.6919655799865723
translation,31,105,results,our approach,on par with,current state - of - the - art baselines,our approach on par with current state - of - the - art baselines,0.6670554876327515
translation,31,105,results,our approach,has,outperforms,our approach has outperforms,0.6385829448699951
translation,31,105,results,results,has,our approach,results has our approach,0.6050099730491638
translation,31,106,results,hierarchical structure model,using,fully -connected attention model hibert,hierarchical structure model using fully -connected attention model hibert,0.6626661419868469
translation,31,106,results,outperforms,has,hierarchical structure model,outperforms has hierarchical structure model,0.6099495887756348
translation,31,106,results,results,has,longformer,results has longformer,0.5925242304801941
translation,31,107,results,pre-trained models,using,sparse attention,pre-trained models using sparse attention,0.6287038326263428
translation,31,107,results,pre-trained models,using,hetformer,pre-trained models using hetformer,0.6753700375556946
translation,31,107,results,hetformer,considering,heterogeneous graph structure,hetformer considering heterogeneous graph structure,0.7332833409309387
translation,31,107,results,heterogeneous graph structure,among,text input,heterogeneous graph structure among text input,0.5613263845443726
translation,31,107,results,text input,outperforms,longformer,text input outperforms longformer,0.727761447429657
translation,31,107,results,pre-trained models,has,hetformer,pre-trained models has hetformer,0.5875537395477295
translation,31,107,results,sparse attention,has,hetformer,sparse attention has hetformer,0.5585198998451233
translation,31,107,results,results,Comparing to,pre-trained models,results Comparing to pre-trained models,0.6344121098518372
translation,31,108,results,hetformer,achieves,competitive performance,hetformer achieves competitive performance,0.7415661811828613
translation,31,108,results,competitive performance,compared with,gnn - based models,competitive performance compared with gnn - based models,0.703322172164917
translation,31,108,results,gnn - based models,such as,hsg,gnn - based models such as hsg,0.6675828695297241
translation,31,108,results,gnn - based models,such as,hahsum,gnn - based models such as hahsum,0.6437774300575256
translation,31,108,results,results,has,hetformer,results has hetformer,0.5745108723640442
translation,31,111,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,31,111,results,outperforms,has,all the extractive and abstractive baselines,outperforms has all the extractive and abstractive baselines,0.5907129645347595
translation,31,111,results,results,has,our model,results has our model,0.5871725678443909
translation,32,163,ablation-analysis,increase,is,1.7,increase is 1.7,0.5999075770378113
translation,32,163,ablation-analysis,increase,is,1.3,increase is 1.3,0.601611316204071
translation,32,163,ablation-analysis,increase,is,0.83,increase is 0.83,0.5913978815078735
translation,32,163,ablation-analysis,1.7,in,rouge -1,1.7 in rouge -1,0.5555053949356079
translation,32,163,ablation-analysis,1.3,in,rouge - 2,1.3 in rouge - 2,0.5649093985557556
translation,32,163,ablation-analysis,0.83,in,rouge -l,0.83 in rouge -l,0.5888769626617432
translation,32,163,ablation-analysis,pubmed dataset,has,increase,pubmed dataset has increase,0.6249698996543884
translation,32,163,ablation-analysis,ablation analysis,For,pubmed dataset,ablation analysis For pubmed dataset,0.5653231739997864
translation,32,203,ablation-analysis,global semantics,into,model,global semantics into model,0.5902392864227295
translation,32,203,ablation-analysis,global semantics,makes,performance improvement,global semantics makes performance improvement,0.6403927206993103
translation,32,203,ablation-analysis,model,makes,performance improvement,model makes performance improvement,0.6454198360443115
translation,32,203,ablation-analysis,performance improvement,has,drop,performance improvement has drop,0.5730001330375671
translation,32,203,ablation-analysis,drop,has,strongly,drop has strongly,0.6507994532585144
translation,32,203,ablation-analysis,ablation analysis,plainly incorporating,global semantics,ablation analysis plainly incorporating global semantics,0.6315553784370422
translation,32,204,ablation-analysis,decreases,has,more than 2 points,decreases has more than 2 points,0.641840934753418
translation,32,208,ablation-analysis,latent vector,brings is,downgraded,latent vector brings is downgraded,0.6666120886802673
translation,32,208,ablation-analysis,nearly 0.4,of,rouge - 1,nearly 0.4 of rouge - 1,0.6478511095046997
translation,32,208,ablation-analysis,rouge - 1,for using,contextualized gating,rouge - 1 for using contextualized gating,0.7071521282196045
translation,32,208,ablation-analysis,0.53,of,rouge - 1,0.53 of rouge - 1,0.5930294990539551
translation,32,208,ablation-analysis,0.53,in,non-gating case,0.53 in non-gating case,0.5104681253433228
translation,32,208,ablation-analysis,rouge - 1,in,non-gating case,rouge - 1 in non-gating case,0.5373559594154358
translation,32,208,ablation-analysis,normalizing flow,has,improvement,normalizing flow has improvement,0.5892946720123291
translation,32,208,ablation-analysis,downgraded,has,nearly 0.4,downgraded has nearly 0.4,0.6114192008972168
translation,32,208,ablation-analysis,ablation analysis,without,normalizing flow,ablation analysis without normalizing flow,0.747631847858429
translation,32,149,baselines,pointer- generator baseline,allows,switching,pointer- generator baseline allows switching,0.7463895082473755
translation,32,149,baselines,switching,between,generating words,switching between generating words,0.6696822047233582
translation,32,149,baselines,switching,between,copying words,switching between copying words,0.7148125171661377
translation,32,149,baselines,generating words,from,vocabulary,generating words from vocabulary,0.597035825252533
translation,32,149,baselines,copying words,from,source,copying words from source,0.6121547818183899
translation,32,149,baselines,ptgen,has,pointer- generator baseline,ptgen has pointer- generator baseline,0.5496931672096252
translation,32,159,experiments,our model,outperforms,pegasus,our model outperforms pegasus,0.7595049738883972
translation,32,159,experiments,pegasus,by,1.3,pegasus by 1.3,0.5834873914718628
translation,32,159,experiments,pegasus,by,0.4,pegasus by 0.4,0.5886878371238708
translation,32,159,experiments,pegasus,by,1.5,pegasus by 1.5,0.5864103436470032
translation,32,159,experiments,pegasus,in,rouge -l,pegasus in rouge -l,0.6388320326805115
translation,32,159,experiments,1.3,in,rouge -1,1.3 in rouge -1,0.5514888763427734
translation,32,159,experiments,0.4,in,rouge - 2,0.4 in rouge - 2,0.5692858099937439
translation,32,159,experiments,0.4,in,rouge -l,0.4 in rouge -l,0.5982099771499634
translation,32,159,experiments,1.5,in,rouge -l,1.5 in rouge -l,0.5924564599990845
translation,32,159,experiments,reddit tifu,has,our model,reddit tifu has our model,0.6401704549789429
translation,32,192,hyperparameters,both baselines,execute,1000 iterations,both baselines execute 1000 iterations,0.6677234768867493
translation,32,192,hyperparameters,1000 iterations,to assure,convergence,1000 iterations to assure convergence,0.6574474573135376
translation,32,192,hyperparameters,hyperparameters,For,both baselines,hyperparameters For both baselines,0.5357088446617126
translation,32,192,hyperparameters,hyperparameters,execute,1000 iterations,hyperparameters execute 1000 iterations,0.6233019232749939
translation,32,6,model,neural topic model,empowered with,normalizing flow,neural topic model empowered with normalizing flow,0.6159362196922302
translation,32,6,model,normalizing flow,to capture,global semantics,normalizing flow to capture global semantics,0.649724006652832
translation,32,6,model,global semantics,of,document,global semantics of document,0.6066713333129883
translation,32,6,model,global semantics,integrated into,summarization model,global semantics integrated into summarization model,0.6740859150886536
translation,32,6,model,model,introducing,neural topic model,model introducing neural topic model,0.6172143816947937
translation,32,7,model,global semantics,supplied to,text generation module,global semantics supplied to text generation module,0.6632263660430908
translation,32,49,model,novel method,integrates,neural topic model,novel method integrates neural topic model,0.6081893444061279
translation,32,49,model,neural topic model,into,summarization architecture,neural topic model into summarization architecture,0.5352760553359985
translation,32,49,model,model,propose,novel method,model propose novel method,0.7230806350708008
translation,32,50,model,posterior distribution,learned from,neural topic model,posterior distribution learned from neural topic model,0.6468982696533203
translation,32,50,model,neural topic model,as,approximation,neural topic model as approximation,0.5411157011985779
translation,32,50,model,approximation,of,global semantics,approximation of global semantics,0.5351466536521912
translation,32,50,model,global semantics,of,document,global semantics of document,0.6066713333129883
translation,32,50,model,better understanding,of,overall document,better understanding of overall document,0.5666446089744568
translation,32,50,model,model,utilize,posterior distribution,model utilize posterior distribution,0.5691445469856262
translation,32,153,results,outperforms,on,five standard datasets,outperforms on five standard datasets,0.521507740020752
translation,32,153,results,prior works,on,five standard datasets,prior works on five standard datasets,0.4460097849369049
translation,32,153,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,32,153,results,outperforms,has,prior works,outperforms has prior works,0.5963264107704163
translation,32,153,results,results,has,our model,results has our model,0.5871725678443909
translation,32,154,results,cnn / dailymail,achieve,absolute improvement,cnn / dailymail achieve absolute improvement,0.6796098947525024
translation,32,154,results,absolute improvement,of,0.35,absolute improvement of 0.35,0.5330636501312256
translation,32,154,results,absolute improvement,of,0.48,absolute improvement of 0.48,0.518726646900177
translation,32,154,results,absolute improvement,of,0.28,absolute improvement of 0.28,0.5244995951652527
translation,32,154,results,0.35,in,rouge -1,0.35 in rouge -1,0.5492193102836609
translation,32,154,results,0.48,in,rouge - 2,0.48 in rouge - 2,0.561512291431427
translation,32,154,results,0.28,in,rouge -l,0.28 in rouge -l,0.5937789082527161
translation,32,154,results,rouge -l,over,pegasus,rouge -l over pegasus,0.6892698407173157
translation,32,154,results,results,For,cnn / dailymail,results For cnn / dailymail,0.5737385153770447
translation,32,155,results,our model,obtains,better results,our model obtains better results,0.6238764524459839
translation,32,155,results,better results,than,previous topic-aware model bart + ta,better results than previous topic-aware model bart + ta,0.5627415180206299
translation,32,155,results,previous topic-aware model bart + ta,in,rouge - 2,previous topic-aware model bart + ta in rouge - 2,0.5418665409088135
translation,32,155,results,rouge - 2,with,0.6 points,rouge - 2 with 0.6 points,0.6705514192581177
translation,32,155,results,results,has,our model,results has our model,0.5871725678443909
translation,32,157,results,xsum dataset,is,more abstractive,xsum dataset is more abstractive,0.5911570191383362
translation,32,157,results,more abstractive,than,cnn / dailymail,more abstractive than cnn / dailymail,0.6194583773612976
translation,32,157,results,xsum dataset,has,our gain,xsum dataset has our gain,0.6140416264533997
translation,32,157,results,cnn / dailymail,has,our gain,cnn / dailymail has our gain,0.6306656002998352
translation,32,157,results,"cardie , 2020 )",has,our gain,"cardie , 2020 ) has our gain",0.6227790713310242
translation,32,157,results,results,On,xsum dataset,results On xsum dataset,0.569536030292511
translation,32,158,results,bart + ta,achieve,3.8 absolute improvement,bart + ta achieve 3.8 absolute improvement,0.6298980116844177
translation,32,158,results,bart + ta,achieve,2.4,bart + ta achieve 2.4,0.6318305134773254
translation,32,158,results,bart + ta,achieve,3.8,bart + ta achieve 3.8,0.6352186799049377
translation,32,158,results,3.8 absolute improvement,in,rouge -1,3.8 absolute improvement in rouge -1,0.5263300538063049
translation,32,158,results,3.8 absolute improvement,in,rouge -l.,3.8 absolute improvement in rouge -l.,0.5517491698265076
translation,32,158,results,2.4,in,rouge - 2,2.4 in rouge - 2,0.5716829299926758
translation,32,158,results,3.8,in,rouge -l.,3.8 in rouge -l.,0.593341588973999
translation,32,158,results,results,Compared with,bart + ta,results Compared with bart + ta,0.673656165599823
translation,32,160,results,global semantics,capable of helping,model,global semantics capable of helping model,0.6466190814971924
translation,32,160,results,model,generate,better target summaries,model generate better target summaries,0.7090376019477844
translation,32,160,results,results,show,global semantics,results show global semantics,0.641984224319458
translation,32,161,results,arxiv and pubmed dataset,achieve,improvement,arxiv and pubmed dataset achieve improvement,0.6518892645835876
translation,32,161,results,improvement,over,baseline pegasus,improvement over baseline pegasus,0.6405203342437744
translation,32,161,results,baseline pegasus,designed specifically for,abstractive text summarization,baseline pegasus designed specifically for abstractive text summarization,0.6146560311317444
translation,32,161,results,results,For,arxiv and pubmed dataset,results For arxiv and pubmed dataset,0.5775366425514221
translation,32,162,results,arxiv dataset,gain,increase,arxiv dataset gain increase,0.7303118705749512
translation,32,162,results,increase,of,0.71,increase of 0.71,0.5930149555206299
translation,32,162,results,increase,of,2.48,increase of 2.48,0.5836063027381897
translation,32,162,results,increase,of,1.46,increase of 1.46,0.5954821109771729
translation,32,162,results,0.71,in,rouge -1,0.71 in rouge -1,0.5425077676773071
translation,32,162,results,2.48,in,rouge - 2,2.48 in rouge - 2,0.5698817372322083
translation,32,162,results,2.48,in,rouge -l.,2.48 in rouge -l.,0.5797740817070007
translation,32,162,results,1.46,in,rouge -l.,1.46 in rouge -l.,0.5786029100418091
translation,32,162,results,results,for,arxiv dataset,results for arxiv dataset,0.5774884223937988
translation,32,175,results,our generated summaries,favor,human judgements,our generated summaries favor human judgements,0.6753817200660706
translation,32,175,results,more likely,to maintain,important content,more likely to maintain important content,0.640224814414978
translation,32,175,results,important content,in,original text,important content in original text,0.40999898314476013
translation,32,175,results,important content,than,other systems ' summaries,important content than other systems ' summaries,0.586407482624054
translation,32,175,results,results,show,our generated summaries,results show our generated summaries,0.6433230042457581
translation,32,180,results,our topic-oriented module,able to,improve,our topic-oriented module able to improve,0.6849955916404724
translation,32,180,results,improve,has,performance,improve has performance,0.5578044652938843
translation,32,184,results,neural topic models,has,significantly outperform,neural topic models has significantly outperform,0.6039696931838989
translation,32,184,results,significantly outperform,has,approaches,significantly outperform has approaches,0.6101477146148682
translation,32,193,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,32,193,results,outperforms,has,traditional topic models,outperforms has traditional topic models,0.5640228986740112
translation,32,193,results,results,has,our model,results has our model,0.5871725678443909
translation,33,163,ablation-analysis,k,is,smaller,k is smaller,0.6928496360778809
translation,33,163,ablation-analysis,recall improvement,caused by,adversarial augmentation,recall improvement caused by adversarial augmentation,0.5970723032951355
translation,33,163,ablation-analysis,adversarial augmentation,is,relatively significant,adversarial augmentation is relatively significant,0.5450127124786377
translation,33,163,ablation-analysis,k,has,recall improvement,k has recall improvement,0.5928635001182556
translation,33,163,ablation-analysis,smaller,has,recall improvement,smaller has recall improvement,0.5900436043739319
translation,33,163,ablation-analysis,ablation analysis,When,k,ablation analysis When k,0.7004925012588501
translation,33,88,baselines,"factcc ( kryscinski et al. , 2020 )",contain,441 consistent samples,"factcc ( kryscinski et al. , 2020 ) contain 441 consistent samples",0.6347129940986633
translation,33,88,baselines,"factcc ( kryscinski et al. , 2020 )",contain,62 inconsistent samples,"factcc ( kryscinski et al. , 2020 ) contain 62 inconsistent samples",0.6222323179244995
translation,33,111,experimental-setup,max length,of,input,max length of input,0.5867661237716675
translation,33,111,experimental-setup,input,is,512,input is 512,0.5982403755187988
translation,33,111,experimental-setup,experimental setup,has,max length,experimental setup has max length,0.52577805519104
translation,33,112,experimental-setup,adam,used for,optimization,adam used for optimization,0.6671109795570374
translation,33,112,experimental-setup,adam,with,initial learning rate,adam with initial learning rate,0.6089812517166138
translation,33,112,experimental-setup,adam,with,batch size,adam with batch size,0.641900360584259
translation,33,112,experimental-setup,optimization,with,initial learning rate,optimization with initial learning rate,0.5914350748062134
translation,33,112,experimental-setup,initial learning rate,of,1e - 5,initial learning rate of 1e - 5,0.612038254737854
translation,33,112,experimental-setup,batch size,is,16,batch size is 16,0.643535315990448
translation,33,112,experimental-setup,experimental setup,has,adam,experimental setup has adam,0.4964992105960846
translation,33,112,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,33,113,experimental-setup,training epoch,up to,3,training epoch up to 3,0.6650152802467346
translation,33,113,experimental-setup,validation set,has,every 1000 steps,validation set has every 1000 steps,0.5649785399436951
translation,33,113,experimental-setup,experimental setup,set,training epoch,experimental setup set training epoch,0.640143096446991
translation,33,116,experimental-setup,amplitude,of,adversarial perturbation,amplitude of adversarial perturbation,0.5926867127418518
translation,33,116,experimental-setup,adversarial perturbation,obtained by,heuristic method,adversarial perturbation obtained by heuristic method,0.6308765411376953
translation,33,116,experimental-setup,heuristic method,range of,2e - 3 to 1e - 2,heuristic method range of 2e - 3 to 1e - 2,0.7242965698242188
translation,33,116,experimental-setup,experimental setup,has,amplitude,experimental setup has amplitude,0.5127542614936829
translation,33,119,experimental-setup,training stage,of,our model,training stage of our model,0.5731043815612793
translation,33,119,experimental-setup,our model,lasts about,2.0 hours,our model lasts about 2.0 hours,0.7137693762779236
translation,33,119,experimental-setup,2.0 hours,per epoch,four pieces of tesla- v100 - sxm2 ( 32gb ),2.0 hours per epoch four pieces of tesla- v100 - sxm2 ( 32gb ),0.7454103827476501
translation,33,119,experimental-setup,2.0 hours,on,four pieces of tesla- v100 - sxm2 ( 32gb ),2.0 hours on four pieces of tesla- v100 - sxm2 ( 32gb ),0.5376599431037903
translation,33,119,experimental-setup,experimental setup,has,training stage,experimental setup has training stage,0.5289889574050903
translation,33,35,hyperparameters,artificial datasets,based on,benchmark summarization datasets,artificial datasets based on benchmark summarization datasets,0.5106621384620667
translation,33,35,hyperparameters,hyperparameters,construct,artificial datasets,hyperparameters construct artificial datasets,0.656883955001831
translation,33,6,model,generate highly abstract yet factually correct summaries,proposed,efficient weaksupervised adversarial data augmentation approach,generate highly abstract yet factually correct summaries proposed efficient weaksupervised adversarial data augmentation approach,0.649244487285614
translation,33,6,model,efficient weaksupervised adversarial data augmentation approach,to form,factual consistency dataset,efficient weaksupervised adversarial data augmentation approach to form factual consistency dataset,0.5781251788139343
translation,33,6,model,model,proposed,efficient weaksupervised adversarial data augmentation approach,model proposed efficient weaksupervised adversarial data augmentation approach,0.6673676371574402
translation,33,34,model,model,propose,robust weaksupervised factual consistency evaluation model,model propose robust weaksupervised factual consistency evaluation model,0.6147406101226807
translation,33,36,model,implicit augmentation,to obtain,hard factual inconsistent examples,implicit augmentation to obtain hard factual inconsistent examples,0.5612143278121948
translation,33,36,model,hard factual inconsistent examples,by,adversarial attack,hard factual inconsistent examples by adversarial attack,0.5610602498054504
translation,33,36,model,model,propose,implicit augmentation,model propose implicit augmentation,0.6738697290420532
translation,33,165,model,effective error detection,can be carried out through,gradient distribution,effective error detection can be carried out through gradient distribution,0.6963444352149963
translation,33,165,model,our proposed adversarial augmentation,optimize,gradient distribution,our proposed adversarial augmentation optimize gradient distribution,0.7022743225097656
translation,33,165,model,model,proved,effective error detection,model proved effective error detection,0.7145269513130188
translation,33,123,results,our evaluation model,gains,higher accuracy,our evaluation model gains higher accuracy,0.7500746846199036
translation,33,123,results,higher accuracy,on,both datasets ' ground truth references,higher accuracy on both datasets ' ground truth references,0.49212831258773804
translation,33,123,results,higher accuracy,are,significantly better,higher accuracy are significantly better,0.5727684497833252
translation,33,123,results,significantly better,than,factcc,significantly better than factcc,0.6108701229095459
translation,33,123,results,significantly better,than,factccx,significantly better than factccx,0.616121768951416
translation,33,123,results,results,has,our evaluation model,results has our evaluation model,0.5595352053642273
translation,33,124,results,model,corrupts,reference summary,model corrupts reference summary,0.6768337488174438
translation,33,124,results,reference summary,rather than,fragment,reference summary rather than fragment,0.679628312587738
translation,33,124,results,fragment,of,source document,fragment of source document,0.5892860293388367
translation,33,124,results,fits better,with,abstractive summarization,fits better with abstractive summarization,0.6753517985343933
translation,33,125,results,factual consistent dataset,of,cnn / dm,factual consistent dataset of cnn / dm,0.6022456884384155
translation,33,125,results,significantly outperform,by,3.6 % ( marco - f1 ),significantly outperform by 3.6 % ( marco - f1 ),0.6237246990203857
translation,33,125,results,fec,by,2.5 % ( accuracy ),fec by 2.5 % ( accuracy ),0.6121280193328857
translation,33,125,results,fec,by,6.8 % ( balance accuracy ),fec by 6.8 % ( balance accuracy ),0.6072811484336853
translation,33,125,results,fec,by,3.6 % ( marco - f1 ),fec by 3.6 % ( marco - f1 ),0.6191769242286682
translation,33,125,results,factual consistent dataset,has,our model,factual consistent dataset has our model,0.5946292877197266
translation,33,125,results,cnn / dm,has,our model,cnn / dm has our model,0.567014217376709
translation,33,125,results,our model,has,significantly outperform,our model has significantly outperform,0.6035624742507935
translation,33,125,results,significantly outperform,has,fec,significantly outperform has fec,0.6339491009712219
translation,33,125,results,results,On,factual consistent dataset,results On factual consistent dataset,0.5859805941581726
translation,33,126,results,xsum,gains,consistent improvement,xsum gains consistent improvement,0.7850754261016846
translation,33,126,results,our model,gains,consistent improvement,our model gains consistent improvement,0.7975962162017822
translation,33,126,results,consistent improvement,on,all metrics,consistent improvement on all metrics,0.5012156963348389
translation,33,126,results,xsum,has,our model,xsum has our model,0.6311904788017273
translation,33,126,results,results,On,xsum,results On xsum,0.6111196279525757
translation,33,127,results,every model,performs,poorer,every model performs poorer,0.7110440135002136
translation,33,127,results,poorer,on,xsum,poorer on xsum,0.7006240487098694
translation,33,127,results,poorer,than,cnn / dm,poorer than cnn / dm,0.6089869141578674
translation,33,127,results,xsum,than,cnn / dm,xsum than cnn / dm,0.6046767830848694
translation,33,127,results,results,has,every model,results has every model,0.5685828328132629
translation,33,129,results,implicitly augment,through,adversarial attack,implicitly augment through adversarial attack,0.6967158317565918
translation,33,129,results,data,through,adversarial attack,data through adversarial attack,0.699005663394928
translation,33,129,results,improvement,on,cnn / dm,improvement on cnn / dm,0.5570889115333557
translation,33,129,results,implicitly augment,has,data,implicitly augment has data,0.5865358710289001
translation,33,129,results,adversarial attack,has,significantly benefits,adversarial attack has significantly benefits,0.5758488774299622
translation,33,129,results,significantly benefits,has,evaluation,significantly benefits has evaluation,0.5934385061264038
translation,33,129,results,results,shows that,implicitly augment,results shows that implicitly augment,0.7264211773872375
translation,33,161,results,performance,of,error tracing,performance of error tracing,0.6325482726097107
translation,33,161,results,performance,gains,consistent improvement,performance gains consistent improvement,0.8309783935546875
translation,33,161,results,error tracing,gains,consistent improvement,error tracing gains consistent improvement,0.7913964986801147
translation,33,161,results,consistent improvement,on,token level and span level,consistent improvement on token level and span level,0.5364118218421936
translation,33,161,results,adversarial augmentation,has,performance,adversarial augmentation has performance,0.549363911151886
translation,33,161,results,results,indicate,adversarial augmentation,results indicate adversarial augmentation,0.5802455544471741
translation,33,161,results,results,with,adversarial augmentation,results with adversarial augmentation,0.6548485159873962
translation,33,172,results,results,on,cnn / dm artificial dataset,results on cnn / dm artificial dataset,0.49503156542778015
translation,34,142,ablation-analysis,ablation analysis,has,single issue,ablation analysis has single issue,0.5701960325241089
translation,34,176,experimental-setup,"transformer ( vaswani et al. , 2017 ) implementation",for,surface realization,"transformer ( vaswani et al. , 2017 ) implementation for surface realization",0.5981407165527344
translation,34,176,experimental-setup,experimental setup,use,"transformer ( vaswani et al. , 2017 ) implementation","experimental setup use transformer ( vaswani et al. , 2017 ) implementation",0.5810261964797974
translation,34,177,experimental-setup,"off-the-shelf neural crf tagger ( yang and zhang , 2018 )",for,entity extraction,"off-the-shelf neural crf tagger ( yang and zhang , 2018 ) for entity extraction",0.5471277236938477
translation,34,177,experimental-setup,experimental setup,train,"off-the-shelf neural crf tagger ( yang and zhang , 2018 )","experimental setup train off-the-shelf neural crf tagger ( yang and zhang , 2018 )",0.6515032052993774
translation,34,6,model,hybrid generation approach,inspired by,traditional concept- to - text systems,hybrid generation approach inspired by traditional concept- to - text systems,0.6708900928497314
translation,34,6,model,model,introduce,hybrid generation approach,model introduce hybrid generation approach,0.6414549350738525
translation,34,7,model,pertinent relations,from,input documents,pertinent relations from input documents,0.5247782468795776
translation,34,7,model,model,first learns to extract,pertinent relations,model first learns to extract pertinent relations,0.7343896627426147
translation,34,175,model,policy network,is,three layer feedforward neural network,policy network is three layer feedforward neural network,0.5552447438240051
translation,34,175,model,model,has,policy network,model has policy network,0.5456021428108215
translation,34,54,results,seq2seq models,receive,competent fluency scores,seq2seq models receive competent fluency scores,0.637830376625061
translation,34,54,results,our method,performs,stronger,our method performs stronger,0.6934271454811096
translation,34,54,results,stronger,on,task -specific metrics,stronger on task -specific metrics,0.5409306287765503
translation,34,54,results,task -specific metrics,including,relevance,task -specific metrics including relevance,0.687506377696991
translation,34,54,results,task -specific metrics,including,content faithfulness,task -specific metrics including content faithfulness,0.6631952524185181
translation,34,54,results,task -specific metrics,including,aggregation cognisance,task -specific metrics including aggregation cognisance,0.697128176689148
translation,34,55,results,summaries,receive,absolute 20 % more,summaries receive absolute 20 % more,0.6550297141075134
translation,34,55,results,summaries,receive,absolute 7 % more,summaries receive absolute 7 % more,0.6599255204200745
translation,34,55,results,summaries,receive,7 %,summaries receive 7 %,0.6900215744972229
translation,34,55,results,absolute 20 % more,on,aggregation cognisance,absolute 20 % more on aggregation cognisance,0.5479181408882141
translation,34,55,results,absolute 7 % more,on,content relevance,absolute 7 % more on content relevance,0.4942251443862915
translation,34,55,results,7 %,on,faithfulness,7 % on faithfulness,0.5215969085693359
translation,34,55,results,faithfulness,to,input documents,faithfulness to input documents,0.5787371397018433
translation,34,55,results,next best baseline,in,traditional and update settings,next best baseline in traditional and update settings,0.5245360136032104
translation,34,193,results,transformer,produces,fluent summaries,transformer produces fluent summaries,0.6321830749511719
translation,34,193,results,results,has,transformer,results has transformer,0.4226538836956024
translation,34,194,results,performance,is,poorer,performance is poorer,0.613436222076416
translation,34,194,results,poorer,for,"copy-gen , entity data2text and graphwriter models","poorer for copy-gen , entity data2text and graphwriter models",0.6374857425689697
translation,34,194,results,results,has,performance,results has performance,0.5972660779953003
translation,34,197,results,our model,performs,better,our model performs better,0.6546649932861328
translation,34,197,results,better,than,baselines,better than baselines,0.6307952404022217
translation,34,197,results,better,both,kg ( i ) and kg ( g ) metrics,better both kg ( i ) and kg ( g ) metrics,0.7083562612533569
translation,34,197,results,baselines,on,kg ( i ) and kg ( g ) metrics,baselines on kg ( i ) and kg ( g ) metrics,0.5520655512809753
translation,34,197,results,baselines,both,kg ( i ) and kg ( g ) metrics,baselines both kg ( i ) and kg ( g ) metrics,0.7064425945281982
translation,34,197,results,results,has,our model,results has our model,0.5871725678443909
translation,34,199,results,our model,scores,7 % higher,our model scores 7 % higher,0.7389926910400391
translation,34,199,results,our model,scores,17 % higher,our model scores 17 % higher,0.7310581207275391
translation,34,199,results,7 % higher,on,kg ( g ),7 % higher on kg ( g ),0.5649778842926025
translation,34,199,results,7 % higher,on,kg ( i ),7 % higher on kg ( i ),0.5926821231842041
translation,34,199,results,17 % higher,on,kg ( i ),17 % higher on kg ( i ),0.5894486904144287
translation,34,199,results,17 % higher,compared to,next best performance,17 % higher compared to next best performance,0.6829471588134766
translation,34,199,results,results,has,our model,results has our model,0.5871725678443909
translation,34,200,results,outputs,produced by,our method,outputs produced by our method,0.6606922149658203
translation,34,200,results,outputs,produced by,transformer baseline,outputs produced by transformer baseline,0.6400769948959351
translation,34,200,results,transformer baseline,on,benefits of whole - grains,transformer baseline on benefits of whole - grains,0.5453855991363525
translation,34,201,results,our method,conveys,"more relevant , factual and organized information","our method conveys more relevant , factual and organized information",0.7006209492683411
translation,34,201,results,"more relevant , factual and organized information",in,concise manner,"more relevant , factual and organized information in concise manner",0.4799434244632721
translation,34,201,results,results,has,our method,results has our method,0.5589964985847473
translation,34,204,results,our model,able to select and fuse,more relevant information,our model able to select and fuse more relevant information,0.7129130363464355
translation,34,210,results,entity extraction model,receives,token - level f1 score,entity extraction model receives token - level f1 score,0.6032686829566956
translation,34,210,results,token - level f1 score,of,79 %,token - level f1 score of 79 %,0.5466129183769226
translation,34,210,results,entity extraction model,has,crf-based sequence tagging model,entity extraction model has crf-based sequence tagging model,0.5288966298103333
translation,34,211,results,relation classification model,receives,accuracy,relation classification model receives accuracy,0.6525790095329285
translation,34,211,results,accuracy,of,69 %,accuracy of 69 %,0.6086074113845825
translation,34,211,results,relation classification model,has,bert based text classifier,relation classification model has bert based text classifier,0.5872222185134888
translation,34,211,results,results,has,relation classification model,results has relation classification model,0.57330322265625
translation,35,8,baselines,facetsum,provides,multiple summaries,facetsum provides multiple summaries,0.6644474864006042
translation,35,8,baselines,multiple summaries,targeted at,specific sections,multiple summaries targeted at specific sections,0.6616974472999573
translation,35,8,baselines,specific sections,of,long document,specific sections of long document,0.6293317079544067
translation,35,8,baselines,specific sections,including,purpose,specific sections including purpose,0.7727811336517334
translation,35,112,experimental-setup,bart experiments,finetuned using,"fairseq ( ott et al. , 2019 )","bart experiments finetuned using fairseq ( ott et al. , 2019 )",0.720905065536499
translation,35,112,experimental-setup,"fairseq ( ott et al. , 2019 )",with,learning rate,"fairseq ( ott et al. , 2019 ) with learning rate",0.583095133304596
translation,35,112,experimental-setup,"fairseq ( ott et al. , 2019 )",with,batch size,"fairseq ( ott et al. , 2019 ) with batch size",0.5839835405349731
translation,35,112,experimental-setup,"fairseq ( ott et al. , 2019 )",with,max tokens per batch,"fairseq ( ott et al. , 2019 ) with max tokens per batch",0.5940964221954346
translation,35,112,experimental-setup,"fairseq ( ott et al. , 2019 )",with,update frequency,"fairseq ( ott et al. , 2019 ) with update frequency",0.6175500750541687
translation,35,112,experimental-setup,learning rate,of,3e ?5,learning rate of 3e ?5,0.6395402550697327
translation,35,112,experimental-setup,batch size,of,1,batch size of 1,0.6655300855636597
translation,35,112,experimental-setup,max tokens per batch,of,"10,000","max tokens per batch of 10,000",0.59520423412323
translation,35,112,experimental-setup,update frequency,of,4,update frequency of 4,0.6715508699417114
translation,35,113,experimental-setup,all models,for,"20,000 steps","all models for 20,000 steps",0.6407946348190308
translation,35,113,experimental-setup,all models,with,single nvidia tesla v100 16gb,all models with single nvidia tesla v100 16gb,0.6276987195014954
translation,35,113,experimental-setup,experimental setup,finetune,all models,experimental setup finetune all models,0.7003800868988037
translation,35,83,results,heuristic models,do not perform,well,heuristic models do not perform well,0.7505597472190857
translation,35,83,results,well,on,full,well on full,0.5937249660491943
translation,35,83,results,results,observe,heuristic models,results observe heuristic models,0.5845237374305725
translation,35,84,results,all models,perform,poorly,all models perform poorly,0.6480908393859863
translation,35,84,results,poorly,on,summarizing,poorly on summarizing,0.5850221514701843
translation,35,84,results,unsupervised models,fail to perform,better,unsupervised models fail to perform better,0.8027237057685852
translation,35,84,results,better,than,simple heuristics,better than simple heuristics,0.605701208114624
translation,35,84,results,summarizing,has,individual facets,summarizing has individual facets,0.5724907517433167
translation,35,84,results,results,has,all models,results has all models,0.5029959678649902
translation,35,94,results,outperform,by,large margin,outperform by large margin,0.6385749578475952
translation,35,94,results,unsupervised baselines,by,large margin,unsupervised baselines by large margin,0.5607677102088928
translation,35,94,results,supervised models,has,outperform,supervised models has outperform,0.5981801748275757
translation,35,94,results,outperform,has,unsupervised baselines,outperform has unsupervised baselines,0.5860959887504578
translation,35,94,results,results,has,supervised models,results has supervised models,0.5369096994400024
translation,35,95,results,two training strategies,has,bart - facet,two training strategies has bart - facet,0.6035534739494324
translation,35,95,results,bart - facet,has,outperforms,bart - facet has outperforms,0.6590983867645264
translation,35,95,results,outperforms,has,bart,outperforms has bart,0.6937707662582397
translation,35,95,results,outperforms,has,significantly,outperforms has significantly,0.6533594727516174
translation,35,95,results,bart,has,significantly,bart has significantly,0.7038794755935669
translation,35,95,results,results,Comparing between,two training strategies,results Comparing between two training strategies,0.6493411660194397
translation,35,99,results,i+c,as,source text,i+c as source text,0.5532018542289734
translation,35,99,results,both training strategies,exhibit,much better results,both training strategies exhibit much better results,0.5923125147819519
translation,35,99,results,much better results,than using,full paper,much better results than using full paper,0.6508851647377014
translation,35,99,results,i+c,has,both training strategies,i+c has both training strategies,0.5890843272209167
translation,35,99,results,source text,has,both training strategies,source text has both training strategies,0.5718544721603394
translation,35,99,results,results,With,i+c,results With i+c,0.6131404042243958
translation,36,29,experiments,best approach,for modeling and learning,factuality,best approach for modeling and learning factuality,0.7109457850456238
translation,36,29,experiments,factuality,particularly for,highly abstractive summarization settings,factuality particularly for highly abstractive summarization settings,0.6003413200378418
translation,36,43,experiments,cnn / dailymail,has,multi-sentence abstrac-tive summary dataset,cnn / dailymail has multi-sentence abstrac-tive summary dataset,0.5087509751319885
translation,36,31,model,prior factuality detection model,capable of leveraging,fine - grained annotations,prior factuality detection model capable of leveraging fine - grained annotations,0.6568828225135803
translation,36,31,model,errors,within,generated texts,errors within generated texts,0.6581099629402161
translation,36,31,model,errors,within,generated texts,errors within generated texts,0.6581099629402161
translation,36,31,model,model,use,prior factuality detection model,model use prior factuality detection model,0.6128852963447571
translation,36,30,results,utility,of,fine- grained human annotations,utility of fine- grained human annotations,0.5741685032844543
translation,36,30,results,fine- grained human annotations,with,sentence - level factuality annotations,fine- grained human annotations with sentence - level factuality annotations,0.5701354742050171
translation,36,30,results,results,compare,utility,results compare utility,0.6073794960975647
translation,36,102,results,models,trained on,xsum,models trained on xsum,0.7353853583335876
translation,36,102,results,xsum,learn to,hallucinate,xsum learn to hallucinate,0.658103346824646
translation,36,102,results,xsum,produce,extrinsic errors,xsum produce extrinsic errors,0.6635544896125793
translation,36,102,results,hallucinate,has,new content,hallucinate has new content,0.638434112071991
translation,36,102,results,results,has,models,results has models,0.5335168838500977
translation,36,156,results,models,trained on,ent -c,models trained on ent -c,0.7672005295753479
translation,36,156,results,models,trained on,gen -c,models trained on gen -c,0.7634449601173401
translation,36,156,results,models,perform,slightly better,models perform slightly better,0.5782269835472107
translation,36,156,results,results,show,models,results show models,0.6068246960639954
translation,36,170,results,all models,trained on,synthetic factuality datasets,all models trained on synthetic factuality datasets,0.6598755121231079
translation,36,170,results,very poorly,close to,majority label baseline,very poorly close to majority label baseline,0.690790593624115
translation,36,180,results,performance,with,models that leverage,performance with models that leverage,0.6385744214057922
translation,36,180,results,fine- grained factuality annotations,has,significantly boosts,fine- grained factuality annotations has significantly boosts,0.570236086845398
translation,36,180,results,significantly boosts,has,performance,significantly boosts has performance,0.5811278820037842
translation,36,180,results,models that leverage,has,significantly outperforming,models that leverage has significantly outperforming,0.6102983355522156
translation,36,180,results,information ( dae ),has,significantly outperforming,information ( dae ) has significantly outperforming,0.5891208648681641
translation,36,180,results,significantly outperforming,has,sentence - level models,significantly outperforming has sentence - level models,0.5962076187133789
translation,36,180,results,results,availability of,fine- grained factuality annotations,results availability of fine- grained factuality annotations,0.6247239112854004
translation,36,181,results,fine- grained annotations,see that,dae - weak model,fine- grained annotations see that dae - weak model,0.637723982334137
translation,36,181,results,dae - weak model,decomposes,error computation,dae - weak model decomposes error computation,0.7251506447792053
translation,36,181,results,dae - weak model,explicitly tries to localize,errors,dae - weak model explicitly tries to localize errors,0.7681431770324707
translation,36,181,results,errors,is,better,errors is better,0.608124315738678
translation,36,181,results,better,than,sentence - level model,better than sentence - level model,0.5625485181808472
translation,36,181,results,results,Even in the absence of,fine- grained annotations,results Even in the absence of fine- grained annotations,0.6532673239707947
translation,36,182,results,best model,achieves,accuracy,best model achieves accuracy,0.682331383228302
translation,36,182,results,accuracy,of,55.9,accuracy of 55.9,0.5827658772468567
translation,36,182,results,cnn / dm,has,best model,cnn / dm has best model,0.575404703617096
translation,36,190,results,dae - weak model,at,both levels of granularity,dae - weak model at both levels of granularity,0.5274657011032104
translation,36,190,results,dae model,has,outperforms,dae model has outperforms,0.6300978064537048
translation,36,190,results,outperforms,has,dae - weak model,outperforms has dae - weak model,0.5812498331069946
translation,36,190,results,results,has,dae model,results has dae model,0.544569730758667
translation,36,194,results,dae model 's best checkpoint,on,test data ( best-ckpt ),dae model 's best checkpoint on test data ( best-ckpt ),0.5561200380325317
translation,36,194,results,dae model 's best checkpoint,achieves,recall,dae model 's best checkpoint achieves recall,0.7120733261108398
translation,36,194,results,recall,of,83.9,recall of 83.9,0.5907145738601685
translation,36,194,results,results,select,dae model 's best checkpoint,results select dae model 's best checkpoint,0.6735113263130188
translation,36,205,results,our dae - masked training,leads to,better factuality performance,our dae - masked training leads to better factuality performance,0.6020965576171875
translation,36,205,results,results,show,our dae - masked training,results show our dae - masked training,0.5788785815238953
translation,36,211,results,outperforms,both,baseline model,outperforms both baseline model,0.6487284898757935
translation,36,211,results,outperforms,both,loss truncation approach,outperforms both loss truncation approach,0.7023799419403076
translation,36,211,results,proposed approach,has,outperforms,proposed approach has outperforms,0.6428829431533813
translation,36,211,results,results,show,proposed approach,results show proposed approach,0.6772235631942749
translation,37,19,experiments,first benchmark,to simulate,low-resource domain adaptation setting,first benchmark to simulate low-resource domain adaptation setting,0.6479602456092834
translation,37,19,experiments,low-resource domain adaptation setting,for,abstractive summarization,low-resource domain adaptation setting for abstractive summarization,0.5668096542358398
translation,37,19,experiments,existing datasets,across,six diverse domains,existing datasets across six diverse domains,0.7289126515388489
translation,37,19,experiments,adaptsum,has,first benchmark,adaptsum has first benchmark,0.599931538105011
translation,37,19,experiments,email,has,"zhang and tetreault , 2019 )","email has zhang and tetreault , 2019 )",0.5783013701438904
translation,37,6,model,second phase,of,pre-training,second phase of pre-training,0.5922977328300476
translation,37,6,model,pre-training,has,on large-scale generative models,pre-training has on large-scale generative models,0.49740591645240784
translation,37,6,model,model,investigate,second phase,model investigate second phase,0.7006511092185974
translation,37,22,model,adding a second phase,of,pre-training,adding a second phase of pre-training,0.5733082890510559
translation,37,22,model,pre-training,on,large-scale generative models,pre-training on large-scale generative models,0.4986274242401123
translation,37,22,model,source domain pre-training ( sdpt ),based on,labeled source domain summarization dataset,source domain pre-training ( sdpt ) based on labeled source domain summarization dataset,0.627505362033844
translation,37,22,model,domain- adaptive pre-training ( dapt ),based on,unlabeled substantial domain- related corpus,domain- adaptive pre-training ( dapt ) based on unlabeled substantial domain- related corpus,0.5957162976264954
translation,37,22,model,task - adaptive pre-training ( tapt ),based on,unlabeled smallscale task - related corpus,task - adaptive pre-training ( tapt ) based on unlabeled smallscale task - related corpus,0.5831636786460876
translation,38,59,ablation-analysis,factual score,of,summaries,factual score of summaries,0.5874058604240417
translation,38,59,ablation-analysis,factual score,of,summaries,factual score of summaries,0.5874058604240417
translation,38,59,ablation-analysis,factual score,of,summaries,factual score of summaries,0.5874058604240417
translation,38,59,ablation-analysis,summaries,from,bottomup,summaries from bottomup,0.5990631580352783
translation,38,59,ablation-analysis,summaries,from,tconvs2s,summaries from tconvs2s,0.6628857851028442
translation,38,59,ablation-analysis,summaries,from,tconvs2s,summaries from tconvs2s,0.6628857851028442
translation,38,59,ablation-analysis,1.4 %,on,cnn / dailymail,1.4 % on cnn / dailymail,0.5082911849021912
translation,38,59,ablation-analysis,0.9 %,on,xsum,0.9 % on xsum,0.6219802498817444
translation,38,59,ablation-analysis,0.9 %,on,xsum,0.9 % on xsum,0.6219802498817444
translation,38,59,ablation-analysis,score,of,summaries,score of summaries,0.5987358689308167
translation,38,59,ablation-analysis,summaries,from,tconvs2s,summaries from tconvs2s,0.6628857851028442
translation,38,59,ablation-analysis,tconvs2s,increases,3.1 %,tconvs2s increases 3.1 %,0.7180136442184448
translation,38,59,ablation-analysis,3.1 %,on,xsum,3.1 % on xsum,0.5985488891601562
translation,38,59,ablation-analysis,fc,has,factual score,fc has factual score,0.5821402668952942
translation,38,59,ablation-analysis,ablation analysis,correction by,fc,ablation analysis correction by fc,0.7773891687393188
translation,38,178,ablation-analysis,proposed knowledge graph component,can help,train-from-scratch fasum model,proposed knowledge graph component can help train-from-scratch fasum model,0.6272311806678772
translation,38,178,ablation-analysis,train-from-scratch fasum model,to excel in,factual consistency,train-from-scratch fasum model to excel in factual consistency,0.6741800904273987
translation,38,178,ablation-analysis,ablation analysis,has,proposed knowledge graph component,ablation analysis has proposed knowledge graph component,0.5540669560432434
translation,38,180,ablation-analysis,clear drop,in,factual score,clear drop in factual score,0.5485692024230957
translation,38,180,ablation-analysis,2.8 %,in,cnn / dailymail,2.8 % in cnn / dailymail,0.5118074417114258
translation,38,180,ablation-analysis,0.9 %,in,xsum,0.9 % in xsum,0.6086588501930237
translation,38,180,ablation-analysis,factual score,has,2.8 %,factual score has 2.8 %,0.5637142658233643
translation,38,180,ablation-analysis,factual score,has,0.9 %,factual score has 0.9 %,0.5499242544174194
translation,38,185,ablation-analysis,correction model fc,effectively enhance,factual consistency,correction model fc effectively enhance factual consistency,0.573969304561615
translation,38,185,ablation-analysis,factual consistency,of,summaries,factual consistency of summaries,0.5797967910766602
translation,38,185,ablation-analysis,factual consistency,especially when,original summary,factual consistency especially when original summary,0.648666501045227
translation,38,185,ablation-analysis,summaries,generated by,various baseline models,summaries generated by various baseline models,0.6092395782470703
translation,38,185,ablation-analysis,original summary,has,relatively low factual consistency,original summary has relatively low factual consistency,0.49468299746513367
translation,38,185,ablation-analysis,ablation analysis,has,correction model fc,ablation analysis has correction model fc,0.531351625919342
translation,38,186,ablation-analysis,factual score,of,bottomup,factual score of bottomup,0.5754631161689758
translation,38,186,ablation-analysis,bottomup,increases by,1.4 %,bottomup increases by 1.4 %,0.7085614204406738
translation,38,186,ablation-analysis,1.4 %,after,correction,1.4 % after correction,0.6984384059906006
translation,38,186,ablation-analysis,cnn / dm,has,factual score,cnn / dm has factual score,0.5742088556289673
translation,38,186,ablation-analysis,ablation analysis,on,cnn / dm,ablation analysis on cnn / dm,0.5614622235298157
translation,38,187,ablation-analysis,xsum,after,correction,xsum after correction,0.7001277208328247
translation,38,187,ablation-analysis,increase,by,0.2 % to 3.1 %,increase by 0.2 % to 3.1 %,0.6348614692687988
translation,38,187,ablation-analysis,0.2 % to 3.1 %,for,all baseline models,0.2 % to 3.1 % for all baseline models,0.5744290351867676
translation,38,187,ablation-analysis,xsum,has,factual scores,xsum has factual scores,0.59902024269104
translation,38,187,ablation-analysis,correction,has,factual scores,correction has factual scores,0.5579783320426941
translation,38,187,ablation-analysis,factual scores,has,increase,factual scores has increase,0.6037084460258484
translation,38,187,ablation-analysis,ablation analysis,On,xsum,ablation analysis On xsum,0.6147519946098328
translation,38,189,ablation-analysis,correction,improve,rouge scores,correction improve rouge scores,0.694196343421936
translation,38,189,ablation-analysis,rather small impact,on,rouge score,rather small impact on rouge score,0.5385685563087463
translation,38,189,ablation-analysis,rather small impact,on,rouge scores,rather small impact on rouge scores,0.5444989800453186
translation,38,189,ablation-analysis,rouge score,of,most models,rouge score of most models,0.5406633615493774
translation,38,189,ablation-analysis,rouge scores,of,most models,rouge scores of most models,0.5589479804039001
translation,38,189,ablation-analysis,most models,in,xsum dataset,most models in xsum dataset,0.5674638152122498
translation,38,189,ablation-analysis,correction,has,rather small impact,correction has rather small impact,0.5829187035560608
translation,38,189,ablation-analysis,ablation analysis,has,correction,ablation analysis has correction,0.5236008763313293
translation,38,169,baselines,tconvs2s,based on,topic modeling,tconvs2s based on topic modeling,0.6635276079177856
translation,38,169,baselines,tconvs2s,based on,convolutional neural networks,tconvs2s based on convolutional neural networks,0.6577370166778564
translation,38,169,baselines,),based on,topic modeling,) based on topic modeling,0.6850533485412598
translation,38,169,baselines,),based on,convolutional neural networks,) based on convolutional neural networks,0.6894699335098267
translation,38,169,baselines,tconvs2s,has,),tconvs2s has ),0.6721383333206177
translation,38,169,baselines,baselines,has,tconvs2s,baselines has tconvs2s,0.6015908122062683
translation,38,170,baselines,bottomup,uses,bottom - up approach,bottomup uses bottom - up approach,0.6649259924888611
translation,38,170,baselines,bottom - up approach,to generate,summarization,bottom - up approach to generate summarization,0.7097688317298889
translation,38,170,baselines,baselines,has,bottomup,baselines has bottomup,0.5836814641952515
translation,38,171,baselines,"unilm ( dong et al. , 2019 )",utilizes,large-scale pretraining,"unilm ( dong et al. , 2019 ) utilizes large-scale pretraining",0.5806499719619751
translation,38,171,baselines,large-scale pretraining,to produce,state - of - the - art abstractive summaries,large-scale pretraining to produce state - of - the - art abstractive summaries,0.6666973829269409
translation,38,171,baselines,baselines,has,"unilm ( dong et al. , 2019 )","baselines has unilm ( dong et al. , 2019 )",0.5101092457771301
translation,38,144,hyperparameters,number of beams,is,4,number of beams is 4,0.6153006553649902
translation,38,144,hyperparameters,number of beams,is,6,number of beams is 6,0.6171212196350098
translation,38,144,hyperparameters,4,for,cnn / dailymail,4 for cnn / dailymail,0.6257439255714417
translation,38,144,hyperparameters,6,for,xsum,6 for xsum,0.6620886325836182
translation,38,144,hyperparameters,hyperparameters,has,number of beams,hyperparameters has number of beams,0.5442430377006531
translation,38,146,hyperparameters,teacher forcing,used in,training,teacher forcing used in training,0.7097183465957642
translation,38,146,hyperparameters,hyperparameters,has,teacher forcing,hyperparameters has teacher forcing,0.5390803813934326
translation,38,147,hyperparameters,"adam ( kingma and ba , 2014 )",as,optimizer,"adam ( kingma and ba , 2014 ) as optimizer",0.5086410641670227
translation,38,147,hyperparameters,optimizer,with,learning rate,optimizer with learning rate,0.6228271722793579
translation,38,147,hyperparameters,learning rate,of,2e - 4,learning rate of 2e - 4,0.6387818455696106
translation,38,147,hyperparameters,hyperparameters,use,"adam ( kingma and ba , 2014 )","hyperparameters use adam ( kingma and ba , 2014 )",0.5926938652992249
translation,38,148,hyperparameters,bi-lstm,to produce,initial embedding,bi-lstm to produce initial embedding,0.6800448894500732
translation,38,148,hyperparameters,bi-lstm,to produce,graph attention network ( gat ),bi-lstm to produce graph attention network ( gat ),0.6944060921669006
translation,38,148,hyperparameters,bi-lstm,to produce,hidden state,bi-lstm to produce hidden state,0.688571035861969
translation,38,148,hyperparameters,initial embedding,of,graph nodes,initial embedding of graph nodes,0.575086236000061
translation,38,148,hyperparameters,hidden state,of size,64,hidden state of size 64,0.7465900778770447
translation,38,148,hyperparameters,hidden state,size,50,hidden state size 50,0.7920359373092651
translation,38,148,hyperparameters,initial embedding,has,hidden state,initial embedding has hidden state,0.5480834245681763
translation,38,148,hyperparameters,graph nodes,has,hidden state,graph nodes has hidden state,0.5461874604225159
translation,38,148,hyperparameters,graph attention network ( gat ),has,8 heads,graph attention network ( gat ) has 8 heads,0.6037308573722839
translation,38,148,hyperparameters,hyperparameters,has,bi-lstm,hyperparameters has bi-lstm,0.5417085289955139
translation,38,149,hyperparameters,dropout rate,is,0.6,dropout rate is 0.6,0.5466409921646118
translation,38,149,hyperparameters,dropout rate,is,0.1,dropout rate is 0.1,0.53923100233078
translation,38,149,hyperparameters,0.6,in,gat,0.6 in gat,0.5960467457771301
translation,38,149,hyperparameters,0.1,has,elsewhere,0.1 has elsewhere,0.5528321862220764
translation,38,149,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,38,150,hyperparameters,hyperparameters,use,subword tokenizer sentencepiece,hyperparameters use subword tokenizer sentencepiece,0.5497138500213623
translation,38,152,hyperparameters,size,of,32 k,size of 32 k,0.6312406659126282
translation,38,152,hyperparameters,dimension,of,720,dimension of 720,0.7012558579444885
translation,38,152,hyperparameters,vocabulary,has,size,vocabulary has size,0.592721164226532
translation,38,152,hyperparameters,hyperparameters,has,vocabulary,hyperparameters has vocabulary,0.5417485237121582
translation,38,153,hyperparameters,correction model fc,follows,"unilm ( dong et al. , 2019 ) architecture","correction model fc follows unilm ( dong et al. , 2019 ) architecture",0.6534702181816101
translation,38,153,hyperparameters,"unilm ( dong et al. , 2019 ) architecture",initialized with,weights,"unilm ( dong et al. , 2019 ) architecture initialized with weights",0.7544317841529846
translation,38,153,hyperparameters,weights,from,roberta - large,weights from roberta - large,0.6124418377876282
translation,38,153,hyperparameters,hyperparameters,has,correction model fc,hyperparameters has correction model fc,0.5511257648468018
translation,38,154,hyperparameters,model,for,5 epochs,model for 5 epochs,0.653461217880249
translation,38,154,hyperparameters,model,with,learning rate,model with learning rate,0.6086345314979553
translation,38,154,hyperparameters,model,with,linear warmup,model with linear warmup,0.6614894866943359
translation,38,154,hyperparameters,5 epochs,with,learning rate,5 epochs with learning rate,0.6657905578613281
translation,38,154,hyperparameters,learning rate,of,1e - 5,learning rate of 1e - 5,0.6323861479759216
translation,38,154,hyperparameters,linear warmup,over,linear decay,linear warmup over linear decay,0.6866487860679626
translation,38,154,hyperparameters,linear warmup,one -fifths of,total steps,linear warmup one -fifths of total steps,0.6301420331001282
translation,38,154,hyperparameters,hyperparameters,fine - tune,model,hyperparameters fine - tune model,0.7390584349632263
translation,38,155,hyperparameters,decoding,uses,beam search,decoding uses beam search,0.6001933217048645
translation,38,155,hyperparameters,decoding,blocks,trigram duplicates,decoding blocks trigram duplicates,0.6443073749542236
translation,38,155,hyperparameters,beam search,with,width,beam search with width,0.6351640820503235
translation,38,155,hyperparameters,width,of,2,width of 2,0.6903644800186157
translation,38,155,hyperparameters,hyperparameters,During,decoding,hyperparameters During decoding,0.6597015857696533
translation,38,156,hyperparameters,batch size,during,finetuning,batch size during finetuning,0.7270157337188721
translation,38,156,hyperparameters,finetuning,is,24,finetuning is 24,0.5922707319259644
translation,38,156,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,38,6,model,fact - aware summarization model fasum,to extract and integrate,factual relations,fact - aware summarization model fasum to extract and integrate factual relations,0.6762291789054871
translation,38,6,model,factual relations,into,summary generation process,factual relations into summary generation process,0.5669465661048889
translation,38,6,model,summary generation process,via,graph attention,summary generation process via graph attention,0.6127796769142151
translation,38,6,model,model,propose,fact - aware summarization model fasum,model propose fact - aware summarization model fasum,0.6612987518310547
translation,38,7,model,factual corrector model fc,to automatically correct,factual errors,factual corrector model fc to automatically correct factual errors,0.7420850396156311
translation,38,7,model,factual errors,from,summaries,factual errors from summaries,0.6023284792900085
translation,38,7,model,summaries,generated by,existing systems,summaries generated by existing systems,0.663246214389801
translation,38,7,model,model,design,factual corrector model fc,model design factual corrector model fc,0.5901003479957581
translation,38,50,model,fact- aware summarization model,has,fasum,fact- aware summarization model has fasum,0.584714949131012
translation,38,50,model,model,as,fact- aware summarization model,model as fact- aware summarization model,0.5089676380157471
translation,38,51,model,factual corrector model,to help improve,factual consistency,factual corrector model to help improve factual consistency,0.6642751097679138
translation,38,51,model,factual consistency,of,any given summary,factual consistency of any given summary,0.5853445529937744
translation,38,51,model,factual corrector model,has,fc,factual corrector model has fc,0.6055025458335876
translation,38,51,model,model,propose,factual corrector model,model propose factual corrector model,0.6522594690322876
translation,38,145,model,encoder and decoder,10 layers of,10 heads,encoder and decoder 10 layers of 10 heads,0.7382556200027466
translation,38,145,model,10 heads,for,attention,10 heads for attention,0.6589257121086121
translation,38,145,model,model,In,fasum,model In fasum,0.6227262616157532
translation,38,227,model,seq2seq model,generates,summaries,seq2seq model generates summaries,0.6268211007118225
translation,38,227,model,summaries,with,less factual consistency,summaries with less factual consistency,0.6073625683784485
translation,38,227,model,summaries,with,informativeness,summaries with informativeness,0.5994412302970886
translation,38,227,model,knowledge graph component,has,seq2seq model,knowledge graph component has seq2seq model,0.5656521320343018
translation,38,227,model,model,without,knowledge graph component,model without knowledge graph component,0.6741046905517578
translation,38,174,results,all baseline systems,in,factual consistency scores,all baseline systems in factual consistency scores,0.4923346936702728
translation,38,174,results,factual consistency scores,in,cnn / dailymail,factual consistency scores in cnn / dailymail,0.5313141942024231
translation,38,174,results,our model fasum,has,outperforms,our model fasum has outperforms,0.6678268909454346
translation,38,174,results,outperforms,has,all baseline systems,outperforms has all baseline systems,0.5887711644172668
translation,38,174,results,results,has,our model fasum,results has our model fasum,0.5998467206954956
translation,38,175,results,bottomup,in,factual score,bottomup in factual score,0.5261666774749756
translation,38,175,results,cnn / dailymail,has,fa -sum,cnn / dailymail has fa -sum,0.6663180589675903
translation,38,175,results,results,In,cnn / dailymail,results In cnn / dailymail,0.5233896374702454
translation,38,188,results,fc,boost,factual consistency,fc boost factual consistency,0.7092300653457642
translation,38,188,results,factual consistency,of,our fasum model,factual consistency of our fasum model,0.588715672492981
translation,38,188,results,results,has,fc,results has fc,0.4968360960483551
translation,38,199,results,fasum,achieves,closest ratio,fasum achieves closest ratio,0.6972406506538391
translation,38,199,results,fasum,higher than,bottomup and unilm,fasum higher than bottomup and unilm,0.7295650839805603
translation,38,199,results,closest ratio,of,novel n-gram,closest ratio of novel n-gram,0.5807700157165527
translation,38,199,results,novel n-gram,compared with,reference summaries,novel n-gram compared with reference summaries,0.6342406868934631
translation,38,199,results,results,shows,fasum,results shows fasum,0.6905786991119385
translation,38,216,results,fa - sum,achieves,lowest ratio,fa - sum achieves lowest ratio,0.6988946199417114
translation,38,216,results,fc,helps further reducing,conflicting facts,fc helps further reducing conflicting facts,0.73749840259552
translation,38,216,results,conflicting facts,in,generated summaries,conflicting facts in generated summaries,0.5292966365814209
translation,38,216,results,results,has,fa - sum,results has fa - sum,0.5289326906204224
translation,38,224,results,our model fasum,achieves,highest factual consistency score,our model fasum achieves highest factual consistency score,0.6650805473327637
translation,38,224,results,our model fasum,achieves,considerably outperforming,our model fasum achieves considerably outperforming,0.7126304507255554
translation,38,224,results,highest factual consistency score,higher than,unilm,highest factual consistency score higher than unilm,0.7071326375007629
translation,38,224,results,considerably outperforming,has,bottomup,considerably outperforming has bottomup,0.6080169677734375
translation,38,224,results,results,has,our model fasum,results has our model fasum,0.5998467206954956
translation,38,225,results,statistical test,compared with,unilm,statistical test compared with unilm,0.7137437462806702
translation,38,225,results,our model 's score,is,statistically significant,our model 's score is statistically significant,0.5724453926086426
translation,38,225,results,statistical test,has,our model 's score,statistical test has our model 's score,0.5834146738052368
translation,38,225,results,unilm,has,our model 's score,unilm has our model 's score,0.581670343875885
translation,38,226,results,our model,comparable with,unilm,our model comparable with unilm,0.7267431020736694
translation,38,226,results,informativeness,has,our model,informativeness has our model,0.5472273230552673
translation,38,226,results,outperforms,has,bottomup,outperforms has bottomup,0.6500919461250305
translation,38,226,results,results,In terms of,informativeness,results In terms of informativeness,0.708037793636322
translation,38,234,results,corrected summaries,are,significantly more likely,corrected summaries are significantly more likely,0.5756034851074219
translation,38,234,results,significantly more likely,to be judged as,more factually correct,significantly more likely to be judged as more factually correct,0.5689618587493896
translation,38,234,results,more factually correct,for,both baseline models,more factually correct for both baseline models,0.5788341164588928
translation,39,137,baselines,mclas,in,low-resource scenarios,mclas in low-resource scenarios,0.5841398239135742
translation,39,137,baselines,baselines,compare,mclas,baselines compare mclas,0.7237935662269592
translation,39,119,experimental-setup,multilingual bert ( mbert ),to initialize,transformer encoder,multilingual bert ( mbert ) to initialize transformer encoder,0.7030525803565979
translation,39,119,experimental-setup,experimental setup,use,multilingual bert ( mbert ),experimental setup use multilingual bert ( mbert ),0.5855706334114075
translation,39,122,experimental-setup,hidden size,of,decoder 's self-attention,hidden size of decoder 's self-attention,0.5626451373100281
translation,39,122,experimental-setup,hidden size,of,feed -forward network,hidden size of feed -forward network,0.5903554558753967
translation,39,122,experimental-setup,decoder 's self-attention,is,768,decoder 's self-attention is 768,0.5819633603096008
translation,39,122,experimental-setup,feed -forward network,is,2048,feed -forward network is 2048,0.6267820596694946
translation,39,122,experimental-setup,experimental setup,has,hidden size,experimental setup has hidden size,0.5616794228553772
translation,39,125,experimental-setup,encoder 's learning rate ? e,set as,0.005,encoder 's learning rate ? e set as 0.005,0.6206517219543457
translation,39,125,experimental-setup,experimental setup,has,encoder 's learning rate ? e,experimental setup has encoder 's learning rate ? e,0.5569790005683899
translation,39,125,experimental-setup,experimental setup,has,decoder 's learning rate ? d,experimental setup has decoder 's learning rate ? d,0.5620670914649963
translation,39,126,experimental-setup,warmup-steps,for,encoder,warmup-steps for encoder,0.6574819087982178
translation,39,126,experimental-setup,warmup-steps,for,decoder,warmup-steps for decoder,0.6763496994972229
translation,39,126,experimental-setup,encoder,are,"10,000 and 5,000","encoder are 10,000 and 5,000",0.6424916982650757
translation,39,126,experimental-setup,"10,000 and 5,000",for,decoder,"10,000 and 5,000 for decoder",0.6931006908416748
translation,39,126,experimental-setup,experimental setup,has,warmup-steps,experimental setup has warmup-steps,0.5155088305473328
translation,39,127,experimental-setup,model,on,two titan rtx gpus,model on two titan rtx gpus,0.5364278554916382
translation,39,127,experimental-setup,two titan rtx gpus,for,one day,two titan rtx gpus for one day,0.5929604172706604
translation,39,127,experimental-setup,one day,with,gradient accumulation,one day with gradient accumulation,0.6250938177108765
translation,39,127,experimental-setup,gradient accumulation,every,5 steps,gradient accumulation every 5 steps,0.6305671334266663
translation,39,127,experimental-setup,experimental setup,train,model,experimental setup train model,0.6514950394630432
translation,39,133,experimental-setup,decoding stage,use,beam search ( size 5 ),decoding stage use beam search ( size 5 ),0.6501566171646118
translation,39,133,experimental-setup,trigram block,to avoid,repetition,trigram block to avoid repetition,0.6495500802993774
translation,39,133,experimental-setup,experimental setup,During,decoding stage,experimental setup During decoding stage,0.6840110421180725
translation,39,134,experimental-setup,length penalty,set between,0.6 and 1,length penalty set between 0.6 and 1,0.6888121366500854
translation,39,134,experimental-setup,experimental setup,has,length penalty,experimental setup has length penalty,0.4614953398704529
translation,39,138,experimental-setup,low-resource scenarios,initialize,our model,low-resource scenarios initialize our model,0.7470499277114868
translation,39,138,experimental-setup,low-resource scenarios,use,few samples,low-resource scenarios use few samples,0.6890287399291992
translation,39,138,experimental-setup,our model,with,pretrained ms model,our model with pretrained ms model,0.5996454358100891
translation,39,138,experimental-setup,our model,use,few samples,our model use few samples,0.6857283711433411
translation,39,138,experimental-setup,experimental setup,In,low-resource scenarios,experimental setup In low-resource scenarios,0.5305768847465515
translation,39,192,experiments,monolingual summaries,with,models,monolingual summaries with models,0.6160995960235596
translation,39,192,experiments,models,trained in,maximum lowresource scenario,models trained in maximum lowresource scenario,0.7392792701721191
translation,39,8,model,novel multi-task framework,for,cross -lingual abstractive summarization ( mclas ),novel multi-task framework for cross -lingual abstractive summarization ( mclas ),0.5837274193763733
translation,39,8,model,cross -lingual abstractive summarization ( mclas ),in,low-resource setting,cross -lingual abstractive summarization ( mclas ) in low-resource setting,0.4882791340351105
translation,39,8,model,model,propose,novel multi-task framework,model propose novel multi-task framework,0.6661131978034973
translation,39,10,model,shared decoder,learns,interactions,shared decoder learns interactions,0.6788543462753296
translation,39,10,model,interactions,involving,alignments and summary patterns,interactions involving alignments and summary patterns,0.6914612650871277
translation,39,10,model,interactions,encourages,attaining knowledge transfer,interactions encourages attaining knowledge transfer,0.6403748393058777
translation,39,10,model,alignments and summary patterns,across,languages,alignments and summary patterns across languages,0.6752925515174866
translation,39,10,model,alignments and summary patterns,encourages,attaining knowledge transfer,alignments and summary patterns encourages attaining knowledge transfer,0.6143972873687744
translation,39,10,model,model,has,shared decoder,model has shared decoder,0.5979683995246887
translation,39,26,model,one decoder,shared by,ms and cls tasks,one decoder shared by ms and cls tasks,0.7089415192604065
translation,39,26,model,one decoder,setting,generation target,one decoder setting generation target,0.49513304233551025
translation,39,26,model,generation target,as,sequential concatenation,generation target as sequential concatenation,0.522901713848114
translation,39,26,model,sequential concatenation,of,monolingual summary,sequential concatenation of monolingual summary,0.5659976601600647
translation,39,26,model,sequential concatenation,of,corresponding cross-lingual summary,sequential concatenation of corresponding cross-lingual summary,0.5944585204124451
translation,39,26,model,model,has,one decoder,model has one decoder,0.602988064289093
translation,39,27,model,decoder,conducts,translation task,decoder conducts translation task,0.6495106220245361
translation,39,27,model,monolingual and cross-lingual summaries,has,decoder,monolingual and cross-lingual summaries has decoder,0.5522359013557434
translation,39,27,model,model,Sequentially generating,monolingual and cross-lingual summaries,model Sequentially generating monolingual and cross-lingual summaries,0.7316725850105286
translation,39,32,model,training strategy,under,limited resources,training strategy under limited resources,0.6404954195022583
translation,39,32,model,training strategy,pre-train,mclas,training strategy pre-train mclas,0.8030567765235901
translation,39,32,model,mclas,on,large-scale monolingual document -summary parallel datasets,mclas on large-scale monolingual document -summary parallel datasets,0.4804890751838684
translation,39,32,model,mclas,to well - equip,decoder,mclas to well - equip decoder,0.7525190711021423
translation,39,32,model,decoder,with,general summary capability,decoder with general summary capability,0.6612997055053711
translation,39,32,model,model,In terms of,training strategy,model In terms of training strategy,0.7255447506904602
translation,39,32,model,model,pre-train,mclas,model pre-train mclas,0.7651633620262146
translation,39,120,model,decoder,is,transformer decoder,decoder is transformer decoder,0.6057046055793762
translation,39,120,model,transformer decoder,with,6 layers,transformer decoder with 6 layers,0.6463626027107239
translation,39,120,model,model,has,decoder,model has decoder,0.6226420402526855
translation,39,121,model,attention module,has,8 different attention heads,attention module has 8 different attention heads,0.569387674331665
translation,39,121,model,model,has,attention module,model has attention module,0.5680129528045654
translation,39,158,results,mclas,achieves,significant improvements,mclas achieves significant improvements,0.7047879099845886
translation,39,158,results,significant improvements,over,baselines,significant improvements over baselines,0.6777081489562988
translation,39,158,results,baselines,in,all the low-resource scenarios,baselines in all the low-resource scenarios,0.5168524384498596
translation,39,158,results,zh2ensum and en2desum datasets,has,mclas,zh2ensum and en2desum datasets has mclas,0.6213879585266113
translation,39,158,results,results,In,zh2ensum and en2desum datasets,results In zh2ensum and en2desum datasets,0.5314471125602722
translation,39,159,results,ncls + ms,in,our experiments,ncls + ms in our experiments,0.6005671620368958
translation,39,159,results,does not bring much improvement,to,ncls model,does not bring much improvement to ncls model,0.570877194404602
translation,39,159,results,our experiments,has,does not bring much improvement,our experiments has does not bring much improvement,0.6204293370246887
translation,39,159,results,results,combining,ncls + ms,results combining ncls + ms,0.6777582168579102
translation,39,161,results,mclas,not perform,as well,mclas not perform as well,0.6700260639190674
translation,39,161,results,mclas,not perform,other two datasets,mclas not perform other two datasets,0.6509742140769958
translation,39,161,results,as well,as,other two datasets,as well as other two datasets,0.5494765639305115
translation,39,161,results,en2zhsum dataset,has,mclas,en2zhsum dataset has mclas,0.5795111060142517
translation,39,161,results,results,in,en2zhsum dataset,results in en2zhsum dataset,0.5218455195426941
translation,39,174,results,all the models,achieve,better results,all the models achieve better results,0.6042379140853882
translation,39,174,results,data size,has,increases,data size has increases,0.6059015393257141
translation,39,174,results,increases,has,all the models,increases has all the models,0.6279631853103638
translation,39,175,results,proposed mclas,has,outperformed,proposed mclas has outperformed,0.6252602338790894
translation,39,175,results,outperformed,has,ncls and ncls + ms,outperformed has ncls and ncls + ms,0.6085422039031982
translation,39,175,results,results,has,proposed mclas,results has proposed mclas,0.650841236114502
translation,39,176,results,especially strong,in,conciseness,especially strong in conciseness,0.5064236521720886
translation,39,176,results,results,notice,mclas,results notice mclas,0.6738675832748413
translation,39,183,results,initialization methods,bring,huge improvement,initialization methods bring huge improvement,0.6198971271514893
translation,39,183,results,huge improvement,to,all of the models,huge improvement to all of the models,0.5674163699150085
translation,39,183,results,three datasets,has,initialization methods,three datasets has initialization methods,0.5195412039756775
translation,39,183,results,results,Among,three datasets,results Among three datasets,0.5717463493347168
translation,39,187,results,interactions,enable,mclas,interactions enable mclas,0.7741520404815674
translation,39,187,results,mclas,to generate,shorter summaries,mclas to generate shorter summaries,0.6820361018180847
translation,39,187,results,shorter summaries,than,other models,shorter summaries than other models,0.5460538864135742
translation,39,187,results,results,show,interactions,results show interactions,0.5472081303596497
translation,39,188,results,mclas,keep,summary,mclas keep summary,0.6966030597686768
translation,39,188,results,summary,in,fairly appropriate length,summary in fairly appropriate length,0.49144336581230164
translation,39,188,results,summary,leading to,concise generated summaries,summary leading to concise generated summaries,0.6778680086135864
translation,39,188,results,results,safely conclude,mclas,results safely conclude mclas,0.6610164046287537
translation,39,193,results,mclas,retains,more monolingual summarization knowledge,mclas retains more monolingual summarization knowledge,0.7050014138221741
translation,39,193,results,more monolingual summarization knowledge,in,zh2ensum dataset,more monolingual summarization knowledge in zh2ensum dataset,0.5150951147079468
translation,39,193,results,results,see that,mclas,results see that mclas,0.6764321327209473
translation,39,194,results,en2desum dataset,has,monolingual summarization performance,en2desum dataset has monolingual summarization performance,0.5088599920272827
translation,39,194,results,monolingual summarization performance,has,is even significantly improved,monolingual summarization performance has is even significantly improved,0.5851524472236633
translation,39,194,results,results,In,en2desum dataset,results In en2desum dataset,0.5459292531013489
translation,39,197,results,rouge improvement,mainly resulted from,precision,rouge improvement mainly resulted from precision,0.5838109254837036
translation,39,197,results,recall,has,barely decrease,recall has barely decrease,0.6110541224479675
translation,39,197,results,barely decrease,has,performances,barely decrease has performances,0.5722779035568237
translation,39,197,results,results,find that,rouge improvement,results find that rouge improvement,0.6087628602981567
translation,41,96,ablation-analysis,ablation analysis,on,activitynet captions,ablation analysis on activitynet captions,0.5105698704719543
translation,41,96,ablation-analysis,ablation analysis,has,models b@4 m c r@4,ablation analysis has models b@4 m c r@4,0.5828657150268555
translation,41,99,ablation-analysis,trigram blocking ( w/ o tri-blk ),has,performance,trigram blocking ( w/ o tri-blk ) has performance,0.5925735831260681
translation,41,99,ablation-analysis,performance,has,degrades,performance has degrades,0.5837839841842651
translation,41,99,ablation-analysis,ablation analysis,remove,trigram blocking ( w/ o tri-blk ),ablation analysis remove trigram blocking ( w/ o tri-blk ),0.7079439759254456
translation,41,101,ablation-analysis,bert pretrained weights,are,not the major factor,bert pretrained weights are not the major factor,0.6049880385398865
translation,41,101,ablation-analysis,not the major factor,to,final performance,not the major factor to final performance,0.5575953722000122
translation,41,101,ablation-analysis,ablation analysis,see that,bert pretrained weights,ablation analysis see that bert pretrained weights,0.6252735257148743
translation,41,89,baselines,soft - nms,uses,soft-nms,soft - nms uses soft-nms,0.6588125228881836
translation,41,89,baselines,soft - nms,uses,proposal captioning model,soft - nms uses proposal captioning model,0.5388148427009583
translation,41,89,baselines,soft - nms,uses,esgn model,soft - nms uses esgn model,0.566224217414856
translation,41,89,baselines,soft - nms,uses,proposal captioning model,soft - nms uses proposal captioning model,0.5388148427009583
translation,41,89,baselines,soft - nms,uses,esgn model,soft - nms uses esgn model,0.566224217414856
translation,41,89,baselines,soft - nms,uses,esgn model,soft - nms uses esgn model,0.566224217414856
translation,41,89,baselines,soft-nms,to select,event segments,soft-nms to select event segments,0.7212948799133301
translation,41,89,baselines,soft-nms,to select,event segments,soft-nms to select event segments,0.7212948799133301
translation,41,89,baselines,event segments,from,bmn proposals,event segments from bmn proposals,0.5580947399139404
translation,41,89,baselines,event segments,from,bmn proposals,event segments from bmn proposals,0.5580947399139404
translation,41,89,baselines,event segments,from,bmn proposals,event segments from bmn proposals,0.5580947399139404
translation,41,89,baselines,proposal captioning model,to generate,captions,proposal captioning model to generate captions,0.6690187454223633
translation,41,89,baselines,esgn,uses,esgn model,esgn uses esgn model,0.6142772436141968
translation,41,89,baselines,esgn model,to select,event segments,esgn model to select event segments,0.6894308924674988
translation,41,89,baselines,event segments,from,bmn proposals,event segments from bmn proposals,0.5580947399139404
translation,41,89,baselines,v- trans,has,vanilla transformer model,v- trans has vanilla transformer model,0.5285618901252747
translation,41,89,baselines,trans- xl,has,transformer - xl model,trans- xl has transformer - xl model,0.5800548195838928
translation,41,89,baselines,baselines,has,soft - nms,baselines has soft - nms,0.5641879439353943
translation,41,102,baselines,our summarization model,with,unsupervised methods,our summarization model with unsupervised methods,0.5684995651245117
translation,41,102,baselines,unsupervised methods,has,"lexrank ( erkan and radev , 2004 )","unsupervised methods has lexrank ( erkan and radev , 2004 )",0.5396666526794434
translation,41,80,experimental-setup,video preprocessing,use,appearance and optical flow features,video preprocessing use appearance and optical flow features,0.6204528212547302
translation,41,80,experimental-setup,experimental setup,For,video preprocessing,experimental setup For video preprocessing,0.5400585532188416
translation,41,83,experimental-setup,caption summarization model,use,base bert model,caption summarization model use base bert model,0.5624252557754517
translation,41,83,experimental-setup,2 stacked transformer layers,with,hidden size,2 stacked transformer layers with hidden size,0.5873488187789917
translation,41,83,experimental-setup,hidden size,set to,768,hidden size set to 768,0.7564138770103455
translation,41,83,experimental-setup,number of heads,set to,8,number of heads set to 8,0.7167339324951172
translation,41,83,experimental-setup,base bert model,has,2 stacked transformer layers,base bert model has 2 stacked transformer layers,0.5716710090637207
translation,41,83,experimental-setup,experimental setup,For,caption summarization model,experimental setup For caption summarization model,0.536807119846344
translation,41,84,experimental-setup,max input length,to,"1,700","max input length to 1,700",0.5794134140014648
translation,41,84,experimental-setup,max input length,to,"1,000","max input length to 1,000",0.5902429819107056
translation,41,84,experimental-setup,max input length,to,"1,000","max input length to 1,000",0.5902429819107056
translation,41,84,experimental-setup,batch size,to,10,batch size to 10,0.6650561690330505
translation,41,84,experimental-setup,batch size,to,1,batch size to 1,0.6180254220962524
translation,41,84,experimental-setup,batch size,to,", ? to 1","batch size to , ? to 1",0.6392185091972351
translation,41,84,experimental-setup,to 1,for,activitynet captions,to 1 for activitynet captions,0.6443542838096619
translation,41,84,experimental-setup,to 1,for,youcookii,to 1 for youcookii,0.7343106865882874
translation,41,84,experimental-setup,to 1,for,youcookii,to 1 for youcookii,0.7343106865882874
translation,41,84,experimental-setup,max input length,to,"1,000","max input length to 1,000",0.5902429819107056
translation,41,84,experimental-setup,", ? to 1",for,youcookii,", ? to 1 for youcookii",0.7548806667327881
translation,41,84,experimental-setup,1,has,", ? to 1","1 has , ? to 1",0.6317054033279419
translation,41,84,experimental-setup,experimental setup,set,max input length,experimental setup set max input length,0.6739605069160461
translation,41,84,experimental-setup,experimental setup,set,batch size,experimental setup set batch size,0.6767397522926331
translation,41,84,experimental-setup,experimental setup,set,to 1,experimental setup set to 1,0.7116703987121582
translation,41,84,experimental-setup,experimental setup,set,max input length,experimental setup set max input length,0.6739605069160461
translation,41,85,experimental-setup,warmup steps,set to,step num,warmup steps set to step num,0.7133263349533081
translation,41,85,experimental-setup,step num,of,1 epoch,step num of 1 epoch,0.6198189854621887
translation,41,85,experimental-setup,experimental setup,has,warmup steps,experimental setup has warmup steps,0.498988538980484
translation,41,100,experimental-setup,our vpcsum,without,bert pretrained weights ( w/ o pretrain ),our vpcsum without bert pretrained weights ( w/ o pretrain ),0.7084116339683533
translation,41,31,experiments,outperforms,using,ground - truth event segment labels,outperforms using ground - truth event segment labels,0.5824725031852722
translation,41,31,experiments,some previous methods,using,ground - truth event segment labels,some previous methods using ground - truth event segment labels,0.5582010746002197
translation,41,31,experiments,activ-itynet captions dataset,has,our method,activ-itynet captions dataset has our method,0.5423166751861572
translation,41,31,experiments,our method,has,outperforms,our method has outperforms,0.6322360634803772
translation,41,31,experiments,outperforms,has,some previous methods,outperforms has some previous methods,0.5855492353439331
translation,41,41,experiments,proposal captioning,choose,tsrm - rnn model,proposal captioning choose tsrm - rnn model,0.6237244009971619
translation,41,41,experiments,proposal captioning,choose,"vtransformer model ( lei et al. , 2020 )","proposal captioning choose vtransformer model ( lei et al. , 2020 )",0.68314129114151
translation,41,41,experiments,tsrm - rnn model,for,activitynet captions,tsrm - rnn model for activitynet captions,0.5680986642837524
translation,41,41,experiments,"vtransformer model ( lei et al. , 2020 )",for,youcookii,"vtransformer model ( lei et al. , 2020 ) for youcookii",0.6191010475158691
translation,41,41,experiments,youcookii,according to,proposal captioning performance,youcookii according to proposal captioning performance,0.650971531867981
translation,41,82,experiments,esgn model,use,transformer encoder,esgn model use transformer encoder,0.5786490440368652
translation,41,82,experiments,transformer encoder,instead of,rnn encoder,transformer encoder instead of rnn encoder,0.6107732057571411
translation,41,82,experiments,transformer encoder,with,hidden size,transformer encoder with hidden size,0.633111834526062
translation,41,82,experiments,transformer encoder,with,number of heads,transformer encoder with number of heads,0.6289364099502563
translation,41,82,experiments,transformer encoder,with,number of layers,transformer encoder with number of layers,0.6177542209625244
translation,41,82,experiments,hidden size,set to,512,hidden size set to 512,0.7357071042060852
translation,41,82,experiments,number of heads,set to,8,number of heads set to 8,0.7167339324951172
translation,41,82,experiments,number of layers,set to,3,number of layers set to 3,0.7173470854759216
translation,41,6,model,model,propose,novel framework,model propose novel framework,0.720393180847168
translation,41,7,model,sentence - level captions,focusing on,different video clips,sentence - level captions focusing on different video clips,0.6414898633956909
translation,41,7,model,sentence - level captions,summarize,captions,sentence - level captions summarize captions,0.5913823843002319
translation,41,7,model,captions,to obtain,final paragraph caption,captions to obtain final paragraph caption,0.5606732368469238
translation,41,47,model,summation,of,token embeddings,summation of token embeddings,0.5618452429771423
translation,41,47,model,summation,of,segment embeddings,summation of segment embeddings,0.5997824668884277
translation,41,47,model,summation,of,position embeddings,summation of position embeddings,0.6238905787467957
translation,41,47,model,summation,to represent,each word,summation to represent each word,0.7058864235877991
translation,41,47,model,position embeddings,to represent,each word,position embeddings to represent each word,0.5997730493545532
translation,41,47,model,model,use,summation,model use summation,0.7183815836906433
translation,41,93,results,our model vpcsum,within,new framework,our model vpcsum within new framework,0.6956594586372375
translation,41,93,results,our model vpcsum,generate,better paragraph captions,our model vpcsum generate better paragraph captions,0.6486742496490479
translation,41,93,results,better paragraph captions,with,higher bleu@4,better paragraph captions with higher bleu@4,0.6351155042648315
translation,41,93,results,better paragraph captions,with,meteor,better paragraph captions with meteor,0.5790343880653381
translation,41,93,results,better paragraph captions,with,cider,better paragraph captions with cider,0.6428070664405823
translation,41,93,results,better paragraph captions,with,lower repetition score r@4,better paragraph captions with lower repetition score r@4,0.6459950804710388
translation,41,93,results,activitynet captions,has,our model vpcsum,activitynet captions has our model vpcsum,0.5847041606903076
translation,41,93,results,outperforming,has,v-trans,outperforming has v-trans,0.5986823439598083
translation,41,93,results,outperforming,has,mart * models,outperforming has mart * models,0.562946081161499
translation,41,93,results,results,on,activitynet captions,results on activitynet captions,0.5146124958992004
translation,41,94,results,our model,inferior to,models,our model inferior to models,0.7009769678115845
translation,41,94,results,outperforms,inferior to,models,outperforms inferior to models,0.7508741617202759
translation,41,94,results,models,using,ground - truth segments,models using ground - truth segments,0.5947045087814331
translation,41,94,results,models,using,ground - truth segments,models using ground - truth segments,0.5947045087814331
translation,41,94,results,youcookii dataset,has,our model,youcookii dataset has our model,0.5720970034599304
translation,41,94,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,41,94,results,outperforms,has,models,outperforms has models,0.6708464026451111
translation,41,94,results,results,On,youcookii dataset,results On youcookii dataset,0.5275710225105286
translation,41,97,results,full model ( full ),has,traditional extractive summarization annotation method ( hard- label ),full model ( full ) has traditional extractive summarization annotation method ( hard- label ),0.5387912392616272
translation,41,97,results,results,Compared to,full model ( full ),results Compared to full model ( full ),0.6457055807113647
translation,41,124,results,our vpcsum model,performs,better,our vpcsum model performs better,0.6721013188362122
translation,41,124,results,better,in,relevance and diversity,better in relevance and diversity,0.5112349390983582
translation,41,124,results,results,has,our vpcsum model,results has our vpcsum model,0.5498150587081909
translation,42,130,ablation-analysis,lower factual consistency scores,compared to,q-c,lower factual consistency scores compared to q-c,0.6053835153579712
translation,42,130,ablation-analysis,lower factual consistency scores,shows that,negative loss,lower factual consistency scores shows that negative loss,0.6131725311279297
translation,42,130,ablation-analysis,negative loss,in,conseq,negative loss in conseq,0.5829423666000366
translation,42,130,ablation-analysis,negative loss,useful to boost,factual consistency,negative loss useful to boost factual consistency,0.7553324103355408
translation,42,128,experimental-setup,6 examples,in,mini-batch,6 examples in mini-batch,0.47054827213287354
translation,42,128,experimental-setup,s + and s ? per gpu,total of,40,s + and s ? per gpu total of 40,0.6240196228027344
translation,42,128,experimental-setup,40,has,gpus,40 has gpus,0.5314270853996277
translation,42,128,experimental-setup,experimental setup,sample about,6 examples,experimental setup sample about 6 examples,0.625278890132904
translation,42,129,experiments,lower factual consistency scores,compared to,q-c.,lower factual consistency scores compared to q-c.,0.6580851078033447
translation,42,129,experiments,q-p ( quals - positive ),use,positive summaries,q-p ( quals - positive ) use positive summaries,0.6884644031524658
translation,42,129,experiments,positive summaries,has,+ ),positive summaries has + ),0.5991726517677307
translation,42,90,hyperparameters,conseq,with,mle baseline models,conseq with mle baseline models,0.6370106935501099
translation,42,90,hyperparameters,hyperparameters,initialize,conseq,hyperparameters initialize conseq,0.7720233798027039
translation,42,91,hyperparameters,conseq,use,learning rate,conseq use learning rate,0.6399323344230652
translation,42,91,hyperparameters,learning rate,of,3 ? 10 ?6,learning rate of 3 ? 10 ?6,0.6361813545227051
translation,42,91,hyperparameters,hyperparameters,In,conseq,hyperparameters In conseq,0.5112422704696655
translation,42,92,hyperparameters,summaries,using,beam search,summaries using beam search,0.7211344838142395
translation,42,92,hyperparameters,beam search,with,beam sizes,beam search with beam sizes,0.6189088821411133
translation,42,92,hyperparameters,beam sizes,of,4 and 6,beam sizes of 4 and 6,0.6642171740531921
translation,42,92,hyperparameters,4 and 6,for,cnndm and xsum,4 and 6 for cnndm and xsum,0.639312744140625
translation,42,7,model,efficient automatic evaluation metric,to measure,factual consistency,efficient automatic evaluation metric to measure factual consistency,0.6780735850334167
translation,42,7,model,novel learning algorithm,maximizes,proposed metric,novel learning algorithm maximizes proposed metric,0.719947099685669
translation,42,7,model,proposed metric,during,model training,proposed metric during model training,0.6969427466392517
translation,42,7,model,model,propose,efficient automatic evaluation metric,model propose efficient automatic evaluation metric,0.6482669115066528
translation,42,7,model,model,propose,novel learning algorithm,model propose novel learning algorithm,0.710736870765686
translation,42,27,model,efficient automatic evaluation metric,for,factual consistency,efficient automatic evaluation metric for factual consistency,0.5712239742279053
translation,42,27,model,factual consistency,is,simplification,factual consistency is simplification,0.5568844079971313
translation,42,27,model,simplification,of,published qags protocol,simplification of published qags protocol,0.6092639565467834
translation,42,27,model,model,propose,efficient automatic evaluation metric,model propose efficient automatic evaluation metric,0.6482669115066528
translation,42,28,results,results,has,evaluating qags,results has evaluating qags,0.5848224759101868
translation,42,107,results,proposed method quals - conseq ( q-c ),achieves,more than 4 points improvement,proposed method quals - conseq ( q-c ) achieves more than 4 points improvement,0.6723880171775818
translation,42,107,results,proposed method quals - conseq ( q-c ),about,2 points improvement,proposed method quals - conseq ( q-c ) about 2 points improvement,0.5821872353553772
translation,42,107,results,more than 4 points improvement,in,qags,more than 4 points improvement in qags,0.5374048948287964
translation,42,107,results,mle baseline,in,xsum,mle baseline in xsum,0.5784622430801392
translation,42,107,results,2 points improvement,in,cnndm,2 points improvement in cnndm,0.5375304222106934
translation,42,107,results,results,observe,proposed method quals - conseq ( q-c ),results observe proposed method quals - conseq ( q-c ),0.6234427690505981
translation,42,111,results,poorer qags,than,mle baseline,poorer qags than mle baseline,0.612966001033783
translation,42,111,results,results,results in,poorer qags,results results in poorer qags,0.757692813873291
translation,42,121,results,q-f1 - c,achieves,slightly higher qags,q-f1 - c achieves slightly higher qags,0.7170444130897522
translation,42,121,results,slightly higher qags,than,q-c,slightly higher qags than q-c,0.6372247338294983
translation,42,121,results,results,observe,q-f1 - c,results observe q-f1 - c,0.6161924004554749
translation,42,122,results,q-f1 - c,performs,worse,q-f1 - c performs worse,0.7074483633041382
translation,42,122,results,worse,than,q-c,worse than q-c,0.7081241011619568
translation,42,122,results,results,overall,q-f1 - c,results overall q-f1 - c,0.7144442796707153
translation,42,122,results,results,has,q-f1 - c,results has q-f1 - c,0.5814099907875061
translation,42,127,results,factual consistency,over,mle baseline,factual consistency over mle baseline,0.6669036149978638
translation,42,127,results,not as much,as,q-c.,not as much as q-c.,0.6380435228347778
translation,42,127,results,q-c.,In,q-c-o,q-c. In q-c-o,0.6373889446258545
translation,42,127,results,q-c-o,use,online version of conseq,q-c-o use online version of conseq,0.6329464912414551
translation,42,127,results,q-c-o,has,- online ),q-c-o has - online ),0.690686821937561
translation,42,127,results,results,improves,factual consistency,results improves factual consistency,0.5992673635482788
translation,42,132,results,q-c,achieves,over 4 points improvements,q-c achieves over 4 points improvements,0.7081649303436279
translation,42,132,results,over 4 points improvements,in,factcc score,over 4 points improvements in factcc score,0.5329098701477051
translation,42,132,results,factcc score,over,mle baseline,factcc score over mle baseline,0.6463248133659363
translation,42,132,results,cnndm,has,q-c,cnndm has q-c,0.6435567736625671
translation,42,132,results,results,for,cnndm,results for cnndm,0.6253750324249268
translation,42,149,results,results,represent,significant improvements,results represent significant improvements,0.5828847885131836
translation,42,163,results,quals - conseq,improves,factual consistency,quals - conseq improves factual consistency,0.7322161793708801
translation,42,163,results,factual consistency,of,generated summaries,factual consistency of generated summaries,0.5803831815719604
translation,42,163,results,factual consistency,compared to,bart - large mle baseline,factual consistency compared to bart - large mle baseline,0.6444836258888245
translation,43,46,baselines,unsupervised representation learning technique,for,content extraction,unsupervised representation learning technique for content extraction,0.5547946095466614
translation,43,46,baselines,vq - vae,has,unsupervised representation learning technique,vq - vae has unsupervised representation learning technique,0.5630156397819519
translation,43,46,baselines,baselines,consider,vq - vae,baselines consider vq - vae,0.6618979573249817
translation,43,153,baselines,sum-basic,assigns,higher scores,sum-basic assigns higher scores,0.6502693891525269
translation,43,153,baselines,higher scores,to,sentences,higher scores to sentences,0.5899418592453003
translation,43,153,baselines,sentences,containing,frequently occurring content words,sentences containing frequently occurring content words,0.6364566087722778
translation,43,153,baselines,baselines,has,sum-basic,baselines has sum-basic,0.5826700329780579
translation,43,154,baselines,graph - based summarization,for,speech transcripts,graph - based summarization for speech transcripts,0.5793419480323792
translation,43,154,baselines,flucovrank,groups,utterances,flucovrank groups utterances,0.6688534021377563
translation,43,154,baselines,flucovrank,generates,abstractive sentence,flucovrank generates abstractive sentence,0.605763852596283
translation,43,154,baselines,flucovrank,selects,best elements,flucovrank selects best elements,0.7281565070152283
translation,43,154,baselines,utterances,into,clusters,utterances into clusters,0.6051632761955261
translation,43,154,baselines,abstractive sentence,from,each cluster,abstractive sentence from each cluster,0.5733398795127869
translation,43,154,baselines,best elements,from,abstractive sentences,best elements from abstractive sentences,0.5183138251304626
translation,43,154,baselines,abstractive sentences,under,budget constraint,abstractive sentences under budget constraint,0.5959198474884033
translation,43,155,baselines,"quantized transformer ( angelidis et al. , 2021 )",uses,clustering interpretation,"quantized transformer ( angelidis et al. , 2021 ) uses clustering interpretation",0.6130298376083374
translation,43,155,baselines,clustering interpretation,of,quantized space,clustering interpretation of quantized space,0.5945302248001099
translation,43,155,baselines,clustering interpretation,of,two -step sampling algorithm,clustering interpretation of two -step sampling algorithm,0.5937992930412292
translation,43,155,baselines,two -step sampling algorithm,to extract,summary sentences,two -step sampling algorithm to extract summary sentences,0.7483543753623962
translation,43,155,baselines,summary sentences,from,reviews,summary sentences from reviews,0.598875880241394
translation,43,158,experimental-setup,hidden size ( h ),of,768,hidden size ( h ) of 768,0.6286555528640747
translation,43,160,experimental-setup,hidden size,of,768,hidden size of 768,0.6723939776420593
translation,43,161,experimental-setup,convolutional encoder and decoder,use,kernel size,convolutional encoder and decoder use kernel size,0.6535869240760803
translation,43,161,experimental-setup,kernel size,of,3,kernel size of 3,0.6748871803283691
translation,43,161,experimental-setup,experimental setup,has,convolutional encoder and decoder,experimental setup has convolutional encoder and decoder,0.5415355563163757
translation,43,167,experimental-setup,models,trained for,30 epochs,models trained for 30 epochs,0.7691686749458313
translation,43,167,experimental-setup,30 epochs,on,dual nvidia v100 gpus,30 epochs on dual nvidia v100 gpus,0.5068092942237854
translation,43,167,experimental-setup,dual nvidia v100 gpus,with,gradient accumulation,dual nvidia v100 gpus with gradient accumulation,0.6131630539894104
translation,43,167,experimental-setup,gradient accumulation,every,ten steps,gradient accumulation every ten steps,0.6360247731208801
translation,43,167,experimental-setup,experimental setup,trained for,30 epochs,experimental setup trained for 30 epochs,0.7293474674224854
translation,43,167,experimental-setup,experimental setup,has,models,experimental setup has models,0.5060054659843445
translation,43,169,experimental-setup,number of latent codes,varied in,"k = { 512 , 1024 , 2048 }","number of latent codes varied in k = { 512 , 1024 , 2048 }",0.6450190544128418
translation,43,169,experimental-setup,experimental setup,has,number of latent codes,experimental setup has number of latent codes,0.5150807499885559
translation,43,170,experimental-setup,coefficient,used for,commitment loss,coefficient used for commitment loss,0.6450314521789551
translation,43,170,experimental-setup,commitment loss,set to,0.25,commitment loss set to 0.25,0.6785410642623901
translation,43,170,experimental-setup,experimental setup,has,coefficient,experimental setup has coefficient,0.5140364766120911
translation,43,57,experiments,leading social media platform ( behance.net ),supported by,adobe creative cloud,leading social media platform ( behance.net ) supported by adobe creative cloud,0.5044695138931274
translation,43,57,experiments,adobe creative cloud,features,livestreams of creative work,adobe creative cloud features livestreams of creative work,0.6016520261764526
translation,43,57,experiments,livestreams of creative work,by,artists and designers,livestreams of creative work by artists and designers,0.5430097579956055
translation,43,95,experiments,unsupervised summarizer,leverages,vector-quantized variational autoencoders ( vq - vae,unsupervised summarizer leverages vector-quantized variational autoencoders ( vq - vae,0.6654687523841858
translation,43,95,experiments,unsupervised summarizer,identifies,summary utterances,unsupervised summarizer identifies summary utterances,0.5641627907752991
translation,43,168,experiments,"different numbers of filters , d = { 64 , 100 , 128 }",for,convolutional encoder and decoder,"different numbers of filters , d = { 64 , 100 , 128 } for convolutional encoder and decoder",0.5848770141601562
translation,43,7,model,model,present,streamhover,model present streamhover,0.717023491859436
translation,43,9,model,neural extractive summarization,leverages,vector-quantized variational autoencoder,neural extractive summarization leverages vector-quantized variational autoencoder,0.6251072287559509
translation,43,9,model,vector-quantized variational autoencoder,to learn,latent vector representations,vector-quantized variational autoencoder to learn latent vector representations,0.534308135509491
translation,43,9,model,latent vector representations,of,spoken utterances,latent vector representations of spoken utterances,0.572011411190033
translation,43,9,model,model,explore,neural extractive summarization,model explore neural extractive summarization,0.6466289162635803
translation,43,24,model,vector-quantized variational autoencoders,to identify,salient utterances,vector-quantized variational autoencoders to identify salient utterances,0.6094889640808105
translation,43,24,model,vector-quantized variational autoencoders,has,vq - vae,vector-quantized variational autoencoders has vq - vae,0.5679530501365662
translation,43,24,model,model,make use of,vector-quantized variational autoencoders,model make use of vector-quantized variational autoencoders,0.5853155255317688
translation,43,159,model,6layer transformer decoder,used as,generator generate ? ( ? ),6layer transformer decoder used as generator generate ? ( ? ),0.6607873439788818
translation,43,159,model,generator generate ? ( ? ),to reconstruct,original utterance,generator generate ? ( ? ) to reconstruct original utterance,0.6674293279647827
translation,43,159,model,model,has,6layer transformer decoder,model has 6layer transformer decoder,0.5971230268478394
translation,43,182,results,consistently outperforms,across,all lengths,consistently outperforms across all lengths,0.6943154335021973
translation,43,182,results,other summarization systems,across,all lengths,other summarization systems across all lengths,0.7117336392402649
translation,43,182,results,streamhover,has,consistently outperforms,streamhover has consistently outperforms,0.6238987445831299
translation,43,182,results,consistently outperforms,has,other summarization systems,consistently outperforms has other summarization systems,0.5831129550933838
translation,43,182,results,results,find that,streamhover,results find that streamhover,0.6848536133766174
translation,43,191,results,streamhover,yields,highest scores,streamhover yields highest scores,0.7073238492012024
translation,43,191,results,highest scores,on,r - 2 and r-l metrics,highest scores on r - 2 and r-l metrics,0.5313151478767395
translation,43,191,results,results,observe,streamhover,results observe streamhover,0.6144630312919617
translation,44,5,model,modulated,to decide,what types of information,modulated to decide what types of information,0.7279693484306335
translation,44,5,model,modulated,to decide,perspective,modulated to decide perspective,0.7223649024963379
translation,44,5,model,summaries,to tackle,under-constrained problem,summaries to tackle under-constrained problem,0.6870839595794678
translation,44,5,model,under-constrained problem,in,summarization tasks,under-constrained problem in summarization tasks,0.4867398142814636
translation,44,5,model,model,has,conditional sequences,model has conditional sequences,0.6057878136634827
translation,44,7,model,training,exploit,occurrence planning,training exploit occurrence planning,0.7363252639770508
translation,44,7,model,occurrence planning,of,personal named entities and coreference information,occurrence planning of personal named entities and coreference information,0.5229179263114929
translation,44,7,model,personal named entities and coreference information,to improve,temporal coherence,personal named entities and coreference information to improve temporal coherence,0.6219443082809448
translation,44,7,model,hallucination,in,neural generation,hallucination in neural generation,0.5215574502944946
translation,44,7,model,model,During,training,model During training,0.714866042137146
translation,44,33,model,model,introduce,controllable dialogue summarization framework,model introduce controllable dialogue summarization framework,0.5906299352645874
translation,44,38,model,coreference resolution information,into,contextual representation,coreference resolution information into contextual representation,0.4833811819553375
translation,44,38,model,contextual representation,by,graph - based neural component,contextual representation by graph - based neural component,0.5190343856811523
translation,44,38,model,graph - based neural component,to further reduce,incorrect reasoning,graph - based neural component to further reduce incorrect reasoning,0.6449204683303833
translation,45,6,baselines,dataset,consisting of,pairs of papers,dataset consisting of pairs of papers,0.6493380069732666
translation,45,6,baselines,dataset,consisting of,corresponding slides decks,dataset consisting of corresponding slides decks,0.6980540752410889
translation,45,6,baselines,corresponding slides decks,from,"recent years ' nlp and ml conferences ( e.g. , acl )","corresponding slides decks from recent years ' nlp and ml conferences ( e.g. , acl )",0.5710015892982483
translation,45,172,baselines,model,fine-tuned to,retrieved context,model fine-tuned to retrieved context,0.7567650675773621
translation,45,172,baselines,retrieved context,on,our unfiltered training dataset,retrieved context on our unfiltered training dataset,0.5395442843437195
translation,45,172,baselines,"liu and lapata , 2019 )",has,model,"liu and lapata , 2019 ) has model",0.5216994881629944
translation,45,136,experimental-setup,training,done on,two 16gb p100 gpus,training done on two 16gb p100 gpus,0.6503863334655762
translation,45,136,experimental-setup,two 16gb p100 gpus,in parallel,pytorch,two 16gb p100 gpus in parallel pytorch,0.7272324562072754
translation,45,136,experimental-setup,experimental setup,has,training,experimental setup has training,0.5312813520431519
translation,45,137,experimental-setup,transformer models,from,"hug-gingface ( wolf et al. , 2020 )","transformer models from hug-gingface ( wolf et al. , 2020 )",0.5847722291946411
translation,45,139,experimental-setup,distilled uncased bert miniature,with,8 - layers,distilled uncased bert miniature with 8 - layers,0.718601644039154
translation,45,139,experimental-setup,distilled uncased bert miniature,with,768 hidden units,distilled uncased bert miniature with 768 hidden units,0.6605145335197449
translation,45,139,experimental-setup,experimental setup,has,distilled uncased bert miniature,experimental setup has distilled uncased bert miniature,0.5673812031745911
translation,45,140,experimental-setup,bert model,computes,all sentence embeddings,bert model computes all sentence embeddings,0.6863372921943665
translation,45,140,experimental-setup,all sentence embeddings,in,128 - dimensional vectors,all sentence embeddings in 128 - dimensional vectors,0.48831698298454285
translation,45,140,experimental-setup,experimental setup,has,bert model,experimental setup has bert model,0.5457624197006226
translation,45,181,experimental-setup,batch size,of,4,batch size of 4,0.6922571659088135
translation,45,181,experimental-setup,batch size,with,initial learning rate,batch size with initial learning rate,0.5960331559181213
translation,45,181,experimental-setup,4,with,initial learning rate,4 with initial learning rate,0.6169115304946899
translation,45,181,experimental-setup,initial learning rate,of,5e - 5,initial learning rate of 5e - 5,0.6373112797737122
translation,45,182,experimental-setup,maximum input token length,at,1024,maximum input token length at 1024,0.5157186985015869
translation,45,182,experimental-setup,experimental setup,set,maximum input token length,experimental setup set maximum input token length,0.6244441270828247
translation,45,186,experimental-setup,qa model,on,filtered dataset,qa model on filtered dataset,0.5919512510299683
translation,45,186,experimental-setup,qa model,with,batch size,qa model with batch size,0.6217734813690186
translation,45,186,experimental-setup,qa model,with,initial learning rate,qa model with initial learning rate,0.5967720746994019
translation,45,186,experimental-setup,batch size,of,4,batch size of 4,0.6922571659088135
translation,45,186,experimental-setup,initial learning rate,of,5e - 5,initial learning rate of 5e - 5,0.6373112797737122
translation,45,186,experimental-setup,experimental setup,fine- tune,qa model,experimental setup fine- tune qa model,0.7003116011619568
translation,45,187,experimental-setup,maximum input token length,set to,1024,maximum input token length set to 1024,0.6431220769882202
translation,45,187,experimental-setup,experimental setup,has,maximum input token length,experimental setup has maximum input token length,0.5133613348007202
translation,45,59,experiments,user-centered slide titles,as,questions,user-centered slide titles as questions,0.5403156280517578
translation,45,59,experiments,user-centered slide titles,as,paper document,user-centered slide titles as paper document,0.5614033937454224
translation,45,59,experiments,paper document,as,corpus,paper document as corpus,0.5705150961875916
translation,45,60,experiments,information retrieval ( ir ),to collect,most relevant text snippets,information retrieval ( ir ) to collect most relevant text snippets,0.6180820465087891
translation,45,60,experiments,most relevant text snippets,from,paper,most relevant text snippets from paper,0.525769829750061
translation,45,60,experiments,paper,for,given title,paper for given title,0.5996337532997131
translation,45,60,experiments,qa module,for,sequence - tosequence generation,qa module for sequence - tosequence generation,0.6400157809257507
translation,45,141,experiments,qa model,fine-tuned over,bart - large - cnn,qa model fine-tuned over bart - large - cnn,0.7475922703742981
translation,45,7,model,document- to-slides task,with,two-step approach,document- to-slides task with two-step approach,0.6651966571807861
translation,45,7,model,model,present,d2s,model present d2s,0.7235089540481567
translation,45,22,model,interactive two -step architecture,use,dense vector ir module,interactive two -step architecture use dense vector ir module,0.6261827945709229
translation,45,22,model,short text,as,slide title,short text as slide title,0.49651652574539185
translation,45,22,model,dense vector ir module,to identify,most relevant sections / sentences,dense vector ir module to identify most relevant sections / sentences,0.642439603805542
translation,45,22,model,most relevant sections / sentences,as,figures / tables,most relevant sections / sentences as figures / tables,0.5123081207275391
translation,45,22,model,figures / tables,from,corresponding paper,figures / tables from corresponding paper,0.5449484586715698
translation,45,22,model,model,propose,interactive two -step architecture,model propose interactive two -step architecture,0.6801963448524475
translation,45,23,model,qa model,to generate,abstractive summary ( answer ),qa model to generate abstractive summary ( answer ),0.6776518225669861
translation,45,23,model,abstractive summary ( answer ),of,retrieved text,abstractive summary ( answer ) of retrieved text,0.5707404613494873
translation,45,23,model,abstractive summary ( answer ),as,final slide text content,abstractive summary ( answer ) as final slide text content,0.5160200595855713
translation,45,23,model,retrieved text,based on,given slide title,retrieved text based on given slide title,0.6654800176620483
translation,45,61,model,qa module,by integrating,title-specific key phrases,qa module by integrating title-specific key phrases,0.5751617550849915
translation,45,61,model,title-specific key phrases,to guide,model,title-specific key phrases to guide model,0.7218666672706604
translation,45,61,model,model,to generate,slide content,model to generate slide content,0.7085950374603271
translation,45,61,model,model,improve,qa module,model improve qa module,0.7430707812309265
translation,45,142,results,pilot experiments,showed,bart - large - cnn,pilot experiments showed bart - large - cnn,0.6561447978019714
translation,45,142,results,other state - of - the - art pre-trained language generation models,on,dev dataset,other state - of - the - art pre-trained language generation models on dev dataset,0.46675029397010803
translation,45,142,results,bart - large - cnn,has,outperforms,bart - large - cnn has outperforms,0.5893564224243164
translation,45,142,results,outperforms,has,bart - large,outperforms has bart - large,0.6353045105934143
translation,45,142,results,outperforms,has,other state - of - the - art pre-trained language generation models,outperforms has other state - of - the - art pre-trained language generation models,0.5107671618461609
translation,45,158,results,idf - recall scores,for,ir method,idf - recall scores for ir method,0.6114664077758789
translation,45,158,results,classical ir ( bm25 ),=,0.5112,classical ir ( bm25 ) = 0.5112,0.6671442985534668
translation,45,158,results,dense-text ir,=,0.5476,dense-text ir = 0.5476,0.6606038808822632
translation,45,158,results,results,has,idf - recall scores,results has idf - recall scores,0.5470810532569885
translation,45,159,results,other dense ir models,that rank exclusively by,text or keywords,other dense ir models that rank exclusively by text or keywords,0.6180329322814941
translation,45,159,results,dense ir model,has,outperforming,dense ir model has outperforming,0.5426252484321594
translation,45,159,results,outperforming,has,classical ir approach,outperforming has classical ir approach,0.5757837891578674
translation,45,159,results,? = 0.75 - weighted mix dense ir model,has,outperforming,? = 0.75 - weighted mix dense ir model has outperforming,0.5803674459457397
translation,45,159,results,outperforming,has,other dense ir models,outperforming has other dense ir models,0.5363802313804626
translation,45,162,results,header - awareness,leads to,better retrieval,header - awareness leads to better retrieval,0.681740403175354
translation,45,162,results,better retrieval,in cases where,title,better retrieval in cases where title,0.6159847378730774
translation,45,162,results,title,corresponds well with,section headers,title corresponds well with section headers,0.6226795315742493
translation,45,162,results,results,has,header - awareness,results has header - awareness,0.5047323107719421
translation,45,168,results,"0.38 , 0.60 , and 0.77",on,"p@1 , p@3 , and p@5","0.38 , 0.60 , and 0.77 on p@1 , p@3 , and p@5",0.5701879858970642
translation,45,192,results,dense-mix ir approach,provides,better context,dense-mix ir approach provides better context,0.6546159386634827
translation,45,192,results,better context,for,downstream summarization models,better context for downstream summarization models,0.6175556778907776
translation,45,192,results,results,has,dense-mix ir approach,results has dense-mix ir approach,0.5893083810806274
translation,45,193,results,bartkeyword model,superior to,abstractive and extractive summarization models,bartkeyword model superior to abstractive and extractive summarization models,0.6913493275642395
translation,45,193,results,abstractive and extractive summarization models,in,all rouge metrics ( 1/2/l ),abstractive and extractive summarization models in all rouge metrics ( 1/2/l ),0.505287766456604
translation,45,193,results,results,has,bartkeyword model,results has bartkeyword model,0.5326678156852722
translation,45,194,results,performs better,than,extractive model,performs better than extractive model,0.586379885673523
translation,45,194,results,abstractive summarization model,has,performs better,abstractive summarization model has performs better,0.5891218781471252
translation,45,194,results,results,has,abstractive summarization model,results has abstractive summarization model,0.5569102168083191
translation,45,202,results,rouge f-score,for,non-author generated slides,rouge f-score for non-author generated slides,0.6281710863113403
translation,45,202,results,non-author generated slides,compared to,our d2s system,non-author generated slides compared to our d2s system,0.6632714867591858
translation,45,202,results,results,results of,rouge f-score,results results of rouge f-score,0.6930031180381775
translation,45,203,results,our model 's performance,is,similar to or sometimes better,our model 's performance is similar to or sometimes better,0.5966543555259705
translation,45,203,results,similar to or sometimes better,than,non-author generated ones,similar to or sometimes better than non-author generated ones,0.6288356781005859
translation,45,230,results,bertsummext model,performs,significantly worse,bertsummext model performs significantly worse,0.6274320483207703
translation,45,230,results,significantly worse,than,other three models,significantly worse than other three models,0.578885555267334
translation,45,230,results,readability dimension,has,bertsummext model,readability dimension has bertsummext model,0.5714002847671509
translation,45,230,results,results,show,readability dimension,results show readability dimension,0.5503810048103333
translation,45,230,results,results,for,readability dimension,results for readability dimension,0.5445767641067505
translation,45,232,results,most informative slides,generated by,humans,most informative slides generated by humans,0.7131471037864685
translation,45,232,results,results,has,most informative slides,results has most informative slides,0.5368826389312744
translation,45,233,results,bartkeyword,came in,second,bartkeyword came in second,0.7376769185066223
translation,45,233,results,outperformed,has,bertsummext,outperformed has bertsummext,0.6131200790405273
translation,45,233,results,bertsummext,has,significantly,bertsummext has significantly,0.6588658690452576
translation,45,233,results,bartsumm,has,insignificantly,bartsumm has insignificantly,0.6247378587722778
translation,45,233,results,results,has,bartkeyword,results has bartkeyword,0.521877110004425
translation,45,235,results,consistency,between,generated slide content and the author 's original slide,consistency between generated slide content and the author 's original slide,0.6448484659194946
translation,45,235,results,significant difference,in,ratings,significant difference in ratings,0.5465002059936523
translation,45,235,results,ratings,across,methods,ratings across methods,0.650117814540863
translation,45,235,results,results,Regarding,consistency,results Regarding consistency,0.6116265058517456
translation,45,236,results,human-generated slides,has,outperformed,human-generated slides has outperformed,0.616344153881073
translation,45,236,results,outperformed,has,ml models,outperformed has ml models,0.6040109395980835
translation,45,236,results,bartkeyword,has,significantly outperformed,bartkeyword has significantly outperformed,0.6124675273895264
translation,45,236,results,significantly outperformed,has,other two,significantly outperformed has other two,0.6256901025772095
translation,45,236,results,results,has,human-generated slides,results has human-generated slides,0.5137479305267334
translation,45,242,results,training bartkeyword,on,filtered training dataset,training bartkeyword on filtered training dataset,0.5239327549934387
translation,45,242,results,training bartkeyword,helps improve,performance,training bartkeyword helps improve performance,0.6813740730285645
translation,45,242,results,performance,in,unfiltered test set,performance in unfiltered test set,0.5574086904525757
translation,45,242,results,results,shows,training bartkeyword,results shows training bartkeyword,0.6305315494537354
translation,45,256,results,testing dataset,found that,original slide contents,testing dataset found that original slide contents,0.5414459109306335
translation,45,256,results,original slide contents,much higher proportion of,novel n-grams,original slide contents much higher proportion of novel n-grams,0.5399859547615051
translation,45,256,results,novel n-grams,compared to,automatically generated ones,novel n-grams compared to automatically generated ones,0.5588085055351257
translation,45,256,results,results,On,testing dataset,results On testing dataset,0.5651152729988098
translation,46,212,ablation-analysis,sample modifications,using,more global context,sample modifications using more global context,0.6760132908821106
translation,46,212,ablation-analysis,more global context,helps with,cl training,more global context helps with cl training,0.7130175232887268
translation,46,154,experimental-setup,beam sizes,of,6 and 4,beam sizes of 6 and 4,0.6696262359619141
translation,46,154,experimental-setup,6 and 4,for,xsum and cnn / dm,6 and 4 for xsum and cnn / dm,0.6578192710876465
translation,46,5,model,novel contrastive learning formulation,leverages,reference summaries,novel contrastive learning formulation leverages reference summaries,0.7291358113288879
translation,46,5,model,novel contrastive learning formulation,leverages,automatically generated erroneous summaries,novel contrastive learning formulation leverages automatically generated erroneous summaries,0.6659009456634521
translation,46,5,model,reference summaries,as,positive training data,reference summaries as positive training data,0.5191119909286499
translation,46,5,model,automatically generated erroneous summaries,as,negative training data,automatically generated erroneous summaries as negative training data,0.5109612345695496
translation,46,5,model,automatically generated erroneous summaries,to train,summarization,automatically generated erroneous summaries to train summarization,0.6518595218658447
translation,46,5,model,model,has,novel contrastive learning formulation,model has novel contrastive learning formulation,0.5309022665023804
translation,46,31,model,framework,uses,contrastive learning,framework uses contrastive learning,0.5889156460762024
translation,46,31,model,contrastive learning,for improving,faithfulness and factuality,contrastive learning for improving faithfulness and factuality,0.6632176637649536
translation,46,31,model,faithfulness and factuality,of,generated summaries,faithfulness and factuality of generated summaries,0.5878352522850037
translation,46,34,model,task-specific cl formulation,teaches,summarizer,task-specific cl formulation teaches summarizer,0.7072335481643677
translation,46,34,model,summarizer,to expand,margin,summarizer to expand margin,0.7054092288017273
translation,46,34,model,margin,between,factually consistent summaries and their incorrect peers,margin between factually consistent summaries and their incorrect peers,0.6750925183296204
translation,46,34,model,model,design,task-specific cl formulation,model design task-specific cl formulation,0.6243665218353271
translation,46,35,model,four types of strategies,with,different variants,four types of strategies with different variants,0.6359216570854187
translation,46,35,model,different variants,to construct,negative samples,different variants to construct negative samples,0.643622100353241
translation,46,35,model,negative samples,by editing,reference summaries,negative samples by editing reference summaries,0.7585039734840393
translation,46,35,model,reference summaries,via rewriting,entity -/ relation - anchored text,reference summaries via rewriting entity -/ relation - anchored text,0.7612887620925903
translation,46,35,model,system generated summaries,may contain,unfaithful errors,system generated summaries may contain unfaithful errors,0.7134692072868347
translation,46,35,model,model,design,four types of strategies,model design four types of strategies,0.6251670718193054
translation,46,33,results,cl,improves,representation learning,cl improves representation learning,0.6901733875274658
translation,46,33,results,representation learning,by compacting,positive samples,representation learning by compacting positive samples,0.6753203272819519
translation,46,33,results,contrasting,with,negative samples,contrasting with negative samples,0.6761733889579773
translation,46,33,results,results,has,cl,results has cl,0.5053845047950745
translation,46,39,results,unlikelihood training method,that penalizes,same negative samples,unlikelihood training method that penalizes same negative samples,0.693554162979126
translation,46,39,results,our summaries,obtain,consistently better questeval scores,our summaries obtain consistently better questeval scores,0.5884214043617249
translation,46,39,results,unlikelihood training method,has,our summaries,unlikelihood training method has our summaries,0.5863896608352661
translation,46,39,results,results,compared with,unlikelihood training method,results compared with unlikelihood training method,0.6548851132392883
translation,46,179,results,almost all cliff models,trained with,different negative samples,almost all cliff models trained with different negative samples,0.7618891596794128
translation,46,179,results,different negative samples,produce,higher questeval scores,different negative samples produce higher questeval scores,0.6422164440155029
translation,46,179,results,higher questeval scores,across,datasets,higher questeval scores across datasets,0.7073067426681519
translation,46,179,results,higher questeval scores,with,both large models,higher questeval scores with both large models,0.6529231071472168
translation,46,180,results,rouge scores,for,cliff models,rouge scores for cliff models,0.6488085985183716
translation,46,180,results,cliff models,are,comparable or better,cliff models are comparable or better,0.6106157302856445
translation,46,180,results,comparable or better,than,baselines,comparable or better than baselines,0.5953620076179504
translation,46,180,results,results,has,rouge scores,results has rouge scores,0.541054368019104
translation,46,182,results,entailrank,tends to yield,significantly higher factcc scores,entailrank tends to yield significantly higher factcc scores,0.7430927157402039
translation,46,183,results,pegasus,on,xsum and cnn / dm,pegasus on xsum and cnn / dm,0.6068230867385864
translation,46,189,results,outperforms,in,11 setups,outperforms in 11 setups,0.5395498871803284
translation,46,189,results,pegasus,has,cliff,pegasus has cliff,0.6061657071113586
translation,46,189,results,cliff,has,outperforms,cliff has outperforms,0.6716837286949158
translation,46,189,results,results,Using,pegasus,results Using pegasus,0.6319506764411926
translation,46,192,results,cliff,trained with,low confidence summaries,cliff trained with low confidence summaries,0.7155850529670715
translation,46,192,results,low confidence summaries,as,negative samples,low confidence summaries as negative samples,0.5631186366081238
translation,46,192,results,low confidence summaries,obtains,best questeval scores,low confidence summaries obtains best questeval scores,0.6251242756843567
translation,46,192,results,best questeval scores,on,more abstractive dataset,best questeval scores on more abstractive dataset,0.5387179851531982
translation,46,192,results,all variants,has,cliff,all variants has cliff,0.6219403147697449
translation,46,192,results,results,among,all variants,results among all variants,0.5945279598236084
translation,46,193,results,low confidence summaries,improves,factcc scores,low confidence summaries improves factcc scores,0.734302282333374
translation,46,193,results,low confidence summaries,improves,factual consistency,low confidence summaries improves factual consistency,0.694066047668457
translation,46,193,results,low confidence summaries,enhances,rouge - l,low confidence summaries enhances rouge - l,0.6985254287719727
translation,46,193,results,low confidence summaries,enhances,factual consistency,low confidence summaries enhances factual consistency,0.6660245656967163
translation,46,193,results,factcc scores,on,both datasets,factcc scores on both datasets,0.5377759337425232
translation,46,193,results,factual consistency,has,factual,factual consistency has factual,0.579468309879303
translation,46,193,results,results,using,low confidence summaries,results using low confidence summaries,0.6551631689071655
translation,46,194,results,krippendorff 's ?s,are,0.33 and 0.62,krippendorff 's ?s are 0.33 and 0.62,0.5650097727775574
translation,46,194,results,krippendorff 's ?s,are,0.34 and 0.89,krippendorff 's ?s are 0.34 and 0.89,0.5591743588447571
translation,46,194,results,0.33 and 0.62,for,two aspects,0.33 and 0.62 for two aspects,0.6231622099876404
translation,46,194,results,two aspects,on,xsum,two aspects on xsum,0.6185621023178101
translation,46,194,results,0.34 and 0.89,on,cnn / dm,0.34 and 0.89 on cnn / dm,0.6050660014152527
translation,46,194,results,results,has,krippendorff 's ?s,results has krippendorff 's ?s,0.5850176811218262
translation,46,195,results,cl method,using,low confidence summaries,cl method using low confidence summaries,0.6651524305343628
translation,46,195,results,low confidence summaries,more frequently rated as,better,low confidence summaries more frequently rated as better,0.6533524990081787
translation,46,195,results,better,for,informativeness and factuality,better for informativeness and factuality,0.5813436508178711
translation,46,195,results,informativeness and factuality,on,more abstractive dataset xsum,informativeness and factuality on more abstractive dataset xsum,0.5282340049743652
translation,46,195,results,results,has,cl method,results has cl method,0.5684811472892761
translation,46,200,results,more abstractive xsum data cl,trained with,low confidence samples,more abstractive xsum data cl trained with low confidence samples,0.7377470135688782
translation,46,200,results,low confidence samples,more frequently rated as being,more informative and more factual,low confidence samples more frequently rated as being more informative and more factual,0.6134411692619324
translation,46,200,results,more informative and more factual,than,crsentropy summaries,more informative and more factual than crsentropy summaries,0.5552917718887329
translation,46,200,results,results,on,more abstractive xsum data cl,results on more abstractive xsum data cl,0.5979021787643433
translation,46,202,results,all models,trained with,negative samples,all models trained with negative samples,0.7543284296989441
translation,46,202,results,negative samples,produce,summaries,negative samples produce summaries,0.7051379084587097
translation,46,202,results,summaries,with,better informativeness and faithfulness,summaries with better informativeness and faithfulness,0.6158326268196106
translation,46,202,results,cnn / dm,has,all models,cnn / dm has all models,0.5707891583442688
translation,46,202,results,results,On,cnn / dm,results On cnn / dm,0.548747181892395
translation,46,203,results,entailrank summaries,less distinguishable from,outputs,entailrank summaries less distinguishable from outputs,0.648354172706604
translation,46,203,results,crsentropy,on,both datasets,crsentropy on both datasets,0.5177216529846191
translation,46,203,results,results,has,entailrank summaries,results has entailrank summaries,0.5678171515464783
translation,47,79,experiments,pegasus,as,base system,pegasus as base system,0.5988075137138367
translation,47,45,hyperparameters,pretrained bart,as,base generation model ( origin ),pretrained bart as base generation model ( origin ),0.5285186767578125
translation,47,45,hyperparameters,hyperparameters,use,pretrained bart,hyperparameters use pretrained bart,0.5845470428466797
translation,47,5,results,minor modification,over,existing topscoring systems,minor modification over existing topscoring systems,0.6438506841659546
translation,47,5,results,simcls,improve,performance,simcls improve performance,0.7249672412872314
translation,47,5,results,performance,of,existing top-performing models,performance of existing top-performing models,0.5565834045410156
translation,47,5,results,performance,by,large margin,performance by large margin,0.6227608919143677
translation,47,5,results,minor modification,has,simcls,minor modification has simcls,0.6188899278640747
translation,47,5,results,existing topscoring systems,has,simcls,existing topscoring systems has simcls,0.6221956014633179
translation,47,5,results,results,show,minor modification,results show minor modification,0.6954352855682373
translation,47,5,results,results,show,simcls,results show simcls,0.6384403109550476
translation,47,5,results,results,with,minor modification,results with minor modification,0.6545648574829102
translation,47,44,results,results,on,cnndm dataset,results on cnndm dataset,0.5553026795387268
translation,47,47,results,max oracle,selects,best candidate,max oracle selects best candidate,0.7530578374862671
translation,47,47,results,best candidate,has,much better performance,best candidate has much better performance,0.5858487486839294
translation,47,47,results,much better performance,than,original outputs,much better performance than original outputs,0.5871469974517822
translation,47,47,results,best candidate,has,much better performance,best candidate has much better performance,0.5858487486839294
translation,47,47,results,results,has,max oracle,results has max oracle,0.5926129817962646
translation,47,49,results,our method,able to outperform,baseline model,our method able to outperform baseline model,0.734438419342041
translation,47,49,results,baseline model,on,all metrics,baseline model on all metrics,0.4610290229320526
translation,47,49,results,results,has,our method,results has our method,0.5589964985847473
translation,47,69,results,generated summaries,of,our method,generated summaries of our method,0.6028510928153992
translation,47,69,results,generated summaries,is,more similar,generated summaries is more similar,0.5335772633552551
translation,47,69,results,more similar,to,reference summaries,more similar to reference summaries,0.5560073852539062
translation,47,69,results,reference summaries,at,sentence level,reference summaries at sentence level,0.5143480896949768
translation,47,69,results,results,demonstrate,generated summaries,results demonstrate generated summaries,0.6374753713607788
translation,47,78,results,method,on,xsum dataset,method on xsum dataset,0.544942319393158
translation,47,78,results,results,test,method,results test method,0.6896118521690369
translation,48,274,model,efficient alternative,run,sentence level presence probing,efficient alternative run sentence level presence probing,0.6811004281044006
translation,48,274,model,efficient alternative,run,attribution methods,efficient alternative run attribution methods,0.6515530347824097
translation,48,274,model,sentence level presence probing,on,full document,sentence level presence probing on full document,0.5662407875061035
translation,48,274,model,attribution methods,on,top-k sentences,attribution methods on top-k sentences,0.5059842467308044
translation,48,274,model,locally,on,top-k sentences,locally on top-k sentences,0.5689559578895569
translation,48,274,model,attribution methods,has,locally,attribution methods has locally,0.5926487445831299
translation,48,187,results,token- level and sentencelevel comparison,of,attribution methods,token- level and sentencelevel comparison of attribution methods,0.5468130707740784
translation,48,187,results,attribution methods,on,ctx examples,attribution methods on ctx examples,0.5085855722427368
translation,48,187,results,ctx examples,in,xsum,ctx examples in xsum,0.5721026659011841
translation,48,187,results,results,show,token- level and sentencelevel comparison,results show token- level and sentencelevel comparison,0.5557242035865784
translation,48,188,results,intgrad,is,best technique,intgrad is best technique,0.6344922184944153
translation,48,188,results,best technique,with,inpgrad,best technique with inpgrad,0.6879953742027283
translation,48,188,results,inpgrad,achieving,similar performance,inpgrad achieving similar performance,0.6613181233406067
translation,48,188,results,results,has,intgrad,results has intgrad,0.5679656863212585
translation,48,189,results,other techniques,when,more tokens,other techniques when more tokens,0.6824836134910583
translation,48,189,results,occlusion,has,underperforms,occlusion has underperforms,0.587735652923584
translation,48,189,results,underperforms,has,other techniques,underperforms has other techniques,0.5499491691589355
translation,48,189,results,more tokens,has,are removed,more tokens has are removed,0.6057285666465759
translation,48,189,results,results,has,occlusion,results has occlusion,0.4564204216003418
translation,49,198,ablation-analysis,both models,use,efficient attentions,both models use efficient attentions,0.6575488448143005
translation,49,198,ablation-analysis,efficient attentions,reduce,unfaithfulness,efficient attentions reduce unfaithfulness,0.7469833493232727
translation,49,198,ablation-analysis,unfaithfulness,especially,hallucination errors,unfaithfulness especially hallucination errors,0.686073899269104
translation,49,198,ablation-analysis,unfaithfulness,compared with,full attention model,unfaithfulness compared with full attention model,0.6852941513061523
translation,49,198,ablation-analysis,ablation analysis,has,both models,ablation analysis has both models,0.5447645783424377
translation,49,36,baselines,questions,generated from,references,questions generated from references,0.6295716762542725
translation,49,36,baselines,qa answers,by,system summary,qa answers by system summary,0.6000958681106567
translation,49,36,baselines,qa answers,reading,source,qa answers reading source,0.7130767703056335
translation,49,36,baselines,qa answers,reading,system summary,qa answers reading system summary,0.7093111872673035
translation,49,160,baselines,baselines,For,encoder attentions,baselines For encoder attentions,0.5893179774284363
translation,49,163,baselines,"global , stride , and ran - dom",with,window and adaspan,"global , stride , and ran - dom with window and adaspan",0.6225539445877075
translation,49,166,baselines,baselines,has,govreport ( new ) pubmed system,baselines has govreport ( new ) pubmed system,0.5831739902496338
translation,49,176,baselines,encoder variants,pick,best performing model,encoder variants pick best performing model,0.7133392691612244
translation,49,176,baselines,best performing model,from,fixed patterns,best performing model from fixed patterns,0.57131427526474
translation,49,176,baselines,full encoder- decoder attention,i.e.,sliding window,full encoder- decoder attention i.e. sliding window,0.63874751329422
translation,49,176,baselines,full encoder- decoder attention,i.e.,low-rank method ( lin . ),full encoder- decoder attention i.e. low-rank method ( lin . ),0.7249417304992676
translation,49,176,baselines,full encoder- decoder attention,i.e.,learnable patterns ( lsh and sinkhorm ),full encoder- decoder attention i.e. learnable patterns ( lsh and sinkhorm ),0.6902443170547485
translation,49,176,baselines,sliding window,has,with stride ( stride ),sliding window has with stride ( stride ),0.5987817645072937
translation,49,176,baselines,baselines,For,encoder variants,baselines For encoder variants,0.6057279706001282
translation,49,151,experimental-setup,our models,with,"py-torch ( paszke et al. , 2019 )","our models with py-torch ( paszke et al. , 2019 )",0.6285037994384766
translation,49,151,experimental-setup,our models,with,"fairseq ( ott et al. , 2019 )","our models with fairseq ( ott et al. , 2019 )",0.6154308915138245
translation,49,151,experimental-setup,experimental setup,implement,our models,experimental setup implement our models,0.6547874808311462
translation,49,152,experimental-setup,initialized randomly,for,models,initialized randomly for models,0.6359444260597229
translation,49,152,experimental-setup,models,that handle,longer inputs,models that handle longer inputs,0.7087780833244324
translation,49,152,experimental-setup,experimental setup,has,additional position embeddings,experimental setup has additional position embeddings,0.5274512767791748
translation,49,153,experimental-setup,learning rate,set to,1 ? 10 ?4,learning rate set to 1 ? 10 ?4,0.7306780219078064
translation,49,153,experimental-setup,learning rate warm - up,applied for,"first 10,000 steps","learning rate warm - up applied for first 10,000 steps",0.6694693565368652
translation,49,153,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,49,153,experimental-setup,experimental setup,has,learning rate warm - up,experimental setup has learning rate warm - up,0.5251268744468689
translation,49,154,experimental-setup,"adafactor ( shazeer and stern , 2018 ) optimizer",with,gradient clipping,"adafactor ( shazeer and stern , 2018 ) optimizer with gradient clipping",0.5973094701766968
translation,49,154,experimental-setup,gradient clipping,of,0.1,gradient clipping of 0.1,0.5792549252510071
translation,49,154,experimental-setup,experimental setup,has,"adafactor ( shazeer and stern , 2018 ) optimizer","experimental setup has adafactor ( shazeer and stern , 2018 ) optimizer",0.5144562721252441
translation,49,155,experimental-setup,two quadro rtx 6000 gpus,with,24gb memory,two quadro rtx 6000 gpus with 24gb memory,0.5864892601966858
translation,49,155,experimental-setup,one quadro rtx 8000,with,48 gb memory,one quadro rtx 8000 with 48 gb memory,0.5500982403755188
translation,49,155,experimental-setup,experimental setup,trained on,two quadro rtx 6000 gpus,experimental setup trained on two quadro rtx 6000 gpus,0.6794910430908203
translation,49,155,experimental-setup,experimental setup,trained on,one quadro rtx 8000,experimental setup trained on one quadro rtx 8000,0.6693475246429443
translation,49,156,experimental-setup,batch size,of,2 per step,batch size of 2 per step,0.6331350207328796
translation,49,156,experimental-setup,gradient,every,32 steps,gradient every 32 steps,0.6035048365592957
translation,49,156,experimental-setup,experimental setup,set,batch size,experimental setup set batch size,0.6767397522926331
translation,49,156,experimental-setup,experimental setup,accumulate,gradient,experimental setup accumulate gradient,0.5255685448646545
translation,49,157,experimental-setup,beam size,of,4,beam size of 4,0.6962505578994751
translation,49,157,experimental-setup,length penalty,of,2,length penalty of 2,0.6180238127708435
translation,49,157,experimental-setup,2,on,all datasets,2 on all datasets,0.4736682176589966
translation,49,162,experimental-setup,lsh attentions,select,l = 4 rounds of hashing,lsh attentions select l = 4 rounds of hashing,0.6884702444076538
translation,49,162,experimental-setup,experimental setup,For,lsh attentions,experimental setup For lsh attentions,0.6129791140556335
translation,49,164,experiments,linformer,to,encoder-decoder attentions,linformer to encoder-decoder attentions,0.5952920317649841
translation,49,5,model,novel efficient encoder-decoder attention,with,head - wise positional strides,novel efficient encoder-decoder attention with head - wise positional strides,0.6146605014801025
translation,49,5,model,head - wise positional strides,to effectively pinpoint,salient information,head - wise positional strides to effectively pinpoint salient information,0.7152858376502991
translation,49,5,model,hepos,has,novel efficient encoder-decoder attention,hepos has novel efficient encoder-decoder attention,0.5905775427818298
translation,49,5,model,salient information,has,from the source,salient information has from the source,0.5439300537109375
translation,49,5,model,model,propose,hepos,model propose hepos,0.6897967457771301
translation,49,24,model,efficient encoder-decoder attention,with,head - wise positional strides ( hepos ),efficient encoder-decoder attention with head - wise positional strides ( hepos ),0.6611682176589966
translation,49,24,model,head - wise positional strides ( hepos ),where,attention heads,head - wise positional strides ( hepos ) where attention heads,0.5897575616836548
translation,49,24,model,attention heads,follow,strided pattern,attention heads follow strided pattern,0.6498323082923889
translation,49,24,model,model,propose,efficient encoder-decoder attention,model propose efficient encoder-decoder attention,0.663542628288269
translation,49,177,model,learnable patterns,with,hepos,learnable patterns with hepos,0.70322185754776
translation,49,177,model,model,combine,learnable patterns,model combine learnable patterns,0.7214788794517517
translation,49,25,results,hepos,reduces,computational and memory costs,hepos reduces computational and memory costs,0.6438469290733337
translation,49,25,results,hepos,maintaining,power,hepos maintaining power,0.7638351917266846
translation,49,25,results,hepos,preserving,global context per head,hepos preserving global context per head,0.7889221906661987
translation,49,25,results,power,of,emphasizing important tokens,power of emphasizing important tokens,0.6100665330886841
translation,49,25,results,results,has,hepos,results has hepos,0.546559751033783
translation,49,26,results,hepos,successfully doubles,processed input sequence size,hepos successfully doubles processed input sequence size,0.7370678186416626
translation,49,26,results,processed input sequence size,combined with,any encoder,processed input sequence size combined with any encoder,0.660753071308136
translation,49,26,results,results,has,hepos,results has hepos,0.546559751033783
translation,49,32,results,documents,of,same length,documents of same length,0.6225084066390991
translation,49,32,results,hepos attention,yields,significantly better rouge scores,hepos attention yields significantly better rouge scores,0.7323882579803467
translation,49,32,results,significantly better rouge scores,than,non-trivial comparison,significantly better rouge scores than non-trivial comparison,0.5976220965385437
translation,49,32,results,non-trivial comparison,that projects,attentions,non-trivial comparison that projects attentions,0.737838864326477
translation,49,32,results,attentions,into,low-rank space,attentions into low-rank space,0.5859413743019104
translation,49,32,results,documents,has,hepos attention,documents has hepos attention,0.6647037863731384
translation,49,32,results,results,when summarizing,documents,results when summarizing documents,0.7193809747695923
translation,49,33,results,hepos attention,combined with,sparse encoder attentions,hepos attention combined with sparse encoder attentions,0.6282723546028137
translation,49,33,results,hepos attention,able to,read,hepos attention able to read,0.653195321559906
translation,49,33,results,hepos attention,obtains,significantly higher rouge scores,hepos attention obtains significantly higher rouge scores,0.6447212100028992
translation,49,33,results,significantly higher rouge scores,on,govreport,significantly higher rouge scores on govreport,0.5789909958839417
translation,49,33,results,significantly higher rouge scores,on,new state - of - the - art results,significantly higher rouge scores on new state - of - the - art results,0.5600838661193848
translation,49,33,results,new state - of - the - art results,on,pubmed,new state - of - the - art results on pubmed,0.5659688115119934
translation,49,33,results,new state - of - the - art results,compared with,full encoder-decoder attention models,new state - of - the - art results compared with full encoder-decoder attention models,0.6077053546905518
translation,49,33,results,full encoder-decoder attention models,which can process,at most 5 k input words,full encoder-decoder attention models which can process at most 5 k input words,0.673493504524231
translation,49,33,results,same gpu,has,hepos attention,same gpu has hepos attention,0.6175775527954102
translation,49,33,results,read,has,more than 10 k words,read has more than 10 k words,0.6127014756202698
translation,49,111,results,documents and summaries,in,govreport,documents and summaries in govreport,0.6067586541175842
translation,49,111,results,documents and summaries,are,significantly longer,documents and summaries are significantly longer,0.5683373808860779
translation,49,111,results,govreport,are,significantly longer,govreport are significantly longer,0.6100507378578186
translation,49,111,results,significantly longer,than,prior datasets,significantly longer than prior datasets,0.582870602607727
translation,49,111,results,results,has,documents and summaries,results has documents and summaries,0.5681575536727905
translation,49,167,results,learnable patterns,perform,best,learnable patterns perform best,0.6118566989898682
translation,49,167,results,learnable patterns,approaching,performance,learnable patterns approaching performance,0.7697815895080566
translation,49,167,results,performance,of,full attentions,performance of full attentions,0.5607575178146362
translation,49,167,results,full attentions,on,govreport and pubmed,full attentions on govreport and pubmed,0.5484043955802917
translation,49,167,results,all encoder variants,has,learnable patterns,all encoder variants has learnable patterns,0.5752744078636169
translation,49,167,results,results,Among,all encoder variants,results Among all encoder variants,0.5814818739891052
translation,49,168,results,sinkhorn attention,consistently obtains,better rouge scores,sinkhorn attention consistently obtains better rouge scores,0.6438117623329163
translation,49,168,results,learnable patterns,has,sinkhorn attention,learnable patterns has sinkhorn attention,0.5910335779190063
translation,49,168,results,results,Within,learnable patterns,results Within learnable patterns,0.617271900177002
translation,49,169,results,techniques,in,fixed patterns,techniques in fixed patterns,0.5578972101211548
translation,49,169,results,techniques,is,more effective,techniques is more effective,0.5482550859451294
translation,49,169,results,fixed patterns,using,window - based sparse attentions,fixed patterns using window - based sparse attentions,0.650808572769165
translation,49,169,results,more effective,than,window - based sparse attentions,more effective than window - based sparse attentions,0.5326138734817505
translation,49,169,results,more effective,using,window - based sparse attentions,more effective using window - based sparse attentions,0.6210784316062927
translation,49,169,results,window - based sparse attentions,with,increased memory cost,window - based sparse attentions with increased memory cost,0.6198956966400146
translation,49,169,results,results,combining,techniques,results combining techniques,0.6555589437484741
translation,49,170,results,hepos,consistently yields,higher rouge scores,hepos consistently yields higher rouge scores,0.6251816153526306
translation,49,170,results,higher rouge scores,than,linformer,higher rouge scores than linformer,0.6390813589096069
translation,49,170,results,linformer,on,both datasets,linformer on both datasets,0.558676540851593
translation,49,170,results,encoder-decoder attentions,has,hepos,encoder-decoder attentions has hepos,0.6457622647285461
translation,49,170,results,results,For,encoder-decoder attentions,results For encoder-decoder attentions,0.6121826171875
translation,49,171,results,our model 's performance,matches,variant,our model 's performance matches variant,0.7580006122589111
translation,49,171,results,variant,using,full encoder attention,variant using full encoder attention,0.6949940919876099
translation,49,171,results,sinkhorn attention,has,our model 's performance,sinkhorn attention has our model 's performance,0.5772882103919983
translation,49,173,results,reading more input,Boosts,informativeness,reading more input Boosts informativeness,0.6997694969177246
translation,49,173,results,results,has,reading more input,results has reading more input,0.5704255104064941
translation,49,180,results,models,read,more text,models read more text,0.7108874917030334
translation,49,180,results,more text,obtain,higher rouge scores,more text obtain higher rouge scores,0.5208030939102173
translation,49,180,results,different encoder variants,with,full encoder-decoder attentions,different encoder variants with full encoder-decoder attentions,0.6502431035041809
translation,49,180,results,different encoder variants,attain,better results,different encoder variants attain better results,0.5841658115386963
translation,49,180,results,better results,than,full attentions baseline,better results than full attentions baseline,0.5841382741928101
translation,49,180,results,full attentions baseline,except,linformer,full attentions baseline except linformer,0.6503440737724304
translation,49,180,results,results,has,models,results has models,0.5335168838500977
translation,49,181,results,hepos encoderdecoder attention,almost doubles,words,hepos encoderdecoder attention almost doubles words,0.7333506941795349
translation,49,181,results,hepos encoderdecoder attention,improves,performance,hepos encoderdecoder attention improves performance,0.673091471195221
translation,49,181,results,words,can be,processed,words can be processed,0.6901524066925049
translation,49,181,results,results,adding,hepos encoderdecoder attention,results adding hepos encoderdecoder attention,0.7053149342536926
translation,49,183,results,hepos,with,lsh encoder,hepos with lsh encoder,0.6787374019622803
translation,49,183,results,lsh encoder,achieves,new state - of- the - art results,lsh encoder achieves new state - of- the - art results,0.6663258075714111
translation,49,183,results,new state - of- the - art results,on,pubmed,new state - of- the - art results on pubmed,0.5659688115119934
translation,49,183,results,outperforming,has,bigbird,outperforming has bigbird,0.5841727256774902
translation,49,183,results,results,has,hepos,results has hepos,0.546559751033783
translation,49,184,results,best models,with,hepos,best models with hepos,0.6705625057220459
translation,49,185,results,our model,reads in,10 k tokens,our model reads in 10 k tokens,0.6897746920585632
translation,49,185,results,10 k tokens,generates,more informative summary,10 k tokens generates more informative summary,0.664272129535675
translation,49,185,results,more informative summary,than,full attention model,more informative summary than full attention model,0.540403425693512
translation,49,185,results,full attention model,that only processes,1 k tokens,full attention model that only processes 1 k tokens,0.6075514554977417
translation,49,186,results,rouge - 2 scores,can be,consistently lifted,rouge - 2 scores can be consistently lifted,0.6923849582672119
translation,49,186,results,consistently lifted,when,reading,consistently lifted when reading,0.7027928829193115
translation,49,186,results,reading,has,more input,reading has more input,0.6152951121330261
translation,49,186,results,results,shows that,rouge - 2 scores,results shows that rouge - 2 scores,0.5994296669960022
translation,49,188,results,reading more input,Improves,faithfulness,reading more input Improves faithfulness,0.7328694462776184
translation,49,188,results,results,has,reading more input,results has reading more input,0.5704255104064941
translation,49,195,results,reading,has,more text,reading has more text,0.5181243419647217
translation,49,195,results,more text,has,significantly improves,more text has significantly improves,0.6090528964996338
translation,49,195,results,significantly improves,has,informativeness,significantly improves has informativeness,0.5881998538970947
translation,49,195,results,reduces,has,fabricated content,reduces has fabricated content,0.6091045141220093
translation,49,195,results,results,has,reading,results has reading,0.48930060863494873
translation,49,196,results,hepos attention,combined with,sinkhorn encoder,hepos attention combined with sinkhorn encoder,0.6893070340156555
translation,49,196,results,hepos attention,obtains,better informativeness scores,hepos attention obtains better informativeness scores,0.6230709552764893
translation,49,196,results,better informativeness scores,than,comparisons,better informativeness scores than comparisons,0.5939481854438782
translation,49,196,results,comparisons,read in,less text,comparisons read in less text,0.7431005835533142
translation,49,196,results,less text,on,both datasets,less text on both datasets,0.5066639184951782
translation,49,196,results,results,observe,hepos attention,results observe hepos attention,0.6184723377227783
translation,49,199,results,models,read,more content,models read more content,0.7042456865310669
translation,49,199,results,more content,learn to,surface,more content learn to surface,0.7208431363105774
translation,49,199,results,more factual and richer content,in,summaries,more factual and richer content in summaries,0.5389918088912964
translation,49,199,results,surface,has,more factual and richer content,surface has more factual and richer content,0.5875211358070374
translation,49,199,results,results,As,models,results As models,0.5873361229896545
translation,49,204,results,our models,reduce,errors,our models reduce errors,0.7299795150756836
translation,49,204,results,errors,across,sections,errors across sections,0.7316826581954956
translation,49,204,results,our models,has,consistently improve,our models has consistently improve,0.5626916289329529
translation,49,204,results,consistently improve,has,informativeness,consistently improve has informativeness,0.5531691908836365
translation,49,204,results,results,has,our models,results has our models,0.5733726620674133
translation,49,205,results,full attention model,tends to produce,fabricated numbers,full attention model tends to produce fabricated numbers,0.7436821460723877
translation,49,205,results,fabricated numbers,in,resultant summaries,fabricated numbers in resultant summaries,0.5437483191490173
translation,49,205,results,results,find that,full attention model,results find that full attention model,0.6463070511817932
translation,50,135,ablation-analysis,both dimensions,into,account,both dimensions into account,0.5950987339019775
translation,50,135,ablation-analysis,average correlation,by,18 % ( 28.4 to 33.5 ),average correlation by 18 % ( 28.4 to 33.5 ),0.5188945531845093
translation,50,136,ablation-analysis,our question weighter,is,relevance,our question weighter is relevance,0.60040682554245
translation,50,136,ablation-analysis,relevance,has,+ 4 %,relevance has + 4 %,0.6079850792884827
translation,50,136,ablation-analysis,ablation analysis,has,dimension,ablation analysis has dimension,0.5598704814910889
translation,50,7,model,unified framework,named,qu,unified framework named qu,0.6518999338150024
translation,50,7,model,model,propose,unified framework,model propose unified framework,0.6881625652313232
translation,50,30,model,reference -less metric,to improve,correlation,reference -less metric to improve correlation,0.6937930583953857
translation,50,30,model,correlation,with,humans judgments,correlation with humans judgments,0.6480666995048523
translation,50,134,results,summaqa,correlates better,relevance,summaqa correlates better relevance,0.8042996525764465
translation,50,134,results,relevance,than,consistency ( 26.2 vs 8.3 ),relevance than consistency ( 26.2 vs 8.3 ),0.48710888624191284
translation,50,139,results,improvement,is,remarkable,improvement is remarkable,0.6203098297119141
translation,50,139,results,other metrics,has,improvement,other metrics has improvement,0.5438967943191528
translation,50,139,results,remarkable,has,33.5 vs 11.8,remarkable has 33.5 vs 11.8,0.5533638596534729
translation,50,139,results,results,Compared to,other metrics,results Compared to other metrics,0.6189091205596924
translation,50,163,results,percentage of questions,answered but,not important,percentage of questions answered but not important,0.6362555027008057
translation,50,163,results,percentage of questions,correlate with,relevance,percentage of questions correlate with relevance,0.6993667483329773
translation,50,163,results,not important,correlate with,relevance,not important correlate with relevance,0.6799372434616089
translation,50,163,results,results,has,percentage of questions,results has percentage of questions,0.5529026389122009
translation,52,182,ablation-analysis,rnn cells,are,more preferred,rnn cells are more preferred,0.6209374666213989
translation,52,182,ablation-analysis,more preferred,when,architecture,more preferred when architecture,0.6738607287406921
translation,52,182,ablation-analysis,architecture,restricted to,"fewer layers ( like 2 , 5 , 6 )","architecture restricted to fewer layers ( like 2 , 5 , 6 )",0.7346035242080688
translation,52,182,ablation-analysis,convolutional layers,with,larger stride,convolutional layers with larger stride,0.6245699524879456
translation,52,182,ablation-analysis,ablation analysis,see that,rnn cells,ablation analysis see that rnn cells,0.6076124310493469
translation,52,144,hyperparameters,autosumm -create experiments,perform,20 - layer neural architectural search,autosumm -create experiments perform 20 - layer neural architectural search,0.6532114148139954
translation,52,144,hyperparameters,20 - layer neural architectural search,for,encoder,20 - layer neural architectural search for encoder,0.6085996627807617
translation,52,144,hyperparameters,hyperparameters,In,autosumm -create experiments,hyperparameters In autosumm -create experiments,0.4979957938194275
translation,52,146,hyperparameters,glove word embeddings,while providing,input,glove word embeddings while providing input,0.5719582438468933
translation,52,146,hyperparameters,input,to,generated model,input to generated model,0.5957973599433899
translation,52,146,hyperparameters,hyperparameters,use,glove word embeddings,hyperparameters use glove word embeddings,0.5590203404426575
translation,52,147,hyperparameters,batch size,as,128,batch size as 128,0.5939822196960449
translation,52,147,hyperparameters,max input length,as,64,max input length as 64,0.5423985123634338
translation,52,147,hyperparameters,hidden unit dimension,for,each layer,hidden unit dimension for each layer,0.5669893026351929
translation,52,147,hyperparameters,each layer,as,32,each layer as 32,0.5643171072006226
translation,52,147,hyperparameters,dropout ratio,as,0.5,dropout ratio as 0.5,0.5507219433784485
translation,52,147,hyperparameters,hyperparameters,set,batch size,hyperparameters set batch size,0.6402808427810669
translation,52,147,hyperparameters,hyperparameters,set,max input length,hyperparameters set max input length,0.6147192716598511
translation,52,147,hyperparameters,hyperparameters,set,hidden unit dimension,hyperparameters set hidden unit dimension,0.6376860737800598
translation,52,147,hyperparameters,hyperparameters,set,dropout ratio,hyperparameters set dropout ratio,0.6064027547836304
translation,52,147,hyperparameters,hyperparameters,set,l2 regularization,hyperparameters set l2 regularization,0.5666295289993286
translation,52,148,hyperparameters,learning rate decay,with,cosine annealing,learning rate decay with cosine annealing,0.6326505541801453
translation,52,148,hyperparameters,hyperparameters,utilize,adam optimizer,hyperparameters utilize adam optimizer,0.5550990104675293
translation,52,148,hyperparameters,hyperparameters,utilize,learning rate decay,hyperparameters utilize learning rate decay,0.5363263487815857
translation,52,7,model,knowledge distillation ( kd ) techniques,to perform,model search and compression,knowledge distillation ( kd ) techniques to perform model search and compression,0.6614441871643066
translation,52,22,model,alternate method,for,summarization model generation,alternate method for summarization model generation,0.5813402533531189
translation,52,22,model,summarization model generation,using,transformer distillation,summarization model generation using transformer distillation,0.6603596806526184
translation,52,22,model,model,propose,alternate method,model propose alternate method,0.7052618265151978
translation,52,48,model,approaches,distills,knowledge,approaches distills knowledge,0.7305837869644165
translation,52,48,model,knowledge,from,language -model based teacher network,knowledge from language -model based teacher network,0.569911003112793
translation,52,48,model,knowledge,to generate,encoder-decoder - based child model,knowledge to generate encoder-decoder - based child model,0.7122227549552917
translation,52,49,model,two algorithms,aid in,auto-creation of different types of resulting ' child ' models,two algorithms aid in auto-creation of different types of resulting ' child ' models,0.678219735622406
translation,52,49,model,model,with,convolutional and recurrent units,model with convolutional and recurrent units,0.6037219762802124
translation,52,49,model,model,present,two algorithms,model present two algorithms,0.7042034268379211
translation,52,55,model,predictions,from,teacher model,predictions from teacher model,0.536008358001709
translation,52,55,model,teacher model,used for,distillation,teacher model used for distillation,0.679399311542511
translation,52,55,model,distillation,i.e.,sentences classification scores,distillation i.e. sentences classification scores,0.594883382320404
translation,52,55,model,sentences classification scores,for,extractive,sentences classification scores for extractive,0.608491063117981
translation,52,55,model,probability distributions,over,vocabulary,probability distributions over vocabulary,0.6962608695030212
translation,52,55,model,probability distributions,augmented to,ground truth,probability distributions augmented to ground truth,0.6770387291908264
translation,52,55,model,vocabulary,for,abstractive,vocabulary for abstractive,0.5762325525283813
translation,52,55,model,model,has,predictions,model has predictions,0.5742831230163574
translation,52,71,model,rnn controller network,samples,model architecture,rnn controller network samples model architecture,0.6762357354164124
translation,52,71,model,rnn controller network,samples,rl reward,rnn controller network samples rl reward,0.645685613155365
translation,52,71,model,model architecture,from,search space,model architecture from search space,0.539213240146637
translation,52,71,model,rl reward,to nudge,controller,rl reward to nudge controller,0.6179602742195129
translation,52,71,model,controller,towards generating,optimal architecture,controller towards generating optimal architecture,0.6582196950912476
translation,52,71,model,model,consists of,rnn controller network,model consists of rnn controller network,0.6726705431938171
translation,52,113,model,decoder,is,6 - layer transformer,decoder is 6 - layer transformer,0.5882114171981812
translation,52,113,model,abstractive summarization,has,decoder,abstractive summarization has decoder,0.5589610934257507
translation,52,113,model,model,For,abstractive summarization,model For abstractive summarization,0.5606654286384583
translation,52,145,model,decoders,are,task -specific,decoders are task -specific,0.5810464024543762
translation,52,145,model,decoders,are,predefined,decoders are predefined,0.6524795889854431
translation,52,145,model,model,has,decoders,model has decoders,0.6080074906349182
translation,52,181,model,cnn and rnn layers,are,major constituents,cnn and rnn layers are major constituents,0.5757560729980469
translation,52,181,model,major constituents,in,architectures,major constituents in architectures,0.5164344906806946
translation,52,181,model,model,observe,cnn and rnn layers,model observe cnn and rnn layers,0.6255649328231812
translation,52,203,model,auto-generation of ml models,for,extract and generate tasks,auto-generation of ml models for extract and generate tasks,0.6058735251426697
translation,52,157,results,rouge scores,show that,summaries,rouge scores show that summaries,0.5353240370750427
translation,52,157,results,summaries,by,auto-generated models,summaries by auto-generated models,0.6247583031654358
translation,52,157,results,auto-generated models,from,our proposed framework,auto-generated models from our proposed framework,0.5663205981254578
translation,52,157,results,auto-generated models,close to,state - of - the - art bert baseline,auto-generated models close to state - of - the - art bert baseline,0.6413059234619141
translation,52,157,results,results,has,rouge scores,results has rouge scores,0.541054368019104
translation,52,164,results,abstractive -summarization models,on,gigaword dataset,abstractive -summarization models on gigaword dataset,0.4940764009952545
translation,52,165,results,our proposed summarization model,with,transformer distillation ft -tinybert - abs,our proposed summarization model with transformer distillation ft -tinybert - abs,0.6841267347335815
translation,52,165,results,transformer distillation ft -tinybert - abs,beats,ft - bert - abs,transformer distillation ft -tinybert - abs beats ft - bert - abs,0.7521522045135498
translation,52,165,results,ft - bert - abs,with,huge margin,ft - bert - abs with huge margin,0.7187584042549133
translation,52,165,results,huge margin,across,"all r -1 , r - 2 , r-l","huge margin across all r -1 , r - 2 , r-l",0.7419081926345825
translation,52,165,results,results,noted that,our proposed summarization model,results noted that our proposed summarization model,0.6193683743476868
translation,52,180,results,extractive summarization,on,cnn / dm dataset,extractive summarization on cnn / dm dataset,0.5279361605644226
translation,52,184,results,model,with,15 layers,model with 15 layers,0.6916335225105286
translation,52,184,results,model,gives,best performance,model gives best performance,0.6371784806251526
translation,52,184,results,15 layers,gives,best performance,15 layers gives best performance,0.6360833644866943
translation,52,184,results,performance,does not,drop too much,performance does not drop too much,0.6607494354248047
translation,52,184,results,drop too much,with,varying number of layers,drop too much with varying number of layers,0.6854840517044067
translation,52,198,results,summary size,of,3 sentences,summary size of 3 sentences,0.5966431498527527
translation,52,198,results,summary size,yields,best result,summary size yields best result,0.6869417428970337
translation,52,198,results,proposed framework,allows generating,shorter or longer summaries,proposed framework allows generating shorter or longer summaries,0.7729347944259644
translation,52,198,results,shorter or longer summaries,without,significant loss,shorter or longer summaries without significant loss,0.6765217185020447
translation,52,198,results,significant loss,in,performance,significant loss in performance,0.5411703586578369
translation,52,200,results,auto-generated models,generated using,proposed nas and transformer -distillation based frameworks,auto-generated models generated using proposed nas and transformer -distillation based frameworks,0.6991456151008606
translation,52,200,results,auto-generated models,report,near state - of - the - art performance,auto-generated models report near state - of - the - art performance,0.6509189605712891
translation,52,200,results,near state - of - the - art performance,for,extractive and abstractive summarization,near state - of - the - art performance for extractive and abstractive summarization,0.6061531901359558
translation,53,161,ablation-analysis,significantly drops,after incorporating,f ? score,significantly drops after incorporating f ? score,0.722139298915863
translation,53,161,ablation-analysis,performance,has,significantly drops,performance has significantly drops,0.6223364472389221
translation,53,176,ablation-analysis,performance,on,redundancy evaluation,performance on redundancy evaluation,0.564985990524292
translation,53,176,ablation-analysis,redundancy evaluation,on,cnndm,redundancy evaluation on cnndm,0.545418918132782
translation,53,176,ablation-analysis,redundancy score,has,significantly degrades,redundancy score has significantly degrades,0.6357030272483826
translation,53,176,ablation-analysis,significantly degrades,has,performance,significantly degrades has performance,0.5908101797103882
translation,53,176,ablation-analysis,ablation analysis,removing,redundancy score,ablation analysis removing redundancy score,0.7658669948577881
translation,53,140,baselines,"repear ( rioux et al. , 2014 )",as,traditional reference -free baselines,"repear ( rioux et al. , 2014 ) as traditional reference -free baselines",0.5079149007797241
translation,53,140,baselines,tf -idf,has,"js ( louis and nenkova , 2013 )","tf -idf has js ( louis and nenkova , 2013 )",0.5605601668357849
translation,53,140,baselines,baselines,choose,tf -idf,baselines choose tf -idf,0.6136095523834229
translation,53,145,baselines,"rouge -1/2/l ( lin , 2004 )",into,referencefree scenario,"rouge -1/2/l ( lin , 2004 ) into referencefree scenario",0.5628685355186462
translation,53,145,baselines,s+wms,into,referencefree scenario,s+wms into referencefree scenario,0.581938624382019
translation,53,145,baselines,referencefree scenario,via building,pseudo reference,referencefree scenario via building pseudo reference,0.7766230702400208
translation,53,145,baselines,pseudo reference,with,pacsumtopm method,pseudo reference with pacsumtopm method,0.6276355981826782
translation,53,145,baselines,s+wms,has,"clark et al. , 2019 )","s+wms has clark et al. , 2019 )",0.5964407324790955
translation,53,6,model,model,propose,training -free and reference -free summarization evaluation metric,model propose training -free and reference -free summarization evaluation metric,0.6089813113212585
translation,53,33,model,novel training -free and reference -free metric,for,multiple and single document summarization evaluation,novel training -free and reference -free metric for multiple and single document summarization evaluation,0.5333977341651917
translation,53,33,model,model,propose,novel training -free and reference -free metric,model propose novel training -free and reference -free metric,0.6682491898536682
translation,53,36,model,hybrid way,contains,token - level representations,hybrid way contains token - level representations,0.6128710508346558
translation,53,36,model,hybrid way,contains,sentence - level representations,hybrid way contains sentence - level representations,0.615706205368042
translation,53,36,model,sentence - level representations,to encode,document and the summary,sentence - level representations to encode document and the summary,0.749863862991333
translation,53,36,model,model,engage,hybrid way,model engage hybrid way,0.7295905947685242
translation,53,52,model,f ? based relevance score,pays,more attention,f ? based relevance score pays more attention,0.6887434124946594
translation,53,52,model,more attention,to,recall,more attention to recall,0.5417292714118958
translation,53,52,model,self-referenced redundancy score,utilizes,self-masked bertscore,self-referenced redundancy score utilizes self-masked bertscore,0.6287946105003357
translation,53,52,model,self-masked bertscore,to detect,duplicated information,self-masked bertscore to detect duplicated information,0.6995561718940735
translation,53,52,model,duplicated information,of,given summary,duplicated information of given summary,0.6380129456520081
translation,53,52,model,model,develop,f ? based relevance score,model develop f ? based relevance score,0.5915570855140686
translation,53,49,results,outperforms,on,all datasets,outperforms on all datasets,0.5181515216827393
translation,53,49,results,all the sota baselines,on,all datasets,all the sota baselines on all datasets,0.48637938499450684
translation,53,49,results,outperforms,has,all the sota baselines,outperforms has all the sota baselines,0.6028977036476135
translation,53,49,results,results,show that,f 1based method,results show that f 1based method,0.49358615279197693
translation,53,153,results,consistently outperforms,has,all the baselines,consistently outperforms has all the baselines,0.597512423992157
translation,53,153,results,results,find that,our f 1 version,results find that our f 1 version,0.6683005690574646
translation,53,154,results,our f ? version,further improve,performance,our f ? version further improve performance,0.7093377113342285
translation,53,154,results,performance,of,multi-document summarization evaluation,performance of multi-document summarization evaluation,0.5089960098266602
translation,53,154,results,results,demonstrate,our f ? version,results demonstrate our f ? version,0.6164045333862305
translation,53,155,results,ours ( f ? ) - pacsumtopm,see that,pseudo reference building process,ours ( f ? ) - pacsumtopm see that pseudo reference building process,0.7030221819877625
translation,53,155,results,ours ( f ? ) - all,see that,pseudo reference building process,ours ( f ? ) - all see that pseudo reference building process,0.6872321963310242
translation,53,155,results,significantly improve,has,performance,significantly improve has performance,0.5721634030342102
translation,53,155,results,results,comparing,ours ( f ? ) - pacsumtopm,results comparing ours ( f ? ) - pacsumtopm,0.7083668112754822
translation,53,160,results,outperforms,manifests,high generalization ability,outperforms manifests high generalization ability,0.6421486139297485
translation,53,160,results,all the baselines,manifests,high generalization ability,all the baselines manifests high generalization ability,0.6714819073677063
translation,53,160,results,outperforms,has,all the baselines,outperforms has all the baselines,0.596747100353241
translation,53,160,results,results,note,f 1 version,results note f 1 version,0.5998989343643188
translation,53,174,results,rank - based spearman 's ?,after removing,one of the three components,rank - based spearman 's ? after removing one of the three components,0.7272199988365173
translation,53,174,results,one of the three components,i.e.,centrality weighting,one of the three components i.e. centrality weighting,0.7103044390678406
translation,53,174,results,one of the three components,i.e.,hybrid representation,one of the three components i.e. hybrid representation,0.7274851202964783
translation,53,174,results,one of the three components,i.e.,redundancy score,one of the three components i.e. redundancy score,0.6916449666023254
translation,53,174,results,performance,of,our methods,performance of our methods,0.5525400638580322
translation,53,174,results,performance,become,worse,performance become worse,0.6597403287887573
translation,53,174,results,our methods,become,worse,our methods become worse,0.6045319437980652
translation,53,174,results,worse,in,most cases,worse in most cases,0.5984767079353333
translation,53,174,results,rank - based spearman 's ?,has,performance,rank - based spearman 's ? has performance,0.5999218821525574
translation,53,174,results,one of the three components,has,performance,one of the three components has performance,0.5648490786552429
translation,53,174,results,results,of,rank - based spearman 's ?,results of rank - based spearman 's ?,0.5244839191436768
translation,53,180,results,performance,of,original moverscore,performance of original moverscore,0.5423713326454163
translation,53,180,results,performance,on,multi-document summarization evaluation,performance on multi-document summarization evaluation,0.4960889518260956
translation,53,180,results,original moverscore,on,single-document summarization evaluation,original moverscore on single-document summarization evaluation,0.4645650088787079
translation,53,180,results,significantly improve,has,performance,significantly improve has performance,0.5721634030342102
translation,53,181,results,outperforms,on,  overall   and   redundancy   aspects,outperforms on   overall   and   redundancy   aspects,0.550502598285675
translation,53,181,results,ours ( f 1 ),on,  overall   and   redundancy   aspects,ours ( f 1 ) on   overall   and   redundancy   aspects,0.5885347127914429
translation,53,181,results,cnndm,has,enhanced mover - score,cnndm has enhanced mover - score,0.6061827540397644
translation,53,181,results,enhanced mover - score,has,outperforms,enhanced mover - score has outperforms,0.6313686966896057
translation,53,181,results,outperforms,has,ours ( f 1 ),outperforms has ours ( f 1 ),0.599750280380249
translation,53,181,results,results,On,cnndm,results On cnndm,0.5700629353523254
translation,53,189,results,large encoding models,obtain,better performance,large encoding models obtain better performance,0.5634651780128479
translation,53,189,results,better performance,than,base encoding models,better performance than base encoding models,0.5671541094779968
translation,54,4,experiments,largescale media interview dataset,consisting of,463.6 k transcripts,largescale media interview dataset consisting of 463.6 k transcripts,0.6811637282371521
translation,54,4,experiments,463.6 k transcripts,with,abstractive summaries,463.6 k transcripts with abstractive summaries,0.602008044719696
translation,54,4,experiments,mediasum,has,largescale media interview dataset,mediasum has largescale media interview dataset,0.5842763781547546
translation,54,5,experiments,interview transcripts,from,npr and cnn,interview transcripts from npr and cnn,0.5591152310371399
translation,54,5,experiments,overview and topic descriptions,as,summaries,overview and topic descriptions as summaries,0.5727939009666443
translation,54,102,results,lead - 3 baseline,has,relatively weak performance,lead - 3 baseline has relatively weak performance,0.5833470225334167
translation,54,104,results,pre-trained models,such as,bart and unilm,pre-trained models such as bart and unilm,0.6173880696296692
translation,54,104,results,bart and unilm,has,outperform,bart and unilm has outperform,0.6577351689338684
translation,54,104,results,outperform,has,non-pretrained ptgen model,outperform has non-pretrained ptgen model,0.5831829309463501
translation,54,104,results,results,has,pre-trained models,results has pre-trained models,0.5230302810668945
translation,54,108,results,all three datasets,training on,mediasum,all three datasets training on mediasum,0.7461757063865662
translation,54,108,results,mediasum,leads to,improvement,mediasum leads to improvement,0.6925893425941467
translation,54,108,results,improvement,on,target dataset,improvement on target dataset,0.5444322228431702
translation,54,108,results,results,on,all three datasets,results on all three datasets,0.48488834500312805
translation,55,162,baselines,"textrank ( mihalcea and tarau , 2004 )",for,initial summary and suggested queries,"textrank ( mihalcea and tarau , 2004 ) for initial summary and suggested queries",0.5761160254478455
translation,55,162,baselines,"textrank ( mihalcea and tarau , 2004 )",for,queryresponse generation approach,"textrank ( mihalcea and tarau , 2004 ) for queryresponse generation approach",0.574690580368042
translation,55,162,baselines,"textrank ( mihalcea and tarau , 2004 )",both,initial summary and suggested queries,"textrank ( mihalcea and tarau , 2004 ) both initial summary and suggested queries",0.5987363457679749
translation,55,162,baselines,queryresponse generation approach,combining,semantic and lexical similarity,queryresponse generation approach combining semantic and lexical similarity,0.7036834359169006
translation,55,162,baselines,semantic and lexical similarity,between,query and sentences,semantic and lexical similarity between query and sentences,0.6266095638275146
translation,55,276,baselines,first variant,utilizes,semantic ( w2v - based ) sentence representations,first variant utilizes semantic ( w2v - based ) sentence representations,0.5779255628585815
translation,55,276,baselines,q sem,utilizes,semantic ( w2v - based ) sentence representations,q sem utilizes semantic ( w2v - based ) sentence representations,0.6203727126121521
translation,55,276,baselines,semantic ( w2v - based ) sentence representations,prepared in,initialization process,semantic ( w2v - based ) sentence representations prepared in initialization process,0.5783607959747314
translation,55,276,baselines,first variant,has,q sem,first variant has q sem,0.6263624429702759
translation,55,276,baselines,baselines,has,first variant,baselines has first variant,0.590368390083313
translation,55,305,experiments,intsumm systems,on,intel xeon cpu e5 - 2670 v3 @ 2.30 ghz server,intsumm systems on intel xeon cpu e5 - 2670 v3 @ 2.30 ghz server,0.5020449161529541
translation,55,305,experiments,intel xeon cpu e5 - 2670 v3 @ 2.30 ghz server,with,50 gb ram,intel xeon cpu e5 - 2670 v3 @ 2.30 ghz server with 50 gb ram,0.5284304022789001
translation,55,305,experiments,50 gb ram,running,centos linux 7,50 gb ram running centos linux 7,0.6134527325630188
translation,55,256,hyperparameters,all sentences,in,document set,all sentences in document set,0.5062758922576904
translation,55,256,hyperparameters,all sentences,separately assigned,representation,all sentences separately assigned representation,0.7812294363975525
translation,55,256,hyperparameters,representation,by averaging,300 - dimensional word2vec ( w2v ) embeddings,representation by averaging 300 - dimensional word2vec ( w2v ) embeddings,0.7114576101303101
translation,55,256,hyperparameters,300 - dimensional word2vec ( w2v ) embeddings,within,each sentence,300 - dimensional word2vec ( w2v ) embeddings within each sentence,0.6073247790336609
translation,55,256,hyperparameters,hyperparameters,has,all sentences,hyperparameters has all sentences,0.5304702520370483
translation,55,266,hyperparameters,number of sentence clusters ( 30 ),tested,values,number of sentence clusters ( 30 ) tested values,0.7029494047164917
translation,55,266,hyperparameters,number of sentence clusters ( 30 ),for,similarity threshold ( 0.95 ),number of sentence clusters ( 30 ) for similarity threshold ( 0.95 ),0.5803451538085938
translation,55,266,hyperparameters,values,between,10 and 50,values between 10 and 50,0.7010983228683472
translation,55,266,hyperparameters,similarity threshold ( 0.95 ),tested,several options,similarity threshold ( 0.95 ) tested several options,0.6909010410308838
translation,55,266,hyperparameters,several options,within,0 to 1 range,several options within 0 to 1 range,0.6338533163070679
translation,55,266,hyperparameters,hyperparameters,For,number of sentence clusters ( 30 ),hyperparameters For number of sentence clusters ( 30 ),0.573704183101654
translation,55,266,hyperparameters,hyperparameters,For,similarity threshold ( 0.95 ),hyperparameters For similarity threshold ( 0.95 ),0.561910092830658
translation,55,266,hyperparameters,hyperparameters,for,similarity threshold ( 0.95 ),hyperparameters for similarity threshold ( 0.95 ),0.561910092830658
translation,55,7,model,evaluation measures,relying on,summarization standards,evaluation measures relying on summarization standards,0.6394345760345459
translation,55,7,model,evaluation measures,adapted to reflect,interaction,evaluation measures adapted to reflect interaction,0.6942036747932434
translation,55,7,model,procedure,has,of collecting real user sessions,procedure has of collecting real user sessions,0.5341212749481201
translation,55,21,model,framework,starts with,real user session collection,framework starts with real user session collection,0.6634548306465149
translation,55,21,model,real user session collection,on,system,real user session collection on system,0.5665267109870911
translation,55,21,model,real user session collection,via,concrete process,real user session collection via concrete process,0.6561383008956909
translation,55,21,model,model,has,framework,model has framework,0.5441871285438538
translation,55,37,model,model,describe,controlled crowdsourcing procedure,model describe controlled crowdsourcing procedure,0.6635028719902039
translation,55,303,model,app,communicates with,backend,app communicates with backend,0.755774736404419
translation,55,303,model,backend,over,standard http post requests,backend over standard http post requests,0.6445624828338623
translation,55,303,model,standard http post requests,in,json format,standard http post requests in json format,0.5343124866485596
translation,55,303,model,model,has,app,model has app,0.6316155195236206
translation,55,204,results,users,more satisfied by,s 2,users more satisfied by s 2,0.7533628344535828
translation,55,204,results,results,has,users,results has users,0.5371229648590088
translation,55,306,results,intel core i7-6600 cpu @ 2.60 ghz laptop,with,16gb ram,intel core i7-6600 cpu @ 2.60 ghz laptop with 16gb ram,0.54633629322052
translation,55,306,results,16gb ram,running,windows 10,16gb ram running windows 10,0.6470985412597656
translation,55,306,results,results,has,run times,results has run times,0.4787640869617462
translation,56,4,ablation-analysis,news articles,has,lead bias,news articles has lead bias,0.5370038151741028
translation,56,72,ablation-analysis,position cues ( no position encoding ),by using,semantic representation,position cues ( no position encoding ) by using semantic representation,0.6074309945106506
translation,56,72,ablation-analysis,position cues ( no position encoding ),only,semantic representation,position cues ( no position encoding ) only semantic representation,0.6360507011413574
translation,56,72,ablation-analysis,semantic representation,as,input,semantic representation as input,0.5220313668251038
translation,56,72,ablation-analysis,drops considerably,on,cnn / dm,drops considerably on cnn / dm,0.5953318476676941
translation,56,72,ablation-analysis,remarkably increase,on,xsum,remarkably increase on xsum,0.6610134840011597
translation,56,72,ablation-analysis,position cues ( no position encoding ),has,model 's performance,position cues ( no position encoding ) has model 's performance,0.5529066920280457
translation,56,72,ablation-analysis,model 's performance,has,drops considerably,model 's performance has drops considerably,0.605440616607666
translation,56,68,experiments,top - 3 sentences,to form,final summary,top - 3 sentences to form final summary,0.609824001789093
translation,56,68,experiments,final summary,for,cnn / dm,final summary for cnn / dm,0.6591299772262573
translation,56,68,experiments,top - 1 sentence,for,xsum,top - 1 sentence for xsum,0.5633271336555481
translation,56,68,experiments,xsum,due to,different average summary lengths,xsum due to different average summary lengths,0.6822808980941772
translation,56,66,hyperparameters,same setting,as,standard transformer,same setting as standard transformer,0.5735436677932739
translation,56,66,hyperparameters,same setting,with,8 heads per layer,same setting with 8 heads per layer,0.6933339834213257
translation,56,67,hyperparameters,adam,to train,all the models,adam to train all the models,0.7136621475219727
translation,56,67,hyperparameters,all the models,with,scheduled learning rate,all the models with scheduled learning rate,0.640032947063446
translation,56,67,hyperparameters,all the models,with,warm - up,all the models with warm - up,0.6638748645782471
translation,56,67,hyperparameters,all the models,with,initial learning rate lr,all the models with initial learning rate lr,0.6015956401824951
translation,56,67,hyperparameters,scheduled learning rate,with,warm - up,scheduled learning rate with warm - up,0.699532687664032
translation,56,67,hyperparameters,warm - up,has,initial learning rate lr,warm - up has initial learning rate lr,0.5339219570159912
translation,56,69,hyperparameters,class number,of,sentence position m,class number of sentence position m,0.6240252256393433
translation,56,69,hyperparameters,class number,of,trade - off parameter ?,class number of trade - off parameter ?,0.601771354675293
translation,56,69,hyperparameters,sentence position m,set to,10,sentence position m set to 10,0.7267637252807617
translation,56,69,hyperparameters,trade - off parameter ?,set to,0.9,trade - off parameter ? set to 0.9,0.6957359910011292
translation,56,69,hyperparameters,0.9,searched from,0 to 1,0.9 searched from 0 to 1,0.3166881501674652
translation,56,69,hyperparameters,hyperparameters,has,class number,hyperparameters has class number,0.5175925493240356
translation,56,69,hyperparameters,hyperparameters,has,trade - off parameter ?,hyperparameters has trade - off parameter ?,0.5652910470962524
translation,56,5,model,novel technique,to demote,lead bias,novel technique to demote lead bias,0.7340225577354431
translation,56,5,model,novel technique,summarizer focus more on,content semantics,novel technique summarizer focus more on content semantics,0.7234424352645874
translation,56,5,model,model,introduce,novel technique,model introduce novel technique,0.7118059992790222
translation,56,12,model,extractive summarizer,for,news,extractive summarizer for news,0.6293686628341675
translation,56,90,model,lead bias demoting method,to make,news extractive summarizers,lead bias demoting method to make news extractive summarizers,0.6379482746124268
translation,56,90,model,more robust,across,datastets,more robust across datastets,0.6894357800483704
translation,56,90,model,news extractive summarizers,has,more robust,news extractive summarizers has more robust,0.5338772535324097
translation,56,90,model,model,propose,lead bias demoting method,model propose lead bias demoting method,0.6564627885818481
translation,56,77,results,our proposal and shuffling,give,significant performance boosting,our proposal and shuffling give significant performance boosting,0.6348459720611572
translation,56,77,results,significant performance boosting,on,xsum,significant performance boosting on xsum,0.5825176239013672
translation,56,77,results,learned - mixin,results in,performance decrease,learned - mixin results in performance decrease,0.652606189250946
translation,56,77,results,performance decrease,on,both datasets,performance decrease on both datasets,0.5246691107749939
translation,56,79,results,transformer,without,position encoding,transformer without position encoding,0.7345995903015137
translation,56,79,results,transformer,achieves,best performance,transformer achieves best performance,0.6977461576461792
translation,56,79,results,best performance,on,xsum,best performance on xsum,0.5705977082252502
translation,56,88,results,shuffling,effectively demote,extreme trend,shuffling effectively demote extreme trend,0.8487978577613831
translation,56,88,results,method,effectively demote,extreme trend,method effectively demote extreme trend,0.8086345791816711
translation,56,88,results,extreme trend,towards selecting,sentences,extreme trend towards selecting sentences,0.7093756794929504
translation,56,88,results,extreme trend,towards selecting,sentences,extreme trend towards selecting sentences,0.7093756794929504
translation,56,88,results,sentences,in,lead position,sentences in lead position,0.5098997354507446
translation,56,88,results,sentences,with,higher relative position,sentences with higher relative position,0.6162766814231873
translation,56,88,results,our method,seems to be,sightly better,our method seems to be sightly better,0.7023389339447021
translation,56,88,results,sightly better,at encouraging,model,sightly better at encouraging model,0.7175878286361694
translation,56,88,results,model,to select,sentences,model to select sentences,0.7385075688362122
translation,56,88,results,sentences,with,higher relative position,sentences with higher relative position,0.6162766814231873
translation,56,88,results,results,has,shuffling,results has shuffling,0.5517935752868652
translation,57,6,experiments,review corpus,create,synthetic training dataset,review corpus create synthetic training dataset,0.575122058391571
translation,57,6,experiments,synthetic training dataset,of,"( review , summary ) pairs","synthetic training dataset of ( review , summary ) pairs",0.5450950860977173
translation,57,6,experiments,"( review , summary ) pairs",enriched with,aspect controllers,"( review , summary ) pairs enriched with aspect controllers",0.7626839876174927
translation,57,6,experiments,aspect controllers,induced by,multi-instance learning model,aspect controllers induced by multi-instance learning model,0.6569768190383911
translation,57,6,experiments,multi-instance learning model,that predicts,aspects,multi-instance learning model that predicts aspects,0.6720561385154724
translation,57,6,experiments,aspects,of,document,aspects of document,0.6729514002799988
translation,57,6,experiments,aspects,at,different levels of granularity,aspects at different levels of granularity,0.5591729283332825
translation,57,82,experiments,corpus,of,reviews,corpus of reviews,0.6018186807632446
translation,57,82,experiments,corpus,construct,synthetic training dataset,corpus construct synthetic training dataset,0.7149226665496826
translation,57,82,experiments,reviews,on,"entities ( e.g. , hotels , television sets )","reviews on entities ( e.g. , hotels , television sets )",0.5359932780265808
translation,57,82,experiments,synthetic training dataset,consisting of,reviews,synthetic training dataset consisting of reviews,0.6984809637069702
translation,57,82,experiments,synthetic training dataset,consisting of,pseudo-summary,synthetic training dataset consisting of pseudo-summary,0.6993719935417175
translation,57,82,experiments,synthetic training dataset,consisting of,three types of aspect controllers,synthetic training dataset consisting of three types of aspect controllers,0.6961462497711182
translation,57,82,experiments,three types of aspect controllers,reflect,different levels of granularity,three types of aspect controllers reflect different levels of granularity,0.6601776480674744
translation,57,84,experiments,aspect-enriched dataset,fine - tune,"pretrained model ( raffel et al. , 2020 )","aspect-enriched dataset fine - tune pretrained model ( raffel et al. , 2020 )",0.663287341594696
translation,57,84,experiments,"pretrained model ( raffel et al. , 2020 )",on,summary generation,"pretrained model ( raffel et al. , 2020 ) on summary generation",0.5118327736854553
translation,57,183,experiments,opo - sum +,trained,separate controller induction models,opo - sum + trained separate controller induction models,0.6988394856452942
translation,57,183,experiments,separate controller induction models,for,different domains,separate controller induction models for different domains,0.6029396057128906
translation,57,182,hyperparameters,controller induction model,with,learning rate,controller induction model with learning rate,0.603084146976471
translation,57,182,hyperparameters,learning rate,of,1e?4,learning rate of 1e?4,0.6287522315979004
translation,57,182,hyperparameters,learning rate,using,h = 12 heads,learning rate using h = 12 heads,0.675956130027771
translation,57,182,hyperparameters,1e?4,for,100k steps,1e?4 for 100k steps,0.6551005244255066
translation,57,182,hyperparameters,hyperparameters,trained,controller induction model,hyperparameters trained controller induction model,0.7369459867477417
translation,57,184,hyperparameters,aspect controllers,selected,10 - best keywords,aspect controllers selected 10 - best keywords,0.5920723080635071
translation,57,184,hyperparameters,500 tokens,to fit in,pretrained model,500 tokens to fit in pretrained model,0.599460780620575
translation,57,184,hyperparameters,aspect controllers,has,review sentences,aspect controllers has review sentences,0.6171729564666748
translation,57,184,hyperparameters,10 - best keywords,has,review sentences,10 - best keywords has review sentences,0.5517005920410156
translation,57,184,hyperparameters,hyperparameters,For,aspect controllers,hyperparameters For aspect controllers,0.5852280259132385
translation,57,185,hyperparameters,summarization,used,learning rate,summarization used learning rate,0.5421478152275085
translation,57,185,hyperparameters,summarization,used,500 k training steps,summarization used 500 k training steps,0.5378462076187134
translation,57,185,hyperparameters,learning rate,of,1e ? 6,learning rate of 1e ? 6,0.6273410320281982
translation,57,185,hyperparameters,learning rate,of,500 k training steps,learning rate of 500 k training steps,0.5707266330718994
translation,57,185,hyperparameters,hyperparameters,For,summarization,hyperparameters For summarization,0.5930113196372986
translation,57,186,hyperparameters,adam,with,"weight decay ( loshchilov and hutter , 2019 )","adam with weight decay ( loshchilov and hutter , 2019 )",0.643652081489563
translation,57,186,hyperparameters,"weight decay ( loshchilov and hutter , 2019 )",to optimize,both models,"weight decay ( loshchilov and hutter , 2019 ) to optimize both models",0.6941457390785217
translation,57,186,hyperparameters,hyperparameters,used,adam,hyperparameters used adam,0.6175551414489746
translation,57,187,hyperparameters,linear learning rate warm - up,for,first 10 k steps,linear learning rate warm - up for first 10 k steps,0.6150435209274292
translation,57,187,hyperparameters,hyperparameters,added,linear learning rate warm - up,hyperparameters added linear learning rate warm - up,0.6459735035896301
translation,57,188,hyperparameters,summaries,with,beam search,summaries with beam search,0.6724643707275391
translation,57,188,hyperparameters,beam search,has,of size 2,beam search has of size 2,0.6043633818626404
translation,57,188,hyperparameters,size,has,3,size has 3,0.672459602355957
translation,57,188,hyperparameters,hyperparameters,generate,summaries,hyperparameters generate summaries,0.6575161814689636
translation,57,5,model,generation of customized summaries,based on,aspect queries,generation of customized summaries based on aspect queries,0.6143638491630554
translation,57,101,model,effective method,to create,synthetic datasets,effective method to create synthetic datasets,0.6530261635780334
translation,57,101,model,synthetic datasets,for,aspect-guided opinion summarization,synthetic datasets for aspect-guided opinion summarization,0.5380146503448486
translation,57,101,model,model,introduce,effective method,model introduce effective method,0.636439859867096
translation,57,210,results,acesum,performs,best,acesum performs best,0.5367536544799805
translation,57,210,results,acesum,shows that,our controllers,acesum shows that our controllers,0.7577796578407288
translation,57,210,results,best,across,datasets and metrics,best across datasets and metrics,0.6651722192764282
translation,57,210,results,best,shows that,our controllers,best shows that our controllers,0.7047582864761353
translation,57,210,results,datasets and metrics,shows that,our controllers,datasets and metrics shows that our controllers,0.6797393560409546
translation,57,210,results,summaries,based on,aspect queries,summaries based on aspect queries,0.6210648417472839
translation,57,210,results,effectively customize,has,summaries,effectively customize has summaries,0.5132780075073242
translation,57,210,results,results,has,acesum,results has acesum,0.5584033727645874
translation,57,211,results,acesumext,performs,best,acesumext performs best,0.6469610929489136
translation,57,211,results,extractive models,has,acesumext,extractive models has acesumext,0.5617623329162598
translation,57,211,results,results,amongst,extractive models,results amongst extractive models,0.5346030592918396
translation,57,213,results,t5 models,perform,substantially worse,t5 models perform substantially worse,0.6248186826705933
translation,57,213,results,results,has,t5 models,results has t5 models,0.5214856266975403
translation,57,222,results,"multiple experts ( i.e. , attention heads )",yields,better aspect predictions,"multiple experts ( i.e. , attention heads ) yields better aspect predictions",0.6880601644515991
translation,57,222,results,results,using,"multiple experts ( i.e. , attention heads )","results using multiple experts ( i.e. , attention heads )",0.575578510761261
translation,57,224,results,selecting sentences randomly,based on,aspect,selecting sentences randomly based on aspect,0.6821882724761963
translation,57,224,results,aspect,has,hurts,aspect has hurts,0.6172550916671753
translation,57,224,results,hurts,has,performance,hurts has performance,0.6043882966041565
translation,57,224,results,results,has,selecting sentences randomly,results has selecting sentences randomly,0.5565446019172668
translation,57,225,results,model performance,in,oposum +,model performance in oposum +,0.5631836652755737
translation,57,225,results,aspect codes,has,substantially increase,aspect codes has substantially increase,0.5905815362930298
translation,57,225,results,substantially increase,has,model performance,substantially increase has model performance,0.5414441823959351
translation,57,225,results,results,find,aspect codes,results find aspect codes,0.6009039282798767
translation,57,237,results,high,on,informativeness,high on informativeness,0.5104635953903198
translation,57,237,results,qt,has,high,qt has high,0.601288914680481
translation,57,244,results,t5 - similar,mostly produces,general summaries,t5 - similar mostly produces general summaries,0.719780445098877
translation,57,244,results,general summaries,partially discuss,given aspect,general summaries partially discuss given aspect,0.6793266534805298
translation,57,244,results,74.8 %,partially discuss,given aspect,74.8 % partially discuss given aspect,0.6263802647590637
translation,57,244,results,general summaries,has,74.8 %,general summaries has 74.8 %,0.5207422375679016
translation,57,244,results,results,has,t5 - similar,results has t5 - similar,0.5663262605667114
translation,57,246,results,automatic systems,perform,worse,automatic systems perform worse,0.5991523265838623
translation,57,246,results,worse,on,opo - sum +,worse on opo - sum +,0.5928969383239746
translation,57,246,results,results,has,automatic systems,results has automatic systems,0.600107729434967
translation,57,247,results,big gap,between,model and human performance,big gap between model and human performance,0.604218065738678
translation,57,247,results,results,observe,big gap,results observe big gap,0.6161162853240967
translation,57,265,results,gold summaries,highest percentage of,fully supported sentences,gold summaries highest percentage of fully supported sentences,0.6744303107261658
translation,57,265,results,fully supported sentences,for,general and aspectspecific summaries,fully supported sentences for general and aspectspecific summaries,0.5968587398529053
translation,57,265,results,results,has,gold summaries,results has gold summaries,0.5430855751037598
translation,57,266,results,acesum and t5 - similar,present,similar proportions,acesum and t5 - similar present similar proportions,0.6825581789016724
translation,57,266,results,similar proportions,of,supported sentences,similar proportions of supported sentences,0.6312349438667297
translation,57,266,results,supported sentences,when it comes to,general summaries,supported sentences when it comes to general summaries,0.6559656262397766
translation,57,266,results,ace - sum,having,slight advantage,ace - sum having slight advantage,0.6601647734642029
translation,57,266,results,results,has,acesum and t5 - similar,results has acesum and t5 - similar,0.5563063621520996
translation,57,280,results,our model,performs,better,our model performs better,0.6546649932861328
translation,57,280,results,better,than,qt,better than qt,0.6015453338623047
translation,57,280,results,qt,on,both datasets,qt on both datasets,0.5705655217170715
translation,57,280,results,results,has,our model,results has our model,0.5871725678443909
translation,58,264,ablation-analysis,ablation analysis,use of,rating deviation,ablation analysis use of rating deviation,0.7036848068237305
translation,58,165,baselines,clustroid,selects,one review,clustroid selects one review,0.7175610661506653
translation,58,165,baselines,one review,gets,highest rouge -l score,one review gets highest rouge -l score,0.6326984167098999
translation,58,165,baselines,highest rouge -l score,with,other reviews,highest rouge -l score with other reviews,0.5731369853019714
translation,58,165,baselines,other reviews,of,entity,other reviews of entity,0.5884393453598022
translation,58,165,baselines,baselines,has,clustroid,baselines has clustroid,0.5923356413841248
translation,58,173,baselines,"denoisesum ( amplayo and lapata , 2020 )",generates,summary,"denoisesum ( amplayo and lapata , 2020 ) generates summary",0.6221714019775391
translation,58,173,baselines,summary,by denoising,source reviews,summary by denoising source reviews,0.6690464615821838
translation,58,173,baselines,baselines,has,"denoisesum ( amplayo and lapata , 2020 )","baselines has denoisesum ( amplayo and lapata , 2020 )",0.5979518890380859
translation,58,174,baselines,copycat,uses,hierarchical variational autoencoder model,copycat uses hierarchical variational autoencoder model,0.5130742788314819
translation,58,174,baselines,copycat,generates,summary,copycat generates summary,0.6979002952575684
translation,58,174,baselines,"titov , 2020 )",uses,hierarchical variational autoencoder model,"titov , 2020 ) uses hierarchical variational autoencoder model",0.5470798015594482
translation,58,174,baselines,summary,from,mean latent codes,summary from mean latent codes,0.5624246597290039
translation,58,174,baselines,mean latent codes,of,source reviews,mean latent codes of source reviews,0.6026447415351868
translation,58,174,baselines,baselines,has,copycat,baselines has copycat,0.6126583218574524
translation,58,152,experimental-setup,transformers library,from,"hugging face ( wolf et al. , 2020 )","transformers library from hugging face ( wolf et al. , 2020 )",0.5855118036270142
translation,58,152,experimental-setup,transformers library,as,backbone skeleton,transformers library as backbone skeleton,0.5147696733474731
translation,58,152,experimental-setup,"hugging face ( wolf et al. , 2020 )",as,backbone skeleton,"hugging face ( wolf et al. , 2020 ) as backbone skeleton",0.4709811508655548
translation,58,152,experimental-setup,experimental setup,used,transformers library,experimental setup used transformers library,0.6319246292114258
translation,58,153,experimental-setup,text encoder and decoder,initialized using,bart - large,text encoder and decoder initialized using bart - large,0.7174711227416992
translation,58,153,experimental-setup,text encoder and decoder,further pretrained using,training review corpus,text encoder and decoder further pretrained using training review corpus,0.7424888610839844
translation,58,153,experimental-setup,training review corpus,with,same objective,training review corpus with same objective,0.6213813424110413
translation,58,153,experimental-setup,experimental setup,has,text encoder and decoder,experimental setup has text encoder and decoder,0.5432509779930115
translation,58,155,experimental-setup,entire models,using,adam optimizer,entire models using adam optimizer,0.6477835774421692
translation,58,155,experimental-setup,adam optimizer,),linear learning rate decay,adam optimizer ) linear learning rate decay,0.5286591053009033
translation,58,155,experimental-setup,adam optimizer,with,linear learning rate decay,adam optimizer with linear learning rate decay,0.5811677575111389
translation,58,155,experimental-setup,linear learning rate decay,on,nvidia v100s,linear learning rate decay on nvidia v100s,0.511376678943634
translation,58,155,experimental-setup,experimental setup,trained,entire models,experimental setup trained entire models,0.7454808950424194
translation,58,156,experimental-setup,model weights,with,0.1,model weights with 0.1,0.5966885685920715
translation,58,156,experimental-setup,experimental setup,decayed,model weights,experimental setup decayed model weights,0.7181854248046875
translation,58,157,experimental-setup,pipeline,set,"different batch sizes , epochs , learning rates","pipeline set different batch sizes , epochs , learning rates",0.6296854615211487
translation,58,158,experimental-setup,label smoothing,with,0.1,label smoothing with 0.1,0.6070365309715271
translation,58,158,experimental-setup,label smoothing,set,maximum norm,label smoothing set maximum norm,0.6359844207763672
translation,58,158,experimental-setup,maximum norm,of,gradients,maximum norm of gradients,0.6041646599769592
translation,58,158,experimental-setup,gradients,as,1,gradients as 1,0.6615253686904907
translation,58,158,experimental-setup,1,for,other modalities pretraining,1 for other modalities pretraining,0.5754376649856567
translation,58,158,experimental-setup,1,for,multiple -modalities training,1 for multiple -modalities training,0.5403795838356018
translation,58,158,experimental-setup,experimental setup,used,label smoothing,experimental setup used label smoothing,0.5488037467002869
translation,58,159,experimental-setup,testing,used,beam search,testing used beam search,0.5991323590278625
translation,58,159,experimental-setup,testing,discarded,hypotheses,testing discarded hypotheses,0.6342913508415222
translation,58,159,experimental-setup,beam search,with,early stopping,beam search with early stopping,0.6488112807273865
translation,58,159,experimental-setup,beam search,discarded,hypotheses,beam search discarded hypotheses,0.6967085599899292
translation,58,159,experimental-setup,hypotheses,that contain,twice the same trigram,hypotheses that contain twice the same trigram,0.6482878923416138
translation,58,159,experimental-setup,experimental setup,During,testing,experimental setup During testing,0.6925565600395203
translation,58,74,experiments,imagenet pretrained resnet101,widely used as,backbone network,imagenet pretrained resnet101 widely used as backbone network,0.6676854491233826
translation,58,169,experiments,abstractive models,used,non-neural and neural models,abstractive models used non-neural and neural models,0.6266478896141052
translation,58,311,hyperparameters,image encoder,based on,resnet101,image encoder based on resnet101,0.5872801542282104
translation,58,311,hyperparameters,hyperparameters,has,image encoder,hyperparameters has image encoder,0.520417332649231
translation,58,325,hyperparameters,batch size,according to,memory usage,batch size according to memory usage,0.5890190005302429
translation,58,325,hyperparameters,other values,according to,amount of learning required,other values according to amount of learning required,0.641362726688385
translation,58,325,hyperparameters,hyperparameters,set,batch size,hyperparameters set batch size,0.6402808427810669
translation,58,325,hyperparameters,hyperparameters,set,other values,hyperparameters set other values,0.6424607634544373
translation,58,325,hyperparameters,hyperparameters,set,other values,hyperparameters set other values,0.6424607634544373
translation,58,326,hyperparameters,ranges,for,epochs and lr ( learning rate ),ranges for epochs and lr ( learning rate ),0.6863218545913696
translation,58,326,hyperparameters,epochs and lr ( learning rate ),were,"[ 3 , 5 , 10 , 15 , 20 ]","epochs and lr ( learning rate ) were [ 3 , 5 , 10 , 15 , 20 ]",0.556888222694397
translation,58,326,hyperparameters,hyperparameters,for,epochs and lr ( learning rate ),hyperparameters for epochs and lr ( learning rate ),0.6104675531387329
translation,58,6,model,self-supervised multimodal opinion summarization framework,called,mul-timodalsum,self-supervised multimodal opinion summarization framework called mul-timodalsum,0.6409217119216919
translation,58,6,model,model,propose,self-supervised multimodal opinion summarization framework,model propose self-supervised multimodal opinion summarization framework,0.6217877864837646
translation,58,7,model,our framework,obtains,representation,our framework obtains representation,0.6458845734596252
translation,58,7,model,representation,of,each modality,representation of each modality,0.5998579859733582
translation,58,7,model,representation,using,separate encoder,representation using separate encoder,0.7119072079658508
translation,58,7,model,each modality,using,separate encoder,each modality using separate encoder,0.7035350203514099
translation,58,7,model,separate encoder,for,each modality,separate encoder for each modality,0.6417656540870667
translation,58,7,model,text decoder,generates,summary,text decoder generates summary,0.6794502139091492
translation,58,7,model,model,has,our framework,model has our framework,0.566365122795105
translation,58,8,model,inherent heterogeneity,of,multimodal data,inherent heterogeneity of multimodal data,0.5529114007949829
translation,58,8,model,inherent heterogeneity,propose,multimodal training pipeline,inherent heterogeneity propose multimodal training pipeline,0.6216957569122314
translation,58,8,model,model,To resolve,inherent heterogeneity,model To resolve inherent heterogeneity,0.7231311798095703
translation,58,9,model,text encoder-decoder,based solely on,text modality data,text encoder-decoder based solely on text modality data,0.5995238423347473
translation,58,9,model,model,pretrain,text encoder-decoder,model pretrain text encoder-decoder,0.7309259176254272
translation,58,10,model,non-text modality encoders,by considering,pretrained text decoder,non-text modality encoders by considering pretrained text decoder,0.6577845215797424
translation,58,10,model,pretrained text decoder,as,pivot,pretrained text decoder as pivot,0.5166766047477722
translation,58,10,model,pivot,for,homogeneous representation,pivot for homogeneous representation,0.62373286485672
translation,58,10,model,homogeneous representation,of,multimodal data,homogeneous representation of multimodal data,0.5721769332885742
translation,58,10,model,model,pretrain,non-text modality encoders,model pretrain non-text modality encoders,0.6608430743217468
translation,58,11,model,multimodal representations,train,entire framework,multimodal representations train entire framework,0.6348636150360107
translation,58,11,model,entire framework,in,end-to - end manner,entire framework in end-to - end manner,0.557025671005249
translation,58,11,model,model,to fuse,multimodal representations,model to fuse multimodal representations,0.6743728518486023
translation,58,22,model,our framework,receives,"source reviews , images , and a table","our framework receives source reviews , images , and a table",0.6455616354942322
translation,58,22,model,our framework,generates,pseudo summary,our framework generates pseudo summary,0.6670388579368591
translation,58,22,model,"source reviews , images , and a table",on,specific business or product,"source reviews , images , and a table on specific business or product",0.5282105803489685
translation,58,22,model,"source reviews , images , and a table",as,input,"source reviews , images , and a table as input",0.5184838175773621
translation,58,22,model,model,has,our framework,model has our framework,0.566365122795105
translation,58,127,model,multi-modality fusion,applied to,multi-head self-attention layer,multi-modality fusion applied to multi-head self-attention layer,0.6413271427154541
translation,58,127,model,multi-head self-attention layer,of,text decoder,multi-head self-attention layer of text decoder,0.5361608862876892
translation,58,312,model,resnet101,composed of,1 convolution layer,resnet101 composed of 1 convolution layer,0.6369472146034241
translation,58,312,model,resnet101,composed of,4 convolution layer blocks,resnet101 composed of 4 convolution layer blocks,0.6724626421928406
translation,58,312,model,resnet101,composed of,1 fully connected layer block,resnet101 composed of 1 fully connected layer block,0.6511875987052917
translation,58,312,model,model,has,resnet101,model has resnet101,0.5749613046646118
translation,58,186,results,opinion summarization,on,two datasets,opinion summarization on two datasets,0.4737815260887146
translation,58,186,results,multimodalsum,showed,superior results,multimodalsum showed superior results,0.6981332898139954
translation,58,186,results,superior results,compared with,extractive and abstractive baselines,superior results compared with extractive and abstractive baselines,0.7210560441017151
translation,58,186,results,extractive and abstractive baselines,for,token - level and document- level measures,extractive and abstractive baselines for token - level and document- level measures,0.576565682888031
translation,58,186,results,results,for,opinion summarization,results for opinion summarization,0.5738664269447327
translation,58,232,results,"multimodalsum , and gold summaries",scored,"- 0.527 , -0.113 , +0.260 , and + 0.380","multimodalsum , and gold summaries scored - 0.527 , -0.113 , +0.260 , and + 0.380",0.6695602536201477
translation,58,232,results,"- 0.527 , -0.113 , +0.260 , and + 0.380",on,yelp dataset,"- 0.527 , -0.113 , +0.260 , and + 0.380 on yelp dataset",0.536191999912262
translation,58,232,results,overall criterion,has,self & control,overall criterion has self & control,0.5262027382850647
translation,58,232,results,overall criterion,has,"multimodalsum , and gold summaries","overall criterion has multimodalsum , and gold summaries",0.5760537981987
translation,58,232,results,results,For,overall criterion,results For overall criterion,0.6406630873680115
translation,58,233,results,multimodalsum,showed,superior performance,multimodalsum showed superior performance,0.7178496718406677
translation,58,233,results,superior performance,in,human evaluation,superior performance in human evaluation,0.5058810710906982
translation,58,233,results,results,has,multimodalsum,results has multimodalsum,0.5606602430343628
translation,58,260,results,bart,achieved,comparable or better results,bart achieved comparable or better results,0.6948220729827881
translation,58,260,results,comparable or better results,than,many extractive and abstractive baselines,comparable or better results than many extractive and abstractive baselines,0.5454934239387512
translation,58,260,results,further pretraining,using,review corpus,further pretraining using review corpus,0.7055092453956604
translation,58,260,results,review corpus,brought,performance improvements,review corpus brought performance improvements,0.6318132877349854
translation,58,261,results,bart,with,further pretraining,bart with further pretraining,0.7215831279754639
translation,58,261,results,bart,generated,rich expressions,bart generated rich expressions,0.6986684799194336
translation,58,261,results,further pretraining,generated,more diverse words,further pretraining generated more diverse words,0.6925439238548279
translation,58,261,results,further pretraining,generated,rich expressions,further pretraining generated rich expressions,0.653647243976593
translation,58,261,results,rich expressions,from,review corpus,rich expressions from review corpus,0.49293088912963867
translation,58,261,results,results,has,bart,results has bart,0.40022552013397217
translation,58,263,results,unimodalsum,achieved,superior results,unimodalsum achieved superior results,0.7146972417831421
translation,58,263,results,bart - review,has,unimodalsum,bart - review has unimodalsum,0.634169340133667
translation,58,263,results,results,on,bart - review,results on bart - review,0.5484408140182495
translation,58,265,results,learning,to generate,reviews,learning to generate reviews,0.6882002353668213
translation,58,265,results,reviews,based on,wide ranges of rating deviations,reviews based on wide ranges of rating deviations,0.7069584727287292
translation,58,265,results,reviews,helps to generate,better summary,reviews helps to generate better summary,0.7182380557060242
translation,58,265,results,wide ranges of rating deviations,including,0,wide ranges of rating deviations including 0,0.727573573589325
translation,58,265,results,0,during,training,0 during training,0.7665526866912842
translation,58,265,results,better summary,of,average semantics,better summary of average semantics,0.5615373849868774
translation,58,265,results,average semantics,of,input reviews,average semantics of input reviews,0.5819334983825684
translation,58,265,results,results,conclude,learning,results conclude learning,0.6255870461463928
translation,58,268,results,both modalities,improved,summarization quality,both modalities improved summarization quality,0.6969569325447083
translation,58,268,results,both modalities,brought,additional improvements,both modalities brought additional improvements,0.6522094011306763
translation,58,268,results,summarization quality,compared with,unimodalsum,summarization quality compared with unimodalsum,0.6393007636070251
translation,58,268,results,results,showed,both modalities,results showed both modalities,0.6254428625106812
translation,58,270,results,utility,of,table modality,utility of table modality,0.638316810131073
translation,58,270,results,table modality,higher than,image modality,table modality higher than image modality,0.6736119389533997
translation,58,270,results,results,has,utility,results has utility,0.5101466178894043
translation,58,277,results,multimodalsum without text modality pretraining,whose,image and table encoders,multimodalsum without text modality pretraining whose image and table encoders,0.6111065149307251
translation,58,277,results,image and table encoders,pretrained using,bart - review,image and table encoders pretrained using bart - review,0.6812383532524109
translation,58,277,results,results,has,multimodalsum without text modality pretraining,results has multimodalsum without text modality pretraining,0.5652766823768616
translation,58,329,results,self & control,achieved,very poor performance,self & control achieved very poor performance,0.6803141832351685
translation,58,329,results,very poor performance,for,all criteria,very poor performance for all criteria,0.6173492074012756
translation,58,329,results,very poor performance,due to,flaws,very poor performance due to flaws,0.669428825378418
translation,58,330,results,gold summaries,for,two criteria,gold summaries for two criteria,0.6608501672744751
translation,58,330,results,multimodalsum,has,outperformed,multimodalsum has outperformed,0.6458661556243896
translation,58,330,results,outperformed,has,gold summaries,outperformed has gold summaries,0.6155993938446045
translation,58,330,results,lagged behind,has,gold,lagged behind has gold,0.5977112650871277
translation,58,330,results,results,has,multimodalsum,results has multimodalsum,0.5606602430343628
translation,58,342,results,triplet,showed,good performance,triplet showed good performance,0.7010502815246582
translation,58,342,results,good performance,on,table,good performance on table,0.5404648184776306
translation,58,342,results,results,has,triplet,results has triplet,0.41789573431015015
translation,58,506,results,first finding,results of,table,first finding results of table,0.6807128190994263
translation,58,506,results,outperformed,of,image,outperformed of image,0.6474781632423401
translation,58,506,results,table,has,outperformed,table has outperformed,0.6899327635765076
translation,58,506,results,results,has,first finding,results has first finding,0.5171782374382019
translation,58,508,results,our method,based on,text decoder,our method based on text decoder,0.6439891457557678
translation,58,508,results,our method,based on,text encoder,our method based on text encoder,0.6091631054878235
translation,58,508,results,triplet,based on,text encoder,triplet based on text encoder,0.6762084364891052
translation,58,508,results,text decoder,has,outperformed,text decoder has outperformed,0.6226096153259277
translation,58,508,results,outperformed,has,triplet,outperformed has triplet,0.6500336527824402
translation,59,16,ablation-analysis,partial masking,found to be,most effective,partial masking found to be most effective,0.6210607290267944
translation,59,16,ablation-analysis,most effective,indicating,strong collaborative effect,most effective indicating strong collaborative effect,0.6727323532104492
translation,59,16,ablation-analysis,ablation analysis,has,partial masking,ablation analysis has partial masking,0.5569784045219421
translation,59,66,ablation-analysis,top layer,yields,most rouge - 1 improvement,top layer yields most rouge - 1 improvement,0.6641489267349243
translation,59,66,ablation-analysis,oracle masking,has,top layer,oracle masking has top layer,0.6035050749778748
translation,59,66,ablation-analysis,ablation analysis,with,oracle masking,ablation analysis with oracle masking,0.6360286474227905
translation,59,71,ablation-analysis,most rouge - 1 improvement,achieved by,masking,most rouge - 1 improvement achieved by masking,0.7453569769859314
translation,59,71,ablation-analysis,15 ( out of 16 ) heads,at,top layer,15 ( out of 16 ) heads at top layer,0.5584145188331604
translation,59,71,ablation-analysis,masking,has,15 ( out of 16 ) heads,masking has 15 ( out of 16 ) heads,0.5921461582183838
translation,59,71,ablation-analysis,ablation analysis,has,most rouge - 1 improvement,ablation analysis has most rouge - 1 improvement,0.5563177466392517
translation,59,40,experiments,large pre-trained sequence-to-sequence transformer models,for,abstractive summarization,large pre-trained sequence-to-sequence transformer models for abstractive summarization,0.5210941433906555
translation,59,4,model,simple - yet-effective attention head masking technique,applied on,encoderdecoder attentions,simple - yet-effective attention head masking technique applied on encoderdecoder attentions,0.6947156190872192
translation,59,4,model,encoderdecoder attentions,to pinpoint,salient content,encoderdecoder attentions to pinpoint salient content,0.6950696706771851
translation,59,4,model,model,present,simple - yet-effective attention head masking technique,model present simple - yet-effective attention head masking technique,0.6530430316925049
translation,59,13,model,inference -time attention head masking mechanism,works on,encoder-decoder attentions,inference -time attention head masking mechanism works on encoder-decoder attentions,0.7274987101554871
translation,59,13,model,encoder-decoder attentions,to underscore,salient content,encoder-decoder attentions to underscore salient content,0.6474859714508057
translation,59,13,model,encoder-decoder attentions,improve,quality of abstractive summaries,encoder-decoder attentions improve quality of abstractive summaries,0.6709617972373962
translation,59,13,model,salient content,from,source,salient content from source,0.532175600528717
translation,59,13,model,model,propose,inference -time attention head masking mechanism,model propose inference -time attention head masking mechanism,0.6147961616516113
translation,59,31,model,content selection,modeled as,extra component,content selection modeled as extra component,0.6444236040115356
translation,59,31,model,content selection,learned within,end-to - end trained model,content selection learned within end-to - end trained model,0.6635481715202332
translation,59,31,model,propagation of selection errors,has,content selection,propagation of selection errors has content selection,0.5561710000038147
translation,59,31,model,model,minimize,propagation of selection errors,model minimize propagation of selection errors,0.762718677520752
translation,59,58,model,attention head masking,based on,oracle content selection labels,attention head masking based on oracle content selection labels,0.6384059190750122
translation,59,58,model,model,apply,attention head masking,model apply attention head masking,0.6383440494537354
translation,59,18,results,consistently outperforms,has,"fine- tuned bart ( lewis et al. , 2020 )","consistently outperforms has fine- tuned bart ( lewis et al. , 2020 )",0.5752878189086914
translation,59,19,results,summaries,generated by,our model,summaries generated by our model,0.6668679714202881
translation,59,19,results,summaries,considered to have,better informativeness,summaries considered to have better informativeness,0.5644115805625916
translation,59,19,results,better informativeness,by,human judges,better informativeness by human judges,0.5499072670936584
translation,59,19,results,results,has,summaries,results has summaries,0.5243220329284668
translation,59,22,results,content selector,trained on,nyt,content selector trained on nyt,0.7003936171531677
translation,59,22,results,bart,fine-tuned on,cnn / dm,bart fine-tuned on cnn / dm,0.7651085257530212
translation,59,22,results,cnn / dm,gains,more than three points,cnn / dm gains more than three points,0.7656275629997253
translation,59,22,results,more than three points,of,rouge scores,more than three points of rouge scores,0.5815377831459045
translation,59,22,results,more than three points,tested on,nyt articles,more than three points tested on nyt articles,0.7150812745094299
translation,59,22,results,content selector,has,bart,content selector has bart,0.5156025290489197
translation,59,22,results,nyt,has,bart,nyt has bart,0.6542715430259705
translation,59,22,results,results,With,content selector,results With content selector,0.5664793252944946
translation,59,95,results,final decision boundaries,used for,taggers,final decision boundaries used for taggers,0.6958621740341187
translation,59,95,results,taggers,trained on,"cnn / dm , nyt , xsum","taggers trained on cnn / dm , nyt , xsum",0.735235333442688
translation,59,95,results,taggers,achieving,rouge -1 f1,taggers achieving rouge -1 f1,0.6248904466629028
translation,59,95,results,"cnn / dm , nyt , xsum",are,"0.20 , 0.24 , and 0.18","cnn / dm , nyt , xsum are 0.20 , 0.24 , and 0.18",0.581788182258606
translation,59,95,results,rouge -1 f1,of,"43.70 , 44.10 , and 31.56","rouge -1 f1 of 43.70 , 44.10 , and 31.56",0.5691501498222351
translation,59,95,results,results,has,final decision boundaries,results has final decision boundaries,0.5158413052558899
translation,59,100,results,our attention head masking technique,on,bart,our attention head masking technique on bart,0.6442034244537354
translation,59,100,results,our attention head masking technique,obtains,significantly better results,our attention head masking technique obtains significantly better results,0.621508777141571
translation,59,100,results,significantly better results,on,cnn / dm and nyt,significantly better results on cnn / dm and nyt,0.5635682940483093
translation,59,100,results,significantly better results,compared to,several top performing abstractive summarization models,significantly better results compared to several top performing abstractive summarization models,0.6043118834495544
translation,59,100,results,several top performing abstractive summarization models,trained with,large transformers,several top performing abstractive summarization models trained with large transformers,0.7288332581520081
translation,59,100,results,results,shows,our attention head masking technique,results shows our attention head masking technique,0.6319869160652161
translation,59,100,results,results,applying,our attention head masking technique,results applying our attention head masking technique,0.6663132309913635
translation,59,101,results,more pronounced,for,cnn / dm,more pronounced for cnn / dm,0.644105076789856
translation,59,101,results,cnn / dm,than,other two datasets,cnn / dm than other two datasets,0.5285934209823608
translation,59,101,results,results,has,improvement,results has improvement,0.6248279809951782
translation,59,103,results,more extractive summaries,compared to,other datasets,more extractive summaries compared to other datasets,0.6382555365562439
translation,59,103,results,cnn / dm,has,more extractive summaries,cnn / dm has more extractive summaries,0.5789437890052795
translation,59,103,results,results,has,cnn / dm,results has cnn / dm,0.5689362287521362
translation,59,109,results,summaries,generated with,attention head masking,summaries generated with attention head masking,0.6259714961051941
translation,59,109,results,attention head masking,considered to have,better informativeness,attention head masking considered to have better informativeness,0.5643795728683472
translation,59,109,results,substantial improvement,on,faithfulness,substantial improvement on faithfulness,0.5386720895767212
translation,59,113,results,our masking technique,consistently increases,rouge scores,our masking technique consistently increases rouge scores,0.8186953067779541
translation,59,113,results,rouge scores,with,varying amounts of training data,rouge scores with varying amounts of training data,0.5931889414787292
translation,59,114,results,our model,trained on,only 30 k samples,our model trained on only 30 k samples,0.7414975166320801
translation,59,114,results,only 30 k samples,with,attention head masking,only 30 k samples with attention head masking,0.6544603109359741
translation,59,114,results,model,trained on,full dataset,model trained on full dataset,0.7324057817459106
translation,59,114,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,59,114,results,attention head masking,has,outperforms,attention head masking has outperforms,0.583614706993103
translation,59,114,results,outperforms,has,model,outperforms has model,0.6832752227783203
translation,59,114,results,results,has,our model,results has our model,0.5871725678443909
translation,59,117,results,selector,only 10k of,target domain samples,selector only 10k of target domain samples,0.741285502910614
translation,59,117,results,performance,by,bart,performance by bart,0.6438637971878052
translation,59,117,results,bart,trained on,cnn / dm only,bart trained on cnn / dm only,0.7830868363380432
translation,59,117,results,significantly improve,has,performance,significantly improve has performance,0.5721634030342102
translation,59,117,results,results,Using,selector,results Using selector,0.6869022250175476
translation,60,135,ablation-analysis,explicit relations,between,phrases,explicit relations between phrases,0.6990984082221985
translation,60,135,ablation-analysis,explicit relations,has,r - 1 metric,explicit relations has r - 1 metric,0.5701034665107727
translation,60,135,ablation-analysis,r - 1 metric,has,drops obviously,r - 1 metric has drops obviously,0.6436772346496582
translation,60,135,ablation-analysis,ablation analysis,after removing,explicit relations,ablation analysis after removing explicit relations,0.7164912819862366
translation,60,137,ablation-analysis,phrase merging,observe,performance decrease,phrase merging observe performance decrease,0.5914055109024048
translation,60,137,ablation-analysis,performance decrease,in,all the metrics,performance decrease in all the metrics,0.509615957736969
translation,60,137,ablation-analysis,performance decrease,indicates,long-distance relations,performance decrease indicates long-distance relations,0.6300921440124512
translation,60,137,ablation-analysis,long-distance relations,benefit,informativeness,long-distance relations benefit informativeness,0.6993651390075684
translation,60,137,ablation-analysis,ablation analysis,After further removing,phrase merging,ablation analysis After further removing phrase merging,0.6507651805877686
translation,60,139,ablation-analysis,supernode and shortcut edges,from,unified semantic graph,supernode and shortcut edges from unified semantic graph,0.523431658744812
translation,60,139,ablation-analysis,supernode and shortcut edges,prove,effectiveness,supernode and shortcut edges prove effectiveness,0.607667088508606
translation,60,139,ablation-analysis,effectiveness,of,graph augmentation methods,effectiveness of graph augmentation methods,0.5718464255332947
translation,60,139,ablation-analysis,graph augmentation methods,in,graph encoder,graph augmentation methods in graph encoder,0.4893222749233246
translation,60,139,ablation-analysis,ablation analysis,removing,supernode and shortcut edges,ablation analysis removing supernode and shortcut edges,0.7673879861831665
translation,60,140,ablation-analysis,experimental results,without,gaph-propagation attention,experimental results without gaph-propagation attention,0.7467862963676453
translation,60,140,ablation-analysis,gaph-propagation attention,confirms,structure,gaph-propagation attention confirms structure,0.6974227428436279
translation,60,140,ablation-analysis,structure,of,unified semantic graph,structure of unified semantic graph,0.5252766013145447
translation,60,140,ablation-analysis,structure,beneficial for,decoding,structure beneficial for decoding,0.7538604140281677
translation,60,140,ablation-analysis,ablation analysis,without,gaph-propagation attention,ablation analysis without gaph-propagation attention,0.762470006942749
translation,60,140,ablation-analysis,ablation analysis,has,experimental results,ablation analysis has experimental results,0.5178031325340271
translation,60,141,ablation-analysis,performance,of,model,performance of model,0.6080846190452576
translation,60,141,ablation-analysis,drops the most,when removing,shortcut edges,drops the most when removing shortcut edges,0.7831543684005737
translation,60,141,ablation-analysis,model,has,drops the most,model has drops the most,0.6319295763969421
translation,60,141,ablation-analysis,ablation analysis,has,performance,ablation analysis has performance,0.5053174495697021
translation,60,142,ablation-analysis,dramatically drops,on,all the metrics,dramatically drops on all the metrics,0.5426309108734131
translation,60,142,ablation-analysis,all the graph-relevant components,has,performance,all the graph-relevant components has performance,0.597317636013031
translation,60,142,ablation-analysis,performance,has,dramatically drops,performance has dramatically drops,0.6219179034233093
translation,60,142,ablation-analysis,ablation analysis,after removing,all the graph-relevant components,ablation analysis after removing all the graph-relevant components,0.7576873302459717
translation,60,144,ablation-analysis,input length,affects,summarization performance,input length affects summarization performance,0.6745373606681824
translation,60,144,ablation-analysis,summarization performance,for,seq2seq models,summarization performance for seq2seq models,0.5787702798843384
translation,60,144,ablation-analysis,seriously,for,seq2seq models,seriously for seq2seq models,0.6558198928833008
translation,60,144,ablation-analysis,summarization performance,has,seriously,summarization performance has seriously,0.5583224296569824
translation,60,149,ablation-analysis,significantly increased,in,3000,significantly increased in 3000,0.6066650748252869
translation,60,149,ablation-analysis,3000,indicates that,unified semantic graph,3000 indicates that unified semantic graph,0.6547669768333435
translation,60,149,ablation-analysis,unified semantic graph,benefits,salient information selection,unified semantic graph benefits salient information selection,0.5346434712409973
translation,60,149,ablation-analysis,rouge - 1 of bass,has,significantly increased,rouge - 1 of bass has significantly increased,0.6161002516746521
translation,60,59,experimental-setup,local features,in,sequence,local features in sequence,0.5352402329444885
translation,60,59,experimental-setup,local features,apply,pre-trained language model roberta,local features apply pre-trained language model roberta,0.5497751832008362
translation,60,59,experimental-setup,pre-trained language model roberta,as,text encoder,pre-trained language model roberta as text encoder,0.4974948763847351
translation,60,59,experimental-setup,experimental setup,To better represent,local features,experimental setup To better represent local features,0.6531546115875244
translation,60,64,experimental-setup,node representations,in,graph,node representations in graph,0.5601242184638977
translation,60,64,experimental-setup,node representations,based on,token representations,node representations based on token representations,0.6793670058250427
translation,60,64,experimental-setup,node representations,based on,token - to - node alignment information,node representations based on token - to - node alignment information,0.6357086300849915
translation,60,64,experimental-setup,token - to - node alignment information,from,graph construction,token - to - node alignment information from graph construction,0.5507524609565735
translation,60,64,experimental-setup,experimental setup,initialize,node representations,experimental setup initialize node representations,0.7763590216636658
translation,60,119,experimental-setup,all the abstractive models,by,max likelihood estimation,all the abstractive models by max likelihood estimation,0.5591425895690918
translation,60,119,experimental-setup,max likelihood estimation,with,label smoothing,max likelihood estimation with label smoothing,0.5998967885971069
translation,60,119,experimental-setup,experimental setup,train,all the abstractive models,experimental setup train all the abstractive models,0.6585022807121277
translation,60,120,experimental-setup,pretrained language model roberta,as,text encoder,pretrained language model roberta as text encoder,0.48125213384628296
translation,60,120,experimental-setup,pretrained language model roberta,apply,"two different adam optimizers ( kingma and ba , 2015 )","pretrained language model roberta apply two different adam optimizers ( kingma and ba , 2015 )",0.569944441318512
translation,60,120,experimental-setup,"two different adam optimizers ( kingma and ba , 2015 )",with,? 1 = 0.9 and ? 2 = 0.998,"two different adam optimizers ( kingma and ba , 2015 ) with ? 1 = 0.9 and ? 2 = 0.998",0.6324870586395264
translation,60,120,experimental-setup,? 1 = 0.9 and ? 2 = 0.998,to train,pre-trained part,? 1 = 0.9 and ? 2 = 0.998 to train pre-trained part,0.7474261522293091
translation,60,120,experimental-setup,experimental setup,fine- tune,pretrained language model roberta,experimental setup fine- tune pretrained language model roberta,0.6625057458877563
translation,60,120,experimental-setup,experimental setup,apply,"two different adam optimizers ( kingma and ba , 2015 )","experimental setup apply two different adam optimizers ( kingma and ba , 2015 )",0.5612245202064514
translation,60,121,experimental-setup,learning rate and warmup steps,are,"2e - 3 and 20,000","learning rate and warmup steps are 2e - 3 and 20,000",0.6112498641014099
translation,60,121,experimental-setup,learning rate and warmup steps,are,"0.1 and 10,000","learning rate and warmup steps are 0.1 and 10,000",0.5746404528617859
translation,60,121,experimental-setup,"2e - 3 and 20,000",for,pretrained part,"2e - 3 and 20,000 for pretrained part",0.6805635094642639
translation,60,121,experimental-setup,"0.1 and 10,000",for,other parts,"0.1 and 10,000 for other parts",0.6479066014289856
translation,60,121,experimental-setup,experimental setup,has,learning rate and warmup steps,experimental setup has learning rate and warmup steps,0.4855553209781647
translation,60,124,experimental-setup,models,trained for,"300,000 steps","models trained for 300,000 steps",0.8061513304710388
translation,60,124,experimental-setup,"300,000 steps",on,bigpatent and wikisum,"300,000 steps on bigpatent and wikisum",0.5760102272033691
translation,60,124,experimental-setup,"300,000 steps",with,8 gpus,"300,000 steps with 8 gpus",0.605650782585144
translation,60,124,experimental-setup,experimental setup,trained for,"300,000 steps","experimental setup trained for 300,000 steps",0.7312495112419128
translation,60,124,experimental-setup,experimental setup,has,models,experimental setup has models,0.5060054659843445
translation,60,125,experimental-setup,dropout,with,probability of 0.1,dropout with probability of 0.1,0.6467273235321045
translation,60,125,experimental-setup,dropout,before,all linear layers,dropout before all linear layers,0.6243862509727478
translation,60,125,experimental-setup,experimental setup,apply,dropout,experimental setup apply dropout,0.5288015604019165
translation,60,126,experimental-setup,graph- encoder layers and graph- decoder layers,set as,2 and 6,graph- encoder layers and graph- decoder layers set as 2 and 6,0.6484482288360596
translation,60,127,experimental-setup,hidden size,of,graph encoding and graph decoding layers,hidden size of graph encoding and graph decoding layers,0.5778006911277771
translation,60,127,experimental-setup,hidden size,both,graph encoding and graph decoding layers,hidden size both graph encoding and graph decoding layers,0.6815906763076782
translation,60,127,experimental-setup,graph encoding and graph decoding layers,is,768,graph encoding and graph decoding layers is 768,0.5873081684112549
translation,60,127,experimental-setup,768,in alignment with,roberta,768 in alignment with roberta,0.6471365094184875
translation,60,127,experimental-setup,feed -forward size,is,2048,feed -forward size is 2048,0.6119586229324341
translation,60,127,experimental-setup,2048,for,parameter efficiency,2048 for parameter efficiency,0.649191677570343
translation,60,127,experimental-setup,experimental setup,has,hidden size,experimental setup has hidden size,0.5616794228553772
translation,60,128,experimental-setup,parameter,is,0.9,parameter is 0.9,0.5858892202377319
translation,60,128,experimental-setup,propagation steps p,is,2,propagation steps p is 2,0.5961276888847351
translation,60,128,experimental-setup,graph - propagation attention,has,parameter,graph - propagation attention has parameter,0.5553563237190247
translation,60,128,experimental-setup,graph - propagation attention,has,propagation steps p,graph - propagation attention has propagation steps p,0.5842348337173462
translation,60,128,experimental-setup,experimental setup,For,graph - propagation attention,experimental setup For graph - propagation attention,0.5680104494094849
translation,60,129,experimental-setup,decoding,apply,beam search,decoding apply beam search,0.6281094551086426
translation,60,129,experimental-setup,beam search,with,beam size 5,beam search with beam size 5,0.7065784335136414
translation,60,129,experimental-setup,beam search,with,length penalty,beam search with length penalty,0.6435361504554749
translation,60,129,experimental-setup,length penalty,with,factor 0.9,length penalty with factor 0.9,0.6246423125267029
translation,60,129,experimental-setup,experimental setup,During,decoding,experimental setup During decoding,0.6438420414924622
translation,60,130,experimental-setup,trigram blocking,reduce,repetitions,trigram blocking reduce repetitions,0.698482871055603
translation,60,130,experimental-setup,experimental setup,has,trigram blocking,experimental setup has trigram blocking,0.5485354065895081
translation,60,5,model,abstractive summarization,based on,unified semantic graph,abstractive summarization based on unified semantic graph,0.5622246861457825
translation,60,5,model,unified semantic graph,aggregates,co-referent phrases,unified semantic graph aggregates co-referent phrases,0.6577469110488892
translation,60,5,model,co-referent phrases,distributing across,long range of context,co-referent phrases distributing across long range of context,0.669797956943512
translation,60,6,model,graph- based encoder-decoder model,proposed to improve,document representation and summary generation process,graph- based encoder-decoder model proposed to improve document representation and summary generation process,0.7087417840957642
translation,60,6,model,document representation and summary generation process,by leveraging,graph structure,document representation and summary generation process by leveraging graph structure,0.6620730757713318
translation,60,7,model,graph augmentation methods,designed to encode,explicit and implicit relations,graph augmentation methods designed to encode explicit and implicit relations,0.7296460270881653
translation,60,7,model,explicit and implicit relations,in,text,explicit and implicit relations in text,0.5500948429107666
translation,60,7,model,graphpropagation attention mechanism,developed in,decoder,graphpropagation attention mechanism developed in decoder,0.6673270463943481
translation,60,7,model,decoder,to select,salient content,decoder to select salient content,0.743412971496582
translation,60,7,model,salient content,into,summary,salient content into summary,0.5699343681335449
translation,60,7,model,model,has,graph augmentation methods,model has graph augmentation methods,0.5396212339401245
translation,60,21,model,fine- grained phrases,extracted from,dependency parsing,fine- grained phrases extracted from dependency parsing,0.5069021582603455
translation,60,21,model,our graph,suitable for,information aggregation,our graph suitable for information aggregation,0.7714472413063049
translation,60,21,model,information aggregation,with the help of,coreference resolution,information aggregation with the help of coreference resolution,0.6249982714653015
translation,60,21,model,coreference resolution,substantially compresses,input,coreference resolution substantially compresses input,0.7160786390304565
translation,60,21,model,fine- grained phrases,has,our graph,fine- grained phrases has our graph,0.5614776611328125
translation,60,21,model,dependency parsing,has,our graph,dependency parsing has our graph,0.5611591339111328
translation,60,21,model,model,Based on,fine- grained phrases,model Based on fine- grained phrases,0.5907582640647888
translation,60,25,model,graph-encoder,effectively encodes,long sequences,graph-encoder effectively encodes long sequences,0.7348794937133789
translation,60,25,model,long sequences,by explicitly modeling,relations,long sequences by explicitly modeling relations,0.7125725746154785
translation,60,25,model,relations,between,phrases,relations between phrases,0.68497234582901
translation,60,25,model,relations,capturing,global structure,relations capturing global structure,0.7173300385475159
translation,60,25,model,global structure,based on,semantic graph,global structure based on semantic graph,0.613298773765564
translation,60,25,model,model,has,graph-encoder,model has graph-encoder,0.5853304862976074
translation,60,26,model,graph augmentation methods,applied during,graph encoding,graph augmentation methods applied during graph encoding,0.6505118608474731
translation,60,26,model,graph encoding,to tap,potential semantic relations,graph encoding to tap potential semantic relations,0.6257562637329102
translation,60,27,model,graph decoder,incorporates,graph structure,graph decoder incorporates graph structure,0.7020463943481445
translation,60,27,model,graph structure,by,graph propagate attention,graph structure by graph propagate attention,0.5347669124603271
translation,60,27,model,graph propagate attention,to guide,summary generation process,graph propagate attention to guide summary generation process,0.6678974032402039
translation,60,27,model,summary generation process,help select,salient content,summary generation process help select salient content,0.6962863802909851
translation,60,27,model,salient content,organize them into,coherent summary,salient content organize them into coherent summary,0.6144323945045471
translation,60,27,model,decoding procedure,has,graph decoder,decoding procedure has graph decoder,0.570501446723938
translation,60,27,model,model,For,decoding procedure,model For decoding procedure,0.6110161542892456
translation,60,72,model,properties,of,united semantic graph,properties of united semantic graph,0.5786898136138916
translation,60,72,model,properties,propose,two novel graph augmentation methods,properties propose two novel graph augmentation methods,0.6427806615829468
translation,60,72,model,model,For better utilizing,properties,model For better utilizing properties,0.7046930193901062
translation,60,72,model,model,propose,two novel graph augmentation methods,model propose two novel graph augmentation methods,0.6737920045852661
translation,60,172,model,unified semantic graph,to improve,performance,unified semantic graph to improve performance,0.6759592890739441
translation,60,172,model,performance,of,neural abstractive models,performance of neural abstractive models,0.6112163066864014
translation,60,172,model,neural abstractive models,for,long-document summarization and mds,neural abstractive models for long-document summarization and mds,0.5922333002090454
translation,60,172,model,model,leverage,unified semantic graph,model leverage unified semantic graph,0.6715794205665588
translation,60,173,model,graph- based encoder-decoder model,to improve,document representation and summary generation process,graph- based encoder-decoder model to improve document representation and summary generation process,0.6637153029441833
translation,60,173,model,model,present,graph- based encoder-decoder model,model present graph- based encoder-decoder model,0.6560934782028198
translation,60,145,results,basic transs2s,achieves,best performance,basic transs2s achieves best performance,0.7115074396133423
translation,60,145,results,best performance,at,input length,best performance at input length,0.5579484701156616
translation,60,145,results,input length,of,800,input length of 800,0.6379815340042114
translation,60,145,results,longer input,has,hurts,longer input has hurts,0.5992187261581421
translation,60,145,results,hurts,has,performance,hurts has performance,0.6043882966041565
translation,60,145,results,results,has,basic transs2s,results has basic transs2s,0.5790234804153442
translation,60,148,results,power,of,sentence - level graph,power of sentence - level graph,0.6026199460029602
translation,60,148,results,graphsum,achieves,best performance,graphsum achieves best performance,0.7114630937576294
translation,60,148,results,best performance,at,"2,400","best performance at 2,400",0.5637054443359375
translation,60,148,results,performance,begins to,decrease,performance begins to decrease,0.7017964124679565
translation,60,148,results,decrease,when,input length,decrease when input length,0.6820501685142517
translation,60,148,results,input length,reaches,3000,input length reaches 3000,0.7185963988304138
translation,60,148,results,power,has,graphsum,power has graphsum,0.6171613931655884
translation,60,148,results,sentence - level graph,has,graphsum,sentence - level graph has graphsum,0.5484499931335449
translation,60,148,results,results,Leveraging,power,results Leveraging power,0.5774688720703125
translation,60,153,results,bass,generates,more abstract summaries,bass generates more abstract summaries,0.6610398292541504
translation,60,153,results,more abstract summaries,comparing to,recent models,more abstract summaries comparing to recent models,0.704329252243042
translation,60,153,results,more abstract summaries,comparing to,ht,more abstract summaries comparing to ht,0.6825106739997864
translation,60,153,results,results,has,bass,results has bass,0.3985755145549774
translation,61,87,ablation-analysis,pre-trained refactor,large number of,candidate summaries,pre-trained refactor large number of candidate summaries,0.6891350746154785
translation,61,87,ablation-analysis,candidate summaries,with,diverse performance ( rouge value ),candidate summaries with diverse performance ( rouge value ),0.5945734977722168
translation,61,87,ablation-analysis,pre-training stage,has,pre-trained refactor,pre-training stage has pre-trained refactor,0.6081688404083252
translation,61,87,ablation-analysis,ablation analysis,in,pre-training stage,ablation analysis in pre-training stage,0.5229983329772949
translation,61,137,ablation-analysis,fine-tuned refactor,boost,bart 's performance,fine-tuned refactor boost bart 's performance,0.6541159749031067
translation,61,137,ablation-analysis,bart 's performance,from,44.26 to 45.15,bart 's performance from 44.26 to 45.15,0.5139329433441162
translation,61,137,ablation-analysis,44.26 to 45.15,on,rouge - 1,44.26 to 45.15 on rouge - 1,0.5823180079460144
translation,61,137,ablation-analysis,ablation analysis,observe,fine-tuned refactor,ablation analysis observe fine-tuned refactor,0.5780543684959412
translation,61,228,ablation-analysis,fine - tuning,further improve,performance,fine - tuning further improve performance,0.7210460901260376
translation,61,228,ablation-analysis,trained refactor,has,already outperform,trained refactor has already outperform,0.6085011959075928
translation,61,228,ablation-analysis,already outperform,has,base systems,already outperform has base systems,0.6285696029663086
translation,61,228,ablation-analysis,ablation analysis,pre-,trained refactor,ablation analysis pre- trained refactor,0.6486822366714478
translation,61,96,baselines,baselines,has,multi-system summary - level,baselines has multi-system summary - level,0.558239221572876
translation,61,98,baselines,baselines,has,multi-system sentence - level,baselines has multi-system sentence - level,0.5409656167030334
translation,61,116,baselines,"bertscore ( zhang et al. , 2019 b )",as,unsupervised baseline,"bertscore ( zhang et al. , 2019 b ) as unsupervised baseline",0.4896601736545563
translation,61,116,baselines,unsupervised baseline,with,neural models,unsupervised baseline with neural models,0.6290980577468872
translation,61,116,baselines,neural models,is,automatic evaluation metric,neural models is automatic evaluation metric,0.5361218452453613
translation,61,116,baselines,automatic evaluation metric,computing,similarity,automatic evaluation metric computing similarity,0.7381713390350342
translation,61,116,baselines,similarity,of,text pairs,similarity of text pairs,0.5975281596183777
translation,61,116,baselines,similarity,based on,corresponding bert - encoded representations,similarity based on corresponding bert - encoded representations,0.6863771080970764
translation,61,119,baselines,ranksvm,as,non-neural baseline,ranksvm as non-neural baseline,0.48769471049308777
translation,61,133,experiments,pre-training,takes,around 40 hours,pre-training takes around 40 hours,0.6369286179542542
translation,61,133,experiments,around 40 hours,on,4 gtx - 1080 - ti gpus,around 40 hours on 4 gtx - 1080 - ti gpus,0.513608992099762
translation,61,133,experiments,fine-tuning,takes,around 20 hours,fine-tuning takes around 20 hours,0.6912239193916321
translation,61,125,hyperparameters,number of candidates,prune,sentences,number of candidates prune sentences,0.6957570314407349
translation,61,125,hyperparameters,sentences,assigned with,lower scores,sentences assigned with lower scores,0.6387266516685486
translation,61,125,hyperparameters,lower scores,by,extractive model,lower scores by extractive model,0.6035694479942322
translation,61,125,hyperparameters,hyperparameters,To reduce,number of candidates,hyperparameters To reduce number of candidates,0.6505010724067688
translation,61,29,model,general framework,named,refactor,general framework named refactor,0.6891034245491028
translation,61,29,model,meta system,to select,best system output,meta system to select best system output,0.6669376492500305
translation,61,29,model,best system output,from,multiple candidates,best system output from multiple candidates,0.5996556878089905
translation,61,29,model,model,proposing,general framework,model proposing general framework,0.6950104832649231
translation,61,30,model,base and meta systems,allows them,share,base and meta systems allows them share,0.6923418045043945
translation,61,30,model,set of parameters,alleviating,base - meta learning gap,set of parameters alleviating base - meta learning gap,0.6885225772857666
translation,61,30,model,share,has,set of parameters,share has set of parameters,0.577869176864624
translation,61,30,model,model,unification of,base and meta systems,model unification of base and meta systems,0.7255015969276428
translation,61,110,results,"gsum ( dou et al. , 2020 )",enhances,performance,"gsum ( dou et al. , 2020 ) enhances performance",0.6967600584030151
translation,61,110,results,performance,of,bart,performance of bart,0.6565227508544922
translation,61,110,results,bart,using,additional guidance information,bart using additional guidance information,0.5600382089614868
translation,61,110,results,results,has,"gsum ( dou et al. , 2020 )","results has gsum ( dou et al. , 2020 )",0.5219680070877075
translation,61,111,results,pegasus,achieves,competitive performance,pegasus achieves competitive performance,0.6987841725349426
translation,61,111,results,pegasus,achieves,current state - of- the - art,pegasus achieves current state - of- the - art,0.6250266432762146
translation,61,111,results,competitive performance,on,various summarization datasets,competitive performance on various summarization datasets,0.4998760223388672
translation,61,111,results,current state - of- the - art,on,xsum dataset,current state - of- the - art on xsum dataset,0.5304306149482727
translation,61,111,results,results,has,pegasus,results has pegasus,0.5411727428436279
translation,61,136,results,refactor,boost,base system 's performance,refactor boost base system 's performance,0.6610442996025085
translation,61,136,results,base system 's performance,by,significant margin,base system 's performance by significant margin,0.5915586352348328
translation,61,136,results,fine- tuned refactor,outperforms,supervised refactor,fine- tuned refactor outperforms supervised refactor,0.7457418441772461
translation,61,136,results,supervised refactor,directly trained on,base system 's outputs,supervised refactor directly trained on base system 's outputs,0.7541112899780273
translation,61,136,results,results,shows,refactor,results shows refactor,0.6643961668014526
translation,61,142,results,sentence - level combination,use,bart,sentence - level combination use bart,0.5829983949661255
translation,61,142,results,sentence - level combination,use,pre-trained refactor,sentence - level combination use pre-trained refactor,0.5986233949661255
translation,61,142,results,pre-trained refactor,as,base,pre-trained refactor as base,0.5194578766822815
translation,61,142,results,similar performance,as,fine-tuned refactor,similar performance as fine-tuned refactor,0.5682464241981506
translation,61,142,results,supervised refactor,has,similar performance,supervised refactor has similar performance,0.5851146578788757
translation,61,142,results,results,For,sentence - level combination,results For sentence - level combination,0.585620641708374
translation,61,145,results,gsum model,in,three -system combination setting,gsum model in three -system combination setting,0.5210124850273132
translation,61,145,results,pre-trained refactor,has,cannot outperform,pre-trained refactor has cannot outperform,0.6238338947296143
translation,61,145,results,cannot outperform,has,gsum model,cannot outperform has gsum model,0.6164115071296692
translation,61,145,results,results,has,pre-trained refactor,results has pre-trained refactor,0.5529017448425293
translation,61,155,results,consistently outperforms,showing,generalization ability,consistently outperforms showing generalization ability,0.7176771759986877
translation,61,155,results,refactor,has,consistently outperforms,refactor has consistently outperforms,0.6341087222099304
translation,61,155,results,consistently outperforms,has,best candidate system,consistently outperforms has best candidate system,0.6264240145683289
translation,61,155,results,results,has,refactor,results has refactor,0.5651337504386902
translation,61,167,results,refactor,able to bring,stable improvement,refactor able to bring stable improvement,0.599559485912323
translation,61,167,results,stable improvement,over,base systems,stable improvement over base systems,0.7158616185188293
translation,61,167,results,results,show,refactor,results show refactor,0.6393598318099976
translation,61,170,results,xsum dataset,has,pre-trained refactor,xsum dataset has pre-trained refactor,0.572327733039856
translation,61,170,results,pre-trained refactor,has,outperforms,pre-trained refactor has outperforms,0.6072815656661987
translation,61,170,results,outperforms,has,fine-tuned refactor,outperforms has fine-tuned refactor,0.6074424386024475
translation,61,170,results,results,On,xsum dataset,results On xsum dataset,0.569536030292511
translation,61,178,results,selection accuracy,increasing as,gap,selection accuracy increasing as gap,0.6184535622596741
translation,61,178,results,becoming larger,indicating,refactor,becoming larger indicating refactor,0.7292701601982117
translation,61,178,results,refactor,performs,better,refactor performs better,0.7204105854034424
translation,61,178,results,better,on,candidate summaries,better on candidate summaries,0.5520980954170227
translation,61,178,results,better,with,diverse performance,better with diverse performance,0.6492246389389038
translation,61,178,results,gap,has,becoming larger,gap has becoming larger,0.6234339475631714
translation,62,42,model,lightweight query language,for,rule- based detection of situations,lightweight query language for rule- based detection of situations,0.6086578369140625
translation,62,43,model,application,for exploring and annotating,multiparticipant chat datasets,application for exploring and annotating multiparticipant chat datasets,0.7432542443275452
translation,62,43,model,model,introduce,chat corpora annotator,model introduce chat corpora annotator,0.5459694266319275
translation,63,6,baselines,sgsum,selects,salient sub-graph,sgsum selects salient sub-graph,0.7159569263458252
translation,63,6,baselines,salient sub-graph,from,relation graph,salient sub-graph from relation graph,0.5335622429847717
translation,63,6,baselines,salient sub-graph,as,summary,salient sub-graph as summary,0.5611397624015808
translation,63,6,baselines,relation graph,as,summary,relation graph as summary,0.5270888805389404
translation,63,162,baselines,"hetergraph and matchsum ( zhong et al. , 2020 )",is,previous extractive sota model,"hetergraph and matchsum ( zhong et al. , 2020 ) is previous extractive sota model",0.5241348743438721
translation,63,162,baselines,previous extractive sota model,on,multinews dataset,previous extractive sota model on multinews dataset,0.5367696285247803
translation,63,163,baselines,abstractive methods,is,previous abstractive sota model,abstractive methods is previous abstractive sota model,0.5578262209892273
translation,63,170,baselines,pg,has,"see et al. , 2017 )","pg has see et al. , 2017 )",0.6023714542388916
translation,63,182,baselines,extractive method,with,pre-trained lm model,extractive method with pre-trained lm model,0.628756582736969
translation,63,182,baselines,encoder-decoder model,exploits,maximal marginal relevance method,encoder-decoder model exploits maximal marginal relevance method,0.6879388093948364
translation,63,182,baselines,maximal marginal relevance method,to select,representative sentences,maximal marginal relevance method to select representative sentences,0.6875952482223511
translation,63,182,baselines,extract + rewrite,scores,sentences,extract + rewrite scores sentences,0.7244969010353088
translation,63,182,baselines,extract + rewrite,generates,title - like summary,extract + rewrite generates title - like summary,0.6872201561927795
translation,63,182,baselines,title - like summary,for,each sentence,title - like summary for each sentence,0.6174576878547668
translation,63,182,baselines,each sentence,using,encoder-decoder model,each sentence using encoder-decoder model,0.6361593008041382
translation,63,182,baselines,pg - mmr,has,encoder-decoder model,pg - mmr has encoder-decoder model,0.559708297252655
translation,63,144,experimental-setup,"base version of roberta ( liu et al. , 2019 b )",to initialize,our models,"base version of roberta ( liu et al. , 2019 b ) to initialize our models",0.7322608828544617
translation,63,144,experimental-setup,experimental setup,use,"base version of roberta ( liu et al. , 2019 b )","experimental setup use base version of roberta ( liu et al. , 2019 b )",0.5891183018684387
translation,63,145,experimental-setup,optimizer,is,"adam ( kingma and ba , 2014 )","optimizer is adam ( kingma and ba , 2014 )",0.5048646926879883
translation,63,145,experimental-setup,"adam ( kingma and ba , 2014 )",with,?1=0.9 and ?2=0.999,"adam ( kingma and ba , 2014 ) with ?1=0.9 and ?2=0.999",0.6532964110374451
translation,63,145,experimental-setup,"adam ( kingma and ba , 2014 )",with,learning rate,"adam ( kingma and ba , 2014 ) with learning rate",0.5989180207252502
translation,63,145,experimental-setup,learning rate,is,0.03,learning rate is 0.03,0.5817162990570068
translation,63,145,experimental-setup,learning rate,is,0.015,learning rate is 0.015,0.5807502865791321
translation,63,145,experimental-setup,0.03,for,multinews,0.03 for multinews,0.6516097784042358
translation,63,145,experimental-setup,0.015,for,duc,0.015 for duc,0.6790558695793152
translation,63,145,experimental-setup,experimental setup,has,optimizer,experimental setup has optimizer,0.5528271794319153
translation,63,146,experimental-setup,learning rate warmup,over,first 10000 steps,learning rate warmup over first 10000 steps,0.632941722869873
translation,63,146,experimental-setup,learning rate warmup,over,decay,learning rate warmup over decay,0.6842504739761353
translation,63,146,experimental-setup,experimental setup,apply,learning rate warmup,experimental setup apply learning rate warmup,0.5720770955085754
translation,63,147,experimental-setup,gradient clipping,with,maximum gradient norm 2.0,gradient clipping with maximum gradient norm 2.0,0.6004931926727295
translation,63,147,experimental-setup,maximum gradient norm 2.0,utilized during,training,maximum gradient norm 2.0 utilized during training,0.6684382557868958
translation,63,147,experimental-setup,experimental setup,has,gradient clipping,experimental setup has gradient clipping,0.4766085147857666
translation,63,148,experimental-setup,4 gpus ( tesla v100 ),for,about 10 epochs,4 gpus ( tesla v100 ) for about 10 epochs,0.5827478170394897
translation,63,149,experimental-setup,dropout,with,probability 0.1,dropout with probability 0.1,0.6266955733299255
translation,63,149,experimental-setup,probability 0.1,before,all linear layers,probability 0.1 before all linear layers,0.6418308615684509
translation,63,149,experimental-setup,experimental setup,apply,dropout,experimental setup apply dropout,0.5288015604019165
translation,63,150,experimental-setup,number of hidden units,in,our models,number of hidden units in our models,0.4497196674346924
translation,63,150,experimental-setup,number of hidden units,set as,256,number of hidden units set as 256,0.6601744890213013
translation,63,150,experimental-setup,our models,set as,256,our models set as 256,0.6566123962402344
translation,63,150,experimental-setup,feedforward hidden size,is,"1,024","feedforward hidden size is 1,024",0.5814740061759949
translation,63,150,experimental-setup,number of heads,is,8,number of heads is 8,0.5963917374610901
translation,63,150,experimental-setup,experimental setup,has,number of hidden units,experimental setup has number of hidden units,0.5046243071556091
translation,63,150,experimental-setup,experimental setup,has,feedforward hidden size,experimental setup has feedforward hidden size,0.5534060597419739
translation,63,150,experimental-setup,experimental setup,has,number of heads,experimental setup has number of heads,0.5273870229721069
translation,63,151,experimental-setup,transformer encoding layers and graph encoding layers,set as,6 and 2,transformer encoding layers and graph encoding layers set as 6 and 2,0.6274526119232178
translation,63,151,experimental-setup,experimental setup,number of,transformer encoding layers and graph encoding layers,experimental setup number of transformer encoding layers and graph encoding layers,0.6692490577697754
translation,63,154,experimental-setup,candidate nodes and sub-graph nodes,as,10/9 and 7/5,candidate nodes and sub-graph nodes as 10/9 and 7/5,0.5462888479232788
translation,63,154,experimental-setup,experimental setup,For,multinews and duc,experimental setup For multinews and duc,0.6222620010375977
translation,63,173,experiments,performance,of,sgsum-extra,performance of sgsum-extra,0.5867191553115845
translation,63,173,experiments,sgsum-extra,add,cnn / dm data,sgsum-extra add cnn / dm data,0.6998453140258789
translation,63,5,model,novel mds framework ( sgsum ),to formulate,mds task,novel mds framework ( sgsum ) to formulate mds task,0.6783267855644226
translation,63,5,model,mds task,as,sub-graph selection problem,mds task as sub-graph selection problem,0.4904208481311798
translation,63,5,model,sub-graph selection problem,in which,source documents,sub-graph selection problem in which source documents,0.556389331817627
translation,63,5,model,candidate summaries,are,subgraphs,candidate summaries are subgraphs,0.6085694432258606
translation,63,5,model,model,propose,novel mds framework ( sgsum ),model propose novel mds framework ( sgsum ),0.6738612055778503
translation,63,19,model,novel mds framework,called,sgsum,novel mds framework called sgsum,0.6420696377754211
translation,63,19,model,mds task,as,sub-graph selection problem,mds task as sub-graph selection problem,0.4904208481311798
translation,63,19,model,model,propose,novel mds framework,model propose novel mds framework,0.6913608312606812
translation,63,20,model,source documents,regarded as,relation graph,source documents regarded as relation graph,0.541682779788971
translation,63,20,model,relation graph,of,sentences,relation graph of sentences,0.5729736685752869
translation,63,20,model,sentences,e.g.,discourse graph,sentences e.g. discourse graph,0.6397250294685364
translation,63,20,model,candidate summaries,are,sub-graphs,candidate summaries are sub-graphs,0.5910320281982422
translation,63,41,model,document graph,helps to extract,salient sentences,document graph helps to extract salient sentences,0.7437706589698792
translation,63,41,model,summary graph,helps to evluate,quality of summary,summary graph helps to evluate quality of summary,0.6962902545928955
translation,63,41,model,model,has,document graph,model has document graph,0.5373718738555908
translation,63,42,model,novel mds framework sgsum,transforms,summarization,novel mds framework sgsum transforms summarization,0.7117660641670227
translation,63,42,model,summarization,problem of,sub-graph selection,summarization problem of sub-graph selection,0.7026880979537964
translation,63,42,model,model,propose,novel mds framework sgsum,model propose novel mds framework sgsum,0.678625226020813
translation,63,43,model,sgsum,captures,relation of sentences,sgsum captures relation of sentences,0.7225023508071899
translation,63,43,model,relation of sentences,both in,whole graph structure ( source documents ),relation of sentences both in whole graph structure ( source documents ),0.5749284029006958
translation,63,43,model,relation of sentences,both in,sub-graph structures ( candidate summaries ),relation of sentences both in sub-graph structures ( candidate summaries ),0.6008846759796143
translation,63,43,model,model,has,sgsum,model has sgsum,0.6167190670967102
translation,63,62,model,hierarchical transformer,processes,each document independently,hierarchical transformer processes each document independently,0.6938784122467041
translation,63,62,model,hierarchical transformer,outputs,sentence representations,hierarchical transformer outputs sentence representations,0.6755605340003967
translation,63,62,model,three main components,has,hierarchical transformer,three main components has hierarchical transformer,0.5928767919540405
translation,63,63,model,graph encoding layer,updates,sentence representations,graph encoding layer updates sentence representations,0.7053308486938477
translation,63,63,model,sentence representations,by modeling,graph structure of documents,sentence representations by modeling graph structure of documents,0.7583259344100952
translation,63,63,model,model,has,graph encoding layer,model has graph encoding layer,0.5277791619300842
translation,63,73,model,source documents,by,hierarchical transformer,source documents by hierarchical transformer,0.5541340708732605
translation,63,73,model,hierarchical transformer,consists of,several sharedweight single transformers,hierarchical transformer consists of several sharedweight single transformers,0.6765406727790833
translation,63,73,model,several sharedweight single transformers,that process,each document independently,several sharedweight single transformers that process each document independently,0.7145240306854248
translation,63,73,model,model,encode,source documents,model encode source documents,0.7435376048088074
translation,63,74,model,tokenized document,as,input,tokenized document as input,0.5425159931182861
translation,63,77,model,relations,between,sentences,relations between sentences,0.6762184500694275
translation,63,77,model,relations,incorporate,explicit graph representations,relations incorporate explicit graph representations,0.6136617064476013
translation,63,77,model,sentences,in,source documents,sentences in source documents,0.45627233386039734
translation,63,77,model,explicit graph representations,of,documents,explicit graph representations of documents,0.6028634309768677
translation,63,77,model,explicit graph representations,into,neural encoding process,explicit graph representations into neural encoding process,0.5751656889915466
translation,63,77,model,neural encoding process,via,graph - informed attention mechanism,neural encoding process via graph - informed attention mechanism,0.6231092214584351
translation,63,77,model,model,To effectively capture,relations,model To effectively capture relations,0.7060186266899109
translation,63,127,model,similarity graph,built based on,tf-idf cosine similarities,similarity graph built based on tf-idf cosine similarities,0.6948044300079346
translation,63,127,model,tf-idf cosine similarities,between,sentences,tf-idf cosine similarities between sentences,0.6423367857933044
translation,63,127,model,sentences,to capture,lexical relations,sentences to capture lexical relations,0.6255194544792175
translation,63,127,model,model,has,similarity graph,model has similarity graph,0.5669412016868591
translation,63,152,model,inference,select,several salient candidate nodes,inference select several salient candidate nodes,0.6916956901550293
translation,63,152,model,several salient candidate nodes,to build up,sub-graphs,several salient candidate nodes to build up sub-graphs,0.6983560919761658
translation,63,152,model,model,during,inference,model during inference,0.6954593062400818
translation,63,167,model,our model,ability to unify,single and multi-document summarization task,our model ability to unify single and multi-document summarization task,0.6884344816207886
translation,63,167,model,graph representation and graph - based multi-document encoder,has,our model,graph representation and graph - based multi-document encoder has our model,0.5498211979866028
translation,63,167,model,model,ability to unify,single and multi-document summarization task,model ability to unify single and multi-document summarization task,0.6745994091033936
translation,63,7,results,graph structure,of,whole document set,graph structure of whole document set,0.5432328581809998
translation,63,7,results,graph structure,of,candidate sub-graphs,graph structure of candidate sub-graphs,0.5652523636817932
translation,63,7,results,integrate summary,in the form of,subgraph,integrate summary in the form of subgraph,0.6793060302734375
translation,63,7,results,subgraph,which is,more informative and coherent,subgraph which is more informative and coherent,0.6188839077949524
translation,63,7,results,results,Comparing with,traditional methods,results Comparing with traditional methods,0.611681342124939
translation,63,24,results,relations,between,sentences,relations between sentences,0.6762184500694275
translation,63,24,results,evaluating summary,as,sub-graph,evaluating summary as sub-graph,0.5494662523269653
translation,63,24,results,our framework,can generate,more informative and coherent summaries,our framework can generate more informative and coherent summaries,0.6958057284355164
translation,63,24,results,more informative and coherent summaries,compared with,traditional extractive mds methods,more informative and coherent summaries compared with traditional extractive mds methods,0.6453321576118469
translation,63,24,results,relations,has,our framework,relations has our framework,0.6079532504081726
translation,63,24,results,evaluating summary,has,our framework,evaluating summary has our framework,0.6031723022460938
translation,63,24,results,sub-graph,has,our framework,sub-graph has our framework,0.6015656590461731
translation,63,24,results,results,By capturing,relations,results By capturing relations,0.6488614082336426
translation,63,166,results,sgsum,achieves,more than 1.1/1.2/0.9 improvements,sgsum achieves more than 1.1/1.2/0.9 improvements,0.6579295992851257
translation,63,166,results,more than 1.1/1.2/0.9 improvements,on,"r -1 , r - 2 and r-l","more than 1.1/1.2/0.9 improvements on r -1 , r - 2 and r-l",0.5702173709869385
translation,63,166,results,previous extractive and abstractive sota models,has,sgsum,previous extractive and abstractive sota models has sgsum,0.6026784181594849
translation,63,166,results,results,Compared with,previous extractive and abstractive sota models,results Compared with previous extractive and abstractive sota models,0.6815413236618042
translation,63,172,results,our model sgsum,has,consistently outperforms,our model sgsum has consistently outperforms,0.6079307198524475
translation,63,172,results,consistently outperforms,has,most baselines,consistently outperforms has most baselines,0.6009925603866577
translation,63,172,results,results,indicate,our model sgsum,results indicate our model sgsum,0.6210734844207764
translation,63,174,results,extra cnn / dm data,to train,similarity model,extra cnn / dm data to train similarity model,0.6894169449806213
translation,63,175,results,performance,of,our model,performance of our model,0.5847885608673096
translation,63,175,results,singledocument data,has,greatly improves,singledocument data has greatly improves,0.5891251564025879
translation,63,175,results,greatly improves,has,performance,greatly improves has performance,0.591407299041748
translation,63,175,results,results,show that,singledocument data,results show that singledocument data,0.4577316641807556
translation,63,187,results,our model,achieves,better performance,our model achieves better performance,0.6816908121109009
translation,63,187,results,better performance,than,several strong unsupervised models,better performance than several strong unsupervised models,0.5393614172935486
translation,63,187,results,results,show,our model,results show our model,0.6888449192047119
translation,63,188,results,sgsum,performs,much better,sgsum performs much better,0.5970609188079834
translation,63,188,results,much better,on,transfer ability,much better on transfer ability,0.5026394724845886
translation,63,188,results,transfer ability,compared with,three baselines,transfer ability compared with three baselines,0.6617084741592407
translation,63,188,results,sds data,has,sgsum,sds data has sgsum,0.5804038643836975
translation,63,189,results,multinews and duc datasets,validate,effectiveness,multinews and duc datasets validate effectiveness,0.6041541695594788
translation,63,190,results,subgraph selection framework,greatly improves,performance,subgraph selection framework greatly improves performance,0.7078546285629272
translation,63,190,results,subgraph selection framework,shows,powerful trans - fer ability,subgraph selection framework shows powerful trans - fer ability,0.5538446307182312
translation,63,190,results,performance,of,mds,performance of mds,0.6480542421340942
translation,63,190,results,performance,of,mds,performance of mds,0.6480542421340942
translation,63,190,results,powerful trans - fer ability,can reduce,resource bottleneck,powerful trans - fer ability can reduce resource bottleneck,0.7199162840843201
translation,63,190,results,resource bottleneck,in,mds,resource bottleneck in mds,0.5680898427963257
translation,63,190,results,results,has,subgraph selection framework,results has subgraph selection framework,0.5438846349716187
translation,63,194,results,results,of,similarity graph,results of similarity graph,0.5870436429977417
translation,63,194,results,results,of,topic graph,results of topic graph,0.576183021068573
translation,63,194,results,results,of,discourse graph,results of discourse graph,0.5507744550704956
translation,63,194,results,results,on,multinews test set,results on multinews test set,0.548920214176178
translation,63,194,results,similarity graph,on,multinews test set,similarity graph on multinews test set,0.5488261580467224
translation,63,194,results,discourse graph,on,multinews test set,discourse graph on multinews test set,0.5507144927978516
translation,63,194,results,results,compare,results,results compare results,0.5562655925750732
translation,63,194,results,results,compare,similarity graph,results compare similarity graph,0.7117377519607544
translation,63,194,results,results,of,similarity graph,results of similarity graph,0.5870436429977417
translation,63,195,results,discourse graph,achieves,best performance,discourse graph achieves best performance,0.7034056186676025
translation,63,195,results,best performance,on,all metrics,best performance on all metrics,0.5029283165931702
translation,63,205,results,sgsum,rated as,best,sgsum rated as best,0.5965613722801208
translation,63,205,results,best,on,informativeness and coherence,best on informativeness and coherence,0.527614951133728
translation,63,205,results,results,demonstrate,sgsum,results demonstrate sgsum,0.6071597933769226
translation,63,206,results,summaries,generated by,sgsum,summaries generated by sgsum,0.6687443852424622
translation,63,206,results,summaries,frequently ranked as,best,summaries frequently ranked as best,0.6888495683670044
translation,63,206,results,sgsum,frequently ranked as,best,sgsum frequently ranked as best,0.6654879450798035
translation,63,206,results,overall ratings,has,summaries,overall ratings has summaries,0.5620239973068237
translation,63,206,results,significantly outperforms,has,other models,significantly outperforms has other models,0.5646228790283203
translation,63,206,results,results,Regarding,overall ratings,results Regarding overall ratings,0.6142169833183289
translation,64,145,ablation-analysis,pretrained and non-pretrained settings,observe,skd,pretrained and non-pretrained settings observe skd,0.6293199062347412
translation,64,145,ablation-analysis,skd,boosts the performance,teacher model ( unilm base and tf - s2s,skd boosts the performance teacher model ( unilm base and tf - s2s,0.7030446529388428
translation,64,145,ablation-analysis,injection of noise,is,beneficial,injection of noise is beneficial,0.6365624666213989
translation,64,145,ablation-analysis,ablation analysis,Under,pretrained and non-pretrained settings,ablation analysis Under pretrained and non-pretrained settings,0.6687275171279907
translation,64,107,baselines,unilmv2,is,transformer - based neural network,unilmv2 is transformer - based neural network,0.5981251001358032
translation,64,107,baselines,baselines,has,unilmv2,baselines has unilmv2,0.5342848896980286
translation,64,106,hyperparameters,"unilmv2 ( bao et al. , 2020 )",as,pretrained model,"unilmv2 ( bao et al. , 2020 ) as pretrained model",0.47288641333580017
translation,64,106,hyperparameters,hyperparameters,adopt,"unilmv2 ( bao et al. , 2020 )","hyperparameters adopt unilmv2 ( bao et al. , 2020 )",0.5829911828041077
translation,64,108,hyperparameters,pseudo-masked language model,on,large corpus,pseudo-masked language model on large corpus,0.4768177270889282
translation,64,108,hyperparameters,hyperparameters,pretrained as,pseudo-masked language model,hyperparameters pretrained as pseudo-masked language model,0.7457916736602783
translation,64,110,hyperparameters,transformer encoder- decoder model,with,6 layers,transformer encoder- decoder model with 6 layers,0.6419901847839355
translation,64,110,hyperparameters,transformer encoder- decoder model,with,768 hidden size,transformer encoder- decoder model with 768 hidden size,0.6546696424484253
translation,64,110,hyperparameters,transformer encoder- decoder model,with,"2,048 feed - forward filter size","transformer encoder- decoder model with 2,048 feed - forward filter size",0.6453131437301636
translation,64,111,hyperparameters,label smoothing,used with,smoothing factor,label smoothing used with smoothing factor,0.6716626882553101
translation,64,111,hyperparameters,smoothing factor,has,0.1,smoothing factor has 0.1,0.514997661113739
translation,64,111,hyperparameters,hyperparameters,has,label smoothing,hyperparameters has label smoothing,0.5023636221885681
translation,64,112,hyperparameters,teacher models,trained from,randomly initialized parameters,teacher models trained from randomly initialized parameters,0.7411615252494812
translation,64,112,hyperparameters,hyperparameters,has,teacher models,hyperparameters has teacher models,0.5152563452720642
translation,64,115,hyperparameters,word drop probability p d,set to,0.1,word drop probability p d set to 0.1,0.726188063621521
translation,64,115,hyperparameters,noisy distillation models,has,word drop probability p d,noisy distillation models has word drop probability p d,0.5999204516410828
translation,64,115,hyperparameters,hyperparameters,For,noisy distillation models,hyperparameters For noisy distillation models,0.5655511617660522
translation,64,116,hyperparameters,candidate length k,for,word replacement,candidate length k for word replacement,0.6157987713813782
translation,64,116,hyperparameters,candidate length k,for,word replacement probability p r,candidate length k for word replacement probability p r,0.6238188743591309
translation,64,116,hyperparameters,word replacement,was,10,word replacement was 10,0.654360294342041
translation,64,116,hyperparameters,word replacement probability p r,was,0.1,word replacement probability p r was 0.1,0.5975605845451355
translation,64,116,hyperparameters,hyperparameters,has,candidate length k,hyperparameters has candidate length k,0.5034109950065613
translation,64,116,hyperparameters,hyperparameters,has,word replacement probability p r,hyperparameters has word replacement probability p r,0.5258281230926514
translation,64,117,hyperparameters,p s,was,0.05,p s was 0.05,0.6469402313232422
translation,64,117,hyperparameters,sentence drop probability,has,p s,sentence drop probability has p s,0.5962619781494141
translation,64,117,hyperparameters,hyperparameters,has,sentence drop probability,hyperparameters has sentence drop probability,0.5274580717086792
translation,64,118,hyperparameters,decoding,used,beam search,decoding used beam search,0.5754576921463013
translation,64,118,hyperparameters,decoding,tuned ? for,length penalty,decoding tuned ? for length penalty,0.6940754652023315
translation,64,118,hyperparameters,decoding,decode until,end-of-sequence token,decoding decode until end-of-sequence token,0.7466995120048523
translation,64,118,hyperparameters,length penalty,between,0.6 and 1,length penalty between 0.6 and 1,0.6361527442932129
translation,64,118,hyperparameters,0.6 and 1,on,validation set,0.6 and 1 on validation set,0.5934702754020691
translation,64,118,hyperparameters,beam search,has,size 5 ),beam search has size 5 ),0.6424779891967773
translation,64,118,hyperparameters,hyperparameters,During,decoding,hyperparameters During decoding,0.6597015857696533
translation,64,5,model,student summarization model,trained with,guidance,student summarization model trained with guidance,0.7275109887123108
translation,64,5,model,guidance,from,teacher,guidance from teacher,0.6145782470703125
translation,64,5,model,teacher,which generates,smoothed labels,teacher which generates smoothed labels,0.7064753770828247
translation,64,5,model,smoothed labels,to help,regularize,smoothed labels to help regularize,0.6517729759216309
translation,64,5,model,regularize,has,training,regularize has training,0.5516571402549744
translation,64,6,model,multiple noise signals,for,teacher and student models,multiple noise signals for teacher and student models,0.6137983798980713
translation,64,6,model,model,to better model uncertainty,training,model to better model uncertainty training,0.784942090511322
translation,64,21,model,model,turning to,knowledge distillation,model turning to knowledge distillation,0.6644484996795654
translation,64,27,model,teacher 's distribution,is,denoised,teacher 's distribution is denoised,0.6266444325447083
translation,64,27,model,teacher 's distribution,( to a certain extent ),denoised,teacher 's distribution ( to a certain extent ) denoised,0.7283733487129211
translation,64,27,model,denoised,enabling,student,denoised enabling student,0.7956354022026062
translation,64,27,model,student,to circumvent,inaccuracies,student to circumvent inaccuracies,0.6736341118812561
translation,64,27,model,inaccuracies,in,training data,inaccuracies in training data,0.49575668573379517
translation,64,27,model,model,has,teacher 's distribution,model has teacher 's distribution,0.5486124753952026
translation,64,28,model,idea,should be robust to,noise,idea should be robust to noise,0.7957046627998352
translation,64,28,model,idea,introduce,several noise injection techniques,idea introduce several noise injection techniques,0.6921567320823669
translation,64,28,model,both the teacher and the student,should be robust to,noise,both the teacher and the student should be robust to noise,0.7562229633331299
translation,64,28,model,both the teacher and the student,introduce,several noise injection techniques,both the teacher and the student introduce several noise injection techniques,0.6062049865722656
translation,64,28,model,model,capitalize on,idea,model capitalize on idea,0.7219878435134888
translation,64,28,model,model,introduce,several noise injection techniques,model introduce several noise injection techniques,0.714925229549408
translation,64,135,results,skd,improves over,teacher models,skd improves over teacher models,0.7169944047927856
translation,64,135,results,teacher models,in,pretrained ( base -size ) and non-pretrained settings,teacher models in pretrained ( base -size ) and non-pretrained settings,0.5183055400848389
translation,64,137,results,competitive results,with,skd and base - size pretrained models,competitive results with skd and base - size pretrained models,0.6005451083183289
translation,64,137,results,unilm large and t5 11b,on,cnn / dailymail dataset,unilm large and t5 11b on cnn / dailymail dataset,0.5273944735527039
translation,64,137,results,manage to outperform,has,unilm large and t5 11b,manage to outperform has unilm large and t5 11b,0.6454220414161682
translation,64,137,results,results,obtain,competitive results,results obtain competitive results,0.5835188627243042
translation,64,138,results,results,on,wi-kicatsum dataset,results on wi-kicatsum dataset,0.537006139755249
translation,64,146,results,performance,vary across,domains,performance vary across domains,0.6872876286506653
translation,64,146,results,film,showing,least gains,film showing least gains,0.763890266418457
translation,64,156,results,application of skd,trained with,noisy data,application of skd trained with noisy data,0.6679425239562988
translation,64,156,results,application of skd,noisy signals and on,noisy data,application of skd noisy signals and on noisy data,0.7485345005989075
translation,64,156,results,application of skd,improves,factual consistency,application of skd improves factual consistency,0.59353107213974
translation,64,156,results,factual consistency,for,non-pretrained and pretrained models,factual consistency for non-pretrained and pretrained models,0.5743353962898254
translation,64,156,results,non-pretrained and pretrained models,on,both datasets,non-pretrained and pretrained models on both datasets,0.4616784453392029
translation,64,157,results,significantly ( p < 0.05 ) more factually correct,compared to,their teachers,significantly ( p < 0.05 ) more factually correct compared to their teachers,0.6900887489318848
translation,64,157,results,significantly ( p < 0.05 ) more factually correct,using,paired student t-test,significantly ( p < 0.05 ) more factually correct using paired student t-test,0.6527554392814636
translation,64,157,results,transformerabs and unilmv2 base ),using,paired student t-test,transformerabs and unilmv2 base ) using paired student t-test,0.7255094051361084
translation,64,157,results,results,has,all + noisy skd students,results has all + noisy skd students,0.5749301314353943
translation,64,167,results,student ( + noisy skd ),as,significantly ( p < 0.05 ) more succinct and informative,student ( + noisy skd ) as significantly ( p < 0.05 ) more succinct and informative,0.49493247270584106
translation,64,167,results,significantly ( p < 0.05 ) more succinct and informative,compared to,teacher ( unilmv2 base ),significantly ( p < 0.05 ) more succinct and informative compared to teacher ( unilmv2 base ),0.7044864296913147
translation,64,167,results,results,On,cnn / dailymail and xsum datasets,results On cnn / dailymail and xsum datasets,0.4850236177444458
translation,64,178,results,crowdworkers,prefer,summaries,crowdworkers prefer summaries,0.7127463221549988
translation,64,178,results,summaries,produced by,student,summaries produced by student,0.6987330317497253
translation,64,178,results,summaries,not for,company,summaries not for company,0.7274355292320251
translation,64,178,results,student,for,animal and film domains,student for animal and film domains,0.6344074606895447
translation,64,178,results,results,has,crowdworkers,results has crowdworkers,0.5043479800224304
translation,64,195,results,skd and noise injection,observe,non-pretrained models,skd and noise injection observe non-pretrained models,0.6172402501106262
translation,64,195,results,improve,observe,non-pretrained models,improve observe non-pretrained models,0.6108412742614746
translation,64,195,results,non-pretrained models,benefit,more,non-pretrained models benefit more,0.6379792094230652
translation,64,195,results,skd and noise injection,has,improve,skd and noise injection has improve,0.6202771663665771
translation,64,195,results,improve,has,results,improve has results,0.5343002676963806
translation,64,195,results,results,observe,non-pretrained models,results observe non-pretrained models,0.552619218826294
translation,64,195,results,results,has,skd and noise injection,results has skd and noise injection,0.5322942137718201
translation,65,8,experiments,data scarcity,used,"modern pretrained abstractive summarizer bart ( lewis et al. , 2020 )","data scarcity used modern pretrained abstractive summarizer bart ( lewis et al. , 2020 )",0.5908457040786743
translation,65,8,experiments,"modern pretrained abstractive summarizer bart ( lewis et al. , 2020 )",achieves,17.9 rouge - l,"modern pretrained abstractive summarizer bart ( lewis et al. , 2020 ) achieves 17.9 rouge - l",0.6930909752845764
translation,65,10,results,compressed documents,to,bart,compressed documents to bart,0.5882914662361145
translation,65,10,results,compressed documents,observe,6.0 rouge -l improvement,compressed documents observe 6.0 rouge -l improvement,0.5445215702056885
translation,65,75,results,4.8,/,6 rouge - 1/l improvement,4.8 / 6 rouge - 1/l improvement,0.5947069525718689
translation,65,75,results,6 rouge - 1/l improvement,compared to,no extractor baseline ( ne ),6 rouge - 1/l improvement compared to no extractor baseline ( ne ),0.6740932464599609
translation,65,75,results,2.3,/,3.2 r- 1/ l improvement,2.3 / 3.2 r- 1/ l improvement,0.6029350757598877
translation,65,75,results,3.2 r- 1/ l improvement,over,strongest extractor baseline ( per metric ),3.2 r- 1/ l improvement over strongest extractor baseline ( per metric ),0.628944456577301
translation,65,75,results,4.8,has,6 rouge - 1/l improvement,4.8 has 6 rouge - 1/l improvement,0.548414945602417
translation,65,75,results,2.3,has,3.2 r- 1/ l improvement,2.3 has 3.2 r- 1/ l improvement,0.5333349704742432
translation,65,76,results,cnn / dm pretrained bart,on,96 amicus documents,cnn / dm pretrained bart on 96 amicus documents,0.5174048542976379
translation,65,76,results,cnn / dm pretrained bart,helps in,domain adaption,cnn / dm pretrained bart helps in domain adaption,0.7128190994262695
translation,65,76,results,cnn / dm pretrained bart,boosts,rouge scores,cnn / dm pretrained bart boosts rouge scores,0.7019047737121582
translation,65,76,results,96 amicus documents,helps in,domain adaption,96 amicus documents helps in domain adaption,0.7116152048110962
translation,65,76,results,rouge scores,of,baselines,rouge scores of baselines,0.555098295211792
translation,65,76,results,rouge scores,of,our method ( f.t. bart ),rouge scores of our method ( f.t. bart ),0.5521120429039001
translation,65,76,results,results,finetuning,cnn / dm pretrained bart,results finetuning cnn / dm pretrained bart,0.7093444466590881
translation,65,77,results,2.1,/,0.5 r- 1/ l boost,2.1 / 0.5 r- 1/ l boost,0.6281868815422058
translation,65,77,results,2.1,/,outperform,2.1 / outperform,0.5279901623725891
translation,65,77,results,0.5 r- 1/ l boost,in,performance,0.5 r- 1/ l boost in performance,0.5331185460090637
translation,65,77,results,best baseline ( per metric ),by,2.0,best baseline ( per metric ) by 2.0,0.5398563742637634
translation,65,77,results,best baseline ( per metric ),by,1.0 r- 1/ l points,best baseline ( per metric ) by 1.0 r- 1/ l points,0.5616828203201294
translation,65,77,results,best baseline ( per metric ),/,1.0 r- 1/ l points,best baseline ( per metric ) / 1.0 r- 1/ l points,0.5838717222213745
translation,65,77,results,2.0,/,1.0 r- 1/ l points,2.0 / 1.0 r- 1/ l points,0.6205260157585144
translation,65,77,results,2.1,has,0.5 r- 1/ l boost,2.1 has 0.5 r- 1/ l boost,0.5577391386032104
translation,65,77,results,outperform,has,best baseline ( per metric ),outperform has best baseline ( per metric ),0.597663938999176
translation,65,89,results,perplexity,performs,"best , 2.4 rouge - 1","perplexity performs best , 2.4 rouge - 1",0.5519604682922363
translation,65,89,results,perplexity,performs,3.41 rouge - 1,perplexity performs 3.41 rouge - 1,0.5750719904899597
translation,65,89,results,better,than,best alternative,better than best alternative,0.6269290447235107
translation,65,89,results,better,than,entailment,better than entailment,0.6362805366516113
translation,65,89,results,better,than,entailment,better than entailment,0.6362805366516113
translation,65,89,results,"best , 2.4 rouge - 1",has,better,"best , 2.4 rouge - 1 has better",0.5719791650772095
translation,65,89,results,3.41 rouge - 1,has,better,3.41 rouge - 1 has better,0.5924370884895325
translation,65,89,results,results,note,perplexity,results note perplexity,0.5834481716156006
translation,66,9,ablation-analysis,coconet,through,pretraining,coconet through pretraining,0.6920492053031921
translation,66,9,ablation-analysis,pretraining,with,suitable corpora,pretraining with suitable corpora,0.629973828792572
translation,66,9,ablation-analysis,suitable corpora,that simulate,copying behaviors,suitable corpora that simulate copying behaviors,0.7255216240882874
translation,66,9,ablation-analysis,ablation analysis,strengthen,coconet,ablation analysis strengthen coconet,0.6992858052253723
translation,66,149,ablation-analysis,coconet model,leads to,best performance,coconet model leads to best performance,0.6407737135887146
translation,66,149,ablation-analysis,+ 0.38%/0.34%/0.39 %,for,rouge-1/rouge-2/rouge -l,+ 0.38%/0.34%/0.39 % for rouge-1/rouge-2/rouge -l,0.6306666135787964
translation,66,149,ablation-analysis,rouge-1/rouge-2/rouge -l,over,bart model,rouge-1/rouge-2/rouge -l over bart model,0.7094132900238037
translation,66,149,ablation-analysis,pre-training,has,coconet model,pre-training has coconet model,0.5823509097099304
translation,66,149,ablation-analysis,best performance,has,+ 0.38%/0.34%/0.39 %,best performance has + 0.38%/0.34%/0.39 %,0.5497074127197266
translation,66,149,ablation-analysis,ablation analysis,continue,pre-training,ablation analysis continue pre-training,0.7534286975860596
translation,66,150,ablation-analysis,pre-training,without,copying mechanism,pre-training without copying mechanism,0.7521857619285583
translation,66,150,ablation-analysis,bart,with,same pretraining data,bart with same pretraining data,0.6738259792327881
translation,66,150,ablation-analysis,bart,with,small margin,bart with small margin,0.7417336702346802
translation,66,150,ablation-analysis,bart,without,copying mechanism,bart without copying mechanism,0.7454763054847717
translation,66,150,ablation-analysis,pre-training,has,bart,pre-training has bart,0.5779328942298889
translation,66,150,ablation-analysis,pre-training,has,result,pre-training has result,0.6031331419944763
translation,66,150,ablation-analysis,result,has,outperforms,result has outperforms,0.6590793132781982
translation,66,150,ablation-analysis,outperforms,has,bart,outperforms has bart,0.6937707662582397
translation,66,150,ablation-analysis,ablation analysis,continue,pre-training,ablation analysis continue pre-training,0.7534286975860596
translation,66,151,ablation-analysis,effectiveness,of,semantic and positional correlation,effectiveness of semantic and positional correlation,0.6128589510917664
translation,66,151,ablation-analysis,semantic and positional correlation,between,source words,semantic and positional correlation between source words,0.6338098645210266
translation,66,151,ablation-analysis,semantic and positional correlation,observe that,semantic and positional correlation,semantic and positional correlation observe that semantic and positional correlation,0.584389328956604
translation,66,151,ablation-analysis,semantic and positional correlation,depriving,positional correlation,semantic and positional correlation depriving positional correlation,0.6205522418022156
translation,66,151,ablation-analysis,semantic and positional correlation,are,useful,semantic and positional correlation are useful,0.6000552773475647
translation,66,151,ablation-analysis,semantic and positional correlation,depriving,positional correlation,semantic and positional correlation depriving positional correlation,0.6205522418022156
translation,66,151,ablation-analysis,semantic and positional correlation,depriving,decreases,semantic and positional correlation depriving decreases,0.71663498878479
translation,66,151,ablation-analysis,positional correlation,has,decreases,positional correlation has decreases,0.6040854454040527
translation,66,151,ablation-analysis,decreases,has,performance,decreases has performance,0.5981842875480652
translation,66,151,ablation-analysis,performance,has,larger,performance has larger,0.6343140006065369
translation,66,151,ablation-analysis,ablation analysis,study,effectiveness,ablation analysis study effectiveness,0.6689013838768005
translation,66,163,ablation-analysis,copying phenomenon,is,more common,copying phenomenon is more common,0.5630241632461548
translation,66,163,ablation-analysis,more common,in,samsum dataset,more common in samsum dataset,0.49881511926651
translation,66,163,ablation-analysis,more common,with,14.4 %,more common with 14.4 %,0.5237241387367249
translation,66,163,ablation-analysis,14.4 %,of,source words,14.4 % of source words,0.5376873016357422
translation,66,163,ablation-analysis,reappearing,in,target summary,reappearing in target summary,0.5480056405067444
translation,66,163,ablation-analysis,source words,has,reappearing,source words has reappearing,0.5063722729682922
translation,66,130,baselines,pgnet,is,hybrid pointergenerator model,pgnet is hybrid pointergenerator model,0.5653600096702576
translation,66,130,baselines,hybrid pointergenerator model,applying,attentional copy mechanism,hybrid pointergenerator model applying attentional copy mechanism,0.6821293830871582
translation,66,130,baselines,baselines,has,pgnet,baselines has pgnet,0.6229699850082397
translation,66,131,baselines,drm,is,deep reinforced model,drm is deep reinforced model,0.5577960014343262
translation,66,131,baselines,deep reinforced model,with,intra-attention mechanism,deep reinforced model with intra-attention mechanism,0.6077373027801514
translation,66,131,baselines,baselines,has,drm,baselines has drm,0.5797340273857117
translation,66,138,baselines,"bertsumextabs ( liu and lapata , 2019 )",applies,bert,"bertsumextabs ( liu and lapata , 2019 ) applies bert",0.6308658719062805
translation,66,138,baselines,"bertsumextabs ( liu and lapata , 2019 )",in,text summarization,"bertsumextabs ( liu and lapata , 2019 ) in text summarization",0.49872463941574097
translation,66,138,baselines,bert,in,text summarization,bert in text summarization,0.5463519096374512
translation,66,138,baselines,baselines,has,"bertsumextabs ( liu and lapata , 2019 )","baselines has bertsumextabs ( liu and lapata , 2019 )",0.5615671873092651
translation,66,140,baselines,sagcopy,fine -tunes,mass,sagcopy fine -tunes mass,0.7116429209709167
translation,66,140,baselines,mass,by incorporating,importance score,mass by incorporating importance score,0.6900869011878967
translation,66,140,baselines,importance score,for,source words,importance score for source words,0.5612266659736633
translation,66,140,baselines,sagcopy,has,"xu et al. , 2020","sagcopy has xu et al. , 2020",0.5730832815170288
translation,66,140,baselines,baselines,has,sagcopy,baselines has sagcopy,0.5881273150444031
translation,66,141,baselines,gapsentence generation,as,pre-training objective,gapsentence generation as pre-training objective,0.5170836448669434
translation,66,154,baselines,fast abs rl,is,hybrid extractive - abstractive model,fast abs rl is hybrid extractive - abstractive model,0.5658127665519714
translation,66,154,baselines,hybrid extractive - abstractive model,with,policy - based reinforcement learning,hybrid extractive - abstractive model with policy - based reinforcement learning,0.6343653202056885
translation,66,154,baselines,baselines,has,fast abs rl,baselines has fast abs rl,0.6020229458808899
translation,66,155,baselines,"transformerabs ( vaswani et al. , 2017 )",is,basic transformer - based seq2seq model,"transformerabs ( vaswani et al. , 2017 ) is basic transformer - based seq2seq model",0.5328110456466675
translation,66,155,baselines,basic transformer - based seq2seq model,without,pre-training,basic transformer - based seq2seq model without pre-training,0.67242032289505
translation,66,155,baselines,baselines,has,"transformerabs ( vaswani et al. , 2017 )","baselines has transformerabs ( vaswani et al. , 2017 )",0.5212312936782837
translation,66,158,baselines,"d-hgn ( feng et al. , 2020 )",is,dialogue heterogeneous graph network,"d-hgn ( feng et al. , 2020 ) is dialogue heterogeneous graph network",0.5481175184249878
translation,66,158,baselines,dialogue heterogeneous graph network,modeling,utterance and commonsense knowledge,dialogue heterogeneous graph network modeling utterance and commonsense knowledge,0.719385027885437
translation,66,158,baselines,baselines,has,"d-hgn ( feng et al. , 2020 )","baselines has d-hgn ( feng et al. , 2020 )",0.5323400497436523
translation,66,159,baselines,tgdga,is,topic-word guided dialogue,tgdga is topic-word guided dialogue,0.556253969669342
translation,66,159,baselines,topic-word guided dialogue,based on,graph attention model,topic-word guided dialogue based on graph attention model,0.6318678259849548
translation,66,159,baselines,baselines,has,tgdga,baselines has tgdga,0.5525380373001099
translation,66,117,experimental-setup,learning rate,set to,3e - 5,learning rate set to 3e - 5,0.7297655940055847
translation,66,117,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,66,117,experimental-setup,experimental setup,has,learning decay,experimental setup has learning decay,0.49578988552093506
translation,66,119,experimental-setup,dropout,with,probability,dropout with probability,0.6311832070350647
translation,66,119,experimental-setup,dropout,with,gradient clipping,dropout with gradient clipping,0.6046308279037476
translation,66,119,experimental-setup,probability,of,0.1,probability of 0.1,0.6106660962104797
translation,66,119,experimental-setup,gradient clipping,of,0.1,gradient clipping of 0.1,0.5792549252510071
translation,66,119,experimental-setup,experimental setup,use,dropout,experimental setup use dropout,0.5759405493736267
translation,66,119,experimental-setup,experimental setup,use,gradient clipping,experimental setup use gradient clipping,0.5653892755508423
translation,66,120,experimental-setup,hyper-parameters,set to,values,hyper-parameters set to values,0.7089634537696838
translation,66,120,experimental-setup,values,used in,bart,values used in bart,0.77493816614151
translation,66,120,experimental-setup,experimental setup,has,hyper-parameters,experimental setup has hyper-parameters,0.5121466517448425
translation,66,121,experimental-setup,clipping distance,of,16,clipping distance of 16,0.6491110324859619
translation,66,121,experimental-setup,16,when computing,positional correlation,16 when computing positional correlation,0.6571484804153442
translation,66,121,experimental-setup,experimental setup,use,clipping distance,experimental setup use clipping distance,0.5794199109077454
translation,66,122,experimental-setup,our model,with,cocopretrain,our model with cocopretrain,0.7413707971572876
translation,66,122,experimental-setup,converges,within,1 m steps,converges within 1 m steps,0.6435601711273193
translation,66,122,experimental-setup,1 m steps,using,batch size,1 m steps using batch size,0.6815502047538757
translation,66,122,experimental-setup,batch size,of,8000,batch size of 8000,0.6684168577194214
translation,66,122,experimental-setup,experimental setup,continually pre-train,our model,experimental setup continually pre-train our model,0.6965129375457764
translation,66,123,experimental-setup,decoding,use,beam search,decoding use beam search,0.6366719603538513
translation,66,123,experimental-setup,beam search,with,beam size,beam search with beam size,0.6320963501930237
translation,66,123,experimental-setup,beam size,of,4,beam size of 4,0.6962505578994751
translation,66,123,experimental-setup,experimental setup,During,decoding,experimental setup During decoding,0.6438420414924622
translation,66,32,experiments,coconet,based on,transformer - based seq2seq architecture,coconet based on transformer - based seq2seq architecture,0.6603643298149109
translation,66,118,experiments,adam optimizer,with,"? 1 = 0.9 , ? 1 = 0.999 , and = 10 ?8","adam optimizer with ? 1 = 0.9 , ? 1 = 0.999 , and = 10 ?8",0.6269769668579102
translation,66,7,model,novel copying scheme,named,correlational copying network ( coconet ),novel copying scheme named correlational copying network ( coconet ),0.7117013335227966
translation,66,7,model,novel copying scheme,enhances,standard copying mechanism,novel copying scheme enhances standard copying mechanism,0.6468297243118286
translation,66,7,model,standard copying mechanism,by keeping track of,copying history,standard copying mechanism by keeping track of copying history,0.6946183443069458
translation,66,7,model,model,propose,novel copying scheme,model propose novel copying scheme,0.6863505244255066
translation,66,8,model,prior copying distributions,at,each time step,prior copying distributions at each time step,0.5069328546524048
translation,66,8,model,prior copying distributions,explicitly encourages,model,prior copying distributions explicitly encourages model,0.7232776880264282
translation,66,8,model,model,to copy,input word,model to copy input word,0.6022124290466309
translation,66,8,model,input word,relevant to,previously copied one,input word relevant to previously copied one,0.632603645324707
translation,66,8,model,model,to copy,input word,model to copy input word,0.6022124290466309
translation,66,31,model,novel copying architecture,named,correlational copying network ( coconet ),novel copying architecture named correlational copying network ( coconet ),0.7106364369392395
translation,66,31,model,novel copying architecture,learn to copy,copying history,novel copying architecture learn to copy copying history,0.7310000658035278
translation,66,31,model,model,propose,novel copying architecture,model propose novel copying architecture,0.6909648180007935
translation,66,33,model,coconet,copies,input text,coconet copies input text,0.7845461368560791
translation,66,33,model,coconet,from,input text,coconet from input text,0.5729786157608032
translation,66,33,model,input text,at,each time step,input text at each time step,0.555651068687439
translation,66,33,model,model,has,coconet,model has coconet,0.6335712671279907
translation,66,43,model,copying,through,self-supervised pre-training,copying through self-supervised pre-training,0.6917597651481628
translation,66,43,model,self-supervised pre-training,on,text span generation,self-supervised pre-training on text span generation,0.5111145377159119
translation,66,43,model,text span generation,with,copying,text span generation with copying,0.6621109843254089
translation,66,43,model,model,enhance,coconet,model enhance coconet,0.7174019813537598
translation,66,128,results,results,on,cnn / dailymail,results on cnn / dailymail,0.5091644525527954
translation,66,145,results,models,with,pretraining,models with pretraining,0.6309548020362854
translation,66,145,results,pretraining,has,outperform,pretraining has outperform,0.6275476813316345
translation,66,145,results,outperform,has,most of the models,outperform has most of the models,0.589011549949646
translation,66,145,results,most of the models,has,without pre-training,most of the models has without pre-training,0.5814524292945862
translation,66,146,results,bart model,with,attentional copying,bart model with attentional copying,0.6315588355064392
translation,66,146,results,bart model,improve,results,bart model improve results,0.5851386189460754
translation,66,146,results,results,over,original bart model,results over original bart model,0.5902309417724609
translation,66,146,results,fine-tuning,has,bart model,fine-tuning has bart model,0.5846378207206726
translation,66,146,results,results,over,original bart model,results over original bart model,0.5902309417724609
translation,66,146,results,results,has,fine-tuning,results has fine-tuning,0.5543118715286255
translation,66,147,results,sagcopy mechanism,to,bart model,sagcopy mechanism to bart model,0.5632593035697937
translation,66,147,results,sagcopy mechanism,obtaining,superior results,sagcopy mechanism obtaining superior results,0.6925866603851318
translation,66,147,results,superior results,over,bart,superior results over bart,0.6812276244163513
translation,66,147,results,bart,0.19%/0.14%/0.15 % for,rouge-1/ rouge-2/rouge -l,bart 0.19%/0.14%/0.15 % for rouge-1/ rouge-2/rouge -l,0.7736021876335144
translation,66,148,results,improvement,for,our proposed coconet model,improvement for our proposed coconet model,0.6085136532783508
translation,66,148,results,our proposed coconet model,is,larger ( + 0.27%/0.20%/0.20 %,our proposed coconet model is larger ( + 0.27%/0.20%/0.20 %,0.5686600804328918
translation,66,148,results,larger ( + 0.27%/0.20%/0.20 %,for,rouge-1/rouge-2/rouge -l ),larger ( + 0.27%/0.20%/0.20 % for rouge-1/rouge-2/rouge -l ),0.6504405736923218
translation,66,160,results,outperform,to,significant extent,outperform to significant extent,0.6537190079689026
translation,66,160,results,models without pretraining,to,significant extent,models without pretraining to significant extent,0.5754177570343018
translation,66,160,results,models with pretraining,has,outperform,models with pretraining has outperform,0.6268076300621033
translation,66,160,results,outperform,has,models without pretraining,outperform has models without pretraining,0.5561339259147644
translation,66,161,results,better performances,than,attentional copying,better performances than attentional copying,0.5830427408218384
translation,66,161,results,better performances,than,self-attention guided copying,better performances than self-attention guided copying,0.5990688800811768
translation,66,161,results,coconet,has,better performances,coconet has better performances,0.6040441393852234
translation,66,169,results,coconet,receives,comparative results,coconet receives comparative results,0.6422286629676819
translation,66,169,results,coconet,shows,significant increase,coconet shows significant increase,0.7165828943252563
translation,66,169,results,comparative results,as,bart,comparative results as bart,0.6205179691314697
translation,66,169,results,comparative results,as,bart,comparative results as bart,0.6205179691314697
translation,66,169,results,comparative results,comparing to,bart,comparative results comparing to bart,0.7681061029434204
translation,66,169,results,significant increase,in,readability,significant increase in readability,0.5092471241950989
translation,66,169,results,readability,comparing to,bart,readability comparing to bart,0.7017414569854736
translation,66,169,results,informativeness,has,coconet,informativeness has coconet,0.5623611807823181
translation,66,169,results,results,For,informativeness,results For informativeness,0.6021713614463806
translation,66,170,results,results,has,effect of pre-training data selection,results has effect of pre-training data selection,0.5385064482688904
translation,66,174,results,coconet,improves,summarization model,coconet improves summarization model,0.6184921264648438
translation,66,174,results,summarization model,has,qualitatively and quantitatively,summarization model has qualitatively and quantitatively,0.5409209132194519
translation,66,174,results,results,demonstrated,coconet,results demonstrated coconet,0.6989787817001343
translation,67,98,ablation-analysis,tackling explainability,improves,prediction and recommendation performance,tackling explainability improves prediction and recommendation performance,0.5921169519424438
translation,67,98,ablation-analysis,prediction and recommendation performance,has,consequentially,prediction and recommendation performance has consequentially,0.5861051678657532
translation,67,98,ablation-analysis,ablation analysis,appears to be,trend,ablation analysis appears to be trend,0.6657675504684448
translation,67,173,ablation-analysis,coupling bert ( a superior review feature extractor ),with,embedding clustering,coupling bert ( a superior review feature extractor ) with embedding clustering,0.6380162835121155
translation,67,173,ablation-analysis,embedding clustering,enables,user and item representations,embedding clustering enables user and item representations,0.6513803005218506
translation,67,173,ablation-analysis,user and item representations,to have,finer granularity,user and item representations to have finer granularity,0.5946711897850037
translation,67,173,ablation-analysis,user and item representations,to have,fewer redundancies,user and item representations to have fewer redundancies,0.6082955002784729
translation,67,222,ablation-analysis,unifying representations and explanations,in the form of,extractive summaries,unifying representations and explanations in the form of extractive summaries,0.6620268225669861
translation,67,222,ablation-analysis,unifying representations and explanations,further enhanced,collaborative filtering accuracy and explainability,unifying representations and explanations further enhanced collaborative filtering accuracy and explainability,0.6478854417800903
translation,67,15,baselines,baselines,has,cf,baselines has cf,0.6367408633232117
translation,67,161,baselines,escofilt,used,same optimizer,escofilt used same optimizer,0.6242849230766296
translation,67,161,baselines,adam,leverages,power,adam leverages power,0.8033528327941895
translation,67,161,baselines,power,of,adaptive learning rates,power of adaptive learning rates,0.608355700969696
translation,67,161,baselines,adaptive learning rates,during,training,adaptive learning rates during training,0.7356020212173462
translation,67,155,experimental-setup,pre-trained bert large model,afforded by,transformers library of huggingface,pre-trained bert large model afforded by transformers library of huggingface,0.6429431438446045
translation,67,155,experimental-setup,experimental setup,utilized,pre-trained bert large model,experimental setup utilized pre-trained bert large model,0.5938525199890137
translation,67,156,experimental-setup,learning rate,has,0.006,learning rate has 0.006,0.5547415018081665
translation,67,156,experimental-setup,quantity of mlp layers,has,4,quantity of mlp layers has 4,0.5931031107902527
translation,67,156,experimental-setup,item summary ratio ( ? i ),has,0.4,item summary ratio ( ? i ) has 0.4,0.5368066430091858
translation,67,156,experimental-setup,user summary ratio ( ? u ),has,0.4,user summary ratio ( ? u ) has 0.4,0.5642199516296387
translation,67,157,experimental-setup,number of epochs,has,"[ 1 , 30 ]","number of epochs has [ 1 , 30 ]",0.5167281031608582
translation,67,157,experimental-setup,latent vector dimension ( m ),has,"{ 32 , 128 , 220 }","latent vector dimension ( m ) has { 32 , 128 , 220 }",0.5435375571250916
translation,67,157,experimental-setup,experimental setup,operated,exhaustive grid search,experimental setup operated exhaustive grid search,0.6542744636535645
translation,67,160,experimental-setup,experimental setup,performed,exhaustive grid search,experimental setup performed exhaustive grid search,0.25262686610221863
translation,67,165,experimental-setup,our experiments,on,nvidia geforce rtx 2080 ti,our experiments on nvidia geforce rtx 2080 ti,0.5623056292533875
translation,67,165,experimental-setup,experimental setup,ran,our experiments,experimental setup ran our experiments,0.5802465677261353
translation,67,4,model,first extractive summarizationbased collaborative filtering,called,es - cofilt,first extractive summarizationbased collaborative filtering called es - cofilt,0.714536726474762
translation,67,4,model,model,pioneer,first extractive summarizationbased collaborative filtering,model pioneer first extractive summarizationbased collaborative filtering,0.7113287448883057
translation,67,9,model,multilayer perceptron,to learn,sentence embeddings,multilayer perceptron to learn sentence embeddings,0.6017604470252991
translation,67,9,model,multilayer perceptron,to learn,representation - explanations,multilayer perceptron to learn representation - explanations,0.6407706141471863
translation,67,9,model,multilayer perceptron,to learn,user-item interactions,multilayer perceptron to learn user-item interactions,0.6157050132751465
translation,67,9,model,model,uniquely integrates,bert,model uniquely integrates bert,0.7276577949523926
translation,67,9,model,model,uniquely integrates,k-means embedding clustering,model uniquely integrates k-means embedding clustering,0.6853223443031311
translation,67,9,model,model,uniquely integrates,multilayer perceptron,model uniquely integrates multilayer perceptron,0.6841269731521606
translation,67,62,model,first extractive summarization - based collaborative filtering model,has,escofilt,first extractive summarization - based collaborative filtering model has escofilt,0.6055793166160583
translation,67,62,model,model,propose,first extractive summarization - based collaborative filtering model,model propose first extractive summarization - based collaborative filtering model,0.6180642247200012
translation,67,75,model,deepconn,is,first deep learning - based model,deepconn is first deep learning - based model,0.5782536268234253
translation,67,75,model,first deep learning - based model,representing,users and items,first deep learning - based model representing users and items,0.6190875172615051
translation,67,75,model,users and items,from,reviews,users and items from reviews,0.5519798398017883
translation,67,75,model,reviews,in,coordinated manner,reviews in coordinated manner,0.5575979351997375
translation,67,75,model,model,has,deepconn,model has deepconn,0.5786690711975098
translation,67,76,model,two parallel networks,powered by,convolutional neural networks ( cnn ),two parallel networks powered by convolutional neural networks ( cnn ),0.5527344942092896
translation,67,76,model,model,consists of,two parallel networks,model consists of two parallel networks,0.6321172714233398
translation,67,77,model,one network,learns,user behavior,one network learns user behavior,0.6493664979934692
translation,67,77,model,user behavior,by examining,all reviews,user behavior by examining all reviews,0.6369799971580505
translation,67,77,model,user behavior,by examining,all reviews,user behavior by examining all reviews,0.6369799971580505
translation,67,77,model,all reviews,has written,other network,all reviews has written other network,0.6766402721405029
translation,67,77,model,other network,models,item properties,other network models item properties,0.7318041324615479
translation,67,77,model,item properties,by exploring,all reviews,item properties by exploring all reviews,0.6415961384773254
translation,67,77,model,model,has,one network,model has one network,0.609742283821106
translation,67,86,model,d- attn,integrates,global and local attention,d- attn integrates global and local attention,0.6661235094070435
translation,67,86,model,global and local attention,to score,each word,global and local attention to score each word,0.6627314686775208
translation,67,86,model,each word,to determine,relevance,each word to determine relevance,0.6857110261917114
translation,67,86,model,relevance,in,review text,relevance in review text,0.3422704041004181
translation,67,86,model,model,has,d- attn,model has d- attn,0.6845280528068542
translation,67,171,results,all baselines,across,all datasets,all baselines across all datasets,0.7087365388870239
translation,67,171,results,proposed model,has,consistently outperforms,proposed model has consistently outperforms,0.6232210397720337
translation,67,171,results,consistently outperforms,has,all baselines,consistently outperforms has all baselines,0.5969254374504089
translation,67,171,results,results,has,proposed model,results has proposed model,0.5938616394996643
translation,67,174,results,bert - based models ( escofilt and benefict ),have,generally better prediction accuracies,bert - based models ( escofilt and benefict ) have generally better prediction accuracies,0.5254032611846924
translation,67,174,results,generally better prediction accuracies,than,rest of the mostly cnn - powered baselines,generally better prediction accuracies than rest of the mostly cnn - powered baselines,0.5471360087394714
translation,67,174,results,two lowest average rmse values,has,bert - based models ( escofilt and benefict ),two lowest average rmse values has bert - based models ( escofilt and benefict ),0.5463114380836487
translation,67,174,results,results,receiving,two lowest average rmse values,results receiving two lowest average rmse values,0.7082520127296448
translation,67,207,results,escofilt - derived explanations,have,highest explainability scores,escofilt - derived explanations have highest explainability scores,0.5388706922531128
translation,67,207,results,five out of seven criteria,has,escofilt - derived explanations,five out of seven criteria has escofilt - derived explanations,0.6034438610076904
translation,67,207,results,results,For,five out of seven criteria,results For five out of seven criteria,0.5647590160369873
translation,67,216,results,our model,produces,most helpful explanations,our model produces most helpful explanations,0.5894498229026794
translation,67,216,results,almost 83 %,of,items,almost 83 % of items,0.6292150616645813
translation,67,216,results,results,has,our model,results has our model,0.5871725678443909
translation,68,147,ablation-analysis,action graphs,received,higher ? weights,action graphs received higher ? weights,0.48944196105003357
translation,68,147,ablation-analysis,higher ? weights,after,training,higher ? weights after training,0.7287879586219788
translation,68,147,ablation-analysis,training,in,both initializing settings,training in both initializing settings,0.5361210107803345
translation,68,147,ablation-analysis,discourse graphs,has,action graphs,discourse graphs has action graphs,0.5806568264961243
translation,68,147,ablation-analysis,ablation analysis,Compared to,discourse graphs,ablation analysis Compared to discourse graphs,0.6102361679077148
translation,68,148,ablation-analysis,both graphs,led to,higher rezero weights,both graphs led to higher rezero weights,0.6858713030815125
translation,68,148,ablation-analysis,ablation analysis,Utilizing,both graphs,ablation analysis Utilizing both graphs,0.6381867527961731
translation,68,90,baselines,baselines,has,"transformer ( vaswani et al. , 2017 )","baselines has transformer ( vaswani et al. , 2017 )",0.5352681279182434
translation,68,91,baselines,transformer seq2seq models,following,opennmt,transformer seq2seq models following opennmt,0.7016320824623108
translation,68,91,baselines,commonsense knowledge,from,"conceptnet ( liu and singh , 2004 )","commonsense knowledge from conceptnet ( liu and singh , 2004 )",0.5595777630805969
translation,68,91,baselines,commonsense knowledge,for,dialogue summarization,commonsense knowledge for dialogue summarization,0.538107693195343
translation,68,92,baselines,baselines,has,"bart ( lewis et al. , 2020 )","baselines has bart ( lewis et al. , 2020 )",0.5562279224395752
translation,68,97,experimental-setup,bart - base model,to initialize,sequence - to-sequence model,bart - base model to initialize sequence - to-sequence model,0.7156531810760498
translation,68,97,experimental-setup,sequence - to-sequence model,for,training,sequence - to-sequence model for training,0.613848865032196
translation,68,97,experimental-setup,experimental setup,used,bart - base model,experimental setup used bart - base model,0.6078587770462036
translation,68,98,experimental-setup,parameters,in,bart encoder / decoder,parameters in bart encoder / decoder,0.5015800595283508
translation,68,98,experimental-setup,parameters,followed,default settings,parameters followed default settings,0.6272156834602356
translation,68,98,experimental-setup,parameters,set,learning rate,parameters set learning rate,0.6370671987533569
translation,68,98,experimental-setup,learning rate,with,120 warm - up steps,learning rate with 120 warm - up steps,0.6170583963394165
translation,68,98,experimental-setup,3e - 5,with,120 warm - up steps,3e - 5 with 120 warm - up steps,0.660943865776062
translation,68,98,experimental-setup,learning rate,has,3e - 5,learning rate has 3e - 5,0.5670245885848999
translation,68,98,experimental-setup,experimental setup,For,parameters,experimental setup For parameters,0.5588853359222412
translation,68,98,experimental-setup,experimental setup,set,learning rate,experimental setup set learning rate,0.6158276796340942
translation,68,99,experimental-setup,graph encoders,set,number of hidden dimensions,graph encoders set number of hidden dimensions,0.632278561592102
translation,68,99,experimental-setup,graph encoders,set,number of attention heads,graph encoders set number of attention heads,0.6462032794952393
translation,68,99,experimental-setup,graph encoders,set,number of layers,graph encoders set number of layers,0.6357137560844421
translation,68,99,experimental-setup,graph encoders,set,dropout rate,graph encoders set dropout rate,0.6402227282524109
translation,68,99,experimental-setup,number of hidden dimensions,as,768,number of hidden dimensions as 768,0.554094135761261
translation,68,99,experimental-setup,number of attention heads,as,2,number of attention heads as 2,0.5864146947860718
translation,68,99,experimental-setup,number of layers,as,2,number of layers as 2,0.5884179472923279
translation,68,99,experimental-setup,dropout rate,as,0.2,dropout rate as 0.2,0.5192973017692566
translation,68,99,experimental-setup,experimental setup,For,graph encoders,experimental setup For graph encoders,0.5594354271888733
translation,68,100,experimental-setup,graph cross attentions,added to,bart decoder layers,graph cross attentions added to bart decoder layers,0.6642032861709595
translation,68,100,experimental-setup,number of attention heads,as,2,number of attention heads as 2,0.5864146947860718
translation,68,100,experimental-setup,experimental setup,For,graph cross attentions,experimental setup For graph cross attentions,0.5767269134521484
translation,68,101,experimental-setup,weights,in,rezero residual connections,weights in rezero residual connections,0.47849270701408386
translation,68,101,experimental-setup,rezero residual connections,initialized with,1,rezero residual connections initialized with 1,0.7910006046295166
translation,68,101,experimental-setup,experimental setup,has,weights,experimental setup has weights,0.5104771852493286
translation,68,102,experimental-setup,learning rate,for,parameters,learning rate for parameters,0.6131263375282288
translation,68,102,experimental-setup,parameters,in,newly added modules,parameters in newly added modules,0.5206587910652161
translation,68,102,experimental-setup,parameters,was,3e - 4,parameters was 3e - 4,0.5901558995246887
translation,68,102,experimental-setup,3e - 4,with,60 warm - up steps,3e - 4 with 60 warm - up steps,0.6676262617111206
translation,68,102,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,68,103,experimental-setup,experiments,performed on,geforce rtx 2080 ti ( 11 gb memory ),experiments performed on geforce rtx 2080 ti ( 11 gb memory ),0.6552010178565979
translation,68,103,experimental-setup,experimental setup,performed on,geforce rtx 2080 ti ( 11 gb memory ),experimental setup performed on geforce rtx 2080 ti ( 11 gb memory ),0.6271165013313293
translation,68,103,experimental-setup,experimental setup,has,experiments,experimental setup has experiments,0.5502888560295105
translation,68,6,model,rich structures,in,conversations,rich structures in conversations,0.6144130825996399
translation,68,6,model,rich structures,in,conversations,rich structures in conversations,0.6144130825996399
translation,68,6,model,more precise and accurate conversation summarization,by first incorporating,discourse relations,more precise and accurate conversation summarization by first incorporating discourse relations,0.6028202772140503
translation,68,6,model,discourse relations,between,utterances,discourse relations between utterances,0.6152561902999878
translation,68,6,model,discourse relations,in,utterances,discourse relations in utterances,0.5388210415840149
translation,68,6,model,action triples (   who - doing - what   ),in,utterances,action triples (   who - doing - what   ) in utterances,0.5159644484519958
translation,68,6,model,utterances,through,structured graphs,utterances through structured graphs,0.6177830100059509
translation,68,6,model,structured graphs,to better encode,conversations,structured graphs to better encode conversations,0.6716447472572327
translation,68,6,model,multi-granularity decoder,to generate,summaries,multi-granularity decoder to generate summaries,0.6650908589363098
translation,68,6,model,summaries,by combining,all levels of information,summaries by combining all levels of information,0.7137077450752258
translation,68,6,model,model,explicitly model,rich structures,model explicitly model rich structures,0.8228711485862732
translation,68,6,model,model,designing,multi-granularity decoder,model designing multi-granularity decoder,0.6746008992195129
translation,68,23,model,structure - aware sequence - to- sequence model,equip,abstractive conversation summarization models,structure - aware sequence - to- sequence model equip abstractive conversation summarization models,0.6546018719673157
translation,68,23,model,abstractive conversation summarization models,with,rich conversation structures,abstractive conversation summarization models with rich conversation structures,0.516836941242218
translation,68,23,model,rich conversation structures,through,two types of graphs,rich conversation structures through two types of graphs,0.6506605744361877
translation,68,23,model,model,present,structure - aware sequence - to- sequence model,model present structure - aware sequence - to- sequence model,0.6613205671310425
translation,68,23,model,model,equip,abstractive conversation summarization models,model equip abstractive conversation summarization models,0.6350758671760559
translation,68,24,model,model,has,discourse relation graphs,model has discourse relation graphs,0.5520154237747192
translation,68,42,model,actions,within,utterances,actions within utterances,0.6348404884338379
translation,68,42,model,actions,within,utterances,actions within utterances,0.6348404884338379
translation,68,42,model,utterances,in,conversations,utterances in conversations,0.5681173205375671
translation,68,42,model,relations,between,utterances,relations between utterances,0.6944836378097534
translation,68,42,model,relations,through,multigranularity decoders,relations through multigranularity decoders,0.6828895807266235
translation,68,42,model,utterances,in,conversations,utterances in conversations,0.5681173205375671
translation,68,42,model,conversations,in,structured way,conversations in structured way,0.5429031848907471
translation,68,42,model,discourse relation graphs and action graphs,through,multigranularity decoders,discourse relation graphs and action graphs through multigranularity decoders,0.6456971764564514
translation,68,42,model,multigranularity decoders,for,abstractive conversation summarization,multigranularity decoders for abstractive conversation summarization,0.546289324760437
translation,68,42,model,model,explicitly model,actions,model explicitly model actions,0.8064755797386169
translation,68,42,model,model,explicitly model,relations,model explicitly model relations,0.8017373085021973
translation,68,105,results,model variants,of,s-bart,model variants of s-bart,0.6306432485580444
translation,68,105,results,model variants,of,bart,model variants of bart,0.618148684501648
translation,68,105,results,significantly higher ratings,than,bart,significantly higher ratings than bart,0.5714866518974304
translation,68,108,results,simple conversation structures,such as,topics and conversation stages,simple conversation structures such as topics and conversation stages,0.6343762278556824
translation,68,108,results,pre-trained models,has,multi-view seq2seq boosted rouge scores,pre-trained models has multi-view seq2seq boosted rouge scores,0.5446701645851135
translation,68,108,results,simple conversation structures,has,multi-view seq2seq boosted rouge scores,simple conversation structures has multi-view seq2seq boosted rouge scores,0.540867805480957
translation,68,108,results,results,equipped with,pre-trained models,results equipped with pre-trained models,0.633100688457489
translation,68,110,results,two different structured graphs,produced,better rouge scores,two different structured graphs produced better rouge scores,0.5971633195877075
translation,68,110,results,better rouge scores,compared to,previous state - of - the - art methods,better rouge scores compared to previous state - of - the - art methods,0.5870271325111389
translation,68,110,results,better rouge scores,compared to,our base models,better rouge scores compared to our base models,0.5841673016548157
translation,68,110,results,our base models,with,increase,our base models with increase,0.7157555818557739
translation,68,110,results,increase,of,2.0 %,increase of 2.0 %,0.6053988337516785
translation,68,110,results,increase,of,4.3 %,increase of 4.3 %,0.5885306000709534
translation,68,110,results,increase,of,1.2 %,increase of 1.2 %,0.604479193687439
translation,68,110,results,2.0 %,on,rouge - 1,2.0 % on rouge - 1,0.5869783759117126
translation,68,110,results,2.0 %,on,rouge -l,2.0 % on rouge -l,0.6179713010787964
translation,68,110,results,4.3 %,on,rouge - 2,4.3 % on rouge - 2,0.5876118540763855
translation,68,110,results,1.2 %,on,rouge -l,1.2 % on rouge -l,0.6220112442970276
translation,68,110,results,rouge -l,compared to,our base model,rouge -l compared to our base model,0.6898528337478638
translation,68,110,results,results,Combining,two different structured graphs,results Combining two different structured graphs,0.6896161437034607
translation,68,119,results,s- bart,utilized,structured information,s- bart utilized structured information,0.6708314418792725
translation,68,119,results,structured information,from,discourse relation graphs and action graphs,structured information from discourse relation graphs and action graphs,0.5467950701713562
translation,68,119,results,structured information,generated,significantly better summaries,structured information generated significantly better summaries,0.6655176877975464
translation,68,119,results,significantly better summaries,with respect to,factualness,significantly better summaries with respect to factualness,0.6610698699951172
translation,68,119,results,significantly better summaries,with respect to,succinctness,significantly better summaries with respect to succinctness,0.6393224000930786
translation,68,119,results,significantly better summaries,with respect to,informativeness,significantly better summaries with respect to informativeness,0.6206599473953247
translation,68,121,results,connections,between,speakers and actions,connections between speakers and actions,0.6683986186981201
translation,68,121,results,speakers and actions,greatly helped generate,more factual summaries,speakers and actions greatly helped generate more factual summaries,0.6629581451416016
translation,68,121,results,more factual summaries,than,baselines,more factual summaries than baselines,0.5978094935417175
translation,68,121,results,results,Modeling,connections,results Modeling connections,0.6240727305412292
translation,68,129,results,action,boosted,rouge scores,action boosted rouge scores,0.733247697353363
translation,68,129,results,rouge scores,compared to,bart,rouge scores compared to bart,0.6727932691574097
translation,68,129,results,results,has,action,results has action,0.46160194277763367
translation,68,130,results,action graphs,led to,much more gains,action graphs led to much more gains,0.6955511569976807
translation,68,130,results,much more gains,than,discourse graphs,much more gains than discourse graphs,0.6504141688346863
translation,68,133,results,discourse&action,achieved,better results,discourse&action achieved better results,0.7015823721885681
translation,68,133,results,better results,with,increase,better results with increase,0.6730682849884033
translation,68,133,results,increase,of,66.2 %,increase of 66.2 %,0.5764358639717102
translation,68,133,results,increase,of,373.4 %,increase of 373.4 %,0.5686060786247253
translation,68,133,results,increase,of,82.2 %,increase of 82.2 %,0.5686987638473511
translation,68,133,results,66.2 %,on,rouge -1,66.2 % on rouge -1,0.5568673014640808
translation,68,133,results,373.4 %,on,rouge - 2,373.4 % on rouge - 2,0.5710338354110718
translation,68,133,results,82.2 %,on,rouge -l,82.2 % on rouge -l,0.5842779874801636
translation,68,133,results,rouge -l,over,bart,rouge -l over bart,0.6259807348251343
translation,68,133,results,results,has,discourse&action,results has discourse&action,0.5357365012168884
translation,68,140,results,s-bart,with,our discourse graphs,s-bart with our discourse graphs,0.6871153712272644
translation,68,140,results,our discourse graphs,has,outperformed,our discourse graphs has outperformed,0.5800697207450867
translation,68,140,results,outperformed,has,1387,outperformed has 1387,0.6542453169822693
translation,68,140,results,results,found that,s-bart,results found that s-bart,0.621183454990387
translation,68,142,results,parallel strategy,showed,better performances,parallel strategy showed better performances,0.673798143863678
translation,68,142,results,sequential ones,did not introduce,gains,sequential ones did not introduce gains,0.7507436275482178
translation,68,142,results,gains,compared to,s-bart,gains compared to s-bart,0.7481338381767273
translation,68,142,results,s-bart,with,single graphs,s-bart with single graphs,0.7014327049255371
translation,68,142,results,results,found,parallel strategy,results found parallel strategy,0.657715916633606
translation,68,151,results,bart,explicitly incorporated,structured graphs,bart explicitly incorporated structured graphs,0.6282370090484619
translation,68,151,results,outperformed,has,bart,outperformed has bart,0.6797277927398682
translation,69,168,ablation-analysis,linke,higher proportion of,errors,linke higher proportion of errors,0.7155779004096985
translation,69,168,ablation-analysis,errors,in,bertsum and bart,errors in bertsum and bart,0.5832768678665161
translation,69,168,ablation-analysis,errors,compared to,other models,errors compared to other models,0.621739387512207
translation,69,168,ablation-analysis,bertsum and bart,compared to,other models,bertsum and bart compared to other models,0.6623793244361877
translation,69,168,ablation-analysis,errors corefe,has,linke,errors corefe has linke,0.6817818880081177
translation,69,168,ablation-analysis,ablation analysis,has,errors corefe,ablation analysis has errors corefe,0.5508830547332764
translation,69,182,baselines,"bertscore ( zhang et al. , 2020 )",computes,"bert ( devlin et al. , 2019 ) contextual embeddings","bertscore ( zhang et al. , 2020 ) computes bert ( devlin et al. , 2019 ) contextual embeddings",0.7336717844009399
translation,69,182,baselines,"bertscore ( zhang et al. , 2020 )",measures,distances,"bertscore ( zhang et al. , 2020 ) measures distances",0.564625084400177
translation,69,182,baselines,"bert ( devlin et al. , 2019 ) contextual embeddings",on,summary and source article,"bert ( devlin et al. , 2019 ) contextual embeddings on summary and source article",0.4984162747859955
translation,69,182,baselines,distances,between,matched embeddings,distances between matched embeddings,0.6691657304763794
translation,69,182,baselines,baselines,has,"bertscore ( zhang et al. , 2020 )","baselines has bertscore ( zhang et al. , 2020 )",0.5344494581222534
translation,69,110,experiments,cnn / dm dataset,use,model outputs,cnn / dm dataset use model outputs,0.6420074701309204
translation,69,110,experiments,model outputs,from,lstm seq -to - seq model ( s2s ),model outputs from lstm seq -to - seq model ( s2s ),0.5294798016548157
translation,69,110,experiments,model outputs,from,pointer-generator network ( pgn ) model,model outputs from pointer-generator network ( pgn ) model,0.5537201166152954
translation,69,110,experiments,model outputs,from,bottom - up summarization ( bus ) model,model outputs from bottom - up summarization ( bus ) model,0.5515989065170288
translation,69,110,experiments,model outputs,from,jointly pretrained transformer based encoder-decoder model bart,model outputs from jointly pretrained transformer based encoder-decoder model bart,0.5494486689567566
translation,69,111,experiments,xsum dataset,collect,model outputs,xsum dataset collect model outputs,0.6785421371459961
translation,69,111,experiments,model outputs,from,topic- aware cnn model,model outputs from topic- aware cnn model,0.5185738205909729
translation,69,111,experiments,model outputs,from,pointer-generator network ( pgn ) model,model outputs from pointer-generator network ( pgn ) model,0.5537201166152954
translation,69,9,results,proportion of different categories of factual errors,in,various summarization models,proportion of different categories of factual errors in various summarization models,0.5159591436386108
translation,69,9,results,proportion of different categories of factual errors,in,benchmark factuality metrics,proportion of different categories of factual errors in benchmark factuality metrics,0.5141358971595764
translation,69,26,results,concept of factuality,in,( relatively ) well - defined and grounded categories,concept of factuality in ( relatively ) well - defined and grounded categories,0.48407870531082153
translation,69,26,results,concept of factuality,makes,final binary decision,concept of factuality makes final binary decision,0.6431893706321716
translation,69,26,results,final binary decision,leading to,near perfect agreement,final binary decision leading to near perfect agreement,0.6970753073692322
translation,69,26,results,near perfect agreement,between,crowd and expert annotators ( ? = 0.86 ),near perfect agreement between crowd and expert annotators ( ? = 0.86 ),0.6009727716445923
translation,69,26,results,final binary decision,has,more objective,final binary decision has more objective,0.5404465198516846
translation,69,26,results,results,decomposing,concept of factuality,results decomposing concept of factuality,0.6764209270477295
translation,69,152,results,23 %,for,bertsum,23 % for bertsum,0.6804741621017456
translation,69,152,results,27 %,for,bart,27 % for bart,0.7364185452461243
translation,69,152,results,still high,has,23 %,still high has 23 %,0.6102327108383179
translation,69,152,results,results,On,cnn / dm dataset,results On cnn / dm dataset,0.5200074911117554
translation,69,159,results,pgn,on,cnn / dm,pgn on cnn / dm,0.5792973637580872
translation,69,159,results,fewer summaries,with,factual errors ( 26 % ),fewer summaries with factual errors ( 26 % ),0.5955169796943665
translation,69,159,results,factual errors ( 26 % ),compared to,s2s,factual errors ( 26 % ) compared to s2s,0.6722528338432312
translation,69,159,results,factual errors ( 26 % ),compared to,bus ( 62 % ),factual errors ( 26 % ) compared to bus ( 62 % ),0.6834229230880737
translation,69,159,results,cnn / dm,has,fewer summaries,cnn / dm has fewer summaries,0.6392797827720642
translation,69,159,results,s2s,has,74 % ),s2s has 74 % ),0.6154595017433167
translation,69,159,results,results,has,pgn,results has pgn,0.5731626749038696
translation,69,161,results,pgn,produces,> 96 % factually incorrect summaries,pgn produces > 96 % factually incorrect summaries,0.6591247916221619
translation,69,161,results,more abstractive dataset xsum,has,pgn,more abstractive dataset xsum has pgn,0.624224841594696
translation,69,161,results,results,On,more abstractive dataset xsum,results On more abstractive dataset xsum,0.5615388751029968
translation,69,162,results,factuality,on,both datasets,factuality on both datasets,0.5002197623252869
translation,69,162,results,large-scale pretrained models,has,improve,large-scale pretrained models has improve,0.49489840865135193
translation,69,162,results,improve,has,factuality,improve has factuality,0.5708212852478027
translation,69,162,results,results,observe,large-scale pretrained models,results observe large-scale pretrained models,0.5339422821998596
translation,69,163,results,cnn / dm,display,half the error rate,cnn / dm display half the error rate,0.7644262909889221
translation,69,163,results,bertsum and bart,display,half the error rate,bertsum and bart display half the error rate,0.6811999678611755
translation,69,163,results,half the error rate,of,bus,half the error rate of bus,0.6590730547904968
translation,69,163,results,cnn / dm,has,bertsum and bart,cnn / dm has bertsum and bart,0.6546807885169983
translation,69,163,results,results,On,cnn / dm,results On cnn / dm,0.548747181892395
translation,69,164,results,berts2s,improves over,non-pretrained models,berts2s improves over non-pretrained models,0.7230718731880188
translation,69,164,results,non-pretrained models,by,? 10 %,non-pretrained models by ? 10 %,0.5580790638923645
translation,69,164,results,xsum,has,berts2s,xsum has berts2s,0.697799801826477
translation,69,164,results,results,on,xsum,results on xsum,0.6111196279525757
translation,69,167,results,pretrained transformer models,observe,improved error-rate,pretrained transformer models observe improved error-rate,0.5926384925842285
translation,69,167,results,improved error-rate,on,cnn / dm dataset,improved error-rate on cnn / dm dataset,0.5206770896911621
translation,69,167,results,improved error-rate,attributed to,improvements,improved error-rate attributed to improvements,0.6571685075759888
translation,69,167,results,improvements,at,"frame level ( prede , ente , circe )","improvements at frame level ( prede , ente , circe )",0.5460565686225891
translation,69,205,results,different metrics capture errors,from,pretrained and non-pretrained models,different metrics capture errors from pretrained and non-pretrained models,0.5511279106140137
translation,69,205,results,different metrics capture errors,On,cnn / dm dataset,different metrics capture errors On cnn / dm dataset,0.5087913274765015
translation,69,205,results,cnn / dm dataset,observe,entailment metrics and qags,cnn / dm dataset observe entailment metrics and qags,0.5763847827911377
translation,69,205,results,entailment metrics and qags,perform,significantly better,entailment metrics and qags perform significantly better,0.6048943996429443
translation,69,205,results,significantly better,on,non-pretrained models,significantly better on non-pretrained models,0.5387951731681824
translation,69,205,results,results,On,cnn / dm dataset,results On cnn / dm dataset,0.5200074911117554
translation,69,205,results,results,has,how well,results has how well,0.5155094265937805
translation,70,193,ablation-analysis,fake summary ( - w/ o fake -sum ),is,critical point,fake summary ( - w/ o fake -sum ) is critical point,0.6104641556739807
translation,70,193,ablation-analysis,critical point,contributes to,model convergence,critical point contributes to model convergence,0.7559471130371094
translation,70,193,ablation-analysis,ablation analysis,has,fake summary ( - w/ o fake -sum ),ablation analysis has fake summary ( - w/ o fake -sum ),0.5826415419578552
translation,70,194,ablation-analysis,tasks,based on,generated summary ( - w/ o sum-task ),tasks based on generated summary ( - w/ o sum-task ),0.6475938558578491
translation,70,194,ablation-analysis,tasks,has,performance,tasks has performance,0.5629233717918396
translation,70,194,ablation-analysis,generated summary ( - w/ o sum-task ),has,performance,generated summary ( - w/ o sum-task ) has performance,0.5769508481025696
translation,70,194,ablation-analysis,performance,has,declines significantly,performance has declines significantly,0.5875765681266785
translation,70,194,ablation-analysis,ablation analysis,remove,tasks,ablation analysis remove tasks,0.7039925456047058
translation,70,196,ablation-analysis,tasks,based on,dialogue ( - w/ o dia- task ),tasks based on dialogue ( - w/ o dia- task ),0.6649883985519409
translation,70,196,ablation-analysis,tasks,based on,dialogue and generated summary,tasks based on dialogue and generated summary,0.666896641254425
translation,70,196,ablation-analysis,kl divergence ( - w/ o kl ),to control,similar effectiveness,kl divergence ( - w/ o kl ) to control similar effectiveness,0.7111750245094299
translation,70,196,ablation-analysis,similar effectiveness,between,dialogue and generated summary,similar effectiveness between dialogue and generated summary,0.665063738822937
translation,70,196,ablation-analysis,similar effectiveness,tends to,harm,similar effectiveness tends to harm,0.7170288562774658
translation,70,196,ablation-analysis,ablation analysis,removing,tasks,ablation analysis removing tasks,0.7292305827140808
translation,70,196,ablation-analysis,ablation analysis,adding,kl divergence ( - w/ o kl ),ablation analysis adding kl divergence ( - w/ o kl ),0.7093858122825623
translation,70,197,ablation-analysis,pre-trained language model ( - w/ o lm ),benefits,bi-gram,pre-trained language model ( - w/ o lm ) benefits bi-gram,0.5477264523506165
translation,70,197,ablation-analysis,significant decrease,in,r - 2,significant decrease in r - 2,0.5381091237068176
translation,70,197,ablation-analysis,ablation analysis,notice,pre-trained language model ( - w/ o lm ),ablation analysis notice pre-trained language model ( - w/ o lm ),0.6684076189994812
translation,70,225,ablation-analysis,meaningful utterance,benefits,performance,meaningful utterance benefits performance,0.5855029821395874
translation,70,225,ablation-analysis,ablation analysis,proves,meaningful utterance,ablation analysis proves meaningful utterance,0.6717192530632019
translation,70,148,baselines,classical textrank,converts,dialogue,classical textrank converts dialogue,0.5774471759796143
translation,70,148,baselines,dialogue,to,weightedgraph,dialogue to weightedgraph,0.5902045965194702
translation,70,148,baselines,weightedgraph,where,each node,weightedgraph where each node,0.65342777967453
translation,70,148,baselines,weightedgraph,where,edge weight,weightedgraph where edge weight,0.660442590713501
translation,70,148,baselines,edge weight,expresses,semantic similarity,edge weight expresses semantic similarity,0.6310279369354248
translation,70,148,baselines,semantic similarity,between,any two utterances,semantic similarity between any two utterances,0.6147142052650452
translation,70,149,baselines,baselines,has,centroid,baselines has centroid,0.5995363593101501
translation,70,150,baselines,"pacsum ( zheng and lapata , 2019 )",improves,textrank,"pacsum ( zheng and lapata , 2019 ) improves textrank",0.7133974432945251
translation,70,150,baselines,textrank,by building,graphs,textrank by building graphs,0.6901686787605286
translation,70,150,baselines,graphs,with,directed edges,graphs with directed edges,0.6424235701560974
translation,70,150,baselines,directed edges,considering,relative positions,directed edges considering relative positions,0.6877796649932861
translation,70,150,baselines,relative positions,of,any two sentences,relative positions of any two sentences,0.5495290160179138
translation,70,150,baselines,any two sentences,contributing to,respective centrality,any two sentences contributing to respective centrality,0.7097234129905701
translation,70,150,baselines,baselines,has,"pacsum ( zheng and lapata , 2019 )","baselines has pacsum ( zheng and lapata , 2019 )",0.5529875755310059
translation,70,151,baselines,abstractive - based methods,compare with,auto-encoder based approaches,abstractive - based methods compare with auto-encoder based approaches,0.6421265602111816
translation,70,136,experimental-setup,proposed model,using,"adam optimizer ( kingma and ba , 2014 )","proposed model using adam optimizer ( kingma and ba , 2014 )",0.6760251522064209
translation,70,136,experimental-setup,"adam optimizer ( kingma and ba , 2014 )",with,learning rate,"adam optimizer ( kingma and ba , 2014 ) with learning rate",0.5956289768218994
translation,70,136,experimental-setup,learning rate,of,3e - 4,learning rate of 3e - 4,0.6384614109992981
translation,70,136,experimental-setup,experimental setup,optimize,proposed model,experimental setup optimize proposed model,0.7014890313148499
translation,70,137,experimental-setup,single teslap100 gpu,with,batch size,single teslap100 gpu with batch size,0.6284570693969727
translation,70,137,experimental-setup,batch size,of,16,batch size of 16,0.6842944622039795
translation,70,137,experimental-setup,experimental setup,train on,single teslap100 gpu,experimental setup train on single teslap100 gpu,0.701697826385498
translation,70,138,experimental-setup,vocabulary size,is,"30,000","vocabulary size is 30,000",0.5921759605407715
translation,70,138,experimental-setup,embedding dimension,for,each word,embedding dimension for each word,0.5856490731239319
translation,70,138,experimental-setup,each word,is,200,each word is 200,0.6321752071380615
translation,70,138,experimental-setup,experimental setup,has,vocabulary size,experimental setup has vocabulary size,0.5189570188522339
translation,70,139,experimental-setup,hidden size,is,200,hidden size is 200,0.6271045804023743
translation,70,139,experimental-setup,200,for,both encoder and decoder,200 for both encoder and decoder,0.6611232161521912
translation,70,139,experimental-setup,experimental setup,has,hidden size,experimental setup has hidden size,0.5616794228553772
translation,70,140,experimental-setup,gumble softmax,set,temperature,gumble softmax set temperature,0.6526924967765808
translation,70,140,experimental-setup,temperature,to,0.5,temperature to 0.5,0.5803983211517334
translation,70,140,experimental-setup,experimental setup,For,gumble softmax,experimental setup For gumble softmax,0.5989803075790405
translation,70,6,model,model,propose,novel unsupervised strategy,model propose novel unsupervised strategy,0.7424160838127136
translation,70,7,model,extractive and abstractive summary,guidance of,followed n th utterance generation and classification tasks,extractive and abstractive summary guidance of followed n th utterance generation and classification tasks,0.7145738005638123
translation,70,28,model,innovative unsupervised strategy,applied to,extractive and abstractive summarization,innovative unsupervised strategy applied to extractive and abstractive summarization,0.7051013708114624
translation,70,28,model,innovative unsupervised strategy,has,dubbed repsum,innovative unsupervised strategy has dubbed repsum,0.603735625743866
translation,70,28,model,model,propose,innovative unsupervised strategy,model propose innovative unsupervised strategy,0.7136143445968628
translation,70,32,model,kl divergence,to curtail,difference,kl divergence to curtail difference,0.658267080783844
translation,70,32,model,difference,between,results,difference between results,0.6447349190711975
translation,70,32,model,results,based on,dialogue and the summary,results based on dialogue and the summary,0.6845234632492065
translation,70,32,model,model,introduce,kl divergence,model introduce kl divergence,0.6236118674278259
translation,70,33,model,summarization,with,essential self-supervised signals,summarization with essential self-supervised signals,0.6425212025642395
translation,70,33,model,essential self-supervised signals,via,auxiliary tasks,essential self-supervised signals via auxiliary tasks,0.6500523686408997
translation,70,34,model,training,from,reconstruction of ae,training from reconstruction of ae,0.6008477210998535
translation,70,34,model,reconstruction of ae,enables,dialogue,reconstruction of ae enables dialogue,0.6207916736602783
translation,70,34,model,model,decouples,training,model decouples training,0.7756673097610474
translation,70,62,model,generalized strategy repsum,incentivizes,summary,generalized strategy repsum incentivizes summary,0.6292452812194824
translation,70,62,model,summary,to complete,auxiliary tasks,summary to complete auxiliary tasks,0.715031623840332
translation,70,62,model,summary,providing,self-training signals,summary providing self-training signals,0.665617823600769
translation,70,62,model,summary,enabling,long texts,summary enabling long texts,0.675352931022644
translation,70,62,model,auxiliary tasks,as,original dialogue,auxiliary tasks as original dialogue,0.5185936093330383
translation,70,62,model,original dialogue,providing,self-training signals,original dialogue providing self-training signals,0.6351063847541809
translation,70,62,model,self-training signals,enabling,long texts,self-training signals enabling long texts,0.6591669321060181
translation,70,62,model,long texts,to be,summarized,long texts to be summarized,0.6233083009719849
translation,70,166,results,proposed repsum - ext,compared with,other four state - of - the - art models,proposed repsum - ext compared with other four state - of - the - art models,0.6305071711540222
translation,70,166,results,other four state - of - the - art models,with,significant improvement,other four state - of - the - art models with significant improvement,0.5869017839431763
translation,70,166,results,significant improvement,in,rouge score,significant improvement in rouge score,0.5205355286598206
translation,70,166,results,results,has,proposed repsum - ext,results has proposed repsum - ext,0.628343403339386
translation,70,167,results,repsum strategy,is,effective,repsum strategy is effective,0.6484783291816711
translation,70,167,results,effective,for,extractive summarization,effective for extractive summarization,0.5885853171348572
translation,70,167,results,results,demonstrates,repsum strategy,results demonstrates repsum strategy,0.6824682354927063
translation,70,170,results,outperforms,especially in,r-l score,outperforms especially in r-l score,0.6306825280189514
translation,70,170,results,all the baselines,especially in,r-l score,all the baselines especially in r-l score,0.6381502747535706
translation,70,170,results,rouge value,has,our model,rouge value has our model,0.5958098769187927
translation,70,170,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,70,170,results,outperforms,has,all the baselines,outperforms has all the baselines,0.596747100353241
translation,70,170,results,results,In terms of,rouge value,results In terms of rouge value,0.701503336429596
translation,70,180,results,perform better,on,fluency,perform better on fluency,0.5496211051940918
translation,70,180,results,abstractive - based methods,has,extractive - based methods,abstractive - based methods has extractive - based methods,0.5765760540962219
translation,70,180,results,extractive - based methods,has,perform better,extractive - based methods has perform better,0.5850018262863159
translation,70,180,results,results,compared to,abstractive - based methods,results compared to abstractive - based methods,0.6345933675765991
translation,70,186,results,two auxiliary tasks,achieves,best performance,two auxiliary tasks achieves best performance,0.65163254737854
translation,70,186,results,best performance,on,extractive and abstractive methods,best performance on extractive and abstractive methods,0.510325014591217
translation,70,186,results,results,combining,two auxiliary tasks,results combining two auxiliary tasks,0.6500954031944275
translation,70,204,results,random selection result,inferior to,extractive summary guidance,random selection result inferior to extractive summary guidance,0.7358046174049377
translation,70,204,results,results,shows that,random selection result,results shows that random selection result,0.6762269139289856
translation,70,226,results,last,leads to,worst result,last leads to worst result,0.6841496229171753
translation,70,226,results,worst result,on,r - 1 and r-l,worst result on r - 1 and r-l,0.574486494064331
translation,70,226,results,worst result,both,r - 1 and r-l,worst result both r - 1 and r-l,0.6774246096611023
translation,70,226,results,worst result,due to,universal utterance,worst result due to universal utterance,0.6746890544891357
translation,70,226,results,r - 1 and r-l,due to,universal utterance,r - 1 and r-l due to universal utterance,0.7307376265525818
translation,70,226,results,universal utterance,end of,dialogue,universal utterance end of dialogue,0.6467483043670654
translation,70,226,results,results,has,last,results has last,0.5495622754096985
translation,71,19,model,simple yet very effective method,to construct,vision guided ( vg ) gplms,simple yet very effective method to construct vision guided ( vg ) gplms,0.6659520268440247
translation,71,19,model,vision guided ( vg ) gplms,for,mas task,vision guided ( vg ) gplms for mas task,0.5997538566589355
translation,71,19,model,vg - bart and vg - t5 ),for,mas task,vg - bart and vg - t5 ) for mas task,0.652666449546814
translation,71,19,model,vision guided ( vg ) gplms,has,vg - bart and vg - t5 ),vision guided ( vg ) gplms has vg - bart and vg - t5 ),0.596591055393219
translation,71,19,model,model,present,simple yet very effective method,model present simple yet very effective method,0.6895434260368347
translation,71,20,model,attention - based add - on layers,to,gplms,attention - based add - on layers to gplms,0.5701020956039429
translation,71,20,model,attention - based add - on layers,to incorporate,visual information,attention - based add - on layers to incorporate visual information,0.6961889266967773
translation,71,20,model,visual information,without modifying,original architecture,visual information without modifying original architecture,0.744684636592865
translation,71,20,model,model,insert,attention - based add - on layers,model insert attention - based add - on layers,0.7009720206260681
translation,71,22,model,two types of attention mechanisms,for,text-vision fusion and interaction,two types of attention mechanisms for text-vision fusion and interaction,0.5942142605781555
translation,71,22,model,model,try with,two types of attention mechanisms,model try with two types of attention mechanisms,0.6474864482879639
translation,71,26,results,best model,surpasses,prior state - of- the - art model,best model surpasses prior state - of- the - art model,0.5923089981079102
translation,71,26,results,prior state - of- the - art model,by,"5.7 rouge -1 , 5.3 rouge - 2 , and 5.1 rouge -l scores","prior state - of- the - art model by 5.7 rouge -1 , 5.3 rouge - 2 , and 5.1 rouge -l scores",0.5237126350402832
translation,71,26,results,results,demonstrate,best model,results demonstrate best model,0.6038082838058472
translation,72,102,baselines,"bertext ( liu and lapata , 2019 )",is,extractive model,"bertext ( liu and lapata , 2019 ) is extractive model",0.5794743895530701
translation,72,102,baselines,extractive model,whose,parameters,extractive model whose parameters,0.6645715832710266
translation,72,102,baselines,parameters,initialized with,"bert ( devlin et al. , 2019 )","parameters initialized with bert ( devlin et al. , 2019 )",0.7536793947219849
translation,72,102,baselines,parameters,initialized with,bert,parameters initialized with bert,0.7544686198234558
translation,72,102,baselines,"bertabs ( liu and lapata , 2019 )",is,abstractive model,"bertabs ( liu and lapata , 2019 ) is abstractive model",0.5780340433120728
translation,72,102,baselines,abstractive model,with,encoder,abstractive model with encoder,0.6679971814155579
translation,72,102,baselines,encoder,initialized with,bert,encoder initialized with bert,0.7713636755943298
translation,72,103,baselines,"matchsum ( zhong et al. , 2020 )",is,extractive model,"matchsum ( zhong et al. , 2020 ) is extractive model",0.5526695251464844
translation,72,103,baselines,extractive model,reranks,candidate summaries,extractive model reranks candidate summaries,0.8201490044593811
translation,72,103,baselines,candidate summaries,produced by,bertext,candidate summaries produced by bertext,0.6861653923988342
translation,72,103,baselines,baselines,has,"matchsum ( zhong et al. , 2020 )","baselines has matchsum ( zhong et al. , 2020 )",0.5261180400848389
translation,72,104,baselines,"bart ( lewis et al. , 2020 )",is,state - of - the - art,"bart ( lewis et al. , 2020 ) is state - of - the - art",0.5816536545753479
translation,72,104,baselines,abstractive summarization model,pretrained with,denoising autoencoding objective,abstractive summarization model pretrained with denoising autoencoding objective,0.6403926014900208
translation,72,104,baselines,state - of - the - art,has,abstractive summarization model,state - of - the - art has abstractive summarization model,0.5168725848197937
translation,72,104,baselines,baselines,has,"bart ( lewis et al. , 2020 )","baselines has bart ( lewis et al. , 2020 )",0.5562279224395752
translation,72,6,model,model,propose,general and extensible guided summarization framework ( gsum ),model propose general and extensible guided summarization framework ( gsum ),0.6586537957191467
translation,72,51,model,transformer model,as,our backbone architecture,transformer model as our backbone architecture,0.5437190532684326
translation,72,51,model,our backbone architecture,instantiated with,bert or bart,our backbone architecture instantiated with bert or bart,0.700114369392395
translation,72,51,model,bert or bart,separated into,encoder and decoder components,bert or bart separated into encoder and decoder components,0.7356435060501099
translation,72,51,model,model,adopt,transformer model,model adopt transformer model,0.6536498665809631
translation,72,28,results,our best model,using,highlighted sentences,our best model using highlighted sentences,0.6919704675674438
translation,72,28,results,our best model,achieve,stateof - the - art performance,our best model achieve stateof - the - art performance,0.594973623752594
translation,72,28,results,highlighted sentences,as,guidance,highlighted sentences as guidance,0.582527220249176
translation,72,28,results,stateof - the - art performance,on,4 out of the 6 datasets,stateof - the - art performance on 4 out of the 6 datasets,0.48420584201812744
translation,72,28,results,stateof - the - art performance,including,1.28/0.79/1.13 rouge -1/2/l,stateof - the - art performance including 1.28/0.79/1.13 rouge -1/2/l,0.6343210339546204
translation,72,28,results,results,has,our best model,results has our best model,0.5419765710830688
translation,72,113,results,oracle,to select,guidance signals,oracle to select guidance signals,0.7067710757255554
translation,72,113,results,baseline performance significantly,with,best-performing model,baseline performance significantly with best-performing model,0.5872064232826233
translation,72,113,results,best-performing model,achieving,rouge - 1 score,best-performing model achieving rouge - 1 score,0.606460690498352
translation,72,113,results,rouge - 1 score,of,55.18,rouge - 1 score of 55.18,0.5008776783943176
translation,72,113,results,oracle,has,all varieties of guidance,oracle has all varieties of guidance,0.606654703617096
translation,72,113,results,guidance signals,has,all varieties of guidance,guidance signals has all varieties of guidance,0.552754282951355
translation,72,113,results,improve,has,baseline performance significantly,improve has baseline performance significantly,0.5545361638069153
translation,72,113,results,results,use,oracle,results use oracle,0.6030020713806152
translation,72,119,results,our model,achieve,over 1 rouge - 1/l point improvements,our model achieve over 1 rouge - 1/l point improvements,0.649959146976471
translation,72,119,results,over 1 rouge - 1/l point improvements,compared with,state - of - the - art models,over 1 rouge - 1/l point improvements compared with state - of - the - art models,0.6461443305015564
translation,72,122,results,model,works,better,model works better,0.6397309303283691
translation,72,122,results,better,when,dataset,better when dataset,0.6599830389022827
translation,72,122,results,dataset,is,more extractive,dataset is more extractive,0.5715637803077698
translation,72,122,results,results,has,model,results has model,0.5339115858078003
translation,72,123,results,abstractive datasets,such as,reddit and xsum,abstractive datasets such as reddit and xsum,0.5845590829849243
translation,72,123,results,our model,cannot achieve,performance increases,our model cannot achieve performance increases,0.7526533603668213
translation,72,123,results,performance increases,when,abstractive summarization base - line,performance increases when abstractive summarization base - line,0.6275603175163269
translation,72,123,results,abstractive summarization base - line,is,rather strong,abstractive summarization base - line is rather strong,0.5439196228981018
translation,72,123,results,abstractive summarization base - line,already,rather strong,abstractive summarization base - line already rather strong,0.6188212037086487
translation,72,123,results,abstractive datasets,has,our model,abstractive datasets has our model,0.555654764175415
translation,72,123,results,reddit and xsum,has,our model,reddit and xsum has our model,0.6070391535758972
translation,72,123,results,results,For,abstractive datasets,results For abstractive datasets,0.5160175561904907
translation,72,124,results,extractive datasets,such as,pubmed and nyt,extractive datasets such as pubmed and nyt,0.6435420513153076
translation,72,124,results,our model,achieve,some improvements,our model achieve some improvements,0.613285481929779
translation,72,124,results,some improvements,over,baselines,some improvements over baselines,0.6958863139152527
translation,72,124,results,extractive datasets,has,our model,extractive datasets has our model,0.5641549825668335
translation,72,124,results,pubmed and nyt,has,our model,pubmed and nyt has our model,0.6146901249885559
translation,72,124,results,results,For,extractive datasets,results For extractive datasets,0.5836689472198486
translation,72,131,results,of our guided models,generate,more novel ngrams,of our guided models generate more novel ngrams,0.6671586632728577
translation,72,131,results,more novel ngrams,than,baseline,more novel ngrams than baseline,0.5852583646774292
translation,72,132,results,our models,cover,more novel n-grams,our models cover more novel n-grams,0.6903826594352722
translation,72,132,results,more novel n-grams,in,gold reference,more novel n-grams in gold reference,0.4999935030937195
translation,72,132,results,gold reference,than,baseline,gold reference than baseline,0.5788136720657349
translation,72,132,results,results,has,our models,results has our models,0.5733726620674133
translation,73,204,ablation-analysis,performance,of,cast a,performance of cast a,0.6799400448799133
translation,73,204,ablation-analysis,performance,of,cast,performance of cast,0.6378477811813354
translation,73,204,ablation-analysis,drops,compared to,cast,drops compared to cast,0.7688311338424683
translation,73,204,ablation-analysis,cast a,has,drops,cast a has drops,0.6647173166275024
translation,73,205,baselines,cast c,has,cast without copy mechanism,cast c has cast without copy mechanism,0.6284371614456177
translation,73,205,baselines,baselines,has,cast c,baselines has cast c,0.5890917778015137
translation,73,172,experiments,experiments,conducted on,server,experiments conducted on server,0.736828625202179
translation,73,172,experiments,server,with,4 gpus,server with 4 gpus,0.580316424369812
translation,73,172,experiments,epoch,for,tl - codesum and funcom,epoch for tl - codesum and funcom,0.744309663772583
translation,73,172,experiments,4 gpus,has,of nvidia tesla v100,4 gpus has of nvidia tesla v100,0.5473371148109436
translation,73,168,hyperparameters,vocabulary sizes,are,10,vocabulary sizes are 10,0.6731555461883545
translation,73,168,hyperparameters,10,",","000 , 30 , 000 and 50 , 000","10 , 000 , 30 , 000 and 50 , 000",0.6522542834281921
translation,73,168,hyperparameters,"000 , 30 , 000 and 50 , 000",for,"ast , code , and summary","000 , 30 , 000 and 50 , 000 for ast , code , and summary",0.607273519039154
translation,73,168,hyperparameters,hyperparameters,has,vocabulary sizes,hyperparameters has vocabulary sizes,0.4966178834438324
translation,73,169,hyperparameters,batch size,set to,128,batch size set to 128,0.7535710334777832
translation,73,169,hyperparameters,maximum number of epochs,is,200/40,maximum number of epochs is 200/40,0.5864218473434448
translation,73,169,hyperparameters,200/40,for,tl - codesum and funcom,200/40 for tl - codesum and funcom,0.7288886904716492
translation,73,169,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,73,169,hyperparameters,hyperparameters,has,maximum number of epochs,hyperparameters has maximum number of epochs,0.5072295069694519
translation,73,170,hyperparameters,optimizer,use,"adamw ( loshchilov and hutter , 2019 )","optimizer use adamw ( loshchilov and hutter , 2019 )",0.6044521927833557
translation,73,170,hyperparameters,"adamw ( loshchilov and hutter , 2019 )",with,learning rate 10 ?4,"adamw ( loshchilov and hutter , 2019 ) with learning rate 10 ?4",0.6516336798667908
translation,73,170,hyperparameters,hyperparameters,For,optimizer,hyperparameters For optimizer,0.5647488832473755
translation,73,171,hyperparameters,overfitting,adopt,early stopping,overfitting adopt early stopping,0.6243805289268494
translation,73,171,hyperparameters,early stopping,with,patience,early stopping with patience,0.6632037162780762
translation,73,171,hyperparameters,hyperparameters,To alleviate,overfitting,hyperparameters To alleviate overfitting,0.6152921319007874
translation,73,7,model,novel model cast,reconstructs,asts,novel model cast reconstructs asts,0.797081470489502
translation,73,7,model,model,propose,novel model cast,model propose novel model cast,0.7123698592185974
translation,73,8,model,large ast,into,set of subtrees,large ast into set of subtrees,0.5758124589920044
translation,73,8,model,recursive neural network,to encode,subtrees,recursive neural network to encode subtrees,0.7325209379196167
translation,73,8,model,model,hierarchically split,large ast,model hierarchically split large ast,0.8042144775390625
translation,73,8,model,model,utilize,recursive neural network,model utilize recursive neural network,0.5972062349319458
translation,73,10,model,ast representation,together with,source code embedding,ast representation together with source code embedding,0.548622727394104
translation,73,10,model,source code embedding,obtained by,vanilla code token encoder,source code embedding obtained by vanilla code token encoder,0.5610319972038269
translation,73,10,model,source code embedding,used for,code summarization,source code embedding used for code summarization,0.5893182158470154
translation,73,10,model,model,has,ast representation,model has ast representation,0.5446459054946899
translation,73,37,model,novel model cast,reconstruction of,abstract syntax trees,novel model cast reconstruction of abstract syntax trees,0.6831258535385132
translation,73,37,model,code summarization,with,hi-erarchical splitting,code summarization with hi-erarchical splitting,0.6409491300582886
translation,73,37,model,code summarization,reconstruction of,abstract syntax trees,code summarization reconstruction of abstract syntax trees,0.5911024212837219
translation,73,37,model,novel model cast,has,code summarization,novel model cast has code summarization,0.5546184182167053
translation,73,37,model,model,propose,novel model cast,model propose novel model cast,0.7123698592185974
translation,73,38,model,ast,into,set of subtrees,ast into set of subtrees,0.5722525715827942
translation,73,38,model,set of subtrees,at,proper granularity,set of subtrees at proper granularity,0.5305057764053345
translation,73,38,model,representation,of,complete ast,representation of complete ast,0.5793477296829224
translation,73,38,model,representation,by aggregating,subtrees ' representation,representation by aggregating subtrees ' representation,0.6974818110466003
translation,73,38,model,subtrees ' representation,learned using,tree - based neural models,subtrees ' representation learned using tree - based neural models,0.7698704600334167
translation,73,38,model,model,split,ast,model split ast,0.7926881313323975
translation,73,39,model,full ast,in,hierarchical way,full ast in hierarchical way,0.5490602254867554
translation,73,39,model,hierarchical way,using,set of carefully designed rules,hierarchical way using set of carefully designed rules,0.6779255867004395
translation,73,39,model,model,split,full ast,model split full ast,0.7819485664367676
translation,73,40,model,tree-based neural model rvnn,to learn,each subtree 's representation,tree-based neural model rvnn to learn each subtree 's representation,0.610884428024292
translation,73,40,model,model,use,tree-based neural model rvnn,model use tree-based neural model rvnn,0.599467396736145
translation,73,41,model,all subtrees ' representation,by,another rvnn,all subtrees ' representation by another rvnn,0.578305721282959
translation,73,41,model,another rvnn,to capture,full tree 's structural and semantic information,another rvnn to capture full tree 's structural and semantic information,0.6973252892494202
translation,73,41,model,model,reconstruct,split asts,model reconstruct split asts,0.7283596992492676
translation,73,49,model,complete ast,using,split asts,complete ast using split asts,0.6773615479469299
translation,73,49,model,model,split,trees,model split trees,0.7954714894294739
translation,73,49,model,model,reconstruct,complete ast,model reconstruct complete ast,0.7632019519805908
translation,73,50,model,high - level hierarchical information,of,asts,high - level hierarchical information of asts,0.6164883375167847
translation,73,50,model,model,has,high - level hierarchical information,model has high - level hierarchical information,0.5576626658439636
translation,73,186,results,all the baselines,on,both datasets,all the baselines on both datasets,0.4742196798324585
translation,73,186,results,cast,has,outperforms,cast has outperforms,0.6379883885383606
translation,73,186,results,outperforms,has,all the baselines,outperforms has all the baselines,0.596747100353241
translation,73,186,results,results,see that,cast,results see that cast,0.5304967164993286
translation,73,188,results,ncs,performs,better,ncs performs better,0.6753681898117065
translation,73,188,results,results,has,ncs,results has ncs,0.37680110335350037
translation,73,189,results,astattgru and codeastnn,has,outperform,astattgru and codeastnn has outperform,0.6236035823822021
translation,73,189,results,outperform,has,attgru,outperform has attgru,0.6770246624946594
translation,73,189,results,results,has,astattgru and codeastnn,results has astattgru and codeastnn,0.5498344898223877
translation,73,190,results,outperforms,even without,copy mechanism or aggregation,outperforms even without copy mechanism or aggregation,0.7350314855575562
translation,73,190,results,other baselines,even without,copy mechanism or aggregation,other baselines even without copy mechanism or aggregation,0.6754186153411865
translation,73,190,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,73,190,results,outperforms,has,other baselines,outperforms has other baselines,0.5879674553871155
translation,73,190,results,results,Note,our model,results Note our model,0.6268720030784607
translation,73,195,results,codeastnn,is,statement - level splitting,codeastnn is statement - level splitting,0.5608755350112915
translation,73,195,results,codeastnn,leading to,subtrees,codeastnn leading to subtrees,0.7377421855926514
translation,73,195,results,subtrees,71 % smaller than,ours,subtrees 71 % smaller than ours,0.7092905044555664
translation,73,195,results,ours,on,tl - codesum,ours on tl - codesum,0.6084805130958557
translation,73,195,results,asts,has,codeastnn,asts has codeastnn,0.6324388980865479
translation,73,212,results,cast,has,outperforms,cast has outperforms,0.6379883885383606
translation,73,212,results,outperforms,has,others,outperforms has others,0.6126620769500732
translation,73,212,results,results,shows,cast,results shows cast,0.5100039839744568
translation,73,213,results,our approach,better than,other approaches,our approach better than other approaches,0.6900761127471924
translation,73,213,results,other approaches,in,informative,other approaches in informative,0.5661475658416748
translation,73,213,results,results,has,our approach,results has our approach,0.6050099730491638
translation,74,159,baselines,pointer-generator network,is,popular sequence - to-sequence model,pointer-generator network is popular sequence - to-sequence model,0.5460047721862793
translation,74,159,baselines,pointer-generator network,acts as,baseline system,pointer-generator network acts as baseline system,0.6514385938644409
translation,74,159,baselines,popular sequence - to-sequence model,with,copy mechanism and coverage loss,popular sequence - to-sequence model with copy mechanism and coverage loss,0.6240705847740173
translation,74,159,baselines,baseline system,in,many generation tasks,baseline system in many generation tasks,0.4767872989177704
translation,74,193,baselines,baselines,introduce,two basic baselines,baselines introduce two basic baselines,0.670009195804596
translation,74,177,experimental-setup,fairseq library,to implement,bart model,fairseq library to implement bart model,0.6886814832687378
translation,74,177,experimental-setup,experimental setup,use,fairseq library,experimental setup use fairseq library,0.6221718788146973
translation,74,8,experiments,qmsum,consists of,"1,808","qmsum consists of 1,808",0.6422565579414368
translation,74,8,experiments,query -summary pairs,over,232 meetings,query -summary pairs over 232 meetings,0.7044023871421814
translation,74,8,experiments,232 meetings,in,multiple domains,232 meetings in multiple domains,0.5569770932197571
translation,74,8,experiments,"1,808",has,query -summary pairs,"1,808 has query -summary pairs",0.5654764175415039
translation,74,209,experiments,indomain and out-domain tests,in,three domains,indomain and out-domain tests in three domains,0.5607402920722961
translation,74,209,experiments,three domains,of,qmsum dataset,three domains of qmsum dataset,0.5745916962623596
translation,74,31,model,two -stage,has,meeting summarization approach,two -stage has meeting summarization approach,0.5303359031677246
translation,74,31,model,model,employ,two -stage,model employ two -stage,0.5753636360168457
translation,74,32,model,model,called,locator,model called locator,0.7387503981590271
translation,74,32,model,locator,to locate,relevant utterances,locator to locate relevant utterances,0.6227012276649475
translation,74,32,model,relevant utterances,in,meeting transcripts,relevant utterances in meeting transcripts,0.5246376991271973
translation,74,32,model,extracted spans,used as,input,extracted spans used as input,0.6120514273643494
translation,74,32,model,input,to,another model,input to another model,0.6057299971580505
translation,74,32,model,another model,called,summarizer,another model called summarizer,0.6901246905326843
translation,74,32,model,summarizer,to generate,query - based summary,summarizer to generate query - based summary,0.6609305143356323
translation,74,32,model,query,has,model,query has model,0.5951438546180725
translation,74,32,model,model,given,query,model given query,0.7787366509437561
translation,74,186,results,random,get,good rouge - l recall score,random get good rouge - l recall score,0.5629734992980957
translation,74,186,results,good rouge - l recall score,used as,baseline,good rouge - l recall score used as baseline,0.5915659070014954
translation,74,186,results,baseline,to measure,performance,baseline to measure performance,0.7349002361297607
translation,74,186,results,performance,of,model,performance of model,0.6080846190452576
translation,74,187,results,similarity,performs,badly,similarity performs badly,0.6246923208236694
translation,74,187,results,badly,worse than,random,badly worse than random,0.8479517102241516
translation,74,187,results,results,has,similarity,results has similarity,0.5556272268295288
translation,74,190,results,hierarchical ranking - based locator,always greatly exceeds,random score,hierarchical ranking - based locator always greatly exceeds random score,0.7214779257774353
translation,74,190,results,results,has,hierarchical ranking - based locator,results has hierarchical ranking - based locator,0.5741788148880005
translation,74,191,results,1/6,of,original text,1/6 of original text,0.5176235437393188
translation,74,191,results,1/6,reach,72.51 rouge -l recall score,1/6 reach 72.51 rouge -l recall score,0.6567745208740234
translation,74,191,results,original text,is,extracted,original text is extracted,0.521379292011261
translation,74,191,results,extracted,reach,72.51 rouge -l recall score,extracted reach 72.51 rouge -l recall score,0.6726990938186646
translation,74,191,results,long text,while ensuring,amount of information,long text while ensuring amount of information,0.6672683954238892
translation,74,191,results,subsequent summarizer processing,has,long text,subsequent summarizer processing has long text,0.5780596137046814
translation,74,191,results,results,if,1/6,results if 1/6,0.5774433612823486
translation,74,198,results,performance,of,three typical neural network models,performance of three typical neural network models,0.603542149066925
translation,74,198,results,performance,is,significantly better,performance is significantly better,0.577980101108551
translation,74,198,results,three typical neural network models,is,significantly better,three typical neural network models is significantly better,0.5705265402793884
translation,74,198,results,significantly better,than,random and textrank,significantly better than random and textrank,0.5910794138908386
translation,74,198,results,results,shows that,performance,results shows that performance,0.6973981261253357
translation,74,199,results,our locator,both,pgnet and bart,our locator both pgnet and bart,0.770481288433075
translation,74,199,results,pgnet and bart,brought evident performance improvements,pgnet,pgnet and bart brought evident performance improvements pgnet,0.8093448877334595
translation,74,199,results,pgnet and bart,brought evident performance improvements,28.74 -> 31.37 r-1,pgnet and bart brought evident performance improvements 28.74 -> 31.37 r-1,0.6999121904373169
translation,74,199,results,pgnet and bart,brought evident performance improvements,bart,pgnet and bart brought evident performance improvements bart,0.7672192454338074
translation,74,199,results,pgnet and bart,brought evident performance improvements,29.20 -> 31.74 r- 1,pgnet and bart brought evident performance improvements 29.20 -> 31.74 r- 1,0.6981149315834045
translation,74,199,results,pgnet,has,28.74 -> 31.37 r-1,pgnet has 28.74 -> 31.37 r-1,0.5791947841644287
translation,74,199,results,bart,has,29.20 -> 31.74 r- 1,bart has 29.20 -> 31.74 r- 1,0.5547046065330505
translation,74,199,results,results,equipped with,our locator,results equipped with our locator,0.6581676006317139
translation,74,200,results,advantage,of,bart *,advantage of bart *,0.6213861703872681
translation,74,200,results,bart *,lies in,rouge -l score ( 1.13 improvement ),bart * lies in rouge -l score ( 1.13 improvement ),0.7070339918136597
translation,74,200,results,pgnet *,has,advantage,pgnet * has advantage,0.6483423709869385
translation,74,200,results,results,Compared to,pgnet *,results Compared to pgnet *,0.689161479473114
translation,74,201,results,meeting summarization model hmnet,achieves,best performance,meeting summarization model hmnet achieves best performance,0.6606156826019287
translation,74,201,results,current state - of- the - art,has,meeting summarization model hmnet,current state - of- the - art has meeting summarization model hmnet,0.562732994556427
translation,74,201,results,results,has,current state - of- the - art,results has current state - of- the - art,0.5216659307479858
translation,74,204,results,models ( pgnet and bart ),need to truncate,input text,models ( pgnet and bart ) need to truncate input text,0.700263261795044
translation,74,204,results,locator,is,approximate solution,locator is approximate solution,0.5701465606689453
translation,74,204,results,models,achieve,comparable results,models achieve comparable results,0.6323614716529846
translation,74,204,results,models,based on,gold span inputs,models based on gold span inputs,0.6118354797363281
translation,74,204,results,comparable results,with,models,comparable results with models,0.7065601944923401
translation,74,204,results,models,based on,gold span inputs,models based on gold span inputs,0.6118354797363281
translation,74,204,results,models ( pgnet and bart ),has,models,models ( pgnet and bart ) has models,0.5936357378959656
translation,74,204,results,results,for,models ( pgnet and bart ),results for models ( pgnet and bart ),0.5970554351806641
translation,74,214,results,models,trained on,single domain,models trained on single domain,0.7600191235542297
translation,74,214,results,models,trained on,qmsum,models trained on qmsum,0.7332512736320496
translation,74,214,results,models,always achieve,comparable results,models always achieve comparable results,0.6846210956573486
translation,74,214,results,models,trained on,qmsum,models trained on qmsum,0.7332512736320496
translation,74,214,results,models,always achieve,comparable results,models always achieve comparable results,0.6846210956573486
translation,74,214,results,models,has,models,models has models,0.6021558046340942
translation,74,214,results,results,Compared with,models,results Compared with models,0.6802673935890198
translation,74,215,results,model,with,multi-domain train - ing,model with multi-domain train - ing,0.6378269791603088
translation,74,215,results,multi-domain train - ing,get,higher rouge - 2 ( 5.05 vs 4.32 ) and rouge -l ( 23.01 vs 22.58 ) scores,multi-domain train - ing get higher rouge - 2 ( 5.05 vs 4.32 ) and rouge -l ( 23.01 vs 22.58 ) scores,0.5059343576431274
translation,74,215,results,academic domain,has,model,academic domain has model,0.5623579025268555
translation,74,215,results,results,In,academic domain,results In academic domain,0.5144926309585571
translation,75,11,ablation-analysis,sparse sentence structure,in,document summarization,sparse sentence structure in document summarization,0.43237119913101196
translation,75,11,ablation-analysis,sparse sentence structure,exploited by,constraining,sparse sentence structure exploited by constraining,0.704416811466217
translation,75,11,ablation-analysis,attention mechanism,to,subset of input sentences,attention mechanism to subset of input sentences,0.5322046875953674
translation,75,11,ablation-analysis,attention mechanism,whilst maintaining,system performance,attention mechanism whilst maintaining system performance,0.6205751299858093
translation,75,11,ablation-analysis,constraining,has,attention mechanism,constraining has attention mechanism,0.5533190965652466
translation,75,10,model,complexity,of,encoder-decoder attention,complexity of encoder-decoder attention,0.5764665603637695
translation,75,10,model,model,examine,complexity,model examine complexity,0.688103437423706
translation,75,12,model,modified architecture,selects,subset of sentences,modified architecture selects subset of sentences,0.7360878586769104
translation,75,12,model,subset of sentences,to constrain,encoder-decoder attention,subset of sentences to constrain encoder-decoder attention,0.6955270171165466
translation,75,12,model,model,propose,modified architecture,model propose modified architecture,0.7142280340194702
translation,75,30,model,modified decoder architecture,dynamically select,salient input sentences,modified decoder architecture dynamically select salient input sentences,0.7035208940505981
translation,75,30,model,salient input sentences,to constrain,encoder-decoder attention,salient input sentences to constrain encoder-decoder attention,0.6710273623466492
translation,75,30,model,encoder-decoder attention,without having to compute,complete attention,encoder-decoder attention without having to compute complete attention,0.5889797806739807
translation,75,30,model,model,propose,modified decoder architecture,model propose modified decoder architecture,0.6694720387458801
translation,75,91,results,performance degradation,of,kl -only system,performance degradation of kl -only system,0.578798234462738
translation,75,91,results,reduced,by,our integrated training,reduced by our integrated training,0.6411952376365662
translation,75,91,results,kl-only system,has,clearly outperforms,kl-only system has clearly outperforms,0.6114257574081421
translation,75,91,results,clearly outperforms,has,random selection baseline,clearly outperforms has random selection baseline,0.5925642848014832
translation,75,91,results,results,show that,kl-only system,results show that kl-only system,0.5103851556777954
translation,75,93,results,best,to use,"i r , apx m","best to use i r , apx m",0.7445045113563538
translation,75,93,results,"i r , apx m",in,integrated training,"i r , apx m in integrated training",0.5848502516746521
translation,75,96,results,performance,of,our proposed method,performance of our proposed method,0.5945354700088501
translation,75,96,results,our proposed method,converges to,full attention baseline,our proposed method converges to full attention baseline,0.675365149974823
translation,75,96,results,full attention baseline,across,all models and datasets,full attention baseline across all models and datasets,0.6778890490531921
translation,75,96,results,results,confirm that,performance,results confirm that performance,0.6209206581115723
translation,76,17,baselines,general multimodal summarization baseline,based on,transcripts,general multimodal summarization baseline based on transcripts,0.6037392020225525
translation,76,17,baselines,transcripts,for,multimodal summarization,transcripts for multimodal summarization,0.5996465086936951
translation,76,17,baselines,multimodal summarization,on,mm - avs,multimodal summarization on mm - avs,0.5588774681091309
translation,76,15,experiments,full-scale multimodal article and video summarization ( mm - avs ) dataset,with,documents,full-scale multimodal article and video summarization ( mm - avs ) dataset with documents,0.6263844966888428
translation,76,15,experiments,full-scale multimodal article and video summarization ( mm - avs ) dataset,with,summaries,full-scale multimodal article and video summarization ( mm - avs ) dataset with summaries,0.608640193939209
translation,76,15,experiments,full-scale multimodal article and video summarization ( mm - avs ) dataset,with,images,full-scale multimodal article and video summarization ( mm - avs ) dataset with images,0.6265164017677307
translation,76,15,experiments,full-scale multimodal article and video summarization ( mm - avs ) dataset,with,captions,full-scale multimodal article and video summarization ( mm - avs ) dataset with captions,0.5597081184387207
translation,76,15,experiments,full-scale multimodal article and video summarization ( mm - avs ) dataset,with,videos,full-scale multimodal article and video summarization ( mm - avs ) dataset with videos,0.6102568507194519
translation,76,15,experiments,full-scale multimodal article and video summarization ( mm - avs ) dataset,with,audios,full-scale multimodal article and video summarization ( mm - avs ) dataset with audios,0.636039137840271
translation,76,15,experiments,full-scale multimodal article and video summarization ( mm - avs ) dataset,with,transcripts,full-scale multimodal article and video summarization ( mm - avs ) dataset with transcripts,0.6156826615333557
translation,76,15,experiments,full-scale multimodal article and video summarization ( mm - avs ) dataset,with,titles,full-scale multimodal article and video summarization ( mm - avs ) dataset with titles,0.6093584299087524
translation,76,15,experiments,titles,has,in english,titles has in english,0.4813098907470703
translation,76,37,experiments,image captions,for,deep descriptions,image captions for deep descriptions,0.5836442112922668
translation,76,37,experiments,deep descriptions,of,images,deep descriptions of images,0.5962945818901062
translation,76,37,experiments,images,as well as,document titles,images as well as document titles,0.6054674983024597
translation,76,37,experiments,document titles,for,topic extraction,document titles for topic extraction,0.5614707469940186
translation,76,86,experiments,full-scale dataset,for,multimodal summarization,full-scale dataset for multimodal summarization,0.5721738934516907
translation,76,86,experiments,multimodal summarization,extensively assembles,documents,multimodal summarization extensively assembles documents,0.7221429347991943
translation,76,86,experiments,multimodal summarization,extensively assembles,summaries,multimodal summarization extensively assembles summaries,0.7030812501907349
translation,76,86,experiments,multimodal summarization,extensively assembles,images,multimodal summarization extensively assembles images,0.726575493812561
translation,76,86,experiments,multimodal summarization,extensively assembles,videos,multimodal summarization extensively assembles videos,0.6829760074615479
translation,76,86,experiments,multimodal summarization,extensively assembles,audios,multimodal summarization extensively assembles audios,0.7245890498161316
translation,76,86,experiments,multimodal summarization,extensively assembles,transcripts,multimodal summarization extensively assembles transcripts,0.7345953583717346
translation,76,86,experiments,multimodal summarization,extensively assembles,titles,multimodal summarization extensively assembles titles,0.694005012512207
translation,76,19,model,multi-task learning,to simultaneously optimize,document and video summarizations,multi-task learning to simultaneously optimize document and video summarizations,0.64692622423172
translation,76,19,model,model,use,multi-task learning,model use multi-task learning,0.6089916825294495
translation,77,63,baselines,dynamic version,where,word unit predictor,dynamic version where word unit predictor,0.6550028324127197
translation,77,63,baselines,word unit predictor,been generated up to,given step,word unit predictor been generated up to given step,0.6357449293136597
translation,77,76,baselines,style transfer model,applied on,output,style transfer model applied on output,0.6767697930335999
translation,77,76,baselines,output,of,bart ( ctrlgen ),output of bart ( ctrlgen ),0.6102494597434998
translation,77,76,baselines,output,of,bart ( trans ),output of bart ( trans ),0.6597698330879211
translation,77,76,baselines,normal- to-simple translation model,fine-tuned from,bart ( trans ),normal- to-simple translation model fine-tuned from bart ( trans ),0.7574638724327087
translation,77,77,baselines,"lightls ( glava ? and ?tajner , 2015 )",has,rule-based lexical simplification model,"lightls ( glava ? and ?tajner , 2015 ) has rule-based lexical simplification model",0.5724170804023743
translation,77,5,model,decoder state adjustment,instantly modifies,decoder final states,decoder state adjustment instantly modifies decoder final states,0.772557258605957
translation,77,5,model,decoder final states,with,externally trained style scorers,decoder final states with externally trained style scorers,0.6106216311454773
translation,77,5,model,decoder final states,to iteratively refine,output,decoder final states to iteratively refine output,0.6841645240783691
translation,77,5,model,output,against,target style,output against target style,0.7020390629768372
translation,77,5,model,model,has,decoder state adjustment,model has decoder state adjustment,0.5473400950431824
translation,77,6,model,word unit prediction,constrains,word usage,word unit prediction constrains word usage,0.6666345000267029
translation,77,6,model,word usage,to impose,strong lexical control,word usage to impose strong lexical control,0.6279613375663757
translation,77,6,model,strong lexical control,during,generation,strong lexical control during generation,0.6512019634246826
translation,77,6,model,model,has,word unit prediction,model has word unit prediction,0.5699073076248169
translation,77,13,model,model,investigate,justin-time style control techniques,model investigate justin-time style control techniques,0.6708811521530151
translation,77,24,model,stronger lexical control,introduce,word unit prediction,stronger lexical control introduce word unit prediction,0.5875278115272522
translation,77,24,model,word unit prediction,directly constrains,output vocabulary,word unit prediction directly constrains output vocabulary,0.685221791267395
translation,77,24,model,model,to offer,stronger lexical control,model to offer stronger lexical control,0.6421316862106323
translation,77,70,results,our simplicity style scorer,achieves,f1 score,our simplicity style scorer achieves f1 score,0.6480685472488403
translation,77,70,results,f1 score,of,89.7,f1 score of 89.7,0.5459864139556885
translation,77,70,results,our class-conditional language model,achieves,perplexity,our class-conditional language model achieves perplexity,0.683632493019104
translation,77,70,results,perplexity,of,30.35,perplexity of 30.35,0.5350523591041565
translation,77,70,results,test set,has,our simplicity style scorer,test set has our simplicity style scorer,0.5758146047592163
translation,77,70,results,results,On,test set,results On test set,0.582119882106781
translation,77,73,results,recalls,for,two predictors,recalls for two predictors,0.649297833442688
translation,77,73,results,two predictors,on,test set,two predictors on test set,0.5644734501838684
translation,77,73,results,two predictors,are,81.5 and 80.0,two predictors are 81.5 and 80.0,0.5377009510993958
translation,77,73,results,test set,are,81.5 and 80.0,test set are 81.5 and 80.0,0.5798165798187256
translation,77,73,results,results,has,recalls,results has recalls,0.5936073660850525
translation,77,79,results,our models ' outputs,have,significantly better simplicity and readability,our models ' outputs have significantly better simplicity and readability,0.5510315895080566
translation,77,79,results,significantly better simplicity and readability,while preserving,fluency,significantly better simplicity and readability while preserving fluency,0.7388316988945007
translation,77,79,results,results,shows,our models ' outputs,results shows our models ' outputs,0.675017237663269
translation,77,82,results,word unit prediction,is,more effective,word unit prediction is more effective,0.5672200918197632
translation,77,82,results,more effective,at,lexical simplification,more effective at lexical simplification,0.488396555185318
translation,77,82,results,more effective,than,updating,more effective than updating,0.5885831713676453
translation,77,82,results,lexical simplification,than,updating,lexical simplification than updating,0.5591486096382141
translation,77,82,results,updating,has,decoder states,updating has decoder states,0.534637987613678
translation,77,82,results,results,find that,word unit prediction,results find that word unit prediction,0.6122673749923706
translation,77,85,results,decoder states,with,style scorer and language model,decoder states with style scorer and language model,0.632740318775177
translation,77,85,results,decoder states,yields,edit distance,decoder states yields edit distance,0.7063280940055847
translation,77,85,results,style scorer and language model,yields,edit distance,style scorer and language model yields edit distance,0.7021316289901733
translation,77,85,results,edit distance,of,45.7 and 47.4,edit distance of 45.7 and 47.4,0.5711090564727783
translation,77,85,results,larger distances,of,56.7 and 54.3,larger distances of 56.7 and 54.3,0.5764493942260742
translation,77,85,results,56.7 and 54.3,given by,word unit prediction,56.7 and 54.3 given by word unit prediction,0.660496175289154
translation,77,85,results,results,find,decoder states,results find decoder states,0.5782294273376465
translation,77,85,results,results,adjusting,decoder states,results adjusting decoder states,0.6518758535385132
translation,77,118,results,42.5 %,of,output pairs,42.5 % of output pairs,0.5715709328651428
translation,77,118,results,output pairs,by,decoder state adjustment model,output pairs by decoder state adjustment model,0.5514997243881226
translation,77,118,results,distinguished,significantly higher than,baselines,distinguished significantly higher than baselines,0.64434814453125
translation,77,118,results,baselines,has,24.5 % and 11.6 % ),baselines has 24.5 % and 11.6 % ),0.5343843102455139
