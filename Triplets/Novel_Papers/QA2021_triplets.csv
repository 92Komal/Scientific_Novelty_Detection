topic,paper_ID,sentence_ID,info-unit,sub,pred,obj,triplets,pred_weights
translation,0,127,ablation-analysis,improvement,remove,25 %,improvement remove 25 %,0.7024867534637451
translation,0,127,ablation-analysis,decreases,remove,25 %,decreases remove 25 %,0.7849303483963013
translation,0,127,ablation-analysis,25 %,of,full pool,25 % of full pool,0.6321275234222412
translation,0,127,ablation-analysis,degrades,remove,only 10 %,degrades remove only 10 %,0.7644282579421997
translation,0,127,ablation-analysis,improvement,has,decreases,improvement has decreases,0.6322592496871948
translation,0,127,ablation-analysis,ablation analysis,has,improvement,ablation analysis has improvement,0.5466791987419128
translation,0,153,experimental-setup,logistic regression,base,pytorch implementation,logistic regression base pytorch implementation,0.7682408094406128
translation,0,153,experimental-setup,pytorch implementation,on,broadly used scikit-learn,pytorch implementation on broadly used scikit-learn,0.4744423031806946
translation,0,153,experimental-setup,default parameters,including,l2 weight decay,default parameters including l2 weight decay,0.6259289383888245
translation,0,153,experimental-setup,experimental setup,When implementing,logistic regression,experimental setup When implementing logistic regression,0.6564841270446777
translation,0,154,experimental-setup,our models,via,stochastic gradient descent,our models via stochastic gradient descent,0.6431313157081604
translation,0,154,experimental-setup,experimental setup,optimize,our models,experimental setup optimize our models,0.6880456805229187
translation,0,164,experimental-setup,deep bayesian active learning methods,estimate,dropout distribution,deep bayesian active learning methods estimate dropout distribution,0.5838721394538879
translation,0,164,experimental-setup,dropout distribution,via,test-time dropout,dropout distribution via test-time dropout,0.6585331559181213
translation,0,164,experimental-setup,multiple forward passes,through,our neural networks,multiple forward passes through our neural networks,0.6447839140892029
translation,0,164,experimental-setup,our neural networks,with,"different , randomly sampled dropout masks","our neural networks with different , randomly sampled dropout masks",0.6122679710388184
translation,0,164,experimental-setup,experimental setup,estimate,dropout distribution,experimental setup estimate dropout distribution,0.6103640794754028
translation,0,97,experiments,active learning performance,fails to outperform,random acquisition,active learning performance fails to outperform random acquisition,0.7367376685142517
translation,0,97,experiments,vqa - sports,has,active learning performance,vqa - sports has active learning performance,0.5424233078956604
translation,0,97,experiments,active learning performance,has,varies,active learning performance has varies,0.5799300670623779
translation,0,14,results,common active learning ailments,has,cold starts,common active learning ailments has cold starts,0.5619373917579651
translation,0,14,results,results,has,negative results,results has negative results,0.5896515250205994
translation,0,15,results,cold start challenge,of needing,representative initial dataset,cold start challenge of needing representative initial dataset,0.6967136859893799
translation,0,15,results,representative initial dataset,by varying,size,representative initial dataset by varying size,0.7260972857475281
translation,0,15,results,size,of,seed set,size of seed set,0.6102426648139954
translation,0,15,results,results,mitigate,cold start challenge,results mitigate cold start challenge,0.6464892625808716
translation,0,95,results,least - confidence,appears to be,slightly more sample efficient,least - confidence appears to be slightly more sample efficient,0.6733602285385132
translation,0,95,results,lstm - cnn,has,least - confidence,lstm - cnn has least - confidence,0.571808397769928
translation,0,95,results,results,For,lstm - cnn,results For lstm - cnn,0.5659276843070984
translation,0,96,results,butd,for,lxmert,butd for lxmert,0.7185090780258179
translation,0,96,results,all methods,on par with,random,all methods on par with random,0.6856732964515686
translation,0,96,results,all methods,on par with,random,all methods on par with random,0.6856732964515686
translation,0,96,results,all methods,for,lxmert,all methods for lxmert,0.671073317527771
translation,0,96,results,lxmert,perform,worse,lxmert perform worse,0.7046026587486267
translation,0,96,results,worse,than,random,worse than random,0.6788857579231262
translation,0,96,results,butd,has,all methods,butd has all methods,0.6111549139022827
translation,0,96,results,results,For,butd,results For butd,0.6718477010726929
translation,0,103,results,active learning,performs,on par,active learning performs on par,0.5802971124649048
translation,0,103,results,on par,with or slightly worse,random,on par with or slightly worse random,0.637407124042511
translation,0,103,results,vqa - 2,has,active learning,vqa - 2 has active learning,0.6157441139221191
translation,0,126,results,2 - 3x improvement,in,sample efficiency,2 - 3x improvement in sample efficiency,0.5258573889732361
translation,0,126,results,sample efficiency,when removing,50 %,sample efficiency when removing 50 %,0.7305634021759033
translation,0,126,results,50 %,has,of the entire data pool,50 % has of the entire data pool,0.5701173543930054
translation,0,126,results,results,observe,2 - 3x improvement,results observe 2 - 3x improvement,0.6546734571456909
translation,0,180,results,active learning,for,lo-greg ( resnet - 101 ) model,active learning for lo-greg ( resnet - 101 ) model,0.6002334356307983
translation,0,180,results,lo-greg ( resnet - 101 ) model,on,vqa - sports,lo-greg ( resnet - 101 ) model on vqa - sports,0.5712674856185913
translation,0,180,results,lo-greg ( resnet - 101 ) model,on,vqa - 2,lo-greg ( resnet - 101 ) model on vqa - 2,0.5681623220443726
translation,0,181,results,active learning,has,failing to outperform,active learning has failing to outperform,0.5731949210166931
translation,0,181,results,failing to outperform,has,random acqusition,failing to outperform has random acqusition,0.6238912343978882
translation,1,136,baselines,aqad,use,electra - base,aqad use electra - base,0.6324357390403748
translation,1,136,baselines,electra - base,trained using,"tanda ( garg et al. , 2020 )","electra - base trained using tanda ( garg et al. , 2020 )",0.69712895154953
translation,1,136,baselines,initial transfer,on,asnq,initial transfer on asnq,0.6041469573974609
translation,1,136,baselines,asnq,as,m,asnq as m,0.7270285487174988
translation,1,137,baselines,question filter f,use,"two different transformer based models ( roberta - base , large )","question filter f use two different transformer based models ( roberta - base , large )",0.6320657730102539
translation,1,137,baselines,"two different transformer based models ( roberta - base , large )",for,datasets,"two different transformer based models ( roberta - base , large ) for datasets",0.6014499664306641
translation,1,118,experiments,as2 dataset,with,questions,as2 dataset with questions,0.6745961904525757
translation,1,118,experiments,questions,from,bing search logs,questions from bing search logs,0.5470215082168579
translation,1,118,experiments,answer candidates,from,wikipedia,answer candidates from wikipedia,0.6079150438308716
translation,1,118,experiments,wikiqa,has,as2 dataset,wikiqa has as2 dataset,0.5936561226844788
translation,1,7,model,transformer - based question models,by distilling,transformer - based answering models,transformer - based question models by distilling transformer - based answering models,0.6580499410629272
translation,1,7,model,model,learn,transformer - based question models,model learn transformer - based question models,0.626778244972229
translation,1,20,model,questions,by means of,question filtering model,questions by means of question filtering model,0.6283829808235168
translation,1,20,model,improving,has,qa system efficiency,improving has qa system efficiency,0.549824059009552
translation,1,34,model,two loss objectives,for training,two variants,two loss objectives for training two variants,0.7080938816070557
translation,1,34,model,two variants,of,question filter,two variants of question filter,0.612149715423584
translation,1,34,model,model,propose,two loss objectives,model propose two loss objectives,0.6845842599868774
translation,1,141,model,first,captures,well -formedness and intelligibility,first captures well -formedness and intelligibility,0.6381831169128418
translation,1,141,model,well -formedness and intelligibility,of,questions,well -formedness and intelligibility of questions,0.5480859279632568
translation,1,141,model,questions,from,human perspective,questions from human perspective,0.58881014585495
translation,1,141,model,model,captures,well -formedness and intelligibility,model captures well -formedness and intelligibility,0.7220563292503357
translation,1,141,model,model,has,first,model has first,0.6180936098098755
translation,1,5,results,answer confidence scores,of,state - of - the - art qa systems,answer confidence scores of state - of - the - art qa systems,0.5612978935241699
translation,1,5,results,answer confidence scores,approximated,well,answer confidence scores approximated well,0.7750150561332703
translation,1,5,results,well,by,models,well by models,0.65086829662323
translation,1,5,results,models,solely using,input question text,models solely using input question text,0.6906248927116394
translation,1,175,results,our question filters,with,classification and regression heads,our question filters with classification and regression heads,0.6789036989212036
translation,1,175,results,our question filters,impart,filtering efficiency gains,our question filters impart filtering efficiency gains,0.6834337115287781
translation,1,175,results,filtering efficiency gains,of,different proportions,filtering efficiency gains of different proportions,0.6103096008300781
translation,1,175,results,small drop,in,recall,small drop in recall,0.5826725959777832
translation,1,175,results,results,observe,our question filters,results observe our question filters,0.6592422127723694
translation,1,178,results,some cases,at,higher thresholds,some cases at higher thresholds,0.5653879642486572
translation,1,178,results,f m,achieves,comparable filtering performance,f m achieves comparable filtering performance,0.6962563991546631
translation,1,178,results,comparable filtering performance,to,f m ? 1,comparable filtering performance to f m ? 1,0.587036669254303
translation,1,178,results,some cases,has,f m,some cases has f m,0.6204404234886169
translation,1,178,results,higher thresholds,has,f m,higher thresholds has f m,0.6215530037879944
translation,1,178,results,results,Barring,some cases,results Barring some cases,0.7188590168952942
translation,1,185,results,wikiqa,observe that,our question filters,wikiqa observe that our question filters,0.614345908164978
translation,1,185,results,our question filters,increase,precision,our question filters increase precision,0.7454617023468018
translation,1,185,results,precision,of,system,precision of system,0.6280489563941956
translation,1,196,results,f m and f m ? 1,for,asnq,f m and f m ? 1 for asnq,0.7344663739204407
translation,1,196,results,both f w and f c,perform,inferior,both f w and f c perform inferior,0.658721923828125
translation,1,196,results,inferior,in terms of,filtering performance,inferior in terms of filtering performance,0.7479274272918701
translation,1,196,results,performance,has,both f w and f c,performance has both f w and f c,0.5963574647903442
translation,1,196,results,results,compared with,performance,results compared with performance,0.714249849319458
translation,2,196,ablation-analysis,positions,of,two words,positions of two words,0.6245435476303101
translation,2,196,ablation-analysis,two words,in,query,two words in query,0.5775836706161499
translation,2,196,ablation-analysis,two words,better maximizes,agreement,two words better maximizes agreement,0.7353439331054688
translation,2,196,ablation-analysis,agreement,between,positive example and the pseudo positive example,agreement between positive example and the pseudo positive example,0.6415225267410278
translation,2,196,ablation-analysis,positive example and the pseudo positive example,than,other two augmentations,positive example and the pseudo positive example than other two augmentations,0.5802650451660156
translation,2,196,ablation-analysis,better examples,to learn,representations,better examples to learn representations,0.6173166632652283
translation,2,196,ablation-analysis,ablation analysis,switching,positions,ablation analysis switching positions,0.7022891640663147
translation,2,206,ablation-analysis,code component,result of removing,documentation,code component result of removing documentation,0.6763822436332703
translation,2,176,experimental-setup,coclr,with,microsoft / codebert - base 4,coclr with microsoft / codebert - base 4,0.6867715716362
translation,2,176,experimental-setup,experimental setup,initialize,coclr,experimental setup initialize coclr,0.7460060715675354
translation,2,177,experimental-setup,adamw optimizer,set,batch size,adamw optimizer set batch size,0.6442095637321472
translation,2,177,experimental-setup,experimental setup,use,adamw optimizer,experimental setup use adamw optimizer,0.6070265769958496
translation,2,177,experimental-setup,experimental setup,set,batch size,experimental setup set batch size,0.6767397522926331
translation,2,178,experimental-setup,code question answering,set,learning rate,code question answering set learning rate,0.6134927272796631
translation,2,178,experimental-setup,code question answering,set,warm - up rate,code question answering set warm - up rate,0.6623480319976807
translation,2,178,experimental-setup,learning rate,to,1e - 5,learning rate to 1e - 5,0.5796148180961609
translation,2,178,experimental-setup,warm - up rate,to,0.1,warm - up rate to 0.1,0.5462769865989685
translation,2,178,experimental-setup,experimental setup,On,code question answering,experimental setup On code question answering,0.5571988224983215
translation,2,180,experimental-setup,hyper- parameters,tuned to,best,hyper- parameters tuned to best,0.7908582091331482
translation,2,180,experimental-setup,best,on,validation set,best on validation set,0.554353654384613
translation,2,180,experimental-setup,experimental setup,has,hyper- parameters,experimental setup has hyper- parameters,0.5121466517448425
translation,2,181,experimental-setup,experiments,performed on,nvidia tesla v100 gpu,experiments performed on nvidia tesla v100 gpu,0.5847019553184509
translation,2,181,experimental-setup,nvidia tesla v100 gpu,with,16gb memory,nvidia tesla v100 gpu with 16gb memory,0.611407458782196
translation,2,181,experimental-setup,experimental setup,has,experiments,experimental setup has experiments,0.5502888560295105
translation,2,179,experiments,code search,set,learning rate,code search set learning rate,0.6485311388969421
translation,2,179,experiments,learning rate,to,1e - 6,learning rate to 1e - 6,0.5687835812568665
translation,2,191,experiments,qra loss,test,three rewriting methods,qra loss test three rewriting methods,0.7565987706184387
translation,2,9,model,contrastive learning method,dubbed,co - clr,contrastive learning method dubbed co - clr,0.7759000062942505
translation,2,9,model,contrastive learning method,works as,data augmenter,contrastive learning method works as data augmenter,0.6899151802062988
translation,2,9,model,co - clr,to enhance,query - code matching,co - clr to enhance query - code matching,0.6943967342376709
translation,2,9,model,data augmenter,to bring,more artificially generated training instances,data augmenter to bring more artificially generated training instances,0.6175480484962463
translation,2,9,model,model,introduce,contrastive learning method,model introduce contrastive learning method,0.5946162343025208
translation,2,29,model,cosqa dataset,for,querycode matching,cosqa dataset for querycode matching,0.5739757418632507
translation,2,29,model,code contrastive learning method ( coclr ),to produce,more artificially generated instances,code contrastive learning method ( coclr ) to produce more artificially generated instances,0.6801189184188843
translation,2,29,model,more artificially generated instances,for,training,more artificially generated instances for training,0.5632014274597168
translation,2,29,model,model,to better leverage,cosqa dataset,model to better leverage cosqa dataset,0.6642385125160217
translation,2,29,model,model,propose,code contrastive learning method ( coclr ),model propose code contrastive learning method ( coclr ),0.6504925489425659
translation,2,10,results,codexglue,training on,cosqa,codexglue training on cosqa,0.7199009656906128
translation,2,10,results,cosqa,improves,accuracy,cosqa improves accuracy,0.7140802145004272
translation,2,10,results,cosqa,incorporating,coclr,cosqa incorporating coclr,0.6987667679786682
translation,2,10,results,accuracy,of,code question answering,accuracy of code question answering,0.5728222131729126
translation,2,10,results,accuracy,by,5.1 %,accuracy by 5.1 %,0.575509786605835
translation,2,10,results,code question answering,by,5.1 %,code question answering by 5.1 %,0.5582543611526489
translation,2,10,results,coclr,brings,further improvement,coclr brings further improvement,0.6585343480110168
translation,2,10,results,further improvement,of,10.5 %,further improvement of 10.5 %,0.5714250206947327
translation,2,10,results,results,evaluated on,codexglue,results evaluated on codexglue,0.6902067065238953
translation,2,10,results,results,incorporating,coclr,results incorporating coclr,0.6955321431159973
translation,2,31,results,code question answering,find that,performance,code question answering find that performance,0.5847194790840149
translation,2,31,results,performance,of,same code-bert model,performance of same code-bert model,0.6328086853027344
translation,2,31,results,performance,improves,5.1 %,performance improves 5.1 %,0.6996799111366272
translation,2,31,results,same code-bert model,improves,5.1 %,same code-bert model improves 5.1 %,0.7198432683944702
translation,2,31,results,5.1 %,after training on,cosqa dataset,5.1 % after training on cosqa dataset,0.6809648871421814
translation,2,31,results,10.5 %,after incorporating,coclr method,10.5 % after incorporating coclr method,0.7070920467376709
translation,2,31,results,results,On,code question answering,results On code question answering,0.5078201293945312
translation,2,184,results,siamese network,with,codebert,siamese network with codebert,0.6888240575790405
translation,2,184,results,siamese network,achieves,overall performance enhancement,siamese network achieves overall performance enhancement,0.7115849852561951
translation,2,184,results,overall performance enhancement,on,two tasks,overall performance enhancement on two tasks,0.5026360154151917
translation,2,184,results,cosqa dataset,has,siamese network,cosqa dataset has siamese network,0.5177896618843079
translation,2,184,results,results,By leveraging,cosqa dataset,results By leveraging cosqa dataset,0.663810670375824
translation,2,186,results,siamese network,with,codebert,siamese network with codebert,0.6888240575790405
translation,2,186,results,significant performance gain,on,both tasks,significant performance gain on both tasks,0.4859839379787445
translation,2,186,results,code contrastive learning method,has,siamese network,code contrastive learning method has siamese network,0.5411475896835327
translation,2,186,results,results,By integrating,code contrastive learning method,results By integrating code contrastive learning method,0.6465816497802734
translation,2,187,results,coclr,achieves,new state - of - the - art result,coclr achieves new state - of - the - art result,0.6626462340354919
translation,2,187,results,new state - of - the - art result,by increasing,15.6 %,new state - of - the - art result by increasing 15.6 %,0.6548713445663452
translation,2,187,results,webquerytest,has,coclr,webquerytest has coclr,0.5756336450576782
translation,2,192,results,iba and qra individually or together,improve,models ' performance,iba and qra individually or together improve models ' performance,0.6842482686042786
translation,2,192,results,results,incorporating,iba and qra individually or together,results incorporating iba and qra individually or together,0.6138361692428589
translation,2,194,results,model,with,qra,model with qra,0.6925755739212036
translation,2,194,results,model,with,other two methods,model with other two methods,0.6499634385108948
translation,2,194,results,qra,by,switching method,qra by switching method,0.575788140296936
translation,2,194,results,better,than,models,better than models,0.5885884165763855
translation,2,194,results,models,with,other two methods,models with other two methods,0.6430172324180603
translation,2,194,results,iba,has,model,iba has model,0.6169556975364685
translation,2,194,results,results,integrating,iba,results integrating iba,0.7203191518783569
translation,2,197,results,iba,achieves,more performance gain,iba achieves more performance gain,0.6993572115898132
translation,2,197,results,more performance gain,than,qra,more performance gain than qra,0.5975471138954163
translation,2,197,results,qra,has,1.25 % versus 9.10 % ),qra has 1.25 % versus 9.10 % ),0.557633101940155
translation,2,198,results,more example,with,iba,more example with iba,0.6805312633514404
translation,2,199,results,mrr,is,55.52 %,mrr is 55.52 %,0.5738629698753357
translation,2,199,results,55.52 %,comparable to,performance,55.52 % comparable to performance,0.6927902698516846
translation,2,199,results,performance,adding,one more example,performance adding one more example,0.7409164309501648
translation,2,199,results,one more example,with,qra,one more example with qra,0.6828168034553528
translation,2,199,results,results,has,mrr,results has mrr,0.5548900961875916
translation,3,27,model,memory - efficient transfer,to,new questions,memory - efficient transfer to new questions,0.5588549375534058
translation,3,27,model,model,consider,single fixed passage index,model consider single fixed passage index,0.733768880367279
translation,3,27,model,model,fine-tune,specialized question encoders,model fine-tune specialized question encoders,0.7395598888397217
translation,3,23,results,dense model,able to,successfully answer questions,dense model able to successfully answer questions,0.6403258442878723
translation,3,23,results,dense model,able to,quickly degrading,dense model able to quickly degrading,0.6021352410316467
translation,3,23,results,successfully answer questions,based on,common entities,successfully answer questions based on common entities,0.6168495416641235
translation,3,23,results,quickly degrading,on,rarer entities,quickly degrading on rarer entities,0.574135422706604
translation,3,23,results,results,discover,dense model,results discover dense model,0.6340646147727966
translation,3,28,results,data augmentation,unable to,consistently improve,data augmentation unable to consistently improve,0.7210603952407837
translation,3,28,results,performance,on,unseen domains,performance on unseen domains,0.5602566003799438
translation,3,28,results,consistently improve,has,performance,consistently improve has performance,0.5731132626533508
translation,3,28,results,results,find that,data augmentation,results find that data augmentation,0.6195405721664429
translation,3,60,results,dpr,trained on,nq,dpr trained on nq,0.6633558869361877
translation,3,60,results,dpr,trained on,significantly underperforms,dpr trained on significantly underperforms,0.7878987789154053
translation,3,60,results,bm25,on,almost all sets of questions,bm25 on almost all sets of questions,0.5433079600334167
translation,3,60,results,nq,has,significantly underperforms,nq has significantly underperforms,0.6337429285049438
translation,3,60,results,significantly underperforms,has,bm25,significantly underperforms has bm25,0.6510165333747864
translation,3,60,results,results,has,dpr,results has dpr,0.5591716170310974
translation,3,81,results,dpr,performs,generally better,dpr performs generally better,0.6399312019348145
translation,3,81,results,generally better,on,entities,generally better on entities,0.4967291057109833
translation,3,81,results,entities,seen during,nq training,entities seen during nq training,0.6712694764137268
translation,3,81,results,results,notable,dpr,results notable dpr,0.7378437519073486
translation,3,90,results,bm25,in terms of,retrieval accuracy,bm25 in terms of retrieval accuracy,0.6450663208961487
translation,3,90,results,all three relations,has,dpr,all three relations has dpr,0.6301817297935486
translation,3,90,results,match or even outperform,has,bm25,match or even outperform has bm25,0.5926561951637268
translation,3,90,results,results,On,all three relations,results On all three relations,0.4847460091114044
translation,3,91,results,training,on,equivalent question pattern,training on equivalent question pattern,0.5766064524650574
translation,3,91,results,equivalent question pattern,achieves,comparable performance,equivalent question pattern achieves comparable performance,0.707865297794342
translation,3,91,results,comparable performance,to,exact pattern,comparable performance to exact pattern,0.5709208846092224
translation,3,91,results,results,has,training,results has training,0.5440090298652649
translation,3,106,results,fine-tuning,only on,single relation,fine-tuning only on single relation,0.6352283358573914
translation,3,106,results,performance,on,nq,performance on nq,0.5971578359603882
translation,3,106,results,improves,has,entity questions,improves has entity questions,0.5985928773880005
translation,3,106,results,degrades,has,performance,degrades has performance,0.5834147930145264
translation,3,106,results,results,find that,fine-tuning,results find that fine-tuning,0.6353985667228699
translation,3,107,results,fine- tuning,on,both relation questions and nq together,fine- tuning on both relation questions and nq together,0.5359460711479187
translation,3,107,results,most of the performance,on,nq,most of the performance on nq,0.6079544425010681
translation,3,107,results,nq,is,retained,nq is retained,0.7078999876976013
translation,3,107,results,gains,on,entity questions,gains on entity questions,0.576928973197937
translation,3,107,results,entity questions,are,much more muted,entity questions are much more muted,0.6212788820266724
translation,3,107,results,fine- tuning,has,most of the performance,fine- tuning has most of the performance,0.5805339813232422
translation,3,107,results,both relation questions and nq together,has,most of the performance,both relation questions and nq together has most of the performance,0.6047940254211426
translation,3,107,results,results,When,fine- tuning,results When fine- tuning,0.6271789073944092
translation,3,116,results,encoder,trained on,paq,encoder trained on paq,0.7060667872428894
translation,3,116,results,encoder,trained on,nq,encoder trained on nq,0.7050448656082153
translation,3,116,results,paq,improves,performance,paq improves performance,0.7051284313201904
translation,3,116,results,performance,over,fine-tuning,performance over fine-tuning,0.7293926477432251
translation,3,116,results,encoder,trained on,nq,encoder trained on nq,0.7050448656082153
translation,3,116,results,fine-tuning,has,encoder,fine-tuning has encoder,0.5534709095954895
translation,3,116,results,results,fine-tuning,encoder,results fine-tuning encoder,0.6487398147583008
translation,4,135,ablation-analysis,number of paragraphs,retrieved at,each reasoning step,number of paragraphs retrieved at each reasoning step,0.40813112258911133
translation,4,135,ablation-analysis,number of paragraphs,remains,effective,number of paragraphs remains effective,0.5872321724891663
translation,4,135,ablation-analysis,effective,to improve,end-to - end performance,effective to improve end-to - end performance,0.7307290434837341
translation,4,135,ablation-analysis,end-to - end performance,of,irrr,end-to - end performance of irrr,0.5871056318283081
translation,4,135,ablation-analysis,effective,has,strategy,effective has strategy,0.570486843585968
translation,4,135,ablation-analysis,ablation analysis,increasing,number of paragraphs,ablation analysis increasing number of paragraphs,0.7035728096961975
translation,4,8,baselines,existing one - and twostep datasets,with,new collection,existing one - and twostep datasets with new collection,0.6296884417533875
translation,4,8,baselines,new collection,of,530 questions,new collection of 530 questions,0.598487377166748
translation,4,8,baselines,530 questions,require,three wikipedia pages,530 questions require three wikipedia pages,0.7050010561943054
translation,4,37,baselines,open-domain qa benchmark,features,questions,open-domain qa benchmark features questions,0.6909269690513611
translation,4,37,baselines,questions,requiring,variable steps of reasoning,questions requiring variable steps of reasoning,0.6984132528305054
translation,4,37,baselines,variable steps of reasoning,to answer,unified wikipedia corpus,variable steps of reasoning to answer unified wikipedia corpus,0.6485645771026611
translation,4,38,baselines,single unified neural network model,performs,all essential subtasks,single unified neural network model performs all essential subtasks,0.6313251852989197
translation,4,38,baselines,all essential subtasks,in,open-domain qa,all essential subtasks in open-domain qa,0.4915330111980438
translation,4,38,baselines,open-domain qa,purely from,text,open-domain qa purely from text,0.6820825934410095
translation,4,45,baselines,irrr,performs,variable - hop retrieval,irrr performs variable - hop retrieval,0.6007996797561646
translation,4,45,baselines,variable - hop retrieval,for,open-domain qa,variable - hop retrieval for open-domain qa,0.6172904372215271
translation,4,109,experimental-setup,"electra large ( clark et al. , 2020 )",as,pre-trained initialization,"electra large ( clark et al. , 2020 ) as pre-trained initialization",0.4905896782875061
translation,4,109,experimental-setup,pre-trained initialization,for,transformer encoder,pre-trained initialization for transformer encoder,0.582426905632019
translation,4,109,experimental-setup,experimental setup,use,"electra large ( clark et al. , 2020 )","experimental setup use electra large ( clark et al. , 2020 )",0.5973748564720154
translation,4,10,experiments,https,:,//beerqa.github.io/,https : //beerqa.github.io/,0.611752986907959
translation,4,33,experiments,questions,from,single - hop squad open,questions from single - hop squad open,0.5621165037155151
translation,4,33,experiments,questions,from,two -hop hotpotqa,questions from two -hop hotpotqa,0.5643867254257202
translation,4,124,experiments,3 + hop challenge set,notice,large performance margin,3 + hop challenge set notice large performance margin,0.7168141007423401
translation,4,124,experiments,large performance margin,between,irrr and grr,large performance margin between irrr and grr,0.6510405540466309
translation,4,5,model,single multi-task transformer model,to perform,necessary subtasks,single multi-task transformer model to perform necessary subtasks,0.7078295350074768
translation,4,5,model,necessary subtasks,retrieving,supporting facts,necessary subtasks retrieving supporting facts,0.7665280699729919
translation,4,5,model,answer,from,all retrieved documents,answer from all retrieved documents,0.5965999960899353
translation,4,5,model,answer,in,iterative fashion,answer in iterative fashion,0.5688536167144775
translation,4,5,model,model,employ,single multi-task transformer model,model employ single multi-task transformer model,0.5615403652191162
translation,4,7,model,open-domain questions,on,any text collection,open-domain questions on any text collection,0.5268977284431458
translation,4,7,model,answer,has,open-domain questions,answer has open-domain questions,0.5654269456863403
translation,4,29,model,"iterative retriever , reader , and reranker ( irrr )",features,single neural network model,"iterative retriever , reader , and reranker ( irrr ) features single neural network model",0.6656479835510254
translation,4,29,model,single neural network model,that performs,all of the subtasks,single neural network model that performs all of the subtasks,0.6703246235847473
translation,4,29,model,all of the subtasks,required to,answer questions,all of the subtasks required to answer questions,0.6368902921676636
translation,4,29,model,answer questions,from,large collection of text,answer questions from large collection of text,0.5748288631439209
translation,4,29,model,model,propose,"iterative retriever , reader , and reranker ( irrr )","model propose iterative retriever , reader , and reranker ( irrr )",0.6551189422607422
translation,4,30,model,irrr,to leverage,off - the-shelf information retrieval systems,irrr to leverage off - the-shelf information retrieval systems,0.6246541738510132
translation,4,30,model,off - the-shelf information retrieval systems,by generating,natural language search queries,off - the-shelf information retrieval systems by generating natural language search queries,0.6520177125930786
translation,4,30,model,model,has,irrr,model has irrr,0.6331787109375
translation,4,57,model,of reasoning paths,from,shared statistical learning,of reasoning paths from shared statistical learning,0.5637986063957214
translation,4,57,model,irrr,implemented as,multitask model,irrr implemented as multitask model,0.6228503584861755
translation,4,57,model,multitask model,built on,pretrained transformer model,multitask model built on pretrained transformer model,0.6668422818183899
translation,4,57,model,multitask model,performs,all three subtasks,multitask model performs all three subtasks,0.5870782732963562
translation,4,57,model,model representations,has,of reasoning paths,model representations has of reasoning paths,0.58165442943573
translation,4,57,model,model representations,has,irrr,model representations has irrr,0.6266783475875854
translation,4,143,model,irrr,to dynamically stop retrieving,paragraphs,irrr to dynamically stop retrieving paragraphs,0.7590028047561646
translation,4,143,model,paragraphs,to answer,question,paragraphs to answer question,0.7543830871582031
translation,4,122,results,irrr,achieves,competitive performance,irrr achieves competitive performance,0.6752617359161377
translation,4,122,results,competitive performance,with,previous work,competitive performance with previous work,0.6275144815444946
translation,4,122,results,outperforms,on,squad open,outperforms on squad open,0.581764280796051
translation,4,122,results,previously published work,on,squad open,previously published work on squad open,0.5285186767578125
translation,4,122,results,squad open,by,large margin,squad open by large margin,0.5626469254493713
translation,4,122,results,large margin,when trained on,combined data,large margin when trained on combined data,0.691471517086029
translation,4,122,results,outperforms,has,previously published work,outperforms has previously published work,0.6242783665657043
translation,4,123,results,outperforms,submitted after,irrr,outperforms submitted after irrr,0.6945390701293945
translation,4,123,results,systems,submitted after,irrr,systems submitted after irrr,0.7310757637023926
translation,4,123,results,irrr,initially submitted to,hotpotqa leaderboard,irrr initially submitted to hotpotqa leaderboard,0.6543877124786377
translation,4,123,results,outperforms,has,systems,outperforms has systems,0.6560349464416504
translation,4,123,results,results,has,outperforms,results has outperforms,0.6657275557518005
translation,4,139,results,irrr 's performance,remains,competitive,irrr 's performance remains competitive,0.6542666554450989
translation,4,139,results,competitive,on,all questions,competitive on all questions,0.5466597080230713
translation,4,139,results,all questions,from,different origins,all questions from different origins,0.5807094573974609
translation,4,139,results,different origins,in,unified benchmark,different origins in unified benchmark,0.5336236357688904
translation,4,144,results,irrr,improves,f 1,irrr improves f 1,0.6971485614776611
translation,4,144,results,f 1,on,squad and hotpotqa questions,f 1 on squad and hotpotqa questions,0.5533515810966492
translation,4,144,results,f 1,by,27.0 and 2.1 points,f 1 by 27.0 and 2.1 points,0.6343247890472412
translation,4,144,results,squad and hotpotqa questions,by,27.0 and 2.1 points,squad and hotpotqa questions by 27.0 and 2.1 points,0.5753254890441895
translation,4,144,results,fixed -step retrieval counterpart,has,dynamically stopping,fixed -step retrieval counterpart has dynamically stopping,0.559707760810852
translation,4,144,results,dynamically stopping,has,irrr,dynamically stopping has irrr,0.6409104466438293
translation,4,144,results,results,Compared to,fixed -step retrieval counterpart,results Compared to fixed -step retrieval counterpart,0.6994742155075073
translation,4,145,results,squad and hotpotqa datasets,beneficial for,both datasets,squad and hotpotqa datasets beneficial for both datasets,0.6278463006019592
translation,4,145,results,both datasets,in,opendomain setting,both datasets in opendomain setting,0.4879083037376404
translation,4,145,results,electra,effective alternative to,bert,electra effective alternative to bert,0.6542254090309143
translation,4,145,results,results,combining,squad and hotpotqa datasets,results combining squad and hotpotqa datasets,0.6206333041191101
translation,5,7,model,large-scale controlled study,focused on,question answering,large-scale controlled study focused on question answering,0.7214006185531616
translation,5,7,model,large-scale controlled study,assigning,workers at random,large-scale controlled study assigning workers at random,0.6204788088798523
translation,5,7,model,workers at random,to compose,questions,workers at random to compose questions,0.6943114399909973
translation,5,7,model,adversarially,with,model in the loop,adversarially with model in the loop,0.6820029616355896
translation,5,7,model,model,conduct,large-scale controlled study,model conduct large-scale controlled study,0.680111825466156
translation,5,24,results,better performance,leads to,worse performance,better performance leads to worse performance,0.6531063914299011
translation,6,10,baselines,transformerbased solution,exploits,recent advances,transformerbased solution exploits recent advances,0.7539981603622437
translation,6,10,baselines,recent advances,in,temporal kg embeddings,recent advances in temporal kg embeddings,0.5175334811210632
translation,6,10,baselines,performance,superior to,all baselines,performance superior to all baselines,0.6956870555877686
translation,6,10,baselines,performance,with,increase,performance with increase,0.688456118106842
translation,6,10,baselines,increase,of,120 %,increase of 120 %,0.6625674962997437
translation,6,10,baselines,120 %,in,accuracy,120 % in accuracy,0.582901120185852
translation,6,10,baselines,accuracy,over,next best performing method,accuracy over next best performing method,0.6576076745986938
translation,6,10,baselines,cronkgqa,has,transformerbased solution,cronkgqa has transformerbased solution,0.6245250105857849
translation,6,10,baselines,baselines,propose,cronkgqa,baselines propose cronkgqa,0.7030168175697327
translation,6,35,baselines,baselines,propose,cronquestions,baselines propose cronquestions,0.6466704607009888
translation,6,43,baselines,cronkgqa,enhancement of,embedkgqa,cronkgqa enhancement of embedkgqa,0.6166006922721863
translation,6,43,baselines,baselines,across,all question types,baselines across all question types,0.7062861919403076
translation,6,43,baselines,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
translation,6,43,baselines,baselines,propose,cronkgqa,baselines propose cronkgqa,0.7030168175697327
translation,6,44,experiments,cronkgqa,achieves,very high accuracy,cronkgqa achieves very high accuracy,0.7024053931236267
translation,6,44,experiments,very high accuracy,on,simple temporal reasoning questions,very high accuracy on simple temporal reasoning questions,0.4953576326370239
translation,6,44,experiments,falls short,to,questions,falls short to questions,0.591334342956543
translation,6,44,experiments,questions,requiring,more complex reasoning,questions requiring more complex reasoning,0.6670618057250977
translation,6,9,results,various state - of- the - art kgqa methods,fall far short of,desired performance,various state - of- the - art kgqa methods fall far short of desired performance,0.6495075225830078
translation,6,9,results,results,find that,various state - of- the - art kgqa methods,results find that various state - of- the - art kgqa methods,0.6029454469680786
translation,7,7,model,novel rationale - enriched answer generator ( reag ),incorporates,extractive mechanism,novel rationale - enriched answer generator ( reag ) incorporates extractive mechanism,0.6915444135665894
translation,7,7,model,extractive mechanism,into,generative model,extractive mechanism into generative model,0.5809080004692078
translation,7,8,model,extraction task,on,encoder,extraction task on encoder,0.5533593893051147
translation,7,8,model,encoder,to obtain,rationale,encoder to obtain rationale,0.6442767977714539
translation,7,8,model,rationale,for,answer,rationale for answer,0.6133882999420166
translation,7,8,model,model,add,extraction task,model add extraction task,0.5987666845321655
translation,7,33,model,novel rationale - enriched answer generator ( reag ),incorporates,extractive mechanism,novel rationale - enriched answer generator ( reag ) incorporates extractive mechanism,0.6915444135665894
translation,7,33,model,extractive mechanism,into,generative model,extractive mechanism into generative model,0.5809080004692078
translation,7,33,model,generative model,to leverage,relevant information,generative model to leverage relevant information,0.7144941091537476
translation,7,33,model,relevant information,to,given question,relevant information to given question,0.5557771325111389
translation,7,33,model,given question,in,contextual passage,given question in contextual passage,0.5265539288520813
translation,7,34,model,extraction task,on,encoder,extraction task on encoder,0.5533593893051147
translation,7,34,model,encoder,to obtain,rationale,encoder to obtain rationale,0.6442767977714539
translation,7,34,model,rationale,for,answer,rationale for answer,0.6133882999420166
translation,7,34,model,model,add,extraction task,model add extraction task,0.5987666845321655
translation,7,35,model,relevance,between,question,relevance between question,0.5786204934120178
translation,7,35,model,relevance,between,passage,relevance between passage,0.6486535668373108
translation,7,35,model,extracted rationale,to guide,answer generation,extracted rationale to guide answer generation,0.6585440039634705
translation,8,155,ablation-analysis,increases to beyond 40 %,towards,high- resource setup,increases to beyond 40 % towards high- resource setup,0.710502564907074
translation,8,155,ablation-analysis,gains,from,salient spans pre-training,gains from salient spans pre-training,0.5612247586250305
translation,8,155,ablation-analysis,saturates,to,ict,saturates to ict,0.6286035776138306
translation,8,155,ablation-analysis,fraction of training data,has,increases to beyond 40 %,fraction of training data has increases to beyond 40 %,0.5939111709594727
translation,8,155,ablation-analysis,salient spans pre-training,has,saturates,salient spans pre-training has saturates,0.5701403021812439
translation,8,194,ablation-analysis,large configuration,obtain,gain,large configuration obtain gain,0.6360779404640198
translation,8,194,ablation-analysis,gain,of,0.7 points,gain of 0.7 points,0.583693265914917
translation,8,194,ablation-analysis,0.7 points,on,triviaqa,0.7 points on triviaqa,0.5270918607711792
translation,8,194,ablation-analysis,ablation analysis,For,large configuration,ablation analysis For large configuration,0.6144216656684875
translation,8,195,ablation-analysis,improve,with,more retrieved documents,improve with more retrieved documents,0.6426182389259338
translation,8,195,ablation-analysis,em scores,has,improve,em scores has improve,0.6105311512947083
translation,8,9,baselines,two approaches,for,end-toend training,two approaches for end-toend training,0.6242433190345764
translation,8,9,baselines,end-toend training,of,reader and retriever components,end-toend training of reader and retriever components,0.5883429050445557
translation,8,9,baselines,reader and retriever components,in,openqa models,reader and retriever components in openqa models,0.5099837183952332
translation,8,9,baselines,baselines,explore,two approaches,baselines explore two approaches,0.6516448855400085
translation,8,13,experimental-setup,https,:,//github.com,https : //github.com,0.6116153597831726
translation,8,13,experimental-setup,https,:,nvidia,https : nvidia,0.6541047096252441
translation,8,13,experimental-setup,https,/,nvidia,https / nvidia,0.6510286331176758
translation,8,13,experimental-setup,//github.com,/,nvidia,//github.com / nvidia,0.5832775235176086
translation,8,13,experimental-setup,nvidia,/,megatron-lm,nvidia / megatron-lm,0.5842670202255249
translation,8,7,model,approach,of,unsupervised pre-training,approach of unsupervised pre-training,0.5632089376449585
translation,8,7,model,unsupervised pre-training,with,inverse cloze task,unsupervised pre-training with inverse cloze task,0.6143282651901245
translation,8,7,model,unsupervised pre-training,with,masked salient spans,unsupervised pre-training with masked salient spans,0.6383485794067383
translation,8,7,model,masked salient spans,followed by,supervised finetuning,masked salient spans followed by supervised finetuning,0.6623449325561523
translation,8,7,model,supervised finetuning,using,question - context pairs,supervised finetuning using question - context pairs,0.5734497308731079
translation,8,7,model,model,propose,approach,model propose approach,0.6953083872795105
translation,8,37,model,two approaches,for,end-to - end supervised training,two approaches for end-to - end supervised training,0.5706958770751953
translation,8,37,model,end-to - end supervised training,of,reader and retriever components,end-to - end supervised training of reader and retriever components,0.5742361545562744
translation,8,37,model,model,explore,two approaches,model explore two approaches,0.7196502685546875
translation,8,38,model,reader,considers,each retrieved document,reader considers each retrieved document,0.7353337407112122
translation,8,38,model,reader,takes as input,all the retrieved documents together,reader takes as input all the retrieved documents together,0.7339702248573303
translation,8,38,model,reader,takes as input,all the retrieved documents together,reader takes as input all the retrieved documents together,0.7339702248573303
translation,8,38,model,first approach,has,reader,first approach has reader,0.591556966304779
translation,8,38,model,first approach,has,reader,first approach has reader,0.591556966304779
translation,8,38,model,second approach,has,reader,second approach has reader,0.6160913109779358
translation,8,110,model,hidden representations,of,all the retrieved documents,hidden representations of all the retrieved documents,0.5305026173591614
translation,8,110,model,decoder,jointly attends to,encoder-decoder attention,decoder jointly attends to encoder-decoder attention,0.6942141652107239
translation,8,110,model,encoder-decoder attention,allowing,more powerful form of information aggregation,encoder-decoder attention allowing more powerful form of information aggregation,0.7364373803138733
translation,8,110,model,more powerful form of information aggregation,from,multiple retrieved documents,more powerful form of information aggregation from multiple retrieved documents,0.5577183961868286
translation,8,110,model,model,stack,hidden representations,model stack hidden representations,0.759638786315918
translation,8,127,model,base configuration,consists of,12 layers,base configuration consists of 12 layers,0.6964387893676758
translation,8,127,model,base configuration,consists of,768 - d hidden size,base configuration consists of 768 - d hidden size,0.6814106106758118
translation,8,127,model,base configuration,consists of,12 attention heads,base configuration consists of 12 attention heads,0.6770180463790894
translation,8,127,model,model,has,base configuration,model has base configuration,0.577606737613678
translation,8,213,model,retriever,with,end-to - end training,retriever with end-to - end training,0.6605740189552307
translation,8,213,model,end-to - end training,leveraging,question - answer pairs,end-to - end training leveraging question - answer pairs,0.6643496155738831
translation,8,213,model,retrieval accuracy,leading to,new state - of - the - art results,retrieval accuracy leading to new state - of - the - art results,0.684101402759552
translation,8,213,model,model,update,retriever,model update retriever,0.825598418712616
translation,8,11,results,natural questions dataset,obtain,top - 20 retrieval accuracy,natural questions dataset obtain top - 20 retrieval accuracy,0.47849878668785095
translation,8,11,results,top - 20 retrieval accuracy,of,84 %,top - 20 retrieval accuracy of 84 %,0.5720074772834778
translation,8,11,results,improvement,of,5 points,improvement of 5 points,0.5328450798988342
translation,8,11,results,5 points,over,recent dpr model,5 points over recent dpr model,0.6542555689811707
translation,8,11,results,results,On,natural questions dataset,results On natural questions dataset,0.4935232400894165
translation,8,12,results,good results,on,answer extraction,good results on answer extraction,0.5116654634475708
translation,8,12,results,outperforming,by,3 + points,outperforming by 3 + points,0.5736504793167114
translation,8,12,results,recent models,by,3 + points,recent models by 3 + points,0.5476674437522888
translation,8,12,results,rag,by,3 + points,rag by 3 + points,0.6434000134468079
translation,8,12,results,good results,has,outperforming,good results has outperforming,0.6001008152961731
translation,8,12,results,outperforming,has,recent models,outperforming has recent models,0.5858444571495056
translation,8,12,results,results,showcase,good results,results showcase good results,0.6677725911140442
translation,8,137,results,our results,obtain,substantial additional gains,our results obtain substantial additional gains,0.593249499797821
translation,8,137,results,in sync,results of,dpr,in sync results of dpr,0.6798499226570129
translation,8,137,results,in sync,obtain,substantial additional gains,in sync obtain substantial additional gains,0.5944573283195496
translation,8,137,results,substantial additional gains,in,performance,substantial additional gains in performance,0.5685215592384338
translation,8,137,results,our results,has,in sync,our results has in sync,0.6356425881385803
translation,8,137,results,results,has,our results,results has our results,0.5639954209327698
translation,8,137,results,results,has,in sync,results has in sync,0.6338889598846436
translation,8,144,results,ict initialization,is,quite effective,ict initialization is quite effective,0.5640667676925659
translation,8,144,results,quite effective,in providing,non-trivial zero-shot accuracy,quite effective in providing non-trivial zero-shot accuracy,0.6204187870025635
translation,8,144,results,non-trivial zero-shot accuracy,further improved by,masked salient spans training,non-trivial zero-shot accuracy further improved by masked salient spans training,0.6773298978805542
translation,8,144,results,masked salient spans training,by,more than 8 points,masked salient spans training by more than 8 points,0.5735080242156982
translation,8,144,results,results,note,ict initialization,results note ict initialization,0.6254864931106567
translation,8,147,results,absolute improvements,of,2 - 3 points,absolute improvements of 2 - 3 points,0.5479277968406677
translation,8,147,results,2 - 3 points,over,already strong supervised training results,2 - 3 points over already strong supervised training results,0.6841027736663818
translation,8,147,results,results,provides,absolute improvements,results provides absolute improvements,0.6561080813407898
translation,8,150,results,ict,with,masked salient spans initialization,ict with masked salient spans initialization,0.6542859077453613
translation,8,150,results,ict,note that,accuracy gains,ict note that accuracy gains,0.6081337928771973
translation,8,150,results,accuracy gains,are,roughly similar,accuracy gains are roughly similar,0.5470387935638428
translation,8,150,results,results,comparing,ict,results comparing ict,0.4922047257423401
translation,8,154,results,masked salient spans,is,much more effective,masked salient spans is much more effective,0.569458544254303
translation,8,154,results,much more effective,than,ict,much more effective than ict,0.6346992254257202
translation,8,154,results,much more effective,leading to,large gains,much more effective leading to large gains,0.6660040616989136
translation,8,154,results,lowresource regime,has,masked salient spans,lowresource regime has masked salient spans,0.5906031727790833
translation,8,154,results,masked salient spans,has,pre-training,masked salient spans has pre-training,0.5635505318641663
translation,8,154,results,results,reveal,lowresource regime,results reveal lowresource regime,0.6511368155479431
translation,8,154,results,results,in,lowresource regime,results in lowresource regime,0.5554249286651611
translation,8,161,results,individual top-k,when,only the query encoder,individual top-k when only the query encoder,0.6360069513320923
translation,8,161,results,individual top-k,tends to improve,retrieval accuracy,individual top-k tends to improve retrieval accuracy,0.7087091207504272
translation,8,161,results,only the query encoder,is,updated,only the query encoder is updated,0.6107235550880432
translation,8,161,results,results,for,individual top-k,results for individual top-k,0.5728159546852112
translation,8,162,results,context encoder,is,updated,context encoder is updated,0.5692617297172546
translation,8,162,results,retrieval accuracy,improves to,75 %,retrieval accuracy improves to 75 %,0.6998484134674072
translation,8,162,results,75 %,at,top - 5,75 % at top - 5,0.5631572604179382
translation,8,162,results,big gain,of,8 points,big gain of 8 points,0.549699604511261
translation,8,162,results,8 points,over,previous best dpr retriever,8 points over previous best dpr retriever,0.6742314100265503
translation,8,162,results,context encoder,has,retrieval accuracy,context encoder has retrieval accuracy,0.5267490148544312
translation,8,162,results,updated,has,retrieval accuracy,updated has retrieval accuracy,0.5653438568115234
translation,8,162,results,top - 5,has,big gain,top - 5 has big gain,0.6046501994132996
translation,8,162,results,results,when,context encoder,results when context encoder,0.6879372000694275
translation,8,181,results,base configuration,on,nq,base configuration on nq,0.5754088163375854
translation,8,181,results,realm and dpr,by,more than 4 points,realm and dpr by more than 4 points,0.6061826348304749
translation,8,181,results,base configuration,has,our model,base configuration has our model,0.5900694131851196
translation,8,181,results,nq,has,our model,nq has our model,0.6335092782974243
translation,8,181,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,8,181,results,results,For,base configuration,results For base configuration,0.6231942176818848
translation,8,182,results,outperforms,by,3.5 + points,outperforms by 3.5 + points,0.6090812087059021
translation,8,182,results,outperforms,by,2.8 points,outperforms by 2.8 points,0.620735228061676
translation,8,182,results,3.5 + points,on,nq,3.5 + points on nq,0.5721628665924072
translation,8,182,results,2.8 points,on,triviaqa,2.8 points on triviaqa,0.5464196801185608
translation,8,182,results,rag model,has,"lewis et al. , 2020 c )","rag model has lewis et al. , 2020 c )",0.5819589495658875
translation,8,182,results,our approach,has,outperforms,our approach has outperforms,0.6385829448699951
translation,8,186,results,context encoder,improves,results,context encoder improves results,0.6982349753379822
translation,8,186,results,results,for,base and large configurations,results for base and large configurations,0.6359626054763794
translation,8,186,results,results,both,base and large configurations,results both base and large configurations,0.70545494556427
translation,8,186,results,results,updating,context encoder,results updating context encoder,0.7530626058578491
translation,8,187,results,performance,of,individual top -k approach,performance of individual top -k approach,0.5659408569335938
translation,8,187,results,individual top -k approach,sensitive to,number of top -k documents,individual top -k approach sensitive to number of top -k documents,0.6543140411376953
translation,8,187,results,decrease,increase in,top-k documents,decrease increase in top-k documents,0.6455739736557007
translation,8,187,results,results,observe,performance,results observe performance,0.6366938948631287
translation,8,193,results,joint topk,outperforms,fid model,joint topk outperforms fid model,0.6901698112487793
translation,8,193,results,fid model,by,1 point,fid model by 1 point,0.6256147623062134
translation,8,193,results,1 point,on,nq,1 point on nq,0.6277516484260559
translation,8,193,results,base configuration,has,joint topk,base configuration has joint topk,0.5747221112251282
translation,8,193,results,results,for,base configuration,results for base configuration,0.6231942176818848
translation,8,201,results,individual top-k approach,helps to,significantly improve,individual top-k approach helps to significantly improve,0.7046449780464172
translation,8,201,results,joint top -k approach,is,more useful,joint top -k approach is more useful,0.5558451414108276
translation,8,201,results,more useful,for,answer extraction,more useful for answer extraction,0.5797950625419617
translation,8,201,results,significantly improve,has,retrieval performance,significantly improve has retrieval performance,0.574329674243927
translation,8,201,results,results,has,individual top-k approach,results has individual top-k approach,0.5444643497467041
translation,9,196,baselines,end-to - end fusion model,employ,"pre-trained auto-encoder bart ( lewis et al. , 2020 )","end-to - end fusion model employ pre-trained auto-encoder bart ( lewis et al. , 2020 )",0.5274163484573364
translation,9,8,experiments,qa - srl,utilizing,question - answer pairs,qa - srl utilizing question - answer pairs,0.6679412126541138
translation,9,8,experiments,question - answer pairs,to capture,predicate - argument relations,question - answer pairs to capture predicate - argument relations,0.7086210250854492
translation,9,6,model,content overlap,align,predicate - argument relations,content overlap align predicate - argument relations,0.6922717690467834
translation,9,6,model,predicate - argument relations,across,texts,predicate - argument relations across texts,0.5956119894981384
translation,9,6,model,model,to explicitly represent,content overlap,model to explicitly represent content overlap,0.7235987186431885
translation,9,7,model,overlap,with respect to,redundancy,overlap with respect to redundancy,0.7105698585510254
translation,9,7,model,redundancy,at,propositional level,redundancy at propositional level,0.5623771548271179
translation,9,7,model,model,has,overlap,model has overlap,0.5683473348617554
translation,9,180,results,lemma baseline performance,is,relatively low,lemma baseline performance is relatively low,0.5933043956756592
translation,9,181,results,pretrained language models,yields,modest yet clear improvement,pretrained language models yields modest yet clear improvement,0.6535423398017883
translation,9,181,results,results,Applying,pretrained language models,results Applying pretrained language models,0.5612839460372925
translation,9,202,results,baseline and fuse-align models,achieve,similar r2,baseline and fuse-align models achieve similar r2,0.5524454116821289
translation,9,202,results,similar r2,of,41 and 40,similar r2 of 41 and 40,0.6370747089385986
translation,9,202,results,results,find that,baseline and fuse-align models,results find that baseline and fuse-align models,0.5908359885215759
translation,10,172,ablation-analysis,caqa and qagen - t5,yield,ablation study,caqa and qagen - t5 yield ablation study,0.6610835790634155
translation,10,172,ablation-analysis,ablation study,quantifying,gain,ablation study quantifying gain,0.8074122071266174
translation,10,172,ablation-analysis,gain,attributed to,contrastive adaptation loss,gain attributed to contrastive adaptation loss,0.6730414032936096
translation,10,172,ablation-analysis,gain,using,contrastive adaptation loss,gain using contrastive adaptation loss,0.6782374978065491
translation,10,172,ablation-analysis,ablation analysis,comparing,caqa and qagen - t5,ablation analysis comparing caqa and qagen - t5,0.7202893495559692
translation,10,200,ablation-analysis,some of the generated synthetic data,cannot perfectly match,characteristics,some of the generated synthetic data cannot perfectly match characteristics,0.7316004633903503
translation,10,200,ablation-analysis,some of the generated synthetic data,still reveal,differences,some of the generated synthetic data still reveal differences,0.7220485806465149
translation,10,200,ablation-analysis,characteristics,of,target domain,characteristics of target domain,0.5830144882202148
translation,10,200,ablation-analysis,domain adaptation,has,some of the generated synthetic data,domain adaptation has some of the generated synthetic data,0.5550534725189209
translation,10,200,ablation-analysis,ablation analysis,Despite,domain adaptation,ablation analysis Despite domain adaptation,0.6542724967002869
translation,10,9,baselines,baselines,has,caqa,baselines has caqa,0.592427670955658
translation,10,170,baselines,one dataset ( hotpotqa ),is,our qagen - t5 variant,one dataset ( hotpotqa ) is our qagen - t5 variant,0.5720067024230957
translation,10,170,baselines,our qagen - t5 variant,without,contrastive adaptation loss,our qagen - t5 variant without contrastive adaptation loss,0.7294137477874756
translation,10,169,experiments,best- performing approach,is,our caqa framework,best- performing approach is our caqa framework,0.5484735369682312
translation,10,169,experiments,our caqa framework,for,three out of four datasets,our caqa framework for three out of four datasets,0.5666907429695129
translation,10,10,model,qa system,on,source data,qa system on source data,0.5443273782730103
translation,10,10,model,qa system,on,generated data,qa system on generated data,0.5758219957351685
translation,10,10,model,generated data,from,target domain,generated data from target domain,0.5508844256401062
translation,10,10,model,contrastive adaptation loss,incorporated in,training objective,contrastive adaptation loss incorporated in training objective,0.584939181804657
translation,10,10,model,model,train,qa system,model train qa system,0.7690511345863342
translation,10,30,model,contrastive domain adaptation,for,question answering ( caqa ),contrastive domain adaptation for question answering ( caqa ),0.6110090017318726
translation,10,31,model,caqa,combines,question generation,caqa combines question generation,0.685206413269043
translation,10,31,model,caqa,combines,contrastive domain adaptation,caqa combines contrastive domain adaptation,0.6640474200248718
translation,10,31,model,contrastive domain adaptation,to learn,domain-invariant features,contrastive domain adaptation to learn domain-invariant features,0.5993055105209351
translation,10,31,model,model,has,caqa,model has caqa,0.6332583427429199
translation,10,33,model,novel contrastive adaptation loss,tailored to,qa,novel contrastive adaptation loss tailored to qa,0.6754041314125061
translation,10,33,model,model,propose,novel contrastive adaptation loss,model propose novel contrastive adaptation loss,0.6505773663520813
translation,10,34,model,contrastive adaptation loss,uses,maximum mean discrepancy ( mmd ),contrastive adaptation loss uses maximum mean discrepancy ( mmd ),0.5577909350395203
translation,10,34,model,maximum mean discrepancy ( mmd ),to measure,discrepancy,maximum mean discrepancy ( mmd ) to measure discrepancy,0.6833222508430481
translation,10,34,model,discrepancy,in,representation,discrepancy in representation,0.5666717886924744
translation,10,34,model,representation,between,source and target features,representation between source and target features,0.6232852339744568
translation,10,34,model,model,has,contrastive adaptation loss,model has contrastive adaptation loss,0.5350902676582336
translation,10,38,results,caqa framework,effective for,limited text corpora,caqa framework effective for limited text corpora,0.6432448625564575
translation,10,38,results,results,has,caqa framework,results has caqa framework,0.5838984847068787
translation,10,41,results,current state - of - the - art baselines,for,domain adaptation,current state - of - the - art baselines for domain adaptation,0.5710899233818054
translation,10,41,results,domain adaptation,by,significant margin,domain adaptation by significant margin,0.5799762606620789
translation,10,41,results,caqa,has,outperforms,caqa has outperforms,0.6242910027503967
translation,10,41,results,outperforms,has,current state - of - the - art baselines,outperforms has current state - of - the - art baselines,0.5611996054649353
translation,10,41,results,results,has,caqa,results has caqa,0.5796493291854858
translation,10,173,results,distinctive performance improvements,due to,our contrastive adaptation loss,distinctive performance improvements due to our contrastive adaptation loss,0.604457676410675
translation,10,173,results,our contrastive adaptation loss,for,three out of four datasets,our contrastive adaptation loss for three out of four datasets,0.5940447449684143
translation,10,173,results,results,find,distinctive performance improvements,results find distinctive performance improvements,0.613598644733429
translation,10,174,results,our caqa framework,is,superior,our caqa framework is superior,0.6334131956100464
translation,10,174,results,question generation baselines,has,our caqa framework,question generation baselines has our caqa framework,0.5775616765022278
translation,10,174,results,results,Compared to,question generation baselines,results Compared to question generation baselines,0.6378305554389954
translation,10,175,results,average improvements,in,em and f1,average improvements in em and f1,0.5485082864761353
translation,10,175,results,average improvements,are,4.06 % and 3.80 %,average improvements are 4.06 % and 3.80 %,0.5573627948760986
translation,10,175,results,em and f1,are,3.78 % and 3.41 %,em and f1 are 3.78 % and 3.41 %,0.5698131322860718
translation,10,175,results,average improvements,are,4.06 % and 3.80 %,average improvements are 4.06 % and 3.80 %,0.5573627948760986
translation,10,175,results,aqgen,has,average improvements,aqgen has average improvements,0.5651795268058777
translation,10,175,results,aqgen,has,average improvements,aqgen has average improvements,0.5651795268058777
translation,10,175,results,compared qagen,has,average improvements,compared qagen has average improvements,0.5684365034103394
translation,10,175,results,results,Compared to,aqgen,results Compared to aqgen,0.6987822651863098
translation,10,176,results,performance,of,caqa,performance of caqa,0.6007217168807983
translation,10,176,results,caqa,close to,supervised training results,caqa close to supervised training results,0.6641399264335632
translation,10,176,results,supervised training results,using,10 k paragraphs,supervised training results using 10 k paragraphs,0.7021638751029968
translation,10,176,results,10 k paragraphs,from,target datasets,10 k paragraphs from target datasets,0.5780348777770996
translation,10,176,results,triviaqa and hotpotqa,has,performance,triviaqa and hotpotqa has performance,0.6017698049545288
translation,10,176,results,results,In the case of,triviaqa and hotpotqa,results In the case of triviaqa and hotpotqa,0.6710280179977417
translation,10,182,results,triviaqa and naturalquestions,For,qagen - t5,triviaqa and naturalquestions For qagen - t5,0.6759551763534546
translation,10,182,results,qagen - t5,see,comparatively large improvement,qagen - t5 see comparatively large improvement,0.6166678071022034
translation,10,182,results,comparatively large improvement,when increasing,size,comparatively large improvement when increasing size,0.743427038192749
translation,10,182,results,size,from,10 k to 20 k,size from 10 k to 20 k,0.6318698525428772
translation,10,182,results,results,For,qagen - t5,results For qagen - t5,0.6186999678611755
translation,10,184,results,caqa,is,superior,caqa is superior,0.6607136130332947
translation,10,184,results,superior,when using,only 10 k context paragraphs,superior when using only 10 k context paragraphs,0.7537765502929688
translation,10,184,results,results,has,caqa,results has caqa,0.5796493291854858
translation,10,194,results,qagen -t5,with,our contrastive adaptation loss,qagen -t5 with our contrastive adaptation loss,0.6587027311325073
translation,10,194,results,our contrastive adaptation loss,yields,caqa,our contrastive adaptation loss yields caqa,0.7311780452728271
translation,10,194,results,results,combining,qagen -t5,results combining qagen -t5,0.6696603894233704
translation,10,195,results,performance,of,almost baselines,performance of almost baselines,0.5977518558502197
translation,10,195,results,almost baselines,can be,improved,almost baselines can be improved,0.7000165581703186
translation,10,195,results,improved,due to,our contrastive adaptation loss,improved due to our contrastive adaptation loss,0.6277833580970764
translation,10,216,results,supervised training,using,10 k hotpotqa and triviaqa,supervised training using 10 k hotpotqa and triviaqa,0.6562491655349731
translation,10,216,results,10 k hotpotqa and triviaqa,yield,"moderate improvements ( 5.02 % , 5.73 % )","10 k hotpotqa and triviaqa yield moderate improvements ( 5.02 % , 5.73 % )",0.6908401846885681
translation,10,216,results,"moderate improvements ( 5.02 % , 5.73 % )",compared to,10.12 % and 42.82 %,"moderate improvements ( 5.02 % , 5.73 % ) compared to 10.12 % and 42.82 %",0.652349591255188
translation,10,216,results,10.12 % and 42.82 %,in,naturalquestions and searchqa,10.12 % and 42.82 % in naturalquestions and searchqa,0.5374810695648193
translation,10,216,results,results,has,supervised training,results has supervised training,0.5646262168884277
translation,11,127,baselines,mlp,has,multilayer perceptron ( mlp ) binary classifier,mlp has multilayer perceptron ( mlp ) binary classifier,0.570936381816864
translation,11,134,baselines,multihop architecture,to learn,token embeddings,multihop architecture to learn token embeddings,0.6019376516342163
translation,11,134,baselines,token embeddings,based on,given question,token embeddings based on given question,0.5956421494483948
translation,11,134,baselines,ga,has,ga,ga has ga,0.6587218642234802
translation,11,134,baselines,baselines,has,ga,baselines has ga,0.6055818200111389
translation,11,137,baselines,bert -c,has,modified version,bert -c has modified version,0.6137804985046387
translation,11,137,baselines,baselines,has,bert -c,baselines has bert -c,0.6240510940551758
translation,11,116,hyperparameters,inputs,converted to,lower case,inputs converted to lower case,0.683519721031189
translation,11,116,hyperparameters,tokenized,by,bert - uncased tokenizer,tokenized by bert - uncased tokenizer,0.6062294840812683
translation,11,116,hyperparameters,hyperparameters,has,inputs,hyperparameters has inputs,0.5048274993896484
translation,11,117,hyperparameters,prediction model,trained using,rmsprop optimizer,prediction model trained using rmsprop optimizer,0.6728874444961548
translation,11,117,hyperparameters,rmsprop optimizer,for,100 epochs,rmsprop optimizer for 100 epochs,0.5353028178215027
translation,11,117,hyperparameters,rmsprop optimizer,with,learning rate,rmsprop optimizer with learning rate,0.5848580598831177
translation,11,117,hyperparameters,learning rate,set to,0.001,learning rate set to 0.001,0.6954665780067444
translation,11,117,hyperparameters,hyperparameters,has,prediction model,hyperparameters has prediction model,0.5413810014724731
translation,11,118,hyperparameters,categorical cross entropy,used as,loss function,categorical cross entropy used as loss function,0.6099413633346558
translation,11,118,hyperparameters,hyperparameters,has,categorical cross entropy,hyperparameters has categorical cross entropy,0.5572830438613892
translation,11,132,hyperparameters,binary cross entropy,used as,loss function,binary cross entropy used as loss function,0.6010120511054993
translation,11,132,hyperparameters,hyperparameters,has,binary cross entropy,hyperparameters has binary cross entropy,0.5435783863067627
translation,11,133,hyperparameters,rmsprop optimizer,for,100 epochs,rmsprop optimizer for 100 epochs,0.5353028178215027
translation,11,133,hyperparameters,rmsprop optimizer,with,learning rate 0.01,rmsprop optimizer with learning rate 0.01,0.6111584901809692
translation,11,133,hyperparameters,100 epochs,with,learning rate 0.01,100 epochs with learning rate 0.01,0.6177680492401123
translation,11,133,hyperparameters,hyperparameters,trained by,rmsprop optimizer,hyperparameters trained by rmsprop optimizer,0.6611394882202148
translation,11,8,model,approach,leverages,pre-trained bert token embeddings,approach leverages pre-trained bert token embeddings,0.6740023493766785
translation,11,8,model,pre-trained bert token embeddings,as,prior knowledge resource,pre-trained bert token embeddings as prior knowledge resource,0.4552774131298065
translation,11,8,model,model,propose,approach,model propose approach,0.6953083872795105
translation,11,119,model,both l1 and l2 regularizers,added at,dense layer,both l1 and l2 regularizers added at dense layer,0.6612220406532288
translation,11,119,model,dense layer,in,placeholder and option learning modules,dense layer in placeholder and option learning modules,0.5177463293075562
translation,11,119,model,over-fitting,has,both l1 and l2 regularizers,over-fitting has both l1 and l2 regularizers,0.5337669253349304
translation,11,119,model,model,To avoid,over-fitting,model To avoid over-fitting,0.6486587524414062
translation,11,128,model,two hidden layers,size,512 and 128,two hidden layers size 512 and 128,0.7152717113494873
translation,11,128,model,two hidden layers,with,rectified linear unit ( relu ) activation function,two hidden layers with rectified linear unit ( relu ) activation function,0.6048306226730347
translation,11,128,model,model,consists of,two hidden layers,model consists of two hidden layers,0.6598857641220093
translation,11,124,results,proposed approach,using,dot product,proposed approach using dot product,0.7323256134986877
translation,11,124,results,dot product,performed,best,dot product performed best,0.2688885033130646
translation,11,124,results,best,compared to,others,best compared to others,0.6838500499725342
translation,11,151,results,proposed approach,performed,better,proposed approach performed better,0.2641126215457916
translation,11,151,results,better,in,subtask 1,better in subtask 1,0.5377892255783081
translation,11,151,results,better,compared to,subtask 2,better compared to subtask 2,0.6899497509002686
translation,11,151,results,subtask 1,compared to,subtask 2,subtask 1 compared to subtask 2,0.6958622932434082
translation,11,156,results,proposed approach,performed,better,proposed approach performed better,0.2641126215457916
translation,11,156,results,better,in,all subtasks,better in all subtasks,0.5059863328933716
translation,11,156,results,baselines,has,proposed approach,baselines has proposed approach,0.6193264126777649
translation,11,156,results,results,compared with,baselines,results compared with baselines,0.6914018392562866
translation,12,102,baselines,unsupervised neural machine translation,to train,qg model,unsupervised neural machine translation to train qg model,0.6724783778190613
translation,12,102,baselines,dependency trees,to generate,questions,dependency trees to generate questions,0.655238151550293
translation,12,102,baselines,cited documents,as,passages,cited documents as passages,0.548892617225647
translation,12,9,experimental-setup,freely available news summary data,transforming,declarative summary sentences,freely available news summary data transforming declarative summary sentences,0.6625398993492126
translation,12,9,experimental-setup,declarative summary sentences,into,appropriate questions,declarative summary sentences into appropriate questions,0.5687862038612366
translation,12,9,experimental-setup,appropriate questions,using,heuristics,appropriate questions using heuristics,0.7400257587432861
translation,12,9,experimental-setup,heuristics,informed by,dependency parsing,heuristics informed by dependency parsing,0.6851568818092346
translation,12,9,experimental-setup,heuristics,informed by,named entity recognition,heuristics informed by named entity recognition,0.6687888503074646
translation,12,9,experimental-setup,heuristics,informed by,semantic role labeling,heuristics informed by semantic role labeling,0.6842511296272278
translation,12,9,experimental-setup,experimental setup,use of,freely available news summary data,experimental setup use of freely available news summary data,0.6098079085350037
translation,12,32,experiments,qg model,to generate,synthetic qa data,qg model to generate synthetic qa data,0.6947221159934998
translation,12,32,experiments,synthetic qa data,to train,qa model,synthetic qa data to train qa model,0.6843710541725159
translation,12,32,experiments,qa model,in,unsupervised setting,qa model in unsupervised setting,0.5138639211654663
translation,12,87,hyperparameters,qg model,employ,bart - base,qg model employ bart - base,0.5511183738708496
translation,12,87,hyperparameters,qg model,is,bart - base,qg model is bart - base,0.5878439545631409
translation,12,87,hyperparameters,hyperparameters,has,qg model,hyperparameters has qg model,0.5291590690612793
translation,12,88,hyperparameters,qg model,on,qg data,qg model on qg data,0.5884768962860107
translation,12,88,hyperparameters,qg model,for,3 epochs,qg model for 3 epochs,0.6555938720703125
translation,12,88,hyperparameters,qg model,with,learning rate,qg model with learning rate,0.5997630953788757
translation,12,88,hyperparameters,qg model,using,adamw optimizer,qg model using adamw optimizer,0.6598852276802063
translation,12,88,hyperparameters,qg data,for,3 epochs,qg data for 3 epochs,0.6012498140335083
translation,12,88,hyperparameters,3 epochs,with,learning rate,3 epochs with learning rate,0.6580738425254822
translation,12,88,hyperparameters,learning rate,of,3 ? 10 ?5,learning rate of 3 ? 10 ?5,0.652306854724884
translation,12,88,hyperparameters,hyperparameters,train,qg model,hyperparameters train qg model,0.6870105266571045
translation,12,98,hyperparameters,document length and stride length,are,364 and 128,document length and stride length are 364 and 128,0.5905517339706421
translation,12,98,hyperparameters,learning rate,set to,1 ? 10 ?5,learning rate set to 1 ? 10 ?5,0.7357043623924255
translation,12,98,hyperparameters,unsupervised qa,are,exact match ( em ),unsupervised qa are exact match ( em ),0.5810995697975159
translation,12,98,hyperparameters,unsupervised qa,are,f - 1 score,unsupervised qa are f - 1 score,0.5865723490715027
translation,12,98,hyperparameters,hyperparameters,has,document length and stride length,hyperparameters has document length and stride length,0.5130524635314941
translation,12,98,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,12,8,model,unsupervised qg method,uses,questions,unsupervised qg method uses questions,0.6461511850357056
translation,12,8,model,questions,generated,heuristically,questions generated heuristically,0.720017671585083
translation,12,8,model,heuristically,from,summaries,heuristically from summaries,0.6641649007797241
translation,12,8,model,summaries,source of,training data,summaries source of training data,0.6436200141906738
translation,12,8,model,training data,for,qg system,training data for qg system,0.6470000743865967
translation,12,8,model,model,propose,unsupervised qg method,model propose unsupervised qg method,0.6688835024833679
translation,12,22,model,new unsupervised approach,frames,qg,new unsupervised approach frames qg,0.7442396283149719
translation,12,22,model,qg,as,summarizationquestioning process,qg as summarizationquestioning process,0.5424497127532959
translation,12,22,model,model,propose,new unsupervised approach,model propose new unsupervised approach,0.7225928902626038
translation,12,23,model,freely available summary data,apply,dependency parsing,freely available summary data apply dependency parsing,0.6294867396354675
translation,12,23,model,freely available summary data,apply,named entity recognition,freely available summary data apply named entity recognition,0.5740895867347717
translation,12,23,model,freely available summary data,apply,semantic role labeling,freely available summary data apply semantic role labeling,0.5996688604354858
translation,12,23,model,semantic role labeling,to,summaries,semantic role labeling to summaries,0.5450180768966675
translation,12,23,model,questions,based on,parsed summaries,questions based on parsed summaries,0.6509854793548584
translation,12,23,model,dependency parsing,has,named entity recognition,dependency parsing has named entity recognition,0.4794151186943054
translation,12,23,model,model,employing,freely available summary data,model employing freely available summary data,0.6335984468460083
translation,12,103,results,models,fine-tuned on,correspond - ing training sets,models fine-tuned on correspond - ing training sets,0.6845040917396545
translation,12,105,results,performance,of,one supervised model,performance of one supervised model,0.5884253978729248
translation,12,105,results,our proposed method,has,outperforms,our proposed method has outperforms,0.6310744285583496
translation,12,105,results,outperforms,has,all unsupervised baselines,outperforms has all unsupervised baselines,0.5897190570831299
translation,12,105,results,exceeds,has,performance,exceeds has performance,0.6372079253196716
translation,12,105,results,results,for,natural questions and triviaqa,results for natural questions and triviaqa,0.5823025703430176
translation,12,107,results,outperforms,obtaining,relative improvements,outperforms obtaining relative improvements,0.6708425283432007
translation,12,107,results,previous state - of - the - art unsupervised methods,by,substantial margin,previous state - of - the - art unsupervised methods by substantial margin,0.5643678307533264
translation,12,107,results,previous state - of - the - art unsupervised methods,obtaining,relative improvements,previous state - of - the - art unsupervised methods obtaining relative improvements,0.5792034268379211
translation,12,107,results,relative improvements,over,best unsupervised baseline model,relative improvements over best unsupervised baseline model,0.6431013941764832
translation,12,107,results,best unsupervised baseline model,of,47 %,best unsupervised baseline model of 47 %,0.5386427640914917
translation,12,107,results,best unsupervised baseline model,of,10 % f-1,best unsupervised baseline model of 10 % f-1,0.556998074054718
translation,12,107,results,best unsupervised baseline model,with respect to,10 % f-1,best unsupervised baseline model with respect to 10 % f-1,0.5990042686462402
translation,12,107,results,47 %,with respect to,em,47 % with respect to em,0.706517219543457
translation,12,107,results,10 % f-1,on,natural questions,10 % f-1 on natural questions,0.5203707218170166
translation,12,107,results,34 % em and 12 % f-1,on,triviaqa,34 % em and 12 % f-1 on triviaqa,0.5741708874702454
translation,12,107,results,our method,has,outperforms,our method has outperforms,0.6322360634803772
translation,12,107,results,outperforms,has,previous state - of - the - art unsupervised methods,outperforms has previous state - of - the - art unsupervised methods,0.5468583703041077
translation,12,107,results,results,has,our method,results has our method,0.5589964985847473
translation,12,108,results,our method,achieves,best performance,our method achieves best performance,0.6578624248504639
translation,12,108,results,best performance,both in terms of,em and f - 1 ),best performance both in terms of em and f - 1 ),0.7275009751319885
translation,12,108,results,best performance,both in terms of,three unsupervised models,best performance both in terms of three unsupervised models,0.6583202481269836
translation,12,108,results,best performance,out of,three unsupervised models,best performance out of three unsupervised models,0.6097716689109802
translation,12,108,results,em and f - 1 ),out of,three unsupervised models,em and f - 1 ) out of three unsupervised models,0.5959208011627197
translation,12,108,results,results,has,our method,results has our method,0.5589964985847473
translation,12,113,results,additionally outperforms,achieving,f1 improvements,additionally outperforms achieving f1 improvements,0.6490057110786438
translation,12,113,results,unsupervised baseline models,on,out-of- domain datasets,unsupervised baseline models on out-of- domain datasets,0.4925383925437927
translation,12,113,results,unsupervised baseline models,achieving,f1 improvements,unsupervised baseline models achieving f1 improvements,0.6177172064781189
translation,12,113,results,f1 improvements,over,previous state - of - the - art methods,f1 improvements over previous state - of - the - art methods,0.6083161234855652
translation,12,113,results,proposed method,has,additionally outperforms,proposed method has additionally outperforms,0.6220488548278809
translation,12,113,results,additionally outperforms,has,unsupervised baseline models,additionally outperforms has unsupervised baseline models,0.6025682687759399
translation,12,113,results,results,show,proposed method,results show proposed method,0.6722329258918762
translation,12,135,results,articles,as,passages,articles as passages,0.577997624874115
translation,12,135,results,lexical overlap,with,summarygenerated questions,lexical overlap with summarygenerated questions,0.6498166918754578
translation,12,135,results,summarygenerated questions,has,greatly improves,summarygenerated questions has greatly improves,0.6042221784591675
translation,12,135,results,greatly improves,has,qa performance,greatly improves has qa performance,0.5924198627471924
translation,12,135,results,results,using,articles,results using articles,0.5699619054794312
translation,12,136,results,outperforms,by,roughly 20 em points,outperforms by roughly 20 em points,0.6361545920372009
translation,12,136,results,outperforms,by,16 f - 1 points,outperforms by 16 f - 1 points,0.6444655060768127
translation,12,136,results,naive - qg,by,roughly 20 em points,naive - qg by roughly 20 em points,0.5998828411102295
translation,12,136,results,naive - qg,by,16 f - 1 points,naive - qg by 16 f - 1 points,0.6061923503875732
translation,12,136,results,summary - qg,has,outperforms,summary - qg has outperforms,0.613674521446228
translation,12,136,results,outperforms,has,naive - qg,outperforms has naive - qg,0.6106508374214172
translation,12,136,results,results,has,summary - qg,results has summary - qg,0.5836509466171265
translation,12,137,results,other heuristics,show,continuously improve,other heuristics show continuously improve,0.5979468822479248
translation,12,137,results,performance,especially,wh-movement,performance especially wh-movement,0.6447816491127014
translation,12,137,results,performance,especially,decomp - verb,performance especially decomp - verb,0.6890290975570679
translation,12,137,results,decomp - verb,make,questions,decomp - verb make questions,0.6881497502326965
translation,12,137,results,questions,in,qg data,questions in qg data,0.5212505459785461
translation,12,137,results,continuously improve,has,performance,continuously improve has performance,0.5828331708908081
translation,12,140,results,our synthetic data,allows,qa model,our synthetic data allows qa model,0.6980165243148804
translation,12,140,results,qa model,to achieve,competitive performance,qa model to achieve competitive performance,0.6971367597579956
translation,12,140,results,competitive performance,with,fewer than 20k examples,competitive performance with fewer than 20k examples,0.6397050619125366
translation,12,140,results,results,show,our synthetic data,results show our synthetic data,0.6290448904037476
translation,13,7,model,multi-modal joint learning approach,with,auxiliary cross-modal relation detection,multi-modal joint learning approach with auxiliary cross-modal relation detection,0.5984765291213989
translation,13,7,model,auxiliary cross-modal relation detection,for,multi-modal aspect-level sentiment analysis ( malsa ),auxiliary cross-modal relation detection for multi-modal aspect-level sentiment analysis ( malsa ),0.5983315706253052
translation,13,7,model,model,jointly perform,multi-modal ate ( mate ),model jointly perform multi-modal ate ( mate ),0.7019463181495667
translation,13,7,model,model,propose,multi-modal joint learning approach,model propose multi-modal joint learning approach,0.6546975374221802
translation,13,8,model,model,build,auxiliary text-image relation detection module,model build auxiliary text-image relation detection module,0.7121216654777527
translation,13,9,model,hierarchical framework,to bridge,multi-modal connection,hierarchical framework to bridge multi-modal connection,0.5640121102333069
translation,13,9,model,multi-modal connection,between,mate and masc,multi-modal connection between mate and masc,0.6692202687263489
translation,13,9,model,model,adopt,hierarchical framework,model adopt hierarchical framework,0.7038043141365051
translation,13,31,model,multi-modal joint learning approach,with,auxiliary cross-modal relation detection,multi-modal joint learning approach with auxiliary cross-modal relation detection,0.5984765291213989
translation,13,31,model,auxiliary cross-modal relation detection,namely,jml,auxiliary cross-modal relation detection namely jml,0.6903570294380188
translation,13,31,model,model,propose,multi-modal joint learning approach,model propose multi-modal joint learning approach,0.6546975374221802
translation,13,32,model,module,of,auxiliary cross-modal relation detection,module of auxiliary cross-modal relation detection,0.5417866706848145
translation,13,32,model,model,design,module,model design module,0.6778693199157715
translation,13,33,model,joint hierarchical framework,to separately attend to,effective visual information,joint hierarchical framework to separately attend to effective visual information,0.6688142418861389
translation,13,33,model,effective visual information,for,each sub-task,effective visual information for each sub-task,0.585896372795105
translation,13,33,model,each sub-task,instead of,collapsed tagging framework,each sub-task instead of collapsed tagging framework,0.6446578502655029
translation,13,33,model,model,leverage,joint hierarchical framework,model leverage joint hierarchical framework,0.711628794670105
translation,14,177,baselines,flat transformer,for,focus prediction and question generation,flat transformer for focus prediction and question generation,0.6118001341819763
translation,14,117,experiments,joint focus prediction and question generation,has,jointgen,joint focus prediction and question generation has jointgen,0.5861636400222778
translation,14,5,model,new question type ontology,differentiates,nuanced nature of questions,new question type ontology differentiates nuanced nature of questions,0.7859658598899841
translation,14,5,model,better,than,widely used question words,better than widely used question words,0.5774107575416565
translation,14,5,model,nuanced nature of questions,has,better,nuanced nature of questions has better,0.5743634104728699
translation,14,5,model,model,define,new question type ontology,model define new question type ontology,0.621850311756134
translation,14,33,model,question type ontology,to capture,deeper levels,question type ontology to capture deeper levels,0.7220571637153625
translation,14,33,model,model,introduce,question type ontology,model introduce question type ontology,0.5760493874549866
translation,14,35,model,type-aware framework,to jointly predict,question focuses ( what to ask about,type-aware framework to jointly predict question focuses ( what to ask about,0.775627851486206
translation,14,35,model,type-aware framework,generate,questions,type-aware framework generate questions,0.7062754034996033
translation,14,35,model,questions,has,how to ask it ),questions has how to ask it ),0.6152651906013489
translation,14,35,model,model,design,type-aware framework,model design type-aware framework,0.5918641686439514
translation,14,36,model,shared representations,to jointly conduct,question focus prediction,shared representations to jointly conduct question focus prediction,0.6438172459602356
translation,14,36,model,shared representations,to jointly conduct,question generation,shared representations to jointly conduct question generation,0.6453532576560974
translation,14,36,model,question generation,while learning,taskspecific knowledge,question generation while learning taskspecific knowledge,0.7178687453269958
translation,14,36,model,large pre-trained bart,has,"lewis et al. , 2020 )","large pre-trained bart has lewis et al. , 2020 )",0.5575879812240601
translation,14,36,model,model,uses,shared representations,model uses shared representations,0.630734384059906
translation,14,37,model,semantic graph,leverages,semantic roles and dependency relations,semantic graph leverages semantic roles and dependency relations,0.6496430039405823
translation,14,37,model,semantic roles and dependency relations,facilitating,long text comprehension,semantic roles and dependency relations facilitating long text comprehension,0.6673136353492737
translation,14,37,model,long text comprehension,to pinpoint,salient concepts,long text comprehension to pinpoint salient concepts,0.6453893184661865
translation,14,37,model,model,augmented by,semantic graph,model augmented by semantic graph,0.6710484027862549
translation,14,38,model,two model variants,use,templates,two model variants use templates,0.6406656503677368
translation,14,38,model,templates,to improve,controllability and generation diversity,templates to improve controllability and generation diversity,0.6972972750663757
translation,14,38,model,one,using,pre-identified exemplars,one using pre-identified exemplars,0.6846356391906738
translation,14,38,model,other,employing,generated templates,other employing generated templates,0.6633940935134888
translation,14,38,model,generated templates,to guide,question writing,generated templates to guide question writing,0.6922560930252075
translation,14,38,model,model,investigate,two model variants,model investigate two model variants,0.6299794316291809
translation,14,38,model,model,employing,generated templates,model employing generated templates,0.6919822692871094
translation,14,116,model,two model variants,consider,automatically extracted template exemplars,two model variants consider automatically extracted template exemplars,0.6124079823493958
translation,14,116,model,two model variants,consider,generated templates,two model variants consider generated templates,0.6946383118629456
translation,14,116,model,generated templates,to achieve,controllability,generated templates to achieve controllability,0.6533917784690857
translation,14,116,model,controllability,enabling,generation of diverse questions,controllability enabling generation of diverse questions,0.7383697032928467
translation,14,116,model,? 4.2 ),enabling,generation of diverse questions,? 4.2 ) enabling generation of diverse questions,0.6636869311332703
translation,14,116,model,controllability,has,? 4.2 ),controllability has ? 4.2 ),0.5984339714050293
translation,14,116,model,model,propose,two model variants,model propose two model variants,0.6461420655250549
translation,14,165,model,explgen,to generate,target template,explgen to generate target template,0.725156843662262
translation,14,165,model,model,reuse,explgen,model reuse explgen,0.7199293971061707
translation,14,42,results,automatic metrics,show that,our type-aware question generation model,automatic metrics show that our type-aware question generation model,0.47488370537757874
translation,14,42,results,our type-aware question generation model,has,outperforms,our type-aware question generation model has outperforms,0.5898115634918213
translation,14,42,results,outperforms,has,competitive comparisons,outperforms has competitive comparisons,0.5875332951545715
translation,14,42,results,results,has,automatic metrics,results has automatic metrics,0.5785090327262878
translation,14,44,results,templates,promotes,question diversity,templates promotes question diversity,0.6980503797531128
translation,14,44,results,question diversity,evaluated by,automatic evaluation,question diversity evaluated by automatic evaluation,0.7555885314941406
translation,14,44,results,question diversity,evaluated by,human assessment,question diversity evaluated by human assessment,0.7457500100135803
translation,14,44,results,results,Adding,templates,results Adding templates,0.5790481567382812
translation,14,180,results,all comparisons,on,both datasets,all comparisons on both datasets,0.500993013381958
translation,14,180,results,both datasets,over,automatic evaluation metrics,both datasets over automatic evaluation metrics,0.6156218647956848
translation,14,180,results,automatic evaluation metrics,except for,meteor on reddit,automatic evaluation metrics except for meteor on reddit,0.569297730922699
translation,14,180,results,jointgen,has,outperforms,jointgen has outperforms,0.6594246625900269
translation,14,180,results,outperforms,has,all comparisons,outperforms has all comparisons,0.5736692547798157
translation,14,180,results,results,has,jointgen,results has jointgen,0.48133862018585205
translation,14,183,results,explgen and tplgen,achieve,stronger controllability,explgen and tplgen achieve stronger controllability,0.6545724272727966
translation,14,183,results,stronger controllability,by respecting,given question types more,stronger controllability by respecting given question types more,0.6849308609962463
translation,14,183,results,higher diversity,than,comparisons,higher diversity than comparisons,0.5983494520187378
translation,14,183,results,comparisons,except for,bart,comparisons except for bart,0.7040758728981018
translation,14,183,results,bart,with,nucleus sampling,bart with nucleus sampling,0.7140200138092041
translation,14,183,results,results,has,explgen and tplgen,results has explgen and tplgen,0.5548575520515442
translation,14,186,results,results,observe,huge performance gap,results observe huge performance gap,0.6001797914505005
translation,14,187,results,question types,helps BART generate,more relevant questions,question types helps BART generate more relevant questions,0.6049123406410217
translation,14,187,results,more relevant questions,than using,question words,more relevant questions than using question words,0.6186545491218567
translation,14,187,results,results,adding,question types,results adding question types,0.574670135974884
translation,14,188,results,our template - based generators,produce,comparable scores,our template - based generators produce comparable scores,0.6301305294036865
translation,14,188,results,ex - plgen and tplgen,trained to comply with,given templates,ex - plgen and tplgen trained to comply with given templates,0.6842121481895447
translation,14,188,results,given templates,produce,comparable scores,given templates produce comparable scores,0.6209456920623779
translation,14,188,results,our template - based generators,has,ex - plgen and tplgen,our template - based generators has ex - plgen and tplgen,0.6084046363830566
translation,14,188,results,results,has,our template - based generators,results has our template - based generators,0.486157089471817
translation,14,207,results,human judges,rate,questions,human judges rate questions,0.7375251650810242
translation,14,207,results,questions,generated by,our explgen and tplgen,questions generated by our explgen and tplgen,0.7130138874053955
translation,14,207,results,greater diversities,over,all aspects,greater diversities over all aspects,0.7041283249855042
translation,14,207,results,results,find that,human judges,results find that human judges,0.611027717590332
translation,14,208,results,two model variants,questions by,tplgen,two model variants questions by tplgen,0.7491990327835083
translation,14,208,results,results,Among,two model variants,results Among two model variants,0.5882493257522583
translation,14,209,results,tplgen,uses,automatically generated templates,tplgen uses automatically generated templates,0.6778784990310669
translation,14,209,results,automatically generated templates,to produce,more focused questions,automatically generated templates to produce more focused questions,0.6241411566734314
translation,14,209,results,more focused questions,with,different answers,more focused questions with different answers,0.5939140319824219
translation,14,209,results,more focused questions,compared to,explgen,more focused questions compared to explgen,0.7174156904220581
translation,14,209,results,explgen,employs,exemplars,explgen employs exemplars,0.5678985714912415
translation,14,223,results,jointgen model,produces,questions,jointgen model produces questions,0.6623102426528931
translation,14,223,results,questions,with,better answerability,questions with better answerability,0.6041586995124817
translation,14,223,results,questions,that cover,broader content,questions that cover broader content,0.7317010760307312
translation,14,223,results,broader content,in,answers,broader content in answers,0.560474157333374
translation,14,223,results,results,has,jointgen model,results has jointgen model,0.5064023733139038
translation,15,166,ablation-analysis,deployment,of,supervised contrastive learning objective,deployment of supervised contrastive learning objective,0.5226244330406189
translation,15,166,ablation-analysis,supervised contrastive learning objective,enhances,noise immunity,supervised contrastive learning objective enhances noise immunity,0.6298536062240601
translation,15,166,ablation-analysis,noise immunity,of,pre-training process,noise immunity of pre-training process,0.5706832408905029
translation,15,166,ablation-analysis,more effective,in learning,implicit sentiment,more effective in learning implicit sentiment,0.6686474680900574
translation,15,166,ablation-analysis,ablation analysis,has,deployment,ablation analysis has deployment,0.5221257209777832
translation,15,169,ablation-analysis,supervised contrastive learning loss ( - scl ),leads to,2.38 % performance drop,supervised contrastive learning loss ( - scl ) leads to 2.38 % performance drop,0.5976243019104004
translation,15,169,ablation-analysis,2.38 % performance drop,on,restaurant,2.38 % performance drop on restaurant,0.5399888157844543
translation,15,169,ablation-analysis,ablation analysis,removing,supervised contrastive learning loss ( - scl ),ablation analysis removing supervised contrastive learning loss ( - scl ),0.7084594368934631
translation,15,170,ablation-analysis,supervised contrastive learning,plays,primary role,supervised contrastive learning plays primary role,0.7317404747009277
translation,15,170,ablation-analysis,primary role,in,scapt,primary role in scapt,0.5582684278488159
translation,15,170,ablation-analysis,ablation analysis,verifies,supervised contrastive learning,ablation analysis verifies supervised contrastive learning,0.6188238263130188
translation,15,171,ablation-analysis,masked aspect prediction and review reconstruction objectives,brings about,performance drop,masked aspect prediction and review reconstruction objectives brings about performance drop,0.674258291721344
translation,15,171,ablation-analysis,ablation analysis,removing of,masked aspect prediction and review reconstruction objectives,ablation analysis removing of masked aspect prediction and review reconstruction objectives,0.6514630317687988
translation,15,126,baselines,knowledge-enhanced methods,has,"transcap ( chen and qian , 2019 )","knowledge-enhanced methods has transcap ( chen and qian , 2019 )",0.5757293701171875
translation,15,130,baselines,bertasp,Directly apply,aspect-aware finetuning,bertasp Directly apply aspect-aware finetuning,0.6333712339401245
translation,15,130,baselines,aspect-aware finetuning,on,bert - base,aspect-aware finetuning on bert - base,0.5649110078811646
translation,15,130,baselines,baselines,has,bertasp,baselines has bertasp,0.621363639831543
translation,15,142,experiments,transencasp+scapt,performs,better,transencasp+scapt performs better,0.7267311215400696
translation,15,142,experiments,better,than,most baselines,better than most baselines,0.5645020008087158
translation,15,142,experiments,most baselines,without,pre-trained knowledge,most baselines without pre-trained knowledge,0.6873877644538879
translation,15,147,experiments,scapt,good at learning,implicit sentiment,scapt good at learning implicit sentiment,0.7854706645011902
translation,15,120,hyperparameters,300 dimensional randomly initialized transformer encoder,with,6 layers,300 dimensional randomly initialized transformer encoder with 6 layers,0.5925702452659607
translation,15,120,hyperparameters,300 dimensional randomly initialized transformer encoder,with,6 heads,300 dimensional randomly initialized transformer encoder with 6 heads,0.6720916032791138
translation,15,120,hyperparameters,300 dimensional randomly initialized transformer encoder,with,bertbase - uncased,300 dimensional randomly initialized transformer encoder with bertbase - uncased,0.6486042737960815
translation,15,120,hyperparameters,hyperparameters,use,300 dimensional randomly initialized transformer encoder,hyperparameters use 300 dimensional randomly initialized transformer encoder,0.5940749049186707
translation,15,121,hyperparameters,pre-training,for,transformer encoder and bert,pre-training for transformer encoder and bert,0.6063214540481567
translation,15,121,hyperparameters,transformer encoder and bert,takes,80 and 8 epochs,transformer encoder and bert takes 80 and 8 epochs,0.6858963966369629
translation,15,121,hyperparameters,hyperparameters,has,pre-training,hyperparameters has pre-training,0.5166658163070679
translation,15,122,hyperparameters,"adam ( kingma and ba , 2015 )",with,warm - up,"adam ( kingma and ba , 2015 ) with warm - up",0.657189130783081
translation,15,122,hyperparameters,warm - up,to optimize,our models,warm - up to optimize our models,0.7518042922019958
translation,15,122,hyperparameters,our models,with,learning rate,our models with learning rate,0.6224355101585388
translation,15,122,hyperparameters,our models,with,5e? 5,our models with 5e? 5,0.7151073217391968
translation,15,122,hyperparameters,1e? 3,for,transformer encoder,1e? 3 for transformer encoder,0.6156834363937378
translation,15,122,hyperparameters,5e? 5,for,bert,5e? 5 for bert,0.6958458423614502
translation,15,122,hyperparameters,learning rate,has,1e? 3,learning rate has 1e? 3,0.5725879073143005
translation,15,122,hyperparameters,hyperparameters,adopt,"adam ( kingma and ba , 2015 )","hyperparameters adopt adam ( kingma and ba , 2015 )",0.6174468994140625
translation,15,123,hyperparameters,fine-tuned,by,aspect- aware fine-tuning,fine-tuned by aspect- aware fine-tuning,0.5026795268058777
translation,15,123,hyperparameters,aspect- aware fine-tuning,with,5e ? 5 learning rate,aspect- aware fine-tuning with 5e ? 5 learning rate,0.6483132243156433
translation,15,123,hyperparameters,hyperparameters,has,pre-trained models,hyperparameters has pre-trained models,0.5199089646339417
translation,15,7,model,supervised contrastive pre-training,on,large-scale sentimentannotated corpora,supervised contrastive pre-training on large-scale sentimentannotated corpora,0.49789318442344666
translation,15,7,model,large-scale sentimentannotated corpora,retrieved from,in - domain language resources,large-scale sentimentannotated corpora retrieved from in - domain language resources,0.5503294467926025
translation,15,7,model,model,adopt,supervised contrastive pre-training,model adopt supervised contrastive pre-training,0.6712905764579773
translation,15,47,model,aspect- aware finetuning,to enhance,ability,aspect- aware finetuning to enhance ability,0.6761718392372131
translation,15,47,model,ability,of,models,ability of models,0.60906982421875
translation,15,47,model,models,on,aspect-based sentiment identification,models on aspect-based sentiment identification,0.5239691138267517
translation,15,47,model,fine-tuning,has,aspect- aware finetuning,fine-tuning has aspect- aware finetuning,0.5304989218711853
translation,15,47,model,model,In,fine-tuning,model In fine-tuning,0.5134802460670471
translation,15,118,model,scapt,to,transformer encoder and bert,scapt to transformer encoder and bert,0.581540584564209
translation,15,118,model,fine-tuned,by,aspect- aware fine-tuning,fine-tuned by aspect- aware fine-tuning,0.5026795268058777
translation,15,118,model,model,apply,scapt,model apply scapt,0.707892656326294
translation,15,150,model,fine-tuning,serves as,complement,fine-tuning serves as complement,0.5968608856201172
translation,15,150,model,complement,to,scapt,complement to scapt,0.6791366338729858
translation,15,127,results,effect,of,scapt,effect of scapt,0.6632857322692871
translation,15,127,results,effect,of,aspectaware fine-tuning,effect of aspectaware fine-tuning,0.5997379422187805
translation,15,127,results,methods,on,restaurant and laptop,methods on restaurant and laptop,0.5144035816192627
translation,15,136,results,our model,capable to identify,implicit sentiment,our model capable to identify implicit sentiment,0.7125455737113953
translation,15,136,results,our model,attributes its effectiveness,supervised contrastive learning,our model attributes its effectiveness supervised contrastive learning,0.7150826454162598
translation,15,136,results,supervised contrastive learning,in,scapt,supervised contrastive learning in scapt,0.5956401228904724
translation,15,136,results,results,reveal,our model,results reveal our model,0.6899428963661194
translation,15,140,results,our model,achieves,sota performance,our model achieves sota performance,0.7261884808540344
translation,15,140,results,results,has,our model,results has our model,0.5871725678443909
translation,15,141,results,current sota model,by,1.97 %,current sota model by 1.97 %,0.5486602187156677
translation,15,141,results,current sota model,by,3.80 %,current sota model by 3.80 %,0.5549768209457397
translation,15,141,results,current sota model,on,restaurant / laptop,current sota model on restaurant / laptop,0.5028263330459595
translation,15,141,results,1.97 %,/,3.80 %,1.97 % / 3.80 %,0.6226115822792053
translation,15,141,results,3.80 %,on,restaurant / laptop,3.80 % on restaurant / laptop,0.49145713448524475
translation,15,141,results,bertasp+scapt,has,outperforms,bertasp+scapt has outperforms,0.6534441709518433
translation,15,141,results,outperforms,has,current sota model,outperforms has current sota model,0.6067540645599365
translation,15,141,results,results,has,bertasp+scapt,results has bertasp+scapt,0.5277132391929626
translation,15,143,results,bertasp +scapt,achieves,best performance,bertasp +scapt achieves best performance,0.7108955383300781
translation,15,143,results,best performance,on,ese / ise slices,best performance on ese / ise slices,0.5756258368492126
translation,15,143,results,ese / ise slices,of,two datasets,ese / ise slices of two datasets,0.5842375159263611
translation,15,143,results,results,has,bertasp +scapt,results has bertasp +scapt,0.5277132391929626
translation,15,144,results,improve significantly,on,absa tasks,improve significantly on absa tasks,0.5247881412506104
translation,15,144,results,scapt,has,models,scapt has models,0.6351556181907654
translation,15,144,results,models,has,improve significantly,models has improve significantly,0.5991112589836121
translation,15,144,results,results,pre-trained with,scapt,results pre-trained with scapt,0.6601470708847046
translation,15,145,results,bertasp,directly fine-tuned on,absa datasets,bertasp directly fine-tuned on absa datasets,0.7551881074905396
translation,15,145,results,bertasp +scapt,achieves,3.31 %,bertasp +scapt achieves 3.31 %,0.6630454063415527
translation,15,145,results,/4.23 % performance gain,on,restaurant / laptop,/4.23 % performance gain on restaurant / laptop,0.49835634231567383
translation,15,145,results,bertasp,has,bertasp +scapt,bertasp has bertasp +scapt,0.6582992672920227
translation,15,145,results,3.31 %,has,/4.23 % performance gain,3.31 % has /4.23 % performance gain,0.5715115666389465
translation,15,145,results,results,Compared with,bertasp,results Compared with bertasp,0.7134621143341064
translation,15,146,results,transencasp+scapt,is,6.29%/11.34 %,transencasp+scapt is 6.29%/11.34 %,0.5893526673316956
translation,15,146,results,6.29%/11.34 %,better that,transencasp,6.29%/11.34 % better that transencasp,0.6718003153800964
translation,15,146,results,results,has,transencasp+scapt,results has transencasp+scapt,0.5526043772697449
translation,15,149,results,performance,on,ese,performance on ese,0.6758587956428528
translation,15,149,results,bertasp +scapt,appears to be,much better,bertasp +scapt appears to be much better,0.6812191605567932
translation,15,149,results,much better,on,ise,much better on ise,0.6200690269470215
translation,15,149,results,performance,has,bertasp +scapt,performance has bertasp +scapt,0.5756464600563049
translation,15,149,results,ese,has,bertasp +scapt,ese has bertasp +scapt,0.6639666557312012
translation,15,151,results,aspectaware fine-tuning,perform,better,aspectaware fine-tuning perform better,0.6028879880905151
translation,15,151,results,better,on,ese slices,better on ese slices,0.5882712602615356
translation,15,151,results,ese slices,of,datasets,ese slices of datasets,0.6087610125541687
translation,15,152,results,bertasp,worse on,ise,bertasp worse on ise,0.7420403957366943
translation,15,152,results,bertasp,better on,ese,bertasp better on ese,0.6878868937492371
translation,15,152,results,ese,compared with,bert - spc,ese compared with bert - spc,0.7064796686172485
translation,15,152,results,results,has,bertasp,results has bertasp,0.6127899289131165
translation,15,158,results,transencasp + scapt,achieves,state - of- the - art,transencasp + scapt achieves state - of- the - art,0.65644770860672
translation,15,158,results,outperforms,that lack,external sentiment knowledge,outperforms that lack external sentiment knowledge,0.6682400703430176
translation,15,158,results,baselines,that lack,external sentiment knowledge,baselines that lack external sentiment knowledge,0.6462886333465576
translation,15,158,results,bertasp +scapt,achieves,state - of- the - art,bertasp +scapt achieves state - of- the - art,0.6594759821891785
translation,15,158,results,state - of- the - art,in,multi-aspect scenario,state - of- the - art in multi-aspect scenario,0.5262468457221985
translation,15,158,results,transencasp + scapt,has,outperforms,transencasp + scapt has outperforms,0.6450316905975342
translation,15,158,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
translation,15,158,results,results,shows,transencasp + scapt,results shows transencasp + scapt,0.6362996101379395
translation,15,160,results,bertasp,performs,much more better,bertasp performs much more better,0.6329836845397949
translation,15,160,results,much more better,than,bert - spc,much more better than bert - spc,0.6357788443565369
translation,15,160,results,bert - spc,in,mams,bert - spc in mams,0.6424346566200256
translation,15,160,results,mams,than in,restaurant / laptop,mams than in restaurant / laptop,0.6318863034248352
translation,15,160,results,results,has,bertasp,results has bertasp,0.6127899289131165
translation,15,165,results,bertasp+scapt,is,4.49 %,bertasp+scapt is 4.49 %,0.5921393632888794
translation,15,165,results,4.49 %,better on,ise,4.49 % better on ise,0.6909835338592529
translation,15,165,results,better performance,on,whole tasks,better performance on whole tasks,0.5383940935134888
translation,15,165,results,bertasp+ cept,has,bertasp+scapt,bertasp+ cept has bertasp+scapt,0.6613691449165344
translation,15,165,results,results,Compared with,bertasp+ cept,results Compared with bertasp+ cept,0.674809992313385
translation,15,183,results,obvious performance drop,in,baseline models,obvious performance drop in baseline models,0.5077483654022217
translation,15,183,results,bertasp +scapt,performs,significantly better,bertasp +scapt performs significantly better,0.661868691444397
translation,15,183,results,significantly better,than,other models,significantly better than other models,0.5623962879180908
translation,15,183,results,other models,with,9.05 %,other models with 9.05 %,0.544816255569458
translation,15,183,results,other models,with,6.63 % decline,other models with 6.63 % decline,0.5696715712547302
translation,15,183,results,9.05 %,/,6.63 % decline,9.05 % / 6.63 % decline,0.634560227394104
translation,15,183,results,6.63 % decline,on,restaurant and laptop,6.63 % decline on restaurant and laptop,0.48437219858169556
translation,15,183,results,obvious performance drop,has,bertasp +scapt,obvious performance drop has bertasp +scapt,0.5879514813423157
translation,15,183,results,baseline models,has,bertasp +scapt,baseline models has bertasp +scapt,0.5681691765785217
translation,15,183,results,results,Comparing to,obvious performance drop,results Comparing to obvious performance drop,0.7432276606559753
translation,15,184,results,models,pre-trained with,scapt,models pre-trained with scapt,0.7336270213127136
translation,15,184,results,scapt,are,more robust,scapt are more robust,0.5943387746810913
translation,15,184,results,more robust,for,aspect-level perturbations,more robust for aspect-level perturbations,0.5785481929779053
translation,15,184,results,more robust,attribute to,better modeling,more robust attribute to better modeling,0.7601926922798157
translation,15,184,results,results,show that,models,results show that models,0.4730059802532196
translation,16,44,ablation-analysis,language -specific knowledge,essential for,tackling,language -specific knowledge essential for tackling,0.6768878102302551
translation,16,44,ablation-analysis,tackling,has,cross-lingual absa task,tackling has cross-lingual absa task,0.5381428599357605
translation,16,44,ablation-analysis,ablation analysis,show,language -specific knowledge,ablation analysis show language -specific knowledge,0.6050970554351807
translation,16,165,ablation-analysis,assumption,that,language -specific knowledge,assumption that language -specific knowledge,0.626315176486969
translation,16,165,ablation-analysis,language -specific knowledge,tackling,cross-lingual absa task,language -specific knowledge tackling cross-lingual absa task,0.6035633683204651
translation,16,165,ablation-analysis,model,on,unlabeled target data,model on unlabeled target data,0.5046809315681458
translation,16,165,ablation-analysis,performance,could be,further improved,performance could be further improved,0.701343834400177
translation,16,165,ablation-analysis,model,has,performance,model has performance,0.5451148748397827
translation,16,165,ablation-analysis,ablation analysis,verifies,assumption,ablation analysis verifies assumption,0.7176623344421387
translation,16,139,experimental-setup,best training hyper-parameters,by conducting,grid search,best training hyper-parameters by conducting grid search,0.6774879693984985
translation,16,139,experimental-setup,grid search,combination of,batch size and learning rate,grid search combination of batch size and learning rate,0.6230162382125854
translation,16,139,experimental-setup,experimental setup,select,best training hyper-parameters,experimental setup select best training hyper-parameters,0.6389145851135254
translation,16,140,experimental-setup,learning rate,has,"{ 2e - 5 , 3e - 5 , 5e - 5 }","learning rate has { 2e - 5 , 3e - 5 , 5e - 5 }",0.5247387886047363
translation,16,140,experimental-setup,learning rate,has,batch size,learning rate has batch size,0.5358124375343323
translation,16,140,experimental-setup,batch size,has,"8 , 16 , 25 }","batch size has 8 , 16 , 25 }",0.6048581600189209
translation,16,142,experimental-setup,mbert,use,learning rate,mbert use learning rate,0.6252483129501343
translation,16,142,experimental-setup,mbert,use,batch size,mbert use batch size,0.660168468952179
translation,16,142,experimental-setup,mbert,use,learning rate,mbert use learning rate,0.6252483129501343
translation,16,142,experimental-setup,mbert,for,xlm -r,mbert for xlm -r,0.6584206819534302
translation,16,142,experimental-setup,mbert,use,learning rate,mbert use learning rate,0.6252483129501343
translation,16,142,experimental-setup,learning rate,being,5e - 5,learning rate being 5e - 5,0.6260045170783997
translation,16,142,experimental-setup,learning rate,being,2e - 5,learning rate being 2e - 5,0.6295531988143921
translation,16,142,experimental-setup,learning rate,of,2e - 5,learning rate of 2e - 5,0.6410878896713257
translation,16,142,experimental-setup,batch size,being,16,batch size being 16,0.6230911612510681
translation,16,142,experimental-setup,batch size,being,8,batch size being 8,0.6359264850616455
translation,16,142,experimental-setup,xlm -r,use,learning rate,xlm -r use learning rate,0.6613675951957703
translation,16,142,experimental-setup,xlm -r,use,batch size,xlm -r use batch size,0.6992930173873901
translation,16,142,experimental-setup,learning rate,of,2e - 5,learning rate of 2e - 5,0.6410878896713257
translation,16,142,experimental-setup,batch size,being,8,batch size being 8,0.6359264850616455
translation,16,142,experimental-setup,experimental setup,For,mbert,experimental setup For mbert,0.6481549143791199
translation,16,142,experimental-setup,experimental setup,for,xlm -r,experimental setup for xlm -r,0.6217503547668457
translation,16,17,experiments,cross-lingual,has,absa,cross-lingual has absa,0.6385807394981384
translation,16,38,experiments,unlabeled target language data,via,knowledge distillation,unlabeled target language data via knowledge distillation,0.6681320667266846
translation,16,38,experiments,knowledge distillation,has,"hinton et al. , 2015 )","knowledge distillation has hinton et al. , 2015 )",0.5692386627197266
translation,16,141,experiments,best choices,selected by,performance,best choices selected by performance,0.7167204022407532
translation,16,141,experiments,performance,on,source language data,performance on source language data,0.5168934464454651
translation,16,7,model,alignment - free label projection method,to obtain,high-quality pseudolabeled data,alignment - free label projection method to obtain high-quality pseudolabeled data,0.5791402459144592
translation,16,7,model,high-quality pseudolabeled data,of,target language,high-quality pseudolabeled data of target language,0.5275003910064697
translation,16,7,model,high-quality pseudolabeled data,help of,translation system,high-quality pseudolabeled data help of translation system,0.6238883137702942
translation,16,7,model,high-quality pseudolabeled data,could preserve,more accurate task -specific knowledge,high-quality pseudolabeled data could preserve more accurate task -specific knowledge,0.7045449018478394
translation,16,7,model,more accurate task -specific knowledge,in,target language,more accurate task -specific knowledge in target language,0.45930418372154236
translation,16,7,model,model,propose,alignment - free label projection method,model propose alignment - free label projection method,0.6150904297828674
translation,16,8,model,aspect code-switching mechanism,to augment,training data,aspect code-switching mechanism to augment training data,0.6637360453605652
translation,16,8,model,training data,with,code-switched bilingual sentences,training data with code-switched bilingual sentences,0.596964418888092
translation,16,8,model,model,design,aspect code-switching mechanism,model design aspect code-switching mechanism,0.5897082090377808
translation,16,30,model,alignment - free label projection method,to obtain,high-quality pseudolabeled target language data,alignment - free label projection method to obtain high-quality pseudolabeled target language data,0.5651364326477051
translation,16,30,model,model,propose,alignment - free label projection method,model propose alignment - free label projection method,0.6150904297828674
translation,16,34,model,aspect code-switching ( acs ) mechanism,switches,aspect terms,aspect code-switching ( acs ) mechanism switches aspect terms,0.7601048946380615
translation,16,34,model,aspect terms,between,source and translated target sentences,aspect terms between source and translated target sentences,0.5266088247299194
translation,16,34,model,source and translated target sentences,to construct,two bilingual sentences,source and translated target sentences to construct two bilingual sentences,0.611903190612793
translation,16,34,model,model,propose,aspect code-switching ( acs ) mechanism,model propose aspect code-switching ( acs ) mechanism,0.6911861896514893
translation,16,208,model,high-quality labeled target data,design,alignment - free label projection method,high-quality labeled target data design alignment - free label projection method,0.505273699760437
translation,16,208,model,alignment - free label projection method,establishes,strong translation - based baseline,alignment - free label projection method establishes strong translation - based baseline,0.5604097843170166
translation,16,208,model,model,To obtain,high-quality labeled target data,model To obtain high-quality labeled target data,0.5788755416870117
translation,16,9,results,above model,on,unlabeled target language data,above model on unlabeled target language data,0.4859589636325836
translation,16,9,results,above model,improves,performance,above model improves performance,0.7639926671981812
translation,16,9,results,unlabeled target language data,improves,performance,unlabeled target language data improves performance,0.6390823125839233
translation,16,9,results,performance,to,same level,performance to same level,0.6317873597145081
translation,16,9,results,same level,of,supervised method,same level of supervised method,0.617517352104187
translation,16,156,results,zero - shot method,based on,mbert,zero - shot method based on mbert,0.7290583252906799
translation,16,156,results,mbert,is,relatively weak,mbert is relatively weak,0.5859703421592712
translation,16,156,results,xlm -r backbone,becomes,competitive baseline,xlm -r backbone becomes competitive baseline,0.5796166062355042
translation,16,158,results,bilingual -ta,trained with,source data,bilingual -ta trained with source data,0.7403568029403687
translation,16,158,results,bilingual -ta,trained with,labeled target data,bilingual -ta trained with labeled target data,0.7413628101348877
translation,16,158,results,labeled target data,from,translate - then - align paradigm,labeled target data from translate - then - align paradigm,0.5802510380744934
translation,16,158,results,even worse,than,zero - shot method,even worse than zero - shot method,0.6093063950538635
translation,16,158,results,results,has,bilingual -ta,results has bilingual -ta,0.5559729933738708
translation,16,159,results,proposed alignmentfree label projection method,establishes,strong baseline,proposed alignmentfree label projection method establishes strong baseline,0.5653381943702698
translation,16,159,results,strong baseline,for,crosslingual absa problem,strong baseline for crosslingual absa problem,0.5854579210281372
translation,16,159,results,results,has,proposed alignmentfree label projection method,results has proposed alignmentfree label projection method,0.5326226949691772
translation,16,160,results,zero - shot method,based on either,mbert or xlm -r,zero - shot method based on either mbert or xlm -r,0.7104638814926147
translation,16,160,results,much better performance,than,previous translation - based approach,much better performance than previous translation - based approach,0.5604400038719177
translation,16,160,results,outperforms,has,zero - shot method,outperforms has zero - shot method,0.5894685983657837
translation,16,161,results,translation -ta,obtains,6.84 % and 8.21 % absolute performance gains,translation -ta obtains 6.84 % and 8.21 % absolute performance gains,0.5980414748191833
translation,16,161,results,6.84 % and 8.21 % absolute performance gains,based on,mbert and xlm -r,6.84 % and 8.21 % absolute performance gains based on mbert and xlm -r,0.668215274810791
translation,16,161,results,results,Compared with,translation -ta,results Compared with translation -ta,0.6773470640182495
translation,16,163,results,proposed acs method,has,further outperforms,proposed acs method has further outperforms,0.5811654925346375
translation,16,163,results,further outperforms,has,translation - af baseline,further outperforms has translation - af baseline,0.5724828243255615
translation,16,163,results,results,has,proposed acs method,results has proposed acs method,0.574910044670105
translation,16,164,results,model,on,unlabeled data,model on unlabeled data,0.508256196975708
translation,16,164,results,unlabeled data,of,target language,unlabeled data of target language,0.5140315294265747
translation,16,164,results,proposed single - teacher distillation ( acs - distill -s ),achieve,greater performance,proposed single - teacher distillation ( acs - distill -s ) achieve greater performance,0.6692807078361511
translation,16,164,results,multi-teacher distillation ( acs - distill -m ),achieve,greater performance,multi-teacher distillation ( acs - distill -m ) achieve greater performance,0.6650066375732422
translation,16,164,results,model,has,proposed single - teacher distillation ( acs - distill -s ),model has proposed single - teacher distillation ( acs - distill -s ),0.5869888067245483
translation,16,164,results,results,By distilling,model,results By distilling model,0.6220720410346985
translation,16,166,results,model,trained with,multiple teachers,model trained with multiple teachers,0.7501482367515564
translation,16,166,results,model,achieves,slightly better performance,model achieves slightly better performance,0.6742340326309204
translation,16,166,results,slightly better performance,than,single -teacher model,slightly better performance than single -teacher model,0.5691604018211365
translation,16,166,results,results,has,model,results has model,0.5339115858078003
translation,16,170,results,multilingual pseudo-labeled translated target data,"our label projection method ( i.e. ,",mtl - af ),"multilingual pseudo-labeled translated target data our label projection method ( i.e. , mtl - af )",0.7247605919837952
translation,16,170,results,multilingual pseudo-labeled translated target data,sets up,quite strong baseline,multilingual pseudo-labeled translated target data sets up quite strong baseline,0.6229647994041443
translation,16,170,results,already outperform,has,mtl - ws,already outperform has mtl - ws,0.6208239793777466
translation,16,170,results,already outperform,has,previous state - of - the - art model,already outperform has previous state - of - the - art model,0.5582608580589294
translation,16,170,results,mtl - ws,has,previous state - of - the - art model,mtl - ws has previous state - of - the - art model,0.5748343467712402
translation,16,170,results,results,training on,multilingual pseudo-labeled translated target data,results training on multilingual pseudo-labeled translated target data,0.613915205001831
translation,16,171,results,distillation,on,unlabeled data,distillation on unlabeled data,0.5583367943763733
translation,16,171,results,distillation,improve,adaptation performance,distillation improve adaptation performance,0.682765543460846
translation,16,174,results,our model,distilled on,multilingual data ( mtl - acs - d ),our model distilled on multilingual data ( mtl - acs - d ),0.7273309826850891
translation,16,174,results,our model,achieves,65.33 average f1 scores,our model achieves 65.33 average f1 scores,0.6104311347007751
translation,16,174,results,65.33 average f1 scores,even close to,f1 scores,65.33 average f1 scores even close to f1 scores,0.6194704174995422
translation,16,174,results,65.33 average f1 scores,showing,superiority,65.33 average f1 scores showing superiority,0.674813985824585
translation,16,174,results,f1 scores,under,supervised setting,f1 scores under supervised setting,0.6583106517791748
translation,16,174,results,xlm -r as backbone,has,our model,xlm -r as backbone has our model,0.615821361541748
translation,16,174,results,results,With,xlm -r as backbone,results With xlm -r as backbone,0.5894785523414612
translation,16,177,results,translated data and the source data,is,most powerful method,translated data and the source data is most powerful method,0.5313620567321777
translation,16,177,results,results,combining,translated data and the source data,results combining translated data and the source data,0.6738789677619934
translation,16,178,results,best performance,when,target language,best performance when target language,0.6493076086044312
translation,16,178,results,target language,is,spanish,target language is spanish,0.5763013362884521
translation,16,190,results,af,produces,2.1 % partially missed aspects,af produces 2.1 % partially missed aspects,0.619411051273346
translation,16,190,results,2.1 % partially missed aspects,when facing,long aspect terms,2.1 % partially missed aspects when facing long aspect terms,0.5837658643722534
translation,16,190,results,results,notice,af,results notice af,0.5822162628173828
translation,17,26,ablation-analysis,performance drop,of,5 - 6 %,performance drop of 5 - 6 %,0.6001412272453308
translation,17,26,ablation-analysis,5 - 6 %,when,test instances,5 - 6 % when test instances,0.6656421422958374
translation,17,26,ablation-analysis,test instances,arise from,topics,test instances arise from topics,0.701984703540802
translation,17,26,ablation-analysis,topics,not seen during,training,topics not seen during training,0.7633423805236816
translation,17,38,baselines,new methods,that uses,unlabeled text and tables,new methods that uses unlabeled text and tables,0.6421911120414734
translation,17,38,baselines,new methods,to create,tableqa models,new methods to create tableqa models,0.7180627584457397
translation,17,38,baselines,unlabeled text and tables,from,target topic,unlabeled text and tables from target topic,0.5155912041664124
translation,17,38,baselines,unlabeled text and tables,to create,tableqa models,unlabeled text and tables to create tableqa models,0.6956632137298584
translation,17,38,baselines,tableqa models,which are,more robust,tableqa models which are more robust,0.6623207330703735
translation,17,38,baselines,more robust,to,topic shift,more robust to topic shift,0.546291172504425
translation,17,150,baselines,tabert + mapo,uses,standard bert,tabert + mapo uses standard bert,0.6372411847114563
translation,17,150,baselines,tabert + mapo,uses,"mapo ( liang et al. , 2018 )","tabert + mapo uses mapo ( liang et al. , 2018 )",0.5770071148872375
translation,17,150,baselines,tabert + mapo,as,base semantic parser,tabert + mapo as base semantic parser,0.508254885673523
translation,17,150,baselines,standard bert,as,table-question encoder,standard bert as table-question encoder,0.5529158711433411
translation,17,150,baselines,"mapo ( liang et al. , 2018 )",as,base semantic parser,"mapo ( liang et al. , 2018 ) as base semantic parser",0.4618682563304901
translation,17,150,baselines,baselines,has,tabert + mapo,baselines has tabert + mapo,0.6001957654953003
translation,17,151,baselines,tabert t + mapo,uses,topic spe- 2019 ),tabert t + mapo uses topic spe- 2019 ),0.60860675573349
translation,17,151,baselines,topic spe- 2019 ),for,question generation,topic spe- 2019 ) for question generation,0.6374925971031189
translation,17,151,baselines,question generation,intialized with,t5 - base,question generation intialized with t5 - base,0.6610633134841919
translation,17,151,baselines,finetuned,using,sql,finetuned using sql,0.696317732334137
translation,17,151,baselines,finetuned,using,corresponding questions,finetuned using corresponding questions,0.6810438632965088
translation,17,151,baselines,corresponding questions,from,wik -isql dataset,corresponding questions from wik -isql dataset,0.6021227240562439
translation,17,151,baselines,baselines,has,tabert t + mapo,baselines has tabert t + mapo,0.6118835806846619
translation,17,149,experimental-setup,experiments,using,variant of tabert + mapo 3 architecture,experiments using variant of tabert + mapo 3 architecture,0.6605223417282104
translation,17,149,experimental-setup,variant of tabert + mapo 3 architecture,with,underlying bert model,variant of tabert + mapo 3 architecture with underlying bert model,0.6363366842269897
translation,17,149,experimental-setup,underlying bert model,initialized with,bert- base-uncased,underlying bert model initialized with bert- base-uncased,0.775015115737915
translation,17,154,experimental-setup,batch-size,of,10,batch-size of 10,0.707580029964447
translation,17,154,experimental-setup,batch-size,with,learning rate,batch-size with learning rate,0.6311206221580505
translation,17,154,experimental-setup,10,with,learning rate,10 with learning rate,0.6691243052482605
translation,17,154,experimental-setup,learning rate,of,10 ?3,learning rate of 10 ?3,0.6398048996925354
translation,17,155,experimental-setup,existing code base,for,tabert + mapo,existing code base for tabert + mapo,0.5550438165664673
translation,17,155,experimental-setup,existing code base,use,bert base,existing code base use bert base,0.6418372392654419
translation,17,155,experimental-setup,bert base,as,encoder,bert base as encoder,0.6009477972984314
translation,17,155,experimental-setup,encoder,for,tables and questions,encoder for tables and questions,0.6827502250671387
translation,17,155,experimental-setup,experimental setup,build upon,existing code base,experimental setup build upon existing code base,0.5969564318656921
translation,17,155,experimental-setup,experimental setup,use,bert base,experimental setup use bert base,0.6177388429641724
translation,17,156,experimental-setup,topic-specific vocabulary,for,bert 's tokenizer,topic-specific vocabulary for bert 's tokenizer,0.6174350380897522
translation,17,156,experimental-setup,topic-specific vocabulary,train it using,mlm ( masked language model ) objective,topic-specific vocabulary train it using mlm ( masked language model ) objective,0.6509224772453308
translation,17,156,experimental-setup,mlm ( masked language model ) objective,for,3 epochs,mlm ( masked language model ) objective for 3 epochs,0.5954433679580688
translation,17,156,experimental-setup,3 epochs,with,p=0.15 chance,3 epochs with p=0.15 chance,0.6174222230911255
translation,17,156,experimental-setup,p=0.15 chance,of masking,topic-specific high frequency,p=0.15 chance of masking topic-specific high frequency,0.6882874369621277
translation,17,156,experimental-setup,experimental setup,use,topic-specific vocabulary,experimental setup use topic-specific vocabulary,0.6142479181289673
translation,17,156,experimental-setup,experimental setup,train it using,mlm ( masked language model ) objective,experimental setup train it using mlm ( masked language model ) objective,0.6386131048202515
translation,17,157,experimental-setup,bert parameters,using,adam optimizer,bert parameters using adam optimizer,0.6371648907661438
translation,17,157,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,17,157,experimental-setup,learning rate,of,5 ? 10 ?5,learning rate of 5 ? 10 ?5,0.6658074855804443
translation,17,157,experimental-setup,experimental setup,optimize,bert parameters,experimental setup optimize bert parameters,0.602453351020813
translation,17,6,experiments,practical topic shift scenario,by designing,novel challenge benchmarks wikisql -ts and wikitq - ts,practical topic shift scenario by designing novel challenge benchmarks wikisql -ts and wikitq - ts,0.6711830496788025
translation,17,6,experiments,novel challenge benchmarks wikisql -ts and wikitq - ts,consisting of,train- dev-test splits,novel challenge benchmarks wikisql -ts and wikitq - ts consisting of train- dev-test splits,0.6605047583580017
translation,17,6,experiments,train- dev-test splits,in,five distinct topic groups,train- dev-test splits in five distinct topic groups,0.545521080493927
translation,17,33,experiments,pragmatic question generator,samples,sql queries,pragmatic question generator samples sql queries,0.6900943517684937
translation,17,33,experiments,pragmatic question generator,transcribes them to,natural language questions,pragmatic question generator transcribes them to natural language questions,0.6976961493492126
translation,17,33,experiments,sql queries,of,various types,sql queries of various types,0.5954176783561707
translation,17,33,experiments,various types,from,target topic table,various types from target topic table,0.5656506419181824
translation,17,33,experiments,natural language questions,to,finetune,natural language questions to finetune,0.5519731044769287
translation,17,33,experiments,tableqa model,on,target topic,tableqa model on target topic,0.5114424824714661
translation,17,33,experiments,finetune,has,tableqa model,finetune has tableqa model,0.562757670879364
translation,17,153,experiments,t5,trained on,wikisql - ts,t5 trained on wikisql - ts,0.7780991196632385
translation,17,153,experiments,wikisql - ts,to generate,synthetic questions,wikisql - ts to generate synthetic questions,0.6988296508789062
translation,17,8,model,pragmatic adaptation framework,for,tableqa,pragmatic adaptation framework for tableqa,0.6086719632148743
translation,17,8,model,pragmatic adaptation framework,comprising of,topic-specific vocabulary injection,pragmatic adaptation framework comprising of topic-specific vocabulary injection,0.6242652535438538
translation,17,8,model,topic-specific vocabulary injection,into,bert,topic-specific vocabulary injection into bert,0.6190356612205505
translation,17,8,model,novel text - to - text transformer generator,based,natural language question generation pipeline,novel text - to - text transformer generator based natural language question generation pipeline,0.6013995409011841
translation,17,8,model,natural language question generation pipeline,focused on generating,topic specific training data,natural language question generation pipeline focused on generating topic specific training data,0.6771823167800903
translation,17,8,model,t3qa ( topic transferable table question answering ),has,pragmatic adaptation framework,t3qa ( topic transferable table question answering ) has pragmatic adaptation framework,0.5515987277030945
translation,17,8,model,model,propose,t3qa ( topic transferable table question answering ),model propose t3qa ( topic transferable table question answering ),0.6356152892112732
translation,17,27,model,novel t3qa framework,for,tableqa training,novel t3qa framework for tableqa training,0.6071414351463318
translation,17,27,model,novel t3qa framework,leads to,greater cross-topic robustness,novel t3qa framework leads to greater cross-topic robustness,0.6217340230941772
translation,17,27,model,model,propose,novel t3qa framework,model propose novel t3qa framework,0.69290691614151
translation,17,32,model,powerful text - to - text transfer transformer module,to generate,synthetic questions,powerful text - to - text transfer transformer module to generate synthetic questions,0.6886847615242004
translation,17,32,model,synthetic questions,for,target topic,synthetic questions for target topic,0.5763756632804871
translation,17,32,model,model,uses,powerful text - to - text transfer transformer module,model uses powerful text - to - text transfer transformer module,0.5629218816757202
translation,17,50,model,tabert,jointly encodes,natural language question and the table,tabert jointly encodes natural language question and the table,0.7120571136474609
translation,17,50,model,tabert,creating,entity links,tabert creating entity links,0.5966838002204895
translation,17,50,model,tabert,creating,relationship,tabert creating relationship,0.656943142414093
translation,17,50,model,entity links,between,question tokens and tablecontent,entity links between question tokens and tablecontent,0.6650846004486084
translation,17,50,model,relationship,has,between table cells,relationship has between table cells,0.5498695373535156
translation,17,50,model,model,has,tabert,model has tabert,0.5728225111961365
translation,17,7,results,pre-training,on,large open-domain text,pre-training on large open-domain text,0.5187272429466248
translation,17,7,results,performance,of,models,performance of models,0.598082423210144
translation,17,7,results,performance,evaluated on,unseen topics,performance evaluated on unseen topics,0.73383629322052
translation,17,7,results,models,evaluated on,unseen topics,models evaluated on unseen topics,0.6982772350311279
translation,17,7,results,degrades significantly,evaluated on,unseen topics,degrades significantly evaluated on unseen topics,0.7078874111175537
translation,17,7,results,pre-training,has,performance,pre-training has performance,0.5681560039520264
translation,17,7,results,models,has,degrades significantly,models has degrades significantly,0.633710503578186
translation,17,7,results,results,empirically show,pre-training,results empirically show pre-training,0.6335887312889099
translation,17,7,results,results,despite,pre-training,results despite pre-training,0.6587851047515869
translation,17,34,results,t3qa,improves,performance,t3qa improves performance,0.6551646590232849
translation,17,34,results,performance,of,tableqa model,performance of tableqa model,0.6008443236351013
translation,17,34,results,tableqa model,with,post-hoc logical form re-ranker,tableqa model with post-hoc logical form re-ranker,0.6522131562232971
translation,17,34,results,post-hoc logical form re-ranker,aided by,entity linking,post-hoc logical form re-ranker aided by entity linking,0.6547430157661438
translation,17,34,results,results,has,t3qa,results has t3qa,0.5323117971420288
translation,17,160,results,-ts,improves over,tabert + mapo,-ts improves over tabert + mapo,0.7648335099220276
translation,17,160,results,tabert t + mapo,improves over,tabert + mapo,tabert t + mapo improves over tabert + mapo,0.7326755523681641
translation,17,160,results,tabert t + mapo,for,four out of five test topics,tabert t + mapo for four out of five test topics,0.6322184205055237
translation,17,160,results,tabert + mapo,for,four out of five test topics,tabert + mapo for four out of five test topics,0.6233349442481995
translation,17,160,results,average of 1.66 %,showing,advantage,average of 1.66 % showing advantage,0.7382895946502686
translation,17,160,results,-ts,has,tabert t + mapo,-ts has tabert t + mapo,0.6670032739639282
translation,17,165,results,tabert + mapo,when finetuned with,target topic samples,tabert + mapo when finetuned with target topic samples,0.7040836811065674
translation,17,165,results,tabert t + mapo,when finetuned with,target topic samples,tabert t + mapo when finetuned with target topic samples,0.7120499014854431
translation,17,165,results,target topic samples,obtained from,qg,target topic samples obtained from qg,0.5933624505996704
translation,17,165,results,qg,has,significantly outperforms,qg has significantly outperforms,0.6216397285461426
translation,17,165,results,significantly outperforms,has,tabert + mapo,significantly outperforms has tabert + mapo,0.5911658406257629
translation,17,165,results,results,has,qg,results has qg,0.5781679749488831
translation,17,166,results,qg,improves,performance,qg improves performance,0.7412697076797485
translation,17,166,results,performance,of,tabert t + mapo,performance of tabert t + mapo,0.6043075323104858
translation,17,166,results,wikisql -ts,has,qg,wikisql -ts has qg,0.6624855399131775
translation,17,166,results,results,In,wikisql -ts,results In wikisql -ts,0.547701895236969
translation,17,167,results,vocabulary extension,ensures,topical tokens,vocabulary extension ensures topical tokens,0.6747153997421265
translation,17,167,results,topical tokens,encoded,qg,topical tokens encoded qg,0.7909067273139954
translation,17,167,results,qg,improves,implicit linking,qg improves implicit linking,0.7175164818763733
translation,17,167,results,implicit linking,between,question and table header tokens,implicit linking between question and table header tokens,0.6631062030792236
translation,17,167,results,question and table header tokens,within,joint encoding,question and table header tokens within joint encoding,0.7040548920631409
translation,17,167,results,joint encoding,of,question -table,joint encoding of question -table,0.6142159104347229
translation,17,167,results,results,has,vocabulary extension,results has vocabulary extension,0.5491849184036255
translation,17,168,results,largest improvement,of,10.53 % and 7.74 %,largest improvement of 10.53 % and 7.74 %,0.5381537675857544
translation,17,168,results,10.53 % and 7.74 %,obtained for,people and culture,10.53 % and 7.74 % obtained for people and culture,0.6406704783439636
translation,17,168,results,results,has,largest improvement,results has largest improvement,0.5710678100585938
translation,17,169,results,tabert + mapo + qg,out-performs,topic performance,tabert + mapo + qg out-performs topic performance,0.7671162486076355
translation,17,169,results,topic performance,of,64.07 % and 67 %,topic performance of 64.07 % and 67 %,0.5714383125305176
translation,17,169,results,64.07 % and 67 %,with,66.27 % and 69.88 %,64.07 % and 67 % with 66.27 % and 69.88 %,0.643915593624115
translation,17,169,results,results,has,tabert + mapo + qg,results has tabert + mapo + qg,0.5860430598258972
translation,17,171,results,outperforms,by,5.4 %,outperforms by 5.4 %,0.6202278137207031
translation,17,171,results,tabert t + mapo,by,5.4 %,tabert t + mapo by 5.4 %,0.6048569083213806
translation,17,171,results,5.4 %,due to,qg,5.4 % due to qg,0.6741040349006653
translation,17,171,results,tabert + mapo + qg,has,outperforms,tabert + mapo + qg has outperforms,0.6497905254364014
translation,17,171,results,outperforms,has,tabert t + mapo,outperforms has tabert t + mapo,0.6253537535667419
translation,17,171,results,results,has,tabert + mapo + qg,results has tabert + mapo + qg,0.5860430598258972
translation,17,173,results,composite dev set,with,50 % real questions,composite dev set with 50 % real questions,0.646905243396759
translation,17,173,results,composite dev set,with,50 % questions,composite dev set with 50 % questions,0.6840617060661316
translation,17,173,results,50 % questions,generated on,tables,50 % questions generated on tables,0.6748773455619812
translation,17,173,results,tables,from,target topic,tables from target topic,0.534972071647644
translation,17,173,results,results,observe,composite dev set,results observe composite dev set,0.6297927498817444
translation,18,91,ablation-analysis,"digit information (   e   ,   10e #   )",along with,numbers,"digit information (   e   ,   10e #   ) along with numbers",0.6412063837051392
translation,18,91,ablation-analysis,"digit information (   e   ,   10e #   )",along with,numbers,"digit information (   e   ,   10e #   ) along with numbers",0.6412063837051392
translation,18,91,ablation-analysis,numbers,has,in their digit form,numbers has in their digit form,0.5446697473526001
translation,18,91,ablation-analysis,modeling,has,numbers,modeling has numbers,0.6099072694778442
translation,18,91,ablation-analysis,ablation analysis,providing,"digit information (   e   ,   10e #   )","ablation analysis providing digit information (   e   ,   10e #   )",0.693305492401123
translation,18,32,results,our e-digit method,successfully generalizes to,out -of- distribution numbers,our e-digit method successfully generalizes to out -of- distribution numbers,0.7714313268661499
translation,18,32,results,all the other surface forms,by,significant margin,all the other surface forms by significant margin,0.628669023513794
translation,18,32,results,outperforms,has,all the other surface forms,outperforms has all the other surface forms,0.6147319078445435
translation,18,32,results,results,has,our e-digit method,results has our e-digit method,0.5511247515678406
translation,18,85,results,minor degradation,in,performance,minor degradation in performance,0.541309118270874
translation,18,85,results,e-digit ( interpolate ),performs,comparably,e-digit ( interpolate ) performs comparably,0.6378925442695618
translation,18,85,results,comparably,to,genbert,comparably to genbert,0.6659865379333496
translation,18,85,results,minor degradation,has,e-digit ( interpolate ),minor degradation has e-digit ( interpolate ),0.5905744433403015
translation,18,85,results,performance,has,e-digit ( interpolate ),performance has e-digit ( interpolate ),0.5956282019615173
translation,18,85,results,results,Despite,minor degradation,results Despite minor degradation,0.7352340221405029
translation,18,90,results,surface form methods,has,outperform,surface form methods has outperform,0.6095209121704102
translation,18,90,results,outperform,has,original models ' subword tok-enization approach,outperform has original models ' subword tok-enization approach,0.5876336097717285
translation,18,90,results,results,has,surface form methods,results has surface form methods,0.46389174461364746
translation,18,94,results,outperforms,notably in,number and date categories,outperforms notably in number and date categories,0.6661927103996277
translation,18,94,results,other forms,notably in,number and date categories,other forms notably in number and date categories,0.5339395403862
translation,18,94,results,e-digit method,has,outperforms,e-digit method has outperforms,0.6422523260116577
translation,18,94,results,outperforms,has,other forms,outperforms has other forms,0.5847077965736389
translation,18,94,results,results,has,e-digit method,results has e-digit method,0.538775622844696
translation,18,96,results,other surface forms,in,date type answers,other surface forms in date type answers,0.4746026396751404
translation,18,96,results,10 - based surface form,has,outperforms,10 - based surface form has outperforms,0.6116624474525452
translation,18,96,results,outperforms,has,other surface forms,outperforms has other surface forms,0.6304792761802673
translation,18,96,results,results,has,10 - based surface form,results has 10 - based surface form,0.556201159954071
translation,19,6,experiments,clips,of,different lengths,clips of different lengths,0.6254750490188599
translation,19,6,experiments,different lengths,to represent,different scales,different lengths to represent different scales,0.615165650844574
translation,19,6,experiments,different scales,of,video,different scales of video,0.6230251789093018
translation,19,94,hyperparameters,adam,used with,initial learning rate,adam used with initial learning rate,0.6440380811691284
translation,19,94,hyperparameters,initial learning rate,of,10 ?4,initial learning rate of 10 ?4,0.6392531991004944
translation,19,94,hyperparameters,batch size,is,64,batch size is 64,0.6388692259788513
translation,19,94,hyperparameters,network,has,adam,network has adam,0.6193957924842834
translation,19,94,hyperparameters,network,has,batch size,network has batch size,0.5767664909362793
translation,19,94,hyperparameters,tgif - qa dataset,has,batch size,tgif - qa dataset has batch size,0.5440555810928345
translation,19,94,hyperparameters,hyperparameters,training,network,hyperparameters training network,0.7444899678230286
translation,19,94,hyperparameters,hyperparameters,For,tgif - qa dataset,hyperparameters For tgif - qa dataset,0.537172794342041
translation,19,5,model,novel multi-scale progressive attention network ( mspan ),to achieve,relational reasoning,novel multi-scale progressive attention network ( mspan ) to achieve relational reasoning,0.6742158532142639
translation,19,5,model,relational reasoning,between,cross-scale video information,relational reasoning between cross-scale video information,0.6273970603942871
translation,19,5,model,model,propose,novel multi-scale progressive attention network ( mspan ),model propose novel multi-scale progressive attention network ( mspan ),0.64955735206604
translation,19,7,model,cliplevel features,aggregated into,node features,cliplevel features aggregated into node features,0.6677402257919312
translation,19,7,model,node features,by using,max -pool,node features by using max -pool,0.6145810484886169
translation,19,7,model,graph,generated for,each scale of clips,graph generated for each scale of clips,0.7168512940406799
translation,19,7,model,model,has,cliplevel features,model has cliplevel features,0.5276111364364624
translation,19,97,results,outperforms,by,2.5 % and 1.9 %,outperforms by 2.5 % and 1.9 %,0.6185242533683777
translation,19,97,results,state - of - the - art methods,by,2.5 % and 1.9 %,state - of - the - art methods by 2.5 % and 1.9 %,0.5404202342033386
translation,19,97,results,2.5 % and 1.9 %,of,accuracy,2.5 % and 1.9 % of accuracy,0.5962446331977844
translation,19,97,results,accuracy,on,action and transition tasks,accuracy on action and transition tasks,0.5094148516654968
translation,19,97,results,outperforms,has,state - of - the - art methods,outperforms has state - of - the - art methods,0.5579615831375122
translation,19,98,results,our method,achieves,best mean square error ( mse ),our method achieves best mean square error ( mse ),0.6506502032279968
translation,19,98,results,best mean square error ( mse ),of,3.57,best mean square error ( mse ) of 3.57,0.5579476952552795
translation,19,98,results,3.57,among,all methods,3.57 among all methods,0.5615347623825073
translation,19,98,results,count task,has,our method,count task has our method,0.5915746688842773
translation,19,98,results,results,For,count task,results For count task,0.6344899535179138
translation,19,99,results,quest,used,multidimension visual features,quest used multidimension visual features,0.6129519939422607
translation,19,99,results,multidimension visual features,containing,more appearance information,multidimension visual features containing more appearance information,0.64410799741745
translation,19,99,results,our method,get,same accuracy,our method get same accuracy,0.5925007462501526
translation,19,99,results,59.7 %,as,quest,59.7 % as quest,0.5862782597541809
translation,19,99,results,quest,on,frameqa task,quest on frameqa task,0.5720033049583435
translation,19,99,results,quest,has,our method,quest has our method,0.6274718046188354
translation,19,99,results,multidimension visual features,has,our method,multidimension visual features has our method,0.5604987740516663
translation,19,99,results,more appearance information,has,our method,more appearance information has our method,0.5204026103019714
translation,19,99,results,same accuracy,has,59.7 %,same accuracy has 59.7 %,0.5900297164916992
translation,19,100,results,our method,makes sense of,multiscale information,our method makes sense of multiscale information,0.7139788866043091
translation,19,100,results,multiscale information,of,video,multiscale information of video,0.5807611346244812
translation,19,100,results,results,has,our method,results has our method,0.5589964985847473
translation,19,101,results,results,on,msvd -qa,results on msvd -qa,0.5432232022285461
translation,19,102,results,our method,improves,overall accuracy,our method improves overall accuracy,0.6443501114845276
translation,19,102,results,overall accuracy,by,4.2 %,overall accuracy by 4.2 %,0.5407313108444214
translation,19,102,results,4.2 %,compared to,recent methods,4.2 % compared to recent methods,0.6517783999443054
translation,19,102,results,results,has,our method,results has our method,0.5589964985847473
translation,19,106,results,our method,achieves,best overall accuracy,our method achieves best overall accuracy,0.6436389684677124
translation,19,106,results,best overall accuracy,of,37.8 %,best overall accuracy of 37.8 %,0.5317971110343933
translation,19,106,results,results,has,our method,results has our method,0.5589964985847473
translation,19,126,results,performances,of,three structures,performances of three structures,0.6344045996665955
translation,19,126,results,poorer,than,our entire network,poorer than our entire network,0.6115604639053345
translation,19,126,results,results,has,performances,results has performances,0.5711642503738403
translation,20,6,model,synthetic adversarial data generation,to make,question answering,synthetic adversarial data generation to make question answering,0.5513660311698914
translation,20,7,model,data generation pipeline,selects,source passages,data generation pipeline selects source passages,0.6727394461631775
translation,20,7,model,data generation pipeline,identifies,candidate answers,data generation pipeline identifies candidate answers,0.6715161204338074
translation,20,7,model,data generation pipeline,generates,questions,data generation pipeline generates questions,0.6858770251274109
translation,20,7,model,filters or relabels,to improve,quality,filters or relabels to improve quality,0.7068780660629272
translation,20,7,model,model,develop,data generation pipeline,model develop data generation pipeline,0.6905627846717834
translation,20,26,model,model,develop,synthetic adversarial data generation pipeline,model develop synthetic adversarial data generation pipeline,0.6190783381462097
translation,20,9,results,our synthetic data,improve,state - of- theart,our synthetic data improve state - of- theart,0.6497623920440674
translation,20,9,results,our synthetic data,improve,model generalisation,our synthetic data improve model generalisation,0.6398181319236755
translation,20,9,results,state - of- theart,on,adversarialqa dataset,state - of- theart on adversarialqa dataset,0.5385571718215942
translation,20,9,results,adversarialqa dataset,by,3.7f 1,adversarialqa dataset by 3.7f 1,0.5722145438194275
translation,20,9,results,model generalisation,on,nine of the twelve mrqa datasets,model generalisation on nine of the twelve mrqa datasets,0.4905230402946472
translation,20,9,results,results,incorporating,our synthetic data,results incorporating our synthetic data,0.6529829502105713
translation,21,145,baselines,first baseline,is,classic bm25 baseline,first baseline is classic bm25 baseline,0.5612543225288391
translation,21,145,baselines,baselines,has,first baseline,baselines has first baseline,0.595583438873291
translation,21,146,baselines,deep passage retrieval ( dpr ) model,from,karpukhin et al . ( 2020 ),deep passage retrieval ( dpr ) model from karpukhin et al . ( 2020 ),0.518729031085968
translation,21,152,experimental-setup,training,used,batch size,training used batch size,0.6306583285331726
translation,21,152,experimental-setup,batch size,of,128,batch size of 128,0.6836684346199036
translation,21,152,experimental-setup,128,for,our models,128 for our models,0.6681172847747803
translation,21,152,experimental-setup,experimental setup,For,training,experimental setup For training,0.5809131264686584
translation,21,155,experimental-setup,dropout,is,0.1,dropout is 0.1,0.5574049949645996
translation,21,155,experimental-setup,0.1,for,all encoders,0.1 for all encoders,0.6084725856781006
translation,21,155,experimental-setup,experimental setup,has,dropout,experimental setup has dropout,0.5067690014839172
translation,21,157,experimental-setup,momentum co-efficient,in,momentum update,momentum co-efficient in momentum update,0.4974639415740967
translation,21,157,experimental-setup,momentum co-efficient,set to,0.001,momentum co-efficient set to 0.001,0.6785829663276672
translation,21,157,experimental-setup,momentum update,set to,0.001,momentum update set to 0.001,0.6455057859420776
translation,21,157,experimental-setup,experimental setup,has,momentum co-efficient,experimental setup has momentum co-efficient,0.5530279874801636
translation,21,158,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,21,158,experimental-setup,adam optimizer,with,linear scheduling,adam optimizer with linear scheduling,0.6306654214859009
translation,21,158,experimental-setup,learning rate,of,3e ? 5,learning rate of 3e ? 5,0.6395402550697327
translation,21,158,experimental-setup,linear scheduling,with,5 % warm - up,linear scheduling with 5 % warm - up,0.6726462244987488
translation,21,158,experimental-setup,experimental setup,used,adam optimizer,experimental setup used adam optimizer,0.5961911082267761
translation,21,161,experimental-setup,training,done on,16 32gb nvidia gpus,training done on 16 32gb nvidia gpus,0.653170645236969
translation,21,161,experimental-setup,experimental setup,has,training,experimental setup has training,0.5312813520431519
translation,21,162,experimental-setup,"faiss ( johnson et al. , 2017 )",for,indexing and retrieving passage vectors,"faiss ( johnson et al. , 2017 ) for indexing and retrieving passage vectors",0.5771853923797607
translation,21,163,experimental-setup,bm25,use,lucene implementation,bm25 use lucene implementation,0.6367742419242859
translation,21,163,experimental-setup,lucene implementation,with,b = 0.4 ( length normalization ),lucene implementation with b = 0.4 ( length normalization ),0.5886169672012329
translation,21,163,experimental-setup,lucene implementation,with,k 1 = 0.9 ( term frequency scaling ),lucene implementation with k 1 = 0.9 ( term frequency scaling ),0.6121454834938049
translation,21,163,experimental-setup,experimental setup,For,bm25,experimental setup For bm25,0.6213721632957458
translation,21,153,experiments,two small datasets trec and wq,trained,model,two small datasets trec and wq trained model,0.6693786978721619
translation,21,153,experiments,two small datasets trec and wq,trained,model,two small datasets trec and wq trained model,0.6693786978721619
translation,21,153,experiments,two small datasets trec and wq,trained,model,two small datasets trec and wq trained model,0.6693786978721619
translation,21,153,experiments,model,for,100 epochs,model for 100 epochs,0.6005324721336365
translation,21,153,experiments,model,for,40 epochs,model for 40 epochs,0.6097289323806763
translation,21,153,experiments,model,for,40 epochs,model for 40 epochs,0.6097289323806763
translation,21,153,experiments,other datasets,trained,model,other datasets trained model,0.7873001098632812
translation,21,153,experiments,model,for,40 epochs,model for 40 epochs,0.6097289323806763
translation,21,156,experiments,queue size,of,negative examples,queue size of negative examples,0.5981473326683044
translation,21,156,experiments,negative examples,in,our model,negative examples in our model,0.5155655145645142
translation,21,156,experiments,negative examples,is,16,negative examples is 16,0.5143612623214722
translation,21,156,experiments,negative examples,is,384,negative examples is 384,0.5562692880630493
translation,21,156,experiments,16,",",384,"16 , 384",0.6752002239227295
translation,21,198,experiments,xmoco,improves,passage retrieval model,xmoco improves passage retrieval model,0.6431026458740234
translation,21,198,experiments,passage retrieval model,by efficiently maintaining,large set of negative examples,passage retrieval model by efficiently maintaining large set of negative examples,0.6189306974411011
translation,21,102,hyperparameters,pre-trained uncased bert - base ) models,as,our encoders,pre-trained uncased bert - base ) models as our encoders,0.5202401280403137
translation,21,102,hyperparameters,our encoders,following,2020 ),our encoders following 2020 ),0.6726966500282288
translation,21,102,hyperparameters,hyperparameters,use,pre-trained uncased bert - base ) models,hyperparameters use pre-trained uncased bert - base ) models,0.5956521034240723
translation,21,5,model,dual-encoder model,learned to encode,questions and passages separately,dual-encoder model learned to encode questions and passages separately,0.7991891503334045
translation,21,5,model,questions and passages separately,into,vector representations,questions and passages separately into vector representations,0.5978354215621948
translation,21,5,model,efficiently retrieved,by,vector space search,efficiently retrieved by vector space search,0.587297260761261
translation,21,5,model,vector space search,during,inference time,vector space search during inference time,0.6775521039962769
translation,21,6,model,learning method,called,cross momentum contrastive learning ( xmoco ),learning method called cross momentum contrastive learning ( xmoco ),0.6569234728813171
translation,21,6,model,dualencoder model,for,query - passage matching,dualencoder model for query - passage matching,0.5694551467895508
translation,21,27,model,contrastive learning method,called,cross momentum contrastive learning ( xmoco ),contrastive learning method called cross momentum contrastive learning ( xmoco ),0.6605167388916016
translation,21,27,model,model,propose,contrastive learning method,model propose contrastive learning method,0.6437715291976929
translation,21,28,model,xmoco,employs,two sets of fast / slow encoders,xmoco employs two sets of fast / slow encoders,0.672661542892456
translation,21,28,model,xmoco,jointly optimizes,question - passage and passage -question matching tasks,xmoco jointly optimizes question - passage and passage -question matching tasks,0.6881272792816162
translation,21,28,model,model,has,xmoco,model has xmoco,0.6242291331291199
translation,21,103,model,question and passage encoders,utilize,two sets of different parameters,question and passage encoders utilize two sets of different parameters,0.5633253455162048
translation,21,103,model,question and passage encoders,initialized from,same bert - base model,question and passage encoders initialized from same bert - base model,0.6830536127090454
translation,21,103,model,model,has,question and passage encoders,model has question and passage encoders,0.5680192112922668
translation,21,166,results,our model,out-performs,bm25 and dpr baselins,our model out-performs bm25 and dpr baselins,0.7401910424232483
translation,21,166,results,bm25 and dpr baselins,in,most settings,bm25 and dpr baselins in most settings,0.6205257773399353
translation,21,166,results,bm25 and dpr baselins,except,squad,bm25 and dpr baselins except squad,0.7167776226997375
translation,21,166,results,top - 20 and top - 100 accuracy,except,squad,top - 20 and top - 100 accuracy except squad,0.6870023608207703
translation,21,166,results,xmoco,slightly worse than,bm25,xmoco slightly worse than bm25,0.7034531831741333
translation,21,166,results,results,has,our model,results has our model,0.5871725678443909
translation,21,169,results,large number,of,negative samples,large number of negative samples,0.608180046081543
translation,21,169,results,large number,leads to,better retrieval model,large number leads to better retrieval model,0.6404045820236206
translation,21,169,results,negative samples,in,xmoco,negative samples in xmoco,0.6086580753326416
translation,21,169,results,results,using,large number,results using large number,0.6791403889656067
translation,21,171,results,bm25 and model scores,does not bring,consistent improvement,bm25 and model scores does not bring consistent improvement,0.6496032476425171
translation,21,171,results,bm25 and model scores,except for,squad dataset,bm25 and model scores except for squad dataset,0.6387245059013367
translation,21,171,results,xmoco 's performance,is,significantly better,xmoco 's performance is significantly better,0.5824595093727112
translation,21,171,results,significantly better,than,bm25,significantly better than bm25,0.6228920817375183
translation,21,171,results,significantly better,except for,squad dataset,significantly better except for squad dataset,0.6402023434638977
translation,21,171,results,bm25,except for,squad dataset,bm25 except for squad dataset,0.635828971862793
translation,21,171,results,results,Linearly adding,bm25 and model scores,results Linearly adding bm25 and model scores,0.6367027759552002
translation,21,172,results,training data,brings,improvement,training data brings improvement,0.6542171835899353
translation,21,172,results,improvement,on,smaller datasets,improvement on smaller datasets,0.5152512788772583
translation,21,172,results,results,on,larger datasets,results on larger datasets,0.5121423006057739
translation,21,172,results,larger datasets,due to,domain differences,larger datasets due to domain differences,0.6556409597396851
translation,21,172,results,hurts,has,results,hurts has results,0.6108469367027283
translation,21,172,results,results,combining,training data,results combining training data,0.6561784148216248
translation,21,194,results,results,from,xmoco,results from xmoco,0.5336139798164368
translation,21,194,results,xmoco,are,generally better,xmoco are generally better,0.6067020893096924
translation,21,194,results,generally better,in,most cases,generally better in most cases,0.5521849989891052
translation,21,194,results,results,from,xmoco,results from xmoco,0.5336139798164368
translation,22,209,ablation-analysis,learned,has,cost,learned has cost,0.5851715803146362
translation,22,209,ablation-analysis,cost,has,decreases a lot,cost has decreases a lot,0.5855766534805298
translation,22,209,ablation-analysis,ablation analysis,replace,learned,ablation analysis replace learned,0.6561527848243713
translation,22,103,baselines,f d,is,dense rf,f d is dense rf,0.7329445481300354
translation,22,103,baselines,f d,implemented as,"dpr ( karpukhin et al. , 2020 )","f d implemented as dpr ( karpukhin et al. , 2020 )",0.589816153049469
translation,22,103,baselines,dense rf,implemented as,"mdr ( xiong et al. , 2021 )","dense rf implemented as mdr ( xiong et al. , 2021 )",0.6039298176765442
translation,22,103,baselines,dense rf,implemented as,"dpr ( karpukhin et al. , 2020 )","dense rf implemented as dpr ( karpukhin et al. , 2020 )",0.6055436730384827
translation,22,103,baselines,"mdr ( xiong et al. , 2021 )",for,multihop questions,"mdr ( xiong et al. , 2021 ) for multihop questions",0.6523425579071045
translation,22,103,baselines,"dpr ( karpukhin et al. , 2020 )",for,single - hop questions,"dpr ( karpukhin et al. , 2020 ) for single - hop questions",0.6153988242149353
translation,22,103,baselines,baselines,has,f d,baselines has f d,0.6154009103775024
translation,22,175,experiments,sparse retrieval,index,all passages,sparse retrieval index all passages,0.6283467411994934
translation,22,175,experiments,sparse retrieval,implement,bm25,sparse retrieval implement bm25,0.6038721203804016
translation,22,175,experiments,sparse retrieval,leverage,trained passage encoder and query encoder,sparse retrieval leverage trained passage encoder and query encoder,0.6923864483833313
translation,22,175,experiments,sparse retrieval,index,all passage vectors,sparse retrieval index all passage vectors,0.628920316696167
translation,22,175,experiments,all passages,in,corpus,all passages in corpus,0.5090680122375488
translation,22,175,experiments,all passages,with,elasticsearch,all passages with elasticsearch,0.5897744297981262
translation,22,175,experiments,corpus,with,elasticsearch,corpus with elasticsearch,0.6469219923019409
translation,22,175,experiments,bm25,For,dense retrieval,bm25 For dense retrieval,0.6230178475379944
translation,22,175,experiments,dense retrieval,leverage,trained passage encoder and query encoder,dense retrieval leverage trained passage encoder and query encoder,0.7112036943435669
translation,22,175,experiments,trained passage encoder and query encoder,from,karpukhin et al . ( 2020 ),trained passage encoder and query encoder from karpukhin et al . ( 2020 ),0.5379077196121216
translation,22,175,experiments,all passage vectors,using,"faiss ( johnson et al. , 2019 )","all passage vectors using faiss ( johnson et al. , 2019 )",0.6283297538757324
translation,22,178,hyperparameters,"electra ( clark et al. , 2020 )",as,initializations,"electra ( clark et al. , 2020 ) as initializations",0.500400185585022
translation,22,178,hyperparameters,initializations,for,encoders,initializations for encoders,0.6350722312927246
translation,22,178,hyperparameters,hyperparameters,use,"electra ( clark et al. , 2020 )","hyperparameters use electra ( clark et al. , 2020 )",0.6022675633430481
translation,22,182,hyperparameters,batch size,is,32,batch size is 32,0.6284153461456299
translation,22,182,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,22,183,hyperparameters,adam optimization,with,learning rate 2 ? 10 ?5,adam optimization with learning rate 2 ? 10 ?5,0.6591773629188538
translation,22,183,hyperparameters,hyperparameters,use,adam optimization,hyperparameters use adam optimization,0.632912814617157
translation,22,7,model,strategy,for,open-domain question answering,strategy for open-domain question answering,0.6093114614486694
translation,22,7,model,strategy,namely,aiso,strategy namely aiso,0.7396712899208069
translation,22,7,model,model,propose,novel adaptive,model propose novel adaptive,0.7367510199546814
translation,22,7,model,model,propose,strategy,model propose strategy,0.6565228700637817
translation,22,8,model,whole retrieval and answer process,modeled as,partially observed markov decision process,whole retrieval and answer process modeled as partially observed markov decision process,0.6508228182792664
translation,22,8,model,partially observed markov decision process,where,three types of retrieval operations,partially observed markov decision process where three types of retrieval operations,0.56380695104599
translation,22,8,model,partially observed markov decision process,where,one answer operation,partially observed markov decision process where one answer operation,0.5928100347518921
translation,22,8,model,three types of retrieval operations,e.g.,bm25,three types of retrieval operations e.g. bm25,0.6806333661079407
translation,22,8,model,three types of retrieval operations,e.g.,dpr,three types of retrieval operations e.g. dpr,0.6776701211929321
translation,22,8,model,three types of retrieval operations,e.g.,hyperlink,three types of retrieval operations e.g. hyperlink,0.6311052441596985
translation,22,8,model,three types of retrieval operations,e.g.,one answer operation,three types of retrieval operations e.g. one answer operation,0.6739336848258972
translation,22,8,model,one answer operation,defined as,actions,one answer operation defined as actions,0.5935742855072021
translation,22,8,model,model,has,whole retrieval and answer process,model has whole retrieval and answer process,0.5313361287117004
translation,22,27,model,adaptive information - seeking approach,for,open-domain qa,adaptive information - seeking approach for open-domain qa,0.6213502287864685
translation,22,27,model,adaptive information - seeking approach,namely,aiso,adaptive information - seeking approach namely aiso,0.6856896281242371
translation,22,27,model,model,propose,adaptive information - seeking approach,model propose adaptive information - seeking approach,0.6587375998497009
translation,22,34,model,policy,in,imitation learning,policy in imitation learning,0.5604418516159058
translation,22,34,model,imitation learning,by cloning,behavior,imitation learning by cloning behavior,0.7592329978942871
translation,22,34,model,behavior,of,oracle online,behavior of oracle online,0.5640981793403625
translation,22,228,model,open-domain qa task,as,pomdp,open-domain qa task as pomdp,0.5070733428001404
translation,22,228,model,pomdp,where,environment,pomdp where environment,0.6469519734382629
translation,22,228,model,environment,contains,large corpus,environment contains large corpus,0.6045351624488831
translation,22,228,model,agent,sequentially select,retrieval function,agent sequentially select retrieval function,0.7291130423545837
translation,22,228,model,agent,reformulate,query,agent reformulate query,0.7523584365844727
translation,22,228,model,query,to collect,evidence,query to collect evidence,0.7107229828834534
translation,22,228,model,model,models,open-domain qa task,model models open-domain qa task,0.7350612282752991
translation,22,194,results,perform better,than,pipeline methods,perform better than pipeline methods,0.592520534992218
translation,22,194,results,almost all the iterative approaches,has,perform better,almost all the iterative approaches has perform better,0.6085520386695862
translation,22,194,results,proposed adaptive information - seeking approach,has,aiso large,proposed adaptive information - seeking approach has aiso large,0.6213735938072205
translation,22,194,results,aiso large,has,outperforms,aiso large has outperforms,0.6276691555976868
translation,22,194,results,outperforms,has,all previous methods,outperforms has all previous methods,0.573925256729126
translation,22,195,results,some baselines,that use,large version of pretrained language models,some baselines that use large version of pretrained language models,0.606578528881073
translation,22,195,results,large version of pretrained language models,such as,hopretriever,large version of pretrained language models such as hopretriever,0.5621468424797058
translation,22,195,results,large version of pretrained language models,such as,grr,large version of pretrained language models such as grr,0.5776257514953613
translation,22,195,results,large version of pretrained language models,such as,irrr,large version of pretrained language models such as irrr,0.5718084573745728
translation,22,195,results,large version of pretrained language models,such as,ddrqa,large version of pretrained language models such as ddrqa,0.5915344953536987
translation,22,195,results,large version of pretrained language models,such as,mdr,large version of pretrained language models such as mdr,0.5747615694999695
translation,22,195,results,aiso base model,has,outperforms,aiso base model has outperforms,0.6452882289886475
translation,22,195,results,outperforms,has,some baselines,outperforms has some baselines,0.6103556156158447
translation,22,195,results,results,has,aiso base model,results has aiso base model,0.5562601685523987
translation,22,200,results,outperforms,across,evaluation metrics,outperforms across evaluation metrics,0.6575023531913757
translation,22,200,results,all existing methods,across,evaluation metrics,all existing methods across evaluation metrics,0.6136778593063354
translation,22,200,results,all existing methods,on,hotpotqa fullwiki and squad open benchmarks,all existing methods on hotpotqa fullwiki and squad open benchmarks,0.440717488527298
translation,22,200,results,evaluation metrics,on,hotpotqa fullwiki and squad open benchmarks,evaluation metrics on hotpotqa fullwiki and squad open benchmarks,0.440424382686615
translation,22,200,results,aiso,has,outperforms,aiso has outperforms,0.6395894885063171
translation,22,200,results,outperforms,has,all existing methods,outperforms has all existing methods,0.5677774548530579
translation,22,200,results,results,has,question answering,results has question answering,0.5357730984687805
translation,22,229,results,aiso,achieves,stateof - the- art results,aiso achieves stateof - the- art results,0.6675780415534973
translation,22,229,results,stateof - the- art results,on,two public datasets,stateof - the- art results on two public datasets,0.4765886962413788
translation,22,229,results,results,has,aiso,results has aiso,0.5918926000595093
translation,23,72,ablation-analysis,one epoch,improves,results,one epoch improves results,0.7385550737380981
translation,23,72,ablation-analysis,results,by,more than 0.3 %,results by more than 0.3 %,0.5675797462463379
translation,23,72,ablation-analysis,ablation analysis,trained for,one epoch,ablation analysis trained for one epoch,0.7766249775886536
translation,23,77,ablation-analysis,more efficient domain adaptation,with,nb - mlm,more efficient domain adaptation with nb - mlm,0.6513762474060059
translation,23,77,ablation-analysis,more efficient domain adaptation,reduces,difference,more efficient domain adaptation reduces difference,0.6775000095367432
translation,23,77,ablation-analysis,difference,to,0.2 %,difference to 0.2 %,0.5357705950737
translation,23,77,ablation-analysis,ablation analysis,has,more efficient domain adaptation,ablation analysis has more efficient domain adaptation,0.5249918699264526
translation,23,59,hyperparameters,validation,randomly selected,5 k positive and 5 k negative examples,validation randomly selected 5 k positive and 5 k negative examples,0.6593404412269592
translation,23,59,hyperparameters,hyperparameters,For,validation,hyperparameters For validation,0.5990188121795654
translation,23,60,hyperparameters,domain and task adaptation,used,batch size,domain and task adaptation used batch size,0.6025341153144836
translation,23,60,hyperparameters,batch size,of,1024,batch size of 1024,0.6398507952690125
translation,23,60,hyperparameters,fine-tuned,with,batch size,fine-tuned with batch size,0.6441797018051147
translation,23,60,hyperparameters,batch size,of,32,batch size of 32,0.6741614937782288
translation,23,60,hyperparameters,hyperparameters,For,domain and task adaptation,hyperparameters For domain and task adaptation,0.5491582155227661
translation,23,61,hyperparameters,learning rate,of,2e - 4,learning rate of 2e - 4,0.6387818455696106
translation,23,61,hyperparameters,learning rate,of,1e - 4,learning rate of 1e - 4,0.6311931014060974
translation,23,61,hyperparameters,learning rate,of,1e - 5,learning rate of 1e - 5,0.6323861479759216
translation,23,61,hyperparameters,2e - 4,for,domain adaptation,2e - 4 for domain adaptation,0.6220231056213379
translation,23,61,hyperparameters,1e - 4,for,task adaptation,1e - 4 for task adaptation,0.6167691946029663
translation,23,61,hyperparameters,1e - 5,for,final fine-tuning,1e - 5 for final fine-tuning,0.6362829208374023
translation,23,61,hyperparameters,hyperparameters,set,learning rate,hyperparameters set learning rate,0.5994082689285278
translation,23,8,model,technique,for,more efficient adaptation,technique for more efficient adaptation,0.6458348631858826
translation,23,8,model,more efficient adaptation,focuses on,predicting words,more efficient adaptation focuses on predicting words,0.7043801546096802
translation,23,8,model,predicting words,with,large weights,predicting words with large weights,0.6512245535850525
translation,23,8,model,large weights,of,naive bayes classifier,large weights of naive bayes classifier,0.5780720114707947
translation,23,8,model,naive bayes classifier,trained for,task at hand,naive bayes classifier trained for task at hand,0.788340151309967
translation,23,8,model,model,propose,technique,model propose technique,0.6912915110588074
translation,23,16,model,method,for,more efficient mlm adaptation,method for more efficient mlm adaptation,0.6295204758644104
translation,23,16,model,model,propose,method,model propose method,0.6280754208564758
translation,23,30,results,roberta,enhances,bert,roberta enhances bert,0.5494688749313354
translation,23,30,results,roberta,selecting,different target words,roberta selecting different target words,0.7372971177101135
translation,23,30,results,bert,by,pre-training,bert by pre-training,0.6439120769500732
translation,23,30,results,bert,getting rid of,next sentence prediction ( nsp ) task,bert getting rid of next sentence prediction ( nsp ) task,0.644635021686554
translation,23,30,results,longer,on,ten times larger corpora,longer on ten times larger corpora,0.5296316742897034
translation,23,30,results,next sentence prediction ( nsp ) task,during,pre-training,next sentence prediction ( nsp ) task during pre-training,0.6576100587844849
translation,23,30,results,different target words,to be,masked,different target words to be masked,0.5927153825759888
translation,23,30,results,pre-training,has,longer,pre-training has longer,0.5985509753227234
translation,23,30,results,each epoch,has,dynamic masking,each epoch has dynamic masking,0.597472071647644
translation,23,30,results,results,has,roberta,results has roberta,0.530095100402832
translation,23,36,results,our method,leveraging,large data,our method leveraging large data,0.7108040452003479
translation,23,36,results,more efficiently,makes,domain adaptation,more efficiently makes domain adaptation,0.6454546451568604
translation,23,36,results,domain adaptation,comparable to,task adaptation,domain adaptation comparable to task adaptation,0.6180518269538879
translation,23,36,results,combination,is,significantly better,combination is significantly better,0.5828802585601807
translation,23,36,results,large data,has,more efficiently,large data has more efficiently,0.5678417086601257
translation,23,36,results,results,find that,our method,results find that our method,0.6377817392349243
translation,23,64,results,our nb - mlm model,significantly helps for,domain adaptation,our nb - mlm model significantly helps for domain adaptation,0.7096347212791443
translation,23,64,results,domain adaptation,on,imdb,domain adaptation on imdb,0.5416913628578186
translation,23,64,results,results,has,our nb - mlm model,results has our nb - mlm model,0.5330774784088135
translation,23,67,results,improvements,from,nb - mlm,improvements from nb - mlm,0.6198707818984985
translation,23,67,results,nb - mlm,are,small but consistent,nb - mlm are small but consistent,0.586183488368988
translation,23,67,results,yelp,has,improvements,yelp has improvements,0.6260426044464111
translation,23,67,results,results,For,yelp,results For yelp,0.6341710686683655
translation,23,71,results,domain adaptation,with,nb - mlm,domain adaptation with nb - mlm,0.6471427083015442
translation,23,71,results,domain adaptation,obtains,results,domain adaptation obtains results,0.5867282748222351
translation,23,71,results,results,similar to,uniform mlm,results similar to uniform mlm,0.6781432032585144
translation,23,71,results,uniform mlm,in,5x fewer training steps and data,uniform mlm in 5x fewer training steps and data,0.5378034114837646
translation,23,71,results,imdb,has,domain adaptation,imdb has domain adaptation,0.5435937643051147
translation,23,71,results,results,For,imdb,results For imdb,0.657120406627655
translation,23,73,results,nb - mlm,gives,much smaller improvement,nb - mlm gives much smaller improvement,0.6327693462371826
translation,23,73,results,task adaptation,has,nb - mlm,task adaptation has nb - mlm,0.5680142045021057
translation,23,73,results,results,For,task adaptation,results For task adaptation,0.5286756157875061
translation,23,74,results,task adaptation,with,uniform mlm,task adaptation with uniform mlm,0.6500149369239807
translation,23,74,results,domain adaptation,that employs,much more data,domain adaptation that employs much more data,0.6375723481178284
translation,23,74,results,much more data,by,almost 0.5 %,much more data by almost 0.5 %,0.5805325508117676
translation,23,74,results,uniform mlm,has,outperforms,uniform mlm has outperforms,0.6258792877197266
translation,23,74,results,outperforms,has,domain adaptation,outperforms has domain adaptation,0.6127426028251648
translation,23,78,results,domain adaptation,followed by,task adaptation,domain adaptation followed by task adaptation,0.6005827188491821
translation,23,78,results,task adaptation,results in,best final performance,task adaptation results in best final performance,0.6419297456741333
translation,23,78,results,results,using,domain adaptation,results using domain adaptation,0.5978401303291321
translation,23,79,results,nb - mlm,gives,0.2 % improvement,nb - mlm gives 0.2 % improvement,0.6150553822517395
translation,23,79,results,nb - mlm,gives,0.1 %,nb - mlm gives 0.1 %,0.5813926458358765
translation,23,79,results,0.2 % improvement,for,short adaptation,0.2 % improvement for short adaptation,0.5954150557518005
translation,23,79,results,0.1 %,for,long adaptation,0.1 % for long adaptation,0.6159226894378662
translation,24,18,baselines,) language models,experimented for,subsystems,) language models experimented for subsystems,0.7297949194908142
translation,24,18,baselines,subsystems,of,cascaded system,subsystems of cascaded system,0.5978049635887146
translation,24,18,baselines,three pretrained transformer - based ( vaswani,has,) language models,three pretrained transformer - based ( vaswani has ) language models,0.5327122211456299
translation,24,18,baselines,baselines,has,three pretrained transformer - based ( vaswani,baselines has three pretrained transformer - based ( vaswani,0.5904808044433594
translation,24,74,hyperparameters,subsystem,fine-tuned for,10 epochs,subsystem fine-tuned for 10 epochs,0.7592382431030273
translation,24,74,hyperparameters,subsystem,employed,base language model variants,subsystem employed base language model variants,0.6812498569488525
translation,24,74,hyperparameters,subsystem,employed,base language model variants,subsystem employed base language model variants,0.6812498569488525
translation,24,74,hyperparameters,subsystems,employed,large language model variants,subsystems employed large language model variants,0.6435797214508057
translation,24,74,hyperparameters,subsystems,employed,base language model variants,subsystems employed base language model variants,0.6752822995185852
translation,24,74,hyperparameters,subsystems,employed,base language model variants,subsystems employed base language model variants,0.6752822995185852
translation,24,74,hyperparameters,large language model variants,trained with,batch size,large language model variants trained with batch size,0.70131915807724
translation,24,74,hyperparameters,batch size,of,2,batch size of 2,0.6764397025108337
translation,24,74,hyperparameters,2,due to,computational constraints,2 due to computational constraints,0.6383723020553589
translation,24,74,hyperparameters,subsystems,employed,base language model variants,subsystems employed base language model variants,0.6752822995185852
translation,24,74,hyperparameters,base language model variants,used,batch size,base language model variants used batch size,0.6129881739616394
translation,24,74,hyperparameters,batch size,of,8,batch size of 8,0.6920418739318848
translation,24,74,hyperparameters,hyperparameters,has,subsystem,hyperparameters has subsystem,0.5571610927581787
translation,24,77,hyperparameters,adam optimizer,training,bilstm networks,adam optimizer training bilstm networks,0.6682442426681519
translation,24,77,hyperparameters,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,24,77,hyperparameters,bilstm networks,of,second subtask,bilstm networks of second subtask,0.5246554017066956
translation,24,77,hyperparameters,learning rate,of,1e - 4,learning rate of 1e - 4,0.6311931014060974
translation,24,77,hyperparameters,hyperparameters,has,adam optimizer,hyperparameters has adam optimizer,0.5012347102165222
translation,24,78,hyperparameters,models,trained for,25 epochs,models trained for 25 epochs,0.7763769030570984
translation,24,78,hyperparameters,25 epochs,using,batch size,25 epochs using batch size,0.5911985635757446
translation,24,78,hyperparameters,batch size,of,16,batch size of 16,0.6842944622039795
translation,24,78,hyperparameters,hyperparameters,trained for,25 epochs,hyperparameters trained for 25 epochs,0.6922116875648499
translation,24,78,hyperparameters,hyperparameters,has,models,hyperparameters has models,0.5447477102279663
translation,24,79,hyperparameters,lstm cells,used,hidden size,lstm cells used hidden size,0.5892115235328674
translation,24,79,hyperparameters,hidden size,of,64,hidden size of 64,0.667656421661377
translation,24,79,hyperparameters,hidden size,with,embedding,hidden size with embedding,0.6734603643417358
translation,24,79,hyperparameters,embedding,of,32 dimensions,embedding of 32 dimensions,0.5877174735069275
translation,24,79,hyperparameters,32 dimensions,for,characters,32 dimensions for characters,0.6382203102111816
translation,24,79,hyperparameters,lstm cells,has,two times,lstm cells has two times,0.5861584544181824
translation,24,79,hyperparameters,hyperparameters,stacked,lstm cells,hyperparameters stacked lstm cells,0.6964383125305176
translation,24,17,model,cascaded system,to solve,stated problem,cascaded system to solve stated problem,0.7180809378623962
translation,24,17,model,subsystem,identifies,quantities,subsystem identifies quantities,0.6757284998893738
translation,24,17,model,subsystem,classifies,value modifiers,subsystem classifies value modifiers,0.7556084990501404
translation,24,17,model,quantities,in,input text,quantities in input text,0.5347708463668823
translation,24,17,model,subsystem,classifies,value modifiers,subsystem classifies value modifiers,0.7556084990501404
translation,24,17,model,subsystem,extracts,measurement unit,subsystem extracts measurement unit,0.6346318125724792
translation,24,17,model,subsystem,finds,"appropriate measured entities , measured properties , and qualifiers","subsystem finds appropriate measured entities , measured properties , and qualifiers",0.5875852108001709
translation,24,17,model,subsystem,extracts,measurement unit,subsystem extracts measurement unit,0.6346318125724792
translation,24,17,model,subsystem,finds,"appropriate measured entities , measured properties , and qualifiers","subsystem finds appropriate measured entities , measured properties , and qualifiers",0.5875852108001709
translation,24,17,model,model,created,cascaded system,model created cascaded system,0.6044471263885498
translation,24,100,model,all the five subtasks,8th task of,semeval - 2021 competition,all the five subtasks 8th task of semeval - 2021 competition,0.8079801797866821
translation,24,100,model,all the five subtasks,in,cascaded manner,all the five subtasks in cascaded manner,0.5506507754325867
translation,24,101,model,quantities,identified as,sequence tagging task,quantities identified as sequence tagging task,0.5274364948272705
translation,24,101,model,sequence tagging task,by using,pretrained language model,sequence tagging task by using pretrained language model,0.560813844203949
translation,24,101,model,pretrained language model,with,crf layer,pretrained language model with crf layer,0.5921059250831604
translation,24,101,model,model,has,quantities,model has quantities,0.5313446521759033
translation,24,81,results,scibert,obtained,highest f1 - score,scibert obtained highest f1 - score,0.640265703201294
translation,24,81,results,highest f1 - score,on,quantity identification,highest f1 - score on quantity identification,0.5157270431518555
translation,24,81,results,highest f1 - score,on,joint entity and relation extraction,highest f1 - score on joint entity and relation extraction,0.45771417021751404
translation,24,82,results,model,achieved,reasonable performance,model achieved reasonable performance,0.7291451692581177
translation,24,82,results,reasonable performance,on,value modifier classification,reasonable performance on value modifier classification,0.5174363255500793
translation,24,82,results,reasonable performance,on,value modifier classification,reasonable performance on value modifier classification,0.5174363255500793
translation,24,82,results,95.75 % f1 - score,on,unit extraction,95.75 % f1 - score on unit extraction,0.5205366015434265
translation,24,82,results,88.94 % f1 - score,on,value modifier classification,88.94 % f1 - score on value modifier classification,0.5099015831947327
translation,24,82,results,second subtasks,has,model,second subtasks has model,0.5966929793357849
translation,24,82,results,reasonable performance,has,95.75 % f1 - score,reasonable performance has 95.75 % f1 - score,0.5448063015937805
translation,24,84,results,roberta,obtained,highest scores,roberta obtained highest scores,0.6576499342918396
translation,24,84,results,highest scores,as,whole system,highest scores as whole system,0.5078639984130859
translation,24,84,results,highest scores,with,overlap f1 - score,highest scores with overlap f1 - score,0.6543809175491333
translation,24,84,results,overlap f1 - score,of,39,overlap f1 - score of 39,0.6466315388679504
translation,24,84,results,39,with,over 6 % and over 3 %,39 with over 6 % and over 3 %,0.6920689940452576
translation,25,143,ablation-analysis,qa f1,on,span-type questions,qa f1 on span-type questions,0.5485763549804688
translation,25,143,ablation-analysis,qa f1,drops,1.2 points,qa f1 drops 1.2 points,0.7611674666404724
translation,25,143,ablation-analysis,ablation analysis,notice,qa f1,ablation analysis notice qa f1,0.7175753712654114
translation,25,144,ablation-analysis,qa f1,on,span,qa f1 on span,0.6332798004150391
translation,25,144,ablation-analysis,ablation analysis,removed,auxiliary loss,ablation analysis removed auxiliary loss,0.7298689484596252
translation,25,151,ablation-analysis,auxiliary loss,on,subset of invalid contexts,auxiliary loss on subset of invalid contexts,0.5210394859313965
translation,25,151,ablation-analysis,subset of invalid contexts,during,marginalization,subset of invalid contexts during marginalization,0.6680330038070679
translation,25,151,ablation-analysis,1.4 points decrease,in,qa performance,1.4 points decrease in qa performance,0.5372918844223022
translation,25,151,ablation-analysis,ablation analysis,Without,auxiliary loss,ablation analysis Without auxiliary loss,0.7407543063163757
translation,25,115,experimental-setup,"numnet + ( ran et al. , 2019 )",for,iirc,"numnet + ( ran et al. , 2019 ) for iirc",0.6301563382148743
translation,25,115,experimental-setup,bert - wwm,for,hotpotqa,bert - wwm for hotpotqa,0.7140348553657532
translation,25,115,experimental-setup,experimental setup,use,"numnet + ( ran et al. , 2019 )","experimental setup use numnet + ( ran et al. , 2019 )",0.5806993842124939
translation,25,115,experimental-setup,experimental setup,use,bert - wwm,experimental setup use bert - wwm,0.5896125435829163
translation,25,127,experimental-setup,weight,for,invalid context loss,weight for invalid context loss,0.589981198310852
translation,25,127,experimental-setup,weight,use,0.5,weight use 0.5,0.6323238611221313
translation,25,127,experimental-setup,weight,use,0,weight use 0,0.66606605052948
translation,25,127,experimental-setup,invalid context loss,use,0.5,invalid context loss use 0.5,0.6191609501838684
translation,25,127,experimental-setup,invalid context loss,use,0,invalid context loss use 0,0.6470220685005188
translation,25,127,experimental-setup,0.5,for,iirc,0.5 for iirc,0.651451051235199
translation,25,127,experimental-setup,0,for,hotpotqa,0 for hotpotqa,0.6927129626274109
translation,25,127,experimental-setup,experimental setup,For,weight,experimental setup For weight,0.5952489972114563
translation,25,128,experimental-setup,memory and storage efficiency,tie,pretrained language model weights,memory and storage efficiency tie pretrained language model weights,0.4828682243824005
translation,25,128,experimental-setup,pretrained language model weights,among,all the components,pretrained language model weights among all the components,0.5669721961021423
translation,25,128,experimental-setup,all the components,in,our joint model,all the components in our joint model,0.5011383891105652
translation,25,128,experimental-setup,experimental setup,For,memory and storage efficiency,experimental setup For memory and storage efficiency,0.5662487149238586
translation,25,129,experimental-setup,30 epochs,for,iirc,30 epochs for iirc,0.6701018214225769
translation,25,129,experimental-setup,5 epochs,for,hotpotqa fullwiki,5 epochs for hotpotqa fullwiki,0.5727614760398865
translation,25,129,experimental-setup,experimental setup,trained for,30 epochs,experimental setup trained for 30 epochs,0.7293474674224854
translation,25,129,experimental-setup,experimental setup,trained for,5 epochs,experimental setup trained for 5 epochs,0.7375892996788025
translation,25,9,model,new parameterization,of,set-valued retrieval,new parameterization of set-valued retrieval,0.5784284472465515
translation,25,9,model,new parameterization,handles,unanswerable queries,new parameterization handles unanswerable queries,0.6774620413780212
translation,25,9,model,new parameterization,show,marginalizing,new parameterization show marginalizing,0.6671581268310547
translation,25,9,model,set-valued retrieval,handles,unanswerable queries,set-valued retrieval handles unanswerable queries,0.6666164398193359
translation,25,9,model,false negatives,in,supporting evidence annotations,false negatives in supporting evidence annotations,0.5134661793708801
translation,25,9,model,mitigate,has,false negatives,mitigate has false negatives,0.6151667833328247
translation,25,9,model,model,develop,new parameterization,model develop new parameterization,0.6444238424301147
translation,25,30,model,probabilities,to,"documents , evidence candidates , and potential answers","probabilities to documents , evidence candidates , and potential answers",0.5428710579872131
translation,25,30,model,probabilities,with,parameterized models,probabilities with parameterized models,0.6381406188011169
translation,25,30,model,"documents , evidence candidates , and potential answers",with,parameterized models,"documents , evidence candidates , and potential answers with parameterized models",0.5739477276802063
translation,25,30,model,potential contexts,by combining,top retrieved evidence,potential contexts by combining top retrieved evidence,0.67442387342453
translation,25,30,model,top retrieved evidence,from,each document,top retrieved evidence from each document,0.545839786529541
translation,25,30,model,top retrieved evidence,allowing,model,top retrieved evidence allowing model,0.7371799349784851
translation,25,30,model,model,to score,false negatives,model to score false negatives,0.6706864237785339
translation,25,30,model,false negatives,has,highly,false negatives has highly,0.635694146156311
translation,25,30,model,model,assign,probabilities,model assign probabilities,0.6947311758995056
translation,25,30,model,model,to score,false negatives,model to score false negatives,0.6706864237785339
translation,25,31,model,retrieval problem,into,document selection,retrieval problem into document selection,0.510100781917572
translation,25,31,model,retrieval problem,into,evidence retrieval,retrieval problem into evidence retrieval,0.5840926170349121
translation,25,11,results,iirc,show that,joint modeling with marginalization,iirc show that joint modeling with marginalization,0.5271527767181396
translation,25,11,results,joint modeling with marginalization,improves,model performance,joint modeling with marginalization improves model performance,0.7063971161842346
translation,25,11,results,joint modeling with marginalization,achieves,new state - of- the - art performance,joint modeling with marginalization achieves new state - of- the - art performance,0.6752150058746338
translation,25,11,results,model performance,by,5.5 f1 points,model performance by 5.5 f1 points,0.5773725509643555
translation,25,11,results,new state - of- the - art performance,of,50.5 f1,new state - of- the - art performance of 50.5 f1,0.5352919697761536
translation,25,11,results,results,On,iirc,results On iirc,0.5740730166435242
translation,25,33,results,2.8 and 4.8 f1 point improvement,on,iirc and hot-potqa,2.8 and 4.8 f1 point improvement on iirc and hot-potqa,0.5394430160522461
translation,25,33,results,2.8 and 4.8 f1 point improvement,by jointly modeling,proposed set-valued retrieval,2.8 and 4.8 f1 point improvement by jointly modeling proposed set-valued retrieval,0.7127357721328735
translation,25,33,results,iirc and hot-potqa,by jointly modeling,proposed set-valued retrieval,iirc and hot-potqa by jointly modeling proposed set-valued retrieval,0.8136075735092163
translation,25,33,results,further 2.7 and 4.1 f1 point improvement,by using,retrieval marginalization,further 2.7 and 4.1 f1 point improvement by using retrieval marginalization,0.6803717613220215
translation,25,33,results,results,see,2.8 and 4.8 f1 point improvement,results see 2.8 and 4.8 f1 point improvement,0.5833294987678528
translation,25,34,results,final result,of,50.5 f1,final result of 50.5 f1,0.5623910427093506
translation,25,34,results,50.5 f1,on,test set,50.5 f1 on test set,0.5403269529342651
translation,25,34,results,test set,of,iirc,test set of iirc,0.6753286123275757
translation,25,34,results,iirc,represents,new state - of- the- art,iirc represents new state - of- the- art,0.5736510753631592
translation,25,34,results,results,has,final result,results has final result,0.5664030909538269
translation,25,134,results,proposed joint model with marginalization,outperforms,pipeline model,proposed joint model with marginalization outperforms pipeline model,0.6780036687850952
translation,25,134,results,pipeline model,by,5.2 and 4.8 points,pipeline model by 5.2 and 4.8 points,0.5769426226615906
translation,25,134,results,5.2 and 4.8 points,for,qa exact match and f1 score,5.2 and 4.8 points for qa exact match and f1 score,0.5982457399368286
translation,25,134,results,results,see that,proposed joint model with marginalization,results see that proposed joint model with marginalization,0.6859323978424072
translation,25,141,results,our proposed method,yields,large performance gains,our proposed method yields large performance gains,0.6898501515388489
translation,25,141,results,our proposed method,with,binary and numerical answers,our proposed method with binary and numerical answers,0.6263795495033264
translation,25,141,results,large performance gains,on,unanswerable questions,large performance gains on unanswerable questions,0.556063175201416
translation,25,141,results,large performance gains,with,binary and numerical answers,large performance gains with binary and numerical answers,0.6376062035560608
translation,25,141,results,results,has,our proposed method,results has our proposed method,0.5673112869262695
translation,25,146,results,marginalization,improves,final qa f1 performance,marginalization improves final qa f1 performance,0.7038303017616272
translation,25,146,results,final qa f1 performance,by,2.7 points,final qa f1 performance by 2.7 points,0.5371834635734558
translation,25,146,results,results,training with,marginalization,results training with marginalization,0.7328208684921265
translation,25,159,results,general effectiveness,of,retrieval marginalization,general effectiveness of retrieval marginalization,0.5935481190681458
translation,25,159,results,general effectiveness,of,joint modeling,general effectiveness of joint modeling,0.6148868203163147
translation,25,159,results,joint modeling,on,hotpotqa fullwiki,joint modeling on hotpotqa fullwiki,0.5350889563560486
translation,25,159,results,results,has,general effectiveness,results has general effectiveness,0.5110824108123779
translation,25,160,results,4.8 qa f1 improvement,with,joint modeling,4.8 qa f1 improvement with joint modeling,0.6111229658126831
translation,25,160,results,further 4.1 points improvement,with,retrieval marginalization,further 4.1 points improvement with retrieval marginalization,0.6641114354133606
translation,25,160,results,results,observe,4.8 qa f1 improvement,results observe 4.8 qa f1 improvement,0.5511947274208069
translation,25,161,results,final qa performance,despite,inferior retrieval scores,final qa performance despite inferior retrieval scores,0.6771469712257385
translation,25,161,results,inferior retrieval scores,evaluated against,annotated supporting evidence,inferior retrieval scores evaluated against annotated supporting evidence,0.756096363067627
translation,25,161,results,joint modeling and retrieval marginalization,has,improve,joint modeling and retrieval marginalization has improve,0.5679391026496887
translation,25,161,results,improve,has,final qa performance,improve has final qa performance,0.5393387079238892
translation,25,170,results,our full model,achieves,qa f1,our full model achieves qa f1,0.7078822255134583
translation,25,170,results,our full model,achieves,em,our full model achieves em,0.7170253992080688
translation,25,170,results,qa f1,of,71.2,qa f1 of 71.2,0.5918639302253723
translation,25,170,results,em,of,58.6,em of 58.6,0.600324273109436
translation,25,170,results,joint modeling and retrieval,has,our full model,joint modeling and retrieval has our full model,0.5695218443870544
translation,25,170,results,results,With,joint modeling and retrieval,results With joint modeling and retrieval,0.608616054058075
translation,26,7,model,generative context selection,for,multi-hop qa,generative context selection for multi-hop qa,0.6140539050102234
translation,26,7,model,model,propose,generative context selection,model propose generative context selection,0.6523185968399048
translation,26,19,model,generative context pair selection model,reasons through,data generation process,generative context pair selection model reasons through data generation process,0.6713467240333557
translation,26,19,model,model,propose,generative context pair selection model,model propose generative context pair selection model,0.6251301169395447
translation,26,20,results,better performance,on,original ( + 2.2 % ) and the adversarial dev set ( + 4.9 % ),better performance on original ( + 2.2 % ) and the adversarial dev set ( + 4.9 % ),0.5173371434211731
translation,26,20,results,proposed passage selection module,has,better performance,proposed passage selection module has better performance,0.5526983737945557
translation,26,20,results,results,show,proposed passage selection module,results show proposed passage selection module,0.6303067207336426
translation,26,54,results,minor improvements,in,passage accuracy,minor improvements in passage accuracy,0.5295134782791138
translation,26,54,results,minor improvements,on using,generative selector,minor improvements on using generative selector,0.6835192441940308
translation,26,54,results,passage accuracy,on using,generative selector,passage accuracy on using generative selector,0.6810452342033386
translation,26,54,results,generative selector,in,wikihop,generative selector in wikihop,0.5231003165245056
translation,26,54,results,results,shows,minor improvements,results shows minor improvements,0.7225786447525024
translation,26,59,results,much higher performance drop ( ? 4 % ),compared to,generative selector ( ? 1 % ),much higher performance drop ( ? 4 % ) compared to generative selector ( ? 1 % ),0.7020958065986633
translation,26,59,results,generative selector ( ? 1 % ),on,"adversarial dev set ( jiang and bansal , 2019 )","generative selector ( ? 1 % ) on adversarial dev set ( jiang and bansal , 2019 )",0.5191380977630615
translation,26,59,results,standard discriminative passage selector,has,much higher performance drop ( ? 4 % ),standard discriminative passage selector has much higher performance drop ( ? 4 % ),0.5453665852546692
translation,26,59,results,results,show,standard discriminative passage selector,results show standard discriminative passage selector,0.625716507434845
translation,26,61,results,decoder,of,generative passage selector,decoder of generative passage selector,0.5284224152565002
translation,26,61,results,generative passage selector,able to generate,multi-hop style questions,generative passage selector able to generate multi-hop style questions,0.7143248319625854
translation,26,61,results,multi-hop style questions,from,pair of contexts,multi-hop style questions from pair of contexts,0.584986686706543
translation,26,61,results,results,shows,decoder,results shows decoder,0.5746163725852966
translation,26,72,results,slight performance improvements,by using,relevant sentences,slight performance improvements by using relevant sentences,0.5733768343925476
translation,26,72,results,relevant sentences,as,additional supervision signal,relevant sentences as additional supervision signal,0.5081493258476257
translation,26,72,results,results,see,slight performance improvements,results see slight performance improvements,0.5798997282981873
translation,27,31,baselines,distdr,starts with,weak retriever,distdr starts with weak retriever,0.6846874356269836
translation,27,31,baselines,baselines,has,distdr,baselines has distdr,0.6329972147941589
translation,27,54,baselines,distdr,trained by,retrieving evidence,distdr trained by retrieving evidence,0.7228947281837463
translation,27,54,baselines,retrieving evidence,from,large corpus,retrieving evidence from large corpus,0.575053334236145
translation,27,54,baselines,large corpus,with,distant supervision,large corpus with distant supervision,0.5906484127044678
translation,27,54,baselines,baselines,has,distdr,baselines has distdr,0.6329972147941589
translation,27,148,baselines,naturalquestions,compare,distdr,naturalquestions compare distdr,0.7068730592727661
translation,27,148,baselines,distdr,use,same model architecture,distdr use same model architecture,0.6288677453994751
translation,27,148,baselines,same model architecture,with,asynchronous negative evidence updates,same model architecture with asynchronous negative evidence updates,0.6601824760437012
translation,27,148,baselines,asynchronous negative evidence updates,during,training,asynchronous negative evidence updates during training,0.7052211165428162
translation,27,148,baselines,baselines,On,naturalquestions,baselines On naturalquestions,0.5561354160308838
translation,27,168,experimental-setup,distdr,for,ten iterations,distdr for ten iterations,0.6039244532585144
translation,27,168,experimental-setup,distdr,using,same hyper-parameters,distdr using same hyper-parameters,0.6732844114303589
translation,27,168,experimental-setup,experimental setup,train,distdr,experimental setup train distdr,0.6912434101104736
translation,27,169,experimental-setup,distdr,on,eight 2080 ti gpus,distdr on eight 2080 ti gpus,0.5260325074195862
translation,27,169,experimental-setup,training,takes,three days,training takes three days,0.6292591094970703
translation,27,169,experimental-setup,experimental setup,run,distdr,experimental setup run distdr,0.7265492081642151
translation,27,8,model,iteratively improves,over,weak retriever,iteratively improves over weak retriever,0.6964631080627441
translation,27,8,model,weak retriever,by alternately finding,evidence,weak retriever by alternately finding evidence,0.7377394437789917
translation,27,8,model,evidence,from,up-to- date model,evidence from up-to- date model,0.6199600100517273
translation,27,8,model,model,to learn,most likely evidence,model to learn most likely evidence,0.6150339841842651
translation,27,8,model,model,introduce,novel approach ( distdr ),model introduce novel approach ( distdr ),0.6482321619987488
translation,27,30,model,computational approach ( distdr ),use,similar techniques,computational approach ( distdr ) use similar techniques,0.6484459638595581
translation,27,30,model,similar techniques,to find,evidence,similar techniques to find evidence,0.6986083388328552
translation,27,30,model,evidence,to answer,given question,evidence to answer given question,0.7350144982337952
translation,27,30,model,model,creates,computational approach ( distdr ),model creates computational approach ( distdr ),0.6512947082519531
translation,27,32,model,evidence,as,latent variable,evidence as latent variable,0.5566388964653015
translation,27,32,model,alternates,using,up-to- date retriever,alternates using up-to- date retriever,0.7254243493080139
translation,27,32,model,alternates,updating,model parameters,alternates updating model parameters,0.7744110226631165
translation,27,32,model,up-to- date retriever,to find,evidence,up-to- date retriever to find evidence,0.6343088150024414
translation,27,32,model,up-to- date retriever,updating,model parameters,up-to- date retriever updating model parameters,0.737902045249939
translation,27,32,model,model parameters,to further encourage,most useful evidence,model parameters to further encourage most useful evidence,0.6303090453147888
translation,27,32,model,most useful evidence,in,next iteration ( m-step ),most useful evidence in next iteration ( m-step ),0.4949522316455841
translation,27,32,model,model,model,evidence,model model evidence,0.8404777646064758
translation,27,32,model,model,develop,hard - em algorithm,model develop hard - em algorithm,0.653777539730072
translation,27,35,model,distdr,provides,iterative feedback,distdr provides iterative feedback,0.6807567477226257
translation,27,35,model,iterative feedback,context of,qa system,iterative feedback context of qa system,0.6913061141967773
translation,27,35,model,iterative feedback,to guide,encoder,iterative feedback to guide encoder,0.704011082649231
translation,27,35,model,encoder,to better find,evidence pieces,encoder to better find evidence pieces,0.7402589321136475
translation,27,35,model,model,has,distdr,model has distdr,0.6469125151634216
translation,27,9,results,distdr,on par with,fully - supervised state - of - theart methods,distdr on par with fully - supervised state - of - theart methods,0.7072718739509583
translation,27,9,results,fully - supervised state - of - theart methods,on,multi-hop and singlehop qa benchmarks,fully - supervised state - of - theart methods on multi-hop and singlehop qa benchmarks,0.5062563419342041
translation,27,9,results,evidence labels,has,distdr,evidence labels has distdr,0.6271371841430664
translation,27,9,results,results,Without using,evidence labels,results Without using evidence labels,0.7043189406394958
translation,27,9,results,results,any,evidence labels,results any evidence labels,0.6569724678993225
translation,27,171,results,results,on,hotpotqa,results on hotpotqa,0.5291514992713928
translation,27,174,results,distdr,slightly better than,beamdr and mdr,distdr slightly better than beamdr and mdr,0.7972787618637085
translation,27,174,results,methods,with,full supervision,methods with full supervision,0.553601086139679
translation,27,174,results,results,has,distdr,results has distdr,0.6189001798629761
translation,27,175,results,same model implementation,with,distant supervision,same model implementation with distant supervision,0.6288859248161316
translation,27,175,results,distdr,competitive to,beamdr,distdr competitive to beamdr,0.6855671405792236
translation,27,175,results,beamdr,on,reader results,beamdr on reader results,0.5367192625999451
translation,27,175,results,same model implementation,has,distdr,same model implementation has distdr,0.61543869972229
translation,27,175,results,distant supervision,has,distdr,distant supervision has distdr,0.6289515495300293
translation,27,175,results,results,using,same model implementation,results using same model implementation,0.7033916711807251
translation,27,176,results,naturalquestions,using,ir,naturalquestions using ir,0.6232656240463257
translation,27,176,results,ir,to find evidence,distant supervision,ir to find evidence distant supervision,0.683148980140686
translation,27,176,results,helpful training signals,confirmed by,small gap,helpful training signals confirmed by small gap,0.6903520226478577
translation,27,176,results,small gap,between,distant and full supervision,small gap between distant and full supervision,0.6577022671699524
translation,27,176,results,results,On,naturalquestions,results On naturalquestions,0.5182446241378784
translation,27,177,results,distdr,beats,weakly - supervised systems,distdr beats weakly - supervised systems,0.7088179588317871
translation,27,177,results,competitive,with,fully - supervised models,competitive with fully - supervised models,0.5740265250205994
translation,27,177,results,distantly - supervised dpr,has,distdr,distantly - supervised dpr has distdr,0.5864163041114807
translation,27,182,results,multiple positive sampling strategies,using,hard em ( top - 1 as positive evidence ),multiple positive sampling strategies using hard em ( top - 1 as positive evidence ),0.6905362010002136
translation,27,182,results,hard em ( top - 1 as positive evidence ),is,slightly better,hard em ( top - 1 as positive evidence ) is slightly better,0.5277799367904663
translation,27,182,results,slightly better,than,randomly sampling,slightly better than randomly sampling,0.590604305267334
translation,27,182,results,randomly sampling,from,top-k positives,randomly sampling from top-k positives,0.5239831209182739
translation,27,182,results,top-k positives,for,m-step update,top-k positives for m-step update,0.6469879150390625
translation,27,182,results,results,compare,multiple positive sampling strategies,results compare multiple positive sampling strategies,0.5721412301063538
translation,27,186,results,distdr,beats,only the answer matching filter,distdr beats only the answer matching filter,0.7326370477676392
translation,27,186,results,distdr,converges,faster,distdr converges faster,0.747440755367279
translation,27,186,results,additional reader filter,has,distdr,additional reader filter has distdr,0.6024707555770874
translation,27,186,results,results,With,additional reader filter,results With additional reader filter,0.6417874693870544
translation,27,192,results,distdr,is,competitive,distdr is competitive,0.7272430062294006
translation,27,192,results,competitive,to,fully -supervised approaches,competitive to fully -supervised approaches,0.5238867998123169
translation,27,192,results,fully -supervised approaches,on,hotpotqa,fully -supervised approaches on hotpotqa,0.5307727456092834
translation,27,192,results,retrieved evidence,has,distdr,retrieved evidence has distdr,0.6245004534721375
translation,27,192,results,results,on,retrieved evidence,results on retrieved evidence,0.42712828516960144
translation,28,154,ablation-analysis,performance,in,subset ph,performance in subset ph,0.6085535883903503
translation,28,154,ablation-analysis,subset ph,achieves,worst,subset ph achieves worst,0.7446592450141907
translation,28,6,experiments,largest-scale chinese multi-choice biomedical qa dataset,collected from,national medical licensing examination,largest-scale chinese multi-choice biomedical qa dataset collected from national medical licensing examination,0.6169242858886719
translation,28,6,experiments,mlec - qa,has,largest-scale chinese multi-choice biomedical qa dataset,mlec - qa has largest-scale chinese multi-choice biomedical qa dataset,0.520220935344696
translation,28,23,experiments,mlec - qa,has,largest-scale chinese multi-choice bqa dataset,mlec - qa has largest-scale chinese multi-choice bqa dataset,0.5128529667854309
translation,28,35,experiments,chinese wikipedia dumps,as,information sources,chinese wikipedia dumps as information sources,0.5067840814590454
translation,28,35,experiments,distributed search and analytics engine,as,document store and document retriever,distributed search and analytics engine as document store and document retriever,0.514196515083313
translation,28,109,experiments,whole chinese wikipedia data,use,distributed search and analytics engine,whole chinese wikipedia data use distributed search and analytics engine,0.5935731530189514
translation,28,109,experiments,distributed search and analytics engine,as,document store and document retriever,distributed search and analytics engine as document store and document retriever,0.514196515083313
translation,28,109,experiments,distributed search and analytics engine,supports,very fast full - text searches,distributed search and analytics engine supports very fast full - text searches,0.6535183787345886
translation,28,109,experiments,document store and document retriever,supports,very fast full - text searches,document store and document retriever supports very fast full - text searches,0.6647143959999084
translation,28,140,hyperparameters,machine reading comprehension models,trained with,12 epochs,machine reading comprehension models trained with 12 epochs,0.7543947696685791
translation,28,140,hyperparameters,machine reading comprehension models,trained with,batch size,machine reading comprehension models trained with batch size,0.7220966219902039
translation,28,140,hyperparameters,initial learning rate,of,2e - 6,initial learning rate of 2e - 6,0.6254706978797913
translation,28,140,hyperparameters,maximum sequence length,of,512,maximum sequence length of 512,0.6141569018363953
translation,28,140,hyperparameters,batch size,of,5,batch size of 5,0.7012521624565125
translation,28,140,hyperparameters,reader,has,machine reading comprehension models,reader has machine reading comprehension models,0.5443452000617981
translation,28,140,hyperparameters,hyperparameters,For,reader,hyperparameters For reader,0.6187366247177124
translation,28,149,results,most retrieved documents,indicate,pm,most retrieved documents indicate pm,0.6620408296585083
translation,28,149,results,pm,with,questions,pm with questions,0.6492488980293274
translation,28,149,results,matching rates,of,em and mm,matching rates of em and mm,0.62708580493927
translation,28,149,results,em and mm,achieve,maximums,em and mm achieve maximums,0.663792610168457
translation,28,149,results,maximums,of,20.83 % ( cwm ) and 50 % ( ph ),maximums of 20.83 % ( cwm ) and 50 % ( ph ),0.5927287340164185
translation,28,149,results,results,has,most retrieved documents,results has most retrieved documents,0.5202216506004333
translation,28,153,results,performance,in,subset cli,performance in subset cli,0.6015939116477966
translation,28,153,results,subset cli,achieves,best,subset cli achieves best,0.7283321619033813
translation,28,153,results,best,as,clinical medicine,best as clinical medicine,0.5697888731956482
translation,28,153,results,different subsets,has,performance,different subsets has performance,0.5923627614974976
translation,28,153,results,results,among,different subsets,results among different subsets,0.6240407228469849
translation,28,155,results,results,has,baseline results,results has baseline results,0.5268478989601135
translation,28,158,results,roberta-wwm-ext-large and bertwwm -ext,perform,better,roberta-wwm-ext-large and bertwwm -ext perform better,0.6327590346336365
translation,28,158,results,better,than,other models,better than other models,0.5785062313079834
translation,28,158,results,other models,on,five subsets,other models on five subsets,0.553888738155365
translation,28,158,results,results,has,roberta-wwm-ext-large and bertwwm -ext,results has roberta-wwm-ext-large and bertwwm -ext,0.5705049633979797
translation,28,160,results,performance,between,kq and cq questions,performance between kq and cq questions,0.6389967203140259
translation,28,160,results,most models,achieve,better performance,most models achieve better performance,0.6037971377372742
translation,28,160,results,better performance,on,cq,better performance on cq,0.5586646795272827
translation,28,160,results,better performance,positively correlated with,cq 's better retrieval performance,better performance positively correlated with cq 's better retrieval performance,0.7054122686386108
translation,28,160,results,performance,has,most models,performance has most models,0.5290466547012329
translation,28,160,results,kq and cq questions,has,most models,kq and cq questions has most models,0.5988560318946838
translation,28,160,results,results,Comparing,performance,results Comparing performance,0.7179481983184814
translation,28,161,results,subset tcm,is,easiest ( 54.95 % ),subset tcm is easiest ( 54.95 % ),0.5327643156051636
translation,28,161,results,easiest ( 54.95 % ),to,answer,easiest ( 54.95 % ) to answer,0.5295649766921997
translation,28,161,results,answer,across,board,answer across board,0.6916053295135498
translation,28,161,results,subset ph,is,hardest ( 40.04 % ),subset ph is hardest ( 40.04 % ),0.5566827654838562
translation,28,161,results,different subsets,has,subset tcm,different subsets has subset tcm,0.643271803855896
translation,28,161,results,results,Among,different subsets,results Among different subsets,0.6240407228469849
translation,28,172,results,best model,achieve,16.88 % higher accuracy,best model achieve 16.88 % higher accuracy,0.6092010140419006
translation,28,172,results,16.88 % higher accuracy,on,test set,16.88 % higher accuracy on test set,0.5267945528030396
translation,28,172,results,16.88 % higher accuracy,than,ours,16.88 % higher accuracy than ours,0.551602840423584
translation,28,172,results,100 % covered materials,has,best model,100 % covered materials has best model,0.5548449158668518
translation,28,172,results,results,with,100 % covered materials,results with 100 % covered materials,0.563822329044342
translation,29,133,ablation-analysis,21.1 and 14.1 f1 points,on,grailqa and webqsp,21.1 and 14.1 f1 points on grailqa and webqsp,0.5740265250205994
translation,29,133,ablation-analysis,checker module,has,performance,checker module has performance,0.5639309883117676
translation,29,133,ablation-analysis,performance,has,drops,performance has drops,0.5993483662605286
translation,29,133,ablation-analysis,drops,has,21.1 and 14.1 f1 points,drops has 21.1 and 14.1 f1 points,0.6050471067428589
translation,29,133,ablation-analysis,ablation analysis,removing,checker module,ablation analysis removing checker module,0.6751741766929626
translation,29,108,hyperparameters,bert,utilize,uncased bert - base model,bert utilize uncased bert - base model,0.5815297961235046
translation,29,108,hyperparameters,uncased bert - base model,from,"transformers library ( wolf et al. , 2020 )","uncased bert - base model from transformers library ( wolf et al. , 2020 )",0.5675539970397949
translation,29,108,hyperparameters,hyperparameters,With respect to,bert,hyperparameters With respect to bert,0.6733400821685791
translation,29,110,hyperparameters,learning rate,set to,1e - 3,learning rate set to 1e - 3,0.7155356407165527
translation,29,110,hyperparameters,1e - 3,except for,bert,1e - 3 except for bert,0.6990224123001099
translation,29,110,hyperparameters,bert,set to,2e - 5,bert set to 2e - 5,0.7494000792503357
translation,29,110,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,29,5,model,retrack,designed as,modular framework,retrack designed as modular framework,0.5139155983924866
translation,29,5,model,modular framework,to maintain,high flexibility,modular framework to maintain high flexibility,0.6234343647956848
translation,29,5,model,model,has,retrack,model has retrack,0.6130422949790955
translation,29,31,model,practical framework,for,large scale kbqa,practical framework for large scale kbqa,0.6156702041625977
translation,29,31,model,retrack,has,practical framework,retrack has practical framework,0.616119384765625
translation,29,31,model,model,present,retrack,model present retrack,0.7363423109054565
translation,29,121,results,re-track,achieves,"improvement ( f1 + 28.5 , em + 24.8 )","re-track achieves improvement ( f1 + 28.5 , em + 24.8 )",0.6233336925506592
translation,29,121,results,"improvement ( f1 + 28.5 , em + 24.8 )",over,previous best transduction - based model bert + transduction,"improvement ( f1 + 28.5 , em + 24.8 ) over previous best transduction - based model bert + transduction",0.5856924057006836
translation,29,121,results,previous best transduction - based model bert + transduction,on,grailqa,previous best transduction - based model bert + transduction on grailqa,0.5683866143226624
translation,29,121,results,re-track,has,significantly outperforms,re-track has significantly outperforms,0.6193079948425293
translation,29,121,results,significantly outperforms,has,"previous sota model bert + ranking ( f1 + 7.3 , em + 7.5 )","significantly outperforms has previous sota model bert + ranking ( f1 + 7.3 , em + 7.5 )",0.5734773874282837
translation,29,121,results,results,has,re-track,results has re-track,0.5907154679298401
translation,29,123,results,models,with,oracle entities,models with oracle entities,0.5903770923614502
translation,29,123,results,predicted entities,has,our model,predicted entities has our model,0.5985901951789856
translation,29,123,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,29,123,results,outperforms,has,previous models,outperforms has previous models,0.6068373918533325
translation,29,123,results,even outperforms,has,models,even outperforms has models,0.6867625713348389
translation,29,123,results,results,Given,predicted entities,results Given predicted entities,0.6589561104774475
translation,29,124,results,performance,of,our model,performance of our model,0.5847885608673096
translation,29,124,results,our model,further boosts,74.7 f1,our model further boosts 74.7 f1,0.6905187964439392
translation,29,124,results,oracle entities,has,performance,oracle entities has performance,0.5797984004020691
translation,29,124,results,results,Given,oracle entities,results Given oracle entities,0.7556377649307251
translation,30,151,ablation-analysis,not fine- tuning,leads to,further dramatic decrease,not fine- tuning leads to further dramatic decrease,0.7010399699211121
translation,30,151,ablation-analysis,bert ( - nf variants ),leads to,further dramatic decrease,bert ( - nf variants ) leads to further dramatic decrease,0.6853693723678589
translation,30,151,ablation-analysis,further dramatic decrease,in,trainable parameters,further dramatic decrease in trainable parameters,0.530387818813324
translation,30,151,ablation-analysis,further dramatic decrease,at the expense of,slightly lower document and snippet map,further dramatic decrease at the expense of slightly lower document and snippet map,0.716062068939209
translation,30,151,ablation-analysis,slightly lower document and snippet map,for,bjpdrmm,slightly lower document and snippet map for bjpdrmm,0.6332641839981079
translation,30,151,ablation-analysis,"7.59 to 6.84 , and 16.82 to 15.77",for,bjpdrmm,"7.59 to 6.84 , and 16.82 to 15.77 for bjpdrmm",0.6265292167663574
translation,30,151,ablation-analysis,not fine- tuning,has,bert ( - nf variants ),not fine- tuning has bert ( - nf variants ),0.6504440903663635
translation,30,151,ablation-analysis,slightly lower document and snippet map,has,"7.59 to 6.84 , and 16.82 to 15.77","slightly lower document and snippet map has 7.59 to 6.84 , and 16.82 to 15.77",0.5400391221046448
translation,30,151,ablation-analysis,ablation analysis,has,not fine- tuning,ablation analysis has not fine- tuning,0.5714953541755676
translation,30,153,ablation-analysis,snippet map,of,bjpdrmm -nf,snippet map of bjpdrmm -nf,0.592757523059845
translation,30,153,ablation-analysis,bjpdrmm -nf,increases from,15.77 to 17.35,bjpdrmm -nf increases from 15.77 to 17.35,0.6879663467407227
translation,30,153,ablation-analysis,document map,increases from,6.84 to 7.42,document map increases from 6.84 to 7.42,0.691935122013092
translation,30,153,ablation-analysis,ablation analysis,has,snippet map,ablation analysis has snippet map,0.5423654913902283
translation,30,157,ablation-analysis,top layers,of,jpdrmm ( sentence pdrmm ),top layers of jpdrmm ( sentence pdrmm ),0.5447863936424255
translation,30,157,ablation-analysis,performance,for,documents and snippets,performance for documents and snippets,0.6406449675559998
translation,30,157,ablation-analysis,performance,both,documents and snippets,performance both documents and snippets,0.6770349144935608
translation,30,157,ablation-analysis,harms,has,performance,harms has performance,0.5827915668487549
translation,30,157,ablation-analysis,ablation analysis,Removing,top layers,ablation analysis Removing top layers,0.7617414593696594
translation,30,8,baselines,two main instantiations,of,architecture,two main instantiations of architecture,0.5879287719726562
translation,30,8,baselines,two main instantiations,based on,posit - drmm ( pdrmm ),two main instantiations based on posit - drmm ( pdrmm ),0.72396320104599
translation,30,8,baselines,two main instantiations,based on,bert - based ranker,two main instantiations based on bert - based ranker,0.6979451775550842
translation,30,8,baselines,baselines,experiment with,two main instantiations,baselines experiment with two main instantiations,0.6842888593673706
translation,30,31,baselines,two main instantiations,of,proposed architecture,two main instantiations of proposed architecture,0.5511348843574524
translation,30,31,baselines,two main instantiations,using,posit - drmm,two main instantiations using posit - drmm,0.6968883275985718
translation,30,31,baselines,two main instantiations,using,bert - based ranker,two main instantiations using bert - based ranker,0.6730621457099915
translation,30,31,baselines,posit - drmm,called,pdrmm,posit - drmm called pdrmm,0.7018958330154419
translation,30,31,baselines,posit - drmm,called,neural text ranker,posit - drmm called neural text ranker,0.6873399019241333
translation,30,31,baselines,posit - drmm,called,bert - based ranker,posit - drmm called bert - based ranker,0.7167213559150696
translation,30,31,baselines,posit - drmm,as,neural text ranker,posit - drmm as neural text ranker,0.5597658157348633
translation,30,31,baselines,posit - drmm,as,bert - based ranker,posit - drmm as bert - based ranker,0.5922924280166626
translation,30,31,baselines,baselines,experiment with,two main instantiations,baselines experiment with two main instantiations,0.6842888593673706
translation,30,95,baselines,baselines,has,bm25 + bm25 baseline pipeline,baselines has bm25 + bm25 baseline pipeline,0.6092557311058044
translation,30,96,baselines,bm25 + bm25 pipeline,to measure,improvement,bm25 + bm25 pipeline to measure improvement,0.6437755227088928
translation,30,96,baselines,improvement,of,proposed models,improvement of proposed models,0.6399298310279846
translation,30,96,baselines,proposed models,on,conventional ir engines,proposed models on conventional ir engines,0.560093343257904
translation,30,181,baselines,best pipeline and joint model,that did not use,bert,best pipeline and joint model that did not use bert,0.7326716184616089
translation,30,181,baselines,best pipeline and joint model,comparing them to,more conventional bm25 + bm25 baseline,best pipeline and joint model comparing them to more conventional bm25 + bm25 baseline,0.6559803485870361
translation,30,181,baselines,baselines,experiment with,best pipeline and joint model,baselines experiment with best pipeline and joint model,0.6118119359016418
translation,30,107,experimental-setup,pdrmm and bcnn,use,biomedical word2vec embeddings,pdrmm and bcnn use biomedical word2vec embeddings,0.567028820514679
translation,30,107,experimental-setup,experimental setup,In,pdrmm and bcnn,experimental setup In pdrmm and bcnn,0.5481623411178589
translation,30,110,experimental-setup,experimental setup,train using,"adam ( kingma and ba , 2015 )","experimental setup train using adam ( kingma and ba , 2015 )",0.7650992274284363
translation,30,9,experiments,experiments,on,biomedical data,experiments on biomedical data,0.5293711423873901
translation,30,9,experiments,biomedical data,from,bioasq,biomedical data from bioasq,0.5484272241592407
translation,30,9,experiments,biomedical data,show,our joint models,biomedical data show our joint models,0.6052488088607788
translation,30,9,experiments,pipelines,in,snippet retrieval,pipelines in snippet retrieval,0.5317173600196838
translation,30,9,experiments,our joint models,has,vastly outperform,our joint models has vastly outperform,0.5716444253921509
translation,30,9,experiments,vastly outperform,has,pipelines,vastly outperform has pipelines,0.5975408554077148
translation,30,108,experiments,galago 10 ir engine,to obtain,top n = 100 documents per query,galago 10 ir engine to obtain top n = 100 documents per query,0.5346396565437317
translation,30,210,experiments,modified version,of,natural questions dataset,modified version of natural questions dataset,0.5371488332748413
translation,30,210,experiments,modified version,suitable for,document and snippet retrieval,modified version suitable for document and snippet retrieval,0.7259343266487122
translation,30,210,experiments,natural questions dataset,suitable for,document and snippet retrieval,natural questions dataset suitable for document and snippet retrieval,0.6678427457809448
translation,30,32,model,pdrmm and bert,can be used to score,documents and snippets,pdrmm and bert can be used to score documents and snippets,0.7416552901268005
translation,30,32,model,documents and snippets,in,pipelines,documents and snippets in pipelines,0.5550975799560547
translation,30,32,model,models,jointly score,documents and snippets,models jointly score documents and snippets,0.7553629875183105
translation,30,32,model,model,show,pdrmm and bert,model show pdrmm and bert,0.7246232628822327
translation,30,13,results,corresponding pipeline,in,snippet retrieval,corresponding pipeline in snippet retrieval,0.5395835638046265
translation,30,13,results,corresponding pipeline,on,modified natural questions dataset,corresponding pipeline on modified natural questions dataset,0.5014339685440063
translation,30,13,results,snippet retrieval,on,modified natural questions dataset,snippet retrieval on modified natural questions dataset,0.49616092443466187
translation,30,13,results,outperforms,has,corresponding pipeline,outperforms has corresponding pipeline,0.6124264001846313
translation,30,13,results,results,has,joint pdrmm - based model,results has joint pdrmm - based model,0.5235939621925354
translation,30,35,results,our joint version of pdrmm,does not use,bert,our joint version of pdrmm does not use bert,0.6707375645637512
translation,30,35,results,fewest parameters,of,all models,fewest parameters of all models,0.5668525099754333
translation,30,35,results,competitive,to,bert - based models,competitive to bert - based models,0.5789787173271179
translation,30,35,results,best system of bioasq,in,document and snippet retrieval,best system of bioasq in document and snippet retrieval,0.5160471796989441
translation,30,35,results,outperforming,has,best system of bioasq,outperforming has best system of bioasq,0.607546865940094
translation,30,35,results,results,show that,our joint version of pdrmm,results show that our joint version of pdrmm,0.48537954688072205
translation,30,38,results,corresponding pipeline,in,snippet retrieval,corresponding pipeline in snippet retrieval,0.5395835638046265
translation,30,38,results,snippet retrieval,on,modified natural questions,snippet retrieval on modified natural questions,0.5358492732048035
translation,30,38,results,joint pdrmmbased model,has,largely outperforms,joint pdrmmbased model has largely outperforms,0.5906999707221985
translation,30,38,results,largely outperforms,has,corresponding pipeline,largely outperforms has corresponding pipeline,0.5773227214813232
translation,30,38,results,results,has,joint pdrmmbased model,results has joint pdrmmbased model,0.5298020243644714
translation,30,39,results,bm25 ranking,of,traditional ir,bm25 ranking of traditional ir,0.5162653923034668
translation,30,39,results,bm25 ranking,on,both datasets,bm25 ranking on both datasets,0.48666509985923767
translation,30,39,results,improve,has,bm25 ranking,improve has bm25 ranking,0.5600234270095825
translation,30,39,results,results,show,all the neural pipelines and joint models,results show all the neural pipelines and joint models,0.5765753984451294
translation,30,141,results,pdrmm + bcnn,in,snippet map ( 9.16 to 5.67 ),pdrmm + bcnn in snippet map ( 9.16 to 5.67 ),0.529832661151886
translation,30,141,results,pdrmm + pdrmm,has,outperforms,pdrmm + pdrmm has outperforms,0.6945974826812744
translation,30,141,results,outperforms,has,pdrmm + bcnn,outperforms has pdrmm + bcnn,0.6132608652114868
translation,30,141,results,results,has,pdrmm + pdrmm,results has pdrmm + pdrmm,0.5605260133743286
translation,30,142,results,pdrmm + bcnn,was,best system,pdrmm + bcnn was best system,0.5972286462783813
translation,30,142,results,best system,in,bioasq,best system in bioasq,0.5639469623565674
translation,30,142,results,bioasq,for,documents and snippets,bioasq for documents and snippets,0.6308674216270447
translation,30,142,results,results,has,pdrmm + bcnn,results has pdrmm + bcnn,0.535629153251648
translation,30,143,results,pdrmm,by,bert,pdrmm by bert,0.6836623549461365
translation,30,143,results,pdrmm,for,document ranking,pdrmm for document ranking,0.5934947729110718
translation,30,143,results,pdrmm,increases,document map,pdrmm increases document map,0.721674919128418
translation,30,143,results,bert,for,document ranking,bert for document ranking,0.6352980732917786
translation,30,143,results,document ranking,in,two pipelines ( bert + bcnn,document ranking in two pipelines ( bert + bcnn,0.52926105260849
translation,30,143,results,document map,by,1.32 points,document map by 1.32 points,0.5766845345497131
translation,30,143,results,marginal increase,in,snippet map,marginal increase in snippet map,0.5256077647209167
translation,30,143,results,snippet map,for,bert + pdrmm ( 9.16 to 9.63 ),snippet map for bert + pdrmm ( 9.16 to 9.63 ),0.6045238375663757
translation,30,143,results,slightly larger increase,for,bert + bcnn ( 5.67 to 6.07 ),slightly larger increase for bert + bcnn ( 5.67 to 6.07 ),0.6206498742103577
translation,30,143,results,slightly larger increase,expense of,massive increase,slightly larger increase expense of massive increase,0.7759917974472046
translation,30,143,results,massive increase,in,trainable parameters,massive increase in trainable parameters,0.5307828783988953
translation,30,143,results,massive increase,due to,bert,massive increase due to bert,0.7747198343276978
translation,30,143,results,trainable parameters,due to,bert,trainable parameters due to bert,0.6585653424263
translation,30,143,results,results,Replacing,pdrmm,results Replacing pdrmm,0.6410854458808899
translation,30,146,results,pipelines,in,snippet,pipelines in snippet,0.5359704494476318
translation,30,146,results,"main joint models ( jpdrmm , bjpdrmm , jbert )",has,vastly outperform,"main joint models ( jpdrmm , bjpdrmm , jbert ) has vastly outperform",0.5212543606758118
translation,30,146,results,vastly outperform,has,pipelines,vastly outperform has pipelines,0.5975408554077148
translation,30,146,results,results,has,"main joint models ( jpdrmm , bjpdrmm , jbert )","results has main joint models ( jpdrmm , bjpdrmm , jbert )",0.49679693579673767
translation,30,149,results,jpdrmm,does not use,bert,jpdrmm does not use bert,0.6447821259498596
translation,30,149,results,fewest parameters,of,all neural models,fewest parameters of all neural models,0.5799852013587952
translation,30,149,results,bert,is,competitive,bert is competitive,0.6416749358177185
translation,30,149,results,competitive,in,snippet retrieval,competitive in snippet retrieval,0.5368729829788208
translation,30,149,results,competitive,models that employ,bert,competitive models that employ bert,0.7435927987098694
translation,30,149,results,results,has,jpdrmm,results has jpdrmm,0.5490630865097046
translation,30,152,results,linear combinations,of,token embeddings,linear combinations of token embeddings,0.5541260838508606
translation,30,152,results,linear combinations,harms,both document and snippet map,linear combinations harms both document and snippet map,0.6609559059143066
translation,30,152,results,linear combinations,harms,beneficial,linear combinations harms beneficial,0.7384337782859802
translation,30,152,results,token embeddings,from,all bert layers ( - adapt variants ),token embeddings from all bert layers ( - adapt variants ),0.5283215045928955
translation,30,152,results,both document and snippet map,when,fine- tuning,both document and snippet map when fine- tuning,0.6275901794433594
translation,30,152,results,beneficial,in,most cases,beneficial in most cases,0.6122235059738159
translation,30,152,results,beneficial,when,not fine-tuning bert ( - nf ),beneficial when not fine-tuning bert ( - nf ),0.6886063814163208
translation,30,152,results,fine- tuning,has,bert,fine- tuning has bert,0.5301835536956787
translation,30,152,results,results,Using,linear combinations,results Using linear combinations,0.6348501443862915
translation,30,169,results,improve,after,expert inspection,improve after expert inspection,0.6509755849838257
translation,30,169,results,substantially,after,expert inspection,substantially after expert inspection,0.7186360955238342
translation,30,169,results,all results,has,improve,all results has improve,0.6096834540367126
translation,30,169,results,improve,has,substantially,improve has substantially,0.6275495886802673
translation,30,169,results,results,has,first striking observation,results has first striking observation,0.5458188652992249
translation,30,170,results,"two joint models ( jpdrmm , bjpdrmm - nf )",has,vastly outperform,"two joint models ( jpdrmm , bjpdrmm - nf ) has vastly outperform",0.5241371989250183
translation,30,170,results,vastly outperform,has,bert + pdrmm pipeline,vastly outperform has bert + pdrmm pipeline,0.6009793877601624
translation,30,170,results,results,has,"two joint models ( jpdrmm , bjpdrmm - nf )","results has two joint models ( jpdrmm , bjpdrmm - nf )",0.4866393506526947
translation,30,177,results,jpdrmm,is,competitive,jpdrmm is competitive,0.6725203990936279
translation,30,177,results,competitive,models that use,bert,competitive models that use bert,0.7471783757209778
translation,30,177,results,results,confirms,jpdrmm,results confirms jpdrmm,0.622016966342926
translation,30,178,results,outperformed,has,competition,outperformed has competition,0.6593561172485352
translation,30,178,results,results,has,all of our methods,results has all of our methods,0.49025633931159973
translation,30,180,results,results,on,modified natural questions dataset,results on modified natural questions dataset,0.5317125916481018
translation,30,183,results,bm25 + bm25 pipeline,in,document and snippet retrieval,bm25 + bm25 pipeline in document and snippet retrieval,0.512100875377655
translation,30,183,results,pdrmm +pdrmm and jpdrmm,has,outperform,pdrmm +pdrmm and jpdrmm has outperform,0.649686872959137
translation,30,183,results,outperform,has,bm25 + bm25 pipeline,outperform has bm25 + bm25 pipeline,0.6197640895843506
translation,30,183,results,results,has,pdrmm +pdrmm and jpdrmm,results has pdrmm +pdrmm and jpdrmm,0.5462457537651062
translation,30,184,results,pdrmm + pdrmm pipeline,in,snippet retrieval,pdrmm + pdrmm pipeline in snippet retrieval,0.5095145106315613
translation,30,184,results,pipeline,performs,better,pipeline performs better,0.6949504613876343
translation,30,184,results,better,in,document retrieval,better in document retrieval,0.5137921571731567
translation,30,184,results,joint jpdrmm model,has,outperforms,joint jpdrmm model has outperforms,0.5781699419021606
translation,30,184,results,outperforms,has,pdrmm + pdrmm pipeline,outperforms has pdrmm + pdrmm pipeline,0.6375095248222351
translation,30,208,results,biomedical data,showed that,two resulting joint models ( pdrmm - based and bert - based ),biomedical data showed that two resulting joint models ( pdrmm - based and bert - based ),0.6919973492622375
translation,30,208,results,corresponding pipelines,in,snippet,corresponding pipelines in snippet,0.5172911882400513
translation,30,208,results,two resulting joint models ( pdrmm - based and bert - based ),has,vastly outperform,two resulting joint models ( pdrmm - based and bert - based ) has vastly outperform,0.5485369563102722
translation,30,208,results,vastly outperform,has,corresponding pipelines,vastly outperform has corresponding pipelines,0.5780254602432251
translation,30,208,results,results,Using,biomedical data,results Using biomedical data,0.5918548107147217
translation,30,209,results,that does not use bert,is,competitive,that does not use bert is competitive,0.6514212489128113
translation,30,209,results,competitive,with,bert - based models,competitive with bert - based models,0.651176929473877
translation,30,209,results,joint model ( pdrmm - based ),has,that does not use bert,joint model ( pdrmm - based ) has that does not use bert,0.6214854121208191
translation,30,209,results,outperforming,has,best bioasq 6 system,outperforming has best bioasq 6 system,0.6462481617927551
translation,30,209,results,our joint models ( pdrmm - and bert - based ),has,outperformed,our joint models ( pdrmm - and bert - based ) has outperformed,0.547621488571167
translation,30,209,results,outperformed,has,all bioasq 7 competitors,outperformed has all bioasq 7 competitors,0.6174746155738831
translation,30,209,results,results,showed that,joint model ( pdrmm - based ),results showed that joint model ( pdrmm - based ),0.6891157031059265
translation,30,211,results,corresponding pipeline,on,open-domain data ( natural questions ),corresponding pipeline on open-domain data ( natural questions ),0.5455909967422485
translation,30,211,results,open-domain data ( natural questions ),in,snippet retrieval,open-domain data ( natural questions ) in snippet retrieval,0.48601630330085754
translation,30,211,results,our joint pdrmm - based model,has,largely outperforms,our joint pdrmm - based model has largely outperforms,0.5854021906852722
translation,30,211,results,largely outperforms,has,corresponding pipeline,largely outperforms has corresponding pipeline,0.5773227214813232
translation,30,211,results,results,showed that,our joint pdrmm - based model,results showed that our joint pdrmm - based model,0.6821174621582031
translation,30,212,results,all the neural pipelines and joint models,improve,traditional bm25 ranking,all the neural pipelines and joint models improve traditional bm25 ranking,0.6414504051208496
translation,30,212,results,traditional bm25 ranking,on,both datasets,traditional bm25 ranking on both datasets,0.4861067831516266
translation,30,212,results,results,showed that,all the neural pipelines and joint models,results showed that all the neural pipelines and joint models,0.6457995772361755
translation,31,190,ablation-analysis,dev ( nao ) set,popularity of,substituted entity,dev ( nao ) set popularity of substituted entity,0.7151671648025513
translation,31,190,ablation-analysis,substituted entity,less predictive of,model behavior,substituted entity less predictive of model behavior,0.6685824990272522
translation,31,190,ablation-analysis,ablation analysis,On,dev ( nao ) set,ablation analysis On dev ( nao ) set,0.6134197115898132
translation,31,28,experiments,automated framework,to create,substitution instances,automated framework to create substitution instances,0.6522771120071411
translation,31,28,experiments,substitution instances,for,"natural questions ( kwiatkowski et al. , 2019 )","substitution instances for natural questions ( kwiatkowski et al. , 2019 )",0.596787691116333
translation,31,28,experiments,newsqa,has,"trischler et al. , 2017a )","newsqa has trischler et al. , 2017a )",0.6094170808792114
translation,31,115,experiments,dense passage retrieval ( dpr ),as,primary retrieval system,dense passage retrieval ( dpr ) as primary retrieval system,0.5148400068283081
translation,31,7,model,simple method,to mitigate,over-reliance,simple method to mitigate over-reliance,0.6932684779167175
translation,31,7,model,simple method,improves,out -of- distribution generalization,simple method improves out -of- distribution generalization,0.6860405802726746
translation,31,7,model,over-reliance,on,parametric knowledge,over-reliance on parametric knowledge,0.5489043593406677
translation,31,7,model,parametric knowledge,minimizes,hallucination,parametric knowledge minimizes hallucination,0.6534020900726318
translation,31,7,model,out -of- distribution generalization,by,4 % ? 7 %,out -of- distribution generalization by 4 % ? 7 %,0.6021704077720642
translation,31,7,model,model,propose,simple method,model propose simple method,0.6824520826339722
translation,31,26,model,automated framework,identifies,qa instances,automated framework identifies qa instances,0.6634791493415833
translation,31,26,model,automated framework,substitutes,mentions,automated framework substitutes mentions,0.7547902464866638
translation,31,26,model,qa instances,with,named entity answers,qa instances with named entity answers,0.5664185285568237
translation,31,26,model,mentions,of,entity,mentions of entity,0.5801299810409546
translation,31,26,model,entity,in,gold document,entity in gold document,0.4910596013069153
translation,31,26,model,model,create,automated framework,model create automated framework,0.6569107174873352
translation,31,201,results,memorization,is,low,memorization is low,0.6054756045341492
translation,31,201,results,low,across,type-pair substitutions,low across type-pair substitutions,0.7115933895111084
translation,31,201,results,nq dev ( nao ),has,memorization,nq dev ( nao ) has memorization,0.6024994850158691
translation,31,201,results,results,On,nq dev ( nao ),results On nq dev ( nao ),0.5436884164810181
translation,31,218,results,memorization ratio,on,all kc datasets,memorization ratio on all kc datasets,0.5168591737747192
translation,31,218,results,memorization ratio,to,negligible levels,memorization ratio to negligible levels,0.5754280090332031
translation,31,218,results,augmented dataset,has,greatly decreases,augmented dataset has greatly decreases,0.6012914180755615
translation,31,218,results,greatly decreases,has,memorization ratio,greatly decreases has memorization ratio,0.5860698223114014
translation,31,218,results,results,training with,augmented dataset,results training with augmented dataset,0.7397394180297852
translation,31,219,results,out - of- domain generalization,on,original instances,out - of- domain generalization on original instances,0.5274952054023743
translation,31,219,results,improves,for,nq dev nao,improves for nq dev nao,0.7069849967956543
translation,31,219,results,out - of- domain generalization,has,improves,out - of- domain generalization has improves,0.5649187564849854
translation,31,219,results,original instances,has,improves,original instances has improves,0.6190812587738037
translation,31,219,results,nq dev nao,has,7 % ),nq dev nao has 7 % ),0.6433231234550476
translation,32,9,ablation-analysis,efficiency,of,our method,efficiency of our method,0.5723916888237
translation,32,9,ablation-analysis,efficiency,use,layer - wise parameter sharing,efficiency use layer - wise parameter sharing,0.6709770560264587
translation,32,9,ablation-analysis,efficiency,use,factorized co-attention,efficiency use factorized co-attention,0.6752267479896545
translation,32,9,ablation-analysis,our method,use,layer - wise parameter sharing,our method use layer - wise parameter sharing,0.6301618814468384
translation,32,9,ablation-analysis,our method,use,factorized co-attention,our method use factorized co-attention,0.6295271515846252
translation,32,9,ablation-analysis,factorized co-attention,that share,parameters,factorized co-attention that share parameters,0.6505885720252991
translation,32,9,ablation-analysis,parameters,between,cross attention blocks,parameters between cross attention blocks,0.607753336429596
translation,32,9,ablation-analysis,minimal impact,on,task performance,minimal impact on task performance,0.5713351964950562
translation,32,9,ablation-analysis,ablation analysis,To further improve,efficiency,ablation analysis To further improve efficiency,0.7146132588386536
translation,32,174,ablation-analysis,parameter sharing,decrease,model size,parameter sharing decrease model size,0.7814997434616089
translation,32,174,ablation-analysis,model size,by,71 %,model size by 71 %,0.5674924850463867
translation,32,174,ablation-analysis,negligible impact,on,model accuracy,negligible impact on model accuracy,0.5483343005180359
translation,32,174,ablation-analysis,ablation analysis,indicate,parameter sharing,ablation analysis indicate parameter sharing,0.5838029384613037
translation,32,175,ablation-analysis,layer - wise parameter sharing,improves,performance,layer - wise parameter sharing improves performance,0.6869350075721741
translation,32,175,ablation-analysis,ablation analysis,has,layer - wise parameter sharing,ablation analysis has layer - wise parameter sharing,0.5132091045379639
translation,32,179,ablation-analysis,further sharing,reduces,size of the model,further sharing reduces size of the model,0.6871567368507385
translation,32,179,ablation-analysis,size of the model,by,70 %,size of the model by 70 %,0.5952891111373901
translation,32,179,ablation-analysis,70 %,compared with,our model,70 % compared with our model,0.6743667125701904
translation,32,179,ablation-analysis,1.5 % relative reduction,in,model accuracy,1.5 % relative reduction in model accuracy,0.5172541737556458
translation,32,179,ablation-analysis,ablation analysis,show,further sharing,ablation analysis show further sharing,0.6786255240440369
translation,32,197,ablation-analysis,advantages,of,sp - block,advantages of sp - block,0.5775008201599121
translation,32,197,ablation-analysis,sp - block,lead to,3.3 % increase,sp - block lead to 3.3 % increase,0.679323673248291
translation,32,197,ablation-analysis,sp - block,lead to,89 % reduction,sp - block lead to 89 % reduction,0.6748701930046082
translation,32,197,ablation-analysis,3.3 % increase,in,performance,3.3 % increase in performance,0.5454117059707642
translation,32,197,ablation-analysis,89 % reduction,in,parameters,89 % reduction in parameters,0.554908037185669
translation,32,197,ablation-analysis,ablation analysis,has,advantages,ablation analysis has advantages,0.5017535090446472
translation,32,159,baselines,spt,with,layer - wise parameter sharing,spt with layer - wise parameter sharing,0.6364359259605408
translation,32,159,baselines,spt,with,mixed sampling,spt with mixed sampling,0.6691460609436035
translation,32,159,baselines,spt,with,summation,spt with summation,0.693239688873291
translation,32,159,baselines,summation,for,cross-modal interactions,summation for cross-modal interactions,0.6335820555686951
translation,32,209,experimental-setup,largest batch size,executed on,single nvidia tesla v100,largest batch size executed on single nvidia tesla v100,0.7325364351272583
translation,32,209,experimental-setup,single nvidia tesla v100,with,16gb vram,single nvidia tesla v100 with 16gb vram,0.622830867767334
translation,32,6,model,model,propose,multimodal sparse phased transformer ( spt ),model propose multimodal sparse phased transformer ( spt ),0.6728518605232239
translation,32,7,model,spt,uses,sampling function,spt uses sampling function,0.654998779296875
translation,32,7,model,spt,compress,long sequence,spt compress long sequence,0.6941305994987488
translation,32,7,model,sampling function,to generate,sparse attention matrix,sampling function to generate sparse attention matrix,0.6983646154403687
translation,32,7,model,long sequence,to,shorter sequence,long sequence to shorter sequence,0.5937726497650146
translation,32,7,model,shorter sequence,of,hidden states,shorter sequence of hidden states,0.6162627935409546
translation,32,7,model,model,has,spt,model has spt,0.6132522821426392
translation,32,8,model,spt,concurrently captures,interactions,spt concurrently captures interactions,0.7560601234436035
translation,32,8,model,interactions,between,hidden states,interactions between hidden states,0.6528207659721375
translation,32,8,model,hidden states,of,different modalities,hidden states of different modalities,0.5819932222366333
translation,32,8,model,different modalities,at,every layer,different modalities at every layer,0.5361409187316895
translation,32,8,model,model,has,spt,model has spt,0.6132522821426392
translation,32,39,model,sp - attention,creates,sparse attention matrix,sp - attention creates sparse attention matrix,0.6078674793243408
translation,32,39,model,sp - attention,propose,multimodal sp - transformer,sp - attention propose multimodal sp - transformer,0.6641334295272827
translation,32,39,model,sparse attention matrix,improves,computational and sampling efficiency,sparse attention matrix improves computational and sampling efficiency,0.6373590230941772
translation,32,39,model,sparse attention matrix,propose,multimodal sp - transformer,sparse attention matrix propose multimodal sp - transformer,0.6294896006584167
translation,32,39,model,computational and sampling efficiency,propose,multimodal sp - transformer,computational and sampling efficiency propose multimodal sp - transformer,0.5816723704338074
translation,32,39,model,multimodal sp - transformer,uses,concurrent structure,multimodal sp - transformer uses concurrent structure,0.613895833492279
translation,32,39,model,concurrent structure,of,blocks,concurrent structure of blocks,0.6473026871681213
translation,32,39,model,blocks,in,each sub-layer,blocks in each sub-layer,0.5535777807235718
translation,32,39,model,blocks,to allow,multimodal signal,blocks to allow multimodal signal,0.7051376700401306
translation,32,39,model,model,propose,multimodal sp - transformer,model propose multimodal sp - transformer,0.6953314542770386
translation,32,39,model,model,has,sp - attention,model has sp - attention,0.5905196070671082
translation,32,40,model,spt,uses,input attention,spt uses input attention,0.5882565975189209
translation,32,40,model,spt,uses,cross-attention,spt uses cross-attention,0.5679013133049011
translation,32,40,model,spt,leverage,factorized co-attention,spt leverage factorized co-attention,0.7057104110717773
translation,32,40,model,input attention,on,source input sequence,input attention on source input sequence,0.5656351447105408
translation,32,40,model,cross-attention,on,hidden state pairs,cross-attention on hidden state pairs,0.5538804531097412
translation,32,40,model,hidden state pairs,of,different modalities,hidden state pairs of different modalities,0.5569576025009155
translation,32,40,model,hidden state pairs,of,self-attention,hidden state pairs of self-attention,0.5762320160865784
translation,32,40,model,self-attention,on,hidden states,self-attention on hidden states,0.5928837060928345
translation,32,40,model,hidden states,of,each modality,hidden states of each modality,0.5771176815032959
translation,32,40,model,factorized co-attention,that use,factorized form,factorized co-attention that use factorized form,0.6832307577133179
translation,32,40,model,factorized form,of,attention computation,factorized form of attention computation,0.5685617923736572
translation,32,40,model,attention computation,based on,affinity matrix,attention computation based on affinity matrix,0.6453105807304382
translation,32,40,model,affinity matrix,to further reduce,number of parameters,affinity matrix to further reduce number of parameters,0.6548743844032288
translation,32,40,model,number of parameters,for,cross attention block ( co - sp ),number of parameters for cross attention block ( co - sp ),0.6040151715278625
translation,32,40,model,model,leverage,factorized co-attention,model leverage factorized co-attention,0.7391678094863892
translation,32,40,model,model,has,spt,model has spt,0.6132522821426392
translation,32,41,model,improve efficiency,of,spt,improve efficiency of spt,0.6049662232398987
translation,32,41,model,spt,by,parameter sharing,spt by parameter sharing,0.577385425567627
translation,32,41,model,parameter sharing,across,all layers,parameter sharing across all layers,0.7275158166885376
translation,32,41,model,model,further,improve efficiency,model further improve efficiency,0.596920907497406
translation,32,77,model,hidden state sequences,for,each pair of two modalities,hidden state sequences for each pair of two modalities,0.5749309659004211
translation,32,77,model,hidden state sequences,interact through,cross attention,hidden state sequences interact through cross attention,0.7045464515686035
translation,32,77,model,each pair of two modalities,interact through,cross attention,each pair of two modalities interact through cross attention,0.7555387616157532
translation,32,77,model,cross attention,by,co-sp - block,cross attention by co-sp - block,0.583130955696106
translation,32,77,model,co-sp - block,using,factorized co-attention,co-sp - block using factorized co-attention,0.6907876133918762
translation,32,77,model,model,has,hidden state sequences,model has hidden state sequences,0.5438729524612427
translation,32,178,model,linear projection,to map,"audio , video , text inputs","linear projection to map audio , video , text inputs",0.7006237506866455
translation,32,178,model,"audio , video , text inputs",to,d model,"audio , video , text inputs to d model",0.5742656588554382
translation,32,178,model,model,use,linear projection,model use linear projection,0.6319994330406189
translation,32,194,model,spt,uses,input attention block,spt uses input attention block,0.5728753209114075
translation,32,194,model,),uses,input attention block,) uses input attention block,0.5911213159561157
translation,32,194,model,input attention block,followed by,self-attention block,input attention block followed by self-attention block,0.6419849395751953
translation,32,194,model,self-attention block,in,each layer,self-attention block in each layer,0.5132891535758972
translation,32,194,model,spt,has,),spt has ),0.6734700202941895
translation,32,194,model,model,has,spt,model has spt,0.6132522821426392
translation,32,223,model,multimodal sp - transformer,uses,sequence of hidden states,multimodal sp - transformer uses sequence of hidden states,0.615723729133606
translation,32,223,model,sequence of hidden states,to sample from,longer multimodal input sequences,sequence of hidden states to sample from longer multimodal input sequences,0.6896897554397583
translation,32,223,model,model,propose,multimodal sp - transformer,model propose multimodal sp - transformer,0.6953314542770386
translation,32,225,model,concurrent structure,enables,more effective capturing,concurrent structure enables more effective capturing,0.7086020112037659
translation,32,225,model,more effective capturing,of,multimodal interaction,more effective capturing of multimodal interaction,0.562516450881958
translation,32,225,model,more effective capturing,resulting in,higher performance,more effective capturing resulting in higher performance,0.6923470497131348
translation,32,225,model,model,has,concurrent structure,model has concurrent structure,0.5541890263557434
translation,32,169,results,concurrent structure,improves,accuracy,concurrent structure improves accuracy,0.7387406229972839
translation,32,169,results,accuracy,compared with,serial structure,accuracy compared with serial structure,0.686896800994873
translation,32,169,results,richer multimodal interactions,at,every layer,richer multimodal interactions at every layer,0.5348236560821533
translation,32,169,results,results,has,concurrent structure,results has concurrent structure,0.5364403128623962
translation,32,195,results,substantial difference,in,performance,substantial difference in performance,0.5666977167129517
translation,32,195,results,performance,for,sp - block,performance for sp - block,0.6560766696929932
translation,32,195,results,results,show,substantial difference,results show substantial difference,0.666009783744812
translation,32,208,results,training time,in,"unaligned cmu - mosi , cmu - mosei , and ur - funny datasets","training time in unaligned cmu - mosi , cmu - mosei , and ur - funny datasets",0.4930068850517273
translation,32,208,results,results,test,training time,results test training time,0.7812061309814453
translation,32,224,results,reduced computational complexity,through,sparse attention,reduced computational complexity through sparse attention,0.6641221642494202
translation,32,224,results,our model,has,reduced computational complexity,our model has reduced computational complexity,0.5680949091911316
translation,33,8,model,two novel rewards,obtained from,downstream tasks,two novel rewards obtained from downstream tasks,0.5784837603569031
translation,33,8,model,two novel rewards,to regularize,question generation model,two novel rewards to regularize question generation model,0.6659077405929565
translation,33,8,model,question - focus recognition,to regularize,question generation model,question - focus recognition to regularize question generation model,0.656629204750061
translation,33,8,model,model,propose,two novel rewards,model propose two novel rewards,0.6932446360588074
translation,33,21,model,framework,for,abstractive question summarization,framework for abstractive question summarization,0.5701790452003479
translation,33,22,model,model,propose,two novel question - aware semantic reward functions,model propose two novel question - aware semantic reward functions,0.6978337168693542
translation,33,103,results,benchmark model,on,meq - sum,benchmark model on meq - sum,0.537034809589386
translation,33,103,results,our proposed model,obtained,improvement,our proposed model obtained improvement,0.65309739112854
translation,33,103,results,improvement,of,9.63 %,improvement of 9.63 %,0.5650222897529602
translation,33,103,results,benchmark model,has,our proposed model,benchmark model has our proposed model,0.5867198705673218
translation,33,103,results,meq - sum,has,our proposed model,meq - sum has our proposed model,0.6298880577087402
translation,33,103,results,results,In comparison to,benchmark model,results In comparison to benchmark model,0.6795220375061035
translation,33,105,results,individual qtr and qfr rewards,improve over,prophetnet and rougebased rewards,individual qtr and qfr rewards improve over prophetnet and rougebased rewards,0.683123767375946
translation,33,105,results,results,show that,individual qtr and qfr rewards,results show that individual qtr and qfr rewards,0.4404984712600708
translation,33,106,results,question - type reward,assists,model,question - type reward assists model,0.655059814453125
translation,33,106,results,model,to capture,underlying question semantics,model to capture underlying question semantics,0.7109671235084534
translation,33,106,results,salient entities,learned from,question - focus reward,salient entities learned from question - focus reward,0.6463996171951294
translation,33,106,results,fewer incorrect summaries,unrelated to,question topic,fewer incorrect summaries unrelated to question topic,0.6717063188552856
translation,33,108,results,downstream tasks,of,question - type identification,downstream tasks of question - type identification,0.5642900466918945
translation,33,108,results,downstream tasks,of,questionfocus recognition,downstream tasks of questionfocus recognition,0.5883509516716003
translation,33,108,results,pre-trained bert model,achieves,f-score,pre-trained bert model achieves f-score,0.6506645679473877
translation,33,108,results,f-score,of,97.10 % and 77.24 %,f-score of 97.10 % and 77.24 %,0.5605337619781494
translation,33,108,results,97.10 % and 77.24 %,on,10 %,97.10 % and 77.24 % on 10 %,0.602226972579956
translation,33,108,results,10 %,of,manually labeled meq - sum pairs,10 % of manually labeled meq - sum pairs,0.5988548398017883
translation,33,108,results,downstream tasks,has,pre-trained bert model,downstream tasks has pre-trained bert model,0.574617326259613
translation,33,108,results,question - type identification,has,pre-trained bert model,question - type identification has pre-trained bert model,0.546405553817749
translation,33,108,results,questionfocus recognition,has,pre-trained bert model,questionfocus recognition has pre-trained bert model,0.5740859508514404
translation,33,108,results,results,On,downstream tasks,results On downstream tasks,0.5158278942108154
translation,33,116,results,our proposed rewards,enhance,model,our proposed rewards enhance model,0.6332380175590515
translation,33,116,results,model,by capturing,underlying semantics and facts,model by capturing underlying semantics and facts,0.7392847537994385
translation,33,116,results,higher proportions,of,perfect and acceptable summaries,higher proportions of perfect and acceptable summaries,0.556489884853363
translation,33,116,results,results,show,our proposed rewards,results show our proposed rewards,0.6733121871948242
translation,34,107,ablation-analysis,four features,used in,query graph generator,four features used in query graph generator,0.6573314666748047
translation,34,107,ablation-analysis,"( relation , entity type , relation ) triplet feature",has,biggest impact,"( relation , entity type , relation ) triplet feature has biggest impact",0.5726401805877686
translation,34,107,ablation-analysis,biggest impact,on,performance,biggest impact on performance,0.5596711039543152
translation,34,107,ablation-analysis,four features,has,"( relation , entity type , relation ) triplet feature","four features has ( relation , entity type , relation ) triplet feature",0.5749357342720032
translation,34,107,ablation-analysis,query graph generator,has,"( relation , entity type , relation ) triplet feature","query graph generator has ( relation , entity type , relation ) triplet feature",0.574081540107727
translation,34,107,ablation-analysis,"( relation , entity type , relation ) triplet feature",has,biggest impact,"( relation , entity type , relation ) triplet feature has biggest impact",0.5726401805877686
translation,34,107,ablation-analysis,ablation analysis,Among,four features,ablation analysis Among four features,0.6016705632209778
translation,34,111,ablation-analysis,triplet facts,into,sequence,triplet facts into sequence,0.6173447370529175
translation,34,111,ablation-analysis,drops,by,1.4 %,drops by 1.4 %,0.6396101117134094
translation,34,131,ablation-analysis,significant f1 score drop,on,cwq dataset,significant f1 score drop on cwq dataset,0.5557334423065186
translation,34,131,ablation-analysis,significant f1 score drop,in,last stage,significant f1 score drop in last stage,0.5447937250137329
translation,34,131,ablation-analysis,cwq dataset,in,last stage,cwq dataset in last stage,0.5274632573127747
translation,34,131,ablation-analysis,ablation analysis,see,significant f1 score drop,ablation analysis see significant f1 score drop,0.6246014833450317
translation,34,90,experiments,entity linking,take,"union of allennlp ( gardner et al. , 2017 )","entity linking take union of allennlp ( gardner et al. , 2017 )",0.5592473745346069
translation,34,90,experiments,"stanford ner ( finkel et al. , 2005 ) outputs",in,cwq experiments,"stanford ner ( finkel et al. , 2005 ) outputs in cwq experiments",0.5148285031318665
translation,34,90,experiments,"s-mart ( yang and chang , 2016 )",in,wqsp experiments,"s-mart ( yang and chang , 2016 ) in wqsp experiments",0.5331112742424011
translation,34,93,hyperparameters,albert training,initialize,model,albert training initialize model,0.735494077205658
translation,34,93,hyperparameters,albert training,fine-tune,corresponding kbqa dataset,albert training fine-tune corresponding kbqa dataset,0.6642051339149475
translation,34,93,hyperparameters,model,with,pre-trained weights,model with pre-trained weights,0.6318606734275818
translation,34,93,hyperparameters,corresponding kbqa dataset,for,5 epochs,corresponding kbqa dataset for 5 epochs,0.6185848712921143
translation,34,93,hyperparameters,hyperparameters,For,albert training,hyperparameters For albert training,0.5677642822265625
translation,34,95,hyperparameters,maximum length,of,input sequence,maximum length of input sequence,0.5969871878623962
translation,34,95,hyperparameters,input sequence,to,128 tokens,input sequence to 128 tokens,0.5890701413154602
translation,34,95,hyperparameters,hyperparameters,set,learning rate,hyperparameters set learning rate,0.5994082689285278
translation,34,95,hyperparameters,hyperparameters,limit,maximum length,hyperparameters limit maximum length,0.6630126237869263
translation,34,5,model,expanded progressively,from,topic entity,expanded progressively from topic entity,0.5645278692245483
translation,34,5,model,topic entity,based on,sequence prediction model,topic entity based on sequence prediction model,0.5943480134010315
translation,34,5,model,model,has,query graph,model has query graph,0.5442538261413574
translation,34,6,model,new solution,to,query graph generation,new solution to query graph generation,0.5074654817581177
translation,34,6,model,gradually shrink,to,desired query graph,gradually shrink to desired query graph,0.5859631299972534
translation,34,6,model,model,propose,new solution,model propose new solution,0.733369767665863
translation,34,10,model,kb graph,consists of,general facts,kb graph consists of general facts,0.6252217292785645
translation,34,10,model,general facts,organized as,entity -relation -entity triplets,general facts organized as entity -relation -entity triplets,0.5695188045501709
translation,34,10,model,entity -relation -entity triplets,with,entities,entity -relation -entity triplets with entities,0.6000550985336304
translation,34,10,model,entity -relation -entity triplets,with,relations as edges,entity -relation -entity triplets with relations as edges,0.6289775371551514
translation,34,10,model,entities,as,vertices,entities as vertices,0.5046578049659729
translation,34,10,model,model,has,kb graph,model has kb graph,0.5812946557998657
translation,34,23,model,novel query graph generation method,start with,entire kb,novel query graph generation method start with entire kb,0.6149411201477051
translation,34,23,model,gradually shrink,to,desired query graph,gradually shrink to desired query graph,0.5859631299972534
translation,34,23,model,model,propose,novel query graph generation method,model propose novel query graph generation method,0.6567143201828003
translation,34,24,model,candidate query graph generation stage,rely on,cheap global features,candidate query graph generation stage rely on cheap global features,0.7253581881523132
translation,34,24,model,cheap global features,that capture,syntactic matches,cheap global features that capture syntactic matches,0.6847397089004517
translation,34,24,model,cheap global features,that capture,structure matches,cheap global features that capture structure matches,0.691331684589386
translation,34,24,model,syntactic matches,with,query,syntactic matches with query,0.6125298142433167
translation,34,24,model,structure matches,with,kb,structure matches with kb,0.6762175559997559
translation,34,24,model,model,In,candidate query graph generation stage,model In candidate query graph generation stage,0.4633605182170868
translation,34,24,model,model,rely on,cheap global features,model rely on cheap global features,0.7382599711418152
translation,34,7,results,efficiency and the accuracy,of,query graph generation,efficiency and the accuracy of query graph generation,0.5842322111129761
translation,34,7,results,query graph generation,especially for,complex multi-hop questions,query graph generation especially for complex multi-hop questions,0.6308000683784485
translation,34,30,results,candidates graphs,missed by,previous methods,candidates graphs missed by previous methods,0.7395254969596863
translation,34,31,results,our method,delivers,consistent performance,our method delivers consistent performance,0.6480218172073364
translation,34,31,results,our method,improves,state - of - the - art results,our method improves state - of - the - art results,0.5958585739135742
translation,34,31,results,our method,produces,competitive result,our method produces competitive result,0.6157906651496887
translation,34,31,results,consistent performance,on,two kbqa datasets,consistent performance on two kbqa datasets,0.5176708698272705
translation,34,31,results,consistent performance,improves,state - of - the - art results,consistent performance improves state - of - the - art results,0.6782955527305603
translation,34,31,results,state - of - the - art results,by,absolute 5.8 %,state - of - the - art results by absolute 5.8 %,0.5563094019889832
translation,34,31,results,absolute 5.8 %,in,f1,absolute 5.8 % in f1,0.5526605844497681
translation,34,31,results,absolute 5.8 %,on,multi-hop kbqa task cwq,absolute 5.8 % on multi-hop kbqa task cwq,0.5500349998474121
translation,34,31,results,competitive result,on,single - hop / two - hop kbqa task wqsp,competitive result on single - hop / two - hop kbqa task wqsp,0.5409279465675354
translation,34,31,results,results,show,our method,results show our method,0.6411243081092834
translation,34,99,results,outperforms,on,cwq,outperforms on cwq,0.5938496589660645
translation,34,99,results,existing methods,on,cwq,existing methods on cwq,0.5091539025306702
translation,34,99,results,competitive,on,wqsp,competitive on wqsp,0.5986847877502441
translation,34,99,results,our method,has,outperforms,our method has outperforms,0.6322360634803772
translation,34,99,results,outperforms,has,existing methods,outperforms has existing methods,0.5777831673622131
translation,34,99,results,staying,has,competitive,staying has competitive,0.6005924940109253
translation,34,99,results,results,show,our method,results show our method,0.6411243081092834
translation,34,100,results,most previous methods,perform,very well,most previous methods perform very well,0.5682828426361084
translation,34,100,results,most previous methods,perform,poorly,most previous methods perform poorly,0.6297414302825928
translation,34,100,results,very well,on,wqsp,very well on wqsp,0.5620153546333313
translation,34,100,results,poorly,on,cwq,poorly on cwq,0.6314089298248291
translation,34,100,results,results,see that,most previous methods,results see that most previous methods,0.6153854131698608
translation,34,112,results,significant performance boost,by predicting,top 5 predictions,significant performance boost by predicting top 5 predictions,0.734837532043457
translation,34,112,results,top 5 predictions,instead of,top 1,top 5 predictions instead of top 1,0.6380345225334167
translation,34,112,results,results,observe,significant performance boost,results observe significant performance boost,0.6425168514251709
translation,34,117,results,gold relations,to,rg q,gold relations to rg q,0.590611457824707
translation,34,117,results,gold relations,got,73.3 % f1,gold relations got 73.3 % f1,0.5686362385749817
translation,34,117,results,84.1 % upper bound f1,in,query graph generator,84.1 % upper bound f1 in query graph generator,0.5072779655456543
translation,34,117,results,73.3 % f1,in,final output,73.3 % f1 in final output,0.5070880651473999
translation,34,117,results,results,adding,gold relations,results adding gold relations,0.6444717645645142
translation,34,120,results,entity linking model,does not perform,very well,entity linking model does not perform very well,0.6996577382087708
translation,34,120,results,very well,on,cwq dataset,very well on cwq dataset,0.5162288546562195
translation,35,39,ablation-analysis,masr,improves,asr,masr improves asr,0.7015671730041504
translation,35,39,ablation-analysis,asr,by,additional 2 %,asr by additional 2 %,0.6360507607460022
translation,35,39,ablation-analysis,asr,confirming,benefit,asr confirming benefit,0.665854811668396
translation,35,39,ablation-analysis,benefit,of,joint modeling,benefit of joint modeling,0.5960713624954224
translation,35,39,ablation-analysis,ablation analysis,has,masr,ablation analysis has masr,0.5350204110145569
translation,35,236,ablation-analysis,significantly improves,by,2 %,significantly improves by 2 %,0.6792787313461304
translation,35,236,ablation-analysis,2 %,achieving,89.71,2 % achieving 89.71,0.5780853033065796
translation,35,236,ablation-analysis,p@1,has,significantly improves,p@1 has significantly improves,0.6494236588478088
translation,35,236,ablation-analysis,ablation analysis,has,p@1,ablation analysis has p@1,0.5971307754516602
translation,35,92,baselines,three joint model baselines,based on,multiclassifier approach,three joint model baselines based on multiclassifier approach,0.6482781767845154
translation,35,92,baselines,three joint model baselines,based on,pairwise joint model,three joint model baselines based on pairwise joint model,0.609303891658783
translation,35,92,baselines,three joint model baselines,based on,our adaptation of kgat model,three joint model baselines based on our adaptation of kgat model,0.6552857756614685
translation,35,92,baselines,multiclassifier approach,a,listwise method,multiclassifier approach a listwise method,0.6038582921028137
translation,35,92,baselines,pairwise joint model,operating over,k + 1 candidates,pairwise joint model operating over k + 1 candidates,0.7153461575508118
translation,35,92,baselines,our adaptation of kgat model,has,pairwise method,our adaptation of kgat model has pairwise method,0.5781137347221375
translation,35,92,baselines,baselines,designed,three joint model baselines,baselines designed three joint model baselines,0.606708288192749
translation,35,94,baselines,first baseline,is,transformer - based architecture,first baseline is transformer - based architecture,0.5589275360107422
translation,35,94,baselines,first baseline,provide,input,first baseline provide input,0.6361584067344666
translation,35,94,baselines,transformer - based architecture,concatenate,question,transformer - based architecture concatenate question,0.7357407808303833
translation,35,94,baselines,question,with,top k + 1 answer can-didates,question with top k + 1 answer can-didates,0.6636554598808289
translation,35,94,baselines,sep ] c 1 [ sep ] c 2 . . . [ sep ] c k + 1 ),provide,input,sep ] c 1 [ sep ] c 2 . . . [ sep ] c k + 1 ) provide input,0.622234582901001
translation,35,94,baselines,sep ] c 1 [ sep ] c 2 . . . [ sep ] c k + 1 ),used for,pointwise reranking,sep ] c 1 [ sep ] c 2 . . . [ sep ] c k + 1 ) used for pointwise reranking,0.6330422163009644
translation,35,94,baselines,input,to,same transformer model,input to same transformer model,0.6030125021934509
translation,35,94,baselines,same transformer model,used for,pointwise reranking,same transformer model used for pointwise reranking,0.686297595500946
translation,35,94,baselines,baselines,has,first baseline,baselines has first baseline,0.595583438873291
translation,35,157,baselines,masr,with,pointwise models,masr with pointwise models,0.673578679561615
translation,35,183,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,35,183,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,35,183,experimental-setup,learning rate,of,2e - 5,learning rate of 2e - 5,0.6410878896713257
translation,35,183,experimental-setup,learning rate,of,1e - 6,learning rate of 1e - 6,0.6236503720283508
translation,35,183,experimental-setup,learning rate,of,1e - 6,learning rate of 1e - 6,0.6236503720283508
translation,35,183,experimental-setup,2e - 5,for,transfer step,2e - 5 for transfer step,0.6439244747161865
translation,35,183,experimental-setup,transfer step,on,"asnq dataset ( garg et al. , 2020 )","transfer step on asnq dataset ( garg et al. , 2020 )",0.5550657510757446
translation,35,183,experimental-setup,learning rate,of,1e - 6,learning rate of 1e - 6,0.6236503720283508
translation,35,183,experimental-setup,1e - 6,for,adapt step,1e - 6 for adapt step,0.64079749584198
translation,35,183,experimental-setup,adapt step,on,target dataset,adapt step on target dataset,0.5860586166381836
translation,35,183,experimental-setup,experimental setup,adopt,adam optimizer,experimental setup adopt adam optimizer,0.6267623901367188
translation,35,184,experimental-setup,early stopping,on,development set,early stopping on development set,0.626629114151001
translation,35,184,experimental-setup,development set,of,target corpus,development set of target corpus,0.5579922199249268
translation,35,184,experimental-setup,development set,for,fine -tuning steps,development set for fine -tuning steps,0.6287121772766113
translation,35,184,experimental-setup,fine -tuning steps,based on,highest map score,fine -tuning steps based on highest map score,0.6301820874214172
translation,35,184,experimental-setup,experimental setup,apply,early stopping,experimental setup apply early stopping,0.6105810403823853
translation,35,185,experimental-setup,max number of epochs,equal to,3 and 9,max number of epochs equal to 3 and 9,0.7133554816246033
translation,35,185,experimental-setup,3 and 9,for,adapt and transfer steps,3 and 9 for adapt and transfer steps,0.6637951731681824
translation,35,185,experimental-setup,experimental setup,set,max number of epochs,experimental setup set max number of epochs,0.6472379565238953
translation,35,186,experimental-setup,maximum sequence length,for,roberta,maximum sequence length for roberta,0.6049983501434326
translation,35,186,experimental-setup,roberta,to,128 tokens,roberta to 128 tokens,0.6119385957717896
translation,35,186,experimental-setup,experimental setup,set,maximum sequence length,experimental setup set maximum sequence length,0.653292715549469
translation,35,187,experimental-setup,kgat and asr training,use,adam optimizer,kgat and asr training use adam optimizer,0.5871374011039734
translation,35,187,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,35,187,experimental-setup,learning rate,of,2e - 6,learning rate of 2e - 6,0.6322615146636963
translation,35,187,experimental-setup,2e - 6,for training,asr model,2e - 6 for training asr model,0.7948963642120361
translation,35,187,experimental-setup,asr model,on,target dataset,asr model on target dataset,0.5224350094795227
translation,35,187,experimental-setup,experimental setup,has,kgat and asr training,experimental setup has kgat and asr training,0.5206352472305298
translation,35,188,experimental-setup,1 tesla v100 gpu,with,32 gb memory,1 tesla v100 gpu with 32 gb memory,0.550624430179596
translation,35,188,experimental-setup,1 tesla v100 gpu,with,train batch size,1 tesla v100 gpu with train batch size,0.5802314877510071
translation,35,188,experimental-setup,train batch size,of,eight,train batch size of eight,0.5983097553253174
translation,35,188,experimental-setup,experimental setup,utilize,1 tesla v100 gpu,experimental setup utilize 1 tesla v100 gpu,0.5436634421348572
translation,35,189,experimental-setup,maximum sequence length,for,roberta base / large,maximum sequence length for roberta base / large,0.6003324389457703
translation,35,189,experimental-setup,roberta base / large,to,130 tokens,roberta base / large to 130 tokens,0.60138338804245
translation,35,189,experimental-setup,number of training epochs,to,20,number of training epochs to 20,0.5706267356872559
translation,35,189,experimental-setup,experimental setup,set,maximum sequence length,experimental setup set maximum sequence length,0.653292715549469
translation,35,189,experimental-setup,experimental setup,set,number of training epochs,experimental setup set number of training epochs,0.615318775177002
translation,35,193,experimental-setup,maximum sequence length,for,roberta,maximum sequence length for roberta,0.6049983501434326
translation,35,193,experimental-setup,roberta,to,128 tokens,roberta to 128 tokens,0.6119385957717896
translation,35,193,experimental-setup,number of epochs,to,20,number of epochs to 20,0.5974097847938538
translation,35,193,experimental-setup,experimental setup,set,maximum sequence length,experimental setup set maximum sequence length,0.653292715549469
translation,35,193,experimental-setup,experimental setup,set,number of epochs,experimental setup set number of epochs,0.6332279443740845
translation,35,5,model,critical step,effectively exploiting,answer set,critical step effectively exploiting answer set,0.6655730605125427
translation,35,5,model,answer set,regards modeling,interrelated information,answer set regards modeling interrelated information,0.8076685070991516
translation,35,5,model,interrelated information,between,pair of answers,interrelated information between pair of answers,0.6560966372489929
translation,35,5,model,model,shows,critical step,model shows critical step,0.7036672830581665
translation,35,6,model,three - way multiclassifier,decides if,answer,three - way multiclassifier decides if answer,0.7674350738525391
translation,35,6,model,three - way multiclassifier,decides if,neutral,three - way multiclassifier decides if neutral,0.726419985294342
translation,35,6,model,neutral,with respect to,another one,neutral with respect to another one,0.7093061804771423
translation,35,6,model,answer,has,supports,answer has supports,0.6819830536842346
translation,35,6,model,model,build,three - way multiclassifier,model build three - way multiclassifier,0.7021507620811462
translation,35,7,model,neural architecture,integrates,state - of - the - art as2 module,neural architecture integrates state - of - the - art as2 module,0.6709859371185303
translation,35,7,model,neural architecture,integrates,joint layer,neural architecture integrates joint layer,0.6581010818481445
translation,35,7,model,state - of - the - art as2 module,with,multi-classifier,state - of - the - art as2 module with multi-classifier,0.668552041053772
translation,35,7,model,joint layer,connecting,all components,joint layer connecting all components,0.7626243233680725
translation,35,7,model,model,has,neural architecture,model has neural architecture,0.5232567191123962
translation,35,38,results,asr,improves,best current model,asr improves best current model,0.702675998210907
translation,35,38,results,best current model,for,as2,best current model for as2,0.6457489728927612
translation,35,38,results,best current model,i.e.,tanda,best current model i.e. tanda,0.6451174020767212
translation,35,38,results,best current model,corresponding to,error reduction,best current model corresponding to error reduction,0.6002454161643982
translation,35,38,results,best current model,obtain,relative improvement,best current model obtain relative improvement,0.5516992807388306
translation,35,38,results,tanda,by,?3 %,tanda by ?3 %,0.6465451717376709
translation,35,38,results,tanda,on,wqa,tanda on wqa,0.6311390399932861
translation,35,38,results,tanda,on,wqa,tanda on wqa,0.6311390399932861
translation,35,38,results,results,show,asr,results show asr,0.6375046372413635
translation,35,213,results,map and mrr,of,stateof - the- art reranker,map and mrr of stateof - the- art reranker,0.559161901473999
translation,35,213,results,stateof - the- art reranker,by,garg et al . ( 2020 ),stateof - the- art reranker by garg et al . ( 2020 ),0.5087575316429138
translation,35,213,results,stateof - the- art reranker,on,wikiqa,stateof - the- art reranker on wikiqa,0.5366847515106201
translation,35,213,results,garg et al . ( 2020 ),on,wikiqa,garg et al . ( 2020 ) on wikiqa,0.5980104207992554
translation,35,214,results,joint model multi-classifier,performs,lower,joint model multi-classifier performs lower,0.619503378868103
translation,35,214,results,lower,than,pr,lower than pr,0.6234669089317322
translation,35,214,results,lower,than,all datasets,lower than all datasets,0.5863717198371887
translation,35,214,results,pr,for,all measures,pr for all measures,0.666786253452301
translation,35,214,results,pr,for,all datasets,pr for all datasets,0.635078489780426
translation,35,214,results,results,has,joint model multi-classifier,results has joint model multi-classifier,0.5673343539237976
translation,35,218,results,our kgat version,for,as2,our kgat version for as2,0.7113161683082581
translation,35,218,results,our kgat version,improves,pr,our kgat version improves pr,0.6918823719024658
translation,35,218,results,our kgat version,improves,almost all measures,our kgat version improves almost all measures,0.7011779546737671
translation,35,218,results,pr,over,all datasets,pr over all datasets,0.7124572992324829
translation,35,218,results,results,has,our kgat version,results has our kgat version,0.5584874153137207
translation,35,220,results,asr,achieves,highest performance,asr achieves highest performance,0.7147558331489563
translation,35,220,results,highest performance,among,all models,highest performance among all models,0.5869600772857666
translation,35,220,results,highest performance,among,all datasets,highest performance among all datasets,0.5523372292518616
translation,35,220,results,masr - fp,on,wqa,masr - fp on wqa,0.5888504981994629
translation,35,220,results,results,has,asr,results has asr,0.5573614239692688
translation,35,225,results,masr - fp,exploiting,fever,masr - fp exploiting fever,0.7545582056045532
translation,35,225,results,fever,for,initialization,fever for initialization,0.5869211554527283
translation,35,225,results,fever,performs,better,fever performs better,0.6133149266242981
translation,35,225,results,initialization,of,asc,initialization of asc,0.5905682444572449
translation,35,225,results,better,than,masr and masr - f,better than masr and masr - f,0.6248953938484192
translation,35,225,results,masr and masr - f,on,wikiqa and trec,masr and masr - f on wikiqa and trec,0.595065176486969
translation,35,225,results,results,has,masr - fp,results has masr - fp,0.5462759137153625
translation,35,226,results,asr,by,2 %,asr by 2 %,0.6372900605201721
translation,35,226,results,2 %,on,wqa,2 % on wqa,0.6741353869438171
translation,35,226,results,significantly outperforms,has,asr,significantly outperforms has asr,0.5969803929328918
translation,35,230,results,statistically significant difference,between,asr and all the baselines,statistically significant difference between asr and all the baselines,0.6156461834907532
translation,35,230,results,statistically significant difference,between,asr and all models ( i.e.,statistically significant difference between asr and all models ( i.e.,0.5532611012458801
translation,35,230,results,results,confirm,statistically significant difference,results confirm statistically significant difference,0.5647105574607849
translation,35,233,results,results,obtaining,slightly lower performance,results obtaining slightly lower performance,0.65909343957901
translation,35,233,results,slightly lower performance,on,wikiqa,slightly lower performance on wikiqa,0.5369560718536377
translation,35,233,results,higher,on,trec - qa,higher on trec - qa,0.617999255657196
translation,35,234,results,kgat,performs,lower,kgat performs lower,0.7054682970046997
translation,35,234,results,lower,than,pr,lower than pr,0.6234669089317322
translation,35,234,results,pr,on,both datasets,pr on both datasets,0.5737866759300232
translation,35,234,results,results,has,kgat,results has kgat,0.5510279536247253
translation,35,235,results,asr,establishes,new state of the art,asr establishes new state of the art,0.6369035243988037
translation,35,235,results,new state of the art,on,wikiqa,new state of the art on wikiqa,0.5839349627494812
translation,35,235,results,new state of the art,with,map,new state of the art with map,0.6661713123321533
translation,35,235,results,map,of,92.80 vs. 92.00,map of 92.80 vs. 92.00,0.5222721099853516
translation,35,235,results,results,has,asr,results has asr,0.5573614239692688
translation,35,237,results,pr,regarding,p@1,pr regarding p@1,0.6464186906814575
translation,35,237,results,trec - qa,has,asr,trec - qa has asr,0.5717166066169739
translation,35,237,results,asr,has,outperforms,asr has outperforms,0.6361175179481506
translation,35,237,results,outperforms,has,all models,outperforms has all models,0.5912554860115051
translation,35,237,results,results,on,trec - qa,results on trec - qa,0.5531675815582275
translation,36,8,model,novel absa model,with,federated learning ( fl ),novel absa model with federated learning ( fl ),0.6584933400154114
translation,36,8,model,novel absa model,incorporate,topic memory ( tm ),novel absa model incorporate topic memory ( tm ),0.6914800405502319
translation,36,8,model,federated learning ( fl ),adopted to overcome,data isolation limitations,federated learning ( fl ) adopted to overcome data isolation limitations,0.751625120639801
translation,36,8,model,topic memory ( tm ),proposed to take,cases of data,topic memory ( tm ) proposed to take cases of data,0.7053586840629578
translation,36,8,model,cases of data,from,diverse sources ( domains ),cases of data from diverse sources ( domains ),0.5671743750572205
translation,36,8,model,model,propose,novel absa model,model propose novel absa model,0.6627575755119324
translation,36,9,model,tm,aims to identify,different isolated data sources,tm aims to identify different isolated data sources,0.6414865255355835
translation,36,9,model,different isolated data sources,due to,data inaccessibility,different isolated data sources due to data inaccessibility,0.7086924910545349
translation,36,9,model,data inaccessibility,by providing,useful categorical information,data inaccessibility by providing useful categorical information,0.6561563611030579
translation,36,9,model,useful categorical information,for,localized predictions,useful categorical information for localized predictions,0.61472487449646
translation,36,9,model,model,has,tm,model has tm,0.63670814037323
translation,36,26,model,neural model,based on,fl,neural model based on fl,0.7306357622146606
translation,36,26,model,neural model,based on,fl,neural model based on fl,0.7306357622146606
translation,36,26,model,neural model,with,topic memory,neural model with topic memory,0.6496762633323669
translation,36,26,model,fl,for,absa,fl for absa,0.7519382834434509
translation,36,26,model,fl,by providing,categorical ( topic ) information,fl by providing categorical ( topic ) information,0.6543477773666382
translation,36,26,model,absa,in,distributed environment,absa in distributed environment,0.5284864902496338
translation,36,26,model,distributed environment,namely,tm - fl,distributed environment namely tm - fl,0.6765508651733398
translation,36,26,model,topic memory,to enhance,fl,topic memory to enhance fl,0.709194004535675
translation,36,26,model,fl,by providing,categorical ( topic ) information,fl by providing categorical ( topic ) information,0.6543477773666382
translation,36,26,model,model,propose,neural model,model propose neural model,0.6627441048622131
translation,36,27,model,topic model,serves as,server-side component,topic model serves as server-side component,0.6213348507881165
translation,36,27,model,server-side component,to read,different inputs,server-side component to read different inputs,0.531682550907135
translation,36,27,model,server-side component,respond with,categorical weights,server-side component respond with categorical weights,0.7310680150985718
translation,36,27,model,different inputs,from,each node,different inputs from each node,0.5898248553276062
translation,36,27,model,categorical weights,to help,backbone absa classifier,categorical weights to help backbone absa classifier,0.6313904523849487
translation,36,27,model,model,has,topic model,model has topic model,0.5422954559326172
translation,36,28,model,absa,by leveraging,extra labeled data,absa by leveraging extra labeled data,0.6621009707450867
translation,36,28,model,extra labeled data,through,fl framework,extra labeled data through fl framework,0.6398017406463623
translation,36,28,model,fl framework,enhanced by,tm,fl framework enhanced by tm,0.7052283883094788
translation,37,108,baselines,"lstm , bert - base , and bert - large",as,text encoder,"lstm , bert - base , and bert - large as text encoder",0.5029577612876892
translation,37,108,baselines,words,in,real samples,words in real samples,0.5081813931465149
translation,37,108,baselines,real samples,with,synonyms,real samples with synonyms,0.5969439744949341
translation,37,108,baselines,synonyms,from,wordnet,synonyms from wordnet,0.47392985224723816
translation,37,108,baselines,synonyms,to generate,synonymous samples,synonyms to generate synonymous samples,0.6089966893196106
translation,37,108,baselines,baselines,employ,"lstm , bert - base , and bert - large","baselines employ lstm , bert - base , and bert - large",0.5001193284988403
translation,37,109,baselines,back- tran,translates,real,back- tran translates real,0.715012788772583
translation,37,109,baselines,back- tran,translates it back to,source language,back- tran translates it back to source language,0.6790032982826233
translation,37,109,baselines,real,to,other language,real to other language,0.5759230852127075
translation,37,109,baselines,source language,to get,synonymous samples,source language to get synonymous samples,0.6582463979721069
translation,37,109,baselines,back- tran,has,),back- tran has ),0.6873273849487305
translation,37,109,baselines,baselines,has,back- tran,baselines has back- tran,0.6146595478057861
translation,37,110,baselines,language model,to obtain,synonyms,language model to obtain synonyms,0.5520969033241272
translation,37,110,baselines,language model,randomly replaces,words,language model randomly replaces words,0.6915287375450134
translation,37,110,baselines,synonyms,for,each word,synonyms for each word,0.5624315142631531
translation,37,110,baselines,words,with,synonyms,words with synonyms,0.5573168992996216
translation,37,110,baselines,words,to obtain,adversarial samples,words to obtain adversarial samples,0.5643887519836426
translation,37,111,baselines,vat,improves,model robustness,vat improves model robustness,0.6785385012626648
translation,37,111,baselines,"al. , 2017 )",improves,model robustness,"al. , 2017 ) improves model robustness",0.6107215881347656
translation,37,111,baselines,model robustness,by adding,random perturbation,model robustness by adding random perturbation,0.7124195098876953
translation,37,111,baselines,random perturbation,to,embedding layer,random perturbation to embedding layer,0.5275986194610596
translation,37,111,baselines,embedding layer,to obtain,new adversarial examples,embedding layer to obtain new adversarial examples,0.566064178943634
translation,37,111,baselines,baselines,has,vat,baselines has vat,0.5670900940895081
translation,37,112,baselines,"lexicalat ( xu et al. , 2019 )",first uses,generator,"lexicalat ( xu et al. , 2019 ) first uses generator",0.7552493214607239
translation,37,112,baselines,"lexicalat ( xu et al. , 2019 )",jointly optimizes,generator and the discriminator,"lexicalat ( xu et al. , 2019 ) jointly optimizes generator and the discriminator",0.732206404209137
translation,37,112,baselines,generator,to randomly replace,words,generator to randomly replace words,0.7517526149749756
translation,37,112,baselines,words,with,"synonym , hyponym or hypernym","words with synonym , hyponym or hypernym",0.6123741269111633
translation,37,112,baselines,"synonym , hyponym or hypernym",to obtain,new samples,"synonym , hyponym or hypernym to obtain new samples",0.5714682340621948
translation,37,112,baselines,generator and the discriminator,based on,adversarial learning,generator and the discriminator based on adversarial learning,0.6645207405090332
translation,37,112,baselines,baselines,has,"lexicalat ( xu et al. , 2019 )","baselines has lexicalat ( xu et al. , 2019 )",0.5249496102333069
translation,37,113,baselines,dsa,first replaces,original words,dsa first replaces original words,0.7079789638519287
translation,37,113,baselines,dsa,employs,original and antonymous samples,dsa employs original and antonymous samples,0.5448059439659119
translation,37,113,baselines,original words,with,antonyms,original words with antonyms,0.5326042771339417
translation,37,113,baselines,antonyms,from,word - net,antonyms from word - net,0.5408233404159546
translation,37,113,baselines,original and antonymous samples,for,dual sentiment analysis,original and antonymous samples for dual sentiment analysis,0.5897910594940186
translation,37,113,baselines,dual sentiment analysis,under,softmax regression,dual sentiment analysis under softmax regression,0.6267041563987732
translation,37,113,baselines,baselines,has,dsa,baselines has dsa,0.5477825403213501
translation,37,114,baselines,"agc ( wang and culotta , 2021 )",first uses,wordnet,"agc ( wang and culotta , 2021 ) first uses wordnet",0.6389485001564026
translation,37,114,baselines,"agc ( wang and culotta , 2021 )",first uses,word substitution method,"agc ( wang and culotta , 2021 ) first uses word substitution method",0.7161074280738831
translation,37,114,baselines,"agc ( wang and culotta , 2021 )",uses,word substitution method,"agc ( wang and culotta , 2021 ) uses word substitution method",0.6050508618354797
translation,37,114,baselines,wordnet,to obtain,antonyms,wordnet to obtain antonyms,0.5822159051895142
translation,37,114,baselines,antonyms,for,n most important words,antonyms for n most important words,0.5581554174423218
translation,37,114,baselines,n most important words,in,corpus,n most important words in corpus,0.5049900412559509
translation,37,114,baselines,word substitution method,to obtain,counterfactual samples,word substitution method to obtain counterfactual samples,0.5801947712898254
translation,37,114,baselines,counterfactual samples,to improve,model robustness,counterfactual samples to improve model robustness,0.6248697638511658
translation,37,114,baselines,baselines,has,"agc ( wang and culotta , 2021 )","baselines has agc ( wang and culotta , 2021 )",0.5473138093948364
translation,37,104,experiments,bert text encoder,set,batch size,bert text encoder set batch size,0.6978698372840881
translation,37,104,experiments,bert text encoder,set,learning rate,bert text encoder set learning rate,0.6678078174591064
translation,37,104,experiments,learning rate,to,8 and 2e - 5,learning rate to 8 and 2e - 5,0.5892360210418701
translation,37,101,hyperparameters,warm up stage,train,generator,warm up stage train generator,0.7232978343963623
translation,37,101,hyperparameters,warm up stage,train,both the generator and the antonymous sentiment predictor,warm up stage train both the generator and the antonymous sentiment predictor,0.6975128054618835
translation,37,101,hyperparameters,warm up stage,train,original sentiment predictor,warm up stage train original sentiment predictor,0.7139649391174316
translation,37,101,hyperparameters,warm up stage,train,both the generator and the antonymous sentiment predictor,warm up stage train both the generator and the antonymous sentiment predictor,0.6975128054618835
translation,37,101,hyperparameters,generator,for,40 epochs,generator for 40 epochs,0.5854642987251282
translation,37,101,hyperparameters,generator,train,original sentiment predictor,generator train original sentiment predictor,0.7097720503807068
translation,37,101,hyperparameters,original sentiment predictor,for,100 epochs,original sentiment predictor for 100 epochs,0.570968508720398
translation,37,101,hyperparameters,both the generator and the antonymous sentiment predictor,based on,reinforcement learning,both the generator and the antonymous sentiment predictor based on reinforcement learning,0.6504738926887512
translation,37,101,hyperparameters,reinforcement learning,for,60 epochs,reinforcement learning for 60 epochs,0.539469301700592
translation,37,101,hyperparameters,hyperparameters,In,warm up stage,hyperparameters In warm up stage,0.49243953824043274
translation,37,101,hyperparameters,hyperparameters,train,original sentiment predictor,hyperparameters train original sentiment predictor,0.6734943389892578
translation,37,101,hyperparameters,hyperparameters,train,both the generator and the antonymous sentiment predictor,hyperparameters train both the generator and the antonymous sentiment predictor,0.6621475219726562
translation,37,102,hyperparameters,generator,set,size,generator set size,0.7279884219169617
translation,37,102,hyperparameters,generator,set,batch size,generator set batch size,0.6991673707962036
translation,37,102,hyperparameters,generator,set,learning rate,generator set learning rate,0.6817117929458618
translation,37,102,hyperparameters,size,of,hidden dimension,size of hidden dimension,0.5746702551841736
translation,37,102,hyperparameters,size,of,batch size,size of batch size,0.6234161257743835
translation,37,102,hyperparameters,size,of,learning rate,size of learning rate,0.5890579223632812
translation,37,102,hyperparameters,size,of,sentence sampling times m,size of sentence sampling times m,0.6078203320503235
translation,37,102,hyperparameters,sentence sampling times m,to,"300 , 8 , 1e - 3 , and 32","sentence sampling times m to 300 , 8 , 1e - 3 , and 32",0.588293194770813
translation,37,102,hyperparameters,hyperparameters,For,generator,hyperparameters For generator,0.5813727974891663
translation,37,103,hyperparameters,lstm text encoder,set,batch size,lstm text encoder set batch size,0.6177190542221069
translation,37,103,hyperparameters,lstm text encoder,set,size of hidden dimension,lstm text encoder set size of hidden dimension,0.6077120900154114
translation,37,103,hyperparameters,lstm text encoder,set,learning rate,lstm text encoder set learning rate,0.5756711363792419
translation,37,103,hyperparameters,lstm text encoder,set,embedding drop rate,lstm text encoder set embedding drop rate,0.5912277698516846
translation,37,103,hyperparameters,lstm text encoder,set,representation dropout rate,lstm text encoder set representation dropout rate,0.5639720559120178
translation,37,103,hyperparameters,representation dropout rate,to,"64 , 300 , 1e -3 , 0.4 , and 0.1","representation dropout rate to 64 , 300 , 1e -3 , 0.4 , and 0.1",0.5469456911087036
translation,37,103,hyperparameters,hyperparameters,For,lstm text encoder,hyperparameters For lstm text encoder,0.5060368180274963
translation,37,106,hyperparameters,parameters,optimized with,adam optimizer,parameters optimized with adam optimizer,0.6940226554870605
translation,37,106,hyperparameters,parameters,tuned on,development set,parameters tuned on development set,0.7338297963142395
translation,37,106,hyperparameters,hyperparameters,has,parameters,hyperparameters has parameters,0.4783959984779358
translation,37,6,model,end-toend reinforcement learning framework,jointly performs,counterfactual data generation,end-toend reinforcement learning framework jointly performs counterfactual data generation,0.7205265760421753
translation,37,6,model,end-toend reinforcement learning framework,jointly performs,dual sentiment classification,end-toend reinforcement learning framework jointly performs dual sentiment classification,0.72553551197052
translation,37,6,model,model,propose,end-toend reinforcement learning framework,model propose end-toend reinforcement learning framework,0.6835977435112
translation,37,7,model,generator,automatically generates,massive and diverse antonymous sentences,generator automatically generates massive and diverse antonymous sentences,0.7836265563964844
translation,37,7,model,generator,iteratively generate,higher -quality antonymous samples,generator iteratively generate higher -quality antonymous samples,0.7532157897949219
translation,37,7,model,discriminator,contains,original - side sentiment predictor,discriminator contains original - side sentiment predictor,0.5719584226608276
translation,37,7,model,discriminator,contains,antonymous -side sentiment predictor,discriminator contains antonymous -side sentiment predictor,0.6077946424484253
translation,37,7,model,discriminator,help,generator,discriminator help generator,0.680752158164978
translation,37,7,model,discriminator,directly used as,final sentiment classifier,discriminator directly used as final sentiment classifier,0.6858039498329163
translation,37,7,model,generator,iteratively generate,higher -quality antonymous samples,generator iteratively generate higher -quality antonymous samples,0.7532157897949219
translation,37,7,model,discriminator,directly used as,final sentiment classifier,discriminator directly used as final sentiment classifier,0.6858039498329163
translation,37,22,model,end-to - end reinforcement learning framework,named,reinforced counterfactual data augmentation ( rcda ),end-to - end reinforcement learning framework named reinforced counterfactual data augmentation ( rcda ),0.6712717413902283
translation,37,22,model,end-to - end reinforcement learning framework,for,dual sentiment classification,end-to - end reinforcement learning framework for dual sentiment classification,0.5502049326896667
translation,37,22,model,reinforced counterfactual data augmentation ( rcda ),for,joint counterfactual data augmentation,reinforced counterfactual data augmentation ( rcda ) for joint counterfactual data augmentation,0.6186631321907043
translation,37,22,model,reinforced counterfactual data augmentation ( rcda ),for,dual sentiment classification,reinforced counterfactual data augmentation ( rcda ) for dual sentiment classification,0.6277786493301392
translation,37,22,model,model,propose,end-to - end reinforcement learning framework,model propose end-to - end reinforcement learning framework,0.6760078072547913
translation,37,23,model,dual sentiment classification modules,regarded as,generator and a discriminator,dual sentiment classification modules regarded as generator and a discriminator,0.5756207704544067
translation,37,23,model,dual sentiment classification modules,integrated in,reinforcement learning framework,dual sentiment classification modules integrated in reinforcement learning framework,0.6362009048461914
translation,37,23,model,model,has,counterfactual sentence generation,model has counterfactual sentence generation,0.5366325974464417
translation,37,24,model,one-tomany antonym and synonym lists,obtained from,wordnet,one-tomany antonym and synonym lists obtained from wordnet,0.6053820848464966
translation,37,24,model,one-tomany antonym and synonym lists,to generate,massive antonymous candidates,one-tomany antonym and synonym lists to generate massive antonymous candidates,0.7121157646179199
translation,37,24,model,massive antonymous candidates,based on,multi-label learning,massive antonymous candidates based on multi-label learning,0.5807567238807678
translation,37,24,model,best antonymous sentence,based on,reinforcement learning,best antonymous sentence based on reinforcement learning,0.6167916655540466
translation,37,32,model,antonymous and original samples,as,pairs,antonymous and original samples as pairs,0.5899985432624817
translation,37,32,model,antonymous and original samples,feed them to,discriminator,antonymous and original samples feed them to discriminator,0.6295415163040161
translation,37,32,model,discriminator,for,dual training and prediction,discriminator for dual training and prediction,0.5969334244728088
translation,37,32,model,discriminator,alleviates,spurious association problem,discriminator alleviates spurious association problem,0.7287067174911499
translation,37,32,model,spurious association problem,in,sentiment classification,spurious association problem in sentiment classification,0.5145894289016724
translation,37,32,model,model,regard,antonymous and original samples,model regard antonymous and original samples,0.6203927993774414
translation,37,32,model,model,feed them to,discriminator,model feed them to discriminator,0.649038553237915
translation,37,117,results,all the compared systems,by using,"lstm , bert - base , and bert - large","all the compared systems by using lstm , bert - base , and bert - large",0.6449898481369019
translation,37,117,results,"lstm , bert - base , and bert - large",as,our text encoder,"lstm , bert - base , and bert - large as our text encoder",0.49499526619911194
translation,37,117,results,our rcda method,has,consistently outperforms,our rcda method has consistently outperforms,0.6045581698417664
translation,37,117,results,consistently outperforms,has,all the compared systems,consistently outperforms has all the compared systems,0.5988839864730835
translation,37,117,results,results,observe,our rcda method,results observe our rcda method,0.5373309850692749
translation,37,118,results,outperforms,by around,2 absolute percentage points,outperforms by around 2 absolute percentage points,0.7714290022850037
translation,37,118,results,baseline approach,by around,2 absolute percentage points,baseline approach by around 2 absolute percentage points,0.6854258179664612
translation,37,118,results,2 absolute percentage points,on,accuracy,2 absolute percentage points on accuracy,0.5429591536521912
translation,37,118,results,accuracy,for,each data set,accuracy for each data set,0.6605020761489868
translation,37,118,results,lstm text encoder,has,rcda,lstm text encoder has rcda,0.5364476442337036
translation,37,118,results,rcda,has,outperforms,rcda has outperforms,0.6361355781555176
translation,37,118,results,outperforms,has,baseline approach,outperforms has baseline approach,0.6255874633789062
translation,37,118,results,results,for,lstm text encoder,results for lstm text encoder,0.5615063309669495
translation,37,119,results,rcda,outperforms,bert - base,rcda outperforms bert - base,0.7588773965835571
translation,37,119,results,bert - base,by,0.46 %,bert - base by 0.46 %,0.5754464864730835
translation,37,119,results,bert - base,by,0.36 %,bert - base by 0.36 %,0.5784401297569275
translation,37,119,results,bert - base,by,1.09 %,bert - base by 1.09 %,0.5870839953422546
translation,37,119,results,bert - base,by,0.4 %,bert - base by 0.4 %,0.5949905514717102
translation,37,119,results,0.46 %,on,sst -2,0.46 % on sst -2,0.5776009559631348
translation,37,119,results,0.46 %,on,sst -5,0.46 % on sst -5,0.5974200367927551
translation,37,119,results,0.36 %,on,sst -5,0.36 % on sst -5,0.6010051965713501
translation,37,119,results,1.09 %,on,rt,1.09 % on rt,0.670504629611969
translation,37,119,results,0.4 %,on,yelp,0.4 % on yelp,0.5976778864860535
translation,37,119,results,bert text encoder,has,rcda,bert text encoder has rcda,0.5541983842849731
translation,37,119,results,results,For,bert text encoder,results For bert text encoder,0.596274733543396
translation,37,120,results,bert - large,reaches,highly competitive results,bert - large reaches highly competitive results,0.7518163919448853
translation,37,120,results,performance,across,four datasets,performance across four datasets,0.6978843212127686
translation,37,120,results,our rcda approach,has,significantly boost,our rcda approach has significantly boost,0.5751357078552246
translation,37,120,results,significantly boost,has,performance,significantly boost has performance,0.5819864869117737
translation,37,120,results,results,has,bert - large,results has bert - large,0.6052632927894592
translation,37,121,results,most existing data augmentation - based methods,including,synda,most existing data augmentation - based methods including synda,0.7194470763206482
translation,37,121,results,most existing data augmentation - based methods,including,conda,most existing data augmentation - based methods including conda,0.6667905449867249
translation,37,121,results,most existing data augmentation - based methods,including,vat,most existing data augmentation - based methods including vat,0.6963180303573608
translation,37,121,results,most existing data augmentation - based methods,including,dsa,most existing data augmentation - based methods including dsa,0.6888183951377869
translation,37,121,results,most existing data augmentation - based methods,including,agc,most existing data augmentation - based methods including agc,0.7091765999794006
translation,37,121,results,agc,across,four datasets,agc across four datasets,0.7461937665939331
translation,37,121,results,rcda approach,has,consistently outperforms,rcda approach has consistently outperforms,0.6248309016227722
translation,37,121,results,consistently outperforms,has,most existing data augmentation - based methods,consistently outperforms has most existing data augmentation - based methods,0.5539087057113647
translation,37,121,results,results,observe that,rcda approach,results observe that rcda approach,0.608716607093811
translation,37,122,results,our rcda method,achieve,better performance,our rcda method achieve better performance,0.6039889454841614
translation,37,122,results,better performance,across,four datasets,better performance across four datasets,0.6609357595443726
translation,37,122,results,bert - large,as,text encoder,bert - large as text encoder,0.5933511853218079
translation,38,85,baselines,distilled version,of,"xlm -r ( conneau et al. , 2020 )","distilled version of xlm -r ( conneau et al. , 2020 )",0.5599778890609741
translation,38,86,baselines,baselines qa no-synth,finetuned,multilingual models,baselines qa no-synth finetuned multilingual models,0.7045964598655701
translation,38,86,baselines,multilingual models,on,squad en,multilingual models on squad en,0.606971263885498
translation,38,86,baselines,baselines,has,baselines qa no-synth,baselines has baselines qa no-synth,0.6112574934959412
translation,38,87,baselines,baselines,has,english as pivot,baselines has english as pivot,0.5930584669113159
translation,38,116,experimental-setup,qg and qa,trained,model,qg and qa trained model,0.7319783568382263
translation,38,116,experimental-setup,model,for,5 epochs,model for 5 epochs,0.653461217880249
translation,38,116,experimental-setup,experimental setup,trained,model,experimental setup trained model,0.6766582131385803
translation,38,117,experimental-setup,single nvidia gtx2080 ti,with,11g ram,single nvidia gtx2080 ti with 11g ram,0.614742636680603
translation,38,117,experimental-setup,training times,amount to,circa 4 and 2 hours,training times amount to circa 4 and 2 hours,0.72402024269104
translation,38,117,experimental-setup,circa 4 and 2 hours,for,question generation,circa 4 and 2 hours for question generation,0.6712014675140381
translation,38,117,experimental-setup,circa 4 and 2 hours,for,question answering,circa 4 and 2 hours for question answering,0.631292462348938
translation,38,117,experimental-setup,circa 4 and 2 hours,for,question answering,circa 4 and 2 hours for question answering,0.631292462348938
translation,38,117,experimental-setup,experimental setup,used,single nvidia gtx2080 ti,experimental setup used single nvidia gtx2080 ti,0.6175698637962341
translation,38,117,experimental-setup,experimental setup,used,training times,experimental setup used training times,0.6367064714431763
translation,38,185,experiments,synthetic with translation ( + synth-trans ),For,minilm + synth-trans,synthetic with translation ( + synth-trans ) For minilm + synth-trans,0.6311688423156738
translation,38,185,experiments,synthetic with translation ( + synth-trans ),obtain,much larger improvement,synthetic with translation ( + synth-trans ) obtain much larger improvement,0.5649541020393372
translation,38,185,experiments,minilm + synth-trans,obtain,much larger improvement,minilm + synth-trans obtain much larger improvement,0.5303274393081665
translation,38,185,experiments,much larger improvement,over,baselines,much larger improvement over baselines,0.6982489228248596
translation,38,185,experiments,much larger improvement,compared to,minilm +synth,much larger improvement compared to minilm +synth,0.6322489380836487
translation,38,185,experiments,much larger improvement,on,mlqa and xquad,much larger improvement on mlqa and xquad,0.5911783576011658
translation,38,22,model,synthetic data,in,cross-lingual fashion,synthetic data in cross-lingual fashion,0.5506853461265564
translation,38,22,model,model,propose to generate,synthetic data,model propose to generate synthetic data,0.7345290780067444
translation,38,26,results,method,leveraging on,translator,method leveraging on translator,0.655489981174469
translation,38,26,results,translator,shown to perform,best,translator shown to perform best,0.75042724609375
translation,38,26,results,results,has,method,results has method,0.49327942728996277
translation,38,27,results,best model,obtains,significant improvements,best model obtains significant improvements,0.5922368764877319
translation,38,27,results,significant improvements,on,xquad and mlqa,significant improvements on xquad and mlqa,0.5721685886383057
translation,38,27,results,xquad and mlqa,over,state - of - the - art,xquad and mlqa over state - of - the - art,0.6726697087287903
translation,38,27,results,state - of - the - art,for,exact match and f1 scores,state - of - the - art for exact match and f1 scores,0.5702877640724182
translation,38,179,results,synthetic without translation ( + synth ),Compared to,minilm baseline,synthetic without translation ( + synth ) Compared to minilm baseline,0.6110180020332336
translation,38,179,results,synthetic without translation ( + synth ),observe,small performance increase,synthetic without translation ( + synth ) observe small performance increase,0.6071969270706177
translation,38,179,results,small performance increase,for,minilm +synth,small performance increase for minilm +synth,0.6083071231842041
translation,38,179,results,29.5 to 33.1,on,xquad,29.5 to 33.1 on xquad,0.5246676206588745
translation,38,179,results,26.0 to 27.5,on,mlqa,26.0 to 27.5 on mlqa,0.5168141722679138
translation,38,179,results,results,has,synthetic without translation ( + synth ),results has synthetic without translation ( + synth ),0.585947573184967
translation,38,186,results,outperforms,has,minilm + squadtrans,outperforms has minilm + squadtrans,0.6028479933738708
translation,38,186,results,results,has,outperforms,results has outperforms,0.6657275557518005
translation,38,201,results,outperforms,more than,4 exact match points,outperforms more than 4 exact match points,0.5783137679100037
translation,38,201,results,baseline,by,4 exact match points,baseline by 4 exact match points,0.543025553226471
translation,38,201,results,baseline,more than,4 exact match points,baseline more than 4 exact match points,0.5405909419059753
translation,38,201,results,xlm -r + synth-trans,obtains,new state - of- the- art,xlm -r + synth-trans obtains new state - of- the- art,0.637572705745697
translation,38,201,results,our minilm + synth-trans model,has,outperforms,our minilm + synth-trans model has outperforms,0.6163621544837952
translation,38,201,results,outperforms,has,baseline,outperforms has baseline,0.6131853461265564
translation,38,201,results,results,has,our minilm + synth-trans model,results has our minilm + synth-trans model,0.5441769361495972
translation,38,202,results,camembert,on,piaf,camembert on piaf,0.6498850584030151
translation,38,202,results,multilingual xlm -r +synth-trans,has,outperforms,multilingual xlm -r +synth-trans has outperforms,0.6228988170623779
translation,38,202,results,outperforms,has,camembert,outperforms has camembert,0.6589992046356201
translation,38,202,results,results,has,multilingual xlm -r +synth-trans,results has multilingual xlm -r +synth-trans,0.5763124227523804
translation,38,219,results,relative improvement,for,best synthetic configuration + synthtrans,relative improvement for best synthetic configuration + synthtrans,0.6158186197280884
translation,38,219,results,best synthetic configuration + synthtrans,over,baseline,best synthetic configuration + synthtrans over baseline,0.6672102808952332
translation,38,219,results,above 60 % em,for,minilm,above 60 % em for minilm,0.7031336426734924
translation,38,219,results,minilm,from,29.5 to 49.5,minilm from 29.5 to 49.5,0.5674781203269958
translation,38,219,results,minilm,from,26.0 to 41.4,minilm from 26.0 to 41.4,0.5403201580047607
translation,38,219,results,29.5 to 49.5,on,xquad,29.5 to 49.5 on xquad,0.5277249217033386
translation,38,219,results,26.0 to 41.4,on,mlqa,26.0 to 41.4 on mlqa,0.4671470820903778
translation,38,219,results,results,has,relative improvement,results has relative improvement,0.5807216763496399
translation,39,4,model,model,has,end - to- end question answering,model has end - to- end question answering,0.5329745411872864
translation,39,6,model,multiple-entity questions,by implementing,new intersection operation,multiple-entity questions by implementing new intersection operation,0.7220702767372131
translation,39,6,model,new intersection operation,identifies,shared elements,new intersection operation identifies shared elements,0.6338419914245605
translation,39,6,model,model,explicitly handles,multiple-entity questions,model explicitly handles multiple-entity questions,0.7123836874961853
translation,39,23,model,multiple-entity questions,in,e2eqa,multiple-entity questions in e2eqa,0.5262748003005981
translation,39,23,model,multiple-entity questions,by learning,intersection,multiple-entity questions by learning intersection,0.7309285998344421
translation,39,23,model,intersection,in,dynamic multi-hop setting,intersection in dynamic multi-hop setting,0.5577836632728577
translation,39,23,model,model,explicitly handle,multiple-entity questions,model explicitly handle multiple-entity questions,0.7169173955917358
translation,39,24,model,our intersection models,learn to,follow relations,our intersection models learn to follow relations,0.6825781464576721
translation,39,24,model,our intersection models,intersect,sets,our intersection models intersect sets,0.6779273152351379
translation,39,24,model,sets,of,resulting entities,sets of resulting entities,0.5858383178710938
translation,39,24,model,sets,to arrive at,correct answer,sets to arrive at correct answer,0.6760862469673157
translation,39,24,model,model,has,our intersection models,model has our intersection models,0.5967901349067688
translation,39,55,model,key improvements,to,rei-fiedkb,key improvements to rei-fiedkb,0.6171467304229736
translation,39,55,model,model,make,key improvements,model make key improvements,0.7180938124656677
translation,39,56,model,"roberta ( liu et al. , 2019 )",as,our encoder,"roberta ( liu et al. , 2019 ) as our encoder",0.5237975120544434
translation,39,56,model,our encoder,instead of,word2vec,our encoder instead of word2vec,0.6265154480934143
translation,39,56,model,model,use,"roberta ( liu et al. , 2019 )","model use roberta ( liu et al. , 2019 )",0.6156731843948364
translation,39,100,model,end-to - end question answering model,using,differentiable knowledge graphs,end-to - end question answering model using differentiable knowledge graphs,0.6130936145782471
translation,39,100,model,differentiable knowledge graphs,to learn,intersection operation,differentiable knowledge graphs to learn intersection operation,0.5851163864135742
translation,39,100,model,model,expand,end-to - end question answering model,model expand end-to - end question answering model,0.633876621723175
translation,39,25,results,our mod-els,score,73.3 %,our mod-els score 73.3 %,0.7549472451210022
translation,39,25,results,our mod-els,score,48.7 %,our mod-els score 48.7 %,0.750718891620636
translation,39,25,results,our mod-els,score,improve,our mod-els score improve,0.6611192226409912
translation,39,25,results,73.3 %,on,webquestionssp,73.3 % on webquestionssp,0.5122502446174622
translation,39,25,results,48.7 %,on,complexwebquestions,48.7 % on complexwebquestions,0.5262911319732666
translation,39,25,results,48.7 %,on,complexwebquestions,48.7 % on complexwebquestions,0.5262911319732666
translation,39,25,results,improve,upon,baseline,improve upon baseline,0.6592898368835449
translation,39,25,results,baseline,on,questions,baseline on questions,0.5453183650970459
translation,39,25,results,questions,with,multiple entities,questions with multiple entities,0.6101043820381165
translation,39,25,results,multiple entities,from,56.3 % to 70.6 %,multiple entities from 56.3 % to 70.6 %,0.5045324563980103
translation,39,25,results,multiple entities,from,36.8 % to 55.8 %,multiple entities from 36.8 % to 55.8 %,0.5016617178916931
translation,39,25,results,56.3 % to 70.6 %,on,webquestionssp,56.3 % to 70.6 % on webquestionssp,0.5670322775840759
translation,39,25,results,36.8 % to 55.8 %,on,complexwebquestions,36.8 % to 55.8 % on complexwebquestions,0.5673023462295532
translation,39,25,results,results,find that,our mod-els,results find that our mod-els,0.6200116872787476
translation,39,101,results,intersection,improves,performance,intersection improves performance,0.6978437900543213
translation,39,101,results,performance,on,webquestionssp and complexwebquestions,performance on webquestionssp and complexwebquestions,0.531686544418335
translation,39,101,results,results,show,intersection,results show intersection,0.588948130607605
translation,39,101,results,results,introducing,intersection,results introducing intersection,0.6549661159515381
translation,39,114,results,rigel-baseline and rigel-intersect,are,comparable,rigel-baseline and rigel-intersect are comparable,0.6243986487388611
translation,39,114,results,rigel-baseline and rigel-intersect,surpasses,rigel - baseline,rigel-baseline and rigel-intersect surpasses rigel - baseline,0.6807758808135986
translation,39,114,results,rigel-baseline and rigel-intersect,on,questions,rigel-baseline and rigel-intersect on questions,0.573737382888794
translation,39,114,results,comparable,on,questions,comparable on questions,0.5794209241867065
translation,39,114,results,comparable,on,questions,comparable on questions,0.5794209241867065
translation,39,114,results,comparable,on,questions,comparable on questions,0.5794209241867065
translation,39,114,results,questions,with,one entity,questions with one entity,0.6275638341903687
translation,39,114,results,questions,with,more than 1 entity,questions with more than 1 entity,0.6210627555847168
translation,39,114,results,questions,with,more than 1 entity,questions with more than 1 entity,0.6210627555847168
translation,39,114,results,rigel - baseline,on,questions,rigel - baseline on questions,0.5847854614257812
translation,39,114,results,rigel - baseline,by,over 14 %,rigel - baseline by over 14 %,0.614296019077301
translation,39,114,results,rigel - baseline,by,19 %,rigel - baseline by 19 %,0.6233248710632324
translation,39,114,results,questions,with,more than 1 entity,questions with more than 1 entity,0.6210627555847168
translation,39,114,results,more than 1 entity,by,over 14 %,more than 1 entity by over 14 %,0.5761235952377319
translation,39,114,results,more than 1 entity,by,19 %,more than 1 entity by 19 %,0.6111951470375061
translation,39,114,results,19 %,on,com-plexwebquestions,19 % on com-plexwebquestions,0.5665810108184814
translation,39,114,results,com-plexwebquestions,has,36.8 % vs. 55.8 %,com-plexwebquestions has 36.8 % vs. 55.8 %,0.5689259171485901
translation,39,114,results,results,has,rigel-baseline and rigel-intersect,results has rigel-baseline and rigel-intersect,0.5504858493804932
translation,40,126,ablation-analysis,answer,with,separate phrases,answer with separate phrases,0.6429473161697388
translation,40,126,ablation-analysis,performance,of,qg,performance of qg,0.6238095760345459
translation,40,126,ablation-analysis,answer,has,performance,answer has performance,0.6032292246818542
translation,40,126,ablation-analysis,separate phrases,has,performance,separate phrases has performance,0.6052975058555603
translation,40,126,ablation-analysis,qg,has,slightly drops,qg has slightly drops,0.647594153881073
translation,40,126,ablation-analysis,ablation analysis,replace,answer,ablation analysis replace answer,0.6922959089279175
translation,40,162,ablation-analysis,key sentences,extracted by,bert - sum,key sentences extracted by bert - sum,0.7378831505775452
translation,40,162,ablation-analysis,ablation analysis,has,key sentences,ablation analysis has key sentences,0.559030294418335
translation,40,108,baselines,lstmbased model,with,pointer machanism,lstmbased model with pointer machanism,0.651598334312439
translation,40,108,baselines,pointer-generator,has,lstmbased model,pointer-generator has lstmbased model,0.552539587020874
translation,40,108,baselines,baselines,has,pointer-generator,baselines has pointer-generator,0.5389614105224609
translation,40,109,baselines,seq2seq model,with,hierarchical encoder structure,seq2seq model with hierarchical encoder structure,0.6158193945884705
translation,40,109,baselines,hierarchical encoder structure,to capture,both word-level and sentence - level information,hierarchical encoder structure to capture both word-level and sentence - level information,0.6395165324211121
translation,40,109,baselines,"hred ( gao et al. , 2019 )",has,seq2seq model,"hred ( gao et al. , 2019 ) has seq2seq model",0.5479016304016113
translation,40,109,baselines,baselines,has,"hred ( gao et al. , 2019 )","baselines has hred ( gao et al. , 2019 )",0.5411937832832336
translation,40,110,baselines,"transformer ( vaswani et al. , 2017 )",has,standard transformer - based seq2seq model,"transformer ( vaswani et al. , 2017 ) has standard transformer - based seq2seq model",0.5276341438293457
translation,40,110,baselines,baselines,has,"transformer ( vaswani et al. , 2017 )","baselines has transformer ( vaswani et al. , 2017 )",0.5352681279182434
translation,40,111,baselines,maxout-pointer model,with,feature -enriched input,maxout-pointer model with feature -enriched input,0.62261962890625
translation,40,111,baselines,elmo,has,maxout-pointer model,elmo has maxout-pointer model,0.5715639591217041
translation,40,111,baselines,baselines,has,elmo,baselines has elmo,0.6173549890518188
translation,40,112,baselines,gated selfattention maxout -pointer model,with,gcn - based encoder,gated selfattention maxout -pointer model with gcn - based encoder,0.6428139209747314
translation,40,112,baselines,gcn - based encoder,to capture,inter-sentences and intrasentence relations,gcn - based encoder to capture inter-sentences and intrasentence relations,0.6679462790489197
translation,40,112,baselines,"aggcn -qg ( jia et al. , 2020 )",has,gated selfattention maxout -pointer model,"aggcn -qg ( jia et al. , 2020 ) has gated selfattention maxout -pointer model",0.5501345992088318
translation,40,112,baselines,baselines,has,"aggcn -qg ( jia et al. , 2020 )","baselines has aggcn -qg ( jia et al. , 2020 )",0.5395759344100952
translation,40,114,baselines,basic prophetnet model,to generate,question and answer,basic prophetnet model to generate question and answer,0.7019398212432861
translation,40,114,baselines,qag,has,basic prophetnet model,qag has basic prophetnet model,0.5593181252479553
translation,40,114,baselines,prophetnet base,has,basic prophetnet model,prophetnet base has basic prophetnet model,0.5617363452911377
translation,40,114,baselines,question and answer,has,independently,question and answer has independently,0.6092793941497803
translation,40,94,experimental-setup,agents,utilize,built - in vocabulary,agents utilize built - in vocabulary,0.5946866273880005
translation,40,94,experimental-setup,agents,utilize,tokenization method,agents utilize tokenization method,0.5930948257446289
translation,40,94,experimental-setup,tokenization method,of,"bert ( devlin et al. , 2019 )","tokenization method of bert ( devlin et al. , 2019 )",0.5273430347442627
translation,40,95,experimental-setup,dimension,of,embedding vector,dimension of embedding vector,0.5403659343719482
translation,40,95,experimental-setup,embedding vector,set to,300,embedding vector set to 300,0.7239511013031006
translation,40,95,experimental-setup,experimental setup,has,dimension,experimental setup has dimension,0.4825572967529297
translation,40,96,experimental-setup,embedding / hidden size,is,1024,embedding / hidden size is 1024,0.5997186303138733
translation,40,96,experimental-setup,feed -forward filter size,is,4096,feed -forward filter size is 4096,0.5779950022697449
translation,40,96,experimental-setup,experimental setup,has,embedding / hidden size,experimental setup has embedding / hidden size,0.5537287592887878
translation,40,96,experimental-setup,experimental setup,has,feed -forward filter size,experimental setup has feed -forward filter size,0.5485490560531616
translation,40,97,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,40,97,experimental-setup,learning rate,of,1?10 procedure,learning rate of 1?10 procedure,0.5825220942497253
translation,40,97,experimental-setup,experimental setup,use,adam optimizer,experimental setup use adam optimizer,0.5987385511398315
translation,40,98,experimental-setup,our model,on,2 rtx 2080 ti gpus,our model on 2 rtx 2080 ti gpus,0.527046799659729
translation,40,98,experimental-setup,2 rtx 2080 ti gpus,for,about three days,2 rtx 2080 ti gpus for about three days,0.5707406401634216
translation,40,98,experimental-setup,experimental setup,train,our model,experimental setup train our model,0.6429243087768555
translation,40,99,experimental-setup,two -stage fine-tuning,for,keyphrase generation,two -stage fine-tuning for keyphrase generation,0.5602214336395264
translation,40,99,experimental-setup,two -stage fine-tuning,set,training epochs,two -stage fine-tuning set training epochs,0.6190582513809204
translation,40,99,experimental-setup,training epochs,as,15 and 10,training epochs as 15 and 10,0.5769386291503906
translation,40,99,experimental-setup,15 and 10,on,squad and race,15 and 10 on squad and race,0.6162441968917847
translation,40,99,experimental-setup,experimental setup,In,two -stage fine-tuning,experimental setup In two -stage fine-tuning,0.5224165916442871
translation,40,100,experimental-setup,training epochs,as,15,training epochs as 15,0.5719847083091736
translation,40,100,experimental-setup,training epochs,as,10,training epochs as 10,0.5934224724769592
translation,40,100,experimental-setup,15,for,qg,15 for qg,0.7064945697784424
translation,40,100,experimental-setup,10,for,keyphrase generation,10 for keyphrase generation,0.5657615661621094
translation,40,121,experiments,answeragnostic prophetnet,yields,7.20 bleu - 4,answeragnostic prophetnet yields 7.20 bleu - 4,0.64263916015625
translation,40,121,experiments,answeragnostic prophetnet,yields,3.78,answeragnostic prophetnet yields 3.78,0.7022676467895508
translation,40,121,experiments,7.20 bleu - 4,on,qg task,7.20 bleu - 4 on qg task,0.5001474618911743
translation,40,121,experiments,3.78,on,ag task,3.78 on ag task,0.5266355276107788
translation,40,7,model,important information,automatically generate,keyphrases,important information automatically generate keyphrases,0.7193133234977722
translation,40,8,model,multi-agent communication model,to generate and optimize,question and keyphrases,multi-agent communication model to generate and optimize question and keyphrases,0.7059375047683716
translation,40,8,model,multi-agent communication model,apply,generated question and keyphrases,multi-agent communication model apply generated question and keyphrases,0.6217852830886841
translation,40,8,model,generated question and keyphrases,to guide,generation of answers,generated question and keyphrases to guide generation of answers,0.6629465818405151
translation,40,8,model,question and keyphrases,has,iteratively,question and keyphrases has iteratively,0.6462230682373047
translation,40,8,model,model,propose,multi-agent communication model,model propose multi-agent communication model,0.6246314644813538
translation,40,25,model,new architecture,deal with,real-world qag task,new architecture deal with real-world qag task,0.7150365710258484
translation,40,25,model,model,propose,new architecture,model propose new architecture,0.7437537312507629
translation,40,26,model,model,consists of,three parts,model consists of three parts,0.7203089594841003
translation,40,69,model,prophetnet,employed in,all the three stages,prophetnet employed in all the three stages,0.6909416317939758
translation,40,69,model,all the three stages,of,our unified model,all the three stages of our unified model,0.56411212682724
translation,40,69,model,model,has,prophetnet,model has prophetnet,0.6111859083175659
translation,40,70,model,generated keyphrases,adopt,two -stage fine-tuning strategy,generated keyphrases adopt two -stage fine-tuning strategy,0.6407551765441895
translation,40,71,model,squad,as,data augmentation,squad as data augmentation,0.5508469343185425
translation,40,71,model,data augmentation,for,first-stage training,data augmentation for first-stage training,0.6058362722396851
translation,40,71,model,model,use,squad,model use squad,0.6712301969528198
translation,40,115,model,prophetnet keyphrase,guided,prophetnet model,prophetnet keyphrase guided prophetnet model,0.7103399038314819
translation,40,115,model,prophetnet model,to generate,question and answer,prophetnet model to generate question and answer,0.7105041742324829
translation,40,115,model,model,has,prophetnet keyphrase,model has prophetnet keyphrase,0.59285569190979
translation,40,116,model,questions,guidance of,golden answer phrases,questions guidance of golden answer phrases,0.6672095656394958
translation,40,116,model,stop words,from,ground - truth answers,stop words from ground - truth answers,0.5402851700782776
translation,40,116,model,model,has,prophetnet with golden phrases,model has prophetnet with golden phrases,0.6151102781295776
translation,40,117,model,prophetnet model,to generate,questions,prophetnet model to generate questions,0.7251182794570923
translation,40,117,model,questions,guidance of,ground - truth answers,questions guidance of ground - truth answers,0.6408156752586365
translation,40,117,model,ground - truth answers,regarded as,upper bound,ground - truth answers regarded as upper bound,0.5351303815841675
translation,40,117,model,upper bound,for,qg,upper bound for qg,0.654822051525116
translation,40,117,model,prophetnet answer-aware,has,prophetnet model,prophetnet answer-aware has prophetnet model,0.5892853140830994
translation,40,117,model,model,has,prophetnet answer-aware,model has prophetnet answer-aware,0.5826404690742493
translation,40,119,results,rnn seq2seq model,gets,4.75 bleu -4,rnn seq2seq model gets 4.75 bleu -4,0.5505017042160034
translation,40,119,results,answer - aware qg,has,rnn seq2seq model,answer - aware qg has rnn seq2seq model,0.5691542029380798
translation,40,119,results,results,For,answer - aware qg,results For answer - aware qg,0.6120657324790955
translation,40,120,results,our model,gets,close performance,our model gets close performance,0.6114790439605713
translation,40,120,results,close performance,with,previous state - of - art answer - guided model aggcn - qg,close performance with previous state - of - art answer - guided model aggcn - qg,0.6299386024475098
translation,40,120,results,close performance,achieving,11.55 bleu -4,close performance achieving 11.55 bleu -4,0.5876765251159668
translation,40,120,results,close performance,achieving,16.13 meteor,close performance achieving 16.13 meteor,0.6415042281150818
translation,40,122,results,our unified model,improves,4.35 points,our unified model improves 4.35 points,0.6563145518302917
translation,40,122,results,our unified model,improves,3.09 points,our unified model improves 3.09 points,0.6677495241165161
translation,40,122,results,4.35 points,for,qg,4.35 points for qg,0.6693258285522461
translation,40,122,results,3.09 points,for,ag,3.09 points for ag,0.6601191163063049
translation,40,122,results,ag,over,basic prophetnet model,ag over basic prophetnet model,0.630195677280426
translation,40,122,results,results,has,our unified model,results has our unified model,0.5698179006576538
translation,40,123,results,iteration epoch m=2,get,best results,iteration epoch m=2 get best results,0.5662289261817932
translation,40,123,results,no obvious improvement,on,results,no obvious improvement on results,0.5771217346191406
translation,40,123,results,results,When,iteration epoch m=2,results When iteration epoch m=2,0.7001661062240601
translation,40,124,results,our question - keyphrase iterative agent,brings,obvious performance gain,our question - keyphrase iterative agent brings obvious performance gain,0.6209855675697327
translation,40,124,results,obvious performance gain,on,ag,obvious performance gain on ag,0.5945981740951538
translation,40,124,results,results,has,our question - keyphrase iterative agent,results has our question - keyphrase iterative agent,0.5976285934448242
translation,40,125,results,right answer,into,prophetnet ( prophetnet answer-aware ),right answer into prophetnet ( prophetnet answer-aware ),0.5474373698234558
translation,40,125,results,right answer,get,quite high performance,right answer get quite high performance,0.6446268558502197
translation,40,125,results,quite high performance,with,20.53 bleu - 4,quite high performance with 20.53 bleu - 4,0.6259956955909729
translation,40,125,results,results,feed,right answer,results feed right answer,0.6725611090660095
translation,40,131,results,keyphrases,generated by,twostage fine-tuning,keyphrases generated by twostage fine-tuning,0.672794759273529
translation,40,131,results,keyphrases,bring,better results,keyphrases bring better results,0.6282824873924255
translation,40,131,results,twostage fine-tuning,bring,better results,twostage fine-tuning bring better results,0.6415475606918335
translation,40,131,results,better results,than,one-stage mixed data,better results than one-stage mixed data,0.596908450126648
translation,40,131,results,results,has,keyphrases,results has keyphrases,0.5379410982131958
translation,40,145,results,shared - encoder model,obtains,3.06 bleu - 4,shared - encoder model obtains 3.06 bleu - 4,0.5542977452278137
translation,40,145,results,shared - encoder model,obtains,1.29,shared - encoder model obtains 1.29,0.5954166650772095
translation,40,145,results,3.06 bleu - 4,on,question,3.06 bleu - 4 on question,0.5308465957641602
translation,40,145,results,1.29,on,answer,1.29 on answer,0.5465021133422852
translation,40,145,results,results,has,shared - encoder model,results has shared - encoder model,0.5393908023834229
translation,40,147,results,iterative training,yields,obvious performance gain,iterative training yields obvious performance gain,0.7236672639846802
translation,40,147,results,obvious performance gain,over,prophetnet base and shared - encoder method,obvious performance gain over prophetnet base and shared - encoder method,0.6521773338317871
translation,40,147,results,obvious performance gain,both,prophetnet base and shared - encoder method,obvious performance gain both prophetnet base and shared - encoder method,0.6222348809242249
translation,40,147,results,results,has,iterative training,results has iterative training,0.5457184910774231
translation,40,148,results,q-a based one,in,qg and ag tasks,q-a based one in qg and ag tasks,0.5773577094078064
translation,40,148,results,q-k iterative method,has,outperforms,q-k iterative method has outperforms,0.604475736618042
translation,40,148,results,outperforms,has,q-a based one,outperforms has q-a based one,0.5855582356452942
translation,40,148,results,results,has,q-k iterative method,results has q-k iterative method,0.5390510559082031
translation,40,161,results,keyphrase based model,achieves,better result,keyphrase based model achieves better result,0.7016081213951111
translation,40,161,results,results,has,keyphrase based model,results has keyphrase based model,0.5662492513656616
translation,40,168,results,spearman correlation coefficients,between,annotators,spearman correlation coefficients between annotators,0.592355489730835
translation,40,168,results,annotators,are,high,annotators are high,0.5614868402481079
translation,40,168,results,results,has,spearman correlation coefficients,results has spearman correlation coefficients,0.4823160469532013
translation,40,169,results,both models,achieve,nearly full marks,both models achieve nearly full marks,0.6719790101051331
translation,40,169,results,nearly full marks,on,fluency and relevancy,nearly full marks on fluency and relevancy,0.4967220425605774
translation,40,169,results,fluency and relevancy,due to,powerful performance,fluency and relevancy due to powerful performance,0.6623691916465759
translation,40,169,results,powerful performance,of,pre-training model,powerful performance of pre-training model,0.5583018064498901
translation,40,169,results,results,has,both models,results has both models,0.5060139894485474
translation,43,108,baselines,verification,of,generated presuppositions,verification of generated presuppositions,0.5693943500518799
translation,43,108,baselines,verification,test,four different strategies,verification test four different strategies,0.7795310020446777
translation,43,108,baselines,generated presuppositions,of,question,generated presuppositions of question,0.5584589242935181
translation,43,108,baselines,question,against,answer source,question against answer source,0.6063880324363708
translation,43,108,baselines,zero-shot transfer,from,natural language inference ( nli ),zero-shot transfer from natural language inference ( nli ),0.5805516242980957
translation,43,108,baselines,zero-shot transfer,NLI model finetuned on,verification,zero-shot transfer NLI model finetuned on verification,0.761263370513916
translation,43,108,baselines,zero-shot transfer,from,fact verification,zero-shot transfer from fact verification,0.5519365668296814
translation,43,108,baselines,zero-shot transfer,from,fact verification,zero-shot transfer from fact verification,0.5519365668296814
translation,43,6,results,user preference study,demonstrate,oracle behavior,user preference study demonstrate oracle behavior,0.6178459525108337
translation,43,6,results,oracle behavior,of,our proposed system,oracle behavior of our proposed system,0.5849407315254211
translation,43,6,results,oracle behavior,of,existing qa systems,oracle behavior of existing qa systems,0.5576269626617432
translation,43,6,results,oracle behavior,provides,responses,oracle behavior provides responses,0.6945828795433044
translation,43,6,results,oracle behavior,of,existing qa systems,oracle behavior of existing qa systems,0.5576269626617432
translation,43,6,results,responses,based on,presupposition failure,responses based on presupposition failure,0.6250134706497192
translation,43,6,results,responses,preferred over,oracle behavior,responses preferred over oracle behavior,0.7033239006996155
translation,43,6,results,oracle behavior,of,existing qa systems,oracle behavior of existing qa systems,0.5576269626617432
translation,43,159,results,presupposition verification,challenging to,existing models,presupposition verification challenging to existing models,0.6907917261123657
translation,43,159,results,results,suggest that,presupposition verification,results suggest that presupposition verification,0.6146397590637207
translation,43,160,results,model,combines,finer-tuning and rule-based document presuppositions,model combines finer-tuning and rule-based document presuppositions,0.7022095322608948
translation,43,160,results,finer-tuning and rule-based document presuppositions,make,modest improve-ment,finer-tuning and rule-based document presuppositions make modest improve-ment,0.6430888175964355
translation,43,160,results,modest improve-ment,over,majority class baseline ( 78 % ? 79 % ),modest improve-ment over majority class baseline ( 78 % ? 79 % ),0.6377048492431641
translation,43,160,results,results,has,model,results has model,0.5339115858078003
translation,43,161,results,substantial,for,all models,substantial for all models,0.725892186164856
translation,43,161,results,all models,has,44 % ? 60 %,all models has 44 % ? 60 %,0.5382463932037354
translation,43,161,results,impact,has,verifiability,impact has verifiability,0.5887250304222107
translation,43,162,results,qnli,provided,most effective zero-shot transfer,qnli provided most effective zero-shot transfer,0.6578483581542969
translation,43,162,results,results,has,qnli,results has qnli,0.5467820763587952
translation,43,196,results,augmentations,use,only the presuppositions,augmentations use only the presuppositions,0.5865238308906555
translation,43,196,results,augmentations,use,only the verification labels,augmentations use only the verification labels,0.6594756841659546
translation,43,196,results,augmentations,use,presuppositions,augmentations use presuppositions,0.6014225482940674
translation,43,196,results,augmentations,lead to,gains,augmentations lead to gains,0.7583017945289612
translation,43,196,results,gains,in,nq performance,gains in nq performance,0.5801237225532532
translation,43,196,results,gains,on,unanswerability classification,gains on unanswerability classification,0.5934191942214966
translation,43,196,results,nq performance,over,baseline,nq performance over baseline,0.6907079219818115
translation,43,196,results,presuppositions,lead to,gains,presuppositions lead to gains,0.7279343008995056
translation,43,196,results,gains,on,unanswerability classification,gains on unanswerability classification,0.5934191942214966
translation,43,196,results,results,shows,augmentations,results shows augmentations,0.681368887424469
translation,43,197,results,both presuppositions and their verifiability,see,minor gains,both presuppositions and their verifiability see minor gains,0.6262502074241638
translation,43,197,results,minor gains,in,average f1 and unanswerability classification,minor gains in average f1 and unanswerability classification,0.5368054509162903
translation,43,197,results,results,When,both presuppositions and their verifiability,results When both presuppositions and their verifiability,0.5756190419197083
translation,44,25,baselines,tat - qa,has,tabular and textual dataset,tat - qa has tabular and textual dataset,0.5644710063934326
translation,44,143,baselines,"bert - rc ( devlin et al. , 2018 )",is,squad - style rc model,"bert - rc ( devlin et al. , 2018 ) is squad - style rc model",0.5512717366218567
translation,44,148,baselines,tapas,for,wik-itablequestion ( wtq ),tapas for wik-itablequestion ( wtq ),0.6684892177581787
translation,44,148,baselines,baselines,employ,tapas,baselines employ tapas,0.5866885781288147
translation,44,2,experiments,tat - qa,has,question answering,tat - qa has question answering,0.6173229217529297
translation,44,26,experiments,hybrid contexts,in,tat - qa,hybrid contexts in tat - qa,0.5858797430992126
translation,44,26,experiments,hybrid contexts,extracted from,real-world financial reports,hybrid contexts extracted from real-world financial reports,0.5570101737976074
translation,44,26,experiments,tat - qa,extracted from,real-world financial reports,tat - qa extracted from real-world financial reports,0.5713918805122375
translation,44,26,experiments,real-world financial reports,composed of,table,real-world financial reports composed of table,0.656663715839386
translation,44,26,experiments,table,with,row / col header and numbers,table with row / col header and numbers,0.5984463691711426
translation,44,38,experiments,three types of qa models,on,tat - qa,three types of qa models on tat - qa,0.570091187953949
translation,44,38,experiments,three types of qa models,specially addressing,tabular,three types of qa models specially addressing tabular,0.7332209944725037
translation,44,149,experiments,tapas,pretrained over,large-scale tables,tapas pretrained over large-scale tables,0.705665647983551
translation,44,149,experiments,tapas,pretrained over,associated text,tapas pretrained over associated text,0.6589325070381165
translation,44,149,experiments,associated text,from,wikipedia,associated text from wikipedia,0.5647301077842712
translation,44,149,experiments,associated text,jointly for,table parsing,associated text jointly for table parsing,0.6670609712600708
translation,44,7,model,novel qa model,termed,tagop,novel qa model termed tagop,0.7249733209609985
translation,44,7,model,novel qa model,capable of,reasoning,novel qa model capable of reasoning,0.6667887568473816
translation,44,7,model,reasoning,over,tables and text,reasoning over tables and text,0.6495057344436646
translation,44,7,model,reasoning,both,tables and text,reasoning both tables and text,0.6810203194618225
translation,44,7,model,model,propose,novel qa model,model propose novel qa model,0.7111940979957581
translation,44,8,model,sequence tagging,to extract,relevant cells,sequence tagging to extract relevant cells,0.7492916584014893
translation,44,8,model,relevant cells,from,table,relevant cells from table,0.49953651428222656
translation,44,8,model,relevant cells,along with,relevant spans,relevant cells along with relevant spans,0.6177905797958374
translation,44,8,model,relevant spans,from,text,relevant spans from text,0.6107807159423828
translation,44,8,model,text,to infer,semantics,text to infer semantics,0.7113367915153503
translation,44,8,model,symbolic reasoning,with,set of aggregation operators,symbolic reasoning with set of aggregation operators,0.5927117466926575
translation,44,8,model,set of aggregation operators,to arrive at,final answer,set of aggregation operators to arrive at final answer,0.6912832260131836
translation,44,8,model,model,adopts,sequence tagging,model adopts sequence tagging,0.6709747314453125
translation,44,30,model,novel tagop model,based on,tat - qa,novel tagop model based on tat - qa,0.6788530349731445
translation,44,30,model,model,propose,novel tagop model,model propose novel tagop model,0.6682777404785156
translation,44,31,model,tagop,applies,sequence tagging,tagop applies sequence tagging,0.7019818425178528
translation,44,31,model,sequence tagging,to extract,relevant cells,sequence tagging to extract relevant cells,0.7492916584014893
translation,44,31,model,sequence tagging,to extract,relevant spans,sequence tagging to extract relevant spans,0.7304853796958923
translation,44,31,model,relevant cells,from,table,relevant cells from table,0.49953651428222656
translation,44,31,model,relevant spans,from,text,relevant spans from text,0.6107807159423828
translation,44,95,model,qa model,named,tagop,qa model named tagop,0.7374048829078674
translation,44,95,model,qa model,first applies,sequence tagging,qa model first applies sequence tagging,0.679989218711853
translation,44,95,model,tagop,first applies,sequence tagging,tagop first applies sequence tagging,0.7364020943641663
translation,44,95,model,sequence tagging,to extract,relevant cells,sequence tagging to extract relevant cells,0.7492916584014893
translation,44,95,model,relevant cells,from,table,relevant cells from table,0.49953651428222656
translation,44,95,model,model,introduce,qa model,model introduce qa model,0.6646674871444702
translation,44,9,results,tagop,achieves,58.0 %,tagop achieves 58.0 %,0.6662083864212036
translation,44,9,results,58.0 %,is,11.1 % absolute increase,58.0 % is 11.1 % absolute increase,0.5553378462791443
translation,44,9,results,11.1 % absolute increase,over,previous best baseline model,11.1 % absolute increase over previous best baseline model,0.6209418773651123
translation,44,9,results,58.0 %,has,in f 1,58.0 % has in f 1,0.5720548629760742
translation,44,9,results,results,has,tagop,results has tagop,0.6109617352485657
translation,44,39,results,tagop,achieves,58.0 %,tagop achieves 58.0 %,0.6662083864212036
translation,44,39,results,58.0 %,in terms of,f 1,58.0 % in terms of f 1,0.6859668493270874
translation,44,39,results,58.0 %,is,11.1 % absolute increase,58.0 % is 11.1 % absolute increase,0.5553378462791443
translation,44,39,results,11.1 % absolute increase,over,best baseline model,11.1 % absolute increase over best baseline model,0.6259421706199646
translation,44,39,results,results,has,tagop,results has tagop,0.6109617352485657
translation,44,169,results,our model,always superior to,other baselines,our model always superior to other baselines,0.7419166564941406
translation,44,169,results,other baselines,in terms of,both metrics,other baselines in terms of both metrics,0.5722431540489197
translation,44,169,results,very large margins,over,second best,very large margins over second best,0.6810808777809143
translation,44,169,results,second best,namely,50.1/58.0,second best namely 50.1/58.0,0.5988360643386841
translation,44,169,results,37.0/46.9,on,test set,37.0/46.9 on test set,0.5450530648231506
translation,44,169,results,results,seen that,our model,results seen that our model,0.734937310218811
translation,44,171,results,numnet + v2,performs,better,numnet + v2 performs better,0.7069929838180542
translation,44,171,results,better,than,bert - rc,better than bert - rc,0.6748557090759277
translation,44,171,results,two textual qa baselines,has,numnet + v2,two textual qa baselines has numnet + v2,0.5598031282424927
translation,44,171,results,results,For,two textual qa baselines,results For two textual qa baselines,0.5314764380455017
translation,44,172,results,tabular qa baseline tapas,for,wtq,tabular qa baseline tapas for wtq,0.6314264535903931
translation,44,172,results,tabular qa baseline tapas,trained with,only tabular data,tabular qa baseline tapas trained with only tabular data,0.7159676551818848
translation,44,172,results,only tabular data,in,tat - qa,only tabular data in tat - qa,0.5711328983306885
translation,44,172,results,only tabular data,showing,very limited capability,only tabular data showing very limited capability,0.7222309112548828
translation,44,172,results,very limited capability,to process,hybrid data,very limited capability to process hybrid data,0.7040688991546631
translation,44,172,results,results,has,tabular qa baseline tapas,results has tabular qa baseline tapas,0.5668420195579529
translation,44,173,results,hybrider,is,worst,hybrider is worst,0.6420873403549194
translation,44,173,results,worst,among,all baseline models,worst among all baseline models,0.6091474294662476
translation,44,173,results,results,has,hybrider,results has hybrider,0.6230677366256714
translation,44,174,results,all the models,perform,significantly worse,all the models perform significantly worse,0.5810866951942444
translation,44,174,results,significantly worse,than,human performance,significantly worse than human performance,0.6102992296218872
translation,44,174,results,tat,has,qa,tat has qa,0.7217928767204285
translation,44,174,results,results,has,all the models,results has all the models,0.5321599841117859
translation,44,178,results,tagop,performs,relatively worse,tagop performs relatively worse,0.6398354768753052
translation,44,178,results,relatively worse,on,arithmetic questions,relatively worse on arithmetic questions,0.5166521668434143
translation,44,178,results,relatively worse,compared with,other types,relatively worse compared with other types,0.6439739465713501
translation,44,178,results,arithmetic questions,compared with,other types,arithmetic questions compared with other types,0.6160249710083008
translation,44,178,results,results,has,tagop,results has tagop,0.6109617352485657
translation,45,153,baselines,updn,is,widely used vqa model,updn is widely used vqa model,0.537220299243927
translation,45,153,baselines,baselines,has,updn,baselines has updn,0.5849818587303162
translation,45,169,baselines,three existing data augmentation methods,for,vqa,three existing data augmentation methods for vqa,0.6004394888877869
translation,45,169,baselines,baselines,compare,simpleaug,baselines compare simpleaug,0.763668417930603
translation,45,134,experimental-setup,vqa v2,collects,images,vqa v2 collects images,0.6673869490623474
translation,45,134,experimental-setup,vqa v2,uses,same training / validation / testing splits,vqa v2 uses same training / validation / testing splits,0.6579069495201111
translation,45,134,experimental-setup,images,from,mscoco,images from mscoco,0.6171778440475464
translation,45,134,experimental-setup,experimental setup,has,vqa v2,experimental setup has vqa v2,0.5664011836051941
translation,45,22,model,vqa model,turn,implicit information,vqa model turn implicit information,0.6638354063034058
translation,45,22,model,implicit information,already in,dataset,implicit information already in dataset,0.671622633934021
translation,45,22,model,implicit information,such as,unique questions,implicit information such as unique questions,0.6561374068260193
translation,45,22,model,implicit information,such as,rich contents,implicit information such as rich contents,0.6495453119277954
translation,45,22,model,implicit information,into,explicit iqa triplets,implicit information into explicit iqa triplets,0.5804880261421204
translation,45,22,model,rich contents,in,training images,rich contents in training images,0.49826836585998535
translation,45,22,model,explicit iqa triplets,be directly used by,vqa models,explicit iqa triplets be directly used by vqa models,0.7045964002609253
translation,45,22,model,vqa models,via,conventional supervised learning,vqa models via conventional supervised learning,0.6566752195358276
translation,45,23,model,simple data augmentation method simpleaug,relies on,original imagequestion - answer triplets,simple data augmentation method simpleaug relies on original imagequestion - answer triplets,0.7206172943115234
translation,45,23,model,simple data augmentation method simpleaug,relies on,midlevel semantic annotations,simple data augmentation method simpleaug relies on midlevel semantic annotations,0.7274783253669739
translation,45,23,model,simple data augmentation method simpleaug,relies on,pretrained object detectors,simple data augmentation method simpleaug relies on pretrained object detectors,0.6925138235092163
translation,45,23,model,midlevel semantic annotations,available on,"training images ( e.g. , object bounding boxes )","midlevel semantic annotations available on training images ( e.g. , object bounding boxes )",0.46508118510246277
translation,45,23,model,model,propose,simple data augmentation method simpleaug,model propose simple data augmentation method simpleaug,0.6832516193389893
translation,45,155,model,updn,uses,question encoder,updn uses question encoder,0.5883772969245911
translation,45,155,model,question encoder,to produce,set of word features,question encoder to produce set of word features,0.6638752222061157
translation,45,155,model,question,has,updn,question has updn,0.6646960377693176
translation,45,155,model,model,Given,question,model Given question,0.7526277899742126
translation,45,163,model,lxmert,leverages,multi-modal transformers,lxmert leverages multi-modal transformers,0.7408663630485535
translation,45,163,model,lxmert,exploits,masking mechanism,lxmert exploits masking mechanism,0.7944287657737732
translation,45,163,model,multi-modal transformers,to extract,multi-modal features,multi-modal transformers to extract multi-modal features,0.7595701217651367
translation,45,163,model,masking mechanism,to better ( pre - ) train,model,masking mechanism to better ( pre - ) train model,0.6911365985870361
translation,45,163,model,model,has,lxmert,model has lxmert,0.6336734294891357
translation,45,170,model,template - based augmentation,generates,new question - answer pairs,template - based augmentation generates new question - answer pairs,0.6799198985099792
translation,45,170,model,new question - answer pairs,using,mscoco annotations,new question - answer pairs using mscoco annotations,0.6449759006500244
translation,45,170,model,model,has,template - based augmentation,model has template - based augmentation,0.5651354193687439
translation,45,29,results,simpleaug,achieve,comparable gains,simpleaug achieve comparable gains,0.6410272121429443
translation,45,29,results,simpleaug,achieve,boost,simpleaug achieve boost,0.7380152940750122
translation,45,29,results,comparable gains,to,other existing methods,comparable gains to other existing methods,0.5399331450462341
translation,45,29,results,other existing methods,on,vqa - cp,other existing methods on vqa - cp,0.5003731846809387
translation,45,29,results,accuracy,on,vqa v2,accuracy on vqa v2,0.5598242878913879
translation,45,29,results,boost,has,accuracy,boost has accuracy,0.5681762099266052
translation,45,29,results,results,With,simpleaug,results With simpleaug,0.6311730146408081
translation,45,167,results,simpleaug,can provide,solid gains,simpleaug can provide solid gains,0.7195930480957031
translation,45,167,results,solid gains,to,lxmert,solid gains to lxmert,0.6822712421417236
translation,45,167,results,lxmert,on,vqa v2 and vqa - cp,lxmert on vqa v2 and vqa - cp,0.6366087794303894
translation,45,178,results,results,on,vqa v2 and vqa - cp v2,results on vqa v2 and vqa - cp v2,0.5569433569908142
translation,45,178,results,results,on,vqa v2 val and vqa - cp v2 test,results on vqa v2 val and vqa - cp v2 test,0.5365897417068481
translation,45,178,results,results,on,vqa v2 val and vqa - cp v2 test,results on vqa v2 val and vqa - cp v2 test,0.5365897417068481
translation,45,180,results,simpleaug,achieves,consistent gains,simpleaug achieves consistent gains,0.6957168579101562
translation,45,180,results,consistent gains,against,base models,consistent gains against base models,0.7425341606140137
translation,45,180,results,base models,on,all answer types,base models on all answer types,0.5110065340995789
translation,45,180,results,results,has,simpleaug,results has simpleaug,0.5582201480865479
translation,45,181,results,simpleaug,obtains,highest accuracy,simpleaug obtains highest accuracy,0.5979383587837219
translation,45,181,results,highest accuracy,on,both datasets,highest accuracy on both datasets,0.4617873728275299
translation,45,181,results,both datasets,except,mutant ( loss ),both datasets except mutant ( loss ),0.725741446018219
translation,45,181,results,mutant ( loss ),applies,extra losses,mutant ( loss ) applies extra losses,0.6285972595214844
translation,45,181,results,lxmert,has,simpleaug,lxmert has simpleaug,0.6469666361808777
translation,45,181,results,extra losses,has,besides data augmentation,extra losses has besides data augmentation,0.5839735269546509
translation,45,181,results,results,When paired with,lxmert,results When paired with lxmert,0.7684653997421265
translation,45,182,results,simpleaug,improves,all answer types,simpleaug improves all answer types,0.7162907123565674
translation,45,182,results,results,has,simpleaug,results has simpleaug,0.5582201480865479
translation,45,183,results,simpleaug,boosts,overall accuracy,simpleaug boosts overall accuracy,0.7287530899047852
translation,45,183,results,overall accuracy,of,updn,overall accuracy of updn,0.6226574182510376
translation,45,183,results,updn,from,39.74 % to 52.65 %,updn from 39.74 % to 52.65 %,0.5244373083114624
translation,45,183,results,vqa -cp v2,has,simpleaug,vqa -cp v2 has simpleaug,0.6480503082275391
translation,45,183,results,results,On,vqa -cp v2,results On vqa -cp v2,0.577746570110321
translation,45,184,results,strength,of,simpleaug,strength of simpleaug,0.4954891502857208
translation,45,184,results,simpleaug,improves,all the answer types,simpleaug improves all the answer types,0.7100405097007751
translation,45,184,results,all the answer types,including,? 2 % gain,all the answer types including ? 2 % gain,0.7020469307899475
translation,45,184,results,? 2 % gain,on,other  ,? 2 % gain on other  ,0.586064875125885
translation,45,184,results,other  ,where,many methods suffer,other   where many methods suffer,0.6648236513137817
translation,45,186,results,simpleaug,achieves,highest accuracy,simpleaug achieves highest accuracy,0.6497048139572144
translation,45,186,results,highest accuracy,using,updn,highest accuracy using updn,0.7077741622924805
translation,45,186,results,highest accuracy,improving,+ 0.86 %,highest accuracy improving + 0.86 %,0.6514580845832825
translation,45,186,results,highest accuracy,improving,+ 0.79 %,highest accuracy improving + 0.79 %,0.6514331102371216
translation,45,186,results,highest accuracy,improving,+ 1.77 %,highest accuracy improving + 1.77 %,0.6546497941017151
translation,45,186,results,+ 0.86 %,on,all  ,+ 0.86 % on all  ,0.5872493982315063
translation,45,186,results,+ 0.79 %,on,yes / no  ,+ 0.79 % on yes / no  ,0.5380820035934448
translation,45,186,results,+ 1.77 %,on,num,+ 1.77 % on num,0.6383693218231201
translation,45,186,results,+ 0.69 %,on,other,+ 0.69 % on other,0.5839480757713318
translation,45,186,results,vqa v2,has,simpleaug,vqa v2 has simpleaug,0.6394779086112976
translation,45,186,results,results,On,vqa v2,results On vqa v2,0.5776327848434448
translation,45,192,results,lmh,largely improve on,vqa v2,lmh largely improve on vqa v2,0.7046339511871338
translation,45,192,results,simpleaug,has,lmh,simpleaug has lmh,0.6829768419265747
translation,45,192,results,results,With,simpleaug,results With simpleaug,0.6311730146408081
translation,45,193,results,lxmert,is,strong transformer - based vqa model,lxmert is strong transformer - based vqa model,0.5588878989219666
translation,45,193,results,results,has,lxmert,results has lxmert,0.5957340598106384
translation,45,194,results,simpleaug,improves on,all cases,simpleaug improves on all cases,0.7312812805175781
translation,45,194,results,simpleaug,as,more general data augmentation method,simpleaug as more general data augmentation method,0.5333019495010376
translation,45,194,results,more general data augmentation method,for,vqa,more general data augmentation method for vqa,0.5981743931770325
translation,45,194,results,results,has,simpleaug,results has simpleaug,0.5582201480865479
translation,45,203,results,accuracy,at,nearly all cases,accuracy at nearly all cases,0.5480751395225525
translation,45,203,results,verification,has,improves,verification has improves,0.6397321820259094
translation,45,203,results,improves,has,accuracy,improves has accuracy,0.5632769465446472
translation,45,203,results,results,has,verification,results has verification,0.5748591423034668
translation,45,210,results,vqa - v2,get,39.62 % overall accuracy,vqa - v2 get 39.62 % overall accuracy,0.5678964853286743
translation,45,210,results,39.62 % overall accuracy,worse than,baseline,39.62 % overall accuracy worse than baseline,0.7037889957427979
translation,45,210,results,baseline,trained with,original data,baseline trained with original data,0.6892756223678589
translation,45,210,results,results,On,vqa - v2,results On vqa - v2,0.5822755694389343
translation,45,212,results,vqa - cp,get,51.60 %,vqa - cp get 51.60 %,0.5938507318496704
translation,45,212,results,51.60 %,better than,baseline,51.60 % better than baseline,0.7642853260040283
translation,45,212,results,51.60 %,worse than,training,51.60 % worse than training,0.7570188045501709
translation,45,212,results,training,has,with both augmented and original triplets,training has with both augmented and original triplets,0.5703848004341125
translation,45,212,results,with both augmented and original triplets,has,52.65 % ),with both augmented and original triplets has 52.65 % ),0.5653886795043945
translation,45,212,results,results,On,vqa - cp,results On vqa - cp,0.5631181597709656
translation,45,224,results,baseline updn model,trained with,all data,baseline updn model trained with all data,0.7373621463775635
translation,45,224,results,simpleaug,has,already be effective,simpleaug has already be effective,0.5609332323074341
translation,45,224,results,outperforming,has,baseline updn model,outperforming has baseline updn model,0.5848017334938049
translation,45,232,results,sim - pleaug,yields,consistent improvements,sim - pleaug yields consistent improvements,0.7689205408096313
translation,45,232,results,results,has,sim - pleaug,results has sim - pleaug,0.5270727872848511
translation,46,197,ablation-analysis,ocr and captioning,further improve,baseline accuracy,ocr and captioning further improve baseline accuracy,0.6338362097740173
translation,46,197,ablation-analysis,baseline accuracy,by,1 % and 1.6 %,baseline accuracy by 1 % and 1.6 %,0.5627227425575256
translation,46,197,ablation-analysis,ablation analysis,Incorporating,ocr and captioning,ablation analysis Incorporating ocr and captioning,0.7435797452926636
translation,46,204,ablation-analysis,open domain evaluation,correlated with,original accuracy evaluation,open domain evaluation correlated with original accuracy evaluation,0.5778033137321472
translation,46,204,ablation-analysis,ablation analysis,observe that,open domain evaluation,ablation analysis observe that open domain evaluation,0.5436630845069885
translation,46,220,ablation-analysis,highest - score strategy,size of,5,highest - score strategy size of 5,0.7441596388816833
translation,46,220,ablation-analysis,5,is,best,5 is best,0.6482020020484924
translation,46,220,ablation-analysis,knowledge size,reduces,performance,knowledge size reduces performance,0.6754612922668457
translation,46,220,ablation-analysis,ablation analysis,for,highest - score strategy,ablation analysis for highest - score strategy,0.5860922336578369
translation,46,221,ablation-analysis,highest - frequency strategy,when,size,highest - frequency strategy when size,0.6834847331047058
translation,46,221,ablation-analysis,highest - frequency strategy,yields,best performance,highest - frequency strategy yields best performance,0.7097663283348083
translation,46,221,ablation-analysis,size,equal to,80,size equal to 80,0.7319151163101196
translation,46,221,ablation-analysis,ablation analysis,for,highest - frequency strategy,ablation analysis for highest - frequency strategy,0.6039848923683167
translation,46,222,ablation-analysis,highest - frequency,negatively impacts,accuracy,highest - frequency negatively impacts accuracy,0.7502915263175964
translation,46,222,ablation-analysis,highest - score strategy,is,preferable,highest - score strategy is preferable,0.5832300186157227
translation,46,222,ablation-analysis,small set of knowledge,has,highest - frequency,small set of knowledge has highest - frequency,0.565784752368927
translation,46,109,baselines,two styles,of,visual retriever,two styles of visual retriever,0.6003134250640869
translation,46,122,baselines,dpr,in,visual domain,dpr in visual domain,0.5117124915122986
translation,46,123,baselines,image - dpr,use,"lxmert ( tan and bansal , 2019 ) dard bert","image - dpr use lxmert ( tan and bansal , 2019 ) dard bert",0.623228907585144
translation,46,123,baselines,"lxmert ( tan and bansal , 2019 ) dard bert",as,context encoder,"lxmert ( tan and bansal , 2019 ) dard bert as context encoder",0.5483275055885315
translation,46,123,baselines,baselines,has,image - dpr,baselines has image - dpr,0.5875276923179626
translation,46,184,baselines,lxmert lxmert,is,bert - based crossmodality model,lxmert lxmert is bert - based crossmodality model,0.5938192009925842
translation,46,184,baselines,bert - based crossmodality model,pretrained on,five different vqa datasets,bert - based crossmodality model pretrained on five different vqa datasets,0.7646030783653259
translation,46,184,baselines,vg - qa,has,"zhu et al. , 2016 )","vg - qa has zhu et al. , 2016 )",0.5598158240318298
translation,46,184,baselines,baselines,has,lxmert lxmert,baselines has lxmert lxmert,0.5679284930229187
translation,46,106,experiments,visual retriever - reader pipeline,for,ok - vqa challenge,visual retriever - reader pipeline for ok - vqa challenge,0.58901047706604
translation,46,106,experiments,visual retriever - reader pipeline,where,visual retriever,visual retriever - reader pipeline where visual retriever,0.6018041968345642
translation,46,106,experiments,visual retriever - reader pipeline,where,visual reader,visual retriever - reader pipeline where visual reader,0.6041375994682312
translation,46,106,experiments,answers,given,knowledge sentences,answers given knowledge sentences,0.7024158239364624
translation,46,106,experiments,predict,has,answers,predict has answers,0.6024604439735413
translation,46,198,experiments,different variations,of,visual retriever - reader,different variations of visual retriever - reader,0.603026807308197
translation,46,198,experiments,best combination,is,caption - dpr,best combination is caption - dpr,0.5818846821784973
translation,46,198,experiments,best combination,is,creader,best combination is creader,0.604557991027832
translation,46,198,experiments,creader,when,retrieved knowledge size,creader when retrieved knowledge size,0.6219913363456726
translation,46,198,experiments,retrieved knowledge size,is,80,retrieved knowledge size is 80,0.637827455997467
translation,46,198,experiments,different variations,has,best combination,different variations has best combination,0.5572662949562073
translation,46,198,experiments,visual retriever - reader,has,best combination,visual retriever - reader has best combination,0.5989183783531189
translation,46,9,model,visual retriever - reader pipeline,to approach,knowledge - based vqa,visual retriever - reader pipeline to approach knowledge - based vqa,0.7210568189620972
translation,46,9,model,model,propose,visual retriever - reader pipeline,model propose visual retriever - reader pipeline,0.6792981028556824
translation,46,58,model,visual - retriever - reader pipeline,adapted from,nlp domain,visual - retriever - reader pipeline adapted from nlp domain,0.5552710294723511
translation,46,58,model,nlp domain,for,knowledge - based vqa task,nlp domain for knowledge - based vqa task,0.590014636516571
translation,46,58,model,model,propose,visual - retriever - reader pipeline,model propose visual - retriever - reader pipeline,0.6871760487556458
translation,46,51,results,image captions,are,very useful,image captions are very useful,0.5856327414512634
translation,46,51,results,very useful,for,visual retriever,very useful for visual retriever,0.6016035079956055
translation,46,51,results,very useful,for,visual reader,very useful for visual reader,0.5514658689498901
translation,46,51,results,results,find that,image captions,results find that image captions,0.608267068862915
translation,46,52,results,neural retriever,has,much better performance,neural retriever has much better performance,0.5784130692481995
translation,46,52,results,much better performance,than,term-based retriever,much better performance than term-based retriever,0.5868765115737915
translation,46,52,results,neural retriever,has,much better performance,neural retriever has much better performance,0.5784130692481995
translation,46,52,results,results,has,neural retriever,results has neural retriever,0.5991800427436829
translation,46,54,results,significant leap,when using,noisy knowledge and high-quality knowledge,significant leap when using noisy knowledge and high-quality knowledge,0.7166879177093506
translation,46,54,results,our visual reader,has,significant leap,our visual reader has significant leap,0.619244396686554
translation,46,195,results,our best model,based on,caption - dpr and ereader,our best model based on caption - dpr and ereader,0.6945324540138245
translation,46,195,results,caption - dpr and ereader,has,outperforms,caption - dpr and ereader has outperforms,0.6188218593597412
translation,46,195,results,outperforms,has,previous methods,outperforms has previous methods,0.5876122713088989
translation,46,195,results,results,shows,our best model,results shows our best model,0.6923298835754395
translation,46,196,results,lxmert baseline,without utilizing,any knowledge,lxmert baseline without utilizing any knowledge,0.7832241058349609
translation,46,196,results,lxmert baseline,achieves,better performance,lxmert baseline achieves better performance,0.6883275508880615
translation,46,196,results,better performance,than,krisp,better performance than krisp,0.6065406799316406
translation,46,196,results,better performance,than,"concept- bert ( gard ? res et al. , 2020 )","better performance than concept- bert ( gard ? res et al. , 2020 )",0.5982602834701538
translation,46,196,results,better performance,leverage,external knowledge,better performance leverage external knowledge,0.7352659702301025
translation,46,196,results,"concept- bert ( gard ? res et al. , 2020 )",leverage,external knowledge,"concept- bert ( gard ? res et al. , 2020 ) leverage external knowledge",0.7214674949645996
translation,46,196,results,results,has,lxmert baseline,results has lxmert baseline,0.5620865821838379
translation,46,200,results,bm25 and caption - dpr,on,various number of retrieved knowledge,bm25 and caption - dpr on various number of retrieved knowledge,0.5192134380340576
translation,46,200,results,consistently outperforms,has,bm25 and caption - dpr,consistently outperforms has bm25 and caption - dpr,0.5789934992790222
translation,46,200,results,results,has,consistently outperforms,results has consistently outperforms,0.628938615322113
translation,46,201,results,caption - dpr,has,outperforms,caption - dpr has outperforms,0.6293544769287109
translation,46,201,results,outperforms,has,bm25,outperforms has bm25,0.5911413431167603
translation,46,201,results,outperforms,has,significantly,outperforms has significantly,0.6533594727516174
translation,46,201,results,bm25,has,significantly,bm25 has significantly,0.6845600605010986
translation,46,201,results,results,interesting to see that,caption - dpr,results interesting to see that caption - dpr,0.6990978121757507
translation,46,219,results,knowledge size,is,small ( equal or less than 5 ),knowledge size is small ( equal or less than 5 ),0.5476128458976746
translation,46,219,results,knowledge size,is,large,knowledge size is large,0.5721704363822937
translation,46,219,results,knowledge size,is,large,knowledge size is large,0.5721704363822937
translation,46,219,results,highest - score strategy,is,better,highest - score strategy is better,0.5645559430122375
translation,46,219,results,highest - score strategy,better than,highest - frequency,highest - score strategy better than highest - frequency,0.7380914688110352
translation,46,219,results,highest - score strategy,better than,highest - score strategy,highest - score strategy better than highest - score strategy,0.7346684336662292
translation,46,219,results,knowledge size,is,large,knowledge size is large,0.5721704363822937
translation,46,219,results,highest - frequency strategy,performs,better,highest - frequency strategy performs better,0.6445927619934082
translation,46,219,results,better,than,highest - score strategy,better than highest - score strategy,0.602519154548645
translation,46,219,results,knowledge size,has,highest - score strategy,knowledge size has highest - score strategy,0.5441772937774658
translation,46,219,results,knowledge size,has,highest - frequency strategy,knowledge size has highest - frequency strategy,0.560848593711853
translation,46,219,results,small ( equal or less than 5 ),has,highest - score strategy,small ( equal or less than 5 ) has highest - score strategy,0.5651547908782959
translation,46,219,results,knowledge size,has,highest - frequency strategy,knowledge size has highest - frequency strategy,0.560848593711853
translation,46,219,results,large,has,highest - frequency strategy,large has highest - frequency strategy,0.603094220161438
translation,46,219,results,results,when,knowledge size,results when knowledge size,0.6138296723365784
translation,46,231,results,complete corpus,helpful for,caption - dpr,complete corpus helpful for caption - dpr,0.6147586703300476
translation,46,232,results,accuracy,of,ereader,accuracy of ereader,0.636867344379425
translation,46,232,results,ereader,using,knowledge,ereader using knowledge,0.7407773733139038
translation,46,232,results,knowledge,retrieved from,two corpora,knowledge retrieved from two corpora,0.5606716275215149
translation,46,232,results,results,compares,accuracy,results compares accuracy,0.7881131172180176
translation,46,233,results,ereader,achieves,higher performance,ereader achieves higher performance,0.7285407185554504
translation,46,233,results,higher performance,knowledge retrieved from,complete corpus,higher performance knowledge retrieved from complete corpus,0.6947551965713501
translation,46,233,results,results,has,ereader,results has ereader,0.5921491980552673
translation,47,78,ablation-analysis,question types,involving,"multiple entities ( ' multi-entity ' , ' multi-hop ' , ' multi-relation ' )","question types involving multiple entities ( ' multi-entity ' , ' multi-hop ' , ' multi-relation ' )",0.6317352056503296
translation,47,78,ablation-analysis,increase,is,greatest,increase is greatest,0.6612043976783752
translation,47,78,ablation-analysis,question types,has,increase,question types has increase,0.5965110659599304
translation,47,78,ablation-analysis,"multiple entities ( ' multi-entity ' , ' multi-hop ' , ' multi-relation ' )",has,increase,"multiple entities ( ' multi-entity ' , ' multi-hop ' , ' multi-relation ' ) has increase",0.5959380865097046
translation,47,78,ablation-analysis,ablation analysis,In,question types,ablation analysis In question types,0.484921932220459
translation,47,66,baselines,stacked blstm encoder,operating over,question and facts,stacked blstm encoder operating over question and facts,0.6835656762123108
translation,47,66,baselines,baselines,has,first baseline,baselines has first baseline,0.595583438873291
translation,47,68,baselines,previously highest performing baseline accuracy,at,50.2 %,previously highest performing baseline accuracy at 50.2 %,0.4752870500087738
translation,47,68,baselines,memnet architecture,has,previously highest performing baseline accuracy,memnet architecture has previously highest performing baseline accuracy,0.5116384029388428
translation,47,68,baselines,baselines,has,second,baselines has second,0.6364123225212097
translation,47,69,experimental-setup,uniter base pretrained model,available at,chenrocks github repository,uniter base pretrained model available at chenrocks github repository,0.5095840096473694
translation,47,69,experimental-setup,uniter base pretrained model,with,custom classification layers ( mlp + softmax output layer ),uniter base pretrained model with custom classification layers ( mlp + softmax output layer ),0.62987220287323
translation,47,69,experimental-setup,experimental setup,use,uniter base pretrained model,experimental setup use uniter base pretrained model,0.5559364557266235
translation,47,71,experimental-setup,textual input stream,tokenised with,huggingface ' bert - base-uncased ' tokeniser,textual input stream tokenised with huggingface ' bert - base-uncased ' tokeniser,0.803816556930542
translation,47,71,experimental-setup,experimental setup,has,textual input stream,experimental setup has textual input stream,0.5153065323829651
translation,47,72,experimental-setup,maximum wordpiece sequences length,to,412,maximum wordpiece sequences length to 412,0.5654386878013611
translation,47,72,experimental-setup,maximum visual objects count,to,100,maximum visual objects count to 100,0.5973213315010071
translation,47,72,experimental-setup,"adamw ( loshchilov and hutter , 2017 )",as,optimizer,"adamw ( loshchilov and hutter , 2017 ) as optimizer",0.4737003743648529
translation,47,72,experimental-setup,experimental setup,set,maximum wordpiece sequences length,experimental setup set maximum wordpiece sequences length,0.6319558620452881
translation,47,72,experimental-setup,experimental setup,set,maximum visual objects count,experimental setup set maximum visual objects count,0.6249375939369202
translation,47,72,experimental-setup,experimental setup,set,learning rate,experimental setup set learning rate,0.6158276796340942
translation,47,72,experimental-setup,experimental setup,use,"adamw ( loshchilov and hutter , 2017 )","experimental setup use adamw ( loshchilov and hutter , 2017 )",0.5747809410095215
translation,47,73,experimental-setup,preprocessing,is,completed,preprocessing is completed,0.5924322605133057
translation,47,73,experimental-setup,preprocessing,train,uniter model,preprocessing train uniter model,0.6853774189949036
translation,47,73,experimental-setup,completed,train,uniter model,completed train uniter model,0.6832094192504883
translation,47,73,experimental-setup,uniter model,with,crossentropy objective function,uniter model with crossentropy objective function,0.5939669609069824
translation,47,73,experimental-setup,crossentropy objective function,for,"80,000 iterations","crossentropy objective function for 80,000 iterations",0.5894286632537842
translation,47,73,experimental-setup,experimental setup,Once,preprocessing,experimental setup Once preprocessing,0.6324937343597412
translation,47,73,experimental-setup,experimental setup,train,uniter model,experimental setup train uniter model,0.5984335541725159
translation,47,54,experiments,person detection,use,"mtcnn and facenet ( schroff et al. , 2015 ) models","person detection use mtcnn and facenet ( schroff et al. , 2015 ) models",0.5524813532829285
translation,47,54,experiments,"mtcnn and facenet ( schroff et al. , 2015 ) models",pretrained on,"ms - celeb - 1 m ( guo et al. , 2016 ) dataset","mtcnn and facenet ( schroff et al. , 2015 ) models pretrained on ms - celeb - 1 m ( guo et al. , 2016 ) dataset",0.8013842701911926
translation,47,54,experiments,"mtcnn and facenet ( schroff et al. , 2015 ) models",to generate,128 - dimensional embeddings,"mtcnn and facenet ( schroff et al. , 2015 ) models to generate 128 - dimensional embeddings",0.5988847613334656
translation,47,6,model,new method,to enhance,reasoning capabilities,new method to enhance reasoning capabilities,0.63898104429245
translation,47,6,model,reasoning capabilities,of,multi-modal pretrained model ( vision + language bert ),reasoning capabilities of multi-modal pretrained model ( vision + language bert ),0.5219424962997437
translation,47,6,model,multi-modal pretrained model ( vision + language bert ),by integrating,facts,multi-modal pretrained model ( vision + language bert ) by integrating facts,0.639629602432251
translation,47,6,model,facts,extracted from,external knowledge base,facts extracted from external knowledge base,0.5417879223823547
translation,47,6,model,model,propose,new method,model propose new method,0.675626814365387
translation,47,21,model,new method,of applying,massively pretrained v+l bert model,new method of applying massively pretrained v+l bert model,0.6292317509651184
translation,47,21,model,massively pretrained v+l bert model,to,kvqa task,massively pretrained v+l bert model to kvqa task,0.5134204030036926
translation,47,21,model,", 2020 )",to,kvqa task,", 2020 ) to kvqa task",0.575465738773346
translation,47,21,model,model,propose,new method,model propose new method,0.675626814365387
translation,47,67,results,overall accuracy,of,48.0 %,overall accuracy of 48.0 %,0.537886381149292
translation,47,76,results,outperforms,with,absolute improvement,outperforms with absolute improvement,0.6897989511489868
translation,47,76,results,absolute improvement,of,19 %,absolute improvement of 19 %,0.55837482213974
translation,47,76,results,our system,has,outperforms,our system has outperforms,0.6423544883728027
translation,47,76,results,outperforms,has,previous baseline memnet setting,outperforms has previous baseline memnet setting,0.5646060109138489
translation,47,76,results,results,observe,our system,results observe our system,0.6120408177375793
translation,47,77,results,learning,to perform,reasoning,learning to perform reasoning,0.6876343488693237
translation,47,77,results,more accurately,than,memnet,more accurately than memnet,0.6189025044441223
translation,47,77,results,reasoning,has,more accurately,reasoning has more accurately,0.5885687470436096
translation,47,77,results,results,show,uniter,results show uniter,0.6576721668243408
translation,47,86,results,analysis uniter,performs,well,analysis uniter performs well,0.6784932613372803
translation,47,86,results,well,at,reasoning tasks,well at reasoning tasks,0.522684633731842
translation,47,86,results,better,at,multi-hop,better at multi-hop,0.5665795207023621
translation,47,86,results,results,has,analysis uniter,results has analysis uniter,0.5729730129241943
translation,47,112,results,similar trend,in,fact and image ablation setting,similar trend in fact and image ablation setting,0.5782894492149353
translation,47,112,results,similar trend,that,model,similar trend that model,0.6762531399726868
translation,47,112,results,model,able to,greater leverage questions,model able to greater leverage questions,0.6542090177536011
translation,47,112,results,greater leverage questions,to make,accurate predictions,greater leverage questions to make accurate predictions,0.6833516359329224
translation,47,112,results,accurate predictions,when,additional modalities,accurate predictions when additional modalities,0.6238142848014832
translation,47,112,results,additional modalities,are,never available,additional modalities are never available,0.5745273232460022
translation,47,112,results,results,observe,similar trend,results observe similar trend,0.5865509510040283
translation,47,117,results,our model,improves on,previous state of the art,our model improves on previous state of the art,0.6669758558273315
translation,47,117,results,previous state of the art,by,substantial margin ( 19.1 % ),previous state of the art by substantial margin ( 19.1 % ),0.5494113564491272
translation,47,117,results,results,evaluated,our model,results evaluated our model,0.7086960673332214
translation,48,134,ablation-analysis,fully connected graph,removal of,aspect-aware words,fully connected graph removal of aspect-aware words,0.625834584236145
translation,48,134,ablation-analysis,aspect-aware words,reduces,performance,aspect-aware words reduces performance,0.6698822975158691
translation,48,134,ablation-analysis,performance,has,seriously,performance has seriously,0.6123871207237244
translation,48,134,ablation-analysis,ablation analysis,Note,fully connected graph,ablation analysis Note fully connected graph,0.5386282801628113
translation,48,136,ablation-analysis,model,without employing,dependency tree,model without employing dependency tree,0.7091712355613708
translation,48,136,ablation-analysis,relations,into,graph,relations into graph,0.6329939961433411
translation,48,136,ablation-analysis,ablation analysis,has,model,ablation analysis has model,0.4912438988685608
translation,48,144,ablation-analysis,1 - layer gcn block,performs,unsatisfactorily,1 - layer gcn block performs unsatisfactorily,0.5728732943534851
translation,48,144,ablation-analysis,ablation analysis,has,1 - layer gcn block,ablation analysis has 1 - layer gcn block,0.5034959316253662
translation,48,145,ablation-analysis,fluctuates,with,increasing layer number,fluctuates with increasing layer number,0.7164456248283386
translation,48,145,ablation-analysis,fluctuates,tends to,decline,fluctuates tends to decline,0.7768949866294861
translation,48,145,ablation-analysis,increasing layer number,of,gcn blocks,increasing layer number of gcn blocks,0.632689356803894
translation,48,145,ablation-analysis,decline,when,layer number,decline when layer number,0.675500214099884
translation,48,145,ablation-analysis,layer number,is,greater than 4,layer number is greater than 4,0.5928710103034973
translation,48,145,ablation-analysis,performance,has,fluctuates,performance has fluctuates,0.5909061431884766
translation,48,145,ablation-analysis,ablation analysis,has,performance,ablation analysis has performance,0.5053174495697021
translation,48,111,baselines,mimlln,has,"li et al. , 2020 b )","mimlln has li et al. , 2020 b )",0.6047934293746948
translation,48,112,baselines,various variants,of,proposed aagcn,various variants of proposed aagcn,0.578541100025177
translation,48,112,baselines,baselines,provide,various variants,baselines provide various variants,0.5712874531745911
translation,48,113,baselines,aagcn - bert,takes,input,aagcn - bert takes input,0.6954612135887146
translation,48,113,baselines,aagcn - bert,as,input,aagcn - bert as input,0.5758327841758728
translation,48,113,baselines,[ cls ] sentence [ sep ] aspect [ sep ],as,input,[ cls ] sentence [ sep ] aspect [ sep ] as input,0.5357915163040161
translation,48,113,baselines,baselines,has,aagcn - bert,baselines has aagcn - bert,0.6368136405944824
translation,48,101,hyperparameters,hidden vector dimension,is,300,hidden vector dimension is 300,0.6152721643447876
translation,48,101,hyperparameters,hyperparameters,has,hidden vector dimension,hyperparameters has hidden vector dimension,0.5345754623413086
translation,48,102,hyperparameters,gcn blocks number,is,2,gcn blocks number is 2,0.6196865439414978
translation,48,102,hyperparameters,hyperparameters,has,gcn blocks number,hyperparameters has gcn blocks number,0.503264307975769
translation,48,103,hyperparameters,hyperparameters,has,coefficients,hyperparameters has coefficients,0.5175530314445496
translation,48,104,hyperparameters,adam,utilized as,optimizer,adam utilized as optimizer,0.6164308786392212
translation,48,104,hyperparameters,adam,with,mini-batch,adam with mini-batch,0.6320023536682129
translation,48,104,hyperparameters,optimizer,with,learning rate,optimizer with learning rate,0.6228271722793579
translation,48,104,hyperparameters,optimizer,with,mini-batch,optimizer with mini-batch,0.6316599249839783
translation,48,104,hyperparameters,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,48,104,hyperparameters,mini-batch,of,16,mini-batch of 16,0.648587167263031
translation,48,104,hyperparameters,hyperparameters,has,adam,hyperparameters has adam,0.5514352917671204
translation,48,105,hyperparameters,dropout,of,0.3,dropout of 0.3,0.6232042908668518
translation,48,105,hyperparameters,0.3,after,embedding layer,0.3 after embedding layer,0.6524583697319031
translation,48,105,hyperparameters,hyperparameters,apply,dropout,hyperparameters apply dropout,0.6019902229309082
translation,48,106,hyperparameters,bert - based models,use,"pre-trained uncased bert - base ( devlin et al. , 2019 )","bert - based models use pre-trained uncased bert - base ( devlin et al. , 2019 )",0.5971066951751709
translation,48,106,hyperparameters,bert - based models,use,learning rate,bert - based models use learning rate,0.623436450958252
translation,48,106,hyperparameters,"pre-trained uncased bert - base ( devlin et al. , 2019 )",with,768 dimensional embedding,"pre-trained uncased bert - base ( devlin et al. , 2019 ) with 768 dimensional embedding",0.6079452633857727
translation,48,106,hyperparameters,learning rate,is,0.00002,learning rate is 0.00002,0.5624144077301025
translation,48,106,hyperparameters,hyperparameters,For,bert - based models,hyperparameters For bert - based models,0.5891440510749817
translation,48,20,model,aspect- aware graph ( s ),for,context,aspect- aware graph ( s ) for context,0.5899978876113892
translation,48,20,model,context,with respect to,corresponding aspect,context with respect to corresponding aspect,0.7207880020141602
translation,48,20,model,model,investigate,acsa,model investigate acsa,0.6622636914253235
translation,48,21,model,distinct aspect,as,distinct,distinct aspect as distinct,0.5883125066757202
translation,48,21,model,aspect-related words,from,external knowledge,aspect-related words from external knowledge,0.5227487087249756
translation,48,21,model,substitutes,of,coarse- grained aspect,substitutes of coarse- grained aspect,0.642871618270874
translation,48,21,model,coarse- grained aspect,to construct,graph,coarse- grained aspect to construct graph,0.673783004283905
translation,48,21,model,graph,of,context,graph of context,0.581571102142334
translation,48,21,model,model,regard,distinct aspect,model regard distinct aspect,0.7176353931427002
translation,48,21,model,model,search,aspect-related words,model search aspect-related words,0.669811487197876
translation,48,31,model,aspect-aware graph convolutional network ( aagcn ) structure,to draw,contextual sentiment dependencies,aspect-aware graph convolutional network ( aagcn ) structure to draw contextual sentiment dependencies,0.6040297150611877
translation,48,31,model,contextual sentiment dependencies,to,aspect,contextual sentiment dependencies to aspect,0.5120059251785278
translation,48,31,model,aspect,for,acsa,aspect for acsa,0.742711067199707
translation,48,35,model,aspect-aware graph convolutional network,to draw,contextual sentiment dependencies,aspect-aware graph convolutional network to draw contextual sentiment dependencies,0.6174504160881042
translation,48,35,model,contextual sentiment dependencies,to,aspect,contextual sentiment dependencies to aspect,0.5120059251785278
translation,48,35,model,aspect,for,sentiment detection,aspect for sentiment detection,0.5783583521842957
translation,48,35,model,model,has,aspect-aware graph convolutional network,model has aspect-aware graph convolutional network,0.5292574167251587
translation,48,51,model,aspect- aware graphs construction,constructs,aspect-aware graphs,aspect- aware graphs construction constructs aspect-aware graphs,0.63124680519104
translation,48,51,model,aspect-aware graphs,of,context,aspect-aware graphs of context,0.5424070358276367
translation,48,107,model,"senticnet ( cambria et al. , 2020 )",contains,affective commonsense relations,"senticnet ( cambria et al. , 2020 ) contains affective commonsense relations",0.5903232097625732
translation,48,107,model,affective commonsense relations,between,words,affective commonsense relations between words,0.6438813209533691
translation,48,107,model,affective commonsense relations,to derive,aspect-aware words,affective commonsense relations to derive aspect-aware words,0.6094300746917725
translation,48,107,model,model,has,"senticnet ( cambria et al. , 2020 )","model has senticnet ( cambria et al. , 2020 )",0.5578114986419678
translation,48,123,results,6 datasets,demonstrate,our proposed model,6 datasets demonstrate our proposed model,0.5814072489738464
translation,48,123,results,our proposed model,performs,consistently better,our proposed model performs consistently better,0.6447668671607971
translation,48,123,results,consistently better,than,comparison models,consistently better than comparison models,0.5946530103683472
translation,48,123,results,comparison models,for,non-bert and bert - based models,comparison models for non-bert and bert - based models,0.638984203338623
translation,48,123,results,comparison models,for,e#a and e aspects,comparison models for e#a and e aspects,0.6361783742904663
translation,48,123,results,results,on,6 datasets,results on 6 datasets,0.4788196384906769
translation,48,125,results,models,without employing,distributions,models without employing distributions,0.7221059799194336
translation,48,125,results,distributions,to derive,aspect-aware weights,distributions to derive aspect-aware weights,0.6543532013893127
translation,48,125,results,overall improved,in,any distribution,overall improved in any distribution,0.5813660621643066
translation,48,125,results,models,has,performance,models has performance,0.5729507207870483
translation,48,125,results,distributions,has,performance,distributions has performance,0.5859212875366211
translation,48,125,results,aspect-aware weights,has,performance,aspect-aware weights has performance,0.5745195746421814
translation,48,125,results,results,Compared with,models,results Compared with models,0.6802673935890198
translation,48,127,results,our proposed aagcn,explores,beta distribution,our proposed aagcn explores beta distribution,0.6677573919296265
translation,48,127,results,beta distribution,to determine,aspect-aware weights,beta distribution to determine aspect-aware weights,0.625140905380249
translation,48,127,results,our proposed aagcn,has,outstandingly outperforms,our proposed aagcn has outstandingly outperforms,0.580000638961792
translation,48,127,results,outstandingly outperforms,has,several related distributions,outstandingly outperforms has several related distributions,0.5454792976379395
translation,48,129,results,both aagcn and aagcn -c,perform,overall better,both aagcn and aagcn -c perform overall better,0.641799807548523
translation,48,129,results,overall better,than,baselines,overall better than baselines,0.5951704978942871
translation,48,129,results,different external knowledge scenarios,has,both aagcn and aagcn -c,different external knowledge scenarios has both aagcn and aagcn -c,0.5881211161613464
translation,48,129,results,results,For,different external knowledge scenarios,results For different external knowledge scenarios,0.5877750515937805
translation,48,130,results,models,based on,conceptnet,models based on conceptnet,0.6861376762390137
translation,48,130,results,models,models with,senticnet,models models with senticnet,0.6836720705032349
translation,48,130,results,senticnet,reveal,considerable superiorities,senticnet reveal considerable superiorities,0.71808922290802
translation,48,130,results,considerable superiorities,for,non-bert and bert - based conditions,considerable superiorities for non-bert and bert - based conditions,0.6558704972267151
translation,48,130,results,results,compared with,models,results compared with models,0.6802673935890198
translation,48,130,results,results,models with,senticnet,results models with senticnet,0.7413524389266968
translation,49,129,ablation-analysis,tense,of,auxiliary sentence,tense of auxiliary sentence,0.5357276797294617
translation,49,129,ablation-analysis,tense,is,influential,tense is influential,0.6310055255889893
translation,49,129,ablation-analysis,auxiliary sentence,is,influential,auxiliary sentence is influential,0.5935754776000977
translation,49,129,ablation-analysis,ablation analysis,has,tense,ablation analysis has tense,0.5693032145500183
translation,49,52,baselines,baselines,has,auto encoding ( ae ),baselines has auto encoding ( ae ),0.5516456961631775
translation,49,55,baselines,baselines,has,sequence-to-sequence ( seq2seq ),baselines has sequence-to-sequence ( seq2seq ),0.5597500801086426
translation,49,56,hyperparameters,seq2seq model,initialized with,pre-trained weights,seq2seq model initialized with pre-trained weights,0.7552522420883179
translation,49,56,hyperparameters,pre-trained weights,of,"bart ( lewis et al. , 2020 )","pre-trained weights of bart ( lewis et al. , 2020 )",0.5397520661354065
translation,49,56,hyperparameters,pre-trained weights,includes,encoder and decoder,pre-trained weights includes encoder and decoder,0.6429527997970581
translation,49,56,hyperparameters,hyperparameters,has,seq2seq model,hyperparameters has seq2seq model,0.5068058371543884
translation,49,7,model,semantics - preservation data augmentation approach,by considering,importance,semantics - preservation data augmentation approach by considering importance,0.7372575998306274
translation,49,7,model,importance,of,each word,importance of each word,0.5962938666343689
translation,49,7,model,each word,in,textual sequence,each word in textual sequence,0.5284222364425659
translation,49,7,model,textual sequence,according to,related aspects and sentiments,textual sequence according to related aspects and sentiments,0.6653958559036255
translation,49,7,model,model,propose,semantics - preservation data augmentation approach,model propose semantics - preservation data augmentation approach,0.6638747453689575
translation,49,8,model,unimportant tokens,with,two replacement strategies,unimportant tokens with two replacement strategies,0.6573323607444763
translation,49,8,model,two replacement strategies,without altering,aspect-level polarity,two replacement strategies without altering aspect-level polarity,0.7079955339431763
translation,49,8,model,model,substitute,unimportant tokens,model substitute unimportant tokens,0.7188136577606201
translation,49,72,results,better performances,on,all settings,better performances on all settings,0.5182134509086609
translation,49,72,results,better performances,with,stable results,better performances with stable results,0.6310062408447266
translation,49,72,results,stable results,has,lower standard deviations ),stable results has lower standard deviations ),0.5044218301773071
translation,49,76,results,proposed spm,has,consistently outperforms,proposed spm has consistently outperforms,0.6170216798782349
translation,49,76,results,consistently outperforms,has,random masking strategy ( c- bert ),consistently outperforms has random masking strategy ( c- bert ),0.6157573461532593
translation,49,76,results,results,has,proposed spm,results has proposed spm,0.6180950999259949
translation,49,77,results,proposed token replacement strategy seq2seq,performs,well,proposed token replacement strategy seq2seq performs well,0.6390923261642456
translation,49,77,results,well,in,acsc and atsc,well in acsc and atsc,0.5778183937072754
translation,49,77,results,ae,achieves,best results,ae achieves best results,0.6488016247749329
translation,49,77,results,best results,in,ate,best results in ate,0.609075665473938
translation,49,77,results,results,has,proposed token replacement strategy seq2seq,results has proposed token replacement strategy seq2seq,0.604755699634552
translation,49,82,results,proposed method,improves,more performance,proposed method improves more performance,0.6925889849662781
translation,49,82,results,more performance,than,other data augmentation methods,more performance than other data augmentation methods,0.5404075980186462
translation,49,82,results,most languages,has,proposed method,most languages has proposed method,0.5716005563735962
translation,49,82,results,results,In,most languages,results In most languages,0.4964330494403839
translation,49,123,results,other data augmentation methods,in,price / risk movement prediction tasks,other data augmentation methods in price / risk movement prediction tasks,0.4736957550048828
translation,49,123,results,proposed methods,has,outperform,proposed methods has outperform,0.6184839606285095
translation,49,123,results,outperform,has,other data augmentation methods,outperform has other data augmentation methods,0.5342132449150085
translation,49,123,results,results,has,proposed methods,results has proposed methods,0.5503098368644714
translation,49,130,results,issue-specific term (   market   ),provide,slight improvement,issue-specific term (   market   ) provide slight improvement,0.6043741106987
translation,50,29,baselines,re,trains,model,re trains model,0.7058243751525879
translation,50,29,baselines,model,through,inferring relations,model through inferring relations,0.6974995732307434
translation,50,29,baselines,inferring relations,from,sentences,inferring relations from sentences,0.5464566349983215
translation,50,29,baselines,rm,through determining,two sentences,rm through determining two sentences,0.6993328332901001
translation,50,29,baselines,two sentences,express,same relation,two sentences express same relation,0.6608989834785461
translation,50,29,baselines,baselines,has,re,baselines has re,0.6660313010215759
translation,50,79,baselines,nsm,has,"he et al. , 2021 )","nsm has he et al. , 2021 )",0.6022212505340576
translation,50,79,baselines,baselines,including,"kv - mem ( miller et al. , 2016 )","baselines including kv - mem ( miller et al. , 2016 )",0.633788526058197
translation,50,79,baselines,baselines,including,nsm,baselines including nsm,0.7005568146705627
translation,50,7,experiments,three relation learning tasks,including,relation extraction,three relation learning tasks including relation extraction,0.6280198693275452
translation,50,7,experiments,three relation learning tasks,including,relation matching,three relation learning tasks including relation matching,0.6091251969337463
translation,50,7,experiments,three relation learning tasks,including,relation reasoning,three relation learning tasks including relation reasoning,0.6199536919593811
translation,50,8,model,align,reason over,missing connections,align reason over missing connections,0.7743977308273315
translation,50,8,model,natural language expressions,to,relations,natural language expressions to relations,0.5079076886177063
translation,50,8,model,natural language expressions,reason over,missing connections,natural language expressions reason over missing connections,0.7512524127960205
translation,50,8,model,relations,in,kb,relations in kb,0.5349292159080505
translation,50,8,model,missing connections,in,kb,missing connections in kb,0.5622919797897339
translation,50,8,model,align,has,natural language expressions,align has natural language expressions,0.5476271510124207
translation,50,8,model,model,By,relation - augmented training,model By relation - augmented training,0.6260678768157959
translation,50,27,model,three auxiliary tasks,for,relation learning,three auxiliary tasks for relation learning,0.5472500324249268
translation,50,27,model,model,reformulate,retrieval - based kbqa task,model reformulate retrieval - based kbqa task,0.6814725995063782
translation,50,27,model,model,propose,three auxiliary tasks,model propose three auxiliary tasks,0.6969847083091736
translation,50,30,model,rr,constructs,training data,rr constructs training data,0.713362455368042
translation,50,30,model,rr,trains,model,rr trains model,0.7869507074356079
translation,50,30,model,training data,from,kb,training data from kb,0.5673628449440002
translation,50,30,model,kb,in,self-supervised manner,kb in self-supervised manner,0.5356681942939758
translation,50,30,model,model,to reason over,missing kb connections,model to reason over missing kb connections,0.7477916479110718
translation,50,30,model,missing kb connections,given,existing paths,missing kb connections given existing paths,0.6860675811767578
translation,50,30,model,model,has,rr,model has rr,0.6357825398445129
translation,50,95,results,bert,has,outperform,bert has outperform,0.7030271887779236
translation,50,95,results,outperform,has,most of the baselines,outperform has most of the baselines,0.6042579412460327
translation,50,95,results,results,find that,bert,results find that bert,0.5305137038230896
translation,50,95,results,results,results with,bert,results results with bert,0.7362503409385681
translation,50,96,results,bert,achieves,relative improvement,bert achieves relative improvement,0.7146718502044678
translation,50,96,results,relative improvement,of,4.6 %,relative improvement of 4.6 %,0.5551480054855347
translation,50,96,results,effectiveness,of solving,kbqa,effectiveness of solving kbqa,0.6234946250915527
translation,50,96,results,pullnet,has,bert,pullnet has bert,0.686416506767273
translation,50,96,results,results,comparing to,pullnet,results comparing to pullnet,0.6666520833969116
translation,50,97,results,all three relation learning tasks,has,72.3 ),all three relation learning tasks has 72.3 ),0.5464423894882202
translation,50,97,results,all three relation learning tasks,has,significantly outperform,all three relation learning tasks has significantly outperform,0.56956946849823
translation,50,97,results,72.3 ),has,significantly outperform,72.3 ) has significantly outperform,0.5310566425323486
translation,50,97,results,significantly outperform,has,bert baseline ( 71.2 ),significantly outperform has bert baseline ( 71.2 ),0.5926280617713928
translation,50,100,results,re and rm,are,two most contributing tasks,re and rm are two most contributing tasks,0.5943722128868103
translation,50,100,results,outperform,training with,all three tasks together,outperform training with all three tasks together,0.6824605464935303
translation,50,101,results,rr,brings,performance improvement,rr brings performance improvement,0.5989258289337158
translation,50,101,results,performance improvement,from,71.2 to 71.7,performance improvement from 71.2 to 71.7,0.5019018054008484
translation,50,101,results,performance improvement,under,pre-train setting,performance improvement under pre-train setting,0.6360646486282349
translation,50,101,results,not as significant,as,re,not as significant as re,0.7267037034034729
translation,50,101,results,results,has,rr,results has rr,0.4645278751850128
translation,50,104,results,pre-training setting,with,joint training,pre-training setting with joint training,0.6571197509765625
translation,50,104,results,pre-training setting,find,both settings,pre-training setting find both settings,0.5552882552146912
translation,50,104,results,both settings,work,well,both settings work well,0.658540666103363
translation,50,104,results,outperform,has,bert baseline,outperform has bert baseline,0.6403790712356567
translation,50,104,results,results,comparing,pre-training setting,results comparing pre-training setting,0.6225336790084839
translation,50,105,results,re and rr,for,rm,re and rr for rm,0.7429418563842773
translation,50,105,results,pre-training,seems better than,joint training,pre-training seems better than joint training,0.7271881103515625
translation,50,105,results,pre-training,seems better than,joint training,pre-training seems better than joint training,0.7271881103515625
translation,50,105,results,joint training,is,slightly better,joint training is slightly better,0.5674759149551392
translation,50,105,results,re and rr,has,pre-training,re and rr has pre-training,0.5706707835197449
translation,50,105,results,re and rr,has,joint training,re and rr has joint training,0.5914993286132812
translation,50,105,results,rm,has,joint training,rm has joint training,0.5564214587211609
translation,50,105,results,results,For,re and rr,results For re and rr,0.6531416177749634
translation,50,105,results,results,for,rm,results for rm,0.49229696393013
translation,50,113,results,other baselines,under both,full kb and the 50 % kb settings,other baselines under both full kb and the 50 % kb settings,0.6968963146209717
translation,50,113,results,our approach,has,consistently outperforms,our approach has consistently outperforms,0.6119859218597412
translation,50,113,results,consistently outperforms,has,other baselines,consistently outperforms has other baselines,0.5855390429496765
translation,50,114,results,adding relation learning tasks,achieve,more performance gain,adding relation learning tasks achieve more performance gain,0.6475682854652405
translation,50,114,results,more performance gain,than with,full kb,more performance gain than with full kb,0.6835567951202393
translation,50,114,results,50 % kb,has,adding relation learning tasks,50 % kb has adding relation learning tasks,0.6058359146118164
translation,50,114,results,full kb,has,+ 2.1 vs + 1.1 ),full kb has + 2.1 vs + 1.1 ),0.5789492726325989
translation,50,114,results,results,With,50 % kb,results With 50 % kb,0.6735849976539612
translation,50,124,results,fine - grained annotations ( head-tail ),better than,coarse-grained ones ( all ),fine - grained annotations ( head-tail ) better than coarse-grained ones ( all ),0.7690244913101196
translation,50,124,results,results,has,fine - grained annotations ( head-tail ),results has fine - grained annotations ( head-tail ),0.5507245063781738
translation,51,87,ablation-analysis,equally crucial,for,short answer performance,equally crucial for short answer performance,0.656587541103363
translation,51,87,ablation-analysis,short answer performance,on,nq,short answer performance on nq,0.5473436713218689
translation,51,87,ablation-analysis,ablation analysis,Having access to,gold answer type and gold paragraph,ablation analysis Having access to gold answer type and gold paragraph,0.6730992197990417
translation,51,120,ablation-analysis,answer type prediction component,of,etc,answer type prediction component of etc,0.5901679992675781
translation,51,127,experiments,question only baseline,achieved,only 63 % accuracy,question only baseline achieved only 63 % accuracy,0.7123921513557434
translation,51,127,experiments,squad 2.0,has,question only baseline,squad 2.0 has question only baseline,0.5756881833076477
translation,51,88,results,long answers,observe,models,long answers observe models,0.6404189467430115
translation,51,88,results,models,rank,paragraphs,models rank paragraphs,0.7024473547935486
translation,51,88,results,struggle,to decide,abstain from answering,struggle to decide abstain from answering,0.752322256565094
translation,51,88,results,paragraphs,has,correctly,paragraphs has correctly,0.6445270776748657
translation,51,88,results,results,For,long answers,results For long answers,0.6235396265983582
translation,51,89,results,gold type,is,given,gold type is given,0.6210423111915588
translation,51,89,results,gold type,reaches,84.6 f1,gold type reaches 84.6 f1,0.660869836807251
translation,51,89,results,84.6 f1,for,long answer task,84.6 f1 for long answer task,0.5591674447059631
translation,51,89,results,significantly outperforms,has,single annotator performance,significantly outperforms has single annotator performance,0.5715593099594116
translation,51,90,results,model 's short answer f1 score,reaches,10 %,model 's short answer f1 score reaches 10 %,0.7018428444862366
translation,51,90,results,10 %,above,single annotator,10 % above single annotator,0.6838573813438416
translation,51,90,results,gold paragraph and answer type (   gold t&p   ),has,model 's short answer f1 score,gold paragraph and answer type (   gold t&p   ) has model 's short answer f1 score,0.5619320869445801
translation,51,90,results,results,Provided,gold paragraph and answer type (   gold t&p   ),results Provided gold paragraph and answer type (   gold t&p   ),0.6595461368560791
translation,51,91,results,short answers,providing,gold paragraph,short answers providing gold paragraph,0.6478304862976074
translation,51,91,results,gold paragraph,improve,etc 's performance,gold paragraph improve etc 's performance,0.6808792352676392
translation,51,91,results,etc 's performance,by,5 points,etc 's performance by 5 points,0.6061453819274902
translation,51,91,results,results,For,short answers,results For short answers,0.5989691615104675
translation,51,92,results,significantly improves,at,small cost,significantly improves at small cost,0.5491505861282349
translation,51,92,results,recall,at,small cost,recall at small cost,0.5242074131965637
translation,51,92,results,small cost,of,precision,small cost of precision,0.5898861885070801
translation,51,92,results,gold answer type information,has,significantly improves,gold answer type information has significantly improves,0.6107634902000427
translation,51,92,results,significantly improves,has,recall,significantly improves has recall,0.6178644299507141
translation,51,92,results,results,Having,gold answer type information,results Having gold answer type information,0.610828697681427
translation,51,94,results,gold type information,has,long answer f1 score,gold type information has long answer f1 score,0.554725706577301
translation,51,94,results,results,Given,gold type information,results Given gold type information,0.6260258555412292
translation,51,119,results,question only models,yield,over 70 % accuracy,question only models yield over 70 % accuracy,0.7342270016670227
translation,51,119,results,over 70 % accuracy,in,nq and tydi qa,over 70 % accuracy in nq and tydi qa,0.5670631527900696
translation,51,119,results,results,has,question only models,results has question only models,0.5644785165786743
translation,51,129,results,context,provided,qa model,context provided qa model,0.6666952967643738
translation,51,129,results,qa model,achieves,almost 95 % accuracy,qa model achieves almost 95 % accuracy,0.7206025123596191
translation,51,129,results,context,has,qa model,context has qa model,0.6086035370826721
translation,51,129,results,results,when,context,results when context,0.6356602907180786
translation,51,199,results,q only and qa models,show,lowest error rate,q only and qa models show lowest error rate,0.673129677772522
translation,51,199,results,lowest error rate,on,invalid questions,lowest error rate on invalid questions,0.5127043724060059
translation,51,199,results,invalid questions,on,nq,invalid questions on nq,0.5723050236701965
translation,51,199,results,results,has,q only and qa models,results has q only and qa models,0.557386040687561
translation,51,200,results,struggle,on,invalid answer category,struggle on invalid answer category,0.5569272637367249
translation,51,200,results,all models,has,struggle,all models has struggle,0.6376453638076782
translation,51,200,results,results,has,all models,results has all models,0.5029959678649902
translation,52,75,ablation-analysis,student state,crucial for,accurately predicting,student state crucial for accurately predicting,0.7139039635658264
translation,52,75,ablation-analysis,question text,leads to,significant improvement,question text leads to significant improvement,0.6448631286621094
translation,52,75,ablation-analysis,accurately predicting,has,duolingo user responses,accurately predicting has duolingo user responses,0.5538969039916992
translation,52,75,ablation-analysis,ablation analysis,incorporating,student state,ablation analysis incorporating student state,0.7400810122489929
translation,52,75,ablation-analysis,ablation analysis,including,question text,ablation analysis including question text,0.717024028301239
translation,52,9,baselines,lm - kt,to specify,objective and data,lm - kt to specify objective and data,0.6510376334190369
translation,52,9,baselines,objective and data,for training,model,objective and data for training model,0.7747721076011658
translation,52,9,baselines,model,to generate,questions,model to generate questions,0.7369361519813538
translation,52,9,baselines,questions,conditioned on,student and target difficulty,questions conditioned on student and target difficulty,0.6903033256530762
translation,52,7,model,pre-trained language models,for,deep knowledge tracing ( lm - kt ),pre-trained language models for deep knowledge tracing ( lm - kt ),0.5964881777763367
translation,52,7,model,model,fine- tune,pre-trained language models,model fine- tune pre-trained language models,0.604514479637146
translation,52,23,model,can be easily leveraged,to adaptively generate,questions,can be easily leveraged to adaptively generate questions,0.7714431881904602
translation,52,23,model,questions,for,given student and target difficulty,questions for given student and target difficulty,0.5608148574829102
translation,52,23,model,given student and target difficulty,in,reverse translation task,given student and target difficulty in reverse translation task,0.463060200214386
translation,52,23,model,difficulty at answering questions,proxy for,more complex future learning objectives,difficulty at answering questions proxy for more complex future learning objectives,0.6500958800315857
translation,52,23,model,pre-trained lms,has,can be easily leveraged,pre-trained lms has can be easily leveraged,0.5855507254600525
translation,52,23,model,model,show,pre-trained lms,model show pre-trained lms,0.6595539450645447
translation,52,24,model,lm - based knowledge tracing model ( lm - kt ),to predict,students ' difficulty,lm - based knowledge tracing model ( lm - kt ) to predict students ' difficulty,0.6913345456123352
translation,52,24,model,students ' difficulty,on,novel questions ( e.g. target phrases to translate ),students ' difficulty on novel questions ( e.g. target phrases to translate ),0.4867424964904785
translation,52,24,model,model,introduce,lm - based knowledge tracing model ( lm - kt ),model introduce lm - based knowledge tracing model ( lm - kt ),0.6227577328681946
translation,52,8,results,probability,of,student answering,probability of student answering,0.6060045957565308
translation,52,8,results,questions,not seen in,training,questions not seen in training,0.710114061832428
translation,52,8,results,student answering,has,question,student answering has question,0.5858585834503174
translation,52,25,results,lm - kt,is,well - calibrated,lm - kt is well - calibrated,0.5967527031898499
translation,52,25,results,learning problem,for,question generator,learning problem for question generator,0.6051211357116699
translation,52,25,results,question generator,given,student state,question generator given student state,0.7323461174964905
translation,52,25,results,student state,generate,question,student state generate question,0.6834036111831665
translation,52,25,results,question,achieve,target difficulty,question achieve target difficulty,0.6345786452293396
translation,52,25,results,results,show that,lm - kt,results show that lm - kt,0.4967654049396515
translation,52,76,results,outperforms,especially on,novel questions,outperforms especially on novel questions,0.6309127807617188
translation,52,76,results,standard dkt,especially on,novel questions,standard dkt especially on novel questions,0.6567835211753845
translation,52,76,results,lm - kt,has,outperforms,lm - kt has outperforms,0.6538053154945374
translation,52,76,results,outperforms,has,standard dkt,outperforms has standard dkt,0.5964853763580322
translation,52,76,results,results,has,lm - kt,results has lm - kt,0.5457725524902344
translation,52,84,results,perplexity,is,lower,perplexity is lower,0.5982967019081116
translation,52,84,results,lower,for,true student / target difficulty inputs,lower for true student / target difficulty inputs,0.5990498661994934
translation,52,84,results,results,shows that,perplexity,results shows that perplexity,0.655540943145752
translation,52,90,results,fine- grained control,over,target difficulty,fine- grained control over target difficulty,0.6057319045066833
translation,52,90,results,fine- grained control,with,average root - mean squared error ( rmse ),fine- grained control with average root - mean squared error ( rmse ),0.6441946625709534
translation,52,90,results,target difficulty,for,spanish and french students,target difficulty for spanish and french students,0.5951939821243286
translation,52,90,results,average root - mean squared error ( rmse ),of,.052,average root - mean squared error ( rmse ) of .052,0.5709220170974731
translation,52,90,results,.052,across,all students and target difficulties,.052 across all students and target difficulties,0.5249988436698914
translation,52,95,results,one nvidia titan xp gpu,find that,our question generation model,one nvidia titan xp gpu find that our question generation model,0.6131789088249207
translation,52,95,results,our question generation model,takes,half the time,our question generation model takes half the time,0.6697583198547363
translation,52,95,results,half the time,to achieve,same quality,half the time to achieve same quality,0.6462015509605408
translation,52,95,results,same quality,as,pool selection,same quality as pool selection,0.5736178755760193
translation,52,95,results,results,On,one nvidia titan xp gpu,results On one nvidia titan xp gpu,0.5298247337341309
translation,53,153,baselines,d-s-rl,optimizes,ag model,d-s-rl optimizes ag model,0.7790408134460449
translation,53,153,baselines,ag model,of,d-s,ag model of d-s,0.6993250846862793
translation,53,153,baselines,ag model,of,d-s,ag model of d-s,0.6993250846862793
translation,53,153,baselines,d-s,using,rl,d-s using rl,0.6865726709365845
translation,53,153,baselines,reward function,defined as,negative question reconstruction loss,reward function defined as negative question reconstruction loss,0.5490791201591492
translation,53,153,baselines,negative question reconstruction loss,calculated by,qg model,negative question reconstruction loss calculated by qg model,0.6039975881576538
translation,53,153,baselines,qg model,of,d-s,qg model of d-s,0.6568593382835388
translation,53,153,baselines,baselines,has,d-s-rl,baselines has d-s-rl,0.5973802804946899
translation,53,174,baselines,qa - gen 2s,is,state - of - the - art model,qa - gen 2s is state - of - the - art model,0.5882008671760559
translation,53,174,baselines,state - of - the - art model,for,qa pairs generation,state - of - the - art model for qa pairs generation,0.567143976688385
translation,53,174,baselines,baselines,has,qa - gen 2s,baselines has qa - gen 2s,0.619027316570282
translation,53,176,baselines,ctrlsum,use,pretrained ctrlsum model,ctrlsum use pretrained ctrlsum model,0.6201831102371216
translation,53,176,baselines,pretrained ctrlsum model,to generate,questiondependent summaries,pretrained ctrlsum model to generate questiondependent summaries,0.6594979166984558
translation,53,176,baselines,baselines,has,ctrlsum,baselines has ctrlsum,0.575089693069458
translation,53,177,baselines,questions,generated by,qg model,questions generated by qg model,0.6931492686271667
translation,53,177,baselines,qg model,of,qd -d.,qg model of qd -d.,0.6215362548828125
translation,53,177,baselines,qg model,of,qa transfer,qg model of qa transfer,0.6166903972625732
translation,53,177,baselines,qd -d.,has,qa transfer,qd -d. has qa transfer,0.6310160160064697
translation,53,177,baselines,baselines,has,questions,baselines has questions,0.5849764347076416
translation,53,240,baselines,best performing baselines,are,qa transfer,best performing baselines are qa transfer,0.6168563365936279
translation,53,240,baselines,best performing baselines,are,qagen 2s,best performing baselines are qagen 2s,0.5633531212806702
translation,53,240,baselines,qa transfer,in,length bucket 0,qa transfer in length bucket 0,0.5859615206718445
translation,53,240,baselines,qagen 2s,in,length bucket 1 and 2,qagen 2s in length bucket 1 and 2,0.5633055567741394
translation,53,240,baselines,baselines,has,best performing baselines,baselines has best performing baselines,0.5600970983505249
translation,53,18,experiments,qa pair generation,corresponding to,news article summary,qa pair generation corresponding to news article summary,0.6006718277931213
translation,53,18,experiments,news article summary,paired with,self-contained question,news article summary paired with self-contained question,0.6566063761711121
translation,53,22,experiments,qa pair generation,has,for varying application -specific answer length requirements,qa pair generation has for varying application -specific answer length requirements,0.5314407348632812
translation,53,175,experiments,qagen 2s,on,our dataset,qagen 2s on our dataset,0.5534788370132446
translation,53,181,experiments,d-s-newsqa and natural questions ( d-s,has,nq ),d-s-newsqa and natural questions ( d-s has nq ),0.64630126953125
translation,53,229,experiments,d-s- newsqa,ask,trivial questions,d-s- newsqa ask trivial questions,0.7787649035453796
translation,53,5,model,new dataset,of,news articles,new dataset of news articles,0.5434927344322205
translation,53,5,model,new dataset,with,questions,new dataset with questions,0.6410621404647827
translation,53,5,model,new dataset,pairing them with,summaries,new dataset pairing them with summaries,0.6868412494659424
translation,53,5,model,questions,as,titles,questions as titles,0.47045183181762695
translation,53,5,model,summaries,of,varying length,summaries of varying length,0.6208342909812927
translation,53,5,model,model,collecting,new dataset,model collecting new dataset,0.6931135654449463
translation,53,6,model,qa pair generation model,producing,summaries,qa pair generation model producing summaries,0.7541446089744568
translation,53,6,model,summaries,as,answers,summaries as answers,0.5767810940742493
translation,53,7,model,qa pair generation process,with,differentiable reward function,qa pair generation process with differentiable reward function,0.6004921793937683
translation,53,7,model,model,reinforce,qa pair generation process,model reinforce qa pair generation process,0.7019153833389282
translation,53,36,model,novel differential reward imitation learning ( dril ) training method,samples,summary answers,novel differential reward imitation learning ( dril ) training method samples summary answers,0.7084092497825623
translation,53,36,model,novel differential reward imitation learning ( dril ) training method,reconstructs,questions,novel differential reward imitation learning ( dril ) training method reconstructs questions,0.7108201384544373
translation,53,36,model,questions,exclusively based on,hidden states,questions exclusively based on hidden states,0.6802523732185364
translation,53,36,model,model,propose,novel differential reward imitation learning ( dril ) training method,model propose novel differential reward imitation learning ( dril ) training method,0.6441139578819275
translation,53,42,model,novel differentiable reward imita-tion learning ( dril ) method,shows,better performance,novel differentiable reward imita-tion learning ( dril ) method shows better performance,0.6032571196556091
translation,53,42,model,better performance,over,maximum likelihood estimation ( mle ),better performance over maximum likelihood estimation ( mle ),0.6848952174186707
translation,53,42,model,reinforcement learning ( rl ),for,qa pair generation,reinforcement learning ( rl ) for qa pair generation,0.5997253060340881
translation,53,42,model,model,propose,novel differentiable reward imita-tion learning ( dril ) method,model propose novel differentiable reward imita-tion learning ( dril ) method,0.6536432504653931
translation,53,194,results,models,generate,questions,models generate questions,0.7126559615135193
translation,53,194,results,models,generate,answers,models generate answers,0.7134520411491394
translation,53,194,results,models,generate,questions,models generate questions,0.7126559615135193
translation,53,194,results,models,generate,answers,models generate answers,0.7134520411491394
translation,53,194,results,questions,based on,answers,questions based on answers,0.6462957262992859
translation,53,194,results,questions,based on,answers,questions based on answers,0.6462957262992859
translation,53,194,results,answers,have,higher qacs and at - 2 accuracy,answers have higher qacs and at - 2 accuracy,0.5954400897026062
translation,53,194,results,higher qacs and at - 2 accuracy,than,models,higher qacs and at - 2 accuracy than models,0.5927991271018982
translation,53,194,results,models,generate,answers,models generate answers,0.7134520411491394
translation,53,194,results,answers,based on,questions,answers based on questions,0.6355968713760376
translation,53,194,results,results,has,models,results has models,0.5335168838500977
translation,53,199,results,qd - d,achieves,best rouge -l scores,qd - d achieves best rouge -l scores,0.701303243637085
translation,53,199,results,best rouge -l scores,while,qacs and at - 2 accuracy,best rouge -l scores while qacs and at - 2 accuracy,0.596921980381012
translation,53,199,results,significantly lower,than,d-s,significantly lower than d-s,0.6098872423171997
translation,53,199,results,results,has,qd - d,results has qd - d,0.5671632885932922
translation,53,210,results,dril and rl,reinforce,ag,dril and rl reinforce ag,0.735892117023468
translation,53,210,results,dril and rl,better reconstruct,ground truth questions,dril and rl better reconstruct ground truth questions,0.6057389974594116
translation,53,210,results,ag,with,question reconstruction loss,ag with question reconstruction loss,0.6473233699798584
translation,53,210,results,ground truth questions,on,validation set,ground truth questions on validation set,0.526391327381134
translation,53,210,results,results,has,dril and rl,results has dril and rl,0.5538358092308044
translation,53,213,results,higher,AT,- 5 accuracy,higher AT - 5 accuracy,0.5663421154022217
translation,53,213,results,- 5 accuracy,than,d-s,- 5 accuracy than d-s,0.6393216848373413
translation,53,213,results,d-s,on,length bucket 0 and 1,d-s on length bucket 0 and 1,0.6064610481262207
translation,53,213,results,d-s-dril,has,about 6 % and 3 %,d-s-dril has about 6 % and 3 %,0.5811521410942078
translation,53,213,results,about 6 % and 3 %,has,higher,about 6 % and 3 % has higher,0.5733118653297424
translation,53,213,results,results,has,d-s-dril,results has d-s-dril,0.5482454895973206
translation,53,214,results,lower at - 5 accuracy,than,d-s,lower at - 5 accuracy than d-s,0.6622948050498962
translation,53,214,results,d-s,on,length bucket 2,d-s on length bucket 2,0.5785864591598511
translation,53,214,results,d-s-dril,has,lower at - 5 accuracy,d-s-dril has lower at - 5 accuracy,0.6291201710700989
translation,53,214,results,results,has,d-s-dril,results has d-s-dril,0.5482454895973206
translation,53,215,results,d-s-dril,shows,better performance,d-s-dril shows better performance,0.7023369073867798
translation,53,215,results,better performance,compared with,d-s- rl,better performance compared with d-s- rl,0.6971103549003601
translation,53,215,results,advantage,of,differentiable question reconstruction loss,advantage of differentiable question reconstruction loss,0.5714266300201416
translation,53,215,results,differentiable question reconstruction loss,over,non-differentiable question reconstruction reward,differentiable question reconstruction loss over non-differentiable question reconstruction reward,0.6232190132141113
translation,53,215,results,results,has,d-s-dril,results has d-s-dril,0.5482454895973206
translation,53,227,results,qg model,trained on,newsqa,qg model trained on newsqa,0.7334450483322144
translation,53,227,results,about 50 % lower,AT,- 1 accuracy,about 50 % lower AT - 1 accuracy,0.5999905467033386
translation,53,227,results,- 1 accuracy,than,other two models,- 1 accuracy than other two models,0.5780202150344849
translation,53,227,results,results,see that,qg model,results see that qg model,0.6792746782302856
translation,53,228,results,qg models,trained on,newsqa and natural questions,qg models trained on newsqa and natural questions,0.738154947757721
translation,53,228,results,qg models,trained on,sc ) 2 qa,qg models trained on sc ) 2 qa,0.7357859015464783
translation,53,228,results,newsqa and natural questions,achieve,73.55 % and 60.03 % lower accuracy,newsqa and natural questions achieve 73.55 % and 60.03 % lower accuracy,0.6181221008300781
translation,53,228,results,73.55 % and 60.03 % lower accuracy,on,at - 5,73.55 % and 60.03 % lower accuracy on at - 5,0.5909661054611206
translation,53,228,results,73.55 % and 60.03 % lower accuracy,averaged over,3 length buckets,73.55 % and 60.03 % lower accuracy averaged over 3 length buckets,0.738182544708252
translation,53,228,results,73.55 % and 60.03 % lower accuracy,compared with,qg model,73.55 % and 60.03 % lower accuracy compared with qg model,0.7016690969467163
translation,53,228,results,qg model,trained on,sc ) 2 qa,qg model trained on sc ) 2 qa,0.749638020992279
translation,53,228,results,results,has,qg models,results has qg models,0.5275816321372986
translation,53,239,results,d-s-dril,performs,significantly better,d-s-dril performs significantly better,0.6782827377319336
translation,53,239,results,significantly better,than,best performing baselines,significantly better than best performing baselines,0.5819653272628784
translation,53,239,results,results,has,d-s-dril,results has d-s-dril,0.5482454895973206
translation,53,241,results,results,observe,"d-d , d-s , d-s-dril and d-s- rl","results observe d-d , d-s , d-s-dril and d-s- rl",0.5487309694290161
translation,53,242,results,outperforms,by,"31.51 % , 34.82 % and 22.92 %","outperforms by 31.51 % , 34.82 % and 22.92 %",0.6123649477958679
translation,53,242,results,"31.51 % , 34.82 % and 22.92 %",in,"length bucket 0 , 1 and 2","31.51 % , 34.82 % and 22.92 % in length bucket 0 , 1 and 2",0.5331873893737793
translation,53,242,results,d-s-dril,has,outperforms,d-s-dril has outperforms,0.6436513066291809
translation,53,242,results,results,has,d-s-dril,results has d-s-dril,0.5482454895973206
translation,53,243,results,dril,has,consistently outperforms,dril has consistently outperforms,0.6294049024581909
translation,53,243,results,consistently outperforms,has,rl,consistently outperforms has rl,0.6155498027801514
translation,53,243,results,results,has,dril,results has dril,0.5895336270332336
translation,53,244,results,d-s- dril,outperforms,d-s and d-s- rl,d-s- dril outperforms d-s and d-s- rl,0.7431555986404419
translation,53,244,results,d-s and d-s- rl,by,3.22 % and 2.80 %,d-s and d-s- rl by 3.22 % and 2.80 %,0.5885355472564697
translation,53,244,results,results,see,d-s- dril,results see d-s- dril,0.5934878587722778
translation,54,195,experiments,splits ),for,convquestions,splits ) for convquestions,0.6996380090713501
translation,54,234,experiments,average depth,of,our lfs ( 2.9 ),average depth of our lfs ( 2.9 ),0.5541584491729736
translation,54,234,experiments,average depth,is,slightly slower,average depth is slightly slower,0.5784452557563782
translation,54,234,experiments,slightly slower,than,d2a grammar,slightly slower than d2a grammar,0.5727199912071228
translation,54,234,experiments,csqa,has,average depth,csqa has average depth,0.5206988453865051
translation,54,234,experiments,d2a grammar,has,3.2 ),d2a grammar has 3.2 ),0.5919455885887146
translation,54,5,model,new logical form ( lf ) grammar,model,wide range of queries,new logical form ( lf ) grammar model wide range of queries,0.7149132490158081
translation,54,5,model,wide range of queries,on,graph,wide range of queries on graph,0.5727288722991943
translation,54,5,model,remaining sufficiently simple,to generate,supervision data,remaining sufficiently simple to generate supervision data,0.7367437481880188
translation,54,5,model,supervision data,has,efficiently,supervision data has efficiently,0.5388407111167908
translation,54,5,model,model,introduce,new logical form ( lf ) grammar,model introduce new logical form ( lf ) grammar,0.6338236927986145
translation,54,6,model,transformer - based model,takes,json - like structure,transformer - based model takes json - like structure,0.6315608620643616
translation,54,6,model,json - like structure,as,input,json - like structure as input,0.5177733302116394
translation,54,6,model,model,has,transformer - based model,model has transformer - based model,0.5355522632598877
translation,54,39,model,new grammar,can model,large range of queries,new grammar can model large range of queries,0.7306209802627563
translation,54,39,model,new grammar,simple enough for,bfs,new grammar simple enough for bfs,0.7512134909629822
translation,54,39,model,large range of queries,on,kg,large range of queries on kg,0.570254385471344
translation,54,39,model,bfs,to work,well,bfs to work well,0.7104885578155518
translation,54,39,model,model,design,new grammar,model design new grammar,0.5996924042701721
translation,54,43,model,semantic parsing model,uses,kg contextual data,semantic parsing model uses kg contextual data,0.5330563187599182
translation,54,43,model,kg contextual data,in addition to,utterances,kg contextual data in addition to utterances,0.6014177203178406
translation,54,46,model,object - aware transformer ( oat ) model,take as input,structured data,object - aware transformer ( oat ) model take as input structured data,0.6373852491378784
translation,54,46,model,structured data,in,json - like format,structured data in json - like format,0.5308176279067993
translation,54,46,model,model,propose,object - aware transformer ( oat ) model,model propose object - aware transformer ( oat ) model,0.6424385905265808
translation,54,47,model,structured input,into,embeddings,structured input into embeddings,0.5912525057792664
translation,54,47,model,model,transforms,structured input,model transforms structured input,0.7576509118080139
translation,54,9,results,our approach,increases,coverage,our approach increases coverage,0.6929472088813782
translation,54,9,results,coverage,from,80 % to 96.2 %,coverage from 80 % to 96.2 %,0.5152294039726257
translation,54,9,results,lf execution accuracy,from,70.6 % to 75.6 %,lf execution accuracy from 70.6 % to 75.6 %,0.5219799876213074
translation,54,9,results,70.6 % to 75.6 %,with respect to,previous state - of - the - art results,70.6 % to 75.6 % with respect to previous state - of - the - art results,0.6632604598999023
translation,54,9,results,csqa,has,our approach,csqa has our approach,0.6096063852310181
translation,54,9,results,results,On,csqa,results On csqa,0.5589053630828857
translation,54,40,results,high coverage,on,two kg - qa datasets,high coverage on two kg - qa datasets,0.5272505879402161
translation,54,40,results,results,obtain,high coverage,results obtain high coverage,0.612166702747345
translation,54,41,results,"csqa ( saha et al. , 2018 )",achieve,coverage,"csqa ( saha et al. , 2018 ) achieve coverage",0.6537351608276367
translation,54,41,results,coverage,of,96 %,coverage of 96 %,0.5979332327842712
translation,54,41,results,16 % improvement,over,baseline,16 % improvement over baseline,0.7029068470001221
translation,54,41,results,96 %,has,16 % improvement,96 % has 16 % improvement,0.611072838306427
translation,54,41,results,results,On,"csqa ( saha et al. , 2018 )","results On csqa ( saha et al. , 2018 )",0.5150545239448547
translation,54,228,results,our grammar,reaches,high coverage,our grammar reaches high coverage,0.7311813831329346
translation,54,228,results,results,has,our grammar,results has our grammar,0.5738901495933533
translation,54,229,results,same numbers of operators,improve,csqa coverage,same numbers of operators improve csqa coverage,0.6635932326316833
translation,54,229,results,csqa coverage,by,16 %,csqa coverage by 16 %,0.5955705046653748
translation,54,229,results,results,With,same numbers of operators,results With same numbers of operators,0.659687340259552
translation,54,231,results,coverage,of,86.2 %,coverage of 86.2 %,0.5416948795318604
translation,54,231,results,86.2 %,on,convquestions,86.2 % on convquestions,0.5500166416168213
translation,54,235,results,qa performance,over,baseline,qa performance over baseline,0.7108950018882751
translation,54,235,results,qa performance,on,both datasets,qa performance on both datasets,0.4853837490081787
translation,54,235,results,results,improve,qa performance,results improve qa performance,0.6782097220420837
translation,54,236,results,our model,forms,baselines,our model forms baselines,0.7124192118644714
translation,54,236,results,outper,-,baselines,outper - baselines,0.6642812490463257
translation,54,236,results,outper,forms,baselines,outper forms baselines,0.7454414963722229
translation,54,236,results,outper,for,all question types,outper for all question types,0.6850041747093201
translation,54,236,results,baselines,for,all question types,baselines for all question types,0.5654813051223755
translation,54,236,results,baselines,for,direct simple questions,baselines for direct simple questions,0.6098059415817261
translation,54,236,results,our model,improves,performance,our model improves performance,0.7238103747367859
translation,54,236,results,performance,by,5 %,performance by 5 %,0.6092576384544373
translation,54,236,results,csqa,has,our model,csqa has our model,0.5871689319610596
translation,54,236,results,our model,has,outper,our model has outper,0.6187851428985596
translation,54,236,results,results,For,csqa,results For csqa,0.6374459862709045
translation,54,237,results,conv-questions,shows,our model,conv-questions shows our model,0.7067755460739136
translation,54,237,results,our model,improves over,baseline,our model improves over baseline,0.7618032097816467
translation,54,237,results,baseline,for,all domains but one,baseline for all domains but one,0.6582809090614319
translation,54,237,results,overall improvement,of,4.7 %,overall improvement of 4.7 %,0.5260658860206604
translation,54,237,results,results,For,conv-questions,results For conv-questions,0.5922014713287354
translation,55,74,baselines,lstm,acts as,baseline,lstm acts as baseline,0.621769368648529
translation,55,74,baselines,lstm,acts as,baselines,lstm acts as baselines,0.6198511719703674
translation,55,74,baselines,baseline,for,models,baseline for models,0.6329545974731445
translation,55,74,baselines,baseline,for,models,baseline for models,0.6329545974731445
translation,55,74,baselines,baseline,for,models,baseline for models,0.6329545974731445
translation,55,74,baselines,models,without using,plm,models without using plm,0.721270740032196
translation,55,74,baselines,models,without using,corresponding plms,models without using corresponding plms,0.7297158241271973
translation,55,74,baselines,models,with,corresponding plms,models with corresponding plms,0.6355447173118591
translation,55,74,baselines,bert - spc and roberta - mlp,are,baselines,bert - spc and roberta - mlp are baselines,0.5908552408218384
translation,55,74,baselines,baselines,for,models,baselines for models,0.679167628288269
translation,55,74,baselines,models,with,corresponding plms,models with corresponding plms,0.6355447173118591
translation,55,74,baselines,baselines,has,lstm,baselines has lstm,0.5395978093147278
translation,55,6,model,grammatical sequential features,from,plm of bert,grammatical sequential features from plm of bert,0.582516610622406
translation,55,6,model,syntactic knowledge,from,dependency graphs,syntactic knowledge from dependency graphs,0.5260922908782959
translation,55,6,model,model,propose,novel model,model propose novel model,0.6891457438468933
translation,55,6,model,model,propose,bert4gcn,model propose bert4gcn,0.6858947277069092
translation,55,7,model,bert4gcn,utilizes,outputs,bert4gcn utilizes outputs,0.7180281281471252
translation,55,7,model,outputs,from,intermediate layers,outputs from intermediate layers,0.5442511439323425
translation,55,7,model,intermediate layers,of,bert,intermediate layers of bert,0.6367949843406677
translation,55,7,model,positional information,between,words,positional information between words,0.653178870677948
translation,55,7,model,words,to augment,gcn ( graph convolutional network ),words to augment gcn ( graph convolutional network ),0.6829253435134888
translation,55,7,model,gcn ( graph convolutional network ),to better encode,dependency graphs,gcn ( graph convolutional network ) to better encode dependency graphs,0.693901538848877
translation,55,7,model,dependency graphs,for,downstream classification,dependency graphs for downstream classification,0.5959062576293945
translation,55,7,model,model,has,bert4gcn,model has bert4gcn,0.6771584749221802
translation,55,21,model,context,with,bilstm ( bidirectional long short - term memory ),context with bilstm ( bidirectional long short - term memory ),0.6065624356269836
translation,55,21,model,context,to capture,contextual information,context to capture contextual information,0.6653507947921753
translation,55,21,model,contextual information,regarding,word orders,contextual information regarding word orders,0.5892640352249146
translation,55,21,model,model,encode,context,model encode context,0.737949550151825
translation,55,22,model,hidden states,of,bilstm,hidden states of bilstm,0.5628228187561035
translation,55,22,model,hidden states,employ,multi-layer gcn,hidden states employ multi-layer gcn,0.5325880646705627
translation,55,22,model,bilstm,to initiate,node representations,bilstm to initiate node representations,0.6218497157096863
translation,55,22,model,multi-layer gcn,on,dependency graph,multi-layer gcn on dependency graph,0.5227571725845337
translation,55,22,model,model,use,hidden states,model use hidden states,0.6849468350410461
translation,55,24,model,bert4gcn,fuse,grammatical sequential features,bert4gcn fuse grammatical sequential features,0.7118731141090393
translation,55,24,model,grammatical sequential features,with,graph - based representations,grammatical sequential features with graph - based representations,0.5962903499603271
translation,55,24,model,model,has,bert4gcn,model has bert4gcn,0.6771584749221802
translation,55,25,model,edge,of,dependency graph,edge of dependency graph,0.5540347695350647
translation,55,25,model,edge,of,dependency graph,edge of dependency graph,0.5540347695350647
translation,55,25,model,edge,based on,self-attention weights,edge based on self-attention weights,0.6998171806335449
translation,55,25,model,dependency graph,based on,self-attention weights,dependency graph based on self-attention weights,0.6414437294006348
translation,55,25,model,self-attention weights,in,"transformer encoder ( vaswani et al. , 2017 )","self-attention weights in transformer encoder ( vaswani et al. , 2017 )",0.4651518762111664
translation,55,25,model,"transformer encoder ( vaswani et al. , 2017 )",of,bert,"transformer encoder ( vaswani et al. , 2017 ) of bert",0.5931496024131775
translation,55,25,model,"transformer encoder ( vaswani et al. , 2017 )",to deal with,parsing errors,"transformer encoder ( vaswani et al. , 2017 ) to deal with parsing errors",0.6538543105125427
translation,55,25,model,bert,to deal with,parsing errors,bert to deal with parsing errors,0.7327443957328796
translation,55,25,model,model,prune and add,edge,model prune and add edge,0.7621790766716003
translation,55,26,model,method,incorporates,relative positional embedding,method incorporates relative positional embedding,0.7065366506576538
translation,55,26,model,relative positional embedding,in,node representations,relative positional embedding in node representations,0.5370160341262817
translation,56,136,baselines,bow lr,compute,bag of words ( bow ) l2normed tf -idf vector,bow lr compute bag of words ( bow ) l2normed tf -idf vector,0.7296773195266724
translation,56,136,baselines,bow lr,perform,logistic regression,bow lr perform logistic regression,0.5857755541801453
translation,56,136,baselines,baselines,has,bow lr,baselines has bow lr,0.5344815850257874
translation,56,138,baselines,ir,use,information retrieval inspired classifier,ir use information retrieval inspired classifier,0.47168898582458496
translation,56,138,baselines,information retrieval inspired classifier,takes,label,information retrieval inspired classifier takes label,0.604312539100647
translation,56,138,baselines,label,of,training example,label of training example,0.5639001131057739
translation,56,138,baselines,baselines,has,ir,baselines has ir,0.6097233891487122
translation,56,140,experimental-setup,fasttext classifier,produce,highly competitive performance,fasttext classifier produce highly competitive performance,0.632841944694519
translation,56,140,experimental-setup,experimental setup,use,fasttext classifier,experimental setup use fasttext classifier,0.6268139481544495
translation,56,141,experimental-setup,n-gram size,of,3,n-gram size of 3,0.6501197814941406
translation,56,141,experimental-setup,vector size,of,300,vector size of 300,0.6624405384063721
translation,56,141,experimental-setup,experimental setup,train for,10 epochs,experimental setup train for 10 epochs,0.7043206095695496
translation,56,142,experimental-setup,bert,use,"bert base classifier ( devlin et al. , 2019 )","bert use bert base classifier ( devlin et al. , 2019 )",0.6191583275794983
translation,56,142,experimental-setup,"bert base classifier ( devlin et al. , 2019 )",is,pretrained deep learning model,"bert base classifier ( devlin et al. , 2019 ) is pretrained deep learning model",0.5192031860351562
translation,56,142,experimental-setup,experimental setup,use,"bert base classifier ( devlin et al. , 2019 )","experimental setup use bert base classifier ( devlin et al. , 2019 )",0.6028497815132141
translation,56,142,experimental-setup,experimental setup,has,bert,experimental setup has bert,0.5013285875320435
translation,56,176,experimental-setup,1.4 billion parameter generative version,of,model,1.4 billion parameter generative version of model,0.5601503849029541
translation,56,176,experimental-setup,experimental setup,use,1.4 billion parameter generative version,experimental setup use 1.4 billion parameter generative version,0.5570993423461914
translation,56,343,experimental-setup,grammar,using,custom designed python package,grammar using custom designed python package,0.6844038963317871
translation,56,343,experimental-setup,experimental setup,specify,grammar,experimental setup specify grammar,0.5839909911155701
translation,56,174,experiments,deep learning research,consider,"blender ( roller et al. , 2020 ) model","deep learning research consider blender ( roller et al. , 2020 ) model",0.6783868670463562
translation,56,157,results,bert classifier,has,greatly outperforms,bert classifier has greatly outperforms,0.6378136873245239
translation,56,157,results,greatly outperforms,has,other classifiers,greatly outperforms has other classifiers,0.581954300403595
translation,56,157,results,results,find,bert classifier,results find bert classifier,0.5904000401496887
translation,56,234,results,improve,on,blender,improve on blender,0.5644656419754028
translation,56,234,results,results,has,blender zs,results has blender zs,0.5601155757904053
translation,57,58,baselines,gpt - 2,use,original causal lm ( clm ) objective,gpt - 2 use original causal lm ( clm ) objective,0.6342961192131042
translation,57,58,baselines,baselines,For,gpt - 2,baselines For gpt - 2,0.6106210350990295
translation,57,8,experiments,few-shot setting,for,semeval 2014,few-shot setting for semeval 2014,0.6195738911628723
translation,57,9,experiments,our method,of reformulating,atsc,our method of reformulating atsc,0.6943092346191406
translation,57,9,experiments,atsc,as,nli task,atsc as nli task,0.5204768180847168
translation,57,9,experiments,supervised sota approaches,by,up to 24.13 accuracy points,supervised sota approaches by up to 24.13 accuracy points,0.5890952348709106
translation,57,9,experiments,supervised sota approaches,by,33.14 macro f1 points,supervised sota approaches by 33.14 macro f1 points,0.5623603463172913
translation,57,9,experiments,task 4 laptop domain,has,our method,task 4 laptop domain has our method,0.553636372089386
translation,57,9,experiments,outperforms,has,supervised sota approaches,outperforms has supervised sota approaches,0.6014392375946045
translation,57,68,hyperparameters,bert - base model,pretrained on,mnli dataset,bert - base model pretrained on mnli dataset,0.782687246799469
translation,57,68,hyperparameters,hyperparameters,use,bert - base model,hyperparameters use bert - base model,0.6352326273918152
translation,57,26,model,two atsc models,based on,natural language prompts,two atsc models based on natural language prompts,0.6321611404418945
translation,57,26,model,first method,appends,cloze question prompts,first method appends cloze question prompts,0.523867130279541
translation,57,26,model,first method,predicts,likely,first method predicts likely,0.7591094970703125
translation,57,26,model,cloze question prompts,to,review text,cloze question prompts to review text,0.5740119814872742
translation,57,26,model,likely,to observe,"good , bad , and ok","likely to observe good , bad , and ok",0.6754434108734131
translation,57,26,model,"good , bad , and ok",as,next word,"good , bad , and ok as next word",0.5170080661773682
translation,57,26,model,model,propose,two atsc models,model propose two atsc models,0.661225438117981
translation,57,69,model,softmax layer,on top of,logits,softmax layer on top of logits,0.686877965927124
translation,57,69,model,softmax layer,to normalize,prediction scores,softmax layer to normalize prediction scores,0.6267380118370056
translation,57,69,model,softmax layer,to finetune,models,softmax layer to finetune models,0.6771717071533203
translation,57,69,model,prediction scores,of,three classes,prediction scores of three classes,0.5984625816345215
translation,57,69,model,models,with,cross-entropy loss,models with cross-entropy loss,0.6021761298179626
translation,57,69,model,cross-entropy loss,when,labeled data,cross-entropy loss when labeled data,0.5812923908233643
translation,57,69,model,labeled data,is,available,labeled data is available,0.6030660271644592
translation,57,69,model,model,apply,softmax layer,model apply softmax layer,0.599711000919342
translation,57,33,results,promptbased models,robust to,domain shifts,promptbased models robust to domain shifts,0.7618956565856934
translation,57,33,results,results,has,promptbased models,results has promptbased models,0.4686737358570099
translation,57,88,results,outperform,in,all cases,outperform in all cases,0.5809333920478821
translation,57,88,results,no-prompt baselines,in,all cases,no-prompt baselines in all cases,0.5485486388206482
translation,57,88,results,both target domains,has,our prompt- based bert models,both target domains has our prompt- based bert models,0.5923322439193726
translation,57,88,results,our prompt- based bert models,has,outperform,our prompt- based bert models has outperform,0.586745023727417
translation,57,88,results,outperform,has,no-prompt baselines,outperform has no-prompt baselines,0.6198127865791321
translation,57,88,results,results,for,both target domains,results for both target domains,0.535052478313446
translation,57,89,results,few-shots,achieve,larger performance gains,few-shots achieve larger performance gains,0.6333398222923279
translation,57,89,results,larger performance gains,as,fewer labels,larger performance gains as fewer labels,0.5346285700798035
translation,57,89,results,fewer labels,are,available,fewer labels are available,0.6033520102500916
translation,57,89,results,results,Especially for,few-shots,results Especially for few-shots,0.6653315424919128
translation,57,93,results,our methods,achieve,good performances,our methods achieve good performances,0.5499653220176697
translation,57,93,results,good performances,in,zero-shot cases,good performances in zero-shot cases,0.5420867204666138
translation,57,93,results,baselines,trained on,16 samples,baselines trained on 16 samples,0.7219739556312561
translation,57,93,results,significantly outperforming,has,baselines,significantly outperforming has baselines,0.6089772582054138
translation,57,93,results,results,observe that,our methods,results observe that our methods,0.5723729729652405
translation,57,95,results,prompt models,with,16 - shot cross-domain training,prompt models with 16 - shot cross-domain training,0.609455406665802
translation,57,95,results,prompt models,achieve,better performance,prompt models achieve better performance,0.6625387668609619
translation,57,95,results,better performance,than,in - and out-domain bert nsp,better performance than in - and out-domain bert nsp,0.6110102534294128
translation,57,96,results,have even exceeded in -domain,for,restaurants domain,have even exceeded in -domain for restaurants domain,0.6390429139137268
translation,57,96,results,crossdomain,has,have even exceeded in -domain,crossdomain has have even exceeded in -domain,0.6131864190101624
translation,57,96,results,results,interesting to note,crossdomain,results interesting to note crossdomain,0.5999849438667297
translation,57,99,results,bert lm model,trained with,merely 16 examples,bert lm model trained with merely 16 examples,0.7127271890640259
translation,57,99,results,bert lm model,achieves,about 77 % accuracy,bert lm model achieves about 77 % accuracy,0.7068583965301514
translation,57,99,results,about 77 % accuracy,on,acsc,about 77 % accuracy on acsc,0.6017430424690247
translation,57,99,results,results,see,bert lm model,results see bert lm model,0.5849559307098389
translation,57,100,results,bert nsp,achieves,around 65 %,bert nsp achieves around 65 %,0.7368992567062378
translation,57,100,results,bert nsp,has,no-prompt baseline,bert nsp has no-prompt baseline,0.5970755815505981
translation,57,100,results,results,has,bert nsp,results has bert nsp,0.561840295791626
translation,57,107,results,significant amount of improvements,over,noprompt baselines,significant amount of improvements over noprompt baselines,0.6997228860855103
translation,57,108,results,our nli model,performs,well,our nli model performs well,0.658847451210022
translation,57,108,results,well,with,lower amounts of training data,well with lower amounts of training data,0.6554457545280457
translation,57,108,results,does better,when,more labels,does better when more labels,0.6782758235931396
translation,57,108,results,bert lm model,has,does better,bert lm model has does better,0.6270426511764526
translation,57,108,results,more labels,has,are available,more labels has are available,0.5680786967277527
translation,58,150,ablation-analysis,static span selection layer,with,qass,static span selection layer with qass,0.6776679158210754
translation,58,150,ablation-analysis,performance,on,few-shot question answering,performance on few-shot question answering,0.5166224837303162
translation,58,150,ablation-analysis,significantly improve,has,performance,significantly improve has performance,0.5721634030342102
translation,58,150,ablation-analysis,ablation analysis,replacing,static span selection layer,ablation analysis replacing static span selection layer,0.6129143238067627
translation,58,106,baselines,- base,shares,same architecture,- base shares same architecture,0.7731593251228333
translation,58,106,baselines,- base,shares,vocabulary ( cased wordpieces ),- base shares vocabulary ( cased wordpieces ),0.7612982392311096
translation,58,106,baselines,- base,shares,number of parameters ( 110m ),- base shares number of parameters ( 110m ),0.7598623037338257
translation,58,106,baselines,number of parameters ( 110m ),with,"spanbert - base ( joshi et al. , 2020 )","number of parameters ( 110m ) with spanbert - base ( joshi et al. , 2020 )",0.60069340467453
translation,58,106,baselines,same architecture,has,number of parameters ( 110m ),same architecture has number of parameters ( 110m ),0.5822334289550781
translation,58,106,baselines,baselines,has,baselines,baselines has baselines,0.6036415696144104
translation,58,117,experimental-setup,splinter - base,using,"adam ( kingma and ba , 2015 )","splinter - base using adam ( kingma and ba , 2015 )",0.7213291525840759
translation,58,117,experimental-setup,"adam ( kingma and ba , 2015 )",for,2.4 m training steps,"adam ( kingma and ba , 2015 ) for 2.4 m training steps",0.5616542100906372
translation,58,117,experimental-setup,2.4 m training steps,with,batches,2.4 m training steps with batches,0.6257309317588806
translation,58,117,experimental-setup,batches,of,256 sequences,batches of 256 sequences,0.6380300521850586
translation,58,117,experimental-setup,256 sequences,of length,512,256 sequences of length 512,0.7081301808357239
translation,58,117,experimental-setup,experimental setup,train,splinter - base,experimental setup train splinter - base,0.6530988216400146
translation,58,119,experimental-setup,learning rate,decays,linearly,learning rate decays linearly,0.7321762442588806
translation,58,119,experimental-setup,warmed up,for,10 k steps,warmed up for 10 k steps,0.636900782585144
translation,58,119,experimental-setup,warmed up,decays,linearly,warmed up decays linearly,0.702881395816803
translation,58,119,experimental-setup,10 k steps,to,maximum value,10 k steps to maximum value,0.604191243648529
translation,58,119,experimental-setup,maximum value,of,10 ?4,maximum value of 10 ?4,0.6029761433601379
translation,58,119,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,58,120,experimental-setup,dropout rate,of,0.1,dropout rate of 0.1,0.5755324363708496
translation,58,120,experimental-setup,0.1,across,all layers,0.1 across all layers,0.6927049160003662
translation,58,120,experimental-setup,experimental setup,use,dropout rate,experimental setup use dropout rate,0.6091165542602539
translation,58,122,experimental-setup,implementation,on,official tensorflow implementation of bert,implementation on official tensorflow implementation of bert,0.521758496761322
translation,58,122,experimental-setup,implementation,train on,single eight- core v3 tpu ( v3 - 8 ),implementation train on single eight- core v3 tpu ( v3 - 8 ),0.6877625584602356
translation,58,122,experimental-setup,single eight- core v3 tpu ( v3 - 8 ),on,google cloud platform,single eight- core v3 tpu ( v3 - 8 ) on google cloud platform,0.511795699596405
translation,58,122,experimental-setup,experimental setup,base,implementation,experimental setup base implementation,0.6877991557121277
translation,58,122,experimental-setup,experimental setup,train on,single eight- core v3 tpu ( v3 - 8 ),experimental setup train on single eight- core v3 tpu ( v3 - 8 ),0.6916374564170837
translation,58,125,experimental-setup,all models,using,"adam ( kingma and ba , 2015 )","all models using adam ( kingma and ba , 2015 )",0.6415153741836548
translation,58,125,experimental-setup,"adam ( kingma and ba , 2015 )",with,bias -corrected moment estimates,"adam ( kingma and ba , 2015 ) with bias -corrected moment estimates",0.6144708395004272
translation,58,125,experimental-setup,bias -corrected moment estimates,for,few-shot learning,bias -corrected moment estimates for few-shot learning,0.5942696332931519
translation,58,125,experimental-setup,few-shot learning,has,"zhang et al. , 2021 )","few-shot learning has zhang et al. , 2021 )",0.5466001033782959
translation,58,125,experimental-setup,experimental setup,train,all models,experimental setup train all models,0.6761652231216431
translation,58,126,experimental-setup,finetuning,on,1024 examples or less,finetuning on 1024 examples or less,0.551705539226532
translation,58,126,experimental-setup,finetuning,train for,10 epochs,finetuning train for 10 epochs,0.7306250929832458
translation,58,126,experimental-setup,finetuning,train for,200 steps,finetuning train for 200 steps,0.7630536556243896
translation,58,126,experimental-setup,1024 examples or less,train for,10 epochs,1024 examples or less train for 10 epochs,0.7297203540802002
translation,58,126,experimental-setup,1024 examples or less,train for,200 steps,1024 examples or less train for 200 steps,0.7237122058868408
translation,58,127,experimental-setup,full-size datasets,train for,2 epochs,full-size datasets train for 2 epochs,0.7524440288543701
translation,58,127,experimental-setup,experimental setup,For,full-size datasets,experimental setup For full-size datasets,0.5432171821594238
translation,58,128,experimental-setup,maximal learning rate,of,3 ? 10 ?5,maximal learning rate of 3 ? 10 ?5,0.6381670832633972
translation,58,128,experimental-setup,warms up,in,first 10 % of the steps,warms up in first 10 % of the steps,0.5500665307044983
translation,58,128,experimental-setup,decays,has,linearly,decays has linearly,0.6281104683876038
translation,58,128,experimental-setup,experimental setup,set,batch size,experimental setup set batch size,0.6767397522926331
translation,58,128,experimental-setup,experimental setup,use,maximal learning rate,experimental setup use maximal learning rate,0.608058512210846
translation,58,6,model,scheme,tailored for,question answering,scheme tailored for question answering,0.7565028071403503
translation,58,7,model,passage,with,multiple sets of recurring spans,passage with multiple sets of recurring spans,0.6813309788703918
translation,58,7,model,passage,ask,model,passage ask model,0.6328248381614685
translation,58,7,model,model,to select,correct span,model to select correct span,0.7254993319511414
translation,58,7,model,correct span,in,passage,correct span in passage,0.49743929505348206
translation,58,7,model,correct span,for,masked span,correct span for masked span,0.5554085969924927
translation,58,7,model,model,Given,passage,model Given passage,0.756255030632019
translation,58,7,model,model,to select,correct span,model to select correct span,0.7254993319511414
translation,58,26,model,novel self-supervised method,for,pretraining,novel self-supervised method for pretraining,0.599096953868866
translation,58,26,model,question answering layer,aligns,representation,question answering layer aligns representation,0.6619000434875488
translation,58,26,model,representation,of,question,representation of question,0.6021957993507385
translation,58,26,model,question,with,text,question with text,0.6373610496520996
translation,58,26,model,pretraining,has,span selection models,pretraining has span selection models,0.5266882181167603
translation,58,26,model,model,propose,novel self-supervised method,model propose novel self-supervised method,0.6853455901145935
translation,58,27,model,pretrained model,for,few-shot question answering,pretrained model for few-shot question answering,0.5331458449363708
translation,58,27,model,splinter ( span-level pointer ),has,pretrained model,splinter ( span-level pointer ) has pretrained model,0.5692963004112244
translation,58,27,model,model,introduce,splinter ( span-level pointer ),model introduce splinter ( span-level pointer ),0.6356351971626282
translation,58,31,model,answer span,introduce,question - aware span selection ( qass ) layer,answer span introduce question - aware span selection ( qass ) layer,0.6294601559638977
translation,58,31,model,question - aware span selection ( qass ) layer,uses,[ question ] token 's representation,question - aware span selection ( qass ) layer uses [ question ] token 's representation,0.5585620403289795
translation,58,31,model,question - aware span selection ( qass ) layer,uses,answer span,question - aware span selection ( qass ) layer uses answer span,0.5566347241401672
translation,58,31,model,[ question ] token 's representation,to select,answer span,[ question ] token 's representation to select answer span,0.6464166641235352
translation,58,51,model,pretraining,directly for,span selection,pretraining directly for span selection,0.6073147058486938
translation,58,51,model,question,with,single vector,question with single vector,0.6887335181236267
translation,58,51,model,answer,in,input text,answer in input text,0.501558780670166
translation,58,51,model,model,explicitly representing,question,model explicitly representing question,0.7609472274780273
translation,58,134,results,large datasets,order of,"100,000 examples","large datasets order of 100,000 examples",0.6218640208244324
translation,58,134,results,splinter,is,competitive,splinter is competitive,0.6669633984565735
translation,58,134,results,competitive,with ( and often better than,baselines,competitive with ( and often better than baselines,0.7124661207199097
translation,58,134,results,large datasets,has,splinter,large datasets has splinter,0.5995122790336609
translation,58,134,results,"100,000 examples",has,splinter,"100,000 examples has splinter",0.609504222869873
translation,58,140,results,best result,in,five out of six datasets,best result in five out of six datasets,0.5044956207275391
translation,59,131,baselines,models,from,bert family,models from bert family,0.5916385650634766
translation,59,132,baselines,s2s extensions,with,copy mechanism,s2s extensions with copy mechanism,0.658540666103363
translation,59,132,baselines,s2s extensions,with,topic modeling,s2s extensions with topic modeling,0.6275195479393005
translation,59,132,baselines,s2s extensions,with,bidirectional attentions,s2s extensions with bidirectional attentions,0.6448782086372375
translation,59,132,baselines,topic modeling,from,"posts ( i.e. , topic )","topic modeling from posts ( i.e. , topic )",0.5262452363967896
translation,59,132,baselines,bidirectional attentions,over,"posts and comments ( i.e. , cmt ( biatt )","bidirectional attentions over posts and comments ( i.e. , cmt ( biatt )",0.6224877238273621
translation,59,134,baselines,two variants - cmt ( ntm ),in,single decoder archetecture,two variants - cmt ( ntm ) in single decoder archetecture,0.5632721185684204
translation,59,134,baselines,two variants - cmt ( ntm ),in,dual decoder version dual dec,two variants - cmt ( ntm ) in dual decoder version dual dec,0.5544801354408264
translation,59,6,experiments,poll questions,exhibiting,severe data sparsity,poll questions exhibiting severe data sparsity,0.6421393156051636
translation,59,6,experiments,short and colloquial social media messages,exhibiting,severe data sparsity,short and colloquial social media messages exhibiting severe data sparsity,0.6071957349777222
translation,59,136,hyperparameters,validation set,via,grid search,validation set via grid search,0.7127037644386292
translation,59,136,hyperparameters,hyperparameters,tuned on,validation set,hyperparameters tuned on validation set,0.7172183990478516
translation,59,137,hyperparameters,ntm,pre-trained for,50 epochs,ntm pre-trained for 50 epochs,0.798392117023468
translation,59,137,hyperparameters,50 epochs,before,joint training,50 epochs before joint training,0.6187481880187988
translation,59,137,hyperparameters,hyperparameters,For,ntm,hyperparameters For ntm,0.5909515619277954
translation,59,139,hyperparameters,hidden size,of,each gru,hidden size of each gru,0.6043791770935059
translation,59,139,hyperparameters,each gru,is,300,each gru is 300,0.6525313854217529
translation,59,139,hyperparameters,hyperparameters,has,hidden size,hyperparameters has hidden size,0.535134494304657
translation,59,140,hyperparameters,size,set to,150,size set to 150,0.7763329744338989
translation,59,140,hyperparameters,word embedding,has,size,word embedding has size,0.5583810210227966
translation,59,140,hyperparameters,word embedding,has,randomly initialized,word embedding has randomly initialized,0.5736517906188965
translation,59,140,hyperparameters,hyperparameters,For,word embedding,hyperparameters For word embedding,0.5246535539627075
translation,59,141,hyperparameters,adam optimizer,with,initial learning rate,adam optimizer with initial learning rate,0.5838022828102112
translation,59,141,hyperparameters,adam optimizer,with,gradient clipping,adam optimizer with gradient clipping,0.6027368903160095
translation,59,141,hyperparameters,adam optimizer,with,early - stopping strategy,adam optimizer with early - stopping strategy,0.5977165699005127
translation,59,141,hyperparameters,initial learning rate,as,1e - 3,initial learning rate as 1e - 3,0.5646333694458008
translation,59,141,hyperparameters,gradient clipping,as,1.0,gradient clipping as 1.0,0.5205872654914856
translation,59,7,model,latent topics,therein as,contexts,latent topics therein as contexts,0.6060137748718262
translation,59,7,model,model,encode,user comments,model encode user comments,0.7764278650283813
translation,59,8,model,sequence - to-sequence ( s2s ) architecture,for,question generation,sequence - to-sequence ( s2s ) architecture for question generation,0.6164140701293945
translation,59,8,model,sequence - to-sequence ( s2s ) architecture,for,extension,sequence - to-sequence ( s2s ) architecture for extension,0.6116193532943726
translation,59,8,model,extension,with,dual decoders,extension with dual decoders,0.6698377132415771
translation,59,8,model,dual decoders,to additionally yield,poll choices ( answers ),dual decoders to additionally yield poll choices ( answers ),0.6658048629760742
translation,59,100,model,topic learning methods,inspired by,neural topic model ( ntm ),topic learning methods inspired by neural topic model ( ntm ),0.6593298316001892
translation,59,100,model,topic learning methods,allows,end-to - end training,topic learning methods allows end-to - end training,0.6357259750366211
translation,59,100,model,neural topic model ( ntm ),based on,variational auto-encoder ( vae ),neural topic model ( ntm ) based on variational auto-encoder ( vae ),0.6404255628585815
translation,59,100,model,neural topic model ( ntm ),allows,end-to - end training,neural topic model ( ntm ) allows end-to - end training,0.6451739072799683
translation,59,100,model,end-to - end training,of,ntm,end-to - end training of ntm,0.5909979343414307
translation,59,100,model,ntm,with,other modules,ntm with other modules,0.6403893828392029
translation,59,100,model,other modules,in,unified neural architecture,other modules in unified neural architecture,0.5071539282798767
translation,59,100,model,model,has,topic learning methods,model has topic learning methods,0.5318903923034668
translation,59,101,model,encoder and a decoder,to resemble,data reconstruction process,encoder and a decoder to resemble data reconstruction process,0.6837337017059326
translation,59,101,model,data reconstruction process,of,comment words,data reconstruction process of comment words,0.5844286680221558
translation,59,101,model,comment words,in,bow,comment words in bow,0.5508130192756653
translation,59,101,model,model,employs,encoder and a decoder,model employs encoder and a decoder,0.6024503707885742
translation,59,138,model,two -layers bidirectional gru,to build,source post encoder,two -layers bidirectional gru to build source post encoder,0.6431841850280762
translation,59,138,model,model,adopt,two -layers bidirectional gru,model adopt two -layers bidirectional gru,0.6535282731056213
translation,59,42,results,automatic evaluation results,show that,latent topics,automatic evaluation results show that latent topics,0.4630003571510315
translation,59,42,results,latent topics,learned from,first few pieces,latent topics learned from first few pieces,0.6811236143112183
translation,59,42,results,latent topics,result in,our models,latent topics result in our models,0.5944191813468933
translation,59,42,results,first few pieces,of,user comments,first few pieces of user comments,0.5503495931625366
translation,59,42,results,significantly better performance,than,s2s baselines,significantly better performance than s2s baselines,0.5575525164604187
translation,59,42,results,trendy extensions,proposed for,other tasks,trendy extensions proposed for other tasks,0.7068740129470825
translation,59,42,results,our models,has,significantly better performance,our models has significantly better performance,0.5840263366699219
translation,59,42,results,results,has,automatic evaluation results,results has automatic evaluation results,0.5548956990242004
translation,59,73,results,each post,exhibits,more voters,each post exhibits more voters,0.6729678511619568
translation,59,73,results,more voters,than,comments,more voters than comments,0.6136494278907776
translation,59,73,results,results,observe,each post,results observe each post,0.6059762835502625
translation,59,162,results,dual dec,has,slightly outperforms,dual dec has slightly outperforms,0.6292301416397095
translation,59,162,results,slightly outperforms,has,single decoder variant cmt ( ntm ),slightly outperforms has single decoder variant cmt ( ntm ),0.5773202180862427
translation,59,162,results,results,notice,dual dec,results notice dual dec,0.6648166179656982
translation,59,168,results,latent topics,exhibit,relatively better relevance,latent topics exhibit relatively better relevance,0.6256594061851501
translation,59,170,results,cmt ( ntm ) and dual dec,perform,best,cmt ( ntm ) and dual dec perform best,0.6168842911720276
translation,59,170,results,best,in,engagingness,best in engagingness,0.5340643525123596
translation,59,170,results,results,has,cmt ( ntm ) and dual dec,results has cmt ( ntm ) and dual dec,0.5016657114028931
translation,59,171,results,outperforms,by,small margin,outperforms by small margin,0.6244364380836487
translation,59,171,results,our models,by,small margin,our models by small margin,0.630513608455658
translation,59,171,results,fluency,has,base,fluency has base,0.6028376221656799
translation,59,171,results,base,has,outperforms,base has outperforms,0.6537463665008545
translation,59,171,results,outperforms,has,our models,outperforms has our models,0.6038249135017395
translation,59,171,results,results,For,fluency,results For fluency,0.5258268713951111
translation,59,178,results,post length,seems not to,affect much,post length seems not to affect much,0.6909832954406738
translation,59,178,results,affect much,on,models ' performance,affect much on models ' performance,0.5759289860725403
translation,59,178,results,results,has,post length,results has post length,0.5223332047462463
translation,59,179,results,two s2s baselines,exhibit,obvious performance drops,two s2s baselines exhibit obvious performance drops,0.6561513543128967
translation,59,179,results,obvious performance drops,when generating,long questions,obvious performance drops when generating long questions,0.6871823072433472
translation,59,179,results,topic and cmt ( ntm ),perform,steadily,topic and cmt ( ntm ) perform steadily,0.6794863343238831
translation,59,179,results,question length,has,two s2s baselines,question length has two s2s baselines,0.5740939974784851
translation,59,179,results,results,for,question length,results for question length,0.558920681476593
translation,59,181,results,cmt ( ntm ),presents,consistently better rouge - 1,cmt ( ntm ) presents consistently better rouge - 1,0.6613996028900146
translation,59,181,results,consistently better rouge - 1,in,diverse scenarios,consistently better rouge - 1 in diverse scenarios,0.5297523736953735
translation,59,181,results,results,has,cmt ( ntm ),results has cmt ( ntm ),0.5292131304740906
translation,59,193,results,dual decoders,boost,results,dual decoders boost results,0.6935290098190308
translation,59,193,results,results,of,base and copy,results of base and copy,0.6076856851577759
translation,59,198,results,source posts,are,helpful,source posts are helpful,0.5824152827262878
translation,59,198,results,helpful,in,answer generation,helpful in answer generation,0.48191219568252563
translation,59,198,results,helpful,results in,outperformance,helpful results in outperformance,0.6107993125915527
translation,59,198,results,outperformance,of,pt + qs over qs,outperformance of pt + qs over qs,0.6264333128929138
translation,59,201,results,cmt ( ntm ),performs,best,cmt ( ntm ) performs best,0.6128937005996704
translation,59,201,results,dual decoders,has,cmt ( ntm ),dual decoders has cmt ( ntm ),0.6035519242286682
translation,59,201,results,results,For,dual decoders,results For dual decoders,0.5989077091217041
translation,59,202,results,scores from cmt ( ntm ),are,competitive,scores from cmt ( ntm ) are competitive,0.578710675239563
translation,59,202,results,best pipeline model ( pt + qs ),has,scores from cmt ( ntm ),best pipeline model ( pt + qs ) has scores from cmt ( ntm ),0.5654314756393433
translation,59,202,results,results,In comparison with,best pipeline model ( pt + qs ),results In comparison with best pipeline model ( pt + qs ),0.6581047177314758
translation,60,131,ablation-analysis,albert,helps achieve,best result,albert helps achieve best result,0.6894986629486084
translation,60,131,ablation-analysis,best result,among,four plms,best result among four plms,0.619240403175354
translation,60,131,ablation-analysis,ablation analysis,shows,albert,ablation analysis shows albert,0.5740904211997986
translation,60,140,ablation-analysis,number of encoder layers,exceeds,3,number of encoder layers exceeds 3,0.6473464369773865
translation,60,140,ablation-analysis,performance,shows,continuous decreasing trend,performance shows continuous decreasing trend,0.6757770776748657
translation,60,140,ablation-analysis,continuous decreasing trend,except,16 rest,continuous decreasing trend except 16 rest,0.764390766620636
translation,60,140,ablation-analysis,16 rest,when,number of encoder layers,16 rest when number of encoder layers,0.6174058318138123
translation,60,140,ablation-analysis,number of encoder layers,increased to,7,number of encoder layers increased to 7,0.6869590878486633
translation,60,140,ablation-analysis,increases,by,nearly 2.5 absolute f 1 score,increases by nearly 2.5 absolute f 1 score,0.6393924355506897
translation,60,140,ablation-analysis,number of encoder layers,has,performance,number of encoder layers has performance,0.5806815028190613
translation,60,140,ablation-analysis,3,has,performance,3 has performance,0.5494488477706909
translation,60,140,ablation-analysis,16 rest,has,performance,16 rest has performance,0.5851522088050842
translation,60,140,ablation-analysis,performance,has,increases,performance has increases,0.5947421193122864
translation,60,140,ablation-analysis,ablation analysis,when,number of encoder layers,ablation analysis when number of encoder layers,0.6506556272506714
translation,60,109,baselines,rinante +,has,"peng et al. , 2020","rinante + has peng et al. , 2020",0.5990360379219055
translation,60,109,baselines,cmla +,has,"peng et al. , 2020 )","cmla + has peng et al. , 2020 )",0.6089118719100952
translation,60,109,baselines,li-unified -r,has,"peng et al. , 2020 )","li-unified -r has peng et al. , 2020 )",0.543563723564148
translation,60,109,baselines,ote-mtl,has,"zhang et al. , 2020a","ote-mtl has zhang et al. , 2020a",0.6091349124908447
translation,60,7,model,joint absa model,focuses on,difference,joint absa model focuses on difference,0.7471405267715454
translation,60,7,model,benefits,of,encoder sharing,benefits of encoder sharing,0.5853569507598877
translation,60,7,model,difference,to improve,effectiveness,difference to improve effectiveness,0.6964927911758423
translation,60,7,model,model,propose,joint absa model,model propose joint absa model,0.6526687145233154
translation,60,8,model,dual-encoder design,in which,pair encoder,dual-encoder design in which pair encoder,0.6268938779830933
translation,60,8,model,pair encoder,focuses on,candidate aspect-opinion pair classification,pair encoder focuses on candidate aspect-opinion pair classification,0.6995263695716858
translation,60,8,model,original encoder,keeps attention on,sequence labeling,original encoder keeps attention on sequence labeling,0.6974812746047974
translation,60,8,model,model,introduce,dual-encoder design,model introduce dual-encoder design,0.6448301076889038
translation,60,32,model,dual-encoder model,based on,pretrained language model,dual-encoder model based on pretrained language model,0.6066051721572876
translation,60,32,model,multiple encoders,on,absa task,multiple encoders on absa task,0.5783410668373108
translation,60,32,model,model,propose,dual-encoder model,model propose dual-encoder model,0.6623518466949463
translation,60,33,model,shared sequence encoder,to represent,aspect terms and opinion terms,shared sequence encoder to represent aspect terms and opinion terms,0.6369184255599976
translation,60,33,model,aspect terms and opinion terms,in,same embedding space,aspect terms and opinion terms in same embedding space,0.5003525614738464
translation,60,34,model,pair encoder,to represent,aspectopinion pair,pair encoder to represent aspectopinion pair,0.6634731292724609
translation,60,34,model,aspectopinion pair,on,span level,aspectopinion pair on span level,0.5438784956932068
translation,60,34,model,model,introduce,pair encoder,model introduce pair encoder,0.7017258405685425
translation,60,107,results,results,on,aste task,results on aste task,0.5079299807548523
translation,60,110,results,our bertbased dual-encoder model,achieves,improvement,our bertbased dual-encoder model achieves improvement,0.6626638174057007
translation,60,110,results,improvement,by,"1.39 , 0.53 , 0.68 , and 2.92 absolute f 1 score","improvement by 1.39 , 0.53 , 0.68 , and 2.92 absolute f 1 score",0.5568260550498962
translation,60,110,results,"1.39 , 0.53 , 0.68 , and 2.92 absolute f 1 score",on,benchmark datasets,"1.39 , 0.53 , 0.68 , and 2.92 absolute f 1 score on benchmark datasets",0.476910799741745
translation,60,110,results,"best baseline model ( huang et al. , 2021 )",has,our bertbased dual-encoder model,"best baseline model ( huang et al. , 2021 ) has our bertbased dual-encoder model",0.5034582614898682
translation,60,110,results,results,Compared with,"best baseline model ( huang et al. , 2021 )","results Compared with best baseline model ( huang et al. , 2021 )",0.6177413463592529
translation,60,111,results,our dual-encoder model,capable of capturing,difference,our dual-encoder model capable of capturing difference,0.7243567705154419
translation,60,111,results,difference,between,at / ot extraction subtask and sc subtask,difference between at / ot extraction subtask and sc subtask,0.6501572728157043
translation,60,111,results,at / ot extraction subtask and sc subtask,with the help of,additional pair encoder,at / ot extraction subtask and sc subtask with the help of additional pair encoder,0.6715391278266907
translation,60,111,results,results,signifies that,our dual-encoder model,results signifies that our dual-encoder model,0.6435863375663757
translation,60,112,results,all the other competitive methods,on,most metrics,all the other competitive methods on most metrics,0.4642323851585388
translation,60,112,results,all the other competitive methods,except for,precision score,all the other competitive methods except for precision score,0.5820450782775879
translation,60,112,results,most metrics,of,"4 datasets 14 rest , 14 lap , 15 rest and 16 rest","most metrics of 4 datasets 14 rest , 14 lap , 15 rest and 16 rest",0.5214417576789856
translation,60,112,results,albert - based model,has,significantly outperforms,albert - based model has significantly outperforms,0.6131628751754761
translation,60,112,results,significantly outperforms,has,all the other competitive methods,significantly outperforms has all the other competitive methods,0.5785022974014282
translation,60,112,results,results,has,albert - based model,results has albert - based model,0.5318671464920044
translation,60,113,results,albert - based model,achieves,improvement,albert - based model achieves improvement,0.6460460424423218
translation,60,113,results,improvement,of,"6.66 , 4.72 , 9.08 , and 4.49 absolute f 1 score","improvement of 6.66 , 4.72 , 9.08 , and 4.49 absolute f 1 score",0.5608571767807007
translation,60,113,results,"6.66 , 4.72 , 9.08 , and 4.49 absolute f 1 score",over,all the baseline models,"6.66 , 4.72 , 9.08 , and 4.49 absolute f 1 score over all the baseline models",0.6179499626159668
translation,60,113,results,all the baseline models,on,four benchmark datasets,all the baseline models on four benchmark datasets,0.4489745795726776
translation,60,113,results,results,has,albert - based model,results has albert - based model,0.5318671464920044
translation,60,120,results,aesc task,For,aesc task,aesc task For aesc task,0.6281108856201172
translation,60,120,results,results,on,aesc task,results on aesc task,0.5013177394866943
translation,60,125,results,performance,of,our dual-encoder model,performance of our dual-encoder model,0.5757023692131042
translation,60,125,results,our dual-encoder model,is,comparable,our dual-encoder model is comparable,0.5772272944450378
translation,60,125,results,comparable,on,aesc task,comparable on aesc task,0.5702894926071167
translation,60,125,results,aesc task,than,single - encoder structure,aesc task than single - encoder structure,0.5581415295600891
translation,60,125,results,results,see that,performance,results see that performance,0.6803712844848633
translation,60,134,results,absolute f 1 score,between,"bert and roberta , albert","absolute f 1 score between bert and roberta , albert",0.6356632709503174
translation,60,134,results,"bert and roberta , albert",is,1.04 and 4.19,"bert and roberta , albert is 1.04 and 4.19",0.5914108753204346
translation,60,134,results,results,has,absolute f 1 score,results has absolute f 1 score,0.5565140247344971
translation,60,144,results,quaddirectional setting,has,significantly outperforms,quaddirectional setting has significantly outperforms,0.6145737171173096
translation,60,144,results,significantly outperforms,has,other two settings,significantly outperforms has other two settings,0.551098108291626
translation,60,144,results,results,observe,quaddirectional setting,results observe quaddirectional setting,0.6090797185897827
translation,61,42,ablation-analysis,quality,of,predicted qa pairs,quality of predicted qa pairs,0.6374281048774719
translation,61,42,ablation-analysis,conditional probability,of,answers,conditional probability of answers,0.5934829115867615
translation,61,42,ablation-analysis,conditional probability,estimated by,answer-generation model,conditional probability estimated by answer-generation model,0.7191140651702881
translation,61,42,ablation-analysis,ablation analysis,refine,quality,ablation analysis refine quality,0.6236995458602905
translation,61,160,ablation-analysis,dpr reader and spanseqgen,generates,11.7 % and 12.3 % more qa pairs,dpr reader and spanseqgen generates 11.7 % and 12.3 % more qa pairs,0.6289284229278564
translation,61,160,ablation-analysis,11.7 % and 12.3 % more qa pairs,result in,boost,11.7 % and 12.3 % more qa pairs result in boost,0.6594592332839966
translation,61,160,ablation-analysis,boost,of,3.7 % and 2.1 %,boost of 3.7 % and 2.1 %,0.6116548776626587
translation,61,160,ablation-analysis,3.7 % and 2.1 %,for,overall performance,3.7 % and 2.1 % for overall performance,0.6214551329612732
translation,61,160,ablation-analysis,round-trip prediction,has,dpr reader and spanseqgen,round-trip prediction has dpr reader and spanseqgen,0.5957174301147461
translation,61,160,ablation-analysis,ablation analysis,With the help of,round-trip prediction,ablation analysis With the help of round-trip prediction,0.6904336810112
translation,61,175,ablation-analysis,question generation pre-training,has,little help,question generation pre-training has little help,0.5628098249435425
translation,61,175,ablation-analysis,little help,for,fine-tuning,little help for fine-tuning,0.6439505815505981
translation,61,175,ablation-analysis,question generation pre-training,has,little help,question generation pre-training has little help,0.5628098249435425
translation,61,175,ablation-analysis,ablation analysis,show,question generation pre-training,ablation analysis show question generation pre-training,0.6222884058952332
translation,61,174,baselines,question generation pre-training ( qgp + qdf ),compare it with,ablation,question generation pre-training ( qgp + qdf ) compare it with ablation,0.666370689868927
translation,61,174,baselines,ablation,has,without any pre-training ( none + qdf ),ablation has without any pre-training ( none + qdf ),0.5732716917991638
translation,61,44,experiments,inference,on,nq - open and triviaqa,inference on nq - open and triviaqa,0.5913181900978088
translation,61,44,experiments,refuel,predicts,single answer,refuel predicts single answer,0.7744704484939575
translation,61,44,experiments,refuel,finds,multiple interpretations,refuel finds multiple interpretations,0.617396354675293
translation,61,44,experiments,multiple interpretations,if,question,multiple interpretations if question,0.6578055620193481
translation,61,44,experiments,question,is,ambiguous,question is ambiguous,0.6169874668121338
translation,61,44,experiments,inference,has,refuel,inference has refuel,0.5641557574272156
translation,61,44,experiments,nq - open and triviaqa,has,refuel,nq - open and triviaqa has refuel,0.6426841020584106
translation,61,62,experiments,dense passage retriever ( dpr ),for,retrieval,dense passage retriever ( dpr ) for retrieval,0.6256816983222961
translation,61,7,model,model,aggregates and combines,evidence,model aggregates and combines evidence,0.7356469631195068
translation,61,7,model,evidence,from,multiple passages,evidence from multiple passages,0.6346926093101501
translation,61,7,model,evidence,to adaptively predict,single answer,evidence to adaptively predict single answer,0.6489897966384888
translation,61,7,model,evidence,to adaptively predict,set of question - answer pairs,evidence to adaptively predict set of question - answer pairs,0.7579739689826965
translation,61,7,model,set of question - answer pairs,for,ambiguous questions,set of question - answer pairs for ambiguous questions,0.5920921564102173
translation,61,7,model,model,present,model,model present model,0.6571184992790222
translation,61,7,model,model,aggregates and combines,evidence,model aggregates and combines evidence,0.7356469631195068
translation,61,8,model,novel round-trip prediction approach,to iteratively generate,additional interpretations,novel round-trip prediction approach to iteratively generate additional interpretations,0.7448093295097351
translation,61,8,model,additional interpretations,that,our model,additional interpretations that our model,0.6696642637252808
translation,61,8,model,incorrect questionanswer pairs,to arrive at,final disambiguated output,incorrect questionanswer pairs to arrive at final disambiguated output,0.6452596187591553
translation,61,8,model,model,propose,novel round-trip prediction approach,model propose novel round-trip prediction approach,0.6550142168998718
translation,61,35,model,refuel,has,round-trip evidence fusion,refuel has round-trip evidence fusion,0.5982761383056641
translation,61,35,model,model,propose,refuel,model propose refuel,0.7025971412658691
translation,61,40,model,round-trip prediction approach,to find,additional interpretations,round-trip prediction approach to find additional interpretations,0.6270919442176819
translation,61,40,model,additional interpretations,that,re,additional interpretations that re,0.7442933917045593
translation,61,40,model,additional interpretations,-,fuel,additional interpretations - fuel,0.6712957620620728
translation,61,40,model,re,-,fuel,re - fuel,0.6016047596931458
translation,61,40,model,fails to predict,in,first pass,fails to predict in first pass,0.5532948970794678
translation,61,40,model,re,has,fuel,re has fuel,0.6019114255905151
translation,61,40,model,fuel,has,fails to predict,fuel has fails to predict,0.6322492957115173
translation,61,40,model,model,propose,round-trip prediction approach,model propose round-trip prediction approach,0.6765949726104736
translation,61,49,model,model,has,insertion - based weighted loss,model has insertion - based weighted loss,0.5668375492095947
translation,61,192,model,model,has,insertion - based weighted loss,model has insertion - based weighted loss,0.5668375492095947
translation,61,193,model,model- agnostic round-trip prediction approach,to find,more interpretations,model- agnostic round-trip prediction approach to find more interpretations,0.6335012316703796
translation,61,193,model,more interpretations,missed in,first prediction pass,more interpretations missed in first prediction pass,0.713735044002533
translation,61,193,model,model,propose,model- agnostic round-trip prediction approach,model propose model- agnostic round-trip prediction approach,0.6668967008590698
translation,61,10,results,proposed round-trip prediction,is,model- agnostic general approach,proposed round-trip prediction is model- agnostic general approach,0.5532279014587402
translation,61,10,results,model- agnostic general approach,for answering,ambiguous open-domain questions,model- agnostic general approach for answering ambiguous open-domain questions,0.7976258397102356
translation,61,10,results,results,has,proposed round-trip prediction,results has proposed round-trip prediction,0.6023213267326355
translation,61,34,results,spanseqgen,predicts,much smaller average number of answers,spanseqgen predicts much smaller average number of answers,0.6835992932319641
translation,61,34,results,much smaller average number of answers,compared to,ground truth data,much smaller average number of answers compared to ground truth data,0.6161447167396545
translation,61,34,results,ground truth data,has,1.17 vs. 2.19 ),ground truth data has 1.17 vs. 2.19 ),0.5473593473434448
translation,61,34,results,results,has,spanseqgen,results has spanseqgen,0.5405837893486023
translation,61,36,results,broad coverage,of,relevant knowledge,broad coverage of relevant knowledge,0.5686464309692383
translation,61,36,results,relevant knowledge,of,question,relevant knowledge of question,0.5769416093826294
translation,61,36,results,refuel,reads,12 times more passages,refuel reads 12 times more passages,0.7216766476631165
translation,61,36,results,12 times more passages,than,spanseqgen,12 times more passages than spanseqgen,0.6331596970558167
translation,61,36,results,spanseqgen,by using,"fusionin-decoder ( izacard and grave , 2020 )","spanseqgen by using fusionin-decoder ( izacard and grave , 2020 )",0.6917853355407715
translation,61,36,results,"fusionin-decoder ( izacard and grave , 2020 )",that processes,each passage individually,"fusionin-decoder ( izacard and grave , 2020 ) that processes each passage individually",0.7110644578933716
translation,61,36,results,each passage individually,in,encoder,each passage individually in encoder,0.5841619372367859
translation,61,36,results,encodings,in,decoder,encodings in decoder,0.5866895914077759
translation,61,36,results,broad coverage,has,refuel,broad coverage has refuel,0.5912430286407471
translation,61,36,results,relevant knowledge,has,refuel,relevant knowledge has refuel,0.5940853953361511
translation,61,43,results,refuel,achieves,new state - of- the- art,refuel achieves new state - of- the- art,0.6455312371253967
translation,61,43,results,new state - of- the- art,on,ambigqa dataset,new state - of- the- art on ambigqa dataset,0.5034216046333313
translation,61,43,results,previous best model spanseqgen,by,9.1 %,previous best model spanseqgen by 9.1 %,0.5351243615150452
translation,61,43,results,previous best model spanseqgen,by,4.4 %,previous best model spanseqgen by 4.4 %,0.5340527296066284
translation,61,43,results,9.1 %,in,answer prediction f1,9.1 % in answer prediction f1,0.5067572593688965
translation,61,43,results,4.4 %,in,edit - f1 score,4.4 % in edit - f1 score,0.5245218873023987
translation,61,43,results,edit - f1 score,for,question disambiguation,edit - f1 score for question disambiguation,0.5951826572418213
translation,61,43,results,outperforming,has,previous best model spanseqgen,outperforming has previous best model spanseqgen,0.6200791597366333
translation,61,43,results,results,has,refuel,results has refuel,0.5842891931533813
translation,61,45,results,human evaluation,shows,refuel,human evaluation shows refuel,0.637092649936676
translation,61,45,results,refuel,correctly generate,more qa pairs,refuel correctly generate more qa pairs,0.811700165271759
translation,61,45,results,more qa pairs,on,all three datasets,more qa pairs on all three datasets,0.5102630257606506
translation,61,45,results,results,has,human evaluation,results has human evaluation,0.5143810510635376
translation,61,148,results,competitive performance,even without,dataset -specific finetuning,competitive performance even without dataset -specific finetuning,0.722346842288971
translation,61,148,results,refuel,has,competitive performance,refuel has competitive performance,0.5492976307868958
translation,61,148,results,results,shows,refuel,results shows refuel,0.6893754005432129
translation,62,24,ablation-analysis,round-trip translation,at,test time,round-trip translation at test time,0.5310289263725281
translation,62,24,ablation-analysis,round-trip translation,consistently reduces,fairness gap,round-trip translation consistently reduces fairness gap,0.7996982336044312
translation,62,24,ablation-analysis,round-trip translation,for,our best models,round-trip translation for our best models,0.5780915021896362
translation,62,24,ablation-analysis,our best models,both,training and test data,our best models both training and test data,0.6726540923118591
translation,62,24,ablation-analysis,svms,stacked on,bert representations,svms stacked on bert representations,0.7008928656578064
translation,62,24,ablation-analysis,disappears,when,training and test data,disappears when training and test data,0.6543428301811218
translation,62,24,ablation-analysis,disappears,both,training and test data,disappears both training and test data,0.712655246257782
translation,62,24,ablation-analysis,training and test data,translated into,foreign language and back,training and test data translated into foreign language and back,0.7183794975280762
translation,62,24,ablation-analysis,our best models,has,svms,our best models has svms,0.5638766884803772
translation,62,24,ablation-analysis,our best models,has,effect,our best models has effect,0.613236129283905
translation,62,24,ablation-analysis,effect,has,disappears,effect has disappears,0.6273092031478882
translation,62,24,ablation-analysis,ablation analysis,find that,round-trip translation,ablation analysis find that round-trip translation,0.6642451286315918
translation,62,47,ablation-analysis,significant decrease,in,kl - divergence,significant decrease in kl - divergence,0.5088932514190674
translation,62,47,ablation-analysis,kl - divergence,for,all groups,kl - divergence for all groups,0.6074047684669495
translation,62,47,ablation-analysis,all groups,after,round -trip translating,all groups after round -trip translating,0.7303268313407898
translation,62,49,ablation-analysis,number of unique words,dropped by,36 %,number of unique words dropped by 36 %,0.6814844012260437
translation,62,49,ablation-analysis,36 %,after,round -trip translation,36 % after round -trip translation,0.6829888224601746
translation,62,49,ablation-analysis,ablation analysis,see that,number of unique words,ablation analysis see that number of unique words,0.6057654619216919
translation,62,58,ablation-analysis,test time normalization,with,round trip translation,test time normalization with round trip translation,0.6487547159194946
translation,62,58,ablation-analysis,overall positive effect,on,cross-group generalization,overall positive effect on cross-group generalization,0.511343240737915
translation,62,58,ablation-analysis,fairness gap,with,up to ? 27 %,fairness gap with up to ? 27 %,0.6602929830551147
translation,62,58,ablation-analysis,test time normalization,has,overall positive effect,test time normalization has overall positive effect,0.5662668347358704
translation,62,58,ablation-analysis,round trip translation,has,overall positive effect,round trip translation has overall positive effect,0.5779203772544861
translation,62,58,ablation-analysis,ablation analysis,has,test time normalization,ablation analysis has test time normalization,0.528400182723999
translation,62,61,ablation-analysis,translating the data,reduces,overall accuracy,translating the data reduces overall accuracy,0.7089628577232361
translation,62,61,ablation-analysis,overall accuracy,of,our document classifiers,overall accuracy of our document classifiers,0.5305272340774536
translation,62,61,ablation-analysis,process of round-trip,has,translating the data,process of round-trip has translating the data,0.5429555177688599
translation,62,66,baselines,round -trip translation,to reduce,group disparity,round -trip translation to reduce group disparity,0.7106284499168396
translation,62,66,baselines,group disparity,of,sentiment classifiers,group disparity of sentiment classifiers,0.6175428032875061
translation,62,66,baselines,sentiment classifiers,for,danish,sentiment classifiers for danish,0.6394626498222351
translation,62,52,hyperparameters,two different pretrained language models,namely,"multilingual laser model ( artetxe and schwenk , 2019 )","two different pretrained language models namely multilingual laser model ( artetxe and schwenk , 2019 )",0.6330909729003906
translation,62,52,hyperparameters,two different pretrained language models,namely,"monolingual bert ( devlin et al. , 2019 )","two different pretrained language models namely monolingual bert ( devlin et al. , 2019 )",0.6247427463531494
translation,62,52,hyperparameters,"monolingual bert ( devlin et al. , 2019 )",trained for,danish,"monolingual bert ( devlin et al. , 2019 ) trained for danish",0.7341945767402649
translation,62,52,hyperparameters,hyperparameters,use,two different pretrained language models,hyperparameters use two different pretrained language models,0.5140702724456787
translation,62,52,hyperparameters,hyperparameters,trained for,danish,hyperparameters trained for danish,0.7159761786460876
translation,62,6,results,impact,of,round-trip translation,impact of round-trip translation,0.6108768582344055
translation,62,6,results,round-trip translation,on,demographic parity,round-trip translation on demographic parity,0.5237032771110535
translation,62,6,results,round-trip translation,show,round -trip translation,round-trip translation show round -trip translation,0.6608357429504395
translation,62,6,results,demographic parity,of,sentiment classifiers,demographic parity of sentiment classifiers,0.5852938890457153
translation,62,6,results,classification fairness,at,test time,classification fairness at test time,0.5175735354423523
translation,62,6,results,round -trip translation,has,consistently improves,round -trip translation has consistently improves,0.6124139428138733
translation,62,6,results,consistently improves,has,classification fairness,consistently improves has classification fairness,0.5590733885765076
translation,62,6,results,test time,has,reducing,test time has reducing,0.5751363039016724
translation,62,6,results,reducing,has,up to 47 %,reducing has up to 47 %,0.6123502254486084
translation,62,6,results,results,explore,impact,results explore impact,0.6217379570007324
translation,62,59,results,increases,up to,39 %,increases up to 39 %,0.6934587359428406
translation,62,59,results,increases,up to,47 %,increases up to 47 %,0.6752683520317078
translation,62,59,results,decreases,up to,47 %,decreases up to 47 %,0.6795217394828796
translation,62,77,results,2/4 classifiers,saw,improvements,2/4 classifiers saw improvements,0.6917025446891785
translation,62,77,results,improvements,for,majority groups,improvements for majority groups,0.615971028804779
translation,62,77,results,results,For,2/4 classifiers,results For 2/4 classifiers,0.5625653266906738
translation,63,61,experiments,cross-attention qa models,fine-tuned from,public english bert,cross-attention qa models fine-tuned from public english bert,0.7254841923713684
translation,63,61,experiments,cross-attention qa models,using,weighted adam optimizer,cross-attention qa models using weighted adam optimizer,0.6375705003738403
translation,63,61,experiments,public english bert,for,10 epochs,public english bert for 10 epochs,0.6491859555244446
translation,63,61,experiments,batch size,of,256,batch size of 256,0.6323750615119934
translation,63,61,experiments,weighted adam optimizer,with,learning rate,weighted adam optimizer with learning rate,0.6175131797790527
translation,63,61,experiments,learning rate,has,3e - 5,learning rate has 3e - 5,0.5670245885848999
translation,63,67,hyperparameters,training,use,batch size,training use batch size,0.657907247543335
translation,63,67,hyperparameters,training,use,weighted adam optimizer,training use weighted adam optimizer,0.6315088868141174
translation,63,67,hyperparameters,batch size,of,64,batch size of 64,0.6741159558296204
translation,63,67,hyperparameters,weighted adam optimizer,with,learning rate 1e - 4,weighted adam optimizer with learning rate 1e - 4,0.62357097864151
translation,63,67,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
translation,63,68,hyperparameters,maximum input length,set to,96,maximum input length set to 96,0.6920250058174133
translation,63,68,hyperparameters,maximum input length,set to,384,maximum input length set to 384,0.7113888263702393
translation,63,68,hyperparameters,96,for,questions,96 for questions,0.6713710427284241
translation,63,68,hyperparameters,384,for,answers,384 for answers,0.7268067002296448
translation,63,68,hyperparameters,hyperparameters,has,maximum input length,hyperparameters has maximum input length,0.4779053330421448
translation,63,69,hyperparameters,models,trained for,200 epochs,models trained for 200 epochs,0.7579374313354492
translation,63,69,hyperparameters,hyperparameters,trained for,200 epochs,hyperparameters trained for 200 epochs,0.6665350794792175
translation,63,69,hyperparameters,hyperparameters,has,models,hyperparameters has models,0.5447477102279663
translation,63,70,hyperparameters,embeddings,are,l 2 normalized,embeddings are l 2 normalized,0.6293474435806274
translation,63,70,hyperparameters,hyperparameters,has,embeddings,hyperparameters has embeddings,0.548468291759491
translation,63,5,model,supervised data mining method,using,accurate early fusion model,supervised data mining method using accurate early fusion model,0.6606526374816895
translation,63,5,model,accurate early fusion model,to improve,training,accurate early fusion model to improve training,0.688093900680542
translation,63,5,model,training,of,efficient late fusion retrieval model,training of efficient late fusion retrieval model,0.5658183097839355
translation,63,5,model,model,present,supervised data mining method,model present supervised data mining method,0.6408337950706482
translation,63,6,model,accurate classification model,with,cross-attention,accurate classification model with cross-attention,0.6182119846343994
translation,63,6,model,cross-attention,between,questions and answers,cross-attention between questions and answers,0.6182489395141602
translation,63,6,model,model,train,accurate classification model,model train accurate classification model,0.7048038840293884
translation,63,7,model,cross-attention model,to,annotate,cross-attention model to annotate,0.5331830978393555
translation,63,7,model,additional passages,to generate,weighted training examples,additional passages to generate weighted training examples,0.6406181454658508
translation,63,7,model,weighted training examples,for,neural retrieval model,weighted training examples for neural retrieval model,0.5747404098510742
translation,63,7,model,annotate,has,additional passages,annotate has additional passages,0.5968013405799866
translation,63,7,model,model,has,cross-attention model,model has cross-attention model,0.5490698218345642
translation,63,66,model,base model,to initialize,dual encoder retrieval model,base model to initialize dual encoder retrieval model,0.6942208409309387
translation,63,66,model,model,has,base model,model has base model,0.5844469666481018
translation,63,76,results,performance,of,crossattention models,performance of crossattention models,0.5465417504310608
translation,63,76,results,crossattention models,compared to,majority baseline,crossattention models compared to majority baseline,0.5971679091453552
translation,63,76,results,majority baseline,always predict,false,majority baseline always predict false,0.8301149010658264
translation,63,76,results,bert dual encoder retrieval model,without,mined examples,bert dual encoder retrieval model without mined examples,0.6874061822891235
translation,63,76,results,mined examples,uses,cosine similarity,mined examples uses cosine similarity,0.5547220706939697
translation,63,76,results,cosine similarity,for,prediction,cosine similarity for prediction,0.6308333277702332
translation,63,77,results,outperform,by,wide margin,outperform by wide margin,0.6461024284362793
translation,63,77,results,baselines,by,wide margin,baselines by wide margin,0.6267645359039307
translation,63,77,results,bert large,achieving,highest performance,bert large achieving highest performance,0.7392566800117493
translation,63,77,results,crossattention based models,has,outperform,crossattention based models has outperform,0.5916465520858765
translation,63,77,results,outperform,has,baselines,outperform has baselines,0.6363358497619629
translation,63,77,results,results,has,crossattention based models,results has crossattention based models,0.49539145827293396
translation,63,78,results,early fusion models,has,outperform,early fusion models has outperform,0.6089031100273132
translation,63,78,results,outperform,has,late fusion based retrieval models,outperform has late fusion based retrieval models,0.5834226012229919
translation,63,79,results,both models,achieve,better performance,both models achieve better performance,0.6460734605789185
translation,63,79,results,better performance,on,squad,better performance on squad,0.5810554027557373
translation,63,79,results,squad,than,nq,squad than nq,0.6865023374557495
translation,63,79,results,results,has,both models,results has both models,0.5060139894485474
translation,63,88,results,mrr@100,for,retrieval models,mrr@100 for retrieval models,0.6057879328727722
translation,63,88,results,retrieval models,on,multireqa - squad and multireqa -nq,retrieval models on multireqa - squad and multireqa -nq,0.5733001828193665
translation,63,89,results,use - qa finetune,reported by,guo et al . ( 2020 ),use - qa finetune reported by guo et al . ( 2020 ),0.7449477314949036
translation,63,90,results,bm25 's performance,on,nq,bm25 's performance on nq,0.5874362587928772
translation,63,90,results,bm25 's performance,is,much lower,bm25 's performance is much lower,0.5844275951385498
translation,63,90,results,nq,is,much lower,nq is much lower,0.6700074076652527
translation,63,90,results,results,has,bm25 's performance,results has bm25 's performance,0.5613864660263062
translation,63,91,results,use - qa,matches,performance,use - qa matches performance,0.792969286441803
translation,63,91,results,use - qa,performs,worse,use - qa performs worse,0.7259852886199951
translation,63,91,results,performance,of,bm25,performance of bm25,0.6073455214500427
translation,63,91,results,bm25,on,nq,bm25 on nq,0.6531642079353333
translation,63,91,results,worse,on,squad,worse on squad,0.611162543296814
translation,63,91,results,results,has,use - qa,results has use - qa,0.5923899412155151
translation,63,92,results,bert dual encoder,performs,well,bert dual encoder performs well,0.6892038583755493
translation,63,92,results,well,compared to,other baselines,well compared to other baselines,0.6978181600570679
translation,63,92,results,well,especially on,nq,well especially on nq,0.6866394877433777
translation,63,92,results,nq,with,+ 6.6 point improvement,nq with + 6.6 point improvement,0.6544115543365479
translation,63,92,results,+ 6.6 point improvement,compared to,use - qa finetune model,+ 6.6 point improvement compared to use - qa finetune model,0.6681963205337524
translation,63,92,results,results,has,bert dual encoder,results has bert dual encoder,0.5572889447212219
translation,63,93,results,p@1,on,squad,p@1 on squad,0.6419955492019653
translation,63,93,results,squad,performs,better,squad performs better,0.6520866751670837
translation,63,93,results,better,than,use - qa and bm25,better than use - qa and bm25,0.6401448249816895
translation,63,93,results,- 3.1 points mrr,worse than,usq - qa finetune,- 3.1 points mrr worse than usq - qa finetune,0.7137030363082886
translation,63,93,results,results,has,p@1,results has p@1,0.6139730215072632
translation,63,94,results,bert dual encoder,is,best,bert dual encoder is best,0.6267117261886597
translation,63,94,results,best,among,baselines,best among baselines,0.6590710878372192
translation,63,95,results,performance,improves,large margin,performance improves large margin,0.776404619216919
translation,63,95,results,large margin,using,augmented training data,large margin using augmented training data,0.5927157402038574
translation,63,95,results,augmented training data,from,our cross-attention qa model,augmented training data from our cross-attention qa model,0.5487949252128601
translation,63,95,results,+ 8.6 and + 7.0 improvement,on,nq p@1 and mrr,+ 8.6 and + 7.0 improvement on nq p@1 and mrr,0.5797162652015686
translation,63,95,results,results,has,performance,results has performance,0.5972660779953003
translation,63,96,results,improvement,on,squad,improvement on squad,0.6000231504440308
translation,63,96,results,squad,is,rather marginal,squad is rather marginal,0.6490762829780579
translation,63,96,results,nq,has,improvement,nq has improvement,0.584182620048523
translation,63,96,results,results,Compare to,nq,results Compare to nq,0.45073220133781433
translation,63,97,results,augmented bert dual encoder retrieval model,achieves,slightly improved performance,augmented bert dual encoder retrieval model achieves slightly improved performance,0.6910350322723389
translation,63,97,results,slightly improved performance,on,squad,slightly improved performance on squad,0.5840350389480591
translation,63,97,results,squad,with,+ 1 points,squad with + 1 points,0.7076283693313599
translation,63,97,results,+ 1 points,for,p@1 and mrr,+ 1 points for p@1 and mrr,0.7017412781715393
translation,63,97,results,results,has,augmented bert dual encoder retrieval model,results has augmented bert dual encoder retrieval model,0.5959118008613586
translation,63,103,results,mrr,of,model,mrr of model,0.6098626255989075
translation,63,103,results,mrr,of,model,mrr of model,0.6098626255989075
translation,63,103,results,mrr,using,nonmodified softmax,mrr using nonmodified softmax,0.6743731498718262
translation,63,103,results,model,using,nonmodified softmax,model using nonmodified softmax,0.6962698101997375
translation,63,103,results,model,using,weighted softmax,model using weighted softmax,0.6921696662902832
translation,63,103,results,model,using,weighted softmax,model using weighted softmax,0.6921696662902832
translation,63,103,results,nonmodified softmax,is,60.1,nonmodified softmax is 60.1,0.5418457388877869
translation,63,103,results,nonmodified softmax,is,71.9,nonmodified softmax is 71.9,0.5491511225700378
translation,63,103,results,60.1,on,multireqa - nq,60.1 on multireqa - nq,0.5601564645767212
translation,63,103,results,71.9,on,multireqa - squad,71.9 on multireqa - squad,0.562444269657135
translation,63,103,results,much worse,than,model,much worse than model,0.6187683939933777
translation,63,103,results,model,using,weighted softmax,model using weighted softmax,0.6921696662902832
translation,63,103,results,results,has,mrr,results has mrr,0.5548900961875916
translation,64,171,ablation-analysis,both parts ( dps and mld ),helpful to,rise,both parts ( dps and mld ) helpful to rise,0.6777124404907227
translation,64,171,ablation-analysis,removing,leads to,decrease,removing leads to decrease,0.7508302330970764
translation,64,171,ablation-analysis,decrease,in,performance,decrease in performance,0.5227082371711731
translation,64,171,ablation-analysis,removing,has,either of them,removing has either of them,0.626396119594574
translation,64,171,ablation-analysis,ablation analysis,show,both parts ( dps and mld ),ablation analysis show both parts ( dps and mld ),0.6559932231903076
translation,64,172,ablation-analysis,drops a lot,in terms of,all metrics,drops a lot in terms of all metrics,0.6430890560150146
translation,64,172,ablation-analysis,- 4,on,canard and cast,- 4 on canard and cast,0.6428140997886658
translation,64,172,ablation-analysis,mld,has,performance,mld has performance,0.5637351870536804
translation,64,172,ablation-analysis,performance,has,drops a lot,performance has drops a lot,0.59557044506073
translation,64,172,ablation-analysis,all metrics,has,3 % and 7 %,all metrics has 3 % and 7 %,0.598300039768219
translation,64,172,ablation-analysis,ablation analysis,Without,mld,ablation analysis Without mld,0.7291284799575806
translation,64,175,ablation-analysis,ablation analysis,Without,dps,ablation analysis Without dps,0.7216194868087769
translation,64,175,ablation-analysis,ablation analysis,results,drop dramatically,ablation analysis results drop dramatically,0.5040406584739685
translation,64,134,baselines,two simple rules,to mimic,two conversational characteristics,two simple rules to mimic two conversational characteristics,0.6429271101951599
translation,64,136,baselines,trans ++,predicts,several word distributions,trans ++ predicts several word distributions,0.717751145362854
translation,64,136,baselines,final word distribution,when generating,each token,final word distribution when generating each token,0.6725849509239197
translation,64,136,baselines,baselines,has,trans ++,baselines has trans ++,0.6032789945602417
translation,64,137,baselines,"gpt - 2 ( radford et al. , 2019 ) model",to generate,conversational question,"gpt - 2 ( radford et al. , 2019 ) model to generate conversational question",0.6883366107940674
translation,64,7,experiments,rise,pay,attention,rise pay attention,0.8039364218711853
translation,64,7,experiments,attention,to,tokens,attention to tokens,0.5975354313850403
translation,64,7,experiments,tokens,related to,conversational characteristics,tokens related to conversational characteristics,0.6708871722221375
translation,64,144,hyperparameters,hidden size,is,768,hidden size is 768,0.6329836249351501
translation,64,144,hyperparameters,phrase vocabulary,is,3461,phrase vocabulary is 3461,0.5838938355445862
translation,64,144,hyperparameters,hyperparameters,has,hidden size,hyperparameters has hidden size,0.535134494304657
translation,64,144,hyperparameters,hyperparameters,has,phrase vocabulary,hyperparameters has phrase vocabulary,0.49967190623283386
translation,64,145,hyperparameters,"bert vocabulary ( 30,522 tokens )",for,all bert - based or bert2bert - based models,"bert vocabulary ( 30,522 tokens ) for all bert - based or bert2bert - based models",0.5877988934516907
translation,64,145,hyperparameters,hyperparameters,use,"bert vocabulary ( 30,522 tokens )","hyperparameters use bert vocabulary ( 30,522 tokens )",0.6046718955039978
translation,64,146,hyperparameters,adam optimizer,to train,all models,adam optimizer to train all models,0.6630079746246338
translation,64,146,hyperparameters,hyperparameters,use,adam optimizer,hyperparameters use adam optimizer,0.6090166568756104
translation,64,147,hyperparameters,models,for,"20,000 warm - up steps","models for 20,000 warm - up steps",0.6588345766067505
translation,64,147,hyperparameters,5 epochs,with,pretrained model parameters frozen,5 epochs with pretrained model parameters frozen,0.6119759678840637
translation,64,147,hyperparameters,20 epochs,for,all parameters,20 epochs for all parameters,0.5643680691719055
translation,64,147,hyperparameters,hyperparameters,train,models,hyperparameters train models,0.6666569709777832
translation,64,148,hyperparameters,maximum editing iteration times,set to,3,maximum editing iteration times set to 3,0.7112244367599487
translation,64,148,hyperparameters,rise,has,maximum editing iteration times,rise has maximum editing iteration times,0.5640757083892822
translation,64,148,hyperparameters,hyperparameters,For,rise,hyperparameters For rise,0.617012083530426
translation,64,148,hyperparameters,hyperparameters,has,maximum editing iteration times,hyperparameters has maximum editing iteration times,0.48186230659484863
translation,64,149,hyperparameters,gradient clipping,with,maximum gradient norm,gradient clipping with maximum gradient norm,0.5783113837242126
translation,64,149,hyperparameters,maximum gradient norm,of,1.0,maximum gradient norm of 1.0,0.5818200707435608
translation,64,6,model,reinforcement iterative sequence editing ( rise ) framework,optimizes,minimum levenshtein distance,reinforcement iterative sequence editing ( rise ) framework optimizes minimum levenshtein distance,0.6603532433509827
translation,64,6,model,minimum levenshtein distance,through,explicit editing actions,minimum levenshtein distance through explicit editing actions,0.6418697834014893
translation,64,6,model,model,introduce,reinforcement iterative sequence editing ( rise ) framework,model introduce reinforcement iterative sequence editing ( rise ) framework,0.6338765025138855
translation,64,8,model,rise,devise,iterative reinforce training ( irt ) algorithm,rise devise iterative reinforce training ( irt ) algorithm,0.7366994023323059
translation,64,8,model,iterative reinforce training ( irt ) algorithm,with,dynamic programming based sampling ( dps ) process,iterative reinforce training ( irt ) algorithm with dynamic programming based sampling ( dps ) process,0.6253474950790405
translation,64,8,model,dynamic programming based sampling ( dps ) process,to improve,exploration,dynamic programming based sampling ( dps ) process to improve exploration,0.6697860360145569
translation,64,8,model,model,To train,rise,model To train rise,0.7312798500061035
translation,64,37,model,dynamic programming based sampling ( dps ) process,adopts,dynamic programming ( dp ) algorithm,dynamic programming based sampling ( dps ) process adopts dynamic programming ( dp ) algorithm,0.6231224536895752
translation,64,37,model,dynamic programming ( dp ) algorithm,to track and model,interdependency,dynamic programming ( dp ) algorithm to track and model interdependency,0.7042333483695984
translation,64,37,model,interdependency,in,irt,interdependency in irt,0.5751030445098877
translation,64,37,model,model,introduce,dynamic programming based sampling ( dps ) process,model introduce dynamic programming based sampling ( dps ) process,0.6712895035743713
translation,64,154,results,rise,has,significantly outperforms,rise has significantly outperforms,0.6323040723800659
translation,64,154,results,significantly outperforms,has,all base -,significantly outperforms has all base -,0.6017746925354004
translation,64,154,results,results,has,rise,results has rise,0.42123404145240784
translation,64,157,results,strongest baseline querysim,by,?4 %,strongest baseline querysim by ?4 %,0.5875934958457947
translation,64,157,results,?4 %,in terms of,rouge -l,?4 % in terms of rouge -l,0.7421041131019592
translation,64,157,results,rise,has,outperforms,rise has outperforms,0.6597474217414856
translation,64,157,results,outperforms,has,strongest baseline querysim,outperforms has strongest baseline querysim,0.5811890959739685
translation,64,157,results,results,has,rise,results has rise,0.42123404145240784
translation,64,159,results,rise,is,more robust,rise is more robust,0.6296789646148682
translation,64,159,results,more robust,generalizes,better,more robust generalizes better,0.7282108664512634
translation,64,159,results,better,to,unseen data,better to unseen data,0.6004995107650757
translation,64,159,results,unseen data,of,cast,unseen data of cast,0.6347910165786743
translation,64,159,results,results,has,rise,results has rise,0.42123404145240784
translation,64,160,results,results,of,neural methods,results of neural methods,0.5302104353904724
translation,64,160,results,neural methods,on,canard,neural methods on canard,0.643882155418396
translation,64,160,results,neural methods,are,much better,neural methods are much better,0.5786236524581909
translation,64,160,results,much better,than,cast,much better than cast,0.6361547708511353
translation,64,160,results,results,of,neural methods,results of neural methods,0.5302104353904724
translation,64,160,results,results,has,results,results has results,0.48582205176353455
translation,64,174,results,mld,generalizes,better,mld generalizes better,0.7688798308372498
translation,64,174,results,mld,with,mle,mld with mle,0.7032957077026367
translation,64,174,results,better,on,unseen cast,better on unseen cast,0.6119121313095093
translation,64,174,results,drops,in,all metrics,drops in all metrics,0.5555421710014343
translation,64,174,results,slightly,in,all metrics,slightly in all metrics,0.5009341239929199
translation,64,174,results,mle,see,drop,mle see drop,0.7062998414039612
translation,64,174,results,drop,of,10 %,drop of 10 %,0.6998642086982727
translation,64,174,results,10 %,in,bleu -1,10 % in bleu -1,0.5123526453971863
translation,64,174,results,drops,has,slightly,drops has slightly,0.6502330899238586
translation,64,174,results,results,has,mld,results has mld,0.5647836327552795
translation,64,190,results,rise,achieves,70.5 %,rise achieves 70.5 %,0.6765344738960266
translation,64,190,results,70.5 %,in,bleu - 4,70.5 % in bleu - 4,0.5362866520881653
translation,64,190,results,bleu - 4,in,first iteration,bleu - 4 in first iteration,0.5427084565162659
translation,64,190,results,71.5 % and 71.6 %,in,second and third iterations,71.5 % and 71.6 % in second and third iterations,0.5535114407539368
translation,64,190,results,results,has,rise,results has rise,0.42123404145240784
translation,65,210,baselines,hred + kvmem,combines,hierarchical encoder,hred + kvmem combines hierarchical encoder,0.7173389196395874
translation,65,210,baselines,hierarchical encoder,with,key-value memory network,hierarchical encoder with key-value memory network,0.6143243312835693
translation,65,210,baselines,baselines,has,hred + kvmem,baselines has hred + kvmem,0.5910574197769165
translation,65,211,baselines,d2a and masp,are,two seq2seq models,d2a and masp are two seq2seq models,0.5935053825378418
translation,65,211,baselines,two seq2seq models,to translate,questions,two seq2seq models to translate questions,0.6796485185623169
translation,65,211,baselines,questions,into,logical forms,questions into logical forms,0.5776479840278625
translation,65,211,baselines,baselines,has,d2a and masp,baselines has d2a and masp,0.5529168248176575
translation,65,176,experimental-setup,our method,by,pytorch,our method by pytorch,0.6320067644119263
translation,65,176,experimental-setup,pytorch,on,nvidia v440.64.00-32gb gpu cards,pytorch on nvidia v440.64.00-32gb gpu cards,0.47381314635276794
translation,65,176,experimental-setup,experimental setup,implement,our method,experimental setup implement our method,0.6305364966392517
translation,65,177,experimental-setup,glove,as,initialized word embeddings,glove as initialized word embeddings,0.5198189616203308
translation,65,177,experimental-setup,glove,set,maximum number of gcn layers,glove set maximum number of gcn layers,0.7103628516197205
translation,65,177,experimental-setup,maximum number of gcn layers,as,10,maximum number of gcn layers as 10,0.5882452130317688
translation,65,177,experimental-setup,experimental setup,employ,glove,experimental setup employ glove,0.5971238017082214
translation,65,177,experimental-setup,experimental setup,set,maximum number of gcn layers,experimental setup set maximum number of gcn layers,0.6312503814697266
translation,65,178,experimental-setup,grid search,through,pre-defined hyper-parameter spaces,grid search through pre-defined hyper-parameter spaces,0.6433141827583313
translation,65,178,experimental-setup,pre-defined hyper-parameter spaces,specifically,hidden dimensionality,pre-defined hyper-parameter spaces specifically hidden dimensionality,0.6627900004386902
translation,65,178,experimental-setup,hidden dimensionality,amongst,"{ 200 , 300 , 400 }","hidden dimensionality amongst { 200 , 300 , 400 }",0.604531466960907
translation,65,178,experimental-setup,learning rate,amongst,"{ 3e ? 3 , 3e ? 4 , 3e ? 5 }","learning rate amongst { 3e ? 3 , 3e ? 4 , 3e ? 5 }",0.5938289165496826
translation,65,178,experimental-setup,dropout ratio,amongst,"{ 0.2 , 0.1 , 0.0 }","dropout ratio amongst { 0.2 , 0.1 , 0.0 }",0.5972678661346436
translation,65,178,experimental-setup,experimental setup,apply,grid search,experimental setup apply grid search,0.6156560182571411
translation,65,179,experimental-setup,best hyper-parameter configuration,based on,best f1 score,best hyper-parameter configuration based on best f1 score,0.6107289791107178
translation,65,179,experimental-setup,best f1 score,on,development set,best f1 score on development set,0.5335245132446289
translation,65,179,experimental-setup,experimental setup,has,best hyper-parameter configuration,experimental setup has best hyper-parameter configuration,0.5432775020599365
translation,65,180,experimental-setup,each neural network model,set,hidden dimensionality,each neural network model set hidden dimensionality,0.6704860329627991
translation,65,180,experimental-setup,hidden dimensionality,to,300,hidden dimensionality to 300,0.6198093295097351
translation,65,180,experimental-setup,experimental setup,for,each neural network model,experimental setup for each neural network model,0.6202397346496582
translation,65,181,experimental-setup,dropout layer,set before,each mlp,dropout layer set before each mlp,0.6568102836608887
translation,65,181,experimental-setup,each mlp,with,ratio,each mlp with ratio,0.7068620324134827
translation,65,181,experimental-setup,ratio,of,0.1,ratio of 0.1,0.6428002119064331
translation,65,181,experimental-setup,experimental setup,has,dropout layer,experimental setup has dropout layer,0.5004676580429077
translation,65,182,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,65,182,experimental-setup,adam optimizer,with,batch size,adam optimizer with batch size,0.606801450252533
translation,65,182,experimental-setup,learning rate,of,3e ? 5,learning rate of 3e ? 5,0.6395402550697327
translation,65,182,experimental-setup,batch size,is,1,batch size is 1,0.6298584342002869
translation,65,182,experimental-setup,experimental setup,use,adam optimizer,experimental setup use adam optimizer,0.5987385511398315
translation,65,183,experimental-setup,training epoch number,is,100,training epoch number is 100,0.5999999642372131
translation,65,183,experimental-setup,experimental setup,has,training epoch number,experimental setup has training epoch number,0.498542457818985
translation,65,6,model,implied entities,refer to as,focal entities,implied entities refer to as focal entities,0.6003792881965637
translation,65,6,model,focal entities,of,conversation,focal entities of conversation,0.5956751108169556
translation,65,6,model,model,model,implied entities,model model implied entities,0.8269276022911072
translation,65,7,model,novel graph - based model,to capture,transitions,novel graph - based model to capture transitions,0.6835376024246216
translation,65,7,model,novel graph - based model,apply,graph neural network,novel graph - based model apply graph neural network,0.5894014239311218
translation,65,7,model,transitions,of,focal entities,transitions of focal entities,0.5951482653617859
translation,65,7,model,transitions,of,focal entities,transitions of focal entities,0.5951482653617859
translation,65,7,model,focal entities,for,each question,focal entities for each question,0.6124379634857178
translation,65,7,model,graph neural network,to derive,probability distribution,graph neural network to derive probability distribution,0.6712899804115295
translation,65,7,model,probability distribution,of,focal entities,probability distribution of focal entities,0.5959560871124268
translation,65,7,model,probability distribution,combined with,standard kbqa module,probability distribution combined with standard kbqa module,0.6934855580329895
translation,65,7,model,focal entities,for,each question,focal entities for each question,0.6124379634857178
translation,65,7,model,standard kbqa module,to perform,answer ranking,standard kbqa module to perform answer ranking,0.637857973575592
translation,65,7,model,model,propose,novel graph - based model,model propose novel graph - based model,0.6657209396362305
translation,65,7,model,model,apply,graph neural network,model apply graph neural network,0.6082252860069275
translation,65,34,model,entity transition graph,to elaborately model,entities,entity transition graph to elaborately model entities,0.6381391882896423
translation,65,34,model,entities,involved in,conversation,entities involved in conversation,0.6824864149093628
translation,65,34,model,graph - based neural network,to derive,focal score,graph - based neural network to derive focal score,0.6836767792701721
translation,65,34,model,focal score,for,each entity,focal score for each entity,0.609856903553009
translation,65,34,model,focal score,represents,probability,focal score represents probability,0.6770754456520081
translation,65,34,model,each entity,in,graph,each entity in graph,0.5201848745346069
translation,65,34,model,conversation,has,as well as their interactions,conversation has as well as their interactions,0.583344042301178
translation,65,34,model,model,construct,entity transition graph,model construct entity transition graph,0.6876530051231384
translation,65,34,model,model,apply,graph - based neural network,model apply graph - based neural network,0.6330419182777405
translation,65,38,results,system,encodes,entire conversation history,system encodes entire conversation history,0.76796954870224
translation,65,38,results,entire conversation history,without handling,focal entities,entire conversation history without handling focal entities,0.7480182647705078
translation,65,38,results,our method,clearly perform,better,our method clearly perform better,0.6616356372833252
translation,65,38,results,better,on,both datasets,better on both datasets,0.5056477785110474
translation,65,44,results,state of the art,by,9.5 percentage points,state of the art by 9.5 percentage points,0.5639444589614868
translation,65,44,results,state of the art,by,14.3 percentage points,state of the art by 14.3 percentage points,0.5484441518783569
translation,65,44,results,9.5 percentage points,on,convquestions,9.5 percentage points on convquestions,0.5533590316772461
translation,65,44,results,14.3 percentage points,on,convcsqa,14.3 percentage points on convcsqa,0.5530690550804138
translation,65,44,results,our method,has,outperform,our method has outperform,0.6298079490661621
translation,65,44,results,outperform,has,state of the art,outperform has state of the art,0.589471161365509
translation,65,44,results,results,has,our method,results has our method,0.5589964985847473
translation,65,186,results,convhistory,on,both datasets,convhistory on both datasets,0.5432170033454895
translation,65,186,results,our method,has,clearly outperforms,our method has clearly outperforms,0.6170369386672974
translation,65,188,results,conversation history,through,standard two -level hierarchical sequence model,conversation history through standard two -level hierarchical sequence model,0.6507393717765808
translation,65,188,results,standard two -level hierarchical sequence model,has,does not consistently improve,standard two -level hierarchical sequence model has does not consistently improve,0.5714389085769653
translation,65,188,results,does not consistently improve,has,performance,does not consistently improve has performance,0.5759842991828918
translation,65,188,results,results,modeling,conversation history,results modeling conversation history,0.7041288018226624
translation,65,197,results,questions,later turns of,conversation,questions later turns of conversation,0.7353692054748535
translation,65,197,results,drops,for,all three methods,drops for all three methods,0.6810296177864075
translation,65,197,results,questions,has,performance,questions has performance,0.5845069289207458
translation,65,197,results,performance,has,drops,performance has drops,0.5993483662605286
translation,65,208,results,our method,achieves,overall 9.5 and 14.3 percentage points of improvement,our method achieves overall 9.5 and 14.3 percentage points of improvement,0.6486880779266357
translation,65,208,results,outperforms,on,most questions,outperforms on most questions,0.5277333855628967
translation,65,208,results,other systems,on,most questions,other systems on most questions,0.5370556712150574
translation,65,208,results,overall 9.5 and 14.3 percentage points of improvement,on,convquestions and convc - sqa,overall 9.5 and 14.3 percentage points of improvement on convquestions and convc - sqa,0.5518079996109009
translation,65,208,results,our method,has,outperforms,our method has outperforms,0.6322360634803772
translation,65,208,results,outperforms,has,other systems,outperforms has other systems,0.6029842495918274
translation,65,208,results,results,has,our method,results has our method,0.5589964985847473
translation,66,151,ablation-analysis,relational embedding,in,encoder,relational embedding in encoder,0.5327911972999573
translation,66,151,ablation-analysis,bleu - 4 score,of,original model,bleu - 4 score of original model,0.541022539138794
translation,66,151,ablation-analysis,original model,drops from,20.33 to 19.43,original model drops from 20.33 to 19.43,0.6760848164558411
translation,66,151,ablation-analysis,relational embedding,has,bleu - 4 score,relational embedding has bleu - 4 score,0.5453709959983826
translation,66,151,ablation-analysis,encoder,has,bleu - 4 score,encoder has bleu - 4 score,0.5303909182548523
translation,66,151,ablation-analysis,ablation analysis,remove,relational embedding,ablation analysis remove relational embedding,0.6488857865333557
translation,66,154,ablation-analysis,bleu - 4 score,drops from,20.33 to 19.01,bleu - 4 score drops from 20.33 to 19.01,0.7044729590415955
translation,66,154,ablation-analysis,ignd,improves,model performance,ignd improves model performance,0.6763066649436951
translation,66,154,ablation-analysis,ablation analysis,has,bleu - 4 score,ablation analysis has bleu - 4 score,0.5355873703956604
translation,66,156,ablation-analysis,original model,scores,20.33,original model scores 20.33,0.7075859904289246
translation,66,156,ablation-analysis,original model,original w/o relational embedding and IGND,drop,original model original w/o relational embedding and IGND drop,0.808758020401001
translation,66,156,ablation-analysis,drop,has,significantly,drop has significantly,0.6707428097724915
translation,66,156,ablation-analysis,significantly,has,almost 2 in bleu - 4 score,significantly has almost 2 in bleu - 4 score,0.6119094491004944
translation,66,156,ablation-analysis,ablation analysis,In comparison to,original model,ablation analysis In comparison to original model,0.6486276984214783
translation,66,157,ablation-analysis,pre-trained bert embedding,considerable impact,performance,pre-trained bert embedding considerable impact performance,0.6937385201454163
translation,66,157,ablation-analysis,pre-trained bert embedding,considerable impact,performance,pre-trained bert embedding considerable impact performance,0.6937385201454163
translation,66,157,ablation-analysis,finetuning bert embedding,improves,performance,finetuning bert embedding improves performance,0.6400949954986572
translation,66,157,ablation-analysis,ablation analysis,find,pre-trained bert embedding,ablation analysis find pre-trained bert embedding,0.5615813732147217
translation,66,157,ablation-analysis,ablation analysis,find,finetuning bert embedding,ablation analysis find finetuning bert embedding,0.5911160111427307
translation,66,163,ablation-analysis,average attention score,is,highest,average attention score is highest,0.5634763240814209
translation,66,163,ablation-analysis,highest,indicates that,syntactic information,highest indicates that syntactic information,0.6844154596328735
translation,66,163,ablation-analysis,syntactic information,improve,performance,syntactic information improve performance,0.6690887808799744
translation,66,163,ablation-analysis,performance,of,encoder,performance of encoder,0.6665438413619995
translation,66,163,ablation-analysis,our model,has,average attention score,our model has average attention score,0.5568248629570007
translation,66,163,ablation-analysis,ablation analysis,In,our model,ablation analysis In our model,0.5655121803283691
translation,66,104,baselines,"mpqg ( song et al. , 2018 )",uses,different matching strategies,"mpqg ( song et al. , 2018 ) uses different matching strategies",0.5951051115989685
translation,66,104,baselines,different matching strategies,to explicitly model,information,different matching strategies to explicitly model information,0.7634373903274536
translation,66,104,baselines,information,between,answer and context,information between answer and context,0.6351662874221802
translation,66,104,baselines,baselines,has,"mpqg ( song et al. , 2018 )","baselines has mpqg ( song et al. , 2018 )",0.5007307529449463
translation,66,107,baselines,s2sa-at-mp-gsa,employs,gated attention encoder,s2sa-at-mp-gsa employs gated attention encoder,0.5972909331321716
translation,66,107,baselines,s2sa-at-mp-gsa,employs,maxout pointer decoder,s2sa-at-mp-gsa employs maxout pointer decoder,0.6074395179748535
translation,66,107,baselines,maxout pointer decoder,to deal with,long text inputs,maxout pointer decoder to deal with long text inputs,0.6369425058364868
translation,66,107,baselines,baselines,has,s2sa-at-mp-gsa,baselines has s2sa-at-mp-gsa,0.5570844411849976
translation,66,108,baselines,"ass2s ( kim et al. , 2019 )",proposes,answer separated seq2seq model,"ass2s ( kim et al. , 2019 ) proposes answer separated seq2seq model",0.6607177257537842
translation,66,108,baselines,answer separated seq2seq model,by replacing,answer,answer separated seq2seq model by replacing answer,0.6769359707832336
translation,66,108,baselines,answer,in,input sequence,answer in input sequence,0.5350935459136963
translation,66,108,baselines,input sequence,with,some specific words,input sequence with some specific words,0.6279411315917969
translation,66,108,baselines,baselines,has,"ass2s ( kim et al. , 2019 )","baselines has ass2s ( kim et al. , 2019 )",0.5241251587867737
translation,66,110,baselines,"qg - pg ( jia et al. , 2020 )",leverages,paraphrase information,"qg - pg ( jia et al. , 2020 ) leverages paraphrase information",0.7499935626983643
translation,66,110,baselines,paraphrase information,to,qg model,paraphrase information to qg model,0.519159197807312
translation,66,110,baselines,baselines,has,"qg - pg ( jia et al. , 2020 )","baselines has qg - pg ( jia et al. , 2020 )",0.5514727234840393
translation,66,111,baselines,"graph2seq recurrent bert ( chan and fan , 2020 )",employs,pre-trained bert language model,"graph2seq recurrent bert ( chan and fan , 2020 ) employs pre-trained bert language model",0.5124661922454834
translation,66,111,baselines,pre-trained bert language model,to tackle,question generation tasks,pre-trained bert language model to tackle question generation tasks,0.5686314702033997
translation,66,111,baselines,baselines,has,"graph2seq recurrent bert ( chan and fan , 2020 )","baselines has graph2seq recurrent bert ( chan and fan , 2020 )",0.5244842767715454
translation,66,161,baselines,nqg ++,ignore,syntactic information,nqg ++ ignore syntactic information,0.7480952143669128
translation,66,161,baselines,graph2seq + rl + bert,use,syntactic structure information,graph2seq + rl + bert use syntactic structure information,0.5898981690406799
translation,66,161,baselines,graph2seq + rl + bert,ignore,dependency relations,graph2seq + rl + bert ignore dependency relations,0.7262879014015198
translation,66,70,hyperparameters,passage embeddings,for,each nodes,passage embeddings for each nodes,0.5776299238204956
translation,66,70,hyperparameters,passage embeddings,initialized to,passage embeddings h p,passage embeddings initialized to passage embeddings h p,0.6568002104759216
translation,66,70,hyperparameters,passage embeddings,initialized to,relational embeddings,passage embeddings initialized to relational embeddings,0.6285297274589539
translation,66,70,hyperparameters,bi-ggnn,has,passage embeddings,bi-ggnn has passage embeddings,0.5805457234382629
translation,66,70,hyperparameters,hyperparameters,In,bi-ggnn,hyperparameters In bi-ggnn,0.4869048595428467
translation,66,115,hyperparameters,300 - dim glove vectors,for,"most frequent 70,000 words","300 - dim glove vectors for most frequent 70,000 words",0.6070850491523743
translation,66,115,hyperparameters,"most frequent 70,000 words",in,training set,"most frequent 70,000 words in training set",0.519243061542511
translation,66,115,hyperparameters,hyperparameters,fix,300 - dim glove vectors,hyperparameters fix 300 - dim glove vectors,0.613073468208313
translation,66,116,hyperparameters,1024 - dim bert embeddings,on the fly,each word,1024 - dim bert embeddings on the fly each word,0.7050389051437378
translation,66,116,hyperparameters,each word,in,text,each word in text,0.5433952212333679
translation,66,116,hyperparameters,trainable weighted sum,of,all bert layer outputs,trainable weighted sum of all bert layer outputs,0.5663732290267944
translation,66,116,hyperparameters,hyperparameters,compute,1024 - dim bert embeddings,hyperparameters compute 1024 - dim bert embeddings,0.6353126168251038
translation,66,117,hyperparameters,embedding sizes,of,"case , answer , copy , pos , and ner tags","embedding sizes of case , answer , copy , pos , and ner tags",0.5628405213356018
translation,66,117,hyperparameters,"case , answer , copy , pos , and ner tags",set of,"3 , 3 , 3 , 12 and 8","case , answer , copy , pos , and ner tags set of 3 , 3 , 3 , 12 and 8",0.6772112250328064
translation,66,117,hyperparameters,hyperparameters,has,embedding sizes,hyperparameters has embedding sizes,0.5035743713378906
translation,66,118,hyperparameters,hidden state size,of,bilstm,hidden state size of bilstm,0.5449387431144714
translation,66,118,hyperparameters,bilstm,to,150,bilstm to 150,0.6785483360290527
translation,66,118,hyperparameters,hyperparameters,set,hidden state size,hyperparameters set hidden state size,0.6345290541648865
translation,66,119,hyperparameters,all other hidden layers,set to,300,all other hidden layers set to 300,0.6899664402008057
translation,66,120,hyperparameters,variational dropout rate,of,0.4,variational dropout rate of 0.4,0.5857706069946289
translation,66,120,hyperparameters,variational dropout rate,of,0.3,variational dropout rate of 0.3,0.5780509114265442
translation,66,120,hyperparameters,0.4,after,word embedding layers,0.4 after word embedding layers,0.6394954919815063
translation,66,120,hyperparameters,0.3,after,rnn layers,0.3 after rnn layers,0.6558988690376282
translation,66,120,hyperparameters,hyperparameters,apply,variational dropout rate,hyperparameters apply variational dropout rate,0.5324305295944214
translation,66,121,hyperparameters,number of gnn hops,in,both encoder and decoder,number of gnn hops in both encoder and decoder,0.5261827111244202
translation,66,121,hyperparameters,number of gnn hops,set to,4,number of gnn hops set to 4,0.7105689644813538
translation,66,121,hyperparameters,both encoder and decoder,set to,4,both encoder and decoder set to 4,0.7225496172904968
translation,66,121,hyperparameters,hyperparameters,has,number of gnn hops,hyperparameters has number of gnn hops,0.5378992557525635
translation,66,122,hyperparameters,"adam ( kingma and ba , 2014 )",as,optimizer,"adam ( kingma and ba , 2014 ) as optimizer",0.5086410641670227
translation,66,122,hyperparameters,learning rate,set to,0.001,learning rate set to 0.001,0.6954665780067444
translation,66,122,hyperparameters,hyperparameters,use,"adam ( kingma and ba , 2014 )","hyperparameters use adam ( kingma and ba , 2014 )",0.5926938652992249
translation,66,126,hyperparameters,training,when,no improvement,training when no improvement,0.6502584218978882
translation,66,126,hyperparameters,no improvement,seen,10 epochs,no improvement seen 10 epochs,0.6986562609672546
translation,66,126,hyperparameters,no improvement,for,10 epochs,no improvement for 10 epochs,0.6431077718734741
translation,66,126,hyperparameters,hyperparameters,stop,training,hyperparameters stop training,0.6751257181167603
translation,66,127,hyperparameters,gradient,at,length 10,gradient at length 10,0.6129990816116333
translation,66,127,hyperparameters,hyperparameters,clip,gradient,hyperparameters clip gradient,0.7621164321899414
translation,66,128,hyperparameters,batch size,set to,60,batch size set to 60,0.740639865398407
translation,66,128,hyperparameters,60,for,squad and marco,60 for squad and marco,0.6673004627227783
translation,66,128,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,66,129,hyperparameters,beam search width,set to,5,beam search width set to 5,0.7671388983726501
translation,66,129,hyperparameters,hyperparameters,has,beam search width,hyperparameters has beam search width,0.5100207328796387
translation,66,9,model,iterative graph network - based decoder ( ignd ),to model,previous generation,iterative graph network - based decoder ( ignd ) to model previous generation,0.6974647641181946
translation,66,9,model,previous generation,using,graph neural network,previous generation using graph neural network,0.6717421412467957
translation,66,9,model,graph neural network,at,each decoding step,graph neural network at each decoding step,0.5531560182571411
translation,66,9,model,model,design,iterative graph network - based decoder ( ignd ),model design iterative graph network - based decoder ( ignd ),0.6008715629577637
translation,66,10,model,graph model,captures,dependency relations,graph model captures dependency relations,0.7558334469795227
translation,66,10,model,dependency relations,in,passage,dependency relations in passage,0.5475507378578186
translation,66,10,model,dependency relations,boost,generation,dependency relations boost generation,0.6156401634216309
translation,66,10,model,model,has,graph model,model has graph model,0.5790111422538757
translation,66,33,model,iterative graph network - based decoder ( ignd ),to model,structure information,iterative graph network - based decoder ( ignd ) to model structure information,0.7038503885269165
translation,66,33,model,structure information,in,previous generation,structure information in previous generation,0.47636422514915466
translation,66,33,model,structure information,at,each decode step,structure information at each decode step,0.5568349957466125
translation,66,33,model,each decode step,using,graph neural network,each decode step using graph neural network,0.6989948749542236
translation,66,33,model,model,design,iterative graph network - based decoder ( ignd ),model design iterative graph network - based decoder ( ignd ),0.6008715629577637
translation,66,40,model,relationalgraph encoder,employs,similar bi-ggnn,relationalgraph encoder employs similar bi-ggnn,0.5197831392288208
translation,66,40,model,similar bi-ggnn,to capture,dependency relations,similar bi-ggnn to capture dependency relations,0.6795984506607056
translation,66,40,model,dependency relations,of,passage,dependency relations of passage,0.6022433638572693
translation,66,40,model,dependency relations,boost,generation,dependency relations boost generation,0.6156401634216309
translation,66,40,model,model,propose,relationalgraph encoder,model propose relationalgraph encoder,0.6963876485824585
translation,66,106,model,answer-focused position - aware model,generates,accurate interrogative word,answer-focused position - aware model generates accurate interrogative word,0.6233975887298584
translation,66,106,model,answer-focused position - aware model,focuses on,important context words,answer-focused position - aware model focuses on important context words,0.717207670211792
translation,66,106,model,model,has,answer-focused position - aware model,model has answer-focused position - aware model,0.5649265050888062
translation,66,109,model,context,extracts,answer-relevant relations,context extracts answer-relevant relations,0.6273636817932129
translation,66,109,model,context,encodes,sentence and relations,context encodes sentence and relations,0.7474012970924377
translation,66,109,model,answer-relevant relations,in,sentence,answer-relevant relations in sentence,0.49719664454460144
translation,66,109,model,sentence and relations,to capture,answer-focused representations,sentence and relations to capture answer-focused representations,0.6790636777877808
translation,66,153,model,model,remove,ignd,model remove ignd,0.7180830240249634
translation,66,162,model,syntactic information,includes,structure information,syntactic information includes structure information,0.6044292449951172
translation,66,162,model,syntactic information,includes,dependency relations,syntactic information includes dependency relations,0.573850691318512
translation,66,162,model,model,uses,syntactic information,model uses syntactic information,0.595289409160614
translation,66,135,results,our model,yields,best results,our model yields best results,0.707749605178833
translation,66,135,results,best results,with,20.33,best results with 20.33,0.6093657612800598
translation,66,135,results,bleu - 4,has,our model,bleu - 4 has our model,0.5734893679618835
translation,66,135,results,results,In terms of,bleu - 4,results In terms of bleu - 4,0.6086405515670776
translation,66,136,results,state - of- the - art results,on,squad,state - of- the - art results on squad,0.6013000011444092
translation,66,136,results,squad,for,sentence - level qg,squad for sentence - level qg,0.607554018497467
translation,66,136,results,results,achieve,state - of- the - art results,results achieve state - of- the - art results,0.5771380066871643
translation,66,148,results,our model,receives,higher scores,our model receives higher scores,0.6524627208709717
translation,66,148,results,higher scores,on,all three metrics,higher scores on all three metrics,0.49556878209114075
translation,66,148,results,results,has,our model,results has our model,0.5871725678443909
translation,66,152,results,performance,of,original w/o ignd model,performance of original w/o ignd model,0.5704001188278198
translation,66,152,results,performance,of,other graph2seq model,performance of other graph2seq model,0.539518415927887
translation,66,152,results,other graph2seq model,to use,syntactic information baseline,other graph2seq model to use syntactic information baseline,0.6214209198951721
translation,66,152,results,syntactic information baseline,such as,graph2seq + rl + bert,syntactic information baseline such as graph2seq + rl + bert,0.5218505859375
translation,66,152,results,original w/o ignd model,has,19.01 bleu - 4 score,original w/o ignd model has 19.01 bleu - 4 score,0.5403710007667542
translation,66,152,results,graph2seq + rl + bert,has,18.30 bleu -4 score,graph2seq + rl + bert has 18.30 bleu -4 score,0.5366732478141785
translation,66,152,results,results,comparing,performance,results comparing performance,0.7179481983184814
translation,66,167,results,our model,with,ignd,our model with ignd,0.7471686601638794
translation,66,167,results,ignd,has,highest average copy probability score,ignd has highest average copy probability score,0.5742166638374329
translation,66,167,results,results,has,our model,results has our model,0.5871725678443909
translation,67,102,experiments,single-dataset adapters,with,frozen transformer,single-dataset adapters with frozen transformer,0.6491233110427856
translation,67,102,experiments,frozen transformer,perform,worse,frozen transformer perform worse,0.623479425907135
translation,67,6,model,multi-dataset question answering,with,ensemble of single - dataset experts,multi-dataset question answering with ensemble of single - dataset experts,0.5998663306236267
translation,67,6,model,"lightweight , dataset -specific adapter modules",share,underlying transformer model,"lightweight , dataset -specific adapter modules share underlying transformer model",0.6581969261169434
translation,67,76,model,optimization,followed by,separate adapter tuning,optimization followed by separate adapter tuning,0.640046238899231
translation,67,76,model,model,has,both phases,model has both phases,0.5685318112373352
translation,67,7,results,all our baselines,in terms of,in-distribution accuracy,all our baselines in terms of in-distribution accuracy,0.6343211531639099
translation,67,7,results,simple methods,on,parameter - averaging,simple methods on parameter - averaging,0.5276129245758057
translation,67,7,results,simple methods,lead to,few-shot transfer performance,simple methods lead to few-shot transfer performance,0.6492084264755249
translation,67,7,results,parameter - averaging,lead to,few-shot transfer performance,parameter - averaging lead to few-shot transfer performance,0.6452369093894958
translation,67,7,results,multi-adapter dataset experts ( made ),has,outperform,multi-adapter dataset experts ( made ) has outperform,0.6133089065551758
translation,67,7,results,outperform,has,all our baselines,outperform has all our baselines,0.5904538631439209
translation,67,7,results,better,has,zero-shot generalization,better has zero-shot generalization,0.5490977168083191
translation,67,7,results,results,find that,multi-adapter dataset experts ( made ),results find that multi-adapter dataset experts ( made ),0.6573534607887268
translation,67,75,results,higher,than,single - and multi-dataset baselines,higher than single - and multi-dataset baselines,0.5889474153518677
translation,67,75,results,results,MADE scores,higher,results MADE scores higher,0.7274795770645142
translation,67,77,results,underlying made transformer,improves,performance,underlying made transformer improves performance,0.739715576171875
translation,67,77,results,performance,compared to,single- dataset adapters,performance compared to single- dataset adapters,0.6548154354095459
translation,67,77,results,results,Jointly optimizing,underlying made transformer,results Jointly optimizing underlying made transformer,0.647244930267334
translation,67,80,results,dynamic sampling,not improve,results,dynamic sampling not improve results,0.6686457395553589
translation,67,80,results,results,has,dynamic sampling,results has dynamic sampling,0.5384084582328796
translation,67,85,results,parameters,of,different made adapters,parameters of different made adapters,0.5638003349304199
translation,67,85,results,parameters,results in,decent predictor,parameters results in decent predictor,0.654148519039154
translation,67,85,results,results,averaging,parameters,results averaging parameters,0.7119576334953308
translation,67,86,results,parameter averaging,works,best,parameter averaging works best,0.6485429406166077
translation,67,86,results,best,for,made without adapter-tuning,best for made without adapter-tuning,0.634280800819397
translation,67,86,results,results,has,parameter averaging,results has parameter averaging,0.5204647779464722
translation,67,87,results,separately - tuned made adapters,gives,best performance,separately - tuned made adapters gives best performance,0.6421511769294739
translation,67,87,results,best performance,at,additional computational cost,best performance at additional computational cost,0.5322383642196655
translation,67,87,results,ensembling,has,separately - tuned made adapters,ensembling has separately - tuned made adapters,0.6317005753517151
translation,67,87,results,results,has,ensembling,results has ensembling,0.5693673491477966
translation,67,99,results,higher transfer accuracy,compared to,baselines,higher transfer accuracy compared to baselines,0.6678776741027832
translation,67,99,results,results,leads to,higher transfer accuracy,results leads to higher transfer accuracy,0.6851545572280884
translation,67,100,results,post-average method,performs,about the same,post-average method performs about the same,0.5862225890159607
translation,67,100,results,post-average method,performs,better,post-average method performs better,0.629566490650177
translation,67,100,results,about the same,as,averaging,about the same as averaging,0.6392472982406616
translation,67,100,results,averaging,at,initialization,averaging at initialization,0.510617196559906
translation,67,100,results,initialization,in,lower - data settings,initialization in lower - data settings,0.5192685127258301
translation,67,100,results,better,with,k = 256,better with k = 256,0.6878319978713989
translation,67,100,results,results,has,post-average method,results has post-average method,0.5625876784324646
translation,68,162,ablation-analysis,our method,elevates,performance,our method elevates performance,0.68255215883255
translation,68,162,ablation-analysis,performance,by,large margin,performance by large margin,0.6227608919143677
translation,68,162,ablation-analysis,large margin,of,23.68 %,large margin of 23.68 %,0.5101267695426941
translation,68,162,ablation-analysis,another reranking - based model rankvqa,has,our method,another reranking - based model rankvqa has our method,0.5556859374046326
translation,68,162,ablation-analysis,ablation analysis,Compared with,another reranking - based model rankvqa,ablation analysis Compared with another reranking - based model rankvqa,0.6676173806190491
translation,68,171,ablation-analysis,severe performance drops,between,vqa v2 and vqa - cp v2,severe performance drops between vqa v2 and vqa - cp v2,0.6630102396011353
translation,68,171,ablation-analysis,performance drop,to,2.49 %,performance drop to 2.49 %,0.5438709855079651
translation,68,171,ablation-analysis,top20 - sar + lmh,has,significantly decreases,top20 - sar + lmh has significantly decreases,0.5966269969940186
translation,68,171,ablation-analysis,significantly decreases,has,performance drop,significantly decreases has performance drop,0.5969901084899902
translation,68,201,ablation-analysis,lxm and lxm +ssl,gain,prominent performance boost,lxm and lxm +ssl gain prominent performance boost,0.7636260390281677
translation,68,201,ablation-analysis,cas + lxm and cas +lxm +ssl,gain,prominent performance boost,cas + lxm and cas +lxm +ssl gain prominent performance boost,0.7483430504798889
translation,68,201,ablation-analysis,prominent performance boost,of,9.35 % and 6.32 %,prominent performance boost of 9.35 % and 6.32 %,0.5683726668357849
translation,68,201,ablation-analysis,cas + lxm + qtd ( r ) and cas + lxm + ssl + qtd ( r ),outperform,cas + lxm ( r ) and cas + lxm + ssl ( r ),cas + lxm + qtd ( r ) and cas + lxm + ssl + qtd ( r ) outperform cas + lxm ( r ) and cas + lxm + ssl ( r ),0.6390931010246277
translation,68,201,ablation-analysis,cas + lxm ( r ) and cas + lxm + ssl ( r ),by,3.93 % and 2.71 %,cas + lxm ( r ) and cas + lxm + ssl ( r ) by 3.93 % and 2.71 %,0.5765445828437805
translation,68,201,ablation-analysis,lxm and lxm +ssl,has,cas + lxm and cas +lxm +ssl,lxm and lxm +ssl has cas + lxm and cas +lxm +ssl,0.6551017165184021
translation,68,201,ablation-analysis,ablation analysis,Compared with,lxm and lxm +ssl,ablation analysis Compared with lxm and lxm +ssl,0.6597849130630493
translation,68,143,experimental-setup,image features,use,pre-trained faster r-cnn,image features use pre-trained faster r-cnn,0.5741599202156067
translation,68,143,experimental-setup,pre-trained faster r-cnn,to encode,each image,pre-trained faster r-cnn to encode each image,0.705909788608551
translation,68,143,experimental-setup,each image,set of,fixed 36 objects,each image set of fixed 36 objects,0.6692214012145996
translation,68,143,experimental-setup,fixed 36 objects,with,2048 - dimensional feature vectors,fixed 36 objects with 2048 - dimensional feature vectors,0.6175352334976196
translation,68,143,experimental-setup,experimental setup,To extract,image features,experimental setup To extract image features,0.7128846049308777
translation,68,143,experimental-setup,experimental setup,use,pre-trained faster r-cnn,experimental setup use pre-trained faster r-cnn,0.5694352984428406
translation,68,144,experimental-setup,tokenizer,of,lxmert,tokenizer of lxmert,0.598558783531189
translation,68,144,experimental-setup,lxmert,to segment,each dense caption,lxmert to segment each dense caption,0.7770407795906067
translation,68,144,experimental-setup,each dense caption,into,words,each dense caption into words,0.6004794836044312
translation,68,144,experimental-setup,experimental setup,use,tokenizer,experimental setup use tokenizer,0.6232596039772034
translation,68,145,experimental-setup,questions,trimmed to,same length,questions trimmed to same length,0.6677286624908447
translation,68,145,experimental-setup,same length,of,15 or 18,same length of 15 or 18,0.6092043519020081
translation,68,145,experimental-setup,experimental setup,has,questions,experimental setup has questions,0.539913535118103
translation,68,147,experimental-setup,experimental setup,trained on,two titan rtx 24gb gpus,experimental setup trained on two titan rtx 24gb gpus,0.6881558895111084
translation,68,148,experimental-setup,sar +ssl,for,20 epochs,sar +ssl for 20 epochs,0.6090454459190369
translation,68,148,experimental-setup,sar +ssl,with,batch size,sar +ssl with batch size,0.6699892282485962
translation,68,148,experimental-setup,sar +ssl,with,sar,sar +ssl with sar,0.6762874126434326
translation,68,148,experimental-setup,sar +ssl,with,sar + lmh,sar +ssl with sar + lmh,0.678105354309082
translation,68,148,experimental-setup,sar +ssl,with,batch size,sar +ssl with batch size,0.6699892282485962
translation,68,148,experimental-setup,sar +ssl,with,batch size,sar +ssl with batch size,0.6699892282485962
translation,68,148,experimental-setup,batch size,of,32,batch size of 32,0.6741614937782288
translation,68,148,experimental-setup,batch size,of,64,batch size of 64,0.6741159558296204
translation,68,148,experimental-setup,batch size,of,64,batch size of 64,0.6741159558296204
translation,68,148,experimental-setup,experimental setup,train,sar +ssl,experimental setup train sar +ssl,0.669876754283905
translation,68,148,experimental-setup,experimental setup,train,sar,experimental setup train sar,0.5879620909690857
translation,68,148,experimental-setup,experimental setup,train,sar + lmh,experimental setup train sar + lmh,0.6919270753860474
translation,68,150,experimental-setup,adam optimizer,adopted with,learning rate 1e - 5,adam optimizer adopted with learning rate 1e - 5,0.6677694916725159
translation,68,150,experimental-setup,experimental setup,has,adam optimizer,experimental setup has adam optimizer,0.5293667316436768
translation,68,151,experimental-setup,question type discriminator,use,300 dimensional glove,question type discriminator use 300 dimensional glove,0.6566824913024902
translation,68,151,experimental-setup,vectors,to initialize,word embeddings,vectors to initialize word embeddings,0.675733745098114
translation,68,151,experimental-setup,unidirectional gru,with,128 hidden units,unidirectional gru with 128 hidden units,0.6245955228805542
translation,68,151,experimental-setup,300 dimensional glove,has,vectors,300 dimensional glove has vectors,0.5867341160774231
translation,68,151,experimental-setup,experimental setup,For,question type discriminator,experimental setup For question type discriminator,0.5776782631874084
translation,68,9,experiments,candidate answers,relevant to,question,candidate answers relevant to question,0.6743711233139038
translation,68,9,experiments,candidate answers,relevant to,image,candidate answers relevant to image,0.693540632724762
translation,68,9,experiments,candidate answers,by,visual entailment task,candidate answers by visual entailment task,0.5183612108230591
translation,68,8,model,select- and - rerank ( sar ) progressive framework,based on,visual entailment,select- and - rerank ( sar ) progressive framework based on visual entailment,0.5717329382896423
translation,68,8,model,model,propose,select- and - rerank ( sar ) progressive framework,model propose select- and - rerank ( sar ) progressive framework,0.6653358340263367
translation,68,13,model,vqa system,determine,correct answer,vqa system determine correct answer,0.6864516735076904
translation,68,13,model,correct answer,in,large prediction space,correct answer in large prediction space,0.45433658361434937
translation,68,25,model,select- and - rerank ( sar ) progressive framework,based on,visual entailment,select- and - rerank ( sar ) progressive framework based on visual entailment,0.5717329382896423
translation,68,25,model,model,propose,select- and - rerank ( sar ) progressive framework,model propose select- and - rerank ( sar ) progressive framework,0.6653358340263367
translation,68,146,model,answer re-ranking module,incorporate,two languagepriors methods,answer re-ranking module incorporate two languagepriors methods,0.6390528678894043
translation,68,146,model,proposed framework sar,dubbed,sar + ssl,proposed framework sar dubbed sar + ssl,0.7739560008049011
translation,68,146,model,proposed framework sar,dubbed,sar + lmh,proposed framework sar dubbed sar + lmh,0.7829107642173767
translation,68,146,model,two languagepriors methods,has,ssl and lmh,two languagepriors methods has ssl and lmh,0.5783701539039612
translation,68,146,model,model,In,answer re-ranking module,model In answer re-ranking module,0.5004153251647949
translation,68,159,results,vqa - cp v2,observe,top20 - sar + lmh,vqa - cp v2 observe top20 - sar + lmh,0.6423968076705933
translation,68,159,results,top20 - sar + lmh,establishes,new state - of- the - art accuracy,top20 - sar + lmh establishes new state - of- the - art accuracy,0.6085683107376099
translation,68,159,results,new state - of- the - art accuracy,of,66.73 %,new state - of- the - art accuracy of 66.73 %,0.5149720311164856
translation,68,159,results,66.73 %,on,vqa - cp v2,66.73 % on vqa - cp v2,0.5867828726768494
translation,68,159,results,previous bestperforming method cl,by,7.55 %,previous bestperforming method cl by 7.55 %,0.5615677833557129
translation,68,159,results,results,on,vqa - cp v2,results on vqa - cp v2,0.577746570110321
translation,68,159,results,results,observe,top20 - sar + lmh,results observe top20 - sar + lmh,0.6096897125244141
translation,68,160,results,language -priors methods,in,answer re-ranking module,language -priors methods in answer re-ranking module,0.4922458529472351
translation,68,160,results,outperforms,by,6.26 %,outperforms by 6.26 %,0.6294485926628113
translation,68,160,results,cl,by,6.26 %,cl by 6.26 %,0.6260197758674622
translation,68,160,results,language -priors methods,has,our model,language -priors methods has our model,0.5398268699645996
translation,68,160,results,answer re-ranking module,has,our model,answer re-ranking module has our model,0.5716452598571777
translation,68,160,results,our model,has,top20 - sar,our model has top20 - sar,0.6192105412483215
translation,68,160,results,top20 - sar,has,outperforms,top20 - sar has outperforms,0.6282935738563538
translation,68,160,results,outperforms,has,cl,outperforms has cl,0.6748024225234985
translation,68,160,results,results,without combining,language -priors methods,results without combining language -priors methods,0.6094226837158203
translation,68,161,results,outstanding effectiveness,of,proposed sar framework,outstanding effectiveness of proposed sar framework,0.6090776324272156
translation,68,161,results,sar + ssl and sar + lmh,achieve,much better performance,sar + ssl and sar + lmh achieve much better performance,0.6511454582214355
translation,68,161,results,much better performance,than,ssl and lmh,much better performance than ssl and lmh,0.578887939453125
translation,68,161,results,results,show,outstanding effectiveness,results show outstanding effectiveness,0.6368781328201294
translation,68,164,results,previous models,not,generalize,previous models not generalize,0.7040290236473083
translation,68,164,results,well,on,all question types,well on all question types,0.507742166519165
translation,68,164,results,generalize,has,well,generalize has well,0.6266511678695679
translation,68,164,results,results,has,previous models,results has previous models,0.5590499043464661
translation,68,165,results,cl,is,previ-ous best,cl is previ-ous best,0.6502967476844788
translation,68,165,results,previ-ous best,on,  yes / no  ,previ-ous best on   yes / no  ,0.5555371046066284
translation,68,165,results,previ-ous best,on,num   questions,previ-ous best on num   questions,0.6002554893493652
translation,68,165,results,previ-ous best,on,lxmert,previ-ous best on lxmert,0.6401394009590149
translation,68,165,results,lxmert,on,  other   questions,lxmert on   other   questions,0.5954316854476929
translation,68,165,results,results,has,cl,results has cl,0.5053845047950745
translation,68,166,results,previous best model,on,  yes / no   questions,previous best model on   yes / no   questions,0.518824577331543
translation,68,166,results,best performance,on,  num   and   other   questions,best performance on   num   and   other   questions,0.5735898613929749
translation,68,166,results,rivals,has,previous best model,rivals has previous best model,0.5939223766326904
translation,68,166,results,improves,has,best performance,improves has best performance,0.6131362915039062
translation,68,169,results,our method,achieves,best accuracy,our method achieves best accuracy,0.641727864742279
translation,68,169,results,best accuracy,of,70.63 %,best accuracy of 70.63 %,0.5240067839622498
translation,68,169,results,70.63 %,amongst,baselines,70.63 % amongst baselines,0.6047207117080688
translation,68,169,results,baselines,specially designed for overcoming,language priors,baselines specially designed for overcoming language priors,0.6087769865989685
translation,68,169,results,results,has,our method,results has our method,0.5589964985847473
translation,68,172,results,css,achieves,better performance gap,css achieves better performance gap,0.6752051115036011
translation,68,172,results,css,sacrifices,performance,css sacrifices performance,0.6864022612571716
translation,68,172,results,better performance gap,sacrifices,performance,better performance gap sacrifices performance,0.6411315202713013
translation,68,172,results,performance,on,vqa v2,performance on vqa v2,0.5623069405555725
translation,68,173,results,n,rises from,12 to 20,n rises from 12 to 20,0.6808983087539673
translation,68,173,results,our models,achieve,better accuracy,our models achieve better accuracy,0.6149545311927795
translation,68,173,results,better accuracy,on,both datasets,better accuracy on both datasets,0.46326249837875366
translation,68,173,results,better accuracy,along with,smaller performance gap,better accuracy along with smaller performance gap,0.622330904006958
translation,68,173,results,n,has,our models,n has our models,0.6455566883087158
translation,68,173,results,12 to 20,has,our models,12 to 20 has our models,0.5764667987823486
translation,68,185,results,outperforms,based on,lmh,outperforms based on lmh,0.6832172870635986
translation,68,185,results,outperforms,based on,lmh,outperforms based on lmh,0.6832172870635986
translation,68,185,results,lmh,is,better vqa model,lmh is better vqa model,0.5548521280288696
translation,68,185,results,better vqa model,in overcoming,language priors,better vqa model in overcoming language priors,0.5696487426757812
translation,68,185,results,language priors,compared with,updn,language priors compared with updn,0.6298187971115112
translation,68,185,results,updn,has,outperforms,updn has outperforms,0.6563843488693237
translation,68,190,results,cas,is,general vqa model updn,cas is general vqa model updn,0.6015661954879761
translation,68,190,results,general vqa model updn,rather than,lmh and ssl,general vqa model updn rather than lmh and ssl,0.640847384929657
translation,68,190,results,improvement,brought from,combination with language -priors method,improvement brought from combination with language -priors method,0.6190604567527771
translation,68,190,results,combination with language -priors method,in,answer re-ranking module,combination with language -priors method in answer re-ranking module,0.4927751421928406
translation,68,190,results,combination with language -priors method,is,more obvious,combination with language -priors method is more obvious,0.5384929776191711
translation,68,191,results,sar,achieves,much better accuracy,sar achieves much better accuracy,0.7072340250015259
translation,68,191,results,much better accuracy,than,previous sota model cl,much better accuracy than previous sota model cl,0.6054719686508179
translation,68,191,results,previous sota model cl,by,2.53 %,previous sota model cl by 2.53 %,0.5605758428573608
translation,68,191,results,updn,has,sar,updn has sar,0.6869310736656189
translation,68,191,results,results,choose,updn,results choose updn,0.7276646494865417
translation,68,194,results,c,consistently outperforms,r,c consistently outperforms r,0.6381077170372009
translation,68,194,results,sar and sar + ssl,has,c,sar and sar + ssl has c,0.6515225768089294
translation,68,194,results,results,On,sar and sar + ssl,results On sar and sar + ssl,0.5152564644813538
translation,68,196,results,sar,with,r?c,sar with r?c,0.721957802772522
translation,68,196,results,sar + ssl and sar + lmh,with,r,sar + ssl and sar + lmh with r,0.7133787274360657
translation,68,196,results,language priors method,has,sar,language priors method has sar,0.5748604536056519
translation,68,196,results,r?c,has,rivals,r?c has rivals,0.62009596824646
translation,68,196,results,outperforms,has,sar + ssl and sar + lmh,outperforms has sar + ssl and sar + lmh,0.5998548269271851
translation,68,196,results,results,Even without,language priors method,results Even without language priors method,0.665280282497406
translation,68,197,results,our framework,with,r?c,our framework with r?c,0.7060717344284058
translation,68,197,results,our framework,gains,slight performance improvement,our framework gains slight performance improvement,0.708986759185791
translation,68,197,results,slight performance improvement,after using,same language -priors methods,slight performance improvement after using same language -priors methods,0.643508791923523
translation,68,197,results,r or c,has,our framework,r or c has our framework,0.5556597113609314
translation,68,197,results,results,compared with,r or c,results compared with r or c,0.7105702757835388
translation,68,203,results,cas + lxm +ssl + qtd,improves,performance,cas + lxm +ssl + qtd improves performance,0.7359490990638733
translation,68,203,results,performance,of,cas +lxm + qtd,performance of cas +lxm + qtd,0.5902048945426941
translation,68,203,results,cas +lxm + qtd,by,2.61 %,cas +lxm + qtd by 2.61 %,0.5698593854904175
translation,68,203,results,results,has,cas + lxm +ssl + qtd,results has cas + lxm +ssl + qtd,0.5435769557952881
translation,68,210,results,sar,performs,better,sar performs better,0.6744917035102844
translation,68,210,results,better,in,question answering,better in question answering,0.5112704634666443
translation,68,210,results,better,in,visual grounding,better in visual grounding,0.5433387160301208
translation,68,210,results,better,in,visual grounding,better in visual grounding,0.5433387160301208
translation,68,210,results,ssl,has,sar,ssl has sar,0.6738591194152832
translation,68,210,results,results,compared with,ssl,results compared with ssl,0.721958577632904
translation,69,141,ablation-analysis,semantic scores,beneficial for,commonsense question answering,semantic scores beneficial for commonsense question answering,0.6532439589500427
translation,69,141,ablation-analysis,ablation analysis,inferred that,semantic scores,ablation analysis inferred that semantic scores,0.6060584783554077
translation,69,188,experiments,cga and self - talk,leverage,lms,cga and self - talk leverage lms,0.7033728957176208
translation,69,188,experiments,lms,to generate,some plausible answers,lms to generate some plausible answers,0.6582022309303284
translation,69,130,hyperparameters,sample size k,of,voters,sample size k of voters,0.6051276326179504
translation,69,130,hyperparameters,voters,set to,500,voters set to 500,0.7006015777587891
translation,69,130,hyperparameters,hyperparameters,has,sample size k,hyperparameters has sample size k,0.5081765055656433
translation,69,9,model,set of plausible answers,with,"generative models ( e.g. , gpt - 2 )","set of plausible answers with generative models ( e.g. , gpt - 2 )",0.5510042905807495
translation,69,9,model,plausible answers,to select,correct choice,plausible answers to select correct choice,0.6677762866020203
translation,69,28,model,correct answers,in,multi-choice commonsense question answering,correct answers in multi-choice commonsense question answering,0.49507540464401245
translation,69,28,model,correct answers,in,unsupervised setting,correct answers in unsupervised setting,0.5010234713554382
translation,69,28,model,correct answers,in,unsupervised setting,correct answers in unsupervised setting,0.5010234713554382
translation,69,28,model,novel semantic - based question answering model,has,seqa,novel semantic - based question answering model has seqa,0.6100311279296875
translation,69,28,model,model,introduce,novel semantic - based question answering model,model introduce novel semantic - based question answering model,0.5625272393226624
translation,69,139,results,accuracy,Among,all the methods,accuracy Among all the methods,0.5755302309989929
translation,69,139,results,seqa,achieved,best performance,seqa achieved best performance,0.7561934590339661
translation,69,139,results,best performance,on,all the datasets,best performance on all the datasets,0.48682036995887756
translation,69,139,results,accuracy,has,seqa,accuracy has seqa,0.5969259142875671
translation,69,139,results,all the methods,has,seqa,all the methods has seqa,0.605110228061676
translation,69,139,results,results,has,accuracy,results has accuracy,0.5888755321502686
translation,69,140,results,best baselines,by,more than 10 points,best baselines by more than 10 points,0.5775347948074341
translation,69,140,results,sct and cosmosqa,has,seqa,sct and cosmosqa has seqa,0.6164413094520569
translation,69,140,results,seqa,has,outperformed,seqa has outperformed,0.6271044015884399
translation,69,140,results,outperformed,has,best baselines,outperformed has best baselines,0.6322875022888184
translation,69,140,results,results,Especially on,sct and cosmosqa,results Especially on sct and cosmosqa,0.6363897323608398
translation,69,142,results,pro -q,performed,better,pro -q performed better,0.28549924492836
translation,69,142,results,better,than,other baselines,better than other baselines,0.5905631184577942
translation,69,142,results,other baselines,on,copa,other baselines on copa,0.4992119371891022
translation,69,142,results,results,has,pro -q,results has pro -q,0.5785843729972839
translation,69,143,results,pro -q,lost,superiority,pro -q lost superiority,0.8088391423225403
translation,69,143,results,superiority,on,another three datasets,superiority on another three datasets,0.5138975381851196
translation,69,143,results,results,has,pro -q,results has pro -q,0.5785843729972839
translation,69,148,results,seqa,is,much more robust,seqa is much more robust,0.5866614580154419
translation,69,148,results,much more robust,than,all baselines,much more robust than all baselines,0.6133379340171814
translation,69,148,results,attack success rate,has,seqa,attack success rate has seqa,0.5491287708282471
translation,69,148,results,accuracy,has,seqa,accuracy has seqa,0.5969259142875671
translation,69,148,results,results,Considering,attack success rate,results Considering attack success rate,0.6832812428474426
translation,69,149,results,attack success rates,on,seqa,attack success rates on seqa,0.5434119701385498
translation,69,149,results,seqa,are,at least 39 points lower,seqa are at least 39 points lower,0.6078925728797913
translation,69,149,results,at least 39 points lower,than,pro-a,at least 39 points lower than pro-a,0.6329469680786133
translation,69,149,results,at least 39 points lower,than,cga,at least 39 points lower than cga,0.5804168581962585
translation,69,149,results,at least 39 points lower,than,self - talk,at least 39 points lower than self - talk,0.5556867718696594
translation,69,149,results,self - talk,on,all datasets,self - talk on all datasets,0.4866093695163727
translation,69,149,results,results,has,attack success rates,results has attack success rates,0.5443307757377625
translation,69,151,results,remarkably lower,than,mi - qa,remarkably lower than mi - qa,0.6416948437690735
translation,69,151,results,remarkably lower,than,pro - q,remarkably lower than pro - q,0.6340696215629578
translation,69,151,results,pro - q,in terms of,attack success rates,pro - q in terms of attack success rates,0.7048722505569458
translation,69,151,results,attack success rates,on,all datasets,attack success rates on all datasets,0.46396031975746155
translation,69,151,results,results,has,seqa,results has seqa,0.5399251580238342
translation,69,152,results,observation,is that,attack success rate,observation is that attack success rate,0.6386309862136841
translation,69,152,results,attack success rate,on,seqa,attack success rate on seqa,0.536726713180542
translation,69,152,results,seqa,on,cosmosqa,seqa on cosmosqa,0.5873297452926636
translation,69,152,results,results,has,observation,results has observation,0.5461027026176453
translation,69,168,results,mi - qa,provided,relatively stable predictions,mi - qa provided relatively stable predictions,0.6493862271308899
translation,69,168,results,relatively stable predictions,compared with,other baseline methods,relatively stable predictions compared with other baseline methods,0.6447691917419434
translation,69,168,results,results,observed,mi - qa,results observed mi - qa,0.651465117931366
translation,69,173,results,much more stable performance,as,answer lengths,much more stable performance as answer lengths,0.5532118678092957
translation,69,173,results,mi - qa,has,seqa,mi - qa has seqa,0.6869385242462158
translation,69,173,results,seqa,has,much more stable performance,seqa has much more stable performance,0.5815869569778442
translation,69,173,results,answer lengths,has,vary,answer lengths has vary,0.5922412276268005
translation,69,173,results,results,Compared with,mi - qa,results Compared with mi - qa,0.6612871885299683
translation,69,199,results,more powerful extractor,led to,higher accuracy,more powerful extractor led to higher accuracy,0.669405460357666
translation,69,199,results,higher accuracy,under,same settings,higher accuracy under same settings,0.6856371164321899
translation,69,199,results,same settings,of,language models,same settings of language models,0.607454776763916
translation,69,199,results,results,has,more powerful extractor,results has more powerful extractor,0.5680404305458069
translation,69,221,results,5.3 %,of,voters,5.3 % of voters,0.5915690064430237
translation,69,221,results,voters,strongly favoring,correct choices,voters strongly favoring correct choices,0.6794825196266174
translation,69,221,results,much less voters ( 1.2 % ),favoring,wrong ones,much less voters ( 1.2 % ) favoring wrong ones,0.6182202100753784
translation,70,150,ablation-analysis,em and f1,of,qa model,em and f1 of qa model,0.6308950185775757
translation,70,150,ablation-analysis,qa model,improved by,2.56 % and 1.69 %,qa model improved by 2.56 % and 1.69 %,0.700793981552124
translation,70,150,ablation-analysis,2.56 % and 1.69 %,when,"100,000 samples","2.56 % and 1.69 % when 100,000 samples",0.6573505401611328
translation,70,150,ablation-analysis,"100,000 samples",of,our generated data,"100,000 samples of our generated data",0.5769075751304626
translation,70,150,ablation-analysis,ablation analysis,has,em and f1,ablation analysis has em and f1,0.5304830074310303
translation,70,103,baselines,2,-,hop questions,2 - hop questions,0.6483219265937805
translation,70,103,baselines,hop questions,in,datasets,hop questions in datasets,0.5260683298110962
translation,70,103,baselines,"adamw ( loshchilov and hutter , 2017 )",used as,optimizer,"adamw ( loshchilov and hutter , 2017 ) used as optimizer",0.5889267921447754
translation,70,103,baselines,"adamw ( loshchilov and hutter , 2017 )",with,initial learning rate,"adamw ( loshchilov and hutter , 2017 ) with initial learning rate",0.5787009596824646
translation,70,103,baselines,"adamw ( loshchilov and hutter , 2017 )",with,adaptively decays,"adamw ( loshchilov and hutter , 2017 ) with adaptively decays",0.6595402956008911
translation,70,103,baselines,optimizer,with,initial learning rate,optimizer with initial learning rate,0.5991587042808533
translation,70,103,baselines,initial learning rate,set to be,6.25 ? 10 ?5,initial learning rate set to be 6.25 ? 10 ?5,0.6963956952095032
translation,70,103,baselines,adaptively decays,during,training,adaptively decays during training,0.7428228855133057
translation,70,103,baselines,2,has,hop questions,2 has hop questions,0.5714672803878784
translation,70,99,experimental-setup,context graph construction,use,coreference resolution toolkit,context graph construction use coreference resolution toolkit,0.5481046438217163
translation,70,99,experimental-setup,context graph construction,use,open information extraction toolkit,context graph construction use open information extraction toolkit,0.5707671046257019
translation,70,99,experimental-setup,coreference resolution toolkit,from,allennlp 1.0.0,coreference resolution toolkit from allennlp 1.0.0,0.5199413299560547
translation,70,99,experimental-setup,open information extraction toolkit,provided by,plasticity developer api,open information extraction toolkit provided by plasticity developer api,0.6437543630599976
translation,70,99,experimental-setup,experimental setup,For,context graph construction,experimental setup For context graph construction,0.5218874216079712
translation,70,133,experimental-setup,two off- the-shelf qa models,provided by,huggingface transformer library,two off- the-shelf qa models provided by huggingface transformer library,0.6754963994026184
translation,70,133,experimental-setup,two off- the-shelf qa models,fine-tuned on,"squad ( rajpurkar et al. , 2016 )","two off- the-shelf qa models fine-tuned on squad ( rajpurkar et al. , 2016 )",0.6901706457138062
translation,70,133,experimental-setup,experimental setup,utilize,two off- the-shelf qa models,experimental setup utilize two off- the-shelf qa models,0.5483270287513733
translation,70,104,experiments,dp - graph,use,released model and code,dp - graph use released model and code,0.6357617974281311
translation,70,104,experiments,released model and code,to perform,experiment,released model and code to perform experiment,0.7433966994285583
translation,70,6,model,question difficulty,as,number of inference steps,question difficulty as number of inference steps,0.5344375371932983
translation,70,6,model,model,redefine,question difficulty,model redefine question difficulty,0.7210254073143005
translation,70,7,model,novel framework,progressively increases,question difficulty,novel framework progressively increases question difficulty,0.7600429058074951
translation,70,7,model,question difficulty,through,step-bystep rewriting,question difficulty through step-bystep rewriting,0.6774929165840149
translation,70,7,model,step-bystep rewriting,guidance of,extracted reasoning chain,step-bystep rewriting guidance of extracted reasoning chain,0.6921730637550354
translation,70,7,model,model,propose,novel framework,model propose novel framework,0.720393180847168
translation,70,19,model,difficulties,of,generated questions,difficulties of generated questions,0.5481804013252258
translation,70,19,model,generated questions,through,step-bystep rewriting,generated questions through step-bystep rewriting,0.6956599354743958
translation,70,19,model,progressively increases,has,difficulties,progressively increases has difficulties,0.6169612407684326
translation,70,19,model,model,propose,highly - controllable qg framework,model propose highly - controllable qg framework,0.6734882593154907
translation,70,20,model,given raw text,into,context graph,given raw text into context graph,0.5599373579025269
translation,70,20,model,context graph,sample,answer,context graph sample answer,0.7063600420951843
translation,70,20,model,context graph,sample,reasoning chain,context graph sample reasoning chain,0.6563949584960938
translation,70,20,model,reasoning chain,for,generated question,reasoning chain for generated question,0.6486164927482605
translation,70,20,model,model,transform,given raw text,model transform given raw text,0.777999222278595
translation,70,111,results,ours 2 - hop and gpt2,perform,consistently better,ours 2 - hop and gpt2 perform consistently better,0.6004475951194763
translation,70,111,results,consistently better,than,others,consistently better than others,0.6075058579444885
translation,70,119,results,ours 2 - hop,performs,consistently better,ours 2 - hop performs consistently better,0.593880295753479
translation,70,119,results,consistently better,than,dp - graph and gpt2,consistently better than dp - graph and gpt2,0.6098341345787048
translation,70,119,results,dp - graph and gpt2,across,all metrics,dp - graph and gpt2 across all metrics,0.7125052213668823
translation,70,119,results,results,see that,ours 2 - hop,results see that ours 2 - hop,0.6464442014694214
translation,70,120,results,our method,performs,especially well,our method performs especially well,0.6311126947402954
translation,70,120,results,especially well,in terms of,concise,especially well in terms of concise,0.7631173133850098
translation,70,120,results,results,has,our method,results has our method,0.5589964985847473
translation,70,123,results,questions,generated by,two baselines,questions generated by two baselines,0.6321675181388855
translation,70,123,results,results,find that,questions,results find that questions,0.6007168292999268
translation,70,124,results,- hop,performs,well,- hop performs well,0.7012552618980408
translation,70,124,results,well,in terms of,answerable and answer matching,well in terms of answerable and answer matching,0.6816987991333008
translation,70,137,results,questions,generated by,ours 2 - hop,questions generated by ours 2 - hop,0.6926740407943726
translation,70,137,results,questions,generated by,ours 1 - hop,questions generated by ours 1 - hop,0.6728601455688477
translation,70,137,results,ours 2 - hop,are,more difficult,ours 2 - hop are more difficult,0.5772998929023743
translation,70,137,results,more difficult,than,ours 1 - hop,more difficult than ours 1 - hop,0.6295986771583557
translation,70,137,results,results,see that,questions,results see that questions,0.6163401007652283
translation,70,138,results,more scattered mix,of,1 - hop and 2 - hop questions,more scattered mix of 1 - hop and 2 - hop questions,0.5876826047897339
translation,70,138,results,performances,on,dp - graph and gpt2,performances on dp - graph and gpt2,0.6056866645812988
translation,70,138,results,dp - graph and gpt2,between,ours 1 - hop,dp - graph and gpt2 between ours 1 - hop,0.6469903588294983
translation,70,138,results,dp - graph and gpt2,between,ours 2 - hop,dp - graph and gpt2 between ours 2 - hop,0.6468209624290466
translation,70,138,results,more scattered mix,has,performances,more scattered mix has performances,0.6109840273857117
translation,70,138,results,1 - hop and 2 - hop questions,has,performances,1 - hop and 2 - hop questions has performances,0.5998473763465881
translation,70,138,results,results,with,more scattered mix,results with more scattered mix,0.6908243894577026
translation,70,146,results,results,compares,qa performance,results compares qa performance,0.7512206435203552
translation,70,147,results,our method,achieves,better performance,our method achieves better performance,0.6458921432495117
translation,70,147,results,better performance,than,gpt2,better performance than gpt2,0.5890967845916748
translation,70,148,results,performance boost,achieved by,our generated data,performance boost achieved by our generated data,0.6610296964645386
translation,70,148,results,more significant and obviously better,than,gpt2,more significant and obviously better than gpt2,0.6198868155479431
translation,70,148,results,low-resource setting,has,performance boost,low-resource setting has performance boost,0.5674311518669128
translation,70,149,results,performance,of,qa model,performance of qa model,0.6555312275886536
translation,70,149,results,steadily improves,when,training dataset,steadily improves when training dataset,0.6676890850067139
translation,70,149,results,training dataset,augmented with,more data,training dataset augmented with more data,0.6994600296020508
translation,70,149,results,qa model,has,steadily improves,qa model has steadily improves,0.6256829500198364
translation,70,149,results,results,has,performance,results has performance,0.5972660779953003
translation,70,156,results,training data,that only contains,1 - hop and 2 - hop questions,training data that only contains 1 - hop and 2 - hop questions,0.6758180856704712
translation,70,156,results,our framework,able to generate,some high-quality 3 - hop questions,our framework able to generate some high-quality 3 - hop questions,0.7090873122215271
translation,70,156,results,training data,has,our framework,training data has our framework,0.5705772042274475
translation,70,156,results,1 - hop and 2 - hop questions,has,our framework,1 - hop and 2 - hop questions has our framework,0.5890462398529053
translation,70,156,results,results,With,training data,results With training data,0.5987916588783264
translation,71,159,ablation-analysis,contribution,of,nli component,contribution of nli component,0.5838293433189392
translation,71,159,ablation-analysis,nli component,resulted in,even higher gains,nli component resulted in even higher gains,0.6817415356636047
translation,71,159,ablation-analysis,even higher gains,in terms of,correlation,even higher gains in terms of correlation,0.7883266806602478
translation,71,159,ablation-analysis,even higher gains,showing,benefit,even higher gains showing benefit,0.7275915741920471
translation,71,159,ablation-analysis,correlation,in comparison to,wow experiments,correlation in comparison to wow experiments,0.7151641845703125
translation,71,159,ablation-analysis,benefit,of using,intricate span comparison method,benefit of using intricate span comparison method,0.7171440124511719
translation,71,159,ablation-analysis,ablation analysis,has,contribution,ablation analysis has contribution,0.5164179801940918
translation,71,22,baselines,question answering ( qa ),for,dialogue generation evaluation,question answering ( qa ) for dialogue generation evaluation,0.5808135271072388
translation,71,25,baselines,novel comparison method,using,natural language inference models ( nli ;,novel comparison method using natural language inference models ( nli ;,0.6847227215766907
translation,71,25,baselines,novel comparison method,more robust to,lexical variability,novel comparison method more robust to lexical variability,0.7168852686882019
translation,71,105,baselines,dodecadialogue,is,multi-task model,dodecadialogue is multi-task model,0.5733237862586975
translation,71,105,baselines,multi-task model,fine-tuned on,wow,multi-task model fine-tuned on wow,0.7665631175041199
translation,71,105,baselines,wow,in,dodecadialogue benchmark,wow in dodecadialogue benchmark,0.540967583656311
translation,71,105,baselines,baselines,has,dodecadialogue,baselines has dodecadialogue,0.5755103230476379
translation,71,4,results,results,has,neural knowledge- grounded generative,results has neural knowledge- grounded generative,0.5485879182815552
translation,71,29,results,q 2,reaches,significantly higher correlations,q 2 reaches significantly higher correlations,0.7267098426818848
translation,71,29,results,significantly higher correlations,with,human judgments,significantly higher correlations with human judgments,0.6207250356674194
translation,71,29,results,human judgments,on,all datasets,human judgments on all datasets,0.4510139226913452
translation,71,29,results,results,has,q 2,results has q 2,0.5348592400550842
translation,71,133,results,inconsistent data,for,all baselines,inconsistent data for all baselines,0.6211280226707458
translation,71,169,results,q 2,performs,better,q 2 performs better,0.7394158244132996
translation,71,169,results,better,than,end-to- end nli baselines,better than end-to- end nli baselines,0.5865022540092468
translation,71,169,results,results,has,q 2,results has q 2,0.5348592400550842
translation,71,171,results,three datasets,demonstrate,"q 2 's zero-shot , reference - response-free capability","three datasets demonstrate q 2 's zero-shot , reference - response-free capability",0.6337693333625793
translation,71,171,results,"q 2 's zero-shot , reference - response-free capability",to generalize to,various dialogue tasks,"q 2 's zero-shot , reference - response-free capability to generalize to various dialogue tasks",0.7021327018737793
translation,71,171,results,results,on,three datasets,results on three datasets,0.49755188822746277
translation,71,178,results,correlations,with,human judgments,correlations with human judgments,0.6459334492683411
translation,71,178,results,barely influenced,showing,robustness,barely influenced showing robustness,0.7207245230674744
translation,71,181,results,smaller qg model,results in,lower q 2 scores,smaller qg model results in lower q 2 scores,0.6594626307487488
translation,71,181,results,lower q 2 scores,for,all data splits,lower q 2 scores for all data splits,0.6340651512145996
translation,71,182,results,higher q 2 scores,in,all cases,higher q 2 scores in all cases,0.5525476932525635
translation,71,182,results,smaller qa model,has,opposite outcome,smaller qa model has opposite outcome,0.5973864197731018
translation,71,182,results,opposite outcome,has,higher q 2 scores,opposite outcome has higher q 2 scores,0.5658314824104309
translation,71,182,results,results,using,smaller qa model,results using smaller qa model,0.7063326239585876
translation,71,231,results,qg - qa based methods,showed,higher correlations,qg - qa based methods showed higher correlations,0.7131431698799133
translation,71,231,results,higher correlations,with,human judgments,higher correlations with human judgments,0.6053839325904846
translation,71,231,results,human judgments,of,factual consistency,human judgments of factual consistency,0.532204806804657
translation,71,231,results,abstractive summaries,has,qg - qa based methods,abstractive summaries has qg - qa based methods,0.5432571172714233
translation,72,207,ablation-analysis,average drop,of,9.3 %,average drop of 9.3 %,0.5640653967857361
translation,72,207,ablation-analysis,9.3 %,in,f 1 scores,9.3 % in f 1 scores,0.588631808757782
translation,72,207,ablation-analysis,f 1 scores,for,both our model variants,f 1 scores for both our model variants,0.623725175857544
translation,72,207,ablation-analysis,ablation analysis,has,average drop,ablation analysis has average drop,0.5845745801925659
translation,72,208,ablation-analysis,pos and dep features,observe,average drop,pos and dep features observe average drop,0.5755753517150879
translation,72,208,ablation-analysis,average drop,of,2.3 %,average drop of 2.3 %,0.5649719834327698
translation,72,208,ablation-analysis,2.3 %,in,f 1 scores,2.3 % in f 1 scores,0.5689826607704163
translation,72,161,baselines,jet,is,first end-to - end approach,jet is first end-to - end approach,0.6047693490982056
translation,72,161,baselines,first end-to - end approach,for,task of aste,first end-to - end approach for task of aste,0.6369946599006653
translation,72,161,baselines,task of aste,leverages,novel position - aware tagging scheme,task of aste leverages novel position - aware tagging scheme,0.6954044103622437
translation,72,161,baselines,baselines,has,jet,baselines has jet,0.6161085963249207
translation,72,175,baselines,gts,uses,double embeddings,gts uses double embeddings,0.6561480164527893
translation,72,175,baselines,double embeddings,has,general glove vectors,double embeddings has general glove vectors,0.5692175626754761
translation,72,175,baselines,baselines,has,gts,baselines has gts,0.5884730219841003
translation,72,145,experimental-setup,word embeddings,using,pre-trained 300 - dim,word embeddings using pre-trained 300 - dim,0.5565847158432007
translation,72,145,experimental-setup,non-bert experiments,has,word embeddings,non-bert experiments has word embeddings,0.5418511629104614
translation,72,145,experimental-setup,experimental setup,For,non-bert experiments,experimental setup For non-bert experiments,0.631851077079773
translation,72,146,experimental-setup,d w,set to,300,d w set to 300,0.6955188512802124
translation,72,146,experimental-setup,experimental setup,has,glove vectors,experimental setup has glove vectors,0.5314568281173706
translation,72,147,experimental-setup,d pos and d dep,set to,50,d pos and d dep set to 50,0.7610130310058594
translation,72,147,experimental-setup,experimental setup,has,d pos and d dep,experimental setup has d pos and d dep,0.5663369297981262
translation,72,148,experimental-setup,d h,set to,300,d h set to 300,0.7320505976676941
translation,72,148,experimental-setup,hidden state dimensions,of,both the lstms ( backward and forward ),hidden state dimensions of both the lstms ( backward and forward ),0.5483797192573547
translation,72,148,experimental-setup,hidden state dimensions,of,bi-lstm - based encoder,hidden state dimensions of bi-lstm - based encoder,0.5531929135322571
translation,72,148,experimental-setup,hidden state dimensions,of,bi-lstm - based encoder,hidden state dimensions of bi-lstm - based encoder,0.5531929135322571
translation,72,148,experimental-setup,hidden state dimensions,set to,150,hidden state dimensions set to 150,0.7180522680282593
translation,72,148,experimental-setup,both the lstms ( backward and forward ),of,bi-lstm - based encoder,both the lstms ( backward and forward ) of bi-lstm - based encoder,0.5629065036773682
translation,72,148,experimental-setup,bi-lstm - based encoder,set to,150,bi-lstm - based encoder set to 150,0.6509420275688171
translation,72,148,experimental-setup,experimental setup,has,d h,experimental setup has d h,0.5988399982452393
translation,72,149,experimental-setup,d p,set to,300,d p set to 300,0.6909915208816528
translation,72,149,experimental-setup,experimental setup,has,d p,experimental setup has d p,0.5736496448516846
translation,72,150,experimental-setup,uncased version,of,"pre-trained bert - base ( devlin et al. , 2019 )","uncased version of pre-trained bert - base ( devlin et al. , 2019 )",0.5130608677864075
translation,72,150,experimental-setup,fine-tuned,to encode,each sentence,fine-tuned to encode each sentence,0.7240579724311829
translation,72,150,experimental-setup,bert experiments,has,uncased version,bert experiments has uncased version,0.5623969435691833
translation,72,150,experimental-setup,experimental setup,For,bert experiments,experimental setup For bert experiments,0.6180595755577087
translation,72,151,experimental-setup,model variants,trained,end-to-end,model variants trained end-to-end,0.7565562129020691
translation,72,151,experimental-setup,end-to-end,on,tesla p100 - pcie 16gb gpu,end-to-end on tesla p100 - pcie 16gb gpu,0.5071946978569031
translation,72,151,experimental-setup,tesla p100 - pcie 16gb gpu,with,adam optimizer,tesla p100 - pcie 16gb gpu with adam optimizer,0.6110146045684814
translation,72,151,experimental-setup,adam optimizer,has,learning rate,adam optimizer has learning rate,0.5073457360267639
translation,72,151,experimental-setup,learning rate,has,10 ?3,learning rate has 10 ?3,0.6000961661338806
translation,72,151,experimental-setup,weight decay,has,10 ?5,weight decay has 10 ?5,0.5990263223648071
translation,72,151,experimental-setup,experimental setup,has,model variants,experimental setup has model variants,0.5390467047691345
translation,72,152,experimental-setup,dropout rate,of,0.5,dropout rate of 0.5,0.6072384119033813
translation,72,152,experimental-setup,dropout rate,applied on,embeddings,dropout rate applied on embeddings,0.6108910441398621
translation,72,152,experimental-setup,0.5,applied on,embeddings,0.5 applied on embeddings,0.6762612462043762
translation,72,152,experimental-setup,embeddings,to avoid,overfitting,embeddings to avoid overfitting,0.6304114460945129
translation,72,152,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,72,8,model,tagging -free solution,for,task,tagging -free solution for task,0.6070723533630371
translation,72,8,model,model,present,tagging -free solution,model present tagging -free solution,0.689847469329834
translation,72,9,model,encoder-decoder architecture,with,pointer network - based decoding framework,encoder-decoder architecture with pointer network - based decoding framework,0.6420943140983582
translation,72,9,model,entire opinion triplet,at,each time step,entire opinion triplet at each time step,0.5430303812026978
translation,72,9,model,model,adapt,encoder-decoder architecture,model adapt encoder-decoder architecture,0.7552173137664795
translation,72,10,model,interactions,between,aspects and opinions,interactions between aspects and opinions,0.674423098564148
translation,72,10,model,interactions,effectively captured by,decoder,interactions effectively captured by decoder,0.7831124663352966
translation,72,10,model,aspects and opinions,effectively captured by,decoder,aspects and opinions effectively captured by decoder,0.7726924419403076
translation,72,10,model,decoder,by considering,entire detected spans,decoder by considering entire detected spans,0.7307143211364746
translation,72,10,model,entire detected spans,while predicting,connecting sentiment,entire detected spans while predicting connecting sentiment,0.7736376523971558
translation,72,10,model,model,has,interactions,model has interactions,0.5665457248687744
translation,72,46,model,semantic role labeling,propose,paste,semantic role labeling propose paste,0.6021109223365784
translation,72,46,model,model,propose,paste,model propose paste,0.6608495712280273
translation,72,47,model,pointer network,effectively captures,aspect-opinion interdependence,pointer network effectively captures aspect-opinion interdependence,0.7575364112854004
translation,72,47,model,aspect-opinion interdependence,while detecting,respective spans,aspect-opinion interdependence while detecting respective spans,0.7337833046913147
translation,72,47,model,model,has,pointer network,model has pointer network,0.5618420243263245
translation,72,54,model,architecture,exploits,aspect-opinion interdependence,architecture exploits aspect-opinion interdependence,0.748745322227478
translation,72,54,model,architecture,models,span-level interactions,architecture models span-level interactions,0.784639835357666
translation,72,54,model,architecture,propose,position - based scheme,architecture propose position - based scheme,0.6931284666061401
translation,72,54,model,aspect-opinion interdependence,during,span detection process,aspect-opinion interdependence during span detection process,0.6107187271118164
translation,72,54,model,span-level interactions,for,sentiment prediction,span-level interactions for sentiment prediction,0.5939781665802002
translation,72,54,model,span-level interactions,propose,position - based scheme,span-level interactions propose position - based scheme,0.6662407517433167
translation,72,54,model,position - based scheme,to uniformly represent,opinion triplet,position - based scheme to uniformly represent opinion triplet,0.7196758389472961
translation,72,54,model,model,propose,position - based scheme,model propose position - based scheme,0.68131422996521
translation,72,179,results,both our variants,perform,comparably,both our variants perform comparably,0.5851083397865295
translation,72,179,results,substantially outperform,has,all the non-bert baselines,substantially outperform has all the non-bert baselines,0.5952956676483154
translation,72,180,results,laptop,achieve,13.1 % f 1 gains,laptop achieve 13.1 % f 1 gains,0.6625638008117676
translation,72,180,results,laptop,achieve,2.2 % f 1 gains,laptop achieve 2.2 % f 1 gains,0.6509658694267273
translation,72,180,results,laptop,on,restaurant,laptop on restaurant,0.541511595249176
translation,72,180,results,laptop,obtain,2.2 % f 1 gains,laptop obtain 2.2 % f 1 gains,0.581407904624939
translation,72,180,results,13.1 % f 1 gains,over,ote - mtl,13.1 % f 1 gains over ote - mtl,0.6505237817764282
translation,72,180,results,13.1 % f 1 gains,over,gts - bilstm,13.1 % f 1 gains over gts - bilstm,0.6575813293457031
translation,72,180,results,restaurant,obtain,2.2 % f 1 gains,restaurant obtain 2.2 % f 1 gains,0.6150373220443726
translation,72,180,results,2.2 % f 1 gains,over,gts - bilstm,2.2 % f 1 gains over gts - bilstm,0.6599964499473572
translation,72,180,results,results,On,laptop,results On laptop,0.5542846322059631
translation,72,180,results,results,on,restaurant,results on restaurant,0.5353488922119141
translation,72,182,results,better performance,attributed to,better recall scores,better performance attributed to better recall scores,0.6396190524101257
translation,72,182,results,better recall scores,with,around 15.6 % recall gains,better recall scores with around 15.6 % recall gains,0.6268534660339355
translation,72,182,results,around 15.6 % recall gains,over,respective strongest baselines,around 15.6 % recall gains over respective strongest baselines,0.6757492423057556
translation,72,182,results,respective strongest baselines,on,laptop and restaurant datasets,respective strongest baselines on laptop and restaurant datasets,0.49875789880752563
translation,72,182,results,results,has,better performance,results has better performance,0.6032086610794067
translation,72,184,results,jet,on,all the datasets,jet on all the datasets,0.5756558179855347
translation,72,184,results,bert,has,comfortably outperform,bert has comfortably outperform,0.6442790627479553
translation,72,184,results,comfortably outperform,has,jet,comfortably outperform has jet,0.6147428750991821
translation,72,184,results,results,With,bert,results With bert,0.4931848347187042
translation,72,185,results,gts - bert,on,laptop,gts - bert on laptop,0.5296063423156738
translation,72,185,results,outperforms,on,all the restaurant datasets,outperforms on all the restaurant datasets,0.5097989439964294
translation,72,194,results,baselines,on,laptop and restaurant datasets,baselines on laptop and restaurant datasets,0.4765833020210266
translation,72,194,results,our core architecture ( w/ o bert ),has,paste,our core architecture ( w/ o bert ) has paste,0.5990115404129028
translation,72,194,results,paste,has,consistently outperforms,paste has consistently outperforms,0.6407350897789001
translation,72,194,results,consistently outperforms,has,baselines,consistently outperforms has baselines,0.6030921339988708
translation,72,195,results,paste,better than,previous tagging - based approaches,paste better than previous tagging - based approaches,0.6442617774009705
translation,72,195,results,aspect-opinion span-level interdependence,during,extraction process,aspect-opinion span-level interdependence during extraction process,0.59868323802948
translation,72,203,results,our baselines,on,aspect and opinion span detection sub-tasks,our baselines on aspect and opinion span detection sub-tasks,0.49138176441192627
translation,72,203,results,substantially outperform,has,our baselines,substantially outperform has our baselines,0.5874319076538086
translation,72,204,results,outperformed,when it comes to,sentiment detection,outperformed when it comes to sentiment detection,0.70235276222229
translation,73,73,baselines,guided generation ( gg ),extends,free generation,guided generation ( gg ) extends free generation,0.7234107255935669
translation,73,73,baselines,free generation,by,answer prefixes,free generation by answer prefixes,0.5713486671447754
translation,73,73,baselines,answer prefixes,prevent,lms from evading,answer prefixes prevent lms from evading,0.6757415533065796
translation,73,73,baselines,lms from evading,has,answering,lms from evading has answering,0.6031532883644104
translation,73,73,baselines,baselines,has,guided generation ( gg ),baselines has guided generation ( gg ),0.5953613519668579
translation,73,64,experimental-setup,underlying structured csk,stored in,postgresql database,underlying structured csk stored in postgresql database,0.6678591370582581
translation,73,64,experimental-setup,underlying structured csk,for,qa part,underlying structured csk for qa part,0.6425343155860901
translation,73,64,experimental-setup,statements,of,all cskbs,statements of all cskbs,0.6194440126419067
translation,73,64,experimental-setup,indexed and queried,via,apache solr,indexed and queried via apache solr,0.6586050391197205
translation,73,64,experimental-setup,indexed and queried,for,fast text - based querying,indexed and queried for fast text - based querying,0.6144208908081055
translation,73,64,experimental-setup,qa part,has,statements,qa part has statements,0.6166188716888428
translation,73,64,experimental-setup,experimental setup,for,qa part,experimental setup for qa part,0.5947878956794739
translation,73,64,experimental-setup,experimental setup,has,underlying structured csk,experimental setup has underlying structured csk,0.5268519520759583
translation,73,65,experimental-setup,components,deployed on,virtual machine,components deployed on virtual machine,0.657637894153595
translation,73,65,experimental-setup,components,8 GB of,ram,components 8 GB of ram,0.7450377941131592
translation,73,65,experimental-setup,virtual machine,with access to,4 virtual cpus,virtual machine with access to 4 virtual cpus,0.6484338045120239
translation,73,65,experimental-setup,virtual machine,8 GB of,ram,virtual machine 8 GB of ram,0.6778908967971802
translation,73,65,experimental-setup,experimental setup,has,components,experimental setup has components,0.44141441583633423
translation,73,6,model,web portal,allows,users,web portal allows users,0.6746704578399658
translation,73,6,model,users,to understand,construction process,users to understand construction process,0.664419412612915
translation,73,6,model,construction process,explore,content,construction process explore content,0.7034271955490112
translation,73,6,model,model,present,web portal,model present web portal,0.6420019268989563
translation,74,230,ablation-analysis,ablation analysis,has,ablation study,ablation analysis has ablation study,0.5261484980583191
translation,74,231,ablation-analysis,language mask,are,important,language mask are important,0.6073449850082397
translation,74,231,ablation-analysis,important,especially for,text form,important especially for text form,0.7157337069511414
translation,74,231,ablation-analysis,ablation analysis,see that,score truncation,ablation analysis see that score truncation,0.6536193490028381
translation,74,193,baselines,baselines kvmemnn,uses,keyvalue memory,baselines kvmemnn uses keyvalue memory,0.5694984793663025
translation,74,193,baselines,keyvalue memory,to store,knowledge,keyvalue memory to store knowledge,0.7558295726776123
translation,74,193,baselines,keyvalue memory,conducts,multi-hop reasoning,keyvalue memory conducts multi-hop reasoning,0.6284207105636597
translation,74,193,baselines,multi-hop reasoning,by iteratively reading,memory,multi-hop reasoning by iteratively reading memory,0.7579796314239502
translation,74,193,baselines,baselines,has,baselines kvmemnn,baselines has baselines kvmemnn,0.5739250183105469
translation,74,194,baselines,vrn,learns,reasoning path,vrn learns reasoning path,0.6854026913642883
translation,74,194,baselines,reasoning path,via,reinforcement learning,reasoning path via reinforcement learning,0.638610303401947
translation,74,194,baselines,baselines,has,vrn,baselines has vrn,0.5917487740516663
translation,74,196,baselines,"srn ( qiu et al. , 2020 )",improves,vrn,"srn ( qiu et al. , 2020 ) improves vrn",0.6671636700630188
translation,74,196,baselines,vrn,by,beam search,vrn by beam search,0.5873036980628967
translation,74,196,baselines,vrn,by,reward shaping strategy,vrn by reward shaping strategy,0.5405299663543701
translation,74,196,baselines,baselines,has,"srn ( qiu et al. , 2020 )","baselines has srn ( qiu et al. , 2020 )",0.5323981046676636
translation,74,197,baselines,graftnet,extracts,questionspecific subgraph,graftnet extracts questionspecific subgraph,0.6538429260253906
translation,74,197,baselines,graftnet,uses,graph neural networks,graftnet uses graph neural networks,0.5913573503494263
translation,74,197,baselines,questionspecific subgraph,from,entire relation graph,questionspecific subgraph from entire relation graph,0.5291445851325989
translation,74,197,baselines,entire relation graph,with,heuristics,entire relation graph with heuristics,0.6599835157394409
translation,74,197,baselines,graph neural networks,to infer,answer,graph neural networks to infer answer,0.7901326417922974
translation,74,197,baselines,baselines,has,graftnet,baselines has graftnet,0.5920066237449646
translation,74,198,baselines,pullnet,improves,graftnet,pullnet improves graftnet,0.7118105888366699
translation,74,198,baselines,subgraph,with,graph cnn,subgraph with graph cnn,0.6303057074546814
translation,74,198,baselines,baselines,has,pullnet,baselines has pullnet,0.5593709945678711
translation,74,199,baselines,reifkb,proposes,scalable implementation,reifkb proposes scalable implementation,0.6884804964065552
translation,74,199,baselines,scalable implementation,of,probability transfer,scalable implementation of probability transfer,0.5906579494476318
translation,74,199,baselines,probability transfer,over,largescale knowledge graph,probability transfer over largescale knowledge graph,0.6535418629646301
translation,74,199,baselines,largescale knowledge graph,of,label form,largescale knowledge graph of label form,0.548828661441803
translation,74,199,baselines,baselines,has,reifkb,baselines has reifkb,0.5836384892463684
translation,74,201,baselines,"embedkgqa ( saxena et al. , 2020 )",takes,kgqa,"embedkgqa ( saxena et al. , 2020 ) takes kgqa",0.6177703738212585
translation,74,201,baselines,"embedkgqa ( saxena et al. , 2020 )",incorporates,knowledge graph embeddings,"embedkgqa ( saxena et al. , 2020 ) incorporates knowledge graph embeddings",0.6925591826438904
translation,74,201,baselines,kgqa,as,link prediction task,kgqa as link prediction task,0.5008004903793335
translation,74,201,baselines,knowledge graph embeddings,to help,predict,knowledge graph embeddings to help predict,0.6645195484161377
translation,74,201,baselines,predict,has,answer,predict has answer,0.6200166940689087
translation,74,201,baselines,baselines,has,"embedkgqa ( saxena et al. , 2020 )","baselines has embedkgqa ( saxena et al. , 2020 )",0.5558923482894897
translation,74,49,experimental-setup,label form,use,"metaqa ( zhang et al. , 2017 )","label form use metaqa ( zhang et al. , 2017 )",0.6166272163391113
translation,74,49,experimental-setup,label form,use,"webqsp ( yih et al. , 2016","label form use webqsp ( yih et al. , 2016",0.6218966841697693
translation,74,49,experimental-setup,label form,use,"compwebq ( talmor and berant , 2018 )","label form use compwebq ( talmor and berant , 2018 )",0.649165153503418
translation,74,49,experimental-setup,experimental setup,For,label form,experimental setup For label form,0.6225824356079102
translation,74,206,experimental-setup,bi-directional gru,as,question encoder,bi-directional gru as question encoder,0.5538662672042847
translation,74,206,experimental-setup,bi-directional gru,set,hidden dimension,bi-directional gru set hidden dimension,0.6396196484565735
translation,74,206,experimental-setup,hidden dimension,as,1024,hidden dimension as 1024,0.5783396363258362
translation,74,206,experimental-setup,experimental setup,used,bi-directional gru,experimental setup used bi-directional gru,0.5563506484031677
translation,74,206,experimental-setup,experimental setup,set,hidden dimension,experimental setup set hidden dimension,0.6768732070922852
translation,74,209,experimental-setup,text form,used,another bi-directional gru,text form used another bi-directional gru,0.6119488477706909
translation,74,209,experimental-setup,another bi-directional gru,as,relation encoder,another bi-directional gru as relation encoder,0.5741605162620544
translation,74,209,experimental-setup,experimental setup,For,text form,experimental setup For text form,0.6251624226570129
translation,74,213,experimental-setup,model,optimized using,"radam ( liu et al. , 2020 )","model optimized using radam ( liu et al. , 2020 )",0.6956183910369873
translation,74,213,experimental-setup,"radam ( liu et al. , 2020 )",with,learning rate,"radam ( liu et al. , 2020 ) with learning rate",0.6019971966743469
translation,74,213,experimental-setup,0.001,for,20 epochs,0.001 for 20 epochs,0.5998625159263611
translation,74,213,experimental-setup,several hours,for,label form,several hours for label form,0.6833638548851013
translation,74,213,experimental-setup,text form,on,single gpu,text form on single gpu,0.5542248487472534
translation,74,213,experimental-setup,single gpu,of,nvidia 1080 ti,single gpu of nvidia 1080 ti,0.5507904887199402
translation,74,213,experimental-setup,learning rate,has,0.001,learning rate has 0.001,0.5318801999092102
translation,74,213,experimental-setup,experimental setup,optimized using,"radam ( liu et al. , 2020 )","experimental setup optimized using radam ( liu et al. , 2020 )",0.6670495867729187
translation,74,213,experimental-setup,experimental setup,has,model,experimental setup has model,0.5338840484619141
translation,74,214,experimental-setup,webqsp and comp-webq,set,step number t = 2,webqsp and comp-webq set step number t = 2,0.7005202174186707
translation,74,215,experimental-setup,"pretrained bert ( devlin et al. , 2018 )",as,question encoder,"pretrained bert ( devlin et al. , 2018 ) as question encoder",0.42291954159736633
translation,74,215,experimental-setup,"pretrained bert ( devlin et al. , 2018 )",finetuned,parameters,"pretrained bert ( devlin et al. , 2018 ) finetuned parameters",0.7269430756568909
translation,74,215,experimental-setup,parameters,on,our task,parameters on our task,0.5473710894584656
translation,74,215,experimental-setup,experimental setup,used,"pretrained bert ( devlin et al. , 2018 )","experimental setup used pretrained bert ( devlin et al. , 2018 )",0.574532687664032
translation,74,50,experiments,transfernet,achieves,100 % accuracy,transfernet achieves 100 % accuracy,0.6973551511764526
translation,74,50,experiments,100 % accuracy,in,2 - hop and 3 - hop questions,100 % accuracy in 2 - hop and 3 - hop questions,0.5466217398643494
translation,74,50,experiments,2 - hop and 3 - hop questions,of,metaqa,2 - hop and 3 - hop questions of metaqa,0.6122384667396545
translation,74,52,experiments,text form,construct,relation graph,text form construct relation graph,0.7049930691719055
translation,74,52,experiments,relation graph,of,metaqa,relation graph of metaqa,0.5537633895874023
translation,74,52,experiments,metaqa,from,wikimovies corpus,metaqa from wikimovies corpus,0.5342431664466858
translation,74,253,experiments,outperforms,has,other state - of - the - art models,outperforms has other state - of - the - art models,0.538033127784729
translation,74,253,experiments,other state - of - the - art models,has,significantly,other state - of - the - art models has significantly,0.529333770275116
translation,74,264,experiments,transfernet,achieves,100 % accuracy,transfernet achieves 100 % accuracy,0.6973551511764526
translation,74,264,experiments,100 % accuracy,in,2 - hop and 3 - hop questions,100 % accuracy in 2 - hop and 3 - hop questions,0.5466217398643494
translation,74,264,experiments,2 - hop and 3 - hop questions,of,metaqa,2 - hop and 3 - hop questions of metaqa,0.6122384667396545
translation,74,281,experiments,transfernet,achieves,71.4 % accuracy,transfernet achieves 71.4 % accuracy,0.6480115056037903
translation,74,281,experiments,transfernet,beating,previous state - of - the - art models ( 68.1 % ),transfernet beating previous state - of - the - art models ( 68.1 % ),0.5289602279663086
translation,74,281,experiments,71.4 % accuracy,beating,previous state - of - the - art models ( 68.1 % ),71.4 % accuracy beating previous state - of - the - art models ( 68.1 % ),0.5141848921775818
translation,74,281,experiments,previous state - of - the - art models ( 68.1 % ),by,large margin,previous state - of - the - art models ( 68.1 % ) by large margin,0.5160617232322693
translation,74,8,model,effective and transparent model,for,multi-hop qa,effective and transparent model for multi-hop qa,0.6482253074645996
translation,74,8,model,relations,in,unified framework,relations in unified framework,0.555180013179779
translation,74,8,model,trans-fernet,has,effective and transparent model,trans-fernet has effective and transparent model,0.6103920936584473
translation,74,8,model,model,propose,trans-fernet,model propose trans-fernet,0.7050167918205261
translation,74,9,model,trans- fernet,jumps across,entities,trans- fernet jumps across entities,0.7624783515930176
translation,74,9,model,entities,at,multiple steps,entities at multiple steps,0.5686898231506348
translation,74,9,model,model,has,trans- fernet,model has trans- fernet,0.6522637009620667
translation,74,10,model,each step,attends to,different parts,each step attends to different parts,0.7048994302749634
translation,74,10,model,each step,computes,activated scores,each step computes activated scores,0.7378795146942139
translation,74,10,model,each step,transfer,previous entity scores,each step transfer previous entity scores,0.6775698065757751
translation,74,10,model,different parts,of,question,different parts of question,0.6269335150718689
translation,74,10,model,activated scores,for,relations,activated scores for relations,0.6709414720535278
translation,74,10,model,activated scores,in,differentiable way,activated scores in differentiable way,0.5581860542297363
translation,74,10,model,previous entity scores,along,activated relations,previous entity scores along activated relations,0.5785875916481018
translation,74,10,model,activated relations,in,differentiable way,activated relations in differentiable way,0.5405966639518738
translation,74,10,model,model,At,each step,model At each step,0.5607273578643799
translation,74,35,model,novel model,for,multi-hop qa,novel model for multi-hop qa,0.6355125308036804
translation,74,35,model,model,propose,novel model,model propose novel model,0.6891457438468933
translation,74,251,model,effective and transparent framework,for,multi-hop qa,effective and transparent framework for multi-hop qa,0.6326765418052673
translation,74,251,model,multi-hop qa,over,knowledge graph,multi-hop qa over knowledge graph,0.6333998441696167
translation,74,251,model,multi-hop qa,over,text -formed relation graph,multi-hop qa over text -formed relation graph,0.6504917740821838
translation,74,251,model,transfernet,has,effective and transparent framework,transfernet has effective and transparent framework,0.5848689079284668
translation,74,251,model,model,proposed,transfernet,model proposed transfernet,0.7806807160377502
translation,74,38,results,outperforms,achieving,100 % accuracy,outperforms achieving 100 % accuracy,0.649850606918335
translation,74,38,results,100 % accuracy,of,2 - hop and 3 - hop questions,100 % accuracy of 2 - hop and 3 - hop questions,0.5913499593734741
translation,74,38,results,2 - hop and 3 - hop questions,in,metaqa dataset,2 - hop and 3 - hop questions in metaqa dataset,0.5237075090408325
translation,74,38,results,transfernet,has,outperforms,transfernet has outperforms,0.6457106471061707
translation,74,38,results,outperforms,has,previous models,outperforms has previous models,0.6068373918533325
translation,74,38,results,results,has,transfernet,results has transfernet,0.5753470063209534
translation,74,51,results,webqsp and compwebq,achieve,significant improvement,webqsp and compwebq achieve significant improvement,0.6742467880249023
translation,74,51,results,significant improvement,over,state - of - the - art models,significant improvement over state - of - the - art models,0.6455559730529785
translation,74,51,results,results,On,webqsp and compwebq,results On webqsp and compwebq,0.5280059576034546
translation,74,53,results,transfernet,surpasses,previous models,transfernet surpasses previous models,0.6678831577301025
translation,74,53,results,previous models,by,large margin,previous models by large margin,0.5685612559318542
translation,74,53,results,previous models,especially for,2 - hop and 3 - hop questions,previous models especially for 2 - hop and 3 - hop questions,0.6411297917366028
translation,74,53,results,results,demonstrate,transfernet,results demonstrate transfernet,0.6195204854011536
translation,74,54,results,transfernet,keeps,superiority,transfernet keeps superiority,0.723089873790741
translation,74,54,results,label form and the text form,has,transfernet,label form and the text form has transfernet,0.6118102073669434
translation,74,192,results,1948 entities,in,each subgraph,1948 entities in each subgraph,0.5258587598800659
translation,74,192,results,recall,is,64 %,recall is 64 %,0.6041756272315979
translation,74,233,results,auxiliary loss,slightly improves,performance,auxiliary loss slightly improves performance,0.7414159178733826
translation,74,233,results,results,has,auxiliary loss,results has auxiliary loss,0.5502908825874329
translation,74,265,results,baseline models,by,large margin,baseline models by large margin,0.5824459791183472
translation,74,265,results,outperforms,has,baseline models,outperforms has baseline models,0.5940393209457397
translation,74,265,results,results,On,webqsp and compwebq,results On webqsp and compwebq,0.5280059576034546
translation,74,268,results,trans - fernet,achieves,48.6 % accuracy,trans - fernet achieves 48.6 % accuracy,0.6757333278656006
translation,74,268,results,48.6 % accuracy,better than,pullnet,48.6 % accuracy better than pullnet,0.7494671940803528
translation,74,268,results,pullnet,has,47.2 % ),pullnet has 47.2 % ),0.5768875479698181
translation,74,268,results,results,has,trans - fernet,results has trans - fernet,0.6143776774406433
translation,74,270,results,transfernet,performs,perfectly,transfernet performs perfectly,0.7093395590782166
translation,74,270,results,perfectly,in,2 - hop and 3 - hop questions,perfectly in 2 - hop and 3 - hop questions,0.5753541588783264
translation,74,270,results,perfectly,achieving,100 % accuracy,perfectly achieving 100 % accuracy,0.678447961807251
translation,74,270,results,2 - hop and 3 - hop questions,of,metaqa,2 - hop and 3 - hop questions of metaqa,0.6122384667396545
translation,74,270,results,results,has,transfernet,results has transfernet,0.5753470063209534
translation,74,271,results,1 - hop ques - tions,of,metaqa,1 - hop ques - tions of metaqa,0.6385735869407654
translation,74,271,results,transfernet,achieves,97.5 %,transfernet achieves 97.5 %,0.6793453097343445
translation,74,271,results,97.5 %,on a par with,previous models,97.5 % on a par with previous models,0.6620123386383057
translation,74,271,results,previous models,like,vrn,previous models like vrn,0.6470961570739746
translation,74,271,results,previous models,like,embed - kgqa,previous models like embed - kgqa,0.6754819750785828
translation,74,271,results,1 - hop ques - tions,has,transfernet,1 - hop ques - tions has transfernet,0.6079128384590149
translation,74,271,results,metaqa,has,transfernet,metaqa has transfernet,0.6218198537826538
translation,74,278,results,transfernet,achieve,100 % accuracy,transfernet achieve 100 % accuracy,0.6583614349365234
translation,74,278,results,results,has,transfernet,results has transfernet,0.5753470063209534
translation,75,82,ablation-analysis,better passage quality,yielded by,ape,better passage quality yielded by ape,0.6740204095840454
translation,75,82,ablation-analysis,ape,helps to improve,end odqa performance,ape helps to improve end odqa performance,0.6576364040374756
translation,75,82,ablation-analysis,end odqa performance,of,model,end odqa performance of model,0.5964292287826538
translation,75,82,ablation-analysis,ablation analysis,indicates,better passage quality,ablation analysis indicates better passage quality,0.6142464876174927
translation,75,68,experiments,pooling operation,in,hasanswer model,pooling operation in hasanswer model,0.5256499648094177
translation,75,68,experiments,pooling operation,found,max-pooling,pooling operation found max-pooling,0.5596068501472473
translation,75,68,experiments,works better,than,mean-pooling and the [ cls ] token,works better than mean-pooling and the [ cls ] token,0.5581764578819275
translation,75,68,experiments,works better,than,max -pooling,works better than max -pooling,0.5717504024505615
translation,75,68,experiments,max-pooling,has,works better,max-pooling has works better,0.5555142164230347
translation,75,66,hyperparameters,"fid ( izacard and grave , 2020 b )",as,base model,"fid ( izacard and grave , 2020 b ) as base model",0.5699772238731384
translation,75,67,hyperparameters,fid-base and fid-large,contain,l = 12 and 24 layers,fid-base and fid-large contain l = 12 and 24 layers,0.6267298460006714
translation,75,67,hyperparameters,fid-base and fid-large,set,budget b = lk,fid-base and fid-large set budget b = lk,0.7195076942443848
translation,75,67,hyperparameters,hyperparameters,set,budget b = lk,hyperparameters set budget b = lk,0.6386352777481079
translation,75,67,hyperparameters,hyperparameters,has,fid-base and fid-large,hyperparameters has fid-base and fid-large,0.57888263463974
translation,75,69,hyperparameters,0.1,during,reinforce training,0.1 during reinforce training,0.6970590353012085
translation,75,69,hyperparameters,reinforce training,of,scheduler,reinforce training of scheduler,0.6415799856185913
translation,75,69,hyperparameters,hyperparameters,discount factor ? =,step penalty c,hyperparameters discount factor ? = step penalty c,0.7442333102226257
translation,75,6,model,ac method,applied to,existing odqa model,ac method applied to existing odqa model,0.6928518414497375
translation,75,6,model,efficiently,on,single gpu,efficiently on single gpu,0.6008023023605347
translation,75,6,model,adaptive passage encoder,has,ac method,adaptive passage encoder has ac method,0.5748363137245178
translation,75,6,model,model,propose,adaptive passage encoder,model propose adaptive passage encoder,0.6771516799926758
translation,75,16,model,efficient approach,to apply,adaptive computation,efficient approach to apply adaptive computation,0.704055666923523
translation,75,16,model,adaptive computation,to,large generative odqa models,adaptive computation to large generative odqa models,0.5152503848075867
translation,75,16,model,model,explore,efficient approach,model explore efficient approach,0.715519368648529
translation,75,17,model,module,not require,confidence calibration,module not require confidence calibration,0.6885223984718323
translation,75,17,model,encoder,of,existing odqa model,encoder of existing odqa model,0.5723605751991272
translation,75,17,model,encoder 's hidden representations,for calculating,ac priorities,encoder 's hidden representations for calculating ac priorities,0.7282614707946777
translation,75,17,model,adaptive passage encoder ( ape ),has,module,adaptive passage encoder ( ape ) has module,0.5687786936759949
translation,75,17,model,model,introduce,adaptive passage encoder ( ape ),model introduce adaptive passage encoder ( ape ),0.6089546084403992
translation,75,85,model,encoder,of,generative odqa models,encoder of generative odqa models,0.5256286859512329
translation,75,85,model,encoder,train,effective adaptive computation policy,encoder train effective adaptive computation policy,0.7010284066200256
translation,75,85,model,generative odqa models,with,our proposed adaptive passage encoder,generative odqa models with our proposed adaptive passage encoder,0.6103774905204773
translation,75,85,model,effective adaptive computation policy,without tuning,base model,effective adaptive computation policy without tuning base model,0.7053059339523315
translation,75,85,model,model,replacing,encoder,model replacing encoder,0.7280545830726624
translation,75,85,model,model,train,effective adaptive computation policy,model train effective adaptive computation policy,0.7077545523643494
translation,75,8,results,our method,improves upon,state - of - theart model,our method improves upon state - of - theart model,0.7094610929489136
translation,75,8,results,state - of - theart model,on,two datasets,state - of - theart model on two datasets,0.5252739787101746
translation,75,8,results,more accurate,than,previous ac methods,more accurate than previous ac methods,0.5825273394584656
translation,75,8,results,more accurate,due to,stronger base odqa model,more accurate due to stronger base odqa model,0.66944420337677
translation,75,8,results,previous ac methods,due to,stronger base odqa model,previous ac methods due to stronger base odqa model,0.6605587005615234
translation,75,18,results,nat-uralquestions and triviaqa,show,our method,nat-uralquestions and triviaqa show our method,0.6260318756103516
translation,75,18,results,our method,improves,performance,our method improves performance,0.667780339717865
translation,75,18,results,our method,producing,more accurate results ( 12.4 % em ),our method producing more accurate results ( 12.4 % em ),0.749424934387207
translation,75,18,results,performance,of,"state - of - the - art model fid ( izacard and grave , 2020 b )","performance of state - of - the - art model fid ( izacard and grave , 2020 b )",0.5528135299682617
translation,75,18,results,more accurate results ( 12.4 % em ),than,ac method,more accurate results ( 12.4 % em ) than ac method,0.5856917500495911
translation,75,74,results,our proposed method,improves upon,fid model,our proposed method improves upon fid model,0.7370038628578186
translation,75,74,results,our proposed method,by,statistically significant margin,our proposed method by statistically significant margin,0.6012097597122192
translation,75,74,results,fid model,on,both datasets,fid model on both datasets,0.5537723302841187
translation,75,74,results,statistically significant margin,on,triviaqa,statistically significant margin on triviaqa,0.5783815979957581
translation,75,75,results,previous ac method,by,12.4 %,previous ac method by 12.4 %,0.5453699231147766
translation,75,75,results,12.4 %,when,k = 10,12.4 % when k = 10,0.6499165892601013
translation,75,75,results,12.4 %,due to,stronger base model,12.4 % due to stronger base model,0.6492189168930054
translation,75,75,results,k = 10,due to,stronger base model,k = 10 due to stronger base model,0.7423880100250244
translation,75,75,results,outperforms,has,previous ac method,outperforms has previous ac method,0.589120090007782
translation,75,75,results,results,has,outperforms,results has outperforms,0.6657275557518005
translation,75,81,results,top-k accuracy,of,selected collection of documents,top-k accuracy of selected collection of documents,0.5914612412452698
translation,75,81,results,selected collection of documents,by,ape,selected collection of documents by ape,0.6553097367286682
translation,75,81,results,ape,is,significantly better,ape is significantly better,0.5956370234489441
translation,75,81,results,significantly better,than,"bm25 , dpr , and fid","significantly better than bm25 , dpr , and fid",0.5555753707885742
translation,75,81,results,strong retrieval baselines,for,odqa,strong retrieval baselines for odqa,0.60393887758255
translation,75,81,results,results,show that,top-k accuracy,results show that top-k accuracy,0.4857165217399597
translation,76,157,ablation-analysis,performance,of,roberta,performance of roberta,0.6133401989936829
translation,76,157,ablation-analysis,roberta,in,our framework,roberta in our framework,0.5767332911491394
translation,76,157,ablation-analysis,qhuman,has,degrades,qhuman has degrades,0.6539903283119202
translation,76,157,ablation-analysis,degrades,has,performance,degrades has performance,0.5834147930145264
translation,76,157,ablation-analysis,ablation analysis,excluding,qhuman,ablation analysis excluding qhuman,0.7679392099380493
translation,76,158,ablation-analysis,qsyn,contributes to,performance improvement,qsyn contributes to performance improvement,0.7490425705909729
translation,76,158,ablation-analysis,ablation analysis,has,qsyn,ablation analysis has qsyn,0.572480320930481
translation,76,7,baselines,excord,generates,self-contained questions,excord generates self-contained questions,0.6901513338088989
translation,76,7,baselines,excord,trains,qa model,excord trains qa model,0.7925345301628113
translation,76,7,baselines,self-contained questions,without,conversation history,self-contained questions without conversation history,0.704489529132843
translation,76,7,baselines,understood,without,conversation history,understood without conversation history,0.7589104771614075
translation,76,7,baselines,qa model,with,pairs,qa model with pairs,0.679673433303833
translation,76,7,baselines,qa model,using,consistency - based regularizer,qa model using consistency - based regularizer,0.6717098355293274
translation,76,7,baselines,pairs,of,original and self-contained questions,pairs of original and self-contained questions,0.5643793940544128
translation,76,7,baselines,baselines,has,excord,baselines has excord,0.6185880899429321
translation,76,119,baselines,bert bert,is,contextualized word representation model,bert bert is contextualized word representation model,0.5683476328849792
translation,76,119,baselines,contextualized word representation model,pretrained on,large corpora,contextualized word representation model pretrained on large corpora,0.7422357797622681
translation,76,119,baselines,baselines,has,bert bert,baselines has bert bert,0.6704959869384766
translation,76,122,baselines,bert + hae,is,bert - based qa model,bert + hae is bert - based qa model,0.5996891260147095
translation,76,122,baselines,bert + hae,is,bert - based qa model,bert + hae is bert - based qa model,0.5996891260147095
translation,76,122,baselines,bert - based qa model,with,cqa - specific module,bert - based qa model with cqa - specific module,0.6228325963020325
translation,76,122,baselines,bert + hae,has,bert + hae,bert + hae has bert + hae,0.679072916507721
translation,76,122,baselines,baselines,has,bert + hae,baselines has bert + hae,0.6228189468383789
translation,76,122,baselines,baselines,has,bert + hae,baselines has bert + hae,0.6228189468383789
translation,76,6,model,abilities,of,qa models,abilities of qa models,0.6416028738021851
translation,76,6,model,novel framework,has,excord,novel framework has excord,0.6052274703979492
translation,76,6,model,model,propose,novel framework,model propose novel framework,0.720393180847168
translation,76,29,model,excord,on how to Resolve Conversational Dependency,novel training framework,excord on how to Resolve Conversational Dependency novel training framework,0.7539060115814209
translation,76,29,model,novel training framework,for,cqa task,novel training framework for cqa task,0.5829442739486694
translation,76,29,model,excord,has,explicit guidance,excord has explicit guidance,0.6017788648605347
translation,76,29,model,excord,has,novel training framework,excord has novel training framework,0.5843768119812012
translation,76,29,model,explicit guidance,has,novel training framework,explicit guidance has novel training framework,0.5588030219078064
translation,76,29,model,model,propose,excord,model propose excord,0.6602756977081299
translation,76,30,model,self-contained questions,using,qr models,self-contained questions using qr models,0.6982204914093018
translation,76,30,model,model,generate,self-contained questions,model generate self-contained questions,0.7230615615844727
translation,76,31,model,self-contained questions,with,original questions,self-contained questions with original questions,0.5527506470680237
translation,76,31,model,qa models,with,consistency regularization,qa models with consistency regularization,0.5970022082328796
translation,76,31,model,model,pair,self-contained questions,model pair self-contained questions,0.7638112902641296
translation,76,32,model,original questions,encourage,qa models,original questions encourage qa models,0.6429808139801025
translation,76,32,model,qa models,to yield,similar answers,qa models to yield similar answers,0.5937957763671875
translation,76,32,model,similar answers,to,self -contained questions,similar answers to self -contained questions,0.5512754917144775
translation,76,32,model,similar answers,when,self -contained questions,similar answers when self -contained questions,0.6022793650627136
translation,76,32,model,model,when,original questions,model when original questions,0.6560917496681213
translation,76,125,results,roberta,improves,bert,roberta improves bert,0.5593051314353943
translation,76,125,results,bert,by using,pretraining techniques,bert by using pretraining techniques,0.6969614624977112
translation,76,125,results,pretraining techniques,to obtain,robustly optimized weights,pretraining techniques to obtain robustly optimized weights,0.5668302178382874
translation,76,125,results,robustly optimized weights,on,larger corpora,robustly optimized weights on larger corpora,0.5099133849143982
translation,76,125,results,results,has,roberta,results has roberta,0.530095100402832
translation,76,137,results,framework,on,quac and canard,framework on quac and canard,0.606009840965271
translation,76,138,results,excord,consistently improves,performance,excord consistently improves performance,0.7734036445617676
translation,76,138,results,performance,of,qa models,performance of qa models,0.6271283626556396
translation,76,138,results,performance,on,both datasets,performance on both datasets,0.4974806010723114
translation,76,138,results,qa models,on,both datasets,qa models on both datasets,0.5164074897766113
translation,76,138,results,end-to- end approach,has,excord,end-to- end approach has excord,0.6049447655677795
translation,76,138,results,results,Compared to,end-to- end approach,results Compared to end-to- end approach,0.6703792810440063
translation,76,139,results,excord,improves,performance,excord improves performance,0.7076721787452698
translation,76,139,results,performance,of,roberta,performance of roberta,0.6133401989936829
translation,76,139,results,bert,by,1.2 and 5.2 f1 scores,bert by 1.2 and 5.2 f1 scores,0.5947064757347107
translation,76,139,results,1.2 and 5.2 f1 scores,on,quac and canard,1.2 and 5.2 f1 scores on quac and canard,0.5546989440917969
translation,76,139,results,significant,has,excord,significant has excord,0.6025521159172058
translation,76,139,results,absolutely,has,1.2 and 2.3 f1 scores,absolutely has 1.2 and 2.3 f1 scores,0.5876296758651733
translation,76,140,results,consistency training,with,original and self-contained 6135 questions,consistency training with original and self-contained 6135 questions,0.5946497321128845
translation,76,140,results,consistency training,enhances,ability,consistency training enhances ability,0.6597536206245422
translation,76,140,results,ability,of,qa models,ability of qa models,0.6370845437049866
translation,76,140,results,ability,to understand,conversational context,ability to understand conversational context,0.6975187659263611
translation,76,140,results,qa models,to understand,conversational context,qa models to understand conversational context,0.5754351615905762
translation,76,140,results,results,conclude that,consistency training,results conclude that consistency training,0.6437788605690002
translation,76,141,results,underperforms,in,all baseline models,underperforms in all baseline models,0.4977002441883087
translation,76,141,results,end-to - end approach,in,all baseline models,end-to - end approach in all baseline models,0.5011293292045593
translation,76,141,results,quac,has,pipeline approach,quac has pipeline approach,0.6467409729957581
translation,76,141,results,pipeline approach,has,underperforms,pipeline approach has underperforms,0.6181280016899109
translation,76,141,results,underperforms,has,end-to - end approach,underperforms has end-to - end approach,0.5887469053268433
translation,76,141,results,results,On,quac,results On quac,0.6250786781311035
translation,76,143,results,excord,improves,qa models,excord improves qa models,0.7319373488426208
translation,76,143,results,qa models,by using,both types of questions,qa models by using both types of questions,0.6433215141296387
translation,76,143,results,results,has,excord,results has excord,0.5634559988975525
translation,76,144,results,baseline approaches,on,quac,baseline approaches on quac,0.5832552313804626
translation,76,144,results,our framework,has,significantly outperforms,our framework has significantly outperforms,0.6090956330299377
translation,76,144,results,significantly outperforms,has,baseline approaches,significantly outperforms has baseline approaches,0.5808980464935303
translation,76,144,results,results,has,our framework,results has our framework,0.6097875237464905
translation,76,145,results,pipeline approach,is,significantly more effective,pipeline approach is significantly more effective,0.5661789774894714
translation,76,145,results,significantly more effective,than,end-to - end approach,significantly more effective than end-to - end approach,0.5915539264678955
translation,76,145,results,canard,has,pipeline approach,canard has pipeline approach,0.6187753677368164
translation,76,145,results,results,On,canard,results On canard,0.6174376606941223
translation,76,146,results,qa,perform,well,qa perform well,0.6582750082015991
translation,76,146,results,well,on,canard questions,well on canard questions,0.5883373022079468
translation,76,146,results,results,has,qa,results has qa,0.4341823160648346
translation,76,147,results,outperforms,in,most cases,outperforms in most cases,0.5715058445930481
translation,76,147,results,pipeline approach,in,most cases,pipeline approach in most cases,0.5385380387306213
translation,76,147,results,outperforms,has,pipeline approach,outperforms has pipeline approach,0.6270266771316528
translation,76,147,results,results,has,ex - cord,results has ex - cord,0.5619367361068726
translation,76,148,results,our framework,improves,performance,our framework improves performance,0.6748889088630676
translation,76,148,results,performance,of,roberta,performance of roberta,0.6133401989936829
translation,76,148,results,performance,absolutely,1.2 f1 score,performance absolutely 1.2 f1 score,0.7562025189399719
translation,76,148,results,roberta,by,1.2 f1 score,roberta by 1.2 f1 score,0.5842363834381104
translation,76,148,results,roberta,absolutely,1.2 f1 score,roberta absolutely 1.2 f1 score,0.7963786125183105
translation,76,148,results,pipeline approach,has,our framework,pipeline approach has our framework,0.6089284420013428
translation,76,148,results,results,Compared to,pipeline approach,results Compared to pipeline approach,0.6712159514427185
translation,76,161,results,performance,of,roberta,performance of roberta,0.6133401989936829
translation,76,161,results,performance,on,quac,performance on quac,0.6048246026039124
translation,76,161,results,performance,on,quac,performance on quac,0.6048246026039124
translation,76,161,results,roberta,on,ca - nard,roberta on ca - nard,0.5219513177871704
translation,76,161,results,question augment,has,slightly improves,question augment has slightly improves,0.6451067328453064
translation,76,161,results,slightly improves,has,performance,slightly improves has performance,0.5808963179588318
translation,76,161,results,results,find that,question augment,results find that question augment,0.658836305141449
translation,76,163,results,consistency training approach,has,significantly improves,consistency training approach has significantly improves,0.6243371367454529
translation,76,163,results,significantly improves,has,performance,significantly improves has performance,0.5962982177734375
translation,76,163,results,results,has,consistency training approach,results has consistency training approach,0.5737767219543457
translation,76,190,results,our framework,performs,well,our framework performs well,0.6716017127037048
translation,76,190,results,well,on,coqa,well on coqa,0.6416443586349487
translation,76,191,results,improvement,in,bert,improvement in bert,0.6371562480926514
translation,76,191,results,bert,is,0.5,bert is 0.5,0.5651424527168274
translation,76,191,results,0.5,based on,overall f1,0.5 based on overall f1,0.6422283053398132
translation,76,191,results,performance,of,roberta,performance of roberta,0.6133401989936829
translation,76,191,results,performance,improved by,overall f1,performance improved by overall f1,0.7243980169296265
translation,76,191,results,roberta,improved by,overall f1,roberta improved by overall f1,0.7004743814468384
translation,76,191,results,overall f1,of,0.6,overall f1 of 0.6,0.5655255913734436
translation,76,191,results,results,has,improvement,results has improvement,0.6248279809951782
translation,76,192,results,consistent,in,most of the documents ' domains,consistent in most of the documents ' domains,0.524077832698822
translation,76,192,results,results,has,improvements,results has improvements,0.615561842918396
translation,77,97,ablation-analysis,question text,is,most crucial input,question text is most crucial input,0.5478724837303162
translation,77,97,ablation-analysis,ablation analysis,has,question text,ablation analysis has question text,0.5525280237197876
translation,77,98,ablation-analysis,ablation analysis,When,amr,ablation analysis When amr,0.6561884880065918
translation,77,100,ablation-analysis,similar effect,observed,qald - 9 and lc - quad 2.0 test sets,similar effect observed qald - 9 and lc - quad 2.0 test sets,0.6908206939697266
translation,77,100,ablation-analysis,similar effect,on,qald - 9 and lc - quad 2.0 test sets,similar effect on qald - 9 and lc - quad 2.0 test sets,0.5727331042289734
translation,77,100,ablation-analysis,qald - 9 and lc - quad 2.0 test sets,when,amr,qald - 9 and lc - quad 2.0 test sets when amr,0.6068116426467896
translation,77,100,ablation-analysis,amr,degrading,performance,amr degrading performance,0.7543936371803284
translation,77,100,ablation-analysis,performance,by,4.0 and 2.9 points,performance by 4.0 and 2.9 points,0.5988883972167969
translation,77,100,ablation-analysis,ablation analysis,has,similar effect,ablation analysis has similar effect,0.5600232481956482
translation,77,104,ablation-analysis,amr,incorporated in,relation linking module,amr incorporated in relation linking module,0.6366094350814819
translation,77,104,ablation-analysis,system performance,on,lc - quad 1.0 test dataset,system performance on lc - quad 1.0 test dataset,0.4802134037017822
translation,77,104,ablation-analysis,system performance,improves by,2.4,system performance improves by 2.4,0.6989495754241943
translation,77,104,ablation-analysis,2.4,achieving,new state - of- the- art f1,2.4 achieving new state - of- the- art f1,0.6252327561378479
translation,77,104,ablation-analysis,new state - of- the- art f1,of,44.5,new state - of- the- art f1 of 44.5,0.5714454650878906
translation,77,104,ablation-analysis,amr,has,system performance,amr has system performance,0.550980269908905
translation,77,104,ablation-analysis,relation linking module,has,system performance,relation linking module has system performance,0.5660299062728882
translation,77,104,ablation-analysis,ablation analysis,When,amr,ablation analysis When amr,0.6561884880065918
translation,77,123,ablation-analysis,our ablation study,including,amr graph,our ablation study including amr graph,0.7184728384017944
translation,77,123,ablation-analysis,performance,with,relatively simple encoding scheme ( plain text ),performance with relatively simple encoding scheme ( plain text ),0.638413667678833
translation,77,123,ablation-analysis,amr graph,has,improves,amr graph has improves,0.6394761204719543
translation,77,123,ablation-analysis,improves,has,performance,improves has performance,0.5770372748374939
translation,77,123,ablation-analysis,ablation analysis,shows,amr graph,ablation analysis shows amr graph,0.6650406718254089
translation,77,123,ablation-analysis,ablation analysis,including,amr graph,ablation analysis including amr graph,0.6871331334114075
translation,77,123,ablation-analysis,ablation analysis,has,our ablation study,ablation analysis has our ablation study,0.5378058552742004
translation,77,118,baselines,semrel,is,"kg - agnostic , single end-to - end neural model","semrel is kg - agnostic , single end-to - end neural model",0.5596293210983276
translation,77,118,baselines,"kg - agnostic , single end-to - end neural model",does not require,various ensemble components,"kg - agnostic , single end-to - end neural model does not require various ensemble components",0.7024705410003662
translation,77,118,baselines,stateof - the - art performance,on,dbpedia and wikidata datasets,stateof - the - art performance on dbpedia and wikidata datasets,0.5026858448982239
translation,77,118,baselines,sling,has,semrel,sling has semrel,0.6709433197975159
translation,77,118,baselines,baselines,Unlike,sling,baselines Unlike sling,0.7495436072349548
translation,77,83,experiments,dbpedia- based benchmarks,compare,semrel,dbpedia- based benchmarks compare semrel,0.640626847743988
translation,77,83,experiments,dbpedia- based benchmarks,compare,sling,dbpedia- based benchmarks compare sling,0.6575212478637695
translation,77,83,experiments,semrel,with,"falcon ( sakor et al. , 2019a )","semrel with falcon ( sakor et al. , 2019a )",0.6036860942840576
translation,77,83,experiments,semrel,with,sling,semrel with sling,0.7534077167510986
translation,77,83,experiments,sling,has,"mihindukulasooriya et al. , 2020 )","sling has mihindukulasooriya et al. , 2020 )",0.5839733481407166
translation,77,48,hyperparameters,"pre-trained bert model ( devlin et al. , 2018 )",to initialize,encoder parameters,"pre-trained bert model ( devlin et al. , 2018 ) to initialize encoder parameters",0.6688696146011353
translation,77,48,hyperparameters,hyperparameters,use,"pre-trained bert model ( devlin et al. , 2018 )","hyperparameters use pre-trained bert model ( devlin et al. , 2018 )",0.5730984210968018
translation,77,7,model,simple transformer - based neural model,for,relation linking,simple transformer - based neural model for relation linking,0.6237967014312744
translation,77,7,model,amr semantic parse,of,sentence,amr semantic parse of sentence,0.5458898544311523
translation,77,7,model,model,propose,simple transformer - based neural model,model propose simple transformer - based neural model,0.6329337358474731
translation,77,17,model,single semantics - aware neural model,for,relation linking,single semantics - aware neural model for relation linking,0.6043509840965271
translation,77,17,model,semrel,has,single semantics - aware neural model,semrel has single semantics - aware neural model,0.5338425636291504
translation,77,17,model,model,propose,semrel,model propose semrel,0.6765686869621277
translation,77,18,model,semrel,takes as input,question text,semrel takes as input question text,0.7067813873291016
translation,77,18,model,semrel,outputs,ranked list of relations,semrel outputs ranked list of relations,0.7333498597145081
translation,77,18,model,question text,annotated with,amr parse and entity information,question text annotated with amr parse and entity information,0.7576886415481567
translation,77,18,model,model,has,semrel,model has semrel,0.5987637639045715
translation,77,120,model,simple transformerbased neural model,for,relation linking,simple transformerbased neural model for relation linking,0.6244097948074341
translation,77,120,model,simple transformerbased neural model,semantic structure of,sentence,simple transformerbased neural model semantic structure of sentence,0.720574676990509
translation,77,120,model,relation linking,semantic structure of,sentence,relation linking semantic structure of sentence,0.6609533429145813
translation,77,120,model,model,present,simple transformerbased neural model,model present simple transformerbased neural model,0.6474487781524658
translation,77,121,model,neural architecture,enables us to adapt,system,neural architecture enables us to adapt system,0.7010905742645264
translation,77,121,model,system,to,multiple kgs ( e.g. dbpedia and wikidata ),system to multiple kgs ( e.g. dbpedia and wikidata ),0.528771698474884
translation,77,88,results,outperforms,with respect to,f1 score,outperforms with respect to f1 score,0.6437540650367737
translation,77,88,results,all baselines,across,all benchmarks,all baselines across all benchmarks,0.6544754505157471
translation,77,88,results,all baselines,with respect to,f1 score,all baselines with respect to f1 score,0.6364508867263794
translation,77,88,results,semrel,has,outperforms,semrel has outperforms,0.6387098431587219
translation,77,88,results,outperforms,has,all baselines,outperforms has all baselines,0.5888515114784241
translation,77,88,results,results,has,semrel,results has semrel,0.5121749043464661
translation,77,103,results,impact,of using,amr,impact of using amr,0.7254871726036072
translation,77,103,results,amr,in,relation linking,amr in relation linking,0.5702255964279175
translation,77,103,results,amr,translates into,nice performance gains,amr translates into nice performance gains,0.6238124966621399
translation,77,103,results,nice performance gains,in,overall kbqa results,nice performance gains in overall kbqa results,0.5037304162979126
translation,77,103,results,results,found that,impact,results found that impact,0.6650253534317017
translation,78,74,experimental-setup,texts,tokenize,dataset,texts tokenize dataset,0.7704921364784241
translation,78,74,experimental-setup,dataset,with,subword model,dataset with subword model,0.6030731797218323
translation,78,74,experimental-setup,subword model,as,byte-pair encoding ( bpe ),subword model as byte-pair encoding ( bpe ),0.5394323468208313
translation,78,74,experimental-setup,experimental setup,After splitting,texts,experimental setup After splitting texts,0.6452207565307617
translation,78,76,experimental-setup,length,of,texts,length of texts,0.6209525465965271
translation,78,76,experimental-setup,texts,in,dataset,texts in dataset,0.5117151141166687
translation,78,76,experimental-setup,max_length,=,512,max_length = 512,0.7101320028305054
translation,78,76,experimental-setup,epochs=5,for,model,epochs=5 for model,0.6840146780014038
translation,78,76,experimental-setup,experimental setup,analyze,length,experimental setup analyze length,0.5612026453018188
translation,78,76,experimental-setup,experimental setup,set,max_length,experimental setup set max_length,0.6904410719871521
translation,78,77,experimental-setup,extensive hyper-parameters,set,learning_rate and drop_out,extensive hyper-parameters set learning_rate and drop_out,0.6633538603782654
translation,78,77,experimental-setup,learning_rate and drop_out,equal to,3e - 5 and 0.1,learning_rate and drop_out equal to 3e - 5 and 0.1,0.6905164122581482
translation,78,77,experimental-setup,experimental setup,After searching for,extensive hyper-parameters,experimental setup After searching for extensive hyper-parameters,0.6720342636108398
translation,78,78,experimental-setup,model,with,5 - fold cross-validation,model with 5 - fold cross-validation,0.6489884257316589
translation,78,78,experimental-setup,experimental setup,train,model,experimental setup train model,0.6514950394630432
translation,78,61,model,two systems,for,toxic spans detection task,two systems for toxic spans detection task,0.5957403779029846
translation,78,61,model,toxic spans detection task,with,ner and qa approaches,toxic spans detection task with ner and qa approaches,0.6015505790710449
translation,78,61,model,model,propose,two systems,model propose two systems,0.6796255707740784
translation,78,75,model,data,into,pre-trained roberta model,data into pre-trained roberta model,0.5952905416488647
translation,78,75,model,model,feed,data,model feed data,0.715168297290802
translation,78,116,results,f1score,of,best system,f1score of best system,0.5767085552215576
translation,78,116,results,f1score,is,66.99 %,f1score is 66.99 %,0.5693921446800232
translation,78,116,results,f1score,is,49.09 %,f1score is 49.09 %,0.5808148384094238
translation,78,116,results,best system,is,66.99 %,best system is 66.99 %,0.5370486974716187
translation,78,116,results,best system,is,3.84 %,best system is 3.84 %,0.5448875427246094
translation,78,116,results,3.84 %,lower than,first rank team,3.84 % lower than first rank team,0.7002416253089905
translation,78,116,results,49.09 %,higher than,baseline model,49.09 % higher than baseline model,0.697683572769165
translation,78,116,results,results,has,f1score,results has f1score,0.5445802807807922
translation,79,146,ablation-analysis,openwikisql dataset,enabling,sql generation,openwikisql dataset enabling sql generation,0.752063512802124
translation,79,146,ablation-analysis,sql generation,brings,more than 10 points improvement,sql generation brings more than 10 points improvement,0.6448995471000671
translation,79,146,ablation-analysis,more than 10 points improvement,on,em scores,more than 10 points improvement on em scores,0.5393036007881165
translation,79,146,ablation-analysis,ablation analysis,On,openwikisql dataset,ablation analysis On openwikisql dataset,0.529592752456665
translation,79,170,ablation-analysis,hypothesis,that,sql generation,hypothesis that sql generation,0.6610647439956665
translation,79,170,ablation-analysis,sql generation,helps in,complex reasoning,sql generation helps in complex reasoning,0.7060034275054932
translation,79,170,ablation-analysis,sql generation,helps in,explainability,sql generation helps in explainability,0.7216037511825562
translation,79,170,ablation-analysis,explainability,for,tabular question answering,explainability for tabular question answering,0.6272025108337402
translation,79,170,ablation-analysis,ablation analysis,support,hypothesis,ablation analysis support hypothesis,0.6373509168624878
translation,79,114,baselines,bert reranker,initialized with,pretrained bert - base-uncased model,bert reranker initialized with pretrained bert - base-uncased model,0.7015911340713501
translation,79,124,baselines,fid model,when,multiple answers,fid model when multiple answers,0.666302502155304
translation,79,124,baselines,fid model,there are,multiple answers,fid model there are multiple answers,0.6775010824203491
translation,79,124,baselines,fid model,randomly sample,one answer,fid model randomly sample one answer,0.7506991028785706
translation,79,124,baselines,multiple answers,for,one question,multiple answers for one question,0.6364573836326599
translation,79,124,baselines,one answer,from,list,one answer from list,0.6233562231063843
translation,79,129,baselines,single modality settings,with,only textual evidence or tabular evidence,single modality settings with only textual evidence or tabular evidence,0.6171538829803467
translation,79,129,baselines,hybrid setting,with,textual and tabular evidence available,hybrid setting with textual and tabular evidence available,0.628146231174469
translation,79,129,baselines,hybrid setting,both,textual and tabular evidence available,hybrid setting both textual and tabular evidence available,0.6838977336883545
translation,79,131,baselines,baseline model,generates,direct answer text,baseline model generates direct answer text,0.6284050941467285
translation,79,131,baselines,fid model,generates,direct answer text,fid model generates direct answer text,0.6960943937301636
translation,79,131,baselines,fid model,make use of,textual and tabular evidence,fid model make use of textual and tabular evidence,0.7359603047370911
translation,79,131,baselines,baseline model,has,fid model,baseline model has fid model,0.5941001176834106
translation,79,131,baselines,fid +,has,fid model,fid + has fid model,0.6304285526275635
translation,79,131,baselines,baselines,consider,baseline model,baselines consider baseline model,0.7315187454223633
translation,79,52,experiments,hybrid open-domain setting,build,two separate search indices,hybrid open-domain setting build two separate search indices,0.6935228705406189
translation,79,52,experiments,two separate search indices,another for,tabular input,two separate search indices another for tabular input,0.682716965675354
translation,79,113,experiments,bm25 retrieval,using,elasticsearch 7.7,bm25 retrieval using elasticsearch 7.7,0.6069703102111816
translation,79,113,experiments,elasticsearch 7.7,with,default settings,elasticsearch 7.7 with default settings,0.561244547367096
translation,79,121,experiments,reranker and fid models,use,adam optimizer,reranker and fid models use adam optimizer,0.610654354095459
translation,79,121,experiments,adam optimizer,with,maximum learning rate,adam optimizer with maximum learning rate,0.5922847986221313
translation,79,121,experiments,adam optimizer,with,dropout rate,adam optimizer with dropout rate,0.5803738236427307
translation,79,121,experiments,maximum learning rate,of,10 4,maximum learning rate of 10 4,0.6562338471412659
translation,79,121,experiments,dropout rate,of,10 %,dropout rate of 10 %,0.5538275241851807
translation,79,69,hyperparameters,top 50 candidates,choose them from,joint pool of all candidates,top 50 candidates choose them from joint pool of all candidates,0.6801515817642212
translation,79,69,hyperparameters,scores,assigned by,reranker,scores assigned by reranker,0.6675831079483032
translation,79,69,hyperparameters,hyperparameters,For,top 50 candidates,hyperparameters For top 50 candidates,0.5683383345603943
translation,79,122,hyperparameters,linearly anneals,to,zero,linearly anneals to zero,0.6167223453521729
translation,79,122,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,79,123,hyperparameters,models,for,10 k gradient steps,models for 10 k gradient steps,0.6364482641220093
translation,79,123,hyperparameters,models,save,checkpoint,models save checkpoint,0.6786158084869385
translation,79,123,hyperparameters,10 k gradient steps,with,batch size,10 k gradient steps with batch size,0.638508677482605
translation,79,123,hyperparameters,batch size,of,32,batch size of 32,0.6741614937782288
translation,79,123,hyperparameters,checkpoint,every,1 k steps,checkpoint every 1 k steps,0.6872228384017944
translation,79,123,hyperparameters,hyperparameters,train,models,hyperparameters train models,0.6666569709777832
translation,79,7,model,hybrid framework,takes,textual and tabular evidence,hybrid framework takes textual and tabular evidence,0.6548840999603271
translation,79,7,model,hybrid framework,generates,direct answers,hybrid framework generates direct answers,0.7100752592086792
translation,79,7,model,textual and tabular evidence,as,input,textual and tabular evidence as input,0.5353564620018005
translation,79,7,model,model,propose,hybrid framework,model propose hybrid framework,0.7191015481948853
translation,79,23,model,current generative odqa models,with,text2sql ability,current generative odqa models with text2sql ability,0.6472684741020203
translation,79,24,model,dual readerparser ( durepa ) framework,take,textual and tabular data,dual readerparser ( durepa ) framework take textual and tabular data,0.603839635848999
translation,79,24,model,dual readerparser ( durepa ) framework,generate,direct answers,dual readerparser ( durepa ) framework generate direct answers,0.6624978184700012
translation,79,24,model,textual and tabular data,as,input,textual and tabular data as input,0.5218401551246643
translation,79,24,model,sql queries,based on,context,sql queries based on context,0.65492182970047
translation,79,24,model,model,propose,dual readerparser ( durepa ) framework,model propose dual readerparser ( durepa ) framework,0.6663516759872437
translation,79,27,model,supporting candidates,of,textual and tabular types,supporting candidates of textual and tabular types,0.5503256916999817
translation,79,27,model,supporting candidates,followed by,joint reranker,supporting candidates followed by joint reranker,0.6541535258293152
translation,79,27,model,joint reranker,predicts,how relevant,joint reranker predicts how relevant,0.6128861904144287
translation,79,27,model,supporting candidate,to,question,supporting candidate to question,0.5955045223236084
translation,79,27,model,"fusion - in- decoder model ( izacard and grave , 2020 )",for,our readerparser,"fusion - in- decoder model ( izacard and grave , 2020 ) for our readerparser",0.6249794363975525
translation,79,27,model,model,retrieve,supporting candidates,model retrieve supporting candidates,0.6422823667526245
translation,79,27,model,model,use,"fusion - in- decoder model ( izacard and grave , 2020 )","model use fusion - in- decoder model ( izacard and grave , 2020 )",0.6512829661369324
translation,79,10,results,baseline models,that only take,homogeneous input,baseline models that only take homogeneous input,0.6771253347396851
translation,79,10,results,homogeneous input,by,large margin,homogeneous input by large margin,0.5845446586608887
translation,79,10,results,several odqa datasets,has,hybrid methods,several odqa datasets has hybrid methods,0.5355737209320068
translation,79,10,results,hybrid methods,has,consistently outperforms,hybrid methods has consistently outperforms,0.6128185987472534
translation,79,10,results,consistently outperforms,has,baseline models,consistently outperforms has baseline models,0.5841743350028992
translation,79,10,results,results,on,several odqa datasets,results on several odqa datasets,0.5148427486419678
translation,79,32,results,durepa,performs,significantly better,durepa performs significantly better,0.6697889566421509
translation,79,32,results,significantly better,than,baseline models,significantly better than baseline models,0.577498733997345
translation,79,32,results,baseline models,trained on,single evidence type,baseline models trained on single evidence type,0.6981157660484314
translation,79,32,results,all question types,has,durepa,all question types has durepa,0.6105262041091919
translation,79,32,results,results,On,all question types,results On all question types,0.46324554085731506
translation,79,132,results,single modality setting,observe,"opensquad , opennq and ott - qa datasets","single modality setting observe opensquad , opennq and ott - qa datasets",0.5747516751289368
translation,79,132,results,single modality setting,observe,textual qa model,single modality setting observe textual qa model,0.5767418146133423
translation,79,132,results,single modality setting,observe,openwikisql,single modality setting observe openwikisql,0.633739173412323
translation,79,132,results,single modality setting,for,openwikisql,single modality setting for openwikisql,0.6021066308021545
translation,79,132,results,"opensquad , opennq and ott - qa datasets",for,openwikisql,"opensquad , opennq and ott - qa datasets for openwikisql",0.5441423654556274
translation,79,132,results,textual qa model,performing,significantly better,textual qa model performing significantly better,0.5970041751861572
translation,79,132,results,textual qa model,for,openwikisql,textual qa model for openwikisql,0.5668346285820007
translation,79,132,results,significantly better,than,tabular qa models,significantly better than tabular qa models,0.6159148216247559
translation,79,132,results,"opensquad , opennq and ott - qa datasets",has,textual qa model,"opensquad , opennq and ott - qa datasets has textual qa model",0.5580589175224304
translation,79,132,results,results,in,single modality setting,results in single modality setting,0.5453659296035767
translation,79,134,results,single modality models,across,datasets,single modality models across datasets,0.7283244132995605
translation,79,134,results,hybrid setting,has,hybrid models,hybrid setting has hybrid models,0.6353805661201477
translation,79,134,results,hybrid models,has,outperform,hybrid models has outperform,0.6012392640113831
translation,79,134,results,outperform,has,single modality models,outperform has single modality models,0.5985060930252075
translation,79,134,results,results,In,hybrid setting,results In hybrid setting,0.5498256683349609
translation,79,136,results,always beneficial,for,extractive questions,always beneficial for extractive questions,0.6609825491905212
translation,79,136,results,extractive questions,like,squad and nq,extractive questions like squad and nq,0.6896541714668274
translation,79,137,results,wikisql - type questions,gain of,sql generation,wikisql - type questions gain of sql generation,0.6691961884498596
translation,79,137,results,sql generation,is,significant,sql generation is significant,0.6273556351661682
translation,79,137,results,results,for,wikisql - type questions,results for wikisql - type questions,0.537865161895752
translation,79,138,results,our durepa model,using,hybrid evidences,our durepa model using hybrid evidences,0.7095217108726501
translation,79,138,results,new state- ofthe - art em score,of,57.0,new state- ofthe - art em score of 57.0,0.5339542031288147
translation,79,138,results,opensquad dataset,has,our durepa model,opensquad dataset has our durepa model,0.5544027090072632
translation,79,138,results,results,On,opensquad dataset,results On opensquad dataset,0.5351563096046448
translation,79,140,results,nq dataset,FID + with,text-only,nq dataset FID + with text-only,0.7254531383514404
translation,79,140,results,lower em score,compared with,fid -base,lower em score compared with fid -base,0.6575544476509094
translation,79,140,results,text-only,has,evidences,text-only has evidences,0.6043680310249329
translation,79,140,results,evidences,has,lower em score,evidences has lower em score,0.567652702331543
translation,79,140,results,results,On,nq dataset,results On nq dataset,0.5883698463439941
translation,79,142,results,durepa model,with,hybrid evidences,durepa model with hybrid evidences,0.6422130465507507
translation,79,142,results,hybrid evidences,improve,em,hybrid evidences improve em,0.7166008353233337
translation,79,142,results,em,by,2.8 points,em by 2.8 points,0.642504870891571
translation,79,142,results,2.8 points,compared to,fid +,2.8 points compared to fid +,0.684481680393219
translation,79,142,results,fid +,using,only text inputs,fid + using only text inputs,0.6903812289237976
translation,79,142,results,results,has,durepa model,results has durepa model,0.5411476492881775
translation,79,143,results,outperforms,by,1.4 points,outperforms by 1.4 points,0.6214780211448669
translation,79,143,results,ir + cr baseline,by,1.4 points,ir + cr baseline by 1.4 points,0.5483880639076233
translation,79,143,results,ott - qa questions,has,our full model,ott - qa questions has our full model,0.6049979329109192
translation,79,143,results,outperforms,has,ir + cr baseline,outperforms has ir + cr baseline,0.5743610858917236
translation,79,143,results,results,On,ott - qa questions,results On ott - qa questions,0.5210713744163513
translation,79,156,results,recall@25,of,ranker,recall@25 of ranker,0.6326314210891724
translation,79,156,results,ranker,even higher than,recall@100,ranker even higher than recall@100,0.6538929343223572
translation,79,156,results,recall@100,of,bm25 retriever,recall@100 of bm25 retriever,0.5794703960418701
translation,79,156,results,textual and tabular candidates,has,recall@25,textual and tabular candidates has recall@25,0.5946494936943054
translation,79,156,results,results,on,textual and tabular candidates,results on textual and tabular candidates,0.5121294260025024
translation,79,156,results,results,both,textual and tabular candidates,results both textual and tabular candidates,0.6567226052284241
translation,79,159,results,recalls,on,hybrid inputs,recalls on hybrid inputs,0.5671827793121338
translation,79,159,results,hybrid inputs,are,almost the same as or even better,hybrid inputs are almost the same as or even better,0.6074483394622803
translation,79,159,results,almost the same as or even better,than,best recalls,almost the same as or even better than best recalls,0.6149009466171265
translation,79,159,results,best recalls,on,individual textual or tabular inputs,best recalls on individual textual or tabular inputs,0.49796968698501587
translation,79,159,results,all datasets,has,recalls,all datasets has recalls,0.6299838423728943
translation,79,159,results,results,across,all datasets,results across all datasets,0.7128980159759521
translation,79,166,results,outperforms,on,test set,outperforms on test set,0.5777032375335693
translation,79,166,results,fid +,on,test set,fid + on test set,0.5962122082710266
translation,79,166,results,durepa,has,outperforms,durepa has outperforms,0.6335368156433105
translation,79,166,results,outperforms,has,fid +,outperforms has fid +,0.640224277973175
translation,79,166,results,results,has,durepa,results has durepa,0.5575500726699829
translation,79,168,results,durepa,achieved,close to 3x and 5x improvements,durepa achieved close to 3x and 5x improvements,0.7265695333480835
translation,79,168,results,close to 3x and 5x improvements,on,wikisql questions,close to 3x and 5x improvements on wikisql questions,0.5652961730957031
translation,79,168,results,wikisql questions,that have,superlative ( max / min ) and calculation ( sum / avg ) operations,wikisql questions that have superlative ( max / min ) and calculation ( sum / avg ) operations,0.6392044425010681
translation,79,168,results,results,has,durepa,results has durepa,0.5575500726699829
translation,79,171,results,hybrid evidence types,leads to,better performance,hybrid evidence types leads to better performance,0.6582829356193542
translation,79,171,results,results,Using,hybrid evidence types,results Using hybrid evidence types,0.6409810185432434
translation,79,173,results,best top - 1 em,is,34.0,best top - 1 em is 34.0,0.5651770234107971
translation,79,173,results,34.0,achieved by,model fid +,34.0 achieved by model fid +,0.6947654485702515
translation,79,173,results,model fid +,using,only textual candidates,model fid + using only textual candidates,0.6641823649406433
translation,79,173,results,single evidence type,has,best top - 1 em,single evidence type has best top - 1 em,0.5705019235610962
translation,79,174,results,hybrid model durepa,attains,significantly better top - 1 em,hybrid model durepa attains significantly better top - 1 em,0.6045607924461365
translation,79,174,results,significantly better top - 1 em,of,47.9,significantly better top - 1 em of 47.9,0.5445065498352051
translation,79,174,results,both evidence types,has,hybrid model durepa,both evidence types has hybrid model durepa,0.595634937286377
translation,79,174,results,results,use,both evidence types,results use both evidence types,0.5909517407417297
translation,79,175,results,better top - 1 em,compared to,fid +,better top - 1 em compared to fid +,0.7031108736991882
translation,79,175,results,model durepa,has,better top - 1 em,model durepa has better top - 1 em,0.5693402290344238
translation,79,175,results,results,observe,model durepa,results observe model durepa,0.6167510151863098
translation,79,179,results,durepa model,using,tabular evidences,durepa model using tabular evidences,0.6988934874534607
translation,79,179,results,durepa model,using,textual evidences,durepa model using textual evidences,0.6372068524360657
translation,79,179,results,behaves better,than,fid + model,behaves better than fid + model,0.6127636432647705
translation,79,179,results,fid + model,using,textual evidences,fid + model using textual evidences,0.6480198502540588
translation,79,179,results,tabular evidences,has,behaves better,tabular evidences has behaves better,0.603888988494873
translation,79,179,results,results,has,durepa model,results has durepa model,0.5411476492881775
translation,79,181,results,durepa and fid + models,behave,significantly worse,durepa and fid + models behave significantly worse,0.6733620762825012
translation,79,181,results,significantly worse,than,hybrid counterparts,significantly worse than hybrid counterparts,0.6087300777435303
translation,79,181,results,results,when using,only one type of evidence,results when using only one type of evidence,0.6043214201927185
translation,79,183,results,proposed framework durepa,brings,significant improvement,proposed framework durepa brings significant improvement,0.608936071395874
translation,79,183,results,significant improvement,on,answering questions,significant improvement on answering questions,0.4901304244995117
translation,79,183,results,answering questions,using,hybrid types of evidence,answering questions using hybrid types of evidence,0.6800298690795898
translation,79,184,results,questions,answered by,both supporting evidence types,questions answered by both supporting evidence types,0.6752292513847351
translation,79,184,results,our multi-modal method,shows,clear advantage,our multi-modal method shows clear advantage,0.652178943157196
translation,79,184,results,clear advantage,over,models,clear advantage over models,0.731482207775116
translation,79,184,results,models,using,single - type knowledge,models using single - type knowledge,0.6517333984375
translation,79,184,results,questions,has,our multi-modal method,questions has our multi-modal method,0.6313523054122925
translation,79,184,results,both supporting evidence types,has,our multi-modal method,both supporting evidence types has our multi-modal method,0.5906887054443359
translation,79,184,results,results,on,questions,results on questions,0.5082646012306213
translation,79,185,results,dual reader - parser,essential to,good performance,dual reader - parser essential to good performance,0.6429932713508606
translation,79,185,results,good performance,of,durepa,good performance of durepa,0.5670489072799683
translation,79,185,results,results,demonstrate,dual reader - parser,results demonstrate dual reader - parser,0.5936814546585083
translation,79,215,results,recalls,on,hybrid inputs,recalls on hybrid inputs,0.5671827793121338
translation,79,215,results,hybrid inputs,are,almost the same as or even better,hybrid inputs are almost the same as or even better,0.6074483394622803
translation,79,215,results,almost the same as or even better,than,best recalls,almost the same as or even better than best recalls,0.6149009466171265
translation,79,215,results,best recalls,on,individual textual or tabular inputs,best recalls on individual textual or tabular inputs,0.49796968698501587
translation,79,215,results,results,has,recalls,results has recalls,0.5936073660850525
translation,80,5,baselines,cltr,extends,current state - of - the - art qa over tables model,cltr extends current state - of - the - art qa over tables model,0.6732209324836731
translation,80,5,baselines,current state - of - the - art qa over tables model,to build,end-to - end table qa architecture,current state - of - the - art qa over tables model to build end-to - end table qa architecture,0.6617145538330078
translation,80,5,baselines,baselines,has,cltr,baselines has cltr,0.5854011178016663
translation,80,25,baselines,pool of tables,from,large table corpus,pool of tables from large table corpus,0.5502581596374512
translation,80,25,baselines,large table corpus,with,coarse-grained but inexpensive ir method,large table corpus with coarse-grained but inexpensive ir method,0.5929219722747803
translation,80,96,baselines,table qa benchmark,with,nlqs,table qa benchmark with nlqs,0.6273926496505737
translation,80,101,experimental-setup,training batch size,=,128,training batch size = 128,0.6441283226013184
translation,80,101,experimental-setup,number of epochs,=,2,number of epochs = 2,0.6719597578048706
translation,80,101,experimental-setup,experimental setup,has,rci model,experimental setup has rci model,0.5337737798690796
translation,80,24,model,table retrieval and qa over tables system,called,cell level table retrieval ( cltr ),table retrieval and qa over tables system called cell level table retrieval ( cltr ),0.7031853795051575
translation,80,24,model,model,propose,table retrieval and qa over tables system,model propose table retrieval and qa over tables system,0.6938705444335938
translation,80,24,model,model,called,cell level table retrieval ( cltr ),model called cell level table retrieval ( cltr ),0.7223793864250183
translation,80,26,model,table cells,as,answers,table cells as answers,0.546125590801239
translation,80,112,results,outperforms,with,average improvement,outperforms with average improvement,0.6694551706314087
translation,80,112,results,current best m t r pair model,on,all metrics,current best m t r pair model on all metrics,0.49416443705558777
translation,80,112,results,current best m t r pair model,with,average improvement,current best m t r pair model with average improvement,0.6170433759689331
translation,80,112,results,average improvement,of,28.73 %,average improvement of 28.73 %,0.5447200536727905
translation,80,112,results,average improvement,of,3.43 %,average improvement of 3.43 %,0.5475846529006958
translation,80,112,results,average improvement,of,13.40 %,average improvement of 13.40 %,0.5445771813392639
translation,80,112,results,28.73 %,on,precision,28.73 % on precision,0.5333061814308167
translation,80,112,results,3.43 %,on,ndcg,3.43 % on ndcg,0.5407118797302246
translation,80,112,results,13.40 %,on,map,13.40 % on map,0.6070194244384766
translation,80,112,results,proposed model,has,outperforms,proposed model has outperforms,0.642342746257782
translation,80,112,results,outperforms,has,current best m t r pair model,outperforms has current best m t r pair model,0.5818002223968506
translation,80,118,results,our approach,able to achieve,promising and consistent performance,our approach able to achieve promising and consistent performance,0.6452019810676575
translation,80,119,results,cltr,performs,better,cltr performs better,0.7109007835388184
translation,80,119,results,better,for,"first benchmark , e2e wtq","better for first benchmark , e2e wtq",0.6051509380340576
translation,80,119,results,results,indicate,cltr,results indicate cltr,0.5925074219703674
translation,80,122,results,current state - of - the - art models,on,table retrieval,current state - of - the - art models on table retrieval,0.5181969404220581
translation,80,122,results,cltr,has,outperforms,cltr has outperforms,0.6726071834564209
translation,80,122,results,outperforms,has,all baselines,outperforms has all baselines,0.5888515114784241
translation,81,256,ablation-analysis,bert ( frozen ) and w2v variants,are,weaker,bert ( frozen ) and w2v variants are weaker,0.602854311466217
translation,81,256,ablation-analysis,ablation analysis,has,bert ( frozen ) and w2v variants,ablation analysis has bert ( frozen ) and w2v variants,0.5541306138038635
translation,81,276,ablation-analysis,both forward and backward transfers,are,effective,both forward and backward transfers are effective,0.6026939749717712
translation,81,276,ablation-analysis,ablation analysis,see,both forward and backward transfers,ablation analysis see both forward and backward transfers,0.6038540601730347
translation,81,279,ablation-analysis,each of the components,is,effective,each of the components is effective,0.6032114028930664
translation,81,279,ablation-analysis,each of the components,work,in concert,each of the components work in concert,0.6175940632820129
translation,81,279,ablation-analysis,in concert,to produce,best final result,in concert to produce best final result,0.7318052053451538
translation,81,279,ablation-analysis,ablation analysis,shows,each of the components,ablation analysis shows each of the components,0.6427532434463501
translation,81,203,baselines,bert,with,fine-tuning,bert with fine-tuning,0.6896287798881531
translation,81,203,baselines,bert ( frozen ),without finetuning,adapter-bert,bert ( frozen ) without finetuning adapter-bert,0.7585071921348572
translation,81,203,baselines,w2v,trained with,amazon review data,w2v trained with amazon review data,0.7338240146636963
translation,81,203,baselines,w2v,using,"fasttext ( grave et al. , 2018 )","w2v using fasttext ( grave et al. , 2018 )",0.6073086261749268
translation,81,203,baselines,word2vec embeddings,trained with,amazon review data,word2vec embeddings trained with amazon review data,0.6618720889091492
translation,81,203,baselines,amazon review data,using,"fasttext ( grave et al. , 2018 )","amazon review data using fasttext ( grave et al. , 2018 )",0.6401327252388
translation,81,203,baselines,adapter-bert,has,"houlsby et al. , 2019 )","adapter-bert has houlsby et al. , 2019 )",0.6043873429298401
translation,81,203,baselines,w2v,has,word2vec embeddings,w2v has word2vec embeddings,0.5312937498092651
translation,81,204,baselines,csc,creates,another 4 variants,csc creates another 4 variants,0.6860532760620117
translation,81,204,baselines,contrastive supervised learning,of,current task,contrastive supervised learning of current task,0.5625358819961548
translation,81,204,baselines,csc,has,contrastive supervised learning,csc has contrastive supervised learning,0.539366602897644
translation,81,205,baselines,asc network,taking,aspect term and review sentence,asc network taking aspect term and review sentence,0.6422182321548462
translation,81,205,baselines,aspect term and review sentence,as,input,aspect term and review sentence as input,0.49519234895706177
translation,81,205,baselines,input,for,bert variants,input for bert variants,0.6873489022254944
translation,81,206,baselines,w2v variants,use,concatenation,w2v variants use concatenation,0.6444248557090759
translation,81,207,baselines,baselines,has,continual learning ( cl ),baselines has continual learning ( cl ),0.5845546722412109
translation,81,208,baselines,38 baselines,in,5 categories,38 baselines in 5 categories,0.4990047216415405
translation,81,208,baselines,cl setting,has,38 baselines,cl setting has 38 baselines,0.5829006433486938
translation,81,208,baselines,baselines,has,cl setting,baselines has cl setting,0.531460165977478
translation,81,215,baselines,original image classification networks,with,cnn,original image classification networks with cnn,0.598374605178833
translation,81,215,baselines,cnn,for,text classification,cnn for text classification,0.5836858153343201
translation,81,217,baselines,ucl,is,recent til method,ucl is recent til method,0.6094633340835571
translation,81,217,baselines,baselines,has,ucl,baselines has ucl,0.6063944697380066
translation,81,235,hyperparameters,adapter,uses,2 layers of fully connected network,adapter uses 2 layers of fully connected network,0.6372610330581665
translation,81,235,hyperparameters,2 layers of fully connected network,dimensions,2000,2 layers of fully connected network dimensions 2000,0.6315926313400269
translation,81,235,hyperparameters,hyperparameters,has,adapter,hyperparameters has adapter,0.5577154755592346
translation,81,236,hyperparameters,task id embeddings,have,2000 dimensions,task id embeddings have 2000 dimensions,0.5407152771949768
translation,81,236,hyperparameters,hyperparameters,has,task id embeddings,hyperparameters has task id embeddings,0.4907762408256531
translation,81,238,hyperparameters,400,for,s max,400 for s max,0.6603901982307434
translation,81,238,hyperparameters,400,for,dropout,400 for dropout,0.6870142221450806
translation,81,238,hyperparameters,dropout,of,0.5,dropout of 0.5,0.6125851273536682
translation,81,238,hyperparameters,0.5,between,fully connected layers,0.5 between fully connected layers,0.6038256883621216
translation,81,238,hyperparameters,hyperparameters,use,400,hyperparameters use 400,0.6339437961578369
translation,81,238,hyperparameters,hyperparameters,use,dropout,hyperparameters use dropout,0.6254391074180603
translation,81,239,hyperparameters,temperature,in,each contrastive objective,temperature in each contrastive objective,0.5396864414215088
translation,81,239,hyperparameters,each contrastive objective,set to,1,each contrastive objective set to 1,0.7198143601417542
translation,81,239,hyperparameters,hyperparameters,has,temperature,hyperparameters has temperature,0.5350812077522278
translation,81,240,hyperparameters,weight,of,each objective,weight of each objective,0.5895471572875977
translation,81,240,hyperparameters,each objective,set to,1,each objective set to 1,0.6929543018341064
translation,81,240,hyperparameters,hyperparameters,has,weight,hyperparameters has weight,0.5009066462516785
translation,81,245,hyperparameters,max length,of,sum of sentence and aspect,max length of sum of sentence and aspect,0.5790707468986511
translation,81,245,hyperparameters,sum of sentence and aspect,is,128,sum of sentence and aspect is 128,0.579586923122406
translation,81,245,hyperparameters,hyperparameters,has,max length,hyperparameters has max length,0.5074495077133179
translation,81,246,hyperparameters,adam optimizer,set,learning rate,adam optimizer set learning rate,0.6309201717376709
translation,81,246,hyperparameters,hyperparameters,use,adam optimizer,hyperparameters use adam optimizer,0.6090166568756104
translation,81,247,hyperparameters,semeval datasets,for,all other datasets,semeval datasets for all other datasets,0.5257278084754944
translation,81,247,hyperparameters,30 epochs,used,results,30 epochs used results,0.6193733215332031
translation,81,247,hyperparameters,30 epochs,based on,results,30 epochs based on results,0.6440513134002686
translation,81,247,hyperparameters,results,from,validation data,results from validation data,0.548320472240448
translation,81,247,hyperparameters,semeval datasets,has,10 epochs,semeval datasets has 10 epochs,0.5844972133636475
translation,81,247,hyperparameters,semeval datasets,has,30 epochs,semeval datasets has 30 epochs,0.5780022740364075
translation,81,247,hyperparameters,all other datasets,has,30 epochs,all other datasets has 30 epochs,0.5683358907699585
translation,81,247,hyperparameters,hyperparameters,For,semeval datasets,hyperparameters For semeval datasets,0.5243437886238098
translation,81,247,hyperparameters,hyperparameters,for,all other datasets,hyperparameters for all other datasets,0.47799068689346313
translation,81,248,hyperparameters,runs,use,batch size,runs use batch size,0.6805530786514282
translation,81,248,hyperparameters,batch size,has,32,batch size has 32,0.630264163017273
translation,81,248,hyperparameters,hyperparameters,has,runs,hyperparameters has runs,0.5537269711494446
translation,81,249,hyperparameters,cl baselines,train,all models,cl baselines train all models,0.6892093420028687
translation,81,249,hyperparameters,all models,with,learning rate,all models with learning rate,0.60528564453125
translation,81,249,hyperparameters,all models,with,early - stop training,all models with early - stop training,0.6125345826148987
translation,81,249,hyperparameters,all models,set,batch size,all models set batch size,0.7037363052368164
translation,81,249,hyperparameters,learning rate,of,0.05,learning rate of 0.05,0.6214359998703003
translation,81,249,hyperparameters,early - stop training,when,no improvement,early - stop training when no improvement,0.6379125714302063
translation,81,249,hyperparameters,no improvement,in,validation loss,no improvement in validation loss,0.4878521263599396
translation,81,249,hyperparameters,validation loss,for,5 epochs,validation loss for 5 epochs,0.5681278705596924
translation,81,249,hyperparameters,hyperparameters,For,cl baselines,hyperparameters For cl baselines,0.5964531898498535
translation,81,8,model,novel model,called,classic,novel model called classic,0.7164389491081238
translation,81,8,model,model,proposes,novel model,model proposes novel model,0.7164400219917297
translation,81,9,model,key novelty,is,contrastive continual learning method,key novelty is contrastive continual learning method,0.5120899081230164
translation,81,9,model,contrastive continual learning method,enables,knowledge transfer,contrastive continual learning method enables knowledge transfer,0.6422131657600403
translation,81,9,model,contrastive continual learning method,enables,knowledge distillation,contrastive continual learning method enables knowledge distillation,0.6401556730270386
translation,81,9,model,knowledge transfer,across,tasks,knowledge transfer across tasks,0.6109059453010559
translation,81,9,model,knowledge distillation,from,old tasks,knowledge distillation from old tasks,0.5357193946838379
translation,81,9,model,model,has,key novelty,model has key novelty,0.5159850716590881
translation,81,37,model,novel model,called,classic,novel model called classic,0.7164389491081238
translation,81,37,model,novel model,in,dil setting,novel model in dil setting,0.5857995748519897
translation,81,37,model,continual and contrastive learning,for,aspect sentiment classification,continual and contrastive learning for aspect sentiment classification,0.6144641637802124
translation,81,37,model,classic,has,continual and contrastive learning,classic has continual and contrastive learning,0.5895887613296509
translation,81,37,model,model,propose,novel model,model propose novel model,0.6891457438468933
translation,81,44,model,new model,called,classic,new model called classic,0.7334461212158203
translation,81,44,model,new model,uses,adapters,new model uses adapters,0.6215675473213196
translation,81,44,model,adapters,to incorporate,pretrained bert,adapters to incorporate pretrained bert,0.7020784616470337
translation,81,44,model,pretrained bert,into,asc continual learning,pretrained bert into asc continual learning,0.6388400197029114
translation,81,44,model,novel contrastive continual learning method,for,knowledge transfer and distillation,novel contrastive continual learning method for knowledge transfer and distillation,0.5947847366333008
translation,81,44,model,task masks,to isolate,task -specific knowledge,task masks to isolate task -specific knowledge,0.6905819177627563
translation,81,44,model,task -specific knowledge,to avoid,cf,task -specific knowledge to avoid cf,0.6976957321166992
translation,81,44,model,model,proposes,new model,model proposes new model,0.7315190434455872
translation,81,44,model,model,called,classic,model called classic,0.73288893699646
translation,81,91,model,classic,uses,three sub-systems,classic uses three sub-systems,0.6446373462677002
translation,81,91,model,contrastive ensemble distillation,for mitigating,cf,contrastive ensemble distillation for mitigating cf,0.7023903131484985
translation,81,91,model,knowledge,of,previous tasks,knowledge of previous tasks,0.5323836207389832
translation,81,91,model,knowledge,to,current task model,knowledge to current task model,0.5062599778175354
translation,81,91,model,previous tasks,to,current task model,previous tasks to current task model,0.5148934721946716
translation,81,91,model,contrastive knowledge sharing ( cks ),to encourage,knowledge transfer,contrastive knowledge sharing ( cks ) to encourage knowledge transfer,0.7008083462715149
translation,81,91,model,contrastive supervised learning,on,current task model ( csc ),contrastive supervised learning on current task model ( csc ),0.5712440609931946
translation,81,91,model,contrastive supervised learning,to improve,current task model accuracy,contrastive supervised learning to improve current task model accuracy,0.635350227355957
translation,81,91,model,current task model ( csc ),to improve,current task model accuracy,current task model ( csc ) to improve current task model accuracy,0.6345348358154297
translation,81,91,model,contrastive ensemble distillation,has,ced ),contrastive ensemble distillation has ced ),0.6175643801689148
translation,81,91,model,model,uses,three sub-systems,model uses three sub-systems,0.5931350588798523
translation,81,91,model,model,has,classic,model has classic,0.5846225619316101
translation,81,198,model,dil,adapt,recent til systems,dil adapt recent til systems,0.810191810131073
translation,81,198,model,dil,by merging,classification heads,dil by merging classification heads,0.7636361122131348
translation,81,198,model,recent til systems,to,dil,recent til systems to dil,0.6408712863922119
translation,81,198,model,dil,by merging,classification heads,dil by merging classification heads,0.7636361122131348
translation,81,198,model,classification heads,to form,dil systems,classification heads to form dil systems,0.6715396642684937
translation,81,198,model,model,adapt,recent til systems,model adapt recent til systems,0.7559109330177307
translation,81,237,model,fully connected layer,with,softmax output,fully connected layer with softmax output,0.5902058482170105
translation,81,237,model,softmax output,used as,classification head,softmax output used as classification head,0.6461495757102966
translation,81,237,model,classification head,in,last layer of bert,classification head in last layer of bert,0.5743628144264221
translation,81,237,model,model,has,fully connected layer,model has fully connected layer,0.5389924645423889
translation,81,254,results,classic,has,outperforms,classic has outperforms,0.6287333369255066
translation,81,254,results,outperforms,has,all baselines,outperforms has all baselines,0.5888515114784241
translation,81,254,results,all baselines,has,markedly,all baselines has markedly,0.5953255891799927
translation,81,254,results,results,shows,classic,results shows classic,0.49953052401542664
translation,81,255,results,adapter - bert,performs,similarly,adapter - bert performs similarly,0.7055947184562683
translation,81,255,results,similarly,to,bert,similarly to bert,0.6434803605079651
translation,81,255,results,non-continual learning baselines,has,adapter - bert,non-continual learning baselines has adapter - bert,0.6144148707389832
translation,81,255,results,results,For,non-continual learning baselines,results For non-continual learning baselines,0.5887065529823303
translation,81,257,results,one variants and ncl variants,see that,w2v,one variants and ncl variants see that w2v,0.6517987251281738
translation,81,257,results,one variants and ncl variants,under,w2v,one variants and ncl variants under w2v,0.6426141858100891
translation,81,257,results,ncl variants,much bet-ter than,one variants,ncl variants much bet-ter than one variants,0.618475615978241
translation,81,257,results,one variants and ncl variants,has,ncl variants,one variants and ncl variants has ncl variants,0.569621205329895
translation,81,257,results,w2v,has,ncl variants,w2v has ncl variants,0.5848621129989624
translation,81,257,results,results,Comparing,one variants and ncl variants,results Comparing one variants and ncl variants,0.6010481119155884
translation,81,263,results,various continual learning ( cl ) baselines,with,bert ( frozen ),various continual learning ( cl ) baselines with bert ( frozen ),0.6410025358200073
translation,81,263,results,markedly weaker,than,classic,markedly weaker than classic,0.6137980222702026
translation,81,263,results,results,has,various continual learning ( cl ) baselines,results has various continual learning ( cl ) baselines,0.5260397791862488
translation,81,266,results,w2v based cl baselines,are,even weaker,w2v based cl baselines are even weaker,0.5405018925666809
translation,81,266,results,results,has,w2v based cl baselines,results has w2v based cl baselines,0.5374657511711121
translation,81,282,results,classic,has,outperforms,classic has outperforms,0.6287333369255066
translation,81,282,results,outperforms,has,state - of - the - art baselines,outperforms has state - of - the - art baselines,0.5583442449569702
translation,82,239,ablation-analysis,meaningful effect,on,performance,meaningful effect on performance,0.5563914775848389
translation,82,239,ablation-analysis,each dataset d,has,meaningful effect,each dataset d has meaningful effect,0.5548297762870789
translation,82,239,ablation-analysis,ablation analysis,has,each dataset d,ablation analysis has each dataset d,0.5642882585525513
translation,82,304,experimental-setup,dimension,of,character - level embedding vectors,dimension of character - level embedding vectors,0.5524727702140808
translation,82,304,experimental-setup,character - level embedding vectors,is,32,character - level embedding vectors is 32,0.5266028046607971
translation,82,304,experimental-setup,experimental setup,has,dimension,experimental setup has dimension,0.4825572967529297
translation,82,305,experimental-setup,number of windows,is,100,number of windows is 100,0.6108194589614868
translation,82,305,experimental-setup,experimental setup,has,number of windows,experimental setup has number of windows,0.5372445583343506
translation,82,306,experimental-setup,dimension,of,hidden vectors,dimension of hidden vectors,0.6041334867477417
translation,82,306,experimental-setup,dimension,is,300,dimension is 300,0.659263551235199
translation,82,306,experimental-setup,hidden vectors,is,300,hidden vectors is 300,0.6077172756195068
translation,82,306,experimental-setup,experimental setup,has,dimension,experimental setup has dimension,0.4825572967529297
translation,82,307,experimental-setup,dimension,of,latent variables,dimension of latent variables,0.5309814810752869
translation,82,307,experimental-setup,latent variables,is,200,latent variables is 200,0.6001316905021667
translation,82,307,experimental-setup,experimental setup,has,dimension,experimental setup has dimension,0.4825572967529297
translation,82,308,experimental-setup,experimental setup,has,lstms,experimental setup has lstms,0.5316953659057617
translation,82,309,experimental-setup,"adam ( kingma and ba , 2014 )",for,optimization,"adam ( kingma and ba , 2014 ) for optimization",0.6054565906524658
translation,82,309,experimental-setup,"adam ( kingma and ba , 2014 )",with,initial learning rate,"adam ( kingma and ba , 2014 ) with initial learning rate",0.5918858647346497
translation,82,309,experimental-setup,optimization,with,initial learning rate,optimization with initial learning rate,0.5914350748062134
translation,82,309,experimental-setup,initial learning rate,has,0.001,initial learning rate has 0.001,0.5123841762542725
translation,82,309,experimental-setup,experimental setup,used,"adam ( kingma and ba , 2014 )","experimental setup used adam ( kingma and ba , 2014 )",0.6106398105621338
translation,82,310,experimental-setup,parameters,initialized with,xavier initialization,parameters initialized with xavier initialization,0.7887275815010071
translation,82,310,experimental-setup,experimental setup,has,parameters,experimental setup has parameters,0.4818422794342041
translation,82,311,experimental-setup,models,trained for,16 epochs,models trained for 16 epochs,0.7627876400947571
translation,82,311,experimental-setup,16 epochs,with,batch size,16 epochs with batch size,0.601567804813385
translation,82,311,experimental-setup,batch size,of,32,batch size of 32,0.6741614937782288
translation,82,311,experimental-setup,experimental setup,trained for,16 epochs,experimental setup trained for 16 epochs,0.7314857244491577
translation,82,311,experimental-setup,experimental setup,has,models,experimental setup has models,0.5060054659843445
translation,82,312,experimental-setup,"dropout ( srivastava et al. , 2014 ) rate",of,0.2,"dropout ( srivastava et al. , 2014 ) rate of 0.2",0.5728970170021057
translation,82,312,experimental-setup,0.2,for,all the lstm layers and attention modules,0.2 for all the lstm layers and attention modules,0.5830399394035339
translation,82,312,experimental-setup,experimental setup,used,"dropout ( srivastava et al. , 2014 ) rate","experimental setup used dropout ( srivastava et al. , 2014 ) rate",0.5878350138664246
translation,82,328,experiments,token,has,total number of the generated words,token has total number of the generated words,0.5619158744812012
translation,82,10,model,variational qag model,generates,multiple diverse qa pairs,variational qag model generates multiple diverse qa pairs,0.645402729511261
translation,82,10,model,multiple diverse qa pairs,from,paragraph,multiple diverse qa pairs from paragraph,0.6089301109313965
translation,82,10,model,model,present,variational qag model,model present variational qag model,0.6512840390205383
translation,82,26,model,model,propose,variational qag model ( vqag ),model propose variational qag model ( vqag ),0.6584787368774414
translation,82,27,model,two independent latent random variables,into,our model,two independent latent random variables into our model,0.5382111668586731
translation,82,27,model,two independent latent random variables,to learn,two one- to -many relationships,two independent latent random variables to learn two one- to -many relationships,0.5964640378952026
translation,82,27,model,two one- to -many relationships,in,ae and qg,two one- to -many relationships in ae and qg,0.5673253536224365
translation,82,27,model,two one- to -many relationships,by utilizing,neural variational inference,two one- to -many relationships by utilizing neural variational inference,0.602158784866333
translation,82,27,model,model,introduce,two independent latent random variables,model introduce two independent latent random variables,0.6753305196762085
translation,82,27,model,model,to learn,two one- to -many relationships,model to learn two one- to -many relationships,0.6452417373657227
translation,82,28,model,randomness,of,latent variables,randomness of latent variables,0.5996643304824829
translation,82,28,model,latent variables,enables,our model,latent variables enables our model,0.6243035793304443
translation,82,28,model,our model,to generate,diverse answers and questions,our model to generate diverse answers and questions,0.6557651162147522
translation,82,28,model,model,Incorporating,randomness,model Incorporating randomness,0.7800903916358948
translation,82,28,model,model,to generate,diverse answers and questions,model to generate diverse answers and questions,0.7083467245101929
translation,82,302,model,pretrained word embeddings,shared by,input layer,pretrained word embeddings shared by input layer,0.5780025720596313
translation,82,302,model,pretrained word embeddings,shared by,input and output layers,pretrained word embeddings shared by input and output layers,0.5731799602508545
translation,82,302,model,input layer,of,context encoder,input layer of context encoder,0.5324243307113647
translation,82,302,model,input and output layers,of,question decoder,input and output layers of question decoder,0.5850071310997009
translation,82,302,model,model,has,pretrained word embeddings,model has pretrained word embeddings,0.5362339019775391
translation,82,29,results,effect,of controlling,kullback - leibler ( kl ) term,effect of controlling kullback - leibler ( kl ) term,0.6842060089111328
translation,82,29,results,kullback - leibler ( kl ) term,in,variational lower bound,kullback - leibler ( kl ) term in variational lower bound,0.5085699558258057
translation,82,29,results,variational lower bound,for mitigating,posterior collapse issue,variational lower bound for mitigating posterior collapse issue,0.6601148247718811
translation,82,29,results,results,study,effect,results study effect,0.7281866073608398
translation,82,208,results,without data augmentation,degraded,performance,without data augmentation degraded performance,0.7574479579925537
translation,82,208,results,performance,on,12 challenge sets,performance on 12 challenge sets,0.5030935406684875
translation,82,208,results,qa model,has,without data augmentation,qa model has without data augmentation,0.5996174812316895
translation,82,208,results,results,discovered that,qa model,results discovered that qa model,0.7016552090644836
translation,82,209,results,data augmentation,using,qag,data augmentation using qag,0.7185182571411133
translation,82,209,results,improved,except for,harqg,improved except for harqg,0.6768694519996643
translation,82,209,results,data augmentation,has,indistribution scores,data augmentation has indistribution scores,0.5671292543411255
translation,82,209,results,qag,has,indistribution scores,qag has indistribution scores,0.5983582735061646
translation,82,209,results,results,With,data augmentation,results With data augmentation,0.6018756628036499
translation,82,210,results,single model setting,on,challenge sets,single model setting on challenge sets,0.5331812500953674
translation,82,210,results,semqg,achieved,best performance,semqg achieved best performance,0.7515363097190857
translation,82,210,results,best performance,on,quo and add,best performance on quo and add,0.608939528465271
translation,82,210,results,single model setting,has,semqg,single model setting has semqg,0.5919414758682251
translation,82,210,results,challenge sets,has,semqg,challenge sets has semqg,0.6038368940353394
translation,82,210,results,results,In,single model setting,results In single model setting,0.5226742625236511
translation,82,211,results,infohcvae,achieved,best performance,infohcvae achieved best performance,0.7681654095649719
translation,82,211,results,best performance,on,news,best performance on news,0.5635433793067932
translation,82,211,results,best performance,on,nq,best performance on nq,0.5571553707122803
translation,82,211,results,best performance,on,hard,best performance on hard,0.5855953693389893
translation,82,211,results,best performance,on,addo,best performance on addo,0.5954092144966125
translation,82,211,results,best performance,on,mt,best performance on mt,0.5631582140922546
translation,82,211,results,best performance,on,asr,best performance on asr,0.554385244846344
translation,82,211,results,best performance,on,kb,best performance on kb,0.556228518486023
translation,82,211,results,results,has,infohcvae,results has infohcvae,0.5731799006462097
translation,82,212,results,vqag,achieved,best performance,vqag achieved best performance,0.7621234059333801
translation,82,212,results,best performance,on,"para , apara , and imp","best performance on para , apara , and imp",0.5228738188743591
translation,82,212,results,results,has,vqag,results has vqag,0.6061160564422607
translation,82,214,results,scores,on,"squad du test , news , mt , and asr","scores on squad du test , news , mt , and asr",0.5194361209869385
translation,82,214,results,"squad du test , news , mt , and asr",further improved with,"{ + sem , + info , +v}","squad du test , news , mt , and asr further improved with { + sem , + info , +v}",0.7192719578742981
translation,82,214,results,ensemble setting,has,scores,ensemble setting has scores,0.5687254667282104
translation,82,214,results,results,In,ensemble setting,results In ensemble setting,0.5359047055244446
translation,82,221,results,most significant performance gap ( > 30 f1 ),between,upper bound and the no data augmentation baseline,most significant performance gap ( > 30 f1 ) between upper bound and the no data augmentation baseline,0.61258465051651
translation,82,221,results,upper bound and the no data augmentation baseline,observed,quo,upper bound and the no data augmentation baseline observed quo,0.6321220397949219
translation,82,221,results,upper bound and the no data augmentation baseline,in,quo,upper bound and the no data augmentation baseline in quo,0.5387519598007202
translation,82,224,results,nq,more prominent than,news,nq more prominent than news,0.7028939127922058
translation,82,224,results,results,improvement in,nq,results improvement in nq,0.5590658783912659
translation,82,232,results,semqg,degraded,scores,semqg degraded scores,0.6518065929412842
translation,82,232,results,scores,on,hard and mt,scores on hard and mt,0.5705715417861938
translation,82,232,results,scores,on,para,scores on para,0.6364766359329224
translation,82,232,results,scores,on,para,scores on para,0.6364766359329224
translation,82,232,results,infohcvae,degraded,score,infohcvae degraded score,0.7257747054100037
translation,82,232,results,score,on,para,score on para,0.6124438047409058
translation,82,232,results,results,has,semqg,results has semqg,0.5238283276557922
translation,82,264,results,results,has,result,results has result,0.5303357243537903
translation,82,265,results,our models,with,c = 0,our models with c = 0,0.6459701657295227
translation,82,265,results,c = 0,superior to,pipeline model,c = 0 superior to pipeline model,0.7566931843757629
translation,82,265,results,c = 0,introducing,latent random variables,c = 0 introducing latent random variables,0.720746636390686
translation,82,265,results,latent random variables,aid,qa pair modeling capacity,latent random variables aid qa pair modeling capacity,0.677169680595398
translation,82,265,results,results,has,our models,results has our models,0.5733726620674133
translation,83,196,ablation-analysis,number of relevant premises,exceeds,4,number of relevant premises exceeds 4,0.6229332089424133
translation,83,196,ablation-analysis,accuracy,starts to,decrease,accuracy starts to decrease,0.6707173585891724
translation,83,196,ablation-analysis,number of relevant premises,has,accuracy,number of relevant premises has accuracy,0.5340333580970764
translation,83,196,ablation-analysis,ablation analysis,when,number of relevant premises,ablation analysis when number of relevant premises,0.6269370913505554
translation,83,5,model,qa,determines,supporting knowledge bases,qa determines supporting knowledge bases,0.7081806063652039
translation,83,5,model,supporting knowledge bases,entail,hypotheses,supporting knowledge bases entail hypotheses,0.7145809531211853
translation,83,5,model,model,has,qa,model has qa,0.6210176348686218
translation,83,6,model,model,investigate,neural-symbolic qa,model investigate neural-symbolic qa,0.6851060390472412
translation,83,7,model,proposed model,gradually bridges,hypothesis,proposed model gradually bridges hypothesis,0.6600043773651123
translation,83,7,model,proposed model,gradually bridges,candidate premises,proposed model gradually bridges candidate premises,0.725675106048584
translation,83,7,model,hypothesis,to build,proof paths,hypothesis to build proof paths,0.7615702152252197
translation,83,7,model,candidate premises,following,natural logic inference steps,candidate premises following natural logic inference steps,0.6148681044578552
translation,83,7,model,model,gradually bridges,hypothesis,model gradually bridges hypothesis,0.7267340421676636
translation,83,7,model,model,gradually bridges,candidate premises,model gradually bridges candidate premises,0.7464561462402344
translation,83,7,model,model,has,proposed model,model has proposed model,0.566501796245575
translation,83,18,model,model,investigate,neural-symbolic qa,model investigate neural-symbolic qa,0.6851060390472412
translation,83,22,model,core idea,of,neunli,core idea of neunli,0.642155110836029
translation,83,22,model,neunli,bridging,hypothesis and candidate premises,neunli bridging hypothesis and candidate premises,0.7444116473197937
translation,83,22,model,neunli,incorporating,neural models,neunli incorporating neural models,0.7427292466163635
translation,83,22,model,hypothesis and candidate premises,by following,natural logic inference steps,hypothesis and candidate premises by following natural logic inference steps,0.6810514330863953
translation,83,22,model,neural models,build,proof paths,neural models build proof paths,0.7039405107498169
translation,83,22,model,model,has,core idea,model has core idea,0.5185322761535645
translation,83,64,model,model,propose,neural natural logic inference ( neunli ) framework,model propose neural natural logic inference ( neunli ) framework,0.668005645275116
translation,83,167,model,bert,for,qa,bert for qa,0.7299748063087463
translation,83,162,results,our method,performs,better,our method performs better,0.6336022615432739
translation,83,162,results,"naturalli ( angeli et al. , 2016 )",has,our method,"naturalli ( angeli et al. , 2016 ) has our method",0.5481876730918884
translation,83,162,results,results,Compared with,"naturalli ( angeli et al. , 2016 )","results Compared with naturalli ( angeli et al. , 2016 )",0.677406907081604
translation,83,164,results,comparison,between,"hyperqa ( tay et al. , 2018 )","comparison between hyperqa ( tay et al. , 2018 )",0.5752477049827576
translation,83,164,results,neunli,shows,natural logicpowered neural networks,neunli shows natural logicpowered neural networks,0.5741940140724182
translation,83,164,results,natural logicpowered neural networks,achieve,better performance,natural logicpowered neural networks achieve better performance,0.6400538086891174
translation,83,164,results,better performance,on,qa datasets,better performance on qa datasets,0.5226972103118896
translation,83,164,results,results,has,comparison,results has comparison,0.5800483226776123
translation,83,166,results,our method,performs,better,our method performs better,0.6336022615432739
translation,83,166,results,better,than,"sem-bert ( zhang et al. , 2020 )","better than sem-bert ( zhang et al. , 2020 )",0.5865683555603027
translation,83,166,results,results,has,our method,results has our method,0.5589964985847473
translation,83,169,results,neunli,outperforms,neunli -e,neunli outperforms neunli -e,0.729171872138977
translation,83,169,results,results,has,neunli,results has neunli,0.5913001298904419
translation,83,170,results,neunli,achieves,best results,neunli achieves best results,0.6678844094276428
translation,83,170,results,best results,on,test set,best results on test set,0.5577713847160339
translation,83,170,results,test set,with,two different knowledge bases,test set with two different knowledge bases,0.6019904613494873
translation,83,170,results,results,has,neunli,results has neunli,0.5913001298904419
translation,83,171,results,model,with,larger knowledge base scitext,model with larger knowledge base scitext,0.6420919895172119
translation,83,171,results,results,notice,model,results notice model,0.6808047294616699
translation,83,172,results,results,on,qa - l test set,results on qa - l test set,0.5753707885742188
translation,83,172,results,results,on,qa - s test set,results on qa - s test set,0.584210216999054
translation,83,186,results,score,of,neunli,score of neunli,0.6322904229164124
translation,83,186,results,neunli,is,significantly higher,neunli is significantly higher,0.6199207305908203
translation,83,186,results,significantly higher,than,naturalli,significantly higher than naturalli,0.635850191116333
translation,83,186,results,results,observe that,score,results observe that score,0.5773732662200928
translation,83,194,results,accuracy,continues to,increase,accuracy continues to increase,0.6916424036026001
translation,83,194,results,increase,as,number of relevant premises,increase as number of relevant premises,0.5275041460990906
translation,83,194,results,increases,from,1 to 4,increases from 1 to 4,0.648023247718811
translation,83,194,results,number of relevant premises,has,increases,number of relevant premises has increases,0.5979055166244507
translation,83,194,results,results,has,accuracy,results has accuracy,0.5888755321502686
translation,83,198,results,neunli,find,performance,neunli find performance,0.6092674732208252
translation,83,198,results,with neunli w/o reasoning,find,performance,with neunli w/o reasoning find performance,0.6354713439941406
translation,83,198,results,neunli,has,with neunli w/o reasoning,neunli has with neunli w/o reasoning,0.6695316433906555
translation,83,198,results,performance,has,improves,performance has improves,0.6178221106529236
translation,83,198,results,improves,has,significantly,improves has significantly,0.6490503549575806
translation,83,198,results,results,Comparing,neunli,results Comparing neunli,0.6372129917144775
translation,83,199,results,57.35 % to 64.71 %,on,qa - s test set,57.35 % to 64.71 % on qa - s test set,0.5724848508834839
translation,83,199,results,qa - s test set,with,barron 's knowledge base,qa - s test set with barron 's knowledge base,0.6256560683250427
translation,83,199,results,results,has,accuracy score,results has accuracy score,0.5445011258125305
translation,84,160,ablation-analysis,logical form accuracy,of,models,logical form accuracy of models,0.6248318552970886
translation,84,160,ablation-analysis,models,trained with,hardem or hardem - thres,models trained with hardem or hardem - thres,0.7903718948364258
translation,84,160,ablation-analysis,drop,to,below 14 %,drop to below 14 %,0.6597377061843872
translation,84,160,ablation-analysis,hard train set,has,logical form accuracy,hard train set has logical form accuracy,0.5691384673118591
translation,84,160,ablation-analysis,hardem or hardem - thres,has,drop,hardem or hardem - thres has drop,0.6802299618721008
translation,84,160,ablation-analysis,ablation analysis,When using,hard train set,ablation analysis When using hard train set,0.7166096568107605
translation,84,165,ablation-analysis,bart base,as,question resconstructor,bart base as question resconstructor,0.5552771091461182
translation,84,165,ablation-analysis,question reconstructor,contributes to,performance improvements,question reconstructor contributes to performance improvements,0.6843157410621643
translation,84,165,ablation-analysis,ablation analysis,used,bart base,ablation analysis used bart base,0.6250385046005249
translation,84,165,ablation-analysis,ablation analysis,investigated,question reconstructor,ablation analysis investigated question reconstructor,0.7232000827789307
translation,84,89,baselines,"hardem ( min et al. , 2019 )",maximizes,"log max z?z p ? ( z|d , q )","hardem ( min et al. , 2019 ) maximizes log max z?z p ? ( z|d , q )",0.7372205257415771
translation,84,89,baselines,baselines,has,"hardem ( min et al. , 2019 )","baselines has hardem ( min et al. , 2019 )",0.5440672636032104
translation,84,90,baselines,hardem - thres,only on,confident solutions,hardem - thres only on confident solutions,0.6823626756668091
translation,84,90,baselines,baselines,has,hardem - thres,baselines has hardem - thres,0.6057292819023132
translation,84,176,baselines,three - layer transformer,without,pre-training,three - layer transformer without pre-training,0.6365359425544739
translation,84,176,baselines,three - layer transformer,pre-trained as,denoising auto-encoder,three - layer transformer pre-trained as denoising auto-encoder,0.7112647294998169
translation,84,176,baselines,denoising auto-encoder,of,questions,denoising auto-encoder of questions,0.593084990978241
translation,84,176,baselines,questions,on,train set,questions on train set,0.5118129253387451
translation,84,176,baselines,t-scratch,has,three - layer transformer,t-scratch has three - layer transformer,0.5464592576026917
translation,84,176,baselines,t-dae,has,three - layer transformer,t-dae has three - layer transformer,0.5300284624099731
translation,84,8,model,spurious solution problem,explicitly exploit,semantic correlations,spurious solution problem explicitly exploit semantic correlations,0.7507286071777344
translation,84,8,model,semantic correlations,by maximizing,mutual information,semantic correlations by maximizing mutual information,0.6458427309989929
translation,84,8,model,mutual information,between,question - answer pairs,mutual information between question - answer pairs,0.6540417075157166
translation,84,8,model,mutual information,between,predicted solutions,mutual information between predicted solutions,0.6692191958427429
translation,84,8,model,model,to alleviate,spurious solution problem,model to alleviate spurious solution problem,0.6974653601646423
translation,84,8,model,model,explicitly exploit,semantic correlations,model explicitly exploit semantic correlations,0.7573091387748718
translation,84,26,model,question reconstructor,devise,effective and unified way,question reconstructor devise effective and unified way,0.7151325941085815
translation,84,26,model,effective and unified way,to encode,solutions,effective and unified way to encode solutions,0.7288721203804016
translation,84,26,model,solutions,in,different tasks,solutions in different tasks,0.528455913066864
translation,84,26,model,model,For,question reconstructor,model For question reconstructor,0.6199283003807068
translation,84,206,model,training,pair,task -specific model,training pair task -specific model,0.7436314225196838
translation,84,206,model,task -specific model,with,question reconstructor,task -specific model with question reconstructor,0.5814301371574402
translation,84,206,model,question reconstructor,guides,task -specific model,question reconstructor guides task -specific model,0.678127646446228
translation,84,206,model,task -specific model,to predict,solutions,task -specific model to predict solutions,0.7517496347427368
translation,84,206,model,solutions,consistent with,questions,solutions consistent with questions,0.6501575708389282
translation,84,206,model,model,During,training,model During training,0.714866042137146
translation,84,107,results,outperforms,on,both datasets,outperforms on both datasets,0.51289302110672
translation,84,107,results,all baselines,on,both datasets,all baselines on both datasets,0.44950437545776367
translation,84,107,results,our method,has,outperforms,our method has outperforms,0.6322360634803772
translation,84,107,results,outperforms,has,all baselines,outperforms has all baselines,0.5888515114784241
translation,84,107,results,results,has,our method,results has our method,0.5589964985847473
translation,84,128,results,significantly outperforms,in terms of,f1 score,significantly outperforms in terms of f1 score,0.6684415340423584
translation,84,128,results,significantly outperforms,on,our test set,significantly outperforms on our test set,0.5509603023529053
translation,84,128,results,all baselines,in terms of,f1 score,all baselines in terms of f1 score,0.6533612012863159
translation,84,128,results,all baselines,on,our test set,all baselines on our test set,0.5058120489120483
translation,84,128,results,f1 score,on,our test set,f1 score on our test set,0.4971856474876404
translation,84,128,results,our method,has,significantly outperforms,our method has significantly outperforms,0.6120707988739014
translation,84,128,results,significantly outperforms,has,all baselines,significantly outperforms has all baselines,0.5895546674728394
translation,84,128,results,results,has,our method,results has our method,0.5589964985847473
translation,84,148,results,our method,achieves,new state - of - the - art results,our method achieves new state - of - the - art results,0.615921139717102
translation,84,148,results,new state - of - the - art results,under,weakly supervised setting,new state - of - the - art results under weakly supervised setting,0.5860152840614319
translation,84,148,results,results,has,our method,results has our method,0.5589964985847473
translation,84,149,results,our execution accuracy,on,test set,our execution accuracy on test set,0.5175148844718933
translation,84,149,results,test set,close to,fully supervised sqlova,test set close to fully supervised sqlova,0.7109464406967163
translation,84,149,results,our execution accuracy,has,"i.e. , accuracy of execution results","our execution accuracy has i.e. , accuracy of execution results",0.5484430193901062
translation,84,159,results,models,trained with,our method,models trained with our method,0.7160465717315674
translation,84,159,results,consistently outperform,in terms of,logical form accuracy,consistently outperform in terms of logical form accuracy,0.7053704857826233
translation,84,159,results,consistently outperform,in terms of,execution accuracy,consistently outperform in terms of execution accuracy,0.6838079690933228
translation,84,159,results,baselines,in terms of,execution accuracy,baselines in terms of execution accuracy,0.6521767973899841
translation,84,159,results,our method,has,consistently outperform,our method has consistently outperform,0.5946176052093506
translation,84,159,results,consistently outperform,has,baselines,consistently outperform has baselines,0.5874765515327454
translation,84,159,results,results,has,models,results has models,0.5335168838500977
translation,84,172,results,hardem,perform,similarly,hardem perform similarly,0.6482361555099487
translation,84,172,results,bart base parser and sqlova,perform,similarly,bart base parser and sqlova perform similarly,0.6363449692726135
translation,84,172,results,underperform,by,large margin,underperform by large margin,0.6201863288879395
translation,84,172,results,our method,by,large margin,our method by large margin,0.5804008841514587
translation,84,172,results,hardem,has,bart base parser and sqlova,hardem has bart base parser and sqlova,0.6446635723114014
translation,84,172,results,underperform,has,our method,underperform has our method,0.5813482403755188
translation,84,174,results,results,is,mutual information maximization objective,results is mutual information maximization objective,0.524368166923523
translation,84,177,results,our method,with,three question reconstructors,our method with three question reconstructors,0.6214226484298706
translation,84,177,results,our method,either of,three question reconstructors,our method either of three question reconstructors,0.5882534384727478
translation,84,177,results,outperforms or is at least competitive,with,baselines,outperforms or is at least competitive with baselines,0.7025921940803528
translation,84,177,results,three question reconstructors,has,outperforms or is at least competitive,three question reconstructors has outperforms or is at least competitive,0.6308993101119995
translation,84,178,results,t-dae,is,competitive,t-dae is competitive,0.6895471811294556
translation,84,178,results,competitive,with,bart base,competitive with bart base,0.7035301923751831
translation,84,178,results,results,using,t-dae,results using t-dae,0.6500557661056519
translation,84,183,results,samples,for which,our method,samples for which our method,0.6344612836837769
translation,84,183,results,samples,produced,correct solution,samples produced correct solution,0.7563180923461914
translation,84,183,results,our method,produced,correct solution,our method produced correct solution,0.6830135583877563
translation,84,183,results,correct solution,is,58 %,correct solution is 58 %,0.5405025482177734
translation,84,183,results,58 %,higher than,hardem,58 % higher than hardem,0.7641491293907166
translation,85,84,baselines,amazon,in,6 languages,amazon in 6 languages,0.5374522805213928
translation,85,84,baselines,dataset,in,6 languages,dataset in 6 languages,0.4955856204032898
translation,85,84,baselines,6 languages,with,210k reviews per language,6 languages with 210k reviews per language,0.6141824126243591
translation,85,84,baselines,amazon,has,dataset,amazon has dataset,0.545841634273529
translation,85,84,baselines,baselines,has,amazon,baselines has amazon,0.6271703839302063
translation,85,182,baselines,pipelined system,with,domain-specific language models,pipelined system with domain-specific language models,0.6220173239707947
translation,85,182,baselines,domain-specific language models,augmented with,adversarial data,domain-specific language models augmented with adversarial data,0.7024226188659668
translation,85,182,baselines,bat,has,"11 ( karimi et al. , 2020 )","bat has 11 ( karimi et al. , 2020 )",0.6086733341217041
translation,85,182,baselines,bat,has,pipelined system,bat has pipelined system,0.6274517178535461
translation,85,182,baselines,"11 ( karimi et al. , 2020 )",has,pipelined system,"11 ( karimi et al. , 2020 ) has pipelined system",0.5910707116127014
translation,85,182,baselines,baselines,has,bat,baselines has bat,0.6228921413421631
translation,85,183,baselines,end-to- end model,based on,song et al . ( 2019 ),end-to- end model based on song et al . ( 2019 ),0.6764373183250427
translation,85,183,baselines,end-to- end model,based on,domain adaptation,end-to- end model based on domain adaptation,0.6692982912063599
translation,85,183,baselines,end-to- end model,based on,local context focus mechanism,end-to- end model based on local context focus mechanism,0.6594476699829102
translation,85,183,baselines,end-to- end model,with,domain adaptation,end-to- end model with domain adaptation,0.6393902897834778
translation,85,183,baselines,end-to- end model,with,local context focus mechanism,end-to- end model with local context focus mechanism,0.6351919174194336
translation,85,183,baselines,lcf,has,"12 ( yang et al. , 2020 )","lcf has 12 ( yang et al. , 2020 )",0.5879112482070923
translation,85,183,baselines,lcf,has,end-to- end model,lcf has end-to- end model,0.5444955229759216
translation,85,183,baselines,"12 ( yang et al. , 2020 )",has,end-to- end model,"12 ( yang et al. , 2020 ) has end-to- end model",0.5576976537704468
translation,85,183,baselines,baselines,has,lcf,baselines has lcf,0.56271892786026
translation,85,184,baselines,racl,has,"13 ( chen and qian , 2020 )","racl has 13 ( chen and qian , 2020 )",0.6166187524795532
translation,85,184,baselines,baselines,has,racl,baselines has racl,0.6466894745826721
translation,85,187,baselines,bert - e2e,has,14,bert - e2e has 14,0.6735124588012695
translation,85,187,baselines,bert - e2e,has,bert - based end-to - end sequence labeling system,bert - e2e has bert - based end-to - end sequence labeling system,0.5556907653808594
translation,85,187,baselines,14,has,bert - based end-to - end sequence labeling system,14 has bert - based end-to - end sequence labeling system,0.5413301587104797
translation,85,187,baselines,baselines,has,bert - e2e,baselines has bert - e2e,0.6101173758506775
translation,85,189,baselines,pipeline,of,mcrf - sa,pipeline of mcrf - sa,0.6171498894691467
translation,85,189,baselines,sc system,utilizing,multiple crf - based structured attention models,sc system utilizing multiple crf - based structured attention models,0.633929967880249
translation,85,189,baselines,hast + mcrf,has,pipeline,hast + mcrf has pipeline,0.6547242999076843
translation,85,189,baselines,hast,has,te system,hast has te system,0.6846380233764648
translation,85,189,baselines,mcrf - sa,has,sc system,mcrf - sa has sc system,0.5742834210395813
translation,85,189,baselines,baselines,has,hast + mcrf,baselines has hast + mcrf,0.6032893061637878
translation,85,10,experiments,github.com,has,/ibm,github.com has /ibm,0.5383796691894531
translation,85,10,experiments,/ibm,has,/ yaso-tsa,/ibm has / yaso-tsa,0.6619590520858765
translation,85,186,hyperparameters,racl - glove variant,based on,pre-trained word embeddings,racl - glove variant based on pre-trained word embeddings,0.5928137898445129
translation,85,186,hyperparameters,hyperparameters,used,racl - glove variant,hyperparameters used racl - glove variant,0.6042591333389282
translation,85,6,model,yaso - a new tsa evaluation dataset,of,open-domain user reviews,yaso - a new tsa evaluation dataset of open-domain user reviews,0.5428785681724548
translation,85,6,model,model,present,yaso - a new tsa evaluation dataset,model present yaso - a new tsa evaluation dataset,0.6651991605758667
translation,85,188,model,bert + linear architecture,computes,pertoken labels,bert + linear architecture computes pertoken labels,0.7619082927703857
translation,85,188,model,pertoken labels,using,linear classification layer,pertoken labels using linear classification layer,0.6571409702301025
translation,85,188,model,model,used,bert + linear architecture,model used bert + linear architecture,0.5843218564987183
translation,85,204,results,bat,trained on,restaurants data,bat trained on restaurants data,0.7305490374565125
translation,85,204,results,restaurants data,was,best-performing system,restaurants data was best-performing system,0.5932638049125671
translation,85,204,results,best-performing system,for,te and the full tsa tasks,best-performing system for te and the full tsa tasks,0.5822087526321411
translation,85,204,results,best-performing system,on,three of the four datasets,best-performing system on three of the four datasets,0.47724759578704834
translation,85,204,results,results,has,bat,results has bat,0.4547613859176636
translation,85,205,results,bert - e2e,was,best model,bert - e2e was best model,0.6322566270828247
translation,85,205,results,best model,on,three datasets,best model on three datasets,0.5135152339935303
translation,85,205,results,sc,has,bert - e2e,sc has bert - e2e,0.6769158840179443
translation,85,205,results,results,For,sc,results For sc,0.6267269849777222
translation,85,206,results,results,for,sc,results for sc,0.6267269849777222
translation,85,206,results,sc,were,relatively high,sc were relatively high,0.6264773011207581
translation,85,206,results,te results,by,some models,te results by some models,0.6270047426223755
translation,85,206,results,some models,may be,very low,some models may be very low,0.685539960861206
translation,85,206,results,results,for,sc,results for sc,0.6267269849777222
translation,85,206,results,results,has,results,results has results,0.48582205176353455
translation,86,164,ablation-analysis,ablation study,taking advantage of,answer information,ablation study taking advantage of answer information,0.6606864929199219
translation,86,164,ablation-analysis,answer information,modeling,shared background,answer information modeling shared background,0.7650688886642456
translation,86,164,ablation-analysis,shared background,in,entire triple,shared background in entire triple,0.5735717415809631
translation,86,164,ablation-analysis,one - to - many mappings,in,pq and qa pairs,one - to - many mappings in pq and qa pairs,0.5570163726806641
translation,86,164,ablation-analysis,one - to - many mappings,enhance,performance,one - to - many mappings enhance performance,0.6661726236343384
translation,86,164,ablation-analysis,performance,of,our,performance of our,0.6322904825210571
translation,86,164,ablation-analysis,performance,of,hierarchical variational model,performance of hierarchical variational model,0.5917692184448242
translation,86,164,ablation-analysis,hierarchical variational model,in terms of,"relevance , coherence and diversity","hierarchical variational model in terms of relevance , coherence and diversity",0.6567268967628479
translation,86,164,ablation-analysis,our,has,hierarchical variational model,our has hierarchical variational model,0.5714110136032104
translation,86,168,ablation-analysis,gtm,produce,more relevant and intriguing questions,gtm produce more relevant and intriguing questions,0.6077796220779419
translation,86,168,ablation-analysis,more relevant and intriguing questions,indicates,effectiveness,more relevant and intriguing questions indicates effectiveness,0.6082359552383423
translation,86,168,ablation-analysis,effectiveness,of modeling,shared background,effectiveness of modeling shared background,0.6879796385765076
translation,86,168,ablation-analysis,ablation analysis,has,gtm,ablation analysis has gtm,0.4996260702610016
translation,86,122,baselines,simple seq2seq model,with,attention mechanism,simple seq2seq model with attention mechanism,0.6082326769828796
translation,86,122,baselines,extra bow loss,to generate,diverse questions,extra bow loss to generate diverse questions,0.6834050416946411
translation,86,122,baselines,s2s - attn,has,simple seq2seq model,s2s - attn has simple seq2seq model,0.5940243005752563
translation,86,123,baselines,kgcvae,is,knowledge- guided cvae,kgcvae is knowledge- guided cvae,0.6038891077041626
translation,86,123,baselines,knowledge- guided cvae,utilizes,some linguistic cues,knowledge- guided cvae utilizes some linguistic cues,0.5736463069915771
translation,86,123,baselines,some linguistic cues,to learn,meaningful latent variables,some linguistic cues to learn meaningful latent variables,0.5908307433128357
translation,86,123,baselines,baselines,has,kgcvae,baselines has kgcvae,0.5593152046203613
translation,86,124,baselines,std,uses,soft typed decoder,std uses soft typed decoder,0.6134217381477356
translation,86,124,baselines,soft typed decoder,estimates,type distribution,soft typed decoder estimates type distribution,0.6346765160560608
translation,86,124,baselines,type distribution,over,word types,type distribution over word types,0.6138340830802917
translation,86,124,baselines,htd,uses,hard typed decoder,htd uses hard typed decoder,0.6030738949775696
translation,86,124,baselines,hard typed decoder,specifies,type of each word explicitly,hard typed decoder specifies type of each word explicitly,0.7436797022819519
translation,86,124,baselines,type of each word explicitly,with,gumbel-softmax,type of each word explicitly with gumbel-softmax,0.6633551120758057
translation,86,124,baselines,reinforcement learning method,regards,coherence score,reinforcement learning method regards coherence score,0.5436254143714905
translation,86,124,baselines,coherence score,of,pair of generated question and answer,coherence score of pair of generated question and answer,0.595192551612854
translation,86,124,baselines,coherence score,as,reward function,coherence score as reward function,0.5096685886383057
translation,86,124,baselines,pair of generated question and answer,as,reward function,pair of generated question and answer as reward function,0.5508028268814087
translation,86,124,baselines,rl - cvae,has,reinforcement learning method,rl - cvae has reinforcement learning method,0.5446107387542725
translation,86,124,baselines,baselines,has,std,baselines has std,0.6246174573898315
translation,86,125,baselines,rl - cvae,is,first work,rl - cvae is first work,0.6110711097717285
translation,86,125,baselines,rl - cvae,to utilize,future information,rl - cvae to utilize future information,0.7363860607147217
translation,86,125,baselines,first work,to utilize,future information,first work to utilize future information,0.6554933190345764
translation,86,125,baselines,state - of- the - art model,for,cqg,state - of- the - art model for cqg,0.6043953895568848
translation,86,125,baselines,baselines,has,rl - cvae,baselines has rl - cvae,0.5743921399116516
translation,86,137,baselines,"average , greedy and extrema metrics",are,embedding - based,"average , greedy and extrema metrics are embedding - based",0.581437885761261
translation,86,137,baselines,semantic similarity,between,words,semantic similarity between words,0.6047403812408447
translation,86,137,baselines,words,in,generated questions and ground -truth questions,words in generated questions and ground -truth questions,0.5263080596923828
translation,86,137,baselines,embedding metrics,has,"average , greedy and extrema metrics","embedding metrics has average , greedy and extrema metrics",0.5551297068595886
translation,86,137,baselines,baselines,has,embedding metrics,baselines has embedding metrics,0.5500439405441284
translation,86,113,experimental-setup,dimension,of,"gru layer , word embedding and latent variables","dimension of gru layer , word embedding and latent variables",0.5638327598571777
translation,86,113,experimental-setup,dimension,is,"300 , 300 , and 100","dimension is 300 , 300 , and 100",0.6131302714347839
translation,86,113,experimental-setup,"gru layer , word embedding and latent variables",is,"300 , 300 , and 100","gru layer , word embedding and latent variables is 300 , 300 , and 100",0.5737980604171753
translation,86,113,experimental-setup,experimental setup,has,dimension,experimental setup has dimension,0.4825572967529297
translation,86,114,experimental-setup,prior networks and mlps,have,one hidden layer,prior networks and mlps have one hidden layer,0.5420874953269958
translation,86,114,experimental-setup,one hidden layer,with,tanh non-linearity,one hidden layer with tanh non-linearity,0.6465954780578613
translation,86,114,experimental-setup,one hidden layer,size,300,one hidden layer size 300,0.7676442861557007
translation,86,114,experimental-setup,one hidden layer,size,tanh non-linearity,one hidden layer size tanh non-linearity,0.7367374300956726
translation,86,114,experimental-setup,number of hidden layers,in,recognition networks,number of hidden layers in recognition networks,0.5159689784049988
translation,86,114,experimental-setup,recognition networks,for,triple - level and utterance - level variables,recognition networks for triple - level and utterance - level variables,0.610349178314209
translation,86,114,experimental-setup,triple - level and utterance - level variables,is,2,triple - level and utterance - level variables is 2,0.5651296377182007
translation,86,114,experimental-setup,experimental setup,has,prior networks and mlps,experimental setup has prior networks and mlps,0.5470497012138367
translation,86,115,experimental-setup,dropout ratio,of,0.2,dropout ratio of 0.2,0.6002745628356934
translation,86,115,experimental-setup,0.2,during,training,0.2 during training,0.7125362157821655
translation,86,115,experimental-setup,experimental setup,apply,dropout ratio,experimental setup apply dropout ratio,0.5938745141029358
translation,86,116,experimental-setup,mini-batch size,is,64,mini-batch size is 64,0.615536093711853
translation,86,116,experimental-setup,experimental setup,has,mini-batch size,experimental setup has mini-batch size,0.5507700443267822
translation,86,117,experimental-setup,optimization,use,"adam ( kingma and ba , 2015 )","optimization use adam ( kingma and ba , 2015 )",0.5987501740455627
translation,86,117,experimental-setup,"adam ( kingma and ba , 2015 )",with,learning rate,"adam ( kingma and ba , 2015 ) with learning rate",0.5996310710906982
translation,86,117,experimental-setup,learning rate,of,1e - 4,learning rate of 1e - 4,0.6311931014060974
translation,86,117,experimental-setup,experimental setup,For,optimization,experimental setup For optimization,0.5891668200492859
translation,86,118,experimental-setup,experimental setup,apply,kl annealing,experimental setup apply kl annealing,0.6015350818634033
translation,86,119,experimental-setup,increases,from,0 to 1,increases from 0 to 1,0.627816915512085
translation,86,119,experimental-setup,word drop probability,is,0.25,word drop probability is 0.25,0.6050416231155396
translation,86,119,experimental-setup,experimental setup,has,kl multiplier,experimental setup has kl multiplier,0.6088147759437561
translation,86,119,experimental-setup,experimental setup,has,word drop probability,experimental setup has word drop probability,0.5332419872283936
translation,86,120,experimental-setup,pytorch,to implement,our model,pytorch to implement our model,0.7456267476081848
translation,86,120,experimental-setup,pytorch,to implement,model,pytorch to implement model,0.7693246603012085
translation,86,120,experimental-setup,model,trained on,titan xp gpus,model trained on titan xp gpus,0.702745795249939
translation,86,8,model,generative triple -wise model,with,hierarchical variations,generative triple -wise model with hierarchical variations,0.6556167602539062
translation,86,8,model,hierarchical variations,for,open-domain conversational question generation ( cqg ),hierarchical variations for open-domain conversational question generation ( cqg ),0.6008075475692749
translation,86,8,model,model,propose,generative triple -wise model,model propose generative triple -wise model,0.674780547618866
translation,86,9,model,latent variables,in,three hierarchies,latent variables in three hierarchies,0.5043858289718628
translation,86,9,model,shared background,of,triple and one- to -many semantic mappings,shared background of triple and one- to -many semantic mappings,0.5667101740837097
translation,86,9,model,triple and one- to -many semantic mappings,in,pq and qa pairs,triple and one- to -many semantic mappings in pq and qa pairs,0.554066002368927
translation,86,9,model,model,has,latent variables,model has latent variables,0.5235121846199036
translation,86,38,model,generative triplewise model ( gtm ),for,cqg,generative triplewise model ( gtm ) for cqg,0.6299988031387329
translation,86,38,model,model,propose,generative triplewise model ( gtm ),model propose generative triplewise model ( gtm ),0.6575793027877808
translation,86,39,model,triple- level variable,to capture,shared background,triple- level variable to capture shared background,0.6900109648704529
translation,86,39,model,shared background,among,pqa,shared background among pqa,0.6419211030006409
translation,86,39,model,model,introduce,triple- level variable,model introduce triple- level variable,0.6518753170967102
translation,86,159,results,top part,results of,baseline models,top part results of baseline models,0.658910870552063
translation,86,159,results,other methods,on,all metrics,other methods on all metrics,0.4495483338832855
translation,86,159,results,gtm,has,outperforms,gtm has outperforms,0.665594220161438
translation,86,159,results,outperforms,has,other methods,outperforms has other methods,0.5689877271652222
translation,86,159,results,results,has,top part,results has top part,0.5684731006622314
translation,86,160,results,dist -2 and ruba,improved by,2.43 % and 1.90 %,dist -2 and ruba improved by 2.43 % and 1.90 %,0.67648845911026
translation,86,160,results,2.43 % and 1.90 %,compared to,state - of- the - art rl - cvae model,2.43 % and 1.90 % compared to state - of- the - art rl - cvae model,0.6425535082817078
translation,86,160,results,results,has,dist -2 and ruba,results has dist -2 and ruba,0.5227948427200317
translation,86,161,results,higher embedding metrics and bleu scores,show that,questions,higher embedding metrics and bleu scores show that questions,0.5669897794723511
translation,86,161,results,questions,generated by,our model,questions generated by our model,0.6584838032722473
translation,86,161,results,questions,similar to,ground truths,questions similar to ground truths,0.5947378873825073
translation,86,161,results,our model,similar to,ground truths,our model similar to ground truths,0.6238943338394165
translation,86,161,results,ground truths,in,both topics and contents,ground truths in both topics and contents,0.5201510787010193
translation,86,161,results,results,has,higher embedding metrics and bleu scores,results has higher embedding metrics and bleu scores,0.5526946783065796
translation,86,162,results,answer,into,account,answer into account,0.6706302165985107
translation,86,162,results,expression of question,improve,consistency,expression of question improve consistency,0.6240525245666504
translation,86,162,results,consistency,of,pq pairs,consistency of pq pairs,0.6352676749229431
translation,86,162,results,results,taking,answer,results taking answer,0.567043125629425
translation,86,166,results,generating dull and deviated questions,compared with,other models,generating dull and deviated questions compared with other models,0.6775160431861877
translation,86,166,results,results,has,gtm,results has gtm,0.5736743807792664
translation,86,180,results,questions,generated by,gtm,questions generated by gtm,0.6993833780288696
translation,86,180,results,gtm,are,more relevant,gtm are more relevant,0.5852265954017639
translation,86,180,results,more relevant,to,posts and answers,more relevant to posts and answers,0.5577470064163208
translation,86,180,results,people,to give,answer,people to give answer,0.7329729199409485
translation,86,180,results,results,has,questions,results has questions,0.5194433331489563
translation,87,173,experimental-setup,batch size,set to,16,batch size set to 16,0.7520541548728943
translation,87,173,experimental-setup,maximum gradient norm,of,1.0,maximum gradient norm of 1.0,0.5818200707435608
translation,87,173,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,87,174,experimental-setup,all the models,for,maximum of 5 epochs,all the models for maximum of 5 epochs,0.6175998449325562
translation,87,174,experimental-setup,all the models,with,early stopping,all the models with early stopping,0.6317868828773499
translation,87,174,experimental-setup,maximum of 5 epochs,with,early stopping,maximum of 5 epochs with early stopping,0.5949273109436035
translation,87,174,experimental-setup,early stopping,using,loss,early stopping using loss,0.7170112729072571
translation,87,174,experimental-setup,loss,on,dev set,loss on dev set,0.5801555514335632
translation,87,174,experimental-setup,experimental setup,trained,all the models,experimental setup trained all the models,0.7037098407745361
translation,87,176,experimental-setup,machine,with,8 tesla v100 - sxm2 gpus,machine with 8 tesla v100 - sxm2 gpus,0.558050811290741
translation,87,176,experimental-setup,machine,with,16 gb memory,machine with 16 gb memory,0.57731693983078
translation,87,176,experimental-setup,8 tesla v100 - sxm2 gpus,with,16 gb memory,8 tesla v100 - sxm2 gpus with 16 gb memory,0.5389178991317749
translation,87,176,experimental-setup,experimental setup,ran on,machine,experimental setup ran on machine,0.7051976323127747
translation,87,209,experiments,entailment approach,reaches,accuracy,entailment approach reaches accuracy,0.679979681968689
translation,87,209,experiments,accuracy,of,0.40,accuracy of 0.40,0.603546142578125
translation,87,209,experiments,0.40,using,more than 1000 instances,0.40 using more than 1000 instances,0.6476784348487854
translation,87,26,model,policy description,converted into,set of questions,policy description converted into set of questions,0.6028064489364624
translation,87,26,model,logical form,represented by,expression tree,logical form represented by expression tree,0.7454988360404968
translation,87,26,model,expression tree,to produce,label,expression tree to produce label,0.7123689651489258
translation,87,26,model,model,has,policy description,model has policy description,0.5138709545135498
translation,87,7,results,better accuracy,especially in,cross-policy setup,better accuracy especially in cross-policy setup,0.5789165496826172
translation,87,7,results,cross-policy setup,where,policies during testing,cross-policy setup where policies during testing,0.5708486437797546
translation,87,33,results,accuracy,of,0.69,accuracy of 0.69,0.5636099576950073
translation,87,33,results,accuracy,of,0.59,accuracy of 0.59,0.5649386644363403
translation,87,33,results,accuracy,of,0.59,accuracy of 0.59,0.5649386644363403
translation,87,33,results,0.69,for,policies unseen,0.69 for policies unseen,0.6087092757225037
translation,87,33,results,policies unseen,during,training,policies unseen during training,0.7114503979682922
translation,87,33,results,policies unseen,during,training,policies unseen during training,0.7114503979682922
translation,87,33,results,increase,of,25 absolute points,increase of 25 absolute points,0.6043737530708313
translation,87,33,results,increase,of,22 absolute points,increase of 22 absolute points,0.5874335169792175
translation,87,33,results,increase,of,22 absolute points,increase of 22 absolute points,0.5874335169792175
translation,87,33,results,increase,of,22 absolute points,increase of 22 absolute points,0.5874335169792175
translation,87,33,results,accuracy,of,0.59,accuracy of 0.59,0.5649386644363403
translation,87,33,results,0.59,when,no,0.59 when no,0.6725532412528992
translation,87,33,results,0.59,when,in-domain data,0.59 when in-domain data,0.6019869446754456
translation,87,33,results,increase,of,22 absolute points,increase of 22 absolute points,0.5874335169792175
translation,87,33,results,no,available for,training,no available for training,0.7268772125244141
translation,87,33,results,in-domain data,available for,training,in-domain data available for training,0.6554083228111267
translation,87,33,results,no,has,in-domain data,no has in-domain data,0.6046652793884277
translation,87,34,results,our approach,is,more robust,our approach is more robust,0.5604674816131592
translation,87,34,results,more robust,compared to,entailment,more robust compared to entailment,0.614170491695404
translation,87,34,results,entailment,when faced with,policies of increasing complexity,entailment when faced with policies of increasing complexity,0.6485090255737305
translation,87,34,results,results,show,our approach,results show our approach,0.6604568362236023
translation,87,181,results,accuracy,of,entailment approach,accuracy of entailment approach,0.5552476644515991
translation,87,181,results,accuracy,of,qa decomposition approach,accuracy of qa decomposition approach,0.6268136501312256
translation,87,181,results,accuracy,of,0.69,accuracy of 0.69,0.5636099576950073
translation,87,181,results,accuracy,of,0.69,accuracy of 0.69,0.5636099576950073
translation,87,181,results,entailment approach,is,0.44,entailment approach is 0.44,0.5187088847160339
translation,87,181,results,qa decomposition approach,reaches,accuracy,qa decomposition approach reaches accuracy,0.7606368660926819
translation,87,181,results,accuracy,of,0.69,accuracy of 0.69,0.5636099576950073
translation,87,181,results,results,has,accuracy,results has accuracy,0.5888755321502686
translation,87,190,results,pcd task,based on,training,pcd task based on training,0.6525336503982544
translation,87,190,results,training,on,boolq,training on boolq,0.5886146426200867
translation,87,190,results,training,on,snli,training on snli,0.6118606328964233
translation,87,190,results,boolq,for,qa - based approach,boolq for qa - based approach,0.6670545339584351
translation,87,191,results,decomposition approach,achieves,accuracy,decomposition approach achieves accuracy,0.6774489879608154
translation,87,191,results,accuracy,of,0.59,accuracy of 0.59,0.5649386644363403
translation,87,191,results,0.59,trained on,boolq data,0.59 trained on boolq data,0.7374519109725952
translation,87,191,results,results,see,decomposition approach,results see decomposition approach,0.5994799137115479
translation,87,192,results,accuracy,of,entailment model,accuracy of entailment model,0.5511124134063721
translation,87,192,results,entailment model,trained on,snli,entailment model trained on snli,0.6911824941635132
translation,87,192,results,entailment model,is,0.37,entailment model is 0.37,0.5247560739517212
translation,87,192,results,entailment model,is,slightly better,entailment model is slightly better,0.5543826222419739
translation,87,192,results,slightly better,than,random system,slightly better than random system,0.6086476445198059
translation,87,192,results,results,has,accuracy,results has accuracy,0.5888755321502686
translation,87,200,results,average performance,of,pdc task,average performance of pdc task,0.5854105353355408
translation,87,200,results,average performance,in,test set,average performance in test set,0.5258220434188843
translation,87,200,results,pdc task,using,qa decomposition approach,pdc task using qa decomposition approach,0.6872824430465698
translation,87,200,results,qa decomposition approach,for,individual policies,qa decomposition approach for individual policies,0.6165386438369751
translation,87,200,results,test set,of,qa4 pc,test set of qa4 pc,0.647595226764679
translation,87,200,results,qa4 pc,versus,complexity,qa4 pc versus complexity,0.6970739364624023
translation,87,200,results,complexity,of,policy description,complexity of policy description,0.5762417316436768
translation,87,208,results,accuracy,of,0.60,accuracy of 0.60,0.5871464014053345
translation,87,208,results,0.60,through,qa decomposition approach,0.60 through qa decomposition approach,0.6765149235725403
