topic,paper_ID,sentence_ID,info-unit,sub,pred,obj,triplets,pred_weights
translation,0,6,experiments,effects of adversarial training,over,multiple qa ranking algorithms,effects of adversarial training over multiple qa ranking algorithms,0.6465975046157837
translation,0,6,experiments,effects of adversarial training,including,state - of- the - art multihop attention network model,effects of adversarial training including state - of- the - art multihop attention network model,0.6259560585021973
translation,0,6,experiments,multiple qa ranking algorithms,including,state - of- the - art multihop attention network model,multiple qa ranking algorithms including state - of- the - art multihop attention network model,0.6421605944633484
translation,0,27,model,novel committee representation,to,adversarial modeling,novel committee representation to adversarial modeling,0.485674113035202
translation,0,27,model,adversarial modeling,for,qa ranking,adversarial modeling for qa ranking,0.5735519528388977
translation,0,27,model,model,propose,novel committee representation,model propose novel committee representation,0.6552169322967529
translation,0,34,model,convolution layers,on,interaction matrix,convolution layers on interaction matrix,0.5494195818901062
translation,0,34,model,interaction matrix,taking,dot product,interaction matrix taking dot product,0.6731160283088684
translation,0,34,model,dot product,of,embeddings,dot product of embeddings,0.5705456733703613
translation,0,34,model,embeddings,of,question words,embeddings of question words,0.5220160484313965
translation,0,34,model,embeddings,with,answer words,embeddings with answer words,0.5684964656829834
translation,0,34,model,question words,with,answer words,question words with answer words,0.5658667683601379
translation,0,8,results,previous state - of - the - art algorithm,as much as,6 %,previous state - of - the - art algorithm as much as 6 %,0.5106384754180908
translation,0,8,results,6 %,in,ndcg ( normalized discounted cumulative gain ),6 % in ndcg ( normalized discounted cumulative gain ),0.5261929631233215
translation,0,8,results,capable of consistently improving,has,all baseline algorithms,capable of consistently improving has all baseline algorithms,0.5822383761405945
translation,0,8,results,outperforms,has,previous state - of - the - art algorithm,outperforms has previous state - of - the - art algorithm,0.560747504234314
translation,0,8,results,results,causes of,degradation,results causes of degradation,0.6522397398948669
translation,0,8,results,results,propose,new representation procedure,results propose new representation procedure,0.6444939374923706
translation,0,84,results,lowest performance scores,except for,few anomalous cases,lowest performance scores except for few anomalous cases,0.6342540383338928
translation,0,84,results,performed better,than,few variants,performed better than few variants,0.5979160666465759
translation,0,84,results,of deep matching network,on,some of the datasets,of deep matching network on some of the datasets,0.4735202491283417
translation,0,84,results,advcom - match pyramid,has,performed better,advcom - match pyramid has performed better,0.6172437071800232
translation,0,84,results,few variants,has,of deep matching network,few variants has of deep matching network,0.5409395694732666
translation,0,84,results,results,has,match pyramid and its variants,results has match pyramid and its variants,0.6052180528640747
translation,0,85,results,vanilla adversarial learning,provides,significant boost,vanilla adversarial learning provides significant boost,0.5656676292419434
translation,0,85,results,significant boost,in,model performance,significant boost in model performance,0.5327010154724121
translation,0,85,results,model performance,for,match pyramid and deep matching network,model performance for match pyramid and deep matching network,0.5512382984161377
translation,0,85,results,performance boost,by,adversarial committee learning,performance boost by adversarial committee learning,0.5674358606338501
translation,0,85,results,adversarial committee learning,was,much better,adversarial committee learning was much better,0.5692967772483826
translation,0,85,results,results,show,vanilla adversarial learning,results show vanilla adversarial learning,0.5509749054908752
translation,0,86,results,base model performance,for,most datasets,base model performance for most datasets,0.5472489595413208
translation,0,86,results,man,has,vanilla adversarial learning,man has vanilla adversarial learning,0.5497666001319885
translation,0,86,results,vanilla adversarial learning,has,significantly worsens,vanilla adversarial learning has significantly worsens,0.5662097930908203
translation,0,86,results,significantly worsens,has,base model performance,significantly worsens has base model performance,0.6091454029083252
translation,0,86,results,results,for,man,results for man,0.5411614179611206
translation,0,95,results,adversarial committee technique,able to,boost,adversarial committee technique able to boost,0.5942119359970093
translation,0,95,results,adversarial committee technique,in,all datasets,adversarial committee technique in all datasets,0.4565342366695404
translation,0,95,results,performance,of,all models,performance of all models,0.5688430666923523
translation,0,95,results,performance,of,all datasets,performance of all datasets,0.5230138301849365
translation,0,95,results,performance,in,all datasets,performance in all datasets,0.4686133861541748
translation,0,95,results,boost,has,performance,boost has performance,0.5791411995887756
translation,0,95,results,results,showed,adversarial committee technique,results showed adversarial committee technique,0.6715593934059143
translation,1,6,model,method,for,reasoning,method for reasoning,0.619199275970459
translation,1,6,model,method,allowing,more complex questions,method allowing more complex questions,0.630815863609314
translation,1,6,model,reasoning,with,open ie knowledge,reasoning with open ie knowledge,0.6227038502693176
translation,1,6,model,reasoning,allowing,more complex questions,reasoning allowing more complex questions,0.7183611989021301
translation,1,6,model,model,presenting,method,model presenting method,0.7077168822288513
translation,1,7,model,new inference model,for,open ie,new inference model for open ie,0.6486696600914001
translation,1,7,model,model,develop,new inference model,model develop new inference model,0.6730284690856934
translation,1,26,model,new ilp - based model of inference,with,tuples,new ilp - based model of inference with tuples,0.6449260711669922
translation,1,26,model,new ilp - based model of inference,implemented in,reasoner,new ilp - based model of inference implemented in reasoner,0.7018334269523621
translation,1,26,model,reasoner,called,tupleinf,reasoner called tupleinf,0.6886628270149231
translation,1,26,model,model,present,new ilp - based model of inference,model present new ilp - based model of inference,0.6851373910903931
translation,1,27,results,tableilp,by,11.8 %,tableilp by 11.8 %,0.6023281812667847
translation,1,27,results,tableilp,on,"broad set of over 1,300 science questions","tableilp on broad set of over 1,300 science questions",0.5488582849502563
translation,1,27,results,tableilp,without requiring,manually curated tables,tableilp without requiring manually curated tables,0.6795251965522766
translation,1,27,results,11.8 %,on,"broad set of over 1,300 science questions","11.8 % on broad set of over 1,300 science questions",0.49869635701179504
translation,1,27,results,manually curated tables,using,substantially simpler ilp formulation,manually curated tables using substantially simpler ilp formulation,0.620915412902832
translation,1,27,results,tupleinf,has,significantly outperforms,tupleinf has significantly outperforms,0.2199167013168335
translation,1,27,results,significantly outperforms,has,tableilp,significantly outperforms has tableilp,0.617797315120697
translation,1,27,results,results,demonstrate,tupleinf,results demonstrate tupleinf,0.5714682340621948
translation,1,103,results,tableilp,with,original curated knowl -,tableilp with original curated knowl -,0.637458324432373
translation,1,103,results,tupleinf,has,with only automatically extracted tuples,tupleinf has with only automatically extracted tuples,0.6371629238128662
translation,1,103,results,with only automatically extracted tuples,has,significantly outperforms,with only automatically extracted tuples has significantly outperforms,0.6142944097518921
translation,1,103,results,significantly outperforms,has,tableilp,significantly outperforms has tableilp,0.617797315120697
translation,1,103,results,results,demonstrate,tupleinf,results demonstrate tupleinf,0.5714682340621948
translation,1,104,results,tupleinf,is,significantly better,tupleinf is significantly better,0.5795533657073975
translation,1,104,results,significantly better,at,structured reasoning,significantly better at structured reasoning,0.5026694536209106
translation,1,104,results,significantly better,than,tableilp,significantly better than tableilp,0.6154061555862427
translation,1,104,results,structured reasoning,than,tableilp,structured reasoning than tableilp,0.5940818786621094
translation,1,104,results,results,has,tupleinf,results has tupleinf,0.567028284072876
translation,1,119,results,outperforms,on,both question sets,outperforms on both question sets,0.5299215912818909
translation,1,119,results,tableilp,on,both question sets,tableilp on both question sets,0.5573733448982239
translation,1,119,results,both question sets,by,more than 11 %,both question sets by more than 11 %,0.6177952885627747
translation,1,119,results,tupleinf,has,outperforms,tupleinf has outperforms,0.5953839421272278
translation,1,119,results,outperforms,has,tableilp,outperforms has tableilp,0.6443718671798706
translation,1,119,results,results,shows,tupleinf,results shows tupleinf,0.6687447428703308
translation,1,123,results,ir + tupleinf,consistently better than,ir + tableilp,ir + tupleinf consistently better than ir + tableilp,0.7413515448570251
translation,1,123,results,results,has,ir + tupleinf,results has ir + tupleinf,0.6128770709037781
translation,1,124,results,tupleinf,achieves,score,tupleinf achieves score,0.6958099007606506
translation,1,124,results,score,of,58.2 %,score of 58.2 %,0.5324063301086426
translation,1,124,results,58.2 %,on,4th grade set,58.2 % on 4th grade set,0.4982556700706482
translation,1,124,results,results,in combination with,ir,results in combination with ir,0.586840033531189
translation,2,13,baselines,baselines,has,vqa models,baselines has vqa models,0.5384897589683533
translation,2,35,experiments,uncertainty,of,vqa model,uncertainty of vqa model,0.5676680207252502
translation,2,35,experiments,pre-trained captioning models,to generate,relevant captions ( or questions ),pre-trained captioning models to generate relevant captions ( or questions ),0.63463294506073
translation,2,36,results,our proposed models,achieve,accuracies,our proposed models achieve accuracies,0.6901068091392517
translation,2,36,results,accuracies,of,92 %,accuracies of 92 %,0.5882551074028015
translation,2,36,results,accuracies,of,74 %,accuracies of 74 %,0.5858172178268433
translation,2,36,results,92 %,for,detecting,92 % for detecting,0.7260470390319824
translation,2,36,results,74 %,for,detecting,74 % for detecting,0.7299114465713501
translation,2,36,results,detecting,has,non-visual,detecting has non-visual,0.6051387786865234
translation,2,36,results,detecting,has,false - premise questions,detecting has false - premise questions,0.5450363755226135
translation,2,36,results,significantly outperform,has,strong baselines,significantly outperform has strong baselines,0.5945266485214233
translation,2,36,results,results,has,our proposed models,results has our proposed models,0.5751316547393799
translation,3,5,model,first approach,re-ranks,single qa system 's outputs,first approach re-ranks single qa system 's outputs,0.8009600639343262
translation,3,5,model,single qa system 's outputs,by using,traditional mbr model,single qa system 's outputs by using traditional mbr model,0.6506861448287964
translation,3,5,model,single qa system 's outputs,by measuring,correlations,single qa system 's outputs by measuring correlations,0.7604162693023682
translation,3,5,model,correlations,between,answer candidates,correlations between answer candidates,0.7069315314292908
translation,3,5,model,second approach,reranks,combined outputs,second approach reranks combined outputs,0.7710739374160767
translation,3,5,model,combined outputs,of,multiple qa systems,combined outputs of multiple qa systems,0.617272138595581
translation,3,5,model,combined outputs,with,heterogenous answer extraction components,combined outputs with heterogenous answer extraction components,0.6104518175125122
translation,3,5,model,heterogenous answer extraction components,by using,mixture model - based mbr model,heterogenous answer extraction components by using mixture model - based mbr model,0.637109100818634
translation,3,5,model,model,has,first approach,model has first approach,0.5919356346130371
translation,3,11,model,model,propose,two mbrbased answer re-ranking ( mbrar ) approaches,model propose two mbrbased answer re-ranking ( mbrar ) approaches,0.6485922932624817
translation,3,12,model,answer outputs,from,single qa system,answer outputs from single qa system,0.5950659513473511
translation,3,12,model,answer outputs,by measuring,correlations,answer outputs by measuring correlations,0.7304854989051819
translation,3,12,model,single qa system,based on,traditional mbr model,single qa system based on traditional mbr model,0.6634712219238281
translation,3,12,model,correlations,between,each answer candidates,correlations between each answer candidates,0.6766720414161682
translation,3,12,model,correlations,between,all the other candidates,correlations between all the other candidates,0.7110446691513062
translation,3,12,model,combined answer outputs,from,multiple qa systems,combined answer outputs from multiple qa systems,0.597935140132904
translation,3,12,model,combined answer outputs,based on,mixture model - based mbr model,combined answer outputs based on mixture model - based mbr model,0.6452029943466187
translation,3,71,results,our mbrar method,on,individual qa systems,our mbrar method on individual qa systems,0.5840620398521423
translation,3,71,results,rankings,of,correct answers,rankings of correct answers,0.5704354047775269
translation,3,71,results,correct answers,are,consistently improved,correct answers are consistently improved,0.5827395915985107
translation,3,71,results,consistently improved,on,jeopardy,consistently improved on jeopardy,0.5545051693916321
translation,3,71,results,our mbrar method,has,rankings,our mbrar method has rankings,0.6322752237319946
translation,3,71,results,results,by leveraging,our mbrar method,results by leveraging our mbrar method,0.680891215801239
translation,3,81,results,ranking performances,of,single qa systems td - qa and ti - qa,ranking performances of single qa systems td - qa and ti - qa,0.5944317579269409
translation,3,81,results,ranking performances,of,mbrar,ranking performances of mbrar,0.5323148965835571
translation,3,81,results,ranking performances,shows,significant improvements,ranking performances shows significant improvements,0.6650257110595703
translation,3,81,results,mbrar,using,two qa systems ' outputs,mbrar using two qa systems ' outputs,0.6807103157043457
translation,3,81,results,mbrar,shows,significant improvements,mbrar shows significant improvements,0.7185428142547607
translation,3,81,results,significant improvements,on,both jeopardy,significant improvements on both jeopardy,0.5754440426826477
translation,3,81,results,results,see,mbrar,results see mbrar,0.6415697336196899
translation,3,81,results,results,comparing to,ranking performances,results comparing to ranking performances,0.6771105527877808
translation,3,83,results,mbrar,on,single qa system,mbrar on single qa system,0.5911170840263367
translation,3,83,results,mbrar,on,multiple qa systems,mbrar on multiple qa systems,0.5944691300392151
translation,3,83,results,mbrar,on,multiple qa systems,mbrar on multiple qa systems,0.5944691300392151
translation,3,83,results,mbrar,provide,extra gains,mbrar provide extra gains,0.6587700843811035
translation,3,83,results,mbrar,on,multiple qa systems,mbrar on multiple qa systems,0.5944691300392151
translation,3,83,results,mbrar,provide,extra gains,mbrar provide extra gains,0.6587700843811035
translation,3,83,results,extra gains,on,both questions sets,extra gains on both questions sets,0.5266119837760925
translation,3,83,results,mbrar,has,mbrar,mbrar has mbrar,0.6480631232261658
translation,3,83,results,results,comparing to,mbrar,results comparing to mbrar,0.6955170035362244
translation,3,83,results,results,comparing to,mbrar,results comparing to mbrar,0.6955170035362244
translation,3,88,results,mbrar,provides,systematic way,mbrar provides systematic way,0.6484637260437012
translation,3,88,results,systematic way,to re-rank,answers,systematic way to re-rank answers,0.7475413680076599
translation,3,88,results,answers,from,single or multiple qa systems,answers from single or multiple qa systems,0.5899916887283325
translation,3,88,results,heterogeneous implementations,of,internal components,heterogeneous implementations of internal components,0.5242863893508911
translation,4,171,baselines,two existing semantic parsers,designed for,wtq,two existing semantic parsers designed for wtq,0.6637156009674072
translation,4,184,baselines,fp,is,feature - rich system,fp is feature - rich system,0.5778849124908447
translation,4,184,baselines,correct semantic parse,in,logical parse language ),correct semantic parse in logical parse language ),0.5371621251106262
translation,4,184,baselines,correct semantic parse,for,given question,correct semantic parse for given question,0.6104943156242371
translation,4,184,baselines,logical parse language ),for,given question,logical parse language ) for given question,0.6528906226158142
translation,4,184,baselines,baselines,has,fp,baselines has fp,0.6258698105812073
translation,4,191,baselines,dynsp,combines,best parts,dynsp combines best parts,0.7408890128135681
translation,4,191,baselines,best parts,of,fp and np,best parts of fp and np,0.6005256772041321
translation,4,191,baselines,baselines,has,dynsp,baselines has dynsp,0.5421954989433289
translation,4,19,experiments,dataset,of,question sequences,dataset of question sequences,0.5567890405654907
translation,4,19,experiments,question sequences,called,sequentialqa,question sequences called sequentialqa,0.7384788990020752
translation,4,19,experiments,question sequences,asking,crowdsourced workers,question sequences asking crowdsourced workers,0.7588846683502197
translation,4,19,experiments,crowdsourced workers,to decompose,complicated questions,crowdsourced workers to decompose complicated questions,0.6907421350479126
translation,4,19,experiments,complicated questions,sampled from,wikitablequestions dataset ( pasupat,complicated questions sampled from wikitablequestions dataset ( pasupat,0.6566239595413208
translation,4,19,experiments,complicated questions,into,multiple easier ones,complicated questions into multiple easier ones,0.6313247680664062
translation,4,19,experiments,sequentialqa,has,sqa,sequentialqa has sqa,0.6008067727088928
translation,4,20,experiments,sqa,contains,"6,066","sqa contains 6,066",0.6309600472450256
translation,4,20,experiments,question sequences,with,"17,553 total question - answer pairs","question sequences with 17,553 total question - answer pairs",0.5805468559265137
translation,4,20,experiments,"6,066",has,question sequences,"6,066 has question sequences",0.5848689675331116
translation,4,235,experiments,"6,066 unique sequences",of,inter-related questions,"6,066 unique sequences of inter-related questions",0.5775319337844849
translation,4,235,experiments,inter-related questions,about,wikipedia tables,inter-related questions about wikipedia tables,0.6730470657348633
translation,4,235,experiments,inter-related questions,with,"17,553 questions - answer pairs","inter-related questions with 17,553 questions - answer pairs",0.5671629905700684
translation,4,203,hyperparameters,model parameters,using,standard stochastic gradient descent,model parameters using standard stochastic gradient descent,0.5846953988075256
translation,4,203,hyperparameters,hyperparameters,optimize,model parameters,hyperparameters optimize model parameters,0.6409103274345398
translation,4,204,hyperparameters,word embeddings,initialized with,100 -d pretrained glove vectors,word embeddings initialized with 100 -d pretrained glove vectors,0.7074493765830994
translation,4,204,hyperparameters,word embeddings,fine-tuned during,training,word embeddings fine-tuned during training,0.6749808192253113
translation,4,204,hyperparameters,training,with,dropout rate 0.5,training with dropout rate 0.5,0.618075966835022
translation,4,204,hyperparameters,hyperparameters,has,word embeddings,hyperparameters has word embeddings,0.4784160554409027
translation,4,208,hyperparameters,our model,for,30 epochs,our model for 30 epochs,0.5736037492752075
translation,4,208,hyperparameters,hyperparameters,train,our model,hyperparameters train our model,0.6687315702438354
translation,4,7,model,sequential question answering,propose,novel dynamic neural semantic parsing framework,sequential question answering propose novel dynamic neural semantic parsing framework,0.5673809051513672
translation,4,7,model,novel dynamic neural semantic parsing framework,trained using,weakly supervised reward - guided search,novel dynamic neural semantic parsing framework trained using weakly supervised reward - guided search,0.6239601373672485
translation,4,21,model,weakly su- pervised structured - output learning approach,based on,reward - guided search,weakly su- pervised structured - output learning approach based on reward - guided search,0.644931435585022
translation,4,21,model,dynamic neural semantic parsing framework ( dynsp ),has,weakly su- pervised structured - output learning approach,dynamic neural semantic parsing framework ( dynsp ) has weakly su- pervised structured - output learning approach,0.5698252320289612
translation,4,50,model,dynamic neural semantic parsing framework ( dynsp ),trained using,reward - guided search pro-cedure,dynamic neural semantic parsing framework ( dynsp ) trained using reward - guided search pro-cedure,0.7088749408721924
translation,4,50,model,reward - guided search pro-cedure,for solving,sqa,reward - guided search pro-cedure for solving sqa,0.7221768498420715
translation,4,50,model,model,propose,dynamic neural semantic parsing framework ( dynsp ),model propose dynamic neural semantic parsing framework ( dynsp ),0.6789215803146362
translation,4,138,model,conceptually simple learning algorithm,for,weakly supervised training,conceptually simple learning algorithm for weakly supervised training,0.5776599049568176
translation,4,138,model,model,propose,conceptually simple learning algorithm,model propose conceptually simple learning algorithm,0.6963096857070923
translation,4,139,model,inference,using,beam search procedure,inference using beam search procedure,0.671082615852356
translation,4,139,model,beam search procedure,guided by,approximate reward function,beam search procedure guided by approximate reward function,0.6699827909469604
translation,4,237,model,dynsp,for solving,sqa,dynsp for solving sqa,0.6497257351875305
translation,4,237,model,dynsp,has,dynamic neural semantic parsing framework,dynsp has dynamic neural semantic parsing framework,0.5266424417495728
translation,4,237,model,model,propose,dynsp,model propose dynsp,0.6473767161369324
translation,4,238,model,semantic parsing,as,state-action search problem,semantic parsing as state-action search problem,0.4937211871147156
translation,4,238,model,modular neural network models,through,reward - guided search,modular neural network models through reward - guided search,0.5966478586196899
translation,4,238,model,model,By formulating,semantic parsing,model By formulating semantic parsing,0.5960558652877808
translation,4,176,results,fp as - is,results in,poor performance,fp as - is results in poor performance,0.6498492360115051
translation,4,176,results,poor performance,on,sqa,poor performance on sqa,0.5523707270622253
translation,4,176,results,results,Using,fp as - is,results Using fp as - is,0.7050620913505554
translation,4,189,results,both systems,on,sqa,both systems on sqa,0.6045132279396057
translation,4,189,results,both systems,consider,all questions,both systems consider all questions,0.6867780089378357
translation,4,189,results,all questions,within,sequence,all questions within sequence,0.6858022212982178
translation,4,189,results,outperforms,has,both systems,outperforms has both systems,0.595029890537262
translation,4,190,results,sequential information,by including,subsequent action,sequential information by including subsequent action,0.6749014258384705
translation,4,190,results,our method,improves,almost 3 %,our method improves almost 3 %,0.6990377306938171
translation,4,190,results,almost 3 %,in,absolute accuracy,almost 3 % in absolute accuracy,0.5675054788589478
translation,4,190,results,sequential information,has,our method,sequential information has our method,0.5674868226051331
translation,4,190,results,subsequent action,has,our method,subsequent action has our method,0.6083620190620422
translation,4,190,results,results,leverage,sequential information,results leverage sequential information,0.6450324058532715
translation,4,213,results,our method,without,any sequential information,our method without any sequential information,0.7849944829940796
translation,4,213,results,our method,when,subsequent action,our method when subsequent action,0.6651660203933716
translation,4,213,results,subsequent action,improve,overall and sequence accuracy,subsequent action improve overall and sequence accuracy,0.6005454063415527
translation,4,213,results,overall and sequence accuracy,over,concatenated - question baselines,overall and sequence accuracy over concatenated - question baselines,0.681492269039154
translation,4,213,results,our method,has,outperforms,our method has outperforms,0.6322360634803772
translation,4,213,results,any sequential information,has,dynsp ),any sequential information has dynsp ),0.6454322934150696
translation,4,213,results,any sequential information,has,outperforms,any sequential information has outperforms,0.649138331413269
translation,4,213,results,dynsp ),has,outperforms,dynsp ) has outperforms,0.6439348459243774
translation,4,213,results,outperforms,has,standard baselines,outperforms has standard baselines,0.5914391875267029
translation,4,213,results,results,has,our method,results has our method,0.5589964985847473
translation,4,214,results,struggle,to,answer,struggle to answer,0.6463472843170166
translation,4,214,results,all questions,within,sequence,all questions within sequence,0.6858022212982178
translation,4,214,results,all of the systems,has,struggle,all of the systems has struggle,0.6276572346687317
translation,4,214,results,answer,has,all questions,answer has all questions,0.5894497632980347
translation,5,152,ablation-analysis,n-grams,added to,model,n-grams added to model,0.6203061938285828
translation,5,152,ablation-analysis,model,by means of,wsk,model by means of wsk,0.6852980256080627
translation,5,152,ablation-analysis,f1,improves,about 1.5 absolute points,f1 improves about 1.5 absolute points,0.7153921723365784
translation,5,152,ablation-analysis,n-grams,has,f1,n-grams has f1,0.5863999128341675
translation,5,168,ablation-analysis,more complex kernels,especially,overall kernel summation,more complex kernels especially overall kernel summation,0.6556031107902527
translation,5,168,ablation-analysis,more complex kernels,not seem to,improve,more complex kernels not seem to improve,0.6542842388153076
translation,5,168,ablation-analysis,improve,has,per-,improve has per-,0.6186532974243164
translation,5,168,ablation-analysis,per-,has,formance,per- has formance,0.6163761019706726
translation,5,168,ablation-analysis,ablation analysis,has,more complex kernels,ablation analysis has more complex kernels,0.5515173673629761
translation,5,135,baselines,baseline model,is,rule-based classifier ( rbc ),baseline model is rule-based classifier ( rbc ),0.5436875820159912
translation,5,135,baselines,baselines,has,baseline model,baselines has baseline model,0.5873855352401733
translation,5,7,model,kernel methods,applied to,syntactic / semantic structures,kernel methods applied to syntactic / semantic structures,0.6891440153121948
translation,5,7,model,kernel methods,for,accurate classification of jeopardy ! definition questions,kernel methods for accurate classification of jeopardy ! definition questions,0.5952594876289368
translation,5,7,model,model,study,kernel methods,model study kernel methods,0.6238153576850891
translation,5,39,model,svms and kernel methods,to,syntactic / semantic structures,svms and kernel methods to syntactic / semantic structures,0.5164709091186523
translation,5,39,model,svms and kernel methods,for modeling,accurate classification of jeopardy,svms and kernel methods for modeling accurate classification of jeopardy,0.7610388994216919
translation,5,39,model,model,apply,svms and kernel methods,model apply svms and kernel methods,0.655439019203186
translation,5,40,model,several levels,of,linguistic information,several levels of linguistic information,0.5807034373283386
translation,5,40,model,model,use,several levels,model use several levels,0.6967101693153381
translation,5,40,model,model,combined them using,state- ofthe - art structural kernels,model combined them using state- ofthe - art structural kernels,0.6549585461616516
translation,5,41,results,our best model,improves,f1,our best model improves f1,0.6983779072761536
translation,5,41,results,f1,of,our baseline model,f1 of our baseline model,0.5752585530281067
translation,5,41,results,f1,by,67 % relative,f1 by 67 % relative,0.6189449429512024
translation,5,41,results,our baseline model,by,67 % relative,our baseline model by 67 % relative,0.6016843914985657
translation,5,41,results,67 % relative,from,40.37 to 67.48,67 % relative from 40.37 to 67.48,0.49933114647865295
translation,5,46,results,watson,with,kernel - based definition classification,watson with kernel - based definition classification,0.662355899810791
translation,5,46,results,watson,achieves,statistically significant improvement,watson achieves statistically significant improvement,0.6747798919677734
translation,5,46,results,specialized definition question processing,achieves,statistically significant improvement,specialized definition question processing achieves statistically significant improvement,0.6517745852470398
translation,5,46,results,statistically significant improvement,compared to,our baseline systems,statistically significant improvement compared to our baseline systems,0.6243612170219421
translation,5,46,results,endto-end evaluations,has,watson,endto-end evaluations has watson,0.5570014119148254
translation,5,46,results,results,show,endto-end evaluations,results show endto-end evaluations,0.6241416931152344
translation,5,46,results,results,in,endto-end evaluations,results in endto-end evaluations,0.5411586761474609
translation,5,147,results,performance,obtained using,different kernels ( feature spaces ),performance obtained using different kernels ( feature spaces ),0.6099755167961121
translation,5,147,results,different kernels ( feature spaces ),with,svms,different kernels ( feature spaces ) with svms,0.6043750047683716
translation,5,148,results,rbc,has,good recall,rbc has good recall,0.6056241989135742
translation,5,151,results,bow,yields,better f1,bow yields better f1,0.6968361735343933
translation,5,151,results,better f1,than,rbc,better f1 than rbc,0.6153507232666016
translation,5,151,results,results,has,bow,results has bow,0.4719364643096924
translation,5,154,results,stk,applied to,ct,stk applied to ct,0.690419614315033
translation,5,154,results,stk,provides,accuracy,stk provides accuracy,0.6533952951431274
translation,5,154,results,accuracy,lower than,bow,accuracy lower than bow,0.7412503361701965
translation,5,158,results,ptk - ct,achieves,highest f1,ptk - ct achieves highest f1,0.6662381887435913
translation,5,158,results,wsk,when used with,different syntactic paradigm,wsk when used with different syntactic paradigm,0.638908326625824
translation,5,158,results,outperforming,has,wsk,outperforming has wsk,0.5687452554702759
translation,5,158,results,results,has,ptk - ct,results has ptk - ct,0.5526707172393799
translation,5,159,results,psk and pass,provide,lower accuracy,psk and pass provide lower accuracy,0.6457116603851318
translation,5,159,results,psk and pass,useful in,kernel combinations,psk and pass useful in kernel combinations,0.7343149185180664
translation,5,159,results,lower accuracy,useful in,kernel combinations,lower accuracy useful in kernel combinations,0.6724830269813538
translation,5,159,results,results,has,psk and pass,results has psk and pass,0.5679662227630615
translation,5,160,results,rather effective,for,classifying,rather effective for classifying,0.7299785017967224
translation,5,160,results,classifying,has,definition questions,classifying has definition questions,0.6187384724617004
translation,5,160,results,results,has,csk,results has csk,0.5687531232833862
translation,5,166,results,csk,complements,wsk information,csk complements wsk information,0.7350283861160278
translation,5,166,results,csk,achieving,substantially better result,csk achieving substantially better result,0.6690093278884888
translation,5,166,results,ptk -ct + csk,performs,even better,ptk -ct + csk performs even better,0.623026430606842
translation,5,166,results,ptk -ct + csk,adding,rbc,ptk -ct + csk adding rbc,0.6877394914627075
translation,5,166,results,even better,than,wsk + csk,even better than wsk + csk,0.6034262776374817
translation,5,166,results,rbc,has,improves further,rbc has improves further,0.6323684453964233
translation,5,166,results,results,note,csk,results note csk,0.6491047143936157
translation,5,166,results,results,note,ptk -ct + csk,results note ptk -ct + csk,0.5886150598526001
translation,5,172,results,component level evaluation,achieved,comparable performance,component level evaluation achieved comparable performance,0.6992439031600952
translation,5,172,results,comparable performance,with,esg,comparable performance with esg,0.7085369825363159
translation,5,172,results,results,has,component level evaluation,results has component level evaluation,0.5188944935798645
translation,5,200,results,statdef,improved upon,nodef baseline,statdef improved upon nodef baseline,0.750027060508728
translation,5,200,results,nodef baseline,more than,ruledef,nodef baseline more than ruledef,0.6332751512527466
translation,5,201,results,accuracy metric,where,all samples,accuracy metric where all samples,0.6417055726051331
translation,5,201,results,accuracy metric,difference in,performance,accuracy metric difference in performance,0.6377059817314148
translation,5,201,results,all samples,are,paired and independent,all samples are paired and independent,0.5911830067634583
translation,5,201,results,performance,between,statdef and nodef systems,performance between statdef and nodef systems,0.667261004447937
translation,5,201,results,performance,between,ruledef and nodef systems,performance between ruledef and nodef systems,0.6492099165916443
translation,5,201,results,performance,statistically significant at,p<0.05,performance statistically significant at p<0.05,0.6416222453117371
translation,5,201,results,results,for,accuracy metric,results for accuracy metric,0.5833677649497986
translation,5,206,results,ruledef system,performed,worse,ruledef system performed worse,0.279661625623703
translation,5,206,results,worse,than,nodef baseline,worse than nodef baseline,0.6093264818191528
translation,5,206,results,outperformed,has,nodef baseline,outperformed has nodef baseline,0.6163333058357239
translation,5,209,results,accuracy improvement,upon,ruledef system,accuracy improvement upon ruledef system,0.6223482489585876
translation,5,209,results,statdef system,has,outperformed,statdef system has outperformed,0.6320472359657288
translation,5,209,results,outperformed,has,two other systems,outperformed has two other systems,0.6292756199836731
translation,5,209,results,outperformed,has,accuracy improvement,outperformed has accuracy improvement,0.6344191431999207
translation,5,209,results,results,has,statdef system,results has statdef system,0.5849160552024841
translation,6,29,ablation-analysis,lexicalized and dependency tree path features,important to,performance of the model,lexicalized and dependency tree path features important to performance of the model,0.6346793174743652
translation,6,29,ablation-analysis,ablation analysis,find that,lexicalized and dependency tree path features,ablation analysis find that lexicalized and dependency tree path features,0.6141616702079773
translation,6,180,ablation-analysis,lexicalized and dependency tree path features,are,most important,lexicalized and dependency tree path features are most important,0.5337546467781067
translation,6,180,ablation-analysis,ablation analysis,indicate,lexicalized and dependency tree path features,ablation analysis indicate lexicalized and dependency tree path features,0.5541669130325317
translation,6,181,ablation-analysis,dependency tree path features,play,much bigger role,dependency tree path features play much bigger role,0.7320362329483032
translation,6,181,ablation-analysis,much bigger role,in,our dataset,much bigger role in our dataset,0.5225440263748169
translation,6,181,ablation-analysis,ablation analysis,note that,dependency tree path features,ablation analysis note that dependency tree path features,0.5892019271850586
translation,6,182,ablation-analysis,model,significantly overfits,training set,model significantly overfits training set,0.7929651737213135
translation,6,182,ablation-analysis,lexicalized features,has,model,lexicalized features has model,0.5819850564002991
translation,6,182,ablation-analysis,hurts,has,performance,hurts has performance,0.6043882966041565
translation,6,182,ablation-analysis,ablation analysis,with,lexicalized features,ablation analysis with lexicalized features,0.6379943490028381
translation,6,193,ablation-analysis,performance,of,logistic regression model,performance of logistic regression model,0.6090540289878845
translation,6,193,ablation-analysis,lower,has,performance,lower has performance,0.6062254309654236
translation,6,193,ablation-analysis,ablation analysis,shows,more divergence,ablation analysis shows more divergence,0.656236469745636
translation,6,132,hyperparameters,multiclass log-likelihood loss,optimized using,adagrad,multiclass log-likelihood loss optimized using adagrad,0.6190845966339111
translation,6,132,hyperparameters,adagrad,with,initial learning rate,adagrad with initial learning rate,0.5808988213539124
translation,6,132,hyperparameters,initial learning rate,of,0.1,initial learning rate of 0.1,0.5805032253265381
translation,6,132,hyperparameters,hyperparameters,has,multiclass log-likelihood loss,hyperparameters has multiclass log-likelihood loss,0.48360568284988403
translation,6,134,hyperparameters,l 2 regularization,with,coefficient,l 2 regularization with coefficient,0.6210079789161682
translation,6,134,hyperparameters,coefficient,of,0.1,coefficient of 0.1,0.5974176526069641
translation,6,134,hyperparameters,0.1,divided by,number of batches,0.1 divided by number of batches,0.6893315315246582
translation,6,134,hyperparameters,hyperparameters,has,l 2 regularization,hyperparameters has l 2 regularization,0.4408474564552307
translation,6,133,model,update,performed on,batch,update performed on batch,0.6906044483184814
translation,6,133,model,of all questions,in,paragraph,of all questions in paragraph,0.5657123327255249
translation,6,133,model,batch,has,of all questions,batch has of all questions,0.625821053981781
translation,6,133,model,model,has,update,model has update,0.5578369498252869
translation,6,6,results,strong logistic regression model,achieves,f1 score,strong logistic regression model achieves f1 score,0.6548228859901428
translation,6,6,results,f1 score,of,51.0 %,f1 score of 51.0 %,0.5363820195198059
translation,6,6,results,significant improvement,over,simple baseline ( 20 % ),significant improvement over simple baseline ( 20 % ),0.640997052192688
translation,6,6,results,results,build,strong logistic regression model,results build strong logistic regression model,0.6762765049934387
translation,6,7,results,human performance,is,much higher,human performance is much higher,0.5855618715286255
translation,6,7,results,86.8 % ),is,much higher,86.8 % ) is much higher,0.5663597583770752
translation,6,7,results,human performance,has,86.8 % ),human performance has 86.8 % ),0.5648579001426697
translation,6,7,results,results,has,human performance,results has human performance,0.5620672106742859
translation,6,30,results,worsens,with,increasing complexity,worsens with increasing complexity,0.674643337726593
translation,6,30,results,increasing complexity,of,answer types,increasing complexity of answer types,0.5760782361030579
translation,6,30,results,increasing complexity,of,syntactic divergence,increasing complexity of syntactic divergence,0.554893970489502
translation,6,30,results,syntactic divergence,between,question and the sentence containing,syntactic divergence between question and the sentence containing,0.6079257130622864
translation,6,30,results,model performance,has,worsens,model performance has worsens,0.6164821982383728
translation,6,30,results,results,find that,model performance,results find that model performance,0.6365314722061157
translation,6,31,results,best model,achieves,f1 score,best model achieves f1 score,0.6391150951385498
translation,6,31,results,f1 score,of,51.0 %,f1 score of 51.0 %,0.5363820195198059
translation,6,31,results,51.0 %,much better than,sliding window baseline ( 20 % ),51.0 % much better than sliding window baseline ( 20 % ),0.7092472314834595
translation,6,31,results,results,has,best model,results has best model,0.5634682774543762
translation,6,176,results,logistic regression model,has,significantly outperforms,logistic regression model has significantly outperforms,0.5918926000595093
translation,6,176,results,significantly outperforms,has,baselines,significantly outperforms has baselines,0.6014122366905212
translation,6,176,results,underperforms,has,humans,underperforms has humans,0.5929281115531921
translation,6,176,results,results,has,logistic regression model,results has logistic regression model,0.5230032801628113
translation,6,177,results,model,able to select,sentence,model able to select sentence,0.7169926166534424
translation,6,177,results,sentence,containing,answer,sentence containing answer,0.6679041981697083
translation,6,177,results,correctly,with,79.3 % accuracy,correctly with 79.3 % accuracy,0.6447962522506714
translation,6,177,results,answer,has,correctly,answer has correctly,0.6051666736602783
translation,6,177,results,results,note,model,results note model,0.5983958840370178
translation,7,122,ablation-analysis,word matching features,help to further boost,cnn results,word matching features help to further boost cnn results,0.6148554086685181
translation,7,122,ablation-analysis,cnn results,by,3 to 4 points,cnn results by 3 to 4 points,0.5829633474349976
translation,7,122,ablation-analysis,cnn results,approximately,3 to 4 points,cnn results approximately 3 to 4 points,0.6788718700408936
translation,7,122,ablation-analysis,3 to 4 points,in,map and mrr,3 to 4 points in map and mrr,0.5828810334205627
translation,7,122,ablation-analysis,ablation analysis,has,word matching features,ablation analysis has word matching features,0.5438527464866638
translation,7,88,baselines,non-stopwords,in,question,non-stopwords in question,0.5312154293060303
translation,7,88,baselines,non-stopwords,occur in,answer sentence,non-stopwords occur in answer sentence,0.6614716053009033
translation,7,88,baselines,baselines,has,first method,baselines has first method,0.5801792740821838
translation,7,98,hyperparameters,pre-trained word2vec embeddings,as,model input,pre-trained word2vec embeddings as model input,0.48384079337120056
translation,7,98,hyperparameters,hyperparameters,use,pre-trained word2vec embeddings,hyperparameters use pre-trained word2vec embeddings,0.5228894352912903
translation,7,32,results,lexical semantic methods,yield,better performance,lexical semantic methods yield better performance,0.7271307110786438
translation,7,32,results,better performance,than,sentence semantic models,better performance than sentence semantic models,0.5648732781410217
translation,7,32,results,sentence semantic models,on,qasent,sentence semantic models on qasent,0.5292143821716309
translation,7,32,results,sentence semantic models,on,wikiqa,sentence semantic models on wikiqa,0.5446566343307495
translation,7,32,results,lexical semantic models,on,wikiqa,lexical semantic models on wikiqa,0.5585342049598694
translation,7,32,results,sentence semantic approaches,has,outperform,sentence semantic approaches has outperform,0.41367387771606445
translation,7,32,results,outperform,has,lexical semantic models,outperform has lexical semantic models,0.5879480242729187
translation,7,32,results,results,show,lexical semantic methods,results show lexical semantic methods,0.5638356804847717
translation,7,32,results,results,show,sentence semantic approaches,results show sentence semantic approaches,0.5669549703598022
translation,7,117,results,two word matching methods,are,very strong baselines,two word matching methods are very strong baselines,0.5332646369934082
translation,7,117,results,qasent dataset,has,two word matching methods,qasent dataset has two word matching methods,0.5278563499450684
translation,7,117,results,sig-nificantly outperform,has,sentence semantic models,sig-nificantly outperform has sentence semantic models,0.6220169067382812
translation,7,117,results,results,On,qasent dataset,results On qasent dataset,0.5340300798416138
translation,7,118,results,lclr,improves,results,lclr improves results,0.6449252367019653
translation,7,118,results,rich lexical semantic information,has,lclr,rich lexical semantic information has lclr,0.5776357054710388
translation,7,118,results,results,incorporating,rich lexical semantic information,results incorporating rich lexical semantic information,0.6711580157279968
translation,7,119,results,cnn - cnt,gives,results,cnn - cnt gives results,0.6347310543060303
translation,7,119,results,cnn - cnt,match,lclr,cnn - cnt match lclr,0.7675432562828064
translation,7,119,results,results,match,lclr,results match lclr,0.7036455273628235
translation,7,119,results,worse,than,story,worse than story,0.6356493830680847
translation,7,119,results,story,on,wikiqa dataset,story on wikiqa dataset,0.5490623116493225
translation,7,119,results,results,has,cnn - cnt,results has cnn - cnt,0.5512800216674805
translation,7,120,results,word matching,are,not sufficient,word matching are not sufficient,0.5861125588417053
translation,7,120,results,not sufficient,to achieve,good results,not sufficient to achieve good results,0.6429536938667297
translation,7,121,results,cnn,performs,slightly better,cnn performs slightly better,0.6191988587379456
translation,7,121,results,slightly better,than,lclr,slightly better than lclr,0.5652050375938416
translation,7,121,results,cnn,has,significantly outperforms,cnn has significantly outperforms,0.61006760597229
translation,7,121,results,significantly outperforms,has,simple word matching methods,significantly outperforms has simple word matching methods,0.5936165452003479
translation,7,121,results,results,has,cnn,results has cnn,0.5897790193557739
translation,7,130,results,substantially improved,by adding,simple qlen feature,substantially improved by adding simple qlen feature,0.7438876628875732
translation,7,132,results,slen features,give,small improvement,slen features give small improvement,0.6803728938102722
translation,7,132,results,small improvement,in,performance,small improvement in performance,0.5586368441581726
translation,7,132,results,slightly negative influence,on,results,slightly negative influence on results,0.522101879119873
translation,7,132,results,qclass feature,has,slightly negative influence,qclass feature has slightly negative influence,0.5578365325927734
translation,7,132,results,results,has,slen features,results has slen features,0.5334537029266357
translation,7,134,results,performance,on,wikiqa dataset,performance on wikiqa dataset,0.5341789126396179
translation,7,134,results,wikiqa dataset,inferior to,qasent dataset,wikiqa dataset inferior to qasent dataset,0.6292366981506348
translation,7,135,results,output,of,cnn - cnt,output of cnn - cnt,0.6230053305625916
translation,7,135,results,output,of,best performing model,output of best performing model,0.6081178784370422
translation,7,135,results,best performing model,on,wikiqa dev set,best performing model on wikiqa dev set,0.5328558683395386
translation,7,135,results,output,has,best performing model,output has best performing model,0.5766022205352783
translation,7,135,results,cnn - cnt,has,best performing model,cnn - cnt has best performing model,0.5758637189865112
translation,7,135,results,results,Examining,output,results Examining output,0.6436815857887268
translation,8,9,experiments,grice ranking,participated to,semeval 2017,grice ranking participated to semeval 2017,0.6725210547447205
translation,8,120,hyperparameters,brown embedding,with,n-grams level,brown embedding with n-grams level,0.6259869337081909
translation,8,120,hyperparameters,brown embedding,with,weight,brown embedding with weight,0.6747642159461975
translation,8,120,hyperparameters,brown embedding,with,weight,brown embedding with weight,0.6747642159461975
translation,8,120,hyperparameters,weight,of,0.5,weight of 0.5,0.6499483585357666
translation,8,120,hyperparameters,weight,of,0.5,weight of 0.5,0.6499483585357666
translation,8,120,hyperparameters,0.5,to,embedding similarity,0.5 to embedding similarity,0.5297774076461792
translation,8,120,hyperparameters,0.5,to,string similarity,0.5 to string similarity,0.5325152277946472
translation,8,120,hyperparameters,0.5,to,string similarity,0.5 to string similarity,0.5325152277946472
translation,8,120,hyperparameters,hyperparameters,used,brown embedding,hyperparameters used brown embedding,0.6029733419418335
translation,8,5,model,community answers ranking system,based on,grice maxims,community answers ranking system based on grice maxims,0.6753790974617004
translation,8,5,model,model,present,community answers ranking system,model present community answers ranking system,0.6401011943817139
translation,8,6,model,ranking system,based on,answer relevancy scores,ranking system based on answer relevancy scores,0.6178601980209351
translation,8,6,model,answer relevancy scores,assigned by,three main components,answer relevancy scores assigned by three main components,0.6680619120597839
translation,8,6,model,model,describe,ranking system,model describe ranking system,0.6559098362922668
translation,8,143,results,our system,obtained,rank 7,our system obtained rank 7,0.6391960978507996
translation,8,143,results,rank 7,of,12,rank 7 of 12,0.6988178491592407
translation,8,143,results,rank 7,of,out of 13 participated systems,rank 7 of out of 13 participated systems,0.5955281257629395
translation,8,143,results,map,of,78.56,map of 78.56,0.5892822742462158
translation,8,143,results,12,has,out of 13 participated systems,12 has out of 13 participated systems,0.6287619471549988
translation,8,143,results,results,has,our system,results has our system,0.5954442024230957
translation,9,206,ablation-analysis,stage,lo- calizes,relevant moments,stage lo- calizes relevant moments,0.7904214262962341
translation,9,206,ablation-analysis,stage,detects,referred objects and people,stage detects referred objects and people,0.7213935256004333
translation,9,206,ablation-analysis,relevant moments,with,temporal miou,relevant moments with temporal miou,0.6880078315734863
translation,9,206,ablation-analysis,temporal miou,of,32.49 %,temporal miou of 32.49 %,0.5627244114875793
translation,9,206,ablation-analysis,referred objects and people,with,map,referred objects and people with map,0.6570098400115967
translation,9,206,ablation-analysis,map,of,27.34 %,map of 27.34 %,0.5871341824531555
translation,9,206,ablation-analysis,ablation analysis,has,stage,ablation analysis has stage,0.5239608287811279
translation,9,235,ablation-analysis,performance,improved,70.23 %,performance improved 70.23 %,0.7188678979873657
translation,9,235,ablation-analysis,temporal supervision,has,performance,temporal supervision has performance,0.5556566715240479
translation,9,235,ablation-analysis,ablation analysis,Adding,temporal supervision,ablation analysis Adding temporal supervision,0.706635594367981
translation,9,199,baselines,baselines,consider,"two-stream model ( lei et al. , 2018 )","baselines consider two-stream model ( lei et al. , 2018 )",0.6287840008735657
translation,9,202,baselines,"st - vqa ( jang et al. , 2017 ) model",designed for,question answering,"st - vqa ( jang et al. , 2017 ) model designed for question answering",0.6455997228622437
translation,9,202,baselines,question answering,on,short videos ( gifs ),question answering on short videos ( gifs ),0.5454627275466919
translation,9,202,baselines,baselines,consider,"st - vqa ( jang et al. , 2017 ) model","baselines consider st - vqa ( jang et al. , 2017 ) model",0.6065691709518433
translation,9,19,experiments,new joint spatio-temporal annotations,for,existing video qa dataset,new joint spatio-temporal annotations for existing video qa dataset,0.5569814443588257
translation,9,135,experiments,text input,use,"bert ( devlin et al. , 2019 )","text input use bert ( devlin et al. , 2019 )",0.6135172247886658
translation,9,135,experiments,"transformer - based language model ( vaswani et al. , 2017 )",achieves,state-,"transformer - based language model ( vaswani et al. , 2017 ) achieves state-",0.6084176898002625
translation,9,135,experiments,"bert ( devlin et al. , 2019 )",has,"transformer - based language model ( vaswani et al. , 2017 )","bert ( devlin et al. , 2019 ) has transformer - based language model ( vaswani et al. , 2017 )",0.5552910566329956
translation,9,133,hyperparameters,pca,to reduce,feature dimension,pca to reduce feature dimension,0.6201512217521667
translation,9,133,hyperparameters,feature dimension,from,2048 to 300,feature dimension from 2048 to 300,0.5887762308120728
translation,9,133,hyperparameters,hyperparameters,keep,top - 20 object proposals,hyperparameters keep top - 20 object proposals,0.5238527655601501
translation,9,137,hyperparameters,768d word - level embeddings,from,second - to - last layer,768d word - level embeddings from second - to - last layer,0.564227283000946
translation,9,137,hyperparameters,second - to - last layer,for,subtitles and each hypothesis,second - to - last layer for subtitles and each hypothesis,0.6446301341056824
translation,9,7,model,spatio-temporal answerer,with,grounded evidence ( stage ),spatio-temporal answerer with grounded evidence ( stage ),0.6486085057258606
translation,9,7,model,unified framework,grounds,evidence,unified framework grounds evidence,0.7228465676307678
translation,9,7,model,evidence,in,spatial and temporal domains,evidence in spatial and temporal domains,0.593621551990509
translation,9,7,model,evidence,to answer,questions,evidence to answer questions,0.719799816608429
translation,9,7,model,questions,about,videos,questions about videos,0.6906497478485107
translation,9,7,model,spatio-temporal answerer,has,unified framework,spatio-temporal answerer has unified framework,0.5466782450675964
translation,9,7,model,grounded evidence ( stage ),has,unified framework,grounded evidence ( stage ) has unified framework,0.5596457123756409
translation,9,7,model,model,propose,spatio-temporal answerer,model propose spatio-temporal answerer,0.6842190623283386
translation,9,17,model,video clips,as well as,spatial regions,video clips as well as spatial regions,0.5974534153938293
translation,9,17,model,spatial regions,for answering,videobased questions,spatial regions for answering videobased questions,0.7698882818222046
translation,9,35,model,spatio-temporal answerer,with,grounded evidence ( stage ),spatio-temporal answerer with grounded evidence ( stage ),0.6486085057258606
translation,9,35,model,question answering,in,unified framework,question answering in unified framework,0.5241129994392395
translation,9,41,model,spatio-temporal answerer,with,grounded evidence ( stage ),spatio-temporal answerer with grounded evidence ( stage ),0.6486085057258606
translation,9,41,model,spatio-temporal answerer,to jointly localize,answer questions,spatio-temporal answerer to jointly localize answer questions,0.7799243927001953
translation,9,41,model,model,design,novel video question answering framework,model design novel video question answering framework,0.5359033942222595
translation,9,112,model,stage,encodes,"video and text ( subtitle , qa )","stage encodes video and text ( subtitle , qa )",0.7532501220703125
translation,9,112,model,"video and text ( subtitle , qa )",via,frame - wise regional visual representations,"video and text ( subtitle , qa ) via frame - wise regional visual representations",0.6753643751144409
translation,9,112,model,"video and text ( subtitle , qa )",via,neural language representations,"video and text ( subtitle , qa ) via neural language representations",0.6459949612617493
translation,9,112,model,model,has,stage,model has stage,0.5860474705696106
translation,9,113,model,contextualized,using,convolutional encoder,contextualized using convolutional encoder,0.6705381274223328
translation,9,113,model,model,has,encoded video and text representations,model has encoded video and text representations,0.5451242327690125
translation,9,114,model,stage,computes,attention scores,stage computes attention scores,0.7332884669303894
translation,9,114,model,attention scores,from,each qa word,attention scores from each qa word,0.5528548359870911
translation,9,114,model,each qa word,to,object regions and subtitle words,each qa word to object regions and subtitle words,0.560499906539917
translation,9,114,model,model,has,stage,model has stage,0.5860474705696106
translation,9,136,model,bert - base model,using,masked language model,bert - base model using masked language model,0.6627304553985596
translation,9,136,model,bert - base model,using,next sentence pre-diction objectives,bert - base model using next sentence pre-diction objectives,0.6579174399375916
translation,9,136,model,next sentence pre-diction objectives,on,subtitles and qa pairs,next sentence pre-diction objectives on subtitles and qa pairs,0.547836422920227
translation,9,136,model,subtitles and qa pairs,from,tvqa + train set,subtitles and qa pairs from tvqa + train set,0.5450895428657532
translation,9,136,model,model,fine - tune,bert - base model,model fine - tune bert - base model,0.6654216051101685
translation,9,143,model,two convolutional encoders,at,two different levels of stage,two convolutional encoders at two different levels of stage,0.554859459400177
translation,9,143,model,two convolutional encoders,with,kernel size 5,two convolutional encoders with kernel size 5,0.6583728194236755
translation,9,143,model,one,with,kernel size 7,one with kernel size 7,0.70450758934021
translation,9,143,model,one,with,kernel size 5,one with kernel size 5,0.7141105532646179
translation,9,143,model,kernel size 7,to encode,raw inputs,kernel size 7 to encode raw inputs,0.6761378049850464
translation,9,143,model,kernel size 5,to encode,fused video-text representation,kernel size 5 to encode fused video-text representation,0.7084753513336182
translation,9,143,model,model,use,two convolutional encoders,model use two convolutional encoders,0.6044220924377441
translation,9,220,model,model,has,span proposal and local feature,model has span proposal and local feature,0.5379939079284668
translation,9,42,results,our model,achieves,significant performance gains,our model achieves significant performance gains,0.676681399345398
translation,9,42,results,significant performance gains,over,baselines,significant performance gains over baselines,0.6981847286224365
translation,9,42,results,all three sub-tasks together,has,our model,all three sub-tasks together has our model,0.6066985726356506
translation,9,42,results,results,performing,all three sub-tasks together,results performing all three sub-tasks together,0.6102603673934937
translation,9,205,results,outperforms,by,large margin,outperforms by large margin,0.6290680766105652
translation,9,205,results,baseline model ( two -stream ),by,large margin,baseline model ( two -stream ) by large margin,0.5719478726387024
translation,9,205,results,large margin,in,qa acc.,large margin in qa acc.,0.5497990846633911
translation,9,205,results,large margin,with,9.83 % relative gains,large margin with 9.83 % relative gains,0.6217847466468811
translation,9,205,results,stage,has,outperforms,stage has outperforms,0.6752707362174988
translation,9,205,results,outperforms,has,baseline model ( two -stream ),outperforms has baseline model ( two -stream ),0.5994144678115845
translation,9,205,results,results,has,stage,results has stage,0.5485928058624268
translation,9,215,results,backbone model,obtains,68.31 %,backbone model obtains 68.31 %,0.5598971843719482
translation,9,215,results,backbone model,obtains,significantly higher,backbone model obtains significantly higher,0.6535007357597351
translation,9,215,results,68.31 %,on,qa acc.,68.31 % on qa acc.,0.5640606880187988
translation,9,215,results,significantly higher,than,baseline 's 65.79 %,significantly higher than baseline 's 65.79 %,0.5366365313529968
translation,9,218,results,model,able to,ground,model able to ground,0.6495181322097778
translation,9,218,results,ground,on,temporal axis,ground on temporal axis,0.5619566440582275
translation,9,218,results,ground,improves,model 's performance,ground improves model 's performance,0.6842761039733887
translation,9,218,results,model 's performance,on,other tasks,model 's performance on other tasks,0.5283454060554504
translation,9,218,results,temporal supervision,has,model,temporal supervision has model,0.5475858449935913
translation,9,218,results,results,After adding,temporal supervision,results After adding temporal supervision,0.6478886008262634
translation,9,219,results,spatial supervision,gives,additional improvements,spatial supervision gives additional improvements,0.5988258123397827
translation,9,219,results,additional improvements,particularly for,grd. map,additional improvements particularly for grd. map,0.6315597295761108
translation,9,219,results,grd. map,with,121.92 % relative gain,grd. map with 121.92 % relative gain,0.6596900224685669
translation,9,219,results,results,Adding,spatial supervision,results Adding spatial supervision,0.6054351329803467
translation,9,223,results,g l,achieve,best performance,g l achieve best performance,0.6270867586135864
translation,9,223,results,best performance,across,all metrics,best performance across all metrics,0.6618552207946777
translation,9,223,results,best performance,indicating,benefit,best performance indicating benefit,0.6890280246734619
translation,9,223,results,benefit,of using,local features,benefit of using local features,0.7308209538459778
translation,9,223,results,results,With,g l,results With g l,0.6193364858627319
translation,9,234,results,stage backbone,able to achieve,3.91 % relative gain,stage backbone able to achieve 3.91 % relative gain,0.6279577016830444
translation,9,234,results,3.91 % relative gain,from,best published result ( multi-task ),3.91 % relative gain from best published result ( multi-task ),0.48886945843696594
translation,9,234,results,3.91 % relative gain,on,tvqa test-public set,3.91 % relative gain on tvqa test-public set,0.5109902024269104
translation,9,234,results,best published result ( multi-task ),on,tvqa test-public set,best published result ( multi-task ) on tvqa test-public set,0.526721179485321
translation,9,234,results,temporal supervision,has,stage backbone,temporal supervision has stage backbone,0.5916602611541748
translation,9,234,results,results,Without,temporal supervision,results Without temporal supervision,0.6428101062774658
translation,9,237,results,stage models,achieve,better results,stage models achieve better results,0.6498961448669434
translation,9,237,results,glove,has,stage models,glove has stage models,0.5611494779586792
translation,9,237,results,results,Using,glove,results Using glove,0.7243146300315857
translation,10,46,experimental-setup,service implementation,based on,scala and the play framework,service implementation based on scala and the play framework,0.6518723964691162
translation,10,46,experimental-setup,experimental setup,has,service implementation,experimental setup has service implementation,0.547193169593811
translation,10,78,experimental-setup,qa - frontend service,implemented in,python,qa - frontend service implemented in python,0.7143369913101196
translation,10,78,experimental-setup,python,with,flask,python with flask,0.6756446361541748
translation,10,78,experimental-setup,experimental setup,has,qa - frontend service,experimental setup has qa - frontend service,0.5526221394538879
translation,10,44,experiments,all existing candidates,of,target dataset,all existing candidates of target dataset,0.5839175581932068
translation,10,44,experiments,all existing candidates,with,elasticsearch,all existing candidates with elasticsearch,0.5990169048309326
translation,10,44,experiments,all existing candidates,with,opensource high- performance search engine,all existing candidates with opensource high- performance search engine,0.6184802651405334
translation,10,44,experiments,target dataset,with,elasticsearch,target dataset with elasticsearch,0.6021847128868103
translation,10,44,experiments,elasticsearch,has,opensource high- performance search engine,elasticsearch has opensource high- performance search engine,0.5258814096450806
translation,10,25,model,models,for,non,models for non,0.7057977914810181
translation,10,25,model,transform,has,models,transform has models,0.6286506652832031
translation,10,25,model,model,present,extensible service architecture,model present extensible service architecture,0.5535590648651123
translation,10,27,model,answer selection,rely on,service orchestration,answer selection rely on service orchestration,0.7055271863937378
translation,10,27,model,service orchestration,integrates,multiple independent webservices,service orchestration integrates multiple independent webservices,0.6316131949424744
translation,10,27,model,multiple independent webservices,with,separate responsibilities,multiple independent webservices with separate responsibilities,0.5681641697883606
translation,10,45,model,unified interface,for,retrieval,unified interface for retrieval,0.6495705842971802
translation,10,45,model,unified interface,query,index,unified interface query index,0.7539177536964417
translation,10,45,model,retrieval,of,answer candidates,retrieval of answer candidates,0.6145935654640198
translation,10,45,model,index,with,question text,index with question text,0.5922008156776428
translation,10,45,model,question text,using,bm25,question text using bm25,0.7193217873573303
translation,10,45,model,bm25,as,similarity measure,bm25 as similarity measure,0.5558696389198303
translation,10,28,results,communicate,using,well - defined http rest api,communicate using well - defined http rest api,0.6535593867301941
translation,10,28,results,our system,achieves,strong extensibility properties,our system achieves strong extensibility properties,0.6234702467918396
translation,11,32,ablation-analysis,rocc,is,more stable,rocc is more stable,0.5796871781349182
translation,11,32,ablation-analysis,more stable,across,different domains,more stable across different domains,0.6941362023353577
translation,11,32,ablation-analysis,more stable,than,supervised strategy,more stable than supervised strategy,0.6007885336875916
translation,11,32,ablation-analysis,different domains,in,multirc dataset,different domains in multirc dataset,0.5228606462478638
translation,11,32,ablation-analysis,supervised strategy,selection of,justification sentences,supervised strategy selection of justification sentences,0.6341358423233032
translation,11,32,ablation-analysis,justification sentences,that relies on,dedicated bert - based classifier,justification sentences that relies on dedicated bert - based classifier,0.6881149411201477
translation,11,32,ablation-analysis,ablation analysis,indicates that,rocc,ablation analysis indicates that rocc,0.6162837743759155
translation,11,157,ablation-analysis,small drops,in,performance and justification scores,small drops in performance and justification scores,0.5249529480934143
translation,11,157,ablation-analysis,small drops,both,performance and justification scores,small drops both performance and justification scores,0.6752270460128784
translation,11,157,ablation-analysis,performance and justification scores,across,datasets,performance and justification scores across datasets,0.6919878721237183
translation,11,157,ablation-analysis,removal of either c( a ) or c ( q ),having,largest impact,removal of either c( a ) or c ( q ) having largest impact,0.6402539014816284
translation,11,157,ablation-analysis,ablation analysis,found,small drops,ablation analysis found small drops,0.6465407013893127
translation,11,20,experiments,kbs,of,unstructured texts,kbs of unstructured texts,0.5270565748214722
translation,11,4,model,unsupervised strategy,for,selection of justification sentences,unsupervised strategy for selection of justification sentences,0.5477955937385559
translation,11,4,model,unsupervised strategy,maximizes,relevance,unsupervised strategy maximizes relevance,0.7392869591712952
translation,11,4,model,unsupervised strategy,minimizes,overlap,unsupervised strategy minimizes overlap,0.6903229355812073
translation,11,4,model,unsupervised strategy,maximizes,coverage,unsupervised strategy maximizes coverage,0.718817949295044
translation,11,4,model,selection of justification sentences,for,multihop question answering ( qa ),selection of justification sentences for multihop question answering ( qa ),0.6171143054962158
translation,11,4,model,relevance,of,selected sentences,relevance of selected sentences,0.5696377158164978
translation,11,4,model,overlap,between,selected facts,overlap between selected facts,0.6973254680633545
translation,11,4,model,coverage,of,question and answer,coverage of question and answer,0.5809174180030823
translation,11,18,model,model,propose,unsupervised algorithm,model propose unsupervised algorithm,0.727708101272583
translation,11,22,model,"unsupervised , non-parametric strategy",selection of,justification sentences,"unsupervised , non-parametric strategy selection of justification sentences",0.6242156624794006
translation,11,22,model,justification sentences,for,multi-hop question answering ( qa ),justification sentences for multi-hop question answering ( qa ),0.6155884861946106
translation,11,22,model,relevance,of,selected sentences,relevance of selected sentences,0.5696377158164978
translation,11,22,model,lexical overlap,between,selected facts,lexical overlap between selected facts,0.6417519450187683
translation,11,22,model,lexical coverage,of,question and answer,lexical coverage of question and answer,0.6090392470359802
translation,11,22,model,model,propose,"unsupervised , non-parametric strategy","model propose unsupervised , non-parametric strategy",0.6804450154304504
translation,11,22,model,model,maximizes,lexical coverage,model maximizes lexical coverage,0.7151538729667664
translation,11,28,model,rocc,with,state - of- theart qa method,rocc with state - of- theart qa method,0.6492844820022583
translation,11,28,model,state - of- theart qa method,relies on,"bert ( devlin et al. , 2018 )","state - of- theart qa method relies on bert ( devlin et al. , 2018 )",0.7147359848022461
translation,11,28,model,"bert ( devlin et al. , 2018 )",to classify,correct answers,"bert ( devlin et al. , 2018 ) to classify correct answers",0.6902161836624146
translation,11,28,model,"bert ( devlin et al. , 2018 )",using,text,"bert ( devlin et al. , 2018 ) using text",0.6965335607528687
translation,11,28,model,correct answers,using,answer,correct answers using answer,0.6512918472290039
translation,11,28,model,text,of,question,text of question,0.6178677678108215
translation,11,28,model,text,of,answer,text of answer,0.6264585256576538
translation,11,28,model,text,of,justification sentences,text of justification sentences,0.604381799697876
translation,11,28,model,justification sentences,as,input,justification sentences as input,0.5216411352157593
translation,11,28,model,model,combine,rocc,model combine rocc,0.7058627009391785
translation,11,8,results,justification sentences,have,higher quality,justification sentences have higher quality,0.5419343709945679
translation,11,8,results,higher quality,than,justifications,higher quality than justifications,0.5809499025344849
translation,11,8,results,justifications,selected by,strong information retrieval baseline,justifications selected by strong information retrieval baseline,0.6541898250579834
translation,11,8,results,results,has,justification sentences,results has justification sentences,0.5226810574531555
translation,11,29,results,multi-sentence reading comprehension ( multirc ) dataset,achieved,gain,multi-sentence reading comprehension ( multirc ) dataset achieved gain,0.7160562872886658
translation,11,29,results,gain,of,8.3 % em0,gain of 8.3 % em0,0.5742921233177185
translation,11,29,results,8.3 % em0,with,rocc justifications,8.3 % em0 with rocc justifications,0.644209086894989
translation,11,29,results,complete comprehension passage,provided to,bert classifier,complete comprehension passage provided to bert classifier,0.697857141494751
translation,11,29,results,results,On,multi-sentence reading comprehension ( multirc ) dataset,results On multi-sentence reading comprehension ( multirc ) dataset,0.5135221481323242
translation,11,30,results,qa approach,enhanced with,rocc justifications,qa approach enhanced with rocc justifications,0.6611896753311157
translation,11,30,results,qa approach,without,justifications,qa approach without justifications,0.7214961647987366
translation,11,30,results,qa method,without,justifications,qa method without justifications,0.7182514071464539
translation,11,30,results,qa method,by,9.15 % accuracy,qa method by 9.15 % accuracy,0.5928398966789246
translation,11,30,results,justifications,by,9.15 % accuracy,justifications by 9.15 % accuracy,0.5841558575630188
translation,11,30,results,top sentences,provided by,bm25,top sentences provided by bm25,0.6102994680404663
translation,11,30,results,bm25,by,2.88 %,bm25 by 2.88 %,0.5916305184364319
translation,11,30,results,ai2's reason-ing challenge ( arc ) dataset,has,qa approach,ai2's reason-ing challenge ( arc ) dataset has qa approach,0.5882982015609741
translation,11,30,results,rocc justifications,has,outperforms,rocc justifications has outperforms,0.6662325263023376
translation,11,30,results,outperforms,has,qa method,outperforms has qa method,0.6306272149085999
translation,11,30,results,results,On,ai2's reason-ing challenge ( arc ) dataset,results On ai2's reason-ing challenge ( arc ) dataset,0.5316386818885803
translation,11,31,results,justification sentences,selected by,rocc,justification sentences selected by rocc,0.6836242079734802
translation,11,31,results,considerably more correct,on,their own,considerably more correct on their own,0.5759271383285522
translation,11,31,results,considerably more correct,on,justifications,considerably more correct on justifications,0.5378714799880981
translation,11,31,results,considerably more correct,than,justifications,considerably more correct than justifications,0.5980101823806763
translation,11,31,results,their own,than,justifications,their own than justifications,0.5671849846839905
translation,11,31,results,justifications,selected by,bm25,justifications selected by bm25,0.7284004092216492
translation,11,31,results,results,show that,justification sentences,results show that justification sentences,0.46773260831832886
translation,11,160,results,autorocc,achieves,higher recall scores,autorocc achieves higher recall scores,0.6478649973869324
translation,11,160,results,autorocc,achieves,worse recall,autorocc achieves worse recall,0.7015876173973083
translation,11,160,results,higher recall scores,on,verbatim questions,higher recall scores on verbatim questions,0.48000338673591614
translation,11,160,results,worse recall,on,question types,worse recall on question types,0.5094940066337585
translation,11,160,results,results,has,autorocc,results has autorocc,0.585239589214325
translation,11,166,results,alignment - based rocc,performs,better,alignment - based rocc performs better,0.6329105496406555
translation,11,166,results,better,than,rocc,better than rocc,0.5909796953201294
translation,11,166,results,rocc,relies on,lexical match,rocc relies on lexical match,0.7197641134262085
translation,12,161,ablation-analysis,question retrieval performance,can be,further improved,question retrieval performance can be further improved,0.6770845651626587
translation,12,161,ablation-analysis,more languages,has,question retrieval performance,more languages has question retrieval performance,0.5661827921867371
translation,12,161,ablation-analysis,ablation analysis,considering,more languages,ablation analysis considering more languages,0.654905378818512
translation,12,182,ablation-analysis,different languages,contribute,unevenly,different languages contribute unevenly,0.5970290899276733
translation,12,182,ablation-analysis,unevenly,for,question retrieval,unevenly for question retrieval,0.6342368125915527
translation,12,182,ablation-analysis,ablation analysis,has,different languages,ablation analysis has different languages,0.5318681001663208
translation,12,135,baselines,map,rewards,methods,map rewards methods,0.7127002477645874
translation,12,135,baselines,map,rewards,correct ranking,map rewards correct ranking,0.724636435508728
translation,12,135,baselines,methods,that return,relevant questions,methods that return relevant questions,0.6032959818840027
translation,12,135,baselines,correct ranking,of,results,correct ranking of results,0.5955752730369568
translation,12,135,baselines,relevant questions,has,early,relevant questions has early,0.5573887228965759
translation,12,135,baselines,baselines,has,map,baselines has map,0.5948289632797241
translation,12,10,model,alternative way,to address,word ambiguity and word mismatch problems,alternative way to address word ambiguity and word mismatch problems,0.6368038654327393
translation,12,10,model,word ambiguity and word mismatch problems,by taking advantage of,potentially rich semantic information,word ambiguity and word mismatch problems by taking advantage of potentially rich semantic information,0.683816134929657
translation,12,10,model,potentially rich semantic information,drawn from,other languages,potentially rich semantic information drawn from other languages,0.589370846748352
translation,12,10,model,model,propose,alternative way,model propose alternative way,0.7471957802772522
translation,12,11,model,statistical machine translation,to improve,question retrieval,statistical machine translation to improve question retrieval,0.605633556842804
translation,12,11,model,question representation,with,translated words,question representation with translated words,0.5965575575828552
translation,12,11,model,translated words,from,other languages,translated words from other languages,0.5821796655654907
translation,12,11,model,translated words,via,matrix factorization,translated words via matrix factorization,0.6656937003135681
translation,12,11,model,model,enriches,question representation,model enriches question representation,0.7223122119903564
translation,12,48,model,statistical machine translation,to improve,question retrieval,statistical machine translation to improve question retrieval,0.605633556842804
translation,12,48,model,question retrieval,via,matrix factorization,question retrieval via matrix factorization,0.6804515719413757
translation,12,48,model,model,leverage,statistical machine translation,model leverage statistical machine translation,0.6912522912025452
translation,12,56,model,word ambiguity and word mismatch problems,expand,question,word ambiguity and word mismatch problems expand question,0.6526235342025757
translation,12,56,model,question,by adding,translation counterparts,question by adding translation counterparts,0.7696937322616577
translation,12,56,model,model,to address,word ambiguity and word mismatch problems,model to address word ambiguity and word mismatch problems,0.6246510744094849
translation,12,56,model,model,expand,question,model expand question,0.6956868171691895
translation,12,170,model,proposed matrix factorization,to,original question representation ( vsm + mf ),proposed matrix factorization to original question representation ( vsm + mf ),0.5382566452026367
translation,12,170,model,model,employ,proposed matrix factorization,model employ proposed matrix factorization,0.5822410583496094
translation,12,196,model,statistical machine translation,to improve,question retrieval,statistical machine translation to improve question retrieval,0.605633556842804
translation,12,196,model,statistical machine translation,enrich,question representation,statistical machine translation enrich question representation,0.5831156373023987
translation,12,196,model,question representation,with,translated words,question representation with translated words,0.5965575575828552
translation,12,196,model,translated words,from,other languages,translated words from other languages,0.5821796655654907
translation,12,196,model,model,enrich,question representation,model enrich question representation,0.6888101100921631
translation,12,157,results,monolingual translation models,has,significantly outperform,monolingual translation models has significantly outperform,0.17986132204532623
translation,12,157,results,significantly outperform,has,vsm and lm,significantly outperform has vsm and lm,0.5874121785163879
translation,12,158,results,potentially rich semantic information,drawn from,other languages,potentially rich semantic information drawn from other languages,0.589370846748352
translation,12,158,results,other languages,via,statistical machine translation,other languages via statistical machine translation,0.5094600319862366
translation,12,158,results,question retrieval performance,can be,significantly improved,question retrieval performance can be significantly improved,0.6869156956672668
translation,12,158,results,potentially rich semantic information,has,question retrieval performance,potentially rich semantic information has question retrieval performance,0.5319386720657349
translation,12,158,results,statistical machine translation,has,question retrieval performance,statistical machine translation has question retrieval performance,0.5107283592224121
translation,12,158,results,results,Taking advantage of,potentially rich semantic information,results Taking advantage of potentially rich semantic information,0.6681642532348633
translation,12,159,results,proposed method,has,significantly outperforms,proposed method has significantly outperforms,0.6125128865242004
translation,12,159,results,significantly outperforms,has,bilingual translation model,significantly outperforms has bilingual translation model,0.5837116837501526
translation,12,159,results,results,has,proposed method,results has proposed method,0.5845219492912292
translation,12,171,results,performance,of,question retrieval,performance of question retrieval,0.595305323600769
translation,12,171,results,our proposed matrix factorization,has,significantly improve,our proposed matrix factorization has significantly improve,0.580211877822876
translation,12,171,results,significantly improve,has,performance,significantly improve has performance,0.5721634030342102
translation,12,171,results,results,has,our proposed matrix factorization,results has our proposed matrix factorization,0.5837699174880981
translation,12,172,results,data spareness and noise,introduced by,statistical machine translation,data spareness and noise introduced by statistical machine translation,0.6716580986976624
translation,12,172,results,results,indicate,our proposed matrix factorization,results indicate our proposed matrix factorization,0.5972594022750854
translation,12,173,results,relative improvements,is,much larger,relative improvements is much larger,0.5841122269630432
translation,12,173,results,relative improvements,is,much larger,relative improvements is much larger,0.5841122269630432
translation,12,173,results,relative improvements,has,relative improvements,relative improvements has relative improvements,0.5660226345062256
translation,12,173,results,results,Compared to,relative improvements,results Compared to relative improvements,0.6955768465995789
translation,12,175,results,performance,of,smt + iem,performance of smt + iem,0.6163597106933594
translation,12,175,results,vsm,has,performance,vsm has performance,0.5801769495010376
translation,12,175,results,results,Compared to,vsm,results Compared to vsm,0.6954964995384216
translation,12,176,results,results,has,impact of the translation language,results has impact of the translation language,0.5532575249671936
translation,12,181,results,potentially rich semantic information,drawn from,other languages,potentially rich semantic information drawn from other languages,0.589370846748352
translation,12,181,results,performance,of,question retrieval,performance of question retrieval,0.595305323600769
translation,12,181,results,significantly improve,has,performance,significantly improve has performance,0.5721634030342102
translation,12,181,results,results,Taking advantage of,potentially rich semantic information,results Taking advantage of potentially rich semantic information,0.6681642532348633
translation,12,185,results,much more languages,not seem to produce,significantly better performance,much more languages not seem to produce significantly better performance,0.6860988140106201
translation,12,185,results,results,Using,much more languages,results Using much more languages,0.6686938405036926
translation,12,192,results,performance,is,degraded,performance is degraded,0.65207439661026
translation,12,192,results,degraded,when,contextual information,degraded when contextual information,0.6812076568603516
translation,12,192,results,contextual information,not considered for,translation,contextual information not considered for translation,0.6714985966682434
translation,12,192,results,translation,has,of questions,translation has of questions,0.6163501739501953
translation,12,192,results,results,see that,performance,results see that performance,0.6803712844848633
translation,13,72,baselines,locality -sensitive hashing ( lsh ),sacrifices,accuracy,locality -sensitive hashing ( lsh ) sacrifices accuracy,0.6768555045127869
translation,13,72,baselines,locality -sensitive hashing ( lsh ),has,approximate nearest neighbor algorithm,locality -sensitive hashing ( lsh ) has approximate nearest neighbor algorithm,0.539941132068634
translation,13,72,baselines,baselines,consider,locality -sensitive hashing ( lsh ),baselines consider locality -sensitive hashing ( lsh ),0.6293555498123169
translation,13,77,hyperparameters,networks,optimized using,"adam ( kingma and ba , 2014 )","networks optimized using adam ( kingma and ba , 2014 )",0.7179052829742432
translation,13,77,hyperparameters,"adam ( kingma and ba , 2014 )",with,learning rate,"adam ( kingma and ba , 2014 ) with learning rate",0.5989180207252502
translation,13,77,hyperparameters,"adam ( kingma and ba , 2014 )",with,batch size,"adam ( kingma and ba , 2014 ) with batch size",0.6181668639183044
translation,13,77,hyperparameters,"adam ( kingma and ba , 2014 )",with,early stopping,"adam ( kingma and ba , 2014 ) with early stopping",0.6345234513282776
translation,13,77,hyperparameters,learning rate,of,10 ?3,learning rate of 10 ?3,0.6398048996925354
translation,13,77,hyperparameters,batch size,of,32,batch size of 32,0.6741614937782288
translation,13,77,hyperparameters,early stopping,based on,validation loss,early stopping based on validation loss,0.6186753511428833
translation,13,77,hyperparameters,hyperparameters,optimized using,"adam ( kingma and ba , 2014 )","hyperparameters optimized using adam ( kingma and ba , 2014 )",0.6821762323379517
translation,13,77,hyperparameters,hyperparameters,has,networks,hyperparameters has networks,0.5614097714424133
translation,13,78,hyperparameters,regularization,apply,40 % dropout,regularization apply 40 % dropout,0.5836415886878967
translation,13,78,hyperparameters,40 % dropout,to,final layer,40 % dropout to final layer,0.531220555305481
translation,13,78,hyperparameters,final layer,of,lstm,final layer of lstm,0.537887454032898
translation,13,78,hyperparameters,hyperparameters,For,regularization,hyperparameters For regularization,0.5561942458152771
translation,13,73,model,embedding vectors,randomly hashed into,short binary encodings,embedding vectors randomly hashed into short binary encodings,0.7439019083976746
translation,13,73,model,short binary encodings,preserve,local information,short binary encodings preserve local information,0.7723473906517029
translation,13,73,model,local information,enabling,nearest neighbor searching,local information enabling nearest neighbor searching,0.7431460022926331
translation,13,73,model,nearest neighbor searching,in,sublinear time,nearest neighbor searching in sublinear time,0.49017417430877686
translation,13,73,model,lsh,has,embedding vectors,lsh has embedding vectors,0.569099485874176
translation,13,73,model,model,With,lsh,model With lsh,0.7099728584289551
translation,13,76,model,variable - length sequence,of,word ids,variable - length sequence of word ids,0.6232951879501343
translation,13,76,model,softmax output layer,for,classification,softmax output layer for classification,0.6356364488601685
translation,13,76,model,model,takes,variable - length sequence,model takes variable - length sequence,0.6853053569793701
translation,13,9,results,maternal support,through,text messaging,maternal support through text messaging,0.6723181009292603
translation,13,9,results,text messaging,in,all 11 official languages,text messaging in all 11 official languages,0.5246660113334656
translation,13,88,results,mnb baseline,performs,quite well,mnb baseline performs quite well,0.5945204496383667
translation,13,88,results,quite well,on,full test set and the lr test set,quite well on full test set and the lr test set,0.531485378742218
translation,13,88,results,results,has,mnb baseline,results has mnb baseline,0.5461052656173706
translation,13,89,results,nearest neighbor models ( k- nn and k- lsh ),show,almost no improvement,nearest neighbor models ( k- nn and k- lsh ) show almost no improvement,0.6426746845245361
translation,13,89,results,nearest neighbor models ( k- nn and k- lsh ),show,worse,nearest neighbor models ( k- nn and k- lsh ) show worse,0.6245216131210327
translation,13,89,results,nearest neighbor models ( k- nn and k- lsh ),do,worse,nearest neighbor models ( k- nn and k- lsh ) do worse,0.5155331492424011
translation,13,89,results,almost no improvement,over,mnb,almost no improvement over mnb,0.7307649850845337
translation,13,89,results,worse,on,lr set,worse on lr set,0.5619011521339417
translation,13,89,results,results,has,nearest neighbor models ( k- nn and k- lsh ),results has nearest neighbor models ( k- nn and k- lsh ),0.5333845019340515
translation,13,90,results,efficient lsh models,perform,almost the same,efficient lsh models perform almost the same,0.5771616101264954
translation,13,90,results,almost the same,as,nn models,almost the same as nn models,0.5916463136672974
translation,13,90,results,nn models,has,approximate,nn models has approximate,0.6087659597396851
translation,13,90,results,results,has,efficient lsh models,results has efficient lsh models,0.5460271239280701
translation,13,91,results,lstm models,seem to perform,best,lstm models seem to perform best,0.695018470287323
translation,13,91,results,results,has,lstm models,results has lstm models,0.5226836204528809
translation,13,92,results,hidden units,improves,accuracy,hidden units improves accuracy,0.7328371405601501
translation,13,92,results,hidden units,improves,accuracy,hidden units improves accuracy,0.7328371405601501
translation,13,92,results,accuracy,on,full test set,accuracy on full test set,0.5087597966194153
translation,13,92,results,accuracy,on,lr set,accuracy on lr set,0.5599382519721985
translation,13,92,results,accuracy,on,lr set,accuracy on lr set,0.5599382519721985
translation,13,92,results,accuracy,on,lr set,accuracy on lr set,0.5599382519721985
translation,13,92,results,decreases,has,accuracy,decreases has accuracy,0.5777829885482788
translation,13,94,results,lstm,shows,significant improvement,lstm shows significant improvement,0.7043994069099426
translation,13,94,results,lstm,reaches,accuracy,lstm reaches accuracy,0.6985453963279724
translation,13,94,results,significant improvement,over,other models,significant improvement over other models,0.6254227161407471
translation,13,94,results,significant improvement,reaches,accuracy,significant improvement reaches accuracy,0.7563624382019043
translation,13,94,results,accuracy,of,only 62.13 %,accuracy of only 62.13 %,0.5425807237625122
translation,13,94,results,accuracy,on,full test set,accuracy on full test set,0.5087597966194153
translation,13,94,results,only 62.13 %,on,full test set,only 62.13 % on full test set,0.5540155172348022
translation,13,94,results,results,has,lstm,results has lstm,0.5593706965446472
translation,13,104,results,lstm networks,trained,end-to - end,lstm networks trained end-to - end,0.7231043577194214
translation,13,104,results,lstm networks,trained,outperformed,lstm networks trained outperformed,0.6786463856697083
translation,13,104,results,outperformed,achieving,accuracies,outperformed achieving accuracies,0.6931778788566589
translation,13,104,results,of about 62 % and 56 %,on,full and low-resource test sets,of about 62 % and 56 % on full and low-resource test sets,0.5748719573020935
translation,13,104,results,end-to - end,has,outperformed,end-to - end has outperformed,0.6141101717948914
translation,13,104,results,outperformed,has,all the other models tested,outperformed has all the other models tested,0.5937498211860657
translation,13,104,results,accuracies,has,of about 62 % and 56 %,accuracies has of about 62 % and 56 %,0.5639054775238037
translation,13,104,results,results,has,lstm networks,results has lstm networks,0.5305174589157104
translation,13,105,results,best lstm,achieved,recall@5,best lstm achieved recall@5,0.6463203430175781
translation,13,105,results,recall@5,of,almost 90 %,recall@5 of almost 90 %,0.5958805680274963
translation,13,105,results,results,has,best lstm,results has best lstm,0.5572924613952637
translation,14,126,ablation-analysis,double-checking technique,further boosts,recall,double-checking technique further boosts recall,0.7503272294998169
translation,14,126,ablation-analysis,double-checking technique,reduces,misclassification,double-checking technique reduces misclassification,0.6816744804382324
translation,14,126,ablation-analysis,misclassification,between,opposite classes,misclassification between opposite classes,0.6965927481651306
translation,14,126,ablation-analysis,ablation analysis,has,double-checking technique,ablation analysis has double-checking technique,0.5592095255851746
translation,14,158,experimental-setup,word embeddings,use,300 - dimensional word2vec vectors,word embeddings use 300 - dimensional word2vec vectors,0.5828063488006592
translation,14,158,experimental-setup,300 - dimensional word2vec vectors,trained on,google news corpus ( 3 billion running words ),300 - dimensional word2vec vectors trained on google news corpus ( 3 billion running words ),0.713172197341919
translation,14,249,experiments,all other systems,by,large margin,all other systems by large margin,0.5928446054458618
translation,14,249,experiments,news and wikipedia genres,has,our system,news and wikipedia genres has our system,0.594757080078125
translation,14,249,experiments,our system,has,outperforms,our system has outperforms,0.6423544883728027
translation,14,249,experiments,outperforms,has,all other systems,outperforms has all other systems,0.5778034329414368
translation,14,268,hyperparameters,input batch size,to counteract,class imbalance,input batch size to counteract class imbalance,0.628853976726532
translation,14,268,hyperparameters,hyperparameters,reduced,input batch size,hyperparameters reduced input batch size,0.605013370513916
translation,14,4,model,"set of simple , uniform in architecture lstmbased models",to recover,different kinds of temporal relations,"set of simple , uniform in architecture lstmbased models to recover different kinds of temporal relations",0.6904171705245972
translation,14,4,model,different kinds of temporal relations,from,text,different kinds of temporal relations from text,0.5743935108184814
translation,14,4,model,model,use,"set of simple , uniform in architecture lstmbased models","model use set of simple , uniform in architecture lstmbased models",0.6622987389564514
translation,14,5,model,shortest dependency path,between,entities,shortest dependency path between entities,0.6441076993942261
translation,14,5,model,shortest dependency path,as,input,shortest dependency path as input,0.5134541988372803
translation,14,5,model,same architecture,to extract,"intra-sentence , crosssentence , and document creation time relations","same architecture to extract intra-sentence , crosssentence , and document creation time relations",0.7159409523010254
translation,14,5,model,shortest dependency path,has,same architecture,shortest dependency path has same architecture,0.575476348400116
translation,14,5,model,entities,has,same architecture,entities has same architecture,0.5899402499198914
translation,14,5,model,model,Using,shortest dependency path,model Using shortest dependency path,0.6863184571266174
translation,14,19,model,several simple lstmbased components,to recover,ordering relations,several simple lstmbased components to recover ordering relations,0.6827576756477356
translation,14,19,model,ordering relations,between,temporally relevant entities ( events and temporal expressions ),ordering relations between temporally relevant entities ( events and temporal expressions ),0.6184378862380981
translation,14,19,model,model,use,several simple lstmbased components,model use several simple lstmbased components,0.6318493485450745
translation,14,68,model,temporal relation ( tlink ) classifier,consists of,four components,temporal relation ( tlink ) classifier consists of four components,0.6852335929870605
translation,14,68,model,lstm - based model,for,intra-sentence entity relations,lstm - based model for intra-sentence entity relations,0.5704294443130493
translation,14,68,model,lstmbased model,for,cross-sentence relations,lstmbased model for cross-sentence relations,0.5846092700958252
translation,14,68,model,lstm - based model,for,relations,lstm - based model for relations,0.6337497234344482
translation,14,68,model,relations,with,document creation time,relations with document creation time,0.5781899094581604
translation,14,68,model,rule- based component,for,timex pairs,rule- based component for timex pairs,0.6489509344100952
translation,14,68,model,four components,has,lstm - based model,four components has lstm - based model,0.5508552193641663
translation,14,68,model,four components,has,lstmbased model,four components has lstmbased model,0.5579879879951477
translation,14,68,model,model,has,temporal relation ( tlink ) classifier,model has temporal relation ( tlink ) classifier,0.618657648563385
translation,14,69,model,four models,perform,tlink classifications,four models perform tlink classifications,0.6192346811294556
translation,14,69,model,combined results,fed into,pruning module,combined results fed into pruning module,0.6677904725074768
translation,14,69,model,pruning module,to remove,conflicting tlinks,pruning module to remove conflicting tlinks,0.6481949090957642
translation,14,69,model,tlink classifications,has,independently,tlink classifications has independently,0.6212820410728455
translation,14,69,model,model,has,four models,model has four models,0.5828269720077515
translation,14,161,model,lstm layer,of,event extraction model,lstm layer of event extraction model,0.5234380960464478
translation,14,161,model,event extraction model,contains,128 lstm units,event extraction model contains 128 lstm units,0.5964152812957764
translation,14,161,model,model,has,lstm layer,model has lstm layer,0.49486640095710754
translation,14,164,model,combined hidden layer,connected with,single-neuron output layer,combined hidden layer connected with single-neuron output layer,0.7165634632110596
translation,14,164,model,model,has,combined hidden layer,model has combined hidden layer,0.5663280487060547
translation,14,12,results,results,build,temporal reasoning framework,results build temporal reasoning framework,0.6374908089637756
translation,14,205,results,precision,is,very high,precision is very high,0.5616708397865295
translation,14,205,results,very high,above,0.90,very high above 0.90,0.6452878713607788
translation,14,214,results,performance,is,similar,performance is similar,0.6087489724159241
translation,14,214,results,similar,with,slight improvement,similar with slight improvement,0.6639072299003601
translation,14,214,results,slight improvement,in,precision,slight improvement in precision,0.5280700922012329
translation,14,214,results,results,has,performance,results has performance,0.5972660779953003
translation,14,215,results,our event and timex tags,work,well,our event and timex tags work well,0.6766707301139832
translation,14,215,results,results,shows,our event and timex tags,results shows our event and timex tags,0.7171375751495361
translation,14,216,results,double -checking technique,boosts,coverage,double -checking technique boosts coverage,0.7770601511001587
translation,14,216,results,coverage,has,lot,coverage has lot,0.6141334176063538
translation,14,216,results,results,has,double -checking technique,results has double -checking technique,0.5688397884368896
translation,14,217,results,doublechecking,with,pruning technique,doublechecking with pruning technique,0.6230518817901611
translation,14,217,results,pruning technique,yields,best results,pruning technique yields best results,0.7148882150650024
translation,14,217,results,best results,with,f1 score 0.58,best results with f1 score 0.58,0.5837446451187134
translation,14,217,results,answering,has,42 out of 79 questions,answering has 42 out of 79 questions,0.5989065766334534
translation,14,217,results,42 out of 79 questions,has,correctly,42 out of 79 questions has correctly,0.5880218148231506
translation,14,217,results,results,Combining,doublechecking,results Combining doublechecking,0.6859999299049377
translation,14,221,results,flat context,instead of,dependency paths,flat context instead of dependency paths,0.6288468241691589
translation,14,221,results,flat context,yields,much weaker performance,flat context yields much weaker performance,0.692478597164154
translation,14,221,results,results,Using,flat context,results Using flat context,0.6509870290756226
translation,14,222,results,hypothesis,that,syntactic dependencies,hypothesis that syntactic dependencies,0.61394864320755
translation,14,222,results,syntactic dependencies,represent,temporal relations,syntactic dependencies represent temporal relations,0.49640092253685
translation,14,222,results,better,than,word windows,better than word windows,0.6219082474708557
translation,14,222,results,temporal relations,has,better,temporal relations has better,0.6016520261764526
translation,14,222,results,results,confirms,hypothesis,results confirms hypothesis,0.6093275547027588
translation,14,253,results,submissions,suggests,resolving event coreference,submissions suggests resolving event coreference,0.6477009654045105
translation,14,253,results,system performance,for,news and wikipedia genres,system performance for news and wikipedia genres,0.5858105421066284
translation,14,253,results,resolving event coreference,has,substantially improves,resolving event coreference has substantially improves,0.5627668499946594
translation,14,253,results,substantially improves,has,system performance,substantially improves has system performance,0.5790716409683228
translation,14,274,results,our   out - of - the-box   model,which uses,uniform configurations,our   out - of - the-box   model which uses uniform configurations,0.7117446660995483
translation,14,274,results,uniform configurations,across,different components,uniform configurations across different components,0.7586821913719177
translation,14,274,results,uniform configurations,obtains,f1 0.505,uniform configurations obtains f1 0.505,0.592031717300415
translation,14,275,results,best result,of,0.519,best result of 0.519,0.5671941041946411
translation,14,275,results,0.519,obtained,tuning hyperparameters,0.519 obtained tuning hyperparameters,0.573233962059021
translation,14,275,results,0.519,by,tuning hyperparameters,0.519 by tuning hyperparameters,0.5203953385353088
translation,14,275,results,tuning hyperparameters,on,"intrasentence , cross-sentence , and dct models","tuning hyperparameters on intrasentence , cross-sentence , and dct models",0.5022491812705994
translation,14,275,results,results,has,best result,results has best result,0.5702329277992249
translation,15,204,ablation-analysis,answer path information,is,most important,answer path information is most important,0.5524052977561951
translation,15,204,ablation-analysis,answer type information,is,more important,answer type information is more important,0.527673065662384
translation,15,204,ablation-analysis,more important,than,answer context information,more important than answer context information,0.5828959345817566
translation,15,204,ablation-analysis,ablation analysis,find that,answer path information,ablation analysis find that answer path information,0.6620384454727173
translation,15,204,ablation-analysis,ablation analysis,find that,answer type information,ablation analysis find that answer type information,0.6417649388313293
translation,15,207,ablation-analysis,question paraphrases,in,multi-task learning manner,question paraphrases in multi-task learning manner,0.5007635354995728
translation,15,207,ablation-analysis,multi-task learning manner,contributes to,performance,multi-task learning manner contributes to performance,0.701393187046051
translation,15,207,ablation-analysis,ablation analysis,using,question paraphrases,ablation analysis using question paraphrases,0.6973403692245483
translation,15,209,ablation-analysis,2 - hops paths,find that,performance,2 - hops paths find that performance,0.6620054841041565
translation,15,209,ablation-analysis,performance,has,drops significantly,performance has drops significantly,0.611855685710907
translation,15,209,ablation-analysis,ablation analysis,Compared to,2 - hops paths,ablation analysis Compared to 2 - hops paths,0.6582472920417786
translation,15,180,hyperparameters,hyperparameters,has,nonlinearity function f = tanh,hyperparameters has nonlinearity function f = tanh,0.5390774011611938
translation,15,181,hyperparameters,dimension,of,word vectors,dimension of word vectors,0.6073312163352966
translation,15,181,hyperparameters,word vectors,set to,25,word vectors set to 25,0.6627654433250427
translation,15,181,hyperparameters,hyperparameters,has,dimension,hyperparameters has dimension,0.514893651008606
translation,15,182,hyperparameters,hyperparameters,initialized by,pre-trained word embeddings,hyperparameters initialized by pre-trained word embeddings,0.6091176271438599
translation,15,183,hyperparameters,window size,of,mccnns,window size of mccnns,0.5970782041549683
translation,15,183,hyperparameters,mccnns,is,5,mccnns is 5,0.7100418210029602
translation,15,183,hyperparameters,hyperparameters,has,window size,hyperparameters has window size,0.5170397162437439
translation,15,184,hyperparameters,dimension,of,answer embeddings,dimension of answer embeddings,0.6001682877540588
translation,15,184,hyperparameters,dimension,dimension of,answer embeddings,dimension dimension of answer embeddings,0.7379104495048523
translation,15,184,hyperparameters,dimension,set to,64,dimension set to 64,0.744541585445404
translation,15,184,hyperparameters,answer embeddings,set to,64,answer embeddings set to 64,0.6453394889831543
translation,15,184,hyperparameters,hyperparameters,has,dimension,hyperparameters has dimension,0.514893651008606
translation,15,185,hyperparameters,hyperparameters,has,parameters,hyperparameters has parameters,0.4783959984779358
translation,15,186,hyperparameters,max value,used for,max-norm regularization,max value used for max-norm regularization,0.6211143136024475
translation,15,186,hyperparameters,max-norm regularization,is,3,max-norm regularization is 3,0.5581369400024414
translation,15,186,hyperparameters,hyperparameters,has,max value,hyperparameters has max value,0.5261765718460083
translation,15,187,hyperparameters,initial learning rate,used in,adagrad,initial learning rate used in adagrad,0.5928949117660522
translation,15,187,hyperparameters,adagrad,set to,0.01,adagrad set to 0.01,0.7030798196792603
translation,15,187,hyperparameters,hyperparameters,has,initial learning rate,hyperparameters has initial learning rate,0.4469831883907318
translation,15,188,hyperparameters,mini-batch,consists of,10 question - answer pairs,mini-batch consists of 10 question - answer pairs,0.6276890635490417
translation,15,188,hyperparameters,randomly sampled,from,candidate set,randomly sampled from candidate set,0.6395664811134338
translation,15,188,hyperparameters,every question - answer pair,has,k negative samples,every question - answer pair has k negative samples,0.5923765301704407
translation,15,188,hyperparameters,hyperparameters,has,mini-batch,hyperparameters has mini-batch,0.5279257297515869
translation,15,6,model,multi-column convolutional neural networks ( mccnns ),to understand,questions,multi-column convolutional neural networks ( mccnns ) to understand questions,0.6775875687599182
translation,15,6,model,multi-column convolutional neural networks ( mccnns ),learn,distributed representations,multi-column convolutional neural networks ( mccnns ) learn distributed representations,0.6600474119186401
translation,15,6,model,questions,from,three different aspects,questions from three different aspects,0.5941113829612732
translation,15,6,model,questions,learn,distributed representations,questions learn distributed representations,0.6464517116546631
translation,15,6,model,three different aspects,namely,answer path,three different aspects namely answer path,0.733393669128418
translation,15,6,model,three different aspects,namely,answer type,three different aspects namely answer type,0.6907053589820862
translation,15,6,model,model,introduce,multi-column convolutional neural networks ( mccnns ),model introduce multi-column convolutional neural networks ( mccnns ),0.6695414185523987
translation,15,7,model,low-dimensional embeddings,of,entities and relations,low-dimensional embeddings of entities and relations,0.5231682658195496
translation,15,7,model,entities and relations,in,knowledge base,entities and relations in knowledge base,0.4915085732936859
translation,15,7,model,model,jointly learn,low-dimensional embeddings,model jointly learn low-dimensional embeddings,0.745893657207489
translation,15,9,model,question paraphrases,to train,column networks,question paraphrases to train column networks,0.6867471933364868
translation,15,9,model,column networks,in,multi-task learning manner,column networks in multi-task learning manner,0.5372499823570251
translation,15,9,model,model,leverage,question paraphrases,model leverage question paraphrases,0.7315131425857544
translation,15,29,model,multi-column convolutional neural networks ( mccnns ),to automatically analyze,questions,multi-column convolutional neural networks ( mccnns ) to automatically analyze questions,0.7915980219841003
translation,15,29,model,questions,from,multiple aspects,questions from multiple aspects,0.5936295986175537
translation,15,29,model,model,introduce,multi-column convolutional neural networks ( mccnns ),model introduce multi-column convolutional neural networks ( mccnns ),0.6695414185523987
translation,15,30,model,same word embeddings,to represent,question words,same word embeddings to represent question words,0.613096296787262
translation,15,30,model,model,shares,same word embeddings,model shares same word embeddings,0.7081267237663269
translation,15,257,model,mccnns,share,same word embeddings,mccnns share same word embeddings,0.6565049886703491
translation,15,257,model,mccnns,use,multiple columns,mccnns use multiple columns,0.6780537962913513
translation,15,257,model,multiple columns,of,convolutional neural networks,multiple columns of convolutional neural networks,0.5333414077758789
translation,15,257,model,representations,of,different aspects of questions,representations of different aspects of questions,0.5950269103050232
translation,15,257,model,model,has,mccnns,model has mccnns,0.6202359795570374
translation,15,259,model,parameters,from,question - answer pairs,parameters from question - answer pairs,0.5609002709388733
translation,15,259,model,question paraphrases,to train,columns of mccnns,question paraphrases to train columns of mccnns,0.6818224787712097
translation,15,259,model,columns of mccnns,in,multi-task learning manner,columns of mccnns in multi-task learning manner,0.5225809216499329
translation,15,259,model,model,estimate,parameters,model estimate parameters,0.7375397086143494
translation,15,259,model,model,use,question paraphrases,model use question paraphrases,0.6643313765525818
translation,15,196,results,our method,achieves,better or comparable results,our method achieves better or comparable results,0.5794090032577515
translation,15,196,results,better or comparable results,than,baseline methods,better or comparable results than baseline methods,0.5793464779853821
translation,15,196,results,baseline methods,on,webquestions,baseline methods on webquestions,0.5196042656898499
translation,15,196,results,results,has,our method,results has our method,0.5589964985847473
translation,15,200,results,methods,that use,sum of word embeddings,methods that use sum of word embeddings,0.61834317445755
translation,15,200,results,sum of word embeddings,as,question representations,sum of word embeddings as question representations,0.5395393967628479
translation,15,200,results,multi-column convolutional neural networks based model,has,outperforms,multi-column convolutional neural networks based model has outperforms,0.5807602405548096
translation,15,200,results,outperforms,has,methods,outperforms has methods,0.6482085585594177
translation,15,200,results,results,has,multi-column convolutional neural networks based model,results has multi-column convolutional neural networks based model,0.5579381585121155
translation,15,206,results,results,indicate,multiple columns,results indicate multiple columns,0.5733665227890015
translation,15,206,results,results,using,multiple columns,results using multiple columns,0.6720538139343262
translation,15,206,results,multiple columns,to understand,questions,multiple columns to understand questions,0.7184799313545227
translation,15,206,results,questions,from,different aspects,questions from different aspects,0.5937972068786621
translation,15,206,results,different aspects,improves,performance,different aspects improves performance,0.7386501431465149
translation,15,206,results,results,using,multiple columns,results using multiple columns,0.6720538139343262
translation,16,5,model,neural passage selection model,leverages,metadata information,neural passage selection model leverages metadata information,0.6672418117523193
translation,16,5,model,metadata information,with,fine- grained encoding strategy,metadata information with fine- grained encoding strategy,0.581621527671814
translation,16,5,model,model,propose,neural passage selection model,model propose neural passage selection model,0.6508065462112427
translation,16,26,model,semantic web hierarchical metadata,into,statistical nlp models,semantic web hierarchical metadata into statistical nlp models,0.5127543210983276
translation,16,26,model,statistical nlp models,for,web- based qa,statistical nlp models for web- based qa,0.5771239399909973
translation,16,26,model,model,incorporate,semantic web hierarchical metadata,model incorporate semantic web hierarchical metadata,0.6209340691566467
translation,16,27,model,fine- grained encoding method,for,metadata predicates,fine- grained encoding method for metadata predicates,0.590612530708313
translation,16,27,model,fine- grained encoding method,to better leverage,semantic information,fine- grained encoding method to better leverage semantic information,0.6884306073188782
translation,16,27,model,model,introduce,fine- grained encoding method,model introduce fine- grained encoding method,0.6210797429084778
translation,16,7,results,significantly outperform,do not incorporate,metadata,significantly outperform do not incorporate metadata,0.8026646375656128
translation,16,7,results,our models,has,significantly outperform,our models has significantly outperform,0.5940237045288086
translation,16,7,results,significantly outperform,has,baseline models,significantly outperform has baseline models,0.5651603937149048
translation,16,7,results,results,show,our models,results show our models,0.6820906400680542
translation,16,29,results,more significant gains,on,subset of queries,more significant gains on subset of queries,0.5833684206008911
translation,16,29,results,candidate passages,contain,richer metadata tags,candidate passages contain richer metadata tags,0.6062909960746765
translation,16,29,results,our approaches,has,outperform,our approaches has outperform,0.6196050643920898
translation,16,29,results,outperform,has,baseline systems,outperform has baseline systems,0.5964462757110596
translation,16,29,results,baseline systems,has,substantially,baseline systems has substantially,0.6066003441810608
translation,16,29,results,results,show,our approaches,results show our approaches,0.6515257358551025
translation,17,163,experiments,best performance,on,squad,best performance on squad,0.5977854132652283
translation,17,164,experiments,pytorch reimple - mentation,of,uncased base model,pytorch reimple - mentation of uncased base model,0.6197513937950134
translation,17,191,experiments,first,is,paraphrasing,first is paraphrasing,0.6139835715293884
translation,17,154,hyperparameters,model,trained on,our dataset,model trained on our dataset,0.721196711063385
translation,17,154,hyperparameters,model,choose,model parameters,model choose model parameters,0.6628487706184387
translation,17,154,hyperparameters,our dataset,for,15 epochs,our dataset for 15 epochs,0.5713734030723572
translation,17,154,hyperparameters,model parameters,that achieve,best bleu - 1 score,model parameters that achieve best bleu - 1 score,0.6061956882476807
translation,17,154,hyperparameters,best bleu - 1 score,on,development set,best bleu - 1 score on development set,0.5111770033836365
translation,17,154,hyperparameters,hyperparameters,trained on,our dataset,hyperparameters trained on our dataset,0.6982834935188293
translation,17,154,hyperparameters,hyperparameters,has,model,hyperparameters has model,0.5282720923423767
translation,17,165,hyperparameters,batch size,set as,12,batch size set as 12,0.6728796362876892
translation,17,165,hyperparameters,model,for,2 epochs,model for 2 epochs,0.6606121063232422
translation,17,165,hyperparameters,model,with,learning rate,model with learning rate,0.6086345314979553
translation,17,165,hyperparameters,learning rate,has,3e - 5,learning rate has 3e - 5,0.5670245885848999
translation,17,165,hyperparameters,hyperparameters,fine - tune,model,hyperparameters fine - tune model,0.7390584349632263
translation,17,165,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,17,28,model,first large-scale dataset,for,qa,first large-scale dataset for qa,0.6110057234764099
translation,17,28,model,model,propose,first large-scale dataset,model propose first large-scale dataset,0.6678397059440613
translation,17,156,model,answer span,in,context,answer span in context,0.5100750923156738
translation,17,156,model,directly predict,has,answer span,directly predict has answer span,0.5563648343086243
translation,17,169,results,large performance gap,between,human performance and all baseline methods,large performance gap between human performance and all baseline methods,0.5993857979774475
translation,17,169,results,large performance gap,achieved,superhuman,large performance gap achieved superhuman,0.7155453562736511
translation,17,169,results,human performance and all baseline methods,including,bert,human performance and all baseline methods including bert,0.593529999256134
translation,17,169,results,human performance and all baseline methods,achieved,superhuman,human performance and all baseline methods achieved superhuman,0.7040449976921082
translation,17,176,results,our generative baseline,yields,better results,our generative baseline yields better results,0.638779878616333
translation,17,176,results,better results,than,bidaf,better results than bidaf,0.6756897568702698
translation,17,176,results,baselines,has,our generative baseline,baselines has our generative baseline,0.5529411435127258
translation,18,160,ablation-analysis,controlled query reformulation,of,air,controlled query reformulation of air,0.6171247959136963
translation,18,160,ablation-analysis,controlled query reformulation,leads to,5.4 % improvement,controlled query reformulation leads to 5.4 % improvement,0.6214941740036011
translation,18,160,ablation-analysis,ablation analysis,has,controlled query reformulation,ablation analysis has controlled query reformulation,0.5403524041175842
translation,18,168,ablation-analysis,knowledge aggregation,from,parallel evidence chains,knowledge aggregation from parallel evidence chains,0.5531966090202332
translation,18,168,ablation-analysis,knowledge aggregation,from,multiple justification sentences,knowledge aggregation from multiple justification sentences,0.536683201789856
translation,18,168,ablation-analysis,gains,from,parallel evidences,gains from parallel evidences,0.6507409811019897
translation,18,168,ablation-analysis,knowledge aggregation,from,parallel evidence chains,knowledge aggregation from parallel evidence chains,0.5531966090202332
translation,18,168,ablation-analysis,parallel evidence chains,lead to,another 3.7 % em0 improvement,parallel evidence chains lead to another 3.7 % em0 improvement,0.6629834771156311
translation,18,168,ablation-analysis,another 3.7 % em0 improvement,on,multirc,another 3.7 % em0 improvement on multirc,0.5835046172142029
translation,18,168,ablation-analysis,another 3.7 % em0 improvement,on,qasc,another 3.7 % em0 improvement on qasc,0.5600342154502869
translation,18,168,ablation-analysis,5.6 %,on,qasc,5.6 % on qasc,0.5678082704544067
translation,18,168,ablation-analysis,qasc,over,single air evidence chain,qasc over single air evidence chain,0.6289288401603699
translation,18,168,ablation-analysis,knowledge aggregation,has,knowledge aggregation,knowledge aggregation has knowledge aggregation,0.5812646746635437
translation,18,168,ablation-analysis,knowledge aggregation,has,gains,knowledge aggregation has gains,0.5778842568397522
translation,18,168,ablation-analysis,multiple justification sentences,has,gains,multiple justification sentences has gains,0.6347782611846924
translation,18,168,ablation-analysis,ablation analysis,has,knowledge aggregation,ablation analysis has knowledge aggregation,0.5465616583824158
translation,18,178,ablation-analysis,roberta retriever,drops from,62.3 %,roberta retriever drops from 62.3 %,0.7001136541366577
translation,18,178,ablation-analysis,drops,to,55.4 %,drops to 55.4 %,0.6232740879058838
translation,18,178,ablation-analysis,62.3 %,has,to 57.6 %,62.3 % has to 57.6 %,0.5350766181945801
translation,18,178,ablation-analysis,air,has,drops,air has drops,0.6413686871528625
translation,18,178,ablation-analysis,ablation analysis,observe,similar performance drops,ablation analysis observe similar performance drops,0.645063579082489
translation,18,140,baselines,alignment method,to retrieve,"top k sentences ( k = 2 , 5 )","alignment method to retrieve top k sentences ( k = 2 , 5 )",0.7248890399932861
translation,18,145,baselines,two baselines,for,qasc,two baselines for qasc,0.673876166343689
translation,18,145,baselines,baselines,considered,two baselines,baselines considered two baselines,0.7045560479164124
translation,18,5,model,unsupervised alignment approach,to soft-align,questions and answers,unsupervised alignment approach to soft-align questions and answers,0.7280110120773315
translation,18,5,model,questions and answers,with,justification sentences,questions and answers with justification sentences,0.5946099758148193
translation,18,5,model,iterative process,reformulates,queries,iterative process reformulates queries,0.805389940738678
translation,18,5,model,queries,focusing on,terms,queries focusing on terms,0.6680441498756409
translation,18,5,model,terms,not covered by,existing justifications,terms not covered by existing justifications,0.7150551080703735
translation,18,5,model,terms,not covered by,retrieved justifications,terms not covered by retrieved justifications,0.7273620367050171
translation,18,5,model,stopping criterion,terminates,retrieval,stopping criterion terminates retrieval,0.7483453154563904
translation,18,5,model,retrieval,when,candidate answers,retrieval when candidate answers,0.6316765546798706
translation,18,5,model,retrieval,terms in,given question,retrieval terms in given question,0.7014922499656677
translation,18,5,model,retrieval,terms in,candidate answers,retrieval terms in candidate answers,0.7243379354476929
translation,18,5,model,candidate answers,covered by,retrieved justifications,candidate answers covered by retrieved justifications,0.7464525103569031
translation,18,5,model,model,introduce,"simple , fast , and unsupervised iterative evidence retrieval method","model introduce simple , fast , and unsupervised iterative evidence retrieval method",0.629126250743866
translation,18,14,model,simple alignmentbased iterative retriever ( air ),retrieves,high-quality evidence sentences,simple alignmentbased iterative retriever ( air ) retrieves high-quality evidence sentences,0.7189227342605591
translation,18,14,model,high-quality evidence sentences,from,unstructured knowledge bases,high-quality evidence sentences from unstructured knowledge bases,0.5402809381484985
translation,18,14,model,model,introduce,simple alignmentbased iterative retriever ( air ),model introduce simple alignmentbased iterative retriever ( air ),0.6331411004066467
translation,18,15,model,evidence sentences,are,useful,evidence sentences are useful,0.60555100440979
translation,18,15,model,evidence sentences,are,considerably improve,evidence sentences are considerably improve,0.625895082950592
translation,18,15,model,useful,to explain,required reasoning steps,useful to explain required reasoning steps,0.7354584336280823
translation,18,15,model,performance,of,qa system,performance of qa system,0.6472135186195374
translation,18,15,model,answer,has,question,answer has question,0.6225500106811523
translation,18,15,model,considerably improve,has,performance,considerably improve has performance,0.5845153331756592
translation,18,15,model,model,demonstrate,evidence sentences,model demonstrate evidence sentences,0.6866274476051331
translation,18,22,model,model,develop,"simple , fast , and unsupervised iterative evidence retrieval method","model develop simple , fast , and unsupervised iterative evidence retrieval method",0.600173830986023
translation,18,6,results,all the previous methods,on,evidence selection task,all the previous methods on evidence selection task,0.5160329937934875
translation,18,6,results,evidence selection task,on,two datasets,evidence selection task on two datasets,0.47671109437942505
translation,18,6,results,outperforms,has,all the previous methods,outperforms has all the previous methods,0.5859152674674988
translation,18,26,results,multiple parallel evidences,improves,qa performance,multiple parallel evidences improves qa performance,0.7204371094703674
translation,18,26,results,qa performance,over,vanilla air,qa performance over vanilla air,0.7107517123222351
translation,18,26,results,qa performance,by,5.2 % accuracy,qa performance by 5.2 % accuracy,0.5757575035095215
translation,18,26,results,3.7 % em0,on,multirc,3.7 % em0 on multirc,0.5945066809654236
translation,18,26,results,5.2 % accuracy,on,qasc datasets,5.2 % accuracy on qasc datasets,0.5216960310935974
translation,18,26,results,results,aggregating,multiple parallel evidences,results aggregating multiple parallel evidences,0.6408888101577759
translation,18,27,results,5 parallel evidences,from,air,5 parallel evidences from air,0.6438689827919006
translation,18,27,results,5 parallel evidences,obtain,36.3 % em0,5 parallel evidences obtain 36.3 % em0,0.6158723831176758
translation,18,27,results,5 parallel evidences,obtain,81.0 % accuracy,5 parallel evidences obtain 81.0 % accuracy,0.5698972344398499
translation,18,27,results,36.3 % em0,on,multirc,36.3 % em0 on multirc,0.5877028107643127
translation,18,27,results,81.0 % accuracy,on,qasc hidden test sets,81.0 % accuracy on qasc hidden test sets,0.5380259156227112
translation,18,27,results,results,with,5 parallel evidences,results with 5 parallel evidences,0.6266350150108337
translation,18,151,results,previous works,in,multirc,previous works in multirc,0.5466891527175903
translation,18,151,results,air vs. unsupervised methods,has,outperforms,air vs. unsupervised methods has outperforms,0.6092416644096375
translation,18,151,results,outperforms,has,all the unsupervised baselines,outperforms has all the unsupervised baselines,0.5910838842391968
translation,18,153,results,air,achieves,5.4 % better f1 score,air achieves 5.4 % better f1 score,0.6771717071533203
translation,18,153,results,5.4 % better f1 score,compared to,best parametric alignment baseline,5.4 % better f1 score compared to best parametric alignment baseline,0.6015512943267822
translation,18,153,results,results,has,air,results has air,0.4768862724304199
translation,18,155,results,outperforms,has,supervised robertaretriver,outperforms has supervised robertaretriver,0.5854238271713257
translation,18,155,results,results,has,air vs. supervised methods,results has air vs. supervised methods,0.5320334434509277
translation,18,158,results,air,achieves,better performance,air achieves better performance,0.695906400680542
translation,18,158,results,better performance,than,supervised roberta- iterativeretriever,better performance than supervised roberta- iterativeretriever,0.567937433719635
translation,18,158,results,better performance,concatenates,retrieved justification,better performance concatenates retrieved justification,0.7263287901878357
translation,18,158,results,supervised roberta- iterativeretriever,concatenates,retrieved justification,supervised roberta- iterativeretriever concatenates retrieved justification,0.734137237071991
translation,18,158,results,retrieved justification,to,query,retrieved justification to query,0.5699995160102844
translation,18,158,results,results,has,air,results has air,0.4768862724304199
translation,18,159,results,roberta- iterative -retriever,achieves,similar performance,roberta- iterative -retriever achieves similar performance,0.7189759612083435
translation,18,159,results,similar performance,as,simple roberta - retriever,similar performance as simple roberta - retriever,0.574036717414856
translation,18,159,results,results,has,roberta- iterative -retriever,results has roberta- iterative -retriever,0.565584659576416
translation,18,161,results,state - of- theart results,for,evidence retrieval,state - of- theart results for evidence retrieval,0.5826793313026428
translation,18,161,results,evidence retrieval,on,multirc,evidence retrieval on multirc,0.6060298085212708
translation,18,162,results,soft-matching,of,air,soft-matching of air,0.613801896572113
translation,18,162,results,soft-matching,of,air,soft-matching of air,0.613801896572113
translation,18,162,results,air,that relies on,lexical matching,air that relies on lexical matching,0.7181840538978577
translation,18,162,results,alignment - based air,is,10.7 % f1,alignment - based air is 10.7 % f1,0.5613991618156433
translation,18,162,results,10.7 % f1,better than,air,10.7 % f1 better than air,0.7462176084518433
translation,18,162,results,air,that relies on,lexical matching,air that relies on lexical matching,0.7181840538978577
translation,18,162,results,lexical matching,rather than,soft matching,lexical matching rather than soft matching,0.6145463585853577
translation,18,162,results,soft-matching,has,alignment - based air,soft-matching has alignment - based air,0.5638876557350159
translation,18,162,results,air,has,alignment - based air,air has alignment - based air,0.6024254560470581
translation,18,162,results,results,has,soft-matching,results has soft-matching,0.5526694655418396
translation,18,165,results,roberta,fine- tuned using,air retrieved evidence chains,roberta fine- tuned using air retrieved evidence chains,0.7600744366645813
translation,18,165,results,state - of - the - art performance,has,roberta,state - of - the - art performance has roberta,0.5432430505752563
translation,18,165,results,state - of - the - art performance,has,outperforms,state - of - the - art performance has outperforms,0.5683003067970276
translation,18,165,results,air retrieved evidence chains,has,outperforms,air retrieved evidence chains has outperforms,0.6530438661575317
translation,18,165,results,outperforms,has,all the previous approaches,outperforms has all the previous approaches,0.6048992276191711
translation,18,167,results,roberta,fine - tuned on,5 parallel evidences,roberta fine - tuned on 5 parallel evidences,0.7191231846809387
translation,18,167,results,roberta,achieves,new state - of - the - art qa results,roberta achieves new state - of - the - art qa results,0.6450076103210449
translation,18,167,results,5 parallel evidences,from,air,5 parallel evidences from air,0.6438689827919006
translation,18,167,results,outperforming,by,10.2 % em0,outperforming by 10.2 % em0,0.6512719392776489
translation,18,167,results,previous state - of - the - art methods,by,7.8 % accuracy,previous state - of - the - art methods by 7.8 % accuracy,0.5169626474380493
translation,18,167,results,previous state - of - the - art methods,by,10.2 % em0,previous state - of - the - art methods by 10.2 % em0,0.5627428293228149
translation,18,167,results,7.8 % accuracy,on,qasc,7.8 % accuracy on qasc,0.5726735591888428
translation,18,167,results,10.2 % em0,on,multirc,10.2 % em0 on multirc,0.6084364056587219
translation,18,167,results,test set,has,roberta,test set has roberta,0.6145909428596497
translation,18,167,results,outperforming,has,previous state - of - the - art methods,outperforming has previous state - of - the - art methods,0.5320193767547607
translation,18,167,results,results,has,test set,results has test set,0.5966588854789734
translation,19,99,ablation-analysis,knowledge enhancement,essential to,performance,knowledge enhancement essential to performance,0.6484956741333008
translation,19,99,ablation-analysis,ablation analysis,see that,query reformulation,ablation analysis see that query reformulation,0.5967509746551514
translation,19,100,ablation-analysis,conditional gating mechanism,is,important,conditional gating mechanism is important,0.5964117646217346
translation,19,100,ablation-analysis,ablation analysis,find,conditional gating mechanism,ablation analysis find conditional gating mechanism,0.5758762359619141
translation,19,101,ablation-analysis,even lower,than,reader,even lower than reader,0.6563103199005127
translation,19,101,ablation-analysis,reader,without,knowledge enhancement,reader without knowledge enhancement,0.7246394753456116
translation,19,76,baselines,kb triples and documents,as,memory cells,kb triples and documents as memory cells,0.5179078578948975
translation,19,78,baselines,former,is,kb - only model,former is kb - only model,0.5696938633918762
translation,19,78,baselines,latter,uses,both kb and text,latter uses both kb and text,0.6611655354499817
translation,19,78,baselines,baselines,has,former,baselines has former,0.6151922941207886
translation,19,79,baselines,latest method graftnet ( gn ),treats,documents,latest method graftnet ( gn ) treats documents,0.6639568209648132
translation,19,79,baselines,latest method graftnet ( gn ),utilizes,"graph convolution ( kipf and welling , 2016 )","latest method graftnet ( gn ) utilizes graph convolution ( kipf and welling , 2016 )",0.519490659236908
translation,19,79,baselines,documents,as,special genre,documents as special genre,0.5309491157531738
translation,19,79,baselines,special genre,of,nodes,special genre of nodes,0.6276240944862366
translation,19,79,baselines,nodes,in,kbs,nodes in kbs,0.5720036029815674
translation,19,79,baselines,"graph convolution ( kipf and welling , 2016 )",to aggregate,information,"graph convolution ( kipf and welling , 2016 ) to aggregate information",0.7549591064453125
translation,19,79,baselines,baselines,compare to,latest method graftnet ( gn ),baselines compare to latest method graftnet ( gn ),0.6405045390129089
translation,19,81,baselines,gn - lf ( late fusion ),consider,kb and text,gn - lf ( late fusion ) consider kb and text,0.6617425084114075
translation,19,81,baselines,gn - ef ( early fusion ),consider,kb and text,gn - ef ( early fusion ) consider kb and text,0.6454588770866394
translation,19,4,model,end-to- end question answering,learns to,aggregate,end-to- end question answering learns to aggregate,0.6865628957748413
translation,19,4,model,answer evidence,from,incomplete knowledge base ( kb ),answer evidence from incomplete knowledge base ( kb ),0.5498253703117371
translation,19,4,model,aggregate,has,answer evidence,aggregate has answer evidence,0.5803069472312927
translation,19,4,model,model,propose,end-to- end question answering,model propose end-to- end question answering,0.6689487099647522
translation,19,5,model,knowledge,of,entities,knowledge of entities,0.544796347618103
translation,19,5,model,entities,from,question - related kb subgraph,entities from question - related kb subgraph,0.502625584602356
translation,19,5,model,question,in,latent space,question in latent space,0.5407050251960754
translation,19,5,model,texts,with,accumulated entity knowledge at hand,texts with accumulated entity knowledge at hand,0.5987880229949951
translation,19,5,model,model,first accumulates,knowledge,model first accumulates knowledge,0.6893373131752014
translation,19,5,model,model,reformulates,question,model reformulates question,0.7769588232040405
translation,19,21,model,qa,over,incomplete kbs,qa over incomplete kbs,0.7026804685592651
translation,19,21,model,simple yet effective subgraph reader,accumulates,knowledge,simple yet effective subgraph reader accumulates knowledge,0.6816661953926086
translation,19,21,model,knowledge,of,each kb entity,knowledge of each kb entity,0.5336344242095947
translation,19,21,model,knowledge,of,entities,knowledge of entities,0.544796347618103
translation,19,21,model,each kb entity,from,question - related kb subgraph,each kb entity from question - related kb subgraph,0.5082051753997803
translation,19,21,model,knowledge - aware text reader,selectively incorporates,learned kb knowledge,knowledge - aware text reader selectively incorporates learned kb knowledge,0.6858254671096802
translation,19,21,model,learned kb knowledge,about,entities,learned kb knowledge about entities,0.5946993827819824
translation,19,21,model,learned kb knowledge,with,novel conditional gating mechanism,learned kb knowledge with novel conditional gating mechanism,0.6067840456962585
translation,19,21,model,entities,with,novel conditional gating mechanism,entities with novel conditional gating mechanism,0.6578366160392761
translation,19,22,model,our model,ability to dynamically determine,how much kb knowledge,our model ability to dynamically determine how much kb knowledge,0.7201634049415588
translation,19,22,model,how much kb knowledge,while encoding,questions and passages,how much kb knowledge while encoding questions and passages,0.7308645844459534
translation,19,22,model,structured knowledge,more compatible with,text information,structured knowledge more compatible with text information,0.6430029273033142
translation,19,22,model,specifically designed gate functions,has,our model,specifically designed gate functions has our model,0.5974258780479431
translation,19,22,model,model,With,specifically designed gate functions,model With specifically designed gate functions,0.6661141514778137
translation,19,63,model,new conditional gating function,explicitly conditions on,question q,new conditional gating function explicitly conditions on question q,0.742051362991333
translation,19,63,model,model,propose,new conditional gating function,model propose new conditional gating function,0.69062340259552
translation,19,23,results,our model,achieves,consistent improvements,our model achieves consistent improvements,0.6810607314109802
translation,19,23,results,consistent improvements,with,much more efficient pipeline,consistent improvements with much more efficient pipeline,0.6233097910881042
translation,19,23,results,previous state - of- the - art,has,our model,previous state - of- the - art has our model,0.5308981537818909
translation,19,90,results,sgreader,achieves,better results,sgreader achieves better results,0.6391437649726868
translation,19,90,results,sgreader,achieves,competitive performance,sgreader achieves competitive performance,0.705036997795105
translation,19,90,results,better results,in,incomplete kb settings,better results in incomplete kb settings,0.5401383638381958
translation,19,90,results,competitive performance,with,full kb,competitive performance with full kb,0.6875751614570618
translation,19,90,results,previous kbqa methods ( kv - kb and gn - kb ),has,sgreader,previous kbqa methods ( kv - kb and gn - kb ) has sgreader,0.558891773223877
translation,19,90,results,results,Compared to,previous kbqa methods ( kv - kb and gn - kb ),results Compared to previous kbqa methods ( kv - kb and gn - kb ),0.595838725566864
translation,19,93,results,sgreader,with,our knowledge - aware reader ( kareader ),sgreader with our knowledge - aware reader ( kareader ),0.639066755771637
translation,19,93,results,sgreader,results in,consistent improvements,sgreader results in consistent improvements,0.6391670107841492
translation,19,93,results,consistent improvements,in,settings,consistent improvements in settings,0.5807867646217346
translation,19,93,results,settings,with,incomplete kbs,settings with incomplete kbs,0.6252137422561646
translation,19,93,results,results,Augmenting,sgreader,results Augmenting sgreader,0.5956923961639404
translation,19,94,results,our model,built upon,stronger kb - qa base model,our model built upon stronger kb - qa base model,0.6415809392929077
translation,19,94,results,our model,achieves,largest absolute improvement,our model achieves largest absolute improvement,0.6225793957710266
translation,19,94,results,results,Compared to,other baselines,results Compared to other baselines,0.688832700252533
translation,20,211,ablation-analysis,mapping words,beyond,surface - form matching,mapping words beyond surface - form matching,0.6683675050735474
translation,20,211,ablation-analysis,mapping words,is,important,mapping words is important,0.6075989007949829
translation,20,211,ablation-analysis,surface - form matching,with the help of,wordnet,surface - form matching with the help of wordnet,0.6141496300697327
translation,20,183,baselines,random scoring,assigns,random score,random scoring assigns random score,0.7235250473022461
translation,20,183,baselines,random score,to,each candidate sentence,random score to each candidate sentence,0.5527796745300293
translation,20,205,baselines,"unstructured , bag-of-words setting",tested,logistic regression ( lr ),"unstructured , bag-of-words setting tested logistic regression ( lr )",0.7250834703445435
translation,20,205,baselines,"unstructured , bag-of-words setting",tested,boosted decision trees ( bdt ),"unstructured , bag-of-words setting tested boosted decision trees ( bdt )",0.6973740458488464
translation,20,240,baselines,boosted decision trees ( bdt ),are,two unstructured models,boosted decision trees ( bdt ) are two unstructured models,0.5622973442077637
translation,20,240,baselines,baselines,has,logistic regression ( lr ),baselines has logistic regression ( lr ),0.4963114559650421
translation,20,26,experiments,answer selection,as,semantic matching problem,answer selection as semantic matching problem,0.4774037301540375
translation,20,26,experiments,semantic matching problem,with,latent word- alignment structure,semantic matching problem with latent word- alignment structure,0.5510756373405457
translation,20,225,experiments,solving,has,answer selection problem,solving has answer selection problem,0.5560889840126038
translation,20,29,results,latent alignment model,improves,result,latent alignment model improves result,0.6642212867736816
translation,20,29,results,result,on,benchmark dataset,result on benchmark dataset,0.49740320444107056
translation,20,29,results,mean reciprocal rank ( mrr ) scores,increased by,25.6 % and 18.8 %,mean reciprocal rank ( mrr ) scores increased by 25.6 % and 18.8 %,0.6827943325042725
translation,20,29,results,wide margin,has,mean average precision ( map ),wide margin has mean average precision ( map ),0.5608329176902771
translation,20,30,results,latent alignment model,performs,better,latent alignment model performs better,0.5971560478210449
translation,20,30,results,better,than,unstructured models,better than unstructured models,0.5908698439598083
translation,20,30,results,diminishes,after adding,enhanced lexical semantics information,diminishes after adding enhanced lexical semantics information,0.690606415271759
translation,20,30,results,results,has,latent alignment model,results has latent alignment model,0.5204343199729919
translation,20,187,results,word count,is,fairly strong,word count is fairly strong,0.5637360215187073
translation,20,187,results,word count,performs,comparably,word count performs comparably,0.6622011065483093
translation,20,187,results,comparably,to,previous systems,comparably to previous systems,0.600773811340332
translation,20,187,results,results,find that,word count,results find that word count,0.6079335808753967
translation,20,189,results,question words,with,idf values,question words with idf values,0.6364278197288513
translation,20,189,results,question words,improves,results,question words improves results,0.6927165985107422
translation,20,189,results,idf values,improves,results,idf values improves results,0.687313973903656
translation,20,189,results,results,weighting,question words,results weighting question words,0.6678920388221741
translation,20,213,results,more information,on,word relations,more information on word relations,0.5055863857269287
translation,20,213,results,more information,gain,approximately 10 points,more information gain approximately 10 points,0.7278032898902893
translation,20,213,results,approximately 10 points,in,map and mrr,approximately 10 points in map and mrr,0.5937894582748413
translation,20,213,results,map and mrr,compared to,surface - form matching,map and mrr compared to surface - form matching,0.6418247222900391
translation,20,213,results,results,incorporating,more information,results incorporating more information,0.6290556788444519
translation,20,226,results,rich lexical semantic information,improves,models,rich lexical semantic information improves models,0.6605789065361023
translation,20,226,results,models,in,unstructured bag-of-words setting,models in unstructured bag-of-words setting,0.49633026123046875
translation,20,226,results,models,in,framework,models in framework,0.5778652429580688
translation,20,227,results,latent structured model,performs,better,latent structured model performs better,0.6373676061630249
translation,20,227,results,better,than,other two unstructured models,better than other two unstructured models,0.5896785855293274
translation,20,227,results,diminishes,after,more information,diminishes after more information,0.6453084349632263
translation,20,227,results,more information,including,enhanced lexical semantic knowledge,more information including enhanced lexical semantic knowledge,0.6895483732223511
translation,20,227,results,more information,including,answer type verification,more information including answer type verification,0.7154200077056885
translation,20,227,results,latent structured model,has,lclr,latent structured model has lclr,0.545448899269104
translation,20,227,results,difference,has,diminishes,difference has diminishes,0.6162728667259216
translation,21,50,baselines,f bow,has,bag-of-words,f bow has bag-of-words,0.5513421893119812
translation,21,50,baselines,feature,has,"( word=x , 1 )","feature has ( word=x , 1 )",0.5893493294715881
translation,21,50,baselines,baselines,has,f bow,baselines has f bow,0.6084243059158325
translation,21,86,baselines,qa retrieval,based on,wikiqa,qa retrieval based on wikiqa,0.6594614386558533
translation,21,105,hyperparameters,"liblinear ( fan et al. , 2008 )",with,heavy l 1 - regularization ( feature selection ),"liblinear ( fan et al. , 2008 ) with heavy l 1 - regularization ( feature selection )",0.5225960612297058
translation,21,105,hyperparameters,heavy l 1 - regularization ( feature selection ),to,maximum likelihood objective,heavy l 1 - regularization ( feature selection ) to maximum likelihood objective,0.5112934708595276
translation,21,105,hyperparameters,hyperparameters,trained using,"liblinear ( fan et al. , 2008 )","hyperparameters trained using liblinear ( fan et al. , 2008 )",0.6839947700500488
translation,21,12,model,other related tasks,in,sublinear time,other related tasks in sublinear time,0.4674932360649109
translation,21,13,model,log-linear model,trained to optimize,objective function,log-linear model trained to optimize objective function,0.7863392233848572
translation,21,95,results,success rate,at,different ks,success rate at different ks,0.6234703063964844
translation,21,95,results,success rate,at,at most ks,success rate at at most ks,0.6322019696235657
translation,21,95,results,different ks,uniformly higher than,lucene,different ks uniformly higher than lucene,0.7762690186500549
translation,21,95,results,at most ks,higher than,model of yao et al .'s.,at most ks higher than model of yao et al .'s.,0.6744884848594666
translation,21,95,results,results,has,success rate,results has success rate,0.49674344062805176
translation,22,15,model,questions,directly from,semantic analysis,questions directly from semantic analysis,0.5073546171188354
translation,22,15,model,questions,without,templates,questions without templates,0.7697398066520691
translation,22,220,results,cq question,for,each sentence,cq question for each sentence,0.6513791084289551
translation,22,220,results,srlbased questions,on average much better than,nn - generated questions,srlbased questions on average much better than nn - generated questions,0.6131604909896851
translation,22,220,results,cq question,has,srlbased questions,cq question has srlbased questions,0.624718189239502
translation,22,220,results,results,pick,cq question,results pick cq question,0.6340113878250122
translation,22,263,results,average quality,of,yes / no questions,average quality of yes / no questions,0.5320292115211487
translation,22,263,results,average quality,of,generated constituent questions,average quality of generated constituent questions,0.5925963521003723
translation,22,263,results,average quality,of,generated constituent questions,average quality of generated constituent questions,0.5925963521003723
translation,22,263,results,yes / no questions,generated by,srl system,yes / no questions generated by srl system,0.6336395144462585
translation,22,263,results,yes / no questions,is,significantly higher,yes / no questions is significantly higher,0.513828456401825
translation,22,263,results,significantly higher,than,average quality,significantly higher than average quality,0.5960114002227783
translation,22,263,results,average quality,of,generated constituent questions,average quality of generated constituent questions,0.5925963521003723
translation,22,263,results,results,has,average quality,results has average quality,0.5712604522705078
translation,23,12,model,annotation scheme,based on,corpora of natural conversations,annotation scheme based on corpora of natural conversations,0.5953702926635742
translation,23,12,model,annotation scheme,provides,several layers of annotations,annotation scheme provides several layers of annotations,0.6181668639183044
translation,23,12,model,corpora of natural conversations,in,"several languages ( english , spanish , and dutch )","corpora of natural conversations in several languages ( english , spanish , and dutch )",0.5026472210884094
translation,23,12,model,several layers of annotations,for,qaps,several layers of annotations for qaps,0.6565872430801392
translation,23,12,model,model,propose,annotation scheme,model propose annotation scheme,0.6600000858306885
translation,24,6,baselines,olelo,has,question answering,olelo has question answering,0.6434105038642883
translation,24,25,baselines,qa system,for,biomedical domain,qa system for biomedical domain,0.6167020797729492
translation,24,25,baselines,olelo,has,qa system,olelo has qa system,0.6352707743644714
translation,24,7,experiments,olelo,built on top of,inmemory database,olelo built on top of inmemory database,0.7153562903404236
translation,24,7,experiments,olelo,integrates,domain resources,olelo integrates domain resources,0.6683404445648193
translation,24,7,experiments,olelo,uses,various natural language processing components,olelo uses various natural language processing components,0.631540060043335
translation,24,7,experiments,domain resources,such as,document collections and terminologies,domain resources such as document collections and terminologies,0.5472205281257629
translation,24,26,experiments,fast in-memory database ( imdb ),for,storage and document indexing,fast in-memory database ( imdb ) for storage and document indexing,0.6029866933822632
translation,24,107,experiments,olelo qa system,for,biomedical domain,olelo qa system for biomedical domain,0.6028587222099304
translation,24,8,results,olelo,is,"fast , intuitive","olelo is fast , intuitive",0.6042120456695557
translation,24,8,results,olelo,is,easy to use,olelo is easy to use,0.5335277318954468
translation,24,8,results,results,has,olelo,results has olelo,0.5580596923828125
translation,25,103,ablation-analysis,performance,in,ba and re,performance in ba and re,0.6040560603141785
translation,25,103,ablation-analysis,adversarial learning,has,degrades,adversarial learning has degrades,0.5867403745651245
translation,25,103,ablation-analysis,degrades,has,performance,degrades has performance,0.5834147930145264
translation,25,103,ablation-analysis,ablation analysis,has,adversarial learning,ablation analysis has adversarial learning,0.5492694973945618
translation,25,27,baselines,bert,randomly masks,some input tokens,bert randomly masks some input tokens,0.8298772573471069
translation,25,27,baselines,bert,predicts,masked tokens,bert predicts masked tokens,0.7695616483688354
translation,25,27,baselines,masked tokens,based on,context,masked tokens based on context,0.6406082510948181
translation,25,89,experimental-setup,bert- base-uncased,with,learning rate 3e - 5,bert- base-uncased with learning rate 3e - 5,0.6752223968505859
translation,25,89,experimental-setup,bert- base-uncased,with,batch size,bert- base-uncased with batch size,0.6949536204338074
translation,25,89,experimental-setup,batch size,of,64,batch size of 64,0.6741159558296204
translation,25,89,experimental-setup,experimental setup,use,bert- base-uncased,experimental setup use bert- base-uncased,0.5983272194862366
translation,25,92,experimental-setup,baseline and adversarial model,trained on,v100 gpu,baseline and adversarial model trained on v100 gpu,0.7332445979118347
translation,25,92,experimental-setup,v100 gpu,for,about 5 gpu hours,v100 gpu for about 5 gpu hours,0.6021101474761963
translation,25,92,experimental-setup,experimental setup,has,baseline and adversarial model,experimental setup has baseline and adversarial model,0.5424432158470154
translation,25,5,model,adversarial training framework,for,domain generalization,adversarial training framework for domain generalization,0.5243690609931946
translation,25,5,model,domain generalization,in,question answering ( qa ) task,domain generalization in question answering ( qa ) task,0.49731558561325073
translation,25,5,model,model,utilize,adversarial training framework,model utilize adversarial training framework,0.5508285760879517
translation,25,6,model,model,consists of,conventional qa model,model consists of conventional qa model,0.6528104543685913
translation,25,6,model,model,consists of,discriminator,model consists of discriminator,0.6710554957389832
translation,25,7,model,training,performed in,adversarial manner,training performed in adversarial manner,0.6861066818237305
translation,25,7,model,adversarial manner,where,two models,adversarial manner where two models,0.6030880212783813
translation,25,7,model,two models,has,constantly compete,two models has constantly compete,0.5619211792945862
translation,25,7,model,model,has,training,model has training,0.5495393872261047
translation,25,62,model,simple yet effective method,to regularize,model,simple yet effective method to regularize model,0.762495219707489
translation,25,62,model,model,learns,domaininvariant features,model learns domaininvariant features,0.7315519452095032
translation,25,62,model,model,propose,simple yet effective method,model propose simple yet effective method,0.6890079379081726
translation,25,63,model,qa model,learns to make,discriminator,qa model learns to make discriminator,0.6872514486312866
translation,25,63,model,discriminator,uncertain about,prediction,discriminator uncertain about prediction,0.7072159051895142
translation,25,63,model,adversarial training procedure,has,qa model,adversarial training procedure has qa model,0.5711938142776489
translation,25,63,model,model,In,adversarial training procedure,model In adversarial training procedure,0.5150327086448669
translation,25,100,results,model,with,adversarial learning,model with adversarial learning,0.5962338447570801
translation,25,100,results,better performance,compared to,baseline,better performance compared to baseline,0.663974404335022
translation,25,100,results,adversarial learning,has,better performance,adversarial learning has better performance,0.564672589302063
translation,25,101,results,average f1 score,of,our model,average f1 score of our model,0.5499445199966431
translation,25,101,results,our model,is about,1.5 point higher,our model is about 1.5 point higher,0.6749621033668518
translation,25,101,results,1.5 point higher,than,baseline,1.5 point higher than baseline,0.5482102632522583
translation,25,101,results,validation datasets,has,average f1 score,validation datasets has average f1 score,0.5394690036773682
translation,25,101,results,results,For,validation datasets,results For validation datasets,0.6047946214675903
translation,25,102,results,outperforms,in,"dp , dr , rc , and ra dataset","outperforms in dp , dr , rc , and ra dataset",0.5311964154243469
translation,25,102,results,outperforms,by,large margin,outperforms by large margin,0.6290680766105652
translation,25,102,results,baseline,in,"dp , dr , rc , and ra dataset","baseline in dp , dr , rc , and ra dataset",0.500522792339325
translation,25,102,results,baseline,by,large margin,baseline by large margin,0.6153011918067932
translation,25,102,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,25,102,results,outperforms,has,baseline,outperforms has baseline,0.6131853461265564
translation,25,102,results,results,has,our model,results has our model,0.5871725678443909
translation,25,105,results,our model,shows,better performance,our model shows better performance,0.6902104020118713
translation,25,105,results,better performance,in terms of,em ( exact match ) and f1,better performance in terms of em ( exact match ) and f1,0.7083852887153625
translation,25,105,results,most of test datasets,except for,st,most of test datasets except for st,0.7322146892547607
translation,25,106,results,superior performance,with,considerable margin,superior performance with considerable margin,0.6648799777030945
translation,25,106,results,considerable margin,of,over 2 point,considerable margin of over 2 point,0.6069735288619995
translation,25,106,results,over 2 point,in,f1,over 2 point in f1,0.5680800080299377
translation,25,106,results,our model,has,superior performance,our model has superior performance,0.5737187266349792
translation,25,106,results,results,has,our model,results has our model,0.5871725678443909
translation,26,6,model,model,introduces,notion of essential question terms,model introduces notion of essential question terms,0.6604784727096558
translation,26,33,results,"f1 ( 0.80 ) and per-sentence mean average precision ( map , 0.90 ) scores",of,our classifier,"f1 ( 0.80 ) and per-sentence mean average precision ( map , 0.90 ) scores of our classifier",0.5471527576446533
translation,26,33,results,our classifier,supercede,closest baselines,our classifier supercede closest baselines,0.6249889135360718
translation,26,33,results,closest baselines,by,3 % - 5 %,closest baselines by 3 % - 5 %,0.6101393103599548
translation,26,33,results,results,has,"f1 ( 0.80 ) and per-sentence mean average precision ( map , 0.90 ) scores","results has f1 ( 0.80 ) and per-sentence mean average precision ( map , 0.90 ) scores",0.5227251052856445
translation,26,35,results,improve,by,1.2 %,improve by 1.2 %,0.6349946856498718
translation,26,35,results,surprisingly effective ir based qa system,by,4 % - 5 %,surprisingly effective ir based qa system by 4 % - 5 %,0.5934028029441833
translation,26,35,results,surprisingly effective ir based qa system,by,1.2 %,surprisingly effective ir based qa system by 1.2 %,0.5611228942871094
translation,26,35,results,4 % - 5 %,on,previously used question sets,4 % - 5 % on previously used question sets,0.5245295763015747
translation,26,35,results,4 % - 5 %,by,1.2 %,4 % - 5 % by 1.2 %,0.6135132908821106
translation,26,35,results,1.2 %,on,larger question set,1.2 % on larger question set,0.5252649188041687
translation,26,35,results,improve,has,surprisingly effective ir based qa system,improve has surprisingly effective ir based qa system,0.5949289798736572
translation,27,163,ablation-analysis,answer - aware loss,improves,performance,answer - aware loss improves performance,0.6974143385887146
translation,27,163,ablation-analysis,performance,has,+ 5.16 bleu4,performance has + 5.16 bleu4,0.5856150388717651
translation,27,163,ablation-analysis,ablation analysis,incorporating,answer - aware loss,ablation analysis incorporating answer - aware loss,0.7361189723014832
translation,27,170,ablation-analysis,answer - aware loss,is,not leveraged ( ? = 0 ),answer - aware loss is not leveraged ( ? = 0 ),0.5779195427894592
translation,27,170,ablation-analysis,advantages,of,performance,advantages of performance,0.6019431352615356
translation,27,170,ablation-analysis,obvious,in,our model,obvious in our model,0.5997932553291321
translation,27,170,ablation-analysis,answer - aware loss,has,advantages,answer - aware loss has advantages,0.5867375731468201
translation,27,170,ablation-analysis,ablation analysis,When,answer - aware loss,ablation analysis When answer - aware loss,0.632806658744812
translation,27,181,ablation-analysis,any component,brings,performance decline,any component brings performance decline,0.6363105177879333
translation,27,181,ablation-analysis,performance decline,on,all metrics,performance decline on all metrics,0.5020269751548767
translation,27,181,ablation-analysis,ablation analysis,removing,any component,ablation analysis removing any component,0.7465980648994446
translation,27,182,ablation-analysis,components,are,useful,components are useful,0.6118482947349548
translation,27,182,ablation-analysis,ablation analysis,demonstrates,components,ablation analysis demonstrates components,0.679794430732727
translation,27,183,ablation-analysis,diversified contexts,with,contexts,diversified contexts with contexts,0.6534799337387085
translation,27,183,ablation-analysis,contexts,used in,elsahar et al . ( 2018 ),contexts used in elsahar et al . ( 2018 ),0.7178640365600586
translation,27,183,ablation-analysis,ablation analysis,replacing,diversified contexts,ablation analysis replacing diversified contexts,0.680608332157135
translation,27,191,ablation-analysis,performance,of,kbqg,performance of kbqg,0.6280648708343506
translation,27,191,ablation-analysis,degraded,without,transe embeddings,degraded without transe embeddings,0.7574723958969116
translation,27,191,ablation-analysis,ablation analysis,shows that,performance,ablation analysis shows that performance,0.6528566479682922
translation,27,200,ablation-analysis,generative questions,weaken,performance,generative questions weaken performance,0.7880493998527527
translation,27,200,ablation-analysis,performance,of,qa,performance of qa,0.6746920347213745
translation,27,200,ablation-analysis,ablation analysis,adding,generative questions,ablation analysis adding generative questions,0.7096625566482544
translation,27,147,baselines,template,randomly chooses,candidate fact f c,template randomly chooses candidate fact f c,0.7107682228088379
translation,27,147,baselines,candidate fact f c,in,training data,candidate fact f c in training data,0.4976027011871338
translation,27,147,baselines,candidate fact f c,to generate,question,candidate fact f c to generate question,0.6891369223594666
translation,27,147,baselines,baselines,has,template,baselines has template,0.5846037864685059
translation,27,148,baselines,baselines,has,serban,baselines has serban,0.6088728308677673
translation,27,154,hyperparameters,rmsprop algorithm,with,decreasing learning rate ( 0.001 ),rmsprop algorithm with decreasing learning rate ( 0.001 ),0.5985226035118103
translation,27,154,hyperparameters,rmsprop algorithm,with,batch size ( 200 ),rmsprop algorithm with batch size ( 200 ),0.6284915804862976
translation,27,154,hyperparameters,batch size ( 200 ),to optimize,model,batch size ( 200 ) to optimize model,0.7273879051208496
translation,27,154,hyperparameters,hyperparameters,utilize,rmsprop algorithm,hyperparameters utilize rmsprop algorithm,0.5660535097122192
translation,27,155,hyperparameters,size,of,kb embeddings,size of kb embeddings,0.6074955463409424
translation,27,155,hyperparameters,size,of,kb embeddings,size of kb embeddings,0.6074955463409424
translation,27,155,hyperparameters,kb embeddings,is,200,kb embeddings is 200,0.6461158394813538
translation,27,155,hyperparameters,kb embeddings,pre-trained by,"transe ( bordes et al. , 2013 )","kb embeddings pre-trained by transe ( bordes et al. , 2013 )",0.7678589820861816
translation,27,155,hyperparameters,hyperparameters,has,size,hyperparameters has size,0.5313372611999512
translation,27,155,hyperparameters,hyperparameters,has,kb embeddings,hyperparameters has kb embeddings,0.5192474722862244
translation,27,156,hyperparameters,word embeddings,initialized by,pre-trained glove word vectors,word embeddings initialized by pre-trained glove word vectors,0.6495243906974792
translation,27,156,hyperparameters,pre-trained glove word vectors,with,200 dimensions,pre-trained glove word vectors with 200 dimensions,0.6207277178764343
translation,27,156,hyperparameters,hyperparameters,has,word embeddings,hyperparameters has word embeddings,0.4784160554409027
translation,27,157,hyperparameters,hidden units d,to,200,hidden units d to 200,0.6179873943328857
translation,27,7,model,neural encoder-decoder model,with,multi-level copy mechanisms,neural encoder-decoder model with multi-level copy mechanisms,0.6241952776908875
translation,27,7,model,multi-level copy mechanisms,to generate,questions,multi-level copy mechanisms to generate questions,0.6991585493087769
translation,27,7,model,model,propose,neural encoder-decoder model,model propose neural encoder-decoder model,0.674688458442688
translation,27,8,model,answer aware loss,to make,generated questions,answer aware loss to make generated questions,0.6431652307510376
translation,27,8,model,generated questions,corresponding to,more definitive answers,generated questions corresponding to more definitive answers,0.6168930530548096
translation,27,8,model,model,has,answer aware loss,model has answer aware loss,0.5961621403694153
translation,27,48,model,multi-level copy mechanism ( kb copy and context copy ),to integrate,diversified contexts,multi-level copy mechanism ( kb copy and context copy ) to integrate diversified contexts,0.728490948677063
translation,27,48,model,diversified contexts,where,multilevel copy mechanism,diversified contexts where multilevel copy mechanism,0.6431845426559448
translation,27,48,model,model,propose,context - augmented fact encoder,model propose context - augmented fact encoder,0.6813268065452576
translation,27,48,model,model,propose,multi-level copy mechanism ( kb copy and context copy ),model propose multi-level copy mechanism ( kb copy and context copy ),0.6814258694648743
translation,27,49,model,questions,correspond to,definitive answers,questions correspond to definitive answers,0.6700219511985779
translation,27,49,model,questions,propose,answer - aware loss,questions propose answer - aware loss,0.721959114074707
translation,27,49,model,answer - aware loss,by optimizing,cross-entropy,answer - aware loss by optimizing cross-entropy,0.6902567148208618
translation,27,49,model,cross-entropy,between,generated question and answer type words,cross-entropy between generated question and answer type words,0.6469757556915283
translation,27,49,model,model,propose,answer - aware loss,model propose answer - aware loss,0.7162101864814758
translation,27,89,model,multi-level copy mechanism ( k- b copy and context copy ),allows,copying,multi-level copy mechanism ( k- b copy and context copy ) allows copying,0.6679567694664001
translation,27,89,model,copying,from,kbs and textual contexts,copying from kbs and textual contexts,0.5869978666305542
translation,27,162,results,remarkably better,than,baselines,remarkably better than baselines,0.5914421081542969
translation,27,162,results,baselines,on,all metrics,baselines on all metrics,0.4551103711128235
translation,27,162,results,increases 4.53,compared with,strongest baseline,increases 4.53 compared with strongest baseline,0.6472356915473938
translation,27,162,results,bleu4 score,has,increases 4.53,bleu4 score has increases 4.53,0.6109827160835266
translation,27,162,results,results,evident that,our model,results evident that our model,0.7340703010559082
translation,27,167,results,significant improvement,in,predicate identification,significant improvement in predicate identification,0.5267438292503357
translation,27,167,results,our model,has,significant improvement,our model has significant improvement,0.554693341255188
translation,27,167,results,results,see that,our model,results see that our model,0.6820751428604126
translation,27,171,results,answer coverage,is,55.23,answer coverage is 55.23,0.5543586015701294
translation,27,171,results,55.23,on,human-labeled questions,55.23 on human-labeled questions,0.5247200727462769
translation,27,171,results,results,Note,answer coverage,results Note answer coverage,0.6162294149398804
translation,27,172,results,our model,does not explicitly capture,answer information,our model does not explicitly capture answer information,0.7547213435173035
translation,27,172,results,our model,obtains,high answer coverage,our model obtains high answer coverage,0.6042888760566711
translation,27,172,results,results,has,our model,results has our model,0.5871725678443909
translation,27,174,results,our model,incorporating,answer - aware loss,our model incorporating answer - aware loss,0.7317901849746704
translation,27,174,results,significant improvement,on,answer coverage,significant improvement on answer coverage,0.5083632469177246
translation,27,174,results,no performance degradation,on,bleu4,no performance degradation on bleu4,0.6126993298530579
translation,27,174,results,answer - aware loss,has,significant improvement,answer - aware loss has significant improvement,0.5938921570777893
translation,27,174,results,results,be seen that,our model,results be seen that our model,0.6753299832344055
translation,27,174,results,results,incorporating,answer - aware loss,results incorporating answer - aware loss,0.7001273036003113
translation,27,201,results,our generated questions,achieve,best performance,our generated questions achieve best performance,0.6491842269897461
translation,27,201,results,best performance,on,qa system,best performance on qa system,0.5811778903007507
translation,27,201,results,results,has,our generated questions,results has our generated questions,0.5604065656661987
translation,27,206,results,our model,achieves,best performances,our model achieves best performances,0.6731583476066589
translation,27,206,results,best performances,on,almost epochs,best performances on almost epochs,0.5201959609985352
translation,27,206,results,results,has,our model,results has our model,0.5871725678443909
translation,28,6,experiments,open semantic answer type detector,for,answer merging,open semantic answer type detector for answer merging,0.5818518996238708
translation,29,181,ablation-analysis,query generation accuracy,at,each turn,query generation accuracy at each turn,0.5523858070373535
translation,29,181,ablation-analysis,query generation accuracy,is,significantly improved,query generation accuracy is significantly improved,0.5964952111244202
translation,29,181,ablation-analysis,utterance - table bert embedding,has,query generation accuracy,utterance - table bert embedding has query generation accuracy,0.538203775882721
translation,29,133,baselines,baselines,has,syntaxsql -con,baselines has syntaxsql -con,0.5881967544555664
translation,29,141,experimental-setup,pretrained 300 - dimensional glove,has,word embedding,pretrained 300 - dimensional glove has word embedding,0.5261337757110596
translation,29,141,experimental-setup,experimental setup,use,pretrained 300 - dimensional glove,experimental setup use pretrained 300 - dimensional glove,0.5974981784820557
translation,29,142,experimental-setup,lstm layers,have,300 hidden size,lstm layers have 300 hidden size,0.5425013303756714
translation,29,142,experimental-setup,lstm layers,use,1 layer,lstm layers use 1 layer,0.5342787504196167
translation,29,142,experimental-setup,lstm layers,use,2 layers,lstm layers use 2 layers,0.5245218276977539
translation,29,142,experimental-setup,1 layer,for,encoder lstms,1 layer for encoder lstms,0.5785424113273621
translation,29,142,experimental-setup,1 layer,for,decoder lstms,1 layer for decoder lstms,0.5533725619316101
translation,29,142,experimental-setup,1 layer,for,decoder lstms,1 layer for decoder lstms,0.5533725619316101
translation,29,142,experimental-setup,2 layers,for,decoder lstms,2 layers for decoder lstms,0.5953096747398376
translation,29,142,experimental-setup,experimental setup,use,1 layer,experimental setup use 1 layer,0.5610706210136414
translation,29,142,experimental-setup,experimental setup,use,2 layers,experimental setup use 2 layers,0.5522706508636475
translation,29,142,experimental-setup,experimental setup,has,lstm layers,experimental setup has lstm layers,0.506779670715332
translation,29,143,experimental-setup,adam optimizer,to minimize,tokenlevel cross-entropy loss,adam optimizer to minimize tokenlevel cross-entropy loss,0.6500491499900818
translation,29,143,experimental-setup,tokenlevel cross-entropy loss,with,batch size,tokenlevel cross-entropy loss with batch size,0.5837743282318115
translation,29,143,experimental-setup,batch size,of,16,batch size of 16,0.6842944622039795
translation,29,143,experimental-setup,experimental setup,use,adam optimizer,experimental setup use adam optimizer,0.5987385511398315
translation,29,144,experimental-setup,model parameters,randomly initialized from,"uniform distribution u [?0.1 , 0.1 ]","model parameters randomly initialized from uniform distribution u [?0.1 , 0.1 ]",0.6814406514167786
translation,29,144,experimental-setup,experimental setup,has,model parameters,experimental setup has model parameters,0.4974170923233032
translation,29,145,experimental-setup,initial learning rate,of,0.001,initial learning rate of 0.001,0.5703312754631042
translation,29,145,experimental-setup,0.8,if,validation loss,0.8 if validation loss,0.6011301875114441
translation,29,145,experimental-setup,increases,compared with,previous epoch,increases compared with previous epoch,0.7114018797874451
translation,29,145,experimental-setup,main model,has,initial learning rate,main model has initial learning rate,0.5060380101203918
translation,29,145,experimental-setup,validation loss,has,increases,validation loss has increases,0.5911926627159119
translation,29,145,experimental-setup,experimental setup,has,main model,experimental setup has main model,0.5375426411628723
translation,29,146,experimental-setup,bert,instead of,glove,bert instead of glove,0.6709027290344238
translation,29,146,experimental-setup,bert,use,pretrained small uncased bert model,bert use pretrained small uncased bert model,0.5950993895530701
translation,29,146,experimental-setup,pretrained small uncased bert model,with,768 hidden size 5,pretrained small uncased bert model with 768 hidden size 5,0.6478744745254517
translation,29,146,experimental-setup,separate constant learning rate,of,0.00001,separate constant learning rate of 0.00001,0.5897342562675476
translation,29,146,experimental-setup,experimental setup,When using,bert,experimental setup When using bert,0.6491361260414124
translation,29,147,experimental-setup,converges,in,10 epochs,converges in 10 epochs,0.5301682949066162
translation,29,147,experimental-setup,experimental setup,has,training,experimental setup has training,0.5312813520431519
translation,29,5,model,interaction history,by editing,previous predicted query,interaction history by editing previous predicted query,0.6444809436798096
translation,29,5,model,previous predicted query,to improve,generation quality,previous predicted query to improve generation quality,0.6538561582565308
translation,29,5,model,model,utilize,interaction history,model utilize interaction history,0.6501513719558716
translation,29,6,model,editing mechanism,views,sql,editing mechanism views sql,0.6635485291481018
translation,29,6,model,editing mechanism,reuses,generation results,editing mechanism reuses generation results,0.7069039344787598
translation,29,6,model,sql,as,sequences,sql as sequences,0.590553343296051
translation,29,6,model,generation results,at,token level,generation results at token level,0.5605263710021973
translation,29,6,model,model,has,editing mechanism,model has editing mechanism,0.5645689368247986
translation,29,8,model,complex table structures,in,different domains,complex table structures in different domains,0.5193459987640381
translation,29,8,model,complex table structures,employ,utterance - table encoder,complex table structures employ utterance - table encoder,0.5576843023300171
translation,29,8,model,complex table structures,employ,table - aware decoder,complex table structures employ table - aware decoder,0.5451706051826477
translation,29,8,model,table - aware decoder,to incorporate,context,table - aware decoder to incorporate context,0.6598455309867859
translation,29,8,model,context,of,user utterance and the table schema,context of user utterance and the table schema,0.5814399123191833
translation,29,8,model,model,to deal with,complex table structures,model to deal with complex table structures,0.6768720149993896
translation,29,8,model,model,employ,utterance - table encoder,model employ utterance - table encoder,0.5730680227279663
translation,29,8,model,model,employ,table - aware decoder,model employ table - aware decoder,0.5765953063964844
translation,29,19,model,editing - based approach,for,cross-domain contextdependent text - to - sql generation task,editing - based approach for cross-domain contextdependent text - to - sql generation task,0.5841512680053711
translation,29,19,model,model,study,editing - based approach,model study editing - based approach,0.6030468344688416
translation,29,20,model,query generation,by editing,query,query generation by editing query,0.6955091953277588
translation,29,20,model,model,propose,query generation,model propose query generation,0.6750401258468628
translation,29,21,model,previous query,as,sequence of tokens,previous query as sequence of tokens,0.5318862199783325
translation,29,21,model,decoder,computes,switch,decoder computes switch,0.7658019661903381
translation,29,21,model,model,encode,previous query,model encode previous query,0.8102423548698425
translation,29,22,model,sequence editing mechanism,models,token - level changes,sequence editing mechanism models token - level changes,0.7393656969070435
translation,29,22,model,sequence editing mechanism,robust to,error propagation,sequence editing mechanism robust to error propagation,0.7332178354263306
translation,29,22,model,model,has,sequence editing mechanism,model has sequence editing mechanism,0.5463473796844482
translation,29,23,model,utterance - table encoder,based on,bert,utterance - table encoder based on bert,0.650751531124115
translation,29,23,model,utterance - table encoder,adopt,table - aware decoder,utterance - table encoder adopt table - aware decoder,0.6555222272872925
translation,29,23,model,user utterance and column headers,with,co-attention,user utterance and column headers with co-attention,0.6657489538192749
translation,29,23,model,table - aware decoder,to perform,sql generation,table - aware decoder to perform sql generation,0.6541936993598938
translation,29,23,model,sql generation,with,attentions,sql generation with attentions,0.689780056476593
translation,29,23,model,attentions,over,user utterance and column headers,attentions over user utterance and column headers,0.7131346464157104
translation,29,23,model,attentions,both,user utterance and column headers,attentions both user utterance and column headers,0.6421786546707153
translation,29,23,model,model,use,utterance - table encoder,model use utterance - table encoder,0.6174665093421936
translation,29,23,model,model,adopt,table - aware decoder,model adopt table - aware decoder,0.6803891062736511
translation,29,49,model,utterance - table encoder,to explicitly encode,user utterance and table schema,utterance - table encoder to explicitly encode user utterance and table schema,0.7210319638252258
translation,29,49,model,user utterance and table schema,at,each turn,user utterance and table schema at each turn,0.573244571685791
translation,29,49,model,turn attention,incorporating,recent history,turn attention incorporating recent history,0.7275738716125488
translation,29,49,model,recent history,for,decoding,recent history for decoding,0.6202684044837952
translation,29,49,model,table - aware decoder,taking into account,context,table - aware decoder taking into account context,0.5992465615272522
translation,29,49,model,table - aware decoder,taking into account,previously generated query,table - aware decoder taking into account previously generated query,0.6170285940170288
translation,29,49,model,context,of,"utterance , the table schema","context of utterance , the table schema",0.5897011756896973
translation,29,49,model,context,of,previously generated query,context of previously generated query,0.5284785032272339
translation,29,49,model,previously generated query,to make,editing decisions,previously generated query to make editing decisions,0.5783994197845459
translation,29,134,model,original context-agnostic syntaxsqlnet,by using,bi-lstms,original context-agnostic syntaxsqlnet by using bi-lstms,0.6351804137229919
translation,29,134,model,bi-lstms,to encode,interaction history,bi-lstms to encode interaction history,0.7459747195243835
translation,29,134,model,interaction history,including,utterance,interaction history including utterance,0.7215805053710938
translation,29,134,model,interaction history,including,associated sql query response,interaction history including associated sql query response,0.7108926177024841
translation,29,134,model,model,adapted from,original context-agnostic syntaxsqlnet,model adapted from original context-agnostic syntaxsqlnet,0.5245276093482971
translation,29,189,model,model,propose,editing - based encoder-decoder model,model propose editing - based encoder-decoder model,0.6404231786727905
translation,29,25,results,our model,delivers,improvement,our model delivers improvement,0.6762264370918274
translation,29,25,results,improvement,of,7 % question match accuracy,improvement of 7 % question match accuracy,0.5599445104598999
translation,29,25,results,improvement,of,11 % interaction match accuracy,improvement of 11 % interaction match accuracy,0.5561356544494629
translation,29,25,results,11 % interaction match accuracy,over,previous state - of - the - art,11 % interaction match accuracy over previous state - of - the - art,0.631280779838562
translation,29,25,results,previous query,has,our model,previous query has our model,0.5986997485160828
translation,29,25,results,results,generating from,previous query,results generating from previous query,0.6635594367980957
translation,29,149,results,embedding,gives,significant improvement,embedding gives significant improvement,0.6757296323776245
translation,29,149,results,57.6 %,on,dev set,57.6 % on dev set,0.5640760064125061
translation,29,149,results,53.4 %,on,test set,53.4 % on test set,0.5288535952568054
translation,29,149,results,results,has,embedding,results has embedding,0.4729495644569397
translation,29,152,results,results,on,sparc dataset,results on sparc dataset,0.5311707258224487
translation,29,153,results,our model,without,previous query,our model without previous query,0.7206020951271057
translation,29,153,results,previous query,as,input,previous query as input,0.5219756364822388
translation,29,153,results,outperforms,achieving,31.4 % question matching accuracy,outperforms achieving 31.4 % question matching accuracy,0.6349081993103027
translation,29,153,results,outperforms,achieving,14.7 % interaction matching accuracy,outperforms achieving 14.7 % interaction matching accuracy,0.645628035068512
translation,29,153,results,outperforms,has,syntaxsql -con,outperforms has syntaxsql -con,0.6382573246955872
translation,29,154,results,our model,enjoys,benefits,our model enjoys benefits,0.6546352505683899
translation,29,154,results,benefits,of,table - utterance encoder,benefits of table - utterance encoder,0.5977336168289185
translation,29,154,results,benefits,of,turn attention,benefits of turn attention,0.580705463886261
translation,29,154,results,benefits,joint consideration of,utterances and table schemas,benefits joint consideration of utterances and table schemas,0.777513861656189
translation,29,154,results,benefits,during,decoding stage,benefits during decoding stage,0.7149195075035095
translation,29,154,results,utterances and table schemas,during,decoding stage,utterances and table schemas during decoding stage,0.709575355052948
translation,29,154,results,cd - seq2seq,has,our model,cd - seq2seq has our model,0.6168668270111084
translation,29,154,results,results,compared with,cd - seq2seq,results compared with cd - seq2seq,0.6320558786392212
translation,29,155,results,performance,by,10 % question accuracy,performance by 10 % question accuracy,0.5842133164405823
translation,29,155,results,performance,by,6 % interaction accuracy,performance by 6 % interaction accuracy,0.5696107149124146
translation,29,155,results,results,boosts,performance,results boosts performance,0.7461101412773132
translation,29,160,results,segment copying,to,cd - seq2seq,segment copying to cd - seq2seq,0.6160387992858887
translation,29,160,results,segment copying,gives,slightly lower performance,segment copying gives slightly lower performance,0.6067491173744202
translation,29,160,results,slightly lower performance,on,question matching,slightly lower performance on question matching,0.5032021403312683
translation,29,160,results,slightly lower performance,on,small gain,slightly lower performance on small gain,0.5774571895599365
translation,29,160,results,small gain,on,interaction matching,small gain on interaction matching,0.5647621154785156
translation,29,160,results,segments,extracted from,gold query,segments extracted from gold query,0.5285876989364624
translation,29,160,results,segments,have,much higher results,segments have much higher results,0.5953647494316101
translation,29,160,results,results,adding,segment copying,results adding segment copying,0.649815022945404
translation,29,170,results,our model,achieves,36.2 % dev,our model achieves 36.2 % dev,0.657133162021637
translation,29,170,results,our model,achieves,43.9 % test string accuracy,our model achieves 43.9 % test string accuracy,0.6287104487419128
translation,29,170,results,results,has,our model,results has our model,0.5871725678443909
translation,29,176,results,results,editing,gold query,results editing gold query,0.7114838361740112
translation,29,179,results,model,without,utterance - table bert embedding,model without utterance - table bert embedding,0.6839110255241394
translation,29,179,results,model,using,predicted query,model using predicted query,0.7165538668632507
translation,29,179,results,predicted query,gives,around 1.5 % improvement,predicted query gives around 1.5 % improvement,0.6159476041793823
translation,29,179,results,results,For,model,results For model,0.6094648838043213
translation,29,182,results,editing approach,delivers,consistent improvements,editing approach delivers consistent improvements,0.6719455718994141
translation,29,182,results,consistent improvements,of,7 % increase,consistent improvements of 7 % increase,0.5992342829704285
translation,29,182,results,consistent improvements,of,11 % increase,consistent improvements of 11 % increase,0.5922300815582275
translation,29,182,results,7 % increase,on,question matching accuracy,7 % increase on question matching accuracy,0.5100967288017273
translation,29,182,results,11 % increase,on,interaction matching accuracy,11 % increase on interaction matching accuracy,0.5287602543830872
translation,29,182,results,results,has,editing approach,results has editing approach,0.5408393144607544
translation,29,183,results,query editing,with,bert,query editing with bert,0.7073700428009033
translation,29,183,results,bert,benefits,all turns,bert benefits all turns,0.5604536533355713
translation,29,183,results,results,shows,query editing,results shows query editing,0.6816292405128479
translation,29,185,results,our vanilla bert model,without,query attention,our vanilla bert model without query attention,0.7196228504180908
translation,29,186,results,best model,improves to,47.2 % question,best model improves to 47.2 % question,0.6826464533805847
translation,29,186,results,best model,improves to,29.5 % interaction matching accuracy,best model improves to 29.5 % interaction matching accuracy,0.6589170694351196
translation,29,186,results,query editing,has,best model,query editing has best model,0.561977207660675
translation,29,186,results,results,With,query editing,results With query editing,0.6278536915779114
translation,29,212,results,results,on,spider dataset,results on spider dataset,0.5408922433853149
translation,29,214,results,our method,achieve,performance,our method achieve performance,0.6141390204429626
translation,29,214,results,performance,of,36.4 %,performance of 36.4 %,0.5516698956489563
translation,29,214,results,performance,of,32.9 %,performance of 32.9 %,0.5572900772094727
translation,29,214,results,performance,serving as,strong model,performance serving as strong model,0.6368472576141357
translation,29,214,results,36.4 %,on,dev set,36.4 % on dev set,0.5647544860839844
translation,29,214,results,32.9 %,on,test set,32.9 % on test set,0.5334177017211914
translation,29,214,results,strong model,for,context- independent cross - domain text - to - sql generation,strong model for context- independent cross - domain text - to - sql generation,0.5852433443069458
translation,29,214,results,results,has,our method,results has our method,0.5589964985847473
translation,30,34,ablation-analysis,our analysis,indicates,alignments,our analysis indicates alignments,0.699971616268158
translation,30,34,ablation-analysis,alignments,over,"character , word , and sentence embeddings","alignments over character , word , and sentence embeddings",0.6566797494888306
translation,30,34,ablation-analysis,alignments,capture,substantially different semantic information,alignments capture substantially different semantic information,0.7401888966560364
translation,30,34,ablation-analysis,ablation analysis,indicates,alignments,ablation analysis indicates alignments,0.6321297883987427
translation,30,34,ablation-analysis,ablation analysis,has,our analysis,ablation analysis has our analysis,0.573619544506073
translation,30,5,baselines,ahe,aligns,each word,ahe aligns each word,0.7438923716545105
translation,30,5,baselines,ahe,weighs,each alignment score,ahe weighs each alignment score,0.7794835567474365
translation,30,5,baselines,each word,in,question,each word in question,0.546829342842102
translation,30,5,baselines,each word,in,candidate answer,each word in candidate answer,0.5065297484397888
translation,30,5,baselines,most similar word,in,retrieved supporting paragraph,most similar word in retrieved supporting paragraph,0.48487406969070435
translation,30,5,baselines,each alignment score,with,inverse document frequency,each alignment score with inverse document frequency,0.5933195352554321
translation,30,5,baselines,inverse document frequency,of,corresponding question / answer term,inverse document frequency of corresponding question / answer term,0.5229827165603638
translation,30,5,baselines,baselines,has,ahe,baselines has ahe,0.5793318748474121
translation,30,24,baselines,ahe,uses,off-the-shelf information retrieval ( ir ) component,ahe uses off-the-shelf information retrieval ( ir ) component,0.6216719150543213
translation,30,24,baselines,off-the-shelf information retrieval ( ir ) component,to retrieve,likely supporting paragraphs,off-the-shelf information retrieval ( ir ) component to retrieve likely supporting paragraphs,0.763460636138916
translation,30,24,baselines,likely supporting paragraphs,from,knowledge base ( kb ),likely supporting paragraphs from knowledge base ( kb ),0.5450842380523682
translation,30,24,baselines,likely supporting paragraphs,given,question,likely supporting paragraphs given question,0.7450848817825317
translation,30,24,baselines,likely supporting paragraphs,given,candidate answer,likely supporting paragraphs given candidate answer,0.7354310154914856
translation,30,24,baselines,baselines,has,ahe,baselines has ahe,0.5793318748474121
translation,30,28,model,different representations,combined through,ensemble approach,different representations combined through ensemble approach,0.7106844186782837
translation,30,28,model,ensemble approach,by default,unsupervised,ensemble approach by default unsupervised,0.6968209743499756
translation,30,28,model,ensemble approach,is,unsupervised,ensemble approach is unsupervised,0.6346122026443481
translation,30,28,model,model,has,different representations,model has different representations,0.5768856406211853
translation,30,30,results,"wik-iqa ( yang et al. , 2015 )",has,74.08 mean reciprocal rank,"wik-iqa ( yang et al. , 2015 ) has 74.08 mean reciprocal rank",0.5349408984184265
translation,30,30,results,arc,has,challenge partition,arc has challenge partition,0.6421217918395996
translation,30,30,results,challenge partition,has,34.1 % precision at 1 ( p@1 ),challenge partition has 34.1 % precision at 1 ( p@1 ),0.5744696855545044
translation,30,30,results,easy,has,64.6 p@1 ),easy has 64.6 p@1 ),0.6267140507698059
translation,30,31,results,outperforms,has,information retrieval methods,outperforms has information retrieval methods,0.5833697319030762
translation,30,31,results,information retrieval methods,has,other unsupervised alignment approaches,information retrieval methods has other unsupervised alignment approaches,0.522214949131012
translation,30,32,results,results,are,robust,results are robust,0.5653069019317627
translation,30,32,results,robust,across,several datasets,robust across several datasets,0.741562008857727
translation,30,32,results,results,are,robust,results are robust,0.5653069019317627
translation,31,107,baselines,baselines,evaluated,three existing model architectures,baselines evaluated three existing model architectures,0.7143622636795044
translation,31,27,experiments,"53,775 new , unanswerable ques-tions",about,same paragraphs,"53,775 new , unanswerable ques-tions about same paragraphs",0.6263704895973206
translation,31,115,results,best model,achieves,only 66.3 f1,best model achieves only 66.3 f1,0.6121827960014343
translation,31,115,results,only 66.3 f1,on,test set,only 66.3 f1 on test set,0.5591884255409241
translation,31,115,results,lower,than,human accuracy,lower than human accuracy,0.6012736558914185
translation,31,115,results,human accuracy,of,89.5 f1,human accuracy of 89.5 f1,0.5559872984886169
translation,31,115,results,best model,has,docqa + elmo,best model has docqa + elmo,0.5444372892379761
translation,31,115,results,23.2 points,has,lower,23.2 points has lower,0.5428868532180786
translation,31,115,results,results,has,best model,results has best model,0.5634682774543762
translation,32,95,hyperparameters,ridge regression model,on,data set,ridge regression model on data set,0.5760272741317749
translation,32,95,hyperparameters,ridge regression model,optimised,regularization hyperparameter alpha,ridge regression model optimised regularization hyperparameter alpha,0.656499445438385
translation,32,95,hyperparameters,regularization hyperparameter alpha,for,total l1 - loss and correlation,regularization hyperparameter alpha for total l1 - loss and correlation,0.5657334327697754
translation,32,95,hyperparameters,regularization hyperparameter alpha,using,predictions,regularization hyperparameter alpha using predictions,0.6158149838447571
translation,32,95,hyperparameters,total l1 - loss and correlation,using,predictions,total l1 - loss and correlation using predictions,0.6696670055389404
translation,32,95,hyperparameters,predictions,across,all users,predictions across all users,0.7066105604171753
translation,32,95,hyperparameters,hyperparameters,trained,ridge regression model,hyperparameters trained ridge regression model,0.7097519040107727
translation,32,95,hyperparameters,hyperparameters,optimised,regularization hyperparameter alpha,hyperparameters optimised regularization hyperparameter alpha,0.6515937447547913
translation,32,96,hyperparameters,hyperparameter alpha,tuned between,alpha = 1 and alpha = 1000,hyperparameter alpha tuned between alpha = 1 and alpha = 1000,0.715949535369873
translation,32,96,hyperparameters,hyperparameters,has,hyperparameter alpha,hyperparameters has hyperparameter alpha,0.5014834403991699
translation,32,97,hyperparameters,knn model,optimised,number of neighbors k,knn model optimised number of neighbors k,0.6836535930633545
translation,32,97,hyperparameters,number of neighbors k,for,total l1 - loss and correlation,number of neighbors k for total l1 - loss and correlation,0.6153485774993896
translation,32,97,hyperparameters,hyperparameters,trained,knn model,hyperparameters trained knn model,0.709498941898346
translation,32,97,hyperparameters,hyperparameters,optimised,number of neighbors k,hyperparameters optimised number of neighbors k,0.664745569229126
translation,32,110,hyperparameters,hyperparameter alpha,tuned between,"alpha = 1 and alpha = 100 , 000","hyperparameter alpha tuned between alpha = 1 and alpha = 100 , 000",0.7207085490226746
translation,32,110,hyperparameters,number of neighbors k,tuned between,k = 5 and k = 450,number of neighbors k tuned between k = 5 and k = 450,0.7197684645652771
translation,32,110,hyperparameters,hyperparameters,has,hyperparameter alpha,hyperparameters has hyperparameter alpha,0.5014834403991699
translation,32,110,hyperparameters,hyperparameters,has,number of neighbors k,hyperparameters has number of neighbors k,0.5287754535675049
translation,32,6,model,method,for predicting,participant 's questionnaire response,method for predicting participant 's questionnaire response,0.6561211347579956
translation,32,6,model,participant 's questionnaire response,their social media texts and the text of,survey question,participant 's questionnaire response their social media texts and the text of survey question,0.6345075368881226
translation,32,6,model,model,propose,method,model propose method,0.6280754208564758
translation,32,7,model,natural language processing ( nlp ) tools,such as,bert embeddings,natural language processing ( nlp ) tools such as bert embeddings,0.6251010894775391
translation,32,7,model,bert embeddings,to represent,participants ( via the text they write ) and survey questions,bert embeddings to represent participants ( via the text they write ) and survey questions,0.6597284078598022
translation,32,7,model,participants ( via the text they write ) and survey questions,as,embeddings vectors,participants ( via the text they write ) and survey questions as embeddings vectors,0.5567066669464111
translation,32,7,model,responses,for,out - of-sample participants and questions,responses for out - of-sample participants and questions,0.5547890067100525
translation,32,7,model,model,use,natural language processing ( nlp ) tools,model use natural language processing ( nlp ) tools,0.6248480677604675
translation,32,146,results,highest correlation,as,r = 0.324 ( p < 0.05 ),highest correlation as r = 0.324 ( p < 0.05 ),0.503376841545105
translation,32,146,results,highest correlation,for,ridge regression,highest correlation for ridge regression,0.627445638179779
translation,32,146,results,ridge regression,with,regularization parameter,ridge regression with regularization parameter,0.5959768891334534
translation,32,146,results,regularization parameter,of,alpha = 10,regularization parameter of alpha = 10,0.5851544141769409
translation,32,146,results,regularization parameter,compared to,baseline correlation,regularization parameter compared to baseline correlation,0.6364444494247437
translation,32,146,results,baseline correlation,of,r = 0.114 ( p < 0.05 ),baseline correlation of r = 0.114 ( p < 0.05 ),0.5243128538131714
translation,32,146,results,results,shows,highest correlation,results shows highest correlation,0.6895987391471863
translation,32,147,results,significantly improve,over,baseline,significantly improve over baseline,0.7181452512741089
translation,32,147,results,predictions,over,baseline,predictions over baseline,0.7632488012313843
translation,32,147,results,questionnaire embeddings,has,significantly improve,questionnaire embeddings has significantly improve,0.5853387117385864
translation,32,147,results,significantly improve,has,predictions,significantly improve has predictions,0.5485026836395264
translation,32,147,results,results,has,questionnaire embeddings,results has questionnaire embeddings,0.5349064469337463
translation,32,162,results,highest correlation,to be,r = 0.421 ( p < 0.05 ),highest correlation to be r = 0.421 ( p < 0.05 ),0.5509824156761169
translation,32,162,results,highest correlation,compared to,baseline correlation,highest correlation compared to baseline correlation,0.6583556532859802
translation,32,162,results,r = 0.421 ( p < 0.05 ),for,ridge regression,r = 0.421 ( p < 0.05 ) for ridge regression,0.572939395904541
translation,32,162,results,ridge regression,with,regularization parameter,ridge regression with regularization parameter,0.5959768891334534
translation,32,162,results,regularization parameter,of,alpha = 1000,regularization parameter of alpha = 1000,0.5814751386642456
translation,32,162,results,results,shows,highest correlation,results shows highest correlation,0.6895987391471863
translation,32,163,results,significant improvement,of,prediction,significant improvement of prediction,0.6093064546585083
translation,32,163,results,prediction,over,baseline,prediction over baseline,0.7461309432983398
translation,32,163,results,results,see,significant improvement,results see significant improvement,0.6090703010559082
translation,32,164,results,utility,of,user embeddings,utility of user embeddings,0.5271704196929932
translation,32,164,results,user embeddings,in predicting,personality,user embeddings in predicting personality,0.6136897802352905
translation,32,164,results,results,reconfirms,utility,results reconfirms utility,0.6878163814544678
translation,32,165,results,improvement,compared to,older lda model,improvement compared to older lda model,0.6075835227966309
translation,32,165,results,older lda model,is,strong model,older lda model is strong model,0.5296713709831238
translation,32,165,results,strong model,proving,bert embeddings,strong model proving bert embeddings,0.7130195498466492
translation,32,165,results,bert embeddings,are,superior,bert embeddings are superior,0.6790062785148621
translation,32,165,results,superior,in capturing,personality,superior in capturing personality,0.699283242225647
translation,32,165,results,results,show,improvement,results show improvement,0.7196874022483826
translation,32,179,results,ridge regression,does n't perform,well,ridge regression does n't perform well,0.6858957409858704
translation,32,179,results,ridge regression,does n't perform,knn model,ridge regression does n't perform knn model,0.6297252178192139
translation,32,179,results,well,as,knn model,well as knn model,0.5843682885169983
translation,32,195,results,much more difficulty,in giving,good predictions,much more difficulty in giving good predictions,0.6444785594940186
translation,32,195,results,good predictions,with,very low correlation,good predictions with very low correlation,0.6221200227737427
translation,32,195,results,good predictions,with,high,good predictions with high,0.6689073443412781
translation,32,195,results,baseline,has,much more difficulty,baseline has much more difficulty,0.6115119457244873
translation,32,195,results,high,has,l1 loss,high has l1 loss,0.5892828106880188
translation,32,202,results,predictions,using,user embeddings,predictions using user embeddings,0.6233710050582886
translation,32,202,results,user embeddings,shows,best performance,user embeddings shows best performance,0.6449400782585144
translation,32,202,results,best performance,in,category openness,best performance in category openness,0.5167970061302185
translation,32,202,results,best performance,followed by,agreeableness,best performance followed by agreeableness,0.6163504719734192
translation,33,227,ablation-analysis,qa performance,on,different answer types,qa performance on different answer types,0.5587677359580994
translation,33,227,ablation-analysis,qa performance,in,development set,qa performance in development set,0.5349265336990356
translation,33,227,ablation-analysis,different answer types,in,development set,different answer types in development set,0.5152897238731384
translation,33,248,ablation-analysis,question attention,from,heterogeneous graph,question attention from heterogeneous graph,0.5387636423110962
translation,33,248,ablation-analysis,heterogeneous graph,weakens,power,heterogeneous graph weakens power,0.815400242805481
translation,33,248,ablation-analysis,power,of,qdgat,power of qdgat,0.6623746752738953
translation,33,248,ablation-analysis,qdgat,to learn,numeracy,qdgat to learn numeracy,0.628425121307373
translation,33,248,ablation-analysis,qdgat,capability of understanding,numbers,qdgat capability of understanding numbers,0.7259137630462646
translation,33,248,ablation-analysis,numbers,in either,digits or word form,numbers in either digits or word form,0.5789334774017334
translation,33,249,ablation-analysis,qdgat,ablating,question directed attention,qdgat ablating question directed attention,0.6549151539802551
translation,33,249,ablation-analysis,question directed attention,leads to,about a 1 point drop,question directed attention leads to about a 1 point drop,0.6900781989097595
translation,33,249,ablation-analysis,ablation analysis,Compared with,qdgat,ablation analysis Compared with qdgat,0.6974020600318909
translation,33,194,baselines,baselines,choose,publicly available methods,baselines choose publicly available methods,0.6352823972702026
translation,33,195,baselines,"kdg ( krishnamurthy et al. , 2017 )",with,different sentence representations,"kdg ( krishnamurthy et al. , 2017 ) with different sentence representations",0.5682070255279541
translation,33,196,baselines,bi-directional attention flow network,to obtain,query - aware context representation,bi-directional attention flow network to obtain query - aware context representation,0.5342757105827332
translation,33,196,baselines,convolution and self-attention models,to answer,questions,convolution and self-attention models to answer questions,0.6955122947692871
translation,33,196,baselines,mrc,has,models,mrc has models,0.622378945350647
translation,33,197,baselines,output layer,of,qanet,output layer of qanet,0.5810706615447998
translation,33,197,baselines,output layer,to,numeric reasoning,output layer to numeric reasoning,0.5445428490638733
translation,33,197,baselines,qanet,to,numeric reasoning,qanet to numeric reasoning,0.598926842212677
translation,33,198,baselines,numnet,supports,multi-span answers,numnet supports multi-span answers,0.6997129321098328
translation,33,198,baselines,numerical properties,into,distributed representation,numerical properties into distributed representation,0.56012362241745
translation,33,198,baselines,distributed representation,by using,gnn,distributed representation by using gnn,0.692158043384552
translation,33,198,baselines,gnn,on,number graph,gnn on number graph,0.5728216171264648
translation,33,198,baselines,numnet + 5,uses,pre-trained roberta model,numnet + 5 uses pre-trained roberta model,0.5622039437294006
translation,33,198,baselines,numnet + 5,supports,multi-span answers,numnet + 5 supports multi-span answers,0.6813146471977234
translation,33,198,baselines,enhanced version of numnet,uses,pre-trained roberta model,enhanced version of numnet uses pre-trained roberta model,0.5725682377815247
translation,33,198,baselines,baselines,has,numnet,baselines has numnet,0.5370360016822815
translation,33,215,baselines,numnet +,is,most relevant one,numnet + is most relevant one,0.59505695104599
translation,33,215,baselines,numnet +,leverages,graph neural network,numnet + leverages graph neural network,0.6954413652420044
translation,33,215,baselines,numnet +,leverages,roberta contextual encoder,numnet + leverages roberta contextual encoder,0.6876446604728699
translation,33,215,baselines,most relevant one,to,our method,most relevant one to our method,0.5652225017547607
translation,33,215,baselines,most relevant one,leverages,graph neural network,most relevant one leverages graph neural network,0.7219790816307068
translation,33,215,baselines,graph neural network,as well as,roberta contextual encoder,graph neural network as well as roberta contextual encoder,0.6141425967216492
translation,33,215,baselines,baselines,has,numnet +,baselines has numnet +,0.5549950003623962
translation,33,216,baselines,qdgat,incorporates,number types and entity mentions,qdgat incorporates number types and entity mentions,0.6837339997291565
translation,33,216,baselines,qdgat,directs,graph reasoning process,qdgat directs graph reasoning process,0.654996395111084
translation,33,216,baselines,number types and entity mentions,into,graph attention network,number types and entity mentions into graph attention network,0.5536224842071533
translation,33,216,baselines,graph reasoning process,with,question,graph reasoning process with question,0.6299312710762024
translation,33,216,baselines,numnet+,has,qdgat,numnet+ has qdgat,0.6504907011985779
translation,33,66,experiments,drop dataset,requires,discrete reasoning,drop dataset requires discrete reasoning,0.7008569836616516
translation,33,66,experiments,discrete reasoning,Over,content,discrete reasoning Over content,0.7206355333328247
translation,33,66,experiments,content,of,paragraph,content of paragraph,0.6085598468780518
translation,33,66,experiments,"subset of the race dataset ( lai et al. , 2017 )",contains,number-related questions,"subset of the race dataset ( lai et al. , 2017 ) contains number-related questions",0.6117674112319946
translation,33,200,hyperparameters,large roberta model,as,contextual encoder,large roberta model as contextual encoder,0.5167199373245239
translation,33,200,hyperparameters,contextual encoder,with,24 layers,contextual encoder with 24 layers,0.6100772619247437
translation,33,200,hyperparameters,contextual encoder,with,16 attention heads,contextual encoder with 16 attention heads,0.6147555112838745
translation,33,200,hyperparameters,contextual encoder,with,1024 embedding dimensions,contextual encoder with 1024 embedding dimensions,0.64703768491745
translation,33,200,hyperparameters,hyperparameters,use,large roberta model,hyperparameters use large roberta model,0.6247130036354065
translation,33,202,hyperparameters,model,trained,end-to - end,model trained end-to - end,0.7512461543083191
translation,33,202,hyperparameters,model,for,5 epochs,model for 5 epochs,0.653461217880249
translation,33,202,hyperparameters,end-to - end,for,5 epochs,end-to - end for 5 epochs,0.6264957785606384
translation,33,202,hyperparameters,5 epochs,using,"adam optimizer ( kingma and ba , 2015 )","5 epochs using adam optimizer ( kingma and ba , 2015 )",0.6462939977645874
translation,33,202,hyperparameters,"adam optimizer ( kingma and ba , 2015 )",with,batch size,"adam optimizer ( kingma and ba , 2015 ) with batch size",0.6134414076805115
translation,33,202,hyperparameters,batch size,of,16,batch size of 16,0.6842944622039795
translation,33,202,hyperparameters,hyperparameters,trained,end-to - end,hyperparameters trained end-to - end,0.7198280096054077
translation,33,202,hyperparameters,hyperparameters,has,model,hyperparameters has model,0.5282720923423767
translation,33,203,hyperparameters,learning rate,is,5e - 5,learning rate is 5e - 5,0.6211683750152588
translation,33,203,hyperparameters,l2 weight decay,is,1e - 6,l2 weight decay is 1e - 6,0.588596522808075
translation,33,203,hyperparameters,roberta,has,learning rate,roberta has learning rate,0.5771343111991882
translation,33,203,hyperparameters,hyperparameters,of,roberta,hyperparameters of roberta,0.580578625202179
translation,33,204,hyperparameters,learning rate,is,1e - 4,learning rate is 1e - 4,0.5861861705780029
translation,33,204,hyperparameters,l2 weight decay,is,5e - 5,l2 weight decay is 5e - 5,0.6179770231246948
translation,33,204,hyperparameters,other parts,has,learning rate,other parts has learning rate,0.560638427734375
translation,33,204,hyperparameters,other parts,has,l2 weight decay,other parts has l2 weight decay,0.5401644706726074
translation,33,205,hyperparameters,t = 4 iterations,of,graph reasoning step,t = 4 iterations of graph reasoning step,0.5576213598251343
translation,33,205,hyperparameters,t = 4 iterations,performs,best,t = 4 iterations performs best,0.6484067440032959
translation,33,205,hyperparameters,hyperparameters,perform,t = 4 iterations,hyperparameters perform t = 4 iterations,0.5538153052330017
translation,33,5,model,heterogeneous graph representation,for,context,heterogeneous graph representation for context,0.5491858720779419
translation,33,5,model,context,of,passage and question,context of passage and question,0.5928444862365723
translation,33,5,model,question directed graph attention network,to drive,multi-step numerical reasoning,question directed graph attention network to drive multi-step numerical reasoning,0.656886637210846
translation,33,5,model,multi-step numerical reasoning,over,context graph,multi-step numerical reasoning over context graph,0.6235629320144653
translation,33,5,model,model,propose,heterogeneous graph representation,model propose heterogeneous graph representation,0.6459206342697144
translation,33,5,model,model,design,question directed graph attention network,model design question directed graph attention network,0.5689666271209717
translation,33,59,model,question directed graph attention network ( qdgat ),for,numerical mrc,question directed graph attention network ( qdgat ) for numerical mrc,0.6165265440940857
translation,33,59,model,question directed graph attention network ( qdgat ),task of,numerical mrc,question directed graph attention network ( qdgat ) task of numerical mrc,0.7147014141082764
translation,33,60,model,qdgat,incorporates,contextual encoding,qdgat incorporates contextual encoding,0.7505837082862854
translation,33,60,model,contextual encoding,of,question,contextual encoding of question,0.5925208330154419
translation,33,60,model,question,in,graph reasoning process,question in graph reasoning process,0.522460401058197
translation,33,62,model,qdgat,collect,information,qdgat collect information,0.676457941532135
translation,33,62,model,information,from,graph,information from graph,0.6009517312049866
translation,33,62,model,information,conditioned on,question,information conditioned on question,0.6993484497070312
translation,33,62,model,question,for,numerical reasoning,question for numerical reasoning,0.6098504066467285
translation,33,62,model,heterogeneous graph,has,qdgat,heterogeneous graph has qdgat,0.6308925747871399
translation,33,62,model,model,With,heterogeneous graph,model With heterogeneous graph,0.5914822220802307
translation,33,64,model,multiple iterations,of,message passing,multiple iterations of message passing,0.619725227355957
translation,33,64,model,message passing,with,graph neural networks,message passing with graph neural networks,0.583109438419342
translation,33,64,model,qdgat,gradually aggregates,node information,qdgat gradually aggregates node information,0.7162895798683167
translation,33,64,model,node information,to answer,question,node information to answer question,0.6498586535453796
translation,33,64,model,multiple iterations,has,qdgat,multiple iterations has qdgat,0.6569274067878723
translation,33,64,model,message passing,has,qdgat,message passing has qdgat,0.63194340467453
translation,33,64,model,graph neural networks,has,qdgat,graph neural networks has qdgat,0.5910640954971313
translation,33,64,model,model,After,multiple iterations,model After multiple iterations,0.7243149876594543
translation,33,99,model,information,between,numbers and entities,information between numbers and entities,0.6085668206214905
translation,33,99,model,information,propose,question directed graph attention network ( qdgat ),information propose question directed graph attention network ( qdgat ),0.6369994282722473
translation,33,99,model,question directed graph attention network ( qdgat ),to make,sophisticated reasoning,question directed graph attention network ( qdgat ) to make sophisticated reasoning,0.6076678037643433
translation,33,99,model,model,To aggregate,information,model To aggregate information,0.8034719228744507
translation,33,152,model,which objects,to,interact with,which objects to interact with,0.5953806638717651
translation,33,152,model,interact with,through,edges in the graph,interact with through edges in the graph,0.6599501371383667
translation,33,152,model,messages,through,graph,messages through graph,0.6856560707092285
translation,33,152,model,graph,to propagate,relational information,graph to propagate relational information,0.7294092178344727
translation,33,152,model,model,dynamically determines,which objects,model dynamically determines which objects,0.7291772365570068
translation,33,67,results,qdgat,achieves,remarkable performance,qdgat achieves remarkable performance,0.7115827798843384
translation,33,67,results,remarkable performance,on,drop dataset,remarkable performance on drop dataset,0.5007785558700562
translation,33,67,results,results,indicate,qdgat,results indicate qdgat,0.5842127203941345
translation,33,211,results,traditional mrc methods bidaf and qanet,achieve,slightly better performance,traditional mrc methods bidaf and qanet achieve slightly better performance,0.6183850765228271
translation,33,211,results,slightly better performance,far from,satisfying,slightly better performance far from satisfying,0.7220963835716248
translation,33,211,results,results,has,traditional mrc methods bidaf and qanet,results has traditional mrc methods bidaf and qanet,0.5157696008682251
translation,33,214,results,qdgat,achieving,86.38 f1 score,qdgat achieving 86.38 f1 score,0.6338839530944824
translation,33,214,results,qdgat,achieving,83.23 em,qdgat achieving 83.23 em,0.652642011642456
translation,33,214,results,qdgat,narrows,human performance gap,qdgat narrows human performance gap,0.7860113978385925
translation,33,214,results,all the existing methods,on,test set,all the existing methods on test set,0.5056754946708679
translation,33,214,results,83.23 em,on,test set,83.23 em on test set,0.5459129214286804
translation,33,214,results,human performance gap,to,less than 11 points,human performance gap to less than 11 points,0.5580061078071594
translation,33,214,results,method,has,qdgat,method has qdgat,0.6202827095985413
translation,33,214,results,qdgat,has,outperforms,qdgat has outperforms,0.6567119359970093
translation,33,214,results,outperforms,has,all the existing methods,outperforms has all the existing methods,0.572834312915802
translation,33,214,results,results,has,method,results has method,0.49327942728996277
translation,33,214,results,results,has,qdgat,results has qdgat,0.5686269402503967
translation,33,218,results,effectiveness,of,qdgat,effectiveness of qdgat,0.657867968082428
translation,33,218,results,outperforms,by,1.23,outperforms by 1.23,0.611792266368866
translation,33,218,results,outperforms,by,1.37,outperforms by 1.37,0.5975533723831177
translation,33,218,results,numnet +,by,1.23,numnet + by 1.23,0.6152816414833069
translation,33,218,results,numnet +,by,1.37,numnet + by 1.37,0.5951148867607117
translation,33,218,results,1.23,in terms of,em,1.23 in terms of em,0.683717668056488
translation,33,218,results,1.37,in terms of,f1 score,1.37 in terms of f1 score,0.6483179330825806
translation,33,218,results,outperforms,has,numnet +,outperforms has numnet +,0.6339229941368103
translation,33,218,results,results,demonstrate,effectiveness,results demonstrate effectiveness,0.5585970878601074
translation,33,219,results,three of our models,with,different random seeds and learning rates,three of our models with different random seeds and learning rates,0.6125595569610596
translation,33,219,results,different random seeds and learning rates,improves,performance,different random seeds and learning rates improves performance,0.6702404022216797
translation,33,219,results,ensembling,has,three of our models,ensembling has three of our models,0.5777814388275146
translation,33,219,results,results,has,ensembling,results has ensembling,0.5693673491477966
translation,33,223,results,qdgat nq,performs,worse,qdgat nq performs worse,0.6497873067855835
translation,33,223,results,results,observe that,qdgat nq,results observe that qdgat nq,0.6147691607475281
translation,33,225,results,qdgat nh,performs,significantly worse,qdgat nh performs significantly worse,0.6951307654380798
translation,33,225,results,results,observe,qdgat nh,results observe qdgat nh,0.5766072273254395
translation,33,228,results,qdgat,works,better,qdgat works better,0.6062625646591187
translation,33,228,results,better,on,questions,better on questions,0.5401336550712585
translation,33,228,results,better,requires,more specific numerical reasoning,better requires more specific numerical reasoning,0.6761822700500488
translation,33,228,results,questions,relating to,numbers and dates,questions relating to numbers and dates,0.6276123523712158
translation,33,228,results,questions,requires,more specific numerical reasoning,questions requires more specific numerical reasoning,0.6544598340988159
translation,33,228,results,more specific numerical reasoning,compared with,span extraction,more specific numerical reasoning compared with span extraction,0.5965088605880737
translation,33,228,results,results,has,qdgat,results has qdgat,0.5686269402503967
translation,33,230,results,performance,in,span extraction,performance in span extraction,0.5239306688308716
translation,33,230,results,performance,be,improved,performance be improved,0.5476747751235962
translation,33,230,results,span extraction,be,improved,span extraction be improved,0.5422291159629822
translation,33,230,results,improved,by,our method,improved by our method,0.5679908394813538
translation,33,230,results,results,has,performance,results has performance,0.5972660779953003
translation,33,239,results,accuracy,of,"numnet+ , qdgat","accuracy of numnet+ , qdgat",0.575707733631134
translation,33,239,results,accuracy,of,ablation variants,accuracy of ablation variants,0.6117407083511353
translation,33,239,results,ablation variants,on,racenum,ablation variants on racenum,0.5602170825004578
translation,33,239,results,results,has,accuracy,results has accuracy,0.5888755321502686
translation,33,241,results,qdgat,achieves,cases,qdgat achieves cases,0.6705499291419983
translation,33,241,results,cases,from,drop dataset,cases from drop dataset,0.5536136627197266
translation,33,241,results,results,has,qdgat,results has qdgat,0.5686269402503967
translation,33,247,results,outperform,by,2 - 3 points margin,outperform by 2 - 3 points margin,0.6604411005973816
translation,33,247,results,numnet +,by,2 - 3 points margin,numnet + by 2 - 3 points margin,0.619768500328064
translation,33,247,results,outperform,has,numnet +,outperform has numnet +,0.6479766368865967
translation,33,247,results,results,has,qdgat nq and qdgat nh,results has qdgat nq and qdgat nh,0.5774010419845581
translation,33,250,results,qdgat nh,removes,number type and entity mentions,qdgat nh removes number type and entity mentions,0.6977880597114563
translation,33,250,results,qdgat nh,performs,consistently worse,qdgat nh performs consistently worse,0.6742808818817139
translation,33,250,results,number type and entity mentions,from,graph,number type and entity mentions from graph,0.5761853456497192
translation,33,250,results,consistently worse,than,qdgat,consistently worse than qdgat,0.6431750655174255
translation,33,250,results,results,For,qdgat nh,results For qdgat nh,0.6182202696800232
translation,34,10,results,fill - in- the - blank ( cloze ) questions,from,generic source materials,fill - in- the - blank ( cloze ) questions from generic source materials,0.5304495692253113
translation,35,81,results,question q10,obtained,better precision,question q10 obtained better precision,0.6889344453811646
translation,35,81,results,question q10,obtained,f-measure,question q10 obtained f-measure,0.6739493608474731
translation,35,81,results,results,has,question q10,results has question q10,0.5468391180038452
translation,35,82,results,0.38,of,recall,0.38 of recall,0.626114547252655
translation,35,82,results,0.38,is,very competitive result,0.38 is very competitive result,0.5327326655387878
translation,35,82,results,very competitive result,for,current state - of- art,very competitive result for current state - of- art,0.5584968328475952
translation,35,82,results,results,has,system,results has system,0.5883707404136658
translation,36,139,hyperparameters,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,36,139,hyperparameters,1e - 3,for,policy network,1e - 3 for policy network,0.6562402844429016
translation,36,139,hyperparameters,1e - 2,for,both rewardnet and value network,1e - 2 for both rewardnet and value network,0.6258541941642761
translation,36,139,hyperparameters,learning rate,has,1e - 3,learning rate has 1e - 3,0.5560299754142761
translation,36,139,hyperparameters,learning rate,has,1e - 2,learning rate has 1e - 2,0.540254533290863
translation,36,139,hyperparameters,hyperparameters,use,adam optimizer,hyperparameters use adam optimizer,0.6090166568756104
translation,36,140,hyperparameters,discounted factor,for calculating,long-term return,discounted factor for calculating long-term return,0.662294328212738
translation,36,140,hyperparameters,long-term return,is,0.99,long-term return is 0.99,0.51971435546875
translation,36,140,hyperparameters,hyperparameters,has,discounted factor,hyperparameters has discounted factor,0.5446531176567078
translation,36,9,model,novel policy - based reinforcement learning ( rl ) method,enables,questioner agent,novel policy - based reinforcement learning ( rl ) method enables questioner agent,0.6776849627494812
translation,36,9,model,questioner agent,to learn,optimal policy,questioner agent to learn optimal policy,0.5924544930458069
translation,36,9,model,optimal policy,of,question selection,optimal policy of question selection,0.5794681906700134
translation,36,9,model,question selection,through,continuous interactions,question selection through continuous interactions,0.6575818657875061
translation,36,9,model,continuous interactions,with,users,continuous interactions with users,0.6906536221504211
translation,36,9,model,model,propose,novel policy - based reinforcement learning ( rl ) method,model propose novel policy - based reinforcement learning ( rl ) method,0.6904051899909973
translation,36,10,model,training,propose to use,reward network,training propose to use reward network,0.729894757270813
translation,36,10,model,reward network,to estimate,more informative reward,reward network to estimate more informative reward,0.6385384798049927
translation,36,10,model,model,To facilitate,training,model To facilitate training,0.7183519005775452
translation,36,27,model,process of question selction,in,game,process of question selction in game,0.48964130878448486
translation,36,27,model,process of question selction,as,markov decision process ( mdp ),process of question selction as markov decision process ( mdp ),0.5517096519470215
translation,36,27,model,novel policy - based rl framework,to learn,optimal policy,novel policy - based rl framework to learn optimal policy,0.5774635076522827
translation,36,27,model,optimal policy,of,question selection,optimal policy of question selection,0.5794681906700134
translation,36,27,model,question selection,in,q20 game,question selection in q20 game,0.5190165638923645
translation,36,27,model,model,formulate,process of question selction,model formulate process of question selction,0.6561077237129211
translation,36,27,model,model,propose,novel policy - based rl framework,model propose novel policy - based rl framework,0.6652741432189941
translation,36,28,model,questioner agent,maintains,probability distribution,questioner agent maintains probability distribution,0.6506668329238892
translation,36,28,model,questioner agent,updates,confidence,questioner agent updates confidence,0.7411962747573853
translation,36,28,model,probability distribution,over,all objects,probability distribution over all objects,0.6744248270988464
translation,36,28,model,probability distribution,to model,confidence,probability distribution to model confidence,0.7350537180900574
translation,36,28,model,probability distribution,to model,confidence,probability distribution to model confidence,0.7350537180900574
translation,36,28,model,confidence,of,target object,confidence of target object,0.5727791786193848
translation,36,28,model,confidence,based on,answers,confidence based on answers,0.695166289806366
translation,36,28,model,confidence,based on,answers,confidence based on answers,0.695166289806366
translation,36,28,model,answers,from,user,answers from user,0.5671762824058533
translation,36,28,model,model,has,questioner agent,model has questioner agent,0.5633095502853394
translation,36,210,model,policy - based rl model,acts as,questioner,policy - based rl model acts as questioner,0.6655407547950745
translation,36,210,model,policy - based rl model,exhibits,superior performance,policy - based rl model exhibits superior performance,0.6581562757492065
translation,36,210,model,questioner,in,q20 game,questioner in q20 game,0.5469632148742676
translation,36,210,model,model,propose,policy - based rl model,model propose policy - based rl model,0.6328750252723694
translation,36,215,model,policy - based rl method,to solve,question selection problem,policy - based rl method to solve question selection problem,0.6601521372795105
translation,36,215,model,question selection problem,in,q20 game,question selection problem in q20 game,0.5109661817550659
translation,36,215,model,model,propose,policy - based rl method,model propose policy - based rl method,0.6627053618431091
translation,36,216,model,object - aware rewardnet,to estimate,appropriate non-zero reward,object - aware rewardnet to estimate appropriate non-zero reward,0.6905149817466736
translation,36,216,model,appropriate non-zero reward,make,long-time return,appropriate non-zero reward make long-time return,0.6310995817184448
translation,36,216,model,long-time return,has,more informative,long-time return has more informative,0.5467724204063416
translation,36,236,model,question,following,question probability distribution ?,question following question probability distribution ?,0.6423038840293884
translation,36,236,model,our agent,to jump out,local optimum,our agent to jump out local optimum,0.7132294774055481
translation,36,236,model,local optimum,caused by,incorrect answers,local optimum caused by incorrect answers,0.7036657333374023
translation,36,33,results,ability to sample questions,compared to,greedy selection,ability to sample questions compared to greedy selection,0.6766218543052673
translation,36,33,results,ability to sample questions,improves,diversity,ability to sample questions improves diversity,0.7193021774291992
translation,36,33,results,diversity,of,questions,diversity of questions,0.5705689787864685
translation,36,33,results,questions,asked by,our agent,questions asked by our agent,0.754902184009552
translation,36,33,results,results,has,ability to sample questions,results has ability to sample questions,0.5353075265884399
translation,36,162,results,rl uniform,achieves,win rate,rl uniform achieves win rate,0.6566573977470398
translation,36,162,results,win rate,very close to,entropymodel,win rate very close to entropymodel,0.6959989070892334
translation,36,162,results,94 %,very close to,entropymodel,94 % very close to entropymodel,0.732358992099762
translation,36,162,results,win rate,has,94 %,win rate has 94 %,0.5753482580184937
translation,36,162,results,results,has,rl uniform,results has rl uniform,0.6396914720535278
translation,36,164,results,rl uniform,competitive to,entropymodel,rl uniform competitive to entropymodel,0.6437706351280212
translation,36,164,results,entropymodel,in,noise-free simulation environment,entropymodel in noise-free simulation environment,0.5331721901893616
translation,36,164,results,our rl method,is,very cost-effective,our rl method is very cost-effective,0.553389847278595
translation,36,164,results,our rl method,makes use of,user data,our rl method makes use of user data,0.6450201869010925
translation,36,164,results,very cost-effective,makes use of,user data,very cost-effective makes use of user data,0.6652499437332153
translation,36,164,results,user data,has,more efficiently,user data has more efficiently,0.5576686859130859
translation,36,178,results,rewardnet and objectrewardnet,achieve,better performance,rewardnet and objectrewardnet achieve better performance,0.612644374370575
translation,36,178,results,better performance,with,win rate,better performance with win rate,0.6288174986839294
translation,36,178,results,win rate,of,94 %,win rate of 94 %,0.595899760723114
translation,36,178,results,results,has,rewardnet and objectrewardnet,results has rewardnet and objectrewardnet,0.522758424282074
translation,36,180,results,objectrewardnet,learns,faster,objectrewardnet learns faster,0.7308136224746704
translation,36,180,results,faster,than,rewardnet,faster than rewardnet,0.6246676445007324
translation,36,180,results,faster,in,early steps,faster in early steps,0.5634251236915588
translation,36,180,results,results,see that,objectrewardnet,results see that objectrewardnet,0.6115740537643433
translation,36,186,results,unifsimulator,achieves,win rate,unifsimulator achieves win rate,0.6759825944900513
translation,36,186,results,win rate,of,80 %,win rate of 80 %,0.6113629341125488
translation,36,186,results,80 %,with,14 questions,80 % with 14 questions,0.6872962713241577
translation,36,186,results,14 questions,in,both settings,14 questions in both settings,0.5401471853256226
translation,36,186,results,results,see that,unifsimulator,results see that unifsimulator,0.6515151858329773
translation,36,217,results,rl method,is,more robust,rl method is more robust,0.5834262371063232
translation,36,217,results,more robust,to,answer noise,more robust to answer noise,0.5938630104064941
translation,36,217,results,answer noise,common in,real-world q20 game,answer noise common in real-world q20 game,0.6835545301437378
translation,36,232,results,entropymodel,by about,4.5 %,entropymodel by about 4.5 %,0.6334140300750732
translation,36,232,results,entropymodel,on,win rate,entropymodel on win rate,0.5206522345542908
translation,36,232,results,4.5 %,on,win rate,4.5 % on win rate,0.5400352478027344
translation,36,232,results,win rate,in,real-world q20 games,win rate in real-world q20 games,0.535201907157898
translation,36,232,results,rl uniform,has,outperforms,rl uniform has outperforms,0.6222827434539795
translation,36,232,results,outperforms,has,entropymodel,outperforms has entropymodel,0.6010470986366272
translation,36,232,results,results,has,rl uniform,results has rl uniform,0.6396914720535278
translation,37,227,ablation-analysis,ablation analysis,has,praline : improvements and ablation,ablation analysis has praline : improvements and ablation,0.593666136264801
translation,37,235,ablation-analysis,only one kb rule,dropping,fup clauses,only one kb rule dropping fup clauses,0.7319696545600891
translation,37,235,ablation-analysis,bigger influence,dropping,acyclic constraint clauses,bigger influence dropping acyclic constraint clauses,0.7509475946426392
translation,37,235,ablation-analysis,fup clauses,has,bigger influence,fup clauses has bigger influence,0.6009415984153748
translation,37,235,ablation-analysis,ablation analysis,when using,only one kb rule,ablation analysis when using only one kb rule,0.7383474707603455
translation,37,11,baselines,mlns,to align,lexical elements,mlns to align lexical elements,0.6155472993850708
translation,37,11,baselines,mlns,define and control,inference,mlns define and control inference,0.6766029000282288
translation,37,79,baselines,structure,imposed by,hard constraints,structure imposed by hard constraints,0.7344478368759155
translation,37,79,baselines,structure,to vastly simplify,groundings,structure to vastly simplify groundings,0.6828206777572632
translation,37,79,baselines,structure,bring them,realm,structure bring them realm,0.6055574417114258
translation,37,79,baselines,hard constraints,to vastly simplify,groundings,hard constraints to vastly simplify groundings,0.6651211380958557
translation,37,79,baselines,realm,of,feasibility,realm of feasibility,0.6246358156204224
translation,37,207,experimental-setup,marginal inference,performed using,"mc - sat ( poon and domingos , 2006 )","marginal inference performed using mc - sat ( poon and domingos , 2006 )",0.6177730560302734
translation,37,207,experimental-setup,"mc - sat ( poon and domingos , 2006 )",with,default parameters,"mc - sat ( poon and domingos , 2006 ) with default parameters",0.5802220106124878
translation,37,207,experimental-setup,500 samples,for,marginal estimation,500 samples for marginal estimation,0.5605149865150452
translation,37,207,experimental-setup,experimental setup,has,marginal inference,experimental setup has marginal inference,0.5054212212562561
translation,37,208,experimental-setup,2 - core 2.5 ghz amazon ec2 linux machine,with,16 gb ram,2 - core 2.5 ghz amazon ec2 linux machine with 16 gb ram,0.5544711351394653
translation,37,208,experimental-setup,experimental setup,used,2 - core 2.5 ghz amazon ec2 linux machine,experimental setup used 2 - core 2.5 ghz amazon ec2 linux machine,0.5632575750350952
translation,37,5,model,system,reasons with,knowledge,system reasons with knowledge,0.7238224744796753
translation,37,5,model,knowledge,derived from,textbooks,knowledge derived from textbooks,0.7077340483665466
translation,37,5,model,knowledge,represented in,subset of firstorder logic,knowledge represented in subset of firstorder logic,0.6384070515632629
translation,37,5,model,model,develop,system,model develop system,0.696818470954895
translation,37,29,model,natural formulation,is,intuitive,natural formulation is intuitive,0.5488862991333008
translation,37,29,model,natural formulation,suffers from,inefficient in-ference,natural formulation suffers from inefficient in-ference,0.7483692765235901
translation,37,29,model,extension,improves,efficiency,extension improves efficiency,0.6903234720230103
translation,37,29,model,extension,brittle to,variation,extension brittle to variation,0.7150208353996277
translation,37,29,model,efficiency,by using,prototypical constants,efficiency by using prototypical constants,0.6117427349090576
translation,37,29,model,variation,in,text and structure,variation in text and structure,0.561779797077179
translation,37,29,model,formulation,with,improved flexibility,formulation with improved flexibility,0.6472803354263306
translation,37,29,model,improved flexibility,to handle,variation,improved flexibility to handle variation,0.6645098328590393
translation,37,29,model,improved flexibility,that is,15 % more accurate,improved flexibility that is 15 % more accurate,0.611316442489624
translation,37,29,model,improved flexibility,that is,10x faster,improved flexibility that is 10x faster,0.627757728099823
translation,37,29,model,variation,in,text and structure,variation in text and structure,0.561779797077179
translation,37,29,model,10x faster,than,other approaches,10x faster than other approaches,0.5706825852394104
translation,37,29,model,three mln - based formulations,has,natural formulation,three mln - based formulations has natural formulation,0.5717854499816895
translation,37,29,model,three mln - based formulations,has,extension,three mln - based formulations has extension,0.5673121809959412
translation,37,29,model,three mln - based formulations,has,formulation,three mln - based formulations has formulation,0.5482271909713745
translation,37,29,model,model,investigate,three mln - based formulations,model investigate three mln - based formulations,0.6498076319694519
translation,37,250,model,alignment based solution,that is both,efficient and accurate,alignment based solution that is both efficient and accurate,0.6029781699180603
translation,37,250,model,praline,has,alignment based solution,praline has alignment based solution,0.593318521976471
translation,37,250,model,model,proposed,praline,model proposed praline,0.7510374784469604
translation,37,12,results,praline,demonstrates,15 % accuracy boost,praline demonstrates 15 % accuracy boost,0.6791426539421082
translation,37,12,results,praline,demonstrates,10x reduction,praline demonstrates 10x reduction,0.7124794125556946
translation,37,12,results,praline,demonstrates,comparable accuracy,praline demonstrates comparable accuracy,0.6967464685440063
translation,37,12,results,10x reduction,in,runtime,10x reduction in runtime,0.5076063275337219
translation,37,12,results,runtime,compared to,other mlnbased methods,runtime compared to other mlnbased methods,0.5852447152137756
translation,37,12,results,comparable accuracy,to,word - based baseline approaches,comparable accuracy to word - based baseline approaches,0.5416827201843262
translation,37,12,results,results,has,praline,results has praline,0.5704434514045715
translation,37,224,results,praline,resulted in,10x speedup,praline resulted in 10x speedup,0.6101324558258057
translation,37,224,results,10x speedup,over,er - mln,10x speedup over er - mln,0.7023885846138
translation,37,224,results,results,has,praline,results has praline,0.5704434514045715
translation,37,225,results,exam performance,by,roughly 15 %,exam performance by roughly 15 %,0.5947327017784119
translation,37,225,results,exam performance,pushing it up to,48.8 %,exam performance pushing it up to 48.8 %,0.6021510362625122
translation,37,225,results,exam performance,pushing it up to,46.3 %,exam performance pushing it up to 46.3 %,0.6049143075942993
translation,37,225,results,48.8 %,on,dev-108,48.8 % on dev-108,0.5767577290534973
translation,37,225,results,46.3 %,on,unseen - 68,46.3 % on unseen - 68,0.5722625851631165
translation,38,157,ablation-analysis,improvements,of,candidate generation step,improvements of candidate generation step,0.5673285126686096
translation,38,157,ablation-analysis,improvements,show,positive effect,improvements show positive effect,0.6301149129867554
translation,38,157,ablation-analysis,ablation analysis,has,improvements,ablation analysis has improvements,0.5440875291824341
translation,38,158,ablation-analysis,f 1 of compq,slightly drops from,42.84 to 42.37,f 1 of compq slightly drops from 42.84 to 42.37,0.7202844619750977
translation,38,158,ablation-analysis,implicit type filtering,has,f 1 of compq,implicit type filtering has f 1 of compq,0.6366733312606812
translation,38,158,ablation-analysis,time interval constraints,has,f 1 of compq,time interval constraints has f 1 of compq,0.6046693325042725
translation,38,158,ablation-analysis,ablation analysis,remove,implicit type filtering,ablation analysis remove implicit type filtering,0.6508899331092834
translation,38,187,ablation-analysis,ablation analysis,on,compq and webq,ablation analysis on compq and webq,0.589098334312439
translation,38,188,ablation-analysis,dependency path information,augmented with,sentential information,dependency path information augmented with sentential information,0.7166739106178284
translation,38,188,ablation-analysis,performance,boosts,0.42,performance boosts 0.42,0.7135053873062134
translation,38,188,ablation-analysis,dependency path information,has,performance,dependency path information has performance,0.5657680630683899
translation,38,188,ablation-analysis,sentential information,has,performance,sentential information has performance,0.5601743459701538
translation,38,188,ablation-analysis,ablation analysis,When,dependency path information,ablation analysis When dependency path information,0.6322764158248901
translation,38,44,baselines,more effective candidate generation strategy,takes advantage of,implicit type information,more effective candidate generation strategy takes advantage of implicit type information,0.6180330514907837
translation,38,44,baselines,implicit type information,in,query graphs and time interval information,implicit type information in query graphs and time interval information,0.4692474603652954
translation,38,44,baselines,query graphs and time interval information,in,kb,query graphs and time interval information in kb,0.5161533355712891
translation,38,138,experiments,knowledge base,with,virtuoso engine,knowledge base with virtuoso engine,0.5880481004714966
translation,38,138,experiments,simpq,has,knowledge base,simpq has knowledge base,0.586239755153656
translation,38,140,hyperparameters,word embeddings,using,glove,word embeddings using glove,0.6562255620956421
translation,38,140,hyperparameters,) word vectors,with,dimensions,) word vectors with dimensions,0.682399332523346
translation,38,140,hyperparameters,dimensions,set to,300,dimensions set to 300,0.7613283395767212
translation,38,140,hyperparameters,dimensions,set to,300,dimensions set to 300,0.7613283395767212
translation,38,140,hyperparameters,dimensions,set to,300,dimensions set to 300,0.7613283395767212
translation,38,140,hyperparameters,size,of,bi-gru hidden layer,size of bi-gru hidden layer,0.5927929878234863
translation,38,140,hyperparameters,size,set to,300,size set to 300,0.7791393995285034
translation,38,140,hyperparameters,bi-gru hidden layer,set to,300,bi-gru hidden layer set to 300,0.72996985912323
translation,38,140,hyperparameters,glove,has,) word vectors,glove has ) word vectors,0.5926128029823303
translation,38,140,hyperparameters,hyperparameters,initialize,word embeddings,hyperparameters initialize word embeddings,0.6877135038375854
translation,38,141,hyperparameters,margin,in,"{ 0.1 , 0.2 , 0.5 }","margin in { 0.1 , 0.2 , 0.5 }",0.5785832405090332
translation,38,141,hyperparameters,ensemble threshold k,in,"2 , 3 , 5 , 10 , + inf }","ensemble threshold k in 2 , 3 , 5 , 10 , + inf }",0.5693095922470093
translation,38,141,hyperparameters,batch size b,in,"{ 16 , 32 , 64 }","batch size b in { 16 , 32 , 64 }",0.5424664616584778
translation,38,141,hyperparameters,hyperparameters,tune,margin,hyperparameters tune margin,0.7583731412887573
translation,38,141,hyperparameters,hyperparameters,tune,batch size b,hyperparameters tune batch size b,0.6895495057106018
translation,38,6,model,complex query structure,into,uniform vector representation,complex query structure into uniform vector representation,0.5707253813743591
translation,38,6,model,interactions,between,individual semantic components,interactions between individual semantic components,0.6592580676078796
translation,38,6,model,individual semantic components,within,complex question,individual semantic components within complex question,0.6919920444488525
translation,38,6,model,model,encode,complex query structure,model encode complex query structure,0.757240891456604
translation,38,31,model,neural network based approach,to improve,performance,neural network based approach to improve performance,0.7174279093742371
translation,38,31,model,performance,of,semantic similarity measurement,performance of semantic similarity measurement,0.5729197859764099
translation,38,31,model,semantic similarity measurement,in,complex question answering,semantic similarity measurement in complex question answering,0.4656921327114105
translation,38,31,model,model,propose,neural network based approach,model propose neural network based approach,0.7190003991127014
translation,38,32,model,candidate query graphs,generated from,one question,candidate query graphs generated from one question,0.6201491355895996
translation,38,32,model,our model,embeds,question surface and predicate sequences,our model embeds question surface and predicate sequences,0.6831464767456055
translation,38,32,model,question surface and predicate sequences,into,uniform vector space,question surface and predicate sequences into uniform vector space,0.6106898784637451
translation,38,32,model,candidate query graphs,has,our model,candidate query graphs has our model,0.5547157526016235
translation,38,32,model,one question,has,our model,one question has our model,0.6035975813865662
translation,38,32,model,model,Given,candidate query graphs,model Given candidate query graphs,0.6895825862884521
translation,38,116,model,large lexicon,by collecting,"all ( mention , entity ) pairs","large lexicon by collecting all ( mention , entity ) pairs",0.6932551860809326
translation,38,116,model,"all ( mention , entity ) pairs",from,"article titles , anchor texts","all ( mention , entity ) pairs from article titles , anchor texts",0.5338162183761597
translation,38,116,model,"all ( mention , entity ) pairs",from,redirects and disambiguation pages,"all ( mention , entity ) pairs from redirects and disambiguation pages",0.5728006958961487
translation,38,116,model,redirects and disambiguation pages,of,wikipedia,redirects and disambiguation pages of wikipedia,0.5896302461624146
translation,38,116,model,model,build,large lexicon,model build large lexicon,0.7600939869880676
translation,38,151,results,our approach,ranks,2nd,our approach ranks 2nd,0.7138102650642395
translation,38,151,results,outperforms,on,compq dataset,outperforms on compq dataset,0.5587677359580994
translation,38,151,results,existing approaches,on,compq dataset,existing approaches on compq dataset,0.5267712473869324
translation,38,151,results,2nd,on,webq,2nd on webq,0.650769054889679
translation,38,151,results,2nd,among,long list of state - of - the - art works,2nd among long list of state - of - the - art works,0.5776757001876831
translation,38,151,results,our approach,has,outperforms,our approach has outperforms,0.6385829448699951
translation,38,151,results,outperforms,has,existing approaches,outperforms has existing approaches,0.5970985293388367
translation,38,151,results,results,ranks,2nd,results ranks 2nd,0.7234723567962646
translation,38,151,results,results,has,our approach,results has our approach,0.6050099730491638
translation,38,155,results,entity enrichment method,improves,results,entity enrichment method improves results,0.6235967874526978
translation,38,155,results,results,on,both datasets,results on both datasets,0.49870386719703674
translation,38,155,results,both datasets,by,large margin ( 0.8 ),both datasets by large margin ( 0.8 ),0.5671888589859009
translation,38,155,results,results,show,entity enrichment method,results show entity enrichment method,0.5908653140068054
translation,38,165,results,semantic matching model,performs,slightly below,semantic matching model performs slightly below,0.5852083563804626
translation,38,165,results,slightly below,has,systems,slightly below has systems,0.6432853937149048
translation,38,165,results,results,Our,semantic matching model,results Our semantic matching model,0.6103726625442505
translation,38,165,results,results,has,semantic matching model,results has semantic matching model,0.5591393113136292
translation,38,180,results,works better,than,average embedding,works better than average embedding,0.5962696075439453
translation,38,180,results,f 1,by,0.65,f 1 by 0.65,0.6051234602928162
translation,38,180,results,f 1,on,both datasets,f 1 on both datasets,0.5741487145423889
translation,38,180,results,0.65,on,both datasets,0.65 on both datasets,0.48839080333709717
translation,38,180,results,id sequences,has,p athemb,id sequences has p athemb,0.6033992767333984
translation,38,180,results,p athemb,has,works better,p athemb has works better,0.6354234218597412
translation,38,180,results,boosting,has,f 1,boosting has f 1,0.6498192548751831
translation,38,182,results,average word embedding method,outperforms,bigru,average word embedding method outperforms bigru,0.7560733556747437
translation,38,182,results,bigru,on,compq,bigru on compq,0.6059052348136902
translation,38,182,results,gap,becomes,smaller,gap becomes smaller,0.7087672352790833
translation,38,182,results,smaller,when running on,webq,smaller when running on webq,0.7410848736763
translation,38,182,results,encoding of word sequences,has,average word embedding method,encoding of word sequences has average word embedding method,0.5798393487930298
translation,38,182,results,results,For,encoding of word sequences,results For encoding of word sequences,0.6024951338768005
translation,38,192,results,semantic composition,has,our max pooling based method,semantic composition has our max pooling based method,0.5660398006439209
translation,38,192,results,our max pooling based method,has,consistently outperforms,our max pooling based method has consistently outperforms,0.5937710404396057
translation,38,192,results,consistently outperforms,has,baseline method,consistently outperforms has baseline method,0.5930160284042358
translation,38,192,results,results,In terms of,semantic composition,results In terms of semantic composition,0.7005059719085693
translation,38,194,results,vanilla sp +nn approach,on,compq,vanilla sp +nn approach on compq,0.5950421094894409
translation,38,194,results,compq,by,1.28,compq by 1.28,0.6093356013298035
translation,38,194,results,significantly outperforms,has,vanilla sp +nn approach,significantly outperforms has vanilla sp +nn approach,0.6063570380210876
translation,39,5,results,system,is,syntax - based,system is syntax - based,0.5915229916572571
translation,39,5,results,system,runs on,dependency parse information,system runs on dependency parse information,0.7106579542160034
translation,39,5,results,system,achieves,high accuracy,system achieves high accuracy,0.7084859609603882
translation,39,5,results,dependency parse information,of,single-sentence input,dependency parse information of single-sentence input,0.547639012336731
translation,39,5,results,high accuracy,in terms of,syntactic correctness,high accuracy in terms of syntactic correctness,0.6193174123764038
translation,39,5,results,high accuracy,in terms of,semantic adequacy,high accuracy in terms of semantic adequacy,0.6333787441253662
translation,39,5,results,high accuracy,in terms of,fluency,high accuracy in terms of fluency,0.6064260005950928
translation,39,5,results,high accuracy,in terms of,uniqueness,high accuracy in terms of uniqueness,0.6091143488883972
translation,39,5,results,results,has,system,results has system,0.5883707404136658
translation,39,102,results,our system,performs,better,our system performs better,0.6538035869598389
translation,39,102,results,better,than,heilman 's state - of - the - art rule based system,better than heilman 's state - of - the - art rule based system,0.5885652303695679
translation,39,102,results,better,while generating questions,complex english sentences,better while generating questions complex english sentences,0.7527011036872864
translation,39,102,results,heilman 's state - of - the - art rule based system,while generating questions,complex english sentences,heilman 's state - of - the - art rule based system while generating questions complex english sentences,0.7716791033744812
translation,40,10,ablation-analysis,relative importance,of,semantic similarity,relative importance of semantic similarity,0.5704834461212158
translation,40,10,ablation-analysis,relative importance,of,word level relevance matching,relative importance of word level relevance matching,0.5671319365501404
translation,40,10,ablation-analysis,word level relevance matching,in,open-domain qa,word level relevance matching in open-domain qa,0.5121250152587891
translation,40,10,ablation-analysis,ablation analysis,analyze,relative importance,ablation analysis analyze relative importance,0.6754845380783081
translation,40,109,baselines,four models,with,publicly available results,four models with publicly available results,0.5613416433334351
translation,40,109,baselines,publicly available results,for,quasar -t dataset,publicly available results for quasar -t dataset,0.5473610162734985
translation,40,110,baselines,r 3,jointly trains,reader,r 3 jointly trains reader,0.7457196712493896
translation,40,110,baselines,reader,using,supervised learning,reader using supervised learning,0.7153317928314209
translation,40,110,baselines,ga,has,gated attention reader,ga has gated attention reader,0.6298946142196655
translation,40,110,baselines,bidaf,has,bidirectional attention flow,bidaf has bidirectional attention flow,0.5697217583656311
translation,40,110,baselines,baselines,has,ga,baselines has ga,0.6055818200111389
translation,40,130,experiments,both the infersent ranker and the rn ranker,using,stochastic gradient descent,both the infersent ranker and the rn ranker using stochastic gradient descent,0.6426949501037598
translation,40,130,experiments,stochastic gradient descent,with,adam optimizer,stochastic gradient descent with adam optimizer,0.5865907669067383
translation,40,111,hyperparameters,each infersent embedding,has,4096 dimensions,each infersent embedding has 4096 dimensions,0.5906493067741394
translation,40,112,hyperparameters,input feature vector,to,our infersent ranker,input feature vector to our infersent ranker,0.5361001491546631
translation,40,112,hyperparameters,our infersent ranker,has,16384 dimensions,our infersent ranker has 16384 dimensions,0.6372706890106201
translation,40,112,hyperparameters,our infersent ranker,has,16384 dimensions,our infersent ranker has 16384 dimensions,0.6372706890106201
translation,40,112,hyperparameters,hyperparameters,has,input feature vector,hyperparameters has input feature vector,0.49348485469818115
translation,40,113,hyperparameters,dimensions,of,two linear layers,dimensions of two linear layers,0.610525906085968
translation,40,113,hyperparameters,two linear layers,are,500 and 1,two linear layers are 500 and 1,0.618891179561615
translation,40,113,hyperparameters,hyperparameters,has,dimensions,hyperparameters has dimensions,0.5247315764427185
translation,40,114,hyperparameters,g ? and f ?,are,three layer feed -forward neural networks,g ? and f ? are three layer feed -forward neural networks,0.6252476572990417
translation,40,114,hyperparameters,three layer feed -forward neural networks,with,"( 300 , 300 , 5 ) and ( 5 , 5 , 1 ) units","three layer feed -forward neural networks with ( 300 , 300 , 5 ) and ( 5 , 5 , 1 ) units",0.6222338080406189
translation,40,114,hyperparameters,relation - networks ( rn ),has,g ? and f ?,relation - networks ( rn ) has g ? and f ?,0.6013516783714294
translation,40,9,model,two neural network rankers,assign,scores,two neural network rankers assign scores,0.6692622303962708
translation,40,9,model,scores,to,different passages,scores to different passages,0.5490118265151978
translation,40,9,model,scores,based on,likelihood,scores based on likelihood,0.7561919093132019
translation,40,9,model,likelihood,of containing,answer,likelihood of containing answer,0.7042012810707092
translation,40,9,model,answer,to,given question,answer to given question,0.6287168860435486
translation,40,9,model,model,propose,two neural network rankers,model propose two neural network rankers,0.7105628252029419
translation,40,31,model,local interactions,between,words,local interactions between words,0.7002540230751038
translation,40,31,model,words,in,document,words in document,0.5785806775093079
translation,40,31,model,words,question and words in,document,words question and words in document,0.7881144285202026
translation,40,33,results,ranker model,focuses on,relevance matching ( relation - networks ranker ),ranker model focuses on relevance matching ( relation - networks ranker ),0.7024168968200684
translation,40,33,results,ranker model,focuses on,semantic similarity ( infersent ranker ),ranker model focuses on semantic similarity ( infersent ranker ),0.7029784321784973
translation,40,33,results,ranker model,achieves,significantly higher retrieval recall,ranker model achieves significantly higher retrieval recall,0.7046757340431213
translation,40,33,results,ranker model,focuses on,semantic similarity ( infersent ranker ),ranker model focuses on semantic similarity ( infersent ranker ),0.7029784321784973
translation,40,33,results,relevance matching ( relation - networks ranker ),achieves,significantly higher retrieval recall,relevance matching ( relation - networks ranker ) achieves significantly higher retrieval recall,0.6699212193489075
translation,40,33,results,ranker model,focuses on,semantic similarity ( infersent ranker ),ranker model focuses on semantic similarity ( infersent ranker ),0.7029784321784973
translation,40,33,results,ranker model,has,better,ranker model has better,0.6121466159820557
translation,40,33,results,semantic similarity ( infersent ranker ),has,better,semantic similarity ( infersent ranker ) has better,0.5818096399307251
translation,40,33,results,semantic similarity ( infersent ranker ),has,better,semantic similarity ( infersent ranker ) has better,0.5818096399307251
translation,40,33,results,better,has,overall qa performance,better has overall qa performance,0.5642167329788208
translation,40,33,results,results,observe,ranker model,results observe ranker model,0.6142359375953674
translation,40,34,results,11.6 percent improvement,in,overall qa performance,11.6 percent improvement in overall qa performance,0.5176609754562378
translation,40,34,results,11.6 percent improvement,by integrating,infersent ranker,11.6 percent improvement by integrating infersent ranker,0.6418541073799133
translation,40,34,results,6.4 percent improvement,by,relation - networks ranker,6.4 percent improvement by relation - networks ranker,0.5727829933166504
translation,40,34,results,infersent ranker,has,6.4 percent improvement,infersent ranker has 6.4 percent improvement,0.5941995978355408
translation,40,34,results,results,achieve,11.6 percent improvement,results achieve 11.6 percent improvement,0.6245872378349304
translation,40,139,results,in-fersent,as,paragraph representation,in-fersent as paragraph representation,0.5582408905029297
translation,40,139,results,recall,in re-ranking,paragraphs,recall in re-ranking paragraphs,0.7360231280326843
translation,40,139,results,paragraphs,for,machine reading,paragraphs for machine reading,0.6554874181747437
translation,40,139,results,results,conclude,in-fersent,results conclude in-fersent,0.6686633825302124
translation,40,139,results,results,using,in-fersent,results using in-fersent,0.6900659203529358
translation,40,140,results,rn ranker,achieves,significantly higher recall,rn ranker achieves significantly higher recall,0.6956418752670288
translation,40,140,results,significantly higher recall,than,r 3 and infersent rankers,significantly higher recall than r 3 and infersent rankers,0.60650235414505
translation,40,140,results,results,has,rn ranker,results has rn ranker,0.5975526571273804
translation,40,145,results,overall qa performance,improves from,exact match accuracy,overall qa performance improves from exact match accuracy,0.6745650172233582
translation,40,145,results,exact match accuracy,of,19.7 to 31.2,exact match accuracy of 19.7 to 31.2,0.5337139368057251
translation,40,145,results,exact match accuracy,when,infersent ranker,exact match accuracy when infersent ranker,0.6115942597389221
translation,40,145,results,results,has,overall qa performance,results has overall qa performance,0.5384048223495483
translation,40,146,results,in- fersent ranker,much better than,rn ranker,in- fersent ranker much better than rn ranker,0.6405969262123108
translation,40,146,results,rn ranker,in terms of,overall qa performance,rn ranker in terms of overall qa performance,0.6911365389823914
translation,40,146,results,results,observe,in- fersent ranker,results observe in- fersent ranker,0.5838515758514404
translation,40,147,results,infersent ranker with drqa,provides,comparable result,infersent ranker with drqa provides comparable result,0.6673261523246765
translation,40,147,results,comparable result,to,sr 2,comparable result to sr 2,0.5936071872711182
translation,40,147,results,results,has,infersent ranker with drqa,results has infersent ranker with drqa,0.6025272607803345
translation,41,182,ablation-analysis,performance gain,over,fine - tuning,performance gain over fine - tuning,0.7216430902481079
translation,41,182,ablation-analysis,performance gain,is,decreasing,performance gain is decreasing,0.6069980263710022
translation,41,182,ablation-analysis,fine - tuning,is,decreasing,fine - tuning is decreasing,0.5818722248077393
translation,41,182,ablation-analysis,increase of the amount of training data,has,performance gain,increase of the amount of training data has performance gain,0.5496181845664978
translation,41,189,experiments,performance,of,mfh,performance of mfh,0.6408092975616455
translation,41,189,experiments,ban and pythia,is,"67.7 % , 69.08 % and 69.21 %","ban and pythia is 67.7 % , 69.08 % and 69.21 %",0.5781488418579102
translation,41,189,experiments,mfh,has,ban and pythia,mfh has ban and pythia,0.6583491563796997
translation,41,189,experiments,under-performing,has,proposed method,under-performing has proposed method,0.574493944644928
translation,41,153,hyperparameters,hidden dimension,of,gru,hidden dimension of gru,0.6294734477996826
translation,41,153,hyperparameters,hidden dimension,after,fusion,hidden dimension after fusion,0.7216005325317383
translation,41,153,hyperparameters,gru,to,1024,gru to 1024,0.6355388760566711
translation,41,153,hyperparameters,hidden dimension,after,fusion,hidden dimension after fusion,0.7216005325317383
translation,41,153,hyperparameters,fusion,to,4096,fusion to 4096,0.6604534983634949
translation,41,153,hyperparameters,hyperparameters,set,hidden dimension,hyperparameters set hidden dimension,0.6433765292167664
translation,41,153,hyperparameters,hyperparameters,set,hidden dimension,hyperparameters set hidden dimension,0.6433765292167664
translation,41,155,hyperparameters,training,apply,warm - up strategy,training apply warm - up strategy,0.6505923271179199
translation,41,155,hyperparameters,warm - up strategy,by gradually increasing,learning rate,warm - up strategy by gradually increasing learning rate,0.7393962144851685
translation,41,155,hyperparameters,learning rate,from,0.001 to 0.01,learning rate from 0.001 to 0.01,0.5118932723999023
translation,41,155,hyperparameters,0.001 to 0.01,in,first 2000 iterations,0.001 to 0.01 in first 2000 iterations,0.5303844213485718
translation,41,155,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
translation,41,156,hyperparameters,0.15,after,every 4000 iterations,0.15 after every 4000 iterations,0.6378008127212524
translation,41,157,hyperparameters,batch size,of,128,batch size of 128,0.6836684346199036
translation,41,160,hyperparameters,trade- off parameters,set as,"j = 0.025 , ? mm = 0.008 , ? v = 0.8 , ? q = 1 , ? c = 0.001 , and ? adv = 0.003","trade- off parameters set as j = 0.025 , ? mm = 0.008 , ? v = 0.8 , ? q = 1 , ? c = 0.001 , and ? adv = 0.003",0.6289100646972656
translation,41,160,hyperparameters,hyperparameters,has,trade- off parameters,hyperparameters has trade- off parameters,0.5024793148040771
translation,41,7,model,novel supervised multi-modal domain adaptation method,for,vqa,novel supervised multi-modal domain adaptation method for vqa,0.6029751300811768
translation,41,7,model,novel supervised multi-modal domain adaptation method,to learn,joint feature embeddings,novel supervised multi-modal domain adaptation method to learn joint feature embeddings,0.5633381009101868
translation,41,7,model,joint feature embeddings,across,different domains and modalities,joint feature embeddings across different domains and modalities,0.6832893490791321
translation,41,7,model,model,proposing,novel supervised multi-modal domain adaptation method,model proposing novel supervised multi-modal domain adaptation method,0.6849761009216309
translation,41,8,model,data distributions,of,source and target domains,data distributions of source and target domains,0.5495270490646362
translation,41,8,model,data distributions,by considering,modalities,data distributions by considering modalities,0.7115914225578308
translation,41,8,model,model,align,data distributions,model align data distributions,0.6632402539253235
translation,41,34,model,novel multi-modal domain adaptation framework,learns,multi-modal feature embedding,novel multi-modal domain adaptation framework learns multi-modal feature embedding,0.6460484266281128
translation,41,34,model,multi-modal feature embedding,simultaneously keeps,each domain invariant,multi-modal feature embedding simultaneously keeps each domain invariant,0.6531129479408264
translation,41,34,model,multi-modal feature embedding,simultaneously keeps,each individual modality discriminative,multi-modal feature embedding simultaneously keeps each individual modality discriminative,0.6679548621177673
translation,41,34,model,each individual modality discriminative,based on,adversarial loss,each individual modality discriminative based on adversarial loss,0.5790560841560364
translation,41,34,model,each individual modality discriminative,based on,classification loss,each individual modality discriminative based on classification loss,0.5985682606697083
translation,41,34,model,model,proposing,novel multi-modal domain adaptation framework,model proposing novel multi-modal domain adaptation framework,0.684146523475647
translation,41,35,model,maximum mean distance ( mmd ),to further reduce,domain mismatch,maximum mean distance ( mmd ) to further reduce domain mismatch,0.6711685061454773
translation,41,35,model,domain mismatch,by learning,embeddings,domain mismatch by learning embeddings,0.6805329322814941
translation,41,35,model,embeddings,from,different modalities,embeddings from different modalities,0.5802937746047974
translation,41,35,model,model,incorporate,maximum mean distance ( mmd ),model incorporate maximum mean distance ( mmd ),0.6570066213607788
translation,41,38,model,method,learns,multi-modal feature embedding,method learns multi-modal feature embedding,0.6906365752220154
translation,41,38,model,multi-modal feature embedding,simultaneously keeps,each domain invariant,multi-modal feature embedding simultaneously keeps each domain invariant,0.6531129479408264
translation,41,38,model,multi-modal feature embedding,simultaneously keeps,each individual modality discriminative,multi-modal feature embedding simultaneously keeps each individual modality discriminative,0.6679548621177673
translation,41,38,model,each individual modality discriminative,with,adversarial loss,each individual modality discriminative with adversarial loss,0.5894005298614502
translation,41,38,model,each individual modality discriminative,with,classification loss,each individual modality discriminative with classification loss,0.6044172048568726
translation,41,38,model,model,propose,method,model propose method,0.6280754208564758
translation,41,24,results,our experiments,validate,directly fine-tuning,our experiments validate directly fine-tuning,0.6539396643638611
translation,41,24,results,model,trained on,vqa 2.0,model trained on vqa 2.0,0.7456575632095337
translation,41,24,results,vqa 2.0,results in,minor improvement,vqa 2.0 results in minor improvement,0.6301353573799133
translation,41,24,results,minor improvement,on,vizwiz,minor improvement on vizwiz,0.5871273279190063
translation,41,24,results,directly fine-tuning,has,model,directly fine-tuning has model,0.5655491352081299
translation,41,46,results,our experiments,reveal,fine-tuning,our experiments reveal fine-tuning,0.7146193385124207
translation,41,46,results,model,trained on,vqa 2.0 dataset,model trained on vqa 2.0 dataset,0.7646064758300781
translation,41,46,results,limited improvement,on,vizwiz,limited improvement on vizwiz,0.6123711466789246
translation,41,46,results,limited improvement,due to,significant difference,limited improvement due to significant difference,0.6093968152999878
translation,41,46,results,fine-tuning,has,model,fine-tuning has model,0.5608916282653809
translation,41,169,results,state - of- theart models,by,significant margin,state - of- theart models by significant margin,0.5924898982048035
translation,41,169,results,our method,has,outperforms,our method has outperforms,0.6322360634803772
translation,41,169,results,outperforms,has,state - of- theart models,outperforms has state - of- theart models,0.5775024890899658
translation,41,169,results,results,clear that,our method,results clear that our method,0.6714054942131042
translation,41,175,results,existing domain adaptation methods,do,not help much,existing domain adaptation methods do not help much,0.46237167716026306
translation,41,175,results,not help much,in,multi-modal task,not help much in multi-modal task,0.5249865651130676
translation,41,175,results,outperforms,by,notable margin,outperforms by notable margin,0.6289628148078918
translation,41,175,results,existing domain adaptation methods,by,notable margin,existing domain adaptation methods by notable margin,0.5472717881202698
translation,41,175,results,direct fine-tuning,has,existing domain adaptation methods,direct fine-tuning has existing domain adaptation methods,0.5343921780586243
translation,41,175,results,our method,has,outperforms,our method has outperforms,0.6322360634803772
translation,41,175,results,results,compared to,direct fine-tuning,results compared to direct fine-tuning,0.6212866902351379
translation,41,184,results,our method,has,stably improve,our method has stably improve,0.5790921449661255
translation,41,184,results,stably improve,has,performance,stably improve has performance,0.5647140741348267
translation,41,184,results,results,has,our method,results has our method,0.5589964985847473
translation,41,188,results,significant improvement,for,our method,significant improvement for our method,0.5819130539894104
translation,41,188,results,significant improvement,against,fine-tuning,significant improvement against fine-tuning,0.6884769797325134
translation,41,188,results,results,observe,significant improvement,results observe significant improvement,0.6078537106513977
translation,41,190,results,our da model,achieves,comparable performance,our da model achieves comparable performance,0.7007681131362915
translation,41,190,results,comparable performance,to,state- of- the- art,comparable performance to state- of- the- art,0.5453924536705017
translation,41,190,results,state- of- the- art,on,vqa 2.0,state- of- the- art on vqa 2.0,0.5036060810089111
translation,41,190,results,results,has,our da model,results has our da model,0.5634636282920837
translation,42,163,ablation-analysis,factoid and list performance drop,of,3.3 and 1.2 percentage points,factoid and list performance drop of 3.3 and 1.2 percentage points,0.562995433807373
translation,42,163,ablation-analysis,ablation analysis,see,factoid and list performance drop,ablation analysis see factoid and list performance drop,0.5975567698478699
translation,42,173,ablation-analysis,performance,on,list questions,performance on list questions,0.5391656160354614
translation,42,173,ablation-analysis,forgetting loss,has,decreases,forgetting loss has decreases,0.5940403342247009
translation,42,173,ablation-analysis,decreases,has,performance,decreases has performance,0.5981842875480652
translation,42,81,experimental-setup,fastqa,employ,1 - dimensional convolutional neural network,fastqa employ 1 - dimensional convolutional neural network,0.561460018157959
translation,42,81,experimental-setup,1 - dimensional convolutional neural network,computes,word embeddings,1 - dimensional convolutional neural network computes word embeddings,0.6496032476425171
translation,42,81,experimental-setup,word embeddings,from,characters,word embeddings from characters,0.5791606903076172
translation,42,81,experimental-setup,characters,of,word,characters of word,0.5847733020782471
translation,42,82,experimental-setup,200 dimensional vectors,trained using,word2vec,200 dimensional vectors trained using word2vec,0.7056996822357178
translation,42,82,experimental-setup,word2vec,on,about 10 million pubmed abstracts,word2vec on about 10 million pubmed abstracts,0.49621257185935974
translation,42,82,experimental-setup,biomedical word2vec embeddings,has,200 dimensional vectors,biomedical word2vec embeddings has 200 dimensional vectors,0.5221194624900818
translation,42,82,experimental-setup,experimental setup,has,biomedical word2vec embeddings,experimental setup has biomedical word2vec embeddings,0.5057852864265442
translation,42,132,experimental-setup,"adam ( kingma and ba , 2014 )",for,optimization,"adam ( kingma and ba , 2014 ) for optimization",0.6054565906524658
translation,42,132,experimental-setup,"adam ( kingma and ba , 2014 )",on,squad,"adam ( kingma and ba , 2014 ) on squad",0.5696623921394348
translation,42,132,experimental-setup,optimization,on,squad,optimization on squad,0.6090942621231079
translation,42,132,experimental-setup,squad,with,learning rate,squad with learning rate,0.6597161889076233
translation,42,132,experimental-setup,learning rate,starting at,10 ?3,learning rate starting at 10 ?3,0.6674710512161255
translation,42,132,experimental-setup,experimental setup,use,"adam ( kingma and ba , 2014 )","experimental setup use adam ( kingma and ba , 2014 )",0.5799989700317383
translation,42,133,experimental-setup,fine-tuning phase,continue,optimization,fine-tuning phase continue optimization,0.6676316261291504
translation,42,133,experimental-setup,optimization,on,bioasq dataset,optimization on bioasq dataset,0.5625082850456238
translation,42,133,experimental-setup,bioasq dataset,with,smaller learning rate,bioasq dataset with smaller learning rate,0.61986243724823
translation,42,133,experimental-setup,smaller learning rate,starting at,10 ?4,smaller learning rate starting at 10 ?4,0.663667619228363
translation,42,133,experimental-setup,experimental setup,During,fine-tuning phase,experimental setup During fine-tuning phase,0.6940856575965881
translation,42,134,experimental-setup,model,regularized by,variational dropout,model regularized by variational dropout,0.7426187992095947
translation,42,134,experimental-setup,both phases,has,model,both phases has model,0.6287341713905334
translation,42,134,experimental-setup,variational dropout,has,of rate 0.5,variational dropout has of rate 0.5,0.5430282950401306
translation,42,134,experimental-setup,experimental setup,During,both phases,experimental setup During both phases,0.6919839382171631
translation,42,145,experimental-setup,"tensor- flow ( abadi et al. , 2016 )",with,hidden size,"tensor- flow ( abadi et al. , 2016 ) with hidden size",0.5683282613754272
translation,42,145,experimental-setup,hidden size,of,100,hidden size of 100,0.6616772413253784
translation,42,145,experimental-setup,experimental setup,implemented using,"tensor- flow ( abadi et al. , 2016 )","experimental setup implemented using tensor- flow ( abadi et al. , 2016 )",0.6573470234870911
translation,42,164,hyperparameters,entity features,to,word vector,entity features to word vector,0.49640944600105286
translation,42,164,hyperparameters,hyperparameters,append,entity features,hyperparameters append entity features,0.5476657152175903
translation,42,9,model,network architecture,based on,state - of- theart qa system,network architecture based on state - of- theart qa system,0.6914401650428772
translation,42,9,model,state - of- theart qa system,extended with,biomedical word embeddings,state - of- theart qa system extended with biomedical word embeddings,0.6363887190818787
translation,42,9,model,state - of- theart qa system,extended with,novel mechanism,state - of- theart qa system extended with novel mechanism,0.7034207582473755
translation,42,9,model,novel mechanism,to answer,list questions,novel mechanism to answer list questions,0.6917543411254883
translation,42,9,model,model,has,network architecture,model has network architecture,0.5380522012710571
translation,42,25,model,various do-main adaptation techniques,to transfer knowledge,"trained , state - of - the - art neural qa system","various do-main adaptation techniques to transfer knowledge trained , state - of - the - art neural qa system",0.6732100248336792
translation,42,25,model,"trained , state - of - the - art neural qa system",to,biomedical domain,"trained , state - of - the - art neural qa system to biomedical domain",0.5559148192405701
translation,42,25,model,biomedical domain,using,much smaller bioasq dataset,biomedical domain using much smaller bioasq dataset,0.6212638020515442
translation,42,25,model,model,employ,various do-main adaptation techniques,model employ various do-main adaptation techniques,0.6017881035804749
translation,42,26,model,list questions,extend,fastqa,list questions extend fastqa,0.7394916415214539
translation,42,26,model,fastqa,with,novel answering mechanism,fastqa with novel answering mechanism,0.6262524724006653
translation,42,26,model,model,to answer,list questions,model to answer list questions,0.7206900119781494
translation,42,26,model,model,extend,fastqa,model extend fastqa,0.755168616771698
translation,42,71,model,words,embedded into,highdimensional vector space,words embedded into highdimensional vector space,0.6185910105705261
translation,42,71,model,first step,has,words,first step has words,0.590426504611969
translation,42,71,model,model,In,first step,model In first step,0.5735508799552917
translation,42,28,results,factoid questions,show that,mere fine- tuning,factoid questions show that mere fine- tuning,0.4755131006240845
translation,42,28,results,mere fine- tuning,reaches,state - of - the - art results,mere fine- tuning reaches state - of - the - art results,0.6989285349845886
translation,42,28,results,state - of - the - art results,improved,forgetting cost regularization,state - of - the - art results improved forgetting cost regularization,0.6773229241371155
translation,42,28,results,results,For,factoid questions,results For factoid questions,0.5724169611930847
translation,42,29,results,results,are,competitive,results are competitive,0.5418685078620911
translation,42,29,results,competitive,to,existing systems,competitive to existing systems,0.6096521615982056
translation,42,29,results,list questions,has,results,list questions has results,0.5547820925712585
translation,42,29,results,results,On,list questions,results On list questions,0.5232493281364441
translation,42,158,results,performance,especially on,list questions,performance especially on list questions,0.6475734114646912
translation,42,158,results,performance,has,increases significantly,performance has increases significantly,0.5933406352996826
translation,42,158,results,results,observe,performance,results observe performance,0.6366938948631287
translation,42,160,results,performance,of,fine-tuned model,performance of fine-tuned model,0.5706822872161865
translation,42,160,results,fine-tuned model,on,both question types,fine-tuned model on both question types,0.49277281761169434
translation,42,160,results,both question types,is,much higher,both question types is much higher,0.5775497555732727
translation,42,160,results,much higher,than,baseline system,much higher than baseline system,0.5718166828155518
translation,42,160,results,baseline system,without,transfer learning,baseline system without transfer learning,0.677228569984436
translation,42,160,results,results,has,performance,results has performance,0.5972660779953003
translation,42,175,results,performance,increase,value of c l2,performance increase value of c l2,0.6972278356552124
translation,42,175,results,decreases,increase,value of c l2,decreases increase value of c l2,0.7661958336830139
translation,42,175,results,performance,has,decreases,performance has decreases,0.599746823310852
translation,42,175,results,results,found that,performance,results found that performance,0.7056670188903809
translation,42,186,results,performance gains,of,3 percentage points,performance gains of 3 percentage points,0.560415506362915
translation,42,186,results,performance gains,of,less than 1 percentage point,performance gains of less than 1 percentage point,0.5456717014312744
translation,42,186,results,performance gains,relative to,best single model,performance gains relative to best single model,0.7439800500869751
translation,42,186,results,3 percentage points,on,factoid questions,3 percentage points on factoid questions,0.540901243686676
translation,42,186,results,less than 1 percentage point,on,list questions,less than 1 percentage point on list questions,0.5280851721763611
translation,42,186,results,results,observe,performance gains,results observe performance gains,0.6379806995391846
translation,42,188,results,results,has,comparison to competing bioasq systems,results has comparison to competing bioasq systems,0.5825985670089722
translation,43,6,baselines,post hoc explanation methods,such as,lime,post hoc explanation methods such as lime,0.6227967143058777
translation,43,6,baselines,post hoc explanation methods,such as,input perturbation ( ip ),post hoc explanation methods such as input perturbation ( ip ),0.6451259851455688
translation,43,6,baselines,post hoc explanation methods,compare them with,self-explanatory attention mechanism,post hoc explanation methods compare them with self-explanatory attention mechanism,0.6025944948196411
translation,43,129,results,all methods,beat,random baseline,all methods beat random baseline,0.7161083221435547
translation,43,129,results,results,has,all methods,results has all methods,0.48065561056137085
translation,43,130,results,ip,is,most successful explanation method,ip is most successful explanation method,0.5611132979393005
translation,43,130,results,most successful explanation method,with,pointing game accuracy,most successful explanation method with pointing game accuracy,0.5642854571342468
translation,43,130,results,pointing game accuracy,of,0.97,pointing game accuracy of 0.97,0.5254983901977539
translation,43,130,results,lime,comes,second,lime comes second,0.7677941918373108
translation,43,130,results,results,has,ip,results has ip,0.5219308137893677
translation,43,134,results,attention weights,at,hop 3,attention weights at hop 3,0.5589689612388611
translation,43,134,results,attention weights,performs,best,attention weights performs best,0.6158977150917053
translation,43,134,results,attention weights,performs,worse,attention weights performs worse,0.6154261827468872
translation,43,134,results,best,among,attention - based methods,best among attention - based methods,0.62013840675354
translation,43,134,results,worse,than,lime and ip,worse than lime and ip,0.6700832843780518
translation,43,134,results,results,has,attention weights,results has attention weights,0.5251908898353577
translation,43,180,results,attention weights,perform,significantly worse,attention weights perform significantly worse,0.5913357734680176
translation,43,180,results,significantly worse,than,other methods,significantly worse than other methods,0.5324316620826721
translation,43,180,results,difference,between,lime and ip,difference between lime and ip,0.7355251908302307
translation,43,180,results,lime and ip,is,insignificant,lime and ip is insignificant,0.6551470756530762
translation,43,180,results,results,has,significance tests,results has significance tests,0.498953253030777
translation,44,214,ablation-analysis,19 % errors,due to,wrong prediction,19 % errors due to wrong prediction,0.7160505652427673
translation,44,214,ablation-analysis,wrong prediction,on recognizing,neutral instances,wrong prediction on recognizing neutral instances,0.7128400206565857
translation,44,214,ablation-analysis,ablation analysis,has,19 % errors,ablation analysis has 19 % errors,0.5540080666542053
translation,44,216,ablation-analysis,16 % errors,due to,comparative opinions,16 % errors due to comparative opinions,0.6951630711555481
translation,44,216,ablation-analysis,ablation analysis,has,16 % errors,ablation analysis has 16 % errors,0.5623167157173157
translation,44,219,ablation-analysis,14 % errors,due to,mistakes,14 % errors due to mistakes,0.6952840685844421
translation,44,219,ablation-analysis,mistakes,during,chinese word segmentation,mistakes during chinese word segmentation,0.6649709939956665
translation,44,219,ablation-analysis,ablation analysis,has,14 % errors,ablation analysis has 14 % errors,0.5558867454528809
translation,44,156,baselines,some qa matching approaches,to,asc - qa,some qa matching approaches to asc - qa,0.5680480599403381
translation,44,156,baselines,some qa matching approaches,implement,several basic versions,some qa matching approaches implement several basic versions,0.6116533875465393
translation,44,156,baselines,several basic versions,of,rban,several basic versions of rban,0.5837368369102478
translation,44,156,baselines,baselines,employ,some qa matching approaches,baselines employ some qa matching approaches,0.5300795435905457
translation,44,159,baselines,standard lstm network,to model,text,standard lstm network to model text,0.6571885943412781
translation,44,159,baselines,text,without considering,aspect information,text without considering aspect information,0.7604554295539856
translation,44,160,baselines,ram,has,"chen et al. , 2017 )","ram has chen et al. , 2017 )",0.5851209759712219
translation,44,160,baselines,baselines,has,ram,baselines has ram,0.6504290103912354
translation,44,162,baselines,baselines,has,gcae,baselines has gcae,0.5812315344810486
translation,44,163,baselines,state- ofthe - art approach,to,asc,state- ofthe - art approach to asc,0.5514159798622131
translation,44,163,baselines,state- ofthe - art approach,combines,cnn and gating mechanisms,state- ofthe - art approach combines cnn and gating mechanisms,0.7212084531784058
translation,44,163,baselines,cnn and gating mechanisms,to learn,text representation,cnn and gating mechanisms to learn text representation,0.6307080984115601
translation,44,164,baselines,baselines,has,s-lstm,baselines has s-lstm,0.521058201789856
translation,44,165,baselines,state - of - the - art approach,to,asc,state - of - the - art approach to asc,0.5300288200378418
translation,44,165,baselines,state - of - the - art approach,considers,structural dependencies,state - of - the - art approach considers structural dependencies,0.6354767680168152
translation,44,165,baselines,structural dependencies,between,targets and opinion terms,structural dependencies between targets and opinion terms,0.6593677401542664
translation,44,166,baselines,bidaf,has,"seo et al. , 2016 )","bidaf has seo et al. , 2016 )",0.5728828310966492
translation,44,166,baselines,baselines,has,bidaf,baselines has bidaf,0.6077240705490112
translation,44,169,baselines,baselines,has,hmn,baselines has hmn,0.6033270955085754
translation,44,171,baselines,baselines,has,mamc,baselines has mamc,0.6124364137649536
translation,44,172,baselines,qa matching approach,to,asc,qa matching approach to asc,0.608647346496582
translation,44,172,baselines,qa matching approach,proposes,hierarchical iterative attention,qa matching approach proposes hierarchical iterative attention,0.6636064052581787
translation,44,172,baselines,hierarchical iterative attention,to learn,aspect-specific text representation,hierarchical iterative attention to learn aspect-specific text representation,0.5936722755432129
translation,44,173,baselines,baselines,has,rban w/o raws,baselines has rban w/o raws,0.5849949717521667
translation,44,7,experiments,high-quality annotated corpus,constructed for,asc - qa,high-quality annotated corpus constructed for asc - qa,0.6852043867111206
translation,44,7,experiments,asc - qa,to facilitate,corresponding research,asc - qa to facilitate corresponding research,0.7020702362060547
translation,44,143,experiments,adam optimizer,with,initial learning rate,adam optimizer with initial learning rate,0.5838022828102112
translation,44,143,experiments,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,44,143,experiments,adam optimizer,adopt,sgd optimizer,adam optimizer adopt sgd optimizer,0.6082381010055542
translation,44,143,experiments,initial learning rate,of,0.01,initial learning rate of 0.01,0.5797136425971985
translation,44,143,experiments,initial learning rate,of,0.002,initial learning rate of 0.002,0.5725244283676147
translation,44,143,experiments,initial learning rate,of,0.002,initial learning rate of 0.002,0.5725244283676147
translation,44,143,experiments,0.01,for,crossentropy training,0.01 for crossentropy training,0.5845634341239929
translation,44,143,experiments,sgd optimizer,with,learning rate,sgd optimizer with learning rate,0.5834531188011169
translation,44,143,experiments,learning rate,of,0.002,learning rate of 0.002,0.5967087745666504
translation,44,143,experiments,0.002,for,all policy gradients training,0.002 for all policy gradients training,0.5869073271751404
translation,44,138,hyperparameters,"fudannlp ( qiu et al. , 2013 )",to perform,word segmentation,"fudannlp ( qiu et al. , 2013 ) to perform word segmentation",0.5910664796829224
translation,44,138,hyperparameters,word segmentation,over,collected 150k chinese qa text pairs,word segmentation over collected 150k chinese qa text pairs,0.6234655380249023
translation,44,138,hyperparameters,hyperparameters,adopt,"fudannlp ( qiu et al. , 2013 )","hyperparameters adopt fudannlp ( qiu et al. , 2013 )",0.5519260168075562
translation,44,139,hyperparameters,qa,to pre-train,200 - dimension word vectors,qa to pre-train 200 - dimension word vectors,0.7630256414413452
translation,44,139,hyperparameters,text pairs,to pre-train,200 - dimension word vectors,text pairs to pre-train 200 - dimension word vectors,0.7100204229354858
translation,44,139,hyperparameters,200 - dimension word vectors,with,skip-gram,200 - dimension word vectors with skip-gram,0.6334572434425354
translation,44,139,hyperparameters,qa,has,text pairs,qa has text pairs,0.6274948716163635
translation,44,139,hyperparameters,hyperparameters,employ,qa,hyperparameters employ qa,0.588706374168396
translation,44,139,hyperparameters,hyperparameters,employ,text pairs,hyperparameters employ text pairs,0.5262492895126343
translation,44,140,hyperparameters,word embeddings,optimized during,training,word embeddings optimized during training,0.706823468208313
translation,44,141,hyperparameters,dimensions,of,lstm hidden states,dimensions of lstm hidden states,0.5235941410064697
translation,44,141,hyperparameters,lstm hidden states,set to be,200,lstm hidden states set to be 200,0.6742611527442932
translation,44,141,hyperparameters,hyperparameters,has,dimensions,hyperparameters has dimensions,0.5247315764427185
translation,44,144,hyperparameters,regularization weight,of,parameters,regularization weight of parameters,0.5313400030136108
translation,44,144,hyperparameters,regularization weight,of,dropout rate,regularization weight of dropout rate,0.4969700872898102
translation,44,144,hyperparameters,regularization weight,of,batch size,regularization weight of batch size,0.5412582159042358
translation,44,144,hyperparameters,parameters,is,10 ?5,parameters is 10 ?5,0.6314475536346436
translation,44,144,hyperparameters,dropout rate,is,0.25,dropout rate is 0.25,0.550621747970581
translation,44,144,hyperparameters,batch size,is,32,batch size is 32,0.6284153461456299
translation,44,144,hyperparameters,hyperparameters,has,regularization weight,hyperparameters has regularization weight,0.4558113217353821
translation,44,144,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,44,144,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,44,25,model,model,propose,reinforced bidirectional attention network approach,model propose reinforced bidirectional attention network approach,0.6539983153343201
translation,44,26,model,word selection model,namely,aspect-relevant word selector ( raws ),word selection model namely aspect-relevant word selector ( raws ),0.6897693276405334
translation,44,26,model,word selection model,to alleviate,effects of noisy words,word selection model to alleviate effects of noisy words,0.6337627172470093
translation,44,26,model,effects of noisy words,for,specific aspect,effects of noisy words for specific aspect,0.6119648218154907
translation,44,26,model,specific aspect,through discarding,noisy words,specific aspect through discarding noisy words,0.7342705130577087
translation,44,26,model,model,propose,word selection model,model propose word selection model,0.643131673336029
translation,44,27,model,raws,develop,reinforced bidirectional attention network ( rban ) approach,raws develop reinforced bidirectional attention network ( rban ) approach,0.614421010017395
translation,44,27,model,reinforced bidirectional attention network ( rban ) approach,to,asc - qa,reinforced bidirectional attention network ( rban ) approach to asc - qa,0.5909473896026611
translation,44,27,model,reinforced bidirectional attention network ( rban ) approach,employs,two fundamental raws modules,reinforced bidirectional attention network ( rban ) approach employs two fundamental raws modules,0.6037705540657043
translation,44,27,model,two fundamental raws modules,to perform,word selection,two fundamental raws modules to perform word selection,0.683227002620697
translation,44,27,model,word selection,over,question and answer text,word selection over question and answer text,0.6213507056236267
translation,44,27,model,model,basis of,raws,model basis of raws,0.755416750907898
translation,44,27,model,model,develop,reinforced bidirectional attention network ( rban ) approach,model develop reinforced bidirectional attention network ( rban ) approach,0.616409957408905
translation,44,29,model,rban,via,reinforcement learning algorithm,rban via reinforcement learning algorithm,0.6661635041236877
translation,44,29,model,reinforcement learning algorithm,i.e.,policy gradient,reinforcement learning algorithm i.e. policy gradient,0.5412319302558899
translation,44,29,model,model,optimize,rban,model optimize rban,0.809605062007904
translation,44,168,model,decoding layer,with,softmax decoder,decoding layer with softmax decoder,0.6268496513366699
translation,44,168,model,softmax decoder,to perform,asc - qa,softmax decoder to perform asc - qa,0.6588106155395508
translation,44,168,model,model,substitute,decoding layer,model substitute decoding layer,0.6956725716590881
translation,44,184,results,all the three state - of- the- art asc approaches,i.e.,ram,all the three state - of- the- art asc approaches i.e. ram,0.6939871907234192
translation,44,184,results,all the three state - of- the- art asc approaches,i.e.,gcae,all the three state - of- the- art asc approaches i.e. gcae,0.7239119410514832
translation,44,184,results,all the three state - of- the- art asc approaches,i.e.,s-lstm,all the three state - of- the- art asc approaches i.e. s-lstm,0.6280314922332764
translation,44,184,results,all the three state - of- the- art asc approaches,perform,better,all the three state - of- the- art asc approaches perform better,0.5761165022850037
translation,44,184,results,better,than,lstm,better than lstm,0.5847610235214233
translation,44,186,results,both the attention based approaches ram and s-lstm,achieve,comparable or better performance,both the attention based approaches ram and s-lstm achieve comparable or better performance,0.6085248589515686
translation,44,186,results,comparable or better performance,than,gcae,comparable or better performance than gcae,0.6110602021217346
translation,44,186,results,results,has,both the attention based approaches ram and s-lstm,results has both the attention based approaches ram and s-lstm,0.5664466023445129
translation,44,188,results,two qa matching approaches,i.e.,bidaf,two qa matching approaches i.e. bidaf,0.6991252303123474
translation,44,188,results,two qa matching approaches,i.e.,hmn,two qa matching approaches i.e. hmn,0.6619784235954285
translation,44,188,results,two qa matching approaches,could achieve,comparable performance,two qa matching approaches could achieve comparable performance,0.6724931597709656
translation,44,188,results,comparable performance,with,three state - of - the - art asc approaches,comparable performance with three state - of - the - art asc approaches,0.6446799635887146
translation,44,188,results,results,has,two qa matching approaches,results has two qa matching approaches,0.5686449408531189
translation,44,190,results,rban w/o raws approach,performs,consistently better,rban w/o raws approach performs consistently better,0.6270319223403931
translation,44,190,results,consistently better,than,mamc,consistently better than mamc,0.6085267066955566
translation,44,190,results,results,has,rban w/o raws approach,results has rban w/o raws approach,0.578523576259613
translation,44,192,results,rban w/o a2q,performs,much better,rban w/o a2q performs much better,0.6466053128242493
translation,44,192,results,much better,than,rban w/o q2a,much better than rban w/o q2a,0.6025804281234741
translation,44,192,results,rban w/o q2a,has,"i.e. , without answer vector s a )","rban w/o q2a has i.e. , without answer vector s a )",0.6325145959854126
translation,44,192,results,results,interesting to notice,rban w/o a2q,results interesting to notice rban w/o a2q,0.714745044708252
translation,44,194,results,rban,best and significantly outperforms,rban w/o raws,rban best and significantly outperforms rban w/o raws,0.7504794001579285
translation,44,194,results,per-forms,best and significantly outperforms,rban w/o raws,per-forms best and significantly outperforms rban w/o raws,0.8060704469680786
translation,44,194,results,raws,has,rban,raws has rban,0.6891473531723022
translation,44,194,results,rban,has,per-forms,rban has per-forms,0.65614253282547
translation,44,194,results,results,when using,raws,results when using raws,0.6782099604606628
translation,44,195,results,rban,achieves,average improvements,rban achieves average improvements,0.6872027516365051
translation,44,195,results,average improvements,of,7.97 % ( f1 ),average improvements of 7.97 % ( f1 ),0.5454778075218201
translation,44,195,results,average improvements,of,8.67 % ( acc. ),average improvements of 8.67 % ( acc. ),0.5483644008636475
translation,44,195,results,8.67 % ( acc. ),in,three domains,8.67 % ( acc. ) in three domains,0.5143699645996094
translation,44,195,results,term-level asc - qa,has,rban,term-level asc - qa has rban,0.6043474078178406
translation,44,195,results,lstm,has,rban,lstm has rban,0.5933518409729004
translation,44,196,results,category - level asc - qa,compared to,lstm,category - level asc - qa compared to lstm,0.6186073422431946
translation,44,196,results,rban,achieves,average improvements,rban achieves average improvements,0.6872027516365051
translation,44,196,results,average improvements,of,9.1 % ( f1 ),average improvements of 9.1 % ( f1 ),0.5535754561424255
translation,44,196,results,average improvements,of,9.23 % ( acc. ),average improvements of 9.23 % ( acc. ),0.5460534691810608
translation,44,196,results,category - level asc - qa,has,rban,category - level asc - qa has rban,0.6066343784332275
translation,44,196,results,lstm,has,rban,lstm has rban,0.5933518409729004
translation,44,215,results,neutral training examples,prediction of,neutral instances very difficult,neutral training examples prediction of neutral instances very difficult,0.5994278192520142
translation,44,215,results,results,shortage of,neutral training examples,results shortage of neutral training examples,0.6718952655792236
translation,45,8,model,combination of bootstrapping and pointwise mutual information,to estimate,strength of association,combination of bootstrapping and pointwise mutual information to estimate strength of association,0.7187240123748779
translation,45,8,model,strength of association,between,word,strength of association between word,0.6323054432868958
translation,45,8,model,word,from,large unannotated set of question - answer threads,word from large unannotated set of question - answer threads,0.5845457315444946
translation,45,8,model,model,use,combination of bootstrapping and pointwise mutual information,model use combination of bootstrapping and pointwise mutual information,0.6058310866355896
translation,45,102,results,virtually no impact,on,results,virtually no impact on results,0.5829118490219116
translation,45,102,results,minimal impact,has,0.2 map points,minimal impact has 0.2 map points,0.5515670776367188
translation,45,102,results,real impact,has,1.8 map points,real impact has 1.8 map points,0.5836717486381531
translation,45,102,results,largest impact,has,6.6 map points,largest impact has 6.6 map points,0.5751211047172546
translation,45,102,results,results,see that,personality features,results see that personality features,0.5770078301429749
translation,45,104,results,competition,fixed,bug,competition fixed bug,0.8166711330413818
translation,45,104,results,bug,in,bootstrapped lexicon construction,bug in bootstrapped lexicon construction,0.5239087343215942
translation,45,104,results,bug,resulted in,sizable improvements,bug resulted in sizable improvements,0.6159657835960388
translation,46,120,ablation-analysis,all features,are,useful,all features are useful,0.5834293961524963
translation,46,120,ablation-analysis,useful,on,training data,useful on training data,0.5177797675132751
translation,46,120,ablation-analysis,system,achieve,better result,system achieve better result,0.6694556474685669
translation,46,120,ablation-analysis,better result,on,test data,better result on test data,0.5523847937583923
translation,46,120,ablation-analysis,better result,exclude,word overlap feature,better result exclude word overlap feature,0.6185297966003418
translation,46,120,ablation-analysis,ablation analysis,observe that,all features,ablation analysis observe that all features,0.5511481165885925
translation,46,121,ablation-analysis,neural matching features,important on,subtask a,neural matching features important on subtask a,0.664545476436615
translation,46,121,ablation-analysis,neural matching features,obtain,5 point gain,neural matching features obtain 5 point gain,0.5470166206359863
translation,46,121,ablation-analysis,neural matching features,obtain,3 point gain,neural matching features obtain 3 point gain,0.5417118668556213
translation,46,121,ablation-analysis,5 point gain,on,training data,5 point gain on training data,0.5602577924728394
translation,46,121,ablation-analysis,3 point gain,on,test data,3 point gain on test data,0.5426837205886841
translation,46,121,ablation-analysis,ablation analysis,has,neural matching features,ablation analysis has neural matching features,0.5335346460342407
translation,46,122,ablation-analysis,meta-data features,are,useful,meta-data features are useful,0.5888186097145081
translation,46,122,ablation-analysis,meta-data features,also,useful,meta-data features also useful,0.581605076789856
translation,46,122,ablation-analysis,ablation analysis,has,meta-data features,ablation analysis has meta-data features,0.49976474046707153
translation,46,123,ablation-analysis,subtask b.,caused,performance drop,subtask b. caused performance drop,0.6304394006729126
translation,46,123,ablation-analysis,subtask b.,has,neural matching features,subtask b. has neural matching features,0.5288619995117188
translation,46,124,ablation-analysis,traditional nlp features,are,useful,traditional nlp features are useful,0.5542563199996948
translation,46,124,ablation-analysis,traditional nlp features,are,word overlap,traditional nlp features are word overlap,0.5374995470046997
translation,46,124,ablation-analysis,useful,on,training data,useful on training data,0.5177797675132751
translation,46,124,ablation-analysis,performance,on,test data,performance on test data,0.5576556324958801
translation,46,124,ablation-analysis,meta-data feature,has,hurt,meta-data feature has hurt,0.5916163325309753
translation,46,124,ablation-analysis,hurt,has,performance,hurt has performance,0.6066785454750061
translation,46,53,baselines,baselines,has,bi-lstm,baselines has bi-lstm,0.5433008670806885
translation,46,23,experiments,subtask b,holds,fifth place,subtask b holds fifth place,0.6790139675140381
translation,46,101,hyperparameters,overfitting,used,early - stopping,overfitting used early - stopping,0.6142245531082153
translation,46,101,hyperparameters,overfitting,used,dropout,overfitting used dropout,0.5841407179832458
translation,46,101,hyperparameters,early - stopping,with,rate,early - stopping with rate,0.6402162909507751
translation,46,101,hyperparameters,dropout,with,rate,dropout with rate,0.5557408928871155
translation,46,101,hyperparameters,"srivastava et al. , 2014 )",with,rate,"srivastava et al. , 2014 ) with rate",0.6147167682647705
translation,46,101,hyperparameters,rate,of,0.5,rate of 0.5,0.6499558687210083
translation,46,101,hyperparameters,early - stopping,has,"giles , 2000 )","early - stopping has giles , 2000 )",0.5642637610435486
translation,46,101,hyperparameters,dropout,has,"srivastava et al. , 2014 )","dropout has srivastava et al. , 2014 )",0.5718802213668823
translation,46,101,hyperparameters,hyperparameters,to prevent,overfitting,hyperparameters to prevent overfitting,0.5773724317550659
translation,46,102,hyperparameters,dimensionality,of,word embedding,dimensionality of word embedding,0.535808265209198
translation,46,102,hyperparameters,word embedding,is,200,word embedding is 200,0.577017605304718
translation,46,102,hyperparameters,bi-lstm and 2d matching network ( 2d mn ),has,dimensionality,bi-lstm and 2d matching network ( 2d mn ) has dimensionality,0.5591434836387634
translation,46,102,hyperparameters,hyperparameters,In,bi-lstm and 2d matching network ( 2d mn ),hyperparameters In bi-lstm and 2d matching network ( 2d mn ),0.5179054141044617
translation,46,103,hyperparameters,word embedding,initialized by,result of word2vec,word embedding initialized by result of word2vec,0.6413427591323853
translation,46,103,hyperparameters,word embedding,updated in,training,word embedding updated in training,0.5842790603637695
translation,46,103,hyperparameters,result of word2vec,trained on,unannotated qatar data,result of word2vec trained on unannotated qatar data,0.7311535477638245
translation,46,103,hyperparameters,hyperparameters,has,word embedding,hyperparameters has word embedding,0.4999687969684601
translation,46,104,hyperparameters,initial learning rate and batch size,as,0.001 and 30,initial learning rate and batch size as 0.001 and 30,0.5562954545021057
translation,46,104,hyperparameters,hyperparameters,set,initial learning rate and batch size,hyperparameters set initial learning rate and batch size,0.6165502071380615
translation,46,6,model,ranking system,capable of capturing,semantic relations,ranking system capable of capturing semantic relations,0.6880353689193726
translation,46,6,model,semantic relations,between,text pairs,semantic relations between text pairs,0.5943232774734497
translation,46,6,model,model,develop,ranking system,model develop ranking system,0.6269064545631409
translation,46,7,model,several neural network based matching features,enable,our system,several neural network based matching features enable our system,0.6593483686447144
translation,46,7,model,our system,to measure,text similarity,our system to measure text similarity,0.6409077644348145
translation,46,7,model,text similarity,beyond,lexicons,text similarity beyond lexicons,0.6641181111335754
translation,46,7,model,model,introduce,several neural network based matching features,model introduce several neural network based matching features,0.6604155898094177
translation,46,16,model,semantic gaps,build,ranking system,semantic gaps build ranking system,0.6872220039367676
translation,46,16,model,ranking system,with,variety of features,ranking system with variety of features,0.6242313385009766
translation,46,16,model,model,To bridge,semantic gaps,model To bridge semantic gaps,0.6607444882392883
translation,46,20,model,features,combined as,ranking model,features combined as ranking model,0.603567361831665
translation,46,20,model,ranking model,by,gradient boosted regression tree,ranking model by gradient boosted regression tree,0.5542581677436829
translation,46,20,model,gradient boosted regression tree,implemented by,xgboost,gradient boosted regression tree implemented by xgboost,0.6668081879615784
translation,46,20,model,model,has,features,model has features,0.5468711853027344
translation,46,8,results,our system,holds,second place,our system holds second place,0.6652534008026123
translation,46,8,results,our system,holds,fifth place,our system holds fifth place,0.6843838095664978
translation,46,8,results,significantly outperforms,holds,fifth place,significantly outperforms holds fifth place,0.7083565592765808
translation,46,8,results,second place,in,subtask a,second place in subtask a,0.5512569546699524
translation,46,8,results,second place,in,subtask b,second place in subtask b,0.5335978865623474
translation,46,8,results,second place,in,subtask b,second place in subtask b,0.5335978865623474
translation,46,8,results,fifth place,in,subtask b,fifth place in subtask b,0.5307013988494873
translation,46,8,results,our system,has,significantly outperforms,our system has significantly outperforms,0.619092583656311
translation,46,8,results,significantly outperforms,has,baseline methods,significantly outperforms has baseline methods,0.5802999138832092
translation,46,8,results,results,has,our system,results has our system,0.5954442024230957
translation,46,21,results,significantly outperforms,on,two subtasks,significantly outperforms on two subtasks,0.5109837651252747
translation,46,21,results,baseline methods,on,two subtasks,baseline methods on two subtasks,0.46928489208221436
translation,46,21,results,our system,has,significantly outperforms,our system has significantly outperforms,0.619092583656311
translation,46,21,results,significantly outperforms,has,baseline methods,significantly outperforms has baseline methods,0.5802999138832092
translation,46,21,results,results,has,our system,results has our system,0.5954442024230957
translation,46,22,results,subtask a,holds,second place,subtask a holds second place,0.6896792054176331
translation,46,22,results,comparable,with,best system,comparable with best system,0.6305117011070251
translation,46,22,results,results,On,subtask a,results On subtask a,0.48677441477775574
translation,46,119,results,conclusion,is,traditional nlp features,conclusion is traditional nlp features,0.5342099070549011
translation,46,119,results,traditional nlp features,are,effective,traditional nlp features are effective,0.5379874110221863
translation,46,119,results,effective,on,both subtasks,effective on both subtasks,0.552666962146759
translation,46,119,results,system performance,on,subtask a.,system performance on subtask a.,0.5326570272445679
translation,46,119,results,our system,with,all features,our system with all features,0.6318737268447876
translation,46,119,results,system,with,one of the features excluded,system with one of the features excluded,0.6413924098014832
translation,46,119,results,improve,has,system performance,improve has system performance,0.5579326748847961
translation,46,119,results,results,has,conclusion,results has conclusion,0.5665753483772278
translation,47,160,ablation-analysis,additional sampling,allows,recovery,additional sampling allows recovery,0.6652225255966187
translation,47,160,ablation-analysis,recovery,from,mistakes,recovery from mistakes,0.5973024964332581
translation,47,160,ablation-analysis,mistakes,in,wikireading long,mistakes in wikireading long,0.5835292935371399
translation,47,160,ablation-analysis,ablation analysis,has,additional sampling,ablation analysis has additional sampling,0.5375227332115173
translation,47,134,experiments,cnn sentence selection model,used,100 filters,cnn sentence selection model used 100 filters,0.5663368105888367
translation,47,134,experiments,cnn sentence selection model,set,filter width,cnn sentence selection model set filter width,0.6157212257385254
translation,47,134,experiments,filter width,as,five,filter width as five,0.6055343747138977
translation,47,152,experiments,bow encoder,for,sentence selection,bow encoder for sentence selection,0.5616927742958069
translation,47,165,experiments,reinforce,finds,approximate gold sentence,reinforce finds approximate gold sentence,0.5966899991035461
translation,47,165,experiments,approximate gold sentence,in,74.4 %,approximate gold sentence in 74.4 %,0.4962921142578125
translation,47,165,experiments,74.4 %,of,examples,74.4 % of examples,0.5481627583503723
translation,47,165,experiments,examples,where,answer,examples where answer,0.6846146583557129
translation,47,165,experiments,answer,is,document,answer is document,0.6536755561828613
translation,47,165,experiments,answer,in,document,answer in document,0.5632256269454956
translation,47,165,experiments,wikireading,has,long,wikireading has long,0.7004349231719971
translation,47,165,experiments,wikireading,has,reinforce,wikireading has reinforce,0.6861044764518738
translation,47,165,experiments,long,has,reinforce,long has reinforce,0.6307933330535889
translation,47,131,hyperparameters,word embedding dimension,set to,256,word embedding dimension set to 256,0.6985997557640076
translation,47,131,hyperparameters,256,for,sentence selection,256 for sentence selection,0.6199989318847656
translation,47,131,hyperparameters,256,for,answer generation models,256 for answer generation models,0.5786480903625488
translation,47,131,hyperparameters,hyperparameters,has,word embedding dimension,hyperparameters has word embedding dimension,0.4810734987258911
translation,47,132,hyperparameters,decay rate,of,0.8,decay rate of 0.8,0.6261535286903381
translation,47,132,hyperparameters,0.8,for,curriculum learning,0.8 for curriculum learning,0.6110373735427856
translation,47,132,hyperparameters,hyperparameters,used,decay rate,hyperparameters used decay rate,0.5911033153533936
translation,47,133,hyperparameters,hidden dimension,fixed at,128,hidden dimension fixed at 128,0.7299488186836243
translation,47,133,hyperparameters,batch size,at,128,batch size at 128,0.5914124250411987
translation,47,133,hyperparameters,gru state cell,at,512,gru state cell at 512,0.6097222566604614
translation,47,133,hyperparameters,vocabulary size,at,100k,vocabulary size at 100k,0.5500643849372864
translation,47,133,hyperparameters,hyperparameters,has,hidden dimension,hyperparameters has hidden dimension,0.528815507888794
translation,47,136,hyperparameters,ranges,for,learning rates,ranges for learning rates,0.6600953340530396
translation,47,136,hyperparameters,learning rates,were,"0.00025 , 0.0005 , 0.001 , 0.002 , 0.004 and 0.5 , 1.0","learning rates were 0.00025 , 0.0005 , 0.001 , 0.002 , 0.004 and 0.5 , 1.0",0.5438677668571472
translation,47,136,hyperparameters,"0.00025 , 0.0005 , 0.001 , 0.002 , 0.004 and 0.5 , 1.0",for,gradient clipping coefficient,"0.00025 , 0.0005 , 0.001 , 0.002 , 0.004 and 0.5 , 1.0 for gradient clipping coefficient",0.5789992213249207
translation,47,136,hyperparameters,hyperparameters,has,ranges,hyperparameters has ranges,0.5199583768844604
translation,47,137,hyperparameters,learning rate,every,25 k steps,learning rate every 25 k steps,0.6411130428314209
translation,47,137,hyperparameters,hyperparameters,halved,learning rate,hyperparameters halved learning rate,0.6459066271781921
translation,47,138,hyperparameters,hyperparameters,use,"adam ( kingma and ba , 2015 ) optimizer","hyperparameters use adam ( kingma and ba , 2015 ) optimizer",0.6104203462600708
translation,47,138,hyperparameters,hyperparameters,use,tensorflow framework,hyperparameters use tensorflow framework,0.5878823399543762
translation,47,6,model,"coarse , fast model",for selecting,relevant sentences,"coarse , fast model for selecting relevant sentences",0.6692405939102173
translation,47,6,model,more expensive rnn,for producing,answer,more expensive rnn for producing answer,0.7409418821334839
translation,47,6,model,answer,from,those sentences,answer from those sentences,0.5751275420188904
translation,47,6,model,model,combine,"coarse , fast model","model combine coarse , fast model",0.741791307926178
translation,47,7,model,sentence selection,as,latent variable,sentence selection as latent variable,0.5111700892448425
translation,47,7,model,latent variable,trained jointly from,answer,latent variable trained jointly from answer,0.6830964088439941
translation,47,7,model,model,treat,sentence selection,model treat sentence selection,0.5635839700698853
translation,47,16,model,coarse- to -fine model,for,question answering,coarse- to -fine model for question answering,0.6057409644126892
translation,47,16,model,model,propose,coarse- to -fine model,model propose coarse- to -fine model,0.6432031989097595
translation,47,30,model,learning procedure,for,qa,learning procedure for qa,0.6486813426017761
translation,47,30,model,qa,has,over long text,qa has over long text,0.6290964484214783
translation,47,30,model,model,present,modular framework,model present modular framework,0.6859310269355774
translation,47,129,model,sentence index,as,one hot vector,sentence index as one hot vector,0.5437532663345337
translation,47,129,model,one hot vector,to,sentence representation,one hot vector to sentence representation,0.5384582877159119
translation,47,129,model,model,add,sentence index,model add sentence index,0.6432173848152161
translation,47,149,results,our numbers,on,wikireading,our numbers on wikireading,0.6734728813171387
translation,47,149,results,wikireading,has,outperform,wikireading has outperform,0.6814473271369934
translation,47,149,results,outperform,has,previously reported numbers,outperform has previously reported numbers,0.6508498191833496
translation,47,149,results,results,has,our numbers,results has our numbers,0.5805239081382751
translation,47,153,results,proposed hierarchical models,match or exceed,performance,proposed hierarchical models match or exceed performance,0.6777699589729309
translation,47,153,results,proposed hierarchical models,reducing,number of rnn steps,proposed hierarchical models reducing number of rnn steps,0.6852207183837891
translation,47,153,results,performance,of,base,performance of base,0.6306381821632385
translation,47,153,results,number of rnn steps,from,300 to 35,number of rnn steps from 300 to 35,0.5908491611480713
translation,47,153,results,later parts,of,document,later parts of document,0.667206346988678
translation,47,153,results,number of rnn steps,has,significantly,number of rnn steps has significantly,0.6154402494430542
translation,47,153,results,results,has,proposed hierarchical models,results has proposed hierarchical models,0.5966813564300537
translation,47,158,results,performance,of,oracle,performance of oracle,0.6124745011329651
translation,47,158,results,performance,showing,strong results,performance showing strong results,0.7495899796485901
translation,47,158,results,oracle,in,wikireading,oracle in wikireading,0.5652386546134949
translation,47,158,results,strong results,in,limited token setting,strong results in limited token setting,0.504759669303894
translation,47,158,results,our system,has,almost reaches,our system has almost reaches,0.618040919303894
translation,47,158,results,almost reaches,has,performance,almost reaches has performance,0.6509958505630493
translation,47,158,results,results,has,our system,results has our system,0.5954442024230957
translation,47,159,results,additional sentence,into,document summary,additional sentence into document summary,0.5342803597450256
translation,47,159,results,additional sentence,increased,performance,additional sentence increased performance,0.6886917948722839
translation,47,159,results,additional sentence,illustrating,flexibility,additional sentence illustrating flexibility,0.746618926525116
translation,47,159,results,document summary,increased,performance,document summary increased performance,0.6991271376609802
translation,47,159,results,performance,in,all datasets,performance in all datasets,0.4686133861541748
translation,47,159,results,results,Sampling,additional sentence,results Sampling additional sentence,0.6920109391212463
translation,47,161,results,hard attention,observe,reinforce,hard attention observe reinforce,0.7380022406578064
translation,47,161,results,reinforce,performed,better,reinforce performed better,0.2988085448741913
translation,47,161,results,better,than,softattend,better than softattend,0.5850049257278442
translation,47,161,results,results,Comparing,hard attention,results Comparing hard attention,0.7426538467407227
translation,47,166,results,wik-isuggest performance,is,67.5 %,wik-isuggest performance is 67.5 %,0.5747338533401489
translation,47,166,results,wik-isuggest performance,at,67.5 %,wik-isuggest performance at 67.5 %,0.5276110768318176
translation,47,167,results,pipeline,performs,slightly better,pipeline performs slightly better,0.649152934551239
translation,47,167,results,pipeline,directly trained towards,noisy eval - uation,pipeline directly trained towards noisy eval - uation,0.7453579306602478
translation,47,167,results,slightly better,directly trained towards,noisy eval - uation,slightly better directly trained towards noisy eval - uation,0.7399649024009705
translation,47,167,results,results,has,pipeline,results has pipeline,0.551896870136261
translation,47,170,results,wikisug - gest bow,performed,best,wikisug - gest bow performed best,0.2725537419319153
translation,47,170,results,wikireading long,has,complex models ( cnn and chunkbow ),wikireading long has complex models ( cnn and chunkbow ),0.6175873279571533
translation,47,170,results,complex models ( cnn and chunkbow ),has,outperform,complex models ( cnn and chunkbow ) has outperform,0.5880535244941711
translation,47,170,results,outperform,has,simple bow,outperform has simple bow,0.6053117513656616
translation,47,170,results,results,On,wikireading long,results On wikireading long,0.5705009698867798
translation,47,170,results,results,on,wikisug - gest bow,results on wikisug - gest bow,0.5376886129379272
translation,48,9,ablation-analysis,all levels of linguistic processing,predicting,item difficulty,all levels of linguistic processing predicting item difficulty,0.6014279127120972
translation,48,144,ablation-analysis,"linguistic , w2v and elmo features",leads to,slight improvement,"linguistic , w2v and elmo features leads to slight improvement",0.6420043706893921
translation,48,144,ablation-analysis,slight improvement,in,performance,slight improvement in performance,0.5302919745445251
translation,48,144,ablation-analysis,performance,over,individual use,performance over individual use,0.6583848595619202
translation,48,144,ablation-analysis,ablation analysis,Combining,"linguistic , w2v and elmo features","ablation analysis Combining linguistic , w2v and elmo features",0.7174177765846252
translation,48,160,ablation-analysis,removal,of,semantic ambiguity and the cognitively - motivated features,removal of semantic ambiguity and the cognitively - motivated features,0.5751898288726807
translation,48,160,ablation-analysis,removal,led to,slightly lower performance,removal led to slightly lower performance,0.7043530344963074
translation,48,160,ablation-analysis,semantic ambiguity and the cognitively - motivated features,led to,slightly lower performance,semantic ambiguity and the cognitively - motivated features led to slightly lower performance,0.6376620531082153
translation,48,160,ablation-analysis,slightly lower performance,for,both crossvalidation,slightly lower performance for both crossvalidation,0.5706295967102051
translation,48,160,ablation-analysis,slightly lower performance,for,test set,slightly lower performance for test set,0.6200507283210754
translation,48,160,ablation-analysis,slightly lower performance,for,test set,slightly lower performance for test set,0.6200507283210754
translation,48,160,ablation-analysis,both crossvalidation,on,training set,both crossvalidation on training set,0.5873450040817261
translation,48,160,ablation-analysis,ablation analysis,has,removal,ablation analysis has removal,0.5308048129081726
translation,48,128,hyperparameters,3 dense layers,of size,100,3 dense layers of size 100,0.7318434119224548
translation,48,128,hyperparameters,learning rate,=,0.001,learning rate = 0.001,0.6349165439605713
translation,48,128,hyperparameters,neural network algorithm,has,activation function,neural network algorithm has activation function,0.5116661190986633
translation,48,128,hyperparameters,neural network algorithm,has,loss function,neural network algorithm has loss function,0.4920913875102997
translation,48,6,model,features,quantifying,difficulty,features quantifying difficulty,0.7783242464065552
translation,48,6,model,difficulty,of,items,difficulty of items,0.5474801659584045
translation,48,8,results,best results,achieved when using,full feature set,best results achieved when using full feature set,0.7264799475669861
translation,48,8,results,full feature set,where,embeddings,full feature set where embeddings,0.602403461933136
translation,48,8,results,highest predictive power,followed by,linguistic features,highest predictive power followed by linguistic features,0.633201003074646
translation,48,8,results,embeddings,has,highest predictive power,embeddings has highest predictive power,0.5683993101119995
translation,48,8,results,results,has,best results,results has best results,0.542218804359436
translation,48,133,results,strongest baseline,was,zeror,strongest baseline was zeror,0.622372031211853
translation,48,133,results,zeror,with,average word length,zeror with average word length,0.668021023273468
translation,48,133,results,root mean squared error ( rmse ),has,strongest baseline,root mean squared error ( rmse ) has strongest baseline,0.5350127220153809
translation,48,133,results,average word length,has,in syllables,average word length has in syllables,0.5706385374069214
translation,48,133,results,results,In terms of,root mean squared error ( rmse ),results In terms of root mean squared error ( rmse ),0.7077675461769104
translation,48,134,results,all other baselines,performed,worse,all other baselines performed worse,0.3143960237503052
translation,48,134,results,worse,than,zeror,worse than zeror,0.7106404900550842
translation,48,134,results,worse,showing,item length,worse showing item length,0.7071100473403931
translation,48,134,results,results,has,all other baselines,results has all other baselines,0.5313310027122498
translation,48,139,results,full feature set,performs,best,full feature set performs best,0.6070371270179749
translation,48,139,results,full feature set,is,statistically significant improvement,full feature set is statistically significant improvement,0.5516076683998108
translation,48,139,results,statistically significant improvement,over,strongest baseline ( zeror ),statistically significant improvement over strongest baseline ( zeror ),0.6413319110870361
translation,48,139,results,statistically significant improvement,with,rmse reduction,statistically significant improvement with rmse reduction,0.6195592284202576
translation,48,139,results,strongest baseline ( zeror ),with,rmse reduction,strongest baseline ( zeror ) with rmse reduction,0.6256102323532104
translation,48,139,results,rmse reduction,of,approximately one point,rmse reduction of approximately one point,0.5857885479927063
translation,48,139,results,results,using,full feature set,results using full feature set,0.6677744388580322
translation,48,141,results,elmo,achieved,comparable performance,elmo achieved comparable performance,0.7432912588119507
translation,48,141,results,individual feature groups,has,linguistic,individual feature groups has linguistic,0.598242998123169
translation,48,141,results,individual feature groups,has,w2v,individual feature groups has w2v,0.552376389503479
translation,48,141,results,individual feature groups,has,elmo,individual feature groups has elmo,0.6227381825447083
translation,48,141,results,results,In terms of,individual feature groups,results In terms of individual feature groups,0.6803409457206726
translation,48,142,results,ir features,performed,notably worse,ir features performed notably worse,0.27803701162338257
translation,48,142,results,notably worse,has,rmse = 23.4,notably worse has rmse = 23.4,0.5585724115371704
translation,48,142,results,does not outperform,has,zeror baseline,does not outperform has zeror baseline,0.6074995994567871
translation,48,142,results,results,has,ir features,results has ir features,0.5669770240783691
translation,48,143,results,next   worst   result,combining,ir and linguistic features,next   worst   result combining ir and linguistic features,0.7162936329841614
translation,48,143,results,next   worst   result,is,significant improvement,next   worst   result is significant improvement,0.5609354972839355
translation,48,143,results,significant improvement,over,zeror,significant improvement over zeror,0.741989254951477
translation,48,143,results,ir and linguistic features,has,rmse = 22.63 ),ir and linguistic features has rmse = 22.63 ),0.5788767337799072
translation,48,159,results,individual classes,lead to,dramatic changes,individual classes lead to dramatic changes,0.6924924850463867
translation,48,159,results,dramatic changes,in,rmse,dramatic changes in rmse,0.5690651535987854
translation,48,159,results,results,removal of,individual classes,results removal of individual classes,0.6714748740196228
translation,48,165,results,best results,achieved when combining,"all types of available features ( linguistic , ir , word2vec , and elmo )","best results achieved when combining all types of available features ( linguistic , ir , word2vec , and elmo )",0.618428647518158
translation,48,165,results,"all types of available features ( linguistic , ir , word2vec , and elmo )",showed,statistically significant improvement,"all types of available features ( linguistic , ir , word2vec , and elmo ) showed statistically significant improvement",0.6594836711883545
translation,48,165,results,statistically significant improvement,over,baselines,statistically significant improvement over baselines,0.6608260869979858
translation,48,165,results,results,has,best results,results has best results,0.542218804359436
translation,48,166,results,ir features,performed,poorly,ir features performed poorly,0.2800993025302887
translation,48,166,results,ir features,outperformed by,"linguistic , word2vec , and elmo features","ir features outperformed by linguistic , word2vec , and elmo features",0.6995564103126526
translation,48,166,results,individual feature classes,has,ir features,individual feature classes has ir features,0.5641904473304749
translation,48,166,results,results,In terms of,individual feature classes,results In terms of individual feature classes,0.6956691741943359
translation,49,192,ablation-analysis,model,through,hard updates,model through hard updates,0.70611971616745
translation,49,192,ablation-analysis,hard updates,play,significant role,hard updates play significant role,0.7763352394104004
translation,49,192,ablation-analysis,significant role,to,performance,significant role to performance,0.5562404990196228
translation,49,192,ablation-analysis,ablation analysis,precomputing,solution set,ablation analysis precomputing solution set,0.8182023763656616
translation,49,192,ablation-analysis,ablation analysis,training,model,ablation analysis training model,0.7077123522758484
translation,49,186,baselines,traditional and recentlydeveloped reward - based algorithms,for,weak supervision,traditional and recentlydeveloped reward - based algorithms for weak supervision,0.5994559526443481
translation,49,186,baselines,traditional and recentlydeveloped reward - based algorithms,including,beam- based mml,traditional and recentlydeveloped reward - based algorithms including beam- based mml,0.6859604120254517
translation,49,186,baselines,mapo,has,memory - augmented policy optimization,mapo has memory - augmented policy optimization,0.5294234752655029
translation,49,186,baselines,baselines,compare with,traditional and recentlydeveloped reward - based algorithms,baselines compare with traditional and recentlydeveloped reward - based algorithms,0.6271973252296448
translation,49,150,experiments,uncased version,of,bert base,uncased version of bert base,0.6223145127296448
translation,49,152,experiments,batch size,of,20,batch size of 20,0.6912065744400024
translation,49,152,experiments,batch size,of,192,batch size of 192,0.6656278371810913
translation,49,152,experiments,20,for,two reading comprehension tasks,20 for two reading comprehension tasks,0.5158293843269348
translation,49,152,experiments,192,for,two open-domain qa tasks,192 for two open-domain qa tasks,0.5333696007728577
translation,49,154,experiments,opendomain qa tasks,retrieve,50 wikipedia articles,opendomain qa tasks retrieve 50 wikipedia articles,0.6343952417373657
translation,49,154,experiments,50 wikipedia articles,through,tf -idf,50 wikipedia articles through tf -idf,0.5867186784744263
translation,49,154,experiments,bm25,to retrieve,20,bm25 to retrieve 20,0.6604409217834473
translation,49,154,experiments,bm25,to retrieve,80,bm25 to retrieve 80,0.6490324139595032
translation,49,155,experiments,"10 , 20 , 40 and 80 paragraphs",on,development set,"10 , 20 , 40 and 80 paragraphs on development set",0.5709112286567688
translation,49,155,experiments,"10 , 20 , 40 and 80 paragraphs",to choose,number of paragraphs,"10 , 20 , 40 and 80 paragraphs to choose number of paragraphs",0.6761054396629333
translation,49,155,experiments,number of paragraphs,to use on,test set,number of paragraphs to use on test set,0.6696751117706299
translation,49,151,hyperparameters,documents,into,set of segments,documents into set of segments,0.5865536332130432
translation,49,151,hyperparameters,set of segments,up to,300 tokens,set of segments up to 300 tokens,0.6600926518440247
translation,49,153,hyperparameters,subset of segments,in,triviaqa,subset of segments in triviaqa,0.5542117953300476
translation,49,153,hyperparameters,subset of segments,through,tf - idf similarity,subset of segments through tf - idf similarity,0.6398704648017883
translation,49,153,hyperparameters,tf - idf similarity,between,segment,tf - idf similarity between segment,0.6324780583381653
translation,49,153,hyperparameters,question,to maintain,reasonable length,question to maintain reasonable length,0.629503071308136
translation,49,153,hyperparameters,hyperparameters,filter,subset of segments,hyperparameters filter subset of segments,0.7959239482879639
translation,49,176,hyperparameters,document,to be,400 words,document to be 400 words,0.6238282322883606
translation,49,176,hyperparameters,document,up to,400 words,document up to 400 words,0.6607813239097595
translation,49,176,hyperparameters,hyperparameters,truncate,document,hyperparameters truncate document,0.7655335068702698
translation,49,177,hyperparameters,batch size,of,14 and 10,batch size of 14 and 10,0.6753751039505005
translation,49,177,hyperparameters,14 and 10,for,qanet and bert,14 and 10 for qanet and bert,0.747985303401947
translation,49,177,hyperparameters,hyperparameters,use,batch size,hyperparameters use batch size,0.6251612901687622
translation,49,6,model,discrete latent variable learning problems,with,"precomputed , task - specific set of possible solutions ( e.g. different mentions or equations )","discrete latent variable learning problems with precomputed , task - specific set of possible solutions ( e.g. different mentions or equations )",0.5295834541320801
translation,49,6,model,"precomputed , task - specific set of possible solutions ( e.g. different mentions or equations )",that contains,one correct option,"precomputed , task - specific set of possible solutions ( e.g. different mentions or equations ) that contains one correct option",0.5917690992355347
translation,49,7,model,hard em learning scheme,computes,gradients,hard em learning scheme computes gradients,0.6929256916046143
translation,49,7,model,gradients,relative to,most likely solution,gradients relative to most likely solution,0.72586989402771
translation,49,7,model,most likely solution,at,each update,most likely solution at each update,0.5470287203788757
translation,49,7,model,model,develop,hard em learning scheme,model develop hard em learning scheme,0.6251067519187927
translation,49,20,model,wide range of weakly supervised qa tasks,as,discrete latent - variable learning problems,wide range of weakly supervised qa tasks as discrete latent - variable learning problems,0.4834078252315521
translation,49,20,model,model,formulate,wide range of weakly supervised qa tasks,model formulate wide range of weakly supervised qa tasks,0.6315979361534119
translation,49,28,model,set of possible solutions,as,discrete latent variable,set of possible solutions as discrete latent variable,0.5159873366355896
translation,49,28,model,learning strategy,uses,hard - em - style parameter updates,learning strategy uses hard - em - style parameter updates,0.5910838842391968
translation,49,28,model,model,model,set of possible solutions,model model set of possible solutions,0.7866813540458679
translation,49,28,model,model,develop,learning strategy,model develop learning strategy,0.6715988516807556
translation,49,29,model,most likely solution,according to,current model,most likely solution according to current model,0.6620219945907593
translation,49,29,model,current model,from,precomputed set,current model from precomputed set,0.5587230324745178
translation,49,29,model,model parameters,to further encourage,own prediction,model parameters to further encourage own prediction,0.6590468287467957
translation,49,29,model,model,predicts,most likely solution,model predicts most likely solution,0.7255035042762756
translation,49,29,model,model,updates,model parameters,model updates model parameters,0.7302922606468201
translation,49,185,model,model,to incorporate,mml objective,model to incorporate mml objective,0.6979445219039917
translation,49,185,model,model,to incorporate,our hard - em learning approach,model to incorporate our hard - em learning approach,0.6711239814758301
translation,49,185,model,our hard - em learning approach,for,weakly -,our hard - em learning approach for weakly -,0.6343114972114563
translation,49,185,model,weakly -,has,supervised setting,weakly - has supervised setting,0.5535730719566345
translation,49,185,model,model,modify,model,model modify model,0.6890363097190857
translation,49,185,model,model,to incorporate,mml objective,model to incorporate mml objective,0.6979445219039917
translation,49,33,results,significantly outperforms,which use,heuristic supervision and mml updates,significantly outperforms which use heuristic supervision and mml updates,0.6432734131813049
translation,49,33,results,significantly outperforms,including,absolute gains,significantly outperforms including absolute gains,0.7158881425857544
translation,49,33,results,previous methods,which use,heuristic supervision and mml updates,previous methods which use heuristic supervision and mml updates,0.6554680466651917
translation,49,33,results,previous methods,including,absolute gains,previous methods including absolute gains,0.6861962676048279
translation,49,33,results,absolute gains,of,2 - 10 %,absolute gains of 2 - 10 %,0.5721089243888855
translation,49,33,results,learning approach,has,significantly outperforms,learning approach has significantly outperforms,0.6232625246047974
translation,49,33,results,significantly outperforms,has,previous methods,significantly outperforms has previous methods,0.5611478686332703
translation,49,33,results,results,has,learning approach,results has learning approach,0.5982778072357178
translation,49,34,results,outperforms,by,13 % absolute percentage,outperforms by 13 % absolute percentage,0.6335940957069397
translation,49,34,results,recent state - of- the - art reward - based semantic parsing algorithms,by,13 % absolute percentage,recent state - of- the - art reward - based semantic parsing algorithms by 13 % absolute percentage,0.5653892159461975
translation,49,34,results,13 % absolute percentage,on,wikisql,13 % absolute percentage on wikisql,0.5523440837860107
translation,49,34,results,outperforms,has,recent state - of- the - art reward - based semantic parsing algorithms,outperforms has recent state - of- the - art reward - based semantic parsing algorithms,0.5329222679138184
translation,49,34,results,results,has,outperforms,results has outperforms,0.6657275557518005
translation,49,160,results,first - only,is,strong baseline,first - only is strong baseline,0.5804408192634583
translation,49,160,results,strong baseline,across,datasets,strong baseline across datasets,0.7375648617744446
translation,49,160,results,results,observe,first - only,results observe first - only,0.5772736072540283
translation,49,162,results,mml,achieves,comparable result,mml achieves comparable result,0.7023136615753174
translation,49,162,results,comparable result,to,first - only baseline,comparable result to first - only baseline,0.5415984392166138
translation,49,162,results,outperforms,by,2 + f1 / rouge -l/ em,outperforms by 2 + f1 / rouge -l/ em,0.676609456539154
translation,49,162,results,outperforms,on,all datasets,outperforms on all datasets,0.5181515216827393
translation,49,162,results,our learning method,has,outperforms,our learning method has outperforms,0.6286894083023071
translation,49,162,results,results,has,mml,results has mml,0.5572437047958374
translation,49,163,results,our method,achieves,new state-ofthe- art,our method achieves new state-ofthe- art,0.6123746037483215
translation,49,163,results,our method,comparable to,state - of- the - art,our method comparable to state - of- the - art,0.5515775680541992
translation,49,163,results,new state-ofthe- art,on,narrativeqa,new state-ofthe- art on narrativeqa,0.5452203154563904
translation,49,163,results,new state-ofthe- art,on,triviaqa -open,new state-ofthe- art on triviaqa -open,0.5944923758506775
translation,49,163,results,new state-ofthe- art,on,naturalquestions - open,new state-ofthe- art on naturalquestions - open,0.5719962120056152
translation,49,163,results,new state-ofthe- art,comparable to,state - of- the - art,new state-ofthe- art comparable to state - of- the - art,0.6105839014053345
translation,49,163,results,results,has,our method,results has our method,0.5589964985847473
translation,49,171,results,method,has,outperforms,method has outperforms,0.6569275856018066
translation,49,171,results,outperforms,has,previous weakly - supervised methods,outperforms has previous weakly - supervised methods,0.5325866341590881
translation,49,171,results,results,has,method,results has method,0.49327942728996277
translation,49,180,results,mml,by,large margin,mml by large margin,0.5445300936698914
translation,49,180,results,our training strategy,has,outperforms,our training strategy has outperforms,0.6296586394309998
translation,49,180,results,outperforms,has,first - only baseline,outperforms has first - only baseline,0.5900642275810242
translation,49,180,results,results,has,our training strategy,results has our training strategy,0.5654891729354858
translation,49,191,results,significantly outperforms,including,10 % gain,significantly outperforms including 10 % gain,0.70287024974823
translation,49,191,results,all the weaklysupervised learning algorithms,including,10 % gain,all the weaklysupervised learning algorithms including 10 % gain,0.6426146626472473
translation,49,191,results,10 % gain,over,previous state of the art,10 % gain over previous state of the art,0.6356233358383179
translation,49,191,results,our training method,has,significantly outperforms,our training method has significantly outperforms,0.6050127148628235
translation,49,191,results,significantly outperforms,has,all the weaklysupervised learning algorithms,significantly outperforms has all the weaklysupervised learning algorithms,0.5733514428138733
translation,49,191,results,results,shows,our training method,results shows our training method,0.644493043422699
translation,49,194,results,previous models,with,full supervision,previous models with full supervision,0.5818666815757751
translation,49,194,results,previous models,has,our results,previous models has our results,0.5191793441772461
translation,49,194,results,full supervision,has,our results,full supervision has our results,0.5415266752243042
translation,49,194,results,outperform,has,most of the published results,outperform has most of the published results,0.6135975122451782
translation,49,194,results,results,Comparing to,previous models,results Comparing to previous models,0.6808536052703857
translation,50,66,baselines,baseline,based on,qg model,baseline based on qg model,0.6539983749389648
translation,50,66,baselines,baselines,has,baseline,baselines has baseline,0.6124745607376099
translation,50,77,experiments,partial copy mechanism,set,threshold,partial copy mechanism set threshold,0.6764705181121826
translation,50,77,experiments,threshold,to,0.7,threshold to 0.7,0.5700178742408752
translation,50,73,hyperparameters,glove,to initialize,word embeddings,glove to initialize word embeddings,0.6981847286224365
translation,50,73,hyperparameters,glove,trained,model,glove trained model,0.729285478591919
translation,50,73,hyperparameters,),to initialize,word embeddings,) to initialize word embeddings,0.7369215488433838
translation,50,73,hyperparameters,glove,has,),glove has ),0.6335808038711548
translation,50,73,hyperparameters,hyperparameters,used,glove,hyperparameters used glove,0.636309802532196
translation,50,75,hyperparameters,"adam ( kingma and ba , 2015 )",as,optimizer,"adam ( kingma and ba , 2015 ) as optimizer",0.5069490075111389
translation,50,75,hyperparameters,optimizer,during,training,optimizer during training,0.7262166738510132
translation,50,75,hyperparameters,hyperparameters,used,"adam ( kingma and ba , 2015 )","hyperparameters used adam ( kingma and ba , 2015 )",0.5725852847099304
translation,50,76,hyperparameters,beam size,set to,20,beam size set to 20,0.7607793807983398
translation,50,76,hyperparameters,20,for,decoder,20 for decoder,0.6392994523048401
translation,50,76,hyperparameters,hyperparameters,has,beam size,hyperparameters has beam size,0.516274631023407
translation,50,5,model,model,propose,two methods,model propose two methods,0.663135290145874
translation,50,6,model,partial copy mechanism,prioritize,words,partial copy mechanism prioritize words,0.677675724029541
translation,50,6,model,partial copy mechanism,By,qa - based reranker,partial copy mechanism By qa - based reranker,0.5766848921775818
translation,50,6,model,words,that are,morphologically close,words that are morphologically close,0.6442387700080872
translation,50,6,model,words,in,input passage,words in input passage,0.517383337020874
translation,50,6,model,morphologically close,to,words,morphologically close to words,0.5728039741516113
translation,50,6,model,words,in,input passage,words in input passage,0.517383337020874
translation,50,6,model,qa - based reranker,n-best list of,question candidates,qa - based reranker n-best list of question candidates,0.7508302927017212
translation,50,6,model,qa - based reranker,select,questions,qa - based reranker select questions,0.6572319269180298
translation,50,6,model,questions,preferred by,qa and qg model,questions preferred by qa and qg model,0.7271066904067993
translation,50,6,model,model,By,partial copy mechanism,model By partial copy mechanism,0.5758596062660217
translation,50,20,model,partial copy method,to enhance,existing copy mechanism,partial copy method to enhance existing copy mechanism,0.6751839518547058
translation,50,20,model,model,present,partial copy method,model present partial copy method,0.6808545589447021
translation,50,21,model,fine- grained partial copy mechanism,enables,qg model,fine- grained partial copy mechanism enables qg model,0.6585444808006287
translation,50,21,model,qg model,to copy,morphologically changed words,qg model to copy morphologically changed words,0.5724325776100159
translation,50,21,model,morphologically changed words,from,passage,morphologically changed words from passage,0.5867725610733032
translation,50,21,model,model,has,fine- grained partial copy mechanism,model has fine- grained partial copy mechanism,0.5357415080070496
translation,50,22,model,qa - based reranker,to rerank,qg results,qa - based reranker to rerank qg results,0.7300282120704651
translation,50,22,model,model,propose,qa - based reranker,model propose qa - based reranker,0.6671820282936096
translation,50,23,model,neural qa model,rerank them,qa model scores,neural qa model rerank them qa model scores,0.7272863984107971
translation,50,23,model,model,use,neural qa model,model use neural qa model,0.6111960411071777
translation,50,51,model,qa - based reranker,to rerank,n-best questions,qa - based reranker to rerank n-best questions,0.7441284656524658
translation,50,51,model,n-best questions,generated by,baseline decoder,n-best questions generated by baseline decoder,0.6147996783256531
translation,50,51,model,model,propose,qa - based reranker,model propose qa - based reranker,0.6671820282936096
translation,50,87,model,method,to enhance,existing copy mechanism,method to enhance existing copy mechanism,0.6563442945480347
translation,50,87,model,more relevant,to,passages,more relevant to passages,0.5939725637435913
translation,50,87,model,more relevant,to,target answers,more relevant to target answers,0.5562308430671692
translation,50,87,model,generated questions,has,more relevant,generated questions has more relevant,0.5508657693862915
translation,50,81,results,substantially improves,in terms of,all evaluation metrics,substantially improves in terms of all evaluation metrics,0.6582970023155212
translation,50,81,results,baseline,in terms of,all evaluation metrics,baseline in terms of all evaluation metrics,0.5788062810897827
translation,50,81,results,our methods,has,substantially improves,our methods has substantially improves,0.5883679389953613
translation,50,81,results,substantially improves,has,baseline,substantially improves has baseline,0.6193900108337402
translation,50,81,results,results,observe,our methods,results observe our methods,0.5726607441902161
translation,50,82,results,"combination of the proposed two methods ( ? 1 = 1 , ? 2 = 0.2 )",achieve,best performance,"combination of the proposed two methods ( ? 1 = 1 , ? 2 = 0.2 ) achieve best performance",0.5900845527648926
translation,50,82,results,best performance,gaining,0.73 bleu - 4,best performance gaining 0.73 bleu - 4,0.6119194626808167
translation,50,82,results,best performance,gaining,nearly 1 me- teor point of improvements,best performance gaining nearly 1 me- teor point of improvements,0.6755315065383911
translation,50,82,results,nearly 1 me- teor point of improvements,over,baseline,nearly 1 me- teor point of improvements over baseline,0.7394589185714722
translation,50,82,results,results,has,"combination of the proposed two methods ( ? 1 = 1 , ? 2 = 0.2 )","results has combination of the proposed two methods ( ? 1 = 1 , ? 2 = 0.2 )",0.5247766971588135
translation,50,83,results,proposed partial copy mechanism,obtain,substantial improvements,proposed partial copy mechanism obtain substantial improvements,0.5449423789978027
translation,50,83,results,substantial improvements,over,baseline,substantial improvements over baseline,0.7634895443916321
translation,50,83,results,substantial improvements,especially,bleu - 1 and bleu -2,substantial improvements especially bleu - 1 and bleu -2,0.637191653251648
translation,50,83,results,substantial improvements,in terms of,bleu - 1 and bleu -2,substantial improvements in terms of bleu - 1 and bleu -2,0.6274107098579407
translation,50,83,results,results,has,proposed partial copy mechanism,results has proposed partial copy mechanism,0.5920055508613586
translation,50,85,results,qabased reranking,obtain,further improvements,qabased reranking obtain further improvements,0.562093198299408
translation,50,85,results,further improvements,over,partial copy mechanism,further improvements over partial copy mechanism,0.7011181712150574
translation,50,85,results,results,application of,qabased reranking,results application of qabased reranking,0.6636366248130798
translation,50,88,results,words ( or their other morphological forms ),copied from,passages,words ( or their other morphological forms ) copied from passages,0.6177092790603638
translation,50,88,results,passages,in,generated questions,passages in generated questions,0.5073651075363159
translation,50,88,results,generated questions,increases from,75.49 %,generated questions increases from 75.49 %,0.6318263411521912
translation,50,88,results,75.49 %,to,78.74 %,75.49 % to 78.74 %,0.5935717225074768
translation,50,88,results,75.49 %,has,baseline ),75.49 % has baseline ),0.563776969909668
translation,51,141,ablation-analysis,cqu method,with,proposed kvqu method,cqu method with proposed kvqu method,0.6216905117034912
translation,51,141,ablation-analysis,kv - memnn model,could further gain,improvement,kv - memnn model could further gain improvement,0.6991186738014221
translation,51,141,ablation-analysis,improvement,of,1.2 %,improvement of 1.2 %,0.5717630982398987
translation,51,141,ablation-analysis,cqu method,has,kv - memnn model,cqu method has kv - memnn model,0.5450974702835083
translation,51,141,ablation-analysis,proposed kvqu method,has,kv - memnn model,proposed kvqu method has kv - memnn model,0.5569524168968201
translation,51,141,ablation-analysis,ablation analysis,replacing,cqu method,ablation analysis replacing cqu method,0.6603690385818481
translation,51,127,baselines,cqu + ar,uses,conventional query updating method ( cqu ),cqu + ar uses conventional query updating method ( cqu ),0.6084636449813843
translation,51,127,baselines,cqu + ar,performs,linear transformation,cqu + ar performs linear transformation,0.6405022740364075
translation,51,127,baselines,conventional query updating method ( cqu ),performs,linear transformation,conventional query updating method ( cqu ) performs linear transformation,0.5898134112358093
translation,51,127,baselines,linear transformation,over,sum of the query and value representations,linear transformation over sum of the query and value representations,0.6718038320541382
translation,51,127,baselines,baselines,has,cqu + ar,baselines has cqu + ar,0.607564389705658
translation,51,129,baselines,kvqu + ar,applies,approach,kvqu + ar applies approach,0.6817086338996887
translation,51,129,baselines,kvqu + ar,considers,key and value representations,kvqu + ar considers key and value representations,0.6751586198806763
translation,51,129,baselines,kvqu + ar,both,key and value representations,kvqu + ar both key and value representations,0.6734061241149902
translation,51,129,baselines,key and value representations,in,query updating,key and value representations in query updating,0.5215213298797607
translation,51,129,baselines,baselines,has,kvqu + ar,baselines has kvqu + ar,0.6123689413070679
translation,51,132,baselines,cqu +sq,uses,cqu method,cqu +sq uses cqu method,0.6555229425430298
translation,51,132,baselines,cqu +sq,applies,sq approach,cqu +sq applies sq approach,0.6015871167182922
translation,51,132,baselines,cqu method,to update,query representations,cqu method to update query representations,0.7373722791671753
translation,51,132,baselines,sq approach,to obtain,answers,sq approach to obtain answers,0.6650673151016235
translation,51,132,baselines,baselines,has,cqu +sq,baselines has cqu +sq,0.6011639833450317
translation,51,133,baselines,kvqu +sq,uses,kvqu method,kvqu +sq uses kvqu method,0.6488324403762817
translation,51,133,baselines,kvqu +sq,uses,sq approach,kvqu +sq uses sq approach,0.6091770529747009
translation,51,133,baselines,kvqu +sq,to update,query representations,kvqu +sq to update query representations,0.744827926158905
translation,51,133,baselines,kvqu +sq,adopts,sq approach,kvqu +sq adopts sq approach,0.6769766807556152
translation,51,133,baselines,kvqu method,to update,query representations,kvqu method to update query representations,0.742360532283783
translation,51,133,baselines,sq approach,to obtain,answers,sq approach to obtain answers,0.6650673151016235
translation,51,133,baselines,baselines,has,kvqu +sq,baselines has kvqu +sq,0.5937622785568237
translation,51,134,baselines,stop + cqu + ar,introduces,stop key,stop + cqu + ar introduces stop key,0.6449002623558044
translation,51,134,baselines,stop + cqu + ar,uses,conventional query updating method,stop + cqu + ar uses conventional query updating method,0.6176190376281738
translation,51,134,baselines,stop + cqu + ar,uses,answer representations,stop + cqu + ar uses answer representations,0.6534950137138367
translation,51,134,baselines,stop key,into,memory,stop key into memory,0.6435943841934204
translation,51,134,baselines,answer representations,to find,answers,answer representations to find answers,0.6269429922103882
translation,51,134,baselines,baselines,has,stop + cqu + ar,baselines has stop + cqu + ar,0.6069679260253906
translation,51,135,baselines,stop + cqu +sq,introduces,stop key,stop + cqu +sq introduces stop key,0.6455509662628174
translation,51,135,baselines,stop + cqu +sq,uses,conventional query updating method,stop + cqu +sq uses conventional query updating method,0.6006348729133606
translation,51,135,baselines,stop + cqu +sq,uses,sq approach,stop + cqu +sq uses sq approach,0.600155234336853
translation,51,135,baselines,stop + cqu +sq,uses,sq approach,stop + cqu +sq uses sq approach,0.600155234336853
translation,51,135,baselines,conventional query updating method,uses,sq approach,conventional query updating method uses sq approach,0.5871499180793762
translation,51,135,baselines,sq approach,to obtain,answers,sq approach to obtain answers,0.6650673151016235
translation,51,135,baselines,baselines,has,stop + cqu +sq,baselines has stop + cqu +sq,0.613135576248169
translation,51,136,baselines,stop + kvqu + ar,introduces,stop key,stop + kvqu + ar introduces stop key,0.6429842114448547
translation,51,136,baselines,stop + kvqu + ar,uses,kvqu approach,stop + kvqu + ar uses kvqu approach,0.6386445760726929
translation,51,136,baselines,stop + kvqu + ar,adopts,ar approach,stop + kvqu + ar adopts ar approach,0.6928088068962097
translation,51,136,baselines,stop key,to,memory,stop key to memory,0.5953104496002197
translation,51,136,baselines,kvqu approach,to update,query representations,kvqu approach to update query representations,0.7129215002059937
translation,51,136,baselines,ar approach,to retrieve,answers,ar approach to retrieve answers,0.761290431022644
translation,51,136,baselines,baselines,has,stop + kvqu + ar,baselines has stop + kvqu + ar,0.6182475686073303
translation,51,137,baselines,stop + kvqu +sq,retrieves,answers,stop + kvqu +sq retrieves answers,0.7533867359161377
translation,51,137,baselines,stop key,applies,kvqu query updating method,stop key applies kvqu query updating method,0.5950785875320435
translation,51,137,baselines,answers,using,post-constructed structured queries,answers using post-constructed structured queries,0.683183491230011
translation,51,137,baselines,baselines,has,stop + kvqu +sq,baselines has stop + kvqu +sq,0.6195349097251892
translation,51,118,hyperparameters,adam optimizer,with,mini-,adam optimizer with mini-,0.6611629724502563
translation,51,118,hyperparameters,mini-,has,batch size 60,mini- has batch size 60,0.6199170351028442
translation,51,118,hyperparameters,hyperparameters,trained using,adam optimizer,hyperparameters trained using adam optimizer,0.6941167116165161
translation,51,119,hyperparameters,learning rate,set to,0.001,learning rate set to 0.001,0.6954665780067444
translation,51,119,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,51,120,hyperparameters,complexity,of,model,complexity of model,0.592278778553009
translation,51,120,hyperparameters,penalized,by adding,l2 regularization,penalized by adding l2 regularization,0.6787317395210266
translation,51,120,hyperparameters,l2 regularization,to,cross entropy loss function,l2 regularization to cross entropy loss function,0.4206262230873108
translation,51,120,hyperparameters,hyperparameters,has,complexity,hyperparameters has complexity,0.5233856439590454
translation,51,121,hyperparameters,gradients,are,clipped,gradients are clipped,0.6525067090988159
translation,51,121,hyperparameters,clipped,when,norm,clipped when norm,0.6805716156959534
translation,51,121,hyperparameters,norm,bigger than,20,norm bigger than 20,0.7433955669403076
translation,51,121,hyperparameters,hyperparameters,has,gradients,hyperparameters has gradients,0.5161670446395874
translation,51,122,hyperparameters,hop size,set to,3,hop size set to 3,0.7345436215400696
translation,51,122,hyperparameters,hyperparameters,has,hop size,hyperparameters has hop size,0.5120497345924377
translation,51,123,hyperparameters,word embeddings,using,pre-trained word representations,word embeddings using pre-trained word representations,0.5909133553504944
translation,51,123,hyperparameters,pre-trained word representations,from,turian et al . [ 2010 ],pre-trained word representations from turian et al . [ 2010 ],0.48572009801864624
translation,51,123,hyperparameters,pre-trained word representations,from,dimension,pre-trained word representations from dimension,0.5442720055580139
translation,51,123,hyperparameters,dimension,of,word embedding,dimension of word embedding,0.5654559135437012
translation,51,123,hyperparameters,word embedding,set to,50,word embedding set to 50,0.6318463683128357
translation,51,123,hyperparameters,hyperparameters,initialize,word embeddings,hyperparameters initialize word embeddings,0.6877135038375854
translation,51,6,model,novel mechanism,to enable,conventional kv - memnns models,novel mechanism to enable conventional kv - memnns models,0.708910346031189
translation,51,6,model,conventional kv - memnns models,to perform,interpretable reasoning,conventional kv - memnns models to perform interpretable reasoning,0.6786234378814697
translation,51,6,model,interpretable reasoning,for,complex questions,interpretable reasoning for complex questions,0.6121284365653992
translation,51,6,model,model,propose,novel mechanism,model propose novel mechanism,0.7055184841156006
translation,51,7,model,new query updating strategy,to mask,previously - addressed memory information,new query updating strategy to mask previously - addressed memory information,0.662143349647522
translation,51,7,model,previously - addressed memory information,from,query representations,previously - addressed memory information from query representations,0.540843665599823
translation,51,7,model,novel stop strategy,to avoid,invalid or repeated memory reading,novel stop strategy to avoid invalid or repeated memory reading,0.6823697686195374
translation,51,7,model,invalid or repeated memory reading,without,strong annotation signals,invalid or repeated memory reading without strong annotation signals,0.7304252982139587
translation,51,7,model,model,design,new query updating strategy,model design new query updating strategy,0.5943624973297119
translation,51,7,model,model,introduce,novel stop strategy,model introduce novel stop strategy,0.7201709747314453
translation,51,30,model,novel solution,to make,conventional kv - memnns,novel solution to make conventional kv - memnns,0.606744110584259
translation,51,30,model,conventional kv - memnns,to,open domain kb - qa,conventional kv - memnns to open domain kb - qa,0.5609800219535828
translation,51,30,model,feasible,to,open domain kb - qa,feasible to open domain kb - qa,0.6098166108131409
translation,51,30,model,conventional kv - memnns,has,feasible,conventional kv - memnns has feasible,0.6051913499832153
translation,51,30,model,model,propose,novel solution,model propose novel solution,0.7334994673728943
translation,51,31,model,flexible kv - memnn solution,can work in both,ir,flexible kv - memnn solution can work in both ir,0.729831337928772
translation,51,31,model,flexible kv - memnn solution,can work in both,semantic parsing style,flexible kv - memnn solution can work in both semantic parsing style,0.7247031331062317
translation,51,31,model,semantic parsing style,with,large-scale memory,semantic parsing style with large-scale memory,0.5500425100326538
translation,51,31,model,model,introduce,flexible kv - memnn solution,model introduce flexible kv - memnn solution,0.618322491645813
translation,51,32,model,novel query updating method,able to decompose,complex questions,novel query updating method able to decompose complex questions,0.7191404700279236
translation,51,32,model,complex questions,precisely address,relevant key,complex questions precisely address relevant key,0.7128734588623047
translation,51,32,model,relevant key,at,each hop,relevant key at each hop,0.5711073875427246
translation,51,32,model,model,present,novel query updating method,model present novel query updating method,0.6415788531303406
translation,51,33,model,new stop strategy,during,memory readings,new stop strategy during memory readings,0.7181291580200195
translation,51,33,model,new stop strategy,guides,our model,new stop strategy guides our model,0.676318347454071
translation,51,33,model,memory readings,imports,special key stop,memory readings imports special key stop,0.8017073273658752
translation,51,33,model,special key stop,into,memory,special key stop into memory,0.6432393789291382
translation,51,33,model,our model,to avoid,repeated or invalid memory readings,our model to avoid repeated or invalid memory readings,0.698698103427887
translation,51,33,model,model,introduce,new stop strategy,model introduce new stop strategy,0.7134556174278259
translation,51,34,model,proposed model,reason over,memory slots,proposed model reason over memory slots,0.7682180404663086
translation,51,34,model,memory slots,with,weak supervision,memory slots with weak supervision,0.6505867838859558
translation,51,34,model,model,has,proposed model,model has proposed model,0.566501796245575
translation,51,211,model,kv - memnns,as,semantic parsing module,kv - memnns as semantic parsing module,0.5054380297660828
translation,51,211,model,semantic parsing module,to approach,open-domain kb - qa task,semantic parsing module to approach open-domain kb - qa task,0.677646815776825
translation,51,211,model,model,apply,kv - memnns,model apply kv - memnns,0.625627338886261
translation,51,212,model,novel stop strategy,to derive,structured queries,novel stop strategy to derive structured queries,0.6871063113212585
translation,51,212,model,structured queries,with,flexible number of query triples,structured queries with flexible number of query triples,0.5853744745254517
translation,51,212,model,new query updating method,considers,already - addressed keys,new query updating method considers already - addressed keys,0.6387967467308044
translation,51,212,model,already - addressed keys,in,previous hops,already - addressed keys in previous hops,0.5362427234649658
translation,51,212,model,already - addressed keys,in,value representations,already - addressed keys in value representations,0.5343640446662903
translation,51,212,model,already - addressed keys,as well as,value representations,already - addressed keys as well as value representations,0.6241673231124878
translation,51,212,model,model,introduce,novel stop strategy,model introduce novel stop strategy,0.7201709747314453
translation,51,138,results,methods,on,test set,methods on test set,0.5530261397361755
translation,51,138,results,test set,of,webquestions,test set of webquestions,0.5825878381729126
translation,51,139,results,our main model ( stop + kvqu + sq ),performs,best,our main model ( stop + kvqu + sq ) performs best,0.6479549407958984
translation,51,139,results,best,among,all its variations,best among all its variations,0.6046523451805115
translation,51,139,results,state - of - the - art methods,on,webquestions,state - of - the - art methods on webquestions,0.5553900599479675
translation,51,139,results,significantly outperforms,has,state - of - the - art methods,significantly outperforms has state - of - the - art methods,0.557465672492981
translation,51,139,results,results,see that,our main model ( stop + kvqu + sq ),results see that our main model ( stop + kvqu + sq ),0.6701313853263855
translation,51,140,results,still outperform,has,traditional semantic parsing models,still outperform has traditional semantic parsing models,0.5883299112319946
translation,51,140,results,results,see that,conventional kv - memnn model,results see that conventional kv - memnn model,0.6121708750724792
translation,51,140,results,results,even,conventional kv - memnn model,results even conventional kv - memnn model,0.6425886154174805
translation,51,142,results,kv - memnn model,capable to perform,proper multi-hop reasoning,kv - memnn model capable to perform proper multi-hop reasoning,0.6621368527412415
translation,51,142,results,proper multi-hop reasoning,over,memory,proper multi-hop reasoning over memory,0.6811357140541077
translation,51,142,results,outperform,by,large margin,outperform by large margin,0.6385749578475952
translation,51,142,results,most existing methods,by,large margin,most existing methods by large margin,0.5839926600456238
translation,51,142,results,stop strategy,has,kv - memnn model,stop strategy has kv - memnn model,0.5778406262397766
translation,51,142,results,outperform,has,most existing methods,outperform has most existing methods,0.5772945284843445
translation,51,142,results,results,introducing,stop strategy,results introducing stop strategy,0.7169741988182068
translation,51,151,results,improve,by,around 4 %,improve by around 4 %,0.6493058204650879
translation,51,151,results,stop strategy,has,almost all models,stop strategy has almost all models,0.5448446869850159
translation,51,151,results,almost all models,has,improve,almost all models has improve,0.5812916159629822
translation,51,151,results,results,when introducing,stop strategy,results when introducing stop strategy,0.7628684043884277
translation,51,170,results,stop + kvqu +sq,achieves,more improvement,stop + kvqu +sq achieves more improvement,0.6520871520042419
translation,51,170,results,more improvement,than,stop + cqu +sq,more improvement than stop + cqu +sq,0.590657651424408
translation,51,170,results,ar to sq,has,stop + kvqu +sq,ar to sq has stop + kvqu +sq,0.6737378239631653
translation,51,180,results,kvqu +sq,suffers from,incorrect structured queries,kvqu +sq suffers from incorrect structured queries,0.69865882396698
translation,51,180,results,results,has,kvqu +sq,results has kvqu +sq,0.5251843929290771
translation,51,181,results,stop key,into,memory,stop key into memory,0.6435943841934204
translation,51,181,results,models,using,structured queries ( stop + *+ sq ),models using structured queries ( stop + *+ sq ),0.6940382719039917
translation,51,181,results,corresponding versions,using,ranking based method ( stop + *+ ar ),corresponding versions using ranking based method ( stop + *+ ar ),0.7218856811523438
translation,51,181,results,stop key,has,models,stop key has models,0.6096855998039246
translation,51,181,results,models,has,significantly outperform,models has significantly outperform,0.6078265309333801
translation,51,181,results,structured queries ( stop + *+ sq ),has,significantly outperform,structured queries ( stop + *+ sq ) has significantly outperform,0.5809997916221619
translation,51,181,results,significantly outperform,has,corresponding versions,significantly outperform has corresponding versions,0.5758808851242065
translation,51,181,results,results,introduce,stop key,results introduce stop key,0.677244246006012
translation,51,189,results,model,with,ar answer retrieval method,model with ar answer retrieval method,0.578164279460907
translation,51,189,results,model,achieves,best performance,model achieves best performance,0.6728841066360474
translation,51,189,results,best performance,with,hop size,best performance with hop size,0.6450459957122803
translation,51,189,results,hop size,of,3,hop size of 3,0.6761822700500488
translation,51,191,results,model,with,sq method,model with sq method,0.6704588532447815
translation,51,191,results,model,achieves,best performance,model achieves best performance,0.6728841066360474
translation,51,191,results,best performance,when,hop size,best performance when hop size,0.6599342823028564
translation,51,191,results,hop size,is,3,hop size is 3,0.644754946231842
translation,51,191,results,results,has,model,results has model,0.5339115858078003
translation,51,202,results,our model,achieve,best performance,our model achieve best performance,0.6268908381462097
translation,51,202,results,best performance,without importing,extra rules,best performance without importing extra rules,0.7807915210723877
translation,51,202,results,results,see that,our model,results see that our model,0.6820751428604126
translation,52,32,ablation-analysis,"larger models ( roberta l , bert l )",tend to have,more bias,"larger models ( roberta l , bert l ) tend to have more bias",0.6808019280433655
translation,52,32,ablation-analysis,more bias,than,smaller counterparts ( roberta b and bert b ),more bias than smaller counterparts ( roberta b and bert b ),0.6167977452278137
translation,52,32,ablation-analysis,fine -tuning,on,qa datasets,fine -tuning on qa datasets,0.5105687975883484
translation,52,32,ablation-analysis,fine -tuning,affects,degree of bias,fine -tuning affects degree of bias,0.7625011801719666
translation,52,32,ablation-analysis,qa datasets,affects,degree of bias,qa datasets affects degree of bias,0.718341052532196
translation,52,32,ablation-analysis,degree of bias,in,model,degree of bias in model,0.5602455735206604
translation,52,32,ablation-analysis,degree of bias,increases with,squad,degree of bias increases with squad,0.7301238775253296
translation,52,32,ablation-analysis,distilled model,reduces,bias,distilled model reduces bias,0.7626120448112488
translation,52,32,ablation-analysis,larger ones,amplify,bias,larger ones amplify bias,0.7857987880706787
translation,52,32,ablation-analysis,fine - tuning,has,distilled model,fine - tuning has distilled model,0.5712885856628418
translation,52,32,ablation-analysis,fine-tuning,has,larger ones,fine-tuning has larger ones,0.6169366240501404
translation,52,7,model,biases,through,underspecified questions,biases through underspecified questions,0.6374599933624268
translation,52,7,model,un - qover,has,general framework,un - qover has general framework,0.6331177949905396
translation,52,7,model,model,present,un - qover,model present un - qover,0.7474986910820007
translation,52,17,model,underspecified questions,to uncover,stereotyping biases,underspecified questions to uncover stereotyping biases,0.6839901208877563
translation,52,17,model,stereotyping biases,in,downstream qa models,stereotyping biases in downstream qa models,0.5194999575614929
translation,52,232,model,unqover,for measuring,stereotyping biases,unqover for measuring stereotyping biases,0.7064720392227173
translation,52,232,model,stereotyping biases,in,qa models,stereotyping biases in qa models,0.5216283798217773
translation,52,177,results,larger qa models,show,more bias,larger qa models show more bias,0.6908895969390869
translation,52,177,results,results,has,larger qa models,results has larger qa models,0.5598087906837463
translation,52,178,results,qa models,see that,bert dist,qa models see that bert dist,0.6563067436218262
translation,52,178,results,bert dist,is,least biased models,bert dist is least biased models,0.6098294854164124
translation,52,178,results,bert dist,among,least biased models,bert dist among least biased models,0.649474561214447
translation,52,178,results,least biased models,across,different biases,least biased models across different biases,0.7094055414199829
translation,52,178,results,results,For,qa models,results For qa models,0.6026058197021484
translation,52,181,results,bert dist model,after,fine-tuning,bert dist model after fine-tuning,0.6920254826545715
translation,52,181,results,bert dist model,shows,much less biases,bert dist model shows much less biases,0.7139240503311157
translation,52,181,results,fine-tuning,on,squad or newsqa,fine-tuning on squad or newsqa,0.5570217967033386
translation,52,181,results,much less biases,across,different bias classes,much less biases across different bias classes,0.7198407053947449
translation,52,181,results,results,has,bert dist model,results has bert dist model,0.5673189163208008
translation,53,106,ablation-analysis,"all ci , tb , tmb , and lss features",over,wm baseline,"all ci , tb , tmb , and lss features over wm baseline",0.6411709189414978
translation,53,106,ablation-analysis,significantly,over,wm baseline,significantly over wm baseline,0.7124132513999939
translation,53,106,ablation-analysis,"all ci , tb , tmb , and lss features",has,significantly,"all ci , tb , tmb , and lss features has significantly",0.5634191036224365
translation,53,106,ablation-analysis,ablation analysis,has,"all ci , tb , tmb , and lss features","ablation analysis has all ci , tb , tmb , and lss features",0.5280112028121948
translation,53,111,ablation-analysis,five types of features,achieve,best performance,five types of features achieve best performance,0.6363117098808289
translation,53,111,ablation-analysis,best performance,for,traditional method,best performance for traditional method,0.6147406101226807
translation,53,111,ablation-analysis,ablation analysis,combination of,five types of features,ablation analysis combination of five types of features,0.6789771318435669
translation,53,119,ablation-analysis,three rank correlations,take down,performance,three rank correlations take down performance,0.6104936003684998
translation,53,119,ablation-analysis,performance,of,traditional nlp method,performance of traditional nlp method,0.5911484360694885
translation,53,119,ablation-analysis,ablation analysis,has,three rank correlations,ablation analysis has three rank correlations,0.5631455779075623
translation,53,121,ablation-analysis,both arc and arr,make,contributions,both arc and arr make contributions,0.6361336708068848
translation,53,121,ablation-analysis,contributions,to,performance,contributions to performance,0.5297154784202576
translation,53,121,ablation-analysis,ablation analysis,has,both arc and arr,ablation analysis has both arc and arr,0.5470576286315918
translation,53,124,ablation-analysis,features,extracted from,q- q pair,features extracted from q- q pair,0.583206832408905
translation,53,124,ablation-analysis,ablation analysis,has,performance,ablation analysis has performance,0.5053174495697021
translation,53,143,ablation-analysis,method,using,two -step filtering operation,method using two -step filtering operation,0.7203292846679688
translation,53,143,ablation-analysis,two -step filtering operation,does not make,obvious contribution,two -step filtering operation does not make obvious contribution,0.6600023508071899
translation,53,143,ablation-analysis,subtask c,has,method,subtask c has method,0.6027721166610718
translation,53,143,ablation-analysis,ablation analysis,In,subtask c,ablation analysis In subtask c,0.5430648922920227
translation,53,5,baselines,subtask a,employed,three different methods,subtask a employed three different methods,0.6636709570884705
translation,53,5,baselines,three different methods,to rank,question - comment pair,three different methods to rank question - comment pair,0.7056801319122314
translation,53,5,baselines,supervised model,using,traditional features,supervised model using traditional features,0.6626124978065491
translation,53,5,baselines,supervised model,using,convolutional neural network,supervised model using convolutional neural network,0.6001002788543701
translation,53,5,baselines,supervised model,using,long- short term memory network,supervised model using long- short term memory network,0.6222017407417297
translation,53,5,baselines,baselines,For,subtask a,baselines For subtask a,0.6316347122192383
translation,53,99,baselines,two algorithms,implemented in,sklearn,two algorithms implemented in sklearn,0.7138081192970276
translation,53,99,baselines,supervised classifier,has,two algorithms,supervised classifier has two algorithms,0.5780899524688721
translation,53,138,baselines,traditional features,adding,q- q pair information,traditional features adding q- q pair information,0.6637382507324219
translation,53,138,baselines,q- q pair information,used as,contrastive2 run,q- q pair information used as contrastive2 run,0.6591892838478088
translation,53,138,baselines,baselines,has,traditional features,baselines has traditional features,0.5556209087371826
translation,53,6,experiments,subtask b,proposed,two novel methods,subtask b proposed two novel methods,0.6761073470115662
translation,53,6,experiments,two novel methods,to improve,semantic similarity estimation,two novel methods to improve semantic similarity estimation,0.650301992893219
translation,53,6,experiments,semantic similarity estimation,between,question -question pair,semantic similarity estimation between question -question pair,0.6040297746658325
translation,53,6,experiments,semantic similarity estimation,by integrating,rank information,semantic similarity estimation by integrating rank information,0.581598162651062
translation,53,6,experiments,rank information,of,questioncomment pair,rank information of questioncomment pair,0.5679789781570435
translation,53,12,experiments,subtask a,built,convolutional neural network ( cnn ) model,subtask a built convolutional neural network ( cnn ) model,0.6317922472953796
translation,53,12,experiments,subtask a,built,bidirectional long short - term memory ( blst - m ) model,subtask a built bidirectional long short - term memory ( blst - m ) model,0.5739755034446716
translation,53,12,experiments,bidirectional long short - term memory ( blst - m ) model,to learn,joint representation,bidirectional long short - term memory ( blst - m ) model to learn joint representation,0.5890894532203674
translation,53,12,experiments,joint representation,for,questioncomment ( q- c ) pair,joint representation for questioncomment ( q- c ) pair,0.6307857632637024
translation,53,13,experiments,subtask b,proposed,two novel methods,subtask b proposed two novel methods,0.6761073470115662
translation,53,13,experiments,two novel methods,to improve,semantic similarity estimation,two novel methods to improve semantic similarity estimation,0.650301992893219
translation,53,13,experiments,semantic similarity estimation,between,question -question ( q - q ) pairs,semantic similarity estimation between question -question ( q - q ) pairs,0.6231620907783508
translation,53,13,experiments,semantic similarity estimation,by integrating,rank information,semantic similarity estimation by integrating rank information,0.581598162651062
translation,53,13,experiments,rank information,of,q-c pairs,rank information of q-c pairs,0.6179365515708923
translation,53,22,experiments,b,proposed,two novel methods,b proposed two novel methods,0.6090658903121948
translation,53,22,experiments,two novel methods,to improve,semantic similarity estimation,two novel methods to improve semantic similarity estimation,0.650301992893219
translation,53,22,experiments,semantic similarity estimation,between,q- q pairs,semantic similarity estimation between q- q pairs,0.6174245476722717
translation,53,22,experiments,semantic similarity estimation,by integrating,rank information,semantic similarity estimation by integrating rank information,0.581598162651062
translation,53,22,experiments,rank information,of,q-c pairs,rank information of q-c pairs,0.6179365515708923
translation,53,45,hyperparameters,three different word vectors,to represent,lss feature,three different word vectors to represent lss feature,0.6114130616188049
translation,53,45,hyperparameters,three different word vectors,i.e.,300 - dimensional version,three different word vectors i.e. 300 - dimensional version,0.6284605264663696
translation,53,45,hyperparameters,300 - dimensional version,of,word2vec,300 - dimensional version of word2vec,0.5130664706230164
translation,53,45,hyperparameters,300 dimensional vectors,pre-trained with,unsupervised neural language model,300 dimensional vectors pre-trained with unsupervised neural language model,0.7139887809753418
translation,53,45,hyperparameters,unsupervised neural language model,on,qatar living data,unsupervised neural language model on qatar living data,0.5112248659133911
translation,53,45,hyperparameters,words,initialized,randomly,words initialized randomly,0.7644031643867493
translation,53,45,hyperparameters,hyperparameters,used,three different word vectors,hyperparameters used three different word vectors,0.5575557351112366
translation,53,100,hyperparameters,logistic regression classifier ( penalized argument c = 1 ),adopted for,all three subtasks,logistic regression classifier ( penalized argument c = 1 ) adopted for all three subtasks,0.6722824573516846
translation,53,100,hyperparameters,all three subtasks,for,good performance,all three subtasks for good performance,0.5829574465751648
translation,53,100,hyperparameters,hyperparameters,has,logistic regression classifier ( penalized argument c = 1 ),hyperparameters has logistic regression classifier ( penalized argument c = 1 ),0.5384407639503479
translation,53,103,hyperparameters,cnn model,number of,filter windows,cnn model number of filter windows,0.6169079542160034
translation,53,103,hyperparameters,filter windows,is,2,filter windows is 2,0.642753005027771
translation,53,103,hyperparameters,feature maps,set to,100,feature maps set to 100,0.6947671175003052
translation,53,103,hyperparameters,learning rate,set to,0.01,learning rate set to 0.01,0.6997436881065369
translation,53,103,hyperparameters,subtask a,has,learning rate,subtask a has learning rate,0.5413520932197571
translation,53,104,hyperparameters,memory size,set to,500,memory size set to 500,0.7466210722923279
translation,53,104,hyperparameters,learning rate,is,0.01,learning rate is 0.01,0.5787386298179626
translation,53,104,hyperparameters,hyperparameters,of,blstm model,hyperparameters of blstm model,0.4999406933784485
translation,53,104,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,53,44,model,lexical semantic similarity feature ( lss ),included,lexical semantic similarity features,lexical semantic similarity feature ( lss ) included lexical semantic similarity features,0.5676609873771667
translation,53,44,model,model,has,lexical semantic similarity feature ( lss ),model has lexical semantic similarity feature ( lss ),0.5731220841407776
translation,53,53,model,convolutional neural network,to model,question -comment sentence,convolutional neural network to model question -comment sentence,0.6429734826087952
translation,53,53,model,model,proposed,convolutional neural network,model proposed convolutional neural network,0.6628486514091492
translation,53,112,results,model based cnn and blstm,achieve,comparable performance,model based cnn and blstm achieve comparable performance,0.5841392874717712
translation,53,112,results,comparable performance,with,traditional method,comparable performance with traditional method,0.6536206007003784
translation,53,112,results,results,has,model based cnn and blstm,results has model based cnn and blstm,0.5653796195983887
translation,53,113,results,combination of three methods,achieve,best performance,combination of three methods achieve best performance,0.608922004699707
translation,53,113,results,best performance,shows that,cnn and blstm,best performance shows that cnn and blstm,0.6417988538742065
translation,53,113,results,cnn and blstm,catch,complementary information,cnn and blstm catch complementary information,0.7316826581954956
translation,53,113,results,complementary information,for,q-c pair,complementary information for q-c pair,0.6545254588127136
translation,53,113,results,q-c pair,with,traditional method,q-c pair with traditional method,0.6245717406272888
translation,53,113,results,results,has,combination of three methods,results has combination of three methods,0.5246425271034241
translation,53,117,results,performance,of,question -question similarity,performance of question -question similarity,0.583759069442749
translation,53,117,results,question -question similarity,over,lucene baseline,question -question similarity over lucene baseline,0.6433238983154297
translation,53,117,results,subtask b,has,traditional nlp features,subtask b has traditional nlp features,0.5675582885742188
translation,53,117,results,traditional nlp features,has,significantly improve,traditional nlp features has significantly improve,0.5849707722663879
translation,53,117,results,significantly improve,has,performance,significantly improve has performance,0.5721634030342102
translation,53,117,results,results,on,subtask b,results on subtask b,0.5078848004341125
translation,53,118,results,"pearson , spearman , and kendall",get,similar performance,"pearson , spearman , and kendall get similar performance",0.5497644543647766
translation,53,118,results,"pearson , spearman , and kendall",do not perform,well,"pearson , spearman , and kendall do not perform well",0.6659941673278809
translation,53,118,results,well,versus,traditional nlp method,well versus traditional nlp method,0.6835612654685974
translation,53,118,results,results,has,"pearson , spearman , and kendall","results has pearson , spearman , and kendall",0.472683846950531
translation,53,122,results,"wmq , tmbq and tbq",represent,extracting word matching,"wmq , tmbq and tbq represent extracting word matching",0.5700094699859619
translation,53,122,results,"wmq , tmbq and tbq",represent,topic model based,"wmq , tmbq and tbq represent topic model based",0.5765442848205566
translation,53,122,results,"wmq , tmbq and tbq",represent,translation based features,"wmq , tmbq and tbq represent translation based features",0.5744900107383728
translation,53,122,results,translation based features,on,original question and related question,translation based features on original question and related question,0.5584219098091125
translation,53,123,results,results,observe,similar results,results observe similar results,0.5688323378562927
translation,53,129,results,best performance,with,filtering operation,best performance with filtering operation,0.6739594340324402
translation,53,129,results,much higher,than,best score ( m ap = 39.39 % ),much higher than best score ( m ap = 39.39 % ),0.5316237807273865
translation,53,139,results,performance,over,traditional method and blstm,performance over traditional method and blstm,0.6236332654953003
translation,53,139,results,subtask a,has,combination of three methods,subtask a has combination of three methods,0.6040601134300232
translation,53,139,results,combination of three methods,has,significantly improve,combination of three methods has significantly improve,0.5670366883277893
translation,53,139,results,significantly improve,has,performance,significantly improve has performance,0.5721634030342102
translation,53,139,results,results,In,subtask a,results In subtask a,0.46824193000793457
translation,53,140,results,result,using,traditional features,result using traditional features,0.6872116327285767
translation,53,140,results,traditional features,higher than,lucene,traditional features higher than lucene,0.6970769166946411
translation,53,140,results,certain gap,with,best result,certain gap with best result,0.6430812478065491
translation,53,140,results,subtask b,has,result,subtask b has result,0.6112905740737915
translation,53,140,results,results,In,subtask b,results In subtask b,0.48625096678733826
translation,53,142,results,results,on,test data,results on test data,0.5338460803031921
translation,54,164,ablation-analysis,improvement,of,9 % - 10 %,improvement of 9 % - 10 %,0.6156471967697144
translation,54,164,ablation-analysis,improvement,about,9 % - 10 %,improvement about 9 % - 10 %,0.6483221054077148
translation,54,164,ablation-analysis,9 % - 10 %,exploiting,our negative set,9 % - 10 % exploiting our negative set,0.7152090072631836
translation,54,164,ablation-analysis,9 % - 10 %,makes,positive contribution,9 % - 10 % makes positive contribution,0.6494719982147217
translation,54,164,ablation-analysis,ablation analysis,has,improvement,ablation analysis has improvement,0.5466791987419128
translation,54,181,ablation-analysis,combining positive examples,from,heterogeneous sources,combining positive examples from heterogeneous sources,0.5531081557273865
translation,54,181,ablation-analysis,combining positive examples,indispensable to tackle,any class of text,combining positive examples indispensable to tackle any class of text,0.6669238805770874
translation,54,181,ablation-analysis,heterogeneous sources,indispensable to tackle,any class of text,heterogeneous sources indispensable to tackle any class of text,0.6909019947052002
translation,54,181,ablation-analysis,ablation analysis,show,both negative examples,ablation analysis show both negative examples,0.624073326587677
translation,54,181,ablation-analysis,ablation analysis,show,combining positive examples,ablation analysis show combining positive examples,0.6387644410133362
translation,54,201,model,corpus acquisition technique,for,definition qa,corpus acquisition technique for definition qa,0.5676153898239136
translation,54,201,model,samples,from,heterogoneous sources,samples from heterogoneous sources,0.5850606560707092
translation,54,167,results,me models,putting together,evidence,me models putting together evidence,0.5683077573776245
translation,54,167,results,evidence,from,kb and non,evidence from kb and non,0.6024765372276306
translation,54,167,results,evidence,from,- kbs,evidence from - kbs,0.632556140422821
translation,54,167,results,evidence,betters,performance,evidence betters performance,0.7811110615730286
translation,54,167,results,kb and non,betters,performance,kb and non betters performance,0.7443499565124512
translation,54,167,results,kb and non,has,- kbs,kb and non has - kbs,0.6336134076118469
translation,54,167,results,results,in the case of,me models,results in the case of me models,0.6888182759284973
translation,54,168,results,lms,not observe,noticeable improvement,lms not observe noticeable improvement,0.7174802422523499
translation,54,168,results,noticeable improvement,when,unifying,noticeable improvement when unifying,0.6992496252059937
translation,54,168,results,unifying,has,both sources,unifying has both sources,0.5309734344482422
translation,54,168,results,results,in the case of,lms,results in the case of lms,0.5931583642959595
translation,54,183,results,our outcomes,indicate,cleanness and quality,our outcomes indicate cleanness and quality,0.5503790378570557
translation,54,183,results,cleanness and quality,are,more important,cleanness and quality are more important,0.5825183391571045
translation,54,183,results,more important,than,size of the corpus,more important than size of the corpus,0.5360235571861267
translation,55,204,ablation-analysis,model performance,drops from,0.557 to 0.534,model performance drops from 0.557 to 0.534,0.6804896593093872
translation,55,204,ablation-analysis,two -layered bidirectional attention network,has,model performance,two -layered bidirectional attention network has model performance,0.462303102016449
translation,55,204,ablation-analysis,ablation analysis,when turning off,two -layered bidirectional attention network,ablation analysis when turning off two -layered bidirectional attention network,0.6752191781997681
translation,55,205,ablation-analysis,all submodules,in,attention network,all submodules in attention network,0.506797194480896
translation,55,205,ablation-analysis,importance module,is,most significant,importance module is most significant,0.5263549089431763
translation,55,205,ablation-analysis,most significant,since,f1 score,most significant since f1 score,0.6004348397254944
translation,55,205,ablation-analysis,all submodules,has,importance module,all submodules has importance module,0.5749854445457458
translation,55,205,ablation-analysis,attention network,has,importance module,attention network has importance module,0.5369101762771606
translation,55,205,ablation-analysis,ablation analysis,Among,all submodules,ablation analysis Among all submodules,0.5912318229675293
translation,55,206,ablation-analysis,kb - aware attention module,with,self-attention,kb - aware attention module with self-attention,0.6057212948799133
translation,55,206,ablation-analysis,modeling,has,kb - to - query attention flow,modeling has kb - to - query attention flow,0.48274335265159607
translation,55,206,ablation-analysis,self-attention,has,significantly degrades,self-attention has significantly degrades,0.5988435745239258
translation,55,206,ablation-analysis,significantly degrades,has,performance,significantly degrades has performance,0.5908101797103882
translation,55,206,ablation-analysis,ablation analysis,On,flip side,ablation analysis On flip side,0.5882085561752319
translation,55,208,ablation-analysis,big influence,on,model performance,big influence on model performance,0.5126785635948181
translation,55,208,ablation-analysis,topic entity delexicalization strategy,has,big influence,topic entity delexicalization strategy has big influence,0.5348688364028931
translation,55,208,ablation-analysis,marginally boosts,has,performance,marginally boosts has performance,0.6004360914230347
translation,55,208,ablation-analysis,ablation analysis,find that,topic entity delexicalization strategy,ablation analysis find that topic entity delexicalization strategy,0.6043123602867126
translation,55,174,experimental-setup,word embeddings,with,pre-trained glove vectors,word embeddings with pre-trained glove vectors,0.5807012915611267
translation,55,174,experimental-setup,pre-trained glove vectors,with,word embedding size d v = 300,pre-trained glove vectors with word embedding size d v = 300,0.6202610731124878
translation,55,174,experimental-setup,experimental setup,initialize,word embeddings,experimental setup initialize word embeddings,0.7035120129585266
translation,55,175,experimental-setup,relation embedding size d p,set as,"128 , 16 and 128","relation embedding size d p set as 128 , 16 and 128",0.6653732657432556
translation,55,175,experimental-setup,hidden size d,set as,"128 , 16 and 128","hidden size d set as 128 , 16 and 128",0.6757931709289551
translation,55,175,experimental-setup,experimental setup,has,relation embedding size d p,experimental setup has relation embedding size d p,0.5515660643577576
translation,55,176,experimental-setup,dropout rates,on,"word embedding layer , question encoder side and the answer encoder side","dropout rates on word embedding layer , question encoder side and the answer encoder side",0.520991861820221
translation,55,176,experimental-setup,"word embedding layer , question encoder side and the answer encoder side",are,"0.3 , 0.3 and 0.2","word embedding layer , question encoder side and the answer encoder side are 0.3 , 0.3 and 0.2",0.5611882209777832
translation,55,176,experimental-setup,experimental setup,has,dropout rates,experimental setup has dropout rates,0.5134919285774231
translation,55,177,experimental-setup,batch size,set as,32,batch size set as 32,0.6738126873970032
translation,55,177,experimental-setup,answer module threshold,? =,0.7,answer module threshold ? = 0.7,0.6571406126022339
translation,55,177,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,55,177,experimental-setup,experimental setup,has,answer module threshold,experimental setup has answer module threshold,0.5781010389328003
translation,55,179,experimental-setup,15 candidates,after,negative sampling,15 candidates after negative sampling,0.7013257145881653
translation,55,179,experimental-setup,negative sampling,in,training time,negative sampling in training time,0.5110769867897034
translation,55,179,experimental-setup,experimental setup,For,each question,experimental setup For each question,0.6040974855422974
translation,55,180,experimental-setup,question,use,cnn,question use cnn,0.6625738143920898
translation,55,180,experimental-setup,cnn,with,filter sizes,cnn with filter sizes,0.6308877468109131
translation,55,180,experimental-setup,filter sizes,has,2 and 3,filter sizes has 2 and 3,0.5995439887046814
translation,55,180,experimental-setup,experimental setup,When,question,experimental setup When question,0.6550378203392029
translation,55,180,experimental-setup,experimental setup,encoding,question,experimental setup encoding question,0.7531123161315918
translation,55,182,experimental-setup,candidate aspect,use,cnn,candidate aspect use cnn,0.6236280202865601
translation,55,182,experimental-setup,cnn,with,filter size 3,cnn with filter size 3,0.709658145904541
translation,55,182,experimental-setup,linear activation and max-pooling,used together with,cnns,linear activation and max-pooling used together with cnns,0.6294300556182861
translation,55,182,experimental-setup,experimental setup,When encoding,candidate aspect,experimental setup When encoding candidate aspect,0.7409689426422119
translation,55,183,experimental-setup,adam optimizer,to train,model,adam optimizer to train model,0.7131099700927734
translation,55,184,experimental-setup,initial learning rate,set as,0.001,initial learning rate set as 0.001,0.6084002256393433
translation,55,184,experimental-setup,0.001,reduced by,factor of 10,0.001 reduced by factor of 10,0.6913594603538513
translation,55,184,experimental-setup,experimental setup,has,initial learning rate,experimental setup has initial learning rate,0.49018073081970215
translation,55,185,experimental-setup,training procedure,stops,no improvement,training procedure stops no improvement,0.7492632269859314
translation,55,185,experimental-setup,no improvement,observed,validation set,no improvement observed validation set,0.7205555438995361
translation,55,185,experimental-setup,validation set,in,10 consecutive epochs,validation set in 10 consecutive epochs,0.5644639134407043
translation,55,185,experimental-setup,experimental setup,has,training procedure,experimental setup has training procedure,0.4894740879535675
translation,55,6,model,two -way flow of interactions,between,questions and the kb,two -way flow of interactions between questions and the kb,0.6671921014785767
translation,55,6,model,two -way flow of interactions,via,novel bidirectional attentive memory network,two -way flow of interactions via novel bidirectional attentive memory network,0.6183387041091919
translation,55,6,model,novel bidirectional attentive memory network,called,bamnet,novel bidirectional attentive memory network called bamnet,0.6126372814178467
translation,55,6,model,model,directly model,two -way flow of interactions,model directly model two -way flow of interactions,0.7550315260887146
translation,55,25,model,novel bidirectional attentive memory network,called,bamnet,novel bidirectional attentive memory network called bamnet,0.6126372814178467
translation,55,25,model,novel bidirectional attentive memory network,captures,mutual interactions,novel bidirectional attentive memory network captures mutual interactions,0.6319777965545654
translation,55,25,model,mutual interactions,between,questions and the underlying kb,mutual interactions between questions and the underlying kb,0.6400755643844604
translation,55,25,model,mutual interactions,stored in,contentaddressable memory,mutual interactions stored in contentaddressable memory,0.6962598562240601
translation,55,25,model,model,introduce,novel bidirectional attentive memory network,model introduce novel bidirectional attentive memory network,0.611417829990387
translation,55,26,model,helpful,for,better understanding,helpful for better understanding,0.6232545971870422
translation,55,26,model,world knowledge,has,kb ),world knowledge has kb ),0.6560501456260681
translation,55,26,model,better understanding,has,questions,better understanding has questions,0.5846409201622009
translation,55,26,model,model,assume,world knowledge,model assume world knowledge,0.7345703840255737
translation,55,181,model,linear projection,applied to,merge features,linear projection applied to merge features,0.6533510684967041
translation,55,181,model,merge features,extracted with,different filters,merge features extracted with different filters,0.5727055668830872
translation,55,181,model,model,has,linear projection,model has linear projection,0.5458139181137085
translation,55,207,model,enhancing module,contributes to,overall model performance,enhancing module contributes to overall model performance,0.7295899987220764
translation,55,207,model,secondary attention layer,has,enhancing module,secondary attention layer has enhancing module,0.5499933958053589
translation,55,190,results,performance,of,bamnet,performance of bamnet,0.5886726379394531
translation,55,190,results,performance,achieves,f1 score,performance achieves f1 score,0.6666135191917419
translation,55,190,results,bamnet,on,webquestions,bamnet on webquestions,0.5811546444892883
translation,55,190,results,bamnet,achieves,f1 score,bamnet achieves f1 score,0.6570268869400024
translation,55,190,results,f1 score,of,0.518,f1 score of 0.518,0.5672777891159058
translation,55,190,results,f1 score,of,0.497,f1 score of 0.497,0.5604561567306519
translation,55,190,results,f1 score,of,0.497,f1 score of 0.497,0.5604561567306519
translation,55,190,results,0.518,using,topic entity predictor,0.518 using topic entity predictor,0.6360652446746826
translation,55,190,results,0.518,is,significantly better,0.518 is significantly better,0.5123410224914551
translation,55,190,results,significantly better,than,f1 score,significantly better than f1 score,0.5796343088150024
translation,55,190,results,f1 score,of,0.497,f1 score of 0.497,0.5604561567306519
translation,55,190,results,0.497,using,freebase search api,0.497 using freebase search api,0.597898006439209
translation,55,190,results,results,for,performance,results for performance,0.6641949415206909
translation,55,191,results,bamnet,has,significantly outperforms,bamnet has significantly outperforms,0.6285245418548584
translation,55,191,results,significantly outperforms,has,previous state - of- the - art irbased methods,significantly outperforms has previous state - of- the - art irbased methods,0.5524921417236328
translation,55,191,results,results,observe,bamnet,results observe bamnet,0.5652286410331726
translation,55,240,results,bamnet,achieves,state - of- the - art performance,bamnet achieves state - of- the - art performance,0.6622938513755798
translation,55,240,results,bamnet,relies only on,very few hand -crafted features,bamnet relies only on very few hand -crafted features,0.7132595181465149
translation,55,240,results,state - of- the - art performance,of,0.518,state - of- the - art performance of 0.518,0.5764192938804626
translation,55,240,results,0.518,without recourse to,any external resources,0.518 without recourse to any external resources,0.636176347732544
translation,55,240,results,results,has,bamnet,results has bamnet,0.5604198575019836
translation,55,241,results,gold-topic entities,given,bamnet,gold-topic entities given bamnet,0.6533656120300293
translation,55,241,results,bamnet,achieves,f1,bamnet achieves f1,0.6875613331794739
translation,55,241,results,f1,of,0.557,f1 of 0.557,0.5615754127502441
translation,55,241,results,results,assume,gold-topic entities,results assume gold-topic entities,0.6026800274848938
translation,56,27,experimental-setup,github.com,/,petrochukm,github.com / petrochukm,0.5967690348625183
translation,56,27,experimental-setup,github.com,/,simple-question,github.com / simple-question,0.5850715041160583
translation,56,27,experimental-setup,petrochukm,/,simple-question,petrochukm / simple-question,0.6467058062553406
translation,56,27,experimental-setup,simple-question,has,-answering,simple-question has -answering,0.6049366593360901
translation,56,92,hyperparameters,word embeddings,initialized with,"glove ( pennington et al. , 2014 )","word embeddings initialized with glove ( pennington et al. , 2014 )",0.7030168175697327
translation,56,92,hyperparameters,word embeddings,initialized with,frozen,word embeddings initialized with frozen,0.7392375469207764
translation,56,92,hyperparameters,hyperparameters,has,word embeddings,hyperparameters has word embeddings,0.4784160554409027
translation,56,93,hyperparameters,"adam ( kingma and ba , 2014 )",initialized with,learning rate,"adam ( kingma and ba , 2014 ) initialized with learning rate",0.7717427015304565
translation,56,93,hyperparameters,"adam ( kingma and ba , 2014 )",to optimize,model weights,"adam ( kingma and ba , 2014 ) to optimize model weights",0.7005749940872192
translation,56,93,hyperparameters,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,56,93,hyperparameters,hyperparameters,has,"adam ( kingma and ba , 2014 )","hyperparameters has adam ( kingma and ba , 2014 )",0.5345408916473389
translation,56,94,hyperparameters,learning rate,if,validation accuracy,learning rate if validation accuracy,0.5999765992164612
translation,56,94,hyperparameters,validation accuracy,not,improved,validation accuracy not improved,0.7074616551399231
translation,56,94,hyperparameters,improved,in,3 epochs,improved in 3 epochs,0.5720590949058533
translation,56,94,hyperparameters,validation accuracy,has,improved,validation accuracy has improved,0.599702000617981
translation,56,94,hyperparameters,hyperparameters,halve,learning rate,hyperparameters halve learning rate,0.6980479955673218
translation,56,103,hyperparameters,word embeddings,initialized with,"fast- text ( bojanowski et al. , 2017 )","word embeddings initialized with fast- text ( bojanowski et al. , 2017 )",0.6992141008377075
translation,56,103,hyperparameters,word embeddings,initialized with,frozen,word embeddings initialized with frozen,0.7392375469207764
translation,56,103,hyperparameters,hyperparameters,has,word embeddings,hyperparameters has word embeddings,0.4784160554409027
translation,56,104,hyperparameters,amsgrad variant,of,"adam ( reddi et al. , 2018 )","amsgrad variant of adam ( reddi et al. , 2018 )",0.5279614925384521
translation,56,104,hyperparameters,amsgrad variant,initialized with,learning rate,amsgrad variant initialized with learning rate,0.7727391719818115
translation,56,104,hyperparameters,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,56,104,hyperparameters,hyperparameters,use,amsgrad variant,hyperparameters use amsgrad variant,0.6185858845710754
translation,56,105,hyperparameters,"batch size ( smith et al. , 2017 )",if,validation accuracy,"batch size ( smith et al. , 2017 ) if validation accuracy",0.5398809313774109
translation,56,105,hyperparameters,validation accuracy,not,improved,validation accuracy not improved,0.7074616551399231
translation,56,105,hyperparameters,improved,in,3 epochs,improved in 3 epochs,0.5720590949058533
translation,56,105,hyperparameters,validation accuracy,has,improved,validation accuracy has improved,0.599702000617981
translation,56,105,hyperparameters,hyperparameters,double,"batch size ( smith et al. , 2017 )","hyperparameters double batch size ( smith et al. , 2017 )",0.7231681942939758
translation,56,11,results,simplequestions benchmark,can be,nearly solved,simplequestions benchmark can be nearly solved,0.5941543579101562
translation,56,11,results,nearly solved,by,standard methods,nearly solved by standard methods,0.6128304600715637
translation,56,11,results,results,present new evidence,simplequestions benchmark,results present new evidence simplequestions benchmark,0.6657333970069885
translation,56,106,results,results,on,simplequestions test set,results on simplequestions test set,0.533169150352478
translation,56,108,results,our model,achieves,78.1 % accuracy,our model achieves 78.1 % accuracy,0.6252194046974182
translation,56,108,results,78.1 % accuracy,on,simplequestions test set,78.1 % accuracy on simplequestions test set,0.5002192258834839
translation,56,108,results,simplequestions test set,has,new state - of - the - art,simplequestions test set has new state - of - the - art,0.5766249299049377
translation,56,108,results,results,has,our model,results has our model,0.5871725678443909
translation,57,145,ablation-analysis,most relevant features,are,word n-gram ones,most relevant features are word n-gram ones,0.5577797293663025
translation,57,145,ablation-analysis,word n-gram ones,followed by,cwasa,word n-gram ones followed by cwasa,0.6501100063323975
translation,57,145,ablation-analysis,word n-gram ones,followed by,"distributed representation - based , and knowledgegraph - based ones","word n-gram ones followed by distributed representation - based , and knowledgegraph - based ones",0.6755518913269043
translation,57,145,ablation-analysis,ablation analysis,manifests,most relevant features,ablation analysis manifests most relevant features,0.7424491047859192
translation,57,176,experimental-setup,context windows,of size,10,context windows of size 10,0.7377144694328308
translation,57,176,experimental-setup,20 negative words,for,sample,20 negative words for sample,0.6339666843414307
translation,57,176,experimental-setup,200 - dimensional vectors,has,context windows,200 - dimensional vectors has context windows,0.589371919631958
translation,57,176,experimental-setup,experimental setup,used,200 - dimensional vectors,experimental setup used 200 - dimensional vectors,0.626611053943634
translation,57,125,experiments,performance,of,our approach,performance of our approach,0.5712711215019226
translation,57,125,experiments,superior,in,subtask b,superior in subtask b,0.5345499515533447
translation,57,125,experiments,superior,in,question - related question similarity ranking,superior in question - related question similarity ranking,0.4931534230709076
translation,57,125,experiments,subtask b,has,question - related question similarity ranking,subtask b has question - related question similarity ranking,0.5450129508972168
translation,57,5,model,instances,by using,lexical and semantic - based similarity measures,instances by using lexical and semantic - based similarity measures,0.6756789684295654
translation,57,5,model,lexical and semantic - based similarity measures,between,text pairs,lexical and semantic - based similarity measures between text pairs,0.6425020694732666
translation,57,7,results,random and google search engine baselines,in,three english subtasks,random and google search engine baselines in three english subtasks,0.49517473578453064
translation,57,7,results,outperform,has,random and google search engine baselines,outperform has random and google search engine baselines,0.5964944958686829
translation,57,8,results,our approach,obtained,highest results,our approach obtained highest results,0.6957047581672668
translation,57,8,results,highest results,of,subtask b,highest results of subtask b,0.5977339148521423
translation,57,8,results,subtask b,compared to,other task participants,subtask b compared to other task participants,0.5843449234962463
translation,57,8,results,results,has,our approach,results has our approach,0.6050099730491638
translation,57,119,results,outperformed,both,random and the search engine baseline,outperformed both random and the search engine baseline,0.6797788739204407
translation,57,119,results,ranking measures,has,our system,ranking measures has our system,0.589733362197876
translation,57,119,results,our system,has,outperformed,our system has outperformed,0.6049097776412964
translation,57,119,results,results,In terms of,ranking measures,results In terms of ranking measures,0.6808158755302429
translation,57,120,results,development set,observed,map improvement,development set observed map improvement,0.7133747935295105
translation,57,120,results,map improvement,of,9.4 %,map improvement of 9.4 %,0.5502718687057495
translation,57,120,results,map improvement,compared with,results,map improvement compared with results,0.6700249910354614
translation,57,120,results,results,obtained by,search engine,results obtained by search engine,0.6055821776390076
translation,57,120,results,results,Using,development set,results Using development set,0.641993522644043
translation,57,120,results,results,obtained by,search engine,results obtained by search engine,0.6055821776390076
translation,57,122,results,classification results,are,superior,classification results are superior,0.5874093770980835
translation,57,122,results,results,has,classification results,results has classification results,0.5381123423576355
translation,57,123,results,improvements,in,accuracy and f1,improvements in accuracy and f1,0.5310074687004089
translation,57,123,results,accuracy and f1,of,24.9 % and 5.2 %,accuracy and f1 of 24.9 % and 5.2 %,0.5513792634010315
translation,57,123,results,results,obtain,improvements,results obtain improvements,0.5979204177856445
translation,57,126,results,improvement,of,"map , avgrec , and mrr","improvement of map , avgrec , and mrr",0.5632306337356567
translation,57,126,results,"map , avgrec , and mrr",of,"4.6 % , 5 % , and 6.4 %","map , avgrec , and mrr of 4.6 % , 5 % , and 6.4 %",0.5633468627929688
translation,57,126,results,"4.6 % , 5 % , and 6.4 %",compared to,search engine baseline,"4.6 % , 5 % , and 6.4 % compared to search engine baseline",0.6410264372825623
translation,57,126,results,development set,has,improvement,development set has improvement,0.5883541703224182
translation,57,126,results,results,using,development set,results using development set,0.641993522644043
translation,57,128,results,random baseline,with,27.4 % and 16.1 %,random baseline with 27.4 % and 16.1 %,0.5794904828071594
translation,57,128,results,27.4 % and 16.1 %,of,accuracy and f1 - measure,27.4 % and 16.1 % of accuracy and f1 - measure,0.561663031578064
translation,57,128,results,outperformed,has,random baseline,outperformed has random baseline,0.6137772798538208
translation,57,128,results,results,With respect to,classification measures,results With respect to classification measures,0.6230083703994751
translation,57,133,results,"map , avgrec , and mrr",improved,"8.7 % , 8.5 % , and 7.5 %","map , avgrec , and mrr improved 8.7 % , 8.5 % , and 7.5 %",0.7205642461776733
translation,57,133,results,"8.7 % , 8.5 % , and 7.5 %",when using,development partition,"8.7 % , 8.5 % , and 7.5 % when using development partition",0.6898893713951111
translation,57,133,results,search engine baseline,has,"map , avgrec , and mrr","search engine baseline has map , avgrec , and mrr",0.53895103931427
translation,57,133,results,results,Compared to,search engine baseline,results Compared to search engine baseline,0.6750298738479614
translation,57,134,results,accuracy and f1 - measure,improved,61.5 % and 12.2 %,accuracy and f1 - measure improved 61.5 % and 12.2 %,0.7155717611312866
translation,57,134,results,results,has,accuracy and f1 - measure,results has accuracy and f1 - measure,0.5261595845222473
translation,57,135,results,largest number of comments,to,rank,largest number of comments to rank,0.5638424754142761
translation,57,135,results,top 10 results,measuring,results,top 10 results measuring results,0.6117606163024902
translation,57,135,results,top 10 results,benefited,our approach,top 10 results benefited our approach,0.5678273439407349
translation,57,135,results,results,has,largest number of comments,results has largest number of comments,0.5475294589996338
translation,57,138,results,our approach,obtained,highest results,our approach obtained highest results,0.6957047581672668
translation,57,138,results,highest results,with,considerable margin ( 1.04 % ),highest results with considerable margin ( 1.04 % ),0.6035040616989136
translation,57,138,results,results,has,our approach,results has our approach,0.6050099730491638
translation,57,144,results,similarity,rather than,question answering,similarity rather than question answering,0.6780216693878174
translation,57,154,results,our approach,obtained,highest results,our approach obtained highest results,0.6957047581672668
translation,57,154,results,highest results,in,subtask b,highest results in subtask b,0.526354968547821
translation,57,154,results,subtask b,compared to,other task participants,subtask b compared to other task participants,0.5843449234962463
translation,57,154,results,results,has,our approach,results has our approach,0.6050099730491638
translation,58,91,ablation-analysis,mtl ( abc ),provides,best map,mtl ( abc ) provides best map,0.6137322783470154
translation,58,91,ablation-analysis,best map,improving,bc and ac,best map improving bc and ac,0.6694374680519104
translation,58,91,ablation-analysis,bc and ac,by,1.29 and 1.38,bc and ac by 1.29 and 1.38,0.5915390253067017
translation,58,91,ablation-analysis,ablation analysis,has,mtl ( abc ),ablation analysis has mtl ( abc ),0.5790533423423767
translation,58,70,experimental-setup,neural networks,mapped,words,neural networks mapped words,0.6526377201080322
translation,58,70,experimental-setup,neural networks,pre-initializing them with,standard skipgram embeddings,neural networks pre-initializing them with standard skipgram embeddings,0.6465891003608704
translation,58,70,experimental-setup,words,to,embeddings,words to embeddings,0.5525301694869995
translation,58,70,experimental-setup,embeddings,has,of size 50,embeddings has of size 50,0.6096551418304443
translation,58,70,experimental-setup,standard skipgram embeddings,has,of dimensionality 50,standard skipgram embeddings has of dimensionality 50,0.5699209570884705
translation,58,70,experimental-setup,experimental setup,has,neural networks,experimental setup has neural networks,0.5093538165092468
translation,58,71,experimental-setup,english wikipedia dump,using,word2vec toolkit,english wikipedia dump using word2vec toolkit,0.5928407907485962
translation,58,72,experimental-setup,input sentence,with,fixed - sized vector,input sentence with fixed - sized vector,0.6009032130241394
translation,58,72,experimental-setup,input sentence,using,convolutional operation,input sentence using convolutional operation,0.6809222102165222
translation,58,72,experimental-setup,input sentence,using,kmax pooling operation,input sentence using kmax pooling operation,0.6255876421928406
translation,58,72,experimental-setup,dimensions,are,100,dimensions are 100,0.6858904361724854
translation,58,72,experimental-setup,convolutional operation,of size,5,convolutional operation of size 5,0.7306442260742188
translation,58,72,experimental-setup,kmax pooling operation,with,k = 1,kmax pooling operation with k = 1,0.618418276309967
translation,58,72,experimental-setup,experimental setup,encoded,input sentence,experimental setup encoded input sentence,0.6986714601516724
translation,58,75,experimental-setup,each mlp,used,non-linear hidden layer,each mlp used non-linear hidden layer,0.6220707297325134
translation,58,75,experimental-setup,non-linear hidden layer,with,"hyperbolic tangent activation , tanh","non-linear hidden layer with hyperbolic tangent activation , tanh",0.6267107129096985
translation,58,75,experimental-setup,non-linear hidden layer,whose,size,non-linear hidden layer whose size,0.6314394474029541
translation,58,75,experimental-setup,experimental setup,For,each mlp,experimental setup For each mlp,0.6292147636413574
translation,58,79,experimental-setup,iterate,until,validation loss,iterate until validation loss,0.6402373313903809
translation,58,79,experimental-setup,validation loss,stops,improving,validation loss stops improving,0.7584502696990967
translation,58,79,experimental-setup,improving,with,patience p = 10,improving with patience p = 10,0.697698175907135
translation,58,79,experimental-setup,experimental setup,set,training,experimental setup set training,0.6610946655273438
translation,58,80,experimental-setup,"dropout ( srivastava et al. , 2014 )",to improve,generalization,"dropout ( srivastava et al. , 2014 ) to improve generalization",0.7065743803977966
translation,58,80,experimental-setup,"dropout ( srivastava et al. , 2014 )",avoid,co-adaptation,"dropout ( srivastava et al. , 2014 ) avoid co-adaptation",0.6570230722427368
translation,58,80,experimental-setup,co-adaptation,of,features,co-adaptation of features,0.559986412525177
translation,58,80,experimental-setup,experimental setup,added,"dropout ( srivastava et al. , 2014 )","experimental setup added dropout ( srivastava et al. , 2014 )",0.601043701171875
translation,58,6,model,data scarcity,by learning,target dnn,data scarcity by learning target dnn,0.6920753121376038
translation,58,6,model,two auxiliary tasks,in,multitask learning setting,two auxiliary tasks in multitask learning setting,0.4825887978076935
translation,58,7,model,strong semantic connection,between,selection of comments,strong semantic connection between selection of comments,0.6411110758781433
translation,58,7,model,selection of comments,relevant to,new questions,selection of comments relevant to new questions,0.7173864841461182
translation,58,7,model,selection of comments,relevant to,forum questions,selection of comments relevant to forum questions,0.6812328100204468
translation,58,7,model,model,exploit,strong semantic connection,model exploit strong semantic connection,0.7367779016494751
translation,58,30,results,our mtl approach,improves,single dnn,our mtl approach improves single dnn,0.6581592559814453
translation,58,30,results,single dnn,for solving,task c,single dnn for solving task c,0.6947668790817261
translation,58,30,results,single dnn,roughly 8 points in,map,single dnn roughly 8 points in map,0.7240689396858215
translation,58,81,results,"different dropout rates ( 0.2 , 0.4 )",for,input,"different dropout rates ( 0.2 , 0.4 ) for input",0.6160656213760376
translation,58,81,results,"different dropout rates ( 0.2 , 0.4 )","( 0.3 , 0.5 , 0.7 )",hidden layers,"different dropout rates ( 0.2 , 0.4 ) ( 0.3 , 0.5 , 0.7 ) hidden layers",0.6748746633529663
translation,58,81,results,hidden layers,obtaining,better results,hidden layers obtaining better results,0.6597998738288879
translation,58,81,results,better results,with,"highest values , i.e. , 0.4 and 0.7","better results with highest values , i.e. , 0.4 and 0.7",0.6411737203598022
translation,58,81,results,"different dropout rates ( 0.2 , 0.4 )",has,hidden layers,"different dropout rates ( 0.2 , 0.4 ) has hidden layers",0.5319172143936157
translation,58,81,results,results,tested,"different dropout rates ( 0.2 , 0.4 )","results tested different dropout rates ( 0.2 , 0.4 )",0.6830019354820251
translation,58,87,results,single network,for,task c,single network for task c,0.6773656606674194
translation,58,87,results,task c,cannot compete with,challenge systems,task c cannot compete with challenge systems,0.7561931014060974
translation,58,87,results,joint representation,highly improves,map,joint representation highly improves map,0.7500013709068298
translation,58,87,results,map,of,basic network,map of basic network,0.6012555360794067
translation,58,87,results,map,on,test set,map on test set,0.5779985189437866
translation,58,87,results,basic network,from,41.95 to 46.99,basic network from 41.95 to 46.99,0.5124137997627258
translation,58,87,results,41.95 to 46.99,on,test set,41.95 to 46.99 on test set,0.5499411225318909
translation,58,87,results,joint representation,has,"q new , q rel , c rel","joint representation has q new , q rel , c rel",0.5802285671234131
translation,58,90,results,shared sentence model,for,q new and q rel,shared sentence model for q new and q rel,0.5940411686897278
translation,58,90,results,shared sentence model,improves,map,shared sentence model improves map,0.7135177254676819
translation,58,90,results,map,on,dev. set,map on dev. set,0.5853531956672668
translation,58,90,results,results,has,shared sentence model,results has shared sentence model,0.5656874179840088
translation,58,92,results,"rel , c rel",by,2.88 points,"rel , c rel by 2.88 points",0.597599446773529
translation,58,92,results,best model,using,joint representation,best model using joint representation,0.6569057106971741
translation,58,92,results,q new,has,"rel , c rel","q new has rel , c rel",0.6436341404914856
translation,58,92,results,results,improves,q new,results improves q new,0.7219399213790894
translation,58,93,results,our full mtl model,ranked,4 th,our full mtl model ranked 4 th,0.6902677416801453
translation,58,93,results,4 th,on,task c,4 th on task c,0.6651584506034851
translation,58,93,results,task c,of,semeval 2016 competition,task c of semeval 2016 competition,0.5410258769989014
translation,58,93,results,results,has,our full mtl model,results has our full mtl model,0.5237194299697876
translation,58,95,results,most powerful feature,used by,top systems,most powerful feature used by top systems,0.7628939747810364
translation,58,95,results,most powerful feature,to,our model,most powerful feature to our model,0.5416328310966492
translation,58,95,results,our system,achieve,map,our system achieve map,0.7118393182754517
translation,58,95,results,map,of,52.67,map of 52.67,0.5843084454536438
translation,58,95,results,most powerful feature,has,our system,most powerful feature has our system,0.5903694033622742
translation,58,95,results,results,add,most powerful feature,results add most powerful feature,0.6118955612182617
translation,59,6,model,logical and linguistic knowledge,to augment,labeled training data,logical and linguistic knowledge to augment labeled training data,0.687907338142395
translation,59,6,model,consistency - based regularizer,to train,model,consistency - based regularizer to train model,0.6924630403518677
translation,59,25,model,data augmentation,uses,set of logical and linguistic knowledge,data augmentation uses set of logical and linguistic knowledge,0.5794488191604614
translation,59,25,model,set of logical and linguistic knowledge,to develop,additional consistent labeled training data,set of logical and linguistic knowledge to develop additional consistent labeled training data,0.6752780675888062
translation,59,25,model,model,has,data augmentation,model has data augmentation,0.5341393351554871
translation,59,26,model,symbolic logic,to incorporate,consistency regularization,symbolic logic to incorporate consistency regularization,0.6225549578666687
translation,59,26,model,consistency regularization,for,additional supervision signal,consistency regularization for additional supervision signal,0.5822679400444031
translation,59,26,model,additional supervision signal,beyond,inductive bias,additional supervision signal beyond inductive bias,0.6546940803527832
translation,59,26,model,inductive bias,given by,data augmentation,inductive bias given by data augmentation,0.674228847026825
translation,59,27,model,previous consistency - promoting methods,for,nli tasks,previous consistency - promoting methods for nli tasks,0.5368287563323975
translation,59,8,results,performance,of,roberta - based models,performance of roberta - based models,0.6108828783035278
translation,59,8,results,roberta - based models,by,1 - 5 %,roberta - based models by 1 - 5 %,0.5999253988265991
translation,59,8,results,1 - 5 %,across,datasets,1 - 5 % across datasets,0.7706009149551392
translation,59,8,results,our method,has,significantly improves,our method has significantly improves,0.5935146808624268
translation,59,8,results,significantly improves,has,performance,significantly improves has performance,0.5962982177734375
translation,59,8,results,results,has,our method,results has our method,0.5589964985847473
translation,59,9,results,state of the art,by,around 5 - 8 %,state of the art by around 5 - 8 %,0.5863952040672302
translation,59,9,results,state of the art,reduce,consistency violations,state of the art reduce consistency violations,0.6520750522613525
translation,59,9,results,around 5 - 8 %,on,wiqa and quarel,around 5 - 8 % on wiqa and quarel,0.6223303079605103
translation,59,9,results,consistency violations,by,58 %,consistency violations by 58 %,0.5772403478622437
translation,59,9,results,58 %,on,hotpotqa,58 % on hotpotqa,0.5660394430160522
translation,59,9,results,results,advance,state of the art,results advance state of the art,0.6386460661888123
translation,59,28,results,significant improvement,over,state of the art,significant improvement over state of the art,0.6282499432563782
translation,59,28,results,multiple choice qa,for,qualitative reasoning,multiple choice qa for qualitative reasoning,0.6030511260032654
translation,59,30,results,our approach,advances,stateof - the- art results,our approach advances stateof - the- art results,0.6648118495941162
translation,59,30,results,stateof - the- art results,on,wiqa and quarel,stateof - the- art results on wiqa and quarel,0.5385542511940002
translation,59,30,results,wiqa and quarel,with,4.7 and 8.4 % absolute accuracy improvement,wiqa and quarel with 4.7 and 8.4 % absolute accuracy improvement,0.6265935897827148
translation,59,30,results,4.7 and 8.4 % absolute accuracy improvement,reducing,inconsistent predictions,4.7 and 8.4 % absolute accuracy improvement reducing inconsistent predictions,0.6808302998542786
translation,59,30,results,results,has,our approach,results has our approach,0.6050099730491638
translation,59,105,results,our methods ( da and da + reg ),constantly give,1 to 5 points improvements,our methods ( da and da + reg ) constantly give 1 to 5 points improvements,0.6731775403022766
translation,59,105,results,1 to 5 points improvements,over,state- of- theart roberta qa 's performance,1 to 5 points improvements over state- of- theart roberta qa 's performance,0.6887630224227905
translation,59,105,results,1 to 5 points improvements,advancing,state - of - the - art scores,1 to 5 points improvements advancing state - of - the - art scores,0.7027198672294617
translation,59,105,results,state - of - the - art scores,on,wiqa and quarel,state - of - the - art scores on wiqa and quarel,0.5307613015174866
translation,59,105,results,wiqa and quarel,by,4.7 % and 8.4 %,wiqa and quarel by 4.7 % and 8.4 %,0.6041412949562073
translation,59,105,results,results,demonstrates,our methods ( da and da + reg ),results demonstrates our methods ( da and da + reg ),0.6341359615325928
translation,59,106,results,all three datasets,has,our method,all three datasets has our method,0.5473779439926147
translation,59,106,results,results,On,all three datasets,results On all three datasets,0.48488834500312805
translation,59,110,results,da ( standard ),does not give,notable improvement,da ( standard ) does not give notable improvement,0.675186276435852
translation,59,110,results,notable improvement,over,baseline model,notable improvement over baseline model,0.6682968139648438
translation,59,110,results,baseline model,both in,accuracy and consistency,baseline model both in accuracy and consistency,0.6002200841903687
translation,59,110,results,results,has,da ( standard ),results has da ( standard ),0.5369247794151306
translation,60,219,ablation-analysis,result,indicates,two systems,result indicates two systems,0.7077785730361938
translation,60,219,ablation-analysis,two systems,complementary to,each other,two systems complementary to each other,0.6947790384292603
translation,60,219,ablation-analysis,simple combination,is,already effective,simple combination is already effective,0.5268653631210327
translation,60,219,ablation-analysis,already effective,in providing,significant performance boost,already effective in providing significant performance boost,0.657691478729248
translation,60,219,ablation-analysis,ablation analysis,has,result,ablation analysis has result,0.5194553136825562
translation,60,173,hyperparameters,number of kernels,of,encoder,number of kernels of encoder,0.608319878578186
translation,60,173,hyperparameters,of ma - cnn,to be,300,of ma - cnn to be 300,0.5536624789237976
translation,60,173,hyperparameters,hyperparameters,set,number of kernels,hyperparameters set number of kernels,0.6144114136695862
translation,60,174,hyperparameters,kernels,widths,3 to 5,kernels widths 3 to 5,0.7666906714439392
translation,60,174,hyperparameters,3 to 5,for,cnn encoder,3 to 5 for cnn encoder,0.5922132134437561
translation,60,174,hyperparameters,hyperparameters,use,kernels,hyperparameters use kernels,0.6408552527427673
translation,60,175,hyperparameters,non-linearities,in,models,non-linearities in models,0.5616983771324158
translation,60,175,hyperparameters,non-linearities,are,rectified linear units nair and hinton ( 2010 ),non-linearities are rectified linear units nair and hinton ( 2010 ),0.5551540851593018
translation,60,175,hyperparameters,hyperparameters,has,non-linearities,hyperparameters has non-linearities,0.5494210124015808
translation,60,176,hyperparameters,"adadelta ( zeiler , 2012 )",as,optimizer,"adadelta ( zeiler , 2012 ) as optimizer",0.5067919492721558
translation,60,176,hyperparameters,"adadelta ( zeiler , 2012 )",use,recommended values,"adadelta ( zeiler , 2012 ) use recommended values",0.6665704250335693
translation,60,176,hyperparameters,optimizer,for,whole ma - cnn,optimizer for whole ma - cnn,0.5762012004852295
translation,60,176,hyperparameters,recommended values,for,hyperparameters,recommended values for hyperparameters,0.5706246495246887
translation,60,176,hyperparameters,hyperparameters,has,"? = 0.9 , = 1 ? 10 ?6 , learning rate = 1.0 )","hyperparameters has ? = 0.9 , = 1 ? 10 ?6 , learning rate = 1.0 )",0.5464325547218323
translation,60,176,hyperparameters,hyperparameters,use,recommended values,hyperparameters use recommended values,0.6471703052520752
translation,60,177,hyperparameters,embeddings,with,word2 vec,embeddings with word2 vec,0.6573508977890015
translation,60,177,hyperparameters,hyperparameters,initialize,embeddings,hyperparameters initialize embeddings,0.7610669732093811
translation,60,178,hyperparameters,episodic training,set,number of shots,episodic training set number of shots,0.6539841294288635
translation,60,178,hyperparameters,number of shots,to be,10,number of shots to be 10,0.6187170147895813
translation,60,178,hyperparameters,hyperparameters,For,episodic training,hyperparameters For episodic training,0.5671020746231079
translation,60,17,model,paraphrasing,be used to create,novel synthetic training items,paraphrasing be used to create novel synthetic training items,0.6356658339500427
translation,60,17,model,model,investigating,low-frequency questions,model investigating low-frequency questions,0.6988456845283508
translation,60,52,model,kaiser et al . 's ( 2017 ) memory module,together with,cnn encoder,kaiser et al . 's ( 2017 ) memory module together with cnn encoder,0.5809727907180786
translation,60,52,model,model,use,kaiser et al . 's ( 2017 ) memory module,model use kaiser et al . 's ( 2017 ) memory module,0.622263491153717
translation,60,52,model,model,as,main model,model as main model,0.5671764612197876
translation,60,52,model,model,as,memoryaugmented cnn classifier ( ma - cnn ),model as memoryaugmented cnn classifier ( ma - cnn ),0.5105863213539124
translation,60,53,model,ma - cnn 's one - shot learning capability,better use of,data augmentation,ma - cnn 's one - shot learning capability better use of data augmentation,0.6181889176368713
translation,60,53,model,data augmentation,to achieve,better performance,data augmentation to achieve better performance,0.6466145515441895
translation,60,53,model,model,take advantage of,ma - cnn 's one - shot learning capability,model take advantage of ma - cnn 's one - shot learning capability,0.6414632201194763
translation,60,172,model,word - based features,in,encoder,word - based features in encoder,0.5213349461555481
translation,60,172,model,model,use,word - based features,model use word - based features,0.6513010859489441
translation,60,18,results,two methods,work,best,two methods work best,0.6732860207557678
translation,60,18,results,two methods,work,in combination,two methods work in combination,0.6521272659301758
translation,60,18,results,better advantage,of,augmented data,better advantage of augmented data,0.6126881241798401
translation,60,18,results,augmented data,created by,paraphrasing,augmented data created by paraphrasing,0.6209425330162048
translation,60,18,results,best,has,in combination,best has in combination,0.5999771356582642
translation,60,18,results,results,find,two methods,results find two methods,0.5447496175765991
translation,60,125,results,multiple word senses,picking,first sense,multiple word senses picking first sense,0.7236650586128235
translation,60,125,results,first sense,produced,higher map score,first sense produced higher map score,0.6612967848777771
translation,60,125,results,higher map score,than,variety of other selection algorithms,higher map score than variety of other selection algorithms,0.5591058731079102
translation,60,125,results,results,in the case of,multiple word senses,results in the case of multiple word senses,0.6212630271911621
translation,60,186,results,ma - cnn,performs,very well,ma - cnn performs very well,0.5766741633415222
translation,60,186,results,very well,on,rare labels,very well on rare labels,0.5444958806037903
translation,60,186,results,results,has,ma - cnn,results has ma - cnn,0.5723671317100525
translation,60,187,results,performance difference,between,stacked cnn model and ma - cnn,performance difference between stacked cnn model and ma - cnn,0.5700235962867737
translation,60,187,results,stacked cnn model and ma - cnn,is,highly significant,stacked cnn model and ma - cnn is highly significant,0.5671559572219849
translation,60,187,results,stacked cnn model and ma - cnn,shows that,pairwise -classification approach,stacked cnn model and ma - cnn shows that pairwise -classification approach,0.6301576495170593
translation,60,187,results,highly significant,shows that,pairwise -classification approach,highly significant shows that pairwise -classification approach,0.7249000072479248
translation,60,187,results,pairwise -classification approach,paired with,episodic training,pairwise -classification approach paired with episodic training,0.7303387522697449
translation,60,187,results,pairwise -classification approach,is,really powerful,pairwise -classification approach is really powerful,0.5750548243522644
translation,60,187,results,episodic training,is,really powerful,episodic training is really powerful,0.578517496585846
translation,60,187,results,really powerful,on,items,really powerful on items,0.5515313744544983
translation,60,187,results,items,which belong to,labels,items which belong to labels,0.7557021975517273
translation,60,187,results,labels,with,few training instances,labels with few training instances,0.607225239276886
translation,60,187,results,results,has,performance difference,results has performance difference,0.5745248198509216
translation,60,188,results,ma - cnn,does not perform,as well,ma - cnn does not perform as well,0.7142590880393982
translation,60,188,results,as well,as,cnn ensemble,as well as cnn ensemble,0.6057742238044739
translation,60,188,results,cnn ensemble,on,all labels,cnn ensemble on all labels,0.5157824754714966
translation,60,188,results,results,see that,ma - cnn,results see that ma - cnn,0.6102305054664612
translation,60,194,results,benefit,in terms of,rare label accuracy,benefit in terms of rare label accuracy,0.6980912685394287
translation,60,194,results,rare label accuracy,by using,augmented dataset,rare label accuracy by using augmented dataset,0.6299754977226257
translation,60,194,results,both models,has,benefit,both models has benefit,0.5876051187515259
translation,60,194,results,results,see that,both models,results see that both models,0.6143448948860168
translation,60,195,results,difference,between,ma - cnn,difference between ma - cnn,0.6447417140007019
translation,60,195,results,ma - cnn,trained with,only the gold dataset and the augmented dataset,ma - cnn trained with only the gold dataset and the augmented dataset,0.7165224552154541
translation,60,195,results,only the gold dataset and the augmented dataset,is,highly significant,only the gold dataset and the augmented dataset is highly significant,0.5509726405143738
translation,60,195,results,even better performance,on,rare labels,even better performance on rare labels,0.5298534631729126
translation,60,195,results,results,has,difference,results has difference,0.5636705756187439
translation,60,196,results,performance,of,both models,performance of both models,0.5794581770896912
translation,60,196,results,both models,does,not significantly change,both models does not significantly change,0.32673248648643494
translation,60,196,results,not significantly change,showing,paraphrases,not significantly change showing paraphrases,0.7475077509880066
translation,60,196,results,paraphrases,are,high enough quality,paraphrases are high enough quality,0.5712409615516663
translation,60,196,results,paraphrases,of,high enough quality,paraphrases of high enough quality,0.5873965620994568
translation,60,196,results,full accuracy,has,performance,full accuracy has performance,0.5797281861305237
translation,60,196,results,results,for,full accuracy,results for full accuracy,0.5919588208198547
translation,60,197,results,effect,of using,pseudo-oracle,effect of using pseudo-oracle,0.6984942555427551
translation,60,197,results,effect,of using,manually filtered data,effect of using manually filtered data,0.7470363974571228
translation,60,197,results,manually filtered data,on,rare labels,manually filtered data on rare labels,0.554827094078064
translation,60,197,results,results,has,effects of data augmentation,results has effects of data augmentation,0.557717502117157
translation,60,198,results,ma - cnn,able to use,data augmentation,ma - cnn able to use data augmentation,0.6347457766532898
translation,60,198,results,data augmentation,directly benefits,rare labels,data augmentation directly benefits rare labels,0.7309991717338562
translation,60,198,results,results,find that,ma - cnn,results find that ma - cnn,0.6213558316230774
translation,60,199,results,ma - cnn,benefits from,human filtered data,ma - cnn benefits from human filtered data,0.6293400526046753
translation,60,199,results,results,has,ma - cnn,results has ma - cnn,0.5723671317100525
translation,60,206,results,simple lexical substitution,already good at providing,information,simple lexical substitution already good at providing information,0.7250518202781677
translation,60,206,results,information,helpful to,ma - cnn,information helpful to ma - cnn,0.6600052714347839
translation,60,206,results,neural machine back translation,is,even better method,neural machine back translation is even better method,0.5589384436607361
translation,60,206,results,even better method,at providing,paraphrases,even better method at providing paraphrases,0.6096914410591125
translation,60,206,results,positive impact,on,rare label accuracy,positive impact on rare label accuracy,0.5673295259475708
translation,60,206,results,results,has,simple lexical substitution,results has simple lexical substitution,0.5573506951332092
translation,60,207,results,paraphrases,generated by,both methods,paraphrases generated by both methods,0.6869454979896545
translation,60,207,results,paraphrases,find that,paraphrases,paraphrases find that paraphrases,0.6511792540550232
translation,60,207,results,paraphrases,from,back translation,paraphrases from back translation,0.5695748329162598
translation,60,207,results,paraphrases,are,more diverse,paraphrases are more diverse,0.6171579360961914
translation,60,207,results,paraphrases,contain,more novel words,paraphrases contain more novel words,0.6018323302268982
translation,60,207,results,more diverse,in,phrasal structure,more diverse in phrasal structure,0.47730162739753723
translation,60,207,results,more novel words,than,lexical substitution,more novel words than lexical substitution,0.6182458996772766
translation,60,207,results,results,inspect,paraphrases,results inspect paraphrases,0.6474801898002625
translation,60,218,results,combiner,get,50.98 % accuracy,combiner get 50.98 % accuracy,0.5751197934150696
translation,60,218,results,combiner,get,79.86 % accuracy,combiner get 79.86 % accuracy,0.5708073973655701
translation,60,218,results,50.98 % accuracy,on,rare labels,50.98 % accuracy on rare labels,0.519324541091919
translation,60,218,results,79.86 % accuracy,on,all labels,79.86 % accuracy on all labels,0.5243445634841919
translation,60,218,results,results,With,combiner,results With combiner,0.6670477390289307
translation,60,220,results,accuracy,on,rare labels,accuracy on rare labels,0.5306203365325928
translation,60,220,results,not as high,as,ma - cnn,not as high as ma - cnn,0.6155799627304077
translation,60,220,results,ma - cnn,higher than,stacked cnn model,ma - cnn higher than stacked cnn model,0.5987172722816467
translation,60,220,results,stacked cnn model,by,5 points,stacked cnn model by 5 points,0.5689067244529724
translation,60,220,results,accuracy increase,on,all labels,accuracy increase on all labels,0.5060574412345886
translation,61,161,ablation-analysis,performance,drops,sharply,performance drops sharply,0.7873720526695251
translation,61,161,ablation-analysis,dpda reader,has,performance,dpda reader has performance,0.5305202603340149
translation,61,161,ablation-analysis,ablation analysis,after removing,dpda reader,ablation analysis after removing dpda reader,0.7038252949714661
translation,61,162,ablation-analysis,greatest impact,on,performance,greatest impact on performance,0.5593242049217224
translation,61,162,ablation-analysis,paragraph dynamic self-attention layer,has,greatest impact,paragraph dynamic self-attention layer has greatest impact,0.5806116461753845
translation,61,162,ablation-analysis,ablation analysis,has,paragraph dynamic self-attention layer,ablation analysis has paragraph dynamic self-attention layer,0.5095558762550354
translation,61,163,ablation-analysis,paragraph attention mask and dynamic attention mask,contribute to,performance improvement,paragraph attention mask and dynamic attention mask contribute to performance improvement,0.6718406081199646
translation,61,163,ablation-analysis,ablation analysis,both,paragraph attention mask and dynamic attention mask,ablation analysis both paragraph attention mask and dynamic attention mask,0.655092179775238
translation,61,170,ablation-analysis,our multi-level prediction,critical to,long answer prediction,our multi-level prediction critical to long answer prediction,0.6918569207191467
translation,61,170,ablation-analysis,ablation analysis,see that,our multi-level prediction,ablation analysis see that our multi-level prediction,0.6764765977859497
translation,61,171,ablation-analysis,performance,of,long and short answers,performance of long and short answers,0.5606175065040588
translation,61,171,ablation-analysis,drops,about,1.0 f1 score,drops about 1.0 f1 score,0.6403388381004333
translation,61,171,ablation-analysis,long and short answers,has,drops,long and short answers has drops,0.6121996641159058
translation,61,171,ablation-analysis,ablation analysis,remove,cascaded structure,ablation analysis remove cascaded structure,0.6783077716827393
translation,61,176,ablation-analysis,question embedding,contributes to,performance improvement,question embedding contributes to performance improvement,0.6687266230583191
translation,61,176,ablation-analysis,ablation analysis,observed that,question embedding,ablation analysis observed that question embedding,0.6893290281295776
translation,61,177,ablation-analysis,dense prediction layers,with,tanh activation function,dense prediction layers with tanh activation function,0.610252857208252
translation,61,177,ablation-analysis,dense prediction layers,with,gaussian error linear unit gelu,dense prediction layers with gaussian error linear unit gelu,0.6313840746879578
translation,61,177,ablation-analysis,ablation analysis,remove,dense prediction layers,ablation analysis remove dense prediction layers,0.6906745433807373
translation,61,177,ablation-analysis,ablation analysis,remove,dense prediction layers,ablation analysis remove dense prediction layers,0.6906745433807373
translation,61,21,baselines,rikinet,employs,proposed dynamic paragraph dual-attention ( dpda ) reader,rikinet employs proposed dynamic paragraph dual-attention ( dpda ) reader,0.5375377535820007
translation,61,21,baselines,proposed dynamic paragraph dual-attention ( dpda ) reader,contains,multiple dpda blocks,proposed dynamic paragraph dual-attention ( dpda ) reader contains multiple dpda blocks,0.5794776082038879
translation,61,139,baselines,better,than,two baselines,better than two baselines,0.6160086393356323
translation,61,185,baselines,"bert joint ( alberti et al. , 2019",modifies,bert,"bert joint ( alberti et al. , 2019 modifies bert",0.6731428503990173
translation,61,185,baselines,bert,for,nq,bert for nq,0.7628147602081299
translation,61,185,baselines,baselines,has,"bert joint ( alberti et al. , 2019","baselines has bert joint ( alberti et al. , 2019",0.557720959186554
translation,61,126,experimental-setup,adam optimizer,with,batch size,adam optimizer with batch size,0.606801450252533
translation,61,126,experimental-setup,batch size,of,36,batch size of 36,0.6642854809761047
translation,61,126,experimental-setup,36,for,model training,36 for model training,0.6034170985221863
translation,61,126,experimental-setup,experimental setup,use,adam optimizer,experimental setup use adam optimizer,0.5987385511398315
translation,61,127,experimental-setup,hyperparameter k,set to,"2 ? 10 ?5 , 0.1 , 2 , 1024 , 2 , and 256","hyperparameter k set to 2 ? 10 ?5 , 0.1 , 2 , 1024 , 2 , and 256",0.6827327013015747
translation,61,127,experimental-setup,experimental setup,has,initial learning rate,experimental setup has initial learning rate,0.49018073081970215
translation,61,127,experimental-setup,experimental setup,has,learning rate warmup proportion,experimental setup has learning rate warmup proportion,0.4858885705471039
translation,61,128,experimental-setup,approximately 24 hours,train with,4 nvidia tesla p40,approximately 24 hours train with 4 nvidia tesla p40,0.7082492113113403
translation,61,130,experimental-setup,google released bert - large model,fine-tuned,synthetic self-training,google released bert - large model fine-tuned synthetic self-training,0.7407692670822144
translation,61,130,experimental-setup,experimental setup,use,google released bert - large model,experimental setup use google released bert - large model,0.6385087370872498
translation,61,6,experiments,rikinet,contains,dynamic paragraph dual-attention reader,rikinet contains dynamic paragraph dual-attention reader,0.6701446175575256
translation,61,6,experiments,rikinet,contains,multi-level cascaded answer predictor,rikinet contains multi-level cascaded answer predictor,0.6410710215568542
translation,61,213,experiments,rikinet,is,first single model,rikinet is first single model,0.5845283269882202
translation,61,213,experiments,first single model,that outperforms,single human performance,first single model that outperforms single human performance,0.7007346153259277
translation,61,213,experiments,natural questions dataset,has,rikinet,natural questions dataset has rikinet,0.5617587566375732
translation,61,5,model,wikipedia pages,for,natural question answering,wikipedia pages for natural question answering,0.5214900374412537
translation,61,5,model,model,called,rikinet,model called rikinet,0.734776496887207
translation,61,22,model,each dpda block,iteratively perform,dual-attention,each dpda block iteratively perform dual-attention,0.7203809022903442
translation,61,22,model,dual-attention,to represent,documents and questions,dual-attention to represent documents and questions,0.6840695738792419
translation,61,22,model,dual-attention,employ,paragraph self-attention,dual-attention employ paragraph self-attention,0.5823037624359131
translation,61,22,model,paragraph self-attention,with,dynamic attention mask,paragraph self-attention with dynamic attention mask,0.6612254977226257
translation,61,22,model,dynamic attention mask,to fuse,key tokens,dynamic attention mask to fuse key tokens,0.716733455657959
translation,61,22,model,key tokens,in,each paragraph,key tokens in each paragraph,0.5326269865036011
translation,61,22,model,model,In,each dpda block,model In each dpda block,0.5330827832221985
translation,61,22,model,model,employ,paragraph self-attention,model employ paragraph self-attention,0.5502334833145142
translation,61,61,model,information fusion,from,question,information fusion from question,0.5663153529167175
translation,61,61,model,information fusion,from,paragraphs,information fusion from paragraphs,0.6302679777145386
translation,61,61,model,information fusion,adapt,dual-attention mechanism,information fusion adapt dual-attention mechanism,0.7490078210830688
translation,61,61,model,question,to,paragraphs,question to paragraphs,0.612128734588623
translation,61,61,model,question,to,paragraphs,question to paragraphs,0.612128734588623
translation,61,61,model,model,To strengthen,information fusion,model To strengthen information fusion,0.6728283166885376
translation,61,61,model,model,adapt,dual-attention mechanism,model adapt dual-attention mechanism,0.7382257580757141
translation,61,175,model,question embedding,used for,answer type prediction,question embedding used for answer type prediction,0.6595554947853088
translation,61,175,model,model,remove,question embedding,model remove question embedding,0.7107573747634888
translation,61,32,results,our single model,obtains,74.3 f1 scores,our single model obtains 74.3 f1 scores,0.5578145384788513
translation,61,32,results,our single model,compared to,"published best single model ( alberti et al. , 2019a )","our single model compared to published best single model ( alberti et al. , 2019a )",0.6426160335540771
translation,61,32,results,74.3 f1 scores,on,long-answer task ( la ),74.3 f1 scores on long-answer task ( la ),0.47971880435943604
translation,61,32,results,74.3 f1 scores,on,short - answer task ( sa ),74.3 f1 scores on short - answer task ( sa ),0.47001197934150696
translation,61,32,results,57.9 f1 scores,on,short - answer task ( sa ),57.9 f1 scores on short - answer task ( sa ),0.4698764681816101
translation,61,32,results,"published best single model ( alberti et al. , 2019a )",of,53.9 f1,"published best single model ( alberti et al. , 2019a ) of 53.9 f1",0.4917464554309845
translation,61,32,results,66.8 f1,on,la,66.8 f1 on la,0.6423158645629883
translation,61,32,results,53.9 f1,on,sa,53.9 f1 on sa,0.6222086548805237
translation,61,32,results,nq test set,has,our single model,nq test set has our single model,0.5792055726051331
translation,61,32,results,results,On,nq test set,results On nq test set,0.5804955363273621
translation,61,140,results,rikinet -bert large,employs,bert large model,rikinet -bert large employs bert large model,0.5662286281585693
translation,61,140,results,single model,achieved,significant improvement,single model achieved significant improvement,0.7373909950256348
translation,61,140,results,significant improvement,over,previously published best model,significant improvement over previously published best model,0.6438310146331787
translation,61,140,results,significant improvement,on,test set,significant improvement on test set,0.5505170822143555
translation,61,140,results,previously published best model,on,test set,previously published best model on test set,0.5390103459358215
translation,61,140,results,test set,has,la,test set has la,0.6474220156669617
translation,61,140,results,results,has,rikinet -bert large,results has rikinet -bert large,0.5402554273605347
translation,61,143,results,bert joint + roberta large,performs,better,bert joint + roberta large performs better,0.6301204562187195
translation,61,143,results,better,than,original bert joint,better than original bert joint,0.6546637415885925
translation,61,143,results,results,has,bert joint + roberta large,results has bert joint + roberta large,0.5505293011665344
translation,61,144,results,rikinet-roberta large,employs,roberta large model,rikinet-roberta large employs roberta large model,0.5896209478378296
translation,61,144,results,rikinet-roberta large,achieves,better performance,rikinet-roberta large achieves better performance,0.662589967250824
translation,61,144,results,better performance,on,la and sa,better performance on la and sa,0.5488650798797607
translation,61,144,results,significantly outperforming,has,bert joint + roberta large,significantly outperforming has bert joint + roberta large,0.6402938365936279
translation,61,144,results,results,our single model of,rikinet-roberta large,results our single model of rikinet-roberta large,0.7183712124824524
translation,61,178,results,proposed dpda reader and multilevel cascaded answer predictor,has,significantly improve,proposed dpda reader and multilevel cascaded answer predictor has significantly improve,0.598763108253479
translation,61,178,results,significantly improve,has,model performance,significantly improve has model performance,0.5450441241264343
translation,61,214,results,rikinet ensemble,achieves,new state - of - the - art results,rikinet ensemble achieves new state - of - the - art results,0.6448865532875061
translation,61,214,results,new state - of - the - art results,at,76.1 f1,new state - of - the - art results at 76.1 f1,0.49903324246406555
translation,61,214,results,new state - of - the - art results,at,61.3 f1,new state - of - the - art results at 61.3 f1,0.5046488642692566
translation,61,214,results,76.1 f1,on,long-answer,76.1 f1 on long-answer,0.5010152459144592
translation,61,214,results,61.3 f1,on,shortanswer tasks,61.3 f1 on shortanswer tasks,0.5001806020736694
translation,61,214,results,significantly outperforms,has,all the other models,significantly outperforms has all the other models,0.5850989818572998
translation,61,214,results,results,has,rikinet ensemble,results has rikinet ensemble,0.5222920775413513
translation,62,146,ablation-analysis,impact,on,qa accuracies,impact on qa accuracies,0.6010853052139282
translation,62,146,ablation-analysis,enlarged answer space,has,impact,enlarged answer space has impact,0.5915610790252686
translation,62,146,ablation-analysis,ablation analysis,has,enlarged answer space,ablation analysis has enlarged answer space,0.5921679139137268
translation,62,139,baselines,summarization model,attend to,specific regions,summarization model attend to specific regions,0.7447713613510132
translation,62,139,baselines,specific regions,of,input documents,specific regions of input documents,0.5741680264472961
translation,62,139,baselines,attention,to traverse,different content,attention to traverse different content,0.7219980359077454
translation,62,139,baselines,different content,of,source document,different content of source document,0.5865876078605652
translation,62,139,baselines,distract,has,attention,distract has attention,0.5993187427520752
translation,62,131,experiments,entity q,uses,qa pairs,entity q uses qa pairs,0.6479874849319458
translation,62,131,experiments,qa pairs,whose,answers,qa pairs whose answers,0.7101381421089172
translation,62,117,hyperparameters,hidden state size,of,bi-lstm,hidden state size of bi-lstm,0.5796380043029785
translation,62,117,hyperparameters,hidden state size,of,single-direction lstm encoder,hidden state size of single-direction lstm encoder,0.527131199836731
translation,62,117,hyperparameters,hidden state size,of,single-direction lstm encoder,hidden state size of single-direction lstm encoder,0.527131199836731
translation,62,117,hyperparameters,bi-lstm,is,256,bi-lstm is 256,0.5635505318641663
translation,62,117,hyperparameters,hidden state size,of,single-direction lstm encoder,hidden state size of single-direction lstm encoder,0.527131199836731
translation,62,117,hyperparameters,single-direction lstm encoder,is,30,single-direction lstm encoder is 30,0.5393884181976318
translation,62,118,hyperparameters,"dropout rate ( srivastava , 2013 )",used twice in,sampling component,"dropout rate ( srivastava , 2013 ) used twice in sampling component",0.6791637539863586
translation,62,118,hyperparameters,hyperparameters,has,"dropout rate ( srivastava , 2013 )","hyperparameters has dropout rate ( srivastava , 2013 )",0.5110914707183838
translation,62,119,hyperparameters,minibatch size,set to,256,minibatch size set to 256,0.6753116250038147
translation,62,119,hyperparameters,hyperparameters,has,minibatch size,hyperparameters has minibatch size,0.4892244338989258
translation,62,120,hyperparameters,early stopping,on,validation set,early stopping on validation set,0.5784603357315063
translation,62,120,hyperparameters,hyperparameters,apply,early stopping,hyperparameters apply early stopping,0.6104239225387573
translation,62,121,hyperparameters,source vocabulary,contains,150 k words,source vocabulary contains 150 k words,0.6349698305130005
translation,62,121,hyperparameters,hyperparameters,has,source vocabulary,hyperparameters has source vocabulary,0.4696158170700073
translation,62,122,hyperparameters,100 - dimensional,initialized by,glove,100 - dimensional initialized by glove,0.6979328989982605
translation,62,122,hyperparameters,word embeddings,remain,trainable,word embeddings remain trainable,0.5644240975379944
translation,62,122,hyperparameters,100 - dimensional,has,word embeddings,100 - dimensional has word embeddings,0.5435778498649597
translation,62,122,hyperparameters,hyperparameters,use,100 - dimensional,hyperparameters use 100 - dimensional,0.630931556224823
translation,62,124,hyperparameters,maximum length,of,input,maximum length of input,0.6013240218162537
translation,62,124,hyperparameters,input,set to,100 words,input set to 100 words,0.6780381202697754
translation,62,124,hyperparameters,input,set to,0.4 ( ?40 words ),input set to 0.4 ( ?40 words ),0.7282130122184753
translation,62,124,hyperparameters,hyperparameters,has,maximum length,hyperparameters has maximum length,0.5061417818069458
translation,62,125,hyperparameters,adam optimizer,with,initial learning rate,adam optimizer with initial learning rate,0.5838022828102112
translation,62,125,hyperparameters,adam optimizer,halve,learning rate,adam optimizer halve learning rate,0.7313622832298279
translation,62,125,hyperparameters,initial learning rate,of,1e - 4,initial learning rate of 1e - 4,0.6140750050544739
translation,62,125,hyperparameters,learning rate,if,objective,learning rate if objective,0.5674840807914734
translation,62,125,hyperparameters,worsens,beyond,threshold ( > 10 % ),worsens beyond threshold ( > 10 % ),0.6768166422843933
translation,62,125,hyperparameters,objective,has,worsens,objective has worsens,0.6153466701507568
translation,62,125,hyperparameters,hyperparameters,use,adam optimizer,hyperparameters use adam optimizer,0.6090166568756104
translation,62,37,model,attention mechanism,to locate,segments,attention mechanism to locate segments,0.6760973334312439
translation,62,37,model,segments,of,summary,segments of summary,0.6561954617500305
translation,62,37,model,summary,relevant to,given question,summary relevant to given question,0.7053570747375488
translation,62,37,model,answer,has,multiple questions,answer has multiple questions,0.5723053812980652
translation,62,37,model,model,use,attention mechanism,model use attention mechanism,0.6464574933052063
translation,62,140,model,words,from,source text,words from source text,0.5468318462371826
translation,62,140,model,words,via,pointing,words via pointing,0.6787258982658386
translation,62,140,model,novel words,through,generator,novel words through generator,0.6875094175338745
translation,62,140,model,copy,has,words,copy has words,0.5742371678352356
translation,62,147,results,entities,as,answers,entities as answers,0.5271488428115845
translation,62,147,results,training accuracy,is,34.8 % ( q5 ),training accuracy is 34.8 % ( q5 ),0.5752133131027222
translation,62,147,results,validation,is,15.4 % ( q5 ),validation is 15.4 % ( q5 ),0.5691683888435364
translation,62,147,results,entities,has,training accuracy,entities has training accuracy,0.5387943387031555
translation,62,147,results,answers,has,training accuracy,answers has training accuracy,0.5849090218544006
translation,62,147,results,results,using,entities,results using entities,0.591489851474762
translation,63,168,baselines,baselines,has,random guess model ( rg ),baselines has random guess model ( rg ),0.5461941957473755
translation,63,180,baselines,baselines,has,skip- gram model ( sg ),baselines has skip- gram model ( sg ),0.5164318680763245
translation,63,181,baselines,word embedding,trained by,"skipgram ( mikolov et al. , 2013 )","word embedding trained by skipgram ( mikolov et al. , 2013 )",0.6842642426490784
translation,63,182,hyperparameters,window size,as,5,window size as 5,0.6092725396156311
translation,63,182,hyperparameters,embedding dimension,as,500,embedding dimension as 500,0.6071677207946777
translation,63,182,hyperparameters,negative sampling count,as,3,negative sampling count as 3,0.6219895482063293
translation,63,182,hyperparameters,epoch number,as,3,epoch number as 3,0.5847741365432739
translation,63,182,hyperparameters,hyperparameters,set,window size,hyperparameters set window size,0.6380046606063843
translation,63,182,hyperparameters,hyperparameters,set,embedding dimension,hyperparameters set embedding dimension,0.6303911805152893
translation,63,182,hyperparameters,hyperparameters,set,negative sampling count,hyperparameters set negative sampling count,0.6232867240905762
translation,63,182,hyperparameters,hyperparameters,set,epoch number,hyperparameters set epoch number,0.6399648785591125
translation,63,183,hyperparameters,pre-trained word embedding,by,google,pre-trained word embedding by google,0.555687665939331
translation,63,183,hyperparameters,pre-trained word embedding,with,dimension,pre-trained word embedding with dimension,0.6057522892951965
translation,63,183,hyperparameters,dimension,of,300,dimension of 300,0.6947354674339294
translation,63,183,hyperparameters,hyperparameters,employed,pre-trained word embedding,hyperparameters employed pre-trained word embedding,0.5437614321708679
translation,63,192,hyperparameters,embedding,on,wiki2014,embedding on wiki2014,0.5936299562454224
translation,63,192,hyperparameters,embedding,set,window size,embedding set window size,0.6491010785102844
translation,63,192,hyperparameters,embedding,set,negative sampling count,embedding set negative sampling count,0.7007192969322205
translation,63,192,hyperparameters,embedding,set,epoch number,embedding set epoch number,0.6444867253303528
translation,63,192,hyperparameters,window size,as,5,window size as 5,0.6092725396156311
translation,63,192,hyperparameters,embedding dimension,as,500,embedding dimension as 500,0.6071677207946777
translation,63,192,hyperparameters,negative sampling count,as,3,negative sampling count as 3,0.6219895482063293
translation,63,192,hyperparameters,epoch number,as,3,epoch number as 3,0.5847741365432739
translation,63,192,hyperparameters,hyperparameters,when learning,embedding,hyperparameters when learning embedding,0.7012712955474854
translation,63,192,hyperparameters,hyperparameters,set,negative sampling count,hyperparameters set negative sampling count,0.6232867240905762
translation,63,193,hyperparameters,online longman dictionary,as,dictionary,online longman dictionary as dictionary,0.5566157102584839
translation,63,193,hyperparameters,dictionary,used in,multi-sense clustering,dictionary used in multi-sense clustering,0.6582441329956055
translation,63,193,hyperparameters,hyperparameters,adopted,online longman dictionary,hyperparameters adopted online longman dictionary,0.6014001369476318
translation,63,7,model,novel framework,to automatically solve,verbal iq questions,novel framework to automatically solve verbal iq questions,0.6833832859992981
translation,63,7,model,verbal iq questions,by leveraging,improved word embedding,verbal iq questions by leveraging improved word embedding,0.6216145753860474
translation,63,7,model,improved word embedding,by jointly considering,multi-sense nature of words,improved word embedding by jointly considering multi-sense nature of words,0.6943069100379944
translation,63,7,model,improved word embedding,by jointly considering,relational information,improved word embedding by jointly considering relational information,0.7139753699302673
translation,63,7,model,relational information,among,words,relational information among words,0.6060084700584412
translation,63,7,model,model,propose,novel framework,model propose novel framework,0.720393180847168
translation,63,24,model,novel framework,consists of,three components,novel framework consists of three components,0.6834304332733154
translation,63,24,model,model,propose,novel framework,model propose novel framework,0.720393180847168
translation,63,25,model,classifier,to recognize,"specific type ( e.g. , analogy , classification , synonym , and antonym )","classifier to recognize specific type ( e.g. , analogy , classification , synonym , and antonym )",0.7015926837921143
translation,63,25,model,"specific type ( e.g. , analogy , classification , synonym , and antonym )",of,verbal questions,"specific type ( e.g. , analogy , classification , synonym , and antonym ) of verbal questions",0.5077642798423767
translation,63,25,model,model,build,classifier,model build classifier,0.7780051231384277
translation,63,29,model,each polysemous word,retrieve,number of senses,each polysemous word retrieve number of senses,0.5915833115577698
translation,63,29,model,each polysemous word,conduct,clustering,each polysemous word conduct clustering,0.6926915645599365
translation,63,29,model,number of senses,from,dictionary,number of senses from dictionary,0.5517992973327637
translation,63,29,model,clustering,on,all its context windows,clustering on all its context windows,0.538915753364563
translation,63,29,model,all its context windows,has,in the corpus,all its context windows has in the corpus,0.592422604560852
translation,63,29,model,model,for,each polysemous word,model for each polysemous word,0.5962991118431091
translation,63,32,model,embedding vectors,for,"relations ( e.g. , synonym and antonym )","embedding vectors for relations ( e.g. , synonym and antonym )",0.5573597550392151
translation,63,32,model,embedding vectors,by incorporating,relational knowledge,embedding vectors by incorporating relational knowledge,0.7182180285453796
translation,63,32,model,relational knowledge,into,objective function,relational knowledge into objective function,0.5237931609153748
translation,63,32,model,objective function,of,word embedding learning algorithm,objective function of word embedding learning algorithm,0.5187203288078308
translation,63,32,model,model,learn,embedding vectors,model learn embedding vectors,0.6833993792533875
translation,63,33,model,learning,of,word-sense representations and relation representations,learning of word-sense representations and relation representations,0.47794464230537415
translation,63,33,model,word-sense representations and relation representations,interacts with,each other,word-sense representations and relation representations interacts with each other,0.7015286684036255
translation,63,33,model,model,has,learning,model has learning,0.5897330045700073
translation,63,34,model,each type of question,propose,specific solver,each type of question propose specific solver,0.6659301519393921
translation,63,34,model,specific solver,based on,obtained distributed word-sense representations and relation representations,specific solver based on obtained distributed word-sense representations and relation representations,0.6468732953071594
translation,63,34,model,model,for,each type of question,model for each type of question,0.6313801407814026
translation,63,201,results,accuracy,of,answering,accuracy of answering,0.6310797929763794
translation,63,201,results,answering,has,verbal questions,answering has verbal questions,0.5795600414276123
translation,63,201,results,results,has,overall accuracy,results has overall accuracy,0.5751331448554993
translation,63,203,results,rk,achieve,best overall accuracy,rk achieve best overall accuracy,0.6546707153320312
translation,63,203,results,best overall accuracy,than,all the other methods,best overall accuracy than all the other methods,0.5366910696029663
translation,63,204,results,rk,raise,overall accuracy,rk raise overall accuracy,0.6416535973548889
translation,63,204,results,overall accuracy,by,4.63 %,overall accuracy by 4.63 %,0.5422452092170715
translation,63,204,results,overall accuracy,about,4.63 %,overall accuracy about 4.63 %,0.5560990571975708
translation,63,204,results,4.63 %,over,hp,4.63 % over hp,0.6635521650314331
translation,63,204,results,empirically superior,to,skip-gram models,empirically superior to skip-gram models,0.5065597295761108
translation,63,204,results,results,has,rk,results has rk,0.48863327503204346
translation,63,206,results,performance difference,between,ms -1/ms -2/ms -3 and sg - 1/ sg - 2 / glove,performance difference between ms -1/ms -2/ms -3 and sg - 1/ sg - 2 / glove,0.6478676795959473
translation,63,206,results,ms -1/ms -2/ms -3 and sg - 1/ sg - 2 / glove,is,not significant,ms -1/ms -2/ms -3 and sg - 1/ sg - 2 / glove is not significant,0.5934097170829773
translation,63,206,results,results,Note,performance difference,results Note performance difference,0.6427510380744934
translation,63,210,results,empirically superior,than,"two multi-sense algorithms ms - 1 , ms - 2 and ms - 3","empirically superior than two multi-sense algorithms ms - 1 , ms - 2 and ms - 3",0.5373462438583374
translation,63,210,results,results,has,rk,results has rk,0.48863327503204346
translation,63,232,results,knowledge,learning,word embedding,knowledge learning word embedding,0.755629301071167
translation,63,232,results,our rk model,improve,accuracy,our rk model improve accuracy,0.7134783864021301
translation,63,232,results,accuracy,over,all question types,accuracy over all question types,0.601775586605072
translation,63,232,results,knowledge,has,our rk model,knowledge has our rk model,0.6160593628883362
translation,63,232,results,word embedding,has,our rk model,word embedding has our rk model,0.5493670701980591
translation,63,232,results,results,After incorporating,knowledge,results After incorporating knowledge,0.668165385723114
translation,63,233,results,rk,result in,big improvement,rk result in big improvement,0.6692039370536804
translation,63,233,results,big improvement,over,hp,big improvement over hp,0.7198615074157715
translation,63,233,results,big improvement,on,question types,big improvement on question types,0.502149760723114
translation,63,233,results,question types,of,synonym and classification,question types of synonym and classification,0.5735024213790894
translation,64,205,ablation-analysis,extra dimension,of determining,extra information is necessary,extra dimension of determining extra information is necessary,0.6839896440505981
translation,64,205,ablation-analysis,extra information is necessary,causes,model,extra information is necessary causes model,0.6692565679550171
translation,64,205,ablation-analysis,model,to become,less confident,model to become less confident,0.7221382856369019
translation,64,205,ablation-analysis,recall,on,link selection,recall on link selection,0.563560962677002
translation,64,205,ablation-analysis,significantly hurting,has,recall,significantly hurting has recall,0.6063945293426514
translation,64,205,ablation-analysis,ablation analysis,Adding,extra dimension,ablation analysis Adding extra dimension,0.7678179144859314
translation,64,21,experiments,iirc,is,crowdsourced dataset,iirc is crowdsourced dataset,0.5245940089225769
translation,64,21,experiments,crowdsourced dataset,of,13441 questions,crowdsourced dataset of 13441 questions,0.5094395875930786
translation,64,21,experiments,13441 questions,over,5698 paragraphs,13441 questions over 5698 paragraphs,0.6432185769081116
translation,64,21,experiments,5698 paragraphs,from,english wikipedia,5698 paragraphs from english wikipedia,0.5822854042053223
translation,64,128,hyperparameters,scoring function,used,single linear layer,scoring function used single linear layer,0.5795624852180481
translation,64,128,hyperparameters,single linear layer,with,sigmoid activation function,single linear layer with sigmoid activation function,0.6157976984977722
translation,64,128,hyperparameters,hyperparameters,For,scoring function,hyperparameters For scoring function,0.5756887793540955
translation,64,129,hyperparameters,score threshold,to select,links,score threshold to select links,0.7027917504310608
translation,64,129,hyperparameters,links,set to,0.5,links set to 0.5,0.6342405080795288
translation,64,129,hyperparameters,hyperparameters,trained using,adam,hyperparameters trained using adam,0.7512266635894775
translation,64,129,hyperparameters,hyperparameters,trained using,score threshold,hyperparameters trained using score threshold,0.6946058869361877
translation,64,138,hyperparameters,single linear layer,with,sigmoid activation,single linear layer with sigmoid activation,0.6565240621566772
translation,64,138,hyperparameters,sigmoid activation,as,scoring function,sigmoid activation as scoring function,0.5754650235176086
translation,64,138,hyperparameters,hyperparameters,used,single linear layer,hyperparameters used single linear layer,0.5739238262176514
translation,64,32,results,model,results in,performance,model results in performance,0.7160879969596863
translation,64,32,results,oracle pipeline components,results in,performance,oracle pipeline components results in performance,0.6466095447540283
translation,64,32,results,performance,of,only 70.3 %,performance of only 70.3 %,0.5304788947105408
translation,64,32,results,model,has,oracle pipeline components,model has oracle pipeline components,0.580799400806427
translation,64,32,results,results,giving,model,results giving model,0.6269973516464233
translation,64,145,results,31.1 % f 1,well below,human performance,31.1 % f 1 well below human performance,0.7381539940834045
translation,64,145,results,human performance,of,88.4 %,human performance of 88.4 %,0.552129864692688
translation,64,146,results,benefit,of,gold input,benefit of gold input,0.5858728289604187
translation,64,146,results,gold input,still room for,improvement,gold input still room for improvement,0.7613371014595032
translation,64,146,results,improvement,on,reasoning,improvement on reasoning,0.5604475140571594
translation,64,146,results,reasoning,over,multiple contexts,reasoning over multiple contexts,0.6109587550163269
translation,64,146,results,results,with,benefit,results with benefit,0.6134142279624939
translation,64,171,results,original passage,has,does not significantly improve,original passage has does not significantly improve,0.6054548025131226
translation,64,171,results,does not significantly improve,has,performance,does not significantly improve has performance,0.5917466878890991
translation,64,171,results,results,adding,original passage,results adding original passage,0.6641116738319397
translation,65,4,experiments,good answers,in,community forum,good answers in community forum,0.5217669010162354
translation,65,6,experiments,our primary submission,ranked,third,our primary submission ranked third,0.6872881650924683
translation,65,6,experiments,third,with,map,third with map,0.7086199522018433
translation,65,6,experiments,third,with,accuracy,third with accuracy,0.6866996884346008
translation,65,6,experiments,map,of,51.68,map of 51.68,0.5893017649650574
translation,65,6,experiments,accuracy,of,69.94,accuracy of 69.94,0.5464432835578918
translation,65,6,experiments,main subtask c,has,our primary submission,main subtask c has our primary submission,0.5937591195106506
translation,65,7,experiments,third,with,map,third with map,0.7086199522018433
translation,65,7,experiments,third,with,accuracy,third with accuracy,0.6866996884346008
translation,65,7,experiments,map,of,77.58,map of 77.58,0.5876713991165161
translation,65,7,experiments,accuracy,of,73.39,accuracy of 73.39,0.5277272462844849
translation,65,7,experiments,subtask a,has,our primary submission,subtask a has our primary submission,0.5875667929649353
translation,66,147,ablation-analysis,aggregate action,is,least important action,aggregate action is least important action,0.5637372732162476
translation,66,147,ablation-analysis,extend action,is,most important one,extend action is most important one,0.5998015403747559
translation,66,147,ablation-analysis,ablation analysis,see that,aggregate action,ablation analysis see that aggregate action,0.6374320983886719
translation,66,131,baselines,"lan et al. , 2019a )",considers,multi-hop relation paths,"lan et al. , 2019a ) considers multi-hop relation paths",0.6338998079299927
translation,66,131,baselines,"lan et al. , 2019a )",uses,constraints,"lan et al. , 2019a ) uses constraints",0.568581223487854
translation,66,131,baselines,constraints,to reduce,search space,constraints to reduce search space,0.6505727767944336
translation,66,131,baselines,constraints,to reduce,search space,constraints to reduce search space,0.6505727767944336
translation,66,109,experimental-setup,bert module,in,ranker,bert module in ranker,0.5454071760177612
translation,66,109,experimental-setup,ranker,with,bert base model,ranker with bert base model,0.6465378999710083
translation,66,109,experimental-setup,experimental setup,initialize,bert module,experimental setup initialize bert module,0.6969411969184875
translation,66,110,experimental-setup,experimental setup,has,other parameters,experimental setup has other parameters,0.4971172511577606
translation,66,111,experimental-setup,hyper-parameters,in,bert model,hyper-parameters in bert model,0.5287864208221436
translation,66,111,experimental-setup,hyper-parameters,set,dropout ratio,hyper-parameters set dropout ratio,0.6474152207374573
translation,66,111,experimental-setup,hyper-parameters,set,hidden size,hyper-parameters set hidden size,0.675035834312439
translation,66,111,experimental-setup,dropout ratio,as,0.1,dropout ratio as 0.1,0.5334866046905518
translation,66,111,experimental-setup,hidden size,as,768,hidden size as 768,0.5889773964881897
translation,66,111,experimental-setup,experimental setup,For,hyper-parameters,experimental setup For hyper-parameters,0.5783532857894897
translation,66,116,experimental-setup,https,has,://developers,https has ://developers,0.5194191336631775
translation,66,120,experimental-setup,number of multi-attention heads,set as,6 and 12,number of multi-attention heads set as 6 and 12,0.6148587465286255
translation,66,120,experimental-setup,experimental setup,has,number of multi-attention heads,experimental setup has number of multi-attention heads,0.5441536903381348
translation,66,121,experimental-setup,latest dump of freebase,as,our kb,latest dump of freebase as our kb,0.5423886775970459
translation,66,121,experimental-setup,experimental setup,use,latest dump of freebase,experimental setup use latest dump of freebase,0.604218065738678
translation,66,122,experimental-setup,beam search,set,beam size k,beam search set beam size k,0.7212925553321838
translation,66,122,experimental-setup,beam size k,to be,3,beam size k to be 3,0.6300348043441772
translation,66,122,experimental-setup,experimental setup,For,beam search,experimental setup For beam search,0.5473336577415466
translation,66,6,model,modified staged query graph generation method,with,more flexible ways,modified staged query graph generation method with more flexible ways,0.6094849109649658
translation,66,6,model,more flexible ways,to generate,query graphs,more flexible ways to generate query graphs,0.6302135586738586
translation,66,6,model,model,propose,modified staged query graph generation method,model propose modified staged query graph generation method,0.6350412368774414
translation,66,21,model,both constraints and multi-hop relations,for,complex kbqa,both constraints and multi-hop relations for complex kbqa,0.6510909795761108
translation,66,21,model,model,handle,both constraints and multi-hop relations,model handle both constraints and multi-hop relations,0.6098231077194214
translation,66,25,results,substantially outperforms,with,improvement,substantially outperforms with improvement,0.7273585796356201
translation,66,25,results,existing methods,with,improvement,existing methods with improvement,0.6478121876716614
translation,66,25,results,improvement,of,3.3 percentage points,improvement of 3.3 percentage points,0.5487897396087646
translation,66,25,results,improvement,of,3.9 percentage points,improvement of 3.9 percentage points,0.542142927646637
translation,66,25,results,3.3 percentage points,in,prec@1,3.3 percentage points in prec@1,0.5598143339157104
translation,66,25,results,3.3 percentage points,in,f1,3.3 percentage points in f1,0.5582590699195862
translation,66,25,results,3.9 percentage points,in,f1,3.9 percentage points in f1,0.5544318556785583
translation,66,25,results,complexwebquestions dataset,has,our method,complexwebquestions dataset has our method,0.5430378317832947
translation,66,25,results,our method,has,substantially outperforms,our method has substantially outperforms,0.6121749877929688
translation,66,25,results,substantially outperforms,has,existing methods,substantially outperforms has existing methods,0.5605924129486084
translation,66,25,results,results,On,complexwebquestions dataset,results On complexwebquestions dataset,0.5136784911155701
translation,66,26,results,our method,achieves,state of the art,our method achieves state of the art,0.569302499294281
translation,66,26,results,benchmark kbqa datasets,has,our method,benchmark kbqa datasets has our method,0.49605539441108704
translation,66,136,results,our method,achieves,best performance,our method achieves best performance,0.6578624248504639
translation,66,136,results,best performance,in terms of,prec@1 and f1,best performance in terms of prec@1 and f1,0.748322069644928
translation,66,136,results,cwq dataset,has,our method,cwq dataset has our method,0.5658499002456665
translation,66,136,results,results,on,cwq dataset,results on cwq dataset,0.5631035566329956
translation,66,137,results,improvement,is,substantial,improvement is substantial,0.6162617206573486
translation,66,137,results,substantial,with,3.3 percentage points,substantial with 3.3 percentage points,0.6015536189079285
translation,66,137,results,substantial,with,3.9 percentage points,substantial with 3.9 percentage points,0.6019465327262878
translation,66,137,results,3.3 percentage points,in,prec@1,3.3 percentage points in prec@1,0.5598143339157104
translation,66,137,results,3.9 percentage points,in,f1,3.9 percentage points in f1,0.5544318556785583
translation,66,139,results,our method,achieves,sota,our method achieves sota,0.7186108231544495
translation,66,139,results,other two datasets,has,our method,other two datasets has our method,0.5566270351409912
translation,66,139,results,wqsp and cq,has,our method,wqsp and cq has our method,0.5867424011230469
translation,66,139,results,outperforming,has,previous methods,outperforming has previous methods,0.5828545689582825
translation,66,139,results,results,For,other two datasets,results For other two datasets,0.5898985862731934
translation,66,143,results,lstm - based version,of,our method,lstm - based version of our method,0.575598418712616
translation,66,143,results,our method,has,still outperform,our method has still outperform,0.6065051555633545
translation,66,143,results,still outperform,has,previous state of the art,still outperform has previous state of the art,0.5408754944801331
translation,67,165,ablation-analysis,transition matrix ( tm ),gives,best performance,transition matrix ( tm ) gives best performance,0.5947675704956055
translation,67,165,ablation-analysis,ablation analysis,following,transition matrix ( tm ),ablation analysis following transition matrix ( tm ),0.6810036301612854
translation,67,176,ablation-analysis,addition of information,from,previous turns ( w/ 1 - ctx ),addition of information from previous turns ( w/ 1 - ctx ),0.5689607858657837
translation,67,176,ablation-analysis,addition of information,helps,significantly,addition of information helps significantly,0.6726446747779846
translation,67,176,ablation-analysis,ablation analysis,has,addition of information,ablation analysis has addition of information,0.5583370327949524
translation,67,177,ablation-analysis,context size,in,bidaf ++,context size in bidaf ++,0.5420951843261719
translation,67,177,ablation-analysis,context size,continues to,help,context size continues to help,0.6896297335624695
translation,67,177,ablation-analysis,bidaf ++,continues to,help,bidaf ++ continues to help,0.6798315644264221
translation,67,177,ablation-analysis,saturation,using,contexts,saturation using contexts,0.7034568190574646
translation,67,177,ablation-analysis,contexts,has,of length 3,contexts has of length 3,0.6160954833030701
translation,67,177,ablation-analysis,ablation analysis,increasing,context size,ablation analysis increasing context size,0.7215853929519653
translation,67,177,ablation-analysis,ablation analysis,observe,saturation,ablation analysis observe saturation,0.6242192387580872
translation,67,152,baselines,re-implementation,of,topperforming squad model,re-implementation of topperforming squad model,0.5988751649856567
translation,67,152,baselines,topperforming squad model,augments,bidirectional attention flow,topperforming squad model augments bidirectional attention flow,0.6439670920372009
translation,67,152,baselines,bidirectional attention flow,with,self-attention and contextualized embeddings,bidirectional attention flow with self-attention and contextualized embeddings,0.6378759741783142
translation,67,4,experiments,quac,for,question answering in context,quac for question answering in context,0.6241096258163452
translation,67,4,experiments,question answering in context,contains,14 k information - seeking qa dialogs,question answering in context contains 14 k information - seeking qa dialogs,0.5695940852165222
translation,67,13,experiments,dialog,present,quac,dialog present quac,0.698646605014801
translation,67,13,experiments,large-scale dataset,for,question answering,large-scale dataset for question answering,0.5468378067016602
translation,67,13,experiments,question answering,contains,14 k crowdsourced qa dialogs,question answering contains 14 k crowdsourced qa dialogs,0.573238730430603
translation,67,13,experiments,quac,has,large-scale dataset,quac has large-scale dataset,0.5146024823188782
translation,67,13,experiments,14 k crowdsourced qa dialogs,has,100 k total qa pairs,14 k crowdsourced qa dialogs has 100 k total qa pairs,0.5746219754219055
translation,67,150,model,simple matching features,e.g.,contextual features,simple matching features e.g. contextual features,0.6470191478729248
translation,67,150,model,"e.g. , matching features",computed with,"previous questions / answers , turn number","e.g. , matching features computed with previous questions / answers , turn number",0.6627769470214844
translation,67,150,model,contextual features,has,"e.g. , matching features","contextual features has e.g. , matching features",0.5412135124206543
translation,67,150,model,model,use,simple matching features,model use simple matching features,0.6765333414077759
translation,67,8,results,underperforms,by,20 f1,underperforms by 20 f1,0.6205085515975952
translation,67,8,results,best model,has,underperforms,best model has underperforms,0.5716190338134766
translation,67,8,results,underperforms,has,humans,underperforms has humans,0.5929281115531921
translation,67,8,results,results,has,best model,results has best model,0.5634682774543762
translation,67,167,results,human upper bound ( 80.8 f1 ),demonstrates,high agreement,human upper bound ( 80.8 f1 ) demonstrates high agreement,0.6244641542434692
translation,67,167,results,results,has,human upper bound ( 80.8 f1 ),results has human upper bound ( 80.8 f1 ),0.5422344207763672
translation,67,168,results,gold sentence + na,perform,well,gold sentence + na perform well,0.59834223985672
translation,67,168,results,results,has,gold sentence + na,results has gold sentence + na,0.5854390859603882
translation,67,171,results,simple text matching baselines,perform,poorly,simple text matching baselines perform poorly,0.5549913644790649
translation,67,171,results,simple text matching baselines,models that incorporate,dialog context,simple text matching baselines models that incorporate dialog context,0.6157717704772949
translation,67,171,results,dialog context,has,significantly outperform,dialog context has significantly outperform,0.6217795610427856
translation,67,171,results,results,has,simple text matching baselines,results has simple text matching baselines,0.5658335089683533
translation,67,172,results,best model,by,large margin,best model by large margin,0.5925209522247314
translation,67,172,results,humans,has,outperform,humans has outperform,0.6483243107795715
translation,67,172,results,outperform,has,best model,outperform has best model,0.6519679427146912
translation,67,172,results,results,has,humans,results has humans,0.4953257739543915
translation,67,175,results,bidaf ++ models,make,significant progress,bidaf ++ models make significant progress,0.7014474868774414
translation,67,175,results,results,has,bidaf ++ models,results has bidaf ++ models,0.5308263301849365
translation,67,178,results,system,achieves,human equivalence,system achieves human equivalence,0.6984540820121765
translation,67,178,results,best model,has,underperforms,best model has underperforms,0.5716190338134766
translation,67,178,results,best model,has,system,best model has system,0.6262853741645813
translation,67,178,results,underperforms,has,humans,underperforms has humans,0.5929281115531921
translation,67,178,results,underperforms,has,system,underperforms has system,0.6563213467597961
translation,67,178,results,humans,has,system,humans has system,0.6462084054946899
translation,67,191,results,context - aware baseline,performs,6 heq - q,context - aware baseline performs 6 heq - q,0.5901952385902405
translation,67,191,results,higher,on,  follow up   questions,higher on   follow up   questions,0.5193517804145813
translation,67,191,results,context- agnostic baseline,shows,no heq - q difference,context- agnostic baseline shows no heq - q difference,0.6499918103218079
translation,67,191,results,no heq - q difference,between,two types of questions,no heq - q difference between two types of questions,0.6419910788536072
translation,67,191,results,6 heq - q,has,higher,6 heq - q has higher,0.6702625155448914
translation,67,191,results,results,has,context - aware baseline,results has context - aware baseline,0.5560711026191711
translation,68,48,baselines,evaluation library,receives,same input,evaluation library receives same input,0.7040290236473083
translation,68,48,baselines,two software packages,has,evaluation library,two software packages has evaluation library,0.5398487448692322
translation,68,49,baselines,"our pytorch ( paszke et al. , 2017 ) based summarizer",optimizes,apes scores,"our pytorch ( paszke et al. , 2017 ) based summarizer optimizes apes scores",0.7464598417282104
translation,68,49,baselines,apes scores,together with,trained models,apes scores together with trained models,0.5823233127593994
translation,68,49,baselines,baselines,has,"our pytorch ( paszke et al. , 2017 ) based summarizer","baselines has our pytorch ( paszke et al. , 2017 ) based summarizer",0.5323416590690613
translation,68,32,model,new automatic evaluation metric,more suitable for,single reference news article datasets,new automatic evaluation metric more suitable for single reference news article datasets,0.6115393042564392
translation,68,32,model,model,introduce,new automatic evaluation metric,model introduce new automatic evaluation metric,0.6312101483345032
translation,68,199,results,our model,achieves,significantly higher apes scores,our model achieves significantly higher apes scores,0.6489255428314209
translation,68,199,results,our model,improves,all rouge metrics,our model improves all rouge metrics,0.6838064789772034
translation,68,199,results,significantly higher apes scores,has,46.1 vs. 39.8 ),significantly higher apes scores has 46.1 vs. 39.8 ),0.5257541537284851
translation,68,199,results,results,has,our model,results has our model,0.5871725678443909
translation,68,200,results,scores,on,validation set,scores on validation set,0.5589589476585388
translation,68,200,results,validation set,are,"46.6 , 41.2 , 18.4 , 38.1","validation set are 46.6 , 41.2 , 18.4 , 38.1",0.5121650099754333
translation,68,200,results,"46.6 , 41.2 , 18.4 , 38.1",for,"apes , r1 , r2 , rl","46.6 , 41.2 , 18.4 , 38.1 for apes , r1 , r2 , rl",0.6024441123008728
translation,68,200,results,results,has,scores,results has scores,0.5219217538833618
translation,68,201,results,our model,increases,corresponding rouge scores,our model increases corresponding rouge scores,0.6867987513542175
translation,68,201,results,apes score,has,our model,apes score has our model,0.5942608714103699
translation,69,182,ablation-analysis,error rate reduction,for,bert large,error rate reduction for bert large,0.6530211567878723
translation,69,182,ablation-analysis,bert large,is,20 % and 19 %,bert large is 20 % and 19 %,0.6493414640426636
translation,69,182,ablation-analysis,20 % and 19 %,for,squad 1.1 and 2.0,20 % and 19 % for squad 1.1 and 2.0,0.6619930267333984
translation,69,182,ablation-analysis,ablation analysis,has,error rate reduction,ablation analysis has error rate reduction,0.5106502175331116
translation,69,7,baselines,baselines,has,span selection pre-training ( sspt ),baselines has span selection pre-training ( sspt ),0.5450961589813232
translation,69,5,experiments,bert,pretrained on,two auxiliary tasks,bert pretrained on two auxiliary tasks,0.7116554975509644
translation,69,4,results,results,has,bert ( bidirectional encoder representations from transformers,results has bert ( bidirectional encoder representations from transformers,0.5754363536834717
translation,69,8,results,significant and consistent improvements,over,bert base and bert large,significant and consistent improvements over bert base and bert large,0.7214789986610413
translation,69,8,results,bert base and bert large,on,multiple machine reading comprehension ( mrc ) datasets,bert base and bert large on multiple machine reading comprehension ( mrc ) datasets,0.517876148223877
translation,69,8,results,results,find,significant and consistent improvements,results find significant and consistent improvements,0.5818962454795837
translation,69,10,results,significant impact,in,hotpotqa,significant impact in hotpotqa,0.5499122738838196
translation,69,10,results,significant impact,improving,answer prediction f1,significant impact improving answer prediction f1,0.6920780539512634
translation,69,10,results,answer prediction f1,by,4 points,answer prediction f1 by 4 points,0.5989753603935242
translation,69,10,results,answer prediction f1,by,1 point,answer prediction f1 by 1 point,0.593727171421051
translation,69,10,results,f1,by,1 point,f1 by 1 point,0.6472205519676208
translation,69,10,results,outperforming,has,previous best system,outperforming has previous best system,0.6560267806053162
translation,69,10,results,results,show,significant impact,results show significant impact,0.6774851083755493
translation,69,179,results,improved substantially,with,span selection pre-training,improved substantially with span selection pre-training,0.6153542995452881
translation,69,179,results,results,has,four question answering datasets,results has four question answering datasets,0.5366081595420837
translation,69,180,results,squad,Relative to,bert base,squad Relative to bert base,0.7373961210250854
translation,69,180,results,squad,find,3 point improvement,squad find 3 point improvement,0.6322535276412964
translation,69,180,results,squad,find,nearly 6 point improvement,squad find nearly 6 point improvement,0.6323027014732361
translation,69,180,results,3 point improvement,in,f1,3 point improvement in f1,0.5549549460411072
translation,69,180,results,3 point improvement,for,squad 1.1,3 point improvement for squad 1.1,0.6179579496383667
translation,69,180,results,nearly 6 point improvement,for,squad 2.0,nearly 6 point improvement for squad 2.0,0.620872974395752
translation,69,180,results,results,has,squad,results has squad,0.5870539546012878
translation,69,181,results,improvement,is,similar,improvement is similar,0.583184540271759
translation,69,181,results,error rate reduction,has,improvement,error rate reduction has improvement,0.5429258346557617
translation,69,181,results,results,In terms of,error rate reduction,results In terms of error rate reduction,0.7021902203559875
translation,69,187,results,improvement,from,sspt,improvement from sspt,0.5649622678756714
translation,69,187,results,sspt,on,squad 1.1 and 2.0,sspt on squad 1.1 and 2.0,0.5666730999946594
translation,69,187,results,squad 1.1 and 2.0,amount of,training data,squad 1.1 and 2.0 amount of training data,0.7043396234512329
translation,69,187,results,training data,has,increases,training data has increases,0.5859135389328003
translation,69,187,results,results,shows,improvement,results shows improvement,0.7450517416000366
translation,69,188,results,significant improvement,at,100 % training,significant improvement at 100 % training,0.5443041920661926
translation,69,188,results,even more pronounced,with,less training data,even more pronounced with less training data,0.6770841479301453
translation,69,193,results,bert large + sspt,provides,1.5 % improvement,bert large + sspt provides 1.5 % improvement,0.6577126383781433
translation,69,193,results,1.5 % improvement,over,best bert - for -qa model performance,1.5 % improvement over best bert - for -qa model performance,0.6601996421813965
translation,69,193,results,results,Our implementation of,bert large + sspt,results Our implementation of bert large + sspt,0.7742177844047546
translation,69,198,results,most substantial gains,of,almost 4 f1 points,most substantial gains of almost 4 f1 points,0.5960859060287476
translation,69,198,results,almost 4 f1 points,for,answer selection,almost 4 f1 points for answer selection,0.6100292801856995
translation,69,198,results,results,find,most substantial gains,results find most substantial gains,0.6206008195877075
translation,69,199,results,improvement,of,almost one point f1,improvement of almost one point f1,0.6082466840744019
translation,69,199,results,results,find,improvement,results find improvement,0.6395144462585449
translation,69,219,results,ten million steps,of,training,ten million steps of training,0.6168441772460938
translation,69,219,results,our data,produces,models,our data produces models,0.6825836896896362
translation,69,219,results,models,give,over one percent better f1,models give over one percent better f1,0.6472129821777344
translation,69,219,results,over one percent better f1,on,squad 1.1 and 2.0,over one percent better f1 on squad 1.1 and 2.0,0.541376531124115
translation,69,219,results,ten million steps,has,our data,ten million steps has our data,0.5707868933677673
translation,69,219,results,training,has,our data,training has our data,0.5488531589508057
translation,69,219,results,results,when moving to,ten million steps,results when moving to ten million steps,0.7421579360961914
translation,70,192,ablation-analysis,larger filter counts,help,test accuracies ( g vs f ),larger filter counts help test accuracies ( g vs f ),0.6734718680381775
translation,70,192,ablation-analysis,larger filter counts,on,test accuracies ( g vs f ),larger filter counts on test accuracies ( g vs f ),0.5261154174804688
translation,70,192,ablation-analysis,test accuracies ( g vs f ),for,convolutional - pooling lstms,test accuracies ( g vs f ) for convolutional - pooling lstms,0.5609117746353149
translation,70,192,ablation-analysis,ablation analysis,has,larger filter counts,ablation analysis has larger filter counts,0.5217537879943848
translation,70,204,ablation-analysis,attention,on,convolutional layer,attention on convolutional layer,0.5388009548187256
translation,70,204,ablation-analysis,attention,gives,over 2 % accuracy improvement,attention gives over 2 % accuracy improvement,0.6199684739112854
translation,70,204,ablation-analysis,over 2 % accuracy improvement,on,both test sets,over 2 % accuracy improvement on both test sets,0.5049851536750793
translation,70,160,baselines,stochastic gradient descent ( sgd ),is,optimization strategy,stochastic gradient descent ( sgd ) is optimization strategy,0.5586421489715576
translation,70,160,baselines,baselines,has,stochastic gradient descent ( sgd ),baselines has stochastic gradient descent ( sgd ),0.5382658243179321
translation,70,176,baselines,metzler-bendersky ir model,employs,weighted combination,metzler-bendersky ir model employs weighted combination,0.5200844407081604
translation,70,176,baselines,weighted combination,of,term-based and term proximity - based features,weighted combination of term-based and term proximity - based features,0.5604986548423767
translation,70,176,baselines,term-based and term proximity - based features,to score,each candidate,term-based and term proximity - based features to score each candidate,0.687049925327301
translation,70,176,baselines,metzler-bendersky ir model,has,model,metzler-bendersky ir model has model,0.536410927772522
translation,70,176,baselines,baselines,has,metzler-bendersky ir model,baselines has metzler-bendersky ir model,0.5233218669891357
translation,70,201,baselines,lstm,in,qa - lstm,lstm in qa - lstm,0.5610049366950989
translation,70,201,baselines,lstm,with,convolutional layer,lstm with convolutional layer,0.6220212578773499
translation,70,154,experimental-setup,proposed models,implemented with,"theano ( bastien et al. , 2012 )","proposed models implemented with theano ( bastien et al. , 2012 )",0.6889969110488892
translation,70,154,experimental-setup,experimental setup,implemented with,"theano ( bastien et al. , 2012 )","experimental setup implemented with theano ( bastien et al. , 2012 )",0.65673828125
translation,70,154,experimental-setup,experimental setup,has,proposed models,experimental setup has proposed models,0.5434987545013428
translation,70,156,experimental-setup,word embeddings,using,"word2vec ( mikolov et al. , 2013 )","word embeddings using word2vec ( mikolov et al. , 2013 )",0.5264298319816589
translation,70,156,experimental-setup,experimental setup,has,word embeddings,experimental setup has word embeddings,0.4932878315448761
translation,70,157,experimental-setup,training data,for,word embeddings,training data for word embeddings,0.5421565771102905
translation,70,157,experimental-setup,word embeddings,is,wikipedia cor-pus,word embeddings is wikipedia cor-pus,0.5170316100120544
translation,70,157,experimental-setup,wikipedia cor-pus,of,164 million tokens,wikipedia cor-pus of 164 million tokens,0.5528369545936584
translation,70,157,experimental-setup,wikipedia cor-pus,combined with,questions and answers,wikipedia cor-pus combined with questions and answers,0.6800597310066223
translation,70,157,experimental-setup,questions and answers,in,insuranceqa training set,questions and answers in insuranceqa training set,0.48691338300704956
translation,70,157,experimental-setup,experimental setup,has,training data,experimental setup has training data,0.5218086242675781
translation,70,158,experimental-setup,word vector size,set to,100,word vector size set to 100,0.7470430135726929
translation,70,158,experimental-setup,experimental setup,has,word vector size,experimental setup has word vector size,0.517385721206665
translation,70,159,experimental-setup,word embeddings,part of,parameters,word embeddings part of parameters,0.5775914788246155
translation,70,159,experimental-setup,word embeddings,optimized during,training,word embeddings optimized during training,0.706823468208313
translation,70,159,experimental-setup,experimental setup,has,word embeddings,experimental setup has word embeddings,0.4932878315448761
translation,70,161,experimental-setup,learning rate,is,1.1,learning rate is 1.1,0.5556809306144714
translation,70,161,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,70,163,experimental-setup,different margins,in,hing loss function,different margins in hing loss function,0.5037272572517395
translation,70,163,experimental-setup,different margins,fixed,margin,different margins fixed margin,0.8149898052215576
translation,70,163,experimental-setup,margin,as,m = 0.2,margin as m = 0.2,0.6015101671218872
translation,70,163,experimental-setup,experimental setup,tried,different margins,experimental setup tried different margins,0.6792210340499878
translation,70,163,experimental-setup,experimental setup,fixed,margin,experimental setup fixed margin,0.6164843440055847
translation,70,164,experimental-setup,our models,in,mini-batches,our models in mini-batches,0.5609138607978821
translation,70,164,experimental-setup,mini-batches,with,batch size,mini-batches with batch size,0.6655074954032898
translation,70,164,experimental-setup,batch size,as,20,batch size as 20,0.5993613600730896
translation,70,164,experimental-setup,maximum length l,of,questions and answers,maximum length l of questions and answers,0.5574639439582825
translation,70,164,experimental-setup,maximum length l,is,200,maximum length l is 200,0.6220186948776245
translation,70,164,experimental-setup,questions and answers,is,200,questions and answers is 200,0.5809327960014343
translation,70,164,experimental-setup,experimental setup,train,our models,experimental setup train our models,0.6514129638671875
translation,70,169,experimental-setup,convolution - based lstm,tune,bilstm hidden vector size,convolution - based lstm tune bilstm hidden vector size,0.673505425453186
translation,70,169,experimental-setup,cnn output,as,282 dimensions,cnn output as 282 dimensions,0.532745897769928
translation,70,169,experimental-setup,experimental setup,For,convolution - based lstm,experimental setup For convolution - based lstm,0.5734291672706604
translation,70,202,experimental-setup,window size k = 3,according to,validation accuracy,window size k = 3 according to validation accuracy,0.6011848449707031
translation,70,202,experimental-setup,experimental setup,set,filter size c,experimental setup set filter size c,0.6693342924118042
translation,70,202,experimental-setup,experimental setup,set,window size k = 3,experimental setup set window size k = 3,0.6755421161651611
translation,70,229,experimental-setup,models,from,epoch,models from epoch,0.5775569677352905
translation,70,229,experimental-setup,models,with,best map,models with best map,0.5810520052909851
translation,70,229,experimental-setup,epoch,with,best map,epoch with best map,0.6148934960365295
translation,70,229,experimental-setup,best map,on,validation set,best map on validation set,0.560073971748352
translation,70,229,experimental-setup,experimental setup,use,models,experimental setup use models,0.6199734807014465
translation,70,213,experiments,cosine similarity,in,models,cosine similarity in models,0.532177746295929
translation,70,213,experiments,gesd,has,outperforms,gesd has outperforms,0.6515345573425293
translation,70,213,experiments,outperforms,has,cosine similarity,outperforms has cosine similarity,0.6024648547172546
translation,70,237,experiments,attentive lstm,is,best proposed model,attentive lstm is best proposed model,0.5799747705459595
translation,70,237,experiments,best proposed model,by,2.2 %,best proposed model by 2.2 %,0.5484492182731628
translation,70,237,experiments,outperforms,by,2.2 %,outperforms by 2.2 %,0.6213132739067078
translation,70,237,experiments,"best baseline ( severyn and moschitti , 2015 )",by,0.7 %,"best baseline ( severyn and moschitti , 2015 ) by 0.7 %",0.5127683281898499
translation,70,237,experiments,"best baseline ( severyn and moschitti , 2015 )",by,2.2 %,"best baseline ( severyn and moschitti , 2015 ) by 2.2 %",0.5177999138832092
translation,70,237,experiments,"best baseline ( severyn and moschitti , 2015 )",on,mrr,"best baseline ( severyn and moschitti , 2015 ) on mrr",0.47015291452407837
translation,70,237,experiments,0.7 %,on,map,0.7 % on map,0.6202843189239502
translation,70,237,experiments,2.2 %,on,mrr,2.2 % on mrr,0.5859264135360718
translation,70,237,experiments,outperforms,has,"best baseline ( severyn and moschitti , 2015 )","outperforms has best baseline ( severyn and moschitti , 2015 )",0.5541596412658691
translation,70,5,model,series of deep learning models,to address,passage answer selection,series of deep learning models to address passage answer selection,0.5616358518600464
translation,70,5,model,model,propose,series of deep learning models,model propose series of deep learning models,0.6299445629119873
translation,70,6,model,passage answers,to,questions,passage answers to questions,0.553650438785553
translation,70,6,model,passage answers,accommodating,complex semantic relations,passage answers accommodating complex semantic relations,0.6131817698478699
translation,70,6,model,hybrid models,that process,text,hybrid models that process text,0.6947712898254395
translation,70,6,model,text,using,convolutional and recurrent neural networks,text using convolutional and recurrent neural networks,0.6733852624893188
translation,70,6,model,model,To match,passage answers,model To match passage answers,0.7636502385139465
translation,70,6,model,model,develop,hybrid models,model develop hybrid models,0.639868438243866
translation,70,7,model,simple but effective attention mechanism,purpose of constructing,better answer representations,simple but effective attention mechanism purpose of constructing better answer representations,0.5765922665596008
translation,70,7,model,better answer representations,according to,input question,better answer representations according to input question,0.6451084613800049
translation,70,7,model,model,develop,simple but effective attention mechanism,model develop simple but effective attention mechanism,0.6316993832588196
translation,70,43,model,model,propose,series of deep learning models,model propose series of deep learning models,0.6299445629119873
translation,70,45,model,useful and irrelevant pieces,presented in,questions and answers,useful and irrelevant pieces presented in questions and answers,0.6207351684570312
translation,70,45,model,model,propose,two independent models,model propose two independent models,0.6685448884963989
translation,70,46,model,independence assumption,of,question and answer embedding,independence assumption of question and answer embedding,0.6002972722053528
translation,70,46,model,independence assumption,introduce,effective attention mechanism,independence assumption introduce effective attention mechanism,0.6152573823928833
translation,70,46,model,effective attention mechanism,to generate,answer representations,effective attention mechanism to generate answer representations,0.6906909942626953
translation,70,46,model,answer representations,according to,question,answer representations according to question,0.7468370199203491
translation,70,46,model,model,by breaking,independence assumption,model by breaking independence assumption,0.6962074637413025
translation,70,46,model,model,introduce,effective attention mechanism,model introduce effective attention mechanism,0.6006297469139099
translation,70,114,model,higher level,build,bidirectional lstms,higher level build bidirectional lstms,0.6560339331626892
translation,70,114,model,bidirectional lstms,extract,long range dependency,bidirectional lstms extract long range dependency,0.6241716742515564
translation,70,114,model,long range dependency,based on,convoluted n-gram,long range dependency based on convoluted n-gram,0.6332765221595764
translation,70,129,model,simple attention model,for,answer vector generation,simple attention model for answer vector generation,0.575276255607605
translation,70,129,model,simple attention model,by dynamically aligning,more informative parts,simple attention model by dynamically aligning more informative parts,0.6782329678535461
translation,70,129,model,more informative parts,of,answers,more informative parts of answers,0.5977898836135864
translation,70,129,model,more informative parts,to,questions,more informative parts to questions,0.5974304676055908
translation,70,129,model,answers,to,questions,answers to questions,0.5806413292884827
translation,70,129,model,model,developing,simple attention model,model developing simple attention model,0.688426673412323
translation,70,130,model,very simple but efficient word-level attention,on,basic model,very simple but efficient word-level attention on basic model,0.5129868388175964
translation,70,130,model,model,develop,very simple but efficient word-level attention,model develop very simple but efficient word-level attention,0.5896022319793701
translation,70,177,model,cnn model,employed to learn,distributed representations of questions and answers,cnn model employed to learn distributed representations of questions and answers,0.7440611124038696
translation,70,230,model,decayed learning rate,to stablize,models ' training,decayed learning rate to stablize models ' training,0.730167031288147
translation,70,162,results,best performances,when,negative answer count k = 50,best performances when negative answer count k = 50,0.6201515793800354
translation,70,162,results,results,get,best performances,results get best performances,0.5833312273025513
translation,70,186,results,last vectors,from,both directions,last vectors from both directions,0.5503751039505005
translation,70,186,results,last vectors,performs,worst,last vectors performs worst,0.6654536724090576
translation,70,187,results,max-pooling,),much better,max-pooling ) much better,0.5246853828430176
translation,70,187,results,max-pooling,is,much better,max-pooling is much better,0.5229450464248657
translation,70,187,results,much better,than,average pooling,much better than average pooling,0.5915953516960144
translation,70,194,results,outperform,by about,1.0 %,outperform by about 1.0 %,0.6802162528038025
translation,70,194,results,outperform,by about,0.7 %,outperform by about 0.7 %,0.6771935820579529
translation,70,194,results,plain qa - lstm ( d ),by about,1.0 %,plain qa - lstm ( d ) by about 1.0 %,0.6045116782188416
translation,70,194,results,1.0 %,on,test1,1.0 % on test1,0.5816513299942017
translation,70,194,results,0.7 %,on,test2,0.7 % on test2,0.5874007344245911
translation,70,194,results,both convolutional models,has,outperform,both convolutional models has outperform,0.6052461862564087
translation,70,194,results,outperform,has,plain qa - lstm ( d ),outperform has plain qa - lstm ( d ),0.6092153787612915
translation,70,194,results,results,has,both convolutional models,results has both convolutional models,0.4960126280784607
translation,70,196,results,max-pooling,better than,avg-pooling,max-pooling better than avg-pooling,0.7166334390640259
translation,70,196,results,results,observe that,max-pooling,results observe that max-pooling,0.5716196894645691
translation,70,197,results,model,shows,over 2 % improvement,model shows over 2 % improvement,0.7185781002044678
translation,70,197,results,model,shows,over 2 % improvement,model shows over 2 % improvement,0.7185781002044678
translation,70,197,results,over 2 % improvement,on,validation and test1 sets,over 2 % improvement on validation and test1 sets,0.5605813264846802
translation,70,197,results,model,has,model,model has model,0.5623406171798706
translation,70,198,results,improvements,over,best baseline,improvements over best baseline,0.6602689623832703
translation,70,198,results,best baseline,by,"3.5 % , 3.7 % and 3.8 %","best baseline by 3.5 % , 3.7 % and 3.8 %",0.5274426341056824
translation,70,198,results,"3.5 % , 3.7 % and 3.8 %",on,"validation , test1 and test2 sets","3.5 % , 3.7 % and 3.8 % on validation , test1 and test2 sets",0.5397269129753113
translation,70,198,results,results,gets,improvements,results gets improvements,0.7051523923873901
translation,70,235,results,qa - lstm ( b ),improves,map and mrr,qa - lstm ( b ) improves map and mrr,0.6810254454612732
translation,70,235,results,map and mrr,in,more than 1 %,map and mrr in more than 1 %,0.5539268851280212
translation,70,235,results,map and mrr,compared to,qa - cnn ( a ),map and mrr compared to qa - cnn ( a ),0.6806362271308899
translation,70,235,results,more than 1 %,compared to,qa - cnn ( a ),more than 1 % compared to qa - cnn ( a ),0.7201299667358398
translation,70,235,results,results,has,qa - lstm ( b ),results has qa - lstm ( b ),0.5695163011550903
translation,70,236,results,convolutionalpooling,performs,better,convolutionalpooling performs better,0.6338405609130859
translation,70,236,results,better,on,map,better on map,0.5868803262710571
translation,70,236,results,better,on,map,better on map,0.5868803262710571
translation,70,236,results,map,by,0.9 %,map by 0.9 %,0.6031051278114319
translation,70,236,results,map,by,0.4 %,map by 0.4 %,0.6156421303749084
translation,70,236,results,map,by,0.4 %,map by 0.4 %,0.6156421303749084
translation,70,236,results,convolution - based models,on,map,convolution - based models on map,0.5972181558609009
translation,70,236,results,map,by,0.4 %,map by 0.4 %,0.6156421303749084
translation,70,236,results,mrr,by,0.8 %,mrr by 0.8 %,0.5611239671707153
translation,70,239,results,attentive lstm model,achieves,higher performance,attentive lstm model achieves higher performance,0.6888436675071716
translation,70,239,results,higher performance,without using,any human-defined features,higher performance without using any human-defined features,0.7479394674301147
translation,70,239,results,results,has,attentive lstm model,results has attentive lstm model,0.5319560766220093
translation,71,34,experiments,"wikimovies ( miller et al. , 2016 )",construct,datasets,"wikimovies ( miller et al. , 2016 ) construct datasets",0.6854124069213867
translation,71,34,experiments,"webquestionssp ( yih et al. , 2016 )",construct,datasets,"webquestionssp ( yih et al. , 2016 ) construct datasets",0.6352759599685669
translation,71,34,experiments,datasets,with,varying amount of training supervision and kb completeness,datasets with varying amount of training supervision and kb completeness,0.5882194638252258
translation,71,34,experiments,datasets,varying degree of,question complexity,datasets varying degree of question complexity,0.7213660478591919
translation,71,7,model,question -specific subgraph,containing,text and kb entities and relations,question -specific subgraph containing text and kb entities and relations,0.6235108375549316
translation,71,7,model,model,propose,graft - net,model propose graft - net,0.6740627288818359
translation,71,27,model,early fusion,allows,more flexibility,early fusion allows more flexibility,0.747204065322876
translation,71,27,model,more flexibility,in combining,information,more flexibility in combining information,0.6797971725463867
translation,71,27,model,information,from,multiple sources,information from multiple sources,0.5527356863021851
translation,71,27,model,model,has,early fusion,model has early fusion,0.555495023727417
translation,71,28,model,early fusion,propose,novel graph convolution based neural network,early fusion propose novel graph convolution based neural network,0.6361428499221802
translation,71,28,model,novel graph convolution based neural network,called,graft - net ( graphs of relations among facts and text networks ),novel graph convolution based neural network called graft - net ( graphs of relations among facts and text networks ),0.6347786784172058
translation,71,28,model,novel graph convolution based neural network,designed to operate over,heterogeneous graphs of kb facts and text sentences,novel graph convolution based neural network designed to operate over heterogeneous graphs of kb facts and text sentences,0.6583946943283081
translation,71,28,model,model,To enable,early fusion,model To enable early fusion,0.7251221537590027
translation,71,28,model,model,propose,novel graph convolution based neural network,model propose novel graph convolution based neural network,0.6341758966445923
translation,71,30,model,heterogeneous update rules,that handle,kb nodes,heterogeneous update rules that handle kb nodes,0.6635814905166626
translation,71,30,model,kb nodes,differently from,text nodes,kb nodes differently from text nodes,0.6572025418281555
translation,71,30,model,kb nodes,differently from,text nodes,kb nodes differently from text nodes,0.6572025418281555
translation,71,30,model,information,into and out of,text nodes,information into and out of text nodes,0.5899017453193665
translation,71,30,model,propagate,has,information,propagate has information,0.6195996999740601
translation,71,30,model,model,propose,heterogeneous update rules,model propose heterogeneous update rules,0.6822707653045654
translation,71,31,model,directed propagation method,inspired by,personalized pagerank,directed propagation method inspired by personalized pagerank,0.6637505292892456
translation,71,31,model,directed propagation method,constrains,propagation,directed propagation method constrains propagation,0.7615556120872498
translation,71,31,model,propagation,of,embeddings,propagation of embeddings,0.595753014087677
translation,71,31,model,embeddings,in,graph,embeddings in graph,0.5476230382919312
translation,71,31,model,embeddings,to follow,paths,embeddings to follow paths,0.7062192559242249
translation,71,31,model,paths,starting from,seed nodes,paths starting from seed nodes,0.7061372399330139
translation,71,31,model,seed nodes,linked to,question,seed nodes linked to question,0.7390483021736145
translation,71,31,model,model,introduce,directed propagation method,model introduce directed propagation method,0.655342698097229
translation,72,54,baselines,baselines,has,"dcn + ( xiong et al. , 2018 )","baselines has dcn + ( xiong et al. , 2018 )",0.5093722343444824
translation,72,107,baselines,gnr,has,fusionnet,gnr has fusionnet,0.6315312385559082
translation,72,110,experiments,top 4,achieves,92.5 accuracy,top 4 achieves 92.5 accuracy,0.6397734880447388
translation,72,110,experiments,dyn,achieves,94.6 accuracy,dyn achieves 94.6 accuracy,0.6756853461265564
translation,72,110,experiments,94.6 accuracy,with,3.9 sentences per example,94.6 accuracy with 3.9 sentences per example,0.602692186832428
translation,72,110,experiments,newsqa,has,top 4,newsqa has top 4,0.6267532110214233
translation,72,115,experiments,squad,achieves,speedup,squad achieves speedup,0.7103956937789917
translation,72,115,experiments,squad,achieves,speedup,squad achieves speedup,0.7103956937789917
translation,72,115,experiments,squad,on,newsqa,squad on newsqa,0.6149151921272278
translation,72,115,experiments,s- reader,on,squad,s- reader on squad,0.6319518089294434
translation,72,115,experiments,s- reader,on,newsqa,s- reader on newsqa,0.6245002150535583
translation,72,115,experiments,s- reader,on,newsqa,s- reader on newsqa,0.6245002150535583
translation,72,115,experiments,6.7 ? training,on,newsqa,6.7 ? training on newsqa,0.548000693321228
translation,72,115,experiments,speedup,on,squad,speedup on squad,0.5624490976333618
translation,72,115,experiments,speedup,on,15.0 ? training,speedup on 15.0 ? training,0.5712242722511292
translation,72,115,experiments,speedup,on,newsqa,speedup on newsqa,0.5905935764312744
translation,72,115,experiments,speedup,on,newsqa,speedup on newsqa,0.5905935764312744
translation,72,115,experiments,speedup,on,newsqa,speedup on newsqa,0.5905935764312744
translation,72,115,experiments,squad,has,s- reader,squad has s- reader,0.6644879579544067
translation,72,17,model,qa system,scalable to,large documents,qa system scalable to large documents,0.7675489783287048
translation,72,17,model,qa system,robust to,adversarial inputs,qa system robust to adversarial inputs,0.7536596655845642
translation,72,17,model,model,develop,qa system,model develop qa system,0.7069550156593323
translation,72,18,model,context,required to,answer,context required to answer,0.6976786255836487
translation,72,18,model,context,by sampling,examples,context by sampling examples,0.7444401979446411
translation,72,18,model,examples,in,dataset,examples in dataset,0.4888726770877838
translation,72,18,model,answer,has,question,answer has question,0.6225500106811523
translation,72,18,model,model,study,context,model study context,0.740865170955658
translation,72,21,model,sentence selector,to select,minimal set of sentences,sentence selector to select minimal set of sentences,0.7288981080055237
translation,72,21,model,minimal set of sentences,to give,qa model,minimal set of sentences to give qa model,0.6554585099220276
translation,72,21,model,qa model,to answer,question,qa model to answer question,0.720697283744812
translation,72,21,model,model,propose,sentence selector,model propose sentence selector,0.6437196731567383
translation,72,23,model,three simple techniques,has,weight transfer,three simple techniques has weight transfer,0.5790700316429138
translation,72,23,model,three simple techniques,has,data modification,three simple techniques has data modification,0.5795817971229553
translation,72,23,model,three simple techniques,has,score normalization,three simple techniques has score normalization,0.5555513501167297
translation,72,51,model,reduced set of sentences,with,high selection scores,reduced set of sentences with high selection scores,0.6284913420677185
translation,72,51,model,high selection scores,to answer,question,high selection scores to answer question,0.6929397583007812
translation,72,51,model,qa model,has,reduced set of sentences,qa model has reduced set of sentences,0.5703428983688354
translation,72,51,model,model,give,qa model,model give qa model,0.6976341605186462
translation,72,22,results,different number of sentences,for,each question,different number of sentences for each question,0.5962134003639221
translation,72,38,results,model,achieves,83.1 f1,model achieves 83.1 f1,0.6298996806144714
translation,72,38,results,model,achieves,85.1 f1,model achieves 85.1 f1,0.6342265009880066
translation,72,38,results,83.1 f1,trained and evaluated using,full document,83.1 f1 trained and evaluated using full document,0.7021984457969666
translation,72,38,results,85.1 f1,trained and evaluated using,oracle sentence,85.1 f1 trained and evaluated using oracle sentence,0.7455787062644958
translation,72,38,results,results,has,model,results has model,0.5339115858078003
translation,72,71,results,substantial improvements,in,sentence selection accuracy,substantial improvements in sentence selection accuracy,0.4975093901157379
translation,72,102,results,tf - idf method and the previous state - of - the - art,by,large margin,tf - idf method and the previous state - of - the - art by large margin,0.5537393093109131
translation,72,102,results,large margin,up to,2.9 % map ),large margin up to 2.9 % map ),0.646693766117096
translation,72,102,results,selector,has,outperforms,selector has outperforms,0.6477119326591492
translation,72,102,results,outperforms,has,tf - idf method and the previous state - of - the - art,outperforms has tf - idf method and the previous state - of - the - art,0.5314822196960449
translation,72,102,results,results,has,selector,results has selector,0.56076979637146
translation,72,103,results,three training techniques,improve,performance,three training techniques improve performance,0.6854690909385681
translation,72,103,results,performance,by,up to 5.6 % map,performance by up to 5.6 % map,0.6108065843582153
translation,72,103,results,three training techniques,has,weight transfer,three training techniques has weight transfer,0.5467478632926941
translation,72,103,results,three training techniques,has,data modification,three training techniques has data modification,0.5518772006034851
translation,72,103,results,three training techniques,has,score normalization,three training techniques has score normalization,0.5444498658180237
translation,72,103,results,results,has,three training techniques,results has three training techniques,0.5119116902351379
translation,72,104,results,dyn method,achieves,higher accuracy,dyn method achieves higher accuracy,0.6685649156570435
translation,72,104,results,higher accuracy,with,less sentences,higher accuracy with less sentences,0.6026378273963928
translation,72,104,results,less sentences,than,top k method,less sentences than top k method,0.5150979161262512
translation,72,104,results,results,has,dyn method,results has dyn method,0.5283619165420532
translation,72,116,results,minimal,achieves,comparable result,minimal achieves comparable result,0.7029327750205994
translation,72,116,results,comparable result,to,full,comparable result to full,0.5996060967445374
translation,72,116,results,comparable result,63.8 vs,63.2 f1,comparable result 63.8 vs 63.2 f1,0.7316240072250366
translation,72,116,results,full,using,s-reader,full using s-reader,0.7169642448425293
translation,72,116,results,79.9,63.8 vs,63.2 f1,79.9 63.8 vs 63.2 f1,0.8127996325492859
translation,72,116,results,vs 79.8 f1,on,squad,vs 79.8 f1 on squad,0.5905228853225708
translation,72,116,results,63.2 f1,on,newsqa,63.2 f1 on newsqa,0.5454047322273254
translation,72,116,results,79.9,has,vs 79.8 f1,79.9 has vs 79.8 f1,0.5607900023460388
translation,72,165,results,results,on,triviaqa,results on triviaqa,0.5297465920448303
translation,72,166,results,mini - mal,obtains,higher f1 and em,mini - mal obtains higher f1 and em,0.6269926428794861
translation,72,166,results,higher f1 and em,over,full,higher f1 and em over full,0.725730299949646
translation,72,166,results,results,has,mini - mal,results has mini - mal,0.5384978652000427
translation,72,167,results,model,with,our sentence selector,model with our sentence selector,0.6082575917243958
translation,72,167,results,model,with,tf - idf selector,model with tf - idf selector,0.6186088919639587
translation,72,167,results,model,with,tf - idf selector,model with tf - idf selector,0.6186088919639587
translation,72,167,results,model,with,tf - idf selector,model with tf - idf selector,0.6186088919639587
translation,72,167,results,our sentence selector,with,dyn,our sentence selector with dyn,0.6888209581375122
translation,72,167,results,our sentence selector,achieves,higher f1 and em,our sentence selector achieves higher f1 and em,0.6868161559104919
translation,72,167,results,higher f1 and em,over,model,higher f1 and em over model,0.7109254002571106
translation,72,167,results,model,with,tf - idf selector,model with tf - idf selector,0.6186088919639587
translation,72,167,results,results,has,model,results has model,0.5339115858078003
translation,72,169,results,published state - of - the - art,on,both dataset,published state - of - the - art on both dataset,0.5056847333908081
translation,72,169,results,outperforms,has,published state - of - the - art,outperforms has published state - of - the - art,0.5705595016479492
translation,72,169,results,results,has,outperforms,results has outperforms,0.6657275557518005
translation,72,174,results,outperforms,achieving,new state - of - the - art,outperforms achieving new state - of - the - art,0.6076083183288574
translation,72,174,results,new state - of - the - art,by,large margin,new state - of - the - art by large margin,0.5389018058776855
translation,72,174,results,+ 11.1 and + 11.5 f1,on,addsent and addonesent,+ 11.1 and + 11.5 f1 on addsent and addonesent,0.5785481333732605
translation,72,174,results,minimal,has,outperforms,minimal has outperforms,0.6998793482780457
translation,72,174,results,outperforms,has,full,outperforms has full,0.6539738178253174
translation,72,174,results,large margin,has,+ 11.1 and + 11.5 f1,large margin has + 11.1 and + 11.5 f1,0.5709084868431091
translation,73,199,ablation-analysis,regularized model and the s&f model,increase,fraction,regularized model and the s&f model increase fraction,0.6739516854286194
translation,73,199,ablation-analysis,fraction,by,10.79 % and 9.17 %,fraction by 10.79 % and 9.17 %,0.6136497259140015
translation,73,199,ablation-analysis,fraction,compared to,qi-2019,fraction compared to qi-2019,0.6949748992919922
translation,73,199,ablation-analysis,ablation analysis,has,regularized model and the s&f model,ablation analysis has regularized model and the s&f model,0.5331507921218872
translation,73,207,ablation-analysis,participants,overestimate,model accuracy,participants overestimate model accuracy,0.7789719700813293
translation,73,207,ablation-analysis,model accuracy,of,qi-2019,model accuracy of qi-2019,0.6090164184570312
translation,73,207,ablation-analysis,model accuracy,by,2.93 %,model accuracy by 2.93 %,0.567597508430481
translation,73,207,ablation-analysis,qi-2019,by,2.93 %,qi-2019 by 2.93 %,0.5727590322494507
translation,73,207,ablation-analysis,regularized model,leads to,0.87 % overestimation,regularized model leads to 0.87 % overestimation,0.6164368987083435
translation,73,207,ablation-analysis,ablation analysis,has,participants,ablation analysis has participants,0.4681606590747833
translation,73,208,ablation-analysis,s&f model,underestimated by,6.40 %,s&f model underestimated by 6.40 %,0.7100021243095398
translation,73,208,ablation-analysis,ablation analysis,has,s&f model,ablation analysis has s&f model,0.5355889797210693
translation,73,34,model,hierarchical neural network architecture,for,xqa,hierarchical neural network architecture for xqa,0.6443539261817932
translation,73,34,model,explanation,used to predict,answer,explanation used to predict answer,0.7530946135520935
translation,73,34,model,regularization term,for,loss function,regularization term for loss function,0.4894113540649414
translation,73,34,model,loss function,explicitly couples,answer and explanation prediction,loss function explicitly couples answer and explanation prediction,0.7849029898643494
translation,73,34,model,answer and explanation prediction,during,training,answer and explanation prediction during training,0.6978285312652588
translation,73,34,model,model,propose,two novel approaches,model propose two novel approaches,0.7303816080093384
translation,74,100,experiments,bertadam optimizer,with,batch size,bertadam optimizer with batch size,0.6315703988075256
translation,74,100,experiments,bertadam optimizer,with,initial learning rate,bertadam optimizer with initial learning rate,0.6274476051330566
translation,74,100,experiments,batch size,of,6,batch size of 6,0.6956941485404968
translation,74,100,experiments,initial learning rate,of,0.0003,initial learning rate of 0.0003,0.5776545405387878
translation,74,42,hyperparameters,input embeddings,used,"bert pretrained models ( devlin et al. , 2018 )","input embeddings used bert pretrained models ( devlin et al. , 2018 )",0.532450258731842
translation,74,42,hyperparameters,"bert pretrained models ( devlin et al. , 2018 )",based on,word piece level tokenization,"bert pretrained models ( devlin et al. , 2018 ) based on word piece level tokenization",0.5979478359222412
translation,74,42,hyperparameters,hyperparameters,For,input embeddings,hyperparameters For input embeddings,0.49551722407341003
translation,74,101,hyperparameters,dropout rate,of,0.2,dropout rate of 0.2,0.5832480192184448
translation,74,101,hyperparameters,0.2,used for,all lstm layers,0.2 used for all lstm layers,0.6052280068397522
translation,74,101,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,74,8,model,attention and fusion,conducted,horizontally and vertically,attention and fusion conducted horizontally and vertically,0.648196816444397
translation,74,8,model,horizontally and vertically,across,layers,horizontally and vertically across layers,0.7596397399902344
translation,74,8,model,layers,at,different levels of granularity,layers at different levels of granularity,0.5622172355651855
translation,74,8,model,different levels of granularity,between,question and paragraph,different levels of granularity between question and paragraph,0.6613253951072693
translation,74,8,model,model,has,attention and fusion,model has attention and fusion,0.5979035496711731
translation,74,29,model,bert,makes use of,"transformer ( vaswani et al. , 2017 )","bert makes use of transformer ( vaswani et al. , 2017 )",0.7058384418487549
translation,74,29,model,"transformer ( vaswani et al. , 2017 )",is,attention mechanism,"transformer ( vaswani et al. , 2017 ) is attention mechanism",0.5187332034111023
translation,74,29,model,attention mechanism,learns,contextual relations,attention mechanism learns contextual relations,0.6618238091468811
translation,74,29,model,contextual relations,between,words ( or sub-words ),contextual relations between words ( or sub-words ),0.6150382161140442
translation,74,29,model,words ( or sub-words ),in,text,words ( or sub-words ) in text,0.5165761113166809
translation,74,29,model,model,has,bert,model has bert,0.6085957288742065
translation,74,30,model,bert,uses,encoder mechanism,bert uses encoder mechanism,0.6771182417869568
translation,74,30,model,encoder mechanism,from,trans-former,encoder mechanism from trans-former,0.6189925670623779
translation,74,30,model,model,has,bert,model has bert,0.6085957288742065
translation,74,108,results,proposed model,achieved,average em,proposed model achieved average em,0.751344621181488
translation,74,108,results,proposed model,achieved,average f1,proposed model achieved average f1,0.7356877326965332
translation,74,108,results,average em,of,41.45,average em of 41.45,0.5852793455123901
translation,74,108,results,average f1,of,56.07,average f1 of 56.07,0.5370979905128479
translation,74,108,results,56.07,on,all datasets ( development and test sets combined ),56.07 on all datasets ( development and test sets combined ),0.48418349027633667
translation,74,108,results,results,has,proposed model,results has proposed model,0.5938616394996643
translation,74,109,results,average f1,obtained for,development datasets,average f1 obtained for development datasets,0.6607680320739746
translation,74,109,results,average f1,obtained for,test datasets,average f1 obtained for test datasets,0.6308668851852417
translation,74,109,results,development datasets,is,50.45,development datasets is 50.45,0.5574769377708435
translation,74,109,results,average f1,obtained for,test datasets,average f1 obtained for test datasets,0.6308668851852417
translation,74,109,results,test datasets,is,61.69,test datasets is 61.69,0.5650323629379272
translation,74,109,results,results,has,average f1,results has average f1,0.5551680326461792
translation,74,109,results,results,has,average f1,results has average f1,0.5551680326461792
translation,75,7,model,end-to - end vqa,into,two steps,end-to - end vqa into two steps,0.6112834811210632
translation,75,7,model,explaining and reasoning,towards,more explainable vqa,explaining and reasoning towards more explainable vqa,0.6852362751960754
translation,75,7,model,model,break up,end-to - end vqa,model break up end-to - end vqa,0.7763226628303528
translation,75,51,results,current system,achieves,comparable performance,current system achieves comparable performance,0.6875393390655518
translation,75,51,results,comparable performance,to,baselines,comparable performance to baselines,0.5383189916610718
translation,75,51,results,improve,with,explanation quality,improve with explanation quality,0.5947521924972534
translation,75,51,results,results,has,current system,results has current system,0.5962529182434082
translation,76,238,baselines,some retrieved sentences,to generate,answers,some retrieved sentences to generate answers,0.670672595500946
translation,76,238,baselines,some retrieved sentences,to generate,answers,some retrieved sentences to generate answers,0.670672595500946
translation,76,238,baselines,answers,based on,graph- based algorithm,answers based on graph- based algorithm,0.6983675956726074
translation,76,238,baselines,answers,by re-ranking,retrieved sentences,answers by re-ranking retrieved sentences,0.7283626198768616
translation,76,238,baselines,answers,by re-ranking,retrieved sentences,answers by re-ranking retrieved sentences,0.7283626198768616
translation,76,4,model,appropriate answers,has,for opinion questions about products,appropriate answers has for opinion questions about products,0.5925559401512146
translation,76,4,model,model,generate,appropriate answers,model generate appropriate answers,0.7447623610496521
translation,76,5,model,hierarchy,organizes,product aspects,hierarchy organizes product aspects,0.6589324474334717
translation,76,5,model,product aspects,as,nodes,product aspects as nodes,0.5207507610321045
translation,76,5,model,nodes,following,parent-child relations,nodes following parent-child relations,0.6823081374168396
translation,76,5,model,model,has,hierarchy,model has hierarchy,0.5793489217758179
translation,76,90,model,( explicit / implicit ) aspects,adopt,hierarchical classification technique,( explicit / implicit ) aspects adopt hierarchical classification technique,0.687658965587616
translation,76,90,model,model,to simultaneously identify,( explicit / implicit ) aspects,model to simultaneously identify ( explicit / implicit ) aspects,0.7468110918998718
translation,76,91,model,associations,between,aspects and sentiment terms,associations between aspects and sentiment terms,0.6432995200157166
translation,76,91,model,aspects and sentiment terms,by,multiple classifiers,aspects and sentiment terms by multiple classifiers,0.5503420829772949
translation,76,91,model,model,discovers,associations,model discovers associations,0.7420740127563477
translation,76,123,model,multiple criteria,in,answer generation process,multiple criteria in answer generation process,0.5163467526435852
translation,76,123,model,multiple criteria,including,answer salience,multiple criteria including answer salience,0.732324481010437
translation,76,123,model,multiple criteria,including,coherence,multiple criteria including coherence,0.6699538826942444
translation,76,123,model,multiple criteria,including,diversity,multiple criteria including diversity,0.6664472818374634
translation,76,123,model,model,incorporate,multiple criteria,model incorporate multiple criteria,0.7434682250022888
translation,76,311,model,new product opinion - qa framework,exploits,hierarchical organization,new product opinion - qa framework exploits hierarchical organization,0.7077999114990234
translation,76,311,model,hierarchical organization,of,consumer reviews,hierarchical organization of consumer reviews,0.6121289730072021
translation,76,311,model,consumer reviews,on,products,consumer reviews on products,0.5482333302497864
translation,76,311,model,model,developed,new product opinion - qa framework,model developed new product opinion - qa framework,0.6711857914924622
translation,76,312,model,our framework,accurately identify,aspects,our framework accurately identify aspects,0.6361100673675537
translation,76,312,model,our framework,discover,subaspects,our framework discover subaspects,0.6808111071586609
translation,76,312,model,aspects,asked in,questions,aspects asked in questions,0.8070269823074341
translation,76,312,model,hierarchical organization,has,our framework,hierarchical organization has our framework,0.5865846276283264
translation,76,312,model,model,With the help of,hierarchical organization,model With the help of hierarchical organization,0.6448431015014648
translation,76,223,results,traditional methods,achieve,encouraging performance,traditional methods achieve encouraging performance,0.6127271056175232
translation,76,223,results,encouraging performance,on,aforementioned tasks,encouraging performance on aforementioned tasks,0.5568196177482605
translation,76,223,results,results,show,traditional methods,results show traditional methods,0.5649437308311462
translation,76,227,results,significantly outperforms,by over,49.4 %,significantly outperforms by over 49.4 %,0.7182567119598389
translation,76,227,results,significantly outperforms,in terms of,average f 1 - measure,significantly outperforms in terms of average f 1 - measure,0.696204662322998
translation,76,227,results,balahur 's method,by over,49.4 %,balahur 's method by over 49.4 %,0.6538114547729492
translation,76,227,results,balahur 's method,in terms of,average f 1 - measure,balahur 's method in terms of average f 1 - measure,0.6899481415748596
translation,76,227,results,49.4 %,in terms of,average f 1 - measure,49.4 % in terms of average f 1 - measure,0.6858331561088562
translation,76,227,results,our approach,has,significantly outperforms,our approach has significantly outperforms,0.6118320226669312
translation,76,227,results,significantly outperforms,has,balahur 's method,significantly outperforms has balahur 's method,0.6036245822906494
translation,76,233,results,our approach,has,significantly outperforms,our approach has significantly outperforms,0.6118320226669312
translation,76,234,results,su 's method,by,9.1 %,su 's method by 9.1 %,0.5741330981254578
translation,76,234,results,su 's method,over,9.1 %,su 's method over 9.1 %,0.6309859752655029
translation,76,234,results,9.1 %,in terms of,average f 1 - measure,9.1 % in terms of average f 1 - measure,0.7072295546531677
translation,76,234,results,results,has,su 's method,results has su 's method,0.5535079836845398
translation,76,239,results,li's method and lloret 's method,by,significant absolute gains,li's method and lloret 's method by significant absolute gains,0.5989456176757812
translation,76,239,results,significant absolute gains,of,"over 23.7 % , and 21.5 %","significant absolute gains of over 23.7 % , and 21.5 %",0.5717146992683411
translation,76,239,results,significant absolute gains,in terms of,average rouge -1,significant absolute gains in terms of average rouge -1,0.6731929183006287
translation,76,239,results,our approach,has,outperforms,our approach has outperforms,0.6385829448699951
translation,76,239,results,outperforms,has,li's method and lloret 's method,outperforms has li's method and lloret 's method,0.5605331659317017
translation,76,240,results,performance,over,two methods,performance over two methods,0.7355064749717712
translation,76,240,results,performance,in terms of,rouge - su4,performance in terms of rouge - su4,0.6865141987800598
translation,76,240,results,two methods,in terms of,average rouge - 2,two methods in terms of average rouge - 2,0.6876296997070312
translation,76,240,results,two methods,in terms of,rouge - su4,two methods in terms of rouge - su4,0.7274782657623291
translation,76,240,results,two methods,by,absolute gains,two methods by absolute gains,0.6126174926757812
translation,76,240,results,two methods,in terms of,rouge - su4,two methods in terms of rouge - su4,0.7274782657623291
translation,76,240,results,average rouge - 2,by,absolute gains,average rouge - 2 by absolute gains,0.5623037815093994
translation,76,240,results,average rouge - 2,by,absolute gains,average rouge - 2 by absolute gains,0.5623037815093994
translation,76,240,results,absolute gains,of,over 9.41 % and 7.87 %,absolute gains of over 9.41 % and 7.87 %,0.5798618793487549
translation,76,240,results,absolute gains,of,over 8.86 % and 7.31 %,absolute gains of over 8.86 % and 7.31 %,0.5862189531326294
translation,76,240,results,absolute gains,of,over 8.86 % and 7.31 %,absolute gains of over 8.86 % and 7.31 %,0.5862189531326294
translation,76,240,results,rouge - su4,by,absolute gains,rouge - su4 by absolute gains,0.6105256676673889
translation,76,240,results,absolute gains,of,over 8.86 % and 7.31 %,absolute gains of over 8.86 % and 7.31 %,0.5862189531326294
translation,76,240,results,results,improves,performance,results improves performance,0.7413748502731323
translation,76,241,results,improvements,use of,hierarchical organization,improvements use of hierarchical organization,0.6630645990371704
translation,76,241,results,improvements,use of,answer generation algorithm,improvements use of answer generation algorithm,0.6231351494789124
translation,76,241,results,answer generation algorithm,exploits,multiple criteria,answer generation algorithm exploits multiple criteria,0.6535881161689758
translation,76,241,results,multiple criteria,especially,parent- child relation,multiple criteria especially parent- child relation,0.66016685962677
translation,76,241,results,parent- child relation,among,aspects,parent- child relation among aspects,0.595718264579773
translation,76,241,results,results,find that,improvements,results find that improvements,0.6680544018745422
translation,76,250,results,performance change,is,sharp,performance change is sharp,0.6158591508865356
translation,76,250,results,sharp,when,? 1 changes,sharp when ? 1 changes,0.7138464450836182
translation,76,250,results,results,find that,performance change,results find that performance change,0.6330010294914246
translation,77,154,ablation-analysis,question representation dynamically ( + dq ),in,each iteration,question representation dynamically ( + dq ) in each iteration,0.5220413208007812
translation,77,154,ablation-analysis,question representation dynamically ( + dq ),benefits,relation extraction,question representation dynamically ( + dq ) benefits relation extraction,0.5966752767562866
translation,77,154,ablation-analysis,relation extraction,in,most cases,relation extraction in most cases,0.5209579467773438
translation,77,154,ablation-analysis,ablation analysis,updating,question representation dynamically ( + dq ),ablation analysis updating question representation dynamically ( + dq ),0.7368572950363159
translation,77,197,ablation-analysis,incorrect relation extraction,is,major error,incorrect relation extraction is major error,0.5365002155303955
translation,77,197,ablation-analysis,pq / pql datasets,has,incorrect relation extraction,pq / pql datasets has incorrect relation extraction,0.574410080909729
translation,77,197,ablation-analysis,ablation analysis,for,pq / pql datasets,ablation analysis for pq / pql datasets,0.5846529603004456
translation,77,198,ablation-analysis,2 - hop testing data,from,pq2 / pql2 and pq + / pql +,2 - hop testing data from pq2 / pql2 and pq + / pql +,0.5941625237464905
translation,77,198,ablation-analysis,2 - hop testing data,observe,long questions,2 - hop testing data observe long questions,0.6549124717712402
translation,77,198,ablation-analysis,long questions,help,learning,long questions help learning,0.7403884530067444
translation,77,198,ablation-analysis,learning,of,short questions,learning of short questions,0.520784318447113
translation,77,198,ablation-analysis,ablation analysis,comparing,2 - hop testing data,ablation analysis comparing 2 - hop testing data,0.7378641963005066
translation,77,202,ablation-analysis,dynamic question representations,significantly benefit,relation extraction ( re ),dynamic question representations significantly benefit relation extraction ( re ),0.6742005348205566
translation,77,202,ablation-analysis,relation extraction ( re ),for,first hop,relation extraction ( re ) for first hop,0.6118162870407104
translation,77,202,ablation-analysis,ablation analysis,has,dynamic question representations,ablation analysis has dynamic question representations,0.5719375610351562
translation,77,203,ablation-analysis,uhop,utilizes,same model,uhop utilizes same model,0.7304102778434753
translation,77,203,ablation-analysis,uhop,relieving,attention,uhop relieving attention,0.7510349154472351
translation,77,203,ablation-analysis,same model,for,relation selection and termination decision,same model for relation selection and termination decision,0.6214816570281982
translation,77,203,ablation-analysis,attention,to,previous relation,attention to previous relation,0.573521614074707
translation,77,203,ablation-analysis,previous relation,in,later selection process,previous relation in later selection process,0.5195815563201904
translation,77,203,ablation-analysis,previous relation,decreases,ambiguity,previous relation decreases ambiguity,0.7354274988174438
translation,77,203,ablation-analysis,ambiguity,in,earlier selection process,ambiguity in earlier selection process,0.555721640586853
translation,77,203,ablation-analysis,ambiguity,in,testing phase,ambiguity in testing phase,0.5401530265808105
translation,77,203,ablation-analysis,ablation analysis,has,uhop,ablation analysis has uhop,0.5579927563667297
translation,77,147,experiments,questions,of,long relation paths,questions of long relation paths,0.5787481665611267
translation,77,147,experiments,questions,applied,dynamic question representations ( dq ),questions applied dynamic question representations ( dq ),0.7289214730262756
translation,77,147,experiments,dynamic question representations ( dq ),in,uhop,dynamic question representations ( dq ) in uhop,0.5356473326683044
translation,77,133,hyperparameters,300 - dimensional pretrained glove,has,word embeddings,300 - dimensional pretrained glove has word embeddings,0.5129171013832092
translation,77,133,hyperparameters,hyperparameters,used,300 - dimensional pretrained glove,hyperparameters used 300 - dimensional pretrained glove,0.5623317360877991
translation,77,134,hyperparameters,abwim,chose,"1 , 3 , 5","abwim chose 1 , 3 , 5",0.644100546836853
translation,77,134,hyperparameters,abwim,chose,150,abwim chose 150,0.7028701901435852
translation,77,134,hyperparameters,"1 , 3 , 5",as,kernel sizes,"1 , 3 , 5 as kernel sizes",0.5153355598449707
translation,77,134,hyperparameters,150,as,number of filters,150 as number of filters,0.529334545135498
translation,77,134,hyperparameters,number of filters,for,three cnn layers,number of filters for three cnn layers,0.5905365347862244
translation,77,134,hyperparameters,hyperparameters,In,abwim,hyperparameters In abwim,0.4917241930961609
translation,77,135,hyperparameters,hidden size,for,"all lstm ( [ 100 , 150 , 256 ] )","hidden size for all lstm ( [ 100 , 150 , 256 ] )",0.562465488910675
translation,77,135,hyperparameters,margin,for,"hinge loss ( [ 0.1 , 0.3 , 0.5 , 0.7 , 1.0 ] )","margin for hinge loss ( [ 0.1 , 0.3 , 0.5 , 0.7 , 1.0 ] )",0.5867406725883484
translation,77,135,hyperparameters,dropout rate,has,"[ 0 , 0.2 , 0.4 ] )","dropout rate has [ 0 , 0.2 , 0.4 ] )",0.5468201637268066
translation,77,135,hyperparameters,hyperparameters,tune,hidden size,hyperparameters tune hidden size,0.7449708580970764
translation,77,135,hyperparameters,hyperparameters,with,grid search,hyperparameters with grid search,0.6160174012184143
translation,77,156,hyperparameters,hyperparameters,randomly initialized,word embeddings,hyperparameters randomly initialized word embeddings,0.7002855539321899
translation,77,157,hyperparameters,learning rate,to,0.001,learning rate to 0.001,0.5506307482719421
translation,77,157,hyperparameters,hidden size,to,256,hidden size to 256,0.6058640480041504
translation,77,157,hyperparameters,embedding size,to,300,embedding size to 300,0.6162461042404175
translation,77,157,hyperparameters,model,using,"rm - sprop ( hinton et al. , 2014 ) algorithm","model using rm - sprop ( hinton et al. , 2014 ) algorithm",0.6331197023391724
translation,77,157,hyperparameters,hyperparameters,set,learning rate,hyperparameters set learning rate,0.5994082689285278
translation,77,157,hyperparameters,hyperparameters,set,hidden size,hyperparameters set hidden size,0.6568804979324341
translation,77,157,hyperparameters,hyperparameters,set,embedding size,hyperparameters set embedding size,0.6328905820846558
translation,77,157,hyperparameters,hyperparameters,optimized,model,hyperparameters optimized model,0.7242774367332458
translation,77,7,model,transition - based search framework,to replace,relation - chain - based search one,transition - based search framework to replace relation - chain - based search one,0.7293903231620789
translation,77,7,model,uhop,has,unrestricted - hop framework,uhop has unrestricted - hop framework,0.5549063086509705
translation,77,7,model,model,propose,uhop,model propose uhop,0.7248257398605347
translation,77,23,model,unrestricted - hop relation extraction framework,to relax,restrictions,unrestricted - hop relation extraction framework to relax restrictions,0.6533651351928711
translation,77,23,model,restrictions,on,candidate path length,restrictions on candidate path length,0.5558913350105286
translation,77,23,model,uhop,has,unrestricted - hop relation extraction framework,uhop has unrestricted - hop relation extraction framework,0.520916759967804
translation,77,23,model,model,propose,uhop,model propose uhop,0.7248257398605347
translation,77,84,model,summarization,de-focus,selected relation,summarization de-focus selected relation,0.7858099341392517
translation,77,84,model,selected relation,by manipulating,weights,selected relation by manipulating weights,0.7148838639259338
translation,77,84,model,weights,in,question representation,weights in question representation,0.5166080594062805
translation,77,84,model,model,de-focus,selected relation,model de-focus selected relation,0.7815068364143372
translation,77,139,results,performance,of,models,performance of models,0.598082423210144
translation,77,139,results,models,within,uhop framework,models within uhop framework,0.6956645250320435
translation,77,139,results,results,has,performance,results has performance,0.5972660779953003
translation,77,148,results,hr -bilstm and abwim,within or independent of,uhop,hr -bilstm and abwim within or independent of uhop,0.6592905521392822
translation,77,148,results,hr -bilstm and abwim,perform,nearly perfectly,hr -bilstm and abwim perform nearly perfectly,0.6229417324066162
translation,77,148,results,nearly perfectly,in,all datasets,nearly perfectly in all datasets,0.5296290516853333
translation,77,148,results,uhop,has,outperform,uhop has outperform,0.7106469869613647
translation,77,148,results,outperform,has,irn,outperform has irn,0.6299563050270081
translation,77,151,results,uhop,performs,comparably,uhop performs comparably,0.6721571087837219
translation,77,151,results,comparably,with,previous work,comparably with previous work,0.6531749367713928
translation,77,151,results,results,has,uhop,results has uhop,0.5721293687820435
translation,77,160,results,uhop,perfectly solves,problem,uhop perfectly solves problem,0.7043065428733826
translation,77,160,results,results,show,uhop,results show uhop,0.633794367313385
translation,77,161,results,neural lp and minerva,use,multi-layer neural networks,neural lp and minerva use multi-layer neural networks,0.5869336128234863
translation,77,161,results,uhop,benefits from,more powerful natural language understanding models,uhop benefits from more powerful natural language understanding models,0.6951464414596558
translation,77,161,results,neural lp and minerva,use,multi-layer neural networks,neural lp and minerva use multi-layer neural networks,0.5869336128234863
translation,77,161,results,multi-layer neural networks,as,policy network,multi-layer neural networks as policy network,0.5688013434410095
translation,77,161,results,neural lp and minerva,has,uhop,neural lp and minerva has uhop,0.6316452026367188
translation,77,163,results,error propagation,leading to,poor performance,error propagation leading to poor performance,0.7144888043403625
translation,77,163,results,poor performance,for,longpath questions,poor performance for longpath questions,0.6180116534233093
translation,77,163,results,longpath questions,in,neural lp and minerva,longpath questions in neural lp and minerva,0.5779473781585693
translation,77,163,results,mitigated,by,relation inference power,mitigated by relation inference power,0.5578762888908386
translation,77,163,results,relation inference power,of,uhop,relation inference power of uhop,0.5627918839454651
translation,77,163,results,relation inference power,performs,well,relation inference power performs well,0.5761800408363342
translation,77,163,results,well,for,all four buckets of questions,well for all four buckets of questions,0.6643121838569641
translation,77,185,results,improve,has,pq / pql performance,improve has pq / pql performance,0.5790978670120239
translation,77,185,results,results,updating,question representations dynamically ( dq ),results updating question representations dynamically ( dq ),0.7390533089637756
translation,77,189,results,outperform,by,more than 7 %,outperform by more than 7 %,0.6405508518218994
translation,77,189,results,original version,by,more than 7 %,original version by more than 7 %,0.5874955654144287
translation,77,189,results,uhop framework,has,both models,uhop framework has both models,0.592383086681366
translation,77,189,results,both models,has,outperform,both models has outperform,0.585362434387207
translation,77,189,results,outperform,has,original version,outperform has original version,0.6124585270881653
translation,77,189,results,results,Within,uhop framework,results Within uhop framework,0.6522030234336853
translation,77,199,results,better,on,2 - hop data,better on 2 - hop data,0.5686938166618347
translation,77,199,results,2 - hop data,trained on both,2 - hop and 3 - hop data,2 - hop data trained on both 2 - hop and 3 - hop data,0.7142732739448547
translation,77,204,results,experiments,trained on,3 - hop,experiments trained on 3 - hop,0.720044732093811
translation,77,204,results,experiments,tested on,2 - hop,experiments tested on 2 - hop,0.7473158240318298
translation,77,204,results,model,does not terminate,correctly,model does not terminate correctly,0.7525622844696045
translation,77,204,results,correctly,on,more than 40 %,correctly on more than 40 %,0.5726205706596375
translation,77,204,results,more than 40 %,of,pql2 data,more than 40 % of pql2 data,0.6191383600234985
translation,77,204,results,experiments,has,model,experiments has model,0.5536225438117981
translation,77,204,results,3 - hop,has,model,3 - hop has model,0.5391702055931091
translation,77,204,results,2 - hop,has,model,2 - hop has model,0.53342205286026
translation,78,104,ablation-analysis,pre-trained word embedding,boosted,our accuracy,pre-trained word embedding boosted our accuracy,0.7056838274002075
translation,78,104,ablation-analysis,our accuracy,for,about 5 % - 10 %,our accuracy for about 5 % - 10 %,0.6190306544303894
translation,78,104,ablation-analysis,about 5 % - 10 %,on,developing set,about 5 % - 10 % on developing set,0.5938076972961426
translation,78,104,ablation-analysis,ablation analysis,has,pre-trained word embedding,ablation analysis has pre-trained word embedding,0.5236545205116272
translation,78,5,model,common sense knowledge,by using,pretrained word embeddings,common sense knowledge by using pretrained word embeddings,0.5486831665039062
translation,78,5,model,common sense knowledge,by dynamically generating,weighted representation,common sense knowledge by dynamically generating weighted representation,0.6872567534446716
translation,78,5,model,pretrained word embeddings,during,contextual embeddings,pretrained word embeddings during contextual embeddings,0.5378127098083496
translation,78,5,model,weighted representation,of,related script knowledge,weighted representation of related script knowledge,0.5858790278434753
translation,78,5,model,model,exploit,common sense knowledge,model exploit common sense knowledge,0.7335378527641296
translation,79,212,baselines,splitrcqa,replace,rc model,splitrcqa replace rc model,0.6458568572998047
translation,79,212,baselines,rc model,with,docqa,rc model with docqa,0.6641413569450378
translation,79,212,baselines,baselines,has,splitrcqa,baselines has splitrcqa,0.5790886282920837
translation,79,213,baselines,googlebox,sample,100 random development set questions,googlebox sample 100 random development set questions,0.6752892136573792
translation,79,213,baselines,googlebox,check,google,googlebox check google,0.6473313570022583
translation,79,213,baselines,googlebox,returns,box,googlebox returns box,0.7495742440223694
translation,79,213,baselines,google,returns,box,google returns box,0.663653552532196
translation,79,213,baselines,baselines,has,googlebox,baselines has googlebox,0.5627195835113525
translation,79,201,hyperparameters,hidden state dimension,of,pointer network,hidden state dimension of pointer network,0.5806020498275757
translation,79,201,hyperparameters,pointer network,is,512,pointer network is 512,0.5953273773193359
translation,79,201,hyperparameters,"adagrad ( duchi et al. , 2010 )",combined with,l 2 regularization,"adagrad ( duchi et al. , 2010 ) combined with l 2 regularization",0.6037184000015259
translation,79,201,hyperparameters,"adagrad ( duchi et al. , 2010 )",combined with,dropout rate,"adagrad ( duchi et al. , 2010 ) combined with dropout rate",0.5973278880119324
translation,79,201,hyperparameters,dropout rate,of,0.25,dropout rate of 0.25,0.5923166275024414
translation,79,201,hyperparameters,hyperparameters,used,"adagrad ( duchi et al. , 2010 )","hyperparameters used adagrad ( duchi et al. , 2010 )",0.4978831708431244
translation,79,201,hyperparameters,hyperparameters,has,hidden state dimension,hyperparameters has hidden state dimension,0.5041183233261108
translation,79,202,hyperparameters,50 - dimensional word embeddings,using,glove,50 - dimensional word embeddings using glove,0.6340804696083069
translation,79,202,hyperparameters,embeddings,for,missing words,embeddings for missing words,0.561926007270813
translation,79,202,hyperparameters,hyperparameters,initialize,50 - dimensional word embeddings,hyperparameters initialize 50 - dimensional word embeddings,0.6873400211334229
translation,79,202,hyperparameters,hyperparameters,learn,embeddings,hyperparameters learn embeddings,0.6614922285079956
translation,79,7,model,answering,has,broad and complex questions,answering has broad and complex questions,0.5702680349349976
translation,79,8,model,complex questions,into,sequence of simple questions,complex questions into sequence of simple questions,0.6098313331604004
translation,79,8,model,final answer,from,sequence of answers,final answer from sequence of answers,0.5359407067298889
translation,79,8,model,model,decompose,complex questions,model decompose complex questions,0.7887986898422241
translation,79,8,model,model,compute,final answer,model compute final answer,0.7141242623329163
translation,79,21,model,framework,for,qa,framework for qa,0.7021846771240234
translation,79,210,model,complex questions,by,decomposition,complex questions by decomposition,0.6202012300491333
translation,79,210,model,model,has,splitqa,model has splitqa,0.6264036893844604
translation,79,35,results,complexwe - bquestionsand,find,question decomposition,complexwe - bquestionsand find question decomposition,0.5869031548500061
translation,79,35,results,question decomposition,substantially improves,precision@1,question decomposition substantially improves precision@1,0.7325429320335388
translation,79,35,results,precision@1,from,20.8 to 27.5,precision@1 from 20.8 to 27.5,0.5380095839500427
translation,79,217,results,simpqa,does not decompose,questions,simpqa does not decompose questions,0.6947733163833618
translation,79,217,results,simpqa,by performing,question decomposition,simpqa by performing question decomposition,0.7099654078483582
translation,79,217,results,questions,obtained,20.8 p@1,questions obtained 20.8 p@1,0.6364231705665588
translation,79,217,results,question decomposition,substantially improve,performance,question decomposition substantially improve performance,0.7125237584114075
translation,79,217,results,performance,to,27.5 p@1,performance to 27.5 p@1,0.5985877513885498
translation,79,217,results,results,has,simpqa,results has simpqa,0.5770164132118225
translation,79,220,results,outperforms,by,3.4 points,outperforms by 3.4 points,0.6237910389900208
translation,79,220,results,rcqa,by,3.4 points,rcqa by 3.4 points,0.5836617946624756
translation,79,220,results,splitrcqa,has,outperforms,splitrcqa has outperforms,0.6281656622886658
translation,79,220,results,outperforms,has,rcqa,outperforms has rcqa,0.5890415906906128
translation,79,220,results,results,has,splitrcqa,results has splitrcqa,0.5667315125465393
translation,79,222,results,googlebox,finds,correct answer,googlebox finds correct answer,0.6209961771965027
translation,79,222,results,correct answer,in,2.5 %,correct answer in 2.5 %,0.4699851870536804
translation,79,222,results,2.5 %,of,cases,2.5 % of cases,0.5763671398162842
translation,79,222,results,results,has,googlebox,results has googlebox,0.5727570056915283
translation,79,223,results,performance,on,answering complex questions,performance on answering complex questions,0.547561764717102
translation,79,223,results,answering complex questions,using,two independent rc models,answering complex questions using two independent rc models,0.6129066348075867
translation,79,223,results,question de-composition,has,substantially improves,question de-composition has substantially improves,0.608291745185852
translation,79,223,results,substantially improves,has,performance,substantially improves has performance,0.5890060663223267
translation,79,223,results,results,demonstrated,question de-composition,results demonstrated question de-composition,0.7428038716316223
translation,79,233,results,model,outputs,exact correct output sequence,model outputs exact correct output sequence,0.7523577213287354
translation,79,233,results,model,allowing,errors,model allowing errors,0.7081460952758789
translation,79,233,results,errors,of,one word,errors of one word,0.6160366535186768
translation,79,233,results,one word,to,left and right,one word to left and right,0.6277006268501282
translation,79,233,results,accuracy,is,77.1 %,accuracy is 77.1 %,0.5549196004867554
translation,79,233,results,accuracy,at,77.1 %,accuracy at 77.1 %,0.5198719501495361
translation,79,233,results,exact correct output sequence,has,60.9 % of the time,exact correct output sequence has 60.9 % of the time,0.5620694160461426
translation,79,233,results,results,find that,model,results find that model,0.6360844373703003
translation,79,234,results,token - level accuracy,is,83.0 %,token - level accuracy is 83.0 %,0.5403674840927124
translation,79,234,results,allowing one- word errors,has,89.7 %,allowing one- word errors has 89.7 %,0.5533641576766968
translation,79,234,results,results,has,token - level accuracy,results has token - level accuracy,0.5298058390617371
translation,80,32,baselines,two neural - based systems,namely,bidirectional attention flow,two neural - based systems namely bidirectional attention flow,0.6590442657470703
translation,80,32,baselines,two neural - based systems,namely,gated-attention reader,two neural - based systems namely gated-attention reader,0.7286786437034607
translation,80,32,baselines,bidirectional attention flow,has,"seo et al. , 2017 )","bidirectional attention flow has seo et al. , 2017 )",0.5181008577346802
translation,80,32,baselines,gated-attention reader,has,"dhingra et al. , 2017 )","gated-attention reader has dhingra et al. , 2017 )",0.5593560338020325
translation,80,32,baselines,baselines,use,two neural - based systems,baselines use two neural - based systems,0.6312872171401978
translation,80,53,experimental-setup,bidirectional attention flow ( bidaf ),used for,answer extraction and description datasets,bidirectional attention flow ( bidaf ) used for answer extraction and description datasets,0.6408348083496094
translation,80,53,experimental-setup,experimental setup,has,bidirectional attention flow ( bidaf ),experimental setup has bidirectional attention flow ( bidaf ),0.5447984337806702
translation,80,36,experiments,narrativeqa,has,mctest ( 160 + 500 ),narrativeqa has mctest ( 160 + 500 ),0.6190741062164307
translation,80,36,experiments,narrativeqa,has,race,narrativeqa has race,0.6221504807472229
translation,80,54,model,bidaf,models,bi-directional attention,bidaf models bi-directional attention,0.7220324277877808
translation,80,54,model,bi-directional attention,between,context and question,bi-directional attention between context and question,0.5787768959999084
translation,80,54,model,model,has,bidaf,model has bidaf,0.6748911142349243
translation,80,7,results,baseline performances,for,hard subsets,baseline performances for hard subsets,0.5459893345832825
translation,80,7,results,remarkably degrade,compared to,entire datasets,remarkably degrade compared to entire datasets,0.6656151413917542
translation,80,55,results,state - of- the - art performance,on,squad dataset,state - of- the - art performance on squad dataset,0.5080657005310059
translation,80,55,results,results,achieved,state - of- the - art performance,results achieved state - of- the - art performance,0.6799122095108032
translation,80,71,results,our implementations,outperformed or showed,comparable performance,our implementations outperformed or showed comparable performance,0.732927680015564
translation,80,71,results,comparable performance,to,official baseline,comparable performance to official baseline,0.5303844809532166
translation,80,71,results,official baseline,on,most datasets,official baseline on most datasets,0.4753058850765228
translation,80,71,results,results,has,our implementations,results has our implementations,0.5423925518989563
translation,80,119,results,addsent and qangaroo,scores,remarkably improved ( > 20 f1 ),addsent and qangaroo scores remarkably improved ( > 20 f1 ),0.6972229480743408
translation,80,119,results,results,In,addsent and qangaroo,results In addsent and qangaroo,0.5682499408721924
translation,80,130,results,baseline performances,on,hard subset,baseline performances on hard subset,0.4790664315223694
translation,80,130,results,remarkably decreased,in,almost all examined datasets,remarkably decreased in almost all examined datasets,0.5592119693756104
translation,80,130,results,baseline performances,has,remarkably decreased,baseline performances has remarkably decreased,0.5677210092544556
translation,80,194,results,word matching,was,more important,word matching was more important,0.6026449203491211
translation,80,194,results,more important,in,easy subsets,more important in easy subsets,0.5268281698226929
translation,80,194,results,more pertinent,to,hard subsets,more pertinent to hard subsets,0.5613045692443848
translation,80,194,results,hard subsets,in,10 of the 12 datasets,hard subsets in 10 of the 12 datasets,0.5172758102416992
translation,80,194,results,results,see that,word matching,results see that word matching,0.5927697420120239
translation,81,23,baselines,pullnet,builds on,"graft - net 1 ( sun et al. , 2018 ) early fusion system","pullnet builds on graft - net 1 ( sun et al. , 2018 ) early fusion system",0.605132520198822
translation,81,23,baselines,baselines,has,pullnet,baselines has pullnet,0.5593709945678711
translation,81,5,experiments,corpus,supplemented with,large but incomplete kb,corpus supplemented with large but incomplete kb,0.640954852104187
translation,81,5,experiments,questions,require,"non-trivial ( e.g. ,   multi-hop   ) reasoning","questions require non-trivial ( e.g. ,   multi-hop   ) reasoning",0.635291576385498
translation,81,6,model,integrated framework,for,learning what to retrieve,integrated framework for learning what to retrieve,0.6398487687110901
translation,81,6,model,reasoning,with,heterogeneous information,reasoning with heterogeneous information,0.6211626529693604
translation,81,6,model,heterogeneous information,to find,best answer,heterogeneous information to find best answer,0.6343615651130676
translation,81,6,model,pullnet,has,integrated framework,pullnet has integrated framework,0.6124342679977417
translation,81,6,model,model,describe,pullnet,model describe pullnet,0.6990102529525757
translation,81,7,model,pullnet,uses,iterative process,pullnet uses iterative process,0.6305661201477051
translation,81,7,model,iterative process,to construct,question -specific subgraph,iterative process to construct question -specific subgraph,0.6901403665542603
translation,81,7,model,question -specific subgraph,contains,information,question -specific subgraph contains information,0.6382074952125549
translation,81,7,model,information,relevant to,question,information relevant to question,0.7135324478149414
translation,81,7,model,model,has,pullnet,model has pullnet,0.6044967174530029
translation,81,21,model,integrated framework,for,learning,integrated framework for learning,0.6823906302452087
translation,81,21,model,integrated framework,combining,heterogeneous information,integrated framework combining heterogeneous information,0.7136890888214111
translation,81,21,model,learning,from,corpus,learning from corpus,0.5948046445846558
translation,81,21,model,learning,from,combination,learning from combination,0.6014742255210876
translation,81,21,model,what to retrieve,from,corpus,what to retrieve from corpus,0.587809681892395
translation,81,21,model,what to retrieve,from,kb,what to retrieve from kb,0.5635202527046204
translation,81,21,model,what to retrieve,from,combination,what to retrieve from combination,0.5795601010322571
translation,81,21,model,heterogeneous information,into,single data structure,heterogeneous information into single data structure,0.5195677876472473
translation,81,21,model,learning,has,what to retrieve,learning has what to retrieve,0.5605223178863525
translation,81,21,model,model,propose,integrated framework,model propose integrated framework,0.6623273491859436
translation,81,24,model,graft - net,uses,heuristics,graft - net uses heuristics,0.6962740421295166
translation,81,24,model,heuristics,to build,questionspecific subgraph,heuristics to build questionspecific subgraph,0.6928940415382385
translation,81,24,model,questionspecific subgraph,contains,sentences,questionspecific subgraph contains sentences,0.6696062684059143
translation,81,24,model,questionspecific subgraph,contains,entities and facts,questionspecific subgraph contains entities and facts,0.6122440695762634
translation,81,24,model,sentences,from,corpus,sentences from corpus,0.5414303541183472
translation,81,24,model,entities and facts,from,kb,entities and facts from kb,0.5634889006614685
translation,81,24,model,model,has,graft - net,model has graft - net,0.5987643599510193
translation,81,31,model,pullnet,when and where to apply,  pull   operations,pullnet when and where to apply   pull   operations,0.6784085035324097
translation,81,31,model,  pull   operations,with,another graph cnn classifier,  pull   operations with another graph cnn classifier,0.6436477303504944
translation,81,31,model,model,has,pullnet,model has pullnet,0.6044967174530029
translation,81,36,results,pullnet,improves over,current state- ofthe - art,pullnet improves over current state- ofthe - art,0.7251448035240173
translation,81,36,results,current state- ofthe - art,for,kb - only qa,current state- ofthe - art for kb - only qa,0.6112983822822571
translation,81,36,results,current state- ofthe - art,on,several benchmark datasets,current state- ofthe - art on several benchmark datasets,0.472778856754303
translation,81,36,results,kb - only qa,on,several benchmark datasets,kb - only qa on several benchmark datasets,0.4761943817138672
translation,81,36,results,results,has,pullnet,results has pullnet,0.5479627847671509
translation,81,40,results,pullnet,obtains,performance,pullnet obtains performance,0.6277010440826416
translation,81,40,results,performance,of,85.2 % hits - at- one,performance of 85.2 % hits - at- one,0.5405917167663574
translation,81,40,results,85.2 % hits - at- one,with,kb,85.2 % hits - at- one with kb,0.6571691036224365
translation,81,40,results,85.2 % hits - at- one,with,kb,85.2 % hits - at- one with kb,0.6571691036224365
translation,81,40,results,85.2 % hits - at- one,if,kb,85.2 % hits - at- one if kb,0.6363221406936646
translation,81,40,results,kb,from which,half of the triples,kb from which half of the triples,0.6469652652740479
translation,81,40,results,kb,supplemented with,corpus,kb supplemented with corpus,0.6371317505836487
translation,81,40,results,kb,supplemented with,corpus,kb supplemented with corpus,0.6371317505836487
translation,81,40,results,results,has,pullnet,results has pullnet,0.5479627847671509
translation,81,42,results,pullnet,improves over,graft - net,pullnet improves over graft - net,0.7573959231376648
translation,81,42,results,graft - net,by,6.8 %,graft - net by 6.8 %,0.5728230476379395
translation,81,42,results,graft - net,on,complexwebquestions dataset,graft - net on complexwebquestions dataset,0.5273833274841309
translation,81,42,results,6.8 %,on,complexwebquestions dataset,6.8 % on complexwebquestions dataset,0.5174028277397156
translation,82,90,ablation-analysis,drops,to,64 %,drops to 64 %,0.6577478051185608
translation,82,90,ablation-analysis,mix of commonsense - based cs - convqa,has,accuracy,mix of commonsense - based cs - convqa has accuracy,0.5568081140518188
translation,82,90,ablation-analysis,accuracy,has,drops,accuracy has drops,0.6229715347290039
translation,82,90,ablation-analysis,ablation analysis,tested with,mix of commonsense - based cs - convqa,ablation analysis tested with mix of commonsense - based cs - convqa,0.7400755286216736
translation,82,108,ablation-analysis,finetuning,is,effective strategy,finetuning is effective strategy,0.6141403317451477
translation,82,108,ablation-analysis,effective strategy,for,synthetic l-convqa split,effective strategy for synthetic l-convqa split,0.6364959478378296
translation,82,108,ablation-analysis,ablation analysis,has,finetuning,ablation analysis has finetuning,0.5503973364830017
translation,82,8,model,metrics,enable,quantitative evaluation,metrics enable quantitative evaluation,0.6236343383789062
translation,82,8,model,quantitative evaluation,of,consistency,quantitative evaluation of consistency,0.5947251319885254
translation,82,8,model,consistency,has,in vqa,consistency has in vqa,0.6117063760757446
translation,82,8,model,model,introduce,"dataset , convqa","model introduce dataset , convqa",0.6164230704307556
translation,82,10,model,consistency - improving data augmentation module,has,consistency teacher module ( ctm ),consistency - improving data augmentation module has consistency teacher module ( ctm ),0.5840010046958923
translation,82,10,model,model,propose,consistency - improving data augmentation module,model propose consistency - improving data augmentation module,0.6871227025985718
translation,82,26,model,consistency,of,vqa models,consistency of vqa models,0.5995433926582336
translation,82,26,model,consistency,propose,consistency teacher module ( ctm ),consistency propose consistency teacher module ( ctm ),0.6236879229545593
translation,82,26,model,consistency teacher module ( ctm ),consists of,question generator,consistency teacher module ( ctm ) consists of question generator,0.6479425430297852
translation,82,26,model,consistency teacher module ( ctm ),consists of,consistency checker,consistency teacher module ( ctm ) consists of consistency checker,0.6388336420059204
translation,82,26,model,question generator,synthesizes,entailed ( or similar-intent ) questions,question generator synthesizes entailed ( or similar-intent ) questions,0.6893388032913208
translation,82,26,model,entailed ( or similar-intent ) questions,given,seed qa pair,entailed ( or similar-intent ) questions given seed qa pair,0.6686131358146667
translation,82,26,model,entailed ( or similar-intent ) questions,given,consistency checker,entailed ( or similar-intent ) questions given consistency checker,0.6731212139129639
translation,82,26,model,consistency checker,examines,answers,consistency checker examines answers,0.6122531294822693
translation,82,26,model,similar-intent questions,are,consistent,similar-intent questions are consistent,0.568051278591156
translation,82,26,model,model,To improve,consistency,model To improve consistency,0.7364407181739807
translation,82,27,model,our ctm,acts as,consistency - based data augmentation scheme,our ctm acts as consistency - based data augmentation scheme,0.6278249621391296
translation,82,27,model,consistency - based data augmentation scheme,trains,vqa model,consistency - based data augmentation scheme trains vqa model,0.6946616768836975
translation,82,27,model,vqa model,with,consistent answers,vqa model with consistent answers,0.6325856447219849
translation,82,27,model,consistent answers,to,entailed questions,consistent answers to entailed questions,0.5600051879882812
translation,82,27,model,consistent vqa model,has,our ctm,consistent vqa model has our ctm,0.6150503754615784
translation,82,27,model,model,For training,consistent vqa model,model For training consistent vqa model,0.7371379137039185
translation,82,59,model,vqa consistency,training,vqa model,vqa consistency training vqa model,0.729907214641571
translation,82,59,model,vqa consistency,performs,consistency - based data augmentation,vqa consistency performs consistency - based data augmentation,0.5861449241638184
translation,82,59,model,vqa model,using,consistency teacher module ( ctm ),vqa model using consistency teacher module ( ctm ),0.7080802321434021
translation,82,59,model,consistency teacher module ( ctm ),generates,entailed questions,consistency teacher module ( ctm ) generates entailed questions,0.6307861804962158
translation,82,59,model,consistency teacher module ( ctm ),performs,consistency - based data augmentation,consistency teacher module ( ctm ) performs consistency - based data augmentation,0.638431966304779
translation,82,59,model,model,To improve,vqa consistency,model To improve vqa consistency,0.7008202075958252
translation,82,60,model,ctm,consists of,two trainable components,ctm consists of two trainable components,0.6996244788169861
translation,82,60,model,two trainable components,has,entailed question generator,two trainable components has entailed question generator,0.5894696116447449
translation,82,60,model,model,has,ctm,model has ctm,0.629252552986145
translation,82,113,model,convqa dataset,consisting of,logic-based and commonsense - based consistent qa pairs,convqa dataset consisting of logic-based and commonsense - based consistent qa pairs,0.6954492926597595
translation,82,113,model,logic-based and commonsense - based consistent qa pairs,about,visual facts,logic-based and commonsense - based consistent qa pairs about visual facts,0.6581724286079407
translation,82,113,model,model,introduced,convqa dataset,model introduced convqa dataset,0.6147038340568542
translation,82,117,model,consistent vqa ( convqa ) dataset,with,diverse qa pairs,consistent vqa ( convqa ) dataset with diverse qa pairs,0.6190510392189026
translation,82,117,model,diverse qa pairs,query,same visual fact,diverse qa pairs query same visual fact,0.7957218289375305
translation,82,117,model,model,construct,consistent vqa ( convqa ) dataset,model construct consistent vqa ( convqa ) dataset,0.7011422514915466
translation,82,118,model,consistency teacher module ( ctm ),improves,vqa consistency,consistency teacher module ( ctm ) improves vqa consistency,0.6940904259681702
translation,82,118,model,vqa consistency,by rewarding,consistent behavior,vqa consistency by rewarding consistent behavior,0.7158885598182678
translation,82,118,model,model,propose,consistency teacher module ( ctm ),model propose consistency teacher module ( ctm ),0.6530018448829651
translation,82,11,results,ctm,automatically generates,entailed ( or similar-intent ) questions,ctm automatically generates entailed ( or similar-intent ) questions,0.7124626040458679
translation,82,11,results,ctm,fine- tunes,vqa model,ctm fine- tunes vqa model,0.6989365220069885
translation,82,11,results,entailed ( or similar-intent ) questions,for,source qa pair,entailed ( or similar-intent ) questions for source qa pair,0.5796105265617371
translation,82,11,results,entailed ( or similar-intent ) questions,for,source qa pair,entailed ( or similar-intent ) questions for source qa pair,0.5796105265617371
translation,82,11,results,entailed ( or similar-intent ) questions,fine- tunes,vqa model,entailed ( or similar-intent ) questions fine- tunes vqa model,0.7240854501724243
translation,82,11,results,vqa model,if,vqa 's answer,vqa model if vqa 's answer,0.6443808674812317
translation,82,11,results,vqa 's answer,to,entailed question,vqa 's answer to entailed question,0.5687458515167236
translation,82,11,results,vqa 's answer,consistent with,source qa pair,vqa 's answer consistent with source qa pair,0.6228775978088379
translation,82,11,results,entailed question,consistent with,source qa pair,entailed question consistent with source qa pair,0.629923939704895
translation,82,11,results,results,has,ctm,results has ctm,0.5517452955245972
translation,82,89,results,high accuracy of classification,on,l-convqa test set,high accuracy of classification on l-convqa test set,0.5269083976745605
translation,82,89,results,consistency checker,has,high accuracy of classification,consistency checker has high accuracy of classification,0.5811206698417664
translation,82,89,results,l-convqa test set,has,90 % ),l-convqa test set has 90 % ),0.5976370573043823
translation,82,89,results,results,has,consistency checker,results has consistency checker,0.5741584300994873
translation,82,104,results,results,on,l-convqa and cs - convqa datasets,results on l-convqa and cs - convqa datasets,0.48985767364501953
translation,82,107,results,baseline vqa system,retains,high top - 1 accuracy,baseline vqa system retains high top - 1 accuracy,0.6348183155059814
translation,82,107,results,high top - 1 accuracy,on,convqa splits,high top - 1 accuracy on convqa splits,0.5110598206520081
translation,82,107,results,63.58 %,on,vqav2,63.58 % on vqav2,0.5570011138916016
translation,82,107,results,/ 60.03 %,on,l-convqa / cs - convqa,/ 60.03 % on l-convqa / cs - convqa,0.5531516671180725
translation,82,107,results,70.34 %,has,/ 60.03 %,70.34 % has / 60.03 %,0.5928815007209778
translation,82,107,results,results,has,baseline vqa system,results has baseline vqa system,0.5646296739578247
translation,82,109,results,finetuning,on,l-convqa,finetuning on l-convqa,0.5375905632972717
translation,82,109,results,18.43 % gains,in,perfect consistency,18.43 % gains in perfect consistency,0.5232441425323486
translation,82,109,results,perfect consistency,on,l-convqa test,perfect consistency on l-convqa test,0.5429204702377319
translation,82,109,results,results,has,finetuning,results has finetuning,0.5693389773368835
translation,83,8,experiments,our primary submission,ranked,fourth,our primary submission ranked fourth,0.6597042083740234
translation,83,8,experiments,fourth,with,map,fourth with map,0.6982133388519287
translation,83,8,experiments,fourth,with,accuracy,fourth with accuracy,0.6825265288352966
translation,83,8,experiments,map,of,13.48,map of 13.48,0.6064683794975281
translation,83,8,experiments,accuracy,of,97.08,accuracy of 97.08,0.5624498128890991
translation,83,8,experiments,main subtask c,has,our primary submission,main subtask c has our primary submission,0.5937591195106506
translation,83,9,experiments,our primary submission,get into,top 50 %,our primary submission get into top 50 %,0.5521225333213806
translation,83,9,experiments,subtask a,has,our primary submission,subtask a has our primary submission,0.5875667929649353
translation,83,107,experiments,svm classifier,choose,"different kernels ( moreno et al. , 2003 )","svm classifier choose different kernels ( moreno et al. , 2003 )",0.6557443141937256
translation,83,107,experiments,best results,with,rbf kernel,best results with rbf kernel,0.5963872671127319
translation,83,131,experiments,performance,of,our approach,performance of our approach,0.5712711215019226
translation,83,131,experiments,superior,in,subtask b,superior in subtask b,0.5345499515533447
translation,83,131,experiments,subtask b,has,questionrelated question similarity ranking,subtask b has questionrelated question similarity ranking,0.546430766582489
translation,83,68,hyperparameters,clusters,obtained from,word2vec model,clusters obtained from word2vec model,0.5315795540809631
translation,83,68,hyperparameters,word2vec model,trained on,qatarliving forums,word2vec model trained on qatarliving forums,0.7152302861213684
translation,83,68,hyperparameters,qatarliving forums,with,vector size,qatarliving forums with vector size,0.6290825605392456
translation,83,68,hyperparameters,vector size,of,100,vector size of 100,0.6704729199409485
translation,83,68,hyperparameters,minimum words frequency,of,5,minimum words frequency of 5,0.6868096590042114
translation,83,68,hyperparameters,window size,has,10,window size has 10,0.6384474635124207
translation,83,68,hyperparameters,skip-gram,has,1,skip-gram has 1,0.6325686573982239
translation,83,68,hyperparameters,hyperparameters,use,clusters,hyperparameters use clusters,0.6919768452644348
translation,83,70,hyperparameters,topic clustering,using,latent dirichlet allocation ( lda ),topic clustering using latent dirichlet allocation ( lda ),0.6701459288597107
translation,83,70,hyperparameters,latent dirichlet allocation ( lda ),on,train1 + train2 questions and comments,latent dirichlet allocation ( lda ) on train1 + train2 questions and comments,0.5415804982185364
translation,83,70,hyperparameters,hyperparameters,perform,topic clustering,hyperparameters perform topic clustering,0.5637660622596741
translation,83,71,hyperparameters,topic models,with,150 topics,topic models with 150 topics,0.6236602663993835
translation,83,71,hyperparameters,hyperparameters,build,topic models,hyperparameters build topic models,0.6383730173110962
translation,83,106,hyperparameters,logistic regression classifier,tune,classifier,logistic regression classifier tune classifier,0.7084605097770691
translation,83,106,hyperparameters,classifier,different values of,c ( cost ) parameter,classifier different values of c ( cost ) parameter,0.6822518110275269
translation,83,106,hyperparameters,best accuracy,on,10 - fold cross-validation,best accuracy on 10 - fold cross-validation,0.5252540707588196
translation,83,106,hyperparameters,10 - fold cross-validation,on,training set,10 - fold cross-validation on training set,0.5489258766174316
translation,83,106,hyperparameters,hyperparameters,For,logistic regression classifier,hyperparameters For logistic regression classifier,0.594482421875
translation,83,119,results,outperform,both,random and the search engine baseline,outperform both random and the search engine baseline,0.6670936346054077
translation,83,119,results,ranking measures,has,our system,ranking measures has our system,0.589733362197876
translation,83,119,results,our system,has,outperform,our system has outperform,0.6532697677612305
translation,83,119,results,results,In terms of,ranking measures,results In terms of ranking measures,0.6808158755302429
translation,83,120,results,map improvement,of,18.15 %,map improvement of 18.15 %,0.5460031628608704
translation,83,120,results,18.15 %,compare with,results,18.15 % compare with results,0.6142579317092896
translation,83,120,results,results,obtained by,search engine,results obtained by search engine,0.6055821776390076
translation,83,120,results,results,obtained by,search engine,results obtained by search engine,0.6055821776390076
translation,83,121,results,second rank,in,semeval - 2016,second rank in semeval - 2016,0.552007794380188
translation,83,121,results,performance,of,our approach,performance of our approach,0.5712711215019226
translation,83,121,results,superior,in,subtask b,superior in subtask b,0.5345499515533447
translation,83,121,results,results,obtain,second rank,results obtain second rank,0.6008255481719971
translation,83,122,results,test set,for,2016,test set for 2016,0.5921450853347778
translation,83,122,results,improvement,of,map and avgrec,improvement of map and avgrec,0.6103925108909607
translation,83,122,results,map and avgrec,of,"1.59 % , 2.37 %","map and avgrec of 1.59 % , 2.37 %",0.5639137029647827
translation,83,122,results,"1.59 % , 2.37 %",compare to,search engine baseline,"1.59 % , 2.37 % compare to search engine baseline",0.623951256275177
translation,83,122,results,test set,has,improvement,test set has improvement,0.6008899807929993
translation,83,122,results,results,using,test set,results using test set,0.6904959678649902
translation,83,124,results,second rank,in,semeval - 2016,second rank in semeval - 2016,0.552007794380188
translation,83,124,results,second rank,For,subtask c,second rank For subtask c,0.6414434909820557
translation,83,124,results,results,obtain,second rank,results obtain second rank,0.6008255481719971
translation,83,125,results,test set,for,2016,test set for 2016,0.5921450853347778
translation,83,125,results,improvement,of,map and avgrec,improvement of map and avgrec,0.6103925108909607
translation,83,125,results,map and avgrec,of,"8.21 % , 0.93 %","map and avgrec of 8.21 % , 0.93 %",0.5568430423736572
translation,83,125,results,"8.21 % , 0.93 %",compare to,search engine baseline,"8.21 % , 0.93 % compare to search engine baseline",0.6012758016586304
translation,83,125,results,test set,has,improvement,test set has improvement,0.6008899807929993
translation,83,125,results,2016,has,improvement,2016 has improvement,0.61678546667099
translation,83,125,results,results,Using,test set,results Using test set,0.6904959678649902
translation,83,128,results,subtask a,has,questioncomment similarity ranking,subtask a has questioncomment similarity ranking,0.5512027740478516
translation,83,128,results,results,of,subtask a,results of subtask a,0.5172532200813293
translation,83,129,results,outperform,both,random and the search engine baseline,outperform both random and the search engine baseline,0.6670936346054077
translation,83,129,results,ranking measures,has,our system,ranking measures has our system,0.589733362197876
translation,83,129,results,results,In terms of,ranking measures,results In terms of ranking measures,0.6808158755302429
translation,83,132,results,test set,for,"2017 ( nakov et al. , 2017 )","test set for 2017 ( nakov et al. , 2017 )",0.5740369558334351
translation,83,132,results,test set,obtain,map,test set obtain map,0.6161456108093262
translation,83,132,results,test set,obtain,avgrec,test set obtain avgrec,0.5850619077682495
translation,83,132,results,map,of,41.11 %,map of 41.11 %,0.5810853838920593
translation,83,132,results,avgrec,of,77.45,avgrec of 77.45,0.5698844194412231
translation,83,132,results,results,using,test set,results using test set,0.6904959678649902
translation,83,133,results,subtask c,Using,test set,subtask c Using test set,0.6712746024131775
translation,83,133,results,test set,improvement of,map and avgrec,test set improvement of map and avgrec,0.7141097187995911
translation,83,133,results,map and avgrec,is,"4.3 % , 2.72 %","map and avgrec is 4.3 % , 2.72 %",0.563048779964447
translation,83,133,results,"4.3 % , 2.72 %",compare to,search engine baseline,"4.3 % , 2.72 % compare to search engine baseline",0.6046072840690613
translation,83,133,results,test set,has,),test set has ),0.6315209865570068
translation,83,133,results,results,For,subtask c,results For subtask c,0.5909541249275208
translation,83,133,results,results,Using,test set,results Using test set,0.6904959678649902
translation,84,7,model,detailed examination data,such as,mouse cursor movements,detailed examination data such as mouse cursor movements,0.6063894033432007
translation,84,7,model,detailed examination data,such as,scrolling,detailed examination data such as scrolling,0.6827152967453003
translation,84,7,model,detailed examination data,to infer,parts of the document,detailed examination data to infer parts of the document,0.752838671207428
translation,84,7,model,detailed examination data,incorporate,signal,detailed examination data incorporate signal,0.6929014921188354
translation,84,7,model,signal,into,passage retrieval,signal into passage retrieval,0.6111698150634766
translation,84,7,model,passage retrieval,for,qa,passage retrieval for qa,0.6575896739959717
translation,84,7,model,parts of the document,has,searcher,parts of the document has searcher,0.5762529373168945
translation,84,7,model,searcher,has,found interesting,searcher has found interesting,0.5954692363739014
translation,84,7,model,model,exploit,detailed examination data,model exploit detailed examination data,0.743389904499054
translation,84,19,model,search behavior data,to improve,passage retrieval,search behavior data to improve passage retrieval,0.6226578950881958
translation,84,19,model,passage retrieval,for,automated question answering on the web,passage retrieval for automated question answering on the web,0.5255184173583984
translation,84,19,model,model,use of,search behavior data,model use of search behavior data,0.6167829632759094
translation,85,191,ablation-analysis,inference,on,unstructured data,inference on unstructured data,0.5498853325843811
translation,85,191,ablation-analysis,inference,helps,default model,inference helps default model,0.6133067011833191
translation,85,191,ablation-analysis,unstructured data,helps,default model,unstructured data helps default model,0.6507495641708374
translation,85,193,ablation-analysis,impact of joint el & re,see that,joint el & re,impact of joint el & re see that joint el & re,0.6756479740142822
translation,85,193,ablation-analysis,joint el & re,gives,performance boost,joint el & re gives performance boost,0.6544986963272095
translation,85,193,ablation-analysis,performance boost,of,3 %,performance boost of 3 %,0.6021906137466431
translation,85,193,ablation-analysis,3 %,from,44.1 to 47.1,3 % from 44.1 to 47.1,0.4725269377231598
translation,85,193,ablation-analysis,ablation analysis,has,impact of joint el & re,ablation analysis has impact of joint el & re,0.6112470030784607
translation,85,199,ablation-analysis,relation prediction accuracy,increases by,9.4 %,relation prediction accuracy increases by 9.4 %,0.7058718800544739
translation,85,199,ablation-analysis,9.4 %,from,45.9 % to 55.3 %,9.4 % from 45.9 % to 55.3 %,0.5156498551368713
translation,85,199,ablation-analysis,9.4 %,when using,joint inference,9.4 % when using joint inference,0.6932882070541382
translation,85,202,ablation-analysis,sentential features,found to be more important than,syntactic features,sentential features found to be more important than syntactic features,0.682034969329834
translation,85,202,ablation-analysis,ablation analysis,has,sentential features,ablation analysis has sentential features,0.5593320727348328
translation,85,143,experimental-setup,"libsvm ( chang and lin , 2011 )",to learn,weights,"libsvm ( chang and lin , 2011 ) to learn weights",0.6290427446365356
translation,85,143,experimental-setup,weights,for,classification,weights for classification,0.6385024785995483
translation,85,143,experimental-setup,experimental setup,use,"libsvm ( chang and lin , 2011 )","experimental setup use libsvm ( chang and lin , 2011 )",0.5885846614837646
translation,85,198,experiments,surrogate gold relations,to evaluate,performance,surrogate gold relations to evaluate performance,0.7174618244171143
translation,85,198,experiments,performance,of,re component,performance of re component,0.6644052267074585
translation,85,198,experiments,re component,on,development set,re component on development set,0.6193681955337524
translation,85,167,hyperparameters,word embeddings,with,turian et al . ( 2010 ) 's word representations,word embeddings with turian et al . ( 2010 ) 's word representations,0.5855494737625122
translation,85,167,hyperparameters,dimensions,set to,50,dimensions set to 50,0.7149037718772888
translation,85,167,hyperparameters,hyperparameters,initialize,word embeddings,hyperparameters initialize word embeddings,0.6877135038375854
translation,85,169,hyperparameters,window size,of,mccnn,window size of mccnn,0.5990110635757446
translation,85,169,hyperparameters,mccnn,set to,3,mccnn set to 3,0.7489882707595825
translation,85,169,hyperparameters,hyperparameters,has,window size,hyperparameters has window size,0.5170397162437439
translation,85,170,hyperparameters,sizes,of,hidden layer 1 and the hidden layer 2,sizes of hidden layer 1 and the hidden layer 2,0.6530803442001343
translation,85,170,hyperparameters,hidden layer 1 and the hidden layer 2,of,two mccnn channels,hidden layer 1 and the hidden layer 2 of two mccnn channels,0.6026094555854797
translation,85,170,hyperparameters,hidden layer 1 and the hidden layer 2,set to,200 and 100,hidden layer 1 and the hidden layer 2 set to 200 and 100,0.7637163400650024
translation,85,170,hyperparameters,two mccnn channels,set to,200 and 100,two mccnn channels set to 200 and 100,0.7521869540214539
translation,85,170,hyperparameters,hyperparameters,has,sizes,hyperparameters has sizes,0.5322444438934326
translation,85,7,model,neural network based relation extractor,to retrieve,candidate answers,neural network based relation extractor to retrieve candidate answers,0.7465466260910034
translation,85,7,model,candidate answers,from,freebase,candidate answers from freebase,0.5444135665893555
translation,85,7,model,infer,to validate,answers,infer to validate answers,0.63958740234375
translation,85,7,model,wikipedia,to validate,answers,wikipedia to validate answers,0.640957236289978
translation,85,7,model,model,present,neural network based relation extractor,model present neural network based relation extractor,0.6570213437080383
translation,85,256,model,relation extraction method,using,mccnn,relation extraction method using mccnn,0.6783913373947144
translation,85,256,model,mccnn,capable of exploiting,syntax,mccnn capable of exploiting syntax,0.7160729169845581
translation,85,256,model,syntax,in addition to,sentential features,syntax in addition to sentential features,0.6207106709480286
translation,85,256,model,model,introduced,relation extraction method,model introduced relation extraction method,0.6328744888305664
translation,85,190,results,joint el and re,performs,better,joint el and re performs better,0.6625683307647705
translation,85,190,results,better,than,default pipelined approach,better than default pipelined approach,0.6112295389175415
translation,85,190,results,semantic parsing based models,searches,partial logical forms,semantic parsing based models searches partial logical forms,0.5456839203834534
translation,85,190,results,partial logical forms,in,strategic order,partial logical forms in strategic order,0.5331390500068665
translation,85,190,results,partial logical forms,by combining,agenda- based parsing,partial logical forms by combining agenda- based parsing,0.6911624670028687
translation,85,190,results,outperforms,has,most,outperforms has most,0.6498701572418213
translation,85,190,results,outperforms,has,semantic parsing based models,outperforms has semantic parsing based models,0.6023321747779846
translation,85,190,results,most,has,semantic parsing based models,most has semantic parsing based models,0.6017778515815735
translation,85,190,results,results,see that,joint el and re,results see that joint el and re,0.668549120426178
translation,85,196,results,our entity linker,correctly find,gold standard topic entities,our entity linker correctly find gold standard topic entities,0.7029828429222107
translation,85,196,results,79.8 % questions,has,our entity linker,79.8 % questions has our entity linker,0.6071614027023315
translation,85,196,results,results,for,79.8 % questions,results for 79.8 % questions,0.5552169680595398
translation,85,197,results,joint inference,improves,result,joint inference improves result,0.6328954696655273
translation,85,197,results,result,to,83.2 %,result to 83.2 %,0.5171501636505127
translation,85,197,results,83.2 %,has,3.4 % improvement,83.2 % has 3.4 % improvement,0.5654501914978027
translation,85,197,results,results,has,joint inference,results has joint inference,0.48508965969085693
translation,85,200,results,impact of individual and joint channels,on,end qa performance,impact of individual and joint channels on end qa performance,0.5959392786026001
translation,85,200,results,results,on,impact of individual and joint channels,results on impact of individual and joint channels,0.5599272847175598
translation,85,200,results,results,has,impact of the syntactic and the sentential channels,results has impact of the syntactic and the sentential channels,0.5406278967857361
translation,85,204,results,both the channels,see,further improvements,both the channels see further improvements,0.636428952217102
translation,85,204,results,further improvements,than using,any one of the channels,further improvements than using any one of the channels,0.6697266101837158
translation,85,207,results,structured inference,augmented with,unstructured inference,structured inference augmented with unstructured inference,0.6781646609306335
translation,85,207,results,structured inference,see,improvement,structured inference see improvement,0.6155291795730591
translation,85,207,results,improvement,of,2.9 %,improvement of 2.9 %,0.5672125816345215
translation,85,207,results,2.9 %,from,44.1 % to 47.0 %,2.9 % from 44.1 % to 47.0 %,0.5182754993438721
translation,85,207,results,results,when,structured inference,results when structured inference,0.6038100719451904
translation,85,208,results,structured + joint,uses,unstructured inference,structured + joint uses unstructured inference,0.6010906100273132
translation,85,208,results,performance boosts,by,6.2 %,performance boosts by 6.2 %,0.5914413332939148
translation,85,208,results,performance boosts,achieving,new state - of - the - art result,performance boosts achieving new state - of - the - art result,0.6556560397148132
translation,85,208,results,6.2 %,from,47.1 % to 53.3 %,6.2 % from 47.1 % to 53.3 %,0.5146193504333496
translation,85,208,results,6.2 %,achieving,new state - of - the - art result,6.2 % achieving new state - of - the - art result,0.6218664646148682
translation,85,208,results,structured + joint,has,performance boosts,structured + joint has performance boosts,0.6273922324180603
translation,85,208,results,unstructured inference,has,performance boosts,unstructured inference has performance boosts,0.5543652176856995
translation,85,257,results,joint entity linking and relation extraction,along with,unstructured inference,joint entity linking and relation extraction along with unstructured inference,0.5794709324836731
translation,85,257,results,joint entity linking and relation extraction,achieves,state - of - the - art results,joint entity linking and relation extraction achieves state - of - the - art results,0.6106066107749939
translation,85,257,results,state - of - the - art results,on,webquestions dataset,state - of - the - art results on webquestions dataset,0.49776801466941833
translation,86,144,baselines,pgnet,is,pointer- generator network,pgnet is pointer- generator network,0.5907923579216003
translation,86,144,baselines,baselines,compare with,state- of - the - art baselines,baselines compare with state- of - the - art baselines,0.611987292766571
translation,86,141,hyperparameters,generating stage,set,maximum length,generating stage set maximum length,0.6864286065101624
translation,86,141,hyperparameters,maximum length,for,output sequence,maximum length for output sequence,0.6121011972427368
translation,86,141,hyperparameters,output sequence,as,15,output sequence as 15,0.5767678022384644
translation,86,141,hyperparameters,beam size k,set to,5,beam size k set to 5,0.7803327441215515
translation,86,141,hyperparameters,hyperparameters,employ,teacher - forcing training,hyperparameters employ teacher - forcing training,0.5427860617637634
translation,86,141,hyperparameters,hyperparameters,in,generating stage,hyperparameters in generating stage,0.4784744381904602
translation,86,8,model,end-to - end neural model,with,coreference alignment,end-to - end neural model with coreference alignment,0.6210051774978638
translation,86,8,model,end-to - end neural model,with,conversation flow modeling,end-to - end neural model with conversation flow modeling,0.6257813572883606
translation,86,8,model,model,propose,end-to - end neural model,model propose end-to - end neural model,0.6396893858909607
translation,86,9,model,coreference alignment modeling,explicitly aligns,coreferent mentions,coreference alignment modeling explicitly aligns coreferent mentions,0.6845994591712952
translation,86,9,model,coreferent mentions,in,conversation history,coreferent mentions in conversation history,0.4823043942451477
translation,86,9,model,conversation history,with,corresponding pronominal references,conversation history with corresponding pronominal references,0.5930268168449402
translation,86,9,model,corresponding pronominal references,in,generated questions,corresponding pronominal references in generated questions,0.5002943277359009
translation,86,9,model,model,has,coreference alignment modeling,model has coreference alignment modeling,0.5145796537399292
translation,86,10,model,conversation flow modeling,builds,coherent conversation,conversation flow modeling builds coherent conversation,0.6662774682044983
translation,86,10,model,conversation flow modeling,smoothly shifting,focus,conversation flow modeling smoothly shifting focus,0.7351734042167664
translation,86,10,model,coherent conversation,by starting,questioning,coherent conversation by starting questioning,0.6889247298240662
translation,86,10,model,coherent conversation,smoothly shifting,focus,coherent conversation smoothly shifting focus,0.816892683506012
translation,86,10,model,questioning,on,first few sentences,questioning on first few sentences,0.5653749704360962
translation,86,10,model,first few sentences,in,text passage,first few sentences in text passage,0.508431077003479
translation,86,10,model,focus,to,later parts,focus to later parts,0.6283074021339417
translation,86,10,model,model,has,conversation flow modeling,model has conversation flow modeling,0.5401861071586609
translation,86,40,model,conversational questions,propose,multi-source encoder,conversational questions propose multi-source encoder,0.6232848167419434
translation,86,40,model,multi-source encoder,to jointly encode,passage and the conversation so far,multi-source encoder to jointly encode passage and the conversation so far,0.7248601913452148
translation,86,40,model,model,To generate,conversational questions,model To generate conversational questions,0.7224117517471313
translation,86,45,model,conversation flow,to transit focus,smoothly,conversation flow to transit focus smoothly,0.8130671381950378
translation,86,45,model,smoothly,across,turns,smoothly across turns,0.7228105664253235
translation,86,45,model,conversations,has,coherent,conversations has coherent,0.6077455282211304
translation,86,45,model,model,make,conversations,model make conversations,0.7364379167556763
translation,86,45,model,model,model,conversation flow,model model conversation flow,0.7764334678649902
translation,86,46,model,conversation flow modeling,via,flow loss,conversation flow modeling via flow loss,0.6957855224609375
translation,86,46,model,model,has,conversation flow modeling,model has conversation flow modeling,0.5401861071586609
translation,86,150,results,nqg,outperforms,pgnet,nqg outperforms pgnet,0.7630237936973572
translation,86,150,results,pgnet,by,large margin,pgnet by large margin,0.604522705078125
translation,86,152,results,corefnet and flownet,has,outperform,corefnet and flownet has outperform,0.6013283729553223
translation,86,152,results,outperform,has,our base model,outperform has our base model,0.6215569376945496
translation,86,154,results,cfnet,is,significantly better,cfnet is significantly better,0.5957027077674866
translation,86,154,results,significantly better,than,"two baselines ( pgnet , nqg )","significantly better than two baselines ( pgnet , nqg )",0.5704773664474487
translation,86,154,results,significantly better,than,our msnet,significantly better than our msnet,0.5923582315444946
translation,86,154,results,significantly better,than,our corefnet,significantly better than our corefnet,0.5895465016365051
translation,86,154,results,results,has,cfnet,results has cfnet,0.5869991183280945
translation,86,155,results,difference,between,our cfnet,difference between our cfnet,0.7153100371360779
translation,86,155,results,difference,between,our flownet,difference between our flownet,0.692836582660675
translation,86,155,results,difference,is,not significant,difference is not significant,0.5336405634880066
translation,86,155,results,our flownet,is,not significant,our flownet is not significant,0.6197882890701294
translation,86,155,results,results,has,difference,results has difference,0.5636705756187439
translation,86,163,results,corefnet,significantly improves,"precision , recall , and fscore","corefnet significantly improves precision , recall , and fscore",0.6874946355819702
translation,86,163,results,"precision , recall , and fscore",of,predicted pronouns,"precision , recall , and fscore of predicted pronouns",0.5524057745933533
translation,86,163,results,coreference alignment,has,corefnet,coreference alignment has corefnet,0.5314110517501831
translation,86,163,results,results,With the help of,coreference alignment,results With the help of coreference alignment,0.657904863357544
translation,86,164,results,performance,on,n-gram overlapping metrics,performance on n-gram overlapping metrics,0.536018967628479
translation,86,164,results,results,has,performance,results has performance,0.5972660779953003
translation,86,187,results,all models,achieve,high scores,all models achieve high scores,0.615431547164917
translation,86,187,results,high scores,on,grammaticality,high scores on grammaticality,0.4745572805404663
translation,86,188,results,msnet and our cfnet,perform,well,msnet and our cfnet perform well,0.610556960105896
translation,86,188,results,well,on,answerability,well on answerability,0.534281849861145
translation,86,188,results,results,has,msnet and our cfnet,results has msnet and our cfnet,0.5769739747047424
translation,86,190,results,other two models,in terms of, ,other two models in terms of  ,0.7173215746879578
translation,86,190,results,other two models,in terms of,interconnectedness,other two models in terms of interconnectedness,0.6664178967475891
translation,86,190,results, ,by,large gap,  by large gap,0.6097261309623718
translation,86,190,results,interconnectedness,by,large gap,interconnectedness by large gap,0.558875322341919
translation,86,190,results,cfnet,has,outperforms,cfnet has outperforms,0.6408712267875671
translation,86,190,results,outperforms,has,other two models,outperforms has other two models,0.5785214304924011
translation,86,190,results, ,has,interconnectedness,  has interconnectedness,0.6032930016517639
translation,86,190,results,results,has,cfnet,results has cfnet,0.5869991183280945
translation,87,175,ablation-analysis,bi-linear alignment plus fusion,between,passage and question,bi-linear alignment plus fusion between passage and question,0.6572097539901733
translation,87,175,ablation-analysis,bi-linear alignment plus fusion,is,most critical,bi-linear alignment plus fusion is most critical,0.5783295631408691
translation,87,175,ablation-analysis,most critical,to,performance,most critical to performance,0.5627790689468384
translation,87,175,ablation-analysis,performance,on,both metrics,performance on both metrics,0.5063078999519348
translation,87,175,ablation-analysis,performance,results in,drop,performance results in drop,0.6607775688171387
translation,87,175,ablation-analysis,drop,of,nearly 15 %,drop of nearly 15 %,0.66147381067276
translation,87,175,ablation-analysis,ablation analysis,has,bi-linear alignment plus fusion,ablation analysis has bi-linear alignment plus fusion,0.5269890427589417
translation,87,177,ablation-analysis,elmo,accounts for,about 5 %,elmo accounts for about 5 %,0.7760571837425232
translation,87,177,ablation-analysis,about 5 %,of,performance degradation,about 5 % of performance degradation,0.6004483103752136
translation,87,177,ablation-analysis,effectiveness,of,language model,effectiveness of language model,0.605835497379303
translation,87,177,ablation-analysis,ablation analysis,has,elmo,ablation analysis has elmo,0.5639440417289734
translation,87,157,experimental-setup,adamax optimizer,with,mini-batch size,adamax optimizer with mini-batch size,0.6141434907913208
translation,87,157,experimental-setup,adamax optimizer,with,initial learning rate,adamax optimizer with initial learning rate,0.6096310615539551
translation,87,157,experimental-setup,mini-batch size,of,32,mini-batch size of 32,0.6429690718650818
translation,87,157,experimental-setup,initial learning rate,of,0.002,initial learning rate of 0.002,0.5725244283676147
translation,87,157,experimental-setup,experimental setup,use,adamax optimizer,experimental setup use adamax optimizer,0.6156953573226929
translation,87,158,experimental-setup,dropout rate,of,0.4,dropout rate of 0.4,0.5964629054069519
translation,87,158,experimental-setup,0.4,used for,all lstm layers,0.4 used for all lstm layers,0.6056516766548157
translation,87,158,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,87,161,experimental-setup,training process,takes,roughly 20 hours,training process takes roughly 20 hours,0.6424494385719299
translation,87,161,experimental-setup,roughly 20 hours,on,single nvidia tesla m40 gpu,roughly 20 hours on single nvidia tesla m40 gpu,0.5106571316719055
translation,87,161,experimental-setup,experimental setup,has,training process,experimental setup has training process,0.5342507362365723
translation,87,34,experiments,"elmo embeddings ( peters et al. , 2018 )",derived from,pre-trained language model,"elmo embeddings ( peters et al. , 2018 ) derived from pre-trained language model",0.5639343857765198
translation,87,34,experiments,"elmo embeddings ( peters et al. , 2018 )",shows,superior performance,"elmo embeddings ( peters et al. , 2018 ) shows superior performance",0.6231444478034973
translation,87,34,experiments,superior performance,in,wide range of nlp problems,superior performance in wide range of nlp problems,0.533286452293396
translation,87,5,model,attention and fusion,conducted,horizontally and vertically,attention and fusion conducted horizontally and vertically,0.648196816444397
translation,87,5,model,horizontally and vertically,across,layers,horizontally and vertically across layers,0.7596397399902344
translation,87,5,model,layers,at,different levels of granularity,layers at different levels of granularity,0.5622172355651855
translation,87,5,model,different levels of granularity,between,question and paragraph,different levels of granularity between question and paragraph,0.6613253951072693
translation,87,6,model,question and paragraph,with,fine- grained language embeddings,question and paragraph with fine- grained language embeddings,0.6425765752792358
translation,87,6,model,model,encode,question and paragraph,model encode question and paragraph,0.730289876461029
translation,87,7,model,multi-granularity fusion approach,to fully fuse,information,multi-granularity fusion approach to fully fuse information,0.7471474409103394
translation,87,7,model,information,from,both global and attended representations,information from both global and attended representations,0.5085412263870239
translation,87,7,model,model,proposes,multi-granularity fusion approach,model proposes multi-granularity fusion approach,0.6973668336868286
translation,87,31,model,hierarchical attention network,gradually focus,attention,hierarchical attention network gradually focus attention,0.718115508556366
translation,87,31,model,hierarchical attention network,capturing,relation,hierarchical attention network capturing relation,0.6969571709632874
translation,87,31,model,attention,on,right part,attention on right part,0.5678653120994568
translation,87,31,model,right part,of,answer boundary,right part of answer boundary,0.5890152454376221
translation,87,31,model,relation,between,question and passage,relation between question and passage,0.6571812033653259
translation,87,31,model,relation,at,different levels of granularity,relation at different levels of granularity,0.565535306930542
translation,87,31,model,question and passage,at,different levels of granularity,question and passage at different levels of granularity,0.5606192946434021
translation,87,31,model,model,propose,hierarchical attention network,model propose hierarchical attention network,0.6313820481300354
translation,87,32,model,encoder layer,where,pretrained language models,encoder layer where pretrained language models,0.5223783254623413
translation,87,32,model,recurrent neural networks,used to build,representation,recurrent neural networks used to build representation,0.6939890384674072
translation,87,32,model,representation,for,questions and passages,representation for questions and passages,0.6505308151245117
translation,87,32,model,attention layer,in which,hierarchical attention networks,attention layer in which hierarchical attention networks,0.5866621136665344
translation,87,32,model,relation,between,question and passage,relation between question and passage,0.6571812033653259
translation,87,32,model,relation,at,different levels of granularity,relation at different levels of granularity,0.565535306930542
translation,87,32,model,match layer,where,refined question and passage,match layer where refined question and passage,0.6134287118911743
translation,87,32,model,refined question and passage,matched under,pointer -network,refined question and passage matched under pointer -network,0.7785938382148743
translation,87,32,model,pointer -network,has,) answer boundary predictor,pointer -network has ) answer boundary predictor,0.6163197159767151
translation,87,32,model,model,consists of,three joint layers,model consists of three joint layers,0.6465239524841309
translation,87,33,model,encoder layer,to better represent,questions and passages,encoder layer to better represent questions and passages,0.6701309084892273
translation,87,33,model,encoder layer,combine,two different embeddings,encoder layer combine two different embeddings,0.645971417427063
translation,87,33,model,two different embeddings,to give,fundamental word representations,two different embeddings to give fundamental word representations,0.6245505809783936
translation,87,33,model,model,In,encoder layer,model In encoder layer,0.502424418926239
translation,87,35,model,representation - aware fusion method,to compute,output elmo embeddings,representation - aware fusion method to compute output elmo embeddings,0.6927435398101807
translation,87,35,model,model,design,representation - aware fusion method,model design representation - aware fusion method,0.5741595029830933
translation,87,38,model,attention layer,propose,hierarchical attention network,attention layer propose hierarchical attention network,0.6240143179893494
translation,87,38,model,hierarchical attention network,by leveraging,co-attention and self-attention mechanism,hierarchical attention network by leveraging co-attention and self-attention mechanism,0.7156490683555603
translation,87,38,model,co-attention and self-attention mechanism,to gradually focus,our attention,co-attention and self-attention mechanism to gradually focus our attention,0.7563384771347046
translation,87,38,model,our attention,on,best answer span,our attention on best answer span,0.5565761923789978
translation,87,38,model,model,In,attention layer,model In attention layer,0.49441075325012207
translation,87,39,model,aligned representations,with,global information,aligned representations with global information,0.6231734752655029
translation,87,39,model,global information,from,previous layer,global information from previous layer,0.5374175310134888
translation,87,39,model,additional fusion layer,to further refine,representations,additional fusion layer to further refine representations,0.7108729481697083
translation,87,160,model,reinforce loss,which take,f1 score,reinforce loss which take f1 score,0.6809952259063721
translation,87,160,model,f1 score,as,reward,f1 score as reward,0.5549424290657043
translation,87,160,model,reward,incorporated with,cross entropy loss,reward incorporated with cross entropy loss,0.6438223719596863
translation,87,160,model,fine-tuning,has,focal loss,fine-tuning has focal loss,0.5418883562088013
translation,87,160,model,model,During,fine-tuning,model During fine-tuning,0.6755337119102478
translation,87,189,model,attention layer,is,most important part,attention layer is most important part,0.5307679176330566
translation,87,189,model,most important part,of,framework,most important part of framework,0.6247848272323608
translation,87,43,results,proposed method,achieves,state - of - the - art results,proposed method achieves state - of - the - art results,0.6013035774230957
translation,87,43,results,state - of - the - art results,against,strong baselines,state - of - the - art results against strong baselines,0.6375096440315247
translation,87,43,results,results,has,proposed method,results has proposed method,0.5845219492912292
translation,87,44,results,our single model,achieves,79.2 % em,our single model achieves 79.2 % em,0.6846859455108643
translation,87,44,results,our single model,achieves,86.6 % f1 score,our single model achieves 86.6 % f1 score,0.6300998330116272
translation,87,44,results,86.6 % f1 score,on,hidden test set,86.6 % f1 score on hidden test set,0.5213977098464966
translation,87,44,results,ensemble model,boosts,performance,ensemble model boosts performance,0.7030006051063538
translation,87,44,results,performance,to,82.4 % em,performance to 82.4 % em,0.5566492080688477
translation,87,44,results,performance,to,88.6 % f1 score,performance to 88.6 % f1 score,0.5350993871688843
translation,87,44,results,results,has,our single model,results has our single model,0.5605776309967041
translation,87,166,results,proposed slqa + ensemble model,achieves,em score,proposed slqa + ensemble model achieves em score,0.6823455095291138
translation,87,166,results,proposed slqa + ensemble model,achieves,f1 score,proposed slqa + ensemble model achieves f1 score,0.6652408242225647
translation,87,166,results,proposed slqa + ensemble model,achieves,outperforming,proposed slqa + ensemble model achieves outperforming,0.6950010657310486
translation,87,166,results,em score,of,82.4,em score of 82.4,0.5309453010559082
translation,87,166,results,f1 score,of,88.6,f1 score of 88.6,0.5473417043685913
translation,87,166,results,outperforming,has,all previous approaches,outperforming has all previous approaches,0.6052617430686951
translation,87,166,results,results,has,proposed slqa + ensemble model,results has proposed slqa + ensemble model,0.5784822702407837
translation,87,171,results,proposed model,get,superior results,proposed model get superior results,0.5979325771331787
translation,87,171,results,superior results,than,all the other competing approaches,superior results than all the other competing approaches,0.5550758838653564
translation,87,180,results,outperforms,by,nearly 5 %,outperforms by nearly 5 %,0.6461060643196106
translation,87,180,results,standard lstm,by,nearly 5 %,standard lstm by nearly 5 %,0.5607292056083679
translation,87,180,results,multi-hop fusion,has,outperforms,multi-hop fusion has outperforms,0.5990875959396362
translation,87,180,results,outperforms,has,standard lstm,outperforms has standard lstm,0.5821800231933594
translation,87,180,results,results,shows that,multi-hop fusion,results shows that multi-hop fusion,0.7064215540885925
translation,87,187,results,different fusion methods,contribute,differently,different fusion methods contribute differently,0.673408567905426
translation,87,187,results,differently,to,final performances,differently to final performances,0.5745039582252502
translation,87,187,results,vector-based fusion method,performs,best,vector-based fusion method performs best,0.6111088991165161
translation,87,187,results,best,with,moderate parameter size,best with moderate parameter size,0.6296398639678955
translation,87,187,results,results,see that,different fusion methods,results see that different fusion methods,0.6350898742675781
translation,87,191,results,bilinear attention,which add,relu,bilinear attention which add relu,0.7115854620933533
translation,87,191,results,relu,after,linearly transforming,relu after linearly transforming,0.7141339182853699
translation,87,191,results,linearly transforming,does,significantly better,linearly transforming does significantly better,0.3226647675037384
translation,87,191,results,significantly better,than,others,significantly better than others,0.615851640701294
translation,87,191,results,results,find,bilinear attention,results find bilinear attention,0.5355808734893799
translation,87,193,results,steep and steady rise,in,accuracy,steep and steady rise in accuracy,0.5912445187568665
translation,87,193,results,steep and steady rise,as,number of layers,steep and steady rise as number of layers,0.5604310631752014
translation,87,193,results,accuracy,as,number of layers,accuracy as number of layers,0.5153481960296631
translation,87,193,results,increased,from,n = 1 to 3,increased from n = 1 to 3,0.6059374213218689
translation,87,193,results,results,see,steep and steady rise,results see steep and steady rise,0.6274325847625732
translation,88,148,baselines,"xlm -roberta ( conneau et al. , 2020 )",is,recent multilingual model,"xlm -roberta ( conneau et al. , 2020 ) is recent multilingual model",0.5729262828826904
translation,88,148,baselines,recent multilingual model,based on,roberta,recent multilingual model based on roberta,0.6118464469909668
translation,88,148,baselines,baselines,has,"xlm -roberta ( conneau et al. , 2020 )","baselines has xlm -roberta ( conneau et al. , 2020 )",0.5685074329376221
translation,88,167,baselines,results,from,mbert,results from mbert,0.558223307132721
translation,88,167,baselines,results,from,xlm -r base,results from xlm -r base,0.5396040678024292
translation,88,167,baselines,results,from,xlm -r,results from xlm -r,0.5139206647872925
translation,88,167,baselines,results,after,finetuning,results after finetuning,0.7041951417922974
translation,88,171,experimental-setup,mbert,on,e?s,mbert on e?s,0.7012720108032227
translation,88,171,experimental-setup,experimental setup,fine- tuned,mbert,experimental setup fine- tuned mbert,0.6909670829772949
translation,88,4,experiments,cross-lingual and multilingual question answering,has,for high school examinations,cross-lingual and multilingual question answering has for high school examinations,0.5162715315818787
translation,88,120,experiments,knowledge,contained in,pre-trained language model,knowledge contained in pre-trained language model,0.6133928298950195
translation,88,120,experiments,knowledge,i.e.,"xlm -r ( conneau et al. , 2020 )","knowledge i.e. xlm -r ( conneau et al. , 2020 )",0.74090975522995
translation,88,120,experiments,knowledge,use it as,answering mechanism,knowledge use it as answering mechanism,0.5342461466789246
translation,88,220,experiments,adam optimizer,with,"? 1 =0.9 , ? 2 =0.999 , and = 1e-08","adam optimizer with ? 1 =0.9 , ? 2 =0.999 , and = 1e-08",0.6198486685752869
translation,88,6,results,fine-grained evaluation framework,across,multiple languages and subjects,fine-grained evaluation framework across multiple languages and subjects,0.6445120573043823
translation,88,158,results,ir,better than,random guessing,ir better than random guessing,0.7209834456443787
translation,88,158,results,results,has,ir,results has ir,0.5195727944374084
translation,88,165,results,training,on,sciens,training on sciens,0.6225876212120056
translation,88,165,results,training,yields,+ 0.5 % improvement,training yields + 0.5 % improvement,0.7064326405525208
translation,88,165,results,sciens,yields,+ 0.5 % improvement,sciens yields + 0.5 % improvement,0.731315553188324
translation,88,165,results,+ 0.5 % improvement,on,e?s,+ 0.5 % improvement on e?s,0.6133570075035095
translation,88,165,results,results,see that,training,results see that training,0.6431185603141785
translation,88,166,results,2.4 % improvement,with,multilingual fine-tuning,2.4 % improvement with multilingual fine-tuning,0.59750896692276
translation,88,166,results,2.4 % improvement,with,+ 0.5 %,2.4 % improvement with + 0.5 %,0.6190133094787598
translation,88,166,results,multilingual fine-tuning,on,e?s,multilingual fine-tuning on e?s,0.6209796071052551
translation,88,166,results,+ 0.5 %,for,english,+ 0.5 % for english,0.6309714913368225
translation,88,166,results,results,see,2.4 % improvement,results see 2.4 % improvement,0.587775468826294
translation,88,168,results,capacity,of,model,capacity of model,0.585544764995575
translation,88,168,results,capacity,yields,improvements,capacity yields improvements,0.687987744808197
translation,88,168,results,model,yields,improvements,model yields improvements,0.7211374044418335
translation,88,168,results,xlm -r,scores,7.4 %,xlm -r scores 7.4 %,0.7395222783088684
translation,88,168,results,xlm -r,compared to,base version ( xlm -r base ),xlm -r compared to base version ( xlm -r base ),0.645259439945221
translation,88,168,results,higher,on,e?s,higher on e?s,0.7012547850608826
translation,88,168,results,more than 14 %,on,english datasets,more than 14 % on english datasets,0.5134956240653992
translation,88,168,results,more than 14 %,compared to,base version ( xlm -r base ),more than 14 % compared to base version ( xlm -r base ),0.7163530588150024
translation,88,168,results,improvements,has,xlm -r,improvements has xlm -r,0.6371486783027649
translation,88,168,results,7.4 %,has,higher,7.4 % has higher,0.567992091178894
translation,88,168,results,results,Increasing,capacity,results Increasing capacity,0.6452246904373169
translation,88,178,results,xlm -r as kb,see,small improvement,xlm -r as kb see small improvement,0.6529064774513245
translation,88,178,results,small improvement,over,random baseline,small improvement over random baseline,0.6809856295585632
translation,88,178,results,2 %,on,r12,2 % on r12,0.6557477712631226
translation,88,178,results,+ 1 %,on,e?s and arc challenge,+ 1 % on e?s and arc challenge,0.6368518471717834
translation,88,178,results,results,With,xlm -r as kb,results With xlm -r as kb,0.6261413097381592
translation,88,182,results,results,from,cross-lingual zero-shot transfer,results from cross-lingual zero-shot transfer,0.4899691343307495
translation,88,182,results,cross-lingual zero-shot transfer,compared to,english-only baseline en all,cross-lingual zero-shot transfer compared to english-only baseline en all,0.5878998041152954
translation,88,182,results,english-only baseline en all,from,xlm -r,english-only baseline en all from xlm -r,0.508502185344696
translation,88,182,results,xlm -r,fine-tuned on,scien,xlm -r fine-tuned on scien,0.7400899529457092
translation,88,182,results,results,from,cross-lingual zero-shot transfer,results from cross-lingual zero-shot transfer,0.4899691343307495
translation,88,187,results,gains,when,source language,gains when source language,0.7018067240715027
translation,88,187,results,source language,contains,more questions,source language contains more questions,0.6584485769271851
translation,88,187,results,more questions,from,largely represented and harder subjects,more questions from largely represented and harder subjects,0.5580025911331177
translation,88,187,results,results,see,gains,results see gains,0.5693391561508179
translation,88,206,results,additional training,on,english science qa,additional training on english science qa,0.517077624797821
translation,88,206,results,english science qa,in,lower school levels,english science qa in lower school levels,0.5159774422645569
translation,88,206,results,no significant effect,on,overall accuracy,no significant effect on overall accuracy,0.5035477876663208
translation,88,206,results,lower school levels,has,no significant effect,lower school levels has no significant effect,0.5536804795265198
translation,88,206,results,results,has,additional training,results has additional training,0.5642982125282288
translation,89,39,ablation-analysis,questions,generated by,initial decoder,questions generated by initial decoder,0.6787200570106506
translation,89,39,ablation-analysis,ablation analysis,inclusion of,refinement decoder,ablation analysis inclusion of refinement decoder,0.6480159759521484
translation,89,149,ablation-analysis,performance,of,preliminary decoder,performance of preliminary decoder,0.6355624198913574
translation,89,149,ablation-analysis,preliminary decoder,has,improves,preliminary decoder has improves,0.6416574716567993
translation,89,149,ablation-analysis,ead model,has,16.84 v/s 17.59 bleu,ead model has 16.84 v/s 17.59 bleu,0.6140022873878479
translation,89,156,ablation-analysis,named entity component score,in,q-metric,named entity component score in q-metric,0.4600294828414917
translation,89,156,ablation-analysis,named entity component score,increases from,32.42,named entity component score increases from 32.42,0.5997212529182434
translation,89,156,ablation-analysis,32.42,for,first draft,32.42 for first draft,0.5469021201133728
translation,89,156,ablation-analysis,first draft,to,37.81,first draft to 37.81,0.5368434190750122
translation,89,156,ablation-analysis,37.81,for,refined draft,37.81 for refined draft,0.6283164024353027
translation,89,156,ablation-analysis,ablation analysis,observe that,named entity component score,ablation analysis observe that named entity component score,0.5709730386734009
translation,89,113,experiments,answertagging,use,embedding size,answertagging use embedding size,0.6180530190467834
translation,89,113,experiments,embedding size,of,3,embedding size of 3,0.662369966506958
translation,89,132,experiments,ead,by,"7.83 % , 7.57 % , 15.25 % and 3.85 %","ead by 7.83 % , 7.57 % , 15.25 % and 3.85 %",0.5527097582817078
translation,89,132,experiments,ead,on,"hotpot - qa , drop and squad ( passage )","ead on hotpot - qa , drop and squad ( passage )",0.6066303849220276
translation,89,132,experiments,"7.83 % , 7.57 % , 15.25 % and 3.85 %",on,"squad ( sentence ) ,","7.83 % , 7.57 % , 15.25 % and 3.85 % on squad ( sentence ) ,",0.5448598861694336
translation,89,132,experiments,"7.83 % , 7.57 % , 15.25 % and 3.85 %",on,"hotpot - qa , drop and squad ( passage )","7.83 % , 7.57 % , 15.25 % and 3.85 % on hotpot - qa , drop and squad ( passage )",0.5610523819923401
translation,89,132,experiments,outperforms,has,ead,outperforms has ead,0.6429204344749451
translation,89,111,hyperparameters,pre-trained glove word embeddings,fixed during,training,pre-trained glove word embeddings fixed during training,0.6452945470809937
translation,89,111,hyperparameters,300 dimensional,has,pre-trained glove word embeddings,300 dimensional has pre-trained glove word embeddings,0.5388586521148682
translation,89,111,hyperparameters,hyperparameters,use,300 dimensional,hyperparameters use 300 dimensional,0.6190760135650635
translation,89,112,hyperparameters,character - level embeddings,initially use,20 dimensional embedding,character - level embeddings initially use 20 dimensional embedding,0.6555734872817993
translation,89,112,hyperparameters,20 dimensional embedding,for,characters,20 dimensional embedding for characters,0.6216485500335693
translation,89,112,hyperparameters,20 dimensional embedding,projected to,100 dimensions,20 dimensional embedding projected to 100 dimensions,0.6837263107299805
translation,89,112,hyperparameters,hyperparameters,For,character - level embeddings,hyperparameters For character - level embeddings,0.519154965877533
translation,89,114,hyperparameters,hidden size,for,all the lstms,hidden size for all the lstms,0.5796810388565063
translation,89,114,hyperparameters,hidden size,fixed to,512,hidden size fixed to 512,0.7677428126335144
translation,89,114,hyperparameters,hyperparameters,has,hidden size,hyperparameters has hidden size,0.535134494304657
translation,89,115,hyperparameters,"2layer , 1 - layer and 2 - layer stacked bilstm",for,"passage encoder , answer encoder and the decoders","2layer , 1 - layer and 2 - layer stacked bilstm for passage encoder , answer encoder and the decoders",0.6070650219917297
translation,89,115,hyperparameters,hyperparameters,use,"2layer , 1 - layer and 2 - layer stacked bilstm","hyperparameters use 2layer , 1 - layer and 2 - layer stacked bilstm",0.5547858476638794
translation,89,116,hyperparameters,"top 30 , 000 frequent words",as,vocabulary,"top 30 , 000 frequent words as vocabulary",0.5004115104675293
translation,89,116,hyperparameters,hyperparameters,take,"top 30 , 000 frequent words","hyperparameters take top 30 , 000 frequent words",0.5653762817382812
translation,89,117,hyperparameters,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,89,117,hyperparameters,learning rate,of,0.0004,learning rate of 0.0004,0.6013473868370056
translation,89,117,hyperparameters,10 epochs,using,cross entropy loss,10 epochs using cross entropy loss,0.634147584438324
translation,89,117,hyperparameters,hyperparameters,use,adam optimizer,hyperparameters use adam optimizer,0.6090166568756104
translation,89,118,hyperparameters,reward - refnet model,fine - tune,pretrained model,reward - refnet model fine - tune pretrained model,0.639034628868103
translation,89,118,hyperparameters,pretrained model,with,loss function,pretrained model with loss function,0.5583925247192383
translation,89,118,hyperparameters,loss function,for,3 epochs,loss function for 3 epochs,0.574183464050293
translation,89,118,hyperparameters,hyperparameters,For,reward - refnet model,hyperparameters For reward - refnet model,0.5228276252746582
translation,89,119,hyperparameters,best model,chosen based on,"bleu ( papineni et al. , 2002 ) score","best model chosen based on bleu ( papineni et al. , 2002 ) score",0.6211724877357483
translation,89,119,hyperparameters,"bleu ( papineni et al. , 2002 ) score",on,validation split,"bleu ( papineni et al. , 2002 ) score on validation split",0.508486807346344
translation,89,119,hyperparameters,hyperparameters,has,best model,hyperparameters has best model,0.5101531147956848
translation,89,120,hyperparameters,beam search decoding,with,beam size,beam search decoding with beam size,0.6320339441299438
translation,89,120,hyperparameters,beam size,of,5,beam size of 5,0.7073217034339905
translation,89,120,hyperparameters,hyperparameters,use,beam search decoding,hyperparameters use beam search decoding,0.6268737316131592
translation,89,8,model,method,tries to mimic,human process,method tries to mimic human process,0.6816175580024719
translation,89,8,model,of generating questions,by first creating,initial draft,of generating questions by first creating initial draft,0.6797943115234375
translation,89,8,model,human process,has,of generating questions,human process has of generating questions,0.59471595287323
translation,89,8,model,model,propose,method,model propose method,0.6280754208564758
translation,89,9,model,refine network ( refnet ),contains,two decoders,refine network ( refnet ) contains two decoders,0.6069368124008179
translation,89,9,model,model,propose,refine network ( refnet ),model propose refine network ( refnet ),0.6766570210456848
translation,89,10,model,second decoder,uses,dual attention network,second decoder uses dual attention network,0.5792292356491089
translation,89,10,model,dual attention network,pays attention to,original passage,dual attention network pays attention to original passage,0.7537591457366943
translation,89,10,model,dual attention network,pays attention to,question ( initial draft ),dual attention network pays attention to question ( initial draft ),0.7326691150665283
translation,89,10,model,question ( initial draft ),generated by,first decoder,question ( initial draft ) generated by first decoder,0.6632157564163208
translation,89,10,model,model,has,second decoder,model has second decoder,0.5928565859794617
translation,89,27,model,refine network ( refnet ),examines,initially generated question,refine network ( refnet ) examines initially generated question,0.6507697701454163
translation,89,27,model,refine network ( refnet ),performs,second pass,refine network ( refnet ) performs second pass,0.6357205510139465
translation,89,27,model,second pass,to generate,revised question,second pass to generate revised question,0.656609058380127
translation,89,27,model,model,propose,refine network ( refnet ),model propose refine network ( refnet ),0.6766570210456848
translation,89,28,model,reward - refnet,uses,explicit reward signals,reward - refnet uses explicit reward signals,0.5474482178688049
translation,89,28,model,explicit reward signals,to achieve,refinement,explicit reward signals to achieve refinement,0.6434969902038574
translation,89,28,model,refinement,focused on,specific properties,refinement focused on specific properties,0.7020734548568726
translation,89,28,model,specific properties,of,question,specific properties of question,0.6042734384536743
translation,89,28,model,specific properties,such as,fluency and answerability,specific properties such as fluency and answerability,0.6081258058547974
translation,89,28,model,model,propose,reward - refnet,model propose reward - refnet,0.6657964587211609
translation,89,29,model,refnet,is,seq2seq based model,refnet is seq2seq based model,0.5850898623466492
translation,89,29,model,seq2seq based model,comprises,two decoders,seq2seq based model comprises two decoders,0.684708833694458
translation,89,29,model,model,has,refnet,model has refnet,0.5613753795623779
translation,89,30,model,refinement decoder,takes,initial draft,refinement decoder takes initial draft,0.6368199586868286
translation,89,30,model,refinement decoder,generates,refined question,refinement decoder generates refined question,0.679161787033081
translation,89,30,model,initial draft,of,question,initial draft of question,0.5529006123542786
translation,89,30,model,question,generated by,preliminary decoder,question generated by preliminary decoder,0.7082314491271973
translation,89,30,model,question,by attending onto,passage,question by attending onto passage,0.7365501523017883
translation,89,30,model,question,by attending onto,initial draft,question by attending onto initial draft,0.7151434421539307
translation,89,30,model,input,along with,passage and answer,input along with passage and answer,0.6631625294685364
translation,89,30,model,refined question,by attending onto,passage,refined question by attending onto passage,0.7610701322555542
translation,89,30,model,refined question,by attending onto,initial draft,refined question by attending onto initial draft,0.7315489053726196
translation,89,30,model,initial draft,using,dual attention network,initial draft using dual attention network,0.6263590455055237
translation,89,30,model,model,has,refinement decoder,model has refinement decoder,0.6108982563018799
translation,89,191,model,refine networks ( refnet ),for,question generation,refine networks ( refnet ) for question generation,0.6199333071708679
translation,89,191,model,refine networks ( refnet ),focus on,refining and improving,refine networks ( refnet ) focus on refining and improving,0.7798318862915039
translation,89,191,model,refining and improving,has,initial version,refining and improving has initial version,0.5864323973655701
translation,89,191,model,model,proposed,refine networks ( refnet ),model proposed refine networks ( refnet ),0.7109629511833191
translation,89,35,results,experiments,show,proposed refnet model,experiments show proposed refnet model,0.6006013751029968
translation,89,35,results,existing state - of- the - art models,on,squad dataset,existing state - of- the - art models on squad dataset,0.5017332434654236
translation,89,35,results,squad dataset,by,12.3 % and 3.7 %,squad dataset by 12.3 % and 3.7 %,0.5755437612533569
translation,89,35,results,12.3 % and 3.7 %,on,bleu,12.3 % and 3.7 % on bleu,0.5646982789039612
translation,89,35,results,12.3 % and 3.7 %,given,relevant sentence and passage,12.3 % and 3.7 % given relevant sentence and passage,0.6441779136657715
translation,89,35,results,proposed refnet model,has,outperforms,proposed refnet model has outperforms,0.6115735769271851
translation,89,35,results,outperforms,has,existing state - of- the - art models,outperforms has existing state - of- the - art models,0.540584921836853
translation,89,36,results,state - of- the - art results,on,hotpot - qa and drop datasets,state - of- the - art results on hotpot - qa and drop datasets,0.5054826736450195
translation,89,36,results,hotpot - qa and drop datasets,with,im-provement,hotpot - qa and drop datasets with im-provement,0.6570022702217102
translation,89,36,results,im-provement,of,7.57 % and 15.25 %,im-provement of 7.57 % and 15.25 %,0.6299473643302917
translation,89,36,results,im-provement,over,single- decoder baseline ( on bleu ),im-provement over single- decoder baseline ( on bleu ),0.6417320966720581
translation,89,36,results,7.57 % and 15.25 %,over,single- decoder baseline ( on bleu ),7.57 % and 15.25 % over single- decoder baseline ( on bleu ),0.6139064431190491
translation,89,36,results,results,achieve,state - of- the - art results,results achieve state - of- the - art results,0.5771380066871643
translation,89,40,results,our human evaluation,of,questions,our human evaluation of questions,0.5493679046630859
translation,89,40,results,questions,generated by,reward - refnet,questions generated by reward - refnet,0.6963019967079163
translation,89,40,results,results,has,our human evaluation,results has our human evaluation,0.5391662120819092
translation,89,131,results,refnet,beats,existing state - of- the - art model,refnet beats existing state - of- the - art model,0.7092556953430176
translation,89,131,results,existing state - of- the - art model,by,"12.30 % , 9.74 % , 17.48 % , and 3.71 %","existing state - of- the - art model by 12.30 % , 9.74 % , 17.48 % , and 3.71 %",0.5229163765907288
translation,89,131,results,existing state - of- the - art model,on,"squad ( sentence ) , hotpot - qa , drop and squad ( passage ) dataset","existing state - of- the - art model on squad ( sentence ) , hotpot - qa , drop and squad ( passage ) dataset",0.48107266426086426
translation,89,131,results,bleu - 4 metric,has,refnet,bleu - 4 metric has refnet,0.5300450921058655
translation,89,131,results,results,On,bleu - 4 metric,results On bleu - 4 metric,0.51012122631073
translation,89,133,results,consistently better,than,existing models,consistently better than existing models,0.589640200138092
translation,89,133,results,existing models,across,all n-gram scores,existing models across all n-gram scores,0.6678929328918457
translation,89,133,results,all n-gram scores,has,"bleu , rouge -l and me - teor )","all n-gram scores has bleu , rouge -l and me - teor )",0.5673684477806091
translation,89,133,results,results,has,refnet,results has refnet,0.5666612386703491
translation,89,134,results,improvements,on,q-bleu4,improvements on q-bleu4,0.5300832390785217
translation,89,142,results,ead model,across,all three metrics,ead model across all three metrics,0.7273113131523132
translation,89,142,results,refnet model,has,outperforms,refnet model has outperforms,0.6235062479972839
translation,89,142,results,outperforms,has,ead model,outperforms has ead model,0.5999940633773804
translation,89,142,results,results,observed,refnet model,results observed refnet model,0.6751042008399963
translation,89,143,results,"over 68.6 % , 66.7 % and 64.2 %",of,generated questions,"over 68.6 % , 66.7 % and 64.2 % of generated questions",0.554655909538269
translation,89,143,results,generated questions,from,refnet,generated questions from refnet,0.5702686905860901
translation,89,143,results,answerable,compared to,ead model,answerable compared to ead model,0.6771224141120911
translation,89,143,results,results,has,"over 68.6 % , 66.7 % and 64.2 %","results has over 68.6 % , 66.7 % and 64.2 %",0.5170234441757202
translation,89,152,results,direct path ( attention network ),between,two decoders,direct path ( attention network ) between two decoders,0.6447083950042725
translation,89,152,results,improves,compared to,preliminary decoder,improves compared to preliminary decoder,0.6976686120033264
translation,89,152,results,refinement decoder,has,improves,refinement decoder has improves,0.6363762021064758
translation,89,152,results,results,add,direct path ( attention network ),results add direct path ( attention network ),0.5962057113647461
translation,89,153,results,both the initial and refined draft,using,qbleu4,both the initial and refined draft using qbleu4,0.7198832035064697
translation,89,153,results,results,evaluate,both the initial and refined draft,results evaluate both the initial and refined draft,0.5572628378868103
translation,89,157,results,qualitative analysis,shows,refnet model,qualitative analysis shows refnet model,0.660183846950531
translation,89,157,results,refnet model,generates,more elaborate questions,refnet model generates more elaborate questions,0.6809812188148499
translation,89,157,results,more elaborate questions,compared to,preliminary decoder,more elaborate questions compared to preliminary decoder,0.6657794117927551
translation,89,157,results,results,shows,refnet model,results shows refnet model,0.6785473227500916
translation,89,157,results,results,has,qualitative analysis,results has qualitative analysis,0.47837314009666443
translation,89,173,results,reward- refnet ( originality ),there is,improvement,reward- refnet ( originality ) there is improvement,0.619774580001831
translation,89,173,results,improvement,in,performance,improvement in performance,0.5151869058609009
translation,89,173,results,performance,where,overlap,performance where overlap,0.6571576595306396
translation,89,173,results,overlap,with,passage,overlap with passage,0.7192659378051758
translation,89,173,results,passage,was,less,passage was less,0.7320213913917542
translation,89,173,results,results,observe,reward- refnet ( originality ),results observe reward- refnet ( originality ),0.6145172715187073
translation,89,173,results,results,with,reward- refnet ( originality ),results with reward- refnet ( originality ),0.5899117588996887
translation,89,174,results,question,generated from,reward - refnet ( originality ),question generated from reward - refnet ( originality ),0.6303074359893799
translation,89,174,results,reward - refnet ( originality ),is,better,reward - refnet ( originality ) is better,0.538898766040802
translation,89,192,results,proposed refnet model,consisting of,preliminary decoder,proposed refnet model consisting of preliminary decoder,0.6975599527359009
translation,89,192,results,proposed refnet model,consisting of,refinement decoder,proposed refnet model consisting of refinement decoder,0.7094365358352661
translation,89,192,results,refinement decoder,with,dual attention network,refinement decoder with dual attention network,0.5925371646881104
translation,89,192,results,existing state - of- the - art models,on,"squad , hotpot - qa and drop datasets","existing state - of- the - art models on squad , hotpot - qa and drop datasets",0.48444369435310364
translation,89,192,results,dual attention network,has,outperforms,dual attention network has outperforms,0.6192395091056824
translation,89,192,results,outperforms,has,existing state - of- the - art models,outperforms has existing state - of- the - art models,0.540584921836853
translation,89,192,results,results,has,proposed refnet model,results has proposed refnet model,0.5800431966781616
translation,90,137,baselines,generate,has,attention generator model,generate has attention generator model,0.569450855255127
translation,90,139,baselines,baselines,has,ptr-generate,baselines has ptr-generate,0.6063989996910095
translation,90,142,baselines,pure pointer - based copy model,with,encoder,pure pointer - based copy model with encoder,0.6625327467918396
translation,90,142,baselines,pure pointer - based copy model,with,decoder,pure pointer - based copy model with decoder,0.6679149270057678
translation,90,142,baselines,concatenation,of,question and conversation history,concatenation of question and conversation history,0.6042134761810303
translation,90,142,baselines,words,from,input sentences,words from input sentences,0.5177467465400696
translation,90,142,baselines,ptr-net,has,pure pointer - based copy model,ptr-net has pure pointer - based copy model,0.5657662153244019
translation,90,142,baselines,baselines,has,ptr-net,baselines has ptr-net,0.5979989171028137
translation,90,143,baselines,pointer copy model,is,question reformulation model,pointer copy model is question reformulation model,0.5423990488052368
translation,90,143,baselines,ptr-copy,has,pointer copy model,ptr-copy has pointer copy model,0.5809167623519897
translation,90,143,baselines,baselines,has,ptr-copy,baselines has ptr-copy,0.5694226026535034
translation,90,157,baselines,lexical matching baseline model,outputting,sentence,lexical matching baseline model outputting sentence,0.6882461309432983
translation,90,157,baselines,sentence,in,paragraph,sentence in paragraph,0.5217270255088806
translation,90,157,baselines,highest cosine,for,question,highest cosine for question,0.6315680146217346
translation,90,157,baselines,pretrained infersent,has,lexical matching baseline model,pretrained infersent has lexical matching baseline model,0.5421959161758423
translation,90,157,baselines,infersent representation,has,highest cosine,infersent representation has highest cosine,0.5723854303359985
translation,90,158,baselines,logistic regression,trained by,vowpal wabbit dataset,logistic regression trained by vowpal wabbit dataset,0.7527304887771606
translation,90,158,baselines,logistic regression model,trained by,vowpal wabbit dataset,logistic regression model trained by vowpal wabbit dataset,0.7660229802131653
translation,90,158,baselines,vowpal wabbit dataset,with,simple matching features,vowpal wabbit dataset with simple matching features,0.5897243618965149
translation,90,158,baselines,vowpal wabbit dataset,with,bias features,vowpal wabbit dataset with bias features,0.5928117632865906
translation,90,158,baselines,vowpal wabbit dataset,with,contextual features,vowpal wabbit dataset with contextual features,0.5867056250572205
translation,90,158,baselines,"al. , 2007 )",with,bias features,"al. , 2007 ) with bias features",0.5873319506645203
translation,90,158,baselines,"al. , 2007 )",with,contextual features,"al. , 2007 ) with contextual features",0.553883969783783
translation,90,158,baselines,single-turn machine comprehension model,based on,"bidaf ( seo et al. , 2016 )","single-turn machine comprehension model based on bidaf ( seo et al. , 2016 )",0.6300684213638306
translation,90,158,baselines,"bidaf ( seo et al. , 2016 )",with,self-attention and contextualized embeddings,"bidaf ( seo et al. , 2016 ) with self-attention and contextualized embeddings",0.6222528219223022
translation,90,158,baselines,logistic regression,has,logistic regression model,logistic regression has logistic regression model,0.5267561674118042
translation,90,158,baselines,bidaf ++( no-ctx ),has,single-turn machine comprehension model,bidaf ++( no-ctx ) has single-turn machine comprehension model,0.6061082482337952
translation,90,158,baselines,baselines,has,logistic regression,baselines has logistic regression,0.52397620677948
translation,90,159,baselines,reformulated quac data,by,ptr-copy model,reformulated quac data by ptr-copy model,0.5672489404678345
translation,90,159,baselines,bert model,with,reformulated quac data,bert model with reformulated quac data,0.6374396681785583
translation,90,160,baselines,answersupervised question reformulation model,for,conversational machine comprehension,answersupervised question reformulation model for conversational machine comprehension,0.5434127449989319
translation,90,160,baselines,conversational machine comprehension,with,reinforcement learning technology,conversational machine comprehension with reinforcement learning technology,0.5779637098312378
translation,90,160,baselines,asqr,has,our asqr model,asqr has our asqr model,0.6240279674530029
translation,90,160,baselines,asqr,has,answersupervised question reformulation model,asqr has answersupervised question reformulation model,0.6150375008583069
translation,90,160,baselines,our asqr model,has,answersupervised question reformulation model,our asqr model has answersupervised question reformulation model,0.6293675303459167
translation,90,160,baselines,baselines,has,asqr,baselines has asqr,0.6016822457313538
translation,90,184,baselines,bidaf ++ w/ k-ctx,integrates,conversation history,bidaf ++ w/ k-ctx integrates conversation history,0.7620949745178223
translation,90,184,baselines,conversation history,by encoding,turn number,conversation history by encoding turn number,0.7405638694763184
translation,90,184,baselines,conversation history,by encoding,previous n answer locations,conversation history by encoding previous n answer locations,0.7757754325866699
translation,90,184,baselines,turn number,to,question embedding,turn number to question embedding,0.6067509055137634
translation,90,184,baselines,turn number,to,previous n answer locations,turn number to previous n answer locations,0.5943195819854736
translation,90,184,baselines,previous n answer locations,to,context embedding,previous n answer locations to context embedding,0.5087494254112244
translation,90,184,baselines,baselines,has,bidaf ++ w/ k-ctx,baselines has bidaf ++ w/ k-ctx,0.578303337097168
translation,90,186,baselines,"sdnet ( zhu et al. , 2018 )",prepends,previous questions and answers,"sdnet ( zhu et al. , 2018 ) prepends previous questions and answers",0.6873328685760498
translation,90,186,baselines,"sdnet ( zhu et al. , 2018 )",leverages,contextual embedding,"sdnet ( zhu et al. , 2018 ) leverages contextual embedding",0.6635213494300842
translation,90,186,baselines,previous questions and answers,to,current question,previous questions and answers to current question,0.5599139332771301
translation,90,186,baselines,contextual embedding,of,bert,contextual embedding of bert,0.6070605516433716
translation,90,186,baselines,contextual embedding,to obtain,understanding,contextual embedding to obtain understanding,0.6051889061927795
translation,90,186,baselines,understanding,of,conversation history,understanding of conversation history,0.47892558574676514
translation,90,186,baselines,baselines,has,"sdnet ( zhu et al. , 2018 )","baselines has sdnet ( zhu et al. , 2018 )",0.4818287789821625
translation,90,161,hyperparameters,reformulated data,by,asqr model,reformulated data by asqr model,0.6010381579399109
translation,90,161,hyperparameters,asqr model,to train,bert model,asqr model to train bert model,0.6584606170654297
translation,90,161,hyperparameters,hyperparameters,use,reformulated data,hyperparameters use reformulated data,0.6457754373550415
translation,90,6,model,model,propose,novel answer -supervised question reformulation ( asqr ),model propose novel answer -supervised question reformulation ( asqr ),0.6536263823509216
translation,90,7,model,asqr,utilizes,pointer-copy - based question reformulation model,asqr utilizes pointer-copy - based question reformulation model,0.5891125202178955
translation,90,7,model,pointer-copy - based question reformulation model,as,agent,pointer-copy - based question reformulation model as agent,0.5667081475257874
translation,90,7,model,pointer-copy - based question reformulation model,takes,action,pointer-copy - based question reformulation model takes action,0.6535727977752686
translation,90,7,model,action,to predict,next word,action to predict next word,0.7085745930671692
translation,90,7,model,action,observes,reward,action observes reward,0.6675983667373657
translation,90,7,model,reward,for,whole sentence state,reward for whole sentence state,0.5841965079307556
translation,90,7,model,whole sentence state,after generating,end-of-sequence token,whole sentence state after generating end-of-sequence token,0.6843587160110474
translation,90,7,model,model,has,asqr,model has asqr,0.6364644765853882
translation,90,38,model,answer -supervised question reformulation model,for,conversational machine comprehension,answer -supervised question reformulation model for conversational machine comprehension,0.5160757303237915
translation,90,38,model,asqr,has,answer -supervised question reformulation model,asqr has answer -supervised question reformulation model,0.5233371257781982
translation,90,38,model,model,present,asqr,model present asqr,0.6820709705352783
translation,90,39,model,novel pointer -copybased question reformulation model,takes,action,novel pointer -copybased question reformulation model takes action,0.6577167510986328
translation,90,39,model,action,to predict,next word,action to predict next word,0.7085745930671692
translation,90,39,model,model,At,asqr model,model At asqr model,0.5887433886528015
translation,90,185,model,flowqa,provides,flow mechanism,flowqa provides flow mechanism,0.7199311256408691
translation,90,185,model,flow mechanism,encodes,intermediate representation,flow mechanism encodes intermediate representation,0.7316850423812866
translation,90,185,model,intermediate representation,of,previous questions,intermediate representation of previous questions,0.5728977918624878
translation,90,185,model,previous questions,to,context embedding,previous questions to context embedding,0.5608949065208435
translation,90,185,model,context embedding,when processing,current question,context embedding when processing current question,0.7005716562271118
translation,90,185,model,model,has,flowqa,model has flowqa,0.6388723850250244
translation,90,152,results,inferior effect,of,ptr-generate and ptr-net models,inferior effect of ptr-generate and ptr-net models,0.6138253808021545
translation,90,152,results,ptr-generate and ptr-net models,over,our ptr-copy model,ptr-generate and ptr-net models over our ptr-copy model,0.6987000703811646
translation,90,152,results,our ptr-copy model,separately encoding,question q and the conversational history d,our ptr-copy model separately encoding question q and the conversational history d,0.7656261920928955
translation,90,152,results,question q and the conversational history d,are,better,question q and the conversational history d are better,0.6033878922462463
translation,90,152,results,better,than,concatenating,better than concatenating,0.6104626655578613
translation,90,152,results,results,has,inferior effect,results has inferior effect,0.5706220865249634
translation,90,153,results,our ptr-copy model,with,previous all question / answers history,our ptr-copy model with previous all question / answers history,0.6731878519058228
translation,90,153,results,previous all question / answers history,performing,well,previous all question / answers history performing well,0.6169475317001343
translation,90,153,results,well,proves that,our question reformulation model,well proves that our question reformulation model,0.6094987988471985
translation,90,153,results,our question reformulation model,can identify,key information,our question reformulation model can identify key information,0.7716658115386963
translation,90,153,results,key information,in the case of,topic switching,key information in the case of topic switching,0.6736230254173279
translation,90,153,results,key information,in the case of,longer sentences,key information in the case of longer sentences,0.6050436496734619
translation,90,153,results,accurately,in the case of,topic switching,accurately in the case of topic switching,0.7053636312484741
translation,90,153,results,key information,has,accurately,key information has accurately,0.5549468398094177
translation,90,155,results,reformulated data,by,our asqr model,reformulated data by our asqr model,0.6268316507339478
translation,90,155,results,our asqr model,are,more effective,our asqr model are more effective,0.5910599231719971
translation,90,155,results,more effective,for,conversational machine comprehension,more effective for conversational machine comprehension,0.5531221628189087
translation,90,155,results,conversational machine comprehension,in,all quac dataset,conversational machine comprehension in all quac dataset,0.4969808757305145
translation,90,155,results,results,validate,reformulated data,results validate reformulated data,0.5611647963523865
translation,90,169,results,heq -d ( 2.9 ) scores,over,baseline models,heq -d ( 2.9 ) scores over baseline models,0.5922644734382629
translation,90,169,results,asqr model,has,best f1 ( 53.7 ),asqr model has best f1 ( 53.7 ),0.5325068831443787
translation,90,169,results,results,has,asqr model,results has asqr model,0.5482696294784546
translation,90,171,results,bert,trained with,original official quac dataset,bert trained with original official quac dataset,0.7422602772712708
translation,90,171,results,bert,observe,2.6 - improvement,bert observe 2.6 - improvement,0.6745291948318481
translation,90,171,results,2.6 - improvement,on,f1 score,2.6 - improvement on f1 score,0.514378547668457
translation,90,171,results,results,Compared with,bert,results Compared with bert,0.5586193799972534
translation,90,172,results,model ptr- copy- bert ( all - qa ),with,all question / answers history,model ptr- copy- bert ( all - qa ) with all question / answers history,0.6305685043334961
translation,90,172,results,all question / answers history,over,model ptr-copy - bert ( 4 - qa ),all question / answers history over model ptr-copy - bert ( 4 - qa ),0.6730073094367981
translation,90,172,results,model ptr-copy - bert ( 4 - qa ),with,part of conversational history,model ptr-copy - bert ( 4 - qa ) with part of conversational history,0.6472573280334473
translation,90,172,results,part of conversational history,has,good performance,part of conversational history has good performance,0.5715897679328918
translation,90,172,results,part of conversational history,has,good performance,part of conversational history has good performance,0.5715897679328918
translation,90,172,results,results,has,model ptr- copy- bert ( all - qa ),results has model ptr- copy- bert ( all - qa ),0.5706417560577393
translation,90,173,results,best performance,on,f1 and heq - q score,best performance on f1 and heq - q score,0.5401894450187683
translation,90,173,results,f1 and heq - q score,of,our asqr model,f1 and heq - q score of our asqr model,0.5859039425849915
translation,90,173,results,our asqr model,compared with,ptr-copy - bert models,our asqr model compared with ptr-copy - bert models,0.6788361668586731
translation,90,173,results,ptr-copy - bert models,prove,our answer -supervised training method,ptr-copy - bert models prove our answer -supervised training method,0.6521463990211487
translation,90,173,results,our answer -supervised training method,is,more effective,our answer -supervised training method is more effective,0.5220018029212952
translation,90,173,results,more effective,than,traditional question label - supervised method,more effective than traditional question label - supervised method,0.5621130466461182
translation,90,173,results,results,has,best performance,results has best performance,0.5759831070899963
translation,91,88,baselines,text encoder,named,q-bert,text encoder named q-bert,0.7225618362426758
translation,91,88,baselines,text encoder,takes,qa pairs,text encoder takes qa pairs,0.7087437510490417
translation,91,88,baselines,q- bert,has,text encoder,q- bert has text encoder,0.5335330963134766
translation,91,88,baselines,baselines,has,q- bert,baselines has q- bert,0.6506785750389099
translation,91,94,baselines,v- bert,concatenate,each qa pair,v- bert concatenate each qa pair,0.733375072479248
translation,91,94,baselines,each qa pair,with,video,each qa pair with video,0.7066353559494019
translation,91,94,baselines,video,to input to,our visual encoder v-bert,video to input to our visual encoder v-bert,0.7313830852508545
translation,91,94,baselines,baselines,has,v- bert,baselines has v- bert,0.657285213470459
translation,91,155,experimental-setup,lower - cased english text,with,masked language modeling task,lower - cased english text with masked language modeling task,0.6082843542098999
translation,91,155,experimental-setup,experimental setup,has,"q-bert , v-bert and s-bert","experimental setup has q-bert , v-bert and s-bert",0.547964334487915
translation,91,157,experimental-setup,12 heads ( h=12 ),for,multi-head attention,12 heads ( h=12 ) for multi-head attention,0.6037895083427429
translation,91,157,experimental-setup,multi-head attention,in,mmft module,multi-head attention in mmft module,0.5351435542106628
translation,91,157,experimental-setup,experimental setup,use,12 heads ( h=12 ),experimental setup use 12 heads ( h=12 ),0.5903350114822388
translation,91,158,experimental-setup,mmft module,with,random weights,mmft module with random weights,0.6512386798858643
translation,91,158,experimental-setup,experimental setup,initialize,mmft module,experimental setup initialize mmft module,0.7297983169555664
translation,91,167,experimental-setup,architecture,implemented using,"pytorch ( paszke et al. , 2019 ) framework","architecture implemented using pytorch ( paszke et al. , 2019 ) framework",0.6616658568382263
translation,91,169,experimental-setup,weight decay,set to,1e - 5,weight decay set to 1e - 5,0.7004510164260864
translation,91,169,experimental-setup,experimental setup,has,weight decay,experimental setup has weight decay,0.4727212190628052
translation,91,170,experimental-setup,experiments,performed under,cuda acceleration,experiments performed under cuda acceleration,0.625547468662262
translation,91,170,experimental-setup,cuda acceleration,with,two nvidia turing ( 24 gb of memory ) gpus,cuda acceleration with two nvidia turing ( 24 gb of memory ) gpus,0.6228428483009338
translation,91,170,experimental-setup,experimental setup,performed under,cuda acceleration,experimental setup performed under cuda acceleration,0.6315157413482666
translation,91,170,experimental-setup,experimental setup,has,experiments,experimental setup has experiments,0.5502888560295105
translation,91,175,experimental-setup,experimental setup,trained for,10 epochs,experimental setup trained for 10 epochs,0.7368022203445435
translation,91,32,experiments,pretrained language - based transformer model,to solve,vqa task,pretrained language - based transformer model to solve vqa task,0.5704358220100403
translation,91,32,experiments,pretrained language - based transformer model,has,"bert ( devlin et al. , 2018 )","pretrained language - based transformer model has bert ( devlin et al. , 2018 )",0.5080716013908386
translation,91,168,experiments,"adam optimizer ( kingma and ba , 2014 )",with,minibatch size,"adam optimizer ( kingma and ba , 2014 ) with minibatch size",0.5705241560935974
translation,91,168,experiments,"adam optimizer ( kingma and ba , 2014 )",with,learning rate,"adam optimizer ( kingma and ba , 2014 ) with learning rate",0.5956289768218994
translation,91,168,experiments,minibatch size,of,8,minibatch size of 8,0.6362479329109192
translation,91,168,experiments,learning rate,of,2e - 5,learning rate of 2e - 5,0.6410878896713257
translation,91,203,experiments,two-stream,by,10.08 %,two-stream by 10.08 %,0.5746749639511108
translation,91,203,experiments,drops,by,0.43 %,drops by 0.43 %,0.6447951197624207
translation,91,203,experiments,0.43 %,compared to,wacv20,0.43 % compared to wacv20,0.6813237071037292
translation,91,203,experiments,our method,has,outperforms,our method has outperforms,0.6322360634803772
translation,91,203,experiments,outperforms,has,two-stream,outperforms has two-stream,0.60626220703125
translation,91,208,experiments,tvqa - visual ( clean ),has,relevant visual concepts,tvqa - visual ( clean ) has relevant visual concepts,0.5869532823562622
translation,91,208,experiments,tvqa - visual ( clean ),has,related concepts,tvqa - visual ( clean ) has related concepts,0.5895228385925293
translation,91,208,experiments,related concepts,to,answer,related concepts to answer,0.5752824544906616
translation,91,208,experiments,answer,in,input,answer in input,0.5348766446113586
translation,91,208,experiments,tvqa - visual ( clean ),has,relevant visual concepts,tvqa - visual ( clean ) has relevant visual concepts,0.5869532823562622
translation,91,5,model,multimodal data ( video and text ),adopting,bert encodings individually,multimodal data ( video and text ) adopting bert encodings individually,0.7300782203674316
translation,91,5,model,multimodal data ( video and text ),using,novel transformerbased fusion method,multimodal data ( video and text ) using novel transformerbased fusion method,0.6765496730804443
translation,91,5,model,model,processing,multimodal data ( video and text ),model processing multimodal data ( video and text ),0.7441424131393433
translation,91,6,model,different sources of modalities,into,different bert instances,different sources of modalities into different bert instances,0.5707277059555054
translation,91,6,model,different sources of modalities,into,variable weights,different sources of modalities into variable weights,0.5415863394737244
translation,91,6,model,different bert instances,with,similar architectures,different bert instances with similar architectures,0.6508442163467407
translation,91,6,model,different bert instances,with,variable weights,different bert instances with variable weights,0.6141713857650757
translation,91,37,model,separate bert encoders,to process,input modalities,separate bert encoders to process input modalities,0.6637756824493408
translation,91,37,model,input modalities,namely,q-bert,input modalities namely q-bert,0.7326340079307556
translation,91,37,model,input modalities,namely,v-bert,input modalities namely v-bert,0.7387041449546814
translation,91,37,model,input modalities,namely,s-bert,input modalities namely s-bert,0.7288762927055359
translation,91,37,model,model,use,separate bert encoders,model use separate bert encoders,0.6816956996917725
translation,91,38,model,bert encoder,takes,input source,bert encoder takes input source,0.6510669589042664
translation,91,38,model,input source,with,question,input source with question,0.6048568487167358
translation,91,38,model,input source,with,candidate answer,input source with candidate answer,0.616256594657898
translation,91,38,model,model,has,bert encoder,model has bert encoder,0.6133244037628174
translation,91,40,model,question and candidate answers,enables,each stream,question and candidate answers enables each stream,0.7063073515892029
translation,91,40,model,each stream,to attend to,relevant knowledge,each stream to attend to relevant knowledge,0.7158532738685608
translation,91,40,model,relevant knowledge,pertinent to,question,relevant knowledge pertinent to question,0.7092496752738953
translation,91,40,model,relevant knowledge,by using,multi-head attention mechanism,relevant knowledge by using multi-head attention mechanism,0.6309930682182312
translation,91,40,model,multi-head attention mechanism,between,question words and a source modality,multi-head attention mechanism between question words and a source modality,0.5896204113960266
translation,91,40,model,model,pairing up,question and candidate answers,model pairing up question and candidate answers,0.7761525511741638
translation,91,41,model,novel transformer based fusion mechanism,to jointly attend to,aggregated knowledge,novel transformer based fusion mechanism to jointly attend to aggregated knowledge,0.6312369108200073
translation,91,41,model,aggregated knowledge,from,each input source,aggregated knowledge from each input source,0.5563876628875732
translation,91,41,model,model,use,novel transformer based fusion mechanism,model use novel transformer based fusion mechanism,0.6428550481796265
translation,91,42,model,two levels,of,question - to - input attention,two levels of question - to - input attention,0.5676249861717224
translation,91,42,model,each bert encoder,to select,relevant input,each bert encoder to select relevant input,0.7217314839363098
translation,91,42,model,fusion level,to fuse,all sources,fusion level to fuse all sources,0.7348945736885071
translation,91,42,model,all sources,to answer,common question,all sources to answer common question,0.6766741275787354
translation,91,42,model,model,using,two levels,model using two levels,0.7219799160957336
translation,91,44,model,novel multi-stream end-to - end trainable architecture,processes,each input source separately,novel multi-stream end-to - end trainable architecture processes each input source separately,0.6413614749908447
translation,91,44,model,each input source separately,followed by,feature fusion,each input source separately followed by feature fusion,0.6544508337974548
translation,91,44,model,feature fusion,over,aggregated source features,feature fusion over aggregated source features,0.6592695116996765
translation,91,44,model,model,propose,novel multi-stream end-to - end trainable architecture,model propose novel multi-stream end-to - end trainable architecture,0.6384749412536621
translation,91,45,model,objective function,to optimize,multiple berts jointly,objective function to optimize multiple berts jointly,0.7205707430839539
translation,91,45,model,model,define,objective function,model define objective function,0.6479346752166748
translation,91,47,model,fusion,among,multiple modalities,fusion among multiple modalities,0.6100616455078125
translation,91,47,model,model,propose,novel multimodal fusion transformer ( mmft ) module,model propose novel multimodal fusion transformer ( mmft ) module,0.6724094152450562
translation,91,74,model,separate bert encoder,for,question - answer pair,separate bert encoder for question - answer pair,0.6604054570198059
translation,91,74,model,model,use,separate bert encoder,model use separate bert encoder,0.6619503498077393
translation,91,78,model,bert encodings ( mmft - bert ),to solve,vqa,bert encodings ( mmft - bert ) to solve vqa,0.6435189843177795
translation,91,78,model,vqa,in,videos,vqa in videos,0.5317301154136658
translation,91,156,model,mmft module,uses,single transformer encoder layer ( l=1 ),mmft module uses single transformer encoder layer ( l=1 ),0.5979792475700378
translation,91,156,model,single transformer encoder layer ( l=1 ),with,multi-head attention,single transformer encoder layer ( l=1 ) with multi-head attention,0.6471297144889832
translation,91,156,model,model,has,mmft module,model has mmft module,0.5699328184127808
translation,91,43,results,experiments,using,q-bert,experiments using q-bert,0.7261287569999695
translation,91,43,results,separate bert encoder,for,question and answer,separate bert encoder for question and answer,0.6344984173774719
translation,91,43,results,separate bert encoder,is,helpful,separate bert encoder is helpful,0.6302595734596252
translation,91,43,results,question and answer,is,helpful,question and answer is helpful,0.5783145427703857
translation,91,43,results,experiments,has,separate bert encoder,experiments has separate bert encoder,0.6362429261207581
translation,91,43,results,q-bert,has,separate bert encoder,q-bert has separate bert encoder,0.5934424996376038
translation,91,43,results,results,using,q-bert,results using q-bert,0.6386352181434631
translation,91,181,results,consistently better results,when using,localized visual concepts and subtitles,consistently better results when using localized visual concepts and subtitles,0.7222184538841248
translation,91,181,results,results,get,consistently better results,results get consistently better results,0.5994139313697815
translation,91,182,results,1.7 % and 0.65 %,with,simple fusion,1.7 % and 0.65 % with simple fusion,0.6444534659385681
translation,91,182,results,improvement,over,"wacv20 ( yang et al. , 2020 )","improvement over wacv20 ( yang et al. , 2020 )",0.661742091178894
translation,91,182,results,simple fusion,for,q+v and q+v+s inputs,simple fusion for q+v and q+v+s inputs,0.6345550417900085
translation,91,182,results,1.7 % and 0.65 %,has,improvement,1.7 % and 0.65 % has improvement,0.5204978585243225
translation,91,182,results,results,get,1.7 % and 0.65 %,results get 1.7 % and 0.65 %,0.49178430438041687
translation,91,183,results,mmft,for,fusion,mmft for fusion,0.6482065320014954
translation,91,183,results,our method,achieves,sota performance,our method achieves sota performance,0.7084254622459412
translation,91,183,results,sota performance,with,all three input settings,sota performance with all three input settings,0.6143556237220764
translation,91,183,results,mmft,has,our method,mmft has our method,0.5961098074913025
translation,91,183,results,fusion,has,our method,fusion has our method,0.6162419319152832
translation,91,183,results,results,When using,mmft,results When using mmft,0.6601945757865906
translation,91,184,results,our fusion approach,contributes to,improved performance,our fusion approach contributes to improved performance,0.7509507536888123
translation,91,184,results,our fusion approach,gives,best results,our fusion approach gives best results,0.6189393401145935
translation,91,184,results,best results,for,localized input,best results for localized input,0.6339362859725952
translation,91,184,results,results,has,our fusion approach,results has our fusion approach,0.595710039138794
translation,91,186,results,our method,with,simple fusion and mmft,our method with simple fusion and mmft,0.6449373364448547
translation,91,186,results,simple fusion and mmft,on,q+v input,simple fusion and mmft on q+v input,0.5622007846832275
translation,91,186,results,simple fusion and mmft,outperforms,two-stream,simple fusion and mmft outperforms two-stream,0.7427276372909546
translation,91,186,results,simple fusion and mmft,by,absolute 6.49 % and 5.59 %,simple fusion and mmft by absolute 6.49 % and 5.59 %,0.5818356275558472
translation,91,186,results,q+v input,outperforms,two-stream,q+v input outperforms two-stream,0.7812924385070801
translation,91,186,results,absolute 6.49 % and 5.59 %,with,simple fusion and mmft,absolute 6.49 % and 5.59 % with simple fusion and mmft,0.6580023765563965
translation,91,186,results,results,has,our method,results has our method,0.5589964985847473
translation,91,191,results,q+s and q +v+s,observe,69.92 % and 65.55 %,q+s and q +v+s observe 69.92 % and 65.55 %,0.62320476770401
translation,91,191,results,q+s and q +v+s,using,mmft,q+s and q +v+s using mmft,0.7477123737335205
translation,91,191,results,69.92 % and 65.55 %,with,simple fusion,69.92 % and 65.55 % with simple fusion,0.6581425070762634
translation,91,191,results,69.92 % and 65.55 %,using,mmft,69.92 % and 65.55 % using mmft,0.6676103472709656
translation,91,191,results,mmft,produces,69.98 % and 66.10 % val,mmft produces 69.98 % and 66.10 % val,0.5921959280967712
translation,91,191,results,results,For,q+s and q +v+s,results For q+s and q +v+s,0.6351822018623352
translation,91,196,results,mmft,improves,results,mmft improves results,0.5767714977264404
translation,91,196,results,results,by,( ? 6.39 % ),results by ( ? 6.39 % ),0.5342599749565125
translation,91,196,results,( ? 6.39 % ),on,q+v.,( ? 6.39 % ) on q+v.,0.597183883190155
translation,91,196,results,( ? 6.39 % ),For,q+v+s,( ? 6.39 % ) For q+v+s,0.6671595573425293
translation,91,196,results,q+v.,For,q+v+s,q+v. For q+v+s,0.6935672163963318
translation,91,196,results,wacv20,reported,73.57 % accuracy,wacv20 reported 73.57 % accuracy,0.6418716907501221
translation,91,196,results,73.57 % accuracy,with,different input arrangement,73.57 % accuracy with different input arrangement,0.6183479428291321
translation,91,196,results,different input arrangement,than,mmft,different input arrangement than mmft,0.6180720329284668
translation,91,196,results,q+v+s,has,wacv20,q+v+s has wacv20,0.670600950717926
translation,91,197,results,model,with,same input,model with same input,0.7032061815261841
translation,91,197,results,mmft,performs,slightly better ( ? 0.17 % ),mmft performs slightly better ( ? 0.17 % ),0.6042794585227966
translation,91,197,results,model,has,mmft,model has mmft,0.615628719329834
translation,91,197,results,same input,has,mmft,same input has mmft,0.603508710861206
translation,91,197,results,results,compared with,model,results compared with model,0.6857913732528687
translation,91,207,results,( ? 11.46 % ) and ( ? 4.17 % ) improvement,for,clean set,( ? 11.46 % ) and ( ? 4.17 % ) improvement for clean set,0.6566123962402344
translation,91,207,results,results,observe,( ? 11.46 % ) and ( ? 4.17 % ) improvement,results observe ( ? 11.46 % ) and ( ? 4.17 % ) improvement,0.5627934336662292
translation,92,36,experiments,number of different models,perform,well,number of different models perform well,0.5999621748924255
translation,92,36,experiments,well,on,babi tasks,well on babi tasks,0.5477787852287292
translation,92,36,experiments,relation - network,has,"santoro et al. , 2017 )","relation - network has santoro et al. , 2017 )",0.5372987389564514
translation,92,146,hyperparameters,memory size,of,50,memory size of 50,0.6551872491836548
translation,92,146,hyperparameters,50,to ensure,memory,50 to ensure memory,0.731320858001709
translation,92,146,hyperparameters,memory,contains,all sentences,memory contains all sentences,0.63592928647995
translation,92,35,model,new dataset,evaluate,model 's capacity,new dataset evaluate model 's capacity,0.6450997591018677
translation,92,35,model,model 's capacity,to reason about,different types of beliefs,model 's capacity to reason about different types of beliefs,0.7346442341804504
translation,92,35,model,model 's capacity,maintains,correct understanding,model 's capacity maintains correct understanding,0.6609603762626648
translation,92,35,model,correct understanding,has,of the world,correct understanding has of the world,0.5726797580718994
translation,92,35,model,model,designing,new dataset,model designing new dataset,0.7021569013595581
translation,92,132,results,recurrent entity network,succeed at,all tasks,recurrent entity network succeed at all tasks,0.7029536962509155
translation,92,132,results,training,has,recurrent entity network,training has recurrent entity network,0.5530237555503845
translation,92,173,results,memn2n,performs,worse,memn2n performs worse,0.7090792059898376
translation,92,173,results,worse,on,all the questions,worse on all the questions,0.5710762739181519
translation,92,173,results,worse,with the exception of,reality question,worse with the exception of reality question,0.6691546440124512
translation,92,173,results,all the questions,with the exception of,reality question,all the questions with the exception of reality question,0.6838129758834839
translation,92,176,results,performance,of,multiple observer model,performance of multiple observer model,0.5777865648269653
translation,92,176,results,performance,is,better,performance is better,0.6231186985969543
translation,92,176,results,multiple observer model,is,better,multiple observer model is better,0.6159626841545105
translation,92,176,results,multiple observer model,is,slightly worse,multiple observer model is slightly worse,0.5784969925880432
translation,92,176,results,better,for,most of the questions,better for most of the questions,0.6085569858551025
translation,92,176,results,better,especially,second-order,better especially second-order,0.7196990251541138
translation,92,176,results,slightly worse,for,memory question,slightly worse for memory question,0.6113603115081787
translation,92,176,results,results,has,performance,results has performance,0.5972660779953003
translation,92,180,results,succeed,on,tom tasks,succeed on tom tasks,0.5152313113212585
translation,92,180,results,multiple observer model,performs,better,multiple observer model performs better,0.6331237554550171
translation,92,180,results,better,compared to,original memn2n model,better compared to original memn2n model,0.6413700580596924
translation,92,184,results,succeeds,at,babi task 1,succeeds at babi task 1,0.5215602517127991
translation,92,184,results,succeeds,at,reality question,succeeds at reality question,0.5805497169494629
translation,92,184,results,reality question,given,true-belief task,reality question given true-belief task,0.7036144137382507
translation,92,184,results,memn2n model,has,succeeds,memn2n model has succeeds,0.6486483216285706
translation,92,184,results,results,has,memn2n model,results has memn2n model,0.5271278619766235
translation,92,193,results,"memn2n , multiple observer and relnet models",perform,poorly,"memn2n , multiple observer and relnet models perform poorly",0.6091622114181519
translation,92,193,results,poorly,some combination of,first - and second-order questions,poorly some combination of first - and second-order questions,0.7444174885749817
translation,92,193,results,successful,at,answering,successful at answering,0.589264452457428
translation,92,193,results,answering,has,reality question,answering has reality question,0.6201464533805847
translation,92,193,results,results,observe,"memn2n , multiple observer and relnet models","results observe memn2n , multiple observer and relnet models",0.5517593026161194
translation,92,193,results,results,each of,"memn2n , multiple observer and relnet models","results each of memn2n , multiple observer and relnet models",0.5907211303710938
translation,92,206,results,relnet model,is,best performer,relnet model is best performer,0.50938481092453
translation,92,206,results,relnet model,most sensitive to,noise,relnet model most sensitive to noise,0.6558798551559448
translation,92,206,results,best performer,amongst,models,best performer amongst models,0.6116275787353516
translation,92,206,results,models,on,tom dataset,models on tom dataset,0.5744184851646423
translation,92,206,results,results,has,relnet model,results has relnet model,0.5381467342376709
translation,92,207,results,explicit memories,for,each agent,explicit memories for each agent,0.5852124691009521
translation,92,207,results,most robust,to,noise,most robust to noise,0.5441784262657166
translation,92,207,results,in accuracy,between,each dataset,in accuracy between each dataset,0.6643691062927246
translation,92,207,results,multiple observer modelwith,has,explicit memories,multiple observer modelwith has explicit memories,0.5471461415290833
translation,92,207,results,minimum decrease,has,in accuracy,minimum decrease has in accuracy,0.5747689008712769
translation,92,207,results,results,has,multiple observer modelwith,results has multiple observer modelwith,0.5617294311523438
translation,92,266,results,succeed,on,tom - easy dataset,succeed on tom - easy dataset,0.5291074514389038
translation,92,266,results,succeed,without,noise,succeed without noise,0.8195184469223022
translation,92,266,results,tom - easy dataset,without,noise,tom - easy dataset without noise,0.7720127105712891
translation,92,268,results,observer model,performs,best,observer model performs best,0.6453371644020081
translation,92,268,results,observer model,performs,best,observer model performs best,0.6453371644020081
translation,92,268,results,observer model,performs,best,observer model performs best,0.6453371644020081
translation,92,268,results,best,on,tom - easy with noise,best on tom - easy with noise,0.5594257116317749
translation,92,268,results,best,on,tom,best on tom,0.6480817198753357
translation,92,268,results,best,on,tom,best on tom,0.6480817198753357
translation,92,268,results,best,on,tom,best on tom,0.6480817198753357
translation,92,268,results,best,on,tom,best on tom,0.6480817198753357
translation,92,268,results,best,on,tom,best on tom,0.6480817198753357
translation,92,268,results,best,with,noise,best with noise,0.6772324442863464
translation,92,268,results,tom - easy with noise,with,noise,tom - easy with noise with noise,0.6910711526870728
translation,92,268,results,relnet,performs,best,relnet performs best,0.6142871975898743
translation,92,268,results,relnet,performs,best,relnet performs best,0.6142871975898743
translation,92,268,results,relnet,performs,best,relnet performs best,0.6142871975898743
translation,92,268,results,best,on,tom,best on tom,0.6480817198753357
translation,92,268,results,best,on,tom,best on tom,0.6480817198753357
translation,92,268,results,best,on,tom,best on tom,0.6480817198753357
translation,92,268,results,tom,with,noise,tom with noise,0.7427120804786682
translation,92,268,results,entnet,performs,best,entnet performs best,0.677115261554718
translation,92,268,results,best,on,tom,best on tom,0.6480817198753357
translation,92,268,results,best,with,noise,best with noise,0.6772324442863464
translation,92,268,results,tom,with,noise,tom with noise,0.7427120804786682
translation,92,268,results,results,has,observer model,results has observer model,0.5473811030387878
translation,93,48,results,results,obtained,basic configuration,results obtained basic configuration,0.6963054537773132
translation,93,48,results,results,with,basic configuration,results with basic configuration,0.6325621008872986
translation,93,48,results,results,obtained,basic configuration,results obtained basic configuration,0.6963054537773132
translation,93,48,results,results,with,basic configuration,results with basic configuration,0.6325621008872986
translation,93,61,results,improved,for,both levels of the hierarchy,improved for both levels of the hierarchy,0.6538919806480408
translation,93,61,results,improved,for,fine grained classes,improved for fine grained classes,0.6469839215278625
translation,93,61,results,fine grained classes,results,increase,fine grained classes results increase,0.49173402786254883
translation,93,61,results,increase,.794 to,.837,increase .794 to .837,0.7959392070770264
translation,93,61,results,results,has,precision,results has precision,0.592319667339325
translation,94,36,ablation-analysis,medical entity information,by including,entity embeddings,medical entity information by including entity embeddings,0.6210544109344482
translation,94,36,ablation-analysis,medical entity information,observe,model accuracy and ability to generalize,medical entity information observe model accuracy and ability to generalize,0.5833362936973572
translation,94,36,ablation-analysis,entity embeddings,via,"ernie ( zhang et al. , 2019a ) architecture","entity embeddings via ernie ( zhang et al. , 2019a ) architecture",0.6420362591743469
translation,94,36,ablation-analysis,model accuracy and ability to generalize,goes up by,? 3 %,model accuracy and ability to generalize goes up by ? 3 %,0.6796171069145203
translation,94,36,ablation-analysis,? 3 %,over,"bert base ( devlin et al. , 2019 )","? 3 % over bert base ( devlin et al. , 2019 )",0.6606563925743103
translation,94,36,ablation-analysis,ablation analysis,incorporate,medical entity information,ablation analysis incorporate medical entity information,0.6847874522209167
translation,94,152,ablation-analysis,clinical entity information,with the help of,further fine-tuning,clinical entity information with the help of further fine-tuning,0.64839106798172
translation,94,152,ablation-analysis,logical form prediction,help,model,logical form prediction help model,0.6918812990188599
translation,94,152,ablation-analysis,model,performing,better,model performing better,0.6794503927230835
translation,94,152,ablation-analysis,better,over,unseen paraphrases,better over unseen paraphrases,0.6538056135177612
translation,94,152,ablation-analysis,unseen paraphrases,by,significant margin,unseen paraphrases by significant margin,0.6097794771194458
translation,94,152,ablation-analysis,clinical entity information,has,entity - enriching,clinical entity information has entity - enriching,0.5092641711235046
translation,94,152,ablation-analysis,ablation analysis,embedding,clinical entity information,ablation analysis embedding clinical entity information,0.7687978148460388
translation,94,154,ablation-analysis,performance,dropped a little,lf prediction information,performance dropped a little lf prediction information,0.6677341461181641
translation,94,154,ablation-analysis,lf prediction information,added to,model,lf prediction information added to model,0.6377193331718445
translation,94,154,ablation-analysis,made -p dataset,has,performance,made -p dataset has performance,0.5813625454902649
translation,94,154,ablation-analysis,ablation analysis,For,made -p dataset,ablation analysis For made -p dataset,0.6585268974304199
translation,94,57,baselines,ernie,uses,bert,ernie uses bert,0.5231779217720032
translation,94,57,baselines,ernie,uses,multi-head attention model,ernie uses multi-head attention model,0.5725425481796265
translation,94,57,baselines,bert,for extracting,contextualized token embeddings,bert for extracting contextualized token embeddings,0.6863353252410889
translation,94,57,baselines,multi-head attention model,to generate,entity embeddings,multi-head attention model to generate entity embeddings,0.591916561126709
translation,94,57,baselines,baselines,has,ernie,baselines has ernie,0.6662265658378601
translation,94,4,model,question answering,on,electronic medical records,question answering on electronic medical records,0.506679356098175
translation,94,7,model,medical entity information,via,"ernie ( zhang et al. , 2019a ) architecture","medical entity information via ernie ( zhang et al. , 2019a ) architecture",0.658120334148407
translation,94,7,model,model,incorporate,medical entity information,model incorporate medical entity information,0.6786249279975891
translation,94,8,results,models,on,large-scale emrqa dataset,models on large-scale emrqa dataset,0.5197314620018005
translation,94,8,results,models,observe,our multi-task entity - enriched models,models observe our multi-task entity - enriched models,0.5714201331138611
translation,94,8,results,our multi-task entity - enriched models,generalize to,paraphrased questions,our multi-task entity - enriched models generalize to paraphrased questions,0.6455516815185547
translation,94,8,results,better,than,baseline bert model,better than baseline bert model,0.5683158040046692
translation,94,8,results,paraphrased questions,has,5 %,paraphrased questions has 5 %,0.5863673090934753
translation,94,8,results,5 %,has,better,5 % has better,0.6044652462005615
translation,94,8,results,results,train,models,results train models,0.5888463258743286
translation,94,139,results,exact match performance,improved by,? 3 ? 4.5,exact match performance improved by ? 3 ? 4.5,0.729093611240387
translation,94,139,results,exact match performance,improved by,1.5 ? 3.25 %,exact match performance improved by 1.5 ? 3.25 %,0.7068025469779968
translation,94,139,results,? 3 ? 4.5,over,bert,? 3 ? 4.5 over bert,0.6657416224479675
translation,94,139,results,1.5 ? 3.25 %,over,cbert,1.5 ? 3.25 % over cbert,0.6616173386573792
translation,94,139,results,results,has,exact match performance,results has exact match performance,0.5942237377166748
translation,94,140,results,performance,in,sentence setting ( - s ),performance in sentence setting ( - s ),0.5333237051963806
translation,94,140,results,performance,improved,relatively more,performance improved relatively more,0.8200778961181641
translation,94,140,results,sentence setting ( - s ),improved,relatively more,sentence setting ( - s ) improved relatively more,0.7544441223144531
translation,94,148,results,performance improvement,is,more,performance improvement is more,0.6190401911735535
translation,94,148,results,more,for,sentence setting ( - s ),more for sentence setting ( - s ),0.6313556432723999
translation,94,148,results,sentence setting ( - s ),compared to,paragraph setting ( - p ),sentence setting ( - s ) compared to paragraph setting ( - p ),0.6670345067977905
translation,94,148,results,results,has,performance improvement,results has performance improvement,0.5912312269210815
translation,94,151,results,performance,of,our proposed model,performance of our proposed model,0.6067494750022888
translation,94,151,results,performance,improves,f1 - score,performance improves f1 - score,0.6578541398048401
translation,94,151,results,our proposed model,improves,f1 - score,our proposed model improves f1 - score,0.6619090437889099
translation,94,151,results,f1 - score,by,1.2 ? 7.7 %,f1 - score by 1.2 ? 7.7 %,0.5683721899986267
translation,94,151,results,exactmatch,by,3.1 ? 6.8 %,exactmatch by 3.1 ? 6.8 %,0.5667398571968079
translation,94,151,results,3.1 ? 6.8 %,over,bert model,3.1 ? 6.8 % over bert model,0.6461684107780457
translation,94,153,results,performance,of,m-cernie,performance of m-cernie,0.6156134009361267
translation,94,153,results,performance,of,cbert model,performance of cbert model,0.630511999130249
translation,94,153,results,m-cernie,still below,upper bound performance,m-cernie still below upper bound performance,0.6583734750747681
translation,94,153,results,upper bound performance,of,cbert model,upper bound performance of cbert model,0.5954416990280151
translation,94,153,results,cbert model,achieved when,all the question templates are observed ( emrqa,cbert model achieved when all the question templates are observed ( emrqa,0.6797207593917847
translation,94,153,results,even better,than,upper bound model performance,even better than upper bound model performance,0.5678454637527466
translation,94,153,results,emrqa,has,performance,emrqa has performance,0.5499165058135986
translation,94,153,results,emrqa,has,performance,emrqa has performance,0.5499165058135986
translation,94,153,results,all the question templates are observed ( emrqa,has,performance,all the question templates are observed ( emrqa has performance,0.625294029712677
translation,94,153,results,results,For,emrqa,results For emrqa,0.5906265377998352
translation,94,165,results,f1 - score,of,0.92,f1 - score of 0.92,0.5369964241981506
translation,94,165,results,f1 - score,of,0.84,f1 - score of 0.84,0.539885938167572
translation,94,165,results,0.92,for,emrqa -s,0.92 for emrqa -s,0.6256850957870483
translation,94,165,results,0.84,for,made -s,0.84 for made -s,0.6927048563957214
translation,94,165,results,made -s,in,relaxed setting,made -s in relaxed setting,0.5544426441192627
translation,94,175,results,multi-task entity enriched model ( m- cernie ),achieved,absolute improvement,multi-task entity enriched model ( m- cernie ) achieved absolute improvement,0.6727838516235352
translation,94,175,results,absolute improvement,of,6 %,absolute improvement of 6 %,0.5679503083229065
translation,94,175,results,absolute improvement,of,4 %,absolute improvement of 4 %,0.5680736303329468
translation,94,175,results,6 %,over,cbert,6 % over cbert,0.6705925464630127
translation,94,175,results,4 %,over,cernie,4 % over cernie,0.6799518465995789
translation,94,175,results,results,has,multi-task entity enriched model ( m- cernie ),results has multi-task entity enriched model ( m- cernie ),0.5535162091255188
translation,95,199,ablation-analysis,context,makes,questions,context makes questions,0.6827480792999268
translation,95,199,ablation-analysis,questions,related to,span,questions related to span,0.7328107357025146
translation,95,199,ablation-analysis,questions,has,less valid,questions has less valid,0.5094184875488281
translation,95,199,ablation-analysis,ablation analysis,show,context,ablation analysis show context,0.6720353364944458
translation,95,199,ablation-analysis,ablation analysis,removing,context,ablation analysis removing context,0.7677730321884155
translation,95,179,baselines,spanonly model,in which,conditioning context,spanonly model in which conditioning context,0.5854146480560303
translation,95,179,baselines,conditioning context,contains,highlighted span,conditioning context contains highlighted span,0.6070650219917297
translation,95,179,baselines,highlighted span,inside,sentence,highlighted span inside sentence,0.7138513326644897
translation,95,179,baselines,sentence,where,question,sentence where question,0.674883246421814
translation,95,200,baselines,weakest model,is,span-only one,weakest model is span-only one,0.5994477868080139
translation,95,200,baselines,baselines,has,weakest model,baselines has weakest model,0.538802444934845
translation,95,27,experiments,question generation models,using,"gpt - 2 ( radford et al. , 2019 )","question generation models using gpt - 2 ( radford et al. , 2019 )",0.6519014239311218
translation,95,27,experiments,"gpt - 2 ( radford et al. , 2019 )",has,state - of - the - art,"gpt - 2 ( radford et al. , 2019 ) has state - of - the - art",0.5459425449371338
translation,95,189,experiments,squad pre-training,finetune,1 epoch,squad pre-training finetune 1 epoch,0.7802096009254456
translation,95,189,experiments,batch size,is,1,batch size is 1,0.6298584342002869
translation,95,189,experiments,squad pre-training,has,batch size,squad pre-training has batch size,0.556559145450592
translation,95,187,hyperparameters,gpt2 - medium model,for,all question generation models,gpt2 - medium model for all question generation models,0.6098341345787048
translation,95,187,hyperparameters,hyperparameters,use,gpt2 - medium model,hyperparameters use gpt2 - medium model,0.6281622052192688
translation,95,188,hyperparameters,batch size,is,2,batch size is 2,0.6474922299385071
translation,95,188,hyperparameters,batch size,fine-tune,7 epochs,batch size fine-tune 7 epochs,0.6533107161521912
translation,95,188,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,95,190,hyperparameters,"adam ( kingma and ba , 2015 ) optimizer",with,"( ? 1 , ? 2 ) = ( 0.9 , 0.999 )","adam ( kingma and ba , 2015 ) optimizer with ( ? 1 , ? 2 ) = ( 0.9 , 0.999 )",0.5698577761650085
translation,95,190,hyperparameters,"adam ( kingma and ba , 2015 ) optimizer",with,learning rate,"adam ( kingma and ba , 2015 ) optimizer with learning rate",0.6114457249641418
translation,95,190,hyperparameters,learning rate,of,5e - 5,learning rate of 5e - 5,0.6587256789207458
translation,95,190,hyperparameters,hyperparameters,use,"adam ( kingma and ba , 2015 ) optimizer","hyperparameters use adam ( kingma and ba , 2015 ) optimizer",0.6104203462600708
translation,95,197,results,inquirer questions,related to,given span,inquirer questions related to given span,0.7403302192687988
translation,95,197,results,inquirer questions,related to,more salient,inquirer questions related to more salient,0.7280055284500122
translation,95,197,results,crowdsourcing workers,has,inquirer questions,crowdsourcing workers has inquirer questions,0.5558602213859558
translation,95,197,results,results,Compared to,crowdsourcing workers,results Compared to crowdsourcing workers,0.6149786114692688
translation,95,198,results,human evaluation results,did,improve,human evaluation results did improve,0.6312297582626343
translation,95,198,results,human evaluation results,not,improve,human evaluation results not improve,0.7109417915344238
translation,95,198,results,squad pre-training,has,human evaluation results,squad pre-training has human evaluation results,0.5489552617073059
translation,95,198,results,results,With,squad pre-training,results With squad pre-training,0.5790951251983643
translation,96,208,ablation-analysis,performance,do not consider,associated passage,performance do not consider associated passage,0.6950580477714539
translation,96,208,ablation-analysis,drops,do not consider,associated passage,drops do not consider associated passage,0.7092800736427307
translation,96,208,ablation-analysis,performance,has,drops,performance has drops,0.5993483662605286
translation,96,208,ablation-analysis,associated passage,has,knowledge ),associated passage has knowledge ),0.6259294748306274
translation,96,208,ablation-analysis,ablation analysis,has,performance,ablation analysis has performance,0.5053174495697021
translation,96,211,ablation-analysis,performance,remove,other components,performance remove other components,0.6785719990730286
translation,96,211,ablation-analysis,degrades,remove,other components,degrades remove other components,0.754016101360321
translation,96,211,ablation-analysis,other components,such as,multi-factor attentive encoding,other components such as multi-factor attentive encoding,0.6310989856719971
translation,96,211,ablation-analysis,other components,such as,joint encoding,other components such as joint encoding,0.6637372970581055
translation,96,211,ablation-analysis,other components,such as,character embedding,other components such as character embedding,0.6356813311576843
translation,96,211,ablation-analysis,performance,has,degrades,performance has degrades,0.5837839841842651
translation,96,211,ablation-analysis,ablation analysis,has,performance,ablation analysis has performance,0.5053174495697021
translation,96,172,baselines,logistic regression,using,derived features,logistic regression using derived features,0.6889561414718628
translation,96,174,baselines,baselines,develop,several neural baseline models,baselines develop several neural baseline models,0.5627523064613342
translation,96,106,hyperparameters,pre-trained vectors,from,glove,pre-trained vectors from glove,0.5962195992469788
translation,96,106,hyperparameters,pre-trained vectors,to obtain,fixed - length word embedding vector,pre-trained vectors to obtain fixed - length word embedding vector,0.5587916374206543
translation,96,106,hyperparameters,fixed - length word embedding vector,for,each token,fixed - length word embedding vector for each token,0.5892040133476257
translation,96,106,hyperparameters,hyperparameters,use,pre-trained vectors,hyperparameters use pre-trained vectors,0.6107510328292847
translation,96,162,hyperparameters,last question,from,conversation history,last question from conversation history,0.560905396938324
translation,96,162,hyperparameters,last question,add,tfidf,last question add tfidf,0.626723051071167
translation,96,162,hyperparameters,conversation history,to,candidate follow - up question,conversation history to candidate follow - up question,0.5703812837600708
translation,96,162,hyperparameters,tfidf,of,overlapping words,tfidf of overlapping words,0.5793372988700867
translation,96,162,hyperparameters,overlapping words,between,concatenated context and the passage,overlapping words between concatenated context and the passage,0.6389275789260864
translation,96,162,hyperparameters,hyperparameters,prepend,last question,hyperparameters prepend last question,0.7326895594596863
translation,96,162,hyperparameters,hyperparameters,add,tfidf,hyperparameters add tfidf,0.6258004903793335
translation,96,164,hyperparameters,two sets of features,for,statistical machine learning models,two sets of features for statistical machine learning models,0.5984838604927063
translation,96,164,hyperparameters,hyperparameters,handcraft,two sets of features,hyperparameters handcraft two sets of features,0.6824807524681091
translation,96,165,hyperparameters,features,consists of,tf-idf weighted glove vectors,features consists of tf-idf weighted glove vectors,0.6162639260292053
translation,96,189,hyperparameters,100 - dimension,has,character - level embedding vectors,100 - dimension has character - level embedding vectors,0.5430721044540405
translation,96,189,hyperparameters,hyperparameters,use,100 - dimension,hyperparameters use 100 - dimension,0.6202448010444641
translation,96,190,hyperparameters,number of hidden units,in,all the lstms,number of hidden units in all the lstms,0.5024504661560059
translation,96,190,hyperparameters,number of hidden units,is,150 ( h = 300 ),number of hidden units is 150 ( h = 300 ),0.5587039589881897
translation,96,190,hyperparameters,hyperparameters,has,number of hidden units,hyperparameters has number of hidden units,0.4962380826473236
translation,96,191,hyperparameters,"dropout ( srivastava et al. , 2014 )",with,probability 0.3,"dropout ( srivastava et al. , 2014 ) with probability 0.3",0.5960097908973694
translation,96,191,hyperparameters,hyperparameters,use,"dropout ( srivastava et al. , 2014 )","hyperparameters use dropout ( srivastava et al. , 2014 )",0.545020341873169
translation,96,192,hyperparameters,number of factors,as,4,number of factors as 4,0.5758976340293884
translation,96,192,hyperparameters,hyperparameters,set,number of factors,hyperparameters set number of factors,0.651902973651886
translation,96,193,hyperparameters,adam optimizer,with,learning rate 0.001,adam optimizer with learning rate 0.001,0.6157158017158508
translation,96,193,hyperparameters,adam optimizer,with,clipnorm,adam optimizer with clipnorm,0.6238067746162415
translation,96,193,hyperparameters,adam optimizer,consider,at most 3 previous questionanswer pairs,adam optimizer consider at most 3 previous questionanswer pairs,0.6523665189743042
translation,96,193,hyperparameters,at most 3 previous questionanswer pairs,in,conversation history,at most 3 previous questionanswer pairs in conversation history,0.496803343296051
translation,96,193,hyperparameters,clipnorm,has,5,clipnorm has 5,0.6739590167999268
translation,96,193,hyperparameters,hyperparameters,use,adam optimizer,hyperparameters use adam optimizer,0.6090166568756104
translation,96,193,hyperparameters,hyperparameters,consider,at most 3 previous questionanswer pairs,hyperparameters consider at most 3 previous questionanswer pairs,0.6430948972702026
translation,96,7,model,three - way attentive pooling network,determines,suitability,three - way attentive pooling network determines suitability,0.5656923055648804
translation,96,7,model,suitability,of,follow - up question,suitability of follow - up question,0.5813567638397217
translation,96,7,model,follow - up question,by capturing,pair-wise interactions,follow - up question by capturing pair-wise interactions,0.7211776375770569
translation,96,7,model,pair-wise interactions,between,associated passage,pair-wise interactions between associated passage,0.6217303276062012
translation,96,7,model,pair-wise interactions,between,conversation history,pair-wise interactions between conversation history,0.6618648171424866
translation,96,7,model,pair-wise interactions,between,candidate follow - up question,pair-wise interactions between candidate follow - up question,0.6725881695747375
translation,96,7,model,model,propose,three - way attentive pooling network,model propose three - way attentive pooling network,0.6264175176620483
translation,96,38,model,three - way attentive pooling network,for,follow - up question identification,three - way attentive pooling network for follow - up question identification,0.585651695728302
translation,96,38,model,follow - up question identification,in,conversational reading comprehension setting,follow - up question identification in conversational reading comprehension setting,0.5237321853637695
translation,96,38,model,model,propose,three - way attentive pooling network,model propose three - way attentive pooling network,0.6264175176620483
translation,96,39,model,candidate follow - up question,based on,two perspectives,candidate follow - up question based on two perspectives,0.6793109774589539
translation,96,39,model,two perspectives,has,topic shift,two perspectives has topic shift,0.5547677874565125
translation,96,39,model,model,evaluates,candidate follow - up question,model evaluates candidate follow - up question,0.7287760972976685
translation,96,40,model,two attention matrices,conditioned over,associated passage,two attention matrices conditioned over associated passage,0.7152343392372131
translation,96,40,model,two attention matrices,to capture,topic shift,two attention matrices to capture topic shift,0.695902407169342
translation,96,40,model,topic shift,in,follow - up question,topic shift in follow - up question,0.5268358588218689
translation,96,40,model,model,makes use of,two attention matrices,model makes use of two attention matrices,0.6802651882171631
translation,96,44,model,lif,derived from,recently released conversational qa dataset quac,lif derived from recently released conversational qa dataset quac,0.6041399240493774
translation,96,44,model,three - way attentive pooling network,aims to capture,topic shift and topic continuity,three - way attentive pooling network aims to capture topic shift and topic continuity,0.5926441550254822
translation,96,44,model,topic shift and topic continuity,for,follow - up question identification,topic shift and topic continuity for follow - up question identification,0.6325916647911072
translation,96,44,model,model,propose,three - way attentive pooling network,model propose three - way attentive pooling network,0.6264175176620483
translation,96,104,model,characters,embedded as,vectors,characters embedded as vectors,0.7301270365715027
translation,96,104,model,vectors,using,character - based lookup table,vectors using character - based lookup table,0.6807927489280701
translation,96,104,model,character - based lookup table,fed to,cnn,character - based lookup table fed to cnn,0.6669223308563232
translation,96,104,model,size,is,input channel size,size is input channel size,0.5407776832580566
translation,96,104,model,model,has,characters,model has characters,0.578011155128479
translation,96,107,model,concatenated,to obtain,final embeddings,concatenated to obtain final embeddings,0.6035758852958679
translation,96,107,model,model,has,both word and character embeddings,model has both word and character embeddings,0.5555973649024963
translation,96,129,model,idea of attentive pooling network,to,proposed three - way attentive pooling network,idea of attentive pooling network to proposed three - way attentive pooling network,0.439591646194458
translation,96,129,model,proposed three - way attentive pooling network,for,follow - up question identification task,proposed three - way attentive pooling network for follow - up question identification task,0.5441640019416809
translation,96,129,model,model,extend,idea of attentive pooling network,model extend idea of attentive pooling network,0.5842016935348511
translation,96,130,model,topic continuation,in,follow - up question,topic continuation in follow - up question,0.5094249844551086
translation,96,130,model,model,aims to capture,topic shift,model aims to capture topic shift,0.709126353263855
translation,96,158,model,two models,based on,contextual similarity scores,two models based on contextual similarity scores,0.6647382974624634
translation,96,158,model,contextual similarity scores,using,infersent sentence embeddings,contextual similarity scores using infersent sentence embeddings,0.6025808453559875
translation,96,158,model,model,develop,two models,model develop two models,0.6084215044975281
translation,96,158,model,model,based on,contextual similarity scores,model based on contextual similarity scores,0.6651668548583984
translation,96,160,model,similarity scores,computed based on,vector cosine similarity,similarity scores computed based on vector cosine similarity,0.7199684977531433
translation,96,160,model,model,has,similarity scores,model has similarity scores,0.5804399251937866
translation,96,161,model,another rule- based model,using,tf - idf weighted token overlap scores,another rule- based model using tf - idf weighted token overlap scores,0.6482896208763123
translation,96,161,model,model,develop,another rule- based model,model develop another rule- based model,0.6186325550079346
translation,96,45,results,proposed model,has,significantly outperforms,proposed model has significantly outperforms,0.6167242527008057
translation,96,45,results,significantly outperforms,has,all the baseline systems,significantly outperforms has all the baseline systems,0.5790873765945435
translation,96,45,results,results,has,proposed model,results has proposed model,0.5938616394996643
translation,96,197,results,outperforms,by,significant margins,outperforms by significant margins,0.6760052442550659
translation,96,197,results,competing baseline models,by,significant margins,competing baseline models by significant margins,0.5895576477050781
translation,96,197,results,significant margins,across,all test sets,significant margins across all test sets,0.7026767134666443
translation,96,197,results,proposed model,has,outperforms,proposed model has outperforms,0.642342746257782
translation,96,197,results,outperforms,has,competing baseline models,outperforms has competing baseline models,0.5795904397964478
translation,96,197,results,results,shows,proposed model,results shows proposed model,0.7134876251220703
translation,96,199,results,performance,of,our proposed model,performance of our proposed model,0.6067494750022888
translation,96,199,results,performance,is,significantly better ( p < 0.01 ),performance is significantly better ( p < 0.01 ),0.5437202453613281
translation,96,199,results,our proposed model,is,significantly better ( p < 0.01 ),our proposed model is significantly better ( p < 0.01 ),0.5621029138565063
translation,96,199,results,significantly better ( p < 0.01 ),than,best baseline system,significantly better ( p < 0.01 ) than best baseline system,0.5462661981582642
translation,96,199,results,best baseline system,which provides,highest macro - f1 score,best baseline system which provides highest macro - f1 score,0.644514799118042
translation,96,199,results,highest macro - f1 score,has,on test -i,highest macro - f1 score has on test -i,0.5508226752281189
translation,96,199,results,results,has,performance,results has performance,0.5972660779953003
translation,96,200,results,lstm - based neural baselines,perform,better,lstm - based neural baselines perform better,0.6067338585853577
translation,96,200,results,better,than,rule-based and statistical machine learning models,better than rule-based and statistical machine learning models,0.5502897500991821
translation,96,200,results,rule-based and statistical machine learning models,in,most cases,rule-based and statistical machine learning models in most cases,0.5399557948112488
translation,96,200,results,results,has,lstm - based neural baselines,results has lstm - based neural baselines,0.5457807183265686
translation,96,201,results,statistical models,predict,valid,statistical models predict valid,0.6954787969589233
translation,96,201,results,number of valid instances,much higher than,invalid instances,number of valid instances much higher than invalid instances,0.596193253993988
translation,96,201,results,number of valid instances,resulting in,high,number of valid instances resulting in high,0.6802246570587158
translation,96,201,results,test - iii,has,statistical models,test - iii has statistical models,0.5954069495201111
translation,96,201,results,invalid instances,has,about 75 % : 25 % ),invalid instances has about 75 % : 25 % ),0.5643439888954163
translation,96,201,results,high,has,valid f1 scores,high has valid f1 scores,0.5745280981063843
translation,96,201,results,results,On,test - iii,results On test - iii,0.563389241695404
translation,96,204,results,identifying follow - up questions,from,same conversation ( test - iii ),identifying follow - up questions from same conversation ( test - iii ),0.5359175801277161
translation,96,204,results,identifying follow - up questions,is,harder,identifying follow - up questions is harder,0.5581129193305969
translation,96,204,results,harder,compared to,other conversations,harder compared to other conversations,0.5602155923843384
translation,96,204,results,results,has,identifying follow - up questions,results has identifying follow - up questions,0.48051902651786804
translation,96,206,results,proposed model,performs,worst,proposed model performs worst,0.6448225378990173
translation,96,206,results,proposed model,do not consider,conversation history,proposed model do not consider conversation history,0.7058414220809937
translation,96,206,results,worst,do not consider,conversation history,worst do not consider conversation history,0.7502068877220154
translation,96,206,results,results,has,proposed model,results has proposed model,0.5938616394996643
translation,96,210,results,better,than,model,better than model,0.5832422375679016
translation,97,117,experimental-setup,"liblinear ( fan et al. , 2008 )",via,scikitlearn python interface,"liblinear ( fan et al. , 2008 ) via scikitlearn python interface",0.6016588807106018
translation,97,117,experimental-setup,experimental setup,applied,"liblinear ( fan et al. , 2008 )","experimental setup applied liblinear ( fan et al. , 2008 )",0.6458138227462769
translation,97,7,experiments,lean qa system,runs,real time,lean qa system runs real time,0.6644208431243896
translation,97,122,results,outperformed,in,both f 1 measures,outperformed in both f 1 measures,0.5516268610954285
translation,97,122,results,all previous systems,in,both f 1 measures,all previous systems in both f 1 measures,0.5180192589759827
translation,97,122,results,outperformed,has,all previous systems,outperformed has all previous systems,0.6278972625732422
translation,97,122,results,results,used,direct supervision,results used direct supervision,0.6545189023017883
translation,98,80,ablation-analysis,training speed,of,proposed method,training speed of proposed method,0.58917635679245
translation,98,80,ablation-analysis,gc objective,degrades,training speed,gc objective degrades training speed,0.7139361500740051
translation,98,80,ablation-analysis,training speed,by,28 %,training speed by 28 %,0.5971681475639343
translation,98,80,ablation-analysis,ablation analysis,show,training speed,ablation analysis show training speed,0.6155002117156982
translation,98,7,experiments,qainfomax,regularize,model,qainfomax regularize model,0.7514380812644958
translation,98,7,experiments,superficial correlation,for,answering questions,superficial correlation for answering questions,0.6408354043960571
translation,98,77,hyperparameters,c,to be,"5 , 1 , 0.5 , 0.3","c to be 5 , 1 , 0.5 , 0.3",0.579036295413971
translation,98,77,hyperparameters,c,add,proposed qainfomax,c add proposed qainfomax,0.6184098124504089
translation,98,77,hyperparameters,proposed qainfomax,into,bert model,proposed qainfomax into bert model,0.589144766330719
translation,98,77,hyperparameters,hyperparameters,set,c,hyperparameters set c,0.6763419508934021
translation,98,6,model,qainfomax,as,regularizer,qainfomax as regularizer,0.5596612691879272
translation,98,6,model,regularizer,in,reading comprehension systems,regularizer in reading comprehension systems,0.51557856798172
translation,98,6,model,regularizer,by maximizing,mutual information,regularizer by maximizing mutual information,0.7268900871276855
translation,98,6,model,mutual information,among,passages,mutual information among passages,0.6493434906005859
translation,98,6,model,model,propose,qainfomax,model propose qainfomax,0.6941750645637512
translation,98,15,model,alternative approach,named,qainfomax,alternative approach named qainfomax,0.6732911467552185
translation,98,15,model,alternative approach,by maximizing,mutual information ( mi ),alternative approach by maximizing mutual information ( mi ),0.660574197769165
translation,98,15,model,mutual information ( mi ),among,"passages , questions , and answers","mutual information ( mi ) among passages , questions , and answers",0.5556277632713318
translation,98,16,model,qainfomax,incorporates,recently proposed deep infomax ( dim ),qainfomax incorporates recently proposed deep infomax ( dim ),0.70253986120224
translation,98,16,model,recently proposed deep infomax ( dim ),in,model,recently proposed deep infomax ( dim ) in model,0.5045273303985596
translation,98,16,model,recently proposed deep infomax ( dim ),proved effective in learning,representations,recently proposed deep infomax ( dim ) proved effective in learning representations,0.6969069242477417
translation,98,16,model,representations,for,"image , audio","representations for image , audio",0.630046546459198
translation,98,16,model,mi,has,qainfomax,mi has qainfomax,0.6231544613838196
translation,98,16,model,model,To efficiently estimate,mi,model To efficiently estimate mi,0.7540209293365479
translation,98,17,model,qainfomax,extends,dim,qainfomax extends dim,0.7113624215126038
translation,98,17,model,qainfomax,encourages,question answering model,qainfomax encourages question answering model,0.639786422252655
translation,98,17,model,dim,to,text domain,dim to text domain,0.6080563068389893
translation,98,17,model,question answering model,to generate,answers,question answering model to generate answers,0.6734206080436707
translation,99,184,ablation-analysis,independent bert encoders ( i.e. unshared parameters ),between,phrase and question embedding models,independent bert encoders ( i.e. unshared parameters ) between phrase and question embedding models,0.6067292094230652
translation,99,184,ablation-analysis,independent bert encoders ( i.e. unshared parameters ),see,large drop,independent bert encoders ( i.e. unshared parameters ) see large drop,0.5723121166229248
translation,99,184,ablation-analysis,ablation analysis,try,independent bert encoders ( i.e. unshared parameters ),ablation analysis try independent bert encoders ( i.e. unshared parameters ),0.6193106770515442
translation,99,184,ablation-analysis,ablation analysis,see,large drop,ablation analysis see large drop,0.6321253180503845
translation,99,186,ablation-analysis,our ablation,excludes,coherency scalar,our ablation excludes coherency scalar,0.7552729249000549
translation,99,186,ablation-analysis,coherency scalar,decreases,denspi 's em score,coherency scalar decreases denspi 's em score,0.7092259526252747
translation,99,186,ablation-analysis,coherency scalar,decreases,f1,coherency scalar decreases f1,0.7234029769897461
translation,99,186,ablation-analysis,denspi 's em score,by,2 %,denspi 's em score by 2 %,0.5962203741073608
translation,99,186,ablation-analysis,f1,by,0.2 %,f1 by 0.2 %,0.6047123670578003
translation,99,186,ablation-analysis,ablation analysis,excludes,coherency scalar,ablation analysis excludes coherency scalar,0.7068461775779724
translation,99,186,ablation-analysis,ablation analysis,has,our ablation,ablation analysis has our ablation,0.5411481261253357
translation,99,172,baselines,lstm +sa and lstm +sa +elmo,encode,phrases,lstm +sa and lstm +sa +elmo encode phrases,0.7072566747665405
translation,99,172,baselines,phrases,independent of,question,phrases independent of question,0.7205692529678345
translation,99,172,baselines,question,using,"lstm , self-attention , and elmo (","question using lstm , self-attention , and elmo (",0.6169566512107849
translation,99,172,baselines,question,using,) encodings,question using ) encodings,0.7365015149116516
translation,99,172,baselines,"lstm , self-attention , and elmo (",has,) encodings,"lstm , self-attention , and elmo ( has ) encodings",0.5517770648002625
translation,99,198,baselines,baselines,include,drqa,baselines include drqa,0.6117445826530457
translation,99,198,baselines,baselines,include,"minimal ( min et al. , 2018 )","baselines include minimal ( min et al. , 2018 )",0.5789346098899841
translation,99,198,baselines,baselines,include,"multi-step-reasoner ( das et al. , 2019 )","baselines include multi-step-reasoner ( das et al. , 2019 )",0.5471614003181458
translation,99,198,baselines,baselines,include,paragraph ranker,baselines include paragraph ranker,0.5704783797264099
translation,99,198,baselines,baselines,include,r 3,baselines include r 3,0.6395123600959778
translation,99,198,baselines,baselines,include,"wang et al. , 2018a )","baselines include wang et al. , 2018a )",0.5672470331192017
translation,99,198,baselines,r 3,has,"wang et al. , 2018a )","r 3 has wang et al. , 2018a )",0.5364477634429932
translation,99,198,baselines,baselines,include,drqa,baselines include drqa,0.6117445826530457
translation,99,198,baselines,baselines,has,baselines,baselines has baselines,0.6036415696144104
translation,99,164,experimental-setup,bert - large ( d = 1024 ),for,text encoders,bert - large ( d = 1024 ) for text encoders,0.6358498334884644
translation,99,164,experimental-setup,bert - large ( d = 1024 ),pretrained on,large text corpus ( wikipedia dump and book corpus ),bert - large ( d = 1024 ) pretrained on large text corpus ( wikipedia dump and book corpus ),0.7638852596282959
translation,99,164,experimental-setup,text encoders,pretrained on,large text corpus ( wikipedia dump and book corpus ),text encoders pretrained on large text corpus ( wikipedia dump and book corpus ),0.7436118125915527
translation,99,164,experimental-setup,experimental setup,use,bert - large ( d = 1024 ),experimental setup use bert - large ( d = 1024 ),0.6289646029472351
translation,99,166,experimental-setup,d b = 480,resulting in,phrase size,d b = 480 resulting in phrase size,0.6908296942710876
translation,99,166,experimental-setup,phrase size,of,2d,phrase size of 2d,0.6175745725631714
translation,99,167,experimental-setup,batch size,of,12,batch size of 12,0.6833499670028687
translation,99,167,experimental-setup,12,on,four p40 gpus,12 on four p40 gpus,0.5303928256034851
translation,99,167,experimental-setup,experimental setup,train with,batch size,experimental setup train with batch size,0.699195384979248
translation,99,9,experiments,squad - open,show,our model,squad - open show our model,0.7226710915565491
translation,99,9,experiments,our model,is,on par with or more accurate,our model is on par with or more accurate,0.6089268326759338
translation,99,9,experiments,on par with or more accurate,than,previous models,on par with or more accurate than previous models,0.6010968685150146
translation,99,9,experiments,previous models,with,6000x reduced computational cost,previous models with 6000x reduced computational cost,0.6078460812568665
translation,99,9,experiments,at least 68x faster,on,cpus,at least 68x faster on cpus,0.518448531627655
translation,99,9,experiments,at least 68x faster,has,end-to - end inference benchmark,at least 68x faster has end-to - end inference benchmark,0.5406960844993591
translation,99,5,model,query -agnostic indexable representations,has,of document phrases,query -agnostic indexable representations has of document phrases,0.5518553256988525
translation,99,5,model,drastically speed up,has,open-domain qa,drastically speed up has open-domain qa,0.5859766602516174
translation,99,5,model,model,introduce,query -agnostic indexable representations,model introduce query -agnostic indexable representations,0.667085587978363
translation,99,6,model,dense-sparse phrase encoding,effectively captures,"syntactic , semantic , and lexical information","dense-sparse phrase encoding effectively captures syntactic , semantic , and lexical information",0.7969850897789001
translation,99,6,model,dense-sparse phrase encoding,eliminates,pipeline filtering,dense-sparse phrase encoding eliminates pipeline filtering,0.6323661804199219
translation,99,6,model,"syntactic , semantic , and lexical information",of,phrases,"syntactic , semantic , and lexical information of phrases",0.5622869729995728
translation,99,6,model,pipeline filtering,of,context documents,pipeline filtering of context documents,0.5647814273834229
translation,99,6,model,model,has,dense-sparse phrase encoding,model has dense-sparse phrase encoding,0.5111463069915771
translation,99,7,model,our model,trained and deployed even in,single 4 - gpu server,our model trained and deployed even in single 4 - gpu server,0.667481541633606
translation,99,7,model,training and inference time,has,our model,training and inference time has our model,0.5607885718345642
translation,99,18,model,indexable query -agnostic phrase representation model,for,real-time opendomain qa,indexable query -agnostic phrase representation model for real-time opendomain qa,0.5716047883033752
translation,99,18,model,dense -sparse phrase index ( denspi ),has,indexable query -agnostic phrase representation model,dense -sparse phrase index ( denspi ) has indexable query -agnostic phrase representation model,0.5469123125076294
translation,99,18,model,model,introduce,dense -sparse phrase index ( denspi ),model introduce dense -sparse phrase index ( denspi ),0.6337841153144836
translation,99,19,model,indexed offline,using,efficient training,indexed offline using efficient training,0.6995223760604858
translation,99,19,model,memoryefficient strategies,for,storage,memoryefficient strategies for storage,0.6273764967918396
translation,99,19,model,model,has,phrase representations,model has phrase representations,0.5595411658287048
translation,99,20,model,input question,mapped to,same representation space,input question mapped to same representation space,0.7327917218208313
translation,99,20,model,phrase,with,maximum inner product search,phrase with maximum inner product search,0.6408354640007019
translation,99,20,model,inference time,has,input question,inference time has input question,0.5844185948371887
translation,99,20,model,model,During,inference time,model During inference time,0.6886980533599854
translation,99,21,model,phrase encoding model,combines,both dense and sparse vectors,phrase encoding model combines both dense and sparse vectors,0.7064809799194336
translation,99,21,model,both dense and sparse vectors,eliminating,pipeline filtering,both dense and sparse vectors eliminating pipeline filtering,0.67360919713974
translation,99,21,model,pipeline filtering,of,context documents,pipeline filtering of context documents,0.5647814273834229
translation,99,21,model,model,has,phrase encoding model,model has phrase encoding model,0.5290730595588684
translation,99,137,model,simple single - layer binary classifier,on top of,start and end vectors,simple single - layer binary classifier on top of start and end vectors,0.6820292472839355
translation,99,137,model,simple single - layer binary classifier,each of,start and end vectors,simple single - layer binary classifier each of start and end vectors,0.5972967743873596
translation,99,137,model,model,train,simple single - layer binary classifier,model train simple single - layer binary classifier,0.6767145991325378
translation,99,8,results,phrases,as,pointers,phrases as pointers,0.5397679805755615
translation,99,8,results,phrases,in,entire english wikipedia,phrases in entire english wikipedia,0.4664424657821655
translation,99,8,results,pointers,to,start and end tokens,pointers to start and end tokens,0.5551475882530212
translation,99,8,results,phrases,in,entire english wikipedia,phrases in entire english wikipedia,0.4664424657821655
translation,99,8,results,phrases,using,under 2tb,phrases using under 2tb,0.7306753993034363
translation,99,8,results,results,representing,phrases,results representing phrases,0.497751384973526
translation,99,158,results,our model,achieves,3.6 % better accuracy,our model achieves 3.6 % better accuracy,0.6323336362838745
translation,99,158,results,end-to - end inference time,than,previous work,end-to - end inference time than previous work,0.5013805627822876
translation,99,158,results,nearly 68 times faster,has,end-to - end inference time,nearly 68 times faster has end-to - end inference time,0.5520755648612976
translation,99,158,results,results,show,our model,results show our model,0.6888449192047119
translation,99,179,results,query - agnostic models,process,( read ) words,query - agnostic models process ( read ) words,0.7414547204971313
translation,99,179,results,much faster,than,query - dependent representation models,much faster than query - dependent representation models,0.5714362263679504
translation,99,179,results,( read ) words,has,much faster,( read ) words has much faster,0.584130048751831
translation,99,179,results,results,has,query - agnostic models,results has query - agnostic models,0.5832517147064209
translation,99,180,results,controlled environment,where,all information,controlled environment where all information,0.6512730717658997
translation,99,180,results,all information,is,memory,all information is memory,0.5830663442611694
translation,99,180,results,all information,in,memory,all information in memory,0.5327732563018799
translation,99,180,results,denspi,process,28.7 million words per second,denspi process 28.7 million words per second,0.7198969125747681
translation,99,180,results,28.7 million words per second,is,"6,000 times faster","28.7 million words per second is 6,000 times faster",0.5423672795295715
translation,99,180,results,28.7 million words per second,is,"563,000 times faster","28.7 million words per second is 563,000 times faster",0.542302131652832
translation,99,180,results,"6,000 times faster",than,drqa,"6,000 times faster than drqa",0.615106463432312
translation,99,180,results,"563,000 times faster",than,bert,"563,000 times faster than bert",0.557191789150238
translation,99,180,results,controlled environment,has,denspi,controlled environment has denspi,0.6837254762649536
translation,99,180,results,all information,has,denspi,all information has denspi,0.5918301939964294
translation,99,180,results,bert,has,without any approximation,bert has without any approximation,0.6696488857269287
translation,99,180,results,results,In,controlled environment,results In controlled environment,0.531266987323761
translation,99,183,results,huge drop,in,performance,huge drop in performance,0.5450350046157837
translation,99,183,results,results,see,huge drop,results see huge drop,0.6409643888473511
translation,99,202,results,results,on,squad - open,results on squad - open,0.5056798458099365
translation,99,203,results,outperforms,achieving,68 times faster inference speed,outperforms achieving 68 times faster inference speed,0.6363741159439087
translation,99,203,results,drqa,by,3.6 % em,drqa by 3.6 % em,0.62810879945755
translation,99,203,results,drqa,achieving,68 times faster inference speed,drqa achieving 68 times faster inference speed,0.6184011101722717
translation,99,203,results,denspi,has,outperforms,denspi has outperforms,0.6426674723625183
translation,99,203,results,outperforms,has,drqa,outperforms has drqa,0.6045196056365967
translation,99,205,results,denspi,is,0.2 % f1,denspi is 0.2 % f1,0.6161152124404907
translation,99,205,results,denspi,is,3.8 % f1,denspi is 3.8 % f1,0.6088221073150635
translation,99,205,results,0.2 % f1,behind,minimal,0.2 % f1 behind minimal,0.672311007976532
translation,99,205,results,3.8 % f1,behind,bertserini,3.8 % f1 behind bertserini,0.6346080303192139
translation,99,205,results,results,has,denspi,results has denspi,0.5650525093078613
translation,100,170,ablation-analysis,external knowledge,as,supplementary information,external knowledge as supplementary information,0.44157227873802185
translation,100,170,ablation-analysis,natural answer generation,by removing,supplementary knowledge,natural answer generation by removing supplementary knowledge,0.654278039932251
translation,100,170,ablation-analysis,natural answer generation,by removing,corresponding fact selection module,natural answer generation by removing corresponding fact selection module,0.6868589520454407
translation,100,170,ablation-analysis,corresponding fact selection module,from,keag 's architecture,corresponding fact selection module from keag 's architecture,0.5609481334686279
translation,100,170,ablation-analysis,ablation analysis,incorporating,external knowledge,ablation analysis incorporating external knowledge,0.6857693195343018
translation,100,171,ablation-analysis,knowledge component,plays,important role,knowledge component plays important role,0.6909919381141663
translation,100,171,ablation-analysis,important role,in,generating,important role in generating,0.5451485514640808
translation,100,171,ablation-analysis,high-quality answers,with,drop,high-quality answers with drop,0.5986612439155579
translation,100,171,ablation-analysis,drop,to,49.98,drop to 49.98,0.5994320511817932
translation,100,171,ablation-analysis,49.98,on,rouge - l,49.98 on rouge - l,0.5866379141807556
translation,100,171,ablation-analysis,49.98,after,supplementary knowledge,49.98 after supplementary knowledge,0.5788863301277161
translation,100,171,ablation-analysis,supplementary knowledge,is,removed,supplementary knowledge is removed,0.5731832981109619
translation,100,171,ablation-analysis,generating,has,high-quality answers,generating has high-quality answers,0.5311198830604553
translation,100,171,ablation-analysis,ablation analysis,seen that,knowledge component,ablation analysis seen that knowledge component,0.6551896333694458
translation,100,172,ablation-analysis,latent indicators y,leads to,degradation,latent indicators y leads to degradation,0.6656147241592407
translation,100,172,ablation-analysis,degradation,to,gqa,degradation to gqa,0.6048636436462402
translation,100,173,ablation-analysis,our learning method,proves to be,effective,our learning method proves to be effective,0.7331789135932922
translation,100,173,ablation-analysis,effective,with,drop,effective with drop,0.69498610496521
translation,100,173,ablation-analysis,drop,of,about 5 %,drop of about 5 %,0.6703758835792542
translation,100,173,ablation-analysis,drop,of,about 6 %,drop of about 6 %,0.6586529612541199
translation,100,173,ablation-analysis,about 5 %,on,rouge -l,about 5 % on rouge -l,0.6433653235435486
translation,100,173,ablation-analysis,about 6 %,on,bleu - 1,about 6 % on bleu - 1,0.5644913911819458
translation,100,173,ablation-analysis,bleu - 1,after,ablation,bleu - 1 after ablation,0.6934047341346741
translation,100,173,ablation-analysis,ablation analysis,has,our learning method,ablation analysis has our learning method,0.578441858291626
translation,100,174,ablation-analysis,source selector,have,new model,source selector have new model,0.553337037563324
translation,100,174,ablation-analysis,new model,that generates,answer words,new model that generates answer words,0.6915857195854187
translation,100,174,ablation-analysis,answer words,from,vocabulary alone,answer words from vocabulary alone,0.5410107374191284
translation,100,174,ablation-analysis,ablation analysis,for ablating,source selector,ablation analysis for ablating source selector,0.7385762929916382
translation,100,143,baselines,bidaf model,followed by,additional sequence - to-sequence model,bidaf model followed by additional sequence - to-sequence model,0.7096803784370422
translation,100,143,baselines,additional sequence - to-sequence model,for,answer generation,additional sequence - to-sequence model for answer generation,0.6290766000747681
translation,100,143,baselines,bidaf + seq2seq,has,bidaf model,bidaf + seq2seq has bidaf model,0.5959150791168213
translation,100,143,baselines,baselines,has,bidaf + seq2seq,baselines has bidaf + seq2seq,0.5642794370651245
translation,100,144,baselines,extraction - thensynthesis framework,to synthesize,answers,extraction - thensynthesis framework to synthesize answers,0.6469601988792419
translation,100,144,baselines,answers,from,extracted evidences,answers from extracted evidences,0.5285489559173584
translation,100,144,baselines,s- net model,followed by,additional sequence - to-sequence model,s- net model followed by additional sequence - to-sequence model,0.6870508193969727
translation,100,144,baselines,additional sequence - to-sequence model,for,answer generation,additional sequence - to-sequence model for answer generation,0.6290766000747681
translation,100,144,baselines,s- net+ seq2seq,has,s- net model,s- net+ seq2seq has s- net model,0.5880827307701111
translation,100,144,baselines,qfs,has,) passages,qfs has ) passages,0.6714257597923279
translation,100,144,baselines,gqa,has,),gqa has ),0.6883525252342224
translation,100,153,baselines,gqa w/ crwe,is,reading architecture,gqa w/ crwe is reading architecture,0.627373993396759
translation,100,153,baselines,reading architecture,with,dynamic integration,reading architecture with dynamic integration,0.6268782615661621
translation,100,153,baselines,dynamic integration,of,background knowledge,dynamic integration of background knowledge,0.5516652464866638
translation,100,153,baselines,background knowledge,based on,contextual refinement,background knowledge based on contextual refinement,0.5895203351974487
translation,100,153,baselines,background knowledge,by leveraging,supplementary knowledge,background knowledge by leveraging supplementary knowledge,0.6063542366027832
translation,100,153,baselines,contextual refinement,of,word embeddings,contextual refinement of word embeddings,0.5187342166900635
translation,100,153,baselines,contextual refinement,by leveraging,supplementary knowledge,contextual refinement by leveraging supplementary knowledge,0.6965169310569763
translation,100,153,baselines,baselines,has,gqa w/ crwe,baselines has gqa w/ crwe,0.5958144068717957
translation,100,157,baselines,gqa w/ kblstm and gqa w/ crwe,extend,gqa,gqa w/ kblstm and gqa w/ crwe extend gqa,0.7040920257568359
translation,100,157,baselines,gqa,with,module,gqa with module,0.6706902980804443
translation,100,157,baselines,module,consumes,knowledge,module consumes knowledge,0.7601439356803894
translation,100,157,baselines,module,consumes,knowledge,module consumes knowledge,0.7601439356803894
translation,100,157,baselines,module,consumes,knowledge,module consumes knowledge,0.7601439356803894
translation,100,157,baselines,knowledge,with,selectively - gated attention,knowledge with selectively - gated attention,0.6544153690338135
translation,100,157,baselines,knowledge,in,answer generation,knowledge in answer generation,0.5156667232513428
translation,100,157,baselines,mhpgm,incorporates,knowledge,mhpgm incorporates knowledge,0.7122839093208313
translation,100,157,baselines,knowledge,with,selectively - gated attention,knowledge with selectively - gated attention,0.6544153690338135
translation,100,157,baselines,decoder,does not leverage,words,decoder does not leverage words,0.6400611400604248
translation,100,157,baselines,knowledge,in,answer generation,knowledge in answer generation,0.5156667232513428
translation,100,157,baselines,baselines,has,gqa w/ kblstm and gqa w/ crwe,baselines has gqa w/ kblstm and gqa w/ crwe,0.5867677927017212
translation,100,158,baselines,two stochastic selectors,to determine,fact to use,two stochastic selectors to determine fact to use,0.7541788816452026
translation,100,131,experimental-setup,keag,use,300 - dimensional pre-trained glove word embeddings,keag use 300 - dimensional pre-trained glove word embeddings,0.5807552337646484
translation,100,131,experimental-setup,experimental setup,In,keag,experimental setup In keag,0.6044560074806213
translation,100,132,experimental-setup,fact representation f,has,500 dimensions,fact representation f has 500 dimensions,0.6068373918533325
translation,100,132,experimental-setup,experimental setup,has,fact representation f,experimental setup has fact representation f,0.5752465128898621
translation,100,136,experimental-setup,training and test stages,truncate,passage,training and test stages truncate passage,0.7269703149795532
translation,100,136,experimental-setup,training and test stages,limit,length,training and test stages limit length,0.7565963864326477
translation,100,136,experimental-setup,passage,to,800 words,passage to 800 words,0.6042143106460571
translation,100,136,experimental-setup,length,of,answer,length of answer,0.5969738364219666
translation,100,136,experimental-setup,answer,to,120 words,answer to 120 words,0.5890821814537048
translation,100,136,experimental-setup,experimental setup,At,training and test stages,experimental setup At training and test stages,0.5056455135345459
translation,100,136,experimental-setup,experimental setup,both,training and test stages,experimental setup both training and test stages,0.6516034007072449
translation,100,136,experimental-setup,experimental setup,limit,length,experimental setup limit length,0.6948102712631226
translation,100,137,experimental-setup,single tesla m40 gpu,with,batch size,single tesla m40 gpu with batch size,0.6221546530723572
translation,100,137,experimental-setup,batch size,of,16,batch size of 16,0.6842944622039795
translation,100,137,experimental-setup,experimental setup,train on,single tesla m40 gpu,experimental setup train on single tesla m40 gpu,0.6901922225952148
translation,100,138,experimental-setup,answers,generated using,beam search,answers generated using beam search,0.7044556736946106
translation,100,138,experimental-setup,beam search,beam size of,4,beam search beam size of 4,0.7735639214515686
translation,100,138,experimental-setup,test time,has,answers,test time has answers,0.5920985341072083
translation,100,138,experimental-setup,experimental setup,At,test time,experimental setup At test time,0.5272576808929443
translation,100,138,experimental-setup,experimental setup,has,answers,experimental setup has answers,0.5546883344650269
translation,100,155,experiments,mhpgm,observed that,keag,mhpgm observed that keag,0.6586955189704895
translation,100,155,experiments,keag,performs,best,keag performs best,0.6612707376480103
translation,100,155,experiments,best,with,highest rouge -l and bleu - 1 scores,best with highest rouge -l and bleu - 1 scores,0.5939226746559143
translation,100,155,experiments,highest rouge -l and bleu - 1 scores,among,knowledge-enriched answer generation models,highest rouge -l and bleu - 1 scores among knowledge-enriched answer generation models,0.5724247694015503
translation,100,6,model,new neural model,able to compose,natural answer,new neural model able to compose natural answer,0.719709575176239
translation,100,6,model,knowledge -enriched answer generator ( keag ),able to compose,natural answer,knowledge -enriched answer generator ( keag ) able to compose natural answer,0.699302077293396
translation,100,6,model,natural answer,by exploiting and aggregating,evidence,natural answer by exploiting and aggregating evidence,0.6903214454650879
translation,100,6,model,evidence,from,all four information sources available,evidence from all four information sources available,0.5548072457313538
translation,100,6,model,model,propose,new neural model,model propose new neural model,0.6358945965766907
translation,100,7,model,which fact,from,knowledge,which fact from knowledge,0.5803424119949341
translation,100,7,model,knowledge,is,useful,knowledge is useful,0.5527422428131104
translation,100,19,model,neural model,encodes,pre-selected knowledge,neural model encodes pre-selected knowledge,0.7205663323402405
translation,100,19,model,neural model,learns to include,available knowledge,neural model learns to include available knowledge,0.7488894462585449
translation,100,19,model,pre-selected knowledge,relevant to,given questions,pre-selected knowledge relevant to given questions,0.7037378549575806
translation,100,19,model,available knowledge,as,enrichment,available knowledge as enrichment,0.5181496739387512
translation,100,19,model,enrichment,to,given textual information,enrichment to given textual information,0.5586246252059937
translation,100,19,model,model,design,neural model,model design neural model,0.5910447239875793
translation,100,20,model,new neural architecture,designed to generate,natural answers,new neural architecture designed to generate natural answers,0.6834295392036438
translation,100,20,model,natural answers,integration of,external knowledge,natural answers integration of external knowledge,0.5780635476112366
translation,100,20,model,new neural architecture,has,knowledge -enriched answer generator ( keag ),new neural architecture has knowledge -enriched answer generator ( keag ),0.5981995463371277
translation,100,20,model,model,propose,new neural architecture,model propose new neural architecture,0.6966041922569275
translation,100,21,model,keag,leveraging,symbolic knowledge,keag leveraging symbolic knowledge,0.7120236158370972
translation,100,21,model,symbolic knowledge,from,knowledge base,symbolic knowledge from knowledge base,0.5297240614891052
translation,100,21,model,symbolic knowledge,as it generates,each word,symbolic knowledge as it generates each word,0.6947222948074341
translation,100,21,model,each word,in,answer,each word in answer,0.5222238302230835
translation,100,21,model,model,has,keag,model has keag,0.6481752991676331
translation,100,30,model,new differentiable samplingbased method,to learn,keag model,new differentiable samplingbased method to learn keag model,0.5831719040870667
translation,100,30,model,keag model,in the presence of,discrete latent variables,keag model in the presence of discrete latent variables,0.6646565794944763
translation,100,30,model,model,introduce,new differentiable samplingbased method,model introduce new differentiable samplingbased method,0.6464899778366089
translation,100,142,model,multi-stage hierarchical process,represents,context,multi-stage hierarchical process represents context,0.6362245082855225
translation,100,142,model,multi-stage hierarchical process,using,bi-directional attention flow mechanism,multi-stage hierarchical process using bi-directional attention flow mechanism,0.6305440664291382
translation,100,142,model,context,at,different levels of granularity,context at different levels of granularity,0.5547928810119629
translation,100,142,model,bi-directional attention flow mechanism,for,answer extraction,bi-directional attention flow mechanism for answer extraction,0.555641770362854
translation,100,152,model,gqa,to make use of,external knowledge,gqa to make use of external knowledge,0.6571247577667236
translation,100,152,model,external knowledge,for,natural answer generation,external knowledge for natural answer generation,0.5600482821464539
translation,100,152,model,model,plug it into,gqa,model plug it into gqa,0.6214953064918518
translation,100,154,model,gqa,with,refined word embedding,gqa with refined word embedding,0.6241670250892639
translation,100,154,model,model,extend,gqa,model extend gqa,0.7367944121360779
translation,100,201,model,source selector,allows for,learning,source selector allows for learning,0.7565306425094604
translation,100,201,model,appropriate tradeoff,for blending,external knowledge,appropriate tradeoff for blending external knowledge,0.6855836510658264
translation,100,201,model,external knowledge,with,information,external knowledge with information,0.5932705402374268
translation,100,201,model,information,from,textual context,information from textual context,0.46415552496910095
translation,100,201,model,learning,has,appropriate tradeoff,learning has appropriate tradeoff,0.6048882603645325
translation,100,202,model,stochastic fact selection modules,to complete,answer,stochastic fact selection modules to complete answer,0.704770565032959
translation,100,202,model,answer,with,relevant facts,answer with relevant facts,0.6086049675941467
translation,100,202,model,model,has,related fact extraction,model has related fact extraction,0.5793501138687134
translation,100,150,results,significantly outperforms,demonstrates,effectiveness,significantly outperforms demonstrates effectiveness,0.7057499885559082
translation,100,150,results,significantly outperforms,demonstrates,benefit,significantly outperforms demonstrates benefit,0.7066885232925415
translation,100,150,results,gqa,demonstrates,benefit,gqa demonstrates benefit,0.6693148016929626
translation,100,150,results,benefit,of,knowledge integration,benefit of knowledge integration,0.5464314222335815
translation,100,150,results,keag,has,significantly outperforms,keag has significantly outperforms,0.6283997893333435
translation,100,150,results,significantly outperforms,has,gqa,significantly outperforms has gqa,0.5998128652572632
translation,100,150,results,results,has,result,results has result,0.5303357243537903
translation,100,164,results,keag model,surpasses,all the others,keag model surpasses all the others,0.6559851765632629
translation,100,164,results,all the others,in generating,correct answers,all the others in generating correct answers,0.6273934245109558
translation,100,164,results,correct answers,has,syntactically and substantively,correct answers has syntactically and substantively,0.5566802024841309
translation,100,164,results,results,has,keag model,results has keag model,0.5345411896705627
translation,100,165,results,keag and mhpgm,perform,well,keag and mhpgm perform well,0.6453628540039062
translation,100,165,results,syntactic correctness,has,keag and mhpgm,syntactic correctness has keag and mhpgm,0.5728563070297241
translation,100,165,results,results,In terms of,syntactic correctness,results In terms of syntactic correctness,0.7098578214645386
translation,100,166,results,all compared models,in generating,substantively correct answers,all compared models in generating substantively correct answers,0.6460446119308472
translation,100,166,results,keag,has,significantly outperforms,keag has significantly outperforms,0.6283997893333435
translation,100,166,results,significantly outperforms,has,all compared models,significantly outperforms has all compared models,0.5861613154411316
translation,100,166,results,results,has,keag,results has keag,0.5656212568283081
translation,101,6,model,logistic regression model,uses,33 syntactic features,logistic regression model uses 33 syntactic features,0.5846368074417114
translation,101,6,model,33 syntactic features,of,edit sequences,33 syntactic features of edit sequences,0.5726420879364014
translation,101,6,model,33 syntactic features,to classify,sentence pairs,33 syntactic features to classify sentence pairs,0.667014479637146
translation,101,6,model,model,describe,logistic regression model,model describe logistic regression model,0.6172207593917847
translation,101,165,results,tree edit model,produced,competitive results,tree edit model produced competitive results,0.6282997131347656
translation,101,165,results,tree edit model,has,did not outperform,tree edit model has did not outperform,0.629607081413269
translation,101,165,results,did not outperform,has,other systems,did not outperform has other systems,0.6142423748970032
translation,101,183,results,tree edit model,does not use,lexical semantics knowledge,tree edit model does not use lexical semantics knowledge,0.6268634796142578
translation,101,183,results,tree edit model,produced,best result reported to date,tree edit model produced best result reported to date,0.6247622966766357
translation,101,183,results,results,has,tree edit model,results has tree edit model,0.5506680607795715
translation,101,184,results,results,for,tree edit model,results for tree edit model,0.6153093576431274
translation,101,184,results,tree edit model,are,statistically significantly different,tree edit model are statistically significantly different,0.5969913005828857
translation,101,184,results,wang et al . ( 2007 ) system,with,wordnet ( p > 0.05 ),wang et al . ( 2007 ) system with wordnet ( p > 0.05 ),0.5923903584480286
translation,101,184,results,results,for,tree edit model,results for tree edit model,0.6153093576431274
translation,101,184,results,results,has,results,results has results,0.48582205176353455
translation,102,192,ablation-analysis,microaveraged accuracy,of,nlprolog,microaveraged accuracy of nlprolog,0.5883948802947998
translation,102,192,ablation-analysis,microaveraged accuracy,shows,absolute increase,microaveraged accuracy shows absolute increase,0.6925946474075317
translation,102,192,ablation-analysis,absolute increase,of,3.08 pp,absolute increase of 3.08 pp,0.5726202130317688
translation,102,192,ablation-analysis,bidaf ( fastqa ),augmented with,sent2vec,bidaf ( fastqa ) augmented with sent2vec,0.7159913182258606
translation,102,192,ablation-analysis,sent2vec,decreases by,3.26,sent2vec decreases by 3.26,0.6966197490692139
translation,102,192,ablation-analysis,3.26,has,( 3.63 ) pp,3.26 has ( 3.63 ) pp,0.5738098621368408
translation,102,7,model,multi-hop reasoning tasks,has,over natural language,multi-hop reasoning tasks has over natural language,0.5351088047027588
translation,102,8,model,prolog prover,to utilize,similarity function,prolog prover to utilize similarity function,0.7078567147254944
translation,102,8,model,similarity function,over,pretrained sentence encoders,similarity function over pretrained sentence encoders,0.6383658647537231
translation,102,8,model,model,propose to use,prolog prover,model propose to use prolog prover,0.6962316632270813
translation,102,9,model,representations,for,similarity function,representations for similarity function,0.6278666853904724
translation,102,9,model,representations,via,backpropagation,representations via backpropagation,0.6588841676712036
translation,102,9,model,model,fine - tune,representations,model fine - tune representations,0.7232758402824402
translation,102,10,model,rulebased reasoning,to,natural language,rulebased reasoning to natural language,0.42270419001579285
translation,102,10,model,domain-specific rules,from,training data,domain-specific rules from training data,0.5245431661605835
translation,102,10,model,model,induce,domain-specific rules,model induce domain-specific rules,0.6775875687599182
translation,102,24,model,system,combining,symbolic reasoner,system combining symbolic reasoner,0.6662534475326538
translation,102,24,model,rulelearning method,with,distributed sentence and entity representations,rulelearning method with distributed sentence and entity representations,0.5878300666809082
translation,102,24,model,distributed sentence and entity representations,to perform,rule- based multihop reasoning,distributed sentence and entity representations to perform rule- based multihop reasoning,0.6264143586158752
translation,102,24,model,rule- based multihop reasoning,on,natural language input,rule- based multihop reasoning on natural language input,0.543715238571167
translation,102,24,model,nlprolog,has,system,nlprolog has system,0.6303477883338928
translation,102,24,model,model,introduce,nlprolog,model introduce nlprolog,0.655278205871582
translation,102,25,model,nlpro - log,generates,partially interpretable and explain-able models,nlpro - log generates partially interpretable and explain-able models,0.653932511806488
translation,102,25,model,nlpro - log,allows for,easy incorporation,nlpro - log allows for easy incorporation,0.776349663734436
translation,102,25,model,easy incorporation,of,prior knowledge,easy incorporation of prior knowledge,0.587807297706604
translation,102,25,model,model,has,nlpro - log,model has nlpro - log,0.6309791803359985
translation,102,28,model,end-to - end differentiable sentence encoders,initialized with,pretrained sentence embeddings,end-to - end differentiable sentence encoders initialized with pretrained sentence embeddings,0.730090856552124
translation,102,28,model,end-to - end differentiable sentence encoders,finetuned on,downstream task,end-to - end differentiable sentence encoders finetuned on downstream task,0.7389994859695435
translation,102,28,model,model,use,end-to - end differentiable sentence encoders,model use end-to - end differentiable sentence encoders,0.6137670874595642
translation,102,29,model,differentiable fine-tuning objective,learning,domainspecific logic rules,differentiable fine-tuning objective learning domainspecific logic rules,0.7644433379173279
translation,102,29,model,model,has,differentiable fine-tuning objective,model has differentiable fine-tuning objective,0.5340375900268555
translation,102,171,results,nlprolog,achieving,same accuracy,nlprolog achieving same accuracy,0.6497287750244141
translation,102,171,results,same accuracy,as,best performing qa model,same accuracy as best performing qa model,0.5526944994926453
translation,102,171,results,all predicates but developer,has,nlprolog,all predicates but developer has nlprolog,0.6592018008232117
translation,102,171,results,nlprolog,has,strongly outperforms,nlprolog has strongly outperforms,0.6294987201690674
translation,102,171,results,strongly outperforms,has,all tested neural qa models,strongly outperforms has all tested neural qa models,0.5985597968101501
translation,102,171,results,results,For,all predicates but developer,results For all predicates but developer,0.6276863217353821
translation,102,172,results,nlpro - log,on,hidden test set,nlpro - log on hidden test set,0.5953333973884583
translation,102,172,results,nlpro - log,obtained,accuracy,nlpro - log obtained accuracy,0.6890329122543335
translation,102,172,results,hidden test set,of,medhop,hidden test set of medhop,0.6504513621330261
translation,102,172,results,accuracy,of,29.3 %,accuracy of 29.3 %,0.5652140974998474
translation,102,172,results,29.3 %,6.1 pp better than,fastqa,29.3 % 6.1 pp better than fastqa,0.8109694123268127
translation,102,172,results,29.3 %,18.5 pp worse than,bidaf,29.3 % 18.5 pp worse than bidaf,0.8038819432258606
translation,102,172,results,results,evaluated,nlpro - log,results evaluated nlpro - log,0.6740328073501587
translation,102,177,results,decreases,when,no rules can be used,decreases when no rules can be used,0.6978734731674194
translation,102,177,results,markedly,when,no rules can be used,markedly when no rules can be used,0.6958881616592407
translation,102,177,results,does not change,on,remaining two data sets,does not change on remaining two data sets,0.5641909837722778
translation,102,177,results,three of the five evaluated data sets,has,performance,three of the five evaluated data sets has performance,0.5649653077125549
translation,102,177,results,performance,has,decreases,performance has decreases,0.599746823310852
translation,102,177,results,decreases,has,markedly,decreases has markedly,0.6385740637779236
translation,102,177,results,results,On,three of the five evaluated data sets,results On three of the five evaluated data sets,0.4752528965473175
translation,103,149,ablation-analysis,disabling type-specific transformation,results in,? 1.3 % drop,disabling type-specific transformation results in ? 1.3 % drop,0.6157704591751099
translation,103,149,ablation-analysis,? 1.3 % drop,in,performance,? 1.3 % drop in performance,0.5507647395133972
translation,103,150,ablation-analysis,structured relational attention mechanism,is,critical,structured relational attention mechanism is critical,0.566163957118988
translation,103,150,ablation-analysis,ablation analysis,has,structured relational attention mechanism,ablation analysis has structured relational attention mechanism,0.5256950855255127
translation,103,128,baselines,knowledge - agnostic finetuning,of,pre-trained lms,knowledge - agnostic finetuning of pre-trained lms,0.5742307305335999
translation,103,128,baselines,models,incorporate,external kg,models incorporate external kg,0.7361620664596558
translation,103,134,baselines,rgcn,has,rn,rgcn has rn,0.6532888412475586
translation,103,134,baselines,baselines,take,rgcn,baselines take rgcn,0.5551994442939758
translation,103,135,baselines,gconattn,generalizes,"match -lstm ( wang and jiang , 2016 )","gconattn generalizes match -lstm ( wang and jiang , 2016 )",0.7157161235809326
translation,103,135,baselines,gconattn,achieves,success,gconattn achieves success,0.7362543344497681
translation,103,135,baselines,success,in,language inference tasks,success in language inference tasks,0.5019270181655884
translation,103,135,baselines,baselines,has,gconattn,baselines has gconattn,0.6110197901725769
translation,103,142,experiments,openbookqa,use,official split,openbookqa use official split,0.689539909362793
translation,103,142,experiments,openbookqa,build,models,openbookqa build models,0.7258689403533936
translation,103,142,experiments,models,with,roberta - large,models with roberta - large,0.6699721813201904
translation,103,142,experiments,roberta - large,as,text encoder,roberta - large as text encoder,0.5675980448722839
translation,103,5,model,novel knowledge - aware approach,equips,pretrained language models ( ptlms ),novel knowledge - aware approach equips pretrained language models ( ptlms ),0.6611150503158569
translation,103,5,model,pretrained language models ( ptlms ),with,multi-hop relational reasoning module,pretrained language models ( ptlms ) with multi-hop relational reasoning module,0.6322497129440308
translation,103,5,model,multi-hop relational reasoning module,named,multi-hop graph relation network ( mhgrn ),multi-hop relational reasoning module named multi-hop graph relation network ( mhgrn ),0.6886785626411438
translation,103,5,model,model,propose,novel knowledge - aware approach,model propose novel knowledge - aware approach,0.6961418390274048
translation,103,6,model,"multi-hop , multi-relational reasoning",over,subgraphs,"multi-hop , multi-relational reasoning over subgraphs",0.6275845170021057
translation,103,6,model,subgraphs,extracted from,external knowledge graphs,subgraphs extracted from external knowledge graphs,0.5373653769493103
translation,103,6,model,model,performs,"multi-hop , multi-relational reasoning","model performs multi-hop , multi-relational reasoning",0.6092055439949036
translation,103,7,model,proposed reasoning module,unifies,path- based reasoning methods,proposed reasoning module unifies path- based reasoning methods,0.6627786755561829
translation,103,7,model,proposed reasoning module,unifies,graph neural networks,proposed reasoning module unifies graph neural networks,0.641297459602356
translation,103,7,model,proposed reasoning module,results in,better interpretability and scalability,proposed reasoning module results in better interpretability and scalability,0.6185325384140015
translation,103,7,model,model,has,proposed reasoning module,model has proposed reasoning module,0.5975874662399292
translation,103,30,model,novel graph encoding architecture,combines,strengths,novel graph encoding architecture combines strengths,0.6985593438148499
translation,103,30,model,novel graph encoding architecture,has,multi-hop graph relation network ( mhgrn ),novel graph encoding architecture has multi-hop graph relation network ( mhgrn ),0.5665065050125122
translation,103,30,model,model,propose,novel graph encoding architecture,model propose novel graph encoding architecture,0.6743563413619995
translation,103,171,model,neural graph encoding,incorporates,attention mechanism,neural graph encoding incorporates attention mechanism,0.6650046110153198
translation,103,171,model,attention mechanism,in,feature aggregation,attention mechanism in feature aggregation,0.4935298562049866
translation,103,171,model,rgcn,proposes,relational message passing,rgcn proposes relational message passing,0.645829975605011
translation,103,171,model,model,has,neural graph encoding,model has neural graph encoding,0.5207880139350891
translation,103,31,results,model,inherits,scalability,model inherits scalability,0.7004294395446777
translation,103,31,results,scalability,from,gnns,scalability from gnns,0.5682125091552734
translation,103,31,results,scalability,by preserving,message passing formulation,scalability by preserving message passing formulation,0.7023380398750305
translation,103,31,results,results,has,model,results has model,0.5339115858078003
translation,103,141,results,kg - augmented models,achieve,performance gain,kg - augmented models achieve performance gain,0.6504631042480469
translation,103,141,results,performance gain,over,vanilla pre-trained lms,performance gain over vanilla pre-trained lms,0.667679488658905
translation,103,141,results,results,has,kg - augmented models,results has kg - augmented models,0.5042433142662048
translation,103,143,results,mhgrn,surpasses,all implemented baselines,mhgrn surpasses all implemented baselines,0.649315595626831
translation,103,143,results,all implemented baselines,with,absolute increase,all implemented baselines with absolute increase,0.6392199993133545
translation,103,143,results,absolute increase,on,test,absolute increase on test,0.5767925977706909
translation,103,143,results,of ?2 %,on,test,of ?2 % on test,0.6346645951271057
translation,103,143,results,absolute increase,has,of ?2 %,absolute increase has of ?2 %,0.578224778175354
translation,103,143,results,results,has,mhgrn,results has mhgrn,0.5470818877220154
translation,105,8,model,basic sequence - to-sequence qg model,with,"dynamic , paragraph -specific dictionary","basic sequence - to-sequence qg model with dynamic , paragraph -specific dictionary",0.6158238053321838
translation,105,8,model,basic sequence - to-sequence qg model,with,copy attention,basic sequence - to-sequence qg model with copy attention,0.6396865844726562
translation,105,8,model,persistent,across,corpus,persistent across corpus,0.7561632394790649
translation,105,8,model,persistent,without requiring,features,persistent without requiring features,0.7202252745628357
translation,105,8,model,model,augment,basic sequence - to-sequence qg model,model augment basic sequence - to-sequence qg model,0.6570555567741394
translation,105,33,model,model,propose,simple yet effective paragraph - level question generation,model propose simple yet effective paragraph - level question generation,0.6535073518753052
translation,105,34,model,standard sequence - to-sequence model,based on,bidirectional lstm,standard sequence - to-sequence model based on bidirectional lstm,0.5980243682861328
translation,105,34,model,bidirectional lstm,with,two components,bidirectional lstm with two components,0.6305896639823914
translation,105,34,model,persistent,across,paragraphs,persistent across paragraphs,0.6967968940734863
translation,105,34,model,two components,has,"dynamic , paragraph -specific dictionary","two components has dynamic , paragraph -specific dictionary",0.5552462935447693
translation,105,34,model,two components,has,copy attention mechanism,two components has copy attention mechanism,0.5516027212142944
translation,105,34,model,model,augment,standard sequence - to-sequence model,model augment standard sequence - to-sequence model,0.6604589223861694
translation,105,45,model,copy attention,enables,model,copy attention enables model,0.665336549282074
translation,105,45,model,model,to predict,question words,model to predict question words,0.7596707344055176
translation,105,45,model,question words,from,extended vocabulary ( complete vocabulary + paragraph vocabulary ),question words from extended vocabulary ( complete vocabulary + paragraph vocabulary ),0.5264418125152588
translation,105,45,model,model,has,copy attention,model has copy attention,0.5672178268432617
translation,105,46,model,copy attention,operates over,union of words,copy attention operates over union of words,0.7436813116073608
translation,105,46,model,union of words,in,vocabulary and paragraph words,union of words in vocabulary and paragraph words,0.5241746306419373
translation,105,46,model,model,has,copy attention,model has copy attention,0.5672178268432617
translation,105,48,model,two -layer bidirectional long short -term memory ( bi - lstm ) network stack,as,paragraph encoder,two -layer bidirectional long short -term memory ( bi - lstm ) network stack as paragraph encoder,0.5049620270729065
translation,105,48,model,model,use,two -layer bidirectional long short -term memory ( bi - lstm ) network stack,model use two -layer bidirectional long short -term memory ( bi - lstm ) network stack,0.6705690622329712
translation,105,36,results,6 - point increase,with respect to,bleu - 4,6 - point increase with respect to bleu - 4,0.6429951786994934
translation,105,36,results,bleu - 4,over,mpgsn 's best system,bleu - 4 over mpgsn 's best system,0.6357088088989258
translation,105,36,results,results,achieve,6 - point increase,results achieve 6 - point increase,0.6408251523971558
translation,105,37,results,human evaluation,of,model,human evaluation of model,0.5788296461105347
translation,105,37,results,human evaluation,with and without,copy attention,human evaluation with and without copy attention,0.6452196836471558
translation,105,37,results,model,with and without,copy attention,model with and without copy attention,0.6863027811050415
translation,105,37,results,model,with and without,copy attention,model with and without copy attention,0.6863027811050415
translation,105,37,results,27 % more relevant questions,when,copy attention,27 % more relevant questions when copy attention,0.5928816795349121
translation,105,90,results,state - of- the - art mpgsn,on,all metrics,state - of- the - art mpgsn on all metrics,0.5159493684768677
translation,105,90,results,our model,has,significantly outperforms,our model has significantly outperforms,0.6134821176528931
translation,105,90,results,significantly outperforms,has,state - of- the - art mpgsn,significantly outperforms has state - of- the - art mpgsn,0.5734122395515442
translation,105,90,results,results,has,our model,results has our model,0.5871725678443909
translation,105,91,results,improvements,on,bleu,improvements on bleu,0.4990209639072418
translation,105,91,results,bleu - 4 score,of,mpgsn,bleu - 4 score of mpgsn,0.5898841619491577
translation,105,91,results,mpgsn,is,16.38,mpgsn is 16.38,0.6028507351875305
translation,105,91,results,improvement,of,6.24,improvement of 6.24,0.5881565809249878
translation,105,91,results,improvement,of,38 %,improvement of 38 %,0.5949031710624695
translation,105,91,results,results,has,improvements,results has improvements,0.615561842918396
translation,105,98,results,copy attention,improves,performance,copy attention improves performance,0.672670304775238
translation,105,98,results,performance,especially on,relevance,performance especially on relevance,0.6997312903404236
translation,105,98,results,results,incorporation of,copy attention,results incorporation of copy attention,0.5719957947731018
translation,106,5,model,natural language strings,to automatically assemble,neural networks,natural language strings to automatically assemble neural networks,0.5760828256607056
translation,106,5,model,neural networks,from,collection of composable modules,neural networks from collection of composable modules,0.5559036731719971
translation,106,5,model,model,uses,natural language strings,model uses natural language strings,0.5243717432022095
translation,106,6,model,parameters,learned jointly with,network - assembly parameters,parameters learned jointly with network - assembly parameters,0.707477867603302
translation,106,6,model,network - assembly parameters,via,reinforcement learning,network - assembly parameters via reinforcement learning,0.6587827801704407
translation,106,6,model,reinforcement learning,with,"only ( world , question , answer ) triples","reinforcement learning with only ( world , question , answer ) triples",0.6464874148368835
translation,106,6,model,"only ( world , question , answer ) triples",as,supervision,"only ( world , question , answer ) triples as supervision",0.5148305892944336
translation,106,6,model,model,has,parameters,model has parameters,0.49046650528907776
translation,106,10,model,questions,to,dynamically assembled neural networks,questions to dynamically assembled neural networks,0.5775719285011292
translation,106,10,model,networks,to,world representations ( images or knowledge bases ),networks to world representations ( images or knowledge bases ),0.5356926321983337
translation,106,10,model,world representations ( images or knowledge bases ),to produce,answers,world representations ( images or knowledge bases ) to produce answers,0.7175136804580688
translation,106,10,model,model,translates from,questions,model translates from questions,0.6959307789802551
translation,106,10,model,model,applies,networks,model applies networks,0.6391379237174988
translation,106,12,model,neural networks,instead of,logical forms,neural networks instead of logical forms,0.5452271103858948
translation,106,12,model,our model,leverages,best aspects,our model leverages best aspects,0.7906308174133301
translation,106,12,model,best aspects,of both,linguistic compositionality,best aspects of both linguistic compositionality,0.6689983606338501
translation,106,12,model,best aspects,of both,continuous representations,best aspects of both continuous representations,0.7132517099380493
translation,106,12,model,neural networks,has,our model,neural networks has our model,0.5705316066741943
translation,106,12,model,logical forms,has,our model,logical forms has our model,0.5981902480125427
translation,106,12,model,model,By constructing,neural networks,model By constructing neural networks,0.630275309085846
translation,106,33,model,behavior,for,collection of heterogeneous modules,behavior for collection of heterogeneous modules,0.60542231798172
translation,106,33,model,collection of heterogeneous modules,from,"( world , question , answer ) triples","collection of heterogeneous modules from ( world , question , answer ) triples",0.5542191863059998
translation,106,33,model,model,learn,behavior,model learn behavior,0.7135955095291138
translation,106,38,model,model,call,dynamic neural module network,model call dynamic neural module network,0.6558767557144165
translation,106,7,results,our approach,term,dynamic neural module network,our approach term dynamic neural module network,0.6967935562133789
translation,106,7,results,our approach,achieves,state - of - theart results,our approach achieves state - of - theart results,0.5949847102165222
translation,106,7,results,dynamic neural module network,achieves,state - of - theart results,dynamic neural module network achieves state - of - theart results,0.6340262293815613
translation,106,7,results,state - of - theart results,on,benchmark datasets,state - of - theart results on benchmark datasets,0.48924705386161804
translation,106,7,results,benchmark datasets,in,visual and structured domains,benchmark datasets in visual and structured domains,0.46106022596359253
translation,106,7,results,results,has,our approach,results has our approach,0.6050099730491638
translation,106,159,results,dynamic networks,provides,small gain,dynamic networks provides small gain,0.555209755897522
translation,106,159,results,small gain,on,yes / no questions,small gain on yes / no questions,0.5375232696533203
translation,106,159,results,small gain,has,most noticeably,small gain has most noticeably,0.567348301410675
translation,106,159,results,results,use of,dynamic networks,results use of dynamic networks,0.6526556015014648
translation,106,160,results,previous approach,using,neural module networks,previous approach using neural module networks,0.6810638308525085
translation,106,160,results,state - of- theart results,has,outperforming,state - of- theart results has outperforming,0.548106849193573
translation,106,160,results,outperforming,has,highly effective visual bag-of-words model,outperforming has highly effective visual bag-of-words model,0.5462830066680908
translation,106,160,results,outperforming,has,previous approach,outperforming has previous approach,0.6428447961807251
translation,106,160,results,results,achieve,state - of- theart results,results achieve state - of- theart results,0.5616343021392822
translation,106,180,results,outperforms,both,logical ( lsp - f ) and perceptual models ( lsp - w ),outperforms both logical ( lsp - f ) and perceptual models ( lsp - w ),0.7128745913505554
translation,106,180,results,dynamic model ( d - nmn ),has,outperforms,dynamic model ( d - nmn ) has outperforms,0.5932046175003052
translation,106,180,results,results,has,dynamic model ( d - nmn ),results has dynamic model ( d - nmn ),0.5373795032501221
translation,106,181,results,dataset with quantifiers,where,dynamic structure prediction,dataset with quantifiers where dynamic structure prediction,0.5963459610939026
translation,106,181,results,dynamic structure prediction,produces,20 % relative improvement,dynamic structure prediction produces 20 % relative improvement,0.6385945081710815
translation,106,181,results,20 % relative improvement,over,fixed baseline,20 % relative improvement over fixed baseline,0.7136949896812439
translation,106,181,results,results,has,improvement,results has improvement,0.6248279809951782
translation,107,67,baselines,svm - based method,with,bag-of-words ( textual features ),svm - based method with bag-of-words ( textual features ),0.5921202898025513
translation,107,67,baselines,svm - based method,with,nontextual features,svm - based method with nontextual features,0.6407775282859802
translation,107,67,baselines,svm - based method,with,features,svm - based method with features,0.6428463459014893
translation,107,67,baselines,svm - based method,based on,topic model,svm - based method based on topic model,0.6551553606987
translation,107,67,baselines,features,based on,topic model,features based on topic model,0.5979896783828735
translation,107,67,baselines,topic model,"i.e. ,","latent dirichlet allocation , lda )","topic model i.e. , latent dirichlet allocation , lda )",0.637573778629303
translation,107,67,baselines,"svm ( huang et al. , 2007 )",has,svm - based method,"svm ( huang et al. , 2007 ) has svm - based method",0.5286332964897156
translation,107,68,baselines,crf - based method,using,same features,crf - based method using same features,0.690365195274353
translation,107,68,baselines,same features,as,svm approach,same features as svm approach,0.5688586235046387
translation,107,68,baselines,"crf ( ding et al. , 2008 )",has,crf - based method,"crf ( ding et al. , 2008 ) has crf - based method",0.5234086513519287
translation,107,68,baselines,baselines,has,"crf ( ding et al. , 2008 )","baselines has crf ( ding et al. , 2008 )",0.5187578201293945
translation,107,69,baselines,baselines,has,"dbn ( wang et al. , 2010 )","baselines has dbn ( wang et al. , 2010 )",0.5097571015357971
translation,107,80,hyperparameters,convolution,for,each layer,convolution for each layer,0.6119756102561951
translation,107,80,hyperparameters,convolution,window sizes of,pooling,convolution window sizes of pooling,0.7328831553459167
translation,107,80,hyperparameters,each layer,are,"[ 1 ? 1 , 2 ? 2 , 2 ? 2 ]","each layer are [ 1 ? 1 , 2 ? 2 , 2 ? 2 ]",0.5696373581886292
translation,107,80,hyperparameters,pooling,are,"[ 2 ? 2 , 2 ? 2 , 1 ? 1 ]","pooling are [ 2 ? 2 , 2 ? 2 , 1 ? 1 ]",0.5914849638938904
translation,107,80,hyperparameters,hyperparameters,window sizes of,convolution,hyperparameters window sizes of convolution,0.7248416543006897
translation,107,81,hyperparameters,lstm unit,size of,input gate,lstm unit size of input gate,0.7067669034004211
translation,107,81,hyperparameters,lstm unit,size of,"forget gate , output gate , and memory cell","lstm unit size of forget gate , output gate , and memory cell",0.6691796779632568
translation,107,81,hyperparameters,lstm unit,sizes of,"forget gate , output gate , and memory cell","lstm unit sizes of forget gate , output gate , and memory cell",0.6582057476043701
translation,107,81,hyperparameters,input gate,set to,200,input gate set to 200,0.7562934756278992
translation,107,81,hyperparameters,"forget gate , output gate , and memory cell",set to,360,"forget gate , output gate , and memory cell set to 360",0.6567802429199219
translation,107,81,hyperparameters,hyperparameters,For,lstm unit,hyperparameters For lstm unit,0.5319520831108093
translation,107,82,hyperparameters,stochastic gradient descent ( sgd ) algorithm,via,back - propagation,stochastic gradient descent ( sgd ) algorithm via back - propagation,0.632229745388031
translation,107,82,hyperparameters,back - propagation,through,time,back - propagation through time,0.6868414878845215
translation,107,82,hyperparameters,time,to train,model,time to train model,0.7241063714027405
translation,107,82,hyperparameters,hyperparameters,has,stochastic gradient descent ( sgd ) algorithm,hyperparameters has stochastic gradient descent ( sgd ) algorithm,0.5414143800735474
translation,107,84,hyperparameters,learning rate,initialized to be,0.01,learning rate initialized to be 0.01,0.6622319221496582
translation,107,84,hyperparameters,updated dynamically,according to,gradient descent,updated dynamically according to gradient descent,0.64107346534729
translation,107,84,hyperparameters,gradient descent,using,adadelta method,gradient descent using adadelta method,0.6524684429168701
translation,107,84,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,107,85,hyperparameters,activation functions,in,our model,activation functions in our model,0.4962279796600342
translation,107,85,hyperparameters,activation functions,adopt,rectified linear unit ( relu ),activation functions adopt rectified linear unit ( relu ),0.6259230375289917
translation,107,85,hyperparameters,", ? )",in,our model,", ? ) in our model",0.5783061981201172
translation,107,85,hyperparameters,", ? )",adopt,rectified linear unit ( relu ),", ? ) adopt rectified linear unit ( relu )",0.6557207703590393
translation,107,85,hyperparameters,our model,adopt,rectified linear unit ( relu ),our model adopt rectified linear unit ( relu ),0.62995845079422
translation,107,85,hyperparameters,activation functions,has,", ? )","activation functions has , ? )",0.5775501728057861
translation,107,85,hyperparameters,hyperparameters,has,activation functions,hyperparameters has activation functions,0.4861687421798706
translation,107,5,model,convolution neural networks ( cnns ),to learning,joint representation of questionanswer pair,convolution neural networks ( cnns ) to learning joint representation of questionanswer pair,0.56943678855896
translation,107,5,model,convolution neural networks ( cnns ),to learning,joint representation,convolution neural networks ( cnns ) to learning joint representation,0.5770854353904724
translation,107,5,model,joint representation,as input,long shortterm memory ( lstm ),joint representation as input long shortterm memory ( lstm ),0.7244262099266052
translation,107,5,model,long shortterm memory ( lstm ),to learn,answer sequence,long shortterm memory ( lstm ) to learn answer sequence,0.6288230419158936
translation,107,5,model,answer sequence,of,question,answer sequence of question,0.6271629929542542
translation,107,5,model,answer sequence,for labeling,matching quality,answer sequence for labeling matching quality,0.7189093232154846
translation,107,5,model,question,for labeling,matching quality,question for labeling matching quality,0.7166412472724915
translation,107,5,model,matching quality,of,each answer,matching quality of each answer,0.5802711248397827
translation,107,26,model,cnns,to learn,joint representation of question answer ( qa ) pair,cnns to learn joint representation of question answer ( qa ) pair,0.5892345309257507
translation,107,26,model,model,has,cnns,model has cnns,0.5418620109558105
translation,107,45,model,each step,uses,pre-trained word embeddings,each step uses pre-trained word embeddings,0.5693014860153198
translation,107,45,model,pre-trained word embeddings,to encode,sentences,pre-trained word embeddings to encode sentences,0.678312361240387
translation,107,45,model,pre-trained word embeddings,used as,input vectors,pre-trained word embeddings used as input vectors,0.523036777973175
translation,107,45,model,sentences,of,qa pair,sentences of qa pair,0.6214907765388489
translation,107,45,model,input vectors,of,model,input vectors of model,0.6191213726997375
translation,107,45,model,model,At,each step,model At each step,0.5607273578643799
translation,107,74,model,cnn,Using,word embedding,cnn Using word embedding,0.6544621586799622
translation,107,74,model,cnns based model,to learn,representations,cnns based model to learn representations,0.5900557637214661
translation,107,74,model,representations,of,questions and answers,representations of questions and answers,0.6054825186729431
translation,107,74,model,logistic regression classifier,to predict,class of answers,logistic regression classifier to predict class of answers,0.7444802522659302
translation,107,74,model,cnn,has,cnns based model,cnn has cnns based model,0.5525883436203003
translation,107,74,model,word embedding,has,cnns based model,word embedding has cnns based model,0.5252053737640381
translation,107,74,model,model,has,cnn,model has cnn,0.577612578868866
translation,107,79,model,qa joint representation learning,have,3 hidden layers,qa joint representation learning have 3 hidden layers,0.5588025450706482
translation,107,79,model,3 hidden layers,for modeling,question and answer sentence,3 hidden layers for modeling question and answer sentence,0.7145220041275024
translation,107,79,model,100 feature maps,for,convolution and pooling operators,100 feature maps for convolution and pooling operators,0.5704400539398193
translation,107,79,model,each layer,has,100 feature maps,each layer has 100 feature maps,0.5449385643005371
translation,107,79,model,model,has,cnns,model has cnns,0.5418620109558105
translation,107,98,model,sentence,into,one tensor,sentence into one tensor,0.6070781350135803
translation,107,98,model,representation,contains,more semantic features,representation contains more semantic features,0.6067156195640564
translation,107,98,model,more semantic features,than,bag-of-words representation,more semantic features than bag-of-words representation,0.5468235015869141
translation,107,98,model,bag-of-words representation,in,dbn and mdbn,bag-of-words representation in dbn and mdbn,0.500802218914032
translation,107,111,model,answer sequence learning model r-cnn,for,answer selection task,answer sequence learning model r-cnn for answer selection task,0.5454083681106567
translation,107,111,model,answer sequence learning model r-cnn,by integrating,lstm unit and cnns,answer sequence learning model r-cnn by integrating lstm unit and cnns,0.6003355979919434
translation,107,111,model,model,propose,answer sequence learning model r-cnn,model propose answer sequence learning model r-cnn,0.6342432498931885
translation,107,112,model,semantic link,between,successive answers,semantic link between successive answers,0.680670440196991
translation,107,89,results,outperforms,over,macro-averaged metrics,outperforms over macro-averaged metrics,0.6777682304382324
translation,107,89,results,competitor methods,over,macro-averaged metrics,competitor methods over macro-averaged metrics,0.6558945178985596
translation,107,89,results,proposed r-cnn approach,has,outperforms,proposed r-cnn approach has outperforms,0.5878128409385681
translation,107,89,results,outperforms,has,competitor methods,outperforms has competitor methods,0.5724404454231262
translation,107,89,results,results,clear to see that,proposed r-cnn approach,results clear to see that proposed r-cnn approach,0.6873418092727661
translation,107,91,results,joint representation,of,qa pair,joint representation of qa pair,0.6320644021034241
translation,107,91,results,joint representation,captures,richer matching patterns,joint representation captures richer matching patterns,0.7387789487838745
translation,107,91,results,qa pair,learnt by,cnns,qa pair learnt by cnns,0.720504641532898
translation,107,91,results,qa pair,captures,richer matching patterns,qa pair captures richer matching patterns,0.7844424247741699
translation,107,91,results,richer matching patterns,between,question and answer,richer matching patterns between question and answer,0.6819348335266113
translation,107,91,results,richer matching patterns,than,other methods,richer matching patterns than other methods,0.5937941074371338
translation,107,91,results,question and answer,than,other methods,question and answer than other methods,0.5381057858467102
translation,107,91,results,results,has,joint representation,results has joint representation,0.5504578351974487
translation,107,92,results,methods,based on,deep learning,methods based on deep learning,0.6178203225135803
translation,107,92,results,methods,perform,more powerful,methods perform more powerful,0.5855545997619629
translation,107,92,results,more powerful,than,svm and crf,more powerful than svm and crf,0.5792490243911743
translation,107,92,results,more powerful,especially for,"complicate answers ( e.g. , potential answers )","more powerful especially for complicate answers ( e.g. , potential answers )",0.6401086449623108
translation,107,92,results,results,notable that,methods,results notable that methods,0.5516669750213623
translation,107,93,results,svm and crf,using,large amount of features,svm and crf using large amount of features,0.6347140073776245
translation,107,93,results,svm and crf,perform,better,svm and crf perform better,0.6178354620933533
translation,107,93,results,better,for,answers,better for answers,0.6394248604774475
translation,107,93,results,better,that have,"obvious tendency ( e.g. , good and bad answers )","better that have obvious tendency ( e.g. , good and bad answers )",0.6159350275993347
translation,107,96,results,cnn and r - cnn,show,superiority,cnn and r - cnn show superiority,0.6104131937026978
translation,107,96,results,superiority,in modeling,qa pair,superiority in modeling qa pair,0.7139165997505188
translation,107,96,results,dbn and mdbn,has,cnn and r - cnn,dbn and mdbn has cnn and r - cnn,0.6010033488273621
translation,107,96,results,results,Compared to,dbn and mdbn,results Compared to dbn and mdbn,0.6513543128967285
translation,107,100,results,improvement,achieved by,r-cnn,improvement achieved by r-cnn,0.5743128657341003
translation,107,100,results,r-cnn,over,c-nn,r-cnn over c-nn,0.5808927416801453
translation,107,100,results,r-cnn,demonstrates that,answer sequence learning,r-cnn demonstrates that answer sequence learning,0.6420866847038269
translation,107,100,results,answer sequence learning,able to improve,performance,answer sequence learning able to improve performance,0.7237955927848816
translation,107,100,results,performance,of,answer selection,performance of answer selection,0.6134339570999146
translation,107,100,results,answer selection,in,cqa,answer selection in cqa,0.5009386539459229
translation,107,100,results,results,has,improvement,results has improvement,0.6248279809951782
translation,108,167,baselines,template - based baseline,is,relatively strong model,template - based baseline is relatively strong model,0.5183273553848267
translation,108,167,baselines,baselines,has,template - based baseline,baselines has template - based baseline,0.5417659282684326
translation,108,24,experiments,best performing model,to construct,new factoid question - answer corpus,best performing model to construct new factoid question - answer corpus,0.6380500793457031
translation,108,24,experiments,new factoid question - answer corpus,has,30m factoid question - answer corpus,new factoid question - answer corpus has 30m factoid question - answer corpus,0.578934371471405
translation,108,146,hyperparameters,neural network models,optimized,log-likelihood,neural network models optimized log-likelihood,0.6754475235939026
translation,108,146,hyperparameters,log-likelihood,using,"first-order gradient - based optimization algorithm adam ( kingma and ba , 2015 )","log-likelihood using first-order gradient - based optimization algorithm adam ( kingma and ba , 2015 )",0.6395448446273804
translation,108,146,hyperparameters,hyperparameters,To train,neural network models,hyperparameters To train neural network models,0.664189338684082
translation,108,149,hyperparameters,transe embeddings,with,embedding dimensionality,transe embeddings with embedding dimensionality,0.5885148644447327
translation,108,149,hyperparameters,200,for,"each subject , relationship and object","200 for each subject , relationship and object",0.6327605247497559
translation,108,149,hyperparameters,embedding dimensionality,has,200,embedding dimensionality has 200,0.5989653468132019
translation,108,149,hyperparameters,hyperparameters,trained,transe embeddings,hyperparameters trained transe embeddings,0.6979868412017822
translation,108,150,hyperparameters,neural network models,fixed,learning rate,neural network models fixed learning rate,0.7228313088417053
translation,108,150,hyperparameters,neural network models,fixed,clipped parameter gradients,neural network models fixed clipped parameter gradients,0.6664223074913025
translation,108,150,hyperparameters,learning rate,to,0.00025,learning rate to 0.00025,0.559012770652771
translation,108,150,hyperparameters,clipped parameter gradients,with,norms,clipped parameter gradients with norms,0.6521628499031067
translation,108,150,hyperparameters,norms,larger than,0.1,norms larger than 0.1,0.6420297622680664
translation,108,151,hyperparameters,embedding dimensionality,of,words,embedding dimensionality of words,0.5539857149124146
translation,108,151,hyperparameters,words,to be,200,words to be 200,0.6268294453620911
translation,108,151,hyperparameters,hidden state,of,decoder rnn,hidden state of decoder rnn,0.5664080381393433
translation,108,151,hyperparameters,decoder rnn,dimensionality,600,decoder rnn dimensionality 600,0.7326299548149109
translation,108,151,hyperparameters,hyperparameters,fixed,embedding dimensionality,hyperparameters fixed embedding dimensionality,0.6784464120864868
translation,108,151,hyperparameters,hyperparameters,fixed,hidden state,hyperparameters fixed hidden state,0.7413667440414429
translation,108,9,results,all evaluation criteria,has,questiongeneration model,all evaluation criteria has questiongeneration model,0.5524764060974121
translation,108,9,results,questiongeneration model,has,outperforms,questiongeneration model has outperforms,0.6286323070526123
translation,108,9,results,outperforms,has,competing template - based baseline,outperforms has competing template - based baseline,0.5668866634368896
translation,108,9,results,results,Across,all evaluation criteria,results Across all evaluation criteria,0.588687002658844
translation,108,166,results,outperform,by,clear margin,outperform by clear margin,0.6598119139671326
translation,108,166,results,templatebased baseline,by,clear margin,templatebased baseline by clear margin,0.5679417252540588
translation,108,166,results,clear margin,across,all metrics,clear margin across all metrics,0.7056918144226074
translation,108,166,results,neural network models,has,outperform,neural network models has outperform,0.6106013059616089
translation,108,166,results,outperform,has,templatebased baseline,outperform has templatebased baseline,0.5853198766708374
translation,108,166,results,results,has,neural network models,results has neural network models,0.5481110215187073
translation,108,171,results,best performing models,models where,transe,best performing models models where transe,0.7688461542129517
translation,108,171,results,transe,trained on,largest set of triples ( transe + + ),transe trained on largest set of triples ( transe + + ),0.6975070834159851
translation,108,171,results,results,appears that,best performing models,results appears that best performing models,0.6720954179763794
translation,109,143,baselines,- used sequence classification models,including,convolutional neural network ( cnn ),- used sequence classification models including convolutional neural network ( cnn ),0.6823174953460693
translation,109,143,baselines,- used sequence classification models,including,long -short term memory network ( lstm ),- used sequence classification models including long -short term memory network ( lstm ),0.676628589630127
translation,109,143,baselines,recurrent convolutional neural network ( rcnn ),has,"lai et al. , 2015","recurrent convolutional neural network ( rcnn ) has lai et al. , 2015",0.5224118232727051
translation,109,146,baselines,dynamic memory network ( dmn ),has,"kumar et al. , 2016","dynamic memory network ( dmn ) has kumar et al. , 2016",0.5309825539588928
translation,109,146,baselines,baselines,use,two representative neural networks,baselines use two representative neural networks,0.610931396484375
translation,109,7,experiments,new clarification dataset,with,nearly 40 k open-domain examples,new clarification dataset with nearly 40 k open-domain examples,0.5825877785682678
translation,109,7,experiments,new clarification dataset,has,claqua,new clarification dataset has claqua,0.6513705253601074
translation,109,21,experiments,open-domain clarification corpus,for,kbqa,open-domain clarification corpus for kbqa,0.5939199924468994
translation,109,53,experiments,best accuracy,is,74.7 %,best accuracy is 74.7 %,0.5400845408439636
translation,109,53,experiments,clarification - based question answering,has,best accuracy,clarification - based question answering has best accuracy,0.5705153346061707
translation,109,44,model,data annotation pipeline,divided into,three steps,data annotation pipeline divided into three steps,0.6188840270042419
translation,109,145,model,structured models,use,separate structures,structured models use separate structures,0.6580423712730408
translation,109,145,model,structured models,adopt,additional structure,structured models adopt additional structure,0.6711011528968811
translation,109,145,model,separate structures,to encode,different source information,separate structures to encode different source information,0.7458764910697937
translation,109,145,model,additional structure,to model,inter-relation,additional structure to model inter-relation,0.7361264824867249
translation,109,145,model,inter-relation,of,source information,inter-relation of source information,0.5822698473930359
translation,109,145,model,model,adopt,additional structure,model adopt additional structure,0.7317591309547424
translation,109,145,model,model,has,structured models,model has structured models,0.6119409203529358
translation,109,52,results,proposed coarse- to-fine model,achieves,bleu score,proposed coarse- to-fine model achieves bleu score,0.6562235951423645
translation,109,52,results,proposed coarse- to-fine model,better than,strong baseline models,proposed coarse- to-fine model better than strong baseline models,0.7220965623855591
translation,109,52,results,bleu score,of,45.02,bleu score of 45.02,0.53455650806427
translation,109,52,results,45.02,better than,strong baseline models,45.02 better than strong baseline models,0.7537676692008972
translation,109,52,results,clarification question generation,has,proposed coarse- to-fine model,clarification question generation has proposed coarse- to-fine model,0.5607904195785522
translation,109,52,results,results,For,clarification question generation,results For clarification question generation,0.6048048138618469
translation,109,180,results,structured models,have,best performance,structured models have best performance,0.5502914190292358
translation,109,180,results,best performance,with,86.6 % accuracy,best performance with 86.6 % accuracy,0.6202717423439026
translation,109,180,results,best performance,with,80.5 % accuracy,best performance with 80.5 % accuracy,0.6177635192871094
translation,109,180,results,86.6 % accuracy,on,single- turn cases,86.6 % accuracy on single- turn cases,0.5146462917327881
translation,109,180,results,80.5 % accuracy,on,multi-turn cases,80.5 % accuracy on multi-turn cases,0.5234079360961914
translation,109,181,results,structured architecture,brings,obvious performance improvements,structured architecture brings obvious performance improvements,0.6267674565315247
translation,109,181,results,obvious performance improvements,with,2.6 % and 6.7 % accuracy increases,obvious performance improvements with 2.6 % and 6.7 % accuracy increases,0.6446849703788757
translation,109,181,results,best-performing unstructured model,has,structured architecture,best-performing unstructured model has structured architecture,0.56326824426651
translation,109,181,results,results,Compared to,best-performing unstructured model,results Compared to best-performing unstructured model,0.6960130929946899
translation,109,183,results,models,achieve,better results,models achieve better results,0.6328539252281189
translation,109,183,results,better results,on,multi-turn cases,better results on multi-turn cases,0.552901566028595
translation,109,183,results,results,find that,models,results find that models,0.6197972297668457
translation,109,186,results,seq2seq,achieves,low bleu scores,seq2seq achieves low bleu scores,0.6704846024513245
translation,109,187,results,transformer,achieves,higher performance,transformer achieves higher performance,0.6709505319595337
translation,109,187,results,higher performance,than,seq2seq,higher performance than seq2seq,0.6031420826911926
translation,109,187,results,results,has,transformer,results has transformer,0.4226538836956024
translation,109,188,results,proposed coarse- to-fine model,demonstrates,new state of the art,proposed coarse- to-fine model demonstrates new state of the art,0.6071318984031677
translation,109,188,results,new state of the art,improving,current highest baseline result,new state of the art improving current highest baseline result,0.6720144152641296
translation,109,188,results,current highest baseline result,by,3.35 and 0.60 bleu scores,current highest baseline result by 3.35 and 0.60 bleu scores,0.5271949768066406
translation,109,188,results,results,has,proposed coarse- to-fine model,results has proposed coarse- to-fine model,0.5915963053703308
translation,109,189,results,multi-turn cases,obtain,higher bleu scores,multi-turn cases obtain higher bleu scores,0.5250696539878845
translation,109,189,results,higher bleu scores,than,single-turn cases,higher bleu scores than single-turn cases,0.5823878645896912
translation,109,189,results,results,find that,multi-turn cases,results find that multi-turn cases,0.6325893402099609
translation,109,195,results,unstructured models,perform,better,unstructured models perform better,0.6277729272842407
translation,109,195,results,better,than,structured models,better than structured models,0.6026215553283691
translation,110,166,results,overall accuracy,show,svm ranking model,overall accuracy show svm ranking model,0.5952808260917664
translation,110,166,results,svm ranking model,obtains,best performance,svm ranking model obtains best performance,0.6246840953826904
translation,110,166,results,svm ranking model,by far,best performance,svm ranking model by far best performance,0.6798045635223389
translation,110,166,results,best performance,on,both datasets,best performance on both datasets,0.45760658383369446
translation,110,166,results,substantial 10 %,higher than,cos,substantial 10 % higher than cos,0.7846916913986206
translation,110,166,results,results,has,overall accuracy,results has overall accuracy,0.5751331448554993
translation,110,167,results,random baseline,assigning,random similarity value,random baseline assigning random similarity value,0.6954150795936584
translation,110,167,results,random similarity value,to,each pair,random similarity value to each pair,0.5740313529968262
translation,110,167,results,each pair,questionsresults in,50 % accuracy,each pair questionsresults in 50 % accuracy,0.7085139155387878
translation,110,167,results,results,has,random baseline,results has random baseline,0.551530659198761
translation,111,68,baselines,"biomedical version of bert ( devlin et al. , 2019 )",is,"deeply bidirectional transformer ( vaswani et al. , 2017 )","biomedical version of bert ( devlin et al. , 2019 ) is deeply bidirectional transformer ( vaswani et al. , 2017 )",0.5487334132194519
translation,111,68,baselines,"deeply bidirectional transformer ( vaswani et al. , 2017 )",able to incorporate,rich context,"deeply bidirectional transformer ( vaswani et al. , 2017 ) able to incorporate rich context",0.6788721680641174
translation,111,68,baselines,rich context,into,encoding or embedding process,rich context into encoding or embedding process,0.6042184829711914
translation,111,68,baselines,rich context,pre-trained on,wikipedia and pubmed corpora,rich context pre-trained on wikipedia and pubmed corpora,0.7666743993759155
translation,111,68,baselines,encoding or embedding process,pre-trained on,wikipedia and pubmed corpora,encoding or embedding process pre-trained on wikipedia and pubmed corpora,0.777684211730957
translation,111,68,baselines,biobert,has,"biomedical version of bert ( devlin et al. , 2019 )","biobert has biomedical version of bert ( devlin et al. , 2019 )",0.5850294828414917
translation,112,165,results,our system,achieved,44 % reduction,our system achieved 44 % reduction,0.6931484937667847
translation,112,165,results,44 % reduction,in,error rate,44 % reduction in error rate,0.5270693898200989
translation,112,165,results,error rate,relative to both,"heilman and smith ,","error rate relative to both heilman and smith ,",0.7023595571517944
translation,112,165,results,error rate,relative to both,lindberg et al. system,error rate relative to both lindberg et al. system,0.6912047266960144
translation,112,165,results,results,has,our system,results has our system,0.5954442024230957
translation,114,4,model,knowledge graphs ( kg ),are,multi-relational graphs,knowledge graphs ( kg ) are multi-relational graphs,0.5563891530036926
translation,114,4,model,multi-relational graphs,consisting of,entities,multi-relational graphs consisting of entities,0.7277688384056091
translation,114,4,model,entities,as,nodes,entities as nodes,0.5266874432563782
translation,114,4,model,model,has,knowledge graphs ( kg ),model has knowledge graphs ( kg ),0.598268985748291
translation,114,6,model,model,has,multi-hop kgqa,model has multi-hop kgqa,0.5535217523574829
translation,114,37,model,novel system,leverages,kg embeddings,novel system leverages kg embeddings,0.7721818089485168
translation,114,37,model,kg embeddings,to perform,multi-hop kgqa,kg embeddings to perform multi-hop kgqa,0.6964961290359497
translation,114,37,model,embedkgqa,has,novel system,embedkgqa has novel system,0.596250057220459
translation,114,37,model,model,propose,embedkgqa,model propose embedkgqa,0.7235300540924072
translation,114,41,model,answer selection,from,pre-specified local neighborhood,answer selection from pre-specified local neighborhood,0.5685490369796753
translation,114,41,model,model,has,embedkgqa,model has embedkgqa,0.6394355893135071
translation,114,12,results,effective,in performing,multi-hop kgqa,effective in performing multi-hop kgqa,0.6529232263565063
translation,114,12,results,multi-hop kgqa,over,sparse kgs,multi-hop kgqa over sparse kgs,0.6809749603271484
translation,114,12,results,results,has,embedkgqa,results has embedkgqa,0.5952277183532715
translation,114,13,results,answer selection,from,prespecified neighborhood,answer selection from prespecified neighborhood,0.5598565340042114
translation,114,13,results,sub-optimal constraint,enforced by,previous multi-hop kgqa methods,sub-optimal constraint enforced by previous multi-hop kgqa methods,0.7498270869255066
translation,114,13,results,answer selection,has,sub-optimal constraint,answer selection has sub-optimal constraint,0.5562665462493896
translation,114,13,results,prespecified neighborhood,has,sub-optimal constraint,prespecified neighborhood has sub-optimal constraint,0.5407314300537109
translation,114,13,results,results,has,embedkgqa,results has embedkgqa,0.5952277183532715
translation,114,40,results,effective,in performing,multi-hop kgqa,effective in performing multi-hop kgqa,0.6529232263565063
translation,114,40,results,multi-hop kgqa,over,sparse kgs,multi-hop kgqa over sparse kgs,0.6809749603271484
translation,114,40,results,results,has,embedkgqa,results has embedkgqa,0.5952277183532715
translation,115,139,ablation-analysis,accuracy,on,testunseen,accuracy on testunseen,0.584169864654541
translation,115,139,ablation-analysis,testunseen,improves to,76.0 %,testunseen improves to 76.0 %,0.7052504420280457
translation,115,139,ablation-analysis,76.0 %,/,69.5 %,76.0 % / 69.5 %,0.6170026063919067
translation,115,139,ablation-analysis,simplest form of adapter,has,accuracy,simplest form of adapter has accuracy,0.543940007686615
translation,115,139,ablation-analysis,ablation analysis,With,simplest form of adapter,ablation analysis With simplest form of adapter,0.6296500563621521
translation,115,157,ablation-analysis,relation detection,find that,boost,relation detection find that boost,0.6383609771728516
translation,115,157,ablation-analysis,boost,of,relation detection,boost of relation detection,0.5690850019454956
translation,115,157,ablation-analysis,boost,lead to,improvement,boost lead to improvement,0.7104038000106812
translation,115,157,ablation-analysis,improvement,of,kbqa system,improvement of kbqa system,0.580299973487854
translation,115,157,ablation-analysis,ablation analysis,result of,relation detection,ablation analysis result of relation detection,0.7018837332725525
translation,115,110,hyperparameters,"rm - prop ( tieleman and hinton , 2012 )",as,optimization strategy,"rm - prop ( tieleman and hinton , 2012 ) as optimization strategy",0.519837498664856
translation,115,110,hyperparameters,optimization strategy,to train,proposed adapter,optimization strategy to train proposed adapter,0.7054398655891418
translation,115,110,hyperparameters,hyperparameters,use,"rm - prop ( tieleman and hinton , 2012 )","hyperparameters use rm - prop ( tieleman and hinton , 2012 )",0.5864705443382263
translation,115,111,hyperparameters,learning rate,set as,10 ?4,learning rate set as 10 ?4,0.6457265615463257
translation,115,111,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,115,112,hyperparameters,batch size,as,256,batch size as 256,0.5569828748703003
translation,115,112,hyperparameters,hyperparameters,set,batch size,hyperparameters set batch size,0.6402808427810669
translation,115,113,hyperparameters,parameters,of,discriminator,parameters of discriminator,0.564517080783844
translation,115,113,hyperparameters,discriminator,into,"[ ? c , c ]","discriminator into [ ? c , c ]",0.5804883241653442
translation,115,113,hyperparameters,discriminator,into,0.1,discriminator into 0.1,0.5465312600135803
translation,115,114,hyperparameters,dropout rate,set as,0.2,dropout rate set as 0.2,0.565522313117981
translation,115,114,hyperparameters,0.2,to regularize,adapter,0.2 to regularize adapter,0.6945852637290955
translation,115,114,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,115,117,hyperparameters,dimension,of,relation representation,dimension of relation representation,0.6036003232002258
translation,115,117,hyperparameters,dimension,is,300,dimension is 300,0.659263551235199
translation,115,117,hyperparameters,relation representation,is,300,relation representation is 300,0.5659348964691162
translation,115,117,hyperparameters,hyperparameters,has,dimension,hyperparameters has dimension,0.514893651008606
translation,115,118,hyperparameters,dimension,for,hidden state,dimension for hidden state,0.5811557173728943
translation,115,118,hyperparameters,hidden state,of,bi-lstm,hidden state of bi-lstm,0.5831786394119263
translation,115,118,hyperparameters,bi-lstm,set to,256,bi-lstm set to 256,0.6477510333061218
translation,115,118,hyperparameters,hyperparameters,has,dimension,hyperparameters has dimension,0.514893651008606
translation,115,119,hyperparameters,parameters,in,neural models,parameters in neural models,0.5151941776275635
translation,115,119,hyperparameters,initialized,using,uniform sampling,initialized using uniform sampling,0.6961369514465332
translation,115,119,hyperparameters,hyperparameters,has,parameters,hyperparameters has parameters,0.4783959984779358
translation,115,120,hyperparameters,negative sampled relations,is,256,negative sampled relations is 256,0.5890971422195435
translation,115,121,hyperparameters,hinge loss,set to,0.1,hinge loss set to 0.1,0.6600267887115479
translation,115,9,model,simple mapping method,named,representation adapter,simple mapping method named representation adapter,0.735115647315979
translation,115,9,model,simple mapping method,to learn,representation mapping,simple mapping method to learn representation mapping,0.5911979675292969
translation,115,9,model,representation mapping,for,seen and unseen relations,representation mapping for seen and unseen relations,0.6198462247848511
translation,115,9,model,seen and unseen relations,based on,previously learned relation embedding,seen and unseen relations based on previously learned relation embedding,0.6038643717765808
translation,115,9,model,model,propose,simple mapping method,model propose simple mapping method,0.6360837817192078
translation,115,10,model,reconstruction objective,to improve,mapping performance,reconstruction objective to improve mapping performance,0.6793851256370544
translation,115,10,model,model,employ,adversarial objective,model employ adversarial objective,0.5754043459892273
translation,115,10,model,model,employ,reconstruction objective,model employ reconstruction objective,0.5951769948005676
translation,115,38,model,mapping mechanism,called,representation adapter,mapping mechanism called representation adapter,0.653770923614502
translation,115,38,model,mapping mechanism,called,adapter,mapping mechanism called adapter,0.6783949732780457
translation,115,38,model,mapping mechanism,to incorporate,learned representations,mapping mechanism to incorporate learned representations,0.6810271143913269
translation,115,38,model,learned representations,into,relation detection model,learned representations into relation detection model,0.537762463092804
translation,115,38,model,model,propose,mapping mechanism,model propose mapping mechanism,0.6663407683372498
translation,115,39,model,simple mean square error loss,for,non-trivial training,simple mean square error loss for non-trivial training,0.6000072360038757
translation,115,39,model,simple mean square error loss,propose to incorporate,adversarial and reconstruction objectives,simple mean square error loss propose to incorporate adversarial and reconstruction objectives,0.6862329840660095
translation,115,39,model,non-trivial training,of,adapter,non-trivial training of adapter,0.5820987820625305
translation,115,39,model,adversarial and reconstruction objectives,to improve,training process,adversarial and reconstruction objectives to improve training process,0.6809214949607849
translation,115,39,model,model,start with,simple mean square error loss,model start with simple mean square error loss,0.6870700716972351
translation,115,39,model,model,propose to incorporate,adversarial and reconstruction objectives,model propose to incorporate adversarial and reconstruction objectives,0.7472221851348877
translation,115,93,model,proposed adapter g,on,relation - level representations,proposed adapter g on relation - level representations,0.5869688391685486
translation,115,93,model,proposed adapter g,to solve,unseen relation detection problem,proposed adapter g to solve unseen relation detection problem,0.6877534985542297
translation,115,93,model,),on,relation - level representations,) on relation - level representations,0.5572316646575928
translation,115,93,model,proposed adapter g,has,),proposed adapter g has ),0.622852087020874
translation,115,93,model,model,employ,proposed adapter g,model employ proposed adapter g,0.5797755718231201
translation,115,134,results,model,achieves,much better results,model achieves much better results,0.6717555522918701
translation,115,134,results,much better results,on,test-unseen,much better results on test-unseen,0.5531517267227173
translation,115,134,results,model,has,without finetuning,model has without finetuning,0.6087285876274109
translation,115,134,results,results,training,model,results training model,0.7113556861877441
translation,115,144,results,all the techniques,gets,score,all the techniques gets score,0.6793732047080994
translation,115,144,results,score,of,77.3 %,score of 77.3 %,0.5284609794616699
translation,115,144,results,score,of,84.9 %,score of 84.9 %,0.5287625789642334
translation,115,144,results,77.3 %,/,73.0 %,77.3 % / 73.0 %,0.6100487112998962
translation,115,144,results,73.0 %,on,test-unseen,73.0 % on test-unseen,0.5144996047019958
translation,115,144,results,84.9 %,on,union,84.9 % on union,0.5536583662033081
translation,115,144,results,/ 81.1 %,on,union,/ 81.1 % on union,0.5854013562202454
translation,115,144,results,union,of,test-seen and test-unseen,union of test-seen and test-unseen,0.6197801232337952
translation,115,144,results,test-seen and test-unseen,in,micro / macro average accuracy,test-seen and test-unseen in micro / macro average accuracy,0.5431317090988159
translation,115,144,results,77.3 %,has,73.0 %,77.3 % has 73.0 %,0.5685533285140991
translation,115,144,results,84.9 %,has,/ 81.1 %,84.9 % has / 81.1 %,0.5663228631019592
translation,115,144,results,results,using,all the techniques,results using all the techniques,0.6470383405685425
translation,115,146,results,results,of,our model,results of our model,0.6090980172157288
translation,115,146,results,our model,on,testseen,our model on testseen,0.6344885230064392
translation,115,146,results,testseen,are,slightly lower,testseen are slightly lower,0.6583224534988403
translation,115,146,results,slightly lower,than,hr - bilstm,slightly lower than hr - bilstm,0.6169775128364563
translation,115,146,results,results,notice,results,results notice results,0.6149131655693054
translation,115,146,results,results,notice,our model,results notice our model,0.7440767884254456
translation,115,146,results,results,of,our model,results of our model,0.6090980172157288
translation,115,156,results,proposed adapter method,improve,kbqa,proposed adapter method improve kbqa,0.6571052670478821
translation,115,156,results,kbqa,from,48.5 % to 63.7 %,kbqa from 48.5 % to 63.7 %,0.5440756678581238
translation,115,171,results,perform better,than,hr - bilstm,perform better than hr - bilstm,0.6196829676628113
translation,115,171,results,different number of relations,has,our model,different number of relations has our model,0.5641206502914429
translation,115,171,results,our model,has,perform better,our model has perform better,0.5975356101989746
translation,115,192,results,joint - nre *,further improve,unseen relation detection performance,joint - nre * further improve unseen relation detection performance,0.6646751165390015
translation,115,192,results,unseen relation detection performance,has,77.5 % v.s. 77.3 %,unseen relation detection performance has 77.5 % v.s. 77.3 %,0.5634374618530273
translation,115,192,results,results,using,joint - nre *,results using joint - nre *,0.6278684139251709
translation,116,166,baselines,random,Given,post,random Given post,0.7669769525527954
translation,116,166,baselines,post,randomly permute,set of 10 candidate questions uniformly,post randomly permute set of 10 candidate questions uniformly,0.7585310339927673
translation,116,5,model,model,build,neural network model,model build neural network model,0.6810835003852844
translation,116,26,model,ranking clarification question,built on,framework of expected value of perfect information,ranking clarification question built on framework of expected value of perfect information,0.616495668888092
translation,116,27,model,novel dataset,derived from,stackexchange,novel dataset derived from stackexchange,0.6752313375473022
translation,116,27,model,novel dataset,to learn,model,novel dataset to learn model,0.6228769421577454
translation,116,27,model,model,to ask,clarifying questions,model to ask clarifying questions,0.7042540311813354
translation,116,27,model,clarifying questions,by looking at,types of questions people,clarifying questions by looking at types of questions people,0.6616300940513611
translation,116,27,model,types of questions people,has,ask,types of questions people has ask,0.5366676449775696
translation,116,168,model,set of 10 question and answer candidates,construct,bag-of-ngrams representation,set of 10 question and answer candidates construct bag-of-ngrams representation,0.6734753251075745
translation,116,168,model,bag-of-ngrams representation,for,"post , question and answer","bag-of-ngrams representation for post , question and answer",0.5991013050079346
translation,116,168,model,model,Given,post,model Given post,0.7719433307647705
translation,116,168,model,model,construct,bag-of-ngrams representation,model construct bag-of-ngrams representation,0.6509240865707397
translation,116,178,model,"neural( p , q , a )",has,input,"neural( p , q , a ) has input",0.564468264579773
translation,116,178,model,model,has,"neural( p , q , a )","model has neural( p , q , a )",0.6004090309143066
translation,116,30,results,outperforms,evaluated against,expert human annotations,outperforms evaluated against expert human annotations,0.7377945184707642
translation,116,30,results,baseline models,evaluated against,expert human annotations,baseline models evaluated against expert human annotations,0.6957048177719116
translation,116,30,results,evpi model,has,outperforms,evpi model has outperforms,0.6354491114616394
translation,116,30,results,outperforms,has,baseline models,outperforms has baseline models,0.5940393209457397
translation,116,30,results,results,find that,evpi model,results find that evpi model,0.6536442637443542
translation,116,165,results,evpi formalism,provide leverage,similarly expressive feedforward network,evpi formalism provide leverage similarly expressive feedforward network,0.6590176224708557
translation,116,165,results,useful,in identifying,right question ?,useful in identifying right question ?,0.6616910099983215
translation,116,165,results,right question ?,models perform when evaluated on,candidate questions,right question ? models perform when evaluated on candidate questions,0.6899173259735107
translation,116,165,results,candidate questions,excluding,original,candidate questions excluding original,0.738632082939148
translation,116,165,results,answers,has,useful,answers has useful,0.5893651247024536
translation,116,165,results,results,Does,evpi formalism,results Does evpi formalism,0.3108006417751312
translation,116,186,results,non-neural baselines,find that,bag-of-ngrams baseline,non-neural baselines find that bag-of-ngrams baseline,0.5753355622291565
translation,116,186,results,bag-of-ngrams baseline,performs,slightly better,bag-of-ngrams baseline performs slightly better,0.5625207424163818
translation,116,186,results,slightly better,than,random,slightly better than random,0.6104652881622314
translation,116,186,results,results,Among,non-neural baselines,results Among non-neural baselines,0.5912598967552185
translation,116,187,results,community qa baseline,performs,better,community qa baseline performs better,0.6133033037185669
translation,116,187,results,better,than,"neural baseline ( neural ( p , q ) )","better than neural baseline ( neural ( p , q ) )",0.5767076015472412
translation,116,187,results,results,has,community qa baseline,results has community qa baseline,0.5470646023750305
translation,116,188,results,neural baselines with answers,has,"neural ( p , q , a )","neural baselines with answers has neural ( p , q , a )",0.593450129032135
translation,116,188,results,neural baselines with answers,has,outperform,neural baselines with answers has outperform,0.6272832155227661
translation,116,188,results,outperform,has,neural baseline without answers,outperform has neural baseline without answers,0.5982125401496887
translation,116,188,results,neural baseline without answers,has,"neural ( p , q ) )","neural baseline without answers has neural ( p , q ) )",0.6110869646072388
translation,116,188,results,results,has,neural baselines with answers,results has neural baselines with answers,0.6024457812309265
translation,116,189,results,"neural ( p , q , a ) baseline",across,most metrics,"neural ( p , q , a ) baseline across most metrics",0.6804409623146057
translation,116,189,results,evpi,has,outperforms,evpi has outperforms,0.6507065892219543
translation,116,189,results,outperforms,has,"neural ( p , q , a ) baseline","outperforms has neural ( p , q , a ) baseline",0.5871414542198181
translation,116,189,results,results,has,evpi,results has evpi,0.5459231734275818
translation,116,196,results,bag-of-ngrams baseline,performs,similar,bag-of-ngrams baseline performs similar,0.6128250956535339
translation,116,196,results,similar,to,random,similar to random,0.6488715410232544
translation,116,196,results,results,has,bag-of-ngrams baseline,results has bag-of-ngrams baseline,0.5233740210533142
translation,116,197,results,community qa baseline,has,outperforms,community qa baseline has outperforms,0.6071341037750244
translation,116,197,results,outperforms,has,"neural ( p , q ) model","outperforms has neural ( p , q ) model",0.6124317049980164
translation,116,197,results,results,has,community qa baseline,results has community qa baseline,0.5470646023750305
translation,116,198,results,neural baselines,make use of,answer,neural baselines make use of answer,0.6409316658973694
translation,116,198,results,neural baselines,make use of,answer,neural baselines make use of answer,0.6409316658973694
translation,116,198,results,outperform,does not use,answer,outperform does not use answer,0.7651952505111694
translation,116,198,results,evpi model,performs,significantly better,evpi model performs significantly better,0.6204902529716492
translation,116,198,results,significantly better,than,"neural ( p , q , a )","significantly better than neural ( p , q , a )",0.5727570056915283
translation,116,198,results,answer,has,outperform,answer has outperform,0.6766027212142944
translation,116,203,results,neural models,beat,non-neural baselines,neural models beat non-neural baselines,0.7255911827087402
translation,116,203,results,results,find that,neural models,results find that neural models,0.606279730796814
translation,116,204,results,differences,between,all the neural models,differences between all the neural models,0.683176577091217
translation,116,204,results,all the neural models,are,statistically insignificant,all the neural models are statistically insignificant,0.5806646347045898
translation,116,204,results,results,has,differences,results has differences,0.5601350665092468
translation,117,211,results,average em,is,0.0691,average em is 0.0691,0.6038756966590881
translation,117,211,results,average f1,is,0.1001,average f1 is 0.1001,0.5767531991004944
translation,117,211,results,0.1001,on,training dataset,0.1001 on training dataset,0.5185978412628174
translation,117,211,results,results,has,open source implementation,results has open source implementation,0.5079003572463989
translation,118,186,ablation-analysis,question bodies,improves,performance,question bodies improves performance,0.729105532169342
translation,118,186,ablation-analysis,performance,of,rcnn model,performance of rcnn model,0.5552489757537842
translation,118,186,ablation-analysis,performance,achieving,1 % to 3 % improvement,performance achieving 1 % to 3 % improvement,0.6794767379760742
translation,118,186,ablation-analysis,1 % to 3 % improvement,with,both model variations,1 % to 3 % improvement with both model variations,0.616733729839325
translation,118,186,ablation-analysis,ablation analysis,inclusion of,question bodies,ablation analysis inclusion of question bodies,0.7477151155471802
translation,118,156,baselines,questions,using,cosine similarity,questions using cosine similarity,0.671048641204834
translation,118,156,baselines,cosine similarity,based on,vector-based word representation,cosine similarity based on vector-based word representation,0.6526169776916504
translation,118,156,baselines,vector-based word representation,for,each question,vector-based word representation for each question,0.5957714319229126
translation,118,165,experimental-setup,neural network models,used,"adam ( kingma and ba , 2015 )","neural network models used adam ( kingma and ba , 2015 )",0.5797638893127441
translation,118,165,experimental-setup,"adam ( kingma and ba , 2015 )",as,optimization method,"adam ( kingma and ba , 2015 ) as optimization method",0.5344868898391724
translation,118,165,experimental-setup,optimization method,with,default setting,optimization method with default setting,0.6201276779174805
translation,118,166,experimental-setup,learning rate,has,"1e ? 3 , 3e ? 4 }","learning rate has 1e ? 3 , 3e ? 4 }",0.5908178687095642
translation,118,166,experimental-setup,"dropout ( hinton et al. , 2012 ) probability",has,"0.1 , 0.2 , 0.3 }","dropout ( hinton et al. , 2012 ) probability has 0.1 , 0.2 , 0.3 }",0.5753356218338013
translation,118,166,experimental-setup,cnn feature width,has,"2 , 3 , 4 }","cnn feature width has 2 , 3 , 4 }",0.5788514018058777
translation,118,166,experimental-setup,experimental setup,optimized,other hyper-parameters,experimental setup optimized other hyper-parameters,0.722502589225769
translation,118,171,experimental-setup,"word2vec ( mikolov et al. , 2013 )",to obtain,200 - dimensional word embeddings,"word2vec ( mikolov et al. , 2013 ) to obtain 200 - dimensional word embeddings",0.5032305121421814
translation,118,171,experimental-setup,200 - dimensional word embeddings,using,all stack exchange data,200 - dimensional word embeddings using all stack exchange data,0.6371248960494995
translation,118,171,experimental-setup,200 - dimensional word embeddings,using,large wikipedia corpus,200 - dimensional word embeddings using large wikipedia corpus,0.5446024537086487
translation,118,171,experimental-setup,all stack exchange data,has,excluding stack - overflow,all stack exchange data has excluding stack - overflow,0.5889544486999512
translation,118,171,experimental-setup,experimental setup,ran,"word2vec ( mikolov et al. , 2013 )","experimental setup ran word2vec ( mikolov et al. , 2013 )",0.49838247895240784
translation,118,172,experimental-setup,fixed,to avoid,over-fitting,fixed to avoid over-fitting,0.6868876814842224
translation,118,172,experimental-setup,over-fitting,across,all experiments,over-fitting across all experiments,0.7100407481193542
translation,118,172,experimental-setup,experimental setup,has,word vectors,experimental setup has word vectors,0.5082172751426697
translation,118,7,model,recurrent and convolutional model,to effectively map,questions,recurrent and convolutional model to effectively map questions,0.6839361786842346
translation,118,7,model,gated convolution ),to effectively map,questions,gated convolution ) to effectively map questions,0.6797980070114136
translation,118,7,model,questions,to,semantic representations,questions to semantic representations,0.5260907411575317
translation,118,7,model,recurrent and convolutional model,has,gated convolution ),recurrent and convolutional model has gated convolution ),0.5992067456245422
translation,118,7,model,model,design,recurrent and convolutional model,model design recurrent and convolutional model,0.5493831634521484
translation,118,8,model,models,pre-trained within,encoder-decoder framework,models pre-trained within encoder-decoder framework,0.7147353291511536
translation,118,8,model,models,fine-tuned discriminatively from,limited annotations,models fine-tuned discriminatively from limited annotations,0.7239483594894409
translation,118,8,model,encoder-decoder framework,on the basis of,entire raw corpus,encoder-decoder framework on the basis of entire raw corpus,0.6452840566635132
translation,118,8,model,model,pre-trained within,encoder-decoder framework,model pre-trained within encoder-decoder framework,0.7274496555328369
translation,118,8,model,model,has,models,model has models,0.5616568326950073
translation,118,36,model,model,design,neural network model,model design neural network model,0.5652236938476562
translation,118,37,model,encoder,to map,"title , body , or the combination","encoder to map title , body , or the combination",0.7473260164260864
translation,118,39,model,several departures,from,typical architectures,several departures from typical architectures,0.6372108459472656
translation,118,39,model,typical architectures,on,finer level,typical architectures on finer level,0.5466599464416504
translation,118,39,model,model,introduce,several departures,model introduce several departures,0.718647301197052
translation,118,40,model,adaptive gating,in,non-consecutive cnns,adaptive gating in non-consecutive cnns,0.5605524778366089
translation,118,40,model,adaptive gating,to focus,temporal averaging,adaptive gating to focus temporal averaging,0.6771858334541321
translation,118,40,model,temporal averaging,in,models,temporal averaging in models,0.5373683571815491
translation,118,40,model,temporal averaging,on,key pieces,temporal averaging on key pieces,0.5526902675628662
translation,118,40,model,models,on,key pieces,models on key pieces,0.5538251996040344
translation,118,40,model,key pieces,of,questions,key pieces of questions,0.5742175579071045
translation,118,40,model,model,incorporate,adaptive gating,model incorporate adaptive gating,0.7567557096481323
translation,118,57,model,training paradigm,utilizes,entire corpus,training paradigm utilizes entire corpus,0.6287621855735779
translation,118,57,model,entire corpus,of,unannotated questions,entire corpus of unannotated questions,0.5372920036315918
translation,118,57,model,entire corpus,in,semi-supervised manner,entire corpus in semi-supervised manner,0.5193725228309631
translation,118,57,model,model,propose,training paradigm,model propose training paradigm,0.6386357545852661
translation,118,49,results,our full model,achieves,mrr,our full model achieves mrr,0.7385820746421814
translation,118,49,results,our full model,achieves,4 %,our full model achieves 4 %,0.6842586398124695
translation,118,49,results,mrr,of,75.6 %,mrr of 75.6 %,0.5476124286651611
translation,118,49,results,mrr,of,62.0 %,mrr of 62.0 %,0.5379005670547485
translation,118,49,results,mrr,of,4 %,mrr of 4 %,0.5924130082130432
translation,118,49,results,mrr,yielding,8 % absolute improvement,mrr yielding 8 % absolute improvement,0.6661534905433655
translation,118,49,results,mrr,yielding,4 %,mrr yielding 4 %,0.6812502145767212
translation,118,49,results,p@1,of,62.0 %,p@1 of 62.0 %,0.6007835268974304
translation,118,49,results,8 % absolute improvement,over,standard ir baseline,8 % absolute improvement over standard ir baseline,0.6560627222061157
translation,118,49,results,4 %,over,standard neural network architectures,4 % over standard neural network architectures,0.6570137739181519
translation,118,49,results,standard neural network architectures,including,"cnns , lstms and grus","standard neural network architectures including cnns , lstms and grus",0.6825515627861023
translation,118,49,results,results,has,our full model,results has our full model,0.5411279201507568
translation,118,175,results,best performance,across,all metrics,best performance across all metrics,0.6618552207946777
translation,118,175,results,best performance,on both,dev and test sets,best performance on both dev and test sets,0.6589852571487427
translation,118,175,results,our full model,has,rcnns with pre-training,our full model has rcnns with pre-training,0.5594490170478821
translation,118,175,results,results,show that,our full model,results show that our full model,0.5052582025527954
translation,118,176,results,full model,gets,p@1,full model gets p@1,0.6023731827735901
translation,118,176,results,p@1,of,62.0 %,p@1 of 62.0 %,0.6007835268974304
translation,118,176,results,p@1,on,test set,p@1 on test set,0.5698051452636719
translation,118,176,results,62.0 %,on,test set,62.0 % on test set,0.535435676574707
translation,118,176,results,word matching - based method bm25,by,over 8 percent points,word matching - based method bm25 by over 8 percent points,0.5916327238082886
translation,118,176,results,outperforming,has,word matching - based method bm25,outperforming has word matching - based method bm25,0.6046832203865051
translation,118,176,results,results,has,full model,results has full model,0.523858904838562
translation,118,177,results,baselines,across,all metrics,baselines across all metrics,0.6647815108299255
translation,118,177,results,rcnn model,has,outperforms,rcnn model has outperforms,0.6166774034500122
translation,118,177,results,outperforms,has,other neural encoder models,outperforms has other neural encoder models,0.5885334014892578
translation,118,177,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
translation,118,177,results,results,has,rcnn model,results has rcnn model,0.5463765859603882
translation,118,181,results,rcnn model,has,outperforms,rcnn model has outperforms,0.6166774034500122
translation,118,181,results,outperforms,has,other neural models,outperforms has other neural models,0.5875930786132812
translation,118,182,results,last hidden state,as,final representation,last hidden state as final representation,0.5104162096977234
translation,118,182,results,better results,for,rcnn model,better results for rcnn model,0.6036711931228638
translation,118,182,results,results,using,last hidden state,results using last hidden state,0.6988197565078735
translation,118,184,results,performance,of,tf - idf baseline,performance of tf - idf baseline,0.5076480507850647
translation,118,184,results,performance,of,rcnn model,performance of rcnn model,0.5552489757537842
translation,118,184,results,rcnn model,when using,question titles only,rcnn model when using question titles only,0.6741170883178711
translation,118,184,results,rcnn model,when using,question titles,rcnn model when using question titles,0.6890471577644348
translation,118,184,results,rcnn model,when using,question titles,rcnn model when using question titles,0.6890471577644348
translation,118,184,results,question titles,along with,question bodies,question titles along with question bodies,0.5761685371398926
translation,118,184,results,results,compares,performance,results compares performance,0.7928104996681213
translation,118,185,results,tf - idf 's performance,changes,very little,tf - idf 's performance changes very little,0.7044200897216797
translation,118,185,results,very little,when,question bodies,very little when question bodies,0.7442951798439026
translation,118,185,results,question bodies,included,mrr,question bodies included mrr,0.6566476821899414
translation,118,185,results,results,has,tf - idf 's performance,results has tf - idf 's performance,0.5498272776603699
translation,118,193,results,representations,generated by,rcnn encoder,representations generated by rcnn encoder,0.6135846972465515
translation,118,193,results,representations,perform,quite well,representations perform quite well,0.6166802048683167
translation,118,193,results,quite well,resulting in,perplexity,quite well resulting in perplexity,0.6389310956001282
translation,118,193,results,perplexity,of,25 and over 68 % mrr,perplexity of 25 and over 68 % mrr,0.5778547525405884
translation,118,193,results,25 and over 68 % mrr,without,subsequent fine-tuning,25 and over 68 % mrr without subsequent fine-tuning,0.7424207329750061
translation,118,194,results,lstm and gru networks,obtain,similar perplexity,lstm and gru networks obtain similar perplexity,0.5674548745155334
translation,118,194,results,lstm and gru networks,achieve,much worse mrr,lstm and gru networks achieve much worse mrr,0.603067934513092
translation,118,194,results,similar perplexity,on,heldout set,similar perplexity on heldout set,0.50869220495224
translation,118,194,results,much worse mrr,for,similar question retrieval,much worse mrr for similar question retrieval,0.6153611540794373
translation,118,194,results,results,has,lstm and gru networks,results has lstm and gru networks,0.5525712966918945
translation,119,210,ablation-analysis,ablation experiments,show that,performance,ablation experiments show that performance,0.5228453278541565
translation,119,210,ablation-analysis,drops,when,nmt paraphrases,drops when nmt paraphrases,0.701198160648346
translation,119,210,ablation-analysis,nmt paraphrases,are,removed,nmt paraphrases are removed,0.6108905673027039
translation,119,210,ablation-analysis,performance,has,drops,performance has drops,0.5993483662605286
translation,119,210,ablation-analysis,drops,has,most,drops has most,0.6665229797363281
translation,119,179,baselines,first baseline,uses,base qa models,first baseline uses base qa models,0.6361061930656433
translation,119,179,baselines,baselines,has,first baseline,baselines has first baseline,0.595583438873291
translation,119,182,baselines,third baseline ( dataaugment ),employs,paraphrases,third baseline ( dataaugment ) employs paraphrases,0.5912560224533081
translation,119,182,baselines,paraphrases,for,data augmentation,paraphrases for data augmentation,0.6181538105010986
translation,119,182,baselines,data augmentation,during,training,data augmentation during training,0.7118473052978516
translation,119,182,baselines,baselines,has,third baseline ( dataaugment ),baselines has third baseline ( dataaugment ),0.5756312608718872
translation,119,252,baselines,cnt,has,word matching features,cnt has word matching features,0.5692717432975769
translation,119,252,baselines,baselines,has,cnt,baselines has cnt,0.61871737241745
translation,119,157,experimental-setup,paraphrase scoring model,used,"glove ( pennington et al. , 2014 ) vectors","paraphrase scoring model used glove ( pennington et al. , 2014 ) vectors",0.5696261525154114
translation,119,157,experimental-setup,"glove ( pennington et al. , 2014 ) vectors",pretrained on,wikipedia 2014 and gigaword,"glove ( pennington et al. , 2014 ) vectors pretrained on wikipedia 2014 and gigaword",0.7794057130813599
translation,119,157,experimental-setup,wikipedia 2014 and gigaword,to initialize,word embedding matrix,wikipedia 2014 and gigaword to initialize word embedding matrix,0.6937963962554932
translation,119,157,experimental-setup,experimental setup,For,paraphrase scoring model,experimental setup For paraphrase scoring model,0.5779418349266052
translation,119,163,experimental-setup,dimensions,of,hidden vectors and word embeddings,dimensions of hidden vectors and word embeddings,0.5156239867210388
translation,119,163,experimental-setup,dimensions,selected from,"{ 50 , 100 , 200 } and { 100 , 200 }","dimensions selected from { 50 , 100 , 200 } and { 100 , 200 }",0.629310131072998
translation,119,163,experimental-setup,hidden vectors and word embeddings,selected from,"{ 50 , 100 , 200 } and { 100 , 200 }","hidden vectors and word embeddings selected from { 50 , 100 , 200 } and { 100 , 200 }",0.5741745233535767
translation,119,163,experimental-setup,experimental setup,has,dimensions,experimental setup has dimensions,0.4716714024543762
translation,119,164,experimental-setup,dropout rate,selected from,"{ 0.2 , 0.3 , 0.4 }","dropout rate selected from { 0.2 , 0.3 , 0.4 }",0.5751639604568481
translation,119,164,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,119,166,experimental-setup,parameters,randomly initialized from,"uniform distribution u ( ?0.08 , 0.08 )","parameters randomly initialized from uniform distribution u ( ?0.08 , 0.08 )",0.7230663299560547
translation,119,166,experimental-setup,experimental setup,has,parameters,experimental setup has parameters,0.4818422794342041
translation,119,167,experimental-setup,learning rate and decay rate,of,rmsprop,learning rate and decay rate of rmsprop,0.5842811465263367
translation,119,167,experimental-setup,rmsprop,were,0.01 and 0.95,rmsprop were 0.01 and 0.95,0.6040905117988586
translation,119,167,experimental-setup,experimental setup,has,learning rate and decay rate,experimental setup has learning rate and decay rate,0.5055229663848877
translation,119,168,experimental-setup,batch size,set to,150,batch size set to 150,0.7718634605407715
translation,119,168,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,119,169,experimental-setup,gradient norm,clipped to,5,gradient norm clipped to 5,0.6723389029502869
translation,119,169,experimental-setup,exploding gradient problem,has,gradient norm,exploding gradient problem has gradient norm,0.47669845819473267
translation,119,169,experimental-setup,experimental setup,To alleviate,exploding gradient problem,experimental setup To alleviate exploding gradient problem,0.6584437489509583
translation,119,49,experiments,range of paraphrase models,based on,paraphrase database ( ppdb,range of paraphrase models based on paraphrase database ( ppdb,0.6252647638320923
translation,119,49,experiments,range of paraphrase models,based on,neural machine translation,range of paraphrase models based on neural machine translation,0.5992313027381897
translation,119,49,experiments,rules,mined from,wikianswers corpus,rules mined from wikianswers corpus,0.5936704874038696
translation,119,5,model,paraphrases,present,general framework,paraphrases present general framework,0.6637699604034424
translation,119,5,model,general framework,learns,felicitous paraphrases,general framework learns felicitous paraphrases,0.6204560995101929
translation,119,5,model,felicitous paraphrases,for,various qa tasks,felicitous paraphrases for various qa tasks,0.5958354473114014
translation,119,5,model,model,turn to,paraphrases,model turn to paraphrases,0.7394744753837585
translation,119,5,model,model,present,general framework,model present general framework,0.6815142035484314
translation,119,5,model,model,learns,felicitous paraphrases,model learns felicitous paraphrases,0.7170357704162598
translation,119,6,model,end-toend,using,question - answer pairs,end-toend using question - answer pairs,0.6768569350242615
translation,119,6,model,question - answer pairs,as,supervision signal,question - answer pairs as supervision signal,0.5087800621986389
translation,119,6,model,model,trained,end-toend,model trained end-toend,0.76163250207901
translation,119,40,model,general framework,for learning,paraphrases,general framework for learning paraphrases,0.720929741859436
translation,119,40,model,paraphrases,for,question answering tasks,paraphrases for question answering tasks,0.5785316228866577
translation,119,40,model,model,present,general framework,model present general framework,0.6815142035484314
translation,119,41,model,probability distribution,over,candidate answers,probability distribution over candidate answers,0.6847801208496094
translation,119,41,model,model,Given,natural language question,model Given natural language question,0.6700777411460876
translation,119,44,model,paraphrases and the original question,fed into,qa model,paraphrases and the original question fed into qa model,0.6448453664779663
translation,119,44,model,qa model,predicts,distribution,qa model predicts distribution,0.7822369337081909
translation,119,44,model,distribution,over,answers,distribution over answers,0.7464317679405212
translation,119,44,model,answers,given,question,answers given question,0.7911509275436401
translation,119,44,model,model,has,paraphrases and the original question,model has paraphrases and the original question,0.5559870600700378
translation,119,45,model,end-to - end,using,question - answer pairs,end-to - end using question - answer pairs,0.6720443964004517
translation,119,45,model,question - answer pairs,as,supervision signal,question - answer pairs as supervision signal,0.5087800621986389
translation,119,50,results,three datasets,show,our framework,three datasets show our framework,0.5919396877288818
translation,119,50,results,our framework,has,consistently improves,our framework has consistently improves,0.6006665825843811
translation,119,50,results,consistently improves,has,performance,consistently improves has performance,0.5987896919250488
translation,119,176,results,paraphrase,compared to,ppdb,paraphrase compared to ppdb,0.6676375865936279
translation,119,176,results,paraphrase,has,more,paraphrase has more,0.6410611867904663
translation,119,190,results,classification accuracy,on,dev set,classification accuracy on dev set,0.5720568299293518
translation,119,190,results,dev set,was,80.6 %,dev set was 80.6 %,0.6285520792007446
translation,119,190,results,results,has,classification accuracy,results has classification accuracy,0.540421724319458
translation,119,201,results,outperforms,do not employ,paraphrase scoring,outperforms do not employ paraphrase scoring,0.7048672437667847
translation,119,201,results,baselines,do not employ,paraphrases,baselines do not employ paraphrases,0.7277747988700867
translation,119,201,results,baselines,do not employ,paraphrase scoring,baselines do not employ paraphrase scoring,0.6952394247055054
translation,119,201,results,para4qa,has,outperforms,para4qa has outperforms,0.6254183650016785
translation,119,201,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
translation,119,201,results,paraphrases,has,simplegraph,paraphrases has simplegraph,0.5818962454795837
translation,119,201,results,paraphrase scoring,has,avgpara,paraphrase scoring has avgpara,0.589801013469696
translation,119,201,results,not jointly trained,has,seppara,not jointly trained has seppara,0.6405564546585083
translation,119,201,results,results,observe,para4qa,results observe para4qa,0.6117913722991943
translation,119,202,results,previous state of the art,by,wide margin,previous state of the art by wide margin,0.5606529116630554
translation,119,202,results,graphquestions,has,our model para4qa,graphquestions has our model para4qa,0.6150410771369934
translation,119,202,results,our model para4qa,has,outperforms,our model para4qa has outperforms,0.623715341091156
translation,119,202,results,outperforms,has,previous state of the art,outperforms has previous state of the art,0.5638121962547302
translation,119,202,results,results,On,graphquestions,results On graphquestions,0.5404661297798157
translation,119,209,results,para4qa,has,outperforms,para4qa has outperforms,0.6254183650016785
translation,119,209,results,outperforms,has,related baselines,outperforms has related baselines,0.5923151969909668
translation,119,209,results,results,observe,para4qa,results observe para4qa,0.6117913722991943
translation,119,211,results,word matching features,used,para4qa,word matching features used para4qa,0.5829692482948303
translation,119,211,results,para4qa,reaches,state of the art performance,para4qa reaches state of the art performance,0.6594177484512329
translation,119,211,results,word matching features,has,para4qa,word matching features has para4qa,0.5775583982467651
translation,119,230,results,improvements,for,both types of questions,improvements for both types of questions,0.635014533996582
translation,119,230,results,impact,on,simple questions,impact on simple questions,0.5767451524734497
translation,119,230,results,impact,being,more pronounced,impact being more pronounced,0.6803173422813416
translation,119,230,results,simple questions,being,more pronounced,simple questions being more pronounced,0.6764678955078125
translation,119,230,results,results,observe,improvements,results observe improvements,0.6329426169395447
translation,120,210,ablation-analysis,auxiliary loss function,on,selected contents,auxiliary loss function on selected contents,0.5108354687690735
translation,120,210,ablation-analysis,selected contents,can make,big difference,selected contents can make big difference,0.6963645219802856
translation,120,210,ablation-analysis,ablation analysis,observe that,auxiliary loss function,ablation analysis observe that auxiliary loss function,0.5997326374053955
translation,120,211,ablation-analysis,learning tricks,about,cvae,learning tricks about cvae,0.704535722732544
translation,120,211,ablation-analysis,cvae,contribute to,more informative latent variable,cvae contribute to more informative latent variable,0.6431837677955627
translation,120,211,ablation-analysis,cvae,improve,diversity,cvae improve diversity,0.6064383387565613
translation,120,211,ablation-analysis,ablation analysis,has,learning tricks,ablation analysis has learning tricks,0.5586747527122498
translation,120,165,baselines,mixture decoder,has,"shen et al. , 2019 )","mixture decoder has shen et al. , 2019 )",0.5655503273010254
translation,120,165,baselines,mixture content selection,has,"cho et al. , 2019 )","mixture content selection has cho et al. , 2019 )",0.5679405927658081
translation,120,156,experimental-setup,dimension,of,word embedding,dimension of word embedding,0.5654559135437012
translation,120,156,experimental-setup,dimension,of,hidden size,dimension of hidden size,0.5720718502998352
translation,120,156,experimental-setup,word embedding,to,300,word embedding to 300,0.591537356376648
translation,120,156,experimental-setup,hidden size,to,512,hidden size to 512,0.6113746166229248
translation,120,156,experimental-setup,experimental setup,set,dimension,experimental setup set dimension,0.59503173828125
translation,120,156,experimental-setup,experimental setup,set,hidden size,experimental setup set hidden size,0.6804612278938293
translation,120,157,experimental-setup,representations,of,lexical features and focus indicator,representations of lexical features and focus indicator,0.5732783079147339
translation,120,157,experimental-setup,representations,randomly initialized as,16 - dimensional vectors,representations randomly initialized as 16 - dimensional vectors,0.600434422492981
translation,120,157,experimental-setup,lexical features and focus indicator,randomly initialized as,16 - dimensional vectors,lexical features and focus indicator randomly initialized as 16 - dimensional vectors,0.5878591537475586
translation,120,157,experimental-setup,experimental setup,has,representations,experimental setup has representations,0.47335103154182434
translation,120,158,experimental-setup,dimension,of,latent variable z,dimension of latent variable z,0.5998904705047607
translation,120,158,experimental-setup,hidden size,of,question type predictor,hidden size of question type predictor,0.5887031555175781
translation,120,158,experimental-setup,hidden size,set to,128,hidden size set to 128,0.7163619995117188
translation,120,158,experimental-setup,question type predictor,set to,128,question type predictor set to 128,0.6877020001411438
translation,120,158,experimental-setup,experimental setup,has,dimension,experimental setup has dimension,0.4825572967529297
translation,120,159,experimental-setup,number of layers,for,rnn,number of layers for rnn,0.6062538623809814
translation,120,159,experimental-setup,rnn,set to,1,rnn set to 1,0.6603472232818604
translation,120,159,experimental-setup,1,in,encoder,1 in encoder,0.5512129068374634
translation,120,159,experimental-setup,1,in,decoder,1 in decoder,0.5738803744316101
translation,120,159,experimental-setup,experimental setup,has,number of layers,experimental setup has number of layers,0.5300881266593933
translation,120,160,experimental-setup,model parameters,using,"adam optimizer ( kingma and ba , 2014 )","model parameters using adam optimizer ( kingma and ba , 2014 )",0.6350218653678894
translation,120,160,experimental-setup,"adam optimizer ( kingma and ba , 2014 )",with,learning rate,"adam optimizer ( kingma and ba , 2014 ) with learning rate",0.5956289768218994
translation,120,160,experimental-setup,"adam optimizer ( kingma and ba , 2014 )",with,momentum parameters,"adam optimizer ( kingma and ba , 2014 ) with momentum parameters",0.5889694094657898
translation,120,160,experimental-setup,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,120,160,experimental-setup,momentum parameters,has,? 1 = 0.9 and ? 1 = 0.999,momentum parameters has ? 1 = 0.9 and ? 1 = 0.999,0.558273196220398
translation,120,160,experimental-setup,experimental setup,update,model parameters,experimental setup update model parameters,0.7341856956481934
translation,120,161,experimental-setup,batch size,set to,64,batch size set to 64,0.7451491355895996
translation,120,161,experimental-setup,64,during,training,64 during training,0.7107371687889099
translation,120,161,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,120,7,model,model,explore,diversity of question generation,model explore diversity of question generation,0.691653847694397
translation,120,8,model,contextual focuses,with,content selectors,contextual focuses with content selectors,0.6518074870109558
translation,120,8,model,continuous latent variable,with,conditional variational auto-encoder ( cvae ),continuous latent variable with conditional variational auto-encoder ( cvae ),0.6375662684440613
translation,120,8,model,continuous latent variable,technique of,conditional variational auto-encoder ( cvae ),continuous latent variable technique of conditional variational auto-encoder ( cvae ),0.6680149435997009
translation,120,8,model,model,relate,contextual focuses,model relate contextual focuses,0.7299902439117432
translation,120,9,model,multimodal prior distribution,to allow for,more diverse content selectors,multimodal prior distribution to allow for more diverse content selectors,0.6693504452705383
translation,120,9,model,cvae,has,multimodal prior distribution,cvae has multimodal prior distribution,0.5134511590003967
translation,120,9,model,model,realization of,cvae,model realization of cvae,0.7032754421234131
translation,120,191,results,quality of generated questions,with,our method ( n - m. prior ),quality of generated questions with our method ( n - m. prior ),0.6040146946907043
translation,120,191,results,quality of generated questions,scores,comparable bleu - 4,quality of generated questions scores comparable bleu - 4,0.7362616658210754
translation,120,191,results,comparable bleu - 4,to,state - of - the - art,comparable bleu - 4 to state - of - the - art,0.5240069031715393
translation,120,191,results,comparable bleu - 4,is,much superior,comparable bleu - 4 is much superior,0.5549478530883789
translation,120,191,results,much superior,compared with,methods,much superior compared with methods,0.7281460762023926
translation,120,191,results,methods,based on,beam search and sampling,methods based on beam search and sampling,0.6280336976051331
translation,120,192,results,our method,performs,evidently better,our method performs evidently better,0.6208404302597046
translation,120,192,results,evidently better,than,other mixture models,evidently better than other mixture models,0.5798022747039795
translation,120,192,results,evidently better,resulting in,best trade - off,evidently better resulting in best trade - off,0.7108058929443359
translation,120,192,results,best trade - off,between,diversity and quality,best trade - off between diversity and quality,0.6504273414611816
translation,120,192,results,diversity,has,our method,diversity has our method,0.6187639236450195
translation,120,192,results,results,perspective of,diversity,results perspective of diversity,0.6468315720558167
translation,120,193,results,measurements,concerning,question types,measurements concerning question types,0.6253296732902527
translation,120,193,results,measurements,find that,our model,measurements find that our model,0.6832533478736877
translation,120,193,results,our model,demonstrates,significant improvements,our model demonstrates significant improvements,0.6333850622177124
translation,120,193,results,significant improvements,from,coverage and the diversity,significant improvements from coverage and the diversity,0.6016837954521179
translation,120,193,results,significant improvements,both,coverage and the diversity,significant improvements both coverage and the diversity,0.7273041605949402
translation,120,193,results,results,focusing on,measurements,results focusing on measurements,0.6914232969284058
translation,120,193,results,results,find that,our model,results find that our model,0.6804299354553223
translation,120,208,results,proposed diversity - promoting algorithm,clearly improve,generation diversity,proposed diversity - promoting algorithm clearly improve generation diversity,0.6121990084648132
translation,120,208,results,generation diversity,with nearly no negative impact,quality,generation diversity with nearly no negative impact quality,0.7196072340011597
translation,120,208,results,decay,is,small,decay is small,0.6316981315612793
translation,120,209,results,content selection,incorporating,influence,content selection incorporating influence,0.7198143005371094
translation,120,209,results,influence,in,encoder-decoder architecture,influence in encoder-decoder architecture,0.5681267380714417
translation,120,209,results,influence,improves,overall metric,influence improves overall metric,0.7313689589500427
translation,120,209,results,encoder-decoder architecture,improves,overall metric,encoder-decoder architecture improves overall metric,0.6397901177406311
translation,120,215,results,"number of prior modes ( k = 1 , 3 , 5 )",has,an effect,"number of prior modes ( k = 1 , 3 , 5 ) has an effect",0.5610339641571045
translation,120,215,results,an effect,on,metrics,an effect on metrics,0.5977779030799866
translation,120,215,results,metrics,when generating,"multiple questions ( n = 3 , 5 )","metrics when generating multiple questions ( n = 3 , 5 )",0.6921631693840027
translation,120,215,results,"number of prior modes ( k = 1 , 3 , 5 )",has,an effect,"number of prior modes ( k = 1 , 3 , 5 ) has an effect",0.5610339641571045
translation,120,216,results,multimodal prior,ability to improve,generation diversity,multimodal prior ability to improve generation diversity,0.6825948357582092
translation,120,216,results,generation diversity,compared with,standard one,generation diversity compared with standard one,0.7030425071716309
translation,120,216,results,results,see that,multimodal prior,results see that multimodal prior,0.6468043327331543
translation,120,217,results,almost all of the metrics,are,better,almost all of the metrics are better,0.5803587436676025
translation,120,217,results,setting n = k,has,almost all of the metrics,setting n = k has almost all of the metrics,0.5798973441123962
translation,121,38,model,knowledge - aware neural baseline,utilize,open book f,knowledge - aware neural baseline utilize open book f,0.652947723865509
translation,121,38,model,knowledge - aware neural baseline,utilize,common knowledge,knowledge - aware neural baseline utilize common knowledge,0.5769219398498535
translation,121,38,model,common knowledge,retrieved from,sources,common knowledge retrieved from sources,0.6441364288330078
translation,121,38,model,sources,such as,conceptnet,sources such as conceptnet,0.5894619822502136
translation,121,38,model,model,propose,knowledge - aware neural baseline,model propose knowledge - aware neural baseline,0.665389358997345
translation,121,9,results,human performance,on,openbookqa,human performance on openbookqa,0.5819869041442871
translation,121,9,results,openbookqa,close to,92 %,openbookqa close to 92 %,0.7467262148857117
translation,121,9,results,results,has,human performance,results has human performance,0.5620672106742859
translation,121,40,results,both f and k,increases,accuracy,both f and k increases accuracy,0.7388826608657837
translation,121,40,results,both f and k,still far from,human level performance,both f and k still far from human level performance,0.6995184421539307
translation,121,40,results,accuracy,to,76 %,accuracy to 76 %,0.5599386096000671
translation,121,40,results,results,Using,both f and k,results Using both f and k,0.6563631892204285
translation,122,65,baselines,random baseline,assigns,random label,random baseline assigns random label,0.7186822891235352
translation,122,65,baselines,random label,to,each sentence,random label to each sentence,0.5667409896850586
translation,122,65,baselines,baselines,has,random baseline,baselines has random baseline,0.5988871455192566
translation,122,68,baselines,lreg w/ bow,is,logistic regression model,lreg w/ bow is logistic regression model,0.5976603627204895
translation,122,68,baselines,logistic regression model,with,bag-of-words features,logistic regression model with bag-of-words features,0.6170275807380676
translation,122,68,baselines,baselines,has,lreg w/ bow,baselines has lreg w/ bow,0.5614394545555115
translation,122,69,baselines,lreg w/ para .- level,is,feature - rich lreg model,lreg w/ para .- level is feature - rich lreg model,0.555938720703125
translation,122,69,baselines,lreg w/ para .- level,is,features,lreg w/ para .- level is features,0.5859453082084656
translation,122,69,baselines,features,include,sentence length,features include sentence length,0.5549610257148743
translation,122,69,baselines,features,include,position of sentence,features include position of sentence,0.5451409816741943
translation,122,69,baselines,features,include,number of named entities in the sentence,features include number of named entities in the sentence,0.5317812561988831
translation,122,69,baselines,features,include,number of sentences in the paragraph,features include number of sentences in the paragraph,0.5501865148544312
translation,122,69,baselines,features,include,sentence - tosentence cohesion,features include sentence - tosentence cohesion,0.561231791973114
translation,122,69,baselines,features,include,sentence - to - paragraph relevance,features include sentence - to - paragraph relevance,0.5658441185951233
translation,122,69,baselines,baselines,has,lreg w/ para .- level,baselines has lreg w/ para .- level,0.5553581118583679
translation,122,59,hyperparameters,840b.300d pre-trained embeddings,),initialization,840b.300d pre-trained embeddings ) initialization,0.5714868307113647
translation,122,59,hyperparameters,840b.300d pre-trained embeddings,),the full nqg model,840b.300d pre-trained embeddings ) the full nqg model,0.5829506516456604
translation,122,59,hyperparameters,840b.300d pre-trained embeddings,for,initialization,840b.300d pre-trained embeddings for initialization,0.5906303524971008
translation,122,59,hyperparameters,initialization,of,embedding layer,initialization of embedding layer,0.5444258451461792
translation,122,59,hyperparameters,initialization,of,the full nqg model,initialization of the full nqg model,0.5854595899581909
translation,122,59,hyperparameters,embedding layer,for,our sentence selection model,embedding layer for our sentence selection model,0.544062077999115
translation,122,59,hyperparameters,hyperparameters,has,840b.300d pre-trained embeddings,hyperparameters has 840b.300d pre-trained embeddings,0.5228981375694275
translation,122,60,hyperparameters,embeddings,used for calculating,sentence similarity feature,embeddings used for calculating sentence similarity feature,0.5851880311965942
translation,122,60,hyperparameters,sentence similarity feature,of,baseline linear model ( lreg ),sentence similarity feature of baseline linear model ( lreg ),0.5184652209281921
translation,122,60,hyperparameters,glove,has,embeddings,glove has embeddings,0.6075528264045715
translation,122,60,hyperparameters,hyperparameters,has,glove,hyperparameters has glove,0.5672336220741272
translation,122,6,model,model,propose,hierarchical neural sentence - level sequence tagging model,model propose hierarchical neural sentence - level sequence tagging model,0.5883168578147888
translation,122,14,model,question - worthy sentences,in,each paragraph,question - worthy sentences in each paragraph,0.4933794140815735
translation,122,14,model,each paragraph,of,reading comprehension passage,each paragraph of reading comprehension passage,0.5378416776657104
translation,122,15,model,model,propose,hierarchical neural sentence - level sequence,model propose hierarchical neural sentence - level sequence,0.621207058429718
translation,122,67,model,convolutional neural networks ( cnn ) sentence classification model,has,similar structure,convolutional neural networks ( cnn ) sentence classification model has similar structure,0.5505340099334717
translation,122,67,model,model,has,convolutional neural networks ( cnn ) sentence classification model,model has convolutional neural networks ( cnn ) sentence classification model,0.5301409959793091
translation,122,77,results,our models,with,sum or cnn,our models with sum or cnn,0.630439043045044
translation,122,77,results,sum or cnn,as,sentence encoder,sum or cnn as sentence encoder,0.4962637424468994
translation,122,77,results,feature - rich lreg,as well as,other baselines,feature - rich lreg as well as other baselines,0.5907922387123108
translation,122,77,results,other baselines,in terms of,f-measure,other baselines in terms of f-measure,0.6371055245399475
translation,122,77,results,sentence encoder,has,significantly outperform,sentence encoder has significantly outperform,0.3078032433986664
translation,122,77,results,significantly outperform,has,feature - rich lreg,significantly outperform has feature - rich lreg,0.5932788252830505
translation,122,77,results,results,has,our models,results has our models,0.5733726620674133
translation,123,8,experiments,retriever,iterates between,reading context,retriever iterates between reading context,0.710922360420227
translation,123,8,experiments,retriever,retrieving,more supporting documents,retriever retrieving more supporting documents,0.7744179368019104
translation,123,8,experiments,more supporting documents,to answer,open-domain multi-hop questions,more supporting documents to answer open-domain multi-hop questions,0.7374023199081421
translation,123,9,experiments,natural language search queries,given,question and available context,natural language search queries given question and available context,0.6847139000892639
translation,123,9,experiments,off -the-shelf information retrieval systems,to query for,missing entities,off -the-shelf information retrieval systems to query for missing entities,0.7334263920783997
translation,123,27,model,novel iterative retrieve - and - read framework,capable of,multi-hop reasoning,novel iterative retrieve - and - read framework capable of multi-hop reasoning,0.6525797247886658
translation,123,27,model,multi-hop reasoning,in,open-domain qa,multi-hop reasoning in open-domain qa,0.5441926121711731
translation,123,27,model,natural language query generation approach,guarantees,interpretability,natural language query generation approach guarantees interpretability,0.6932224631309509
translation,123,27,model,interpretability,in,multi-hop evidence gathering process,interpretability in multi-hop evidence gathering process,0.5122069120407104
translation,123,27,model,efficient training procedure,to enable,query generation,efficient training procedure to enable query generation,0.7061850428581238
translation,123,27,model,query generation,with,minimal supervision signal,query generation with minimal supervision signal,0.6305397152900696
translation,123,27,model,minimal supervision signal,significantly boosts,recall,minimal supervision signal significantly boosts recall,0.7808433175086975
translation,123,27,model,recall,of,gold supporting documents,recall of gold supporting documents,0.5831323266029358
translation,123,27,model,gold supporting documents,in,retrieval,gold supporting documents in retrieval,0.5129513144493103
translation,123,26,results,qa module,extends,bidaf ++,qa module extends bidaf ++,0.7419883012771606
translation,123,26,results,outperforms,on,open-domain ( fullwiki ) setting,outperforms on open-domain ( fullwiki ) setting,0.5506079196929932
translation,123,26,results,outperforms,of,hotpotqa,outperforms of hotpotqa,0.5595001578330994
translation,123,26,results,best previously published system,on,open-domain ( fullwiki ) setting,best previously published system on open-domain ( fullwiki ) setting,0.5124693512916565
translation,123,26,results,best previously published system,of,hotpotqa,best previously published system of hotpotqa,0.5659373998641968
translation,123,26,results,hotpotqa,without using,powerful pretrained language models,hotpotqa without using powerful pretrained language models,0.686683177947998
translation,123,26,results,powerful pretrained language models,like,"bert ( devlin et al. , 2019 )","powerful pretrained language models like bert ( devlin et al. , 2019 )",0.5664663314819336
translation,123,26,results,qa module,has,our final system,qa module has our final system,0.6072863936424255
translation,123,26,results,bidaf ++,has,our final system,bidaf ++ has our final system,0.6528182029724121
translation,123,26,results,our final system,has,outperforms,our final system has outperforms,0.6442874073982239
translation,123,26,results,outperforms,has,best previously published system,outperforms has best previously published system,0.6099599599838257
translation,123,26,results,results,Combined with,qa module,results Combined with qa module,0.6915148496627808
translation,123,150,results,hand-engineered ir engine,with,elasticsearch,hand-engineered ir engine with elasticsearch,0.6099005937576294
translation,123,150,results,elasticsearch,result in,some gain,elasticsearch result in some gain,0.6711934208869934
translation,123,150,results,some gain,in,recall,some gain in recall,0.5628723502159119
translation,123,150,results,recall,of,gold documents,recall of gold documents,0.595203161239624
translation,123,150,results,significant improvement,in,qa performance,significant improvement in qa performance,0.5422152876853943
translation,123,150,results,results,replacing,hand-engineered ir engine,results replacing hand-engineered ir engine,0.6481568813323975
translation,123,155,results,g 1,with,oracle,g 1 with oracle,0.7232120633125305
translation,123,155,results,g 1,with,oracle,g 1 with oracle,0.7232120633125305
translation,123,155,results,g 1,substituting,g 2,g 1 substituting g 2,0.546878457069397
translation,123,155,results,g 1,with,oracle,g 1 with oracle,0.7232120633125305
translation,123,155,results,g 1,yields,significant improvement,g 1 yields significant improvement,0.749566912651062
translation,123,155,results,g 2,with,oracle,g 2 with oracle,0.7261030673980713
translation,123,155,results,g 2,yields,significant improvement,g 2 yields significant improvement,0.7571604251861572
translation,123,155,results,slightly improves,has,end-toend performance,slightly improves has end-toend performance,0.5843313336372375
translation,123,155,results,results,replacing,g 1,results replacing g 1,0.6272309422492981
translation,123,155,results,results,substituting,g 2,results substituting g 2,0.5146357417106628
translation,123,161,results,performance,of,g 2,performance of g 2,0.6799128651618958
translation,123,161,results,g 2,worse than,g 1,g 2 worse than g 1,0.6732264757156372
translation,123,162,results,generated queries,perform,only slightly better,generated queries perform only slightly better,0.5376662611961365
translation,123,162,results,only slightly better,on,d 1,only slightly better on d 1,0.6068578362464905
translation,123,162,results,d 1,when,total of 10 documents,d 1 when total of 10 documents,0.6493865847587585
translation,123,162,results,total of 10 documents,are,retrieved,total of 10 documents are retrieved,0.563271164894104
translation,123,162,results,significantly more effective,for,d 2,significantly more effective for d 2,0.6292238831520081
translation,123,162,results,pipeline,has,generated queries,pipeline has generated queries,0.6193699836730957
translation,123,162,results,retrieved,has,89.91 % vs 87.85 % ),retrieved has 89.91 % vs 87.85 % ),0.581747829914093
translation,123,162,results,d 2,has,61.01 % vs 36.91 % ),d 2 has 61.01 % vs 36.91 % ),0.5533986687660217
translation,123,163,results,zoom in,on,retrieval performance,zoom in on retrieval performance,0.5845609307289124
translation,123,163,results,retrieval performance,on,noncomparison questions,retrieval performance on noncomparison questions,0.5723609328269958
translation,123,163,results,noncomparison questions,finding,two entities involved,noncomparison questions finding two entities involved,0.6905167102813721
translation,123,163,results,two entities involved,is,less trivial,two entities involved is less trivial,0.6049286723136902
translation,123,163,results,recall,on,d 2,recall on d 2,0.6426772475242615
translation,123,163,results,improves,from,27.88 %,improves from 27.88 %,0.604647159576416
translation,123,163,results,27.88 %,to,53.23 %,27.88 % to 53.23 %,0.601518988609314
translation,124,65,baselines,two publicly available neural network models,has,ga reader,two publicly available neural network models has ga reader,0.5722744464874268
translation,124,65,baselines,baselines,work with,two publicly available neural network models,baselines work with two publicly available neural network models,0.6518315076828003
translation,124,22,experiments,proposed system,on,three datasets,proposed system on three datasets,0.537962794303894
translation,124,22,experiments,three datasets,from,different domains,three datasets from different domains,0.5334823131561279
translation,124,84,experiments,semi-supervised qa system,use,bidaf +sa model,semi-supervised qa system use bidaf +sa model,0.5946630835533142
translation,124,17,model,proposed system,consists of,three stages,proposed system consists of three stages,0.7373341917991638
translation,124,17,model,model,has,proposed system,model has proposed system,0.6011408567428589
translation,124,18,model,cloze-style questions,from,unlabeled corpus,cloze-style questions from unlabeled corpus,0.5040321946144104
translation,124,18,model,cloze-style questions,use,generated clozes,cloze-style questions use generated clozes,0.6421230435371399
translation,124,18,model,generated clozes,to pre-train,powerful neural network model,generated clozes to pre-train powerful neural network model,0.7080245614051819
translation,124,18,model,powerful neural network model,for,extractive qa,powerful neural network model for extractive qa,0.6372226476669312
translation,124,18,model,model,on,small set of provided qa pairs,model on small set of provided qa pairs,0.5647871494293213
translation,124,18,model,model,construct,cloze-style questions,model construct cloze-style questions,0.7736196517944336
translation,124,18,model,model,use,generated clozes,model use generated clozes,0.7098830342292786
translation,124,23,results,significant improvements,in,low-resource setting,significant improvements in low-resource setting,0.5474905967712402
translation,124,23,results,low-resource setting,across,all three datasets,low-resource setting across all three datasets,0.7327095866203308
translation,124,23,results,results,observe,significant improvements,results observe significant improvements,0.5997344851493835
translation,124,24,results,squad and triviaqa,attain,f1 score,squad and triviaqa attain f1 score,0.6607432961463928
translation,124,24,results,f1 score,of,more than 50 %,f1 score of more than 50 %,0.5552675127983093
translation,124,24,results,more than 50 %,by merely using,1 %,more than 50 % by merely using 1 %,0.6787945628166199
translation,124,24,results,1 %,of,training data,1 % of training data,0.6022763252258301
translation,124,24,results,results,For,squad and triviaqa,results For squad and triviaqa,0.60398268699646
translation,124,25,results,approaches,for,semi-supervised qa,approaches for semi-supervised qa,0.6261790990829468
translation,124,25,results,language modeling objective,for,pretraining,language modeling objective for pretraining,0.5495588183403015
translation,124,25,results,outperforms,has,approaches,outperforms has approaches,0.6612938046455383
translation,124,26,results,best performing system,improving over,baseline,best performing system improving over baseline,0.7007525563240051
translation,124,26,results,outperform,has,best performing system,outperform has best performing system,0.6434345841407776
translation,124,26,results,results,In,bioasq challenge,results In bioasq challenge,0.5401370525360107
translation,124,66,results,performance,of,bidaf + sa,performance of bidaf + sa,0.5968868136405945
translation,124,66,results,bidaf + sa,on,dev set of the ( wikipedia ) cloze questions,bidaf + sa on dev set of the ( wikipedia ) cloze questions,0.5373926758766174
translation,124,66,results,dev set of the ( wikipedia ) cloze questions,is,0.58 f1 score,dev set of the ( wikipedia ) cloze questions is 0.58 f1 score,0.5278998017311096
translation,124,66,results,dev set of the ( wikipedia ) cloze questions,is,0.55 exact match ( em ) score,dev set of the ( wikipedia ) cloze questions is 0.55 exact match ( em ) score,0.5278908610343933
translation,124,66,results,pretraining,has,performance,pretraining has performance,0.5653539896011353
translation,124,66,results,results,After,pretraining,results After pretraining,0.6852666735649109
translation,124,93,results,gdan baseline,from using,same squad dataset splits,gdan baseline from using same squad dataset splits,0.6466958522796631
translation,124,93,results,cloze pretraining,has,outperforms,cloze pretraining has outperforms,0.6278044581413269
translation,124,93,results,outperforms,has,gdan baseline,outperforms has gdan baseline,0.5746845602989197
translation,124,93,results,results,has,cloze pretraining,results has cloze pretraining,0.5141826272010803
translation,124,100,results,questions,with,short answers,questions with short answers,0.6156946420669556
translation,124,101,results,passages and questions,tokens infrequent in,squad training corpus,passages and questions tokens infrequent in squad training corpus,0.8046643137931824
translation,124,101,results,passages and questions,receive,large boost,passages and questions receive large boost,0.6394765973091125
translation,124,101,results,large boost,after,pretraining,large boost after pretraining,0.7149194478988647
translation,124,101,results,results,has,passages and questions,results has passages and questions,0.5172088146209717
translation,124,107,results,abbreviation questions,receive,large boost,abbreviation questions receive large boost,0.6728647351264954
translation,124,108,results,  why   questions,show,least improvement,  why   questions show least improvement,0.7057650685310364
translation,124,108,results,results,has,  why   questions,results has   why   questions,0.5432007312774658
translation,126,153,baselines,baselines,has,extractive qa,baselines has extractive qa,0.5844939947128296
translation,126,182,baselines,recurrent +elmo model,uses,language model,recurrent +elmo model uses language model,0.5338655710220337
translation,126,182,baselines,language model,to provide,contextualized embeddings,language model to provide contextualized embeddings,0.5623355507850647
translation,126,182,baselines,contextualized embeddings,to,baseline model,contextualized embeddings to baseline model,0.47135844826698303
translation,126,182,baselines,baselines,has,recurrent +elmo model,baselines has recurrent +elmo model,0.5624637007713318
translation,126,220,experiments,best single-step transfer learning results,from using,pre-trained bert l model,best single-step transfer learning results from using pre-trained bert l model,0.6765739917755127
translation,126,220,experiments,best single-step transfer learning results,from using,multinli,best single-step transfer learning results from using multinli,0.6773358583450317
translation,126,183,hyperparameters,openai gpt model,fine- tunes,12 layer 768 dimensional uni-directional transformer,openai gpt model fine- tunes 12 layer 768 dimensional uni-directional transformer,0.6828706860542297
translation,126,183,hyperparameters,hyperparameters,has,openai gpt model,hyperparameters has openai gpt model,0.5175742506980896
translation,126,185,hyperparameters,24 layer 1024 dimensional transformer,from,devlin et al . ( 2018 ),24 layer 1024 dimensional transformer from devlin et al . ( 2018 ),0.5276288986206055
translation,126,185,hyperparameters,24 layer 1024 dimensional transformer,trained on,next-sentence -selection,24 layer 1024 dimensional transformer trained on next-sentence -selection,0.7354537844657898
translation,126,185,hyperparameters,24 layer 1024 dimensional transformer,trained on,masked language modelling,24 layer 1024 dimensional transformer trained on masked language modelling,0.7067604064941406
translation,126,185,hyperparameters,masked language modelling,on,book corpus and wikipedia,masked language modelling on book corpus and wikipedia,0.5183882713317871
translation,126,187,hyperparameters,batch size,of,24,batch size of 24,0.6606289148330688
translation,126,187,hyperparameters,batch size,of,6,batch size of 6,0.6956941485404968
translation,126,187,hyperparameters,learning rate,of,1e - 5,learning rate of 1e - 5,0.6323861479759216
translation,126,187,hyperparameters,5 training epochs,for,bert,5 training epochs for bert,0.6028607487678528
translation,126,187,hyperparameters,learning rate,of,6.25e - 5,learning rate of 6.25e - 5,0.5975595712661743
translation,126,187,hyperparameters,batch size,of,6,batch size of 6,0.6956941485404968
translation,126,187,hyperparameters,language model loss,of,0.5,language model loss of 0.5,0.5773234963417053
translation,126,187,hyperparameters,3 training epochs,for,openai gpt,3 training epochs for openai gpt,0.5804376602172852
translation,126,187,hyperparameters,hyperparameters,for,openai gpt,hyperparameters for openai gpt,0.5530327558517456
translation,126,8,results,entailment data,is,more effective,entailment data is more effective,0.5753878355026245
translation,126,8,results,more effective,transferring from,paraphrase or extractive qa data,more effective transferring from paraphrase or extractive qa data,0.7174253463745117
translation,126,8,results,results,transferring from,entailment data,results transferring from entailment data,0.5740121006965637
translation,126,42,results,unsupervised pre-training,in,bert,unsupervised pre-training in bert,0.5114803910255432
translation,126,42,results,unsupervised pre-training,gave,best results,unsupervised pre-training gave best results,0.6830143928527832
translation,126,42,results,results,transferring from,multinli,results transferring from multinli,0.6012026071548462
translation,126,44,results,best model,reaches,80.43 % accuracy,best model reaches 80.43 % accuracy,0.6614230275154114
translation,126,44,results,80.43 % accuracy,compared to,62.31 %,80.43 % accuracy compared to 62.31 %,0.6575686931610107
translation,126,44,results,80.43 % accuracy,compared to,90 % human accuracy,80.43 % accuracy compared to 90 % human accuracy,0.6373504996299744
translation,126,44,results,62.31 %,for,majority baseline,62.31 % for majority baseline,0.597771167755127
translation,126,44,results,62.31 %,for,90 % human accuracy,62.31 % for 90 % human accuracy,0.6026105284690857
translation,126,44,results,results,has,best model,results has best model,0.5634682774543762
translation,126,138,results,training,on,train set,training on train set,0.5952573418617249
translation,126,138,results,training,to be,relatively ineffective,training to be relatively ineffective,0.5997938513755798
translation,126,138,results,models,on,train set,models on train set,0.5139741897583008
translation,126,138,results,models,to be,relatively ineffective,models to be relatively ineffective,0.5684308409690857
translation,126,138,results,training,has,models,training has models,0.5681478977203369
translation,126,138,results,results,find,training,results find training,0.5576973557472229
translation,126,139,results,best model,reaches,69.6 % accuracy,best model reaches 69.6 % accuracy,0.665081262588501
translation,126,139,results,better,than,majority baseline,better than majority baseline,0.5782834887504578
translation,126,139,results,8 %,has,better,8 % has better,0.6092270016670227
translation,126,139,results,results,has,best model,results has best model,0.5634682774543762
translation,126,190,results,pre-trained bert l model,reached,64.48 %,pre-trained bert l model reached 64.48 %,0.6546081304550171
translation,126,190,results,dev set accuracy,using,question,dev set accuracy using question,0.6906386613845825
translation,126,190,results,66.74 %,using,passage,66.74 % using passage,0.7130758166313171
translation,126,190,results,64.48 %,has,dev set accuracy,64.48 % has dev set accuracy,0.573583722114563
translation,126,190,results,results,has,pre-trained bert l model,results has pre-trained bert l model,0.5658965110778809
translation,126,205,results,better results,using,qnli,better results using qnli,0.679987370967865
translation,126,205,results,even better results,using,nq,even better results using nq,0.6866167187690735
translation,126,205,results,results,got,better results,results got better results,0.7039555311203003
translation,126,205,results,results,got,even better results,results got even better results,0.7079229354858398
translation,126,208,results,multinli dataset,out -performed,all other supervised methods,multinli dataset out -performed all other supervised methods,0.6612775921821594
translation,126,208,results,all other supervised methods,by,large margin,all other supervised methods by large margin,0.5256314277648926
translation,126,208,results,results,has,multinli dataset,results has multinli dataset,0.5610736012458801
translation,126,218,results,bert l,to be,most effective unsupervised method,bert l to be most effective unsupervised method,0.6020388007164001
translation,126,218,results,bert l,surpassing,all other methods,bert l surpassing all other methods,0.7008858323097229
translation,126,218,results,all other methods,of,pretraining,all other methods of pretraining,0.5737183690071106
translation,126,218,results,results,found,bert l,results found bert l,0.6570197343826294
translation,126,223,results,number of training epochs,to,3,number of training epochs to 3,0.5726457834243774
translation,126,223,results,number of training epochs,resulted in,slight improvement,number of training epochs resulted in slight improvement,0.6335937976837158
translation,126,223,results,slight improvement,model pre-trained on,multinli,slight improvement model pre-trained on multinli,0.7075173258781433
translation,126,223,results,results,decreasing,number of training epochs,results decreasing number of training epochs,0.6660671234130859
translation,126,226,results,additional gain,of,3.5 points,additional gain of 3.5 points,0.5522556900978088
translation,126,226,results,3.5 points,due to using,multinli,3.5 points due to using multinli,0.6791279911994934
translation,126,226,results,multinli,is,remarkable,multinli is remarkable,0.7379831075668335
translation,127,31,baselines,dsmn,uses,vector embeddings,dsmn uses vector embeddings,0.5800825357437134
translation,127,31,baselines,dsmn,uses,memory modules,dsmn uses memory modules,0.6337513327598572
translation,127,31,baselines,vector embeddings,of,questions,vector embeddings of questions,0.5559022426605225
translation,127,31,baselines,vector embeddings,of,memory modules,vector embeddings of memory modules,0.5782208442687988
translation,127,31,baselines,memory modules,to perform,reasoning,memory modules to perform reasoning,0.6681336164474487
translation,127,31,baselines,baselines,has,dsmn,baselines has dsmn,0.5807904005050659
translation,127,233,baselines,"lstm ( hochreiter and schmidhuber , 1997 )",is,popular neural network,"lstm ( hochreiter and schmidhuber , 1997 ) is popular neural network",0.525020182132721
translation,127,233,baselines,popular neural network,for,sequence processing tasks,popular neural network for sequence processing tasks,0.568569540977478
translation,127,234,baselines,two versions,of,lstm - based baselines,two versions of lstm - based baselines,0.5553330183029175
translation,127,234,baselines,baselines,use,two versions,baselines use two versions,0.641245424747467
translation,127,235,baselines,baseline,for,textual qa,baseline for textual qa,0.5955168008804321
translation,127,235,baselines,baselines,has,lstm - 1,baselines has lstm - 1,0.542164146900177
translation,127,236,baselines,lstm - 1,concatenate,all the sentences and the question,lstm - 1 concatenate all the sentences and the question,0.7070612907409668
translation,127,236,baselines,lstm - 1,to,single string,lstm - 1 to single string,0.5766371488571167
translation,127,236,baselines,all the sentences and the question,to,single string,all the sentences and the question to single string,0.5818615555763245
translation,127,240,baselines,another version,of,lstm,another version of lstm,0.5638675093650818
translation,127,240,baselines,another version,call,lstm - 2,another version call lstm - 2,0.5743768215179443
translation,127,240,baselines,lstm,call,lstm - 2,lstm call lstm - 2,0.5891651511192322
translation,127,240,baselines,question,concatenated to,description,question concatenated to description,0.7596198320388794
translation,127,243,experiments,floorplanqa,use,lstm,floorplanqa use lstm,0.5772279500961304
translation,127,243,experiments,lstm,to get,sentence embeddings,lstm to get sentence embeddings,0.6037770509719849
translation,127,243,experiments,shapeintersection,use,series of fc layers,shapeintersection use series of fc layers,0.6678637862205505
translation,127,253,experiments,early stopping,after,80 epochs,early stopping after 80 epochs,0.7070339918136597
translation,127,253,experiments,80 epochs,if,validation accuracy,80 epochs if validation accuracy,0.6198940873146057
translation,127,253,experiments,validation accuracy,not,increase,validation accuracy not increase,0.6668054461479187
translation,127,253,experiments,floorplanqa,has,all models,floorplanqa has all models,0.5896955132484436
translation,127,254,hyperparameters,maximum number of epochs,for,shapeintersection,maximum number of epochs for shapeintersection,0.5487538576126099
translation,127,254,hyperparameters,shapeintersection,is,800 epochs,shapeintersection is 800 epochs,0.5737069249153137
translation,127,254,hyperparameters,early stopping,after,80 epochs,early stopping after 80 epochs,0.7070339918136597
translation,127,254,hyperparameters,hyperparameters,has,maximum number of epochs,hyperparameters has maximum number of epochs,0.5072295069694519
translation,127,5,model,answering questions,admit,latent visual representations,answering questions admit latent visual representations,0.6560361385345459
translation,127,5,model,model,introduce,dynamic spatial memory network ( dsmn ),model introduce dynamic spatial memory network ( dsmn ),0.6542283892631531
translation,127,6,model,dsmn,generate and reason over,representations,dsmn generate and reason over representations,0.7844038605690002
translation,127,6,model,model,has,dsmn,model has dsmn,0.6055992841720581
translation,127,24,model,model,investigate,geometric reasoning,model investigate geometric reasoning,0.6186010837554932
translation,127,24,model,model,model,geometric reasoning,model model geometric reasoning,0.788287341594696
translation,127,30,model,virtual imagery,for,qa,virtual imagery for qa,0.6649436950683594
translation,127,30,model,model,propose,dynamic spatial memory network ( dsmn ),model propose dynamic spatial memory network ( dsmn ),0.6756629943847656
translation,127,167,model,qa,with,geometric reasoning,qa with geometric reasoning,0.6437746286392212
translation,127,167,model,model,propose,dynamic spatial memory network ( dsmn ),model propose dynamic spatial memory network ( dsmn ),0.6756629943847656
translation,127,33,results,strong baselines,on,floorplanqa and shapeintersection,strong baselines on floorplanqa and shapeintersection,0.5257919430732727
translation,127,33,results,internal visual representation,has,dsmn,internal visual representation has dsmn,0.5676700472831726
translation,127,33,results,spatial memory,has,dsmn,spatial memory has dsmn,0.5552608370780945
translation,127,33,results,dsmn,has,outperforms,dsmn has outperforms,0.6460471749305725
translation,127,33,results,outperforms,has,strong baselines,outperforms has strong baselines,0.6115288734436035
translation,127,34,results,explicitly learning,to create,visual representations,explicitly learning to create visual representations,0.5983057022094727
translation,127,34,results,improves,has,performance,improves has performance,0.5770372748374939
translation,127,34,results,results,demonstrate,explicitly learning,results demonstrate explicitly learning,0.5873128175735474
translation,127,35,results,dsmn,is,substantially better,dsmn is substantially better,0.6018556952476501
translation,127,35,results,substantially better,than,baselines,substantially better than baselines,0.6204237341880798
translation,127,35,results,results,show that,dsmn,results show that dsmn,0.49007049202919006
translation,127,259,results,dmn +,by,large margin,dmn + by large margin,0.5847941637039185
translation,127,259,results,dsmn *,has,outperforms,dsmn * has outperforms,0.6513556241989136
translation,127,259,results,outperforms,has,dmn +,outperforms has dmn +,0.6223022937774658
translation,127,259,results,results,has,dsmn *,results has dsmn *,0.5258053541183472
translation,128,157,baselines,bert + frv,Same as,bert + fop,bert + frv Same as bert + fop,0.6175124049186707
translation,128,157,baselines,bert + frv,Same as,fop,bert + frv Same as fop,0.6233442425727844
translation,128,157,baselines,bert + frv,used,frv,bert + frv used frv,0.5433563590049744
translation,128,157,baselines,bert + frv,instead of,fop,bert + frv instead of fop,0.6460964679718018
translation,128,157,baselines,frv,instead of,fop,frv instead of fop,0.6789689660072327
translation,128,157,baselines,frv,for producing,compact -answer representation,frv for producing compact -answer representation,0.6941251158714294
translation,128,157,baselines,baselines,has,bert + frv,baselines has bert + frv,0.5923024415969849
translation,128,84,hyperparameters,pre-trained word embeddings,used in,"encoder(t ; ? , q )","pre-trained word embeddings used in encoder(t ; ? , q )",0.6147932410240173
translation,128,84,hyperparameters,pre-trained word embeddings,obtained by concatenating,two types of d-dimensional word embeddings,pre-trained word embeddings obtained by concatenating two types of d-dimensional word embeddings,0.6703649759292603
translation,128,84,hyperparameters,hyperparameters,has,pre-trained word embeddings,hyperparameters has pre-trained word embeddings,0.46373438835144043
translation,128,126,hyperparameters,weights,in,cnns,weights in cnns,0.5246089100837708
translation,128,126,hyperparameters,initialized,using,he 's method,initialized using he 's method,0.7430484294891357
translation,128,126,hyperparameters,initialized,using,),initialized using ),0.76227205991745
translation,128,126,hyperparameters,initialized,using,why - qa model,initialized using why - qa model,0.6782945394515991
translation,128,126,hyperparameters,initialized,using,randomly,initialized using randomly,0.6329415440559387
translation,128,126,hyperparameters,other weights,in,why - qa model,other weights in why - qa model,0.5081343054771423
translation,128,126,hyperparameters,other weights,initialized,randomly,other weights initialized randomly,0.7883353233337402
translation,128,126,hyperparameters,randomly,with,uniform distribution,randomly with uniform distribution,0.6485945582389832
translation,128,126,hyperparameters,uniform distribution,in,range,uniform distribution in range,0.5448866486549377
translation,128,126,hyperparameters,range,of,"( - 0.01 , 0.01 )","range of ( - 0.01 , 0.01 )",0.5832655429840088
translation,128,127,hyperparameters,cnn - based components,set,window size,cnn - based components set window size,0.6214808821678162
translation,128,127,hyperparameters,window size,of,filters,window size of filters,0.6269427537918091
translation,128,127,hyperparameters,filters,to,"1,2,3","filters to 1,2,3",0.6096094250679016
translation,128,127,hyperparameters,filters,with,100 filters,filters with 100 filters,0.7012206315994263
translation,128,127,hyperparameters,hyperparameters,For,cnn - based components,hyperparameters For cnn - based components,0.5301457047462463
translation,128,128,hyperparameters,"dropout ( srivastava et al. , 2014 )",with,probability 0.5,"dropout ( srivastava et al. , 2014 ) with probability 0.5",0.6054396033287048
translation,128,128,hyperparameters,probability 0.5,on,final logistic regression layer,probability 0.5 on final logistic regression layer,0.5240389108657837
translation,128,130,hyperparameters,learned parameters,with,"adam stochastic gradient descent ( kingma and ba , 2015 )","learned parameters with adam stochastic gradient descent ( kingma and ba , 2015 )",0.585579514503479
translation,128,130,hyperparameters,hyperparameters,optimized,learned parameters,hyperparameters optimized learned parameters,0.6802859306335449
translation,128,131,hyperparameters,learning rate,set to,0.001,learning rate set to 0.001,0.6954665780067444
translation,128,131,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,128,131,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,128,175,hyperparameters,learning rates,of,"{ 1e - 5 , 2e - 5 , 3e - 5 }","learning rates of { 1e - 5 , 2e - 5 , 3e - 5 }",0.5931084156036377
translation,128,175,hyperparameters,performance,on,development data,performance on development data,0.5651078820228577
translation,128,175,hyperparameters,hyperparameters,tested,"combinations of epochs { 1 , 2 , 3 , 4 , 5 }","hyperparameters tested combinations of epochs { 1 , 2 , 3 , 4 , 5 }",0.7009162306785583
translation,128,214,hyperparameters,300 - dimensional glove word embeddings,learned from,840 billion tokens,300 - dimensional glove word embeddings learned from 840 billion tokens,0.6167261600494385
translation,128,214,hyperparameters,300 - dimensional glove word embeddings,as,general word embeddings,300 - dimensional glove word embeddings as general word embeddings,0.4932593107223511
translation,128,214,hyperparameters,840 billion tokens,in,web crawl data,840 billion tokens in web crawl data,0.511006236076355
translation,128,214,hyperparameters,hyperparameters,used,300 - dimensional glove word embeddings,hyperparameters used 300 - dimensional glove word embeddings,0.5463320016860962
translation,128,4,model,method,for,whyquestion answering ( why - qa ),method for whyquestion answering ( why - qa ),0.5803686380386353
translation,128,4,model,method,uses,adversarial learning framework,method uses adversarial learning framework,0.5356507301330566
translation,128,4,model,model,propose,method,model propose method,0.6280754208564758
translation,128,48,model,representations,of,why -questions and answer passages,representations of why -questions and answer passages,0.6031087040901184
translation,128,48,model,representations,generated by,convolutional neural networks ( cnns ),representations generated by convolutional neural networks ( cnns ),0.6451492309570312
translation,128,48,model,why -questions and answer passages,generated by,convolutional neural networks ( cnns ),why -questions and answer passages generated by convolutional neural networks ( cnns ),0.6370643377304077
translation,128,48,model,convolutional neural networks ( cnns ),augmented by,two types of attention mechanisms,convolutional neural networks ( cnns ) augmented by two types of attention mechanisms,0.7175137996673584
translation,128,48,model,general word embeddings,computed by,word2vec,general word embeddings computed by word2vec,0.5786947011947632
translation,128,48,model,general word embeddings,using,wikipedia and causal word embeddings,general word embeddings using wikipedia and causal word embeddings,0.6341714859008789
translation,128,48,model,model,has,representations,model has representations,0.5900989174842834
translation,128,215,model,resulting fake - representation generator f,in,agr,resulting fake - representation generator f in agr,0.5335472226142883
translation,128,215,model,resulting fake - representation generator f,with,state - of- the - art ds - qa method,resulting fake - representation generator f with state - of- the - art ds - qa method,0.6170006394386292
translation,128,215,model,model,combined,resulting fake - representation generator f,model combined resulting fake - representation generator f,0.7144932746887207
translation,128,39,results,interesting point,is that,performance,interesting point is that performance,0.6704069375991821
translation,128,39,results,improved,replaced,inputs,improved replaced inputs,0.7000998854637146
translation,128,39,results,inputs,to,agr,inputs to agr,0.6151480674743652
translation,128,39,results,word embedding vectors,represent,answer passage,word embedding vectors represent answer passage,0.558541476726532
translation,128,39,results,word embedding vectors,with,random vector,word embedding vectors with random vector,0.5975790023803711
translation,128,39,results,answer passage,with,random vector,answer passage with random vector,0.653380811214447
translation,128,39,results,performance,has,improved,performance has improved,0.6221855282783508
translation,128,185,results,our proposed method,has,ours ( op ),our proposed method has ours ( op ),0.5762293338775635
translation,128,185,results,our proposed method,has,outperformed,our proposed method has outperformed,0.6112541556358337
translation,128,185,results,ours ( op ),has,outperformed,ours ( op ) has outperformed,0.6128712892532349
translation,128,185,results,outperformed,has,all the other methods,outperformed has all the other methods,0.6037989258766174
translation,128,185,results,results,has,our proposed method,results has our proposed method,0.5673112869262695
translation,128,186,results,starting point,i.e.,base,starting point i.e. base,0.6299387216567993
translation,128,186,results,starting point,superior to,methods,starting point superior to methods,0.6881916522979736
translation,128,186,results,base,superior to,methods,base superior to methods,0.6826491951942444
translation,128,186,results,results,has,starting point,results has starting point,0.520023763179779
translation,128,187,results,ours ( op ),gave,3.4 % and 2.8 % improvement,ours ( op ) gave 3.4 % and 2.8 % improvement,0.6637500524520874
translation,128,187,results,3.4 % and 2.8 % improvement,in,p@1,3.4 % and 2.8 % improvement in p@1,0.5837037563323975
translation,128,187,results,base and base + addtr,has,ours ( op ),base and base + addtr has ours ( op ),0.6310640573501587
translation,128,187,results,results,Compared with,base and base + addtr,results Compared with base and base + addtr,0.7242708802223206
translation,128,188,results,base + cans and base + cenc,generated,compact -answer representations,base + cans and base + cenc generated compact -answer representations,0.6237971186637878
translation,128,188,results,base + enc,trained,fake - representation generator,base + enc trained fake - representation generator,0.7327860593795776
translation,128,188,results,fake - representation generator,without,adversarial learning,fake - representation generator without adversarial learning,0.7091417908668518
translation,128,188,results,outperformed,has,base + cans and base + cenc,outperformed has base + cans and base + cenc,0.644501268863678
translation,128,190,results,fakerepresentation generator f,boosted,performance,fakerepresentation generator f boosted performance,0.7526816129684448
translation,128,190,results,performance,of,bert - based models,performance of bert - based models,0.6137188076972961
translation,128,190,results,ours ( op ),has,outperformed,ours ( op ) has outperformed,0.6128712892532349
translation,128,190,results,outperformed,has,all the bertbased models,outperformed has all the bertbased models,0.6107111573219299
translation,128,190,results,results,has,ours ( op ),results has ours ( op ),0.5458098649978638
translation,128,227,results,outperformed,except for,f1 score,outperformed except for f1 score,0.6405826807022095
translation,128,227,results,previous methods,except for,f1 score,previous methods except for f1 score,0.6097437739372253
translation,128,227,results,f1 score,for,triviaqa dataset,f1 score for triviaqa dataset,0.5893557071685791
translation,128,227,results,outperformed,has,previous methods,outperformed has previous methods,0.6202203631401062
translation,128,228,results,improvements,over,previous state- ofthe - art method,improvements over previous state- ofthe - art method,0.6452530026435852
translation,128,228,results,improvements,were,statistically significant,improvements were statistically significant,0.6204957962036133
translation,128,228,results,results,Some of,improvements,results Some of improvements,0.6805525422096252
translation,129,87,ablation-analysis,gradient supervision ( gs ),for,counterfactuals,gradient supervision ( gs ) for counterfactuals,0.5963466167449951
translation,129,87,ablation-analysis,gradient supervision ( gs ),brings,smaller gain ( + 0.80 % ),gradient supervision ( gs ) brings smaller gain ( + 0.80 % ),0.5758212208747864
translation,129,87,ablation-analysis,smaller gain ( + 0.80 % ),from,unshuf-fling + cf,smaller gain ( + 0.80 % ) from unshuf-fling + cf,0.5590636730194092
translation,129,87,ablation-analysis,ablation analysis,has,gradient supervision ( gs ),ablation analysis has gradient supervision ( gs ),0.5094990730285645
translation,129,83,baselines,san based methods,including,gvqa,san based methods including gvqa,0.6803227066993713
translation,129,84,baselines,unshuffling based methods,including,cf,unshuffling based methods including cf,0.6426154375076294
translation,129,84,baselines,unshuffling based methods,including,cf + gs,unshuffling based methods including cf + gs,0.6677649617195129
translation,129,85,baselines,updn based methods,including,areg,updn based methods including areg,0.6809213757514954
translation,129,85,baselines,updn based methods,including,grl,updn based methods including grl,0.6832985281944275
translation,129,85,baselines,updn based methods,including,rubi,updn based methods including rubi,0.678289532661438
translation,129,85,baselines,updn based methods,including,lmh,updn based methods including lmh,0.66182941198349
translation,129,85,baselines,updn based methods,including,css,updn based methods including css,0.6681049466133118
translation,129,85,baselines,updn based methods,including,hint,updn based methods including hint,0.689944863319397
translation,129,85,baselines,updn based methods,including,scr,updn based methods including scr,0.6592077612876892
translation,129,7,model,novel selfsupervised contrastive learning mechanism,to learn,relationship,novel selfsupervised contrastive learning mechanism to learn relationship,0.5972533822059631
translation,129,7,model,relationship,between,"original samples , factual samples and counterfactual samples","relationship between original samples , factual samples and counterfactual samples",0.6436654925346375
translation,129,7,model,model,introduce,novel selfsupervised contrastive learning mechanism,model introduce novel selfsupervised contrastive learning mechanism,0.6271137595176697
translation,129,25,model,vqa model,to understand,impact,vqa model to understand impact,0.6622014045715332
translation,129,25,model,vqa model,introduce,novel contrastive learning mechanism,vqa model introduce novel contrastive learning mechanism,0.5801963210105896
translation,129,25,model,novel contrastive learning mechanism,into,training,novel contrastive learning mechanism into training,0.5645062923431396
translation,129,25,model,training,with,counterfactual samples,training with counterfactual samples,0.6366480588912964
translation,129,25,model,model,to enable,vqa model,model to enable vqa model,0.6970992684364319
translation,129,86,results,our contrastive learning ( cl ),building on top of,updn+lmh + css,our contrastive learning ( cl ) building on top of updn+lmh + css,0.6667352318763733
translation,129,86,results,overall accuracy,from,57.74 %,overall accuracy from 57.74 %,0.4858379662036896
translation,129,86,results,57.74 %,to,59.18 % ( + 1.44 % ),57.74 % to 59.18 % ( + 1.44 % ),0.5977023243904114
translation,129,86,results,updn+lmh + css,has,outperforms,updn+lmh + css has outperforms,0.6489194631576538
translation,129,86,results,outperforms,has,previous results,outperforms has previous results,0.6147018671035767
translation,129,86,results,results,show,our contrastive learning ( cl ),results show our contrastive learning ( cl ),0.6119063496589661
translation,129,89,results,outperforms,by,1.88 %,outperforms by 1.88 %,0.6245783567428589
translation,129,89,results,lmh +css + gs,by,1.88 %,lmh +css + gs by 1.88 %,0.5687130093574524
translation,129,89,results,outperforms,has,lmh +css + gs,outperforms has lmh +css + gs,0.6195631623268127
translation,129,98,results,our auxiliary training objective,not only pull up,original sample and factual sample,our auxiliary training objective not only pull up original sample and factual sample,0.711553156375885
translation,129,98,results,original sample and counterfactual sample,in,embedding space,original sample and counterfactual sample in embedding space,0.5434988737106323
translation,129,98,results,original sample and counterfactual sample,build,better causal vqa model,original sample and counterfactual sample build better causal vqa model,0.6488467454910278
translation,129,98,results,lmh + css,has,our auxiliary training objective,lmh + css has our auxiliary training objective,0.5471242666244507
translation,129,98,results,results,compared with,lmh + css,results compared with lmh + css,0.6512902975082397
translation,130,61,model,stagg,formulates,output semantic parse,stagg formulates output semantic parse,0.6602463126182556
translation,130,61,model,output semantic parse,in,query graph representation,output semantic parse in query graph representation,0.5068762898445129
translation,130,61,model,model,has,stagg,model has stagg,0.6185052394866943
translation,130,86,results,average f 1 score,is,4.9 - point higher,average f 1 score is 4.9 - point higher,0.5538283586502075
translation,130,86,results,labeled parses,has,average f 1 score,labeled parses has average f 1 score,0.5234662294387817
translation,130,86,results,4.9 - point higher,has,71.7 % vs. 66.8 % ),4.9 - point higher has 71.7 % vs. 66.8 % ),0.5374770164489746
translation,130,86,results,results,With,labeled parses,results With labeled parses,0.6244089007377625
translation,131,80,results,word embeddings method,achieved,"highest accuracy , recall , and f1 - score","word embeddings method achieved highest accuracy , recall , and f1 - score",0.6598371267318726
translation,131,80,results,rouge approach,obtained,highest precision,rouge approach obtained highest precision,0.6252672672271729
translation,131,80,results,results,see that,word embeddings method,results see that word embeddings method,0.5848724842071533
translation,131,95,results,accuracy,of,question matching,accuracy of question matching,0.5892987847328186
translation,131,95,results,question matching,for,all approaches,question matching for all approaches,0.6122468709945679
translation,131,95,results,threshold,has,accuracy,threshold has accuracy,0.5890214443206787
translation,131,95,results,results,increase,threshold,results increase threshold,0.6949396133422852
translation,132,193,ablation-analysis,aided greatly,when including,bert features,aided greatly when including bert features,0.7447913885116577
translation,132,193,ablation-analysis,features,have,limited impact,features have limited impact,0.5562807321548462
translation,132,193,ablation-analysis,limited impact,on,m bert,limited impact on m bert,0.6554627418518066
translation,132,193,ablation-analysis,ablation analysis,has,m feat,ablation analysis has m feat,0.5939787030220032
translation,132,165,hyperparameters,m feat classifier,train it for,at most 30 epochs,m feat classifier train it for at most 30 epochs,0.6711969375610352
translation,132,165,hyperparameters,at most 30 epochs,using,"adam ( kingma and ba , 2014 )","at most 30 epochs using adam ( kingma and ba , 2014 )",0.6369947791099548
translation,132,165,hyperparameters,"adam ( kingma and ba , 2014 )",with,learning rate 1e ?3,"adam ( kingma and ba , 2014 ) with learning rate 1e ?3",0.6263251304626465
translation,132,165,hyperparameters,m feat classifier,has,50 hidden units,m feat classifier has 50 hidden units,0.5898905992507935
translation,132,165,hyperparameters,hyperparameters,has,m feat classifier,hyperparameters has m feat classifier,0.523806095123291
translation,132,171,hyperparameters,classifier hidden units,to,1000,classifier hidden units to 1000,0.6176466345787048
translation,132,171,hyperparameters,20 epochs,using,adam,20 epochs using adam,0.6778091192245483
translation,132,171,hyperparameters,adam,with,learning rate 1e ?3,adam with learning rate 1e ?3,0.6516451239585876
translation,132,171,hyperparameters,hyperparameters,run,20 epochs,hyperparameters run 20 epochs,0.6836355328559875
translation,132,172,hyperparameters,training,if,aupr,training if aupr,0.6895829439163208
translation,132,172,hyperparameters,does not improve,for,3 epochs,does not improve for 3 epochs,0.6176034808158875
translation,132,172,hyperparameters,aupr,has,does not improve,aupr has does not improve,0.6431411504745483
translation,132,172,hyperparameters,hyperparameters,stop,training,hyperparameters stop training,0.6751257181167603
translation,132,27,model,automatic distractor selection,combine,simple features,automatic distractor selection combine simple features,0.6973418593406677
translation,132,27,model,simple features,with,representations,simple features with representations,0.6569303274154663
translation,132,27,model,representations,from,pretrained models,representations from pretrained models,0.5049595236778259
translation,132,27,model,pretrained models,like,bert and elmo,pretrained models like bert and elmo,0.6356264352798462
translation,132,28,results,improve,leading to,performance,improve leading to performance,0.6776461601257324
translation,132,28,results,performance drastically,over,feature - based models,performance drastically over feature - based models,0.6547213196754456
translation,132,28,results,performance drastically,leading to,performance,performance drastically leading to performance,0.7171606421470642
translation,132,28,results,performance drastically,leading to,rivaling,performance drastically leading to rivaling,0.7836867570877075
translation,132,28,results,pretrained models,has,improve,pretrained models has improve,0.5642434358596802
translation,132,28,results,improve,has,performance drastically,improve has performance drastically,0.5284191966056824
translation,132,28,results,performance,has,rivaling,performance has rivaling,0.6041070818901062
translation,132,28,results,humans asked to perform,has,same task,humans asked to perform has same task,0.5479820966720581
translation,132,28,results,results,show,pretrained models,results show pretrained models,0.5682028532028198
translation,132,159,results,human performance,leading to,f1 scores,human performance leading to f1 scores,0.6510027050971985
translation,132,159,results,f1 scores,range of,45 - 61 %,f1 scores range of 45 - 61 %,0.7063578963279724
translation,132,159,results,f1 scores,range of,25 - 34 %,f1 scores range of 25 - 34 %,0.7125933766365051
translation,132,159,results,45 - 61 %,for,mcdsent,45 - 61 % for mcdsent,0.6845369935035706
translation,132,159,results,25 - 34 %,for,mcdpara,25 - 34 % for mcdpara,0.702506959438324
translation,132,159,results,results,has,human performance,results has human performance,0.5620672106742859
translation,132,180,results,much better,than,trivial baseline,much better than trivial baseline,0.5564252734184265
translation,132,180,results,results,has,feature - based model,results has feature - based model,0.5460286736488342
translation,132,181,results,bert features,in,m feat,bert features in m feat,0.6039214134216309
translation,132,181,results,bert features,improves,performance greatly,bert features improves performance greatly,0.7795581817626953
translation,132,181,results,results,Including,bert features,results Including bert features,0.6693790555000305
translation,132,184,results,outperforms,by,wide margin,outperforms by wide margin,0.6322848796844482
translation,132,184,results,m feat,by,wide margin,m feat by wide margin,0.5714057087898254
translation,132,184,results,features,has,m elmo,features has m elmo,0.6600745916366577
translation,132,184,results,m elmo,has,outperforms,m elmo has outperforms,0.7193557024002075
translation,132,184,results,outperforms,has,m feat,outperforms has m feat,0.6847010850906372
translation,132,184,results,results,Even without,features,results Even without features,0.6909865736961365
translation,132,185,results,adding features,to,m elmo,adding features to m elmo,0.6335389018058777
translation,132,185,results,m elmo,improves,f1,m elmo improves f1,0.6882939338684082
translation,132,185,results,f1,by,2 - 5 %,f1 by 2 - 5 %,0.6544380187988281
translation,132,185,results,f1,by,5 - 6 %,f1 by 5 - 6 %,0.6589898467063904
translation,132,185,results,2 - 5 %,for,mcdsent,2 - 5 % for mcdsent,0.7021065354347229
translation,132,185,results,5 - 6 %,for,mcdpara,5 - 6 % for mcdpara,0.7139797806739807
translation,132,185,results,results,has,adding features,results has adding features,0.5833167433738708
translation,132,186,results,f1 score,for,m elmo,f1 score for m elmo,0.6407232284545898
translation,132,186,results,f1 score,outperforms,humans,f1 score outperforms humans,0.7575287818908691
translation,132,186,results,m elmo,on,mcdsent,m elmo on mcdsent,0.565277636051178
translation,132,186,results,m elmo,on,mcdpara,m elmo on mcdpara,0.6336833238601685
translation,132,186,results,m elmo,close to,human performance,m elmo close to human performance,0.7216006517410278
translation,132,186,results,m elmo,on,mcdpara,m elmo on mcdpara,0.6336833238601685
translation,132,186,results,mcdsent,close to,human performance,mcdsent close to human performance,0.7021841406822205
translation,132,186,results,f1 score,outperforms,humans,f1 score outperforms humans,0.7575287818908691
translation,132,186,results,mcdpara,has,f1 score,mcdpara has f1 score,0.5773932337760925
translation,132,186,results,results,on,mcdpara,results on mcdpara,0.5446933507919312
translation,132,186,results,results,has,f1 score,results has f1 score,0.5469173192977905
translation,132,191,results,m bert,with,bert - base,m bert with bert - base,0.6689509749412537
translation,132,191,results,m bert,has,higher aupr,m bert has higher aupr,0.6733106970787048
translation,132,191,results,higher aupr,on,dev,higher aupr on dev,0.6391152143478394
translation,132,191,results,test performance,close to,m elmo,test performance close to m elmo,0.7285100817680359
translation,132,191,results,bert - base,has,higher aupr,bert - base has higher aupr,0.6086797714233398
translation,132,192,results,adding features,improves,performance,adding features improves performance,0.759345531463623
translation,132,192,results,performance,for,mcdpara ( 3 - 5 % f1 ),performance for mcdpara ( 3 - 5 % f1 ),0.6482682228088379
translation,132,192,results,results,has,adding features,results has adding features,0.5833167433738708
translation,132,196,results,dev,has,m elmo and m bert,dev has m elmo and m bert,0.6675484776496887
translation,132,196,results,m elmo and m bert,has,outperform,m elmo and m bert has outperform,0.6258517503738403
translation,132,196,results,outperform,has,m elmo,outperform has m elmo,0.7367857098579407
translation,132,196,results,outperform,has,m bert,outperform has m bert,0.7480692267417908
translation,132,196,results,results,On,dev,results On dev,0.4821946918964386
translation,132,197,results,test aupr results,are,better,test aupr results are better,0.5793246626853943
translation,132,197,results,better,when using,longer context,better when using longer context,0.7217179536819458
translation,132,197,results,results,has,test aupr results,results has test aupr results,0.5700331926345825
translation,132,206,results,m elmo,is,significantly better ( p < 0.01 ),m elmo is significantly better ( p < 0.01 ),0.6001755595207214
translation,132,206,results,both do not use features,has,m elmo,both do not use features has m elmo,0.6855074763298035
translation,132,206,results,results,when,both do not use features,results when both do not use features,0.6370505094528198
translation,132,211,results,features,use,m bert,features use m bert,0.6978147029876709
translation,132,211,results,m bert,with,bert - base-cased,m bert with bert - base-cased,0.7012967467308044
translation,132,211,results,m bert,when,both use features ( p = 0.062 ),m bert when both use features ( p = 0.062 ),0.6661810278892517
translation,132,211,results,m elmo,is,significantly better ( p < 0.01 ),m elmo is significantly better ( p < 0.01 ),0.6001755595207214
translation,132,211,results,m elmo,is,nearly significantly better,m elmo is nearly significantly better,0.6235238313674927
translation,132,211,results,nearly significantly better,than,m bert,nearly significantly better than m bert,0.6331698894500732
translation,132,211,results,m bert,when,both use features ( p = 0.062 ),m bert when both use features ( p = 0.062 ),0.6661810278892517
translation,132,211,results,features,has,m elmo,features has m elmo,0.6600745916366577
translation,132,211,results,m bert,has,m elmo,m bert has m elmo,0.6604149341583252
translation,132,211,results,mcdpara,has,m elmo,mcdpara has m elmo,0.6197381019592285
translation,132,211,results,results,add,features,results add features,0.5998578667640686
translation,132,212,results,features,for,both models,features for both models,0.6356281042098999
translation,132,212,results,both models,makes,m bert,both models makes m bert,0.7085784673690796
translation,132,212,results,dropping,has,features,dropping has features,0.6207950711250305
translation,132,212,results,m bert,has,significantly outperform,m bert has significantly outperform,0.6474064588546753
translation,132,212,results,significantly outperform,has,m elmo,significantly outperform has m elmo,0.630355715751648
translation,132,212,results,significantly outperform,has,p = 0.044 ),significantly outperform has p = 0.044 ),0.5823224186897278
translation,132,212,results,m elmo,has,p = 0.044 ),m elmo has p = 0.044 ),0.5684094429016113
translation,132,212,results,results,has,dropping,results has dropping,0.4815814197063446
translation,133,212,baselines,nff,has,"hu et al. , 2018 )","nff has hu et al. , 2018 )",0.5861244797706604
translation,133,213,baselines,quint,is,model,quint is model,0.6165785789489746
translation,133,213,baselines,quint,is,aqqu,quint is aqqu,0.6750429272651672
translation,133,213,baselines,model,used in,original com-plexquestions paper,model used in original com-plexquestions paper,0.673508882522583
translation,133,213,baselines,aqqu,is,best publicly available qa system,aqqu is best publicly available qa system,0.5214640498161316
translation,133,213,baselines,best publicly available qa system,on,webquestions,best publicly available qa system on webquestions,0.5080142021179199
translation,133,213,baselines,baselines,has,quint,baselines has quint,0.6376553773880005
translation,133,215,baselines,aqqu,defines,three query templates,aqqu defines three query templates,0.7063394784927368
translation,133,215,baselines,aqqu,try to match,test questions,aqqu try to match test questions,0.7067111134529114
translation,133,215,baselines,three query templates,for,we- bquestions,three query templates for we- bquestions,0.690878689289093
translation,133,215,baselines,test questions,to,predefined templates,test questions to predefined templates,0.5967264175415039
translation,133,215,baselines,baselines,has,aqqu,baselines has aqqu,0.6316252946853638
translation,133,218,baselines,quint,is,system,quint is system,0.6555613279342651
translation,133,218,baselines,quint,automatically learns,utterance -query templates,quint automatically learns utterance -query templates,0.7387235760688782
translation,133,218,baselines,utterance -query templates,from,pairs of question and answer set,utterance -query templates from pairs of question and answer set,0.563339352607727
translation,133,218,baselines,baselines,has,quint,baselines has quint,0.6376553773880005
translation,133,219,baselines,nff,builds,relation paraphrase dictionary,nff builds relation paraphrase dictionary,0.6557981371879578
translation,133,219,baselines,relation paraphrase dictionary,to extract,relations,relation paraphrase dictionary to extract relations,0.6469057202339172
translation,133,219,baselines,relations,from,questions,relations from questions,0.6062990427017212
translation,133,219,baselines,baselines,has,nff,baselines has nff,0.5679892897605896
translation,133,220,baselines,stagg,proposes,state-transition based query graph generation,stagg proposes state-transition based query graph generation,0.6583133935928345
translation,133,220,baselines,baselines,has,stagg,baselines has stagg,0.6253111362457275
translation,133,196,experiments,dbpedia,use,qald - 6 benchmark,dbpedia use qald - 6 benchmark,0.5837045311927795
translation,133,197,experiments,freebase,use,"webquestions ( berant et al. , 2013 )","freebase use webquestions ( berant et al. , 2013 )",0.6162802577018738
translation,133,197,experiments,freebase,use,"complexquestions ( abujabal et al. , 2017 )","freebase use complexquestions ( abujabal et al. , 2017 )",0.6301717162132263
translation,133,5,model,state transition - based approach,to translate,complex natural language question n,state transition - based approach to translate complex natural language question n,0.695955753326416
translation,133,5,model,complex natural language question n,to,semantic query graph ( sqg ) q s,complex natural language question n to semantic query graph ( sqg ) q s,0.5763258337974548
translation,133,5,model,semantic query graph ( sqg ) q s,to match,underlying knowledge graph,semantic query graph ( sqg ) q s to match underlying knowledge graph,0.6757831573486328
translation,133,5,model,underlying knowledge graph,to find,answers to question n,underlying knowledge graph to find answers to question n,0.6351078748703003
translation,133,5,model,model,propose,state transition - based approach,model propose state transition - based approach,0.7138477563858032
translation,133,6,model,q s,propose,four primitive operations,q s propose four primitive operations,0.6475757956504822
translation,133,6,model,q s,propose,learning - based state transition approach,q s propose learning - based state transition approach,0.6809735298156738
translation,133,6,model,four primitive operations,has,expand,four primitive operations has expand,0.5928108096122742
translation,133,6,model,model,to generate,q s,model to generate q s,0.7059956789016724
translation,133,47,model,state transition,learn,reward function,state transition learn reward function,0.6550968289375305
translation,133,47,model,reward function,using,svm ranking algorithm,reward function using svm ranking algorithm,0.6571410298347473
translation,133,47,model,svm ranking algorithm,to greedily select,subsequent state,svm ranking algorithm to greedily select subsequent state,0.6606964468955994
translation,133,47,model,model,To guide,state transition,model To guide state transition,0.7092186808586121
translation,133,283,model,query graph,by,state transition,query graph by state transition,0.5218024253845215
translation,133,283,model,model,build,query graph,model build query graph,0.6940094828605652
translation,133,291,model,state transition framework,to utilize,neural networks,state transition framework to utilize neural networks,0.641059935092926
translation,133,291,model,neural networks,to answer,complex questions,neural networks to answer complex questions,0.6712787747383118
translation,133,291,model,neural networks,generates,semantic query graph,neural networks generates semantic query graph,0.6046797633171082
translation,133,291,model,semantic query graph,based on,four primitive operations,semantic query graph based on four primitive operations,0.6147817969322205
translation,133,291,model,model,propose,state transition framework,model propose state transition framework,0.6754161715507507
translation,133,292,model,blstm - crf model,to recognize,nodes,blstm - crf model to recognize nodes,0.7143408060073853
translation,133,292,model,nodes,including,entities and variables,nodes including entities and variables,0.6660851240158081
translation,133,292,model,entities and variables,from,question sentence,entities and variables from question sentence,0.5154029130935669
translation,133,292,model,model,train,blstm - crf model,model train blstm - crf model,0.658329427242279
translation,133,293,model,relations,propose,mccnn model,relations propose mccnn model,0.6614140272140503
translation,133,293,model,mccnn model,tackle,explicit and implicit relations,mccnn model tackle explicit and implicit relations,0.6957240700721741
translation,133,293,model,model,To extract,relations,model To extract relations,0.7055231332778931
translation,133,211,results,test set,of,webquestions ( wq ),test set of webquestions ( wq ),0.5618053078651428
translation,133,211,results,test set,of,complexquestions ( cq ),test set of complexquestions ( cq ),0.5779447555541992
translation,133,211,results,test set,based on,freebase,test set based on freebase,0.6421321034431458
translation,133,211,results,complexquestions ( cq ),based on,freebase,complexquestions ( cq ) based on freebase,0.6345470547676086
translation,133,211,results,results,on,webquestions and complexquestions,results on webquestions and complexquestions,0.49545812606811523
translation,133,211,results,results,on,test set,results on test set,0.582119882106781
translation,133,211,results,results,on,test set,results on test set,0.582119882106781
translation,133,214,results,average f1,of,our system,average f1 of our system,0.5479342937469482
translation,133,214,results,average f1,better than,other systems,average f1 better than other systems,0.7798961997032166
translation,133,214,results,our system,better than,other systems,our system better than other systems,0.7007576823234558
translation,133,214,results,53.6 %,for,wq,53.6 % for wq,0.6582680940628052
translation,133,214,results,53.6 %,better than,other systems,53.6 % better than other systems,0.7152787446975708
translation,133,214,results,54.3 %,for,cq,54.3 % for cq,0.6625332236289978
translation,133,214,results,54.3 %,better than,other systems,54.3 % better than other systems,0.7115393280982971
translation,133,214,results,cq,better than,other systems,cq better than other systems,0.7880675196647644
translation,133,214,results,our system,has,53.6 %,our system has 53.6 %,0.5509737133979797
translation,133,214,results,results,has,average f1,results has average f1,0.5551680326461792
translation,133,217,results,aqqu ++,shows,result ( 46.7 % ),aqqu ++ shows result ( 46.7 % ),0.6704603433609009
translation,133,217,results,aqqu ++,getting,intersection,aqqu ++ getting intersection,0.6326423287391663
translation,133,217,results,result ( 46.7 % ),by taking,manually decomposed subquestions,result ( 46.7 % ) by taking manually decomposed subquestions,0.5810366868972778
translation,133,217,results,result ( 46.7 % ),getting,intersection,result ( 46.7 % ) getting intersection,0.6469220519065857
translation,133,217,results,manually decomposed subquestions,as,input,manually decomposed subquestions as input,0.48057472705841064
translation,133,217,results,intersection,has,of subquestions ' answer sets,intersection has of subquestions ' answer sets,0.6061497330665588
translation,133,217,results,results,has,aqqu ++,results has aqqu ++,0.5471957325935364
translation,133,226,results,webquestions benchmark,prove,our approach,webquestions benchmark prove our approach,0.6339073181152344
translation,133,226,results,our approach,able to obtain,good performance,our approach able to obtain good performance,0.623976469039917
translation,133,226,results,good performance,on,simple questions,good performance on simple questions,0.5172144174575806
translation,133,226,results,results,on,webquestions benchmark,results on webquestions benchmark,0.5009753704071045
translation,133,227,results,complexquestions benchmark,more complex than,webquestions,complexquestions benchmark more complex than webquestions,0.6776743531227112
translation,133,227,results,results,has,complexquestions benchmark,results has complexquestions benchmark,0.5613198280334473
translation,133,230,results,our system,has,outperforms,our system has outperforms,0.6423544883728027
translation,133,230,results,results,has,our system,results has our system,0.5954442024230957
translation,133,232,results,test set,of,qald -6,test set of qald -6,0.6502482891082764
translation,133,232,results,results,on,test set,results on test set,0.582119882106781
translation,133,237,results,all systems,in,qald - 6 campaign,all systems in qald - 6 campaign,0.5946937203407288
translation,133,237,results,qald - 6 campaign,in,f - 1,qald - 6 campaign in f - 1,0.6084967851638794
translation,133,237,results,f - 1,except for,canali ( mazzeo and zaniolo ),f - 1 except for canali ( mazzeo and zaniolo ),0.7060274481773376
translation,134,154,ablation-analysis,uni-attention model,eliminated,attention,uni-attention model eliminated attention,0.6301801204681396
translation,134,154,ablation-analysis,uni-attention model,used,module,uni-attention model used module,0.5857651233673096
translation,134,154,ablation-analysis,attention,on,sentences,attention on sentences,0.5643457770347595
translation,134,154,ablation-analysis,module,for,attention,module for attention,0.6290864944458008
translation,134,154,ablation-analysis,attention,on,questions,attention on questions,0.5848429203033447
translation,134,154,ablation-analysis,ablation analysis,In,uni-attention model,ablation analysis In uni-attention model,0.5263369083404541
translation,134,155,hyperparameters,300 - dimensional pre-trained glove word vectors,to generate,embedding matrix,300 - dimensional pre-trained glove word vectors to generate embedding matrix,0.6696459650993347
translation,134,155,hyperparameters,300 - dimensional pre-trained glove word vectors,kept,embedding matrix,300 - dimensional pre-trained glove word vectors kept embedding matrix,0.578957736492157
translation,134,155,hyperparameters,embedding matrix,updated through,training,embedding matrix updated through training,0.6597287654876709
translation,134,155,hyperparameters,embedding matrix,updated through,training,embedding matrix updated through training,0.6597287654876709
translation,134,156,hyperparameters,128 - dimensional lstms,for,all recurrent networks,128 - dimensional lstms for all recurrent networks,0.5955644845962524
translation,134,156,hyperparameters,128 - dimensional lstms,used,adam,128 - dimensional lstms used adam,0.5539355874061584
translation,134,156,hyperparameters,adam,parameters,"learning rate =0.001 , ? 1 = 0.9 , ? 2 = 0.999","adam parameters learning rate =0.001 , ? 1 = 0.9 , ? 2 = 0.999",0.6780679225921631
translation,134,156,hyperparameters,"learning rate =0.001 , ? 1 = 0.9 , ? 2 = 0.999",for,optimization,"learning rate =0.001 , ? 1 = 0.9 , ? 2 = 0.999 for optimization",0.6055242419242859
translation,134,156,hyperparameters,hyperparameters,used,128 - dimensional lstms,hyperparameters used 128 - dimensional lstms,0.5828389525413513
translation,134,156,hyperparameters,hyperparameters,used,adam,hyperparameters used adam,0.6175551414489746
translation,134,156,hyperparameters,hyperparameters,used,adam,hyperparameters used adam,0.6175551414489746
translation,134,157,hyperparameters,dropout rate,to,0.5,dropout rate to 0.5,0.552266538143158
translation,134,157,hyperparameters,0.5,for,all lstms and embedding layers,0.5 for all lstms and embedding layers,0.5781875848770142
translation,134,157,hyperparameters,hyperparameters,set,batch size,hyperparameters set batch size,0.6402808427810669
translation,134,157,hyperparameters,hyperparameters,set,dropout rate,hyperparameters set dropout rate,0.5970823764801025
translation,134,5,model,architecture,for,textual encoding,architecture for textual encoding,0.5835186839103699
translation,134,5,model,architecture,introduce,deep end-to - end neural model,architecture introduce deep end-to - end neural model,0.5986835956573486
translation,134,5,model,textual encoding,introduce,deep end-to - end neural model,textual encoding introduce deep end-to - end neural model,0.5685727000236511
translation,134,5,model,model,elaborate on,architecture,model elaborate on architecture,0.721490204334259
translation,134,6,model,bilateral attention mechanism,helps,model,bilateral attention mechanism helps model,0.5251940488815308
translation,134,6,model,model,to focus on,question,model to focus on question,0.7722604274749756
translation,134,6,model,model,to focus on,answer sentence,model to focus on answer sentence,0.6919261813163757
translation,134,7,model,output,of,constituency parser,output of constituency parser,0.5752437710762024
translation,134,7,model,output,into,model,output into model,0.6429453492164612
translation,134,7,model,constituency parser,into,model,constituency parser into model,0.5696030855178833
translation,134,7,model,linguistic constituents,into,network,linguistic constituents into network,0.5858634114265442
translation,134,7,model,chunks,of,answer,chunks of answer,0.6378090977668762
translation,134,7,model,chunks,for generating,more natural output,chunks for generating more natural output,0.6911635398864746
translation,134,7,model,model,feed,output,model feed output,0.768112063407898
translation,134,161,results,results,of,uni- / biattention and word / constituent - base models,results of uni- / biattention and word / constituent - base models,0.486868292093277
translation,134,161,results,uni- / biattention and word / constituent - base models,see that,proposed bi-attention mechanism,uni- / biattention and word / constituent - base models see that proposed bi-attention mechanism,0.6222437620162964
translation,134,161,results,proposed bi-attention mechanism,with,linguistic constituents,proposed bi-attention mechanism with linguistic constituents,0.6151657104492188
translation,134,161,results,proposed bi-attention mechanism,integrated into,significant improvement,proposed bi-attention mechanism integrated into significant improvement,0.7229762673377991
translation,134,161,results,proposed bi-attention mechanism,makes,significant improvement,proposed bi-attention mechanism makes significant improvement,0.6148707270622253
translation,134,161,results,significant improvement,on,answer extraction,significant improvement on answer extraction,0.5149719715118408
translation,134,161,results,results,contrasting,results,results contrasting results,0.6419586539268494
translation,134,161,results,results,contrasting,uni- / biattention and word / constituent - base models,results contrasting uni- / biattention and word / constituent - base models,0.6485099196434021
translation,134,161,results,results,of,uni- / biattention and word / constituent - base models,results of uni- / biattention and word / constituent - base models,0.486868292093277
translation,134,161,results,results,see that,proposed bi-attention mechanism,results see that proposed bi-attention mechanism,0.6724944114685059
translation,134,162,results,exact match metric,benefits from,restriction,exact match metric benefits from restriction,0.68764328956604
translation,134,162,results,restriction,to,constituents as answers,restriction to constituents as answers,0.5641210079193115
translation,134,162,results,results,has,interesting observation,results has interesting observation,0.5519416928291321
translation,134,188,results,constituents,instead of,arbitrary string of words,constituents instead of arbitrary string of words,0.677232563495636
translation,134,188,results,constituents,improves,system performance,constituents improves system performance,0.7493044137954712
translation,134,188,results,arbitrary string of words,in,answers,arbitrary string of words in answers,0.5389301776885986
translation,134,188,results,results,use of,constituents,results use of constituents,0.6319253444671631
translation,134,190,results,small gap,between,f1 and the exact match metrics,small gap between f1 and the exact match metrics,0.6051037311553955
translation,134,190,results,f1 and the exact match metrics,in,our system,f1 and the exact match metrics in our system,0.5181342959403992
translation,134,190,results,ratio,of,exact-match answers,ratio of exact-match answers,0.593708336353302
translation,134,190,results,exact-match answers,in,our system,exact-match answers in our system,0.5283964276313782
translation,134,190,results,exact-match answers,higher than,other ones,exact-match answers higher than other ones,0.7172641158103943
translation,134,190,results,results,looking at,small gap,results looking at small gap,0.6236036419868469
translation,135,9,ablation-analysis,deformer versions,of,bert and xlnet,deformer versions of bert and xlnet,0.5969264507293701
translation,135,9,ablation-analysis,deformer versions,used to,speed up,deformer versions used to speed up,0.6377472877502441
translation,135,9,ablation-analysis,speed up,with,simple distillation - based losses,speed up with simple distillation - based losses,0.6351388096809387
translation,135,9,ablation-analysis,simple distillation - based losses,incur,1 % drop,simple distillation - based losses incur 1 % drop,0.7665873169898987
translation,135,9,ablation-analysis,1 % drop,in,accuracy,1 % drop in accuracy,0.5509111285209656
translation,135,9,ablation-analysis,speed up,has,qa,speed up has qa,0.6152685880661011
translation,135,9,ablation-analysis,ablation analysis,show,deformer versions,ablation analysis show deformer versions,0.6412767767906189
translation,135,124,ablation-analysis,auxiliary losses,for,fine-tuning deformer - bert,auxiliary losses for fine-tuning deformer - bert,0.6558424830436707
translation,135,124,ablation-analysis,fine-tuning deformer - bert,on,squad dataset,fine-tuning deformer - bert on squad dataset,0.5061463117599487
translation,135,124,ablation-analysis,ablation analysis,contribution of,auxiliary losses,ablation analysis contribution of auxiliary losses,0.6776407957077026
translation,135,98,experimental-setup,all models,in,"tensorflow 1.15 ( abadi et al. , 2015 )","all models in tensorflow 1.15 ( abadi et al. , 2015 )",0.4532052278518677
translation,135,98,experimental-setup,"tensorflow 1.15 ( abadi et al. , 2015 )",based on,"original bert ( devlin et al. , 2019 )","tensorflow 1.15 ( abadi et al. , 2015 ) based on original bert ( devlin et al. , 2019 )",0.6259788870811462
translation,135,98,experimental-setup,experimental setup,implement,all models,experimental setup implement all models,0.6613198518753052
translation,135,99,experimental-setup,all experiments,on,one tpu v3 - 8 node,all experiments on one tpu v3 - 8 node,0.5708822011947632
translation,135,99,experimental-setup,one tpu v3 - 8 node,with,bfloat16 format,one tpu v3 - 8 node with bfloat16 format,0.6941376328468323
translation,135,99,experimental-setup,one tpu v3 - 8 node,has,8 cores,one tpu v3 - 8 node has 8 cores,0.577969491481781
translation,135,99,experimental-setup,one tpu v3 - 8 node,has,128gb memory,one tpu v3 - 8 node has 128gb memory,0.5474619269371033
translation,135,99,experimental-setup,experimental setup,perform,all experiments,experimental setup perform all experiments,0.6174839735031128
translation,135,100,experimental-setup,flops and memory consumption,through,tensorflow profiler,flops and memory consumption through tensorflow profiler,0.6188002824783325
translation,135,100,experimental-setup,tensorflow profiler,For,deformer models,tensorflow profiler For deformer models,0.5899607539176941
translation,135,100,experimental-setup,deformer models,tune,hyperparameters,deformer models tune hyperparameters,0.7069883346557617
translation,135,100,experimental-setup,hyperparameters,for weighting,different losses,hyperparameters for weighting different losses,0.7111359238624573
translation,135,100,experimental-setup,hyperparameters,with,50 iterations,hyperparameters with 50 iterations,0.5635609030723572
translation,135,100,experimental-setup,different losses,using,"bayesian optimizaiton libray ( nogueira , fernando , 2019 )","different losses using bayesian optimizaiton libray ( nogueira , fernando , 2019 )",0.6949160099029541
translation,135,100,experimental-setup,"bayesian optimizaiton libray ( nogueira , fernando , 2019 )",with,50 iterations,"bayesian optimizaiton libray ( nogueira , fernando , 2019 ) with 50 iterations",0.6378981471061707
translation,135,100,experimental-setup,50 iterations,on,tune split,50 iterations on tune split,0.590329110622406
translation,135,100,experimental-setup,experimental setup,measure,flops and memory consumption,experimental setup measure flops and memory consumption,0.6362370848655701
translation,135,100,experimental-setup,experimental setup,For,deformer models,experimental setup For deformer models,0.5559401512145996
translation,135,100,experimental-setup,experimental setup,tune,hyperparameters,experimental setup tune hyperparameters,0.6542923450469971
translation,135,6,model,full self-attention,with,question - wide and passage - wide self-attentions,full self-attention with question - wide and passage - wide self-attentions,0.643274188041687
translation,135,6,model,question - wide and passage - wide self-attentions,in,lower layers,question - wide and passage - wide self-attentions in lower layers,0.5159412026405334
translation,135,6,model,deformer,has,decomposed transformer,deformer has decomposed transformer,0.6229504346847534
translation,135,6,model,model,introduce,deformer,model introduce deformer,0.6699735522270203
translation,135,7,model,question - independent processing,of,input text representations,question - independent processing of input text representations,0.5644267201423645
translation,135,7,model,model,allows for,question - independent processing,model allows for question - independent processing,0.731264591217041
translation,135,37,model,two distillation - like auxiliary losses,minimize,output-level and the layer - level divergences,two distillation - like auxiliary losses minimize output-level and the layer - level divergences,0.7369555830955505
translation,135,37,model,output-level and the layer - level divergences,between,decomposed and original models,output-level and the layer - level divergences between decomposed and original models,0.600544273853302
translation,135,37,model,model,add,two distillation - like auxiliary losses,model add two distillation - like auxiliary losses,0.6319164633750916
translation,135,74,model,auxiliary losses,that make,deformer predictions and its upper layer representations,auxiliary losses that make deformer predictions and its upper layer representations,0.5803943872451782
translation,135,74,model,deformer predictions and its upper layer representations,closer to,predictions,deformer predictions and its upper layer representations closer to predictions,0.6557369232177734
translation,135,74,model,deformer predictions and its upper layer representations,closer to,corresponding layer representations,deformer predictions and its upper layer representations closer to corresponding layer representations,0.6346002221107483
translation,135,74,model,corresponding layer representations,of,full transformer,corresponding layer representations of full transformer,0.6194909811019897
translation,135,74,model,model,add,auxiliary losses,model add auxiliary losses,0.6415581703186035
translation,135,141,model,auxiliary supervision,of,upper layers,auxiliary supervision of upper layers,0.5330719947814941
translation,135,141,model,desired effect,of,forcing deformer,desired effect of forcing deformer,0.6027438640594482
translation,135,141,model,forcing deformer,to produce,representations,forcing deformer to produce representations,0.703444242477417
translation,135,141,model,representations,closer to,original model,representations closer to original model,0.7147961258888245
translation,135,141,model,auxiliary supervision,has,desired effect,auxiliary supervision has desired effect,0.5788371562957764
translation,135,141,model,upper layers,has,desired effect,upper layers has desired effect,0.5867894887924194
translation,135,141,model,model,using,auxiliary supervision,model using auxiliary supervision,0.6863840818405151
translation,135,8,results,deformer,with,pre-training weights,deformer with pre-training weights,0.6362496018409729
translation,135,8,results,deformer,directly fine-tune,target qa dataset,deformer directly fine-tune target qa dataset,0.658872663974762
translation,135,8,results,pre-training weights,of,standard transformer,pre-training weights of standard transformer,0.5620277523994446
translation,135,8,results,results,initialize,deformer,results initialize deformer,0.7138298749923706
translation,135,38,results,deformer,achieves,substantial speedup ( 2.7 to 4.3 x ),deformer achieves substantial speedup ( 2.7 to 4.3 x ),0.6603077054023743
translation,135,38,results,deformer,reduction in,memory ( 65.8 % to 72.9 % ),deformer reduction in memory ( 65.8 % to 72.9 % ),0.7231850028038025
translation,135,38,results,to 1.8 points ),for,qa,to 1.8 points ) for qa,0.5836485028266907
translation,135,38,results,results,evaluate,deformer versions,results evaluate deformer versions,0.574622631072998
translation,135,39,results,deformer version of bert - large,is,faster,deformer version of bert - large is faster,0.6022725105285645
translation,135,39,results,faster,than,original version,faster than original version,0.5845324397087097
translation,135,39,results,original version,of,smaller bertbase model,original version of smaller bertbase model,0.5743275880813599
translation,135,39,results,results,find that,deformer version of bert - large,results find that deformer version of bert - large,0.6533718109130859
translation,135,106,results,significant memory reduction,in,all the datasets,significant memory reduction in all the datasets,0.48604002594947815
translation,135,106,results,most of the original model 's effectiveness,as,98.4 %,most of the original model 's effectiveness as 98.4 %,0.5164985060691833
translation,135,106,results,most of the original model 's effectiveness,as,99.8 %,most of the original model 's effectiveness as 99.8 %,0.5071739554405212
translation,135,106,results,most of the original model 's effectiveness,much as,98.4 %,most of the original model 's effectiveness much as 98.4 %,0.5786597728729248
translation,135,106,results,most of the original model 's effectiveness,much as,99.8 %,most of the original model 's effectiveness much as 99.8 %,0.5713613629341125
translation,135,106,results,98.4 %,on,squad,98.4 % on squad,0.5662974119186401
translation,135,106,results,99.8 %,on,qqp datasets,99.8 % on qqp datasets,0.5332448482513428
translation,135,106,results,results,observe,substantial speedup,results observe substantial speedup,0.6323851943016052
translation,135,106,results,results,retaining,most of the original model 's effectiveness,results retaining most of the original model 's effectiveness,0.7115233540534973
translation,135,107,results,decomposition,brings,2x speedup,decomposition brings 2x speedup,0.6227170825004578
translation,135,107,results,decomposition,more than half of,memory reduction,decomposition more than half of memory reduction,0.5646559000015259
translation,135,107,results,2x speedup,in,inference,2x speedup in inference,0.5531284809112549
translation,135,107,results,memory reduction,on,qqp and mnli datasets,memory reduction on qqp and mnli datasets,0.49372175335884094
translation,135,107,results,qqp and mnli datasets,take,pairwise input sequences,qqp and mnli datasets take pairwise input sequences,0.5681270360946655
translation,135,107,results,results,shows,decomposition,results shows decomposition,0.6039606332778931
translation,135,112,results,larger model,turns out to be,more effective,larger model turns out to be more effective,0.5711150169372559
translation,135,112,results,more effective,than using,smaller base model,more effective than using smaller base model,0.723328173160553
translation,135,112,results,smaller base model,has,+ 2.3 points ),smaller base model has + 2.3 points ),0.5826998949050903
translation,135,112,results,results,Decomposing,larger model,results Decomposing larger model,0.7546965479850769
translation,135,122,results,all devices,get,more than three times,all devices get more than three times,0.6084068417549133
translation,135,122,results,more than three times,has,speedup,more than three times has speedup,0.5946919322013855
translation,135,122,results,results,On,all devices,results On all devices,0.4974324107170105
translation,135,126,results,effectiveness and inference speed,of,deformer - bert,effectiveness and inference speed of deformer - bert,0.6050874590873718
translation,135,126,results,changes,as,separation layer,changes as separation layer,0.6185935139656067
translation,135,126,results,changes,change,separation layer,changes change separation layer,0.8034244179725647
translation,135,126,results,deformer - bert,has,changes,deformer - bert has changes,0.6395138502120972
translation,135,126,results,results,show how,effectiveness and inference speed,results show how effectiveness and inference speed,0.6746983528137207
translation,135,140,results,lower layer representations,of,passage and questions,lower layer representations of passage and questions,0.5688859820365906
translation,135,140,results,passage and questions,for,both models,passage and questions for both models,0.6410126686096191
translation,135,140,results,both models,remain,similar,both models remain similar,0.6877617239952087
translation,135,140,results,upper layer representations,has,differ significantly,upper layer representations has differ significantly,0.5608431100845337
translation,135,140,results,results,has,lower layer representations,results has lower layer representations,0.5192063450813293
translation,136,114,ablation-analysis,both videos and subtitles ( v+ s +q ),brings,accuracy,both videos and subtitles ( v+ s +q ) brings accuracy,0.6334156394004822
translation,136,114,ablation-analysis,accuracy,to,89.41 %,accuracy to 89.41 %,0.5312098264694214
translation,136,114,ablation-analysis,ablation analysis,Adding,both videos and subtitles ( v+ s +q ),ablation analysis Adding both videos and subtitles ( v+ s +q ),0.7403140068054199
translation,136,129,experimental-setup,"resnet101 ( he et al. , 2016 )",trained on,"imagenet ( deng et al. , 2009 )","resnet101 ( he et al. , 2016 ) trained on imagenet ( deng et al. , 2009 )",0.6809207201004028
translation,136,129,experimental-setup,"imagenet ( deng et al. , 2009 )",to extract,whole image features,"imagenet ( deng et al. , 2009 ) to extract whole image features",0.6882337927818298
translation,136,172,experimental-setup,sentences,into,vectors,sentences into vectors,0.5866346955299377
translation,136,172,experimental-setup,sentences,using,skipthought,sentences using skipthought,0.6757240295410156
translation,136,172,experimental-setup,vectors,using,tfidf,vectors using tfidf,0.6830790042877197
translation,136,172,experimental-setup,vectors,using,skipthought,vectors using skipthought,0.7386749982833862
translation,136,172,experimental-setup,cosine similarity,for,each questionanswer pair or subtitle - answer pair,cosine similarity for each questionanswer pair or subtitle - answer pair,0.5921618342399597
translation,136,172,experimental-setup,averaged glove,has,"pennington et al. , 2014 ) word vectors","averaged glove has pennington et al. , 2014 ) word vectors",0.5579559803009033
translation,136,172,experimental-setup,experimental setup,embed,sentences,experimental setup embed sentences,0.6397377848625183
translation,136,6,experiments,largescale video qa dataset,based on,6 popular tv shows,largescale video qa dataset based on 6 popular tv shows,0.5019202828407288
translation,136,6,experiments,tvqa,has,largescale video qa dataset,tvqa has largescale video qa dataset,0.4693242609500885
translation,136,7,experiments,tvqa,consists of,"152,545 qa pairs","tvqa consists of 152,545 qa pairs",0.6575899720191956
translation,136,7,experiments,"152,545 qa pairs",from,"21,793 clips","152,545 qa pairs from 21,793 clips",0.5621748566627502
translation,136,154,model,context matching module,adopted from,contextquery attention layer,context matching module adopted from contextquery attention layer,0.5513643622398376
translation,136,154,model,model,has,context matching module,model has context matching module,0.5780428647994995
translation,136,113,results,human accuracy,based only on,question - answer pairs ( q ),human accuracy based only on question - answer pairs ( q ),0.6183292269706726
translation,136,113,results,human accuracy,adding,"videos ( v+ q ) , or subtitles ( s+ q )","human accuracy adding videos ( v+ q ) , or subtitles ( s+ q )",0.7041078805923462
translation,136,113,results,human accuracy,has,significantly improves,human accuracy has significantly improves,0.6318269968032837
translation,136,113,results,"videos ( v+ q ) , or subtitles ( s+ q )",has,significantly improves,"videos ( v+ q ) , or subtitles ( s+ q ) has significantly improves",0.586556077003479
translation,136,113,results,significantly improves,has,human performance,significantly improves has human performance,0.6070736646652222
translation,136,113,results,results,compared to,human accuracy,results compared to human accuracy,0.6328331828117371
translation,136,116,results,workers,obtain,31.84 % accuracy,workers obtain 31.84 % accuracy,0.597011923789978
translation,136,116,results,31.84 % accuracy,given,questionanswer pairs,31.84 % accuracy given questionanswer pairs,0.6533551812171936
translation,136,116,results,31.84 % accuracy,higher than,random guessing,31.84 % accuracy higher than random guessing,0.7001786828041077
translation,136,116,results,results,observe,workers,results observe workers,0.4584735929965973
translation,136,183,results,30.41 %,compared to,random chance,30.41 % compared to random chance,0.6756032109260559
translation,136,183,results,results,of,longest answer baseline,results of longest answer baseline,0.5618846416473389
translation,136,183,results,results,has,baseline comparison,results has baseline comparison,0.5458804368972778
translation,136,184,results,answer-question similarity based methods,perform,rather poorly,answer-question similarity based methods perform rather poorly,0.5857738852500916
translation,136,184,results,results,has,retrieval - based methods,results has retrieval - based methods,0.5616127848625183
translation,136,185,results,subtitle - answer similarity,to choose,correct answers,subtitle - answer similarity to choose correct answers,0.6394721865653992
translation,136,185,results,subtitle - answer similarity,achieve,significant improvement,subtitle - answer similarity achieve significant improvement,0.6171373128890991
translation,136,185,results,glove,achieve,significant improvement,glove achieve significant improvement,0.6759960055351257
translation,136,185,results,tfidf based approaches,achieve,significant improvement,tfidf based approaches achieve significant improvement,0.6118597388267517
translation,136,185,results,significant improvement,over,question - answer similarity,significant improvement over question - answer similarity,0.6337156891822815
translation,136,185,results,subtitle - answer similarity,has,glove,subtitle - answer similarity has glove,0.5919922590255737
translation,136,185,results,subtitle - answer similarity,has,tfidf based approaches,subtitle - answer similarity has tfidf based approaches,0.5726448893547058
translation,136,185,results,correct answers,has,glove,correct answers has glove,0.6107061505317688
translation,136,185,results,results,using,subtitle - answer similarity,results using subtitle - answer similarity,0.6717930436134338
translation,136,193,results,best performance,achieved by using,all the contextual sources,best performance achieved by using all the contextual sources,0.6457163691520691
translation,136,193,results,all the contextual sources,including,and videos,all the contextual sources including and videos,0.6925106644630432
translation,136,193,results,results,has,best performance,results has best performance,0.5759831070899963
translation,138,126,baselines,bilstm,concatenate,question and context / long answer,bilstm concatenate question and context / long answer,0.7171216011047363
translation,138,126,baselines,question and context / long answer,with,learnable segment embeddings,question and context / long answer with learnable segment embeddings,0.6258487701416016
translation,138,126,baselines,learnable segment embeddings,appended to,biomedical word2vec embeddings,learnable segment embeddings appended to biomedical word2vec embeddings,0.4738257825374603
translation,138,126,baselines,biomedical word2vec embeddings,of,each token,biomedical word2vec embeddings of each token,0.5169004797935486
translation,138,126,baselines,baselines,has,bilstm,baselines has bilstm,0.5800488591194153
translation,138,32,experimental-setup,over 25 million references,of,biomedical articles,over 25 million references of biomedical articles,0.48768582940101624
translation,138,32,experimental-setup,experimental setup,turn to,pubmed,experimental setup turn to pubmed,0.6566721200942993
translation,138,96,experimental-setup,biobert,initialized with,"bert ( devlin et al. , 2018 )","biobert initialized with bert ( devlin et al. , 2018 )",0.7493959665298462
translation,138,96,experimental-setup,biobert,pretrained on,pubmed abstracts and pmc 7 articles,biobert pretrained on pubmed abstracts and pmc 7 articles,0.6640027761459351
translation,138,96,experimental-setup,experimental setup,has,biobert,experimental setup has biobert,0.589751124382019
translation,138,128,experimental-setup,esim with bioelmo,use,pre-trained biomedical contextualized embeddings bioelmo ( jin,esim with bioelmo use pre-trained biomedical contextualized embeddings bioelmo ( jin,0.574186384677887
translation,138,128,experimental-setup,2019 ),for,word representations,2019 ) for word representations,0.637700080871582
translation,138,128,experimental-setup,experimental setup,has,esim with bioelmo,experimental setup has esim with bioelmo,0.6101771593093872
translation,138,40,experiments,biomedical qa dataset,for answering,research questions,biomedical qa dataset for answering research questions,0.7424379587173462
translation,138,40,experiments,research questions,using,yes / no / maybe,research questions using yes / no / maybe,0.65692138671875
translation,138,40,experiments,pubmedqa,has,biomedical qa dataset,pubmedqa has biomedical qa dataset,0.5804342031478882
translation,138,95,experiments,biobert,on,pub-medqa,biobert on pub-medqa,0.657193660736084
translation,138,10,results,multi- phase fine-tuning,of,biobert,multi- phase fine-tuning of biobert,0.6112127304077148
translation,138,10,results,biobert,with,long answer bag-of - word statistics,biobert with long answer bag-of - word statistics,0.6409773230552673
translation,138,10,results,long answer bag-of - word statistics,as,additional supervision,long answer bag-of - word statistics as additional supervision,0.48170360922813416
translation,138,10,results,68.1 % accuracy,compared to,single human performance,68.1 % accuracy compared to single human performance,0.6426379084587097
translation,138,10,results,single human performance,of,78.0 % accuracy,single human performance of 78.0 % accuracy,0.5418443083763123
translation,138,10,results,single human performance,of,majority - baseline,single human performance of majority - baseline,0.5445706844329834
translation,138,10,results,majority - baseline,of,55.2 % accuracy,majority - baseline of 55.2 % accuracy,0.5163067579269409
translation,138,10,results,best performing model,has,multi- phase fine-tuning,best performing model has multi- phase fine-tuning,0.5569009780883789
translation,138,10,results,results,has,best performing model,results has best performing model,0.5340396165847778
translation,138,138,results,reasoning -free setting,where,annotator,reasoning -free setting where annotator,0.5715447664260864
translation,138,138,results,annotator,see,conclusions,annotator see conclusions,0.6025862097740173
translation,138,138,results,single human,achieves,90.4 % accuracy,single human achieves 90.4 % accuracy,0.6583074331283569
translation,138,138,results,single human,achieves,84.2 % macro - f1,single human achieves 84.2 % macro - f1,0.655626654624939
translation,138,138,results,reasoning -free setting,has,single human,reasoning -free setting has single human,0.5803272128105164
translation,138,138,results,annotator,has,single human,annotator has single human,0.5772429704666138
translation,138,138,results,conclusions,has,single human,conclusions has single human,0.5989812612533569
translation,138,138,results,results,Under,reasoning -free setting,results Under reasoning -free setting,0.5936226844787598
translation,138,143,results,multi- phase fine-tuning,of,biobert,multi- phase fine-tuning of biobert,0.6112127304077148
translation,138,143,results,biobert,with,additional supervision,biobert with additional supervision,0.5616669058799744
translation,138,143,results,outperforms,by,large margins,outperforms by large margins,0.6253014206886292
translation,138,143,results,other baselines,by,large margins,other baselines by large margins,0.5795930624008179
translation,138,143,results,additional supervision,has,outperforms,additional supervision has outperforms,0.6334304809570312
translation,138,143,results,outperforms,has,other baselines,outperforms has other baselines,0.5879674553871155
translation,138,143,results,results,has,multi- phase fine-tuning,results has multi- phase fine-tuning,0.4961880147457123
translation,138,144,results,trend,of,majority,trend of majority,0.5761405825614929
translation,138,144,results,different training schedules,on,macro - f1,different training schedules on macro - f1,0.5587319731712341
translation,138,144,results,comparison of models,has,trend,comparison of models has trend,0.5612114667892456
translation,138,144,results,comparison of models,has,bilstm,comparison of models has bilstm,0.5851002335548401
translation,138,144,results,comparison of models,has,shallow features,comparison of models has shallow features,0.5637709498405457
translation,138,144,results,comparison of models,has,majority,comparison of models has majority,0.5466473698616028
translation,138,144,results,trend,has,bilstm,trend has bilstm,0.6649795174598694
translation,138,144,results,trend,has,shallow features,trend has shallow features,0.5903261303901672
translation,138,144,results,trend,has,majority,trend has majority,0.5601915717124939
translation,138,144,results,biobert > esim w/ bioelmo,has,bilstm,biobert > esim w/ bioelmo has bilstm,0.6890097856521606
translation,138,144,results,biobert > esim w/ bioelmo,has,shallow features,biobert > esim w/ bioelmo has shallow features,0.6451550722122192
translation,138,144,results,biobert > esim w/ bioelmo,has,majority,biobert > esim w/ bioelmo has majority,0.6368544101715088
translation,138,144,results,bilstm,has,shallow features,bilstm has shallow features,0.5756939053535461
translation,138,144,results,bilstm,has,majority,bilstm has majority,0.672778308391571
translation,138,144,results,shallow features,has,majority,shallow features has majority,0.5688751339912415
translation,138,144,results,results,has,comparison of models,results has comparison of models,0.5396395325660706
translation,138,145,results,fine-tuned biobert,better than,state- of- theart recurrent model,fine-tuned biobert better than state- of- theart recurrent model,0.707591712474823
translation,138,145,results,state- of- theart recurrent model,of,esim w/ bioelmo,state- of- theart recurrent model of esim w/ bioelmo,0.5881364345550537
translation,138,145,results,results,has,fine-tuned biobert,results has fine-tuned biobert,0.5860013365745544
translation,138,146,results,multiphase fine- tuning setting,gets,5 out of 9 modelwise best accuracy / macro- f1,multiphase fine- tuning setting gets 5 out of 9 modelwise best accuracy / macro- f1,0.5969361662864685
translation,138,146,results,comparison of training schedules,has,multiphase fine- tuning setting,comparison of training schedules has multiphase fine- tuning setting,0.5543317794799805
translation,138,146,results,results,has,comparison of training schedules,results has comparison of training schedules,0.5297563672065735
translation,138,147,results,similar results,as,majority baseline,similar results as majority baseline,0.6236594319343567
translation,138,148,results,phase i + final setting,where,some models,phase i + final setting where some models,0.6610008478164673
translation,138,148,results,significant improvements,on,accuracy and macro - f1,significant improvements on accuracy and macro - f1,0.5344797968864441
translation,138,148,results,some models,achieve,best accuracy,some models achieve best accuracy,0.5735536813735962
translation,138,148,results,results,In,phase i + final setting,results In phase i + final setting,0.5270533561706543
translation,138,150,results,improvements,observed in,phase ii + final setting,improvements observed in phase ii + final setting,0.6737787127494812
translation,138,150,results,less significant,than,phase i + final,less significant than phase i + final,0.5958443880081177
translation,138,150,results,results,has,improvements,results has improvements,0.615561842918396
translation,138,151,results,multi- phase finetuning schedule,better than,single - phase,multi- phase finetuning schedule better than single - phase,0.7427142262458801
translation,138,151,results,results,has,multi- phase finetuning schedule,results has multi- phase finetuning schedule,0.5486634969711304
translation,138,152,results,most results ( 28/40 ),are,better,most results ( 28/40 ) are better,0.5373477935791016
translation,138,152,results,performance,has,most results ( 28/40 ),performance has most results ( 28/40 ),0.5403268933296204
translation,138,152,results,results,has,additional supervision,results has additional supervision,0.5302478671073914
translation,138,156,results,pqa -a,is,imbalanced,pqa -a is imbalanced,0.6590273380279541
translation,138,156,results,imbalanced,due to,collection process,imbalanced due to collection process,0.7275451421737671
translation,138,156,results,trivial majority baseline,gets,92.76 % accuracy,trivial majority baseline gets 92.76 % accuracy,0.5574381351470947
translation,138,156,results,pqa -a,has,trivial majority baseline,pqa -a has trivial majority baseline,0.5710995197296143
translation,138,156,results,collection process,has,trivial majority baseline,collection process has trivial majority baseline,0.5558686852455139
translation,138,157,results,other models,have,better accuracy,other models have better accuracy,0.5016204714775085
translation,138,157,results,other models,have,macro - f1,other models have macro - f1,0.5476731657981873
translation,138,157,results,other models,especially,macro - f1,other models especially macro - f1,0.6781846284866333
translation,138,157,results,better accuracy,especially,macro - f1,better accuracy especially macro - f1,0.678719162940979
translation,138,157,results,macro - f1,than,majority baseline,macro - f1 than majority baseline,0.5711901187896729
translation,138,157,results,results,has,other models,results has other models,0.5536667704582214
translation,138,158,results,finetuned biobert,performs,best,finetuned biobert performs best,0.6498408317565918
translation,138,158,results,results,has,finetuned biobert,results has finetuned biobert,0.5598103404045105
translation,138,162,results,models,perform,better,models perform better,0.6155754923820496
translation,138,162,results,models,for,pqa-l,models for pqa-l,0.7054764628410339
translation,138,162,results,better,in,reasoning - free setting,better in reasoning - free setting,0.560471773147583
translation,138,162,results,better,than,reasoning -required setting,better than reasoning -required setting,0.615627110004425
translation,138,162,results,reasoning - free setting,than,reasoning -required setting,reasoning - free setting than reasoning -required setting,0.584362268447876
translation,138,162,results,reasoning -required setting,for,pqa-a,reasoning -required setting for pqa-a,0.6641667485237122
translation,138,162,results,results,has,models,results has models,0.5335168838500977
translation,139,128,ablation-analysis,hard and soft constraints,in,decoder,hard and soft constraints in decoder,0.5583034157752991
translation,139,128,ablation-analysis,hard and soft constraints,improved,extraction performance,hard and soft constraints improved extraction performance,0.7211070656776428
translation,139,128,ablation-analysis,ablation analysis,integrating,hard and soft constraints,ablation analysis integrating hard and soft constraints,0.7567979693412781
translation,139,4,baselines,baselines,has,open information extraction ( openie,baselines has open information extraction ( openie,0.5848211646080017
translation,139,154,baselines,tuple extraction,from,qa pairs,tuple extraction from qa pairs,0.5944002866744995
translation,139,154,baselines,neuralope -nie,has,"cui et al. , 2018 )","neuralope -nie has cui et al. , 2018 )",0.5611558556556702
translation,139,219,baselines,another neuron model,uses,"hole ( nickel et al. , 2016 )","another neuron model uses hole ( nickel et al. , 2016 )",0.5802770853042603
translation,139,219,baselines,"hole ( nickel et al. , 2016 )",for,relevance scoring,"hole ( nickel et al. , 2016 ) for relevance scoring",0.600259006023407
translation,139,171,experimental-setup,encoder,used,3layer bidirectional lstm,encoder used 3layer bidirectional lstm,0.546576201915741
translation,139,171,experimental-setup,encoder,used,3 - layer bidirectional lstm,encoder used 3 - layer bidirectional lstm,0.5565861463546753
translation,139,171,experimental-setup,decoder,used,3 - layer bidirectional lstm,decoder used 3 - layer bidirectional lstm,0.5561426877975464
translation,139,171,experimental-setup,experimental setup,has,encoder,experimental setup has encoder,0.5011709332466125
translation,139,172,experimental-setup,vocabulary size,of,50k,vocabulary size of 50k,0.6083453297615051
translation,139,173,experimental-setup,word embeddings,initialized with,pretrained glove embeddings,word embeddings initialized with pretrained glove embeddings,0.6663203239440918
translation,139,173,experimental-setup,experimental setup,has,word embeddings,experimental setup has word embeddings,0.4932878315448761
translation,139,174,experimental-setup,initial learning rate,of,1,initial learning rate of 1,0.5969468951225281
translation,139,174,experimental-setup,model,with,stochastic gradient descent,model with stochastic gradient descent,0.6087816953659058
translation,139,174,experimental-setup,experimental setup,optimized,model,experimental setup optimized model,0.7037535905838013
translation,139,175,experimental-setup,decay rate,of,0.7,decay rate of 0.7,0.6121354103088379
translation,139,175,experimental-setup,dropout rate,of,0.3,dropout rate of 0.3,0.5798346996307373
translation,139,175,experimental-setup,batch size,of,64,batch size of 64,0.6741159558296204
translation,139,176,experimental-setup,1 m steps,for,conciergeqa dataset,1 m steps for conciergeqa dataset,0.6010422706604004
translation,139,176,experimental-setup,100k steps,for,amazonqa dataset,100k steps for amazonqa dataset,0.5515278577804565
translation,139,176,experimental-setup,experimental setup,trained for,1 m steps,experimental setup trained for 1 m steps,0.7113796472549438
translation,139,176,experimental-setup,experimental setup,trained for,100k steps,experimental setup trained for 100k steps,0.7124843001365662
translation,139,177,experimental-setup,tesla k80 16gb gpu,for training,models,tesla k80 16gb gpu for training models,0.7359811067581177
translation,139,178,experimental-setup,ke models,for,relevance scoring,ke models for relevance scoring,0.5978378057479858
translation,139,178,experimental-setup,relevance scoring,using,our bootstrapped training dataset,relevance scoring using our bootstrapped training dataset,0.6304691433906555
translation,139,178,experimental-setup,experimental setup,trained,ke models,experimental setup trained ke models,0.6961467266082764
translation,139,10,model,extraction,as,multi-source sequence - to-sequence learning task,extraction as multi-source sequence - to-sequence learning task,0.502998411655426
translation,139,10,model,distributed representations,to generate,knowledge facts,distributed representations to generate knowledge facts,0.6289000511169434
translation,139,31,model,model,describes,neuron,model describes neuron,0.7084754109382629
translation,139,33,model,"multi-encoder , constrained - decoder framework",uses,two encoders,"multi-encoder , constrained - decoder framework uses two encoders",0.5767697691917419
translation,139,33,model,two encoders,to encode,question and answer,two encoders to encode question and answer,0.7634114027023315
translation,139,33,model,question and answer,to,internal representation,question and answer to internal representation,0.5906068086624146
translation,139,33,model,model,propose,"multi-encoder , constrained - decoder framework","model propose multi-encoder , constrained - decoder framework",0.6612052917480469
translation,139,43,model,neuron,is,novel multi-encoder constrained - decoder method,neuron is novel multi-encoder constrained - decoder method,0.5387885570526123
translation,139,43,model,model,has,neuron,model has neuron,0.5601962208747864
translation,139,44,model,vocabulary and syntax,as,hard constraints,vocabulary and syntax as hard constraints,0.5121226906776428
translation,139,44,model,prior knowledge,as,soft constraints,prior knowledge as soft constraints,0.4450280964374542
translation,139,44,model,soft constraints,in,decoder,soft constraints in decoder,0.5525753498077393
translation,139,44,model,model,incorporates,vocabulary and syntax,model incorporates vocabulary and syntax,0.6815662384033203
translation,139,66,model,decoder,decodes,vector representations,decoder decodes vector representations,0.8207079768180847
translation,139,66,model,vector representations,into,variable - length sequence,vector representations into variable - length sequence,0.5893287062644958
translation,139,66,model,variable - length sequence,corresponding to,tuple,variable - length sequence corresponding to tuple,0.6568230390548706
translation,139,66,model,model,has,decoder,model has decoder,0.6226420402526855
translation,139,187,results,neuron,achieves,higher precision,neuron achieves higher precision,0.6882153153419495
translation,139,187,results,higher precision,on,datasets,higher precision on datasets,0.5278908610343933
translation,139,187,results,results,has,neuron,results has neuron,0.42139631509780884
translation,139,192,results,taskspecific hard constraints,helps further improve,overall precision and recall,taskspecific hard constraints helps further improve overall precision and recall,0.6876911520957947
translation,139,193,results,tuples,based on,soft constraints,tuples based on soft constraints,0.6770862340927124
translation,139,193,results,tuples,improves,performance,tuples improves performance,0.7196916341781616
translation,139,193,results,soft constraints,derived from,existing kb,soft constraints derived from existing kb,0.6046000719070435
translation,139,193,results,performance,of,both methods,performance of both methods,0.5570728778839111
translation,139,193,results,performance,of,neuralope - nie,performance of neuralope - nie,0.636875331401825
translation,139,193,results,both methods,in,conciergeqa,both methods in conciergeqa,0.608394205570221
translation,139,193,results,neuralope - nie,in,amazonqa,neuralope - nie in amazonqa,0.6138516068458557
translation,139,193,results,results,Re-ranking,tuples,results Re-ranking tuples,0.6136590838432312
translation,139,195,results,neuron,discovered,"significant additional , unique tuples","neuron discovered significant additional , unique tuples",0.794948160648346
translation,139,195,results,"significant additional , unique tuples",missed by,neuralopenie,"significant additional , unique tuples missed by neuralopenie",0.8157006502151489
translation,139,195,results,neuron,has,significant relative coverage,neuron has significant relative coverage,0.5632138252258301
translation,139,195,results,results,found,neuron,results found neuron,0.5331287980079651
translation,139,196,results,slight decrease,in,performance,slight decrease in performance,0.5411154627799988
translation,139,196,results,performance,for,neuron,performance for neuron,0.6772457361221313
translation,139,196,results,neuron,after,soft constraints,neuron after soft constraints,0.667803168296814
translation,139,196,results,results,shows,slight decrease,results shows slight decrease,0.7376075983047485
translation,139,198,results,ralopenie,improved,slightly,ralopenie improved slightly,0.7644277811050415
translation,139,198,results,lower quality ke model,has,ralopenie,lower quality ke model has ralopenie,0.6422699093818665
translation,139,204,results,results,has,end -to - end extraction,results has end -to - end extraction,0.5503759384155273
translation,139,220,results,hole and transe models,achieved,same precision,hole and transe models achieved same precision,0.7376201152801514
translation,139,220,results,hole and transe models,achieving,slightly higher recall ( + 1.4 % ),hole and transe models achieving slightly higher recall ( + 1.4 % ),0.5963476896286011
translation,139,220,results,same precision,of,80.7 %,same precision of 80.7 %,0.5644257664680481
translation,139,220,results,hole,achieving,slightly higher recall ( + 1.4 % ),hole achieving slightly higher recall ( + 1.4 % ),0.6398993730545044
translation,139,220,results,results,has,hole and transe models,results has hole and transe models,0.525703489780426
translation,139,223,results,upper-bound precision,was,85.0 %,upper-bound precision was 85.0 %,0.5785760283470154
translation,139,223,results,85.0 %,on,conciergeqa,85.0 % on conciergeqa,0.5939328670501709
translation,139,223,results,results,has,upper-bound precision,results has upper-bound precision,0.5998750329017639
translation,140,102,experiments,"squad , nq , and newsqa",achieve,over 90 %,"squad , nq , and newsqa achieve over 90 %",0.6664689183235168
translation,140,102,experiments,over 90 %,of,baseline score,over 90 % of baseline score,0.5621489882469177
translation,140,102,experiments,over 90 %,showing,robustness,over 90 % showing robustness,0.7389000058174133
translation,140,102,experiments,robustness,to,reasonable level of noise,robustness to reasonable level of noise,0.5424989461898804
translation,140,118,experiments,squad and quac,get,93 %,squad and quac get 93 %,0.633392870426178
translation,140,118,experiments,squad and quac,over,93 %,squad and quac over 93 %,0.7267982363700867
translation,140,118,experiments,93 %,of,original baselines,93 % of original baselines,0.565955638885498
translation,140,118,experiments,93 %,with,randomly shuffled contexts,93 % with randomly shuffled contexts,0.6699948310852051
translation,140,86,results,our models,do not generalize,well,our models do not generalize well,0.7648125290870667
translation,140,86,results,well,to,different datasets,well to different datasets,0.5260964035987854
translation,140,86,results,results,show,our models,results show our models,0.6820906400680542
translation,140,101,results,training sets,with,10 % random labels,training sets with 10 % random labels,0.6047840714454651
translation,140,101,results,all models,see,f1 score drop,all models see f1 score drop,0.5972440242767334
translation,140,101,results,training sets,has,all models,training sets has all models,0.5753611922264099
translation,140,101,results,10 % random labels,has,all models,10 % random labels has all models,0.5482147336006165
translation,140,101,results,results,On,training sets,results On training sets,0.5477133989334106
translation,140,117,results,triviaqa,sees,largest drop,triviaqa sees largest drop,0.689899206161499
translation,140,117,results,largest drop,in,performance,largest drop in performance,0.5237601399421692
translation,140,117,results,66 %,of,baseline,66 % of baseline,0.619575560092926
translation,140,117,results,66 %,of,baseline,66 % of baseline,0.619575560092926
translation,140,117,results,66 %,followed by,newsqa,66 % followed by newsqa,0.6839683651924133
translation,140,117,results,66 %,of,baseline,66 % of baseline,0.619575560092926
translation,140,117,results,newsqa,with,80 %,newsqa with 80 %,0.6904367208480835
translation,140,117,results,80 %,of,baseline,80 % of baseline,0.641778826713562
translation,140,131,results,squad,achieves,65 %,squad achieves 65 %,0.7278067469596863
translation,140,131,results,65 %,of,baseline,65 % of baseline,0.6138011813163757
translation,140,131,results,65 %,given,question,65 % given question,0.750242292881012
translation,140,131,results,results,has,squad,results has squad,0.5870539546012878
translation,140,132,results,nq,achieves,up to 68 %,nq achieves up to 68 %,0.7288261651992798
translation,140,132,results,nq,achieves,up to 44 %,nq achieves up to 44 %,0.716667652130127
translation,140,132,results,up to 68 %,of,baseline f1 score,up to 68 % of baseline f1 score,0.5738015174865723
translation,140,132,results,up to 44 %,given,no question,up to 44 % given no question,0.6744367480278015
translation,140,132,results,results,has,nq,results has nq,0.4012843072414398
translation,140,140,results,newsqa,able to achieve,over 40 %,newsqa able to achieve over 40 %,0.6655259728431702
translation,140,140,results,over 40 %,of,baseline performance,over 40 % of baseline performance,0.5871054530143738
translation,140,140,results,baseline performance,with,ner system,baseline performance with ner system,0.5573617219924927
translation,140,140,results,results,With the exception of,newsqa,results With the exception of newsqa,0.7131201028823853
translation,140,142,results,nq and squad,achieve,33 - 35 %,nq and squad achieve 33 - 35 %,0.7029093503952026
translation,140,142,results,33 - 35 %,of,baseline,33 - 35 % of baseline,0.6197208166122437
translation,140,142,results,33 - 35 %,by only extracting,first person entity,33 - 35 % by only extracting first person entity,0.6875341534614563
translation,140,143,results,triviaqa,sees,much larger drop,triviaqa sees much larger drop,0.7124465703964233
translation,140,143,results,much larger drop,when using,only person entities,much larger drop when using only person entities,0.6114016175270081
translation,140,143,results,results,has,triviaqa,results has triviaqa,0.5624147057533264
translation,140,164,results,outperforms,by giving,original answer,outperforms by giving original answer,0.7041099071502686
translation,140,164,results,all the other models,by giving,original answer,all the other models by giving original answer,0.6379455924034119
translation,140,164,results,original answer,to,negative question,original answer to negative question,0.5558388829231262
translation,140,164,results,negative question,less than,3 % of the time,negative question less than 3 % of the time,0.6636976599693298
translation,140,164,results,squad,has,outperforms,squad has outperforms,0.6497575640678406
translation,140,164,results,outperforms,has,all the other models,outperforms has all the other models,0.5910621881484985
translation,140,164,results,results,see that,squad,results see that squad,0.7041308283805847
translation,141,90,baselines,several baselines,including,logistic regression model,several baselines including logistic regression model,0.731306254863739
translation,141,90,baselines,several baselines,including,four different neural network models,several baselines including four different neural network models,0.6857256293296814
translation,141,90,baselines,drqa,has,"chen et al. , 2017 )","drqa has chen et al. , 2017 )",0.5936939716339111
translation,141,111,baselines,seq2seq model,based on,sequence to sequence model,seq2seq model based on sequence to sequence model,0.6913321614265442
translation,141,111,baselines,seq2seq model,includes,attention model,seq2seq model includes attention model,0.5815616846084595
translation,141,111,baselines,sequence to sequence model,includes,attention model,sequence to sequence model includes attention model,0.6224702596664429
translation,141,111,baselines,baselines,has,seq2seq model,baselines has seq2seq model,0.5350337028503418
translation,141,127,baselines,baselines,has,bidaf -m bidaf ( bidirectional attention flow networks ),baselines has bidaf -m bidaf ( bidirectional attention flow networks ),0.5362126231193542
translation,141,141,baselines,baselines,has,drqa -m drqa,baselines has drqa -m drqa,0.5847485661506653
translation,141,107,experimental-setup,training,optimize,cross-entropy loss,training optimize cross-entropy loss,0.7060621380805969
translation,141,107,experimental-setup,cross-entropy loss,using,"adam ( kingma and ba , 2014 )","cross-entropy loss using adam ( kingma and ba , 2014 )",0.6134330034255981
translation,141,107,experimental-setup,"adam ( kingma and ba , 2014 )",with,initial learning rate,"adam ( kingma and ba , 2014 ) with initial learning rate",0.5918858647346497
translation,141,107,experimental-setup,initial learning rate,of,0.01,initial learning rate of 0.01,0.5797136425971985
translation,141,107,experimental-setup,experimental setup,During,training,experimental setup During training,0.6835477948188782
translation,141,108,experimental-setup,batch size,of,10,batch size of 10,0.7046762704849243
translation,141,108,experimental-setup,batch size,of,000,batch size of 000,0.6863389015197754
translation,141,108,experimental-setup,10,",",000,"10 , 000",0.6823309063911438
translation,141,108,experimental-setup,experimental setup,train with,5 epochs,experimental setup train with 5 epochs,0.7446064949035645
translation,141,109,experimental-setup,training,takes,roughly 10 minutes,training takes roughly 10 minutes,0.6591885089874268
translation,141,109,experimental-setup,roughly 10 minutes,for,each domain,roughly 10 minutes for each domain,0.6493752002716064
translation,141,109,experimental-setup,each domain,on,titan x gpu,each domain on titan x gpu,0.5245333909988403
translation,141,109,experimental-setup,experimental setup,has,training,experimental setup has training,0.5312813520431519
translation,141,115,experimental-setup,experimental setup,use,"word embeddings ( zou et al. , 2013 )","experimental setup use word embeddings ( zou et al. , 2013 )",0.5361196994781494
translation,141,116,experimental-setup,"3 gru ( cho et al. , 2014 ) connected layers",with,capacity,"3 gru ( cho et al. , 2014 ) connected layers with capacity",0.6140254735946655
translation,141,116,experimental-setup,capacity,of,256,capacity of 256,0.6581568121910095
translation,141,117,experimental-setup,batch size,set to,16,batch size set to 16,0.7520541548728943
translation,141,117,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,141,118,experimental-setup,gradient descent,with,initial learning rate,gradient descent with initial learning rate,0.5826337933540344
translation,141,118,experimental-setup,gradient descent,with,decay factor,gradient descent with decay factor,0.5778003334999084
translation,141,118,experimental-setup,gradient descent,with,000 steps ( 5 epochs ),gradient descent with 000 steps ( 5 epochs ),0.5757237672805786
translation,141,118,experimental-setup,gradient descent,iterating on,data,gradient descent iterating on data,0.7851254940032959
translation,141,118,experimental-setup,initial learning rate,of,0.5,initial learning rate of 0.5,0.6004254221916199
translation,141,118,experimental-setup,decay factor,of,0.99,decay factor of 0.99,0.5839599370956421
translation,141,118,experimental-setup,data,for,50,data for 50,0.6705218553543091
translation,141,118,experimental-setup,50,",",000 steps ( 5 epochs ),"50 , 000 steps ( 5 epochs )",0.583279550075531
translation,141,118,experimental-setup,experimental setup,use,gradient descent,experimental setup use gradient descent,0.5762200355529785
translation,141,119,experimental-setup,training process,for,each domain,training process for each domain,0.61454176902771
translation,141,119,experimental-setup,training process,took,approximately 48 hours,training process took approximately 48 hours,0.6283252835273743
translation,141,119,experimental-setup,approximately 48 hours,on,titan x gpu,approximately 48 hours on titan x gpu,0.5045173764228821
translation,141,119,experimental-setup,experimental setup,has,training process,experimental setup has training process,0.5342507362365723
translation,141,126,experimental-setup,model,for,100 epochs,model for 100 epochs,0.6005324721336365
translation,141,126,experimental-setup,each domain,on,titan x gpu,each domain on titan x gpu,0.5245333909988403
translation,141,126,experimental-setup,experimental setup,train,model,experimental setup train model,0.6514950394630432
translation,141,137,experimental-setup,50 1d filters,for,cnn character embedding,50 1d filters for cnn character embedding,0.5277439951896667
translation,141,137,experimental-setup,50 1d filters,with,width,50 1d filters with width,0.660378098487854
translation,141,137,experimental-setup,width,of,5,width of 5,0.7088584303855896
translation,141,137,experimental-setup,experimental setup,use,50 1d filters,experimental setup use 50 1d filters,0.6002519726753235
translation,141,138,experimental-setup,word embedding size,is,300,word embedding size is 300,0.6083614230155945
translation,141,138,experimental-setup,hidden dimension,for,lstms,hidden dimension for lstms,0.6163105368614197
translation,141,138,experimental-setup,hidden dimension,is,128,hidden dimension is 128,0.607699990272522
translation,141,138,experimental-setup,experimental setup,has,word embedding size,experimental setup has word embedding size,0.5138497352600098
translation,141,138,experimental-setup,experimental setup,has,hidden dimension,experimental setup has hidden dimension,0.5457288026809692
translation,141,139,experimental-setup,optimization,use,"adam ( kingma and ba , 2014 )","optimization use adam ( kingma and ba , 2014 )",0.5983548760414124
translation,141,139,experimental-setup,"adam ( kingma and ba , 2014 )",with,initial learning rate,"adam ( kingma and ba , 2014 ) with initial learning rate",0.5918858647346497
translation,141,139,experimental-setup,"adam ( kingma and ba , 2014 )",use,minibatch size,"adam ( kingma and ba , 2014 ) use minibatch size",0.5986596941947937
translation,141,139,experimental-setup,initial learning rate,of,0.001,initial learning rate of 0.001,0.5703312754631042
translation,141,139,experimental-setup,minibatch size,of,32,minibatch size of 32,0.6299940943717957
translation,141,139,experimental-setup,32,for,15 epochs,32 for 15 epochs,0.6386046409606934
translation,141,139,experimental-setup,experimental setup,For,optimization,experimental setup For optimization,0.5891668200492859
translation,141,140,experimental-setup,training process,takes,roughly 20 hours,training process takes roughly 20 hours,0.6424494385719299
translation,141,140,experimental-setup,roughly 20 hours,for,each domain,roughly 20 hours for each domain,0.638369619846344
translation,141,140,experimental-setup,each domain,on,titan x gpu,each domain on titan x gpu,0.5245333909988403
translation,141,140,experimental-setup,experimental setup,has,training process,experimental setup has training process,0.5342507362365723
translation,141,149,experimental-setup,training,takes,roughly 10 hours,training takes roughly 10 hours,0.6678021550178528
translation,141,149,experimental-setup,roughly 10 hours,for,each domain,roughly 10 hours for each domain,0.6454243659973145
translation,141,149,experimental-setup,each domain,on,titan x gpu,each domain on titan x gpu,0.5245333909988403
translation,141,149,experimental-setup,experimental setup,has,training,experimental setup has training,0.5312813520431519
translation,141,30,experiments,qa,over,text,qa over text,0.7191069722175598
translation,141,7,model,dynamic narrative,describes,entities and relations,dynamic narrative describes entities and relations,0.5868443846702576
translation,141,7,model,dynamic narrative,paired with,variably compositional questions,dynamic narrative paired with variably compositional questions,0.6366111636161804
translation,141,7,model,lightweight python- based framework,call,textworlds,lightweight python- based framework call textworlds,0.6076158881187439
translation,141,7,model,textworldsqa,has,set of five diverse datasets,textworldsqa has set of five diverse datasets,0.5362880825996399
translation,141,7,model,model,generate and release,textworldsqa,model generate and release textworldsqa,0.6136670708656311
translation,141,7,model,model,release,lightweight python- based framework,model release lightweight python- based framework,0.6601466536521912
translation,141,120,model,memn2n end-to - end memory network ( memn2n ),is,neural architecture,memn2n end-to - end memory network ( memn2n ) is neural architecture,0.5592573285102844
translation,141,120,model,neural architecture,encodes,long-term and short - term context,neural architecture encodes long-term and short - term context,0.6925069093704224
translation,141,120,model,long-term and short - term context,into,memory,long-term and short - term context into memory,0.5310351848602295
translation,141,120,model,model,has,memn2n end-to - end memory network ( memn2n ),model has memn2n end-to - end memory network ( memn2n ),0.567058801651001
translation,141,159,results,more compositional questions,are,more challenging,more compositional questions are more challenging,0.5885629057884216
translation,141,159,results,more challenging,for,most models,more challenging for most models,0.5918076634407043
translation,141,159,results,performance,with,number of relations,performance with number of relations,0.6365815997123718
translation,141,159,results,number of relations,composed in,question,number of relations composed in question,0.6680768728256226
translation,141,159,results,results,observe that,more compositional questions,results observe that more compositional questions,0.5869450569152832
translation,141,161,results,surprising observation,is that,performance,surprising observation is that performance,0.6850048899650574
translation,141,161,results,performance,on,questions,performance on questions,0.5618568658828735
translation,141,161,results,performance,on,questions,performance on questions,0.5618568658828735
translation,141,161,results,questions,ask about,single relation,questions ask about single relation,0.7385473847389221
translation,141,161,results,questions,ask about,single relation,questions ask about single relation,0.7385473847389221
translation,141,161,results,questions,ask about,single relation,questions ask about single relation,0.7385473847389221
translation,141,161,results,single answer,is,lower,single answer is lower,0.6151629090309143
translation,141,161,results,lower,than,questions,lower than questions,0.5616340041160583
translation,141,161,results,questions,ask about,single relation,questions ask about single relation,0.7385473847389221
translation,141,161,results,results,is that,performance,results is that performance,0.7117780447006226
translation,141,161,results,results,has,surprising observation,results has surprising observation,0.5666312575340271
translation,141,164,results,bidaf -m and drqa - m,perform,surprisingly well,bidaf -m and drqa - m perform surprisingly well,0.6271620392799377
translation,141,164,results,surprisingly well,in,within-world evaluation,surprisingly well in within-world evaluation,0.5391420722007751
translation,141,164,results,results,see that,bidaf -m and drqa - m,results see that bidaf -m and drqa - m,0.6750076413154602
translation,141,165,results,bidaf -m,suggests,modeling question -context interactions,bidaf -m suggests modeling question -context interactions,0.6381641030311584
translation,141,165,results,drqa -m,has,outperforms,drqa -m has outperforms,0.648637592792511
translation,141,165,results,outperforms,has,bidaf -m,outperforms has bidaf -m,0.6382836699485779
translation,141,165,results,results,has,drqa -m,results has drqa -m,0.5954141616821289
translation,141,170,results,results,has,seq2seq model,results has seq2seq model,0.5315564870834351
translation,141,172,results,better,when,training and testing,better when training and testing,0.6699288487434387
translation,141,172,results,training and testing,on,shorter stories ( limited to 30 statements ),training and testing on shorter stories ( limited to 30 statements ),0.5217623114585876
translation,141,173,results,logistic regression baseline,performs,on a par,logistic regression baseline performs on a par,0.6282251477241516
translation,141,173,results,on a par,with,memn2n,on a par with memn2n,0.6879091858863831
translation,141,173,results,large performance gap,to,bidaf -m and drqa -m,large performance gap to bidaf -m and drqa -m,0.5689884424209595
translation,141,173,results,results,has,logistic regression baseline,results has logistic regression baseline,0.51777184009552
translation,141,174,results,performance,of,all methods,performance of all methods,0.5471567511558533
translation,141,174,results,across-world setting,has,performance,across-world setting has performance,0.58710116147995
translation,141,174,results,all methods,has,dramatically decreases,all methods has dramatically decreases,0.5741097927093506
translation,141,174,results,results,In,across-world setting,results In across-world setting,0.5547269582748413
translation,142,124,ablation-analysis,our vocabulary size,reduced to,less than 6 k,our vocabulary size reduced to less than 6 k,0.7285388112068176
translation,142,124,ablation-analysis,byte-pair encoding,has,our vocabulary size,byte-pair encoding has our vocabulary size,0.5614757537841797
translation,142,124,ablation-analysis,ablation analysis,using,byte-pair encoding,ablation analysis using byte-pair encoding,0.6928657293319702
translation,142,177,ablation-analysis,copy loss mechanism,contributes to,stable 0.48 increment of bleu -4,copy loss mechanism contributes to stable 0.48 increment of bleu -4,0.6648921966552734
translation,142,180,ablation-analysis,copy loss,brings,absolute 4.53 % increment,copy loss brings absolute 4.53 % increment,0.6061869859695435
translation,142,180,ablation-analysis,absolute 4.53 % increment,on,copying,absolute 4.53 % increment on copying,0.5657677054405212
translation,142,180,ablation-analysis,words,from,source sentences,words from source sentences,0.5024463534355164
translation,142,180,ablation-analysis,model,generate,higher quality questions,model generate higher quality questions,0.7097877264022827
translation,142,180,ablation-analysis,copying,has,words,copying has words,0.6221392154693604
translation,142,180,ablation-analysis,ablation analysis,has,copy loss,ablation analysis has copy loss,0.5262378454208374
translation,142,148,baselines,baselines,has,seq2seqatt ( du ),baselines has seq2seqatt ( du ),0.5733991265296936
translation,142,159,baselines,baseline model,is,general sequence - to-sequence attention model,baseline model is general sequence - to-sequence attention model,0.4706094264984131
translation,142,159,baselines,general sequence - to-sequence attention model,enhanced with,copy mechanism,general sequence - to-sequence attention model enhanced with copy mechanism,0.6964214444160461
translation,142,159,baselines,baselines,has,baseline model,baselines has baseline model,0.5873855352401733
translation,142,160,baselines,baseline + type,adds,question type module,baseline + type adds question type module,0.6407368779182434
translation,142,160,baselines,question type module,to,baseline model,question type module to baseline model,0.5294060707092285
translation,142,160,baselines,baselines,has,baseline + type,baselines has baseline + type,0.6003222465515137
translation,142,161,baselines,baseline + copyloss,calculates and minimizes,additional copy loss,baseline + copyloss calculates and minimizes additional copy loss,0.6368623971939087
translation,142,161,baselines,baselines,has,baseline + copyloss,baselines has baseline + copyloss,0.5758525729179382
translation,142,162,baselines,baseline + copyloss + type,is,full version,baseline + copyloss + type is full version,0.529863715171814
translation,142,162,baselines,baselines,has,baseline + copyloss + type,baselines has baseline + copyloss + type,0.5679211020469666
translation,142,45,experiments,zhao,uses,paragraph information,zhao uses paragraph information,0.6067728400230408
translation,142,45,experiments,paragraph information,to do,answer - aware qg task,paragraph information to do answer - aware qg task,0.3852313756942749
translation,142,125,hyperparameters,256,for,maximum length,256 for maximum length,0.5990392565727234
translation,142,125,hyperparameters,maximum length,of,inputs,maximum length of inputs,0.5998622179031372
translation,142,125,hyperparameters,50,for,maximum length,50 for maximum length,0.6132888197898865
translation,142,125,hyperparameters,maximum length,of,target questions,maximum length of target questions,0.5941621661186218
translation,142,125,hyperparameters,hyperparameters,choose,256,hyperparameters choose 256,0.702161967754364
translation,142,125,hyperparameters,hyperparameters,choose,50,hyperparameters choose 50,0.7087876796722412
translation,142,128,hyperparameters,hidden units,is,600,hidden units is 600,0.6097846031188965
translation,142,128,hyperparameters,dimension,of,word embedding and question type embedding,dimension of word embedding and question type embedding,0.5711681246757507
translation,142,128,hyperparameters,dimension,both,word embedding and question type embedding,dimension both word embedding and question type embedding,0.6792308688163757
translation,142,128,hyperparameters,dimension,is,300,dimension is 300,0.659263551235199
translation,142,128,hyperparameters,word embedding and question type embedding,is,300,word embedding and question type embedding is 300,0.5627593398094177
translation,142,128,hyperparameters,hyperparameters,has,dimension,hyperparameters has dimension,0.514893651008606
translation,142,130,hyperparameters,drop rate,between,each layer,drop rate between each layer,0.6177161335945129
translation,142,130,hyperparameters,each layer,is,0.3,each layer is 0.3,0.5586317181587219
translation,142,130,hyperparameters,hyperparameters,has,drop rate,hyperparameters has drop rate,0.5420039892196655
translation,142,131,hyperparameters,"adam ( kingma and ba , 2014 )",with,learning rate,"adam ( kingma and ba , 2014 ) with learning rate",0.5989180207252502
translation,142,131,hyperparameters,"adam ( kingma and ba , 2014 )",after training,5 epochs,"adam ( kingma and ba , 2014 ) after training 5 epochs",0.743960440158844
translation,142,131,hyperparameters,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,142,131,hyperparameters,learning rate,of,0.01,learning rate of 0.01,0.6152973175048828
translation,142,131,hyperparameters,learning rate,of,0.01,learning rate of 0.01,0.6152973175048828
translation,142,131,hyperparameters,0.001,for,fast training,0.001 for fast training,0.5805510878562927
translation,142,131,hyperparameters,stochastic gradient descent ( sgd ),with,learning rate,stochastic gradient descent ( sgd ) with learning rate,0.6208106875419617
translation,142,131,hyperparameters,learning rate,of,0.01,learning rate of 0.01,0.6152973175048828
translation,142,131,hyperparameters,learning rate,used for,fine-tuning,learning rate used for fine-tuning,0.6505192518234253
translation,142,131,hyperparameters,0.01,used for,fine-tuning,0.01 used for fine-tuning,0.6308296918869019
translation,142,131,hyperparameters,hyperparameters,firstly use,"adam ( kingma and ba , 2014 )","hyperparameters firstly use adam ( kingma and ba , 2014 )",0.6784622669219971
translation,142,132,hyperparameters,model,for,15 epochs,model for 15 epochs,0.6331382989883423
translation,142,132,hyperparameters,model,with,mini-batch size,model with mini-batch size,0.6544046998023987
translation,142,132,hyperparameters,15 epochs,with,mini-batch size,15 epochs with mini-batch size,0.6006804704666138
translation,142,132,hyperparameters,mini-batch size,of,64,mini-batch size of 64,0.6429709196090698
translation,142,132,hyperparameters,hyperparameters,train,model,hyperparameters train model,0.7064553499221802
translation,142,133,hyperparameters,hyperparameter k,set to,1,hyperparameter k set to 1,0.6930251121520996
translation,142,133,hyperparameters,hyperparameter k,when,decoding,hyperparameter k when decoding,0.6913781762123108
translation,142,133,hyperparameters,decoding,do,beam search,decoding do beam search,0.47736579179763794
translation,142,133,hyperparameters,beam search,with,beam size,beam search with beam size,0.6320963501930237
translation,142,133,hyperparameters,beam size,of,4,beam size of 4,0.6962505578994751
translation,142,133,hyperparameters,training,has,hyperparameter k,training has hyperparameter k,0.5264900922775269
translation,142,133,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
translation,142,5,model,model,propose,two new strategies,model propose two new strategies,0.7009268999099731
translation,142,6,model,question type module,predict,types of questions,question type module predict types of questions,0.6853302717208862
translation,142,6,model,multiple types of questions,for,same source sentence,multiple types of questions for same source sentence,0.6081570982933044
translation,142,6,model,model,has,question type module,model has question type module,0.5822161436080933
translation,142,7,model,new copy loss,enhances,original copy mechanism,new copy loss enhances original copy mechanism,0.6338126063346863
translation,142,7,model,original copy mechanism,to make sure,every important word,original copy mechanism to make sure every important word,0.7813079357147217
translation,142,7,model,every important word,in,source sentence,every important word in source sentence,0.4787864089012146
translation,142,7,model,been copied,generating,questions,been copied generating questions,0.7064990997314453
translation,142,7,model,model,has,new copy loss,model has new copy loss,0.5442323684692383
translation,142,27,model,question type driven framework,for,ag - qg task,question type driven framework for ag - qg task,0.624722957611084
translation,142,27,model,model,propose,question type driven framework,model propose question type driven framework,0.7155748605728149
translation,142,44,model,zhou,leverage,lexical features ( part - of-speech and named entity tags ),zhou leverage lexical features ( part - of-speech and named entity tags ),0.7295573949813843
translation,142,44,model,zhou,to help,model,zhou to help model,0.6518502831459045
translation,142,44,model,lexical features ( part - of-speech and named entity tags ),to help,model,lexical features ( part - of-speech and named entity tags ) to help model,0.581217885017395
translation,142,44,model,model,get,better encoder representation,model get better encoder representation,0.582455039024353
translation,142,44,model,model,has,zhou,model has zhou,0.6107999086380005
translation,142,127,model,2 - layers bi-directional lstm,for,encoding,2 - layers bi-directional lstm for encoding,0.5806834697723389
translation,142,127,model,1 - layer lstm,for,decoding,1 - layer lstm for decoding,0.5562506318092346
translation,142,127,model,model,adopt,2 - layers bi-directional lstm,model adopt 2 - layers bi-directional lstm,0.6016820073127747
translation,142,127,model,model,adopt,1 - layer lstm,model adopt 1 - layer lstm,0.5982956886291504
translation,142,8,results,outperforms,achieving,bleu - 4 score,outperforms achieving bleu - 4 score,0.6213818788528442
translation,142,8,results,state- of - theart approach,in,answer -agnostic question generation,state- of - theart approach in answer -agnostic question generation,0.5157018899917603
translation,142,8,results,state- of - theart approach,achieving,bleu - 4 score,state- of - theart approach achieving bleu - 4 score,0.6149423718452454
translation,142,8,results,bleu - 4 score,of,13.9,bleu - 4 score of 13.9,0.5540771484375
translation,142,8,results,bleu - 4 score,on,squad,bleu - 4 score on squad,0.5416741967201233
translation,142,8,results,integrated model,has,outperforms,integrated model has outperforms,0.6230608820915222
translation,142,8,results,outperforms,has,state- of - theart approach,outperforms has state- of - theart approach,0.5920459032058716
translation,142,8,results,results,has,integrated model,results has integrated model,0.5474998950958252
translation,142,35,results,question type module and the new copy loss,improve,performance,question type module and the new copy loss improve performance,0.6595999598503113
translation,142,35,results,performance,over,baseline model,performance over baseline model,0.7133670449256897
translation,142,35,results,our full model,combining,two modules,our full model combining two modules,0.7184459567070007
translation,142,35,results,new state - of - the - art performance,with,bleu - 4,new state - of - the - art performance with bleu - 4,0.6561039686203003
translation,142,35,results,bleu - 4,of,13.9,bleu - 4 of 13.9,0.5945678353309631
translation,142,35,results,results,Both,question type module and the new copy loss,results Both question type module and the new copy loss,0.6827341318130493
translation,142,35,results,results,has,question type module and the new copy loss,results has question type module and the new copy loss,0.5661483407020569
translation,142,39,results,our model,achieves,new state - of- the - art performance,our model achieves new state - of- the - art performance,0.6508355140686035
translation,142,39,results,new state - of- the - art performance,for,challenging ag - qg,new state - of- the - art performance for challenging ag - qg,0.6271580457687378
translation,142,39,results,results,has,our model,results has our model,0.5871725678443909
translation,142,154,results,full version,of,our model,full version of our model,0.5828294157981873
translation,142,154,results,our model,uses,question type module and copy loss mechanism,our model uses question type module and copy loss mechanism,0.613332211971283
translation,142,154,results,our model,obtains,best results,our model obtains best results,0.6301198601722717
translation,142,154,results,best results,on,all of metrics,best results on all of metrics,0.49130895733833313
translation,142,154,results,best results,achieving,new state - of - the - art result,best results achieving new state - of - the - art result,0.6299459338188171
translation,142,154,results,new state - of - the - art result,of,bleu -4 13.90,new state - of - the - art result of bleu -4 13.90,0.5044815540313721
translation,142,154,results,results,has,full version,results has full version,0.48959586024284363
translation,142,155,results,outperforms,beats,previous best result,outperforms beats previous best result,0.7222993969917297
translation,142,155,results,baseline model,with,0.73 points,baseline model with 0.73 points,0.5472888350486755
translation,142,155,results,previous best result,by,0.67 points,previous best result by 0.67 points,0.5547281503677368
translation,142,155,results,outperforms,has,baseline model,outperforms has baseline model,0.5998825430870056
translation,142,155,results,results,has,outperforms,results has outperforms,0.6657275557518005
translation,142,168,results,question type module,brings,slight performance gain,question type module brings slight performance gain,0.6047470569610596
translation,142,168,results,baseline model,has,question type module,baseline model has question type module,0.5318072438240051
translation,142,168,results,results,Comparing with,baseline model,results Comparing with baseline model,0.6776164174079895
translation,142,172,results,predict part,achieves,69 % accuracy,predict part achieves 69 % accuracy,0.7136241793632507
translation,142,172,results,predict part,overall,69 % accuracy,predict part overall 69 % accuracy,0.7676467299461365
translation,142,172,results,question type,has,predict part,question type has predict part,0.6202146410942078
translation,142,172,results,results,has,question type,results has question type,0.49611207842826843
translation,142,187,results,consistency,among,three annotators,consistency among three annotators,0.5289265513420105
translation,142,187,results,consistency,is,satisfying,consistency is satisfying,0.6220767498016357
translation,142,187,results,our generated questions,are,better,our generated questions are better,0.6276357769966125
translation,142,187,results,better,from,different perspectives,better from different perspectives,0.6034131050109863
translation,142,187,results,results,shows that,consistency,results shows that consistency,0.596806526184082
translation,142,188,results,further two -tailed t-test,proves that,our generated questions,further two -tailed t-test proves that our generated questions,0.6041795611381531
translation,142,188,results,our generated questions,better than,baseline model,our generated questions better than baseline model,0.7051569223403931
translation,142,188,results,baseline model,has,significantly,baseline model has significantly,0.5976307392120361
translation,143,97,results,questionstory pairs,with,false beliefs,questionstory pairs with false beliefs,0.6175806522369385
translation,143,97,results,all models,fail to provide,correct answers,all models fail to provide correct answers,0.6961184144020081
translation,143,97,results,questionstory pairs,has,all models,questionstory pairs has all models,0.6123704314231873
translation,143,97,results,false beliefs,has,all models,false beliefs has all models,0.555885910987854
translation,143,97,results,correct answers,has,consistently,correct answers has consistently,0.5805008411407471
translation,143,97,results,results,for,questionstory pairs,results for questionstory pairs,0.6156612038612366
translation,143,98,results,recurrent entity networks,performing,best,recurrent entity networks performing best,0.6508017182350159
translation,143,98,results,best,on,false beliefs tasks,best on false beliefs tasks,0.5293628573417664
translation,143,98,results,results,has,recurrent entity networks,results has recurrent entity networks,0.5393573641777039
translation,144,168,ablation-analysis,lmh,leads to,drop,lmh leads to drop,0.7511583566665649
translation,144,168,ablation-analysis,drop,in,performance,drop in performance,0.5523719787597656
translation,144,168,ablation-analysis,performance,when used in combination with,mutant,performance when used in combination with mutant,0.6245229840278625
translation,144,192,ablation-analysis,new state - of - the - art accuracy,on,vqa - cp - v2 dataset,new state - of - the - art accuracy on vqa - cp - v2 dataset,0.5264368653297424
translation,144,192,ablation-analysis,gap,between,model performance,gap between model performance,0.6565134525299072
translation,144,192,ablation-analysis,model performance,on,vqa - v2 data,model performance on vqa - v2 data,0.5662383437156677
translation,144,130,baselines,scr,has,"wu and mooney , 2019 )","scr has wu and mooney , 2019 )",0.5832834243774414
translation,144,130,baselines,css,has,"chen et al. , 2020a )","css has chen et al. , 2020a )",0.5686627626419067
translation,144,166,baselines,lmh,implements,learned mixing strategy,lmh implements learned mixing strategy,0.6702712774276733
translation,144,166,baselines,learned mixing strategy,by using,main model,learned mixing strategy by using main model,0.6917378306388855
translation,144,166,baselines,main model,in combination with,biasonly model,main model in combination with biasonly model,0.5277993083000183
translation,144,166,baselines,biasonly model,trained only with,question,biasonly model trained only with question,0.7013213038444519
translation,144,166,baselines,question,without,image,question without image,0.7416902184486389
translation,144,166,baselines,baselines,has,lmh,baselines has lmh,0.5794985294342041
translation,144,127,experimental-setup,models,trained on,two nvidia tesla v100 16gb gpus,models trained on two nvidia tesla v100 16gb gpus,0.729537844657898
translation,144,127,experimental-setup,two nvidia tesla v100 16gb gpus,for,10 epochs,two nvidia tesla v100 16gb gpus for 10 epochs,0.5768591165542603
translation,144,127,experimental-setup,two nvidia tesla v100 16gb gpus,with,batch size,two nvidia tesla v100 16gb gpus with batch size,0.6200944185256958
translation,144,127,experimental-setup,two nvidia tesla v100 16gb gpus,with,learning rate,two nvidia tesla v100 16gb gpus with learning rate,0.6225054860115051
translation,144,127,experimental-setup,10 epochs,with,batch size,10 epochs with batch size,0.6104865074157715
translation,144,127,experimental-setup,10 epochs,with,learning rate,10 epochs with learning rate,0.6625245213508606
translation,144,127,experimental-setup,batch size,of,32,batch size of 32,0.6741614937782288
translation,144,127,experimental-setup,learning rate,has,1e- 5,learning rate has 1e- 5,0.5607917308807373
translation,144,127,experimental-setup,experimental setup,trained on,two nvidia tesla v100 16gb gpus,experimental setup trained on two nvidia tesla v100 16gb gpus,0.6967218518257141
translation,144,127,experimental-setup,experimental setup,has,models,experimental setup has models,0.5060054659843445
translation,144,128,experimental-setup,each epoch,takes,approximately three hours,each epoch takes approximately three hours,0.6869980096817017
translation,144,128,experimental-setup,each epoch,takes,four hours,each epoch takes four hours,0.6682050228118896
translation,144,128,experimental-setup,approximately three hours,for,updn,approximately three hours for updn,0.7281814813613892
translation,144,128,experimental-setup,four hours,for,lxmert,four hours for lxmert,0.7019441723823547
translation,144,128,experimental-setup,experimental setup,has,each epoch,experimental setup has each epoch,0.5484756827354431
translation,144,142,experiments,our method,achieves,best performance,our method achieves best performance,0.6578624248504639
translation,144,142,experiments,best performance,amongst,methods,best performance amongst methods,0.6273886561393738
translation,144,142,experiments,best performance,with,accuracy,best performance with accuracy,0.6443129181861877
translation,144,142,experiments,methods,designed specifically for,ood generalization,methods designed specifically for ood generalization,0.6881861090660095
translation,144,142,experiments,accuracy,of,70.24 %,accuracy of 70.24 %,0.5556480884552002
translation,144,142,experiments,vqa - v2 dataset,has,our method,vqa - v2 dataset has our method,0.5695447325706482
translation,144,6,model,"perceptually similar , yet semantically distinct mutations",of,input,"perceptually similar , yet semantically distinct mutations of input",0.5549741983413696
translation,144,6,model,"perceptually similar , yet semantically distinct mutations",to improve,ood generalization,"perceptually similar , yet semantically distinct mutations to improve ood generalization",0.6701918840408325
translation,144,6,model,model,present,mutant,model present mutant,0.7099771499633789
translation,144,7,model,consistency - constrained training objective,to understand,effect of semantic changes,consistency - constrained training objective to understand effect of semantic changes,0.6296353340148926
translation,144,7,model,effect of semantic changes,in,input ( question - image pair ),effect of semantic changes in input ( question - image pair ),0.5186331272125244
translation,144,7,model,effect of semantic changes,on,output ( answer ),effect of semantic changes on output ( answer ),0.544791579246521
translation,144,7,model,input ( question - image pair ),on,output ( answer ),input ( question - image pair ) on output ( answer ),0.5593392848968506
translation,144,7,model,model,utilize,consistency - constrained training objective,model utilize consistency - constrained training objective,0.5885734558105469
translation,144,31,model,mutation,of,inputs ( questions and images ),mutation of inputs ( questions and images ),0.5618910193443298
translation,144,31,model,mutation,to expose,vqa model,mutation to expose vqa model,0.7284134030342102
translation,144,31,model,inputs ( questions and images ),to expose,vqa model,inputs ( questions and images ) to expose vqa model,0.6766115427017212
translation,144,31,model,vqa model,to,perceptually similar yet semantically dissimilar samples,vqa model to perceptually similar yet semantically dissimilar samples,0.5555497407913208
translation,144,31,model,model,to enable,mutation,model to enable mutation,0.7833591103553772
translation,144,42,model,projection layer,projects,cross-modal features and true answers,projection layer projects cross-modal features and true answers,0.6918958425521851
translation,144,42,model,projection layer,to,learned manifold,projection layer to learned manifold,0.5143031477928162
translation,144,42,model,projection layer,uses,noise -contrastive estimation loss,projection layer uses noise -contrastive estimation loss,0.5349419116973877
translation,144,42,model,cross-modal features and true answers,to,learned manifold,cross-modal features and true answers to learned manifold,0.49268484115600586
translation,144,42,model,noise -contrastive estimation loss,for minimizing,distance,noise -contrastive estimation loss for minimizing distance,0.7078909873962402
translation,144,42,model,distance,between,two vectors,distance between two vectors,0.6957018375396729
translation,144,42,model,model,includes,projection layer,model includes projection layer,0.6489440202713013
translation,144,42,model,model,uses,noise -contrastive estimation loss,model uses noise -contrastive estimation loss,0.5875964760780334
translation,144,189,model,method,uses,input mutations,method uses input mutations,0.6459217667579651
translation,144,189,model,input mutations,to train,vqa models,input mutations to train vqa models,0.7033933401107788
translation,144,189,model,vqa models,goal of,out-of- distribution generalization,vqa models goal of out-of- distribution generalization,0.5927277207374573
translation,144,190,model,novel answer projection module,trained for,minimizing distance,novel answer projection module trained for minimizing distance,0.7137255072593689
translation,144,190,model,minimizing distance,between,answer and input projections,minimizing distance between answer and input projections,0.5988637208938599
translation,144,190,model,answer and input projections,complements,canonical vqa classification task,answer and input projections complements canonical vqa classification task,0.6703726649284363
translation,144,190,model,model,has,novel answer projection module,model has novel answer projection module,0.5824301838874817
translation,144,191,model,our type exposure model,allows,our network,our type exposure model allows our network,0.6778222322463989
translation,144,191,model,our network,to consider,all valid answers,our network to consider all valid answers,0.671992838382721
translation,144,191,model,all valid answers,as,equally probable answer candidates,all valid answers as equally probable answer candidates,0.4941370189189911
translation,144,191,model,all valid answers,has,per question type,all valid answers has per question type,0.5874671339988708
translation,144,191,model,model,has,our type exposure model,model has our type exposure model,0.5640609860420227
translation,144,9,results,mutant,establishes,new state- ofthe - art accuracy,mutant establishes new state- ofthe - art accuracy,0.6574794054031372
translation,144,9,results,new state- ofthe - art accuracy,on,vqa - cp,new state- ofthe - art accuracy on vqa - cp,0.5532271862030029
translation,144,9,results,vqa - cp,with,10.57 % improvement,vqa - cp with 10.57 % improvement,0.6350101232528687
translation,144,9,results,results,has,mutant,results has mutant,0.5017001628875732
translation,144,43,results,new state - of- the - art accuracy,of,69.52 %,new state - of- the - art accuracy of 69.52 %,0.5131215453147888
translation,144,43,results,69.52 %,on,vqa - cp - v2 benchmark,69.52 % on vqa - cp - v2 benchmark,0.5529475808143616
translation,144,43,results,current best models,by,10.57 %,current best models by 10.57 %,0.547135055065155
translation,144,43,results,outperforming,has,current best models,outperforming has current best models,0.5733166933059692
translation,144,43,results,results,establish,new state - of- the - art accuracy,results establish new state - of- the - art accuracy,0.5933558940887451
translation,144,48,results,our method,on,vqa - cp dataset,our method on vqa - cp dataset,0.4899652302265167
translation,144,48,results,new state - of- theart,of,69.52 %,new state - of- theart of 69.52 %,0.5326694846153259
translation,144,48,results,improvement,of,10.57 %,improvement of 10.57 %,0.563715398311615
translation,144,139,results,debiasing method,improves on,two sota models,debiasing method improves on two sota models,0.7402268052101135
translation,144,139,results,outperforms,has,all of the above baselines,outperforms has all of the above baselines,0.5881306529045105
translation,144,139,results,results,show,debiasing method,results show debiasing method,0.6136162281036377
translation,144,148,results,overall accuracy,of,67.63 %,overall accuracy of 67.63 %,0.5277109146118164
translation,144,148,results,67.63 %,with,88.56 %,67.63 % with 88.56 %,0.6395676136016846
translation,144,148,results,67.63 %,with,50.76 %,67.63 % with 50.76 %,0.6475476622581482
translation,144,148,results,67.63 %,with,54.56 %,67.63 % with 54.56 %,0.638401448726654
translation,144,148,results,88.56 %,on,yes - no questions,88.56 % on yes - no questions,0.4772256016731262
translation,144,148,results,50.76 %,on,number- based questions,50.76 % on number- based questions,0.4888974130153656
translation,144,148,results,54.56 %,on,other questions,54.56 % on other questions,0.4837135970592499
translation,144,149,results,all existing vqa - cp models,explicitly trained on,vqa - v2,all existing vqa - cp models explicitly trained on vqa - v2,0.7759481072425842
translation,144,151,results,results,has,effect of training with mutant samples,results has effect of training with mutant samples,0.5635757446289062
translation,144,153,results,both models,improve,updn,both models improve updn,0.7125055193901062
translation,144,153,results,updn,by,10.42 %,updn by 10.42 %,0.5935432314872742
translation,144,153,results,lxmert,by,13.46 %,lxmert by 13.46 %,0.5899679660797119
translation,144,154,results,markedly significant jump,in,performance,markedly significant jump in performance,0.5092559456825256
translation,144,154,results,markedly significant jump,for,yes -no and number categories,markedly significant jump for yes -no and number categories,0.64457768201828
translation,144,154,results,performance,for,both models,performance for both models,0.6045624017715454
translation,144,155,results,updn,benefits from,mutant samples,updn benefits from mutant samples,0.6924219131469727
translation,144,155,results,mutant samples,in terms of,accuracy,mutant samples in terms of accuracy,0.684457540512085
translation,144,155,results,accuracy,on,numeric questions,accuracy on numeric questions,0.5167070627212524
translation,144,155,results,boost,of,23.94 %,boost of 23.94 %,0.5974482893943787
translation,144,155,results,results,has,updn,results has updn,0.6217653751373291
translation,144,156,results,trained,only with,image mutations,trained only with image mutations,0.65923011302948
translation,144,156,results,trained,only with,question mutations,trained only with question mutations,0.7132845520973206
translation,144,156,results,trained,only with,question mutations,trained only with question mutations,0.7132845520973206
translation,144,160,results,performance,on,numeric questions,performance on numeric questions,0.517618715763092
translation,144,160,results,performance,on,yes -no questions,performance on yes -no questions,0.5410202741622925
translation,144,160,results,pairwise consistency loss,has,significantly boosts,pairwise consistency loss has significantly boosts,0.5675863027572632
translation,144,160,results,significantly boosts,has,performance,significantly boosts has performance,0.5811278820037842
translation,144,160,results,results,observe,pairwise consistency loss,results observe pairwise consistency loss,0.5687074065208435
translation,145,30,experiments,"stagg ( yih et al. , 2015 )",as,base question answering system,"stagg ( yih et al. , 2015 ) as base question answering system",0.5052863955497742
translation,145,87,hyperparameters,word embeddings,initialized with,"pretrained glove ( pennington et al. , 2014 ) vectors","word embeddings initialized with pretrained glove ( pennington et al. , 2014 ) vectors",0.711143434047699
translation,145,87,hyperparameters,word embeddings,updated during,training,word embeddings updated during training,0.6712310910224915
translation,145,87,hyperparameters,hyperparameters,has,word embeddings,hyperparameters has word embeddings,0.4784160554409027
translation,145,88,hyperparameters,size,of,lstm hidden layer,size of lstm hidden layer,0.551964521408081
translation,145,88,hyperparameters,values,in,"{ 50 , 100 , 200 , 300 }","values in { 50 , 100 , 200 , 300 }",0.5718074440956116
translation,145,88,hyperparameters,lstm hidden layer,has,equal,lstm hidden layer has equal,0.579875111579895
translation,145,88,hyperparameters,hyperparameters,take,dimension,hyperparameters take dimension,0.6320171356201172
translation,145,88,hyperparameters,hyperparameters,take,size,hyperparameters take size,0.6341296434402466
translation,145,88,hyperparameters,hyperparameters,experiment with,values,hyperparameters experiment with values,0.6523897051811218
translation,145,89,hyperparameters,dropout regularization,on,both input and output,dropout regularization on both input and output,0.5502604246139526
translation,145,89,hyperparameters,both input and output,of,lstm encoder,both input and output of lstm encoder,0.579936683177948
translation,145,89,hyperparameters,both input and output,with,probability 0.5,both input and output with probability 0.5,0.657111406326294
translation,145,89,hyperparameters,lstm encoder,with,probability 0.5,lstm encoder with probability 0.5,0.6528475880622864
translation,145,89,hyperparameters,hyperparameters,apply,dropout regularization,hyperparameters apply dropout regularization,0.5069227814674377
translation,145,90,hyperparameters,hyperparameters,hand tuned,penalty margin scalar,hyperparameters hand tuned penalty margin scalar,0.6786654591560364
translation,145,91,hyperparameters,model parameters,optimized using,"adam ( kingma and ba , 2015 )","model parameters optimized using adam ( kingma and ba , 2015 )",0.6673375368118286
translation,145,91,hyperparameters,"adam ( kingma and ba , 2015 )",with,batch size,"adam ( kingma and ba , 2015 ) with batch size",0.6160398721694946
translation,145,91,hyperparameters,batch size,of,32,batch size of 32,0.6741614937782288
translation,145,91,hyperparameters,hyperparameters,has,model parameters,hyperparameters has model parameters,0.45928311347961426
translation,145,92,hyperparameters,our models,in,tensorflow,our models in tensorflow,0.5400971174240112
translation,145,92,hyperparameters,hyperparameters,implemented,our models,hyperparameters implemented our models,0.712181806564331
translation,145,8,model,scoring mechanism,over,revised question encodings,scoring mechanism over revised question encodings,0.6873575448989868
translation,145,8,model,scoring mechanism,to refine,predictions,scoring mechanism to refine predictions,0.7655705809593201
translation,145,8,model,revised question encodings,to refine,predictions,revised question encodings to refine predictions,0.7160923480987549
translation,145,8,model,predictions,of,base qa system,predictions of base qa system,0.591019332408905
translation,145,8,model,model,develop,scoring mechanism,model develop scoring mechanism,0.6336643695831299
translation,145,28,model,question revisions,using,bidirectional lstm,question revisions using bidirectional lstm,0.6898025274276733
translation,145,28,model,model,encode,question revisions,model encode question revisions,0.7285577654838562
translation,145,31,results,our approach,able to,improve,our approach able to improve,0.7076438069343567
translation,145,31,results,f 1 performance,of,"stagg ( yih et al. , 2015 )","f 1 performance of stagg ( yih et al. , 2015 )",0.5298589468002319
translation,145,31,results,"stagg ( yih et al. , 2015 )",from,52.5 % to 53.9 %,"stagg ( yih et al. , 2015 ) from 52.5 % to 53.9 %",0.5232722163200378
translation,145,31,results,52.5 % to 53.9 %,on,"benchmark dataset webquestions ( berant et al. , 2013 )","52.5 % to 53.9 % on benchmark dataset webquestions ( berant et al. , 2013 )",0.49616575241088867
translation,145,31,results,improve,has,f 1 performance,improve has f 1 performance,0.5724664330482483
translation,145,31,results,results,has,our approach,results has our approach,0.6050099730491638
translation,145,101,results,our approach,improves,performance,our approach improves performance,0.6848096251487732
translation,145,101,results,our approach,achieves,53.9 %,our approach achieves 53.9 %,0.623049795627594
translation,145,101,results,performance,of,strong base qa system,performance of strong base qa system,0.6247552633285522
translation,145,101,results,strong base qa system,by,1.4 %,strong base qa system by 1.4 %,0.5746212005615234
translation,145,101,results,results,has,our approach,results has our approach,0.6050099730491638
translation,145,105,results,improvements,of,quesrev,improvements of quesrev,0.6845112442970276
translation,145,105,results,improvements,over,stagg and stagg - rank,improvements over stagg and stagg - rank,0.7360717058181763
translation,145,105,results,quesrev,over,stagg and stagg - rank,quesrev over stagg and stagg - rank,0.7374415397644043
translation,145,105,results,stagg and stagg - rank,are,statistically significant,stagg and stagg - rank are statistically significant,0.6240400075912476
translation,145,105,results,results,has,improvements,results has improvements,0.615561842918396
translation,145,107,results,ac model,yields to,best refinement results,ac model yields to best refinement results,0.6568523049354553
translation,145,107,results,best refinement results,trained only on,webques - tions data,best refinement results trained only on webques - tions data,0.7464563250541687
translation,145,107,results,results,observe,ac model,results observe ac model,0.6178058981895447
translation,145,110,results,successful prediction refinements,on,stagg,successful prediction refinements on stagg,0.5723185539245605
translation,145,110,results,question revision approach,has,consistently outperforms,question revision approach has consistently outperforms,0.6304280161857605
translation,146,122,ablation-analysis,number of top -n passages,gains,even more importance,number of top -n passages gains even more importance,0.7206074595451355
translation,146,122,ablation-analysis,paragraph - level information retrieval,has,number of top -n passages,paragraph - level information retrieval has number of top -n passages,0.5435850024223328
translation,146,122,ablation-analysis,ablation analysis,owed to,paragraph - level information retrieval,ablation analysis owed to paragraph - level information retrieval,0.48312804102897644
translation,146,93,hyperparameters,threshold - based model,set,? = 15,threshold - based model set ? = 15,0.7253031730651855
translation,146,93,hyperparameters,threshold - based model,set,confidence threshold,threshold - based model set confidence threshold,0.6521601676940918
translation,146,93,hyperparameters,confidence threshold,to,? = 0.75,confidence threshold to ? = 0.75,0.5755380392074585
translation,146,93,hyperparameters,hyperparameters,For,threshold - based model,hyperparameters For threshold - based model,0.5982872247695923
translation,146,11,model,machine comprehension module,extracts,final answer,machine comprehension module extracts final answer,0.6204870343208313
translation,146,11,model,final answer,from,previously -selected documents,final answer from previously -selected documents,0.5245904326438904
translation,146,100,results,adaptive scheme,yields,best-performing system,adaptive scheme yields best-performing system,0.7154929041862488
translation,146,100,results,best-performing system,for,three out of four datasets,best-performing system for three out of four datasets,0.5587906241416931
translation,146,100,results,results,effectiveness of,adaptive scheme,results effectiveness of adaptive scheme,0.6781570315361023
translation,146,107,results,top - 1 system,reaches,slightly higher rate,top - 1 system reaches slightly higher rate,0.7072657346725464
translation,146,107,results,top - 1 system,ranked,last,top - 1 system ranked last,0.7884676456451416
translation,146,107,results,slightly higher rate,of,exact matches,slightly higher rate of exact matches,0.5904747843742371
translation,146,107,results,exact matches,for,small corpus sizes,exact matches for small corpus sizes,0.5705248117446899
translation,146,107,results,last,when considering,complete corpus,last when considering complete corpus,0.7112859487533569
translation,146,107,results,results,has,top - 1 system,results has top - 1 system,0.5775223970413208
translation,146,109,results,top - 10 system,accomplishes,best performance,top - 10 system accomplishes best performance,0.6323370933532715
translation,146,109,results,best performance,on,complete corpus,best performance on complete corpus,0.5119128227233887
translation,146,109,results,fails,to obtain,acceptable performance,fails to obtain acceptable performance,0.6243821978569031
translation,146,109,results,acceptable performance,for,smaller corpus sizes,acceptable performance for smaller corpus sizes,0.5843680500984192
translation,146,109,results,results,has,top - 10 system,results has top - 10 system,0.5967488884925842
translation,146,123,results,system,has,outperform,system has outperform,0.6443614959716797
translation,146,123,results,outperform,has,system without adaptive retrieval,outperform has system without adaptive retrieval,0.6284518241882324
translation,146,123,results,results,has,both variations,results has both variations,0.552476167678833
translation,147,131,ablation-analysis,entity type prediction,removed from,entity detection task,entity type prediction removed from entity detection task,0.5659908056259155
translation,147,131,ablation-analysis,entity type prediction,results in,9 % drop,entity type prediction results in 9 % drop,0.5898258686065674
translation,147,131,ablation-analysis,9 % drop,of,overall f1 score,9 % drop of overall f1 score,0.5928217172622681
translation,147,131,ablation-analysis,ablation analysis,has,entity type prediction,ablation analysis has entity type prediction,0.5326461791992188
translation,147,134,ablation-analysis,both precision and recall,of,entity linking,both precision and recall of entity linking,0.5967581868171692
translation,147,134,ablation-analysis,drop,without filtering,entity linking results,drop without filtering entity linking results,0.7851340174674988
translation,147,134,ablation-analysis,significantly,without filtering,entity linking results,significantly without filtering entity linking results,0.7562801837921143
translation,147,134,ablation-analysis,entity linking results,w.r.t.,predicted entity type,entity linking results w.r.t. predicted entity type,0.5000956058502197
translation,147,134,ablation-analysis,entity linking,has,drop,entity linking has drop,0.5911775827407837
translation,147,134,ablation-analysis,drop,has,significantly,drop has significantly,0.6707428097724915
translation,147,139,ablation-analysis,accuracy,for,each component,accuracy for each component,0.5850250720977783
translation,147,139,ablation-analysis,each component,of,pointer -equipped logical form,each component of pointer -equipped logical form,0.5986815094947815
translation,147,139,ablation-analysis,drops,with,separate learning,drops with separate learning,0.7169947028160095
translation,147,139,ablation-analysis,pointer -equipped logical form,has,drops,pointer -equipped logical form has drops,0.5930113196372986
translation,147,140,ablation-analysis,0.1 % f1 score reduction,for,entity detection subtask,0.1 % f1 score reduction for entity detection subtask,0.574357807636261
translation,147,140,ablation-analysis,entity detection subtask,compared to,model,entity detection subtask compared to model,0.5742219090461731
translation,147,140,ablation-analysis,model,without,multi-task learning,model without multi-task learning,0.6640601754188538
translation,147,140,ablation-analysis,0.1 % f1 score reduction,has,99.4 % vs. 99.5 %,0.1 % f1 score reduction has 99.4 % vs. 99.5 %,0.5536689758300781
translation,147,140,ablation-analysis,ablation analysis,found,0.1 % f1 score reduction,ablation analysis found 0.1 % f1 score reduction,0.5924710035324097
translation,147,143,ablation-analysis,improvement,of,11 %,improvement of 11 %,0.6198607683181763
translation,147,143,ablation-analysis,improvement,verifies,advantage,improvement verifies advantage,0.6484906077384949
translation,147,143,ablation-analysis,11 %,of,f1 score,11 % of f1 score,0.6171072125434875
translation,147,143,ablation-analysis,11 %,verifies,advantage,11 % verifies advantage,0.6858788728713989
translation,147,143,ablation-analysis,advantage,of,our proposed framework,advantage of our proposed framework,0.5929349064826965
translation,147,148,ablation-analysis,beam search size,from,4 to 8,beam search size from 4 to 8,0.5885117053985596
translation,147,148,ablation-analysis,4 to 8,during,decoding,4 to 8 during decoding,0.7401431798934937
translation,147,148,ablation-analysis,4 to 8,leads to,2.3 % f1 score increase,4 to 8 leads to 2.3 % f1 score increase,0.6380061507225037
translation,147,148,ablation-analysis,decoding,in,inference phase,decoding in inference phase,0.513189435005188
translation,147,148,ablation-analysis,decoding,for,standard settings,decoding for standard settings,0.6250964403152466
translation,147,148,ablation-analysis,inference phase,for,standard settings,inference phase for standard settings,0.6005812287330627
translation,147,148,ablation-analysis,standard settings,leads to,2.3 % f1 score increase,standard settings leads to 2.3 % f1 score increase,0.6428474187850952
translation,147,148,ablation-analysis,ablation analysis,increased,beam search size,ablation analysis increased beam search size,0.6828885674476624
translation,147,96,experimental-setup,buffer size,in,bfs,buffer size in bfs,0.5465661287307739
translation,147,96,experimental-setup,bfs,set to,1000,bfs set to 1000,0.7596099972724915
translation,147,96,experimental-setup,experimental setup,has,buffer size,experimental setup has buffer size,0.5316854119300842
translation,147,97,experimental-setup,embedding and hidden sizes,in,model,embedding and hidden sizes in model,0.5526815056800842
translation,147,97,experimental-setup,embedding and hidden sizes,set to,300d,embedding and hidden sizes set to 300d,0.7320013642311096
translation,147,97,experimental-setup,model,set to,300d,model set to 300d,0.7813264727592468
translation,147,97,experimental-setup,embeddings,loaded for,initialization,embeddings loaded for initialization,0.706031322479248
translation,147,97,experimental-setup,positional encodings,are,randomly initialized and learnable,positional encodings are randomly initialized and learnable,0.5681635737419128
translation,147,97,experimental-setup,experimental setup,has,embedding and hidden sizes,experimental setup has embedding and hidden sizes,0.5424251556396484
translation,147,97,experimental-setup,experimental setup,has,positional encodings,experimental setup has positional encodings,0.5192217230796814
translation,147,98,experimental-setup,head number,of,multi-head attention,head number of multi-head attention,0.6025417447090149
translation,147,98,experimental-setup,head number,of,activation function,head number of activation function,0.5941699147224426
translation,147,98,experimental-setup,multi-head attention,is,6,multi-head attention is 6,0.5725404620170593
translation,147,98,experimental-setup,activation function,inside,ffn ( ? ),activation function inside ffn ( ? ),0.7078390717506409
translation,147,98,experimental-setup,ffn ( ? ),is,gelu ( ? ),ffn ( ? ) is gelu ( ? ),0.6400365829467773
translation,147,98,experimental-setup,experimental setup,has,head number,experimental setup has head number,0.4883655607700348
translation,147,99,experimental-setup,"adam ( kingma and ba , 2015 )",to optimize,loss function,"adam ( kingma and ba , 2015 ) to optimize loss function",0.6505290269851685
translation,147,99,experimental-setup,"adam ( kingma and ba , 2015 )",to optimize,learning rate,"adam ( kingma and ba , 2015 ) to optimize learning rate",0.6453675031661987
translation,147,99,experimental-setup,loss function,set to,1.5,loss function set to 1.5,0.6968922019004822
translation,147,99,experimental-setup,learning rate,set to,10 ?4,learning rate set to 10 ?4,0.7256936430931091
translation,147,100,experimental-setup,training batch size,is,128,training batch size is 128,0.5648654699325562
translation,147,100,experimental-setup,128,for,6 epochs,128 for 6 epochs,0.6629886627197266
translation,147,100,experimental-setup,experimental setup,has,training batch size,experimental setup has training batch size,0.5232828259468079
translation,147,101,experimental-setup,learning rate warmup,within,first 1 % steps,learning rate warmup within first 1 % steps,0.6572304368019104
translation,147,101,experimental-setup,linear decay,within,rest,linear decay within rest,0.7054477334022522
translation,147,101,experimental-setup,experimental setup,employed,learning rate warmup,experimental setup employed learning rate warmup,0.6347365975379944
translation,147,101,experimental-setup,experimental setup,employed,linear decay,experimental setup employed linear decay,0.6215198040008545
translation,147,6,model,innovative multi-task learning framework,where,pointer -equipped semantic parsing model,innovative multi-task learning framework where pointer -equipped semantic parsing model,0.5578343272209167
translation,147,6,model,pointer -equipped semantic parsing model,designed to resolve,coreference,pointer -equipped semantic parsing model designed to resolve coreference,0.5908225774765015
translation,147,6,model,coreference,in,conversations,coreference in conversations,0.5398272275924683
translation,147,6,model,model,propose,innovative multi-task learning framework,model propose innovative multi-task learning framework,0.6338788866996765
translation,147,21,model,novel multi-task semantic parsing framework,for,kb - qa,novel multi-task semantic parsing framework for kb - qa,0.6009188890457153
translation,147,21,model,model,propose,novel multi-task semantic parsing framework,model propose novel multi-task semantic parsing framework,0.6012700796127319
translation,147,22,model,innovative pointerequipped semantic parsing model,explicitly takes into account,context,innovative pointerequipped semantic parsing model explicitly takes into account context,0.7432196140289307
translation,147,22,model,built - in pointer network,toward,positions,built - in pointer network toward positions,0.6618444919586182
translation,147,22,model,built - in pointer network,empower,multi-task learning,built - in pointer network empower multi-task learning,0.672964870929718
translation,147,22,model,positions,of,entity mentions,positions of entity mentions,0.5461143851280212
translation,147,22,model,positions,of,entity mentions,positions of entity mentions,0.5461143851280212
translation,147,22,model,positions,of,entity mentions,positions of entity mentions,0.5461143851280212
translation,147,22,model,multi-task learning,with conjunction of,upstream sequence labeling subtask,multi-task learning with conjunction of upstream sequence labeling subtask,0.689349889755249
translation,147,22,model,context,of,entity mentions,context of entity mentions,0.524561882019043
translation,147,22,model,context,by using,supervision,context by using supervision,0.6332231163978577
translation,147,22,model,supervision,of,pointer network,supervision of pointer network,0.5764221549034119
translation,147,23,model,type-aware entity detection method,to produce,accurate entity linking results,type-aware entity detection method to produce accurate entity linking results,0.6532260179519653
translation,147,23,model,joint prediction space,combining,entity detection and entity type,joint prediction space combining entity detection and entity type,0.7531802654266357
translation,147,23,model,predicted type,to filter,entity linking results,predicted type to filter entity linking results,0.6314468383789062
translation,147,23,model,entity linking results,during,inference phase,entity linking results during inference phase,0.6457438468933105
translation,147,26,model,joint learning framework,combining,entity mention detection with type prediction,joint learning framework combining entity mention detection with type prediction,0.6700549721717834
translation,147,26,model,entity mention detection with type prediction,leverages,contextual information,entity mention detection with type prediction leverages contextual information,0.671541154384613
translation,147,26,model,entity mention detection with type prediction,reduces,errors in entity linking,entity mention detection with type prediction reduces errors in entity linking,0.5335116386413574
translation,147,35,model,semantic parsing model,used to produce,logical form,semantic parsing model used to produce logical form,0.5788140296936035
translation,147,35,model,logical form,executed on,kb,logical form executed on kb,0.7071101069450378
translation,147,35,model,kb,to retrieve,answer,kb to retrieve answer,0.806150496006012
translation,147,35,model,question,has,semantic parsing model,question has semantic parsing model,0.5444344282150269
translation,147,35,model,model,given,question,model given question,0.7526277899742126
translation,147,95,model,bfs method,to search,valid logical forms,bfs method to search valid logical forms,0.6881329417228699
translation,147,95,model,valid logical forms,for,questions,valid logical forms for questions,0.626973569393158
translation,147,95,model,questions,in,training data,questions in training data,0.49359819293022156
translation,147,95,model,model,leveraged,bfs method,model leveraged bfs method,0.6903838515281677
translation,147,116,model,proposed pointer -equipped logical form decoder,in,multi-task learning framework,proposed pointer -equipped logical form decoder in multi-task learning framework,0.5000494122505188
translation,147,116,model,proposed pointer -equipped logical form decoder,handles,coreference,proposed pointer -equipped logical form decoder handles coreference,0.6968692541122437
translation,147,116,model,model,has,proposed pointer -equipped logical form decoder,model has proposed pointer -equipped logical form decoder,0.575675368309021
translation,147,27,results,approach,beneficial to,coreference resolution,approach beneficial to coreference resolution,0.6688499450683594
translation,147,27,results,coreference resolution,for,conversational qa,coreference resolution for conversational qa,0.5304202437400818
translation,147,27,results,conversational qa,due to,rich contextual features,conversational qa due to rich contextual features,0.6180002689361572
translation,147,27,results,rich contextual features,captured for,entity mention,rich contextual features captured for entity mention,0.6777939796447754
translation,147,27,results,results,has,approach,results has approach,0.5518386363983154
translation,147,30,results,overall f1 score,improved by,12.56 %,overall f1 score improved by 12.56 %,0.6872650384902954
translation,147,30,results,12.56 %,compared with,strong baselines,12.56 % compared with strong baselines,0.6893128156661987
translation,147,30,results,consistent,for,all question types,consistent for all question types,0.6138666272163391
translation,147,30,results,results,show that,overall f1 score,results show that overall f1 score,0.466795414686203
translation,147,111,results,semantic parsing based d2a,has,significantly outperforms,semantic parsing based d2a has significantly outperforms,0.28245094418525696
translation,147,111,results,significantly outperforms,has,memory network based text generation approach ( hred + kvmem ),significantly outperforms has memory network based text generation approach ( hred + kvmem ),0.6108882427215576
translation,147,112,results,our proposed approach ( masp ),achieves,new state - of - the - art performance,our proposed approach ( masp ) achieves new state - of - the - art performance,0.6643800139427185
translation,147,112,results,new state - of - the - art performance,where,overall f1 score,new state - of - the - art performance where overall f1 score,0.5761050581932068
translation,147,112,results,overall f1 score,improved by,?12 %,overall f1 score improved by ?12 %,0.7045771479606628
translation,147,112,results,results,has,our proposed approach ( masp ),results has our proposed approach ( masp ),0.577313244342804
translation,147,113,results,consistent,for,all question types,consistent for all question types,0.6138666272163391
translation,147,113,results,consistent,ranges,2 % to 25 %,consistent ranges 2 % to 25 %,0.7743806838989258
translation,147,113,results,consistent,from,2 % to 25 %,consistent from 2 % to 25 %,0.582575798034668
translation,147,113,results,results,has,improvement,results has improvement,0.6248279809951782
translation,147,115,results,our approach,predicts,entities,our approach predicts entities,0.6825026869773865
translation,147,115,results,entities,where,accuracy,entities where accuracy,0.5805378556251526
translation,147,115,results,entities,in,final logical forms,entities in final logical forms,0.518260657787323
translation,147,115,results,more accurately,where,accuracy,more accurately where accuracy,0.6642788648605347
translation,147,115,results,accuracy,of,entities,accuracy of entities,0.6360244750976562
translation,147,115,results,entities,in,final logical forms,entities in final logical forms,0.518260657787323
translation,147,115,results,increases,from,55 % to 72 %,increases from 55 % to 72 %,0.633589506149292
translation,147,115,results,55 % to 72 %,compared with,d2a,55 % to 72 % compared with d2a,0.6825436353683472
translation,147,115,results,entities,has,more accurately,entities has more accurately,0.5726174712181091
translation,147,115,results,results,has,our approach,results has our approach,0.6050099730491638
translation,147,137,results,f1 score,for,every question type,f1 score for every question type,0.6088517308235168
translation,147,137,results,consistently drops,range of,3 % to 14 %,consistently drops range of 3 % to 14 %,0.7570651173591614
translation,147,137,results,3 % to 14 %,compared with,multi-task learning,3 % to 14 % compared with multi-task learning,0.6536772847175598
translation,147,137,results,3 % to 14 %,with,multi-task learning,3 % to 14 % with multi-task learning,0.6367796063423157
translation,147,137,results,every question type,has,consistently drops,every question type has consistently drops,0.6201467514038086
translation,147,141,results,multi-task learning framework,increases,accuracy,multi-task learning framework increases accuracy,0.6316198110580444
translation,147,141,results,accuracy,of,pointer - based logical form generation,accuracy of pointer - based logical form generation,0.5835133790969849
translation,147,141,results,accuracy,keeping,satisfactory performance,accuracy keeping satisfactory performance,0.6574261784553528
translation,147,141,results,satisfactory performance,of,entity detection,satisfactory performance of entity detection,0.5654740929603577
translation,147,141,results,satisfactory performance,improves,final question answering performance,satisfactory performance improves final question answering performance,0.6284911036491394
translation,148,81,experimental-setup,available open source code,with,default parameters,available open source code with default parameters,0.5675018429756165
translation,148,80,experiments,squad,evaluate,"bidaf ( seo et al. , 2017 )","squad evaluate bidaf ( seo et al. , 2017 )",0.6400965452194214
translation,148,80,experiments,squad,evaluate,bidaf,squad evaluate bidaf,0.749518096446991
translation,148,80,experiments,squad,evaluate,"rnet ( wang et al. , 2017 )","squad evaluate rnet ( wang et al. , 2017 )",0.6731105446815491
translation,148,80,experiments,squad,evaluate,"mnemonic reader ( mnem ; hu et al. , 2018 )","squad evaluate mnemonic reader ( mnem ; hu et al. , 2018 )",0.6578251123428345
translation,148,80,experiments,bidaf,with,elmo embeddings,bidaf with elmo embeddings,0.7137469053268433
translation,148,80,experiments,bidaf,with,"mnemonic reader ( mnem ; hu et al. , 2018 )","bidaf with mnemonic reader ( mnem ; hu et al. , 2018 )",0.6026299595832825
translation,148,80,experiments,"bidaf ( seo et al. , 2017 )",has,bidaf,"bidaf ( seo et al. , 2017 ) has bidaf",0.5760903358459473
translation,148,80,experiments,elmo embeddings,has,bidaf+e,elmo embeddings has bidaf+e,0.6791111826896667
translation,148,103,experiments,saaa,on,gqa dataset,saaa on gqa dataset,0.5559712648391724
translation,148,29,model,simple data augmentation procedure,results in,models,simple data augmentation procedure results in models,0.6715962886810303
translation,148,29,model,models,nearly as accurate as,original models,models nearly as accurate as original models,0.6702576875686646
translation,148,29,model,models,on,original data,models on original data,0.5047024488449097
translation,148,29,model,original models,on,original data,original models on original data,0.5077625513076782
translation,148,29,model,model,propose,simple data augmentation procedure,model propose simple data augmentation procedure,0.6779276728630066
translation,148,102,results,accuracy,on,validation set,accuracy on validation set,0.5693098306655884
translation,148,102,results,validation set,remains,comparable,validation set remains comparable,0.67888343334198
translation,148,102,results,comparable,after,augmentation,comparable after augmentation,0.7679075598716736
translation,148,102,results,consistency,on,generated and worker - provided implications,consistency on generated and worker - provided implications,0.5605485439300537
translation,148,102,results,improves,across,models and tasks,improves across models and tasks,0.6440643668174744
translation,148,102,results,generated and worker - provided implications,has,improves,generated and worker - provided implications has improves,0.6039634943008423
translation,148,102,results,results,has,accuracy,results has accuracy,0.5888755321502686
translation,149,220,baselines,templates,using,questionanswer pairs,templates using questionanswer pairs,0.6790327429771423
translation,149,220,baselines,templates,relies on,imitation learning,templates relies on imitation learning,0.685187816619873
translation,149,220,baselines,templates,uses,rules,templates uses rules,0.6783772706985474
translation,149,220,baselines,bast and haussmann ( 2015 ),instantiates,hand -crafted query templates,bast and haussmann ( 2015 ) instantiates hand -crafted query templates,0.7241470217704773
translation,149,220,baselines,bast and haussmann ( 2015 ),relies on,agenda- based parsing,bast and haussmann ( 2015 ) relies on agenda- based parsing,0.7372649908065796
translation,149,220,baselines,bast and haussmann ( 2015 ),relies on,imitation learning,bast and haussmann ( 2015 ) relies on imitation learning,0.7366977334022522
translation,149,220,baselines,bast and haussmann ( 2015 ),uses,rules,bast and haussmann ( 2015 ) uses rules,0.5908142924308777
translation,149,220,baselines,hand -crafted query templates,followed by,query ranking,hand -crafted query templates followed by query ranking,0.6393716335296631
translation,149,220,baselines,hand -crafted query templates,relies on,imitation learning,hand -crafted query templates relies on imitation learning,0.6802173852920532
translation,149,220,baselines,hand -crafted query templates,uses,rules,hand -crafted query templates uses rules,0.5929386019706726
translation,149,220,baselines,berant and liang ( 2015 ),relies on,agenda- based parsing,berant and liang ( 2015 ) relies on agenda- based parsing,0.7399080991744995
translation,149,220,baselines,berant and liang ( 2015 ),relies on,imitation learning,berant and liang ( 2015 ) relies on imitation learning,0.7405608892440796
translation,149,220,baselines,berant and liang ( 2015 ),uses,rules,berant and liang ( 2015 ) uses rules,0.6256605386734009
translation,149,220,baselines,berant et al. ( 2013 ),uses,rules,berant et al. ( 2013 ) uses rules,0.6080988049507141
translation,149,220,baselines,berant et al. ( 2013 ),maps,questions,berant et al. ( 2013 ) maps questions,0.7079430222511292
translation,149,220,baselines,rules,to build,queries,rules to build queries,0.6806600689888
translation,149,220,baselines,rules,to build,queries,rules to build queries,0.6806600689888
translation,149,220,baselines,queries,from,questions,queries from questions,0.5558825135231018
translation,149,220,baselines,queries,over,open vocabulary facts,queries over open vocabulary facts,0.6553782224655151
translation,149,220,baselines,questions,to,queries,questions to queries,0.5681679844856262
translation,149,220,baselines,fader et al . ( 2013 ),maps,questions,fader et al . ( 2013 ) maps questions,0.7232984900474548
translation,149,220,baselines,questions,to,queries,questions to queries,0.5681679844856262
translation,149,220,baselines,queries,over,open vocabulary facts,queries over open vocabulary facts,0.6553782224655151
translation,149,220,baselines,open vocabulary facts,extracted from,web documents,open vocabulary facts extracted from web documents,0.5502793192863464
translation,149,6,experiments,comqa questions,come from,wikianswers community qa platform,comqa questions come from wikianswers community qa platform,0.6675707697868347
translation,149,230,results,baselines,on,comqa test set,baselines on comqa test set,0.5179513096809387
translation,149,231,results,systems,achieved,poor performance,systems achieved poor performance,0.7059276700019836
translation,149,231,results,results,has,systems,results has systems,0.5238543152809143
translation,150,35,baselines,pipelined system,to tackle,squash,pipelined system to tackle squash,0.6565895676612854
translation,150,35,baselines,squash,along with,crowdsourced methods,squash along with crowdsourced methods,0.5896115303039551
translation,150,35,baselines,a4,has,pipelined system,a4 has pipelined system,0.6093863248825073
translation,150,35,baselines,baselines,has,a4,baselines has a4,0.631263792514801
translation,150,34,experiments,novel text generation task ( squash ),converts,documents,novel text generation task ( squash ) converts documents,0.6166301965713501
translation,150,34,experiments,documents,into,specificity - based hierarchies,documents into specificity - based hierarchies,0.5983142256736755
translation,150,34,experiments,a3,has,novel text generation task ( squash ),a3 has novel text generation task ( squash ),0.5530827045440674
translation,150,34,experiments,specificity - based hierarchies,has,of qa pairs,specificity - based hierarchies has of qa pairs,0.5624153017997742
translation,150,79,experiments,models,in,pytorch v0.4,models in pytorch v0.4,0.5178821086883545
translation,150,79,experiments,best-performing model,achieves,perplexity,best-performing model achieves perplexity,0.612804651260376
translation,150,79,experiments,perplexity,of,11.1,perplexity of 11.1,0.5717094540596008
translation,150,79,experiments,11.1,on,validation set,11.1 on validation set,0.48160794377326965
translation,150,83,hyperparameters,beam search,with,beam size,beam search with beam size,0.6320963501930237
translation,150,83,hyperparameters,beam size,of,3,beam size of 3,0.6917811036109924
translation,150,83,hyperparameters,3,to generate,three highly - probable question candidates,3 to generate three highly - probable question candidates,0.5860805511474609
translation,150,83,hyperparameters,hyperparameters,use,beam search,hyperparameters use beam search,0.6655463576316833
translation,150,175,results,general question,preferred over,specific one,general question preferred over specific one,0.8058741092681885
translation,150,175,results,89.5 % instances,has,general question,89.5 % instances has general question,0.5820828676223755
translation,151,149,ablation-analysis,cont 1 feature,turns out to be,most important one,cont 1 feature turns out to be most important one,0.58610600233078
translation,151,149,ablation-analysis,other similarity features,does not yield,any further improvements,other similarity features does not yield any further improvements,0.7068525552749634
translation,151,149,ablation-analysis,ablation analysis,has,cont 1 feature,ablation analysis has cont 1 feature,0.5638740658760071
translation,151,152,ablation-analysis,heuristic features,seem to be,most useful ones,heuristic features seem to be most useful ones,0.6093826293945312
translation,151,152,ablation-analysis,most useful ones,followed by,context - based ones,most useful ones followed by context - based ones,0.6364068388938904
translation,151,41,baselines,three approaches,to build,wordembedding vector representations,three approaches to build wordembedding vector representations,0.6455938220024109
translation,151,41,baselines,wordembedding vector representations,using,latent semantic analysis,wordembedding vector representations using latent semantic analysis,0.5864560008049011
translation,151,41,baselines,qatar living corpus,with,word co-occurrence window,qatar living corpus with word co-occurrence window,0.6164385676383972
translation,151,41,baselines,word co-occurrence window,of size,?3,word co-occurrence window of size ?3,0.7134657502174377
translation,151,41,baselines,vector,of,250 dimensions,vector of 250 dimensions,0.6324989795684814
translation,151,41,baselines,250 dimensions,with,svd,250 dimensions with svd,0.6560978889465332
translation,151,41,baselines,glove,using,model,glove using model,0.7089402675628662
translation,151,41,baselines,model,pre-trained on,common crawl ( 42b tokens ),model pre-trained on common crawl ( 42b tokens ),0.7682786583900452
translation,151,41,baselines,common crawl ( 42b tokens ),with,300 dimensions,common crawl ( 42b tokens ) with 300 dimensions,0.6358723640441895
translation,151,41,baselines,baselines,apply,three approaches,baselines apply three approaches,0.5877593755722046
translation,151,6,experiments,best performing one,in,arabic subtask,best performing one in arabic subtask,0.4939385652542114
translation,151,6,experiments,third best,in,two english subtasks,third best in two english subtasks,0.4981215000152588
translation,151,127,results,performance,of,two contrastive submissions,performance of two contrastive submissions,0.568907618522644
translation,151,127,results,two contrastive submissions,below but close to,our primary submission,two contrastive submissions below but close to our primary submission,0.6305137872695923
translation,151,127,results,our primary submission,particularly for,irrelevant comments,our primary submission particularly for irrelevant comments,0.6411884427070618
translation,151,127,results,results,has,performance,results has performance,0.5972660779953003
translation,151,148,results,n-gram features,together with,cont 1 submission,n-gram features together with cont 1 submission,0.6238983273506165
translation,151,148,results,n-gram features,allow for,slightly better performance,n-gram features allow for slightly better performance,0.6965852975845337
translation,151,148,results,slightly better performance,than,our - already winning -primary submission,slightly better performance than our - already winning -primary submission,0.5895230174064636
translation,151,148,results,results,has,n-gram features,results has n-gram features,0.525908350944519
translation,151,154,results,n-grams,improves over,performance,n-grams improves over performance,0.7023505568504333
translation,151,154,results,performance,of,our primary run,performance of our primary run,0.5945553183555603
translation,151,154,results,all features but,has,n-grams,all features but has n-grams,0.6003804206848145
translation,151,154,results,our primary run,has,f 1 = 55.17,our primary run has f 1 = 55.17,0.5858604311943054
translation,151,154,results,results,using,all features but,results using all features but,0.7310066223144531
translation,151,164,results,bug-free implementation,from,training set,bug-free implementation from training set,0.5111062526702881
translation,151,164,results,bug-free implementation,yielded,much higher f 1,bug-free implementation yielded much higher f 1,0.6215540766716003
translation,151,164,results,much higher f 1,of,69.35,much higher f 1 of 69.35,0.5629361271858215
translation,151,164,results,69.35,on,test dataset,69.35 on test dataset,0.4972201883792877
translation,151,164,results,results,learning with,bug-free implementation,results learning with bug-free implementation,0.6353715658187866
translation,151,182,results,our performance,for,all three subtasks,our performance for all three subtasks,0.5561741590499878
translation,151,182,results,improve,has,our performance,improve has our performance,0.5544538497924805
translation,152,41,experimental-setup,two topic models,trained in,training data,two topic models trained in training data,0.6834424138069153
translation,152,41,experimental-setup,first one,trained in,training data,first one trained in training data,0.7704602479934692
translation,152,41,experimental-setup,second one,trained in,wikipedia data,second one trained in wikipedia data,0.757317304611206
translation,152,41,experimental-setup,wikipedia data,using,gensim toolkit,wikipedia data using gensim toolkit,0.644099235534668
translation,152,41,experimental-setup,wikipedia data,using,"mallet toolkit ( mccallum , 2002 )","wikipedia data using mallet toolkit ( mccallum , 2002 )",0.6757498383522034
translation,152,41,experimental-setup,two topic models,has,first one,two topic models has first one,0.5764939188957214
translation,152,41,experimental-setup,two topic models,has,second one,two topic models has second one,0.5879252552986145
translation,152,41,experimental-setup,experimental setup,build,two topic models,experimental setup build two topic models,0.6336252093315125
translation,152,45,experimental-setup,word2 vec model,built with,word vector size,word2 vec model built with word vector size,0.662094235420227
translation,152,45,experimental-setup,word2 vec model,built with,window size,word2 vec model built with window size,0.6874498128890991
translation,152,45,experimental-setup,word2 vec model,built with,minimum word frequency,word2 vec model built with minimum word frequency,0.6905729174613953
translation,152,45,experimental-setup,word vector size,of,300,word vector size of 300,0.6491621732711792
translation,152,45,experimental-setup,window size,of,"3 ( n-skip-gram , n=3 )","window size of 3 ( n-skip-gram , n=3 )",0.5912803411483765
translation,152,45,experimental-setup,minimum word frequency,of,1,minimum word frequency of 1,0.6538429856300354
translation,152,45,experimental-setup,experimental setup,has,word2 vec model,experimental setup has word2 vec model,0.5279579758644104
translation,152,44,experiments,two word vector representation models,built using,word2vec tool,two word vector representation models built using word2vec tool,0.719820499420166
translation,152,44,experiments,second one,trained from,qatar living forum data,second one trained from qatar living forum data,0.7663716077804565
translation,152,44,experiments,two word vector representation models,has,first one,two word vector representation models has first one,0.5792703032493591
translation,153,161,baselines,full wiki setting,compare,three methods of retrieval,full wiki setting compare three methods of retrieval,0.6319160461425781
translation,153,161,baselines,tf - idf,in which,only the tf - idf heuristic,tf - idf in which only the tf - idf heuristic,0.6417522430419922
translation,153,268,experimental-setup,our models,using,tensorflow,our models using tensorflow,0.6735852360725403
translation,153,268,experimental-setup,experimental setup,implement,our models,experimental setup implement our models,0.6547874808311462
translation,153,270,experimental-setup,word- level embeddings,use,glove 300 - dimensional embeddings,word- level embeddings use glove 300 - dimensional embeddings,0.5677332878112793
translation,153,270,experimental-setup,glove 300 - dimensional embeddings,pretrained on,840b common crawl corpus,glove 300 - dimensional embeddings pretrained on 840b common crawl corpus,0.7756370306015015
translation,153,270,experimental-setup,experimental setup,For,word- level embeddings,experimental setup For word- level embeddings,0.5276378393173218
translation,153,271,experimental-setup,characterlevel embeddings,use,20 - dimensional character embeddings,characterlevel embeddings use 20 - dimensional character embeddings,0.5806337594985962
translation,153,271,experimental-setup,characterlevel embeddings,use,1 - dimensional cnn,characterlevel embeddings use 1 - dimensional cnn,0.5606383085250854
translation,153,271,experimental-setup,characterlevel embeddings,use,1 - dimensional cnn,characterlevel embeddings use 1 - dimensional cnn,0.5606383085250854
translation,153,271,experimental-setup,1 - dimensional cnn,with,100 filters,1 - dimensional cnn with 100 filters,0.6349371671676636
translation,153,271,experimental-setup,1 - dimensional cnn,with,"dropout ( srivastava et al. , 2014 ) rate","1 - dimensional cnn with dropout ( srivastava et al. , 2014 ) rate",0.5981937646865845
translation,153,271,experimental-setup,100 filters,of,size,100 filters of size,0.6282867789268494
translation,153,271,experimental-setup,"dropout ( srivastava et al. , 2014 ) rate",of,0.2,"dropout ( srivastava et al. , 2014 ) rate of 0.2",0.5728970170021057
translation,153,271,experimental-setup,size,has,5,size has 5,0.677672266960144
translation,153,271,experimental-setup,experimental setup,For,characterlevel embeddings,experimental setup For characterlevel embeddings,0.5261794924736023
translation,153,272,experimental-setup,encoder,concatenate,"elmo ( peters et al. , 2018 ) embeddings","encoder concatenate elmo ( peters et al. , 2018 ) embeddings",0.6765561103820801
translation,153,272,experimental-setup,encoder,concatenate,token representations,encoder concatenate token representations,0.7469122409820557
translation,153,272,experimental-setup,"elmo ( peters et al. , 2018 ) embeddings",with,dropout rate,"elmo ( peters et al. , 2018 ) embeddings with dropout rate",0.5907402038574219
translation,153,272,experimental-setup,"elmo ( peters et al. , 2018 ) embeddings",with,token representations,"elmo ( peters et al. , 2018 ) embeddings with token representations",0.5867615938186646
translation,153,272,experimental-setup,dropout rate,of,0.5,dropout rate of 0.5,0.6072384119033813
translation,153,272,experimental-setup,token representations,from,output of embedding layer,token representations from output of embedding layer,0.5057608485221863
translation,153,272,experimental-setup,output of embedding layer,to form,final token representations,output of embedding layer to form final token representations,0.6001025438308716
translation,153,272,experimental-setup,experimental setup,For,encoder,experimental setup For encoder,0.5619007349014282
translation,153,273,experimental-setup,elmo weights,pretrained on,5.5b dataset,elmo weights pretrained on 5.5b dataset,0.7597278356552124
translation,153,273,experimental-setup,experimental setup,use,elmo weights,experimental setup use elmo weights,0.6367730498313904
translation,153,278,experimental-setup,"variational dropout ( gal and ghahramani , 2016 )",where,same dropout mask,"variational dropout ( gal and ghahramani , 2016 ) where same dropout mask",0.5670866370201111
translation,153,278,experimental-setup,same dropout mask,applied at,each time step,same dropout mask applied at each time step,0.6339324712753296
translation,153,278,experimental-setup,same dropout mask,applied on,inputs,same dropout mask applied on inputs,0.6735081076622009
translation,153,278,experimental-setup,inputs,of,all recurrent layers,inputs of all recurrent layers,0.5684587359428406
translation,153,278,experimental-setup,inputs,with,dropout rate,inputs with dropout rate,0.6234537363052368
translation,153,278,experimental-setup,dropout rate,of,0.2,dropout rate of 0.2,0.5832480192184448
translation,153,278,experimental-setup,experimental setup,has,"variational dropout ( gal and ghahramani , 2016 )","experimental setup has variational dropout ( gal and ghahramani , 2016 )",0.4785112142562866
translation,153,279,experimental-setup,encoding size,to be,d = 1024,encoding size to be d = 1024,0.5658511519432068
translation,153,279,experimental-setup,experimental setup,set,encoding size,experimental setup set encoding size,0.6310187578201294
translation,153,280,experimental-setup,paragraph reader,used for,hotpotqa,paragraph reader used for hotpotqa,0.6982659697532654
translation,153,280,experimental-setup,paragraph reader,use,state size,paragraph reader use state size,0.6680058240890503
translation,153,280,experimental-setup,hotpotqa,use,state size,hotpotqa use state size,0.6503616571426392
translation,153,280,experimental-setup,state size,of,150,state size of 150,0.686160147190094
translation,153,280,experimental-setup,150,for,bidirectional grus,150 for bidirectional grus,0.6903210282325745
translation,153,280,experimental-setup,experimental setup,For,paragraph reader,experimental setup For paragraph reader,0.6009041666984558
translation,153,281,experimental-setup,size,of,hidden layer,size of hidden layer,0.6069005727767944
translation,153,281,experimental-setup,hidden layer,in,mlp,hidden layer in mlp,0.5590221285820007
translation,153,281,experimental-setup,mlp,used for,supporting fact prediction,mlp used for supporting fact prediction,0.6345857977867126
translation,153,281,experimental-setup,supporting fact prediction,set to,150,supporting fact prediction set to 150,0.6855762004852295
translation,153,281,experimental-setup,experimental setup,has,size,experimental setup has size,0.5329226851463318
translation,153,282,experimental-setup,variational dropout,with,dropout rate,variational dropout with dropout rate,0.546349823474884
translation,153,282,experimental-setup,dropout rate,of,0.2,dropout rate of 0.2,0.5832480192184448
translation,153,282,experimental-setup,0.2,applied on,inputs,0.2 applied on inputs,0.7003133893013
translation,153,282,experimental-setup,0.2,applied on,attention mechanisms,0.2 applied on attention mechanisms,0.6496878266334534
translation,153,282,experimental-setup,inputs,of,all recurrent layers,inputs of all recurrent layers,0.5684587359428406
translation,153,282,experimental-setup,inputs,of,attention mechanisms,inputs of attention mechanisms,0.5882104635238647
translation,153,282,experimental-setup,experimental setup,has,variational dropout,experimental setup has variational dropout,0.4783385097980499
translation,153,166,experiments,our sentencelevel method,established,state - of - the - art results,our sentencelevel method established state - of - the - art results,0.5492022633552551
translation,153,166,experiments,"current non-bert ( devlin et al. , 2018 ) state - of- the - art",by,4.6 ( 13 % ) and 3.6 ( 8 % ) em and f 1 points,"current non-bert ( devlin et al. , 2018 ) state - of- the - art by 4.6 ( 13 % ) and 3.6 ( 8 % ) em and f 1 points",0.5391138195991516
translation,153,166,experiments,hotpotqa full wiki setting,has,our sentencelevel method,hotpotqa full wiki setting has our sentencelevel method,0.5660110116004944
translation,153,166,experiments,squad - open,has,our sentencelevel method,squad - open has our sentencelevel method,0.5814917683601379
translation,153,207,experiments,muppet,show,efficacy,muppet show efficacy,0.7139021754264832
translation,153,207,experiments,novel method,for,multihop paragraph retrieval,novel method for multihop paragraph retrieval,0.5702717304229736
translation,153,207,experiments,efficacy,in,single - and multi-hop qa datasets,efficacy in single - and multi-hop qa datasets,0.5618096590042114
translation,153,207,experiments,muppet,has,novel method,muppet has novel method,0.6276776790618896
translation,153,7,model,supporting paragraphs,by forming,joint vector representation,supporting paragraphs by forming joint vector representation,0.69987553358078
translation,153,7,model,joint vector representation,of both,question,joint vector representation of both question,0.6520290970802307
translation,153,7,model,joint vector representation,of both,paragraph,joint vector representation of both paragraph,0.7184469699859619
translation,153,8,model,retrieval,performed by considering,contextualized sentence -level representations,retrieval performed by considering contextualized sentence -level representations,0.6945174336433411
translation,153,8,model,contextualized sentence -level representations,of,paragraphs,contextualized sentence -level representations of paragraphs,0.5529468059539795
translation,153,8,model,paragraphs,in,knowledge source,paragraphs in knowledge source,0.5022746324539185
translation,153,8,model,model,has,retrieval,model has retrieval,0.6230049729347229
translation,153,53,model,scheme,consisting of,two main components,scheme consisting of two main components,0.7538604140281677
translation,153,53,model,model,call,muppet ( multi-hop paragraph retrieval ),model call muppet ( multi-hop paragraph retrieval ),0.6302182674407959
translation,153,54,model,encoder,trained to encode,paragraphs,encoder trained to encode paragraphs,0.7545295357704163
translation,153,54,model,encoder,to encode,questions,encoder to encode questions,0.7887636423110962
translation,153,54,model,paragraphs,into,d-dimensional vectors,paragraphs into d-dimensional vectors,0.6149683594703674
translation,153,54,model,questions,into,search vectors,questions into search vectors,0.559667706489563
translation,153,54,model,search vectors,in,same vector space,search vectors in same vector space,0.5534452199935913
translation,153,54,model,model,has,encoder,model has encoder,0.5940273404121399
translation,153,164,model,paragraph - level,use,muppet,paragraph - level use muppet,0.6564065217971802
translation,153,164,model,muppet,with,paragraph - level encodings,muppet with paragraph - level encodings,0.6708347797393799
translation,153,164,model,model,has,paragraph - level,model has paragraph - level,0.5686962008476257
translation,153,160,results,our paragraph reader,greatly improves,results,our paragraph reader greatly improves results,0.7451062798500061
translation,153,160,results,results,of,baseline reader,results of baseline reader,0.5430651903152466
translation,153,160,results,joint em and f 1 scores,by,17.12 ( 148 % ) and 13.22 ( 32 % ) points,joint em and f 1 scores by 17.12 ( 148 % ) and 13.22 ( 32 % ) points,0.5527402758598328
translation,153,160,results,hotpotqa distractor setting,has,our paragraph reader,hotpotqa distractor setting has our paragraph reader,0.6169581413269043
translation,153,160,results,results,In,hotpotqa distractor setting,results In hotpotqa distractor setting,0.5214437246322632
translation,153,165,results,both methods,has,significantly outperform,both methods has significantly outperform,0.5781120657920837
translation,153,165,results,significantly outperform,has,na?ve tf - idf retriever,significantly outperform has na?ve tf - idf retriever,0.560587465763092
translation,153,165,results,results,see that,both methods,results see that both methods,0.5754765272140503
translation,153,169,results,performance,of,tf -idf retriever,performance of tf -idf retriever,0.5612159967422485
translation,153,169,results,tf -idf retriever,for,hotpotqa,tf -idf retriever for hotpotqa,0.6096253395080566
translation,153,169,results,results,analyze,performance,results analyze performance,0.6696720719337463
translation,153,170,results,retriever,fails at retrieving,both gold paragraphs,retriever fails at retrieving both gold paragraphs,0.7992885112762451
translation,153,170,results,succeeds,in,retrieving,succeeds in retrieving,0.5959559679031372
translation,153,170,results,at least one of the gold paragraphs,for,each question,at least one of the gold paragraphs for each question,0.6306217312812805
translation,153,170,results,above 90 %,with,top -32 paragraphs,above 90 % with top -32 paragraphs,0.6752396821975708
translation,153,170,results,retriever,has,succeeds,retriever has succeeds,0.6582711935043335
translation,153,170,results,retrieving,has,at least one of the gold paragraphs,retrieving has at least one of the gold paragraphs,0.6023991107940674
translation,153,170,results,results,see that,retriever,results see that retriever,0.743012011051178
translation,153,180,results,sentence - level encodings,vital for improving,state - of - the - art results,sentence - level encodings vital for improving state - of - the - art results,0.6243002414703369
translation,153,180,results,state - of - the - art results,on,squad - open,state - of - the - art results on squad - open,0.5379152297973633
translation,153,180,results,results,has,sentence - level encodings,results has sentence - level encodings,0.483433336019516
translation,154,140,ablation-analysis,performance gain,lifted to,+ 1.05 %,performance gain lifted to + 1.05 %,0.6775985956192017
translation,154,140,ablation-analysis,ablation analysis,has,performance gain,ablation analysis has performance gain,0.5206633806228638
translation,154,179,ablation-analysis,performance,of,our model,performance of our model,0.5847885608673096
translation,154,179,ablation-analysis,our model,decreases from,66.57 %,our model decreases from 66.57 %,0.6520934104919434
translation,154,179,ablation-analysis,66.57 %,to,65.52 %,66.57 % to 65.52 %,0.5980920791625977
translation,154,179,ablation-analysis,i - gcn,has,performance,i - gcn has performance,0.5611333847045898
translation,154,179,ablation-analysis,ablation analysis,When removing,i - gcn,ablation analysis When removing i - gcn,0.7487422823905945
translation,154,180,ablation-analysis,performance,of,our model,performance of our model,0.5847885608673096
translation,154,180,ablation-analysis,slightly decreases,from,66.57 % to 66.15 %,slightly decreases from 66.57 % to 66.15 %,0.6079257726669312
translation,154,180,ablation-analysis,q-gcn,has,performance,q-gcn has performance,0.5727790594100952
translation,154,180,ablation-analysis,our model,has,slightly decreases,our model has slightly decreases,0.5970408916473389
translation,154,180,ablation-analysis,ablation analysis,When removing,q-gcn,ablation analysis When removing q-gcn,0.7413632273674011
translation,154,187,ablation-analysis,relations,like,det,relations like det,0.662065327167511
translation,154,187,ablation-analysis,relations,like,case,relations like case,0.6369240880012512
translation,154,187,ablation-analysis,relations,like,aux and advmod,relations like aux and advmod,0.6094025373458862
translation,154,187,ablation-analysis,trivial influence,to,semantic representations,trivial influence to semantic representations,0.5697643160820007
translation,154,187,ablation-analysis,semantic representations,of,question,semantic representations of question,0.5995720028877258
translation,154,187,ablation-analysis,relations,has,trivial influence,relations has trivial influence,0.5852721333503723
translation,154,187,ablation-analysis,ablation analysis,removing,relations,ablation analysis removing relations,0.8096579909324646
translation,154,130,baselines,mcan,utilizes,deep modular networks,mcan utilizes deep modular networks,0.565640389919281
translation,154,130,baselines,deep modular networks,to learn,multimodal feature representations,deep modular networks to learn multimodal feature representations,0.5831626057624817
translation,154,130,baselines,multimodal feature representations,is,state - of - the - art approach,multimodal feature representations is state - of - the - art approach,0.5500240325927734
translation,154,130,baselines,state - of - the - art approach,on,vqa - v2 dataset,state - of - the - art approach on vqa - v2 dataset,0.5249344110488892
translation,154,130,baselines,baselines,has,mcan,baselines has mcan,0.5759053230285645
translation,154,116,hyperparameters,adam optimizer,with,parameters,adam optimizer with parameters,0.6029804944992065
translation,154,116,hyperparameters,parameters,has,"? = 0.0001 , ? 1 = 0.9 , and ? 2 = 0.99","parameters has ? = 0.0001 , ? 1 = 0.9 , and ? 2 = 0.99",0.5473737716674805
translation,154,116,hyperparameters,hyperparameters,use,adam optimizer,hyperparameters use adam optimizer,0.6090166568756104
translation,154,117,hyperparameters,size,of,answer vocabulary,size of answer vocabulary,0.5766217708587646
translation,154,117,hyperparameters,answer vocabulary,set to,"m = 3,129","answer vocabulary set to m = 3,129",0.6870076656341553
translation,154,117,hyperparameters,hyperparameters,has,size,hyperparameters has size,0.5313372611999512
translation,154,118,hyperparameters,base learning rate,set to,0.0001,base learning rate set to 0.0001,0.7031055092811584
translation,154,118,hyperparameters,hyperparameters,has,base learning rate,hyperparameters has base learning rate,0.48160797357559204
translation,154,119,hyperparameters,learning rate,decayed by,1/5,learning rate decayed by 1/5,0.7251017093658447
translation,154,119,hyperparameters,1/5,every,2 epochs,1/5 every 2 epochs,0.6554816961288452
translation,154,119,hyperparameters,15 epochs,has,learning rate,15 epochs has learning rate,0.5677056312561035
translation,154,119,hyperparameters,hyperparameters,After,15 epochs,hyperparameters After 15 epochs,0.6360228061676025
translation,154,119,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,154,120,hyperparameters,models,trained,up to 20 epochs,models trained up to 20 epochs,0.7510947585105896
translation,154,120,hyperparameters,up to 20 epochs,with,same batch size 64,up to 20 epochs with same batch size 64,0.6243831515312195
translation,154,120,hyperparameters,up to 20 epochs,with,hidden size 512,up to 20 epochs with hidden size 512,0.6309229731559753
translation,154,120,hyperparameters,hyperparameters,trained,up to 20 epochs,hyperparameters trained up to 20 epochs,0.6819154620170593
translation,154,120,hyperparameters,hyperparameters,has,models,hyperparameters has models,0.5447477102279663
translation,154,121,hyperparameters,padded and truncated,to,same length 14,padded and truncated to same length 14,0.5649883151054382
translation,154,121,hyperparameters,each image,has,"? ? [ 10 , 100 ]","each image has ? ? [ 10 , 100 ]",0.5664466619491577
translation,154,121,hyperparameters,each image,has,object regions,each image has object regions,0.5590805411338806
translation,154,121,hyperparameters,each image,has,padded and truncated,each image has padded and truncated,0.582930326461792
translation,154,121,hyperparameters,each image,has,same length 14,each image has same length 14,0.6066566705703735
translation,154,121,hyperparameters,"? ? [ 10 , 100 ]",has,object regions,"? ? [ 10 , 100 ] has object regions",0.5220500230789185
translation,154,121,hyperparameters,hyperparameters,has,each image,hyperparameters has each image,0.541347324848175
translation,154,6,model,relations,between,objects,relations between objects,0.6129071116447449
translation,154,6,model,relations,propose,novel dual channel graph convolutional network ( dc - gcn ),relations propose novel dual channel graph convolutional network ( dc - gcn ),0.624833881855011
translation,154,6,model,objects,in,image,objects in image,0.5408445596694946
translation,154,6,model,syntactic dependency relations,between,words,syntactic dependency relations between words,0.5898933410644531
translation,154,6,model,words,in,question,words in question,0.5680095553398132
translation,154,6,model,model,To simultaneously capture,relations,model To simultaneously capture relations,0.6981332302093506
translation,154,6,model,model,To simultaneously capture,syntactic dependency relations,model To simultaneously capture syntactic dependency relations,0.6182246804237366
translation,154,6,model,model,propose,novel dual channel graph convolutional network ( dc - gcn ),model propose novel dual channel graph convolutional network ( dc - gcn ),0.6329209804534912
translation,154,7,model,dc - gcn model,consists of,three parts,dc - gcn model consists of three parts,0.6878821849822998
translation,154,7,model,i -gcn module,to capture,relations,i -gcn module to capture relations,0.7015281915664673
translation,154,7,model,i -gcn module,to capture,syntactic dependency relations,i -gcn module to capture syntactic dependency relations,0.6289977431297302
translation,154,7,model,relations,between,objects,relations between objects,0.6129071116447449
translation,154,7,model,objects,in,image,objects in image,0.5408445596694946
translation,154,7,model,q- gcn module,to capture,syntactic dependency relations,q- gcn module to capture syntactic dependency relations,0.6416581869125366
translation,154,7,model,syntactic dependency relations,words in,question,syntactic dependency relations words in question,0.7601073980331421
translation,154,7,model,attention alignment module,to align,image representations and question representations,attention alignment module to align image representations and question representations,0.6446966528892517
translation,154,7,model,three parts,has,i -gcn module,three parts has i -gcn module,0.5621570944786072
translation,154,7,model,three parts,has,q- gcn module,three parts has q- gcn module,0.5898573398590088
translation,154,7,model,model,has,dc - gcn model,model has dc - gcn model,0.5508164167404175
translation,154,23,model,dual channel graph convolutional network ( dc - gcn ),to simultaneously capture,relations,dual channel graph convolutional network ( dc - gcn ) to simultaneously capture relations,0.7161063551902771
translation,154,23,model,dual channel graph convolutional network ( dc - gcn ),to simultaneously capture,syntactic dependency relations,dual channel graph convolutional network ( dc - gcn ) to simultaneously capture syntactic dependency relations,0.6428764462471008
translation,154,23,model,relations,between,objects,relations between objects,0.6129071116447449
translation,154,23,model,objects,in,image,objects in image,0.5408445596694946
translation,154,23,model,syntactic dependency relations,between,words,syntactic dependency relations between words,0.5898933410644531
translation,154,23,model,words,in,question,words in question,0.5680095553398132
translation,154,23,model,model,propose,dual channel graph convolutional network ( dc - gcn ),model propose dual channel graph convolutional network ( dc - gcn ),0.6478261351585388
translation,154,24,model,dc - gcn model,consists of,image - gcn ( i - gcn ) module,dc - gcn model consists of image - gcn ( i - gcn ) module,0.6648523807525635
translation,154,24,model,dc - gcn model,consists of,question gcn ( q- gcn ) module,dc - gcn model consists of question gcn ( q- gcn ) module,0.6598300933837891
translation,154,24,model,dc - gcn model,consists of,attention alignment module,dc - gcn model consists of attention alignment module,0.6117085814476013
translation,154,25,model,i-gcn module,captures,relations,i-gcn module captures relations,0.7112776637077332
translation,154,25,model,i-gcn module,captures,syntactic dependency relations,i-gcn module captures syntactic dependency relations,0.6851422190666199
translation,154,25,model,relations,between,objects,relations between objects,0.6129071116447449
translation,154,25,model,objects,in,image,objects in image,0.5408445596694946
translation,154,25,model,q-gcn module,captures,syntactic dependency relations,q-gcn module captures syntactic dependency relations,0.692728579044342
translation,154,25,model,syntactic dependency relations,between,words,syntactic dependency relations between words,0.5898933410644531
translation,154,25,model,words,in,question,words in question,0.5680095553398132
translation,154,25,model,attention alignment module,to,align,attention alignment module to align,0.5566372275352478
translation,154,25,model,two representations,of,image and question,two representations of image and question,0.6180106401443481
translation,154,25,model,align,has,two representations,align has two representations,0.5784949660301208
translation,154,25,model,model,has,i-gcn module,model has i-gcn module,0.5673522353172302
translation,154,122,model,attention alignment module,are,4,attention alignment module are 4,0.5611392259597778
translation,154,122,model,model,levels of,stacked layer l,model levels of stacked layer l,0.626396119594574
translation,154,131,results,our model,increases,overall accuracy,our model increases overall accuracy,0.725658655166626
translation,154,131,results,overall accuracy,of,dfaf and mcan,overall accuracy of dfaf and mcan,0.5815526843070984
translation,154,131,results,dfaf and mcan,by,1.2 % and 0.6 %,dfaf and mcan by 1.2 % and 0.6 %,0.572073757648468
translation,154,131,results,1.2 % and 0.6 %,on,test- std set,1.2 % and 0.6 % on test- std set,0.5764724612236023
translation,154,131,results,results,has,our model,results has our model,0.5871725678443909
translation,154,134,results,dc - gcn,achieves,outstanding performance,dc - gcn achieves outstanding performance,0.6890607476234436
translation,154,134,results,outstanding performance,on,vqa - v2 dataset,outstanding performance on vqa - v2 dataset,0.494253009557724
translation,154,134,results,results,has,dc - gcn,results has dc - gcn,0.5255340337753296
translation,154,139,results,dc - gcn model,surpasses,murel and regat,dc - gcn model surpasses murel and regat,0.6794912815093994
translation,154,139,results,murel and regat,on,vqa -,murel and regat on vqa -,0.6808110475540161
translation,154,139,results,results,has,dc - gcn model,results has dc - gcn model,0.5123844742774963
translation,155,194,ablation-analysis,combination weight,used in,m -net,combination weight used in m -net,0.6701640486717224
translation,155,194,ablation-analysis,combination weight,plays,important role,combination weight plays important role,0.727595329284668
translation,155,194,ablation-analysis,important role,in producing,high quality word embedding,important role in producing high quality word embedding,0.6255374550819397
translation,155,194,ablation-analysis,ablation analysis,has,combination weight,ablation analysis has combination weight,0.5111435055732727
translation,155,204,hyperparameters,dimension,of,word embedding,dimension of word embedding,0.5654559135437012
translation,155,204,hyperparameters,word embedding,set as,"50,100 and 300","word embedding set as 50,100 and 300",0.6385384798049927
translation,155,204,hyperparameters,hyperparameters,has,dimension,hyperparameters has dimension,0.514893651008606
translation,155,8,model,continuous word embeddings,with,metadata,continuous word embeddings with metadata,0.6398778557777405
translation,155,8,model,metadata,of,category information,metadata of category information,0.5419465899467468
translation,155,8,model,category information,within,cqa pages,category information within cqa pages,0.6676608920097351
translation,155,8,model,cqa pages,for,question retrieval,cqa pages for question retrieval,0.6135466694831848
translation,155,8,model,model,propose to learn,continuous word embeddings,model propose to learn continuous word embeddings,0.7428198456764221
translation,155,9,model,variable size,of,word embedding vectors,variable size of word embedding vectors,0.5933058857917786
translation,155,9,model,variable size,employ,framework of fisher kernel,variable size employ framework of fisher kernel,0.5401924848556519
translation,155,9,model,framework of fisher kernel,to aggregated them into,fixedlength vectors,framework of fisher kernel to aggregated them into fixedlength vectors,0.6231006979942322
translation,155,9,model,model,To deal with,variable size,model To deal with variable size,0.675722062587738
translation,155,9,model,model,employ,framework of fisher kernel,model employ framework of fisher kernel,0.47997960448265076
translation,155,32,model,embedding of words,in,continuous space,embedding of words in continuous space,0.5746347308158875
translation,155,32,model,continuous space,for,question representations,continuous space for question representations,0.6733624339103699
translation,155,32,model,model,incorporate,embedding of words,model incorporate embedding of words,0.7007424831390381
translation,155,33,model,words,in,question,words in question,0.5680095553398132
translation,155,33,model,words,into,continuous vector representations,words into continuous vector representations,0.5537655353546143
translation,155,33,model,continuous vector representations,by looking up,tables,continuous vector representations by looking up tables,0.6414282321929932
translation,155,33,model,model,firstly transform,words,model firstly transform words,0.6811532378196716
translation,155,44,model,question,as,bag-of-embeddedwords ( boew ),question as bag-of-embeddedwords ( boew ),0.5039578080177307
translation,155,44,model,bag-of-embeddedwords ( boew ),in,continuous space,bag-of-embeddedwords ( boew ) in continuous space,0.5164176821708679
translation,155,44,model,novel method,to aggregate,variable - cardinality boew,novel method to aggregate variable - cardinality boew,0.7304694652557373
translation,155,44,model,variable - cardinality boew,into,fixed - length vector,variable - cardinality boew into fixed - length vector,0.5813679695129395
translation,155,44,model,fixed - length vector,by using,fk,fixed - length vector by using fk,0.6908471584320068
translation,155,44,model,model,represent,question,model represent question,0.6269286274909973
translation,155,44,model,model,introduce,novel method,model introduce novel method,0.6845430135726929
translation,155,85,model,skip-gram model,for learning,word embeddings,skip-gram model for learning word embeddings,0.6905412077903748
translation,155,85,model,model,consider,context - aware predicting model,model consider context - aware predicting model,0.654006838798523
translation,155,214,results,continuous word embedding representations,for,question retrieval,continuous word embedding representations for question retrieval,0.5430812835693359
translation,155,214,results,topic-based approaches,on,all evaluation metrics,topic-based approaches on all evaluation metrics,0.46993422508239746
translation,155,214,results,learning,has,continuous word embedding representations,learning has continuous word embedding representations,0.46129781007766724
translation,155,214,results,outperform,has,translation - based approaches,outperform has translation - based approaches,0.5871046185493469
translation,155,214,results,outperform,has,topic-based approaches,outperform has topic-based approaches,0.587924063205719
translation,155,214,results,results,see that,learning,results see that learning,0.6630239486694336
translation,155,219,results,improvements,between,proposed m-net + fv,improvements between proposed m-net + fv,0.6463180184364319
translation,155,219,results,improvements,between,two groups of compared methods,improvements between two groups of compared methods,0.6639097332954407
translation,155,219,results,improvements,between,skip-gram + fv,improvements between skip-gram + fv,0.5894036293029785
translation,155,219,results,improvements,between,skip-gram + fv,improvements between skip-gram + fv,0.5894036293029785
translation,155,219,results,two groups of compared methods,are,statistically significant ( p < 0.05 ),two groups of compared methods are statistically significant ( p < 0.05 ),0.5535614490509033
translation,155,219,results,improvements,between,skip-gram + fv,improvements between skip-gram + fv,0.5894036293029785
translation,155,219,results,improvements,between,translation - based approaches,improvements between translation - based approaches,0.6621012687683105
translation,155,219,results,improvements,are,mildly significant ( p < 0.08 ),improvements are mildly significant ( p < 0.08 ),0.5705558061599731
translation,155,219,results,translation - based approaches,are,mildly significant ( p < 0.08 ),translation - based approaches are mildly significant ( p < 0.08 ),0.5534250736236572
translation,155,219,results,results,show that,improvements,results show that improvements,0.5038777589797974
translation,155,220,results,metadata of category information powered model ( m- net + fv ),yields,largest improvements,metadata of category information powered model ( m- net + fv ) yields largest improvements,0.7154788970947266
translation,155,220,results,metadata of category information powered model ( m- net + fv ),has,outperforms,metadata of category information powered model ( m- net + fv ) has outperforms,0.6079750657081604
translation,155,220,results,outperforms,has,baseline skip-gram model ( skip-gram + fv ),outperforms has baseline skip-gram model ( skip-gram + fv ),0.5903468728065491
translation,155,220,results,results,has,metadata of category information powered model ( m- net + fv ),results has metadata of category information powered model ( m- net + fv ),0.5522493124008179
translation,155,221,results,metadata powered word embedding,of,higher quality,metadata powered word embedding of higher quality,0.5728216767311096
translation,155,221,results,higher quality,than,baseline model,higher quality than baseline model,0.6104992628097534
translation,155,221,results,baseline model,with no,metadata information regularization,baseline model with no metadata information regularization,0.687420666217804
translation,155,221,results,results,imply that,metadata powered word embedding,results imply that metadata powered word embedding,0.281523734331131
translation,155,222,results,higher dimension,brings,more improvements,higher dimension brings more improvements,0.661882758140564
translation,155,222,results,more improvements,for,question retrieval task,more improvements for question retrieval task,0.6023129224777222
translation,155,222,results,results,note,higher dimension,results note higher dimension,0.6374805569648743
translation,155,222,results,results,setting,higher dimension,results setting higher dimension,0.5275543928146362
translation,155,223,results,translation - based methods,has,significantly outperform,translation - based methods has significantly outperform,0.5760930776596069
translation,155,223,results,significantly outperform,has,lm,significantly outperform has lm,0.6240837574005127
translation,155,223,results,results,has,translation - based methods,results has translation - based methods,0.5120702981948853
translation,155,224,results,phrase - based translation model,is,more effective,phrase - based translation model is more effective,0.5006891489028931
translation,155,224,results,results,note,phrase - based translation model,results note phrase - based translation model,0.5593270659446716
translation,155,227,results,topic- based models,achieve,comparable performance,topic- based models achieve comparable performance,0.6210605502128601
translation,155,227,results,topic- based models,perform better than,lm,topic- based models perform better than lm,0.677750289440155
translation,155,227,results,comparable performance,with,translation - based models,comparable performance with translation - based models,0.6590587496757507
translation,155,227,results,both data sets,has,topic- based models,both data sets has topic- based models,0.5713656544685364
translation,155,227,results,results,On,both data sets,results On both data sets,0.5011186599731445
translation,156,108,experiments,maxent - 2 c,very skewed towards,majority class,maxent - 2 c very skewed towards majority class,0.7449172139167786
translation,156,108,experiments,maxent - 2 c,performs,better,maxent - 2 c performs better,0.6871182918548584
translation,156,108,experiments,better,due to,class imbalance,better due to class imbalance,0.6795361638069153
translation,156,112,experiments,best systems,at,semeval - 2015 task 3,best systems at semeval - 2015 task 3,0.5055894255638123
translation,156,6,model,output structure,at,thread level,output structure at thread level,0.558161199092865
translation,156,6,model,output structure,to make,more consistent global decisions,output structure to make more consistent global decisions,0.6584802269935608
translation,156,6,model,model,exploiting,output structure,model exploiting output structure,0.6876058578491211
translation,156,7,model,relations,between,pairs,relations between pairs,0.6863381266593933
translation,156,7,model,relations,incorporate in,graph - cut,relations incorporate in graph - cut,0.6374505758285522
translation,156,7,model,relations,incorporate in,ilp frameworks,relations incorporate in ilp frameworks,0.6639777421951294
translation,156,7,model,pairs,of,comments,pairs of comments,0.6562680602073669
translation,156,7,model,model,exploit,relations,model exploit relations,0.7404184341430664
translation,156,33,model,model,based on,idea,model based on idea,0.7041103839874268
translation,156,33,model,model,similar comments should have,similar labels,model similar comments should have similar labels,0.6010037064552307
translation,156,33,model,idea,similar comments should have,similar labels,idea similar comments should have similar labels,0.6523867845535278
translation,156,33,model,model,propose,model,model propose model,0.6740307211875916
translation,156,36,results,strong baseline,performing,local comment - based classifications,strong baseline performing local comment - based classifications,0.6674923300743103
translation,156,36,results,significantly improve,has,strong baseline,significantly improve has strong baseline,0.6221399307250977
translation,156,36,results,results,has,global inference models,results has global inference models,0.502005934715271
translation,156,106,results,works better,than,three - class maxent - 3c,works better than three - class maxent - 3c,0.5927748084068298
translation,156,106,results,twoclass maxent - 2c classifier,has,works better,twoclass maxent - 2c classifier has works better,0.5770304203033447
translation,156,106,results,results,see that,twoclass maxent - 2c classifier,results see that twoclass maxent - 2c classifier,0.6027096509933472
translation,156,107,results,maxent - 3c,loses in,f 1 and accuracy,maxent - 3c loses in f 1 and accuracy,0.7216059565544128
translation,156,107,results,maxent - 3c,has,more balanced p and r,maxent - 3c has more balanced p and r,0.6000425219535828
translation,156,107,results,results,has,maxent - 3c,results has maxent - 3c,0.5940804481506348
translation,156,113,results,our maxent classifier,is,competitive,our maxent classifier is competitive,0.561984658241272
translation,156,113,results,competitive,shows,higher accuracy,competitive shows higher accuracy,0.6869065165519714
translation,156,113,results,higher accuracy,than,two,higher accuracy than two,0.6524136066436768
translation,156,113,results,results,see that,our maxent classifier,results see that our maxent classifier,0.5952950119972229
translation,156,116,results,crf model,worse than,maxent,crf model worse than maxent,0.6705669164657593
translation,156,116,results,maxent,on,all measures,maxent on all measures,0.5802788734436035
translation,156,116,results,results,has,crf model,results has crf model,0.5351728796958923
translation,156,118,results,global inference,with,graph- cut and ilp,global inference with graph- cut and ilp,0.6424307227134705
translation,156,118,results,graph- cut and ilp,improves,both f 1 and accuracy,graph- cut and ilp improves both f 1 and accuracy,0.6874575018882751
translation,156,118,results,results,has,global inference,results has global inference,0.5145558714866638
translation,156,119,results,graph- cut,works,better,graph- cut works better,0.6341478824615479
translation,156,119,results,better,than,ilp,better than ilp,0.6406998634338379
translation,156,119,results,higher precision,helps,f 1 and accuracy,higher precision helps f 1 and accuracy,0.6325602531433105
translation,156,119,results,results,has,graph- cut,results has graph- cut,0.546740710735321
translation,156,120,results,statistically significant improvements,over,maxent classifier,statistically significant improvements over maxent classifier,0.6700067520141602
translation,156,120,results,improve,over,state - of- the - art jaist system,improve over state - of- the - art jaist system,0.6647904515266418
translation,156,120,results,results,yield,statistically significant improvements,results yield statistically significant improvements,0.7287073731422424
translation,156,122,results,predictions,of,maxent - 2 c,predictions of maxent - 2 c,0.6518275141716003
translation,156,122,results,maxent - 2 c,in,global classifiers,maxent - 2 c in global classifiers,0.5388855338096619
translation,156,122,results,maxent - 2 c,is,better,maxent - 2 c is better,0.631253182888031
translation,156,122,results,better,than using,maxent - 3c,better than using maxent - 3c,0.7622682452201843
translation,156,122,results,better,from,maxent - 3c,better from maxent - 3c,0.6205918192863464
translation,156,122,results,results,using,predictions,results using predictions,0.6208823919296265
translation,157,189,ablation-analysis,accuracy,observed to,degrade,accuracy observed to degrade,0.6762438416481018
translation,157,189,ablation-analysis,accuracy,comes close to,random decision performance,accuracy comes close to random decision performance,0.7289713621139526
translation,157,189,ablation-analysis,all types of systems,has,accuracy,all types of systems has accuracy,0.6285656690597534
translation,157,189,ablation-analysis,ablation analysis,for,all types of systems,ablation analysis for all types of systems,0.6105982661247253
translation,157,128,experimental-setup,deeplearning4 j 9 toolkit,for creating,initial word representations,deeplearning4 j 9 toolkit for creating initial word representations,0.6496865749359131
translation,157,128,experimental-setup,experimental setup,has,deeplearning4 j 9 toolkit,experimental setup has deeplearning4 j 9 toolkit,0.5158489346504211
translation,157,162,experimental-setup,vectorial representation,uses,embedding layer,vectorial representation uses embedding layer,0.5759401917457581
translation,157,162,experimental-setup,embedding layer,of,300 randomly initiated neurons,embedding layer of 300 randomly initiated neurons,0.545011579990387
translation,157,162,experimental-setup,300 randomly initiated neurons,with,uniform distribution,300 randomly initiated neurons with uniform distribution,0.6205610036849976
translation,157,162,experimental-setup,experimental setup,has,vectorial representation,experimental setup has vectorial representation,0.5277241468429565
translation,157,163,experimental-setup,convolution layer,uses,300 neurons,convolution layer uses 300 neurons,0.5543407797813416
translation,157,163,experimental-setup,300 neurons,for,output of filters,300 neurons for output of filters,0.5990986824035645
translation,157,163,experimental-setup,output of filters,with,kernel size,output of filters with kernel size,0.622980535030365
translation,157,163,experimental-setup,kernel size,of,15 units,kernel size of 15 units,0.6365216374397278
translation,157,163,experimental-setup,each deep layer,has,50 neurons,each deep layer has 50 neurons,0.5197990536689758
translation,157,163,experimental-setup,experimental setup,has,convolution layer,experimental setup has convolution layer,0.525818943977356
translation,157,120,model,third system,adopts,hybrid architecture,third system adopts hybrid architecture,0.683580756187439
translation,157,120,model,hybrid architecture,combining,key ingredients,hybrid architecture combining key ingredients,0.7904516458511353
translation,157,120,model,model,has,third system,model has third system,0.5885496735572815
translation,157,133,results,system,performs,73.40 % accuracy,system performs 73.40 % accuracy,0.596327543258667
translation,157,133,results,askubuntutb,has,system,askubuntutb has system,0.6527159214019775
translation,157,153,results,best accuracy,obtained with,askubuntutb,best accuracy obtained with askubuntutb,0.6374461650848389
translation,157,153,results,slight drop,with,askubuntuto,slight drop with askubuntuto,0.6996846795082092
translation,157,153,results,askubuntutb,has,78.65 %,askubuntutb has 78.65 %,0.5903947353363037
translation,157,153,results,results,has,best accuracy,results has best accuracy,0.5664627552032471
translation,157,164,results,resulting dqd resolver,scores,better,resulting dqd resolver scores better,0.680353045463562
translation,157,164,results,better,over,askubun -tuto,better over askubun -tuto,0.7291876673698425
translation,157,164,results,better,over,askubuntutb,better over askubuntutb,0.7132419943809509
translation,157,164,results,better,scoring,79.67 %,better scoring 79.67 %,0.6783519983291626
translation,157,164,results,askubun -tuto,scoring,79.67 %,askubun -tuto scoring 79.67 %,0.6328200697898865
translation,157,164,results,79.67 %,than over,askubuntutb,79.67 % than over askubuntutb,0.6105918884277344
translation,157,172,results,segments,from,84 words per segment,segments from 84 words per segment,0.5530985593795776
translation,157,172,results,segments,has,overall negative impact,segments has overall negative impact,0.6322395205497742
translation,157,172,results,84 words per segment,with,askubun - tutb,84 words per segment with askubun - tutb,0.6691357493400574
translation,157,172,results,askubun - tutb,to,8 or 10 words,askubun - tutb to 8 or 10 words,0.6073771119117737
translation,157,172,results,8 or 10 words,with,askubuntuto or quora,8 or 10 words with askubuntuto or quora,0.6451120376586914
translation,157,172,results,overall negative impact,on,accuracy,overall negative impact on accuracy,0.5451992154121399
translation,157,172,results,accuracy,of,systems,accuracy of systems,0.6223611235618591
translation,157,172,results,accuracy,except for,dcnn,accuracy except for dcnn,0.6204750537872314
translation,157,179,results,non nn - based solutions,that,more robust,non nn - based solutions that more robust,0.6351026296615601
translation,157,179,results,non nn - based solutions,appear as,more robust,non nn - based solutions appear as more robust,0.5977299809455872
translation,157,179,results,more robust,to,generalization,more robust to generalization,0.5509598255157471
translation,157,179,results,of the domain,than,nn - based ones,of the domain than nn - based ones,0.5832713842391968
translation,157,179,results,ones,with,specific domain,ones with specific domain,0.6677880883216858
translation,157,179,results,results,is,non nn - based solutions,results is non nn - based solutions,0.5982227921485901
translation,157,180,results,cnn approach,offers,worst result,cnn approach offers worst result,0.6532410383224487
translation,157,180,results,generic domain,has,cnn approach,generic domain has cnn approach,0.5713104009628296
translation,157,180,results,results,for,generic domain,results for generic domain,0.6243454813957214
translation,157,181,results,dnn,overcomes,best svm approach,dnn overcomes best svm approach,0.7297168374061584
translation,157,181,results,best svm approach,less than,2 points,best svm approach less than 2 points,0.6464318037033081
translation,157,181,results,results,has,dnn,results has dnn,0.5851234793663025
translation,157,182,results,dcnn,overcomes,overall second - best,dcnn overcomes overall second - best,0.6493064761161804
translation,157,182,results,dcnn,by,modest margin,dcnn by modest margin,0.5920550227165222
translation,157,182,results,results,has,dcnn,results has dcnn,0.5687397718429565
translation,157,183,results,rule- based approach,is one of,two second best,rule- based approach is one of two second best,0.6980561017990112
translation,157,183,results,general domain,has,rule- based approach,general domain has rule- based approach,0.5981558561325073
translation,157,183,results,results,for,general domain,results for general domain,0.6286841630935669
translation,157,187,results,accuracies,of,57.63 %,accuracies of 57.63 %,0.5599658489227295
translation,157,187,results,accuracies,of,53.50 %,accuracies of 53.50 %,0.5601826310157776
translation,157,187,results,accuracies,of,56.42 %,accuracies of 56.42 %,0.5648872256278992
translation,157,187,results,dropping,has,almost 15 points,dropping has almost 15 points,0.6345459818840027
translation,157,187,results,53.50 %,has,dropping over 17 points,53.50 % has dropping over 17 points,0.5797099471092224
translation,157,187,results,56.42 %,has,dropping over 15 points,56.42 % has dropping over 15 points,0.5818479657173157
translation,157,187,results,results,has,"rule- based , the advanced svm and the dcnn","results has rule- based , the advanced svm and the dcnn",0.5526585578918457
translation,157,211,results,range of neural network architectures,experimented with,convoluted to deep networks,range of neural network architectures experimented with convoluted to deep networks,0.7062864303588867
translation,157,211,results,range of neural network architectures,from,convoluted to deep networks,range of neural network architectures from convoluted to deep networks,0.5650848150253296
translation,157,212,results,novel neural network architecture,propose,best performance,novel neural network architecture propose best performance,0.6771549582481384
translation,157,212,results,novel neural network architecture,presents,best performance,novel neural network architecture presents best performance,0.6991039514541626
translation,157,212,results,best performance,of,all resolvers tested,best performance of all resolvers tested,0.5686466097831726
translation,157,212,results,results,has,novel neural network architecture,results has novel neural network architecture,0.5711997747421265
translation,158,19,baselines,mmr framework,for,relevant sentence selection,mmr framework for relevant sentence selection,0.5630456805229187
translation,158,19,baselines,relevant sentence selection,from,chosen snippets,relevant sentence selection from chosen snippets,0.5238679051399231
translation,158,5,experiments,question types,including,factoid,question types including factoid,0.7105330228805542
translation,158,5,experiments,question types,including,list based,question types including list based,0.7464417815208435
translation,158,5,experiments,question types,including,summary,question types including summary,0.750139057636261
translation,158,5,experiments,question types,including,yes / no type questions,question types including yes / no type questions,0.6778762340545654
translation,158,7,model,novel natural language inference ( nli ) based framework,to answer,yes / no questions,novel natural language inference ( nli ) based framework to answer yes / no questions,0.7124884724617004
translation,158,7,model,model,propose,novel natural language inference ( nli ) based framework,model propose novel natural language inference ( nli ) based framework,0.6492691040039062
translation,158,16,model,novel embedding projection technique,allows for,effective transfer learning,novel embedding projection technique allows for effective transfer learning,0.7094890475273132
translation,158,16,model,from models,trained on,larger datasets,from models trained on larger datasets,0.7586933970451355
translation,158,16,model,larger datasets,with,different vocabulary,larger datasets with different vocabulary,0.625505268573761
translation,158,16,model,different vocabulary,to work well,much smaller bioasq dataset,different vocabulary to work well much smaller bioasq dataset,0.7069379687309265
translation,158,16,model,effective transfer learning,has,from models,effective transfer learning has from models,0.60527104139328
translation,158,16,model,model,introduce,novel embedding projection technique,model introduce novel embedding projection technique,0.6455941200256348
translation,158,95,model,assertions,from,questions,assertions from questions,0.6061044931411743
translation,158,95,model,assertions,evaluate,entailment or contradiction,assertions evaluate entailment or contradiction,0.670976459980011
translation,158,95,model,entailment or contradiction,using,recognizing textual entailment ( rte ) model,entailment or contradiction using recognizing textual entailment ( rte ) model,0.6968676447868347
translation,158,95,model,model,generate,assertions,model generate assertions,0.6921801567077637
translation,158,187,model,exact answers,incorporate,neural entailment models,exact answers incorporate neural entailment models,0.5743799805641174
translation,158,187,model,exact answers,employ,letor ranking models,exact answers employ letor ranking models,0.5243484377861023
translation,158,187,model,neural entailment models,along with,novel embedding transformation technique,neural entailment models along with novel embedding transformation technique,0.6251784563064575
translation,158,187,model,novel embedding transformation technique,for answering,yes / no questions,novel embedding transformation technique for answering yes / no questions,0.7528467774391174
translation,158,187,model,letor ranking models,to answer,factoid / list based questions,letor ranking models to answer factoid / list based questions,0.6754913330078125
translation,158,187,model,model,For,exact answers,model For exact answers,0.6243025660514832
translation,158,187,model,model,employ,letor ranking models,model employ letor ranking models,0.5548791885375977
translation,158,6,results,summarytype questions,combine,effective irbased techniques,summarytype questions combine effective irbased techniques,0.6588699817657471
translation,158,6,results,summarytype questions,combine,diversification,summarytype questions combine diversification,0.7444040179252625
translation,158,6,results,effective irbased techniques,for,retrieval,effective irbased techniques for retrieval,0.6374837160110474
translation,158,6,results,diversification,of,relevant snippets,diversification of relevant snippets,0.6213054656982422
translation,158,6,results,relevant snippets,for,question,relevant snippets for question,0.6178169846534729
translation,158,6,results,question,to create,end-to - end system,question to create end-to - end system,0.7062970995903015
translation,158,6,results,end-to - end system,achieves,rouge - 2 score,end-to - end system achieves rouge - 2 score,0.6904823184013367
translation,158,6,results,end-to - end system,achieves,rouge - su4 score,end-to - end system achieves rouge - su4 score,0.6903488636016846
translation,158,6,results,end-to - end system,on,ideal answer questions,end-to - end system on ideal answer questions,0.554962694644928
translation,158,6,results,rouge - 2 score,of,0.72,rouge - 2 score of 0.72,0.5226050615310669
translation,158,6,results,rouge - 2 score,of,0.71,rouge - 2 score of 0.71,0.5278367400169373
translation,158,6,results,rouge - su4 score,of,0.71,rouge - su4 score of 0.71,0.5430508255958557
translation,158,6,results,0.71,on,ideal answer questions,0.71 on ideal answer questions,0.47986704111099243
translation,158,6,results,results,For,summarytype questions,results For summarytype questions,0.5934252738952637
translation,158,183,results,"all the n-grams ( n = 1 , 2 , 3 , 4 )",from,snippets,"all the n-grams ( n = 1 , 2 , 3 , 4 ) from snippets",0.5728135704994202
translation,158,183,results,"all the n-grams ( n = 1 , 2 , 3 , 4 )",using,ner tags,"all the n-grams ( n = 1 , 2 , 3 , 4 ) using ner tags",0.6733359694480896
translation,158,183,results,snippets,as,candidate answers,snippets as candidate answers,0.521267294883728
translation,158,183,results,ner tags,as,letor features,ner tags as letor features,0.47604280710220337
translation,158,183,results,improved,to,mrr,improved to mrr,0.5945411324501038
translation,158,183,results,mrr,of,0.195,mrr of 0.195,0.5891175270080566
translation,158,183,results,mrr,of,0.234,mrr of 0.234,0.5638391375541687
translation,158,183,results,mrr,of,0.234,mrr of 0.234,0.5638391375541687
translation,158,183,results,0.195,on,factoid type questions,0.195 on factoid type questions,0.5075370669364929
translation,158,183,results,f1 score,of,0.234,f1 score of 0.234,0.5304014086723328
translation,158,183,results,0.234,on,list type questions,0.234 on list type questions,0.4446508586406708
translation,158,183,results,"all the n-grams ( n = 1 , 2 , 3 , 4 )",has,performance,"all the n-grams ( n = 1 , 2 , 3 , 4 ) has performance",0.565648078918457
translation,158,183,results,ner tags,has,performance,ner tags has performance,0.5656818151473999
translation,158,183,results,letor features,has,performance,letor features has performance,0.6106477975845337
translation,158,183,results,results,having,"all the n-grams ( n = 1 , 2 , 3 , 4 )","results having all the n-grams ( n = 1 , 2 , 3 , 4 )",0.5722359418869019
translation,159,24,model,two models,to predict,information need,two models to predict information need,0.7321971654891968
translation,159,24,model,information need,based on,query question,information need based on query question,0.6192415952682495
translation,159,24,model,model,propose,two models,model propose two models,0.6457875967025757
translation,159,24,model,model,to predict,information need,model to predict information need,0.729072630405426
translation,159,76,model,information need prediction method,based on,statistical machine translation model,information need prediction method based on statistical machine translation model,0.6269717216491699
translation,159,76,model,model,propose,information need prediction method,model propose information need prediction method,0.6090661287307739
translation,159,100,model,open source software afterthedeadline,to automatically correct,spelling errors,open source software afterthedeadline to automatically correct spelling errors,0.7227804064750671
translation,159,100,model,spelling errors,in,question and information need texts,spelling errors in question and information need texts,0.5152026414871216
translation,159,100,model,model,use,open source software afterthedeadline,model use open source software afterthedeadline,0.6632229089736938
translation,159,136,results,tfidf and lda1 methods,perform,better,tfidf and lda1 methods perform better,0.591388463973999
translation,159,136,results,better,for,recommending questions,better for recommending questions,0.6455586552619934
translation,159,136,results,recommending questions,than,others,recommending questions than others,0.6259704232215881
translation,159,136,results,results,show,tfidf and lda1 methods,results show tfidf and lda1 methods,0.5692558288574219
translation,159,139,results,top recommended questions ' information needs,share,less common words,top recommended questions ' information needs share less common words,0.6841249465942383
translation,159,139,results,less common words,with,query question 's,less common words with query question 's,0.6492354273796082
translation,159,139,results,top recommended questions,span,wider topics,top recommended questions span wider topics,0.719463586807251
translation,159,139,results,l-da1 method,has,outperforms,l-da1 method has outperforms,0.6096122860908508
translation,159,139,results,outperforms,has,tfidf method,outperforms has tfidf method,0.5928651690483093
translation,159,139,results,results,has,l-da1 method,results has l-da1 method,0.5117524862289429
translation,159,141,results,knowledge - based methods,shown to perform,worse,knowledge - based methods shown to perform worse,0.6676380634307861
translation,159,141,results,worse,than,tfidf and lda1,worse than tfidf and lda1,0.6159206628799438
translation,159,141,results,results,has,knowledge - based methods,results has knowledge - based methods,0.5172243714332581
translation,159,144,results,mean reciprocal rank score,for,tfidf and lda1,mean reciprocal rank score for tfidf and lda1,0.6281596422195435
translation,159,144,results,tfidf and lda1,are,more than 80 %,tfidf and lda1 are more than 80 %,0.5738573670387268
translation,159,144,results,results,has,mean reciprocal rank score,results has mean reciprocal rank score,0.53629070520401
translation,159,170,results,performance,of,most similarity methods,performance of most similarity methods,0.5607825517654419
translation,159,170,results,improved,by making use of,information need predic-tion,improved by making use of information need predic-tion,0.6387872099876404
translation,159,170,results,results,see that,performance,results see that performance,0.6803712844848633
translation,159,171,results,different similarity measures,received,different degrees of improvement,different similarity measures received different degrees of improvement,0.46326547861099243
translation,159,171,results,results,has,different similarity measures,results has different similarity measures,0.5541933178901672
translation,159,172,results,lda1,obtained,highest improvement,lda1 obtained highest improvement,0.6387832760810852
translation,159,172,results,highest improvement,followed by,tfidf based method,highest improvement followed by tfidf based method,0.6553944945335388
translation,159,172,results,results,has,lda1,results has lda1,0.591627299785614
translation,159,182,results,top five precision and the top ten precision scores,using,tfidf and lda1 methods,top five precision and the top ten precision scores using tfidf and lda1 methods,0.6444114446640015
translation,159,182,results,top five precision and the top ten precision scores,received,different degrees of improvement,top five precision and the top ten precision scores received different degrees of improvement,0.4794186055660248
translation,159,182,results,results,has,top five precision and the top ten precision scores,results has top five precision and the top ten precision scores,0.574816882610321
translation,160,177,ablation-analysis,weights,for,mentionsubject matches,weights for mentionsubject matches,0.5846401453018188
translation,160,177,ablation-analysis,losses,for,mentionsubject matches,losses for mentionsubject matches,0.601150631904602
translation,160,177,ablation-analysis,mentionsubject matches,has,increase,mentionsubject matches has increase,0.5851998925209045
translation,160,177,ablation-analysis,mentionsubject matches,has,increase,mentionsubject matches has increase,0.5851998925209045
translation,160,186,ablation-analysis,subject accuracy,increases by,4.0 %,subject accuracy increases by 4.0 %,0.6983060240745544
translation,160,186,ablation-analysis,4.0 %,due to,subgraph ranking,4.0 % due to subgraph ranking,0.6724585294723511
translation,160,186,ablation-analysis,ablation analysis,has,subject accuracy,ablation analysis has subject accuracy,0.4977715313434601
translation,160,187,ablation-analysis,relation accuracy,increases by,7.8 %,relation accuracy increases by 7.8 %,0.7030931711196899
translation,160,187,ablation-analysis,ablation analysis,has,relation accuracy,ablation analysis has relation accuracy,0.5228925943374634
translation,160,147,experiments,subgraph selection,use,only unigrams,subgraph selection use only unigrams,0.6331143379211426
translation,160,147,experiments,subgraph selection,rank them by,proposed relevance score,subgraph selection rank them by proposed relevance score,0.6275299787521362
translation,160,147,experiments,only unigrams,of,tagged mention,only unigrams of tagged mention,0.5739514827728271
translation,160,147,experiments,tagged mention,to retrieve,candidate facts,tagged mention to retrieve candidate facts,0.6886347532272339
translation,160,147,experiments,proposed relevance score,with,tuned weight ? = 0.9,proposed relevance score with tuned weight ? = 0.9,0.6220579147338867
translation,160,239,experiments,0.001,has,0.01,0.001 has 0.01,0.517971396446228
translation,160,153,hyperparameters,optimizer,for,training,optimizer for training,0.6382174491882324
translation,160,153,hyperparameters,models,is,"adam ( kingma and ba , 2014 )","models is adam ( kingma and ba , 2014 )",0.565372109413147
translation,160,153,hyperparameters,training,has,models,training has models,0.5681478977203369
translation,160,153,hyperparameters,hyperparameters,has,optimizer,hyperparameters has optimizer,0.5107399225234985
translation,160,240,hyperparameters,64 32,# of epochs,50,64 32 # of epochs 50,0.5796737670898438
translation,160,240,hyperparameters,batch size,has,64 32,batch size has 64 32,0.6306912302970886
translation,160,240,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,160,41,model,wellorder loss,to improve,fact selection,wellorder loss to improve fact selection,0.7004417181015015
translation,160,41,model,model,propose,lowcomplexity joint-scoring cnn model,model propose lowcomplexity joint-scoring cnn model,0.6265347599983215
translation,160,42,model,subject matching and the relation matching,by learning,order - preserving scores,subject matching and the relation matching by learning order - preserving scores,0.6524617671966553
translation,160,42,model,weights,of,scores,weights of scores,0.5732411742210388
translation,160,42,model,model,couples,subject matching and the relation matching,model couples subject matching and the relation matching,0.7061514854431152
translation,160,222,model,joint-scoring approach,to improve,performance,joint-scoring approach to improve performance,0.7106831669807434
translation,160,222,model,performance,of,kbsqa,performance of kbsqa,0.6022911667823792
translation,160,222,model,model,propose,subgraph ranking method,model propose subgraph ranking method,0.6088570952415466
translation,160,224,model,jointscoring model,with,well - order loss,jointscoring model with well - order loss,0.6276872754096985
translation,160,224,model,jointscoring model,enforces,order of scores,jointscoring model enforces order of scores,0.7856776118278503
translation,160,224,model,well - order loss,couples,dependency,well - order loss couples dependency,0.7496852874755859
translation,160,224,model,dependency,has,of subject matching and relation matching,dependency has of subject matching and relation matching,0.5895586013793945
translation,160,224,model,model,has,jointscoring model,model has jointscoring model,0.5512787103652954
translation,160,43,results,better performance,than,previous state of the art,better performance than previous state of the art,0.5482900738716125
translation,160,43,results,better performance,surpassing,best baseline,better performance surpassing best baseline,0.7509273886680603
translation,160,43,results,previous state of the art,on,simplequestions dataset,previous state of the art on simplequestions dataset,0.4911685585975647
translation,160,43,results,best baseline,by,large margin,best baseline by large margin,0.5868273973464966
translation,160,43,results,better performance,has,85.44 % in accuracy,better performance has 85.44 % in accuracy,0.556542158126831
translation,160,43,results,results,achieve,better performance,results achieve better performance,0.6580345034599304
translation,160,163,results,literal score,used in,baseline,literal score used in baseline,0.6444039940834045
translation,160,163,results,literal score,used in,baseline,literal score used in baseline,0.6444039940834045
translation,160,163,results,literal score,performs,well,literal score performs well,0.6415815949440002
translation,160,163,results,literal score,using,semantic score,literal score using semantic score,0.6415674090385437
translation,160,163,results,does not outperform,has,baseline,does not outperform has baseline,0.6321004033088684
translation,160,163,results,results,see that,literal score,results see that literal score,0.6734156012535095
translation,160,165,results,outperforms,by,large margin,outperforms by large margin,0.6290680766105652
translation,160,165,results,baseline,by,large margin,baseline by large margin,0.6153011918067932
translation,160,165,results,literal score and semantic score,has,outperforms,literal score and semantic score has outperforms,0.6334651708602905
translation,160,165,results,outperforms,has,baseline,outperforms has baseline,0.6131853461265564
translation,160,165,results,results,combining,literal score and semantic score,results combining literal score and semantic score,0.6644912958145142
translation,160,166,results,ranking approach,surpasses,baseline,ranking approach surpasses baseline,0.5965849757194519
translation,160,166,results,baseline,by,"11.9 % , 5.4 % , 4.6 % , 3.9 % , 4.1 %","baseline by 11.9 % , 5.4 % , 4.6 % , 3.9 % , 4.1 %",0.525534987449646
translation,160,166,results,"top -1 , 5 , 10 , 20 , 50 recall",has,ranking approach,"top -1 , 5 , 10 , 20 , 50 recall has ranking approach",0.5711327791213989
translation,160,166,results,results,For,"top -1 , 5 , 10 , 20 , 50 recall","results For top -1 , 5 , 10 , 20 , 50 recall",0.6043437123298645
translation,160,167,results,our approach,surpasses,other baselines,our approach surpasses other baselines,0.5907348990440369
translation,160,167,results,results,has,our approach,results has our approach,0.6050099730491638
translation,160,182,results,accuracy,of,baseline,accuracy of baseline,0.5853604078292847
translation,160,182,results,accuracy,see,1.3 % improvement,accuracy see 1.3 % improvement,0.5646482110023499
translation,160,182,results,baseline,with,proposed well - order loss,baseline with proposed well - order loss,0.6456490755081177
translation,160,183,results,accuracy,of,joint-scoring ( js ) model,accuracy of joint-scoring ( js ) model,0.5814523100852966
translation,160,183,results,accuracy,see,3 % improvement,accuracy see 3 % improvement,0.6012950539588928
translation,160,183,results,joint-scoring ( js ) model,with,well - order loss,joint-scoring ( js ) model with well - order loss,0.600868284702301
translation,160,183,results,joint-scoring ( js ) model,see,3 % improvement,joint-scoring ( js ) model see 3 % improvement,0.5762426853179932
translation,160,183,results,3 % improvement,over,best baseline 3,3 % improvement over best baseline 3,0.6576712131500244
translation,160,185,results,accuracy,of,our jointscoring model,accuracy of our jointscoring model,0.5791673064231873
translation,160,185,results,our jointscoring model,with,well - order loss,our jointscoring model with well - order loss,0.6519500017166138
translation,160,185,results,our jointscoring model,with,top - 50 ranked subgraph,our jointscoring model with top - 50 ranked subgraph,0.5972213745117188
translation,160,185,results,our jointscoring model,see,further 4.3 % improvement,our jointscoring model see further 4.3 % improvement,0.5839528441429138
translation,160,185,results,further 4.3 % improvement,over,our model,further 4.3 % improvement over our model,0.6639319062232971
translation,160,185,results,our model,without,subgraph ranking,our model without subgraph ranking,0.7005133032798767
translation,160,185,results,7.3 % improvement,over,best baseline,7.3 % improvement over best baseline,0.6158910393714905
translation,160,188,results,effectiveness,of,subgraph ranking and joint-scoring approach,effectiveness of subgraph ranking and joint-scoring approach,0.580764651298523
translation,160,188,results,results,demonstrates,effectiveness,results demonstrates effectiveness,0.6364638805389404
translation,160,223,results,ranking method,combines,literal and semantic scores,ranking method combines literal and semantic scores,0.717963457107544
translation,160,223,results,ranking method,achieves,better subgraph selection results,ranking method achieves better subgraph selection results,0.620357871055603
translation,160,223,results,literal and semantic scores,to deal with,inexact match,literal and semantic scores to deal with inexact match,0.6869068741798401
translation,160,223,results,better subgraph selection results,than,state of the art,better subgraph selection results than state of the art,0.5391536355018616
translation,160,223,results,results,has,ranking method,results has ranking method,0.5701037049293518
translation,160,225,results,our proposed approach,achieves,new state of the art,our proposed approach achieves new state of the art,0.5977956652641296
translation,160,225,results,new state of the art,on,simplequestions dataset,new state of the art on simplequestions dataset,0.49576279520988464
translation,160,225,results,new state of the art,surpassing,best baseline,new state of the art surpassing best baseline,0.7647578120231628
translation,160,225,results,best baseline,by,large margin,best baseline by large margin,0.5868273973464966
translation,160,225,results,results,has,our proposed approach,results has our proposed approach,0.5936056971549988
translation,161,10,results,best systems,achieved,official score ( map ),best systems achieved official score ( map ),0.7078936100006104
translation,161,10,results,official score ( map ),of,"79.19 , 76.70 , 55.41 , and 45.83","official score ( map ) of 79.19 , 76.70 , 55.41 , and 45.83",0.5221306681632996
translation,161,10,results,"79.19 , 76.70 , 55.41 , and 45.83",in,"subtasks a , b , c , and d","79.19 , 76.70 , 55.41 , and 45.83 in subtasks a , b , c , and d",0.5010508894920349
translation,161,10,results,results,has,best systems,results has best systems,0.5547273755073547
translation,161,12,results,best system,improved over,2015 winner,best system improved over 2015 winner,0.7239363193511963
translation,161,12,results,2015 winner,by,3 points absolute,2015 winner by 3 points absolute,0.63129061460495
translation,161,12,results,3 points absolute,in terms of,accuracy,3 points absolute in terms of accuracy,0.6847027540206909
translation,161,12,results,subtask a,has,best system,subtask a has best system,0.63811194896698
translation,161,12,results,results,For,subtask a,results For subtask a,0.5478104948997498
translation,162,233,ablation-analysis,proposed cross-attention model,is,effective,proposed cross-attention model is effective,0.557066023349762
translation,162,233,ablation-analysis,ablation analysis,prove,proposed cross-attention model,ablation analysis prove proposed cross-attention model,0.6296381950378418
translation,162,226,baselines,lstm,employs,unidirectional lstm,lstm employs unidirectional lstm,0.5911860466003418
translation,162,226,baselines,lstm,us - es,last hidden state,lstm us - es last hidden state,0.6155046224594116
translation,162,226,baselines,last hidden state,as,question representation,last hidden state as question representation,0.5359959006309509
translation,162,226,baselines,baselines,has,lstm,baselines has lstm,0.5395978093147278
translation,162,227,baselines,bi lstm,adopts,bidirectional lst -m.,bi lstm adopts bidirectional lst -m.,0.6410473585128784
translation,162,227,baselines,a-q-att,denotes,answer-towards - question attention part,a-q-att denotes answer-towards - question attention part,0.6580134034156799
translation,162,227,baselines,bidirectional lst -m.,has,a-q-att,bidirectional lst -m. has a-q-att,0.6109874844551086
translation,162,227,baselines,baselines,has,bi lstm,baselines has bi lstm,0.5401566028594971
translation,162,193,hyperparameters,kb - qa training,use,mini-batch stochastic gradient descent,kb - qa training use mini-batch stochastic gradient descent,0.5484470725059509
translation,162,193,hyperparameters,mini-batch stochastic gradient descent,to minimize,pairwise training loss,mini-batch stochastic gradient descent to minimize pairwise training loss,0.6375483870506287
translation,162,193,hyperparameters,hyperparameters,For,kb - qa training,hyperparameters For kb - qa training,0.5447064638137817
translation,162,194,hyperparameters,minibatch size,set to,100,minibatch size set to 100,0.7311522960662842
translation,162,194,hyperparameters,hyperparameters,has,minibatch size,hyperparameters has minibatch size,0.4892244338989258
translation,162,195,hyperparameters,learning rate,set to,0.01,learning rate set to 0.01,0.6997436881065369
translation,162,195,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,162,197,hyperparameters,embedding size d,=,512,embedding size d = 512,0.6643293499946594
translation,162,197,hyperparameters,hidden unit size,is,256,hidden unit size is 256,0.5829222798347473
translation,162,197,hyperparameters,hyperparameters,has,embedding size d,hyperparameters has embedding size d,0.4830028712749481
translation,162,9,model,end-to - end neural network model,to represent,questions,end-to - end neural network model to represent questions,0.7143054604530334
translation,162,9,model,corresponding scores,according to,various candidate answer aspects,corresponding scores according to various candidate answer aspects,0.6476410031318665
translation,162,9,model,dynamically,according to,various candidate answer aspects,dynamically according to various candidate answer aspects,0.7138643264770508
translation,162,9,model,various candidate answer aspects,via,cross-attention mechanism,various candidate answer aspects via cross-attention mechanism,0.6497507095336914
translation,162,9,model,corresponding scores,has,dynamically,corresponding scores has dynamically,0.6269552707672119
translation,162,9,model,model,present,end-to - end neural network model,model present end-to - end neural network model,0.6595143675804138
translation,162,10,model,global knowledge,inside,underlying kb,global knowledge inside underlying kb,0.7686501145362854
translation,162,10,model,global knowledge,aiming at integrating,rich kb information,global knowledge aiming at integrating rich kb information,0.6550907492637634
translation,162,10,model,rich kb information,into,representation,rich kb information into representation,0.5966156721115112
translation,162,10,model,representation,of,answers,representation of answers,0.645385205745697
translation,162,10,model,model,leverage,global knowledge,model leverage global knowledge,0.7710080742835999
translation,162,46,model,model,has,cross-attention model,model has cross-attention model,0.5490698218345642
translation,162,64,model,novel cross-attention based nn model,tailored to,kb - qa task,novel cross-attention based nn model tailored to kb - qa task,0.6697514057159424
translation,162,64,model,novel cross-attention based nn model,considers,mutual influence,novel cross-attention based nn model considers mutual influence,0.6090263724327087
translation,162,64,model,mutual influence,between,representation of questions,mutual influence between representation of questions,0.6701325178146362
translation,162,64,model,mutual influence,between,corresponding answer aspects,mutual influence between corresponding answer aspects,0.6748478412628174
translation,162,64,model,model,present,novel cross-attention based nn model,model present novel cross-attention based nn model,0.609524130821228
translation,162,65,model,global kb information,represent,answers,global kb information represent answers,0.6440593600273132
translation,162,65,model,model,leverage,global kb information,model leverage global kb information,0.7664719223976135
translation,162,89,model,each aspect,of,answer,each aspect of answer,0.5927081108093262
translation,162,89,model,each aspect,focuses on,different words,each aspect focuses on different words,0.7272375226020813
translation,162,89,model,different words,of,question,different words of question,0.6213569641113281
translation,162,89,model,model,has,each aspect,model has each aspect,0.545625627040863
translation,162,176,model,model,perform,kb - qa training,model perform kb - qa training,0.6094068288803101
translation,162,176,model,model,perform,transe training,model perform transe training,0.6755955219268799
translation,162,207,results,results,on,webquestions dataset,results on webquestions dataset,0.5159550905227661
translation,162,217,results,our approach,achieves,best performance,our approach achieves best performance,0.6786253452301025
translation,162,217,results,best performance,of,all the end-to - end methods,best performance of all the end-to - end methods,0.5505715012550354
translation,162,217,results,all the end-to - end methods,on,webquestions,all the end-to - end methods on webquestions,0.5339667201042175
translation,162,217,results,results,observe,our approach,results observe our approach,0.6077187061309814
translation,162,231,results,bi lstm +c-att,dramatically improves,f 1 score,bi lstm +c-att dramatically improves f 1 score,0.717552900314331
translation,162,231,results,f 1 score,by,2.7 points,f 1 score by 2.7 points,0.5840163826942444
translation,162,231,results,2.7 points,compared with,bi lstm,2.7 points compared with bi lstm,0.5919786691665649
translation,162,231,results,0.2 points,higher than,bi lstm +a-q-att,0.2 points higher than bi lstm +a-q-att,0.6615105271339417
translation,162,231,results,results,has,bi lstm +c-att,results has bi lstm +c-att,0.5442376732826233
translation,162,232,results,significantly outperforms,improving,bi lstm +a-q-att + gki,significantly outperforms improving bi lstm +a-q-att + gki,0.653734028339386
translation,162,232,results,bi lstm + gki,by,2.5 points,bi lstm + gki by 2.5 points,0.5699779391288757
translation,162,232,results,bi lstm + gki,by,0.3 points,bi lstm + gki by 0.3 points,0.5793803930282593
translation,162,232,results,bi lstm + gki,improving,bi lstm +a-q-att + gki,bi lstm + gki improving bi lstm +a-q-att + gki,0.6058796048164368
translation,162,232,results,bi lstm +a-q-att + gki,by,0.3 points,bi lstm +a-q-att + gki by 0.3 points,0.5917038321495056
translation,162,232,results,bi lstm +c-att + gki,has,significantly outperforms,bi lstm +c-att + gki has significantly outperforms,0.22177913784980774
translation,162,232,results,significantly outperforms,has,bi lstm + gki,significantly outperforms has bi lstm + gki,0.5805997848510742
translation,162,232,results,results,has,bi lstm +c-att + gki,results has bi lstm +c-att + gki,0.5568220019340515
translation,162,234,results,bi lstm + gki,performs,better,bi lstm + gki performs better,0.6354836225509644
translation,162,234,results,bi lstm + gki,achieves,improvement,bi lstm + gki achieves improvement,0.6324768662452698
translation,162,234,results,better,than,bi lstm,better than bi lstm,0.5888508558273315
translation,162,234,results,improvement,of,1.3 points,improvement of 1.3 points,0.5349141359329224
translation,162,234,results,results,has,bi lstm + gki,results has bi lstm + gki,0.5504167675971985
translation,162,235,results,bi lstm +c-att + gki,improves,bi lstm +c-att,bi lstm +c-att + gki improves bi lstm +c-att,0.6708077192306519
translation,162,235,results,bi lstm +c-att,by,1.1 points,bi lstm +c-att by 1.1 points,0.5645466446876526
translation,162,235,results,results,has,bi lstm +c-att + gki,results has bi lstm +c-att + gki,0.5568220019340515
translation,162,236,results,bi lstm +c-att + gki,achieves,best performance,bi lstm +c-att + gki achieves best performance,0.6406237483024597
translation,162,236,results,bi lstm +c-att + gki,improves,original bi lstm,bi lstm +c-att + gki improves original bi lstm,0.6458689570426941
translation,162,236,results,original bi lstm,by,3.8 points,original bi lstm by 3.8 points,0.5468966364860535
translation,162,236,results,results,has,bi lstm +c-att + gki,results has bi lstm +c-att + gki,0.5568220019340515
translation,163,109,ablation-analysis,down - sampling procedure,helps in,increasing,down - sampling procedure helps in increasing,0.7503612637519836
translation,163,109,ablation-analysis,increasing,has,overall precision,increasing has overall precision,0.6106245517730713
translation,163,6,model,novel bootstrapping framework,based on,self-supervision,novel bootstrapping framework based on self-supervision,0.6620070934295654
translation,163,6,model,"diverse , large-scale dataset",of,clarification questions,"diverse , large-scale dataset of clarification questions",0.5338910222053528
translation,163,6,model,clarification questions,based on,post-comment tuples,clarification questions based on post-comment tuples,0.6868001818656921
translation,163,6,model,post-comment tuples,extracted from,stackexchange,post-comment tuples extracted from stackexchange,0.519187867641449
translation,163,6,model,model,devise,novel bootstrapping framework,model devise novel bootstrapping framework,0.7517817616462708
translation,163,7,model,framework,utilises,neural network based architecture,framework utilises neural network based architecture,0.5858256816864014
translation,163,7,model,neural network based architecture,for classifying,clarification questions,neural network based architecture for classifying clarification questions,0.7234938144683838
translation,163,7,model,model,has,framework,model has framework,0.5441871285438538
translation,163,29,model,novel bootstrapping framework,based on,self-supervision,novel bootstrapping framework based on self-supervision,0.6620070934295654
translation,163,29,model,clarification questions,from,various domains of stackexchange,clarification questions from various domains of stackexchange,0.5848493576049805
translation,163,30,model,framework,utilises,neural network based architecture,framework utilises neural network based architecture,0.5858256816864014
translation,163,30,model,neural network based architecture,to classify,clarification questions,neural network based architecture to classify clarification questions,0.7400547862052917
translation,163,30,model,model,has,framework,model has framework,0.5441871285438538
translation,163,31,model,two step procedure,then increases,recall,two step procedure then increases recall,0.7064874172210693
translation,163,31,model,framework,first increases,precision,framework first increases precision,0.7197675704956055
translation,163,31,model,framework,then increases,recall,framework then increases recall,0.7647139430046082
translation,163,31,model,precision,of,classifier,precision of classifier,0.6187026500701904
translation,163,31,model,two step procedure,has,framework,two step procedure has framework,0.571111798286438
translation,163,31,model,model,In,two step procedure,model In two step procedure,0.549229621887207
translation,163,42,model,bootstrapping framework,for training,classifier,bootstrapping framework for training classifier,0.7179393172264099
translation,163,42,model,classifier,capable of identifying,clarifying questions,classifier capable of identifying clarifying questions,0.6964637041091919
translation,163,42,model,model,devise,bootstrapping framework,model devise bootstrapping framework,0.770336389541626
translation,163,135,model,two-step iterative bootstrapping framework,based on,selfsupervision,two-step iterative bootstrapping framework based on selfsupervision,0.6906689405441284
translation,163,135,model,model,created by,two-step iterative bootstrapping framework,model created by two-step iterative bootstrapping framework,0.6168063282966614
translation,163,123,results,clarification question,to,post,clarification question to post,0.6218180656433105
translation,163,123,results,clarification question,help in,improving,clarification question help in improving,0.7663392424583435
translation,163,123,results,improving,has,performance,improving has performance,0.5706568360328674
translation,163,123,results,results,concatenating,clarification question,results concatenating clarification question,0.6349692940711975
translation,164,15,model,model,develop,novel techniques,model develop novel techniques,0.6778204441070557
translation,165,27,baselines,questions,requiring,coreferential reasoning,questions requiring coreferential reasoning,0.6691552996635437
translation,165,69,baselines,four single-span (,has,reading comprehension models,four single-span ( has reading comprehension models,0.5409064292907715
translation,165,35,results,best system performance,is,70.5 % f 1,best system performance is 70.5 % f 1,0.5857536196708679
translation,165,35,results,estimated human performance,is,93.4 %,estimated human performance is 93.4 %,0.543203592300415
translation,165,64,results,baseline model performance,on,quoref,baseline model performance on quoref,0.48719215393066406
translation,165,64,results,results,has,baseline model performance,results has baseline model performance,0.5452715158462524
translation,165,75,results,baseline models,on,quoref,baseline models on quoref,0.513793408870697
translation,165,76,results,best performing model,is,xlnet qa,best performing model is xlnet qa,0.5920630097389221
translation,165,76,results,best performing model,reaches,f 1 score,best performing model reaches f 1 score,0.6736199259757996
translation,165,76,results,f 1 score,of,70.5,f 1 score of 70.5,0.5695704817771912
translation,165,76,results,70.5,in,test set,70.5 in test set,0.5110377669334412
translation,165,76,results,results,has,best performing model,results has best performing model,0.5340396165847778
translation,165,84,results,most frequent entity,in,passage,most frequent entity in passage,0.48851779103279114
translation,165,84,results,passage-only baseline,has,under-performs,passage-only baseline has under-performs,0.5819608569145203
translation,165,84,results,under-performs,has,all other systems,under-performs has all other systems,0.6019129157066345
translation,165,84,results,results,has,passage-only baseline,results has passage-only baseline,0.5238560438156128
translation,166,48,ablation-analysis,evidence vectors,for,each passage word,evidence vectors for each passage word,0.5671351552009583
translation,166,48,ablation-analysis,ablation analysis,decompose,evidence vectors,ablation analysis decompose evidence vectors,0.7625717520713806
translation,166,211,ablation-analysis,nil f1 scores,help to improve,overall f1 scores,nil f1 scores help to improve overall f1 scores,0.696110188961029
translation,166,211,ablation-analysis,ablation analysis,Increasing,nil f1 scores,ablation analysis Increasing nil f1 scores,0.7275986075401306
translation,166,29,baselines,several baseline models,with,pipeline and threshold - based approaches,several baseline models with pipeline and threshold - based approaches,0.5985004901885986
translation,166,29,baselines,baselines,develop,several baseline models,baselines develop several baseline models,0.5953884124755859
translation,166,63,experimental-setup,pre-trained vectors,from,glove,pre-trained vectors from glove,0.5962195992469788
translation,166,63,experimental-setup,glove,for,word - level embeddings,glove for word - level embeddings,0.5978015065193176
translation,166,63,experimental-setup,", 2014 )",for,word - level embeddings,", 2014 ) for word - level embeddings",0.595149576663971
translation,166,63,experimental-setup,experimental setup,use,pre-trained vectors,experimental setup use pre-trained vectors,0.6032812595367432
translation,166,64,experimental-setup,trainable character - based lookup table,followed by,convolutional neural network ( cnn ),trainable character - based lookup table followed by convolutional neural network ( cnn ),0.6670442223548889
translation,166,64,experimental-setup,trainable character - based lookup table,followed by,"max-pooling ( kim , 2014 )","trainable character - based lookup table followed by max-pooling ( kim , 2014 )",0.6379077434539795
translation,166,64,experimental-setup,character embeddings,has,trainable character - based lookup table,character embeddings has trainable character - based lookup table,0.5327221751213074
translation,166,64,experimental-setup,experimental setup,For,character embeddings,experimental setup For character embeddings,0.5549976229667664
translation,166,66,experimental-setup,bi-directional lstm ( bilstm ),on,embedding vectors,bi-directional lstm ( bilstm ) on embedding vectors,0.5144469141960144
translation,166,66,experimental-setup,embedding vectors,to incorporate,contextual information,embedding vectors to incorporate contextual information,0.6986501216888428
translation,166,66,experimental-setup,experimental setup,use,bi-directional lstm ( bilstm ),experimental setup use bi-directional lstm ( bilstm ),0.5774390697479248
translation,166,169,experimental-setup,neural network models,implemented in,pytorch,neural network models implemented in pytorch,0.7414817810058594
translation,166,169,experimental-setup,experimental setup,has,neural network models,experimental setup has neural network models,0.5343267321586609
translation,166,176,experimental-setup,300 hidden units,for,bilstms,300 hidden units for bilstms,0.5391982793807983
translation,166,176,experimental-setup,300 hidden units,total of,300 filters,300 hidden units total of 300 filters,0.6557362079620361
translation,166,176,experimental-setup,300 filters,for,cnn - based models,300 filters for cnn - based models,0.5868648290634155
translation,166,176,experimental-setup,experimental setup,use,300 hidden units,experimental setup use 300 hidden units,0.5767746567726135
translation,166,177,experimental-setup,"dropout ( srivastava et al. , 2014 )",with,probability 0.3,"dropout ( srivastava et al. , 2014 ) with probability 0.3",0.5960097908973694
translation,166,177,experimental-setup,probability 0.3,for,every trainable layer,probability 0.3 for every trainable layer,0.5931618809700012
translation,166,178,experimental-setup,binary crossentropy loss,for training,nil-detection models,binary crossentropy loss for training nil-detection models,0.7210789918899536
translation,166,178,experimental-setup,adam optimizer,for training,nil-detection models,adam optimizer for training nil-detection models,0.731132447719574
translation,166,178,experimental-setup,experimental setup,use,binary crossentropy loss,experimental setup use binary crossentropy loss,0.5925515294075012
translation,166,34,experiments,four machine comprehension models,namely,"bidaf ( seo et al. , 2017 )","four machine comprehension models namely bidaf ( seo et al. , 2017 )",0.6378112435340881
translation,166,34,experiments,four machine comprehension models,namely,r- net,four machine comprehension models namely r- net,0.6520605683326721
translation,166,34,experiments,four machine comprehension models,namely,drqa,four machine comprehension models namely drqa,0.6734535694122314
translation,166,34,experiments,four machine comprehension models,namely,"amanda ( kundu and ng , 2018 )","four machine comprehension models namely amanda ( kundu and ng , 2018 )",0.6639851927757263
translation,166,34,experiments,four machine comprehension models,with,our proposed framework,four machine comprehension models with our proposed framework,0.6087567806243896
translation,166,34,experiments,four machine comprehension models,achieve,significantly better results,four machine comprehension models achieve significantly better results,0.604996919631958
translation,166,34,experiments,significantly better results,compared to,corresponding pipeline and threshold - based models,significantly better results compared to corresponding pipeline and threshold - based models,0.6752129197120667
translation,166,34,experiments,corresponding pipeline and threshold - based models,on,newsqa dataset,corresponding pipeline and threshold - based models on newsqa dataset,0.5459344983100891
translation,166,10,model,proposed framework,easily integrated with,several recently proposed qa models,proposed framework easily integrated with several recently proposed qa models,0.6939966678619385
translation,166,10,model,proposed framework,trained in,endto-end fashion,proposed framework trained in endto-end fashion,0.707324743270874
translation,166,10,model,several recently proposed qa models,developed for,reading comprehension,several recently proposed qa models developed for reading comprehension,0.6249974966049194
translation,166,10,model,several recently proposed qa models,trained in,endto-end fashion,several recently proposed qa models trained in endto-end fashion,0.7219235301017761
translation,166,28,model,evidence decomposition - aggregation,where,evidence vectors,evidence decomposition - aggregation where evidence vectors,0.6066225171089172
translation,166,28,model,evidence vectors,derived by,higher level encoding layer,evidence vectors derived by higher level encoding layer,0.6325513124465942
translation,166,28,model,evidence vectors,first decomposed into,relevant and irrelevant components,evidence vectors first decomposed into relevant and irrelevant components,0.7024810314178467
translation,166,28,model,later aggregated,to infer,existence of a valid answer,later aggregated to infer existence of a valid answer,0.7895504832267761
translation,166,30,model,pipeline model,detection of,nil questions,pipeline model detection of nil questions,0.6392626166343689
translation,166,30,model,model,In,pipeline model,model In pipeline model,0.5391095280647278
translation,166,31,model,answer span extraction model,entirely trained on,questions,answer span extraction model entirely trained on questions,0.7044206857681274
translation,166,31,model,questions,that have,valid answers,questions that have valid answers,0.6000816822052002
translation,166,31,model,returned,based on,confidence threshold,returned based on confidence threshold,0.6728597283363342
translation,166,31,model,threshold - based model,has,answer span extraction model,threshold - based model has answer span extraction model,0.5530186891555786
translation,166,31,model,model,In,threshold - based model,model In threshold - based model,0.5475654006004333
translation,166,47,model,evidence vectors,for,each passage word,evidence vectors for each passage word,0.5671351552009583
translation,166,47,model,evidence encoding layer,with respect to,question - passage joint encoding vectors,evidence encoding layer with respect to question - passage joint encoding vectors,0.6571409106254578
translation,166,47,model,question - passage joint encoding vectors,to derive,semantically relevant and irrelevant components,question - passage joint encoding vectors to derive semantically relevant and irrelevant components,0.6572248339653015
translation,166,47,model,model,decompose,evidence vectors,model decompose evidence vectors,0.7175191044807434
translation,166,115,model,embeddings,of,passage tokens,embeddings of passage tokens,0.5274583101272583
translation,166,115,model,embeddings,consist of,pretrained word vectors,embeddings consist of pretrained word vectors,0.6122524738311768
translation,166,115,model,pretrained word vectors,from,glove,pretrained word vectors from glove,0.5456068515777588
translation,166,115,model,pretrained word vectors,from,several syntactic features,pretrained word vectors from several syntactic features,0.48675891757011414
translation,166,115,model,pretrained word vectors,from,passage -question joint embedding ( aligned question embedding ),pretrained word vectors from passage -question joint embedding ( aligned question embedding ),0.5193154811859131
translation,166,115,model,drqa,has,embeddings,drqa has embeddings,0.6164621114730835
translation,166,115,model,model,In,drqa,model In drqa,0.5638399124145508
translation,166,186,results,end-to- end nil-aware models,achieve,highest overall em and f1 scores,end-to- end nil-aware models achieve highest overall em and f1 scores,0.6202515363693237
translation,166,186,results,highest overall em and f1 scores,compared to,all the corresponding pipeline systems,highest overall em and f1 scores compared to all the corresponding pipeline systems,0.623706042766571
translation,166,186,results,results,shows,end-to- end nil-aware models,results shows end-to- end nil-aware models,0.6622086763381958
translation,166,187,results,mp-bilstm nil detection model,achieves,higher nil f1 scores,mp-bilstm nil detection model achieves higher nil f1 scores,0.6640570163726807
translation,166,187,results,higher nil f1 scores,compared to,lr,higher nil f1 scores compared to lr,0.7025108337402344
translation,166,187,results,higher nil f1 scores,compared to,mp - cnn,higher nil f1 scores compared to mp - cnn,0.6407341361045837
translation,166,187,results,results,Note,mp-bilstm nil detection model,results Note mp-bilstm nil detection model,0.5416791439056396
translation,166,189,results,ap - based models,perform,better,ap - based models perform better,0.6215471029281616
translation,166,189,results,better,compared to,mp - based models,better compared to mp - based models,0.6415318846702576
translation,166,189,results,results,has,ap - based models,results has ap - based models,0.5225769281387329
translation,166,191,results,nil-aware models,manage to achieve,competitive scores,nil-aware models manage to achieve competitive scores,0.635984480381012
translation,166,191,results,competitive scores,compared to,corresponding standalone answer span extractors,competitive scores compared to corresponding standalone answer span extractors,0.6032365560531616
translation,166,191,results,corresponding standalone answer span extractors,on,test set,corresponding standalone answer span extractors on test set,0.5255264639854431
translation,166,191,results,test set,where,nil questions,test set where nil questions,0.6741489171981812
translation,166,192,results,nil-aware models,has,outperform,nil-aware models has outperform,0.6051009297370911
translation,166,192,results,outperform,has,corresponding threshold - based models,outperform has corresponding threshold - based models,0.5823118090629578
translation,166,192,results,results,shows,nil-aware models,results shows nil-aware models,0.6679949760437012
translation,166,193,results,all four answer span extraction models,used in,threshold - based approach,all four answer span extraction models used in threshold - based approach,0.6655746698379517
translation,166,193,results,all four answer span extraction models,produce,low nil precision,all four answer span extraction models produce low nil precision,0.6215479969978333
translation,166,193,results,all four answer span extraction models,produce,relatively higher nil recall,all four answer span extraction models produce relatively higher nil recall,0.6414240002632141
translation,166,193,results,threshold - based approach,for,nil detection,threshold - based approach for nil detection,0.6645582318305969
translation,166,198,results,performances,of,all the nil-aware models,performances of all the nil-aware models,0.542381763458252
translation,166,198,results,all the nil-aware models,are,significantly better ( p < 0.01 ),all the nil-aware models are significantly better ( p < 0.01 ),0.5309937596321106
translation,166,198,results,in terms of overall em and f1 ),are,significantly better ( p < 0.01 ),in terms of overall em and f1 ) are significantly better ( p < 0.01 ),0.5275580286979675
translation,166,198,results,significantly better ( p < 0.01 ),than,corresponding best pipeline models,significantly better ( p < 0.01 ) than corresponding best pipeline models,0.563698410987854
translation,166,198,results,significantly better ( p < 0.01 ),than,threshold - based approaches,significantly better ( p < 0.01 ) than threshold - based approaches,0.5735753774642944
translation,166,198,results,all the nil-aware models,has,in terms of overall em and f1 ),all the nil-aware models has in terms of overall em and f1 ),0.5829697847366333
translation,166,198,results,results,has,performances,results has performances,0.5711642503738403
translation,166,205,results,results,of,namanda,results of namanda,0.5345708131790161
translation,166,205,results,namanda,on,newsqa development set,namanda on newsqa development set,0.602484405040741
translation,166,205,results,newsqa development set,when,different components,newsqa development set when different components,0.6292988657951355
translation,166,205,results,removed,such as,character embeddings,removed such as character embeddings,0.6348236799240112
translation,166,205,results,removed,such as,question - passage joint encoding,removed such as question - passage joint encoding,0.6619037985801697
translation,166,205,results,second lstm,for,answer-ending pointer,second lstm for answer-ending pointer,0.606344997882843
translation,166,205,results,results,of,namanda,results of namanda,0.5345708131790161
translation,166,208,results,question - passage joint encoding,has,highest impact,question - passage joint encoding has highest impact,0.5528808832168579
translation,166,217,results,performances,of,nil-aware models,performances of nil-aware models,0.5799627900123596
translation,166,217,results,nil-aware models,are,worse,nil-aware models are worse,0.5955734252929688
translation,166,217,results,worse,than,corresponding answer extractor models,worse than corresponding answer extractor models,0.5748078227043152
translation,166,217,results,corresponding answer extractor models,on,test set,corresponding answer extractor models on test set,0.5328938364982605
translation,166,217,results,test set,without,nil questions,test set without nil questions,0.7140510678291321
translation,166,217,results,results,has,performances,results has performances,0.5711642503738403
translation,166,219,results,nil questions,in,set,nil questions in set,0.5309873223304749
translation,166,219,results,namanda,by,larger margin,namanda by larger margin,0.6280865669250488
translation,166,219,results,outperforms,by,larger margin,outperforms by larger margin,0.6429824233055115
translation,166,219,results,amanda,by,larger margin,amanda by larger margin,0.6626645922660828
translation,166,219,results,larger margin,on,overall scores,larger margin on overall scores,0.5343500375747681
translation,166,219,results,namanda,has,outperforms,namanda has outperforms,0.7053588628768921
translation,166,219,results,outperforms,has,amanda,outperforms has amanda,0.6695792078971863
translation,167,38,baselines,bertscore,computes,score,bertscore computes score,0.6930631995201111
translation,167,38,baselines,score,by leveraging,contextualized word representations,score by leveraging contextualized word representations,0.7390332818031311
translation,167,38,baselines,contextualized word representations,allowing it to go beyond,exact match,contextualized word representations allowing it to go beyond exact match,0.5727282166481018
translation,167,38,baselines,paraphrases,has,better,paraphrases has better,0.6109033226966858
translation,167,38,baselines,baselines,has,bertscore,baselines has bertscore,0.5821647047996521
translation,167,92,experimental-setup,multi-hop point generator,For,narrativeqa and semeval,multi-hop point generator For narrativeqa and semeval,0.6532817482948303
translation,167,92,experimental-setup,multi-hop point generator,use,multi-hop pointer generator ( mhpg ) model,multi-hop point generator use multi-hop pointer generator ( mhpg ) model,0.6343443989753723
translation,167,92,experimental-setup,mhpg,represents,input,mhpg represents input,0.6826870441436768
translation,167,92,experimental-setup,input,using,elmo embeddings,input using elmo embeddings,0.5740149021148682
translation,167,92,experimental-setup,multi-hop pointer generator ( mhpg ) model,has,mhpg,multi-hop pointer generator ( mhpg ) model has mhpg,0.5507773160934448
translation,167,92,experimental-setup,experimental setup,has,multi-hop point generator,experimental setup has multi-hop point generator,0.5179638862609863
translation,167,40,model,better qa metric,extending,bertscore,better qa metric extending bertscore,0.6616080403327942
translation,167,40,model,bertscore,to incorporate,context,bertscore to incorporate context,0.7322551608085632
translation,167,40,model,bertscore,to incorporate,question,bertscore to incorporate question,0.712296724319458
translation,167,40,model,question,when computing,similarity,question when computing similarity,0.7062028646469116
translation,167,40,model,similarity,between,two answers,similarity between two answers,0.7162371873855591
translation,167,72,model,extension,incorporates,context and question,extension incorporates context and question,0.6817706823348999
translation,167,72,model,context and question,when calculating,answer word representations,context and question when calculating answer word representations,0.6901336908340454
translation,167,72,model,extension,has,to bertscore,extension has to bertscore,0.5812454223632812
translation,167,72,model,model,propose,extension,model propose extension,0.6321065425872803
translation,167,95,model,output layer,consists of,generative decoder,output layer consists of generative decoder,0.6455125212669373
translation,167,95,model,generative decoder,with,copying mechanism,generative decoder with copying mechanism,0.632222056388855
translation,167,95,model,model,has,output layer,model has output layer,0.5563148260116577
translation,167,30,results,generative narra-tiveqa dataset,find that,existing metrics,generative narra-tiveqa dataset find that existing metrics,0.5610676407814026
translation,167,30,results,existing metrics,provide,reasonable correlation,existing metrics provide reasonable correlation,0.5823246240615845
translation,167,30,results,reasonable correlation,with,human accuracy judgements,reasonable correlation with human accuracy judgements,0.6289181113243103
translation,167,30,results,results,For,generative narra-tiveqa dataset,results For generative narra-tiveqa dataset,0.5804502367973328
translation,167,35,results,existing n-gram based metrics,perform,considerably worse,existing n-gram based metrics perform considerably worse,0.555852472782135
translation,167,35,results,considerably worse,in comparison to,narrativeqa,considerably worse in comparison to narrativeqa,0.6676384210586548
translation,167,35,results,results,find,existing n-gram based metrics,results find existing n-gram based metrics,0.5539271831512451
translation,167,39,results,existing metrics,on,all three datasets,existing metrics on all three datasets,0.45910361409187317
translation,167,114,results,considerable room,for,improvement,considerable room for improvement,0.6505463123321533
translation,167,114,results,narrativeqa,has,me- teor,narrativeqa has me- teor,0.6596148610115051
translation,167,114,results,four evaluation metrics,has,me- teor,four evaluation metrics has me- teor,0.5711799263954163
translation,167,114,results,results,Of,narrativeqa,results Of narrativeqa,0.5080368518829346
translation,167,117,results,semeval dataset,converted to,generative qa dataset,semeval dataset converted to generative qa dataset,0.54840487241745
translation,167,117,results,generative qa dataset,from,multiple - choice dataset,generative qa dataset from multiple - choice dataset,0.49831250309944153
translation,167,117,results,existing metrics,do,considerably worse,existing metrics do considerably worse,0.46630921959877014
translation,167,117,results,considerably worse,compared to,narrativeqa,considerably worse compared to narrativeqa,0.7011944055557251
translation,167,117,results,results,For,semeval dataset,results For semeval dataset,0.5865325331687927
translation,167,119,results,human judgements,on,semeval,human judgements on semeval,0.6013575792312622
translation,167,119,results,narrativeqa,has,meteor,narrativeqa has meteor,0.6564706563949585
translation,167,123,results,results,over,bertscore,results over bertscore,0.5896196961402893
translation,167,123,results,bertscore,on,generative qa tasks,bertscore on generative qa tasks,0.5277349352836609
translation,167,123,results,results,improves,results,results improves results,0.5747689604759216
translation,167,123,results,results,over,bertscore,results over bertscore,0.5896196961402893
translation,167,124,results,conditional bertscore,improves over,bertscore,conditional bertscore improves over bertscore,0.6866844296455383
translation,167,124,results,conditional bertscore,has,gains,conditional bertscore has gains,0.631737470626831
translation,167,124,results,bertscore,has,gains,bertscore has gains,0.6295591592788696
translation,167,124,results,results,cases where,conditional bertscore,results cases where conditional bertscore,0.7027631402015686
translation,168,104,baselines,baselines,has,keywords matching baseline ( bm25 ),baselines has keywords matching baseline ( bm25 ),0.563844621181488
translation,168,110,baselines,baselines,experimented,single - layer unidirectional lstm,baselines experimented single - layer unidirectional lstm,0.576486349105835
translation,168,117,baselines,lstm,performs,comparably,lstm performs comparably,0.6201294660568237
translation,168,117,baselines,comparably,to,previous systems,comparably to previous systems,0.600773811340332
translation,168,117,baselines,baselines,has,lstm,baselines has lstm,0.5395978093147278
translation,168,111,experimental-setup,each layer,of,lstm and blstm,each layer of lstm and blstm,0.5709714293479919
translation,168,111,experimental-setup,memory size,of,500,memory size of 500,0.6412248015403748
translation,168,111,experimental-setup,lstm and blstm,has,memory size,lstm and blstm has memory size,0.5448551774024963
translation,168,111,experimental-setup,experimental setup,has,each layer,experimental setup has each layer,0.5453023314476013
translation,168,112,experimental-setup,300 - dimensional vectors,trained and provided by,word2vec tool,300 - dimensional vectors trained and provided by word2vec tool,0.671620786190033
translation,168,112,experimental-setup,word2vec tool,using,part of the google news dataset,word2vec tool using part of the google news dataset,0.6400538682937622
translation,168,112,experimental-setup,experimental setup,use,300 - dimensional vectors,experimental setup use 300 - dimensional vectors,0.6331486105918884
translation,168,5,model,stacked bidirectional long- short term memory ( blstm ) network,to sequentially read,words,stacked bidirectional long- short term memory ( blstm ) network to sequentially read words,0.7349233627319336
translation,168,5,model,stacked bidirectional long- short term memory ( blstm ) network,outputs,relevance scores,stacked bidirectional long- short term memory ( blstm ) network outputs relevance scores,0.7297264337539673
translation,168,5,model,words,from,question and answer sentences,words from question and answer sentences,0.5346589684486389
translation,168,23,model,deep neural network,to address,answer sentence selection problem,deep neural network to address answer sentence selection problem,0.5675277709960938
translation,168,23,model,answer sentence selection problem,for,question answering,answer sentence selection problem for question answering,0.5544671416282654
translation,168,24,model,stacked bidirectional long short - term memory ( blstm ),to sequentially read,words,stacked bidirectional long short - term memory ( blstm ) to sequentially read words,0.7210273742675781
translation,168,24,model,stacked bidirectional long short - term memory ( blstm ),output,relevance scores,stacked bidirectional long short - term memory ( blstm ) output relevance scores,0.7812366485595703
translation,168,24,model,words,from,question and answer sentences,words from question and answer sentences,0.5346589684486389
translation,168,116,model,gradient boosted regression tree ( gbdt ) method,combine,features,gradient boosted regression tree ( gbdt ) method combine features,0.6611418724060059
translation,168,116,model,model,has,gradient boosted regression tree ( gbdt ) method,model has gradient boosted regression tree ( gbdt ) method,0.5485343933105469
translation,168,25,results,full system,combined with,keywords matching,full system combined with keywords matching,0.6521608829498291
translation,168,25,results,previous approaches,without using,syntactic parsing,previous approaches without using syntactic parsing,0.6844563484191895
translation,168,25,results,previous approaches,without using,external knowledge resources,previous approaches without using external knowledge resources,0.7064785957336426
translation,168,25,results,full system,has,outperforms,full system has outperforms,0.6178426146507263
translation,168,25,results,keywords matching,has,outperforms,keywords matching has outperforms,0.4711863696575165
translation,168,25,results,outperforms,has,previous approaches,outperforms has previous approaches,0.6066344380378723
translation,168,25,results,results,has,full system,results has full system,0.5670785903930664
translation,168,119,results,our combined system 's results,are,statistically significantly better,our combined system 's results are statistically significantly better,0.5769327878952026
translation,168,119,results,our combined system 's results,are,outperforms,our combined system 's results are outperforms,0.6173907518386841
translation,168,119,results,statistically significantly better,than,keywords matching baseline,statistically significantly better than keywords matching baseline,0.5587195754051208
translation,168,119,results,outperforms,has,previous state - of - art results,outperforms has previous state - of - art results,0.5461135506629944
translation,168,119,results,results,has,our combined system 's results,results has our combined system 's results,0.5582576394081116
translation,169,83,baselines,solr,is,open-source implementation,solr is open-source implementation,0.566705048084259
translation,169,83,baselines,open-source implementation,of,inverted index search system,open-source implementation of inverted index search system,0.5508926510810852
translation,169,83,baselines,baselines,has,solr,baselines has solr,0.6015295386314392
translation,169,133,experimental-setup,alignment matrices,to obtain,english -hindi bilingual embeddings,alignment matrices to obtain english -hindi bilingual embeddings,0.5925933122634888
translation,169,133,experimental-setup,experimental setup,use,pre-trained word embeddings,experimental setup use pre-trained word embeddings,0.5422800183296204
translation,169,136,experimental-setup,randomly initialised embeddings,between,"[ - 0.25 , 0.25 ]","randomly initialised embeddings between [ - 0.25 , 0.25 ]",0.632347822189331
translation,169,136,experimental-setup,randomly initialised embeddings,for,words without embeddings,randomly initialised embeddings for words without embeddings,0.5565122961997986
translation,169,136,experimental-setup,"[ - 0.25 , 0.25 ]",for,words without embeddings,"[ - 0.25 , 0.25 ] for words without embeddings",0.6116394400596619
translation,169,136,experimental-setup,experimental setup,use,randomly initialised embeddings,experimental setup use randomly initialised embeddings,0.5578816533088684
translation,169,158,experimental-setup,k = 200,for,initial candidate generation step,k = 200 for initial candidate generation step,0.5784966945648193
translation,169,11,model,triplet- siamese-hybrid cnn ( tshcnn ),to re-rank,candidate answers,triplet- siamese-hybrid cnn ( tshcnn ) to re-rank candidate answers,0.7414745688438416
translation,169,11,model,model,propose,triplet- siamese-hybrid cnn ( tshcnn ),model propose triplet- siamese-hybrid cnn ( tshcnn ),0.6289767622947693
translation,169,40,model,k-nearest bilingual embedding transformation ( knbet ),exploits,bilingual embeddings,k-nearest bilingual embedding transformation ( knbet ) exploits bilingual embeddings,0.709761381149292
translation,169,40,model,bilingual embeddings,to outperform,performance,bilingual embeddings to outperform performance,0.6919090151786804
translation,169,40,model,performance,of,lexical translation,performance of lexical translation,0.5931540727615356
translation,169,40,model,model,develop,k-nearest bilingual embedding transformation ( knbet ),model develop k-nearest bilingual embedding transformation ( knbet ),0.6158766746520996
translation,169,41,model,cm qa system,over,kb,cm qa system over kb,0.6436800956726074
translation,169,41,model,cm qa system,named,cmqa,cm qa system named cmqa,0.7459512948989868
translation,169,41,model,cm qa system,using,only monolingual data,cm qa system using only monolingual data,0.6469928622245789
translation,169,41,model,only monolingual data,from,individual languages,only monolingual data from individual languages,0.529470682144165
translation,169,41,model,model,develop,cm qa system,model develop cm qa system,0.6736267805099487
translation,169,88,model,convolutional neural networks ( cnns ),to learn,semantic representation,convolutional neural networks ( cnns ) to learn semantic representation,0.6201759576797485
translation,169,88,model,convolutional neural networks ( cnns ),learn,globally word order invariant features,convolutional neural networks ( cnns ) learn globally word order invariant features,0.6583221554756165
translation,169,88,model,semantic representation,for,input text,semantic representation for input text,0.550881028175354
translation,169,88,model,model,use,convolutional neural networks ( cnns ),model use convolutional neural networks ( cnns ),0.6285476088523865
translation,169,33,results,our model,trained on,english and hindi,our model trained on english and hindi,0.7042098641395569
translation,169,33,results,our model,perform,better,our model perform better,0.6152151823043823
translation,169,33,results,better,on,cm question,better on cm question,0.5500742793083191
translation,169,33,results,better,than,lexical translation,better than lexical translation,0.5994663238525391
translation,169,33,results,cm question,than,lexical translation,cm question than lexical translation,0.5915504097938538
translation,169,33,results,results,show,our model,results show our model,0.6888449192047119
translation,169,168,results,initial candidate generation step,surprisingly surpasses,original bordes et al . ( 2015 ) paper,initial candidate generation step surprisingly surpasses original bordes et al . ( 2015 ) paper,0.7248245477676392
translation,169,168,results,results,has,initial candidate generation step,results has initial candidate generation step,0.5523329377174377
translation,169,177,results,our model tshcnn,trained on,english and hindi questions,our model tshcnn trained on english and hindi questions,0.716090738773346
translation,169,177,results,our model tshcnn,gives,best scores,our model tshcnn gives best scores,0.6065734028816223
translation,169,177,results,results,has,our model tshcnn,results has our model tshcnn,0.5778166055679321
translation,169,178,results,3 - 8 %,for,various cm question variations,3 - 8 % for various cm question variations,0.6416598558425903
translation,169,184,results,improvement,of,17 %,improvement of 17 %,0.6106663942337036
translation,169,184,results,17 %,for,cm-tl questions,17 % for cm-tl questions,0.6890051364898682
translation,169,184,results,17 %,when,network,17 % when network,0.6880709528923035
translation,169,184,results,network,trained on,english and hindi,network trained on english and hindi,0.7602349519729614
translation,169,184,results,english and hindi,using,bilingual embeddings,english and hindi using bilingual embeddings,0.6612246632575989
translation,169,184,results,english and hindi,using,monolingual embeddings,english and hindi using monolingual embeddings,0.625237762928009
translation,169,188,results,improvement,of,11 %,improvement of 11 %,0.6198607683181763
translation,169,188,results,11 %,for,cm-lt questions,11 % for cm-lt questions,0.647270679473877
translation,169,188,results,results,observe,improvement,results observe improvement,0.6390148997306824
translation,169,200,results,hindi questions,helps,network,hindi questions helps network,0.6152840256690979
translation,169,200,results,network,learn,different word orders,network learn different word orders,0.6876313090324402
translation,169,200,results,different word orders,present in,hindi questions,different word orders present in hindi questions,0.6266564130783081
translation,169,200,results,results,Training with,hindi questions,results Training with hindi questions,0.654994547367096
translation,169,202,results,joint training,on,both english and hindi questions,joint training on both english and hindi questions,0.5238140225410461
translation,169,202,results,both english and hindi questions,gives,best results,both english and hindi questions gives best results,0.5983052849769592
translation,169,202,results,results,has,joint training,results has joint training,0.5075324177742004
translation,169,203,results,solr candidates,as,negative samples,solr candidates as negative samples,0.5430347919464111
translation,169,203,results,results,has,scns,results has scns,0.5276262164115906
translation,169,205,results,scores,obtained,combination of both negative sample generation policies,scores obtained combination of both negative sample generation policies,0.6260333061218262
translation,169,205,results,scores,when using,combination of both negative sample generation policies,scores when using combination of both negative sample generation policies,0.7352490425109863
translation,169,205,results,combination of both negative sample generation policies,was,12.7 % higher,combination of both negative sample generation policies was 12.7 % higher,0.6061511635780334
translation,169,205,results,results,has,scores,results has scores,0.5219217538833618
translation,169,208,results,improve-ment,of,34 % - 62 %,improve-ment of 34 % - 62 %,0.6107078790664673
translation,169,208,results,34 % - 62 %,in,our scores,34 % - 62 % in our scores,0.5623900890350342
translation,169,208,results,34 % - 62 %,provide,additional input,34 % - 62 % provide additional input,0.6720432043075562
translation,169,208,results,additional input,in the form of,concatenated question and tuple,additional input in the form of concatenated question and tuple,0.7210906744003296
translation,169,208,results,results,obtain,improve-ment,results obtain improve-ment,0.5657538175582886
translation,169,215,results,cmtl questions,give,highest scores,cmtl questions give highest scores,0.5737795829772949
translation,169,215,results,highest scores,on,network,highest scores on network,0.5614910125732422
translation,169,215,results,network,trained on,english and hindi questions,network trained on english and hindi questions,0.7458515763282776
translation,169,215,results,english and hindi questions,using,bilingual embeddings,english and hindi questions using bilingual embeddings,0.6344133615493774
translation,169,215,results,results,show,cmtl questions,results show cmtl questions,0.5484499931335449
translation,170,78,baselines,random,has,always 50 %,random has always 50 %,0.6463484168052673
translation,170,79,baselines,bert - sci,fine - tuned on,"large , general set of science questions","bert - sci fine - tuned on large , general set of science questions",0.7459996342658997
translation,170,79,baselines,baselines,has,bert - sci,baselines has bert - sci,0.6206361055374146
translation,170,80,baselines,baselines,has,bert ( ir ),baselines has bert ( ir ),0.5621291995048523
translation,170,88,baselines,bert - pft ( no knowledge ),fine-tuned on,quartz,bert - pft ( no knowledge ) fine-tuned on quartz,0.7334089279174805
translation,170,88,baselines,first finetuned,on,race dataset,first finetuned on race dataset,0.5229552388191223
translation,170,88,baselines,bert - pft ( no knowledge ),has,first finetuned,bert - pft ( no knowledge ) has first finetuned,0.6190227270126343
translation,170,88,baselines,baselines,has,bert - pft ( no knowledge ),baselines has bert - pft ( no knowledge ),0.5813731551170349
translation,170,12,experiments,first open-domain dataset,of,qualitative relationship questions,first open-domain dataset of qualitative relationship questions,0.5448468923568726
translation,170,12,experiments,qualitative relationship questions,called,quartz,qualitative relationship questions called quartz,0.6727263927459717
translation,171,42,model,novel method,to construct,datasets,novel method to construct datasets,0.7207344770431519
translation,171,42,model,datasets,combines,traditional independent example collection approach,datasets combines traditional independent example collection approach,0.7091650366783142
translation,171,42,model,traditional independent example collection approach,with,minimal natural perturbations,traditional independent example collection approach with minimal natural perturbations,0.6266317963600159
translation,171,42,model,model,propose,novel method,model propose novel method,0.7230806350708008
translation,171,34,results,more robust,to,minor changes,more robust to minor changes,0.6049274802207947
translation,171,34,results,4.5 % better,across,datasets,4.5 % better across datasets,0.6724810004234314
translation,171,34,results,4.5 % better,than,models,4.5 % better than models,0.5721848607063293
translation,171,34,results,models,trained on,boolq,models trained on boolq,0.7244758009910583
translation,171,34,results,9 %,has,more robust,9 % has more robust,0.5476900339126587
translation,171,38,results,training,on,perturbed data,training on perturbed data,0.5863543152809143
translation,171,38,results,perturbed data,continues to retain,performance,perturbed data continues to retain performance,0.7106972932815552
translation,171,38,results,performance,on,original task,performance on original task,0.5060971975326538
translation,171,38,results,results,find that,training,results find that training,0.60508131980896
translation,171,39,results,worst case cost ratio,of,1.0,worst case cost ratio of 1.0,0.5843238830566406
translation,171,39,results,models,trained on,perturbed examples,models trained on perturbed examples,0.7291929125785828
translation,171,39,results,models,remain,competitive,models remain competitive,0.7162758111953735
translation,171,39,results,perturbed examples,remain,competitive,perturbed examples remain competitive,0.6762048602104187
translation,171,39,results,competitive,on,all our evaluation sets,competitive on all our evaluation sets,0.5506702661514282
translation,171,39,results,worst case cost ratio,has,models,worst case cost ratio has models,0.5786036849021912
translation,171,39,results,1.0,has,models,1.0 has models,0.5803722143173218
translation,171,117,results,system performance,consistently gets,higher,system performance consistently gets higher,0.6875879764556885
translation,171,117,results,higher,with,larger clusters,higher with larger clusters,0.6851009130477905
translation,171,117,results,fixed number of clusters ( r = 0 ),has,system performance,fixed number of clusters ( r = 0 ) has system performance,0.5668969750404358
translation,172,147,baselines,beam search,with,beam size,beam search with beam size,0.6320963501930237
translation,172,147,baselines,beam size,to,12,beam size to 12,0.6324288845062256
translation,172,147,baselines,12,to generate,question,12 to generate question,0.6705159544944763
translation,172,147,baselines,question,approximately maximizes,conditional probability,question approximately maximizes conditional probability,0.804412305355072
translation,172,152,baselines,template based approach,use,method,template based approach use method,0.6530731916427612
translation,172,152,baselines,method,along with,"word2vec ( mikolov et al. , 2015 )","method along with word2vec ( mikolov et al. , 2015 )",0.49385133385658264
translation,172,152,baselines,baselines,For,template based approach,baselines For template based approach,0.6127731204032898
translation,172,139,hyperparameters,encoder,use,bi-directional rnn,encoder use bi-directional rnn,0.5878890156745911
translation,172,139,hyperparameters,bi-directional rnn,containing,one hidden layer,bi-directional rnn containing one hidden layer,0.6256390810012817
translation,172,139,hyperparameters,one hidden layer,of,1000 units,one hidden layer of 1000 units,0.5711721181869507
translation,172,139,hyperparameters,hyperparameters,For,encoder,hyperparameters For encoder,0.5951074957847595
translation,172,140,hyperparameters,each word,in,input vocabulary,each word in input vocabulary,0.4835704565048218
translation,172,140,hyperparameters,randomly initialized and learnt,during,training process,randomly initialized and learnt during training process,0.6508515477180481
translation,172,140,hyperparameters,hyperparameters,has,each word,hyperparameters has each word,0.5258165597915649
translation,172,145,hyperparameters,mini batch stochastic gradient descent algorithm,together with,"adadelta ( zeiler , 2012 )","mini batch stochastic gradient descent algorithm together with adadelta ( zeiler , 2012 )",0.6183914542198181
translation,172,145,hyperparameters,"adadelta ( zeiler , 2012 )",to train,our model,"adadelta ( zeiler , 2012 ) to train our model",0.6853815317153931
translation,172,145,hyperparameters,hyperparameters,use,mini batch stochastic gradient descent algorithm,hyperparameters use mini batch stochastic gradient descent algorithm,0.6046109795570374
translation,172,146,hyperparameters,mini-batch size,of,50,mini-batch size of 50,0.6585855484008789
translation,172,146,hyperparameters,hyperparameters,used,mini-batch size,hyperparameters used mini-batch size,0.5817127227783203
translation,172,146,hyperparameters,hyperparameters,trained,model,hyperparameters trained model,0.723837673664093
translation,172,150,hyperparameters,5 - gram language model,trained on,1m target question sequences,5 - gram language model trained on 1m target question sequences,0.6931456923484802
translation,172,150,hyperparameters,5 - gram language model,tuned,parameters,5 - gram language model tuned parameters,0.670397937297821
translation,172,150,hyperparameters,parameters,of,decoder,parameters of decoder,0.6043993830680847
translation,172,150,hyperparameters,decoder,using,1000 held - out sequence,decoder using 1000 held - out sequence,0.6885960102081299
translation,172,150,hyperparameters,hyperparameters,used,5 - gram language model,hyperparameters used 5 - gram language model,0.5464712381362915
translation,172,150,hyperparameters,hyperparameters,tuned,parameters,hyperparameters tuned parameters,0.6679598093032837
translation,172,154,hyperparameters,templates,using,same 1 m training pairs,templates using same 1 m training pairs,0.6517332792282104
translation,172,154,hyperparameters,same 1 m training pairs,extracted from,wikianswers,same 1 m training pairs extracted from wikianswers,0.5476748943328857
translation,172,154,hyperparameters,hyperparameters,learn,templates,hyperparameters learn templates,0.6482937335968018
translation,172,6,model,model,has,generated question answer ( qa ) pairs,model has generated question answer ( qa ) pairs,0.6171883940696716
translation,172,41,model,rnn model,for generating,natural language questions,rnn model for generating natural language questions,0.6169231534004211
translation,172,41,model,natural language questions,from,sequence of keywords,natural language questions from sequence of keywords,0.48862576484680176
translation,172,41,model,sequence of keywords,using,open domain community question answering ( cqa ) data,sequence of keywords using open domain community question answering ( cqa ) data,0.6680909395217896
translation,172,41,model,model,train,rnn model,model train rnn model,0.7098782062530518
translation,172,141,model,decoder,contains,one hidden layer,decoder contains one hidden layer,0.6428499221801758
translation,172,141,model,one hidden layer,comprising of,1000 units,one hidden layer comprising of 1000 units,0.615375280380249
translation,172,141,model,model,has,decoder,model has decoder,0.6226420402526855
translation,172,142,model,output layer,of,decoder,output layer of decoder,0.6155973672866821
translation,172,142,model,softmax function,gives,distribution,softmax function gives distribution,0.5987430214881897
translation,172,142,model,distribution,over,entire target vocabulary,distribution over entire target vocabulary,0.6609630584716797
translation,172,142,model,decoder,has,softmax function,decoder has softmax function,0.5661623477935791
translation,172,142,model,model,At,output layer,model At output layer,0.5586003661155701
translation,172,214,model,rnn based approach,for generating,natural language questions,rnn based approach for generating natural language questions,0.6397059559822083
translation,172,214,model,natural language questions,from,input keyword sequence,natural language questions from input keyword sequence,0.49610233306884766
translation,172,214,model,model,propose,rnn based approach,model propose rnn based approach,0.7143712043762207
translation,172,11,results,rnn based model,generates,qa pairs,rnn based model generates qa pairs,0.6991831660270691
translation,172,11,results,rnn based model,performs,110.47 percent ( relative ),rnn based model performs 110.47 percent ( relative ),0.5928807258605957
translation,172,11,results,qa pairs,with,accuracy,qa pairs with accuracy,0.6555507183074951
translation,172,11,results,accuracy,of,33.61 percent,accuracy of 33.61 percent,0.5623104572296143
translation,172,11,results,better,than,state - of - the - art template based method,better than state - of - the - art template based method,0.5640347003936768
translation,172,11,results,state - of - the - art template based method,for generating,natural language question,state - of - the - art template based method for generating natural language question,0.6572868824005127
translation,172,11,results,natural language question,from,keywords,natural language question from keywords,0.41582825779914856
translation,172,11,results,110.47 percent ( relative ),has,better,110.47 percent ( relative ) has better,0.5710657835006714
translation,172,11,results,results,has,rnn based model,results has rnn based model,0.5830600261688232
translation,172,169,results,both k2q-rnn and k2q-pbsmt,has,clearly outperform,both k2q-rnn and k2q-pbsmt has clearly outperform,0.5816789269447327
translation,172,169,results,clearly outperform,has,template based method,clearly outperform has template based method,0.5945609211921692
translation,172,176,results,once gain k2q-rnn and k2q-pbsmt,has,outperform,once gain k2q-rnn and k2q-pbsmt has outperform,0.6058711409568787
translation,172,176,results,outperform,has,template based approach,outperform has template based approach,0.6086369156837463
translation,172,176,results,results,has,once gain k2q-rnn and k2q-pbsmt,results has once gain k2q-rnn and k2q-pbsmt,0.5498056411743164
translation,172,182,results,other methods,at,all input sequence sizes,other methods at all input sequence sizes,0.48001909255981445
translation,172,182,results,k2q - rnn,has,clearly outperforms,k2q - rnn has clearly outperforms,0.6015287637710571
translation,172,182,results,clearly outperforms,has,other methods,clearly outperforms has other methods,0.578210175037384
translation,172,182,results,results,see that,k2q - rnn,results see that k2q - rnn,0.6423454880714417
translation,172,202,results,relative improvement,of,5.5 %,relative improvement of 5.5 %,0.5699462294578552
translation,172,202,results,5.5 %,in,f1,5.5 % in f1,0.5739494562149048
translation,172,202,results,5.5 %,by adding,gqa,5.5 % by adding gqa,0.7798757553100586
translation,172,202,results,f1,-,score,f1 - score,0.6307331323623657
translation,172,202,results,score,of,system,score of system,0.5817541480064392
translation,172,203,results,results,has,performance gains,results has performance gains,0.5833740234375
translation,173,4,model,semantic parser,scales up to,freebase,semantic parser scales up to freebase,0.6716722846031189
translation,173,4,model,model,train,semantic parser,model train semantic parser,0.7192076444625854
translation,173,5,model,model,learn from,question - answer pairs,model learn from question - answer pairs,0.7036555409431458
translation,173,192,results,empirical result,is that,our system,empirical result is that our system,0.6093806624412537
translation,173,192,results,our system,trained only on,question - answer pairs,our system trained only on question - answer pairs,0.7422076463699341
translation,173,192,results,62 % accuracy,on,test set,62 % accuracy on test set,0.5535292029380798
translation,173,192,results,outperforming,has,59 % accuracy,outperforming has 59 % accuracy,0.6199195981025696
translation,173,217,results,accuracy,on,test set,accuracy on test set,0.5323967337608337
translation,173,217,results,test set,is,26.9 %,test set is 26.9 %,0.5613342523574829
translation,173,217,results,our full system,obtains,31.4 %,our full system obtains 31.4 %,0.5865885615348816
translation,173,217,results,31.4 %,has,significant improvement,31.4 % has significant improvement,0.5533031225204468
translation,173,217,results,results,has,accuracy,results has accuracy,0.5888755321502686
translation,173,226,results,alignment,better than,bridging,alignment better than bridging,0.7092089056968689
translation,173,226,results,alignment,for,free917,alignment for free917,0.6559024453163147
translation,173,226,results,bridging,on,webquestions,bridging on webquestions,0.5555580258369446
translation,173,226,results,results,for,free917,results for free917,0.573462724685669
translation,173,226,results,results,has,alignment,results has alignment,0.47447526454925537
translation,173,235,results,parser,constructs,derivations,parser constructs derivations,0.6764340996742249
translation,173,235,results,derivations,contain,"about 12,000 distinct binary predicates","derivations contain about 12,000 distinct binary predicates",0.6279584765434265
translation,173,235,results,webquestions,has,parser,webquestions has parser,0.5791403651237488
translation,173,235,results,results,running on,webquestions,results running on webquestions,0.6288793683052063
translation,174,8,model,novel deep learning framework,integrated with,joint inference,novel deep learning framework integrated with joint inference,0.6190146803855896
translation,174,8,model,joint inference,to capture,tag semantic and geographic correlation,joint inference to capture tag semantic and geographic correlation,0.6829915046691895
translation,174,8,model,tag semantic and geographic correlation,between,question and pois,tag semantic and geographic correlation between question and pois,0.6858744621276855
translation,174,8,model,model,present,novel deep learning framework,model present novel deep learning framework,0.5906528234481812
translation,174,9,model,special cross attention question embedding neural network structure,to obtain,question - to - poi and poi - to-question information,special cross attention question embedding neural network structure to obtain question - to - poi and poi - to-question information,0.5618926286697388
translation,174,9,model,model,propose,special cross attention question embedding neural network structure,model propose special cross attention question embedding neural network structure,0.6354746222496033
translation,174,37,model,poi oriented qa model,with,joint inference,poi oriented qa model with joint inference,0.6277256608009338
translation,174,37,model,model,propose,poi oriented qa model,model propose poi oriented qa model,0.6778072118759155
translation,174,38,model,pji,has,two modules,pji has two modules,0.635616660118103
translation,174,38,model,two modules,named as,tag semantic module,two modules named as tag semantic module,0.6584047675132751
translation,174,38,model,two modules,named as,distance correlation module,two modules named as distance correlation module,0.6557775139808655
translation,174,38,model,model,has,pji,model has pji,0.6228227019309998
translation,174,40,model,specific patterns,buried in,questions and pois,specific patterns buried in questions and pois,0.7436314225196838
translation,174,40,model,specific patterns,develop,novel cross attention based question embedding structure,specific patterns develop novel cross attention based question embedding structure,0.6246881484985352
translation,174,40,model,model,to capture,specific patterns,model to capture specific patterns,0.7632973790168762
translation,174,40,model,model,develop,novel cross attention based question embedding structure,model develop novel cross attention based question embedding structure,0.616889476776123
translation,174,43,model,both modules,fused together and optimized in,end-to - end manner,both modules fused together and optimized in end-to - end manner,0.7412386536598206
translation,174,43,model,end-to - end manner,for retrieving,final poi list,end-to - end manner for retrieving final poi list,0.746066689491272
translation,174,43,model,model,has,both modules,model has both modules,0.5641366839408875
translation,174,210,model,double-sided attention,containing,question - to -answer attention,double-sided attention containing question - to -answer attention,0.6076850891113281
translation,174,210,model,double-sided attention,containing,answer-toquestion attention,double-sided attention containing answer-toquestion attention,0.6598863005638123
translation,174,210,model,model,considers,double-sided attention,model considers double-sided attention,0.6314998269081116
translation,174,220,model,our model,puts,strong emphasis,our model puts strong emphasis,0.6749369502067566
translation,174,220,model,strong emphasis,on,distance and tag related patterns,strong emphasis on distance and tag related patterns,0.5696126222610474
translation,174,220,model,distance and tag related patterns,of,questions and pois,distance and tag related patterns of questions and pois,0.5846641063690186
translation,174,213,results,semantic parsing based methods,have,higher % hits@k rate,semantic parsing based methods have higher % hits@k rate,0.5433493852615356
translation,174,213,results,typical neural network based models,has,semantic parsing based methods,typical neural network based models has semantic parsing based methods,0.5589437484741211
translation,174,213,results,results,Compared with,typical neural network based models,results Compared with typical neural network based models,0.6803053617477417
translation,174,214,results,worse,than,our pji model,worse than our pji model,0.591860830783844
translation,174,214,results,lack of flexibility,has,% hits@k rate,lack of flexibility has % hits@k rate,0.58863765001297
translation,174,214,results,results,with,lack of flexibility,results with lack of flexibility,0.6327236890792847
translation,174,215,results,models with attention mechanism,reach,better performance,models with attention mechanism reach better performance,0.7600162029266357
translation,174,215,results,better performance,than,models without,better performance than models without,0.638857364654541
translation,174,215,results,results,has,models with attention mechanism,results has models with attention mechanism,0.564053475856781
translation,174,216,results,bidirectional attention models,achieve,higher % hits@k rate,bidirectional attention models achieve higher % hits@k rate,0.6159906983375549
translation,174,216,results,higher % hits@k rate,than,unidirectional one,higher % hits@k rate than unidirectional one,0.6318649053573608
translation,174,216,results,results,has,bidirectional attention models,results has bidirectional attention models,0.4855901598930359
translation,174,217,results,our model,achieves,best overall performance,our model achieves best overall performance,0.6772801280021667
translation,174,217,results,best overall performance,among,all the models,best overall performance among all the models,0.580743134021759
translation,174,217,results,results,has,our model,results has our model,0.5871725678443909
translation,174,218,results,results,In terms of,% hits@k rate,results In terms of % hits@k rate,0.702576220035553
translation,174,225,results,city level questions,obtain,best result,city level questions obtain best result,0.5969120264053345
translation,174,225,results,best result,compared to,other two types,best result compared to other two types,0.6715249419212341
translation,175,102,hyperparameters,dimensions,of,"word , entity and relation embeddings","dimensions of word , entity and relation embeddings",0.5436738133430481
translation,175,102,hyperparameters,dimensions,of,lstm states,dimensions of lstm states,0.5725926756858826
translation,175,102,hyperparameters,lstm states,set to,d = 50,lstm states set to d = 50,0.7030711770057678
translation,175,102,hyperparameters,hyperparameters,has,dimensions,hyperparameters has dimensions,0.5247315764427185
translation,175,103,hyperparameters,word and entity embeddings,initialized with,"word2vec ( mikolov et al. , 2013 )","word and entity embeddings initialized with word2vec ( mikolov et al. , 2013 )",0.6673939228057861
translation,175,103,hyperparameters,"word2vec ( mikolov et al. , 2013 )",million,clueweb sentences,"word2vec ( mikolov et al. , 2013 ) million clueweb sentences",0.6348488330841064
translation,175,103,hyperparameters,clueweb sentences,containing,entities,clueweb sentences containing entities,0.6210280656814575
translation,175,103,hyperparameters,entities,in,freebase subset of spades,entities in freebase subset of spades,0.509344756603241
translation,175,103,hyperparameters,"word2vec ( mikolov et al. , 2013 )",has,clueweb sentences,"word2vec ( mikolov et al. , 2013 ) has clueweb sentences",0.5446008443832397
translation,175,103,hyperparameters,hyperparameters,has,word and entity embeddings,hyperparameters has word and entity embeddings,0.4901377856731415
translation,175,104,hyperparameters,network weights,initialized using,xavier initialization,network weights initialized using xavier initialization,0.7948911786079407
translation,175,104,hyperparameters,hyperparameters,has,network weights,hyperparameters has network weights,0.5205133557319641
translation,175,105,hyperparameters,2.5 k textual facts,for,question,2.5 k textual facts for question,0.6154460906982422
translation,175,105,hyperparameters,hyperparameters,considered,2.5 k textual facts,hyperparameters considered 2.5 k textual facts,0.5985849499702454
translation,175,105,hyperparameters,hyperparameters,up to,2.5 k textual facts,hyperparameters up to 2.5 k textual facts,0.5888270735740662
translation,175,106,hyperparameters,"adam ( kingma and ba , 2015 )",with,default hyperparameters,"adam ( kingma and ba , 2015 ) with default hyperparameters",0.5829872488975525
translation,175,106,hyperparameters,default hyperparameters,for,optimization,default hyperparameters for optimization,0.5639174580574036
translation,175,106,hyperparameters,default hyperparameters,has,"learning rate =1e - 3 , ? 1 =0.9 , ? 2 =0.999 , ?=1e - 8","default hyperparameters has learning rate =1e - 3 , ? 1 =0.9 , ? 2 =0.999 , ?=1e - 8",0.5259299874305725
translation,175,106,hyperparameters,hyperparameters,used,"adam ( kingma and ba , 2015 )","hyperparameters used adam ( kingma and ba , 2015 )",0.5725852847099304
translation,175,107,hyperparameters,exploding gradients,restricted,magnitude,exploding gradients restricted magnitude,0.762999415397644
translation,175,107,hyperparameters,magnitude,of,2 norm,magnitude of 2 norm,0.6157673597335815
translation,175,107,hyperparameters,2 norm,of,gradient,2 norm of gradient,0.5876482725143433
translation,175,107,hyperparameters,gradient,to,5,gradient to 5,0.6626070141792297
translation,175,107,hyperparameters,hyperparameters,To overcome,exploding gradients,hyperparameters To overcome exploding gradients,0.6488304138183594
translation,175,108,hyperparameters,batch size,during,training,batch size during training,0.7146576642990112
translation,175,108,hyperparameters,training,set to,32,training set to 32,0.6908778548240662
translation,175,108,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,175,109,hyperparameters,unischema model,initialized,parameters,unischema model initialized parameters,0.7435441613197327
translation,175,109,hyperparameters,parameters,from,trained onlykb model,parameters from trained onlykb model,0.5300949215888977
translation,175,109,hyperparameters,hyperparameters,To train,unischema model,hyperparameters To train unischema model,0.6347765922546387
translation,175,8,model,universal schema,to,natural language question answering,universal schema to natural language question answering,0.4739528298377991
translation,175,8,model,universal schema,employing,memory networks,universal schema employing memory networks,0.6195819973945618
translation,175,8,model,memory networks,to attend,large body of facts,memory networks to attend large body of facts,0.6224812269210815
translation,175,8,model,large body of facts,in,combination of text and kb,large body of facts in combination of text and kb,0.5180315971374512
translation,175,8,model,model,extend,universal schema,model extend universal schema,0.7642951011657715
translation,175,9,model,end-to - end fashion,on,question - answer pairs,end-to - end fashion on question - answer pairs,0.5889764428138733
translation,175,9,model,model,trained in,end-to - end fashion,model trained in end-to - end fashion,0.7573421001434326
translation,175,37,model,memory components,as,observed cells,memory components as observed cells,0.5186633467674255
translation,175,37,model,memory components,train,end-to - end qa model,memory components train end-to - end qa model,0.6905767321586609
translation,175,37,model,observed cells,of,universal schema matrix,observed cells of universal schema matrix,0.5852268934249878
translation,175,37,model,end-to - end qa model,on,question - answer pairs,end-to - end qa model on question - answer pairs,0.5533597469329834
translation,175,37,model,model,define,memory components,model define memory components,0.6948636770248413
translation,175,37,model,model,train,end-to - end qa model,model train end-to - end qa model,0.6962742209434509
translation,175,114,results,results,on,spades,results on spades,0.5373375415802002
translation,175,115,results,all our models,exploiting,universal schema,all our models exploiting universal schema,0.7228946089744568
translation,175,115,results,universal schema,for,qa,universal schema for qa,0.6733235120773315
translation,175,115,results,universal schema,is,better,universal schema is better,0.6232891082763672
translation,175,115,results,unis,has,outperforms,unis has outperforms,0.66144198179245
translation,175,115,results,outperforms,has,all our models,outperforms has all our models,0.5915884971618652
translation,175,115,results,results,has,unis,results has unis,0.5858990550041199
translation,175,116,results,spades creation process,friendly to,freebase,spades creation process friendly to freebase,0.6373742818832397
translation,175,116,results,spades creation process,exploiting,text,spades creation process exploiting text,0.7198756337165833
translation,175,116,results,text,provides,significant improvement,text provides significant improvement,0.6876083016395569
translation,175,116,results,results,Despite,spades creation process,results Despite spades creation process,0.6234914660453796
translation,175,128,results,outperforms,showing,joint modeling,outperforms showing joint modeling,0.7247296571731567
translation,175,128,results,en - semble,showing,joint modeling,en - semble showing joint modeling,0.7369899153709412
translation,175,128,results,joint modeling,superior to,ensemble,joint modeling superior to ensemble,0.6793151497840881
translation,175,128,results,ensemble,on,individual models,ensemble on individual models,0.5864368677139282
translation,175,128,results,unischema,has,outperforms,unischema has outperforms,0.6328371167182922
translation,175,128,results,outperforms,has,en - semble,outperforms has en - semble,0.6900482773780823
translation,175,128,results,results,has,unischema,results has unischema,0.553905189037323
translation,175,129,results,state - of- the - art,with,8.5 f 1 points difference,state - of- the - art with 8.5 f 1 points difference,0.6242496371269226
translation,176,83,baselines,baseline,has,feature-enriched pointer-generator model,baseline has feature-enriched pointer-generator model,0.5067506432533264
translation,176,83,baselines,baselines,has,baseline,baselines has baseline,0.6124745607376099
translation,176,84,baselines,baseline model,is,attention - based pointergenerator model,baseline model is attention - based pointergenerator model,0.4747919738292694
translation,176,84,baselines,attention - based pointergenerator model,enhanced with,various rich features,attention - based pointergenerator model enhanced with various rich features,0.6869921684265137
translation,176,84,baselines,baselines,has,baseline model,baselines has baseline model,0.5873855352401733
translation,176,161,baselines,neural question generation system,on,squad,neural question generation system on squad,0.585152804851532
translation,176,161,baselines,rich features,to,embedding layer,rich features to embedding layer,0.4988410472869873
translation,176,161,baselines,embedding layer,of,sequence - to-sequence model,embedding layer of sequence - to-sequence model,0.5560274720191956
translation,176,161,baselines,stateof - the- art,has,neural question generation system,stateof - the- art has neural question generation system,0.5157809257507324
translation,176,152,experiments,size,of,answer embedding,size of answer embedding,0.5913033485412598
translation,176,152,experiments,answer embedding,in,answerfocused model,answer embedding in answerfocused model,0.5430483818054199
translation,176,152,experiments,answer embedding,is,512,answer embedding is 512,0.6116524338722229
translation,176,103,hyperparameters,"dropout ( srivastava et al. , 2014 )",with,"maxout ( goodfellow et al. , 2013 )","dropout ( srivastava et al. , 2014 ) with maxout ( goodfellow et al. , 2013 )",0.5644457936286926
translation,176,103,hyperparameters,"maxout ( goodfellow et al. , 2013 )",to tackle,over-fitting problem,"maxout ( goodfellow et al. , 2013 ) to tackle over-fitting problem",0.6622132658958435
translation,176,103,hyperparameters,pretrained global vectors ( glove ),for,word representation,pretrained global vectors ( glove ) for word representation,0.5782630443572998
translation,176,103,hyperparameters,hyperparameters,introduce,"dropout ( srivastava et al. , 2014 )","hyperparameters introduce dropout ( srivastava et al. , 2014 )",0.5486308336257935
translation,176,103,hyperparameters,hyperparameters,introduce,pretrained global vectors ( glove ),hyperparameters introduce pretrained global vectors ( glove ),0.5723863244056702
translation,176,148,hyperparameters,pre-trained glove word vectors,with,300 dimensions,pre-trained glove word vectors with 300 dimensions,0.6206281781196594
translation,176,148,hyperparameters,pre-trained glove word vectors,to initialize,word embeddings,pre-trained glove word vectors to initialize word embeddings,0.675875186920166
translation,176,148,hyperparameters,300 dimensions,to initialize,word embeddings,300 dimensions to initialize word embeddings,0.6580886244773865
translation,176,148,hyperparameters,be further fine-tuned,in,training stage,be further fine-tuned in training stage,0.572372555732727
translation,176,148,hyperparameters,hyperparameters,use,pre-trained glove word vectors,hyperparameters use pre-trained glove word vectors,0.5621918439865112
translation,176,149,hyperparameters,representations,of,answer position feature and lexical features,representations of answer position feature and lexical features,0.5807275772094727
translation,176,149,hyperparameters,representations,at,embedding layer,representations at embedding layer,0.5547003746032715
translation,176,149,hyperparameters,representations,randomly initialized as,32 dimensional vectors,representations randomly initialized as 32 dimensional vectors,0.5687075853347778
translation,176,149,hyperparameters,answer position feature and lexical features,at,embedding layer,answer position feature and lexical features at embedding layer,0.5088785290718079
translation,176,149,hyperparameters,embedding layer,of,encoder,embedding layer of encoder,0.5838966965675354
translation,176,149,hyperparameters,encoder,randomly initialized as,32 dimensional vectors,encoder randomly initialized as 32 dimensional vectors,0.6498792767524719
translation,176,149,hyperparameters,trainable,during,training,trainable during training,0.7236456274986267
translation,176,149,hyperparameters,hyperparameters,has,representations,hyperparameters has representations,0.5416682362556458
translation,176,150,hyperparameters,hidden size,of,both the encoder and decoder,hidden size of both the encoder and decoder,0.5903543829917908
translation,176,150,hyperparameters,hidden size,is,512,hidden size is 512,0.6110948920249939
translation,176,150,hyperparameters,both the encoder and decoder,is,512,both the encoder and decoder is 512,0.5879077315330505
translation,176,150,hyperparameters,hyperparameters,has,hidden size,hyperparameters has hidden size,0.535134494304657
translation,176,151,hyperparameters,dropout,only in,encoder,dropout only in encoder,0.5855638980865479
translation,176,151,hyperparameters,encoder,with,dropout rate 0.5,encoder with dropout rate 0.5,0.6245630979537964
translation,176,151,hyperparameters,hyperparameters,use,dropout,hyperparameters use dropout,0.6254391074180603
translation,176,153,hyperparameters,position,indicates,relative distance,position indicates relative distance,0.6834399700164795
translation,176,153,hyperparameters,position,ranges from,0 to 80,position ranges from 0 to 80,0.739207923412323
translation,176,153,hyperparameters,relative distance,between,context words and the answer,relative distance between context words and the answer,0.6636679768562317
translation,176,153,hyperparameters,embedding size,in,position - aware model,embedding size in position - aware model,0.5029576420783997
translation,176,153,hyperparameters,position - aware model,is,16,position - aware model is 16,0.5792180895805359
translation,176,153,hyperparameters,hyperparameters,has,position,hyperparameters has position,0.5345064401626587
translation,176,154,hyperparameters,adagrad,with,learning rate,adagrad with learning rate,0.6288444995880127
translation,176,154,hyperparameters,adagrad,with,initial accumulator value,adagrad with initial accumulator value,0.5819831490516663
translation,176,154,hyperparameters,adagrad,with,batch size,adagrad with batch size,0.618696928024292
translation,176,154,hyperparameters,initial accumulator value,of,0.1,initial accumulator value of 0.1,0.6011174917221069
translation,176,154,hyperparameters,batch size,as,64,batch size as 64,0.5817323327064514
translation,176,154,hyperparameters,optimization algorithm,has,adagrad,optimization algorithm has adagrad,0.4837127923965454
translation,176,154,hyperparameters,learning rate,has,0.15,learning rate has 0.15,0.5453540682792664
translation,176,154,hyperparameters,hyperparameters,use,optimization algorithm,hyperparameters use optimization algorithm,0.6438109874725342
translation,176,155,hyperparameters,gradient clipping,with,maximum gradient norm,gradient clipping with maximum gradient norm,0.5783113837242126
translation,176,155,hyperparameters,maximum gradient norm,of,2,maximum gradient norm of 2,0.6099976897239685
translation,176,8,model,model,propose,answer-focused and position - aware neural question generation model,model propose answer-focused and position - aware neural question generation model,0.6563974618911743
translation,176,46,model,model,propose,answer-focused and position - aware neural question generation model,model propose answer-focused and position - aware neural question generation model,0.6563974618911743
translation,176,70,model,model,propose,answer-focused and position - aware neural question generation model,model propose answer-focused and position - aware neural question generation model,0.6563974618911743
translation,176,168,results,feature - enriched pointer- generator model,has,outperforms,feature - enriched pointer- generator model has outperforms,0.5968740582466125
translation,176,168,results,outperforms,has,nqg ++.,outperforms has nqg ++.,0.6143644452095032
translation,176,171,results,pointer - generator model,without,features,pointer - generator model without features,0.6907281875610352
translation,176,171,results,pointer - generator model,does not perform,well,pointer - generator model does not perform well,0.7205798029899597
translation,176,171,results,results,has,pointer - generator model,results has pointer - generator model,0.5361871719360352
translation,176,174,results,hybrid model,shows,best performance,hybrid model shows best performance,0.7154240012168884
translation,176,174,results,outperforms,has,two single models,outperforms has two single models,0.619047224521637
translation,176,174,results,results,has,hybrid model,results has hybrid model,0.5734009146690369
translation,176,182,results,answer-focused model,has,outperforms,answer-focused model has outperforms,0.6132601499557495
translation,176,182,results,outperforms,has,"strong baseline , featureenriched pointer - generator model","outperforms has strong baseline , featureenriched pointer - generator model",0.5658625364303589
translation,176,182,results,results,see that,answer-focused model,results see that answer-focused model,0.6663404107093811
translation,177,96,ablation-analysis,self-attention,makes,significant impact,self-attention makes significant impact,0.6217368245124817
translation,177,96,ablation-analysis,significant impact,on,results,significant impact on results,0.5534848570823669
translation,177,96,ablation-analysis,significant impact,improving,f1,significant impact improving f1,0.7163360118865967
translation,177,96,ablation-analysis,f1,by,2.6 %,f1 by 2.6 %,0.5937655568122864
translation,177,96,ablation-analysis,ablation analysis,addition of,self-attention,ablation analysis addition of self-attention,0.6112703680992126
translation,177,68,baselines,several baselines,for,piqa,several baselines for piqa,0.6181590557098389
translation,177,69,baselines,words,in,d and q,words in d and q,0.6045811176300049
translation,177,69,baselines,words,one of,three embedding mechanisms,words one of three embedding mechanisms,0.6826997995376587
translation,177,91,hyperparameters,default embedding model,is,charcnn,default embedding model is charcnn,0.5206128358840942
translation,177,91,hyperparameters,charcnn,concatenated with,200d glove,charcnn concatenated with 200d glove,0.733275830745697
translation,177,91,hyperparameters,option,to append,elmo vectors,option to append elmo vectors,0.6527145504951477
translation,177,91,hyperparameters,hyperparameters,has,default embedding model,hyperparameters has default embedding model,0.48604634404182434
translation,177,92,hyperparameters,batch size,of,64,batch size of 64,0.6741159558296204
translation,177,92,hyperparameters,20 epochs,with,default adam optimizer,20 epochs with default adam optimizer,0.5800143480300903
translation,177,92,hyperparameters,20 epochs,take,best model,20 epochs take best model,0.6253129243850708
translation,177,92,hyperparameters,best model,on,validation set,best model on validation set,0.5728336572647095
translation,177,92,hyperparameters,hyperparameters,use,batch size,hyperparameters use batch size,0.6251612901687622
translation,177,92,hyperparameters,hyperparameters,train for,20 epochs,hyperparameters train for 20 epochs,0.6661980748176575
translation,177,4,model,new modular variant,of,current question answering tasks,new modular variant of current question answering tasks,0.5479038953781128
translation,177,4,model,new modular variant,by enforcing,complete independence,new modular variant by enforcing complete independence,0.7063832879066467
translation,177,4,model,complete independence,of,document encoder,complete independence of document encoder,0.56350177526474
translation,177,4,model,document encoder,from,question encoder,document encoder from question encoder,0.5755650997161865
translation,177,4,model,model,formalize,new modular variant,model formalize new modular variant,0.7113204598426819
translation,177,18,model,modular variant,enforces,complete independence,modular variant enforces complete independence,0.7309421300888062
translation,177,18,model,complete independence,between,document encoder and question encoder,complete independence between document encoder and question encoder,0.6310383677482605
translation,177,18,model,model,formalize,modular variant,model formalize modular variant,0.7229103446006775
translation,177,19,model,all documents,processed,independently,all documents processed independently,0.6903938055038452
translation,177,19,model,independently,of,any question,independently of any question,0.5936736464500427
translation,177,19,model,phrase index vectors,for,each answer candidate,phrase index vectors for each answer candidate,0.5997152924537659
translation,177,19,model,piqa,has,all documents,piqa has all documents,0.6314740180969238
translation,177,19,model,model,In,piqa,model In piqa,0.5765050649642944
translation,177,94,results,results,for,piqa baselines,results for piqa baselines,0.6111404299736023
translation,177,95,results,tf -idf model,performs,poorly,tf -idf model performs poorly,0.6292951107025146
translation,177,95,results,results,has,tf -idf model,results has tf -idf model,0.5389666557312012
translation,177,97,results,elmo,gives,3.7 % and 2.9 % improvement,elmo gives 3.7 % and 2.9 % improvement,0.6101444959640503
translation,177,97,results,3.7 % and 2.9 % improvement,on,f1,3.7 % and 2.9 % improvement on f1,0.5647687911987305
translation,177,97,results,f1,for,lstm and lstm +sa models,f1 for lstm and lstm +sa models,0.6102668046951294
translation,177,97,results,results,adding,elmo,results adding elmo,0.6293230056762695
translation,177,98,results,best piqa baseline model,is,11.7 % higher,best piqa baseline model is 11.7 % higher,0.5192525386810303
translation,177,98,results,best piqa baseline model,is,26.6 % lower,best piqa baseline model is 26.6 % lower,0.5113294124603271
translation,177,98,results,11.7 % higher,than,first ( unconstrained ) baseline model,11.7 % higher than first ( unconstrained ) baseline model,0.5196065306663513
translation,177,98,results,26.6 % lower,than,state of the art,26.6 % lower than state of the art,0.5382705926895142
translation,177,98,results,results,has,best piqa baseline model,results has best piqa baseline model,0.5459557771682739
translation,178,37,ablation-analysis,qags,robust to,number of factors,qags robust to number of factors,0.6837616562843323
translation,178,37,ablation-analysis,ablation analysis,show via ablations,qags,ablation analysis show via ablations qags,0.7728191614151001
translation,178,6,baselines,automatic evaluation protocol,to identify,factual inconsistencies,automatic evaluation protocol to identify factual inconsistencies,0.6675271987915039
translation,178,6,baselines,factual inconsistencies,in,generated summary,factual inconsistencies in generated summary,0.5123876929283142
translation,178,6,baselines,qags,has,automatic evaluation protocol,qags has automatic evaluation protocol,0.5656588673591614
translation,178,103,experiments,extractive summarization models,perform,poorly,extractive summarization models perform poorly,0.5830946564674377
translation,178,132,experiments,extractive qa models,by,fine-tuning,extractive qa models by fine-tuning,0.519415557384491
translation,178,132,experiments,fine-tuning,on,squad2.0,fine-tuning on squad2.0,0.5574355125427246
translation,178,132,experiments,fine-tuning,has,"bert ( devlin et al. , 2019 )","fine-tuning has bert ( devlin et al. , 2019 )",0.5497755408287048
translation,178,144,experiments,qags,obtains,nearly twice the correlation,qags obtains nearly twice the correlation,0.618685781955719
translation,178,144,experiments,nearly twice the correlation,of,next best automatic metric ( bleu - 1 ),nearly twice the correlation of next best automatic metric ( bleu - 1 ),0.5542792677879333
translation,178,144,experiments,cnn / dm,has,qags,cnn / dm has qags,0.6524752974510193
translation,178,157,experiments,cnn / dm,has,bert - large - wwm,cnn / dm has bert - large - wwm,0.6286556720733643
translation,178,157,experiments,bert - large - wwm,has,slightly underperforms,bert - large - wwm has slightly underperforms,0.611997127532959
translation,178,157,experiments,slightly underperforms,has,bert- base,slightly underperforms has bert- base,0.6318711638450623
translation,178,23,model,three steps,Given,generated text,three steps Given generated text,0.7203885912895203
translation,178,23,model,question generation ( qg ) model,generates,set of questions,question generation ( qg ) model generates set of questions,0.6767186522483826
translation,178,23,model,set of questions,about,text,set of questions about text,0.6502655148506165
translation,178,23,model,three steps,has,question generation ( qg ) model,three steps has question generation ( qg ) model,0.5529792308807373
translation,178,23,model,generated text,has,question generation ( qg ) model,generated text has question generation ( qg ) model,0.5787320137023926
translation,178,32,results,qags,shows,robustness,qags shows robustness,0.7269092202186584
translation,178,32,results,robustness,to,quality,robustness to quality,0.5585073232650757
translation,178,32,results,robustness,to,domain of the models,robustness to domain of the models,0.5673090815544128
translation,178,32,results,quality,of,underlying qg and qa models,quality of underlying qg and qa models,0.6060231328010559
translation,178,32,results,results,has,qags,results has qags,0.5670158863067627
translation,178,141,results,other automatic evaluation metrics,in terms of,correlation,other automatic evaluation metrics in terms of correlation,0.637806236743927
translation,178,141,results,correlation,with,summary - level human judgments of factual consistency,correlation with summary - level human judgments of factual consistency,0.5961493849754333
translation,178,141,results,qags,has,strongly outperforms,qags has strongly outperforms,0.6361513137817383
translation,178,141,results,strongly outperforms,has,other automatic evaluation metrics,strongly outperforms has other automatic evaluation metrics,0.5824556946754456
translation,178,141,results,results,has,qags,results has qags,0.5670158863067627
translation,178,142,results,bleu and rouge,perform,comparably,bleu and rouge perform comparably,0.5601980090141296
translation,178,142,results,lower order n-gram metrics,work,better,lower order n-gram metrics work better,0.6408320069313049
translation,178,142,results,results,has,bleu and rouge,results has bleu and rouge,0.555298388004303
translation,178,143,results,bertscore,matches,best ngram metrics,bertscore matches best ngram metrics,0.7870743274688721
translation,178,143,results,bertscore,matches,worst,bertscore matches worst,0.7645615339279175
translation,178,143,results,best ngram metrics,on,cnn / dm,best ngram metrics on cnn / dm,0.5353625416755676
translation,178,143,results,worst,on,xsum,worst on xsum,0.575076699256897
translation,178,143,results,results,has,bertscore,results has bertscore,0.5357906818389893
translation,178,147,results,all metrics,correlate worse with,human judgments,all metrics correlate worse with human judgments,0.6813086271286011
translation,178,147,results,human judgments,than,cnn / dm,human judgments than cnn / dm,0.6098073124885559
translation,178,147,results,xsum,has,all metrics,xsum has all metrics,0.5988572835922241
translation,178,148,results,outperforms,has,next best automatic metric,outperforms has next best automatic metric,0.6191501021385193
translation,178,148,results,results,has,qags,results has qags,0.5670158863067627
translation,178,156,results,best qa model ( bert-large - wwm ),lead to,best correlations,best qa model ( bert-large - wwm ) lead to best correlations,0.6783502697944641
translation,178,156,results,best correlations,with,human judgments,best correlations with human judgments,0.6014511585235596
translation,178,156,results,results,using,best qa model ( bert-large - wwm ),results using best qa model ( bert-large - wwm ),0.6795933842658997
translation,178,158,results,xsum,has,bert - base,xsum has bert - base,0.6566473841667175
translation,178,158,results,bert - base,has,slightly outperforms,bert - base has slightly outperforms,0.623326301574707
translation,178,158,results,slightly outperforms,has,other two bert variants,slightly outperforms has other two bert variants,0.5884121060371399
translation,178,158,results,results,On,xsum,results On xsum,0.6111196279525757
translation,178,161,results,qags,robust to,qg model quality,qags robust to qg model quality,0.7199594974517822
translation,178,161,results,qg model quality,with,some decrease,qg model quality with some decrease,0.6408833861351013
translation,178,161,results,some decrease,in,correlation,some decrease in correlation,0.5524245500564575
translation,178,161,results,some decrease,as,perplexity,some decrease as perplexity,0.5377671122550964
translation,178,161,results,correlation,with,human judgments,correlation with human judgments,0.6536168456077576
translation,178,161,results,correlation,as,perplexity,correlation as perplexity,0.5467718839645386
translation,178,161,results,perplexity,on,cnn / dm,perplexity on cnn / dm,0.5371401309967041
translation,178,161,results,increases,on,cnn / dm,increases on cnn / dm,0.5990618467330933
translation,178,161,results,perplexity,has,increases,perplexity has increases,0.6041359305381775
translation,178,161,results,results,show,qags,results show qags,0.6542192697525024
translation,178,181,results,qags,has,outperforms,qags has outperforms,0.6682443022727966
translation,178,181,results,results,has,qags,results has qags,0.5670158863067627
translation,179,38,hyperparameters,crowd workers,to label,events,crowd workers to label events,0.7125152349472046
translation,179,38,hyperparameters,crowd workers,to,write and answer questions,crowd workers to write and answer questions,0.529914915561676
translation,179,38,hyperparameters,events,in,text,events in text,0.5658352375030518
translation,179,38,hyperparameters,write and answer questions,that query,temporal relationships,write and answer questions that query temporal relationships,0.776568591594696
translation,179,38,hyperparameters,temporal relationships,between,events,temporal relationships between events,0.6757624745368958
translation,179,38,hyperparameters,hyperparameters,trained,crowd workers,hyperparameters trained crowd workers,0.7202362418174744
translation,179,38,hyperparameters,hyperparameters,to,write and answer questions,hyperparameters to write and answer questions,0.500545084476471
translation,179,198,hyperparameters,batch size,=,6,batch size = 6,0.6593085527420044
translation,179,198,hyperparameters,hyperparameters,fixed,batch size,hyperparameters fixed batch size,0.6912372708320618
translation,179,199,hyperparameters,learning rate,from,"( 1e ?5 , 2e ?5 )","learning rate from ( 1e ?5 , 2e ?5 )",0.5429801344871521
translation,179,199,hyperparameters,training epoch,within,10 ),training epoch within 10 ),0.6728331446647644
translation,179,199,hyperparameters,random seed,from,3 arbitrary ones ),random seed from 3 arbitrary ones ),0.5830520391464233
translation,179,199,hyperparameters,random seed,based on,performance,random seed based on performance,0.7096273899078369
translation,179,199,hyperparameters,performance,on,dev set of torque,performance on dev set of torque,0.5986378788948059
translation,179,199,hyperparameters,hyperparameters,selected,learning rate,hyperparameters selected learning rate,0.576492965221405
translation,179,199,hyperparameters,hyperparameters,selected,random seed,hyperparameters selected random seed,0.6713066101074219
translation,179,186,results,worker agreement with aggregate ( wawa ) f 1,is,94.2 %,worker agreement with aggregate ( wawa ) f 1 is 94.2 %,0.5479359030723572
translation,179,186,results,worker agreement with aggregate ( wawa ) f 1,compare,majority - vote,worker agreement with aggregate ( wawa ) f 1 compare majority - vote,0.6562755703926086
translation,179,186,results,worker agreement with aggregate ( wawa ) f 1,perform,micro-average,worker agreement with aggregate ( wawa ) f 1 perform micro-average,0.5480405688285828
translation,179,186,results,majority - vote,with,all annotators,majority - vote with all annotators,0.6019472479820251
translation,179,186,results,micro-average,on,all instances,micro-average on all instances,0.5749590396881104
translation,179,186,results,results,has,worker agreement with aggregate ( wawa ) f 1,results has worker agreement with aggregate ( wawa ) f 1,0.5447013974189758
translation,179,189,results,wawa f 1,of,answer annotations,wawa f 1 of answer annotations,0.5855507850646973
translation,179,189,results,answer annotations,is,84.7 %,answer annotations is 84.7 %,0.5467618107795715
translation,179,189,results,slightly lower,than,events,slightly lower than events,0.6296441555023193
translation,179,189,results,results,has,wawa f 1,results has wawa f 1,0.5727332830429077
translation,179,205,results,roberta - large,is,best system,roberta - large is best system,0.6086189150810242
translation,179,205,results,roberta - large,expectedly,best system,roberta - large expectedly best system,0.7604355216026306
translation,179,205,results,roberta - large,still far behind,human performance,roberta - large still far behind human performance,0.6881543397903442
translation,179,205,results,trailing,by,about 30 %,trailing by about 30 %,0.6527732014656067
translation,179,205,results,about 30 %,in,em,about 30 % in em,0.6853085160255432
translation,179,205,results,human performance,has,trailing,human performance has trailing,0.5943820476531982
translation,179,205,results,results,has,roberta - large,results has roberta - large,0.5811555981636047
translation,180,29,ablation-analysis,uptraining,with,"2,000 labeled questions","uptraining with 2,000 labeled questions",0.6204957365989685
translation,180,29,ablation-analysis,"2,000 labeled questions",improves,accuracy,"2,000 labeled questions improves accuracy",0.6718354225158691
translation,180,29,ablation-analysis,accuracy,to,84.14 %,accuracy to 84.14 %,0.5315929651260376
translation,180,29,ablation-analysis,accuracy,fully recovering,drop,accuracy fully recovering drop,0.7435246706008911
translation,180,29,ablation-analysis,drop,between,in - domain and out -of- domain accuracy,drop between in - domain and out -of- domain accuracy,0.671257734298706
translation,180,29,ablation-analysis,ablation analysis,Combining,uptraining,ablation analysis Combining uptraining,0.7429349422454834
translation,180,85,ablation-analysis,lexicalized parser,loses,1.5 % f 1,lexicalized parser loses 1.5 % f 1,0.6724631786346436
translation,180,85,ablation-analysis,latent variable parser,loses,only 0.7 %,latent variable parser loses only 0.7 %,0.7092083692550659
translation,180,85,ablation-analysis,training and test data,has,lexicalized parser,training and test data has lexicalized parser,0.5905495882034302
translation,180,85,ablation-analysis,ablation analysis,When,training and test data,ablation analysis When training and test data,0.6534883975982666
translation,180,79,experiments,constituency parsers,observe,lexicalized ( reranking ) parser,constituency parsers observe lexicalized ( reranking ) parser,0.5563641786575317
translation,180,79,experiments,constituency parsers,observe,loses more,constituency parsers observe loses more,0.6155049800872803
translation,180,79,experiments,lexicalized ( reranking ) parser,and Johnson ( 2005 ),loses more,lexicalized ( reranking ) parser and Johnson ( 2005 ) loses more,0.6372930407524109
translation,180,79,experiments,loses more,than,latent variable approach,loses more than latent variable approach,0.5837001800537109
translation,180,79,experiments,lexicalized ( reranking ) parser,has,loses more,lexicalized ( reranking ) parser has loses more,0.5953720211982727
translation,180,9,results,results,comparable to,2 k labeled questions,results comparable to 2 k labeled questions,0.6063216328620911
translation,180,9,results,2 k labeled questions,for,training,2 k labeled questions for training,0.5718625783920288
translation,180,9,results,results,Uptraining with,100k unlabeled questions,results Uptraining with 100k unlabeled questions,0.6818252205848694
translation,180,28,results,accuracy,of,linear time parser,accuracy of linear time parser,0.5599310398101807
translation,180,28,results,linear time parser,on,question test set,linear time parser on question test set,0.5278289318084717
translation,180,28,results,60.06 % ( las ),to,76.94 %,60.06 % ( las ) to 76.94 %,0.5986294746398926
translation,180,28,results,76.94 %,after,uptraining,76.94 % after uptraining,0.6773662567138672
translation,180,28,results,results,has,accuracy,results has accuracy,0.5888755321502686
translation,180,95,results,even a modest amount of labeled data,from,target domain,even a modest amount of labeled data from target domain,0.52399742603302
translation,180,95,results,significantly boost,giving,double - digit improvements,significantly boost giving double - digit improvements,0.6773673295974731
translation,180,95,results,parsing performance,giving,double - digit improvements,parsing performance giving double - digit improvements,0.6756115555763245
translation,180,95,results,double - digit improvements,in,some cases,double - digit improvements in some cases,0.5420073866844177
translation,180,95,results,significantly boost,has,parsing performance,significantly boost has parsing performance,0.5560665726661682
translation,180,127,results,self-training,provides,only modest improvements,self-training provides only modest improvements,0.598347544670105
translation,180,127,results,only modest improvements,of,less than 2 %,only modest improvements of less than 2 %,0.5667251944541931
translation,180,127,results,uptraining,gives,double - digit improvements,uptraining gives double - digit improvements,0.6159704923629761
translation,180,127,results,double - digit improvements,in,some cases,double - digit improvements in some cases,0.5420073866844177
translation,180,131,results,our uptraining procedure,improves,parse quality,our uptraining procedure improves parse quality,0.7067641019821167
translation,180,131,results,parse quality,on,out -of- domain data,parse quality on out -of- domain data,0.5422801971435547
translation,180,131,results,parse quality,level of,in-domain accuracy,parse quality level of in-domain accuracy,0.40270766615867615
translation,180,131,results,results,has,our uptraining procedure,results has our uptraining procedure,0.5208362936973572
translation,180,184,results,large amounts of unlabeled data,gives,similar improvements,large amounts of unlabeled data gives similar improvements,0.5746576189994812
translation,180,184,results,similar improvements,having access to,"2,000 labeled sentences","similar improvements having access to 2,000 labeled sentences",0.5972258448600769
translation,180,184,results,"2,000 labeled sentences",from,target domain,"2,000 labeled sentences from target domain",0.5177401900291443
translation,180,184,results,results,Uptraining with,large amounts of unlabeled data,results Uptraining with large amounts of unlabeled data,0.7072080373764038
translation,181,33,ablation-analysis,prior distribution,of,answer positions,prior distribution of answer positions,0.573618471622467
translation,181,33,ablation-analysis,prior distribution,helps us to build,positionally de-biased models,prior distribution helps us to build positionally de-biased models,0.5944703817367554
translation,181,33,ablation-analysis,answer positions,helps us to build,positionally de-biased models,answer positions helps us to build positionally de-biased models,0.5799725651741028
translation,181,33,ablation-analysis,positionally de-biased models,recovering,performance,positionally de-biased models recovering performance,0.7559903264045715
translation,181,33,ablation-analysis,performance,of,bert,performance of bert,0.6690750122070312
translation,181,33,ablation-analysis,ablation analysis,show,prior distribution,ablation analysis show prior distribution,0.6235385537147522
translation,181,50,ablation-analysis,relative position encodings,in,xlnet,relative position encodings in xlnet,0.5523152351379395
translation,181,50,ablation-analysis,relative position encodings,mitigate,position bias,relative position encodings mitigate position bias,0.6706412434577942
translation,181,50,ablation-analysis,position bias,to,some extent,position bias to some extent,0.5897268652915955
translation,181,50,ablation-analysis,performance,has,degrades,performance has degrades,0.5837839841842651
translation,181,50,ablation-analysis,degrades,has,significantly,degrades has significantly,0.6197282075881958
translation,181,50,ablation-analysis,ablation analysis,has,relative position encodings,ablation analysis has relative position encodings,0.565968930721283
translation,181,179,ablation-analysis,bert,trained on,"biased datasets ( k = 1 and k = 2 , 3 , ... )","bert trained on biased datasets ( k = 1 and k = 2 , 3 , ... )",0.7532252669334412
translation,181,179,ablation-analysis,"biased datasets ( k = 1 and k = 2 , 3 , ... )",significantly suffers from,position bias,"biased datasets ( k = 1 and k = 2 , 3 , ... ) significantly suffers from position bias",0.677722692489624
translation,181,128,experimental-setup,entropy regularization,set to,5,entropy regularization set to 5,0.657124936580658
translation,181,128,experimental-setup,experimental setup,for,entropy regularization,experimental setup for entropy regularization,0.5407974123954773
translation,181,129,experimental-setup,implementation,based on,pytorch library,implementation based on pytorch library,0.5876471996307373
translation,181,30,model,prior distribution,of,answer positions,prior distribution of answer positions,0.573618471622467
translation,181,30,model,answer positions,as,additional bias model,answer positions as additional bias model,0.5057575106620789
translation,181,30,model,models,to learn,reasoning ability,models to learn reasoning ability,0.6321811079978943
translation,181,30,model,reasoning ability,beyond,positional cues,reasoning ability beyond positional cues,0.6541642546653748
translation,181,30,model,model,use,prior distribution,model use prior distribution,0.6787755489349365
translation,181,30,model,model,train,models,model train models,0.6628757119178772
translation,181,73,model,simple remedies,for,bert,simple remedies for bert,0.661008894443512
translation,181,73,model,bias ensemble method,with,answer prior distributions,bias ensemble method with answer prior distributions,0.6045331954956055
translation,181,73,model,answer prior distributions,be applied to,any qa models,answer prior distributions be applied to any qa models,0.696577787399292
translation,181,73,model,simple remedies,has,bias ensemble method,simple remedies has bias ensemble method,0.5559930801391602
translation,181,73,model,model,To prevent,bias ensemble method,model To prevent bias ensemble method,0.6402917504310608
translation,181,73,model,model,introduce,simple remedies,model introduce simple remedies,0.6884732842445374
translation,181,73,model,model,introduce,bias ensemble method,model introduce bias ensemble method,0.6265454888343811
translation,181,189,model,position bias,coming from,prediction structure,position bias coming from prediction structure,0.6217015981674194
translation,181,189,model,position bias,show that,positionally biased models,position bias show that positionally biased models,0.5162700414657593
translation,181,189,model,prediction structure,of,qa models,prediction structure of qa models,0.6293907165527344
translation,181,189,model,positionally biased models,ignore,information,positionally biased models ignore information,0.7599145770072937
translation,181,189,model,information,in,different positions,information in different positions,0.5371537804603577
translation,181,189,model,model,define,position bias,model define position bias,0.6534535884857178
translation,181,189,model,model,show that,positionally biased models,model show that positionally biased models,0.5793778300285339
translation,181,29,results,performance,of,most models,performance of most models,0.5527653098106384
translation,181,29,results,improve,has,test performance,improve has test performance,0.5463531017303467
translation,181,29,results,our ensemble - based de-biasing method,has,largely improves,our ensemble - based de-biasing method has largely improves,0.5610132813453674
translation,181,29,results,largely improves,has,performance,largely improves has performance,0.5754314661026001
translation,181,49,results,performances,of,all models,performances of all models,0.5580928921699524
translation,181,49,results,performances,of,models,performances of models,0.6228134632110596
translation,181,49,results,drop,compared to,models,drop compared to models,0.6611351370811462
translation,181,49,results,models,trained on,squad train,models trained on squad train,0.7563775181770325
translation,181,49,results,models,trained on,squad train ( sampled ),models trained on squad train ( sampled ),0.7630215287208557
translation,181,49,results,all models,has,drop,all models has drop,0.6112540364265442
translation,181,49,results,drop,has,significantly,drop has significantly,0.6707428097724915
translation,181,49,results,results,has,performances,results has performances,0.5711642503738403
translation,181,54,results,sentence order,of,squad k=1 train,sentence order of squad k=1 train,0.5680385828018188
translation,181,54,results,sentence order,recovers,most performance,sentence order recovers most performance,0.6956325173377991
translation,181,54,results,results,Shuffling,sentence order,results Shuffling sentence order,0.6538127660751343
translation,181,136,results,our simple baseline approaches,used in,bert,our simple baseline approaches used in bert,0.7430537939071655
translation,181,136,results,our simple baseline approaches,improve,performance,our simple baseline approaches improve performance,0.6435469388961792
translation,181,136,results,bert,improve,performance,bert improve performance,0.7280694842338562
translation,181,136,results,performance,up to,34.63 % f1 score ( random position ),performance up to 34.63 % f1 score ( random position ),0.6178148984909058
translation,181,136,results,entropy regularization,not,significantly effective,entropy regularization not significantly effective,0.6610632538795471
translation,181,136,results,results,has,our simple baseline approaches,results has our simple baseline approaches,0.5471576452255249
translation,181,137,results,bias ensemble methods,using,answer priors,bias ensemble methods using answer priors,0.6558817028999329
translation,181,137,results,performance,of,all models,performance of all models,0.5688430666923523
translation,181,137,results,answer priors,has,consistently improve,answer priors has consistently improve,0.5722557306289673
translation,181,137,results,consistently improve,has,performance,consistently improve has performance,0.5731132626533508
translation,181,137,results,results,has,bias ensemble methods,results has bias ensemble methods,0.5070342421531677
translation,181,138,results,sentence - level answer prior,works,best,sentence - level answer prior works best,0.5988730192184448
translation,181,138,results,best,obtains,significant gain,best obtains significant gain,0.6293593645095825
translation,181,138,results,significant gain,after applying,learned - mixin method,significant gain after applying learned - mixin method,0.7829951047897339
translation,181,138,results,results,has,sentence - level answer prior,results has sentence - level answer prior,0.5552839040756226
translation,182,169,ablation-analysis,mcb,with,simple concatenation,mcb with simple concatenation,0.6657058596611023
translation,182,169,ablation-analysis,simple concatenation,of,embedded visual feature and the embedded phrase,simple concatenation of embedded visual feature and the embedded phrase,0.5468272566795349
translation,182,169,ablation-analysis,simple concatenation,resulting in,25.48 %,simple concatenation resulting in 25.48 %,0.6377295851707458
translation,182,169,ablation-analysis,embedded visual feature and the embedded phrase,resulting in,25.48 %,embedded visual feature and the embedded phrase resulting in 25.48 %,0.6529927849769592
translation,182,169,ablation-analysis,46.5 %,on,flickr30k entities,46.5 % on flickr30k entities,0.5131017565727234
translation,182,169,ablation-analysis,25.48 %,on,refer-itgame datasets,25.48 % on refer-itgame datasets,0.5181765556335449
translation,182,169,ablation-analysis,ablation analysis,replace,mcb,ablation analysis replace mcb,0.6188364028930664
translation,182,170,ablation-analysis,results,replacing,concatenation,results replacing concatenation,0.5959091186523438
translation,182,170,ablation-analysis,concatenation,with,element - wise product,concatenation with element - wise product,0.6382783651351929
translation,182,170,ablation-analysis,element - wise product,of,both embedded features ( 47.41 % and 27.80 % ),element - wise product of both embedded features ( 47.41 % and 27.80 % ),0.5716944336891174
translation,182,170,ablation-analysis,ablation analysis,replacing,concatenation,ablation analysis replacing concatenation,0.6981297731399536
translation,182,170,ablation-analysis,ablation analysis,has,results,ablation analysis has results,0.4875600337982178
translation,182,171,ablation-analysis,performance,by introducing,additional 2048 - d convolution,performance by introducing additional 2048 - d convolution,0.6656619310379028
translation,182,171,ablation-analysis,additional 2048 - d convolution,after,element- wise product ( 47.86 % and 27.98 % ),additional 2048 - d convolution after element- wise product ( 47.86 % and 27.98 % ),0.648837685585022
translation,182,171,ablation-analysis,slightly increase,has,performance,slightly increase has performance,0.5507421493530273
translation,182,149,baselines,best single model,uses,mcb pooling,best single model uses mcb pooling,0.5616409778594971
translation,182,149,baselines,mcb pooling,with,two attention maps,mcb pooling with two attention maps,0.6181455850601196
translation,182,149,baselines,baselines,has,best single model,baselines has best single model,0.550260603427887
translation,182,152,baselines,each model,in,ensemble of 7 models,each model in ensemble of 7 models,0.5196931958198547
translation,182,152,baselines,ensemble of 7 models,uses,mcb,ensemble of 7 models uses mcb,0.5865442156791687
translation,182,152,baselines,mcb,with,attention,mcb with attention,0.6883046627044678
translation,182,152,baselines,baselines,has,each model,baselines has each model,0.5559836030006409
translation,182,25,experiments,mcb,to predict,answers,mcb to predict answers,0.7787976861000061
translation,182,25,experiments,mcb,to predict,locations,mcb to predict locations,0.7610762715339661
translation,182,25,experiments,answers,for,vqa task,answers for vqa task,0.6342094540596008
translation,182,25,experiments,locations,for,visual grounding task,locations for visual grounding task,0.6136574745178223
translation,182,144,experiments,one attenion map,achieves,64.67 %,one attenion map achieves 64.67 %,0.6826825141906738
translation,182,144,experiments,one attenion map,achieves,two 65.08 % and four 64.24 % accuracy,one attenion map achieves two 65.08 % and four 64.24 % accuracy,0.6551487445831299
translation,182,161,experiments,"adam solver ( kingma and ba , 2014 )",with,learning rate = 0.0001,"adam solver ( kingma and ba , 2014 ) with learning rate = 0.0001",0.5678787231445312
translation,182,79,hyperparameters,image features,using,152 - layer residual network,image features using 152 - layer residual network,0.6486492156982422
translation,182,79,hyperparameters,152 - layer residual network,pretrained on,imagenet data,152 - layer residual network pretrained on imagenet data,0.7833379507064819
translation,182,79,hyperparameters,hyperparameters,extract,image features,hyperparameters extract image features,0.710246741771698
translation,182,122,hyperparameters,adam solver,with,"= 0.0007 , ? 1 = 0.9 , ? 2 = 0.999","adam solver with = 0.0007 , ? 1 = 0.9 , ? 2 = 0.999",0.6314033269882202
translation,182,122,hyperparameters,hyperparameters,use,adam solver,hyperparameters use adam solver,0.5999480485916138
translation,182,123,hyperparameters,dropout,after,lstm layers,dropout after lstm layers,0.6430832743644714
translation,182,123,hyperparameters,dropout,after,fully connected layers,dropout after fully connected layers,0.6131482124328613
translation,182,123,hyperparameters,dropout,in,fully connected layers,dropout in fully connected layers,0.49653321504592896
translation,182,123,hyperparameters,hyperparameters,use,dropout,hyperparameters use dropout,0.6254391074180603
translation,182,125,hyperparameters,early stopping,if,validation score,early stopping if validation score,0.6062118411064148
translation,182,125,hyperparameters,early stopping,evaluate,best iteration,early stopping evaluate best iteration,0.6628313064575195
translation,182,125,hyperparameters,validation score,evaluate,best iteration,validation score evaluate best iteration,0.6947804689407349
translation,182,125,hyperparameters,does not improve,for,"50,000 iterations","does not improve for 50,000 iterations",0.6206985116004944
translation,182,125,hyperparameters,"50,000 iterations",stop,training,"50,000 iterations stop training",0.7155661582946777
translation,182,125,hyperparameters,best iteration,on,test- dev,best iteration on test- dev,0.5561904311180115
translation,182,125,hyperparameters,validation score,has,does not improve,validation score has does not improve,0.6001211404800415
translation,182,125,hyperparameters,hyperparameters,stop,training,hyperparameters stop training,0.6751257181167603
translation,182,151,hyperparameters,learned word embedding,with,pretrained glove vectors,learned word embedding with pretrained glove vectors,0.5819337964057922
translation,182,151,hyperparameters,hyperparameters,concatenate,learned word embedding,hyperparameters concatenate learned word embedding,0.6297315359115601
translation,182,162,hyperparameters,embedding size,is,500,embedding size is 500,0.6184056401252747
translation,182,162,hyperparameters,500,both for,visual and language embeddings,500 both for visual and language embeddings,0.6177071928977966
translation,182,162,hyperparameters,hyperparameters,has,embedding size,hyperparameters has embedding size,0.4976881444454193
translation,182,163,hyperparameters,d = 2048,in,mcb pooling,d = 2048 in mcb pooling,0.5822204351425171
translation,182,163,hyperparameters,d = 2048,found to work,best,d = 2048 found to work best,0.7611345648765564
translation,182,163,hyperparameters,best,for,visual grounding task,best for visual grounding task,0.6106373071670532
translation,182,8,model,multimodal compact bilinear pooling ( mcb ),to efficiently and expressively combine,multimodal features,multimodal compact bilinear pooling ( mcb ) to efficiently and expressively combine multimodal features,0.6533706784248352
translation,182,8,model,model,utilizing,multimodal compact bilinear pooling ( mcb ),model utilizing multimodal compact bilinear pooling ( mcb ),0.6015554070472717
translation,182,11,model,visual question answering,present,architecture,visual question answering present architecture,0.5958889126777649
translation,182,11,model,architecture,uses,mcb,architecture uses mcb,0.6367161273956299
translation,182,11,model,once,for predicting attention,spatial features,once for predicting attention spatial features,0.8458552360534668
translation,182,11,model,attended representation,with,question representation,attended representation with question representation,0.6112896203994751
translation,182,11,model,mcb,has,once,mcb has once,0.6765809655189514
translation,182,11,model,model,For,visual question answering,model For visual question answering,0.5440083742141724
translation,182,18,model,multimodal compact bilinear pooling ( mcb ),to get,joint representation,multimodal compact bilinear pooling ( mcb ) to get joint representation,0.5907497406005859
translation,182,18,model,model,rely on,multimodal compact bilinear pooling ( mcb ),model rely on multimodal compact bilinear pooling ( mcb ),0.6739158034324646
translation,182,19,model,bilinear pooling,computes,outer product,bilinear pooling computes outer product,0.7316617369651794
translation,182,19,model,outer product,between,two vectors,outer product between two vectors,0.6752264499664307
translation,182,19,model,outer product,allows,multiplicative interaction,outer product allows multiplicative interaction,0.6615821719169617
translation,182,19,model,multiplicative interaction,between,all elements,multiplicative interaction between all elements,0.6628621816635132
translation,182,19,model,all elements,has,of both vectors,all elements has of both vectors,0.6008092164993286
translation,182,19,model,model,has,bilinear pooling,model has bilinear pooling,0.49822714924812317
translation,182,24,model,multimodal compact bilinear pooling ( mcb ),convolving,both vectors,multimodal compact bilinear pooling ( mcb ) convolving both vectors,0.7056487202644348
translation,182,24,model,image and text representations,to,higher dimensional space,image and text representations to higher dimensional space,0.5386510491371155
translation,182,24,model,both vectors,by using,element - wise product,both vectors by using element - wise product,0.6730813384056091
translation,182,24,model,element - wise product,in,fast fourier transform ( fft ) space,element - wise product in fast fourier transform ( fft ) space,0.5339222550392151
translation,182,24,model,model,has,multimodal compact bilinear pooling ( mcb ),model has multimodal compact bilinear pooling ( mcb ),0.5453982949256897
translation,182,26,model,open-ended question answering,present,architecture,open-ended question answering present architecture,0.635893702507019
translation,182,26,model,architecture,for,vqa,architecture for vqa,0.6557443141937256
translation,182,26,model,vqa,uses,mcb twice,vqa uses mcb twice,0.6787253022193909
translation,182,26,model,mcb twice,to predict,spatial attention,mcb twice to predict spatial attention,0.7192564606666565
translation,182,26,model,mcb twice,to predict,second time,mcb twice to predict second time,0.741468071937561
translation,182,26,model,once,to predict,spatial attention,once to predict spatial attention,0.7143999338150024
translation,182,26,model,second time,to predict,answer,second time to predict answer,0.7326420545578003
translation,182,26,model,mcb twice,has,once,mcb twice has once,0.6343861222267151
translation,182,26,model,model,For,open-ended question answering,model For open-ended question answering,0.6183542609214783
translation,182,27,model,multiple -choice question answering,introduce,third mcb,multiple -choice question answering introduce third mcb,0.5527071952819824
translation,182,27,model,third mcb,to relate,encoded answer,third mcb to relate encoded answer,0.7184323072433472
translation,182,27,model,model,For,multiple -choice question answering,model For multiple -choice question answering,0.5360770225524902
translation,182,82,model,input questions,tokenized into,words,input questions tokenized into words,0.7279879450798035
translation,182,82,model,input questions,tokenized into,words,input questions tokenized into words,0.7279879450798035
translation,182,82,model,input questions,passed through,learned embedding layer,input questions passed through learned embedding layer,0.6736882328987122
translation,182,82,model,words,passed through,learned embedding layer,words passed through learned embedding layer,0.6160977482795715
translation,182,82,model,model,has,input questions,model has input questions,0.5473405718803406
translation,182,84,model,embedding layer,followed by,2 - layer lstm,embedding layer followed by 2 - layer lstm,0.6109524965286255
translation,182,84,model,2 - layer lstm,with,1024 units,2 - layer lstm with 1024 units,0.570402979850769
translation,182,84,model,1024 units,in,each layer,1024 units in each layer,0.5271475315093994
translation,182,84,model,model,has,embedding layer,model has embedding layer,0.5496405363082886
translation,182,109,model,concatenation,of,visual representation and the encoded phrase,concatenation of visual representation and the encoded phrase,0.5791695713996887
translation,182,109,model,visual representation and the encoded phrase,in,grounder,visual representation and the encoded phrase in grounder,0.5306703448295593
translation,182,109,model,grounder,with,mcb,grounder with mcb,0.6837412118911743
translation,182,109,model,mcb,to combine,both modalities,mcb to combine both modalities,0.7295961976051331
translation,182,109,model,model,replace,concatenation,model replace concatenation,0.6338657140731812
translation,182,10,results,benefit,of,mcb,benefit of mcb,0.6058035492897034
translation,182,10,results,mcb,over,ablations,mcb over ablations,0.6990014314651489
translation,182,10,results,ablations,has,without mcb,ablations has without mcb,0.5976588726043701
translation,182,10,results,results,consistently show,benefit,results consistently show benefit,0.6320281624794006
translation,182,133,results,all non-bilinear pooling methods,such as,eltwise sum,all non-bilinear pooling methods such as eltwise sum,0.5655271410942078
translation,182,133,results,all non-bilinear pooling methods,such as,concatenation,all non-bilinear pooling methods such as concatenation,0.5655801296234131
translation,182,133,results,all non-bilinear pooling methods,such as,eltwise product,all non-bilinear pooling methods such as eltwise product,0.5884195566177368
translation,182,133,results,mcb pooling,has,outperforms,mcb pooling has outperforms,0.6232694983482361
translation,182,133,results,outperforms,has,all non-bilinear pooling methods,outperforms has all non-bilinear pooling methods,0.5412200689315796
translation,182,133,results,results,see that,mcb pooling,results see that mcb pooling,0.6343482732772827
translation,182,140,results,same performance,as,not using attention at all,same performance as not using attention at all,0.5305931568145752
translation,182,140,results,same performance,by,2.67 points,same performance by 2.67 points,0.56597900390625
translation,182,140,results,mcb layer,improves,performance,mcb layer improves performance,0.7110821604728699
translation,182,140,results,performance,by,2.67 points,performance by 2.67 points,0.5663501620292664
translation,182,140,results,concatenation + fc layer,has,same performance,concatenation + fc layer has same performance,0.5867288112640381
translation,182,140,results,results,attending to,concatenation + fc layer,results attending to concatenation + fc layer,0.6721037030220032
translation,182,141,results,output dimensionality,of,multimodal compact bilinear feature,output dimensionality of multimodal compact bilinear feature,0.5307214260101318
translation,182,142,results,bilinear feature,with,"16,000 - d vector","bilinear feature with 16,000 - d vector",0.6588393449783325
translation,182,142,results,bilinear feature,yields,highest accuracy,bilinear feature yields highest accuracy,0.7414317727088928
translation,182,142,results,results,Approximating,bilinear feature,results Approximating bilinear feature,0.6712933778762817
translation,182,147,results,mcb with attention model,performs,better,mcb with attention model performs better,0.6354905366897583
translation,182,147,results,previous state - of - the - art,by,7.9 points,previous state - of - the - art by 7.9 points,0.5449156761169434
translation,182,147,results,better,in,almost every category,better in almost every category,0.514923095703125
translation,182,147,results,mcb with attention model,has,outperforms,mcb with attention model has outperforms,0.636009156703949
translation,182,147,results,outperforms,has,previous state - of - the - art,outperforms has previous state - of - the - art,0.5508202314376831
translation,182,147,results,results,has,mcb with attention model,results has mcb with attention model,0.5426765084266663
translation,182,172,results,mcb pooling,significantly improves over,baseline,mcb pooling significantly improves over baseline,0.7548596858978271
translation,182,172,results,mcb pooling,reaching,state - of - the - art accuracy,mcb pooling reaching state - of - the - art accuracy,0.6844663619995117
translation,182,172,results,baseline,on,both datasets,baseline on both datasets,0.5031464099884033
translation,182,172,results,state - of - the - art accuracy,of,48.69 %,state - of - the - art accuracy of 48.69 %,0.5334243774414062
translation,182,172,results,state - of - the - art accuracy,of,28.91 %,state - of - the - art accuracy of 28.91 %,0.5391939282417297
translation,182,172,results,48.69 %,on,flickr30k entities,48.69 % on flickr30k entities,0.5146031379699707
translation,182,172,results,28.91 %,on,referitgame dataset,28.91 % on referitgame dataset,0.5156443119049072
translation,183,256,ablation-analysis,high-qualified questions,analysis,language use,high-qualified questions analysis language use,0.6242106556892395
translation,183,256,ablation-analysis,language use,affects,number of answers,language use affects number of answers,0.6825130581855774
translation,183,256,ablation-analysis,question,receives,oqrand,question receives oqrand,0.7628934383392334
translation,183,256,ablation-analysis,question,based on,oqrand,question based on oqrand,0.7363119125366211
translation,183,256,ablation-analysis,ablation analysis,To generate,high-qualified questions,ablation analysis To generate high-qualified questions,0.720962405204773
translation,183,238,baselines,seq2seq model,use,copynet,seq2seq model use copynet,0.6281914114952087
translation,183,253,experiments,senti-gan,generates,longest questions ( 11.54 words per question ),senti-gan generates longest questions ( 11.54 words per question ),0.6436612010002136
translation,183,7,model,open-answered questions,from,real- world news,open-answered questions from real- world news,0.5525947213172913
translation,183,7,model,real- world news,for,open discussion ( openqg ),real- world news for open discussion ( openqg ),0.6320457458496094
translation,183,7,model,model,take,first step,model take first step,0.7356380820274353
translation,183,36,model,adversarial training process,perform,reinforcement learning,adversarial training process perform reinforcement learning,0.5479729175567627
translation,183,36,model,reinforcement learning,to introduce,information,reinforcement learning to introduce information,0.7253329753875732
translation,183,36,model,information,from,evaluation model,information from evaluation model,0.46777114272117615
translation,183,36,model,model,During,adversarial training process,model During adversarial training process,0.6603603959083557
translation,183,246,results,our models,get,best performance,our models get best performance,0.5913952589035034
translation,183,246,results,"bleu , rouge -l and meteor",has,our models,"bleu , rouge -l and meteor has our models",0.5995892882347107
translation,183,248,results,full version,of,our model,full version of our model,0.5828294157981873
translation,183,248,results,full version,gets,"best bleu -3 , bleu - 4 and meteor values","full version gets best bleu -3 , bleu - 4 and meteor values",0.5832912921905518
translation,183,248,results,our model,gets,"best bleu -3 , bleu - 4 and meteor values","our model gets best bleu -3 , bleu - 4 and meteor values",0.5857458710670471
translation,183,248,results,"best bleu -3 , bleu - 4 and meteor values",by introducing,linguistic - based question evaluation model,"best bleu -3 , bleu - 4 and meteor values by introducing linguistic - based question evaluation model",0.6230164766311646
translation,183,248,results,linguistic - based question evaluation model,during,adversarial training,linguistic - based question evaluation model during adversarial training,0.6405136585235596
translation,183,248,results,results,has,full version,results has full version,0.48959586024284363
translation,183,249,results,sentigan,gets,best performances,sentigan gets best performances,0.6062831878662109
translation,183,249,results,best performances,on,bleu - 3 and bleu - 4,best performances on bleu - 3 and bleu - 4,0.49173641204833984
translation,183,249,results,baselines,has,sentigan,baselines has sentigan,0.5317648649215698
translation,183,250,results,models,based on,"adversarial training ( seqgan , sentigan and ours )","models based on adversarial training ( seqgan , sentigan and ours )",0.601171612739563
translation,183,250,results,models,get,better results,models get better results,0.6205306053161621
translation,183,250,results,better results,than,others ( seq2seq and copynet ),better results than others ( seq2seq and copynet ),0.5549014210700989
translation,183,251,results,full version,of,our model,full version of our model,0.5828294157981873
translation,183,251,results,our model,gets,best performance,our model gets best performance,0.608686089515686
translation,183,251,results,f s,has,full version,f s has full version,0.5225623250007629
translation,183,251,results,results,comes to,f s,results comes to f s,0.5921310186386108
translation,184,222,baselines,model,for,retrieval methods,model for retrieval methods,0.6459304690361023
translation,184,222,baselines,roberta base,use,fine- tuned roberta base model,roberta base use fine- tuned roberta base model,0.6323237419128418
translation,184,222,baselines,fine- tuned roberta base model,as,text encoder,fine- tuned roberta base model as text encoder,0.5208415985107422
translation,184,222,baselines,fine- tuned roberta base model,as,encoder,fine- tuned roberta base model as encoder,0.557561457157135
translation,184,222,baselines,rnn,use,recurrent neural network ( rnn ),rnn use recurrent neural network ( rnn ),0.6294975876808167
translation,184,222,baselines,recurrent neural network ( rnn ),with,simple recurrent unit recurrence ( sru,recurrent neural network ( rnn ) with simple recurrent unit recurrence ( sru,0.6430527567863464
translation,184,222,baselines,recurrent neural network ( rnn ),with,fasttext word embedding layer,recurrent neural network ( rnn ) with fasttext word embedding layer,0.6103360652923584
translation,184,222,baselines,encoder,together with,fasttext word embedding layer,encoder together with fasttext word embedding layer,0.5881150364875793
translation,184,32,experimental-setup,training,train,shared text encoder,training train shared text encoder,0.712173342704773
translation,184,32,experimental-setup,shared text encoder,to compare,natural language queries,shared text encoder to compare natural language queries,0.6553727984428406
translation,184,32,experimental-setup,shared text encoder,to compare,clarification questions,shared text encoder to compare clarification questions,0.7087681293487549
translation,184,32,experimental-setup,shared text encoder,to compare,user answers,shared text encoder to compare user answers,0.6771115660667419
translation,184,32,experimental-setup,shared text encoder,to compare,classification targets,shared text encoder to compare classification targets,0.672366738319397
translation,184,32,experimental-setup,classification targets,in,same embedding space,classification targets in same embedding space,0.5351088047027588
translation,184,32,experimental-setup,experimental setup,During,training,experimental setup During training,0.6835477948188782
translation,184,243,experiments,bird identification,use,one encoder,bird identification use one encoder,0.6515601277351379
translation,184,243,experiments,bird identification,use,second encoder,bird identification use second encoder,0.6591933369636536
translation,184,243,experiments,one encoder,for,user initial queries and question - answer pairs,one encoder for user initial queries and question - answer pairs,0.6133019328117371
translation,184,243,experiments,second encoder,for,bird names,second encoder for bird names,0.594292402267456
translation,184,244,hyperparameters,policy controller,receives,reward,policy controller receives reward,0.6695617437362671
translation,184,244,hyperparameters,policy controller,receives,negative reward,policy controller receives negative reward,0.6806215643882751
translation,184,244,hyperparameters,policy controller,receives,turn penalty,policy controller receives turn penalty,0.6686914563179016
translation,184,244,hyperparameters,reward,of,20,reward of 20,0.6790489554405212
translation,184,244,hyperparameters,20,returning,correct target label,20 returning correct target label,0.7189393639564514
translation,184,244,hyperparameters,negative reward,of,- 10,negative reward of - 10,0.6248311400413513
translation,184,244,hyperparameters,- 10,for,wrong target,- 10 for wrong target,0.6595078706741333
translation,184,244,hyperparameters,turn penalty,of,- 0.5,turn penalty of - 0.5,0.6104123592376709
translation,184,244,hyperparameters,- 0.5,for,each question asked,- 0.5 for each question asked,0.6083036661148071
translation,184,244,hyperparameters,hyperparameters,has,policy controller,hyperparameters has policy controller,0.5474427342414856
translation,184,14,model,low-overhead approach,to add,limited interaction,low-overhead approach to add limited interaction,0.6482686400413513
translation,184,14,model,limited interaction,to,intent classification,limited interaction to intent classification,0.5616176724433899
translation,184,14,model,model,take,low-overhead approach,model take low-overhead approach,0.6802369952201843
translation,184,27,model,bayesian decomposition,of,posterior distributions,bayesian decomposition of posterior distributions,0.5613909363746643
translation,184,27,model,posterior distributions,over,intent labels and user responses,posterior distributions over intent labels and user responses,0.6555110216140747
translation,184,27,model,intent labels and user responses,through,interaction process,intent labels and user responses through interaction process,0.6760974526405334
translation,184,27,model,model,adopt,bayesian decomposition,model adopt bayesian decomposition,0.6669507026672363
translation,184,28,model,posteriors,to compute,question expected information gain,posteriors to compute question expected information gain,0.7387362718582153
translation,184,28,model,question expected information gain,efficiently select,next question,question expected information gain efficiently select next question,0.6489043235778809
translation,184,28,model,model,use,posteriors,model use posteriors,0.650395393371582
translation,184,30,model,each distribution,in,posterior decomposition,each distribution in posterior decomposition,0.5101454257965088
translation,184,30,model,independently,by,crowdsourcing,independently by crowdsourcing,0.6505557894706726
translation,184,30,model,posterior decomposition,has,independently,posterior decomposition has independently,0.5869145393371582
translation,184,30,model,crowdsourcing,has,initial queries and keywords annotation,crowdsourcing has initial queries and keywords annotation,0.5528869032859802
translation,184,30,model,model,estimate,each distribution,model estimate each distribution,0.762802243232727
translation,184,29,results,of asking additional questions,with,learned policy controller,of asking additional questions with learned policy controller,0.6852055191993713
translation,184,29,results,learned policy controller,decides,ask additional questions,learned policy controller decides ask additional questions,0.7693804502487183
translation,184,29,results,the cost,has,of asking additional questions,the cost has of asking additional questions,0.5633227229118347
translation,184,38,results,our approach,improves,accuracy,our approach improves accuracy,0.6992424130439758
translation,184,38,results,accuracy,of,no-interaction baseline,accuracy of no-interaction baseline,0.5345560312271118
translation,184,38,results,accuracy,by,over 100 %,accuracy by over 100 %,0.5701836347579956
translation,184,38,results,no-interaction baseline,by,over 100 %,no-interaction baseline by over 100 %,0.5739198923110962
translation,184,38,results,over 100 %,on,both tasks,over 100 % on both tasks,0.5366602540016174
translation,184,38,results,over 100 %,on,over 90 %,over 100 % on over 90 %,0.5310059785842896
translation,184,38,results,both tasks,for,simulated evaluation,both tasks for simulated evaluation,0.6174753904342651
translation,184,38,results,over 90 %,for,human evaluation,over 90 % for human evaluation,0.5974251627922058
translation,184,38,results,at most five turns,has,of interaction,at most five turns has of interaction,0.6101895570755005
translation,184,38,results,at most five turns,has,our approach,at most five turns has our approach,0.6055014729499817
translation,184,38,results,of interaction,has,our approach,of interaction has our approach,0.5484548211097717
translation,184,38,results,results,Given,at most five turns,results Given at most five turns,0.6740821003913879
translation,184,39,results,single clarification question,provides,significant accuracy improvements,single clarification question provides significant accuracy improvements,0.6296477913856506
translation,184,39,results,40 %,for,faq suggestion,40 % for faq suggestion,0.7000965476036072
translation,184,39,results,65 %,for,bird identification,65 % for bird identification,0.6146880388259888
translation,184,39,results,bird identification,in,our simulated analysis,bird identification in our simulated analysis,0.5111581087112427
translation,184,39,results,significant accuracy improvements,has,40 %,significant accuracy improvements has 40 %,0.5532138347625732
translation,184,39,results,results,Even,single clarification question,results Even single clarification question,0.685014009475708
translation,184,281,results,all three models,improve,classification performance,all three models improve classification performance,0.6295064091682434
translation,184,281,results,classification performance,with,addition of interaction,classification performance with addition of interaction,0.6657338738441467
translation,184,281,results,results,has,all three models,results has all three models,0.4808449149131775
translation,184,282,results,users,rate,our full approach,users rate our full approach,0.7001214027404785
translation,184,282,results,our full approach,better than,two other interaction variants,our full approach better than two other interaction variants,0.7047362327575684
translation,184,282,results,results,has,users,results has users,0.5371229648590088
translation,184,288,results,relative accuracy boost,of,40 %,relative accuracy boost of 40 %,0.5999632477760315
translation,184,288,results,relative accuracy boost,of,65 %,relative accuracy boost of 65 %,0.5938766598701477
translation,184,288,results,40 %,for,faq,40 % for faq,0.6922754645347595
translation,184,288,results,65 %,for,birds,65 % for birds,0.6749158501625061
translation,186,25,baselines,pointergenerator,reinforced with,features,pointergenerator reinforced with features,0.5761738419532776
translation,186,25,baselines,features,as,baseline model,features as baseline model,0.5127010345458984
translation,186,7,experiments,our multitask learning model,boosts,performance,our multitask learning model boosts performance,0.6887005567550659
translation,186,7,experiments,performance,achieving,state - of - the - art results,performance achieving state - of - the - art results,0.6485694050788879
translation,186,7,experiments,squad and marco datasets,has,our multitask learning model,squad and marco datasets has our multitask learning model,0.5434438586235046
translation,186,5,model,auxiliary task,of,language modeling,auxiliary task of language modeling,0.5612481832504272
translation,186,5,model,auxiliary task,to help,question generation,auxiliary task to help question generation,0.5957726836204529
translation,186,5,model,question generation,in,hierarchical multi-task learning structure,question generation in hierarchical multi-task learning structure,0.4691685140132904
translation,186,10,model,model,focus on,answer - aware qg,model focus on answer - aware qg,0.6933255195617676
translation,186,30,results,language modeling,consistently yields,obvious performance gain,language modeling consistently yields obvious performance gain,0.6329557299613953
translation,186,30,results,obvious performance gain,over,baselines,obvious performance gain over baselines,0.6875391006469727
translation,186,30,results,obvious performance gain,for,all evaluation metrics,obvious performance gain for all evaluation metrics,0.5561526417732239
translation,186,30,results,all evaluation metrics,including,bleu,all evaluation metrics including bleu,0.5895708203315735
translation,186,30,results,all evaluation metrics,including,perplexity,all evaluation metrics including perplexity,0.5845529437065125
translation,186,30,results,all evaluation metrics,including,distinct,all evaluation metrics including distinct,0.7043083310127258
translation,186,30,results,results,show,language modeling,results show language modeling,0.6223277449607849
translation,186,31,results,outperforms,achieving,high bleu - 4 score,outperforms achieving high bleu - 4 score,0.631446361541748
translation,186,31,results,existing state - of - the - art results,achieving,high bleu - 4 score,existing state - of - the - art results achieving high bleu - 4 score,0.618803083896637
translation,186,31,results,high bleu - 4 score,of,16.23,high bleu - 4 score of 16.23,0.5377150177955627
translation,186,31,results,high bleu - 4 score,of,20.88,high bleu - 4 score of 20.88,0.5310121178627014
translation,186,31,results,16.23,on,squad,16.23 on squad,0.5802899599075317
translation,186,31,results,20.88,on,marco,20.88 on marco,0.573654294013977
translation,186,31,results,our full model,has,outperforms,our full model has outperforms,0.633263111114502
translation,186,31,results,outperforms,has,existing state - of - the - art results,outperforms has existing state - of - the - art results,0.5536288619041443
translation,186,31,results,results,has,our full model,results has our full model,0.5411279201507568
translation,186,60,results,our full model ( w/ features + language modeling ),achieves,state - of - the - art results,our full model ( w/ features + language modeling ) achieves state - of - the - art results,0.618956446647644
translation,186,60,results,state - of - the - art results,on,both datasets,state - of - the - art results on both datasets,0.4861527979373932
translation,186,60,results,16.23 bleu - 4 score,on,squad,16.23 bleu - 4 score on squad,0.5187134742736816
translation,186,60,results,20.88,on,marco,20.88 on marco,0.573654294013977
translation,186,60,results,our full model ( w/ features + language modeling ),has,significantly outperforms,our full model ( w/ features + language modeling ) has significantly outperforms,0.5688189268112183
translation,186,60,results,significantly outperforms,has,previous models,significantly outperforms has previous models,0.5841439962387085
translation,186,60,results,results,has,our full model ( w/ features + language modeling ),results has our full model ( w/ features + language modeling ),0.5448880791664124
translation,186,63,results,language modeling task,helps,model,language modeling task helps model,0.5800006985664368
translation,186,63,results,model,to generate,more fluent and readable questions,model to generate more fluent and readable questions,0.6551960706710815
translation,186,63,results,results,indicate,language modeling task,results indicate language modeling task,0.5230537056922913
translation,186,71,results,coefficient,between,human judges,coefficient between human judges,0.6691277027130127
translation,186,71,results,human judges,is,high,human judges is high,0.5902685523033142
translation,186,71,results,human judges,validating,high quality,human judges validating high quality,0.6631178855895996
translation,186,71,results,results,has,coefficient,results has coefficient,0.4593842923641205
translation,187,11,experiments,domain adaptation,detection of,question duplicates,domain adaptation detection of question duplicates,0.6747226119041443
translation,187,11,experiments,question duplicates,in,community question answering forums,question duplicates in community question answering forums,0.5036099553108215
translation,187,5,model,adversarial domain adaptation,has,to explicitly learn both shared and unshared ( domain specific ) representations between two textual domains,adversarial domain adaptation has to explicitly learn both shared and unshared ( domain specific ) representations between two textual domains,0.5149197578430176
translation,187,5,model,model,investigate,gradient reversal,model investigate gradient reversal,0.6556625366210938
translation,187,6,model,gradient reversal,learns,features,gradient reversal learns features,0.6825624704360962
translation,187,6,model,gradient reversal,distilling,domain specific knowledge,gradient reversal distilling domain specific knowledge,0.7250537872314453
translation,187,6,model,features,explicitly compensate for,domain mismatch,features explicitly compensate for domain mismatch,0.7288104891777039
translation,187,6,model,domain specific knowledge,can improve,target domain accuracy,domain specific knowledge can improve target domain accuracy,0.6653845906257629
translation,187,6,model,model,has,gradient reversal,model has gradient reversal,0.5474665760993958
translation,187,9,model,domain adaptation,leverage,source task representations,domain adaptation leverage source task representations,0.7129549384117126
translation,187,9,model,model,Through,domain adaptation,model Through domain adaptation,0.6714518070220947
translation,187,27,model,novel approach,for,adversarial domain adaptation,novel approach for adversarial domain adaptation,0.5817217230796814
translation,187,27,model,adversarial domain adaptation,uses,gradient reversal layers,adversarial domain adaptation uses gradient reversal layers,0.5239840745925903
translation,187,27,model,gradient reversal layers,to discover,shared representations,gradient reversal layers to discover shared representations,0.6090936064720154
translation,187,27,model,shared representations,between,source and target domains,shared representations between source and target domains,0.6472556591033936
translation,187,27,model,source and target domains,on,textual matching tasks,source and target domains on textual matching tasks,0.4893307685852051
translation,187,27,model,model,propose,novel approach,model propose novel approach,0.7168048620223999
translation,187,28,results,outperforms,with,absolute accuracy gains,outperforms with absolute accuracy gains,0.7017725110054016
translation,187,28,results,all other strong baselines and feature sets,on,five different domains,all other strong baselines and feature sets on five different domains,0.49367740750312805
translation,187,28,results,all other strong baselines and feature sets,with,absolute accuracy gains,all other strong baselines and feature sets with absolute accuracy gains,0.6136584877967834
translation,187,28,results,absolute accuracy gains,of,up to 4.5 %,absolute accuracy gains of up to 4.5 %,0.5742253065109253
translation,187,28,results,outperforms,has,all other strong baselines and feature sets,outperforms has all other strong baselines and feature sets,0.5643446445465088
translation,187,29,results,same approach,to,two different textual entailment domains,same approach to two different textual entailment domains,0.5184656977653503
translation,187,29,results,outperforms,as much as,7 % absolute accuracy points,outperforms as much as 7 % absolute accuracy points,0.5500583648681641
translation,187,29,results,other baselines,as much as,7 % absolute accuracy points,other baselines as much as 7 % absolute accuracy points,0.4820508360862732
translation,187,29,results,outperforms,has,other baselines,outperforms has other baselines,0.5879674553871155
translation,187,29,results,results,apply,same approach,results apply same approach,0.6748204231262207
translation,187,83,results,significantly improves,compared to,maximizing,significantly improves compared to maximizing,0.7197538614273071
translation,187,83,results,overall feature learning,compared to,maximizing,overall feature learning compared to maximizing,0.6329371333122253
translation,187,83,results,both t2 and t3,has,outperform,both t2 and t3 has outperform,0.5887085199356079
translation,187,83,results,outperform,has,t1,outperform has t1,0.6571699976921082
translation,187,83,results,grl,has,significantly improves,grl has significantly improves,0.6343340873718262
translation,187,83,results,significantly improves,has,overall feature learning,significantly improves has overall feature learning,0.5721814036369324
translation,187,83,results,maximizing,has,domain classification loss,maximizing has domain classification loss,0.5628067851066589
translation,187,83,results,results,has,both t2 and t3,results has both t2 and t3,0.48542702198028564
translation,187,84,results,and t1,shows,learning exactly the same feature set,and t1 shows learning exactly the same feature set,0.6162051558494568
translation,187,84,results,learning exactly the same feature set,using,grl,learning exactly the same feature set using grl,0.6774758696556091
translation,187,84,results,grl,for,adversarial domain adaptation,grl for adversarial domain adaptation,0.5478124618530273
translation,187,84,results,more effective,than,maximizing,more effective than maximizing,0.5934735536575317
translation,187,84,results,maximizing,has,loss,maximizing has loss,0.6211190223693848
translation,187,85,results,all other models,showing,our proposed approach,all other models showing our proposed approach,0.6895354390144348
translation,187,85,results,our proposed approach,consistently beats,all other settings,our proposed approach consistently beats all other settings,0.714207649230957
translation,187,85,results,all other settings,for,domain adaptation,all other settings for domain adaptation,0.5800237655639648
translation,187,85,results,t3,has,outperforms,t3 has outperforms,0.6404070258140564
translation,187,85,results,outperforms,has,all other models,outperforms has all other models,0.5782700181007385
translation,187,85,results,results,has,t3,results has t3,0.5303347110748291
translation,187,88,results,adversarial domain adaptation,using,gradient reversal,adversarial domain adaptation using gradient reversal,0.6509885191917419
translation,187,88,results,gradient reversal,yields,best knowledge transfer,gradient reversal yields best knowledge transfer,0.6982653141021729
translation,187,88,results,best knowledge transfer,between,all textual domains,best knowledge transfer between all textual domains,0.6118248105049133
translation,188,6,model,potential,of,transformer language models,potential of transformer language models,0.5824942588806152
translation,188,6,model,model,taps on,potential,model taps on potential,0.7738039493560791
translation,188,7,model,language -visual,rely on,pretrained transformers,language -visual rely on pretrained transformers,0.7550515532493591
translation,188,7,model,language -visual,rely on,fine -tuning,language -visual rely on fine -tuning,0.6943120956420898
translation,188,7,model,language -visual,rely on,ensembling,language -visual rely on ensembling,0.7581314444541931
translation,188,8,model,bottom - up and top-down attention,to identify,regions of interest,bottom - up and top-down attention to identify regions of interest,0.6589255928993225
translation,188,8,model,regions of interest,corresponding to,diagram constituents and their relationships,regions of interest corresponding to diagram constituents and their relationships,0.6574674844741821
translation,188,8,model,regions of interest,improving,selection,regions of interest improving selection,0.693211019039154
translation,188,8,model,selection,of,relevant visual information,selection of relevant visual information,0.5778719186782837
translation,188,8,model,relevant visual information,for,each question,relevant visual information for each question,0.5971430540084839
translation,188,8,model,model,add,bottom - up and top-down attention,model add bottom - up and top-down attention,0.5690015554428101
translation,188,9,results,our system,reports,unprecedented success,our system reports unprecedented success,0.6351956725120544
translation,188,9,results,isaaq,reports,unprecedented success,isaaq reports unprecedented success,0.6614734530448914
translation,188,9,results,unprecedented success,in,all tqa question types,unprecedented success in all tqa question types,0.5151966214179993
translation,188,9,results,unprecedented success,with,accuracies,unprecedented success with accuracies,0.6342453360557556
translation,188,9,results,accuracies,of,"81.36 % , 71.11 % and 55.12 %","accuracies of 81.36 % , 71.11 % and 55.12 %",0.5571391582489014
translation,188,9,results,"81.36 % , 71.11 % and 55.12 %",on,"true / false , text-only and diagram multiple choice questions","81.36 % , 71.11 % and 55.12 % on true / false , text-only and diagram multiple choice questions",0.48257553577423096
translation,188,9,results,our system,has,isaaq,our system has isaaq,0.6699504256248474
translation,188,9,results,results,has,our system,results has our system,0.5954442024230957
translation,188,9,results,results,has,isaaq,results has isaaq,0.5954558849334717
translation,189,96,ablation-analysis,different weighting schema,observe that,noise,different weighting schema observe that noise,0.6302533149719238
translation,189,96,ablation-analysis,noise,introduced by,crowd annotation,noise introduced by crowd annotation,0.6634092926979065
translation,189,96,ablation-analysis,noise,be,significantly reduced,noise be significantly reduced,0.5629429817199707
translation,189,96,ablation-analysis,significantly reduced,as,classifier,significantly reduced as classifier,0.6306359171867371
translation,189,96,ablation-analysis,classifier,improves by,+ 1.47 %,classifier improves by + 1.47 %,0.712247371673584
translation,189,96,ablation-analysis,classifier,improves by,+ 0.54 %,classifier improves by + 0.54 %,0.7127304673194885
translation,189,96,ablation-analysis,classifier,improves by,+ 1.85 %,classifier improves by + 1.85 %,0.7168919444084167
translation,189,96,ablation-analysis,+ 1.47 %,in,mrr,+ 1.47 % in mrr,0.5917785167694092
translation,189,96,ablation-analysis,+ 0.54 %,in,map,+ 0.54 % in map,0.6147850751876831
translation,189,96,ablation-analysis,+ 1.85 %,in,p@1,+ 1.85 % in p@1,0.608417272567749
translation,189,96,ablation-analysis,ablation analysis,apply,different weighting schema,ablation analysis apply different weighting schema,0.6391677260398865
translation,189,74,baselines,bm25,used,terrier search engine,bm25 used terrier search engine,0.6156032681465149
translation,189,74,baselines,terrier search engine,provides,bm25 scor - ing model,terrier search engine provides bm25 scor - ing model,0.6190462112426758
translation,189,74,baselines,bm25 scor - ing model,to index,answer passages,bm25 scor - ing model to index answer passages,0.6782062649726868
translation,189,74,baselines,baselines,namely,bm25,baselines namely bm25,0.6889849305152893
translation,189,77,baselines,re ( regular expression ),trained,classifier,re ( regular expression ) trained classifier,0.7215390801429749
translation,189,77,baselines,classifier,with,noisy annotations,classifier with noisy annotations,0.6067413091659546
translation,189,77,baselines,noisy annotations,produced by,labels,noisy annotations produced by labels,0.5892167091369629
translation,189,77,baselines,re,applied to,answer keys,re applied to answer keys,0.7265093922615051
translation,189,77,baselines,baselines,has,re ( regular expression ),baselines has re ( regular expression ),0.5741674900054932
translation,189,78,baselines,ca ( crowd annotations ),train,classier,ca ( crowd annotations ) train classier,0.7005937695503235
translation,189,78,baselines,classier,with,same configuration,classier with same configuration,0.6949076056480408
translation,189,78,baselines,classier,using,majority voting,classier using majority voting,0.7150382995605469
translation,189,78,baselines,same configuration,as,re,same configuration as re,0.6724879145622253
translation,189,78,baselines,majority voting,as a source of,supervision,majority voting as a source of supervision,0.668225109577179
translation,189,78,baselines,baselines,has,ca ( crowd annotations ),baselines has ca ( crowd annotations ),0.5582413673400879
translation,189,89,baselines,baselines,has,lptc,baselines has lptc,0.6044437289237976
translation,189,4,model,methods,to take into account,disagreement,methods to take into account disagreement,0.593055009841919
translation,189,4,model,disagreement,between,crowd annotators,disagreement between crowd annotators,0.628592312335968
translation,189,4,model,their skills,for weighting,instances,their skills for weighting instances,0.7855824828147888
translation,189,4,model,instances,in,learning algorithms,instances in learning algorithms,0.5164183974266052
translation,189,4,model,model,propose,methods,model propose methods,0.5709900259971619
translation,189,24,model,methods,to take into account,disagreement,methods to take into account disagreement,0.593055009841919
translation,189,24,model,disagreement,among,crowd annotators,disagreement among crowd annotators,0.527175784111023
translation,189,24,model,disagreement,skills in,learning algorithms,disagreement skills in learning algorithms,0.6874093413352966
translation,189,25,model,several instance weighting strategies,help,learning algorithm,several instance weighting strategies help learning algorithm,0.6596215963363647
translation,189,25,model,learning algorithm,to deal with,noise,learning algorithm to deal with noise,0.7219424247741699
translation,189,25,model,noise,of,training examples,noise of training examples,0.5614907145500183
translation,189,25,model,noise,producing,higher accuracy,noise producing higher accuracy,0.744468629360199
translation,189,25,model,model,design,several instance weighting strategies,model design several instance weighting strategies,0.5782124400138855
translation,189,33,results,our rerankers,improve on,ir baseline,our rerankers improve on ir baseline,0.7198509573936462
translation,189,33,results,ir baseline,i.e.,bm25,ir baseline i.e. bm25,0.6203200221061707
translation,189,33,results,17.47 % and 19.22 %,in,mrr and p@1,17.47 % and 19.22 % in mrr and p@1,0.5902417898178101
translation,189,33,results,17.47 % and 19.22 %,in,mrr and p@1,17.47 % and 19.22 % in mrr and p@1,0.5902417898178101
translation,189,33,results,our weighting strategy,improves,best reranker,our weighting strategy improves best reranker,0.724287211894989
translation,189,33,results,best reranker,up to,1.47 % and 1.85 %,best reranker up to 1.47 % and 1.85 %,0.639858067035675
translation,189,33,results,1.47 % and 1.85 %,on,mrr and p@1,1.47 % and 1.85 % on mrr and p@1,0.604302167892456
translation,189,33,results,results,show,our rerankers,results show our rerankers,0.6001222729682922
translation,189,33,results,results,show,our weighting strategy,results show our weighting strategy,0.6360899209976196
translation,189,91,results,accuracy,of,bm25,accuracy of bm25,0.6090237498283386
translation,189,91,results,bm25,lower than,rerankers,bm25 lower than rerankers,0.7549769282341003
translation,189,91,results,rerankers,trained on,noisy labels,rerankers trained on noisy labels,0.6994210481643677
translation,189,91,results,- 15.66 %,in,mrr,- 15.66 % in mrr,0.5914711356163025
translation,189,91,results,- 14.5 %,in,map,- 14.5 % in map,0.6098080277442932
translation,189,91,results,noisy labels,has,- 15.66 %,noisy labels has - 15.66 %,0.5592156648635864
translation,189,92,results,some improvement,using,crowd annotations,some improvement using crowd annotations,0.6337063908576965
translation,189,92,results,crowd annotations,for,training,crowd annotations for training,0.5771608948707581
translation,189,92,results,improvement,is,not significant,improvement is not significant,0.581317126750946
translation,189,92,results,+ 0.34 %,in,mrr,+ 0.34 % in mrr,0.590062141418457
translation,189,92,results,+ 0.34 %,in,map,+ 0.34 % in map,0.6136084198951721
translation,189,92,results,not significant,has,+ 0.34 %,not significant has + 0.34 %,0.5472192764282227
translation,190,6,model,multi-source meta transfer ( mmt ),for,low-resource mcqa,multi-source meta transfer ( mmt ) for low-resource mcqa,0.6149678230285645
translation,190,6,model,model,propose,multi-source meta transfer ( mmt ),model propose multi-source meta transfer ( mmt ),0.6720614433288574
translation,190,7,model,meta learning,by incorporating,multiple training sources,meta learning by incorporating multiple training sources,0.700289785861969
translation,190,7,model,multiple training sources,to learn,generalized feature representation,multiple training sources to learn generalized feature representation,0.6066587567329407
translation,190,7,model,generalized feature representation,across,domains,generalized feature representation across domains,0.6874642372131348
translation,190,7,model,model,extend,meta learning,model extend meta learning,0.7144566178321838
translation,190,8,model,distribution gap,between,training sources and the target,distribution gap between training sources and the target,0.6668491959571838
translation,190,8,model,distribution gap,introduce,meta transfer,distribution gap introduce meta transfer,0.6468557119369507
translation,190,8,model,meta transfer,integrated into,multi-source meta training,meta transfer integrated into multi-source meta training,0.6796597838401794
translation,190,8,model,model,To bridge,distribution gap,model To bridge distribution gap,0.6412842869758606
translation,190,8,model,model,introduce,meta transfer,model introduce meta transfer,0.6266884207725525
translation,190,23,model,hidden knowledge,across,multiple data sources,hidden knowledge across multiple data sources,0.6896437406539917
translation,190,23,model,hidden knowledge,propose,novel framework,hidden knowledge propose novel framework,0.6723289489746094
translation,190,23,model,novel framework,termed,multi-source meta transfer ( mmt ),novel framework termed multi-source meta transfer ( mmt ),0.6617592573165894
translation,190,23,model,model,To better discover,hidden knowledge,model To better discover hidden knowledge,0.7261002063751221
translation,190,23,model,model,propose,novel framework,model propose novel framework,0.720393180847168
translation,190,24,model,module,named,multi-source meta learning ( mml ),module named multi-source meta learning ( mml ),0.7035680413246155
translation,190,24,model,traditional meta learning,to,multiple sources,traditional meta learning to multiple sources,0.5275328755378723
translation,190,24,model,multiple sources,where,series of meta-tasks,multiple sources where series of meta-tasks,0.5927588939666748
translation,190,24,model,series of meta-tasks,on,different data resources,series of meta-tasks on different data resources,0.5316995978355408
translation,190,24,model,model,propose,module,model propose module,0.7126700282096863
translation,190,27,results,mmt,discover,knowledge,mmt discover knowledge,0.7209020256996155
translation,190,27,results,knowledge,across,different datasets,knowledge across different datasets,0.7298685312271118
translation,190,27,results,knowledge,transfer it into,target task,knowledge transfer it into target task,0.6481268405914307
translation,191,106,ablation-analysis,more abstractive summaries,trained on,xsum,more abstractive summaries trained on xsum,0.7246602773666382
translation,191,106,ablation-analysis,more abstractive summaries,has,grammaticality scores,more abstractive summaries has grammaticality scores,0.5500091314315796
translation,191,106,ablation-analysis,grammaticality scores,has,drop significantly,grammaticality scores has drop significantly,0.5979012250900269
translation,191,106,ablation-analysis,ablation analysis,on,more abstractive summaries,ablation analysis on more abstractive summaries,0.5282434225082397
translation,191,139,experimental-setup,important text spans,in,sentence,important text spans in sentence,0.48378339409828186
translation,191,139,experimental-setup,sentence,including,noun phrases,sentence including noun phrases,0.6827288866043091
translation,191,139,experimental-setup,noun phrases,extracted by,constituency parser,noun phrases extracted by constituency parser,0.6744974255561829
translation,191,139,experimental-setup,named entities,extracted by,stanford corenlp ner model,named entities extracted by stanford corenlp ner model,0.6069128513336182
translation,191,139,experimental-setup,experimental setup,mask,important text spans,experimental setup mask important text spans,0.6928170919418335
translation,191,107,experiments,bertsum,maintains,good performance,bertsum maintains good performance,0.7282863259315491
translation,191,107,experiments,good performance,on,xsum,good performance on xsum,0.5538702011108398
translation,191,107,experiments,highest grammaticality score,on,both datasets,highest grammaticality score on both datasets,0.4523793160915375
translation,191,138,experiments,natural language questions,from,summary sentence,natural language questions from summary sentence,0.5433933138847351
translation,191,138,experiments,summary sentence,has,automatically,summary sentence has automatically,0.619020938873291
translation,191,9,model,model,propose,automatic question answering ( qa ),model propose automatic question answering ( qa ),0.6519424319267273
translation,191,10,model,questionanswer pairs,generated from,summary,questionanswer pairs generated from summary,0.6343546509742737
translation,191,10,model,questionanswer pairs,generated from,summary,questionanswer pairs generated from summary,0.6343546509742737
translation,191,10,model,qa model,extracts,answers,qa model extracts answers,0.6724572777748108
translation,191,10,model,answers,from,document,answers from document,0.6581774353981018
translation,191,10,model,non-matched answers,indicate,unfaithful information,non-matched answers indicate unfaithful information,0.6068860292434692
translation,191,10,model,unfaithful information,in,summary,unfaithful information in summary,0.5204881429672241
translation,191,10,model,questionanswer pairs,has,qa model,questionanswer pairs has qa model,0.5879899859428406
translation,191,10,model,summary,has,qa model,summary has qa model,0.576707124710083
translation,191,10,model,model,Given,questionanswer pairs,model Given questionanswer pairs,0.7187290191650391
translation,191,29,model,automatically generated qa pairs,to represent,information,automatically generated qa pairs to represent information,0.6595363020896912
translation,191,29,model,information,in,summary,information in summary,0.5359929800033569
translation,191,29,model,model,use,automatically generated qa pairs,model use automatically generated qa pairs,0.6754142045974731
translation,191,30,model,set of   groundtruth   qa pairs,from,summary,set of   groundtruth   qa pairs from summary,0.5371434092521667
translation,191,30,model,set of   groundtruth   qa pairs,using,learned model,set of   groundtruth   qa pairs using learned model,0.5899584889411926
translation,191,30,model,learned model,that converts,declarative sentence,learned model that converts declarative sentence,0.5417485237121582
translation,191,30,model,learned model,that converts,answer span,learned model that converts answer span,0.5850293040275574
translation,191,30,model,answer span,to,question,answer span to question,0.6499432325363159
translation,191,30,model,model,generate,set of   groundtruth   qa pairs,model generate set of   groundtruth   qa pairs,0.6387499570846558
translation,191,25,results,number of unfaithful sentences ( annotated by humans ),increases as,summary,number of unfaithful sentences ( annotated by humans ) increases as summary,0.5975374579429626
translation,191,25,results,summary,becomes,more abstractive,summary becomes more abstractive,0.6160489916801453
translation,191,60,results,cnn / dm,is,more extractive,cnn / dm is more extractive,0.5812218189239502
translation,191,60,results,more extractive,than,xsum,more extractive than xsum,0.6034336090087891
translation,191,61,results,extraction scores,of,reference summaries,extraction scores of reference summaries,0.5614861845970154
translation,191,61,results,reference summaries,in,cnn / dm,reference summaries in cnn / dm,0.5491209030151367
translation,191,61,results,sentences,formed by deleting words,one of the source sentences,sentences formed by deleting words one of the source sentences,0.7581583857536316
translation,191,61,results,results,has,extraction scores,results has extraction scores,0.5318186283111572
translation,191,105,results,outputs,from,all models,outputs from all models,0.6049168109893799
translation,191,105,results,all models,scored high on,grammaticality,all models scored high on grammaticality,0.7072252035140991
translation,191,105,results,grammaticality,with,high inter-annotator agreement,grammaticality with high inter-annotator agreement,0.5336016416549683
translation,191,105,results,results,has,outputs,results has outputs,0.5481858849525452
translation,191,109,results,near-extractive summaries,generated from,models,near-extractive summaries generated from models,0.6426162719726562
translation,191,109,results,models,trained on,cnn / dm,models trained on cnn / dm,0.7248637676239014
translation,191,109,results,models,trained on,xsum,models trained on xsum,0.7353853583335876
translation,191,109,results,cnn / dm,have,significantly higher faithfulness scores,cnn / dm have significantly higher faithfulness scores,0.5656601786613464
translation,191,109,results,significantly higher faithfulness scores,than,highly abstractive summaries,significantly higher faithfulness scores than highly abstractive summaries,0.5574021935462952
translation,191,109,results,highly abstractive summaries,models trained on,xsum,highly abstractive summaries models trained on xsum,0.6852445602416992
translation,191,109,results,results,has,near-extractive summaries,results has near-extractive summaries,0.5453447103500366
translation,191,112,results,human agreement,on,faithfulness,human agreement on faithfulness,0.539417028427124
translation,191,112,results,faithfulness,is,lower,faithfulness is lower,0.6452436447143555
translation,191,112,results,lower,for,abstractive summaries,lower for abstractive summaries,0.6199876666069031
translation,191,112,results,abstractive summaries,from,xsum,abstractive summaries from xsum,0.5699841976165771
translation,191,112,results,results,has,human agreement,results has human agreement,0.48787039518356323
translation,191,114,results,conflicting information,is,more common,conflicting information is more common,0.5688655972480774
translation,191,114,results,more common,among,models,more common among models,0.6320539116859436
translation,191,114,results,more common,among,models,more common among models,0.6320539116859436
translation,191,114,results,more common,among,models,more common among models,0.6320539116859436
translation,191,114,results,models,trained on,cnn / dm,models trained on cnn / dm,0.7248637676239014
translation,191,114,results,models,trained on,xsum,models trained on xsum,0.7353853583335876
translation,191,114,results,more common,among,models,more common among models,0.6320539116859436
translation,191,114,results,models,trained on,xsum,models trained on xsum,0.7353853583335876
translation,191,114,results,results,observe,conflicting information,results observe conflicting information,0.6197967529296875
translation,191,114,results,results,observe,hallucination,results observe hallucination,0.6551441550254822
translation,191,177,results,score,of,qa - based evaluation,score of qa - based evaluation,0.5895345211029053
translation,191,177,results,higher correlation,with,faithfulness,higher correlation with faithfulness,0.6495035290718079
translation,191,177,results,faithfulness,than,other metrics,faithfulness than other metrics,0.5505708456039429
translation,191,177,results,cnn / dm and xsum,has,score,cnn / dm and xsum has score,0.6168350577354431
translation,191,177,results,qa - based evaluation,has,higher correlation,qa - based evaluation has higher correlation,0.5594446659088135
translation,191,177,results,results,observe,cnn / dm and xsum,results observe cnn / dm and xsum,0.593112587928772
translation,191,177,results,results,for,cnn / dm and xsum,results for cnn / dm and xsum,0.6131958365440369
translation,191,178,results,word-overlap based metrics,correlated with,faithfulness,word-overlap based metrics correlated with faithfulness,0.7046412229537964
translation,191,178,results,word-overlap based metrics,correlated with,faithfulness,word-overlap based metrics correlated with faithfulness,0.7046412229537964
translation,191,178,results,faithfulness,in,more extractive settings,faithfulness in more extractive settings,0.5507928133010864
translation,191,178,results,faithfulness,more abstractive settings ( i.e. for,xsum ),faithfulness more abstractive settings ( i.e. for xsum ),0.7176762819290161
translation,191,178,results,results,has,word-overlap based metrics,results has word-overlap based metrics,0.5284029245376587
translation,191,179,results,significantly lower correlation,with,human scores,significantly lower correlation with human scores,0.6480318307876587
translation,191,179,results,human scores,for,xsum,human scores for xsum,0.632544219493866
translation,191,179,results,results,notice,all the metrics,results notice all the metrics,0.6984854340553284
translation,192,7,experimental-setup,classifiers and kernels,implemented within,kernel- based learning platform,classifiers and kernels implemented within kernel- based learning platform,0.7359105348587036
translation,192,7,experimental-setup,kernel- based learning platform,called,kelp,kernel- based learning platform called kelp,0.6773223876953125
translation,192,7,experimental-setup,experimental setup,has,classifiers and kernels,experimental setup has classifiers and kernels,0.5480858683586121
translation,192,20,experimental-setup,classifiers and kernels,implemented within,kernel- based learning platform 2 ( kelp ),classifiers and kernels implemented within kernel- based learning platform 2 ( kelp ),0.7665524482727051
translation,192,20,experimental-setup,experimental setup,has,classifiers and kernels,experimental setup has classifiers and kernels,0.5480858683586121
translation,192,116,experimental-setup,kernel - based learning models,implemented in,kelp,kernel - based learning models implemented in kelp,0.7262144088745117
translation,192,116,experimental-setup,experimental setup,has,kernel - based learning models,experimental setup has kernel - based learning models,0.5260736346244812
translation,192,6,experiments,challenge tasks,modeled as,binary classification problems,challenge tasks modeled as binary classification problems,0.5986089706420898
translation,192,6,experiments,kernel - based classifiers,trained on,semeval datasets,kernel - based classifiers trained on semeval datasets,0.7453340291976929
translation,192,6,experiments,scores,sort,instances,scores sort instances,0.750158429145813
translation,192,6,experiments,scores,produce,final ranking,scores produce final ranking,0.6806820034980774
translation,192,6,experiments,binary classification problems,has,kernel - based classifiers,binary classification problems has kernel - based classifiers,0.5452563762664795
translation,192,21,model,tree kernels,directly to,question and answer texts,tree kernels directly to question and answer texts,0.6491144895553589
translation,192,21,model,question and answer texts,modeled as,pairs,question and answer texts modeled as pairs,0.6192284822463989
translation,192,21,model,pairs,of,linked syntactic trees,pairs of linked syntactic trees,0.6047446727752686
translation,192,21,model,model,applies,tree kernels,model applies tree kernels,0.645976185798645
translation,192,22,model,methods,using,kernels,methods using kernels,0.6330081820487976
translation,192,22,model,kernels,propose,stacking schema,kernels propose stacking schema,0.6692314147949219
translation,192,22,model,features,by adopting,several features,features by adopting several features,0.6837429404258728
translation,192,22,model,classifiers,for,subtask b and c,classifiers for subtask b and c,0.6398054957389832
translation,192,22,model,classifiers,exploit,inferences,classifiers exploit inferences,0.7457964420318604
translation,192,22,model,inferences,obtained in,previous subtasks,inferences obtained in previous subtasks,0.6590492129325867
translation,192,22,model,model,extended,features,model extended features,0.7406039834022522
translation,192,22,model,model,propose,stacking schema,model propose stacking schema,0.6967888474464417
translation,192,42,model,new inter-pair methods,to directly employ,text pairs,new inter-pair methods to directly employ text pairs,0.6507494449615479
translation,192,42,model,text pairs,into,kernel - based learning framework,text pairs into kernel - based learning framework,0.5612186193466187
translation,192,42,model,model,propose,new inter-pair methods,model propose new inter-pair methods,0.7170411348342896
translation,192,8,results,our primary submission,ranked,third,our primary submission ranked third,0.6872881650924683
translation,192,8,results,our primary submission,second in,subtask c,our primary submission second in subtask c,0.6927342414855957
translation,192,8,results,third,in,subtask b,third in subtask b,0.535065233707428
translation,192,8,results,results,has,our primary submission,results has our primary submission,0.5467225909233093
translation,192,10,results,all the other systems,with respect to,all the other challenge metrics,all the other systems with respect to all the other challenge metrics,0.6045655608177185
translation,192,10,results,our approach,has,outperforms,our approach has outperforms,0.6385829448699951
translation,192,10,results,outperforms,has,all the other systems,outperforms has all the other systems,0.5945779085159302
translation,192,10,results,results,has,our approach,results has our approach,0.6050099730491638
translation,192,23,results,primary submission,ranked first in,subtask a,primary submission ranked first in subtask a,0.6397079825401306
translation,192,23,results,primary submission,second in,subtask c,primary submission second in subtask c,0.6365454196929932
translation,192,23,results,third,in,subtask b,third in subtask b,0.535065233707428
translation,192,23,results,results,has,primary submission,results has primary submission,0.4854085147380829
translation,192,129,results,good results,on,10 fold cross validations,good results on 10 fold cross validations,0.5115066170692444
translation,192,129,results,10 fold cross validations,confirmed,official test set,10 fold cross validations confirmed official test set,0.6663150191307068
translation,192,129,results,model,is,very accurate,model is very accurate,0.5741881132125854
translation,192,129,results,model,achieved,first position,model achieved first position,0.7308889031410217
translation,192,129,results,first position,among,12 systems,first position among 12 systems,0.625124454498291
translation,192,129,results,first position,with,best map,first position with best map,0.6189913153648376
translation,192,129,results,official test set,has,model,official test set has model,0.5702216625213623
translation,192,129,results,results,has,good results,results has good results,0.5530260801315308
translation,192,144,results,results,on,subtask,results on subtask,0.548139750957489
translation,192,145,results,our primary submission,achieved,third position,our primary submission achieved third position,0.6291493773460388
translation,192,145,results,third position,w.r.t.,map,third position w.r.t. map,0.6238667368888855
translation,192,145,results,third position,among,11 systems,third position among 11 systems,0.6352074146270752
translation,192,146,results,contrastive systems,achieve,higher map,contrastive systems achieve higher map,0.6805347800254822
translation,192,146,results,contrastive systems,ranked,second,contrastive systems ranked second,0.7115924954414368
translation,192,147,results,primary system,achieves,highest f 1 and accuracy,primary system achieves highest f 1 and accuracy,0.6835417151451111
translation,192,147,results,highest f 1 and accuracy,on,tuning and test stages,highest f 1 and accuracy on tuning and test stages,0.51749587059021
translation,192,147,results,results,has,primary system,results has primary system,0.5567602515220642
translation,192,162,results,primary submission,achieved,second highest map,primary submission achieved second highest map,0.7009185552597046
translation,192,162,results,results,has,primary submission,results has primary submission,0.4854085147380829
translation,192,163,results,f 1 our system,is,best,f 1 our system is best,0.5847048163414001
translation,192,163,results,best,among,10 primary submissions,best among 10 primary submissions,0.535412073135376
translation,192,163,results,results,noted that,f 1 our system,results noted that f 1 our system,0.6553819179534912
translation,193,4,baselines,answering,has,non-obstructive psychological domain-specific questions,answering has non-obstructive psychological domain-specific questions,0.5628068447113037
translation,193,4,baselines,baselines,propose,pal,baselines propose pal,0.6659555435180664
translation,193,12,baselines,baselines,propose,pal,baselines propose pal,0.6659555435180664
translation,193,98,model,knowledge,behind,chatterbot,knowledge behind chatterbot,0.7196294665336609
translation,193,98,model,chatterbot,learned from,q&a pairs,chatterbot learned from q&a pairs,0.7068018913269043
translation,193,98,model,q&a pairs,derived from,online forum,q&a pairs derived from online forum,0.6343522667884827
translation,193,98,model,online forum,using,several extraction strategies,online forum using several extraction strategies,0.660747230052948
translation,193,98,model,model,has,knowledge,model has knowledge,0.6068452596664429
translation,194,9,ablation-analysis,policy gradient methods,increases in,metrics,policy gradient methods increases in metrics,0.6281638145446777
translation,194,9,ablation-analysis,metrics,used as,rewards,metrics used as rewards,0.5744084715843201
translation,194,9,ablation-analysis,ablation analysis,training with,policy gradient methods,ablation analysis training with policy gradient methods,0.7565710544586182
translation,194,50,experimental-setup,model,using,maximum likelihood,model using maximum likelihood,0.6823956966400146
translation,194,50,experimental-setup,maximum likelihood,before,fine tuning,maximum likelihood before fine tuning,0.7011879682540894
translation,194,50,experimental-setup,experimental setup,train,model,experimental setup train model,0.6514950394630432
translation,194,56,experimental-setup,qa system,use,"qanet ( yu et al. , 2018 )","qa system use qanet ( yu et al. , 2018 )",0.6067075133323669
translation,194,56,experimental-setup,experimental setup,For,qa system,experimental setup For qa system,0.6226151585578918
translation,194,55,experiments,language model,is,standard recurrent neural network,language model is standard recurrent neural network,0.525769829750061
translation,194,55,experiments,standard recurrent neural network,formed of,single lstm layer,standard recurrent neural network formed of single lstm layer,0.5777081847190857
translation,194,8,model,objectives,beyond simply replicating,ground truth questions,objectives beyond simply replicating ground truth questions,0.6153894662857056
translation,194,8,model,ground truth questions,including,novel approach,ground truth questions including novel approach,0.6606835722923279
translation,194,8,model,novel approach,using,adversarial discriminator,novel approach using adversarial discriminator,0.6680906414985657
translation,194,8,model,adversarial discriminator,seeks to generate,questions,adversarial discriminator seeks to generate questions,0.7319051623344421
translation,194,8,model,indistinguishable,from,real examples,indistinguishable from real examples,0.5599638223648071
translation,194,8,model,model,optimise directly for,objectives,model optimise directly for objectives,0.6990100741386414
translation,194,48,model,seq2seq model,with,"attention ( bahdanau et al. , 2014 )","seq2seq model with attention ( bahdanau et al. , 2014 )",0.5821613669395447
translation,194,48,model,seq2seq model,add,additional answer encoder layer,seq2seq model add additional answer encoder layer,0.6091281771659851
translation,194,48,model,seq2seq model,initialise,decoder,seq2seq model initialise decoder,0.7259973883628845
translation,194,48,model,copy mechanism,add,additional answer encoder layer,copy mechanism add additional answer encoder layer,0.6310701966285706
translation,194,48,model,decoder,with,hidden state,decoder with hidden state,0.6375681161880493
translation,194,48,model,hidden state,final state of,encoder,hidden state final state of encoder,0.7530442476272583
translation,194,48,model,model,is,seq2seq model,model is seq2seq model,0.5814687609672546
translation,194,48,model,model,initialise,decoder,model initialise decoder,0.7755618691444397
translation,194,73,results,bleu score,has,reduces,bleu score has reduces,0.6188804507255554
translation,194,73,results,results,has,bleu score,results has bleu score,0.5436024069786072
translation,194,74,results,models,achieve,better scores,models achieve better scores,0.6361783742904663
translation,194,74,results,better scores,on,metrics,better scores on metrics,0.5270770788192749
translation,194,74,results,results,has,models,results has models,0.5335168838500977
translation,194,75,results,qa and lm reward,results in,better lm scores,qa and lm reward results in better lm scores,0.6380623579025269
translation,194,75,results,better lm scores,training on,only a lm reward,better lm scores training on only a lm reward,0.7423056960105896
translation,194,75,results,lm score,not,increase,lm score not increase,0.6961773037910461
translation,194,75,results,increase,has,smoothly,increase has smoothly,0.6019602417945862
translation,194,75,results,results,Jointly training on,qa and lm reward,results Jointly training on qa and lm reward,0.6850895285606384
translation,194,76,results,fine tuning,using,policy gradients,fine tuning using policy gradients,0.6026292443275452
translation,194,76,results,policy gradients,be used to attain,higher rewards,policy gradients be used to attain higher rewards,0.5510950088500977
translation,194,76,results,results,conclude that,fine tuning,results conclude that fine tuning,0.6730161309242249
translation,194,78,results,model,fine tuned on,qa and lm objective,model fine tuned on qa and lm objective,0.7116746306419373
translation,194,78,results,results,has,model,results has model,0.5339115858078003
translation,195,126,ablation-analysis,bfs loss,further improve,system performance,bfs loss further improve system performance,0.7296959161758423
translation,195,126,ablation-analysis,system performance,by encouraging learning,answer - aware dynamic entity graph,system performance by encouraging learning answer - aware dynamic entity graph,0.6493884325027466
translation,195,126,ablation-analysis,answer - aware dynamic entity graph,has,better,answer - aware dynamic entity graph has better,0.5719805955886841
translation,195,126,ablation-analysis,ablation analysis,has,bfs loss,ablation analysis has bfs loss,0.5286582112312317
translation,195,129,ablation-analysis,gated context reasoning module,important to,model,gated context reasoning module important to model,0.6300135850906372
translation,195,129,ablation-analysis,ablation analysis,both,gcn - based entity - aware answer encoder module,ablation analysis both gcn - based entity - aware answer encoder module,0.607377290725708
translation,195,130,ablation-analysis,relative contribution,of,2 % - 3 %,relative contribution of 2 % - 3 %,0.6164467930793762
translation,195,130,ablation-analysis,2 % - 3 %,for,overall performance improvement,2 % - 3 % for overall performance improvement,0.6172237396240234
translation,195,131,ablation-analysis,answer-related multi-hop evidence information,cannot be,identified,answer-related multi-hop evidence information cannot be identified,0.688347339630127
translation,195,131,ablation-analysis,gcn - based entityaware answer encoder,has,answer-related multi-hop evidence information,gcn - based entityaware answer encoder has answer-related multi-hop evidence information,0.5244901776313782
translation,195,131,ablation-analysis,ablation analysis,w/o GEAEnc,gcn - based entityaware answer encoder,ablation analysis w/o GEAEnc gcn - based entityaware answer encoder,0.5881156325340271
translation,195,132,ablation-analysis,multi-hop answer encoding,being,updated,multi-hop answer encoding being updated,0.6279523372650146
translation,195,132,ablation-analysis,next step 's answer - aware context encoding,will be,affected,next step 's answer - aware context encoding will be affected,0.6415196657180786
translation,195,132,ablation-analysis,multi-hop answer encoding,has,next step 's answer - aware context encoding,multi-hop answer encoding has next step 's answer - aware context encoding,0.554107666015625
translation,195,132,ablation-analysis,updated,has,next step 's answer - aware context encoding,updated has next step 's answer - aware context encoding,0.5749167799949646
translation,195,132,ablation-analysis,ablation analysis,Without,multi-hop answer encoding,ablation analysis Without multi-hop answer encoding,0.6844536662101746
translation,195,136,ablation-analysis,w/o erg,remove,encoder reasoning gate,w/o erg remove encoder reasoning gate,0.7122328281402588
translation,195,136,ablation-analysis,drops,by,around 3 %,drops by around 3 %,0.6788367629051208
translation,195,136,ablation-analysis,around 3 %,in,bleu -1,around 3 % in bleu -1,0.539871871471405
translation,195,136,ablation-analysis,w/o erg,has,performance,w/o erg has performance,0.5960664749145508
translation,195,136,ablation-analysis,encoder reasoning gate,has,performance,encoder reasoning gate has performance,0.5668157935142517
translation,195,136,ablation-analysis,performance,has,drops,performance has drops,0.5993483662605286
translation,195,136,ablation-analysis,ablation analysis,remove,encoder reasoning gate,ablation analysis remove encoder reasoning gate,0.6763818264007568
translation,195,136,ablation-analysis,ablation analysis,has,w/o erg,ablation analysis has w/o erg,0.5851275324821472
translation,195,109,hyperparameters,word embeddings,initialized by,glove,word embeddings initialized by glove,0.6727533936500549
translation,195,109,hyperparameters,hyperparameters,has,word embeddings,hyperparameters has word embeddings,0.4784160554409027
translation,195,110,hyperparameters,840b.300d,keep,our vocab size,840b.300d keep our vocab size,0.6236178278923035
translation,195,110,hyperparameters,our vocab size,as,45000,our vocab size as 45000,0.5795049071311951
translation,195,110,hyperparameters,hyperparameters,keep,our vocab size,hyperparameters keep our vocab size,0.5962493419647217
translation,195,110,hyperparameters,hyperparameters,has,840b.300d,hyperparameters has 840b.300d,0.5319387912750244
translation,195,111,hyperparameters,two -layer bi-directional lstms,for,encoder,two -layer bi-directional lstms for encoder,0.564456582069397
translation,195,111,hyperparameters,two -layer bi-directional lstms,for,decoder,two -layer bi-directional lstms for decoder,0.5593703985214233
translation,195,111,hyperparameters,two -layer uni-directional lstms,for,decoder,two -layer uni-directional lstms for decoder,0.5728337168693542
translation,195,111,hyperparameters,hidden size,is,300,hidden size is 300,0.6228809952735901
translation,195,111,hyperparameters,hyperparameters,use,two -layer bi-directional lstms,hyperparameters use two -layer bi-directional lstms,0.579109787940979
translation,195,111,hyperparameters,hyperparameters,use,two -layer uni-directional lstms,hyperparameters use two -layer uni-directional lstms,0.5769585371017456
translation,195,111,hyperparameters,hyperparameters,use,hidden size,hyperparameters use hidden size,0.6314985752105713
translation,195,112,hyperparameters,stochastic gradient descent ( sgd ),as,optimizer,stochastic gradient descent ( sgd ) as optimizer,0.5390346050262451
translation,195,112,hyperparameters,hyperparameters,use,stochastic gradient descent ( sgd ),hyperparameters use stochastic gradient descent ( sgd ),0.6222569346427917
translation,195,113,hyperparameters,initial learning rate,is,0.1,initial learning rate is 0.1,0.5426209568977356
translation,195,113,hyperparameters,reduced,during,training stage,reduced during training stage,0.7433761358261108
translation,195,113,hyperparameters,training stage,using,cosine annealing scheduler,training stage using cosine annealing scheduler,0.6716398596763611
translation,195,113,hyperparameters,hyperparameters,has,initial learning rate,hyperparameters has initial learning rate,0.4469831883907318
translation,195,114,hyperparameters,batch size,is,12,batch size is 12,0.6480603814125061
translation,195,114,hyperparameters,beam size,is,10,beam size is 10,0.6545984148979187
translation,195,114,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,195,114,hyperparameters,hyperparameters,has,beam size,hyperparameters has beam size,0.516274631023407
translation,195,115,hyperparameters,dropout probability,for,lstm,dropout probability for lstm,0.5869357585906982
translation,195,115,hyperparameters,lstm,to,0.2 and 0.3,lstm to 0.2 and 0.3,0.5542641282081604
translation,195,115,hyperparameters,0.2 and 0.3,for,gcn,0.2 and 0.3 for gcn,0.6860817074775696
translation,195,115,hyperparameters,hyperparameters,set,dropout probability,hyperparameters set dropout probability,0.6146286725997925
translation,195,116,hyperparameters,maximum number of epochs,set to,20,maximum number of epochs set to 20,0.7205719947814941
translation,195,116,hyperparameters,hyperparameters,has,maximum number of epochs,hyperparameters has maximum number of epochs,0.5072295069694519
translation,195,117,hyperparameters,maximum number of entities,in,each context,maximum number of entities in each context,0.49718669056892395
translation,195,117,hyperparameters,each context,to,80,each context to 80,0.6264703869819641
translation,195,117,hyperparameters,two -layer gcn,in,gcn - based answer encoder module,two -layer gcn in gcn - based answer encoder module,0.49647414684295654
translation,195,117,hyperparameters,hyperparameters,set,maximum number of entities,hyperparameters set maximum number of entities,0.6107422709465027
translation,195,117,hyperparameters,hyperparameters,use,two -layer gcn,hyperparameters use two -layer gcn,0.60646653175354
translation,195,118,hyperparameters,model,for,10 epochs,model for 10 epochs,0.6332400441169739
translation,195,118,hyperparameters,model,fine - tune,mulqg model,model fine - tune mulqg model,0.7110602259635925
translation,195,118,hyperparameters,mulqg model,with the help of,bfs loss,mulqg model with the help of bfs loss,0.6521608233451843
translation,195,118,hyperparameters,hyperparameters,After training,model,hyperparameters After training model,0.7272807955741882
translation,195,6,model,multi-hop encoding fusion network,for,question generation ( mulqg ),multi-hop encoding fusion network for question generation ( mulqg ),0.6400490999221802
translation,195,6,model,multi-hop encoding fusion network,does,context encoding,multi-hop encoding fusion network does context encoding,0.27747583389282227
translation,195,6,model,context encoding,in,multiple hops,context encoding in multiple hops,0.5729722380638123
translation,195,6,model,context encoding,with,encoding fusion,context encoding with encoding fusion,0.6594195365905762
translation,195,6,model,multiple hops,with,graph convolutional network,multiple hops with graph convolutional network,0.6135064959526062
translation,195,6,model,multiple hops,with,encoding fusion,multiple hops with encoding fusion,0.6809449791908264
translation,195,6,model,encoding fusion,via,encoder reasoning gate,encoding fusion via encoder reasoning gate,0.6680653095245361
translation,195,6,model,model,propose,multi-hop encoding fusion network,model propose multi-hop encoding fusion network,0.6577460169792175
translation,195,31,model,novel architecture,named,multi-hop encoding fusion network,novel architecture named multi-hop encoding fusion network,0.6984328627586365
translation,195,31,model,multi-hop encoding fusion network,for,question generation ( mulqg ),multi-hop encoding fusion network for question generation ( mulqg ),0.6400490999221802
translation,195,31,model,model,propose,novel architecture,model propose novel architecture,0.7315564155578613
translation,195,32,model,seq2seq qg framework,from,sing -hop,seq2seq qg framework from sing -hop,0.5960392951965332
translation,195,32,model,sing -hop,to,multi-hop,sing -hop to multi-hop,0.6109797358512878
translation,195,32,model,multi-hop,for,context encoding,multi-hop for context encoding,0.5944387912750244
translation,195,32,model,model,extends,seq2seq qg framework,model extends seq2seq qg framework,0.7157153487205505
translation,195,33,model,graph convolutional network ( gcn ),on,answer - aware dynamic entity graph,graph convolutional network ( gcn ) on answer - aware dynamic entity graph,0.5199878215789795
translation,195,33,model,answer - aware dynamic entity graph,constructed from,entity mentions,answer - aware dynamic entity graph constructed from entity mentions,0.6090134382247925
translation,195,33,model,answer - aware dynamic entity graph,to aggregate,potential evidence,answer - aware dynamic entity graph to aggregate potential evidence,0.7034529447555542
translation,195,33,model,entity mentions,in,answer and input paragraphs,entity mentions in answer and input paragraphs,0.5071873664855957
translation,195,33,model,potential evidence,related to,questions,potential evidence related to questions,0.658545196056366
translation,195,33,model,model,leverages,graph convolutional network ( gcn ),model leverages graph convolutional network ( gcn ),0.6738335490226746
translation,195,34,model,different attention mechanisms,to imitate,reasoning procedures,different attention mechanisms to imitate reasoning procedures,0.7220169305801392
translation,195,34,model,reasoning procedures,of,human beings,reasoning procedures of human beings,0.5298323035240173
translation,195,34,model,human beings,in,multihop generation process,human beings in multihop generation process,0.519679605960846
translation,195,34,model,model,use,different attention mechanisms,model use different attention mechanisms,0.6654888987541199
translation,195,48,model,encoding stage,achieved by,novel multi-hop encoder,encoding stage achieved by novel multi-hop encoder,0.6956496238708496
translation,195,48,model,model,has,encoding stage,model has encoding stage,0.5550634860992432
translation,195,49,model,decoding stage,use,maxout pointer decoder,decoding stage use maxout pointer decoder,0.6303915977478027
translation,195,49,model,model,At,decoding stage,model At decoding stage,0.5805700421333313
translation,195,195,model,mulqg,does,multi-hop context encoding,mulqg does multi-hop context encoding,0.2806948125362396
translation,195,195,model,mulqg,does,encoding fusion,mulqg does encoding fusion,0.3323907256126404
translation,195,195,model,multi-hop context encoding,with,graph convolutional network,multi-hop context encoding with graph convolutional network,0.6012771725654602
translation,195,195,model,encoding fusion,via,gated reasoning module,encoding fusion via gated reasoning module,0.691432535648346
translation,195,195,model,model,propose,mulqg,model propose mulqg,0.6965104341506958
translation,195,169,results,largely outperforms,with,more stable quality,largely outperforms with more stable quality,0.6468126177787781
translation,195,169,results,mp - gsn model,in terms of,fluency,mp - gsn model in terms of fluency,0.7294064164161682
translation,195,169,results,mp - gsn model,in terms of,answerability,mp - gsn model in terms of answerability,0.6948464512825012
translation,195,169,results,mp - gsn model,in terms of,completeness,mp - gsn model in terms of completeness,0.6965959668159485
translation,195,169,results,mp - gsn model,with,more stable quality,mp - gsn model with more stable quality,0.6148736476898193
translation,195,169,results,completeness,with,more stable quality,completeness with more stable quality,0.5864120125770569
translation,195,169,results,our mulqg model,has,largely outperforms,our mulqg model has largely outperforms,0.6043030619621277
translation,195,169,results,largely outperforms,has,mp - gsn model,largely outperforms has mp - gsn model,0.5538223385810852
translation,195,169,results,results,observe,our mulqg model,results observe our mulqg model,0.6141545176506042
translation,195,170,results,our model,tends to generate,more complete question,our model tends to generate more complete question,0.7570880651473999
translation,195,170,results,our model,achieve,comparable completeness score,our model achieve comparable completeness score,0.6299319267272949
translation,195,170,results,comparable completeness score,with,human annotations,comparable completeness score with human annotations,0.6168590784072876
translation,195,170,results,results,has,our model,results has our model,0.5871725678443909
translation,195,171,results,strongest baseline,by,20.8 %,strongest baseline by 20.8 %,0.5431168675422668
translation,195,171,results,strongest baseline,on,multi-hop evaluation,strongest baseline on multi-hop evaluation,0.5253090262413025
translation,195,171,results,outperform,has,strongest baseline,outperform has strongest baseline,0.5937130451202393
translation,195,171,results,results,For,multi-hop evaluation,results For multi-hop evaluation,0.5915095806121826
translation,196,228,baselines,baselines,has,gated recurrent neural network ( grnn ),baselines has gated recurrent neural network ( grnn ),0.5743584632873535
translation,196,179,results,human consensus,achieves,consistently higher scores,human consensus achieves consistently higher scores,0.6645013093948364
translation,196,179,results,consistently higher scores,than,human random,consistently higher scores than human random,0.6006503105163574
translation,196,179,results,human annotations,has,human consensus,human annotations has human consensus,0.5809678435325623
translation,196,179,results,results,among,human annotations,results among human annotations,0.49167555570602417
translation,196,181,results,grnn,performs,best,grnn performs best,0.6363447904586792
translation,196,181,results,best,compared with,all the other models,best compared with all the other models,0.6112852096557617
translation,196,181,results,all the other models,in,2/3 of runs,all the other models in 2/3 of runs,0.5538496375083923
translation,196,182,results,all the models,achieve,best score,all the models achieve best score,0.6014115810394287
translation,196,182,results,best score,on,v qg coco ? 5000,best score on v qg coco ? 5000,0.5745342969894409
translation,196,182,results,results,has,all the models,results has all the models,0.5321599841117859
translation,196,183,results,other models,on,v qg bing ? 5000 dataset,other models on v qg bing ? 5000 dataset,0.5344493985176086
translation,196,183,results,automatic metrics,has,grnn x model,automatic metrics has grnn x model,0.5807054042816162
translation,196,183,results,grnn x model,has,outperforms,grnn x model has outperforms,0.6596418619155884
translation,196,183,results,outperforms,has,other models,outperforms has other models,0.5875559449195862
translation,196,183,results,results,Using,automatic metrics,results Using automatic metrics,0.6610228419303894
translation,196,184,results,most competitive,is,k -nn + min bleu all,most competitive is k -nn + min bleu all,0.5416252017021179
translation,196,184,results,k -nn + min bleu all,performs,best,k -nn + min bleu all performs best,0.5703020095825195
translation,196,184,results,best,on,v qg coco ? 5000 and v qg f lickr ?5000 datasets,best on v qg coco ? 5000 and v qg f lickr ?5000 datasets,0.5202212929725647
translation,196,184,results,best,according to,bleu,best according to bleu,0.5326014757156372
translation,196,184,results,v qg coco ? 5000 and v qg f lickr ?5000 datasets,according to,bleu,v qg coco ? 5000 and v qg f lickr ?5000 datasets according to bleu,0.5482259392738342
translation,196,184,results,retrieval models,has,most competitive,retrieval models has most competitive,0.5406380891799927
translation,196,186,results,boost,from,1 - nn to k -nn models,boost from 1 - nn to k -nn models,0.6293272376060486
translation,196,186,results,1 - nn to k -nn models,is,considerable,1 - nn to k -nn models is considerable,0.6124131679534912
translation,196,186,results,considerable,according to,human and automatic metrics,considerable according to human and automatic metrics,0.6955998539924622
translation,196,186,results,results,has,boost,results has boost,0.4424405097961426
translation,196,187,results,none of the retrieval models,beat,grnn model,none of the retrieval models beat grnn model,0.6797751784324646
translation,196,187,results,grnn model,on,bing dataset,grnn model on bing dataset,0.5221265554428101
translation,196,187,results,results,important to note,none of the retrieval models,results important to note none of the retrieval models,0.5627100467681885
translation,197,7,baselines,mixkmeans,composes,question and answer space similarities,mixkmeans composes question and answer space similarities,0.7697836756706238
translation,197,7,baselines,question and answer space similarities,in a way that,space,question and answer space similarities in a way that space,0.6257390975952148
translation,197,7,baselines,space,on which,match,space on which match,0.5354867577552795
translation,197,7,baselines,match,is,higher,match is higher,0.6376435160636902
translation,197,7,baselines,higher,allowed to,dominate,higher allowed to dominate,0.694915771484375
translation,197,7,baselines,our method,has,mixkmeans,our method has mixkmeans,0.6324191093444824
translation,197,7,baselines,baselines,has,our method,baselines has our method,0.5691280364990234
translation,197,7,baselines,baselines,has,mixkmeans,baselines has mixkmeans,0.62556391954422
translation,197,135,baselines,aenn,refers to,k-means clustering,aenn refers to k-means clustering,0.6903337836265564
translation,197,135,baselines,aenn,refers to,k-means clustering,aenn refers to k-means clustering,0.6903337836265564
translation,197,135,baselines,k-means clustering,in,latent space,k-means clustering in latent space,0.5231824517250061
translation,197,135,baselines,correlated auto-encoders,across,q-a subspaces,correlated auto-encoders across q-a subspaces,0.6836194396018982
translation,197,135,baselines,ghf - art,has,"meng et al. , 2014 )","ghf - art has meng et al. , 2014 )",0.5928456783294678
translation,197,171,baselines,mixkmeans,works by,iteratively optimizing,mixkmeans works by iteratively optimizing,0.7379460334777832
translation,197,171,baselines,iteratively optimizing,has,two sets of parameters,iteratively optimizing has two sets of parameters,0.49739041924476624
translation,197,171,baselines,two sets of parameters,has,cluster assignments,two sets of parameters has cluster assignments,0.5810955762863159
translation,197,171,baselines,baselines,has,mixkmeans,baselines has mixkmeans,0.62556391954422
translation,197,6,model,clustering method,exploits,two -part question - answer structure,clustering method exploits two -part question - answer structure,0.6808033585548401
translation,197,6,model,two -part question - answer structure,in,qa datasets,two -part question - answer structure in qa datasets,0.48985421657562256
translation,197,6,model,two -part question - answer structure,to improve,clustering quality,two -part question - answer structure to improve clustering quality,0.6693383455276489
translation,197,6,model,model,present,clustering method,model present clustering method,0.6681209802627563
translation,197,154,results,mixkmeans,seen to outperform,other methods,mixkmeans seen to outperform other methods,0.6759452819824219
translation,197,154,results,significantly higher,in,f-score,significantly higher in f-score,0.5059244632720947
translation,197,154,results,results,has,mixkmeans,results has mixkmeans,0.5614935159683228
translation,197,156,results,mixkmeans,achieves,f-score improvement,mixkmeans achieves f-score improvement,0.6772946715354919
translation,197,156,results,f-score improvement,of,between 30 ? 100 %,f-score improvement of between 30 ? 100 %,0.563336193561554
translation,197,156,results,between 30 ? 100 %,over,other methods,between 30 ? 100 % over other methods,0.653285026550293
translation,198,232,ablation-analysis,grammatical features,appear to be,most important,grammatical features appear to be most important,0.5826784372329712
translation,198,232,ablation-analysis,grammatical features,removing them from,feature set,grammatical features removing them from feature set,0.6429761052131653
translation,198,232,ablation-analysis,9.0 % absolute drop,in,acceptability,9.0 % absolute drop in acceptability,0.5132896304130554
translation,198,232,ablation-analysis,acceptability,in,top 20 % of questions,acceptability in top 20 % of questions,0.4868081212043762
translation,198,232,ablation-analysis,52.3 %,to,43.3 %,52.3 % to 43.3 %,0.5912092924118042
translation,198,232,ablation-analysis,ablation analysis,has,grammatical features,ablation analysis has grammatical features,0.5516883730888367
translation,198,233,ablation-analysis,features,not appear to be,particularly helpful,features not appear to be particularly helpful,0.5991729497909546
translation,198,233,ablation-analysis,particularly helpful,notably,n,particularly helpful notably n,0.8035355806350708
translation,198,233,ablation-analysis,n,-,gram language model features,n - gram language model features,0.5433933138847351
translation,198,233,ablation-analysis,n,has,gram language model features,n has gram language model features,0.5211393237113953
translation,198,233,ablation-analysis,ablation analysis,Some of,features,ablation analysis Some of features,0.6971060037612915
translation,198,236,ablation-analysis,several types of features,led to,somewhat larger drops,several types of features led to somewhat larger drops,0.727226197719574
translation,198,236,ablation-analysis,somewhat larger drops,in,performance,somewhat larger drops in performance,0.5596100687980652
translation,198,236,ablation-analysis,ablation analysis,removing,several types of features,ablation analysis removing several types of features,0.7352429032325745
translation,198,221,results,quality,of,top fifth,quality of top fifth,0.570586085319519
translation,198,221,results,nearly doubled,by,ranking,nearly doubled by ranking,0.6792492866516113
translation,198,221,results,ranking,with,all the features,ranking with all the features,0.6337018609046936
translation,198,221,results,results,has,quality,results has quality,0.38800710439682007
translation,198,222,results,surface features,to,lesser extent,surface features to lesser extent,0.5569369792938232
translation,198,222,results,surface features,has,improved,surface features has improved,0.6026790142059326
translation,198,222,results,improved,has,question quality,improved has question quality,0.5501083731651306
translation,198,222,results,results,Ranking with,surface features,results Ranking with surface features,0.6210792064666748
translation,199,43,baselines,choice of plausible alternatives,Asking about,plausible cause,choice of plausible alternatives Asking about plausible cause,0.71402508020401
translation,199,43,baselines,choice of plausible alternatives,Asking about,plausible result,choice of plausible alternatives Asking about plausible result,0.6794084906578064
translation,199,43,baselines,certain event,expressed in,simple sentence,certain event expressed in simple sentence,0.6235364079475403
translation,199,43,baselines,copa,has,choice of plausible alternatives,copa has choice of plausible alternatives,0.5669722557067871
translation,199,43,baselines,baselines,has,copa,baselines has copa,0.5647990703582764
translation,199,46,experiments,mc - taco,Questions about,temporal aspects,mc - taco Questions about temporal aspects,0.751010537147522
translation,199,46,experiments,multiple choice temporal commonsense,Questions about,temporal aspects,multiple choice temporal commonsense Questions about temporal aspects,0.7192623019218445
translation,199,46,experiments,temporal aspects,of,events,temporal aspects of events,0.5930497050285339
translation,199,51,experiments,piqa,has,physical interaction question answering,piqa has physical interaction question answering,0.5177041292190552
translation,199,89,hyperparameters,pairs,of,words,pairs of words,0.6442705392837524
translation,199,89,hyperparameters,pairs,extract,joint occurrences (,pairs extract joint occurrences (,0.6758747696876526
translation,199,89,hyperparameters,words,from,answer choices,words from answer choices,0.5761154294013977
translation,199,89,hyperparameters,words,context and question and from,answer choices,words context and question and from answer choices,0.7015711069107056
translation,199,89,hyperparameters,joint occurrences (,minimum frequency of,100 ),joint occurrences ( minimum frequency of 100 ),0.7332749366760254
translation,199,89,hyperparameters,joint occurrences (,in,google n-grams,joint occurrences ( in google n-grams,0.5138005018234253
translation,199,89,hyperparameters,100 ),in,google n-grams,100 ) in google n-grams,0.5596700310707092
translation,199,89,hyperparameters,hyperparameters,For,pairs,hyperparameters For pairs,0.6193116307258606
translation,199,124,results,overall performance,perform,substantially better,overall performance perform substantially better,0.5608567595481873
translation,199,124,results,worse,for,zero-shot models,worse for zero-shot models,0.6430118680000305
translation,199,124,results,zero-shot models,compared to,state - of - the - art supervised models,zero-shot models compared to state - of - the - art supervised models,0.5665188431739807
translation,199,124,results,substantially better,than,majority baselines,substantially better than majority baselines,0.5907607674598694
translation,199,124,results,majority baselines,on,most tasks,majority baselines on most tasks,0.46578311920166016
translation,199,124,results,results,has,overall performance,results has overall performance,0.5951287150382996
translation,199,125,results,few points,from,external knowledge model,few points from external knowledge model,0.5392589569091797
translation,199,125,results,lm - based models,has,self - talk,lm - based models has self - talk,0.5680873394012451
translation,199,125,results,results,Among,lm - based models,results Among lm - based models,0.5779090523719788
translation,199,127,results,comet,achieves,best performance,comet achieves best performance,0.7210606932640076
translation,199,127,results,best performance,across,tasks,best performance across tasks,0.7012199759483337
translation,199,127,results,knowledge informed models,has,comet,knowledge informed models has comet,0.6472562551498413
translation,199,127,results,results,Among,knowledge informed models,results Among knowledge informed models,0.5716895461082458
translation,199,157,results,xlnet,performs,worse,xlnet performs worse,0.7466615438461304
translation,199,157,results,worse,on,all measures,worse on all measures,0.5400877594947815
translation,199,157,results,results,has,xlnet,results has xlnet,0.5467540621757507
translation,199,158,results,results,has,conceptnet 's clarifications,results has conceptnet 's clarifications,0.5587393045425415
translation,200,30,results,relation mappings,from,clueweb,relation mappings from clueweb,0.5729148387908936
translation,200,30,results,indirect qa f 1,improve,large margin,indirect qa f 1 improve large margin,0.6602205038070679
translation,200,30,results,results,directly mine,relation mappings,results directly mine relation mappings,0.6983874440193176
translation,200,189,results,cluewebmapping,gives,reasonably good precision,cluewebmapping gives reasonably good precision,0.6159189939498901
translation,200,189,results,reasonably good precision,on,prediction,reasonably good precision on prediction,0.5106906890869141
translation,200,189,results,results,has,cluewebmapping,results has cluewebmapping,0.5414314866065979
translation,201,7,model,tables,as,semi-structured formalism,tables as semi-structured formalism,0.500839114189148
translation,201,7,model,model,explore,tables,model explore tables,0.6297480463981628
translation,201,8,model,structure of tables,to guide,construction,structure of tables to guide construction,0.6896328926086426
translation,201,8,model,construction,of,dataset,construction of dataset,0.6002949476242065
translation,201,8,model,of over 9000 multiple -choice questions,with,rich alignment annotations,of over 9000 multiple -choice questions with rich alignment annotations,0.6617658734321594
translation,201,8,model,easily and efficiently,via,crowd-sourcing,easily and efficiently via crowd-sourcing,0.647768497467041
translation,201,23,model,tables,represent,general knowledge facts,tables represent general knowledge facts,0.5597851872444153
translation,201,23,model,model,focus on,tables,model focus on tables,0.634682297706604
translation,201,26,model,collection of over 9000 mcqs,with,alignment annotations,collection of over 9000 mcqs with alignment annotations,0.5928758978843689
translation,201,26,model,alignment annotations,to,table elements,alignment annotations to table elements,0.49875515699386597
translation,201,26,model,alignment annotations,using,tables,alignment annotations using tables,0.5973567366600037
translation,201,26,model,table elements,using,tables,table elements using tables,0.5833117365837097
translation,201,26,model,model,crowd-source,collection of over 9000 mcqs,model crowd-source collection of over 9000 mcqs,0.7720807194709778
translation,202,141,experimental-setup,rde model,use,two single layer gated recurrent unit ( gru ),rde model use two single layer gated recurrent unit ( gru ),0.6163514852523804
translation,202,141,experimental-setup,two single layer gated recurrent unit ( gru ),with,300 hidden units,two single layer gated recurrent unit ( gru ) with 300 hidden units,0.632468044757843
translation,202,141,experimental-setup,experimental setup,implement,rde model,experimental setup implement rde model,0.6589698195457458
translation,202,144,experimental-setup,hidden units weight matrix,of,gru,hidden units weight matrix of gru,0.5651276111602783
translation,202,144,experimental-setup,gru,initialized using,orthogonal weights,gru initialized using orthogonal weights,0.7753375172615051
translation,202,144,experimental-setup,input embedding weight matrix,initialized using,pre-trained embedding vector,input embedding weight matrix initialized using pre-trained embedding vector,0.6701329946517944
translation,202,144,experimental-setup,glove,with,300 dimension,glove with 300 dimension,0.7372000813484192
translation,202,144,experimental-setup,experimental setup,has,hidden units weight matrix,experimental setup has hidden units weight matrix,0.5098258256912231
translation,202,144,experimental-setup,experimental setup,has,input embedding weight matrix,experimental setup has input embedding weight matrix,0.4953271150588989
translation,202,146,experimental-setup,adam optimizer,with,gradients,adam optimizer with gradients,0.604223906993866
translation,202,146,experimental-setup,gradients,clipped with,norm value 1,gradients clipped with norm value 1,0.6756489276885986
translation,202,146,experimental-setup,experimental setup,use,adam optimizer,experimental setup use adam optimizer,0.5987385511398315
translation,202,148,experimental-setup,hrde model,use,two single layer gru,hrde model use two single layer gru,0.6247875690460205
translation,202,148,experimental-setup,hrde model,use,two single layer gru,hrde model use two single layer gru,0.6247875690460205
translation,202,148,experimental-setup,two single layer gru,with,300 hidden units,two single layer gru with 300 hidden units,0.630212664604187
translation,202,148,experimental-setup,two single layer gru,with,300 hidden units,two single layer gru with 300 hidden units,0.630212664604187
translation,202,148,experimental-setup,300 hidden units,for,word- level rnn part,300 hidden units for word- level rnn part,0.5617345571517944
translation,202,148,experimental-setup,300 hidden units,for,chunk - level rnn part,300 hidden units for chunk - level rnn part,0.5610054731369019
translation,202,148,experimental-setup,300 hidden units,for,chunk - level rnn part,300 hidden units for chunk - level rnn part,0.5610054731369019
translation,202,148,experimental-setup,two single layer gru,with,300 hidden units,two single layer gru with 300 hidden units,0.630212664604187
translation,202,148,experimental-setup,300 hidden units,for,chunk - level rnn part,300 hidden units for chunk - level rnn part,0.5610054731369019
translation,202,148,experimental-setup,experimental setup,For,hrde model,experimental setup For hrde model,0.5980315804481506
translation,202,151,experimental-setup,latent topic memory dimensions,as,256,latent topic memory dimensions as 256,0.5355797410011292
translation,202,151,experimental-setup,256,in,ubuntu - v1 and ubuntu - v2,256 in ubuntu - v1 and ubuntu - v2,0.5785254240036011
translation,202,151,experimental-setup,experimental setup,choose,latent topic memory dimensions,experimental setup choose latent topic memory dimensions,0.6349110007286072
translation,202,155,experimental-setup,regularization,with,ratio,regularization with ratio,0.6738913059234619
translation,202,155,experimental-setup,ratio,of,0.2,ratio of 0.2,0.6545228958129883
translation,202,155,experimental-setup,ratio,of,0.3,ratio of 0.3,0.6457880735397339
translation,202,155,experimental-setup,ratio,of,0.8,ratio of 0.8,0.6450122594833374
translation,202,155,experimental-setup,0.2,for,rnn,0.2 for rnn,0.6652160286903381
translation,202,155,experimental-setup,rnn,in,rde and the rde - ltc,rnn in rde and the rde - ltc,0.6058782935142517
translation,202,155,experimental-setup,rnn,in,rde -ltc and the hrde - ltc,rnn in rde -ltc and the hrde - ltc,0.5945616364479065
translation,202,155,experimental-setup,rnn,in,rde -ltc and the hrde - ltc,rnn in rde -ltc and the hrde - ltc,0.5945616364479065
translation,202,155,experimental-setup,rnn,in,rde -ltc and the hrde - ltc,rnn in rde -ltc and the hrde - ltc,0.5945616364479065
translation,202,155,experimental-setup,0.3,for,word- level rnn part,0.3 for word- level rnn part,0.5912690162658691
translation,202,155,experimental-setup,word- level rnn part,in,hrde and the hrde - ltc,word- level rnn part in hrde and the hrde - ltc,0.5284299254417419
translation,202,155,experimental-setup,0.8,for,latent topic memory,0.8 for latent topic memory,0.574825644493103
translation,202,155,experimental-setup,latent topic memory,in,rde -ltc and the hrde - ltc,latent topic memory in rde -ltc and the hrde - ltc,0.5156198740005493
translation,202,155,experimental-setup,experimental setup,has,dropout,experimental setup has dropout,0.5067690014839172
translation,202,161,experimental-setup,100 hidden units,for,rde and the rde - ltc,100 hidden units for rde and the rde - ltc,0.622660219669342
translation,202,161,experimental-setup,100 hidden units,for,hrde - ltc,100 hidden units for hrde - ltc,0.6093184351921082
translation,202,161,experimental-setup,300 hidden units,for,hrde,300 hidden units for hrde,0.6174899339675903
translation,202,161,experimental-setup,300 hidden units,for,hrde - ltc,300 hidden units for hrde - ltc,0.6146803498268127
translation,202,161,experimental-setup,300 hidden units,for,hrde - ltc,300 hidden units for hrde - ltc,0.6146803498268127
translation,202,161,experimental-setup,200 hidden units,for,hrde - ltc,200 hidden units for hrde - ltc,0.6130593419075012
translation,202,161,experimental-setup,vocabulary size,of,"28,848","vocabulary size of 28,848",0.5706542730331421
translation,202,161,experimental-setup,experimental setup,use,100 hidden units,experimental setup use 100 hidden units,0.5781092643737793
translation,202,161,experimental-setup,experimental setup,use,300 hidden units,experimental setup use 300 hidden units,0.5767746567726135
translation,202,162,experimental-setup,combined model,with,( h ) rde and ltc,combined model with ( h ) rde and ltc,0.653777003288269
translation,202,162,experimental-setup,combined model,with,dimensions,combined model with dimensions,0.6087576150894165
translation,202,162,experimental-setup,dimensions,of,latent topic memory,dimensions of latent topic memory,0.5161558389663696
translation,202,162,experimental-setup,latent topic memory,is,64,latent topic memory is 64,0.5439009070396423
translation,202,162,experimental-setup,number of latent cluster,is,4,number of latent cluster is 4,0.6011762619018555
translation,202,162,experimental-setup,combined model,has,dimensions,combined model has dimensions,0.5550945401191711
translation,202,162,experimental-setup,( h ) rde and ltc,has,dimensions,( h ) rde and ltc has dimensions,0.5758438110351562
translation,202,145,experiments,vocabulary size,is,"144,953 and 183,045","vocabulary size is 144,953 and 183,045",0.5797983407974243
translation,202,145,experiments,"144,953 and 183,045",for,ubuntu - v1 / v2 case,"144,953 and 183,045 for ubuntu - v1 / v2 case",0.6059775948524475
translation,202,4,model,novel end-to - end neural architecture,adapts,hierarchical recurrent neural network,novel end-to - end neural architecture adapts hierarchical recurrent neural network,0.6616821885108948
translation,202,4,model,novel end-to - end neural architecture,adapts,latent topic clustering module,novel end-to - end neural architecture adapts latent topic clustering module,0.6388481855392456
translation,202,4,model,novel end-to - end neural architecture,has,for ranking candidate answers,novel end-to - end neural architecture has for ranking candidate answers,0.5229843854904175
translation,202,4,model,model,propose,novel end-to - end neural architecture,model propose novel end-to - end neural architecture,0.6703401207923889
translation,202,5,model,text,encoded to,vector representation,text encoded to vector representation,0.7397028207778931
translation,202,5,model,vector representation,from,wordlevel,vector representation from wordlevel,0.550853431224823
translation,202,7,model,latent topic clustering module,extracts,semantic information,latent topic clustering module extracts semantic information,0.5938481688499451
translation,202,7,model,semantic information,from,target samples,semantic information from target samples,0.5627491474151611
translation,202,7,model,model,has,latent topic clustering module,model has latent topic clustering module,0.520812451839447
translation,202,24,model,in ranking answer candidates,which are longer than,single sentence,in ranking answer candidates which are longer than single sentence,0.6798622012138367
translation,202,24,model,performance,has,in ranking answer candidates,performance has in ranking answer candidates,0.6011965274810791
translation,202,24,model,model,investigating,novel neural network architecture,model investigating novel neural network architecture,0.6973966360092163
translation,202,28,model,latent topic clustering ( ltc ) module,to extract,latent information,latent topic clustering ( ltc ) module to extract latent information,0.7136290669441223
translation,202,28,model,latent topic clustering ( ltc ) module,apply,additional information,latent topic clustering ( ltc ) module apply additional information,0.6603495478630066
translation,202,28,model,latent information,from,target dataset,latent information from target dataset,0.5276467800140381
translation,202,28,model,additional information,in,end-to - end training,additional information in end-to - end training,0.5149928331375122
translation,202,28,model,model,propose,latent topic clustering ( ltc ) module,model propose latent topic clustering ( ltc ) module,0.6626117825508118
translation,202,29,model,each data sample,to find,nearest topic cluster,each data sample to find nearest topic cluster,0.6180029511451721
translation,202,29,model,nearest topic cluster,helping,neural network model,nearest topic cluster helping neural network model,0.6854694485664368
translation,202,29,model,neural network model,analyze,entire data,neural network model analyze entire data,0.6832922101020813
translation,202,142,model,gru,to encode,{ context } and { response},gru to encode { context } and { response},0.7211722731590271
translation,202,142,model,model,has,gru,model has gru,0.6196864247322083
translation,202,6,results,our model,shows,very small performance degradations,our model shows very small performance degradations,0.6762396693229675
translation,202,6,results,very small performance degradations,in,longer text comprehension,very small performance degradations in longer text comprehension,0.4923018515110016
translation,202,6,results,hierarchical structure,has,our model,hierarchical structure has our model,0.5890589952468872
translation,202,6,results,results,by adapting,hierarchical structure,results by adapting hierarchical structure,0.701023519039154
translation,202,173,results,our proposed hrde and hrde - ltc models,achieve,best performance,our proposed hrde and hrde - ltc models achieve best performance,0.6419538855552673
translation,202,173,results,best performance,for,ubuntu- v1 dataset,best performance for ubuntu- v1 dataset,0.5391861200332642
translation,202,174,results,rde - ltc model,shows,improvements,rde - ltc model shows improvements,0.6880831122398376
translation,202,174,results,improvements,from,baseline model,improvements from baseline model,0.5538287162780762
translation,202,174,results,improvements,from,rde,improvements from rde,0.6544696092605591
translation,202,174,results,results,find that,rde - ltc model,results find that rde - ltc model,0.6529428958892822
translation,202,183,results,our model,performs,better,our model performs better,0.6546649932861328
translation,202,183,results,better,with,exquisite datasets,better with exquisite datasets,0.6538802981376648
translation,202,185,results,samsung qa case,indicates,"proposed rde - ltc , hrde , and the hrde - ltc model","samsung qa case indicates proposed rde - ltc , hrde , and the hrde - ltc model",0.6042413711547852
translation,202,185,results,"proposed rde - ltc , hrde , and the hrde - ltc model",show,performance improvements,"proposed rde - ltc , hrde , and the hrde - ltc model show performance improvements",0.5856361985206604
translation,202,185,results,performance improvements,compared to,baseline model,performance improvements compared to baseline model,0.6423296928405762
translation,202,185,results,performance improvements,compared to,tf -idf and rde,performance improvements compared to tf -idf and rde,0.655907392501831
translation,202,185,results,results,In,samsung qa case,results In samsung qa case,0.5596595406532288
translation,202,186,results,average accuracy statistics,are,higher,average accuracy statistics are higher,0.6014829874038696
translation,202,186,results,higher,in,samsung qa case,higher in samsung qa case,0.5607903003692627
translation,202,186,results,higher,compared to,ubuntu case,higher compared to ubuntu case,0.6571796536445618
translation,202,186,results,samsung qa case,compared to,ubuntu case,samsung qa case compared to ubuntu case,0.6271207928657532
translation,202,186,results,results,has,average accuracy statistics,results has average accuracy statistics,0.5617614984512329
translation,202,199,results,increase,as,number of latent clusters,increase as number of latent clusters,0.5520380139350891
translation,202,199,results,increase,until,3,increase until 3,0.7405285239219666
translation,202,199,results,increase,until,4,increase until 4,0.7225849032402039
translation,202,199,results,3,for,ubuntu,3 for ubuntu,0.5330172777175903
translation,202,199,results,4,for,samsung qa case,4 for samsung qa case,0.5622045397758484
translation,202,199,results,model performances,has,increase,model performances has increase,0.5672147870063782
translation,202,199,results,number of latent clusters,has,increase,number of latent clusters has increase,0.5847741365432739
translation,202,199,results,results,indicates,model performances,results indicates model performances,0.6433742046356201
translation,203,211,ablation-analysis,2d dependency,in,source and target sentences,2d dependency in source and target sentences,0.47936639189720154
translation,203,211,ablation-analysis,2d dependency,has,system performance,2d dependency has system performance,0.5687215328216553
translation,203,211,ablation-analysis,source and target sentences,has,system performance,source and target sentences has system performance,0.5533135533332825
translation,203,211,ablation-analysis,ablation analysis,by modeling,2d dependency,ablation analysis by modeling 2d dependency,0.8069961667060852
translation,203,214,ablation-analysis,categories,used for,sentence types,categories used for sentence types,0.6399118900299072
translation,203,214,ablation-analysis,sentence types,observe,impact,sentence types observe impact,0.6051563620567322
translation,203,214,ablation-analysis,impact,on,dependence tagging performance,impact on dependence tagging performance,0.5912633538246155
translation,203,214,ablation-analysis,ablation analysis,Regarding,categories,ablation analysis Regarding categories,0.6657112240791321
translation,203,34,experiments,sentence type labeling,define,rich set of categories,sentence type labeling define rich set of categories,0.5779471397399902
translation,203,35,model,linear-chain conditional random fields ( crf ),to take advantage of,many long-distance and non-local features,linear-chain conditional random fields ( crf ) to take advantage of many long-distance and non-local features,0.6533889174461365
translation,203,35,model,model,use,linear-chain conditional random fields ( crf ),model use linear-chain conditional random fields ( crf ),0.6574186682701111
translation,203,188,results,crfs,achieves,significantly better performance,crfs achieves significantly better performance,0.6722621917724609
translation,203,188,results,significantly better performance,than,hmms,significantly better performance than hmms,0.6105666756629944
translation,203,188,results,hmms,for,most categories,hmms for most categories,0.6126720905303955
translation,203,190,results,two major types of problems and answers,has,our system,two major types of problems and answers has our system,0.6262972950935364
translation,203,190,results,results,For,two major types of problems and answers,results For two major types of problems and answers,0.5702892541885376
translation,203,191,results,minority types like feedbacks,performs,reasonably well,minority types like feedbacks performs reasonably well,0.613030195236206
translation,203,191,results,results,Even for,minority types like feedbacks,results Even for minority types like feedbacks,0.6842800974845886
translation,203,192,results,performance,on average,better,performance on average better,0.6979153156280518
translation,203,192,results,better,compared to,finer grained categories,better compared to finer grained categories,0.6663730144500732
translation,203,192,results,coarse types,has,performance,coarse types has performance,0.6088435053825378
translation,203,192,results,results,When using,coarse types,results When using coarse types,0.716987669467926
translation,203,210,results,outperform,for,conditions,outperform for conditions,0.718765914440155
translation,203,210,results,linear-chain crfs,for,conditions,linear-chain crfs for conditions,0.6423394083976746
translation,203,210,results,2d crfs,has,outperform,2d crfs has outperform,0.5918579697608948
translation,203,210,results,outperform,has,linear-chain crfs,outperform has linear-chain crfs,0.5710710883140564
translation,203,210,results,results,see that,2d crfs,results see that 2d crfs,0.5798822641372681
translation,203,212,results,sentence types,when using,automatic sentence type tagging systems,sentence types when using automatic sentence type tagging systems,0.638753354549408
translation,203,212,results,sentence types,is,performance drop,sentence types is performance drop,0.5869105458259583
translation,203,212,results,results,For,sentence types,results For sentence types,0.5335132479667664
translation,203,215,results,performance,using,fine grained types,performance using fine grained types,0.6796275973320007
translation,203,215,results,far behind,using,fine grained types,far behind using fine grained types,0.7230995297431946
translation,203,215,results,general categories,has,performance,general categories has performance,0.5588961243629456
translation,203,215,results,results,When using,general categories,results When using general categories,0.6997756958007812
translation,204,31,ablation-analysis,more visual object features,included in,model 's input,more visual object features included in model 's input,0.6693835854530334
translation,204,31,ablation-analysis,more visual object features,has,better,more visual object features has better,0.547753632068634
translation,204,31,ablation-analysis,model 's input,has,better,model 's input has better,0.5592344403266907
translation,204,31,ablation-analysis,better,has,model,better has model,0.5721622705459595
translation,204,138,ablation-analysis,full b2t2 model,add,visual embeddings,full b2t2 model add visual embeddings,0.5797058343887329
translation,204,138,ablation-analysis,visual embeddings,in,last layer,visual embeddings in last layer,0.4770713746547699
translation,204,138,ablation-analysis,visual embeddings,lose,3.3 % accuracy,visual embeddings lose 3.3 % accuracy,0.641950249671936
translation,204,138,ablation-analysis,last layer,of,bert,last layer of bert,0.6522579789161682
translation,204,138,ablation-analysis,ablation analysis,in,full b2t2 model,ablation analysis in full b2t2 model,0.5318490266799927
translation,204,144,ablation-analysis,size,of,object detection model,size of object detection model,0.5163007378578186
translation,204,144,ablation-analysis,size,is,important,size is important,0.6011995077133179
translation,204,144,ablation-analysis,object detection model,is,important,object detection model is important,0.5147637128829956
translation,204,144,ablation-analysis,ablation analysis,appears that,size,ablation analysis appears that size,0.702410101890564
translation,204,144,ablation-analysis,ablation analysis,even,size,ablation analysis even size,0.6760253310203552
translation,204,145,ablation-analysis,resnet - 152,for,resnet - 50,resnet - 152 for resnet - 50,0.6353675127029419
translation,204,145,ablation-analysis,accuracy,decreases by,1.5 %,accuracy decreases by 1.5 %,0.7018636465072632
translation,204,145,ablation-analysis,resnet - 152,has,accuracy,resnet - 152 has accuracy,0.6132131218910217
translation,204,145,ablation-analysis,resnet - 50,has,accuracy,resnet - 50 has accuracy,0.5872269868850708
translation,204,145,ablation-analysis,ablation analysis,swap out,resnet - 152,ablation analysis swap out resnet - 152,0.7675918340682983
translation,204,85,experimental-setup,resnet - 152,pretrained on,imagenet,resnet - 152 pretrained on imagenet,0.7624134421348572
translation,204,85,experimental-setup,resnet - 152,yields,vector representation,resnet - 152 yields vector representation,0.6826807856559753
translation,204,85,experimental-setup,vector representation,has,of size d = 2048,vector representation has of size d = 2048,0.5796313285827637
translation,204,95,experimental-setup,models,with,grid of hyperparameters,models with grid of hyperparameters,0.6042516827583313
translation,204,95,experimental-setup,learning rate,of,2 ? 10 ?5 and 3 ? 10 ?5,learning rate of 2 ? 10 ?5 and 3 ? 10 ?5,0.6499806642532349
translation,204,95,experimental-setup,learning rate,of,two random seed,learning rate of two random seed,0.6137553453445435
translation,204,95,experimental-setup,learning rate,with,linear learning rate decay,learning rate with linear learning rate decay,0.633202314376831
translation,204,95,experimental-setup,"3 , 4 , and 5 epochs",with,linear learning rate decay,"3 , 4 , and 5 epochs with linear learning rate decay",0.6305249333381653
translation,204,95,experimental-setup,two random seed,for,initialization,two random seed for initialization,0.5942739844322205
translation,204,95,experimental-setup,grid of hyperparameters,has,learning rate,grid of hyperparameters has learning rate,0.5091347098350525
translation,204,4,model,multimodal context,introduce,simple yet powerful neural architecture,multimodal context introduce simple yet powerful neural architecture,0.5624194741249084
translation,204,4,model,simple yet powerful neural architecture,for,data,simple yet powerful neural architecture for data,0.6298127770423889
translation,204,4,model,simple yet powerful neural architecture,combines,vision and natural language,simple yet powerful neural architecture combines vision and natural language,0.6689823865890503
translation,204,4,model,model,To advance,multimodal context,model To advance multimodal context,0.664063572883606
translation,204,5,model,bounding boxes in text transformer,leverages,referential information,bounding boxes in text transformer leverages referential information,0.7020004987716675
translation,204,5,model,binding words,portions of,image,binding words portions of image,0.6693602800369263
translation,204,5,model,image,in,single unified architecture,image in single unified architecture,0.5741646885871887
translation,204,5,model,referential information,has,binding words,referential information has binding words,0.5320055484771729
translation,204,5,model,model,has,bounding boxes in text transformer,model has bounding boxes in text transformer,0.5339398384094238
translation,204,22,model,visual context,in addition to,language,visual context in addition to language,0.5496295690536499
translation,204,22,model,visual context,show,right integration,visual context show right integration,0.6199073791503906
translation,204,22,model,right integration,of,visual and linguistic information,right integration of visual and linguistic information,0.5336957573890686
translation,204,22,model,visual and linguistic information,yield,improvements,visual and linguistic information yield improvements,0.6635257601737976
translation,204,22,model,improvements,in,visual question answering,improvements in visual question answering,0.4789239764213562
translation,204,22,model,model,consider,visual context,model consider visual context,0.6593275666236877
translation,204,22,model,model,show,right integration,model show right integration,0.7158815860748291
translation,204,6,results,highly effective,on,visual commonsense reasoning benchmark,highly effective on visual commonsense reasoning benchmark,0.5156644582748413
translation,204,6,results,highly effective,achieving,new state - of - the - art,highly effective achieving new state - of - the - art,0.6741874814033508
translation,204,6,results,new state - of - the - art,with,25 % relative reduction,new state - of - the - art with 25 % relative reduction,0.6583415865898132
translation,204,6,results,new state - of - the - art,obtaining,best performance,new state - of - the - art obtaining best performance,0.64225172996521
translation,204,6,results,25 % relative reduction,in,error rate,25 % relative reduction in error rate,0.5429221987724304
translation,204,6,results,25 % relative reduction,compared to,published baselines,25 % relative reduction compared to published baselines,0.7194195985794067
translation,204,6,results,best performance,on,public leaderboard,best performance on public leaderboard,0.5250337719917297
translation,204,6,results,results,has,b2t2,results has b2t2,0.5376186370849609
translation,204,123,results,our dual encoder model,worked,surprisingly well,our dual encoder model worked surprisingly well,0.7185107469558716
translation,204,123,results,surprisingly well,compared to,zellers et al . ( 2019 ),surprisingly well compared to zellers et al . ( 2019 ),0.6563692688941956
translation,204,123,results,surprisingly well,surpassing,baseline,surprisingly well surpassing baseline,0.8065653443336487
translation,204,123,results,baseline,without making use of,bounding boxes,baseline without making use of bounding boxes,0.5778936147689819
translation,204,132,results,bounding boxes,obtain,67.5 % accuracy,bounding boxes obtain 67.5 % accuracy,0.5263540148735046
translation,204,132,results,results,Without,bounding boxes,results Without bounding boxes,0.6932395696640015
translation,204,133,results,4,instead of,8 appended bounding boxes,4 instead of 8 appended bounding boxes,0.6234687566757202
translation,204,133,results,4,obtain,71 % accuracy,4 obtain 71 % accuracy,0.627448558807373
translation,204,133,results,results,With,4,results With 4,0.5374302268028259
translation,204,134,results,labels,obtain,70.9 % accuracy,labels obtain 70.9 % accuracy,0.5650050640106201
translation,204,134,results,labels,for,detected objects,labels for detected objects,0.5757761001586914
translation,204,134,results,our model,make use of,labels,our model make use of labels,0.6554998159408569
translation,204,134,results,labels,for,detected objects,labels for detected objects,0.5757761001586914
translation,204,134,results,results,With,8 bounding boxes,results With 8 bounding boxes,0.5994765162467957
translation,204,152,results,model,trained without,positional embeddings,model trained without positional embeddings,0.6760146021842957
translation,204,152,results,0.3 % accuracy,compared to,full model,0.3 % accuracy compared to full model,0.653041660785675
translation,204,152,results,results,has,model,results has model,0.5339115858078003
translation,205,91,baselines,laser,uses,multilingual sentence embeddings,laser uses multilingual sentence embeddings,0.5500954389572144
translation,205,91,baselines,distance or margin criterion,in,embeddings space,distance or margin criterion in embeddings space,0.5094549655914307
translation,205,91,baselines,embeddings space,to detect,parallel sentences,embeddings space to detect parallel sentences,0.6295108795166016
translation,205,91,baselines,baselines,has,laser,baselines has laser,0.6140722036361694
translation,205,7,experiments,mlqa,has,multi-way aligned extractive qa evaluation benchmark,mlqa has multi-way aligned extractive qa evaluation benchmark,0.504655122756958
translation,205,8,experiments,mlqa,contains,qa instances,mlqa contains qa instances,0.6662157773971558
translation,205,8,experiments,qa instances,in,7 languages,qa instances in 7 languages,0.530599057674408
translation,205,8,experiments,qa instances,in,spanish,qa instances in spanish,0.5121004581451416
translation,205,8,experiments,qa instances,in,vietnamese,qa instances in vietnamese,0.5680204033851624
translation,205,8,experiments,qa instances,in,simplified chinese,qa instances in simplified chinese,0.5305572748184204
translation,205,39,results,zero-shot xlm,transfers,best,zero-shot xlm transfers best,0.7108936905860901
translation,205,39,results,lag well,behind,training - language performance,lag well behind training - language performance,0.7176732420921326
translation,205,39,results,all models,has,lag well,all models has lag well,0.5992922782897949
translation,205,39,results,results,find that,zero-shot xlm,results find that zero-shot xlm,0.6366245150566101
translation,205,185,results,results,on,xlt task,results on xlt task,0.5308308601379395
translation,205,186,results,xlm,performs,best overall,xlm performs best overall,0.6356618404388428
translation,205,186,results,results,has,xlm,results has xlm,0.5113705992698669
translation,205,191,results,39.8 % drop,in,mean em score ( 20.9 % f1 ),39.8 % drop in mean em score ( 20.9 % f1 ),0.5394589304924011
translation,205,191,results,39.8 % drop,over,english bert - large baseline,39.8 % drop over english bert - large baseline,0.6248174905776978
translation,205,191,results,mean em score ( 20.9 % f1 ),over,english bert - large baseline,mean em score ( 20.9 % f1 ) over english bert - large baseline,0.6131370067596436
translation,205,191,results,results,for,xlm,results for xlm,0.5694307088851929
translation,205,192,results,struggle,on,arabic and hindi,struggle on arabic and hindi,0.6109333038330078
translation,205,201,results,transfer performance,is,better,transfer performance is better,0.6286289691925049
translation,205,201,results,better,when,model,better when model,0.6904044151306152
translation,205,201,results,model,answers well in,english,model answers well in english,0.7178447246551514
translation,205,201,results,model,far from,zero,model far from zero,0.7606238126754761
translation,205,201,results,zero,when,english answer,zero when english answer,0.6464270949363708
translation,205,201,results,english answer,is,wrong,english answer is wrong,0.5787891149520874
translation,205,201,results,results,shows,transfer performance,results shows transfer performance,0.6669353246688843
translation,205,203,results,xlm,on,g-xlt task,xlm on g-xlt task,0.5869864225387573
translation,205,203,results,results,for,xlm,results for xlm,0.5694307088851929
translation,205,205,results,questions,in,given language,questions in given language,0.5173665285110474
translation,205,205,results,model,performs,best,model performs best,0.6411250829696655
translation,205,205,results,best,when,context language,best when context language,0.6957393288612366
translation,205,205,results,context language,matches,question,context language matches question,0.7329091429710388
translation,205,205,results,question,except for,hindi and arabic,question except for hindi and arabic,0.6511231660842896
translation,205,205,results,questions,has,model,questions has model,0.6059702634811401
translation,205,205,results,given language,has,model,given language has model,0.5945574641227722
translation,205,205,results,results,For,questions,results For questions,0.5702887773513794
translation,206,180,ablation-analysis,without semantic representation,in,hsp,without semantic representation in hsp,0.5904081463813782
translation,206,180,ablation-analysis,without semantic representation,has,model 's bleu - 4 score,without semantic representation has model 's bleu - 4 score,0.5430176258087158
translation,206,180,ablation-analysis,hsp,has,model 's bleu - 4 score,hsp has model 's bleu - 4 score,0.5394060611724854
translation,206,180,ablation-analysis,model 's bleu - 4 score,has,decreases,model 's bleu - 4 score has decreases,0.5977481007575989
translation,206,180,ablation-analysis,decreases,has,2.6 points,decreases has 2.6 points,0.619920551776886
translation,206,180,ablation-analysis,rouge -l score,has,decreases,rouge -l score has decreases,0.5960415601730347
translation,206,180,ablation-analysis,decreases,has,1.9 points,decreases has 1.9 points,0.6111891269683838
translation,206,180,ablation-analysis,ablation analysis,show,without semantic representation,ablation analysis show without semantic representation,0.6393353939056396
translation,206,181,ablation-analysis,bleu - 4 score,by removing,hsp,bleu - 4 score by removing hsp,0.7788721919059753
translation,206,181,ablation-analysis,highest,among,four ablation models,highest among four ablation models,0.6188666224479675
translation,206,181,ablation-analysis,copy mechanism,has,4.1 points,copy mechanism has 4.1 points,0.5389372110366821
translation,206,176,baselines,complex question,into,sub-questions,complex question into sub-questions,0.6153039932250977
translation,206,176,baselines,splitting points,predicted by,pointer network model,splitting points predicted by pointer network model,0.7058550119400024
translation,206,176,baselines,hsp ( sr ),refers to,two -stage hsp model,hsp ( sr ) refers to two -stage hsp model,0.6993998289108276
translation,206,176,baselines,two -stage hsp model,use,semantic representation,two -stage hsp model use semantic representation,0.5928938388824463
translation,206,176,baselines,semantic representation,as,intermediate representation,semantic representation as intermediate representation,0.5269102454185486
translation,206,176,baselines,baselines,has,pointernetwork,baselines has pointernetwork,0.5208567380905151
translation,206,127,hyperparameters,pre-trained 6b tokens,for,vocabulary words,pre-trained 6b tokens for vocabulary words,0.6033214330673218
translation,206,127,hyperparameters,vocabulary words,which do not have,pre-trained embeddings,vocabulary words which do not have pre-trained embeddings,0.6524590849876404
translation,206,127,hyperparameters,vocabulary words,assign them,uniform randomized values,vocabulary words assign them uniform randomized values,0.6611641645431519
translation,206,127,hyperparameters,pre-trained 6b tokens,has,300 dimensional glove word embeddings,pre-trained 6b tokens has 300 dimensional glove word embeddings,0.5486847758293152
translation,206,128,hyperparameters,training,update,all word embeddings,training update all word embeddings,0.7290875911712646
translation,206,128,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
translation,206,130,hyperparameters,categorical pos annotations,map them to,pos embedding vectors,categorical pos annotations map them to pos embedding vectors,0.6108274459838867
translation,206,130,hyperparameters,categorical pos annotations,map them to,pos embedding vectors,categorical pos annotations map them to pos embedding vectors,0.6108274459838867
translation,206,130,hyperparameters,pos embedding vectors,of,dimension,pos embedding vectors of dimension,0.5385727286338806
translation,206,130,hyperparameters,pos embedding vectors,initialized from,"uniform distribution u ( ?0.1 , 0.1 )","pos embedding vectors initialized from uniform distribution u ( ?0.1 , 0.1 )",0.6884684562683105
translation,206,130,hyperparameters,pos embedding vectors,initialized from,"uniform distribution u ( ?0.1 , 0.1 )","pos embedding vectors initialized from uniform distribution u ( ?0.1 , 0.1 )",0.6884684562683105
translation,206,130,hyperparameters,dimension,has,30,dimension has 30,0.6628241539001465
translation,206,130,hyperparameters,hyperparameters,use,categorical pos annotations,hyperparameters use categorical pos annotations,0.5672158598899841
translation,206,132,hyperparameters,hidden size,of,all encoder and decoder units,hidden size of all encoder and decoder units,0.5655665993690491
translation,206,132,hyperparameters,hidden size,to,300,hidden size to 300,0.6187703609466553
translation,206,132,hyperparameters,all encoder and decoder units,to,300,all encoder and decoder units to 300,0.6076809763908386
translation,206,132,hyperparameters,hyperparameters,fix,hidden size,hyperparameters fix hidden size,0.7186470031738281
translation,206,134,hyperparameters,model,using,adam optimizer,model using adam optimizer,0.6418686509132385
translation,206,134,hyperparameters,model,use,dynamic learning rate,model use dynamic learning rate,0.6477677822113037
translation,206,134,hyperparameters,adam optimizer,with,"? 1 = 0.9 , ? 2 = 0.98 and = 10 ?9","adam optimizer with ? 1 = 0.9 , ? 2 = 0.98 and = 10 ?9",0.6243613958358765
translation,206,134,hyperparameters,adam optimizer,use,dynamic learning rate,adam optimizer use dynamic learning rate,0.6246058344841003
translation,206,134,hyperparameters,dynamic learning rate,during,training process,dynamic learning rate during training process,0.6481055617332458
translation,206,134,hyperparameters,hyperparameters,train,model,hyperparameters train model,0.7064553499221802
translation,206,135,hyperparameters,regularization,use,dropout,regularization use dropout,0.6589788198471069
translation,206,135,hyperparameters,regularization,set,dropout rate,regularization set dropout rate,0.5686860680580139
translation,206,135,hyperparameters,dropout,set,label smoothing value,dropout set label smoothing value,0.6320869326591492
translation,206,135,hyperparameters,dropout rate,to,0.2,dropout rate to 0.2,0.5248656868934631
translation,206,135,hyperparameters,label smoothing value,to,0.1,label smoothing value to 0.1,0.5451316237449646
translation,206,135,hyperparameters,dropout,has,"srivastava et al. , 2014 )","dropout has srivastava et al. , 2014 )",0.5718802213668823
translation,206,135,hyperparameters,label smoothing,has,"szegedy et al. , 2016 )","label smoothing has szegedy et al. , 2016 )",0.532588005065918
translation,206,135,hyperparameters,hyperparameters,For,regularization,hyperparameters For regularization,0.5561942458152771
translation,206,135,hyperparameters,hyperparameters,set,dropout rate,hyperparameters set dropout rate,0.5970823764801025
translation,206,135,hyperparameters,hyperparameters,set,label smoothing value,hyperparameters set label smoothing value,0.6110230088233948
translation,206,136,hyperparameters,training,train,our models,training train our models,0.5969794988632202
translation,206,136,hyperparameters,our models,using,minibatches,our models using minibatches,0.6661062836647034
translation,206,136,hyperparameters,minibatches,of,128 samples,minibatches of 128 samples,0.6071087718009949
translation,206,136,hyperparameters,"at most 20,000 steps",selecting,best model,"at most 20,000 steps selecting best model",0.6723393797874451
translation,206,136,hyperparameters,best model,based on,development set performance,best model based on development set performance,0.6503406167030334
translation,206,136,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
translation,206,4,model,novel hierarchical semantic parsing ( hsp ),utilizes,decompositionality,novel hierarchical semantic parsing ( hsp ) utilizes decompositionality,0.6267837285995483
translation,206,4,model,decompositionality,for,semantic parsing,decompositionality for semantic parsing,0.6336530447006226
translation,206,4,model,of complex questions,for,semantic parsing,of complex questions for semantic parsing,0.5850843787193298
translation,206,4,model,decompositionality,has,of complex questions,decompositionality has of complex questions,0.6094585657119751
translation,206,4,model,model,propose,novel hierarchical semantic parsing ( hsp ),model propose novel hierarchical semantic parsing ( hsp ),0.6558314561843872
translation,206,5,model,three -stage parsing architecture,based on,idea of decompositionintegration,three -stage parsing architecture based on idea of decompositionintegration,0.6731049418449402
translation,206,5,model,model,designed within,three -stage parsing architecture,model designed within three -stage parsing architecture,0.7098788619041443
translation,206,6,model,first stage,propose,question decomposer,first stage propose question decomposer,0.6949504017829895
translation,206,6,model,question decomposer,decomposes,complex question,question decomposer decomposes complex question,0.7632724642753601
translation,206,6,model,complex question,into,sequence of subquestions,complex question into sequence of subquestions,0.5865477919578552
translation,206,6,model,model,In,first stage,model In first stage,0.5582395792007446
translation,206,7,model,information extractor,to derive,type and predicate information,information extractor to derive type and predicate information,0.6487240791320801
translation,206,7,model,type and predicate information,of,questions,type and predicate information of questions,0.5909241437911987
translation,206,7,model,model,design,information extractor,model design information extractor,0.5906585454940796
translation,206,25,model,hierarchical semantic parsing ( hsp ) model,designed as,hierarchical neural sequence - to-sequence architecture,hierarchical semantic parsing ( hsp ) model designed as hierarchical neural sequence - to-sequence architecture,0.5062726140022278
translation,206,25,model,model,To parse,complex question,model To parse complex question,0.7755658626556396
translation,206,29,model,hsp model,seen as,multi-stage reasoning process,hsp model seen as multi-stage reasoning process,0.6580068469047546
translation,206,29,model,multi-stage reasoning process,with,each stage,multi-stage reasoning process with each stage,0.6240187883377075
translation,206,29,model,each stage,focusing on,different level of information,each stage focusing on different level of information,0.712992787361145
translation,206,29,model,each stage,reducing,search space,each stage reducing search space,0.7132008075714111
translation,206,29,model,search space,of,logical forms,search space of logical forms,0.5713388919830322
translation,206,29,model,search space,by integrating,previously generated information,search space by integrating previously generated information,0.5835033059120178
translation,206,29,model,logical forms,by integrating,previously generated information,logical forms by integrating previously generated information,0.6167898178100586
translation,206,29,model,logical forms,has,step-by-step,logical forms has step-by-step,0.5837883949279785
translation,206,29,model,model,has,hsp model,model has hsp model,0.5680901408195496
translation,206,129,model,pre-trained stanford- corenlp pos model,in,encoder embedding process,pre-trained stanford- corenlp pos model in encoder embedding process,0.48972898721694946
translation,206,129,model,model,uses,pre-trained stanford- corenlp pos model,model uses pre-trained stanford- corenlp pos model,0.5186303853988647
translation,206,131,model,pos embeddings,concatenated with,word embeddings,pos embeddings concatenated with word embeddings,0.6243435740470886
translation,206,131,model,word embeddings,to generate,word representations,word embeddings to generate word representations,0.6306979060173035
translation,206,131,model,model,has,pos embeddings,model has pos embeddings,0.5008206367492676
translation,206,133,model,encoder and decoder,of,hsp models,encoder and decoder of hsp models,0.5863160490989685
translation,206,133,model,hsp models,stacked by,6 identical layers,hsp models stacked by 6 identical layers,0.7839586138725281
translation,206,133,model,model,has,encoder and decoder,model has encoder and decoder,0.5709518790245056
translation,206,179,model,four main modules,in,hsp model,four main modules in hsp model,0.5604122877120972
translation,206,179,model,model,examine,four main modules,model examine four main modules,0.6898190975189209
translation,206,150,results,sp unit,gets,59.91 % accuracy,sp unit gets 59.91 % accuracy,0.6187015175819397
translation,206,150,results,59.91 % accuracy,on,test set,59.91 % accuracy on test set,0.5334455370903015
translation,206,150,results,8.91 %,higher than,pointer - generator,8.91 % higher than pointer - generator,0.6826519966125488
translation,206,150,results,pointer - generator,matches,51 % golden sparql queries,pointer - generator matches 51 % golden sparql queries,0.75894695520401
translation,206,150,results,results,has,sp unit,results has sp unit,0.5626134276390076
translation,206,151,results,performances,of,seq2seq and seq2tree,performances of seq2seq and seq2tree,0.647076427936554
translation,206,151,results,lower,than,pointer - generator,lower than pointer - generator,0.6096698641777039
translation,206,151,results,two models,get,47.3 % and 49.68 % accuracy,two models get 47.3 % and 49.68 % accuracy,0.5557934045791626
translation,206,151,results,47.3 % and 49.68 % accuracy,on,test set,47.3 % and 49.68 % accuracy on test set,0.5471072196960449
translation,206,151,results,results,observe that,performances,results observe that performances,0.6019083857536316
translation,206,153,results,transformer,achieves,53.41 %,transformer achieves 53.41 %,0.6774852275848389
translation,206,153,results,53.41 %,on,test set,53.41 % on test set,0.5213155150413513
translation,206,153,results,53.41 %,higher than,pointer-generator,53.41 % higher than pointer-generator,0.6901393532752991
translation,206,153,results,6.5 %,lower than,sp unit,6.5 % lower than sp unit,0.747088611125946
translation,206,153,results,6.5 %,higher than,pointer-generator,6.5 % higher than pointer-generator,0.694258451461792
translation,206,153,results,results,has,transformer,results has transformer,0.4226538836956024
translation,206,156,results,coarse2 fine,obtains,53.52 % accuracy,coarse2 fine obtains 53.52 % accuracy,0.5783860087394714
translation,206,156,results,53.52 % accuracy,on,test set,53.52 % accuracy on test set,0.5226216316223145
translation,206,156,results,1.84 % lower,than,sp unit,1.84 % lower than sp unit,0.5697115659713745
translation,206,156,results,results,has,coarse2 fine,results has coarse2 fine,0.582327127456665
translation,206,157,results,hsp model,outperforms,sp unit,hsp model outperforms sp unit,0.7237635254859924
translation,206,157,results,sp unit,by,6.27 % accuracy,sp unit by 6.27 % accuracy,0.5894016027450562
translation,206,157,results,sp unit,is,wide margin,sp unit is wide margin,0.5652498006820679
translation,206,157,results,results,has,hsp model,results has hsp model,0.5424782633781433
translation,206,159,results,hsp,achieves,significant improvement,hsp achieves significant improvement,0.696087121963501
translation,206,159,results,hsp,incorporate,sub-questions and key information,hsp incorporate sub-questions and key information,0.6677162051200867
translation,206,159,results,neural semantic parsing models,has,hsp,neural semantic parsing models has hsp,0.4984101355075836
translation,206,159,results,logical form generation,has,effectively,logical form generation has effectively,0.563478410243988
translation,206,159,results,results,Compared to,neural semantic parsing models,results Compared to neural semantic parsing models,0.5766001343727112
translation,206,159,results,results,other,neural semantic parsing models,results other neural semantic parsing models,0.5165992379188538
translation,206,163,results,highest accuracy,on,four type of question samples,highest accuracy on four type of question samples,0.5054014921188354
translation,206,163,results,highest accuracy,among,three models,highest accuracy among three models,0.6322389841079712
translation,206,163,results,hsp,has,highest accuracy,hsp has highest accuracy,0.5821717977523804
translation,206,164,results,accuracy,of,transformer,accuracy of transformer,0.6688230037689209
translation,206,164,results,transformer,on,composition and conjunction questions,transformer on composition and conjunction questions,0.5618026852607727
translation,206,164,results,composition and conjunction questions,comparable to,coarse2 fine,composition and conjunction questions comparable to coarse2 fine,0.6741893887519836
translation,206,164,results,results,has,accuracy,results has accuracy,0.5888755321502686
translation,206,165,results,accuracy,of,coarse2fine and hsp,accuracy of coarse2fine and hsp,0.611040472984314
translation,206,165,results,accuracy,has been,significantly improved,accuracy has been significantly improved,0.6697753071784973
translation,206,165,results,coarse2fine and hsp,in,comparative and superlative questions,coarse2fine and hsp in comparative and superlative questions,0.5792503952980042
translation,206,165,results,coarse2fine and hsp,has been,significantly improved,coarse2fine and hsp has been significantly improved,0.6709949374198914
translation,206,165,results,transformer,has,accuracy,transformer has accuracy,0.5553866624832153
translation,206,165,results,results,compared to,transformer,results compared to transformer,0.5427732467651367
translation,206,168,results,performance,of,hsp,performance of hsp,0.6775081753730774
translation,206,168,results,hsp,exceeds,other two baselines,hsp exceeds other two baselines,0.6768106818199158
translation,206,169,results,training data volume,higher than,other two models,training data volume higher than other two models,0.7032022476196289
translation,206,169,results,increases,higher than,other two models,increases higher than other two models,0.7387396693229675
translation,206,169,results,performance improvement,higher than,other two models,performance improvement higher than other two models,0.6822847127914429
translation,206,169,results,hsp,higher than,other two models,hsp higher than other two models,0.7152887582778931
translation,206,169,results,training data volume,has,increases,training data volume has increases,0.5716944932937622
translation,206,169,results,increases,has,performance improvement,increases has performance improvement,0.5779315233230591
translation,206,169,results,results,as,training data volume,results as training data volume,0.5383630394935608
translation,206,177,results,other two models,obtain,much better results,other two models obtain much better results,0.5519194006919861
translation,206,177,results,pointernetwork,has,other two models,pointernetwork has other two models,0.5897327661514282
translation,206,177,results,results,compared to,pointernetwork,results compared to pointernetwork,0.6743294596672058
translation,207,213,baselines,first baseline,is,possible worlds model,first baseline is possible worlds model,0.5499556064605713
translation,207,213,baselines,worlds,is,possible worlds model,worlds is possible worlds model,0.49342578649520874
translation,207,213,baselines,possible worlds model,based on,malinowski and fritz ( 2014 ),possible worlds model based on malinowski and fritz ( 2014 ),0.6572225689888
translation,207,213,baselines,first baseline,has,worlds,first baseline has worlds,0.6076558828353882
translation,207,213,baselines,baselines,has,first baseline,baselines has first baseline,0.595583438873291
translation,207,219,baselines,first baseline,is,textonly answer selection model,first baseline is textonly answer selection model,0.47849681973457336
translation,207,219,baselines,lstm,is,textonly answer selection model,lstm is textonly answer selection model,0.5060327053070068
translation,207,219,baselines,first baseline,has,lstm,first baseline has lstm,0.5371661186218262
translation,207,219,baselines,baselines,has,first baseline,baselines has first baseline,0.595583438873291
translation,207,219,baselines,baselines,has,lstm,baselines has lstm,0.5395978093147278
translation,207,220,baselines,vqa,is,neural network,vqa is neural network,0.6024385094642639
translation,207,220,baselines,neural network,for,visual question answering,neural network for visual question answering,0.5403220653533936
translation,207,224,baselines,third baseline,is,neural network,third baseline is neural network,0.5930300951004028
translation,207,224,baselines,dqa,is,neural network,dqa is neural network,0.5906368494033813
translation,207,224,baselines,neural network,rectifies,limitation,neural network rectifies limitation,0.7480832934379578
translation,207,224,baselines,third baseline,has,dqa,third baseline has dqa,0.5572242140769958
translation,207,224,baselines,baselines,has,third baseline,baselines has third baseline,0.5942568778991699
translation,207,110,experiments,beam size,of,100,beam size of 100,0.6846780180931091
translation,207,110,experiments,beam size,of,100 executions,beam size of 100 executions,0.6420224905014038
translation,207,110,experiments,100,in,semantic parser,100 in semantic parser,0.5192708969116211
translation,207,110,experiments,10 highest - scoring logical forms,with,beam,10 highest - scoring logical forms with beam,0.6307790875434875
translation,207,110,experiments,beam,of,100 executions,beam of 100 executions,0.6745061874389648
translation,207,20,model,domain theory,for,task,domain theory for task,0.6085030436515808
translation,207,20,model,task,as,probabilistic program,task as probabilistic program,0.5097267031669617
translation,207,20,model,probabilistic program,train,joint loglinear model,probabilistic program train joint loglinear model,0.575690507888794
translation,207,20,model,joint loglinear model,to semantically parse,questions,joint loglinear model to semantically parse questions,0.7287800908088684
translation,207,20,model,questions,to,logical forms,questions to logical forms,0.5285467505455017
translation,207,20,model,model,define,domain theory,model define domain theory,0.6663549542427063
translation,207,22,model,semantic parsing,to represent,compositionality,semantic parsing to represent compositionality,0.5441166758537292
translation,207,22,model,compositionality,in,language,compositionality in language,0.4842125177383423
translation,207,22,model,probabilistic programming,to specify,background knowledge,probabilistic programming to specify background knowledge,0.5735510587692261
translation,207,22,model,linear-time approximate inference,over,environment,linear-time approximate inference over environment,0.6248854398727417
translation,207,22,model,model,leverages,semantic parsing,model leverages semantic parsing,0.6773948073387146
translation,207,22,model,model,leverages,probabilistic programming,model leverages probabilistic programming,0.6517817974090576
translation,207,221,model,each image,as,vector,each image as vector,0.5783764719963074
translation,207,221,model,vector,using,final layer,vector using final layer,0.6984683871269226
translation,207,221,model,final layer,of,pre-trained vgg19 model,final layer of pre-trained vgg19 model,0.5164704918861389
translation,207,221,model,model,represents,each image,model represents each image,0.6732584238052368
translation,207,267,model,global features,of,logical form and executions,global features of logical form and executions,0.5808494091033936
translation,207,267,model,global features,help,model,global features help model,0.6667351722717285
translation,207,267,model,model,avoid,implausible interpretations,model avoid implausible interpretations,0.6807015538215637
translation,207,267,model,model,includes,global features,model includes global features,0.6716810464859009
translation,207,267,model,model,avoid,implausible interpretations,model avoid implausible interpretations,0.6807015538215637
translation,207,229,results,lstm,performs,well,lstm performs well,0.6439673900604248
translation,207,229,results,results,has,lstm,results has lstm,0.5593706965446472
translation,207,231,results,other neural network models,have,similar performance,other neural network models have similar performance,0.5279754996299744
translation,207,231,results,similar performance,to,lstm,similar performance to lstm,0.559248685836792
translation,207,231,results,results,has,other neural network models,results has other neural network models,0.5458987355232239
translation,207,232,results,p 3,has,outperforms,p 3 has outperforms,0.7141820788383484
translation,207,232,results,outperforms,has,worlds,outperforms has worlds,0.658251166343689
translation,207,232,results,results,find that,p 3,results find that p 3,0.6586187481880188
translation,207,236,results,vqa and dqa,perform,similarly,vqa and dqa perform similarly,0.6636855006217957
translation,207,236,results,similarly,to,lstm,similarly to lstm,0.58184814453125
translation,207,236,results,results,has,vqa and dqa,results has vqa and dqa,0.5352879762649536
translation,207,237,results,accuracies,of,worlds and p 3,accuracies of worlds and p 3,0.6122292876243591
translation,207,237,results,results,find that,accuracies,results find that accuracies,0.6452131271362305
translation,207,241,results,lstm answer selection,improves,accuracy,lstm answer selection improves accuracy,0.6597727537155151
translation,207,241,results,accuracy,by,9 points,accuracy by 9 points,0.6120925545692444
translation,207,241,results,results,find that,lstm answer selection,results find that lstm answer selection,0.6158047318458557
translation,207,242,results,global features,improve,accuracy,global features improve accuracy,0.6946329474449158
translation,207,242,results,accuracy,by,7 points,accuracy by 7 points,0.6120402216911316
translation,207,242,results,results,has,global features,results has global features,0.5657232999801636
translation,207,260,results,p 3,trained with,labeled environments,p 3 trained with labeled environments,0.7131479978561401
translation,207,260,results,slightly outperforms,in,qa condition,slightly outperforms in qa condition,0.5881134867668152
translation,207,260,results,p 3,trained with,labeled environments,p 3 trained with labeled environments,0.7131479978561401
translation,207,260,results,outperforms,trained with,additional logical form labels,outperforms trained with additional logical form labels,0.7477557063102722
translation,207,260,results,prior work,trained with,additional logical form labels,prior work trained with additional logical form labels,0.7115344405174255
translation,207,260,results,p 3,has,slightly outperforms,p 3 has slightly outperforms,0.657215416431427
translation,207,260,results,labeled environments,has,outperforms,labeled environments has outperforms,0.6133755445480347
translation,207,260,results,outperforms,has,prior work,outperforms has prior work,0.5891962051391602
translation,207,260,results,results,has,p 3,results has p 3,0.5780649781227112
translation,208,4,model,novel methodology,to efficiently construct,corpus,novel methodology to efficiently construct corpus,0.7120624780654907
translation,208,4,model,corpus,for,question answering,corpus for question answering,0.6135650873184204
translation,208,4,model,question answering,over,structured data,question answering over structured data,0.6206653714179993
translation,208,4,model,model,introduce,novel methodology,model introduce novel methodology,0.7090634107589722
translation,208,5,model,intermediate representation,based on,logical query plan,intermediate representation based on logical query plan,0.6544139385223389
translation,208,5,model,logical query plan,in,database,logical query plan in database,0.5043794512748718
translation,208,5,model,logical query plan,called,operation trees ( ot ),logical query plan called operation trees ( ot ),0.6915435194969177
translation,208,5,model,model,introduce,intermediate representation,model introduce intermediate representation,0.6792892813682556
translation,208,26,model,new procedure,to increase,speed of the annotation process,new procedure to increase speed of the annotation process,0.6989646553993225
translation,208,26,model,model,propose,new procedure,model propose new procedure,0.7078090906143188
translation,208,27,model,intermediate representation,of,structured queries,intermediate representation of structured queries,0.5607949495315552
translation,208,27,model,intermediate representation,call,operation trees ( ots,intermediate representation call operation trees ( ots,0.5851162075996399
translation,208,27,model,model,introduce,intermediate representation,model introduce intermediate representation,0.6792892813682556
translation,208,288,results,all the databases,except,formula -1,all the databases except formula -1,0.7406209707260132
translation,208,288,results,model,achieves,precision,model achieves precision,0.6397417783737183
translation,208,288,results,precision,between,45.1 % and 47.5 %,precision between 45.1 % and 47.5 %,0.5827591419219971
translation,208,288,results,all the databases,has,model,all the databases has model,0.6187219023704529
translation,208,288,results,formula -1,has,model,formula -1 has model,0.5969802737236023
translation,208,288,results,results,For,all the databases,results For all the databases,0.5834438800811768
translation,208,289,results,model,achieves,score,model achieves score,0.6849765181541443
translation,208,289,results,score,of,26.3 %,score of 26.3 %,0.5377326011657715
translation,208,289,results,formula - 1,has,model,formula - 1 has model,0.5969802737236023
translation,208,289,results,results,For,formula - 1,results For formula - 1,0.6518382430076599
translation,208,301,results,token assignment,increases,scores,token assignment increases scores,0.6675001382827759
translation,208,301,results,scores,by,2 %,scores by 2 %,0.6390970349311829
translation,208,301,results,scores,around,2 %,scores around 2 %,0.7403179407119751
translation,208,301,results,results,show,token assignment,results show token assignment,0.6340038776397705
translation,208,301,results,results,using,token assignment,results using token assignment,0.6689961552619934
translation,208,302,results,even as high as 7 %,showing,model,even as high as 7 % showing model,0.7473962903022766
translation,208,302,results,model,benefit from,additional information,model benefit from additional information,0.6698269844055176
translation,208,302,results,additional information,provided in,token assignments,additional information provided in token assignments,0.6642202138900757
translation,208,302,results,20 % training data,has,gain,20 % training data has gain,0.5771732330322266
translation,208,302,results,results,In the case of,20 % training data,results In the case of 20 % training data,0.6731253862380981
translation,209,88,baselines,"seq2seq ( bahdanau et al. , 2014 )",uses,encoder-decoder structure,"seq2seq ( bahdanau et al. , 2014 ) uses encoder-decoder structure",0.564370334148407
translation,209,88,baselines,encoder-decoder structure,with,attention,encoder-decoder structure with attention,0.6436236500740051
translation,209,88,baselines,attention,for,generation,attention for generation,0.5902257561683655
translation,209,88,baselines,baselines,has,"seq2seq ( bahdanau et al. , 2014 )","baselines has seq2seq ( bahdanau et al. , 2014 )",0.4728436768054962
translation,209,91,baselines,ours ( joint ),is,pipeline model,ours ( joint ) is pipeline model,0.5554537773132324
translation,209,91,baselines,pipeline model,consisting of,three modules,pipeline model consisting of three modules,0.7269543409347534
translation,209,91,baselines,baselines,has,ours ( joint ),baselines has ours ( joint ),0.573284924030304
translation,209,104,baselines,baselines,has,a2a,baselines has a2a,0.5462781190872192
translation,209,106,baselines,f+ a2a,pre-trained on,financial dataset,f+ a2a pre-trained on financial dataset,0.7862343192100525
translation,209,106,baselines,f+ a2a,fine-tuned on,automotive dataset,f+ a2a fine-tuned on automotive dataset,0.694462776184082
translation,209,106,baselines,baselines,has,f+ a2a,baselines has f+ a2a,0.5870104432106018
translation,209,81,hyperparameters,adam,as,optimization method,adam as optimization method,0.5697389841079712
translation,209,81,hyperparameters,adam,set,learning rate,adam set learning rate,0.6595736742019653
translation,209,81,hyperparameters,learning rate,as,0.0001,learning rate as 0.0001,0.5501636862754822
translation,209,81,hyperparameters,hyperparameters,use,adam,hyperparameters use adam,0.6479569673538208
translation,209,81,hyperparameters,hyperparameters,set,learning rate,hyperparameters set learning rate,0.5994082689285278
translation,209,82,hyperparameters,dimension,of,hidden state,dimension of hidden state,0.5578262209892273
translation,209,82,hyperparameters,hidden state,as,128,hidden state as 128,0.5703564882278442
translation,209,82,hyperparameters,hyperparameters,set,dimension,hyperparameters set dimension,0.6708630919456482
translation,209,83,hyperparameters,padding,set,max length,padding set max length,0.6998948454856873
translation,209,83,hyperparameters,max length,as,64,max length as 64,0.5565503835678101
translation,209,83,hyperparameters,hyperparameters,For,padding,hyperparameters For padding,0.6164054274559021
translation,209,84,hyperparameters,bert - chinese tokenizer,to separate,characters,bert - chinese tokenizer to separate characters,0.6619387865066528
translation,209,84,hyperparameters,hyperparameters,use,bert - chinese tokenizer,hyperparameters use bert - chinese tokenizer,0.5891861319541931
translation,209,6,model,pipeline model,based on,templates,pipeline model based on templates,0.6565680503845215
translation,209,6,model,model,propose,pipeline model,model propose pipeline model,0.6896609663963318
translation,209,7,model,three steps,identifies,template,three steps identifies template,0.693259060382843
translation,209,7,model,three steps,retrieves,candidate templates,three steps retrieves candidate templates,0.750953733921051
translation,209,7,model,three steps,fills,candidate templates,three steps fills candidate templates,0.7448980212211609
translation,209,7,model,template,from,input question,template from input question,0.5530622601509094
translation,209,7,model,candidate templates,with,original topic words,candidate templates with original topic words,0.5664092302322388
translation,209,7,model,candidate templates,with,original topic words,candidate templates with original topic words,0.5664092302322388
translation,209,7,model,model,follows,three steps,model follows three steps,0.7270393371582031
translation,209,27,model,template and topic words,in,original question,template and topic words in original question,0.5262080430984497
translation,209,27,model,template and topic words,construct,paraphrase questions,template and topic words construct paraphrase questions,0.7562999725341797
translation,209,27,model,model,try to identify,template and topic words,model try to identify template and topic words,0.701445460319519
translation,209,28,model,template - based framework,to generate,question paraphrase,template - based framework to generate question paraphrase,0.6478459239006042
translation,209,28,model,question paraphrase,in,pipeline,question paraphrase in pipeline,0.5313023924827576
translation,209,28,model,model,propose,template - based framework,model propose template - based framework,0.689473569393158
translation,209,31,model,pipeline model,to identify,template and topic words,pipeline model to identify template and topic words,0.6631326079368591
translation,209,31,model,pipeline model,generate,question paraphrases,pipeline model generate question paraphrases,0.6777781248092651
translation,209,31,model,template and topic words,from,question,template and topic words from question,0.587114155292511
translation,209,31,model,template and topic words,generate,question paraphrases,template and topic words generate question paraphrases,0.6691064238548279
translation,209,31,model,question paraphrases,via,template transforming and filling,question paraphrases via template transforming and filling,0.662891149520874
translation,209,31,model,model,propose,pipeline model,model propose pipeline model,0.6896609663963318
translation,209,74,model,separate training,train,three modules,separate training train three modules,0.7094036340713501
translation,209,74,model,model,For,separate training,model For separate training,0.6664307713508606
translation,209,89,model,pipeline model,consisting of,three modules,pipeline model consisting of three modules,0.7269543409347534
translation,209,90,model,each module,trained,separately,each module trained separately,0.7464222311973572
translation,209,90,model,model,has,each module,model has each module,0.5586554408073425
translation,209,94,results,both of our pipeline models,based on,template,both of our pipeline models based on template,0.7091943025588989
translation,209,94,results,sequence - to-sequence model,in,large margin,sequence - to-sequence model in large margin,0.5107037425041199
translation,209,94,results,large margin,on,all the four datasets,large margin on all the four datasets,0.4934910535812378
translation,209,94,results,all the four datasets,in terms of,all the four metrics,all the four datasets in terms of all the four metrics,0.6132510900497437
translation,209,94,results,template,has,outperform,template has outperform,0.6469250321388245
translation,209,94,results,outperform,has,sequence - to-sequence model,outperform has sequence - to-sequence model,0.5972453951835632
translation,209,94,results,results,has,both of our pipeline models,results has both of our pipeline models,0.5368710160255432
translation,209,95,results,performance,of,ours ( joint ),performance of ours ( joint ),0.5760400295257568
translation,209,95,results,performance,of,ours ( separate ),performance of ours ( separate ),0.5699066519737244
translation,209,95,results,performance,better than,ours ( separate ),performance better than ours ( separate ),0.7795803546905518
translation,209,95,results,ours ( joint ),better than,ours ( separate ),ours ( joint ) better than ours ( separate ),0.7794475555419922
translation,209,95,results,results,has,performance,results has performance,0.5972660779953003
translation,209,108,results,performance,of,ours ( f2a ),performance of ours ( f2a ),0.5672814249992371
translation,209,108,results,performance,of,seq2seq,performance of seq2seq,0.603915274143219
translation,209,108,results,performance,directly applies,model,performance directly applies model,0.6602882742881775
translation,209,108,results,ours ( f2a ),directly applies,model,ours ( f2a ) directly applies model,0.7447142601013184
translation,209,108,results,model,trained on,financial domain,model trained on financial domain,0.683529794216156
translation,209,108,results,financial domain,to,automotive domain,financial domain to automotive domain,0.5638221502304077
translation,209,108,results,results,has,performance,results has performance,0.5972660779953003
translation,209,110,results,ours ( f2a ),worse than,our ( a2 a ),ours ( f2a ) worse than our ( a2 a ),0.7560732364654541
translation,209,110,results,results,has,ours ( f2a ),results has ours ( f2a ),0.5766977071762085
translation,209,111,results,performance,of,ours ( f+ a2a ),performance of ours ( f+ a2a ),0.5939816236495972
translation,209,111,results,performance,of,ours ( a2a ),performance of ours ( a2a ),0.5855531096458435
translation,209,111,results,performance,better than,ours ( a2a ),performance better than ours ( a2a ),0.757463276386261
translation,209,111,results,ours ( f+ a2a ),better than,ours ( a2a ),ours ( f+ a2a ) better than ours ( a2a ),0.7407398819923401
translation,209,111,results,results,has,performance,results has performance,0.5972660779953003
translation,209,113,results,results,on,seq2seq ( f2a ),results on seq2seq ( f2a ),0.5494276285171509
translation,209,113,results,results,on,seq2seq ( a2a ),results on seq2seq ( a2a ),0.5479155778884888
translation,209,113,results,results,on,seq2seq ( f+ a2a ),results on seq2seq ( f+ a2a ),0.548984169960022
translation,210,156,baselines,baselines,has,q-l mapping heuristic models,baselines has q-l mapping heuristic models,0.5361417531967163
translation,210,9,experimental-setup,i2 b2 data,accessible by,everyone,i2 b2 data accessible by everyone,0.7378263473510742
translation,210,9,experimental-setup,everyone,subject to,license agreement,everyone subject to license agreement,0.6894141435623169
translation,210,9,experimental-setup,experimental setup,has,i2 b2 data,experimental setup has i2 b2 data,0.5266621708869934
translation,210,164,experimental-setup,sequence-tosequence ( seq2seq ),with,attention paradigm,sequence-tosequence ( seq2seq ) with attention paradigm,0.6196053624153137
translation,210,164,experimental-setup,sequence-tosequence ( seq2seq ),as,neural baseline,sequence-tosequence ( seq2seq ) as neural baseline,0.5347995758056641
translation,210,164,experimental-setup,neural baseline,has,2 layers,neural baseline has 2 layers,0.5593753457069397
translation,210,164,experimental-setup,experimental setup,train,sequence-tosequence ( seq2seq ),experimental setup train sequence-tosequence ( seq2seq ),0.6384778618812561
translation,210,189,experiments,extractive qa,on,emrs,extractive qa on emrs,0.6033002138137817
translation,210,189,experiments,extractive qa,use,drqa 's document reader,extractive qa use drqa 's document reader,0.6549062728881836
translation,210,189,experiments,emrs,use,drqa 's document reader,emrs use drqa 's document reader,0.6535457968711853
translation,210,189,experiments,drqa 's document reader,which is,multi-layer rnn based mc model,drqa 's document reader which is multi-layer rnn based mc model,0.6186612248420715
translation,210,207,experiments,lfs,need for,medical kb,lfs need for medical kb,0.6497301459312439
translation,210,207,experiments,lfs,performed,poorly,lfs performed poorly,0.32075124979019165
translation,210,29,model,emr qa corpus,by creating,large-scale dataset,emr qa corpus by creating large-scale dataset,0.6329130530357361
translation,210,29,model,large-scale dataset,using,novel gener - ation framework,large-scale dataset using novel gener - ation framework,0.5702865123748779
translation,210,29,model,novel gener - ation framework,allows for,minimal expert involvement,novel gener - ation framework allows for minimal expert involvement,0.7369948625564575
translation,210,29,model,novel gener - ation framework,allows for,re-purposes,novel gener - ation framework allows for re-purposes,0.7568506002426147
translation,210,29,model,existing annotations,available for,other clinical nlp tasks,existing annotations available for other clinical nlp tasks,0.6229279041290283
translation,210,29,model,re-purposes,has,existing annotations,re-purposes has existing annotations,0.5700258612632751
translation,210,29,model,model,of any publicly available,emr qa corpus,model of any publicly available emr qa corpus,0.6544395685195923
translation,210,240,model,novel framework,generate,large-scale qa dataset,novel framework generate large-scale qa dataset,0.5648611187934875
translation,210,240,model,large-scale qa dataset,using,existing resources,large-scale qa dataset using existing resources,0.5960394144058228
translation,210,240,model,large-scale qa dataset,using,minimal expert input,large-scale qa dataset using minimal expert input,0.6313170194625854
translation,210,240,model,model,propose,novel framework,model propose novel framework,0.720393180847168
translation,210,204,results,results,has,relatively low performance,results has relatively low performance,0.5702307820320129
translation,211,96,baselines,"dbrnn ( sha et al. , 2018 )",is,lstm - based framework,"dbrnn ( sha et al. , 2018 ) is lstm - based framework",0.5164322257041931
translation,211,96,baselines,lstm - based framework,leverages,dependency graph information,lstm - based framework leverages dependency graph information,0.6874959468841553
translation,211,96,baselines,dependency graph information,to extract,event triggers and argument roles,dependency graph information to extract event triggers and argument roles,0.6512609720230103
translation,211,97,baselines,"joint3ee ( nguyen and nguyen , 2019 )",is,multi-task model,"joint3ee ( nguyen and nguyen , 2019 ) is multi-task model",0.5574336647987366
translation,211,97,baselines,multi-task model,performs,entity recognition,multi-task model performs entity recognition,0.5345314741134644
translation,211,97,baselines,multi-task model,performs,trigger detection,multi-task model performs trigger detection,0.5780354142189026
translation,211,97,baselines,multi-task model,performs,argument role assignment,multi-task model performs argument role assignment,0.5747827887535095
translation,211,97,baselines,argument role assignment,by,shared bigru hidden representations,argument role assignment by shared bigru hidden representations,0.5636433362960815
translation,211,97,baselines,baselines,has,"joint3ee ( nguyen and nguyen , 2019 )","baselines has joint3ee ( nguyen and nguyen , 2019 )",0.5634835958480835
translation,211,31,experiments,rule-based question generation strategies,for,templates creation,rule-based question generation strategies for templates creation,0.5971661806106567
translation,211,32,model,our framework,extends to,zero-shot setting,our framework extends to zero-shot setting,0.6837475299835205
translation,211,32,model,zero-shot setting,able to extract,event arguments,zero-shot setting able to extract event arguments,0.7045809030532837
translation,211,32,model,event arguments,for,unseen roles,event arguments for unseen roles,0.6292468309402466
translation,211,32,model,model,show,our framework,model show our framework,0.6677002906799316
translation,211,102,results,results,implement,bert fine-tuning baseline,results implement bert fine-tuning baseline,0.6363435387611389
translation,211,103,results,bert_qa_trigger model,with,best trigger questioning strategy,bert_qa_trigger model with best trigger questioning strategy,0.6731027364730835
translation,211,103,results,best trigger questioning strategy,reaches,comparable ( better ) performance,best trigger questioning strategy reaches comparable ( better ) performance,0.7259546518325806
translation,211,103,results,comparable ( better ) performance,with,baseline models,comparable ( better ) performance with baseline models,0.6170911192893982
translation,211,103,results,results,observe,bert_qa_trigger model,results observe bert_qa_trigger model,0.6409943699836731
translation,211,111,results,outperforms,in both,precision and recall,outperforms in both precision and recall,0.6343265771865845
translation,211,111,results,non-ensemble system,in both,precision and recall,non-ensemble system in both precision and recall,0.6083630323410034
translation,211,111,results,ensemble system,has,outperforms,ensemble system has outperforms,0.6198065280914307
translation,211,111,results,outperforms,has,non-ensemble system,outperforms has non-ensemble system,0.5995426177978516
translation,211,111,results,results,has,ensemble system,results has ensemble system,0.5983035564422607
translation,211,122,results,verb,as,question,verb as question,0.5523630976676941
translation,211,122,results,bert_qa_trigger model,achieves,best performance,bert_qa_trigger model achieves best performance,0.701759934425354
translation,211,122,results,best performance,measured by,f1 score,best performance measured by f1 score,0.6904131770133972
translation,211,122,results,verb,has,bert_qa_trigger model,verb has bert_qa_trigger model,0.646388590335846
translation,211,122,results,question,has,bert_qa_trigger model,question has bert_qa_trigger model,0.6612633466720581
translation,211,122,results,results,using,verb,results using verb,0.4805448353290558
translation,211,131,results,template 2,adding,wh_word,template 2 adding wh_word,0.7250345945358276
translation,211,131,results,template 2,achieves,better performance,template 2 achieves better performance,0.7004213333129883
translation,211,131,results,wh_word,to form,questions,wh_word to form questions,0.6617178916931152
translation,211,131,results,better performance,than,template 1,better performance than template 1,0.5819394588470459
translation,211,131,results,results,observe,template 2,results observe template 2,0.5490702986717224
translation,212,22,ablation-analysis,wikisql,show that,state- of- theart logical form accuracy,wikisql show that state- of- theart logical form accuracy,0.49548619985580444
translation,212,22,ablation-analysis,state- of- theart logical form accuracy,drops from,60.7 % to 53.7 %,state- of- theart logical form accuracy drops from 60.7 % to 53.7 %,0.7270909547805786
translation,212,22,ablation-analysis,61.0 %,combine,pseudo-labeled data,61.0 % combine pseudo-labeled data,0.6425431966781616
translation,212,22,ablation-analysis,pseudo-labeled data,generated from,question generation model,pseudo-labeled data generated from question generation model,0.6254820823669434
translation,212,22,ablation-analysis,ablation analysis,on,wikisql,ablation analysis on wikisql,0.5510870218276978
translation,212,140,ablation-analysis,qg over stamp,comes from,prediction of the where clause,qg over stamp comes from prediction of the where clause,0.6792417764663696
translation,212,140,ablation-analysis,ablation analysis,has,qg over stamp,ablation analysis has qg over stamp,0.5806958675384521
translation,212,142,ablation-analysis,combining qg,is,helpful,combining qg is helpful,0.6135376691818237
translation,212,142,ablation-analysis,helpful,when,number of where conditions,helpful when number of where conditions,0.6714988946914673
translation,212,142,ablation-analysis,number of where conditions,is,more than one,number of where conditions is more than one,0.5874616503715515
translation,212,142,ablation-analysis,ablation analysis,see that,combining qg,ablation analysis see that combining qg,0.6612091660499573
translation,212,153,ablation-analysis,latent variable,empowers,model,latent variable empowers model,0.6695067882537842
translation,212,153,ablation-analysis,model,to generate,diverse questions,model to generate diverse questions,0.7477379441261292
translation,212,153,ablation-analysis,diverse questions,for,same intent,diverse questions for same intent,0.635746419429779
translation,212,153,ablation-analysis,ablation analysis,incorporating,latent variable,ablation analysis incorporating latent variable,0.7321861386299133
translation,212,130,baselines,pntnet,is,augmented pointer network,pntnet is augmented pointer network,0.554132878780365
translation,212,130,baselines,pntnet,is,seq2sql,pntnet is seq2sql,0.6378886103630066
translation,212,130,baselines,augmented pointer network,in which,words,augmented pointer network in which words,0.6525636911392212
translation,212,130,baselines,words,of,target sequence,words of target sequence,0.6170212626457214
translation,212,130,baselines,target sequence,come from,source sequence,target sequence come from source sequence,0.6497704386711121
translation,212,130,baselines,seq2sql,extends,aug,seq2sql extends aug,0.7693744897842407
translation,212,130,baselines,baselines,has,pntnet,baselines has pntnet,0.601300060749054
translation,212,15,experiments,state - of - the - art,based on,neural networks,state - of - the - art based on neural networks,0.6472914218902588
translation,212,15,experiments,state - of - the - art,vary,number of supervised training instances,state - of - the - art vary number of supervised training instances,0.6331123113632202
translation,212,15,experiments,end-to - end semantic parser,based on,neural networks,end-to - end semantic parser based on neural networks,0.6294955611228943
translation,212,15,experiments,state - of - the - art,has,end-to - end semantic parser,state - of - the - art has end-to - end semantic parser,0.5344142913818359
translation,212,110,hyperparameters,dimension,of,encoder hidden state,dimension of encoder hidden state,0.5710912346839905
translation,212,110,hyperparameters,dimension,of,latent variable z,dimension of latent variable z,0.5998904705047607
translation,212,110,hyperparameters,encoder hidden state,as,300,encoder hidden state as 300,0.5784210562705994
translation,212,110,hyperparameters,latent variable z,as,64,latent variable z as 64,0.573222815990448
translation,212,110,hyperparameters,hyperparameters,set,dimension,hyperparameters set dimension,0.6708630919456482
translation,212,111,hyperparameters,dropout,with,rate,dropout with rate,0.5557408928871155
translation,212,111,hyperparameters,dropout,applied to,inputs,dropout applied to inputs,0.6898502111434937
translation,212,111,hyperparameters,rate,of,0.5,rate of 0.5,0.6499558687210083
translation,212,111,hyperparameters,inputs,of,rnn,inputs of rnn,0.6246646046638489
translation,212,112,hyperparameters,model parameters,initialized with,uniform distribution,model parameters initialized with uniform distribution,0.7503786683082581
translation,212,112,hyperparameters,model parameters,updated with,stochastic gradient decent,model parameters updated with stochastic gradient decent,0.6588547825813293
translation,212,112,hyperparameters,hyperparameters,has,model parameters,hyperparameters has model parameters,0.45928311347961426
translation,212,113,hyperparameters,word embedding values,initialized with,glove vectors,word embedding values initialized with glove vectors,0.7327097654342651
translation,212,113,hyperparameters,hyperparameters,has,word embedding values,hyperparameters has word embedding values,0.4944782555103302
translation,212,114,hyperparameters,learning rate,as,0.1,learning rate as 0.1,0.5435166954994202
translation,212,114,hyperparameters,batch size,as,32,batch size as 32,0.5678326487541199
translation,212,114,hyperparameters,hyperparameters,set,learning rate,hyperparameters set learning rate,0.5994082689285278
translation,212,114,hyperparameters,hyperparameters,set,batch size,hyperparameters set batch size,0.6402808427810669
translation,212,115,hyperparameters,hyper parameters,use,beam search,hyper parameters use beam search,0.6532863974571228
translation,212,115,hyperparameters,beam search,in,inference process,beam search in inference process,0.5452840924263
translation,212,115,hyperparameters,hyperparameters,tune,hyper parameters,hyperparameters tune hyper parameters,0.7034106254577637
translation,212,115,hyperparameters,hyperparameters,use,beam search,hyperparameters use beam search,0.6655463576316833
translation,212,12,model,training data,accuracy of,neural semantic parsing,training data accuracy of neural semantic parsing,0.6516286134719849
translation,212,12,model,state - of - theart model,with,less training data,state - of - theart model with less training data,0.6588636040687561
translation,212,12,model,model,influence of,training data,model influence of training data,0.6953680515289307
translation,212,12,model,model,train,state - of - theart model,model train state - of - theart model,0.6869022846221924
translation,212,19,model,question generation model,based on,sequenceto-sequence learning,question generation model based on sequenceto-sequence learning,0.6390264630317688
translation,212,19,model,model,has,question generation model,model has question generation model,0.5707156658172607
translation,212,56,model,pointer networks,by incorporating,three   channels,pointer networks by incorporating three   channels,0.6728504300117493
translation,212,56,model,three   channels,in,decoder,three   channels in decoder,0.570907711982727
translation,212,56,model,three   channels,in which,column channel,three   channels in which column channel,0.6655102968215942
translation,212,56,model,three   channels,in which,value channel,three   channels in which value channel,0.6735866665840149
translation,212,56,model,three   channels,in which,sql channel,three   channels in which sql channel,0.6767818927764893
translation,212,56,model,column channel,predicts,column names,column channel predicts column names,0.670137882232666
translation,212,56,model,column channel,predicts,sql keywords,column channel predicts sql keywords,0.7154176235198975
translation,212,56,model,value channel,predicts,table cells,value channel predicts table cells,0.7536610960960388
translation,212,56,model,value channel,predicts,sql keywords,value channel predicts sql keywords,0.6971438527107239
translation,212,56,model,sql channel,predicts,sql keywords,sql channel predicts sql keywords,0.7176088094711304
translation,212,56,model,model,extends,pointer networks,model extends pointer networks,0.7152520418167114
translation,212,76,model,rare words,from,sql queries,rare words from sql queries,0.535497784614563
translation,212,76,model,rare words,adopt,copying mechanism,rare words adopt copying mechanism,0.6469855904579163
translation,212,76,model,model,to replicate,rare words,model to replicate rare words,0.6875120401382446
translation,212,131,model,pntnet,by further learning,two separate classifiers,pntnet by further learning two separate classifiers,0.7525400519371033
translation,212,131,model,two separate classifiers,for,select aggregator,two separate classifiers for select aggregator,0.6443455219268799
translation,212,131,model,two separate classifiers,for,select column,two separate classifiers for select column,0.634618878364563
translation,212,131,model,model,has,pntnet,model has pntnet,0.6269722580909729
translation,212,184,model,supervision,uses,small portion,supervision uses small portion,0.6766254901885986
translation,212,184,model,supervision,incorporate,more generated question -logical form pairs,supervision incorporate more generated question -logical form pairs,0.6995999217033386
translation,212,184,model,small portion,of,question -logical form pairs,small portion of question -logical form pairs,0.6157701015472412
translation,212,184,model,question -logical form pairs,to initialize,qa model,question -logical form pairs to initialize qa model,0.7340755462646484
translation,212,184,model,question -logical form pairs,to initialize,qa model,question -logical form pairs to initialize qa model,0.7340755462646484
translation,212,184,model,more generated question -logical form pairs,to further improve,qa model,more generated question -logical form pairs to further improve qa model,0.7183257341384888
translation,212,184,model,model,In terms of,supervision,model In terms of supervision,0.7287822365760803
translation,212,184,model,model,uses,small portion,model uses small portion,0.6623267531394958
translation,212,184,model,model,incorporate,more generated question -logical form pairs,model incorporate more generated question -logical form pairs,0.7323750257492065
translation,212,6,results,state- ofthe - art neural network based semantic parser,thirty percent of,supervised training data,state- ofthe - art neural network based semantic parser thirty percent of supervised training data,0.6241647601127625
translation,212,6,results,results,demonstrate,question generation,results demonstrate question generation,0.5603792667388916
translation,212,7,results,question generation,to,full supervised training data,question generation to full supervised training data,0.5225855708122253
translation,212,7,results,improves,has,state - of - the - art model,improves has state - of - the - art model,0.5559159517288208
translation,212,7,results,results,applying,question generation,results applying question generation,0.6259477138519287
translation,212,23,results,question generation model,to,full training data,question generation model to full training data,0.5256322622299194
translation,212,23,results,question generation model,brings,further improvements,question generation model brings further improvements,0.6363933086395264
translation,212,23,results,further improvements,with,3.0 % absolute gain,further improvements with 3.0 % absolute gain,0.6176364421844482
translation,212,23,results,results,Applying,question generation model,results Applying question generation model,0.6722897291183472
translation,212,25,results,generated instances,improves,state - of - the - art neural semantic parser,generated instances improves state - of - the - art neural semantic parser,0.6373326182365417
translation,212,25,results,results,show,generated instances,results show generated instances,0.6139572262763977
translation,212,25,results,results,incorporating,generated instances,results incorporating generated instances,0.6816783547401428
translation,212,134,results,stamp,performs,better,stamp performs better,0.6980130076408386
translation,212,134,results,better,than,existing systems,better than existing systems,0.6277368068695068
translation,212,134,results,existing systems,when trained on,full wikisql training dataset,existing systems when trained on full wikisql training dataset,0.641502320766449
translation,212,134,results,full wikisql training dataset,achieving,logical form accuracy,full wikisql training dataset achieving logical form accuracy,0.651867151260376
translation,212,134,results,logical form accuracy,on,wikisql,logical form accuracy on wikisql,0.5491909384727478
translation,212,134,results,results,see that,stamp,results see that stamp,0.5581279397010803
translation,212,150,results,latent variable,improves,qg model performance,latent variable improves qg model performance,0.6721950769424438
translation,212,150,results,qg model performance,especially in,limit-supervision scenarios,qg model performance especially in limit-supervision scenarios,0.6382007598876953
translation,212,150,results,results,incorporating,latent variable,results incorporating latent variable,0.5705645084381104
translation,212,170,results,low accuracy,reflects,different question distribution,low accuracy reflects different question distribution,0.7020841836929321
translation,212,171,results,supervised learning setting,incorporating,qg,supervised learning setting incorporating qg,0.7101931571960449
translation,212,171,results,qg,improves,accuracy,qg improves accuracy,0.7362396121025085
translation,212,171,results,accuracy,of,nsp,accuracy of nsp,0.6395925879478455
translation,212,171,results,nsp,from,43.8 % to 44.2 %,nsp from 43.8 % to 44.2 %,0.5681613683700562
translation,212,171,results,results,In,supervised learning setting,results In supervised learning setting,0.5401096343994141
translation,214,150,ablation-analysis,pathqg,compared to,nqg + ( pl ),pathqg compared to nqg + ( pl ),0.6793305277824402
translation,214,150,ablation-analysis,nqg + ( pl ),necessity of,joint training,nqg + ( pl ) necessity of joint training,0.6745360493659973
translation,214,150,ablation-analysis,ablation analysis,improvement of,pathqg,ablation analysis improvement of pathqg,0.7376388311386108
translation,214,176,ablation-analysis,model pathqg - v,is,more informative and specific,model pathqg - v is more informative and specific,0.5475845336914062
translation,214,176,ablation-analysis,more informative and specific,consists of,information,more informative and specific consists of information,0.6248862147331238
translation,214,176,ablation-analysis,more informative and specific,consists of,late 18th,more informative and specific consists of late 18th,0.6346055269241333
translation,214,176,ablation-analysis,ablation analysis,has,model pathqg - v,ablation analysis has model pathqg - v,0.570115327835083
translation,214,129,baselines,- nqg +,is,attention - based encoder-decoder model,- nqg + is attention - based encoder-decoder model,0.5744071006774902
translation,214,129,baselines,- nqg +,uses,bio tagging scheme,- nqg + uses bio tagging scheme,0.6642696261405945
translation,214,129,baselines,attention - based encoder-decoder model,with,sentence,attention - based encoder-decoder model with sentence,0.6394841074943542
translation,214,129,baselines,sentence,as,input,sentence as input,0.5344812870025635
translation,214,129,baselines,bio tagging scheme,to incorporate,additional entity information ( start and end nodes ),bio tagging scheme to incorporate additional entity information ( start and end nodes ),0.6713498830795288
translation,214,129,baselines,baselines,has,- nqg +,baselines has - nqg +,0.6233404874801636
translation,214,130,baselines,- afpa,combines,answerfocused model,- afpa combines answerfocused model,0.7475060820579529
translation,214,130,baselines,- afpa,combines,position - aware model,- afpa combines position - aware model,0.7523432374000549
translation,214,130,baselines,- afpa,for,question generation,- afpa for question generation,0.645323634147644
translation,214,130,baselines,position - aware model,for,question generation,position - aware model for question generation,0.5729285478591919
translation,214,130,baselines,baselines,has,- afpa,baselines has - afpa,0.5594489574432373
translation,214,138,baselines,pathqg -v,is,variational version,pathqg -v is variational version,0.5865421295166016
translation,214,138,baselines,variational version,of,pathqg,variational version of pathqg,0.6048474907875061
translation,214,138,baselines,variational version,with,additional posterior query learner,variational version with additional posterior query learner,0.6310155987739563
translation,214,138,baselines,baselines,has,pathqg -v,baselines has pathqg -v,0.5753512382507324
translation,214,120,experiments,different vocabularies,for,input texts and questions,different vocabularies for input texts and questions,0.5850670337677002
translation,214,120,experiments,different vocabularies,by keeping,words,different vocabularies by keeping words,0.6925640106201172
translation,214,120,experiments,words,appear,more than twice,words appear more than twice,0.5977118015289307
translation,214,121,hyperparameters,glove,to initialize,word embedding,glove to initialize word embedding,0.6868656873703003
translation,214,121,hyperparameters,glove,to initialize,embedding,glove to initialize embedding,0.726904571056366
translation,214,121,hyperparameters,word embedding,with,dimension,word embedding with dimension,0.6302738189697266
translation,214,121,hyperparameters,embedding,for,bio tag,embedding for bio tag,0.6579525470733643
translation,214,121,hyperparameters,randomly initialized,size,20,randomly initialized size 20,0.7717974781990051
translation,214,121,hyperparameters,dimension,has,300,dimension has 300,0.6396656632423401
translation,214,121,hyperparameters,hyperparameters,has,glove,hyperparameters has glove,0.5672336220741272
translation,214,122,hyperparameters,hidden units,in,lstm cell,hidden units in lstm cell,0.4807050824165344
translation,214,122,hyperparameters,lstm cell,in,all encoders,lstm cell in all encoders,0.538332998752594
translation,214,122,hyperparameters,all encoders,is,300,all encoders is 300,0.6120145916938782
translation,214,122,hyperparameters,generation decoder,is,1200,generation decoder is 1200,0.6142362356185913
translation,214,123,hyperparameters,weights,of,losses,weights of losses,0.6093902587890625
translation,214,123,hyperparameters,beam search,of,beam size 5,beam search of beam size 5,0.7015851736068726
translation,214,123,hyperparameters,hyperparameters,to balance,weights,hyperparameters to balance weights,0.615183413028717
translation,214,6,model,facts,in,text,facts in text,0.5754127502441406
translation,214,6,model,facts,for,question generation,facts for question generation,0.6253947019577026
translation,214,6,model,model,incorporate,facts,model incorporate facts,0.7707309126853943
translation,214,9,model,query representation learning,as,sequence labeling problem,query representation learning as sequence labeling problem,0.48046186566352844
translation,214,9,model,sequence labeling problem,for identifying,involved facts,sequence labeling problem for identifying involved facts,0.7225151658058167
translation,214,9,model,involved facts,to form,query,involved facts to form query,0.5995369553565979
translation,214,9,model,rnn - based generator,for,question generation,rnn - based generator for question generation,0.5967939496040344
translation,214,9,model,model,formulate,query representation learning,model formulate query representation learning,0.6346204280853271
translation,214,9,model,model,employ,rnn - based generator,model employ rnn - based generator,0.6020799279212952
translation,214,10,model,two modules,in,end-to- end fashion,two modules in end-to- end fashion,0.5645264983177185
translation,214,10,model,two modules,enforce,interaction,two modules enforce interaction,0.7748913168907166
translation,214,10,model,jointly,in,end-to- end fashion,jointly in end-to- end fashion,0.5800281763076782
translation,214,10,model,interaction,in,variational framework,interaction in variational framework,0.5435975193977356
translation,214,10,model,two modules,has,jointly,two modules has jointly,0.6280980110168457
translation,214,10,model,model,train,two modules,model train two modules,0.6968069076538086
translation,214,28,model,represent facts,as,knowledge graph ( kg ),represent facts as knowledge graph ( kg ),0.5091829299926758
translation,214,29,model,kg,contains,set of fact triples,kg contains set of fact triples,0.5960748791694641
translation,214,29,model,query path,is,ordered sequence of triples,query path is ordered sequence of triples,0.5786355137825012
translation,214,29,model,ordered sequence of triples,in,kg,ordered sequence of triples in kg,0.5442174077033997
translation,214,29,model,model,has,kg,model has kg,0.5920581221580505
translation,214,37,model,two modules,in,end-to- end fashion,two modules in end-to- end fashion,0.5645264983177185
translation,214,37,model,jointly,in,end-to- end fashion,jointly in end-to- end fashion,0.5800281763076782
translation,214,37,model,two modules,has,jointly,two modules has jointly,0.6280980110168457
translation,214,37,model,model,train,two modules,model train two modules,0.6968069076538086
translation,214,38,model,variational framework,to train,two modules,variational framework to train two modules,0.6815211772918701
translation,214,38,model,learning,as,inference process,learning as inference process,0.5521982312202454
translation,214,38,model,inference process,from,query path,inference process from query path,0.5465525984764099
translation,214,38,model,query path,taking,generated question,query path taking generated question,0.6657490730285645
translation,214,38,model,generated question,as,target,generated question as target,0.5795363187789917
translation,214,38,model,model,employ,variational framework,model employ variational framework,0.5623932480812073
translation,214,71,model,variational inference model pathqg -v,to train,query learner and question generator,variational inference model pathqg -v to train query learner and question generator,0.6806964874267578
translation,214,71,model,query learner and question generator,to further enforce,interaction,query learner and question generator to further enforce interaction,0.6916025876998901
translation,214,71,model,model,propose,variational inference model pathqg -v,model propose variational inference model pathqg -v,0.661350429058075
translation,214,191,model,facts,in,input text,facts in input text,0.5124853849411011
translation,214,191,model,facts,as,knowledge graph,facts as knowledge graph,0.5094671845436096
translation,214,191,model,knowledge graph,for,question generation,knowledge graph for question generation,0.6085176467895508
translation,214,191,model,model,model,facts,model model facts,0.841020405292511
translation,214,193,model,query representation,for,question generation,query representation for question generation,0.5802703499794006
translation,214,193,model,query representation,in,joint model,query representation in joint model,0.4984591603279114
translation,214,193,model,model,learn,query representation,model learn query representation,0.6321484446525574
translation,214,105,results,average information coverage rate,of,input text,average information coverage rate of input text,0.5746634602546692
translation,214,105,results,input text,by,constructed kg,input text by constructed kg,0.5953980684280396
translation,214,105,results,results,has,average information coverage rate,results has average information coverage rate,0.5597102642059326
translation,214,147,results,outperforms,by,considerable margin,outperforms by considerable margin,0.653662919998169
translation,214,147,results,other models,in terms of,all metrics,other models in terms of all metrics,0.6332547664642334
translation,214,147,results,other models,on,both datasets,other models on both datasets,0.4850037097930908
translation,214,147,results,other models,by,considerable margin,other models by considerable margin,0.604303777217865
translation,214,147,results,pathqg - v,has,outperforms,pathqg - v has outperforms,0.6361441612243652
translation,214,147,results,outperforms,has,other models,outperforms has other models,0.5875559449195862
translation,214,149,results,path qg,identifying,involved entities and relations,path qg identifying involved entities and relations,0.6478859186172485
translation,214,149,results,path qg,performs,better,path qg performs better,0.6610790491104126
translation,214,149,results,involved entities and relations,along,path,involved entities and relations along path,0.6553955078125
translation,214,149,results,better,than,nqg +,better than nqg +,0.638110876083374
translation,214,149,results,better,than,afpa,better than afpa,0.6163618564605713
translation,214,149,results,better,than,ass2s,better than ass2s,0.6262184381484985
translation,214,149,results,results,has,path qg,results has path qg,0.6278777122497559
translation,214,151,results,our model,generates,larger improvement,our model generates larger improvement,0.6454401612281799
translation,214,151,results,larger improvement,on,complex dataset,larger improvement on complex dataset,0.5430436134338379
translation,214,151,results,larger improvement,compared to,whole dataset,larger improvement compared to whole dataset,0.6393417119979858
translation,214,151,results,results,has,our model,results has our model,0.5871725678443909
translation,214,155,results,good performance,achieved by,pathqg -v,good performance achieved by pathqg -v,0.7014040946960449
translation,214,163,results,informativeness,contains,specific information,informativeness contains specific information,0.6255695819854736
translation,214,163,results,question,contains,specific information,question contains specific information,0.6608086228370667
translation,214,163,results,informativeness,has,question,informativeness has question,0.5956955552101135
translation,214,163,results,results,has,informativeness,results has informativeness,0.5598773956298828
translation,214,174,results,pathqg -v,achieves,highest overlapping rate,pathqg -v achieves highest overlapping rate,0.6887894868850708
translation,214,174,results,highest overlapping rate,among,all models,highest overlapping rate among all models,0.6239383816719055
translation,214,174,results,our model,better utilize,facts,our model better utilize facts,0.7137148380279541
translation,214,174,results,facts,in,input text,facts in input text,0.5124853849411011
translation,214,174,results,facts,to generate,more relevant questions,facts to generate more relevant questions,0.6423377394676208
translation,214,174,results,both datasets,has,pathqg -v,both datasets has pathqg -v,0.6287444829940796
translation,214,174,results,results,On,both datasets,results On both datasets,0.49870386719703674
translation,214,175,results,improvement,of,pathqg,improvement of pathqg,0.6331019401550293
translation,214,175,results,pathqg,compared to,other models,pathqg compared to other models,0.7154626846313477
translation,214,175,results,involved entities and relations,among,path,involved entities and relations among path,0.6143772602081299
translation,214,175,results,involved entities and relations,for,question generation,involved entities and relations for question generation,0.5921831727027893
translation,214,175,results,results,has,improvement,results has improvement,0.6248279809951782
translation,215,58,experimental-setup,two stacked shared lstms,instead of,single one,two stacked shared lstms instead of single one,0.669805109500885
translation,215,58,experimental-setup,experimental setup,use,two stacked shared lstms,experimental setup use two stacked shared lstms,0.5591955780982971
translation,215,59,experimental-setup,"adam ( kingma and ba , 2014 ) optimisation",instead of,sgd,"adam ( kingma and ba , 2014 ) optimisation instead of sgd",0.5911896228790283
translation,215,59,experimental-setup,cross entropy loss,instead of,margin ranking loss,cross entropy loss instead of margin ranking loss,0.5401471853256226
translation,215,59,experimental-setup,experimental setup,used,"adam ( kingma and ba , 2014 ) optimisation","experimental setup used adam ( kingma and ba , 2014 ) optimisation",0.6011576652526855
translation,215,62,experimental-setup,word embeddings,are,"word2vec ( mikolov et al. , 2013 )","word embeddings are word2vec ( mikolov et al. , 2013 )",0.4573754370212555
translation,215,62,experimental-setup,300,pre-trained on,google news,300 pre-trained on google news,0.6621531248092651
translation,215,62,experimental-setup,dimension,has,300,dimension has 300,0.6396656632423401
translation,215,62,experimental-setup,experimental setup,has,word embeddings,experimental setup has word embeddings,0.4932878315448761
translation,215,144,experimental-setup,visualisation application,is,client-server system,visualisation application is client-server system,0.5736092925071716
translation,215,144,experimental-setup,client-server system,with,web interface,client-server system with web interface,0.6235017776489258
translation,215,144,experimental-setup,experimental setup,has,visualisation application,experimental setup has visualisation application,0.5187821984291077
translation,215,145,experimental-setup,jquery,on,client side,jquery on client side,0.5346186757087708
translation,215,145,experimental-setup,python,on,server side,python on server side,0.5385139584541321
translation,215,145,experimental-setup,experimental setup,uses,jquery,experimental setup uses jquery,0.6424486637115479
translation,215,145,experimental-setup,experimental setup,uses,python,experimental setup uses python,0.6344200968742371
translation,215,146,experimental-setup,application,built with,"flask ( ronacher , 2018 ) framework","application built with flask ( ronacher , 2018 ) framework",0.7253237366676331
translation,215,146,experimental-setup,experimental setup,has,application,experimental setup has application,0.516164243221283
translation,215,147,experimental-setup,text preprocessing,use,"spacy ( honnibal and johnson , 2015 )","text preprocessing use spacy ( honnibal and johnson , 2015 )",0.6034985780715942
translation,215,147,experimental-setup,text preprocessing,use,"nltk ( loper and bird , 2002 )","text preprocessing use nltk ( loper and bird , 2002 )",0.6040441989898682
translation,215,147,experimental-setup,experimental setup,For,text preprocessing,experimental setup For text preprocessing,0.5450654029846191
translation,215,25,model,new interactive web-interface,potentially be adapted to,other models and domains,new interactive web-interface potentially be adapted to other models and domains,0.6564970016479492
translation,215,25,model,model,present,new interactive web-interface,model present new interactive web-interface,0.6228836178779602
translation,216,5,model,navigation and touristic questionanswering,in,integrated fashion,navigation and touristic questionanswering in integrated fashion,0.5671071410179138
translation,216,5,model,navigation and touristic questionanswering,using,shared dialogue context,navigation and touristic questionanswering using shared dialogue context,0.6707789897918701
translation,216,5,model,integrated fashion,using,shared dialogue context,integrated fashion using shared dialogue context,0.6863881349563599
translation,216,9,model,mobile dialogue system,called,spacebook,mobile dialogue system called spacebook,0.6728694438934326
translation,216,141,results,no statistically significant difference,between,systems,no statistically significant difference between systems,0.6230578422546387
translation,216,141,results,systems,in terms of,perceived task success,systems in terms of perceived task success,0.6687576174736023
translation,216,141,results,better task completion rate,in,"tasks 1 - 3 , 5 and 6","better task completion rate in tasks 1 - 3 , 5 and 6",0.5177542567253113
translation,216,141,results,baseline system,has,better task completion rate,baseline system has better task completion rate,0.5449118614196777
translation,216,141,results,results,show,no statistically significant difference,results show no statistically significant difference,0.5637724995613098
translation,216,147,results,our system,not performing,significantly better,our system not performing significantly better,0.6162711977958679
translation,216,147,results,significantly better,than,baseline system,significantly better than baseline system,0.5597087144851685
translation,216,147,results,more interesting,to,interact with,more interesting to interact with,0.6351600289344788
translation,216,147,results,interact with,than,baseline,interact with than baseline,0.6494271755218506
translation,216,147,results,baseline system,has,- sq10 except sq7 ),baseline system has - sq10 except sq7 ),0.629040539264679
translation,216,147,results,results,show,our system,results show our system,0.6763114333152771
translation,217,169,ablation-analysis,mqaee ( - tri ),have,2.3 % degradation,mqaee ( - tri ) have 2.3 % degradation,0.5477547645568848
translation,217,169,ablation-analysis,extracted trigger,has,mqaee ( - tri ),extracted trigger has mqaee ( - tri ),0.6371759176254272
translation,217,169,ablation-analysis,ablation analysis,Without considering,extracted trigger,ablation analysis Without considering extracted trigger,0.7541809678077698
translation,217,137,baselines,"jointbeam ( li et al. , 2013 )",jointly extracts,event triggers and arguments,"jointbeam ( li et al. , 2013 ) jointly extracts event triggers and arguments",0.6883805990219116
translation,217,137,baselines,event triggers and arguments,via,structure prediction,event triggers and arguments via structure prediction,0.6975659132003784
translation,217,137,baselines,structure prediction,by,well designed features,structure prediction by well designed features,0.5498906970024109
translation,217,138,baselines,jointevententity,models,dependencies,jointevententity models dependencies,0.764751672744751
translation,217,138,baselines,jointevententity,jointly extracts,events and entities,jointevententity jointly extracts events and entities,0.6468833684921265
translation,217,138,baselines,dependencies,among,events and entities,dependencies among events and entities,0.6138709783554077
translation,217,138,baselines,rbpb,proposes,regularization - based pattern balancing method,rbpb proposes regularization - based pattern balancing method,0.6800448298454285
translation,217,138,baselines,regularization - based pattern balancing method,to extract,event triggers and arguments,regularization - based pattern balancing method to extract event triggers and arguments,0.7277341485023499
translation,217,138,baselines,rbpb,has,"et al. , 2016 )","rbpb has et al. , 2016 )",0.5756970643997192
translation,217,138,baselines,baselines,has,jointevententity,baselines has jointevententity,0.5684909820556641
translation,217,139,baselines,"dbrnn ( sha et al. , 2018 )",adds,dependency bridges,"dbrnn ( sha et al. , 2018 ) adds dependency bridges",0.617289125919342
translation,217,139,baselines,dependency bridges,over,bi-lstm,dependency bridges over bi-lstm,0.6779685616493225
translation,217,139,baselines,bi-lstm,for,event extraction,bi-lstm for event extraction,0.5971225500106812
translation,217,139,baselines,baselines,has,"dbrnn ( sha et al. , 2018 )","baselines has dbrnn ( sha et al. , 2018 )",0.5132551789283752
translation,217,140,baselines,"dygie ++ ( wadden et al. , 2019",proposes,multi-task framework,"dygie ++ ( wadden et al. , 2019 proposes multi-task framework",0.636634349822998
translation,217,140,baselines,multi-task framework,for,"entity , relation and event extraction","multi-task framework for entity , relation and event extraction",0.5418801307678223
translation,217,140,baselines,"entity , relation and event extraction",with,contextualized span representations,"entity , relation and event extraction with contextualized span representations",0.6207618713378906
translation,217,140,baselines,baselines,has,"dygie ++ ( wadden et al. , 2019","baselines has dygie ++ ( wadden et al. , 2019",0.5643247961997986
translation,217,141,baselines,dy - gie ++( ens ),use of,4 - model ensemble,dy - gie ++( ens ) use of 4 - model ensemble,0.7002964019775391
translation,217,141,baselines,4 - model ensemble,for,trigger detection,4 - model ensemble for trigger detection,0.5604769587516785
translation,217,141,baselines,baselines,has,dy - gie ++( ens ),baselines has dy - gie ++( ens ),0.5995364785194397
translation,217,143,experimental-setup,our model,based on,bert - large model,our model based on bert - large model,0.6771591305732727
translation,217,143,experimental-setup,experimental setup,implement,our model,experimental setup implement our model,0.6453344821929932
translation,217,145,experimental-setup,batch size,set to,8,batch size set to 8,0.770182192325592
translation,217,145,experimental-setup,max sequence length,is,200,max sequence length is 200,0.6106467843055725
translation,217,147,experimental-setup,warming up portion,for,learning rate,warming up portion for learning rate,0.6197090744972229
translation,217,147,experimental-setup,learning rate,is,10 %,learning rate is 10 %,0.6078191995620728
translation,217,147,experimental-setup,experimental setup,has,warming up portion,experimental setup has warming up portion,0.5087123513221741
translation,217,148,experimental-setup,stride,in,sliding window,stride in sliding window,0.5661156177520752
translation,217,148,experimental-setup,sliding window,for,passages,sliding window for passages,0.6563149690628052
translation,217,148,experimental-setup,passages,to,128,passages to 128,0.6138985753059387
translation,217,148,experimental-setup,max question length,to,64,max question length to 64,0.5792775750160217
translation,217,148,experimental-setup,max answer length,to,30,max answer length to 30,0.6002185940742493
translation,217,148,experimental-setup,experimental setup,set,stride,experimental setup set stride,0.6287362575531006
translation,217,149,experimental-setup,training epoch,to,10,training epoch to 10,0.6311436295509338
translation,217,149,experimental-setup,training epoch,for,trigger identifier and the two argument extractors,training epoch for trigger identifier and the two argument extractors,0.6076048016548157
translation,217,149,experimental-setup,training epoch,both,trigger identifier and the two argument extractors,training epoch both trigger identifier and the two argument extractors,0.664280891418457
translation,217,149,experimental-setup,10,for,trigger identifier and the two argument extractors,10 for trigger identifier and the two argument extractors,0.5769972205162048
translation,217,149,experimental-setup,10,both,trigger identifier and the two argument extractors,10 both trigger identifier and the two argument extractors,0.632506251335144
translation,217,149,experimental-setup,experimental setup,set,training epoch,experimental setup set training epoch,0.640143096446991
translation,217,150,experimental-setup,adam weight decay optimizer,with,initial learning rate,adam weight decay optimizer with initial learning rate,0.5805231332778931
translation,217,150,experimental-setup,initial learning rate,of,1e - 5,initial learning rate of 1e - 5,0.612038254737854
translation,217,150,experimental-setup,experimental setup,has,adam weight decay optimizer,experimental setup has adam weight decay optimizer,0.5179386734962463
translation,217,151,experimental-setup,warming up portion,for,learning rate,warming up portion for learning rate,0.6197090744972229
translation,217,151,experimental-setup,warming up portion,for,epoch,warming up portion for epoch,0.651031494140625
translation,217,151,experimental-setup,learning rate,is,10 %,learning rate is 10 %,0.6078191995620728
translation,217,151,experimental-setup,epoch,set to,3,epoch set to 3,0.7560907006263733
translation,217,151,experimental-setup,experimental setup,has,warming up portion,experimental setup has warming up portion,0.5087123513221741
translation,217,151,experimental-setup,experimental setup,has,epoch,experimental setup has epoch,0.5018512606620789
translation,217,146,experiments,models,with,adam weight decay optimizer,models with adam weight decay optimizer,0.5757229924201965
translation,217,146,experiments,adam weight decay optimizer,with,initial learning rate,adam weight decay optimizer with initial learning rate,0.5805231332778931
translation,217,146,experiments,initial learning rate,of,1e - 5,initial learning rate of 1e - 5,0.612038254737854
translation,217,8,model,mqaee,casts,extraction task,mqaee casts extraction task,0.6228733658790588
translation,217,8,model,extraction task,series of,reading comprehension problems,extraction task series of reading comprehension problems,0.6244537234306335
translation,217,8,model,triggers and arguments,from,given sentence,triggers and arguments from given sentence,0.5480782389640808
translation,217,8,model,successively,from,given sentence,successively from given sentence,0.5608985424041748
translation,217,8,model,triggers and arguments,has,successively,triggers and arguments has successively,0.6121166944503784
translation,217,8,model,model,has,mqaee,model has mqaee,0.6456523537635803
translation,217,9,model,history answer embedding strategy,to model,question answering history,history answer embedding strategy to model question answering history,0.6939472556114197
translation,217,9,model,question answering history,in,multi-turn process,question answering history in multi-turn process,0.5257520079612732
translation,217,9,model,model,has,history answer embedding strategy,model has history answer embedding strategy,0.6027210354804993
translation,217,33,model,event extraction,as,multi-turn question answering ( qa ),event extraction as multi-turn question answering ( qa ),0.52256840467453
translation,217,34,model,mqaee,splits,event extraction,mqaee splits event extraction,0.7340466380119324
translation,217,144,model,three sub-models,for,mqaee,three sub-models for mqaee,0.6695410013198853
translation,217,144,model,model,maintain,three sub-models,model maintain three sub-models,0.6927464604377747
translation,217,144,model,model,for,mqaee,model for mqaee,0.6734719276428223
translation,217,211,model,model,splits,event extraction,model splits event extraction,0.7187348008155823
translation,217,212,model,history answer embedding strategy,to effectively model,qa history,history answer embedding strategy to effectively model qa history,0.6502140164375305
translation,217,212,model,multiturn process,has,history answer embedding strategy,multiturn process has history answer embedding strategy,0.6066120862960815
translation,217,212,model,model,Within,multiturn process,model Within multiturn process,0.6780493855476379
translation,217,48,results,significantly outperforms,pushing,final f1 score,significantly outperforms pushing final f1 score,0.6619808673858643
translation,217,48,results,final f1 score,of,argument extraction,final f1 score of argument extraction,0.5629221200942993
translation,217,48,results,argument extraction,to,53.4 % ( + 2.0 % ),argument extraction to 53.4 % ( + 2.0 % ),0.5460114479064941
translation,217,48,results,mqaee,has,significantly outperforms,mqaee has significantly outperforms,0.6231277585029602
translation,217,48,results,significantly outperforms,has,current stateof - the- art,significantly outperforms has current stateof - the- art,0.5635755658149719
translation,217,48,results,results,show,mqaee,results show mqaee,0.6471589207649231
translation,217,49,results,mqaee,generalizes,well,mqaee generalizes well,0.7072129845619202
translation,217,49,results,mqaee,achieving,competitive results,mqaee achieving competitive results,0.6647918224334717
translation,217,49,results,well,to,new event types,well to new event types,0.5894759297370911
translation,217,49,results,competitive results,on,13 new event types,competitive results on 13 new event types,0.528099536895752
translation,217,49,results,results,has,mqaee,results has mqaee,0.5804176330566406
translation,217,154,results,all other models,except,dygie ++( ens ),all other models except dygie ++( ens ),0.6888269186019897
translation,217,154,results,dygie ++( ens ),on,trigger classification,dygie ++( ens ) on trigger classification,0.5700313448905945
translation,217,154,results,mqaee,has,outperforms,mqaee has outperforms,0.6477838158607483
translation,217,154,results,outperforms,has,all other models,outperforms has all other models,0.5782700181007385
translation,217,154,results,results,show,mqaee,results show mqaee,0.6471589207649231
translation,217,156,results,dygie ++,that is,single model,dygie ++ that is single model,0.6429730653762817
translation,217,156,results,single model,based on,bert - large,single model based on bert - large,0.6885092854499817
translation,217,156,results,mqaee,achieves,sharp increase,mqaee achieves sharp increase,0.7199479937553406
translation,217,156,results,sharp increase,of,2.8 %,sharp increase of 2.8 %,0.5700463056564331
translation,217,156,results,2.8 %,on,f1 score,2.8 % on f1 score,0.5325477719306946
translation,217,156,results,dygie ++,has,mqaee,dygie ++ has mqaee,0.6543598175048828
translation,217,156,results,single model,has,mqaee,single model has mqaee,0.6230022311210632
translation,217,156,results,results,Compared to,dygie ++,results Compared to dygie ++,0.6348499059677124
translation,217,158,results,outperformed,on,trigger identification,outperformed on trigger identification,0.5704922080039978
translation,217,158,results,outperformed,on,trigger classification,outperformed on trigger classification,0.5834457874298096
translation,217,158,results,dy - gie ++( ens ),on,trigger identification,dy - gie ++( ens ) on trigger identification,0.5931509137153625
translation,217,158,results,dy - gie ++( ens ),on,trigger classification,dy - gie ++( ens ) on trigger classification,0.5854692459106445
translation,217,158,results,outperformed,has,dy - gie ++( ens ),outperformed has dy - gie ++( ens ),0.6314756274223328
translation,217,158,results,results,see that,mqaee ( ens ),results see that mqaee ( ens ),0.6821951270103455
translation,217,159,results,our model,achieves,best performance,our model achieves best performance,0.684498131275177
translation,217,159,results,argument classification,has,our model,argument classification has our model,0.6062936186790466
translation,217,159,results,results,In terms of,argument classification,results In terms of argument classification,0.7208905816078186
translation,217,159,results,results,results of,argument classification,results results of argument classification,0.8124890923500061
translation,217,160,results,mqaee,achieves,increase,mqaee achieves increase,0.7056398391723633
translation,217,160,results,mqaee,achieves,increase,mqaee achieves increase,0.7056398391723633
translation,217,160,results,increase,of,2.0 %,increase of 2.0 %,0.6053988337516785
translation,217,160,results,increase,of,0.9 %,increase of 0.9 %,0.6120288968086243
translation,217,160,results,2.0 %,on,f1 score,2.0 % on f1 score,0.5436410307884216
translation,217,160,results,2.0 %,compared to,dygie + +,2.0 % compared to dygie + +,0.7006616592407227
translation,217,160,results,2.0 %,compared to,dygie ++( ens ),2.0 % compared to dygie ++( ens ),0.7158856987953186
translation,217,160,results,increase,of,0.9 %,increase of 0.9 %,0.6120288968086243
translation,217,160,results,0.9 %,compared to,dygie ++( ens ),0.9 % compared to dygie ++( ens ),0.705337405204773
translation,217,160,results,results,has,mqaee,results has mqaee,0.5804176330566406
translation,217,161,results,mqaee ( ens ),achieves,sharp increase,mqaee ( ens ) achieves sharp increase,0.7022990584373474
translation,217,161,results,sharp increase,of,2.5 %,sharp increase of 2.5 %,0.5800992846488953
translation,217,161,results,2.5 %,on,f score,2.5 % on f score,0.5560381412506104
translation,217,161,results,2.5 %,compared to,dy - gie ++( ens ),2.5 % compared to dy - gie ++( ens ),0.7172239422798157
translation,217,167,results,mqaee,achieve,better result,mqaee achieve better result,0.6589854955673218
translation,217,167,results,better result,with,improvement,better result with improvement,0.6306741237640381
translation,217,167,results,improvement,of,3.0 %,improvement of 3.0 %,0.5686642527580261
translation,217,167,results,3.0 %,on,f1 score,3.0 % on f1 score,0.5519014596939087
translation,217,167,results,qaee,has,mqaee,qaee has mqaee,0.6669825911521912
translation,217,168,results,mqaee,achieve,better performance,mqaee achieve better performance,0.6628497242927551
translation,217,168,results,mqaee ( rnd ),has,mqaee,mqaee ( rnd ) has mqaee,0.638209342956543
translation,217,168,results,results,Comparing to,mqaee ( rnd ),results Comparing to mqaee ( rnd ),0.6851113438606262
translation,217,176,results,increase,of,history turns,increase of history turns,0.6096941828727722
translation,217,176,results,mqaee,achieves,better performance,mqaee achieves better performance,0.6894364356994629
translation,217,176,results,mqaee,achieves,best performance,mqaee achieves best performance,0.7040902972221375
translation,217,176,results,best performance,with,14 history turns,best performance with 14 history turns,0.684234082698822
translation,217,176,results,increase,has,mqaee,increase has mqaee,0.6261406540870667
translation,217,176,results,history turns,has,mqaee,history turns has mqaee,0.6288309693336487
translation,217,198,results,number of event types,used for,training,number of event types used for training,0.7240381240844727
translation,217,198,results,our model,achieves,better performance,our model achieves better performance,0.6816908121109009
translation,217,198,results,number of event types,has,our model,number of event types has our model,0.5506079196929932
translation,217,198,results,training,has,our model,training has our model,0.5295395851135254
translation,217,198,results,results,increase in,number of event types,results increase in number of event types,0.680879533290863
translation,217,199,results,finetuned,in,13 way,finetuned in 13 way,0.5752860903739929
translation,217,199,results,13 way,with,5 - shot,13 way with 5 - shot,0.6982012391090393
translation,217,199,results,mqaee,achieves,competitive results,mqaee achieves competitive results,0.6909071803092957
translation,217,199,results,competitive results,on,remaining 13 event types,competitive results on remaining 13 event types,0.513498067855835
translation,217,199,results,competitive results,comparing to,our mqaee,competitive results comparing to our mqaee,0.72334885597229
translation,217,199,results,our mqaee,trained with,all 33 event types,our mqaee trained with all 33 event types,0.7369194030761719
translation,217,199,results,top 20 event types,has,mqaee,top 20 event types has mqaee,0.5947940349578857
translation,218,117,baselines,attention - based sequence - to-sequence neural network,for,question generation,attention - based sequence - to-sequence neural network for question generation,0.5817100405693054
translation,218,118,baselines,answer position feature and linguistic features,into,consideration,answer position feature and linguistic features into consideration,0.5742705464363098
translation,218,118,baselines,seq2seq model,with,copy mechanism,seq2seq model with copy mechanism,0.6656879782676697
translation,218,119,baselines,multiperspective matching,between,answer and the sentence,multiperspective matching between answer and the sentence,0.6166982054710388
translation,218,119,baselines,multiperspective matching,to derive,answer -aware sentence representation,multiperspective matching to derive answer -aware sentence representation,0.6240272521972656
translation,218,119,baselines,answer and the sentence,to derive,answer -aware sentence representation,answer and the sentence to derive answer -aware sentence representation,0.6642111539840698
translation,218,119,baselines,answer -aware sentence representation,for,question generation,answer -aware sentence representation for question generation,0.5617852210998535
translation,218,120,baselines,s2s+mp+gsa,introduces,gated self-attention,s2s+mp+gsa introduces gated self-attention,0.6208950281143188
translation,218,120,baselines,s2s+mp+gsa,introduces,maxout pointer mechanism,s2s+mp+gsa introduces maxout pointer mechanism,0.6689744591712952
translation,218,120,baselines,gated self-attention,into,encoder,gated self-attention into encoder,0.5948214530944824
translation,218,120,baselines,maxout pointer mechanism,into,decoder,maxout pointer mechanism into decoder,0.6050176024436951
translation,218,120,baselines,baselines,has,s2s+mp+gsa,baselines has s2s+mp+gsa,0.5471276044845581
translation,218,122,baselines,hybrid,is,hybrid model,hybrid is hybrid model,0.6227304339408875
translation,218,122,baselines,hybrid model,considers,answer embedding,hybrid model considers answer embedding,0.6895759105682373
translation,218,122,baselines,answer embedding,for,question word generation,answer embedding for question word generation,0.5924224853515625
translation,218,122,baselines,answer embedding,position of,context words,answer embedding position of context words,0.6465196013450623
translation,218,122,baselines,relative distance,between,answer,relative distance between answer,0.681297242641449
translation,218,122,baselines,relative distance,context words and the,answer,relative distance context words and the answer,0.7325058579444885
translation,218,122,baselines,baselines,has,hybrid,baselines has hybrid,0.6365060806274414
translation,218,123,baselines,"ass2s ( kim et al. , 2019 )",replaces,answer,"ass2s ( kim et al. , 2019 ) replaces answer",0.7565774917602539
translation,218,123,baselines,answer,in,sentence,answer in sentence,0.5031861066818237
translation,218,123,baselines,special token,to avoid,appearance,special token to avoid appearance,0.6509531140327454
translation,218,123,baselines,appearance,in,generated questions,appearance in generated questions,0.5447167158126831
translation,218,123,baselines,baselines,has,"ass2s ( kim et al. , 2019 )","baselines has ass2s ( kim et al. , 2019 )",0.5241251587867737
translation,218,50,experiments,evaluations,on,squad dataset,evaluations on squad dataset,0.5173087120056152
translation,218,50,experiments,our system,achieves,significant and consistent improvement,our system achieves significant and consistent improvement,0.6482588648796082
translation,218,50,experiments,significant and consistent improvement,compared to,all baseline methods,significant and consistent improvement compared to all baseline methods,0.6378675103187561
translation,218,50,experiments,evaluations,has,our system,evaluations has our system,0.6111583709716797
translation,218,50,experiments,squad dataset,has,our system,squad dataset has our system,0.5913034081459045
translation,218,125,hyperparameters,most frequent 20 k words,as,our vocabulary,most frequent 20 k words as our vocabulary,0.5249836444854736
translation,218,125,hyperparameters,glove word embeddings,for,initialization,glove word embeddings for initialization,0.5425793528556824
translation,218,125,hyperparameters,hyperparameters,take,most frequent 20 k words,hyperparameters take most frequent 20 k words,0.588177502155304
translation,218,125,hyperparameters,hyperparameters,use,glove word embeddings,hyperparameters use glove word embeddings,0.5590203404426575
translation,218,126,hyperparameters,embed-ding dimensions,for,"pos , ner , answer position","embed-ding dimensions for pos , ner , answer position",0.5757643580436707
translation,218,126,hyperparameters,"pos , ner , answer position",set to,20,"pos , ner , answer position set to 20",0.6722990870475769
translation,218,126,hyperparameters,hyperparameters,has,embed-ding dimensions,hyperparameters has embed-ding dimensions,0.517261266708374
translation,218,127,hyperparameters,two - layer lstms,in,both encoder and decoder,two - layer lstms in both encoder and decoder,0.506891131401062
translation,218,127,hyperparameters,lstms hidden unit size,set to,600,lstms hidden unit size set to 600,0.6922243237495422
translation,218,127,hyperparameters,hyperparameters,use,two - layer lstms,hyperparameters use two - layer lstms,0.5957613587379456
translation,218,128,hyperparameters,"dropout ( srivastava et al. , 2014 )",with,probability p = 0.3,"dropout ( srivastava et al. , 2014 ) with probability p = 0.3",0.6094828844070435
translation,218,129,hyperparameters,trainable parameters,except,word embeddings,trainable parameters except word embeddings,0.5791491270065308
translation,218,129,hyperparameters,randomly initialized,with,"xavier uniform in ( ?0.1 , 0.1 )","randomly initialized with xavier uniform in ( ?0.1 , 0.1 )",0.6074435710906982
translation,218,129,hyperparameters,hyperparameters,has,trainable parameters,hyperparameters has trainable parameters,0.475637286901474
translation,218,130,hyperparameters,optimization,in,training,optimization in training,0.5602493286132812
translation,218,130,hyperparameters,optimization,use,sgd,optimization use sgd,0.6316283941268921
translation,218,130,hyperparameters,training,use,sgd,training use sgd,0.7143113017082214
translation,218,130,hyperparameters,sgd,as,optimizer,sgd as optimizer,0.5566533803939819
translation,218,130,hyperparameters,sgd,with,minibatch size,sgd with minibatch size,0.6135682463645935
translation,218,130,hyperparameters,sgd,with,initial learning rate,sgd with initial learning rate,0.5956356525421143
translation,218,130,hyperparameters,optimizer,with,minibatch size,optimizer with minibatch size,0.5723190307617188
translation,218,130,hyperparameters,minibatch size,of,64,minibatch size of 64,0.6199161410331726
translation,218,130,hyperparameters,initial learning rate,of,1.0,initial learning rate of 1.0,0.5808937549591064
translation,218,130,hyperparameters,hyperparameters,For,optimization,hyperparameters For optimization,0.5834490656852722
translation,218,131,hyperparameters,model,for,15 epochs,model for 15 epochs,0.6331382989883423
translation,218,131,hyperparameters,model,start halving,learning rate,model start halving learning rate,0.7969382405281067
translation,218,131,hyperparameters,learning rate,after,8th epoch,learning rate after 8th epoch,0.6651949882507324
translation,218,131,hyperparameters,hyperparameters,train,model,hyperparameters train model,0.7064553499221802
translation,218,132,hyperparameters,gradient norm upper bound,to,3,gradient norm upper bound to 3,0.5265867114067078
translation,218,132,hyperparameters,gradient norm upper bound,during,training,gradient norm upper bound during training,0.6658710241317749
translation,218,132,hyperparameters,hyperparameters,set,gradient norm upper bound,hyperparameters set gradient norm upper bound,0.6053789258003235
translation,218,133,hyperparameters,teacher - forcing,for,training,teacher - forcing for training,0.6302192211151123
translation,218,133,hyperparameters,hyperparameters,adopt,teacher - forcing,hyperparameters adopt teacher - forcing,0.6582732796669006
translation,218,134,hyperparameters,model,with,lowest perplexity,model with lowest perplexity,0.642731249332428
translation,218,134,hyperparameters,beam search,with,size 3,beam search with size 3,0.7223524451255798
translation,218,134,hyperparameters,size 3,employed for,generating questions,size 3 employed for generating questions,0.6586165428161621
translation,218,9,model,method,to jointly model,unstructured sentence and the structured answer-relevant relation,method to jointly model unstructured sentence and the structured answer-relevant relation,0.7130666971206665
translation,218,9,model,method,for,question generation,method for question generation,0.5202239751815796
translation,218,9,model,unstructured sentence and the structured answer-relevant relation,for,question generation,unstructured sentence and the structured answer-relevant relation for question generation,0.5821371078491211
translation,218,10,model,structured answer-relevant relation,acts as,to the point context,structured answer-relevant relation acts as to the point context,0.7000063061714172
translation,218,10,model,structured answer-relevant relation,helps keep,generated question,structured answer-relevant relation helps keep generated question,0.5431171655654907
translation,218,10,model,generated question,to,point,generated question to point,0.6087749600410461
translation,218,10,model,unstructured sentence,provides,full information,unstructured sentence provides full information,0.631130039691925
translation,218,10,model,model,has,structured answer-relevant relation,model has structured answer-relevant relation,0.5808717012405396
translation,218,49,model,dual copy mechanism,based on,encoder-decoder framework,dual copy mechanism based on encoder-decoder framework,0.6777201890945435
translation,218,49,model,dual copy mechanism,former learns to control,information flow,dual copy mechanism former learns to control information flow,0.7588845491409302
translation,218,49,model,information flow,between,unstructured and structured inputs,information flow between unstructured and structured inputs,0.6136742234230042
translation,218,49,model,words,from,two sources,words from two sources,0.618411660194397
translation,218,49,model,words,informativeness and faithfulness of,generated questions,words informativeness and faithfulness of generated questions,0.7075417637825012
translation,218,49,model,copy,has,words,copy has words,0.5742371678352356
translation,218,49,model,model,design,gated attention mechanism,model design gated attention mechanism,0.6082483530044556
translation,218,49,model,model,design,dual copy mechanism,model design dual copy mechanism,0.618659257888794
translation,218,51,results,improvement,is,more significant,improvement is more significant,0.5719013810157776
translation,218,51,results,more significant,with,larger relative distance,more significant with larger relative distance,0.6395497918128967
translation,218,51,results,larger relative distance,between,answer,larger relative distance between answer,0.6799497008323669
translation,218,51,results,larger relative distance,between,other non-stop sentence words,larger relative distance between other non-stop sentence words,0.6298922300338745
translation,218,51,results,results,demonstrate that,improvement,results demonstrate that improvement,0.6819401979446411
translation,218,139,results,our proposed model,combines,structured answer-relevant relations and unstructured sentences,our proposed model combines structured answer-relevant relations and unstructured sentences,0.6677996516227722
translation,218,139,results,our proposed model,achieves,significant improvements,our proposed model achieves significant improvements,0.6565694212913513
translation,218,139,results,structured answer-relevant relations and unstructured sentences,achieves,significant improvements,structured answer-relevant relations and unstructured sentences achieves significant improvements,0.6167578101158142
translation,218,139,results,significant improvements,over,proximity - based answer - aware models,significant improvements over proximity - based answer - aware models,0.6967856884002686
translation,218,139,results,results,has,our proposed model,results has our proposed model,0.5871988534927368
translation,218,144,results,proximity - based answeraware models,perform,poorly,proximity - based answeraware models perform poorly,0.6025301814079285
translation,218,144,results,poorly,when,distance,poorly when distance,0.7333024740219116
translation,218,144,results,distance,between,answer fragment and other non-stop sentence words,distance between answer fragment and other non-stop sentence words,0.6292532086372375
translation,218,144,results,question,is,large,question is large,0.6308184266090393
translation,218,144,results,results,existing,proximity - based answeraware models,results existing proximity - based answeraware models,0.6457158327102661
translation,218,147,results,outperforms,on,all ranges of relative distances,outperforms on all ranges of relative distances,0.5370400547981262
translation,218,147,results,hybrid,on,all ranges of relative distances,hybrid on all ranges of relative distances,0.5775389671325684
translation,218,147,results,our proposed model,has,outperforms,our proposed model has outperforms,0.6381282210350037
translation,218,147,results,outperforms,has,hybrid,outperforms has hybrid,0.6629340052604675
translation,218,147,results,results,find that,our proposed model,results find that our proposed model,0.6697368025779724
translation,218,148,results,performance difference,between,hybrid and our model,performance difference between hybrid and our model,0.621436357498169
translation,218,148,results,performance difference,find,improvements,performance difference find improvements,0.6058238744735718
translation,218,148,results,improvements,become,more significant,improvements become more significant,0.6041905283927917
translation,218,148,results,more significant,when,distance,more significant when distance,0.6996148228645325
translation,218,148,results,increases,from,  0 ? 10   to   > 10  ,increases from   0 ? 10   to   > 10  ,0.657503604888916
translation,218,148,results,distance,has,increases,distance has increases,0.5942487120628357
translation,218,148,results,results,comparing,performance difference,results comparing performance difference,0.7501069903373718
translation,219,5,model,novel and non-redundant community answer summary,segment,complex original multi-sentence question,novel and non-redundant community answer summary segment complex original multi-sentence question,0.655586838722229
translation,219,5,model,novel and non-redundant community answer summary,propose,general conditional random field ( crf ) based answer summary method,novel and non-redundant community answer summary propose general conditional random field ( crf ) based answer summary method,0.6028537750244141
translation,219,5,model,complex original multi-sentence question,into,several sub questions,complex original multi-sentence question into several sub questions,0.548349916934967
translation,219,5,model,general conditional random field ( crf ) based answer summary method,with,group l 1 regularization,general conditional random field ( crf ) based answer summary method with group l 1 regularization,0.5717226266860962
translation,219,5,model,model,to automatically generate,novel and non-redundant community answer summary,model to automatically generate novel and non-redundant community answer summary,0.7064654231071472
translation,219,7,model,four different types of contextual factors,namely,information novelty,four different types of contextual factors namely information novelty,0.675830066204071
translation,219,7,model,four different types of contextual factors,namely,non-redundancy modeling,four different types of contextual factors namely non-redundancy modeling,0.680532693862915
translation,219,7,model,non-redundancy modeling,for,local and non-local sentence interactions,non-redundancy modeling for local and non-local sentence interactions,0.6144505143165588
translation,219,7,model,local and non-local sentence interactions,under,question segmentation,local and non-local sentence interactions under question segmentation,0.6060905456542969
translation,219,7,model,model,explore,four different types of contextual factors,model explore four different types of contextual factors,0.6775774359703064
translation,219,8,model,potential,of,abundant cqa features,potential of abundant cqa features,0.5915433168411255
translation,219,8,model,potential,introduce,group l 1 regularization,potential introduce group l 1 regularization,0.6262749433517456
translation,219,8,model,abundant cqa features,introduce,group l 1 regularization,abundant cqa features introduce group l 1 regularization,0.6329022645950317
translation,219,8,model,group l 1 regularization,for,feature learning,group l 1 regularization for feature learning,0.5764145255088806
translation,219,8,model,model,To further unleash,potential,model To further unleash potential,0.7510063648223877
translation,219,8,model,model,introduce,group l 1 regularization,model introduce group l 1 regularization,0.6286295652389526
translation,219,49,model,answer summary task,as,sequential labeling process,answer summary task as sequential labeling process,0.5021637082099915
translation,219,49,model,sequential labeling process,under,general conditional random fields ( crf ) framework,sequential labeling process under general conditional random fields ( crf ) framework,0.5998361110687256
translation,219,49,model,sentences,with,summary label,sentences with summary label,0.6318958401679993
translation,219,49,model,summary label,to form,final summarized answer,summary label to form final summarized answer,0.7033214569091797
translation,219,49,model,model,tackle,answer summary task,model tackle answer summary task,0.7223578095436096
translation,219,50,model,general crf based framework,incorporate,four different contextual factors,general crf based framework incorporate four different contextual factors,0.6584550142288208
translation,219,50,model,four different contextual factors,based on,question segmentation,four different contextual factors based on question segmentation,0.6454129815101624
translation,219,50,model,four different contextual factors,to model,local and non-local semantic sentence interactions,four different contextual factors to model local and non-local semantic sentence interactions,0.6646714806556702
translation,219,50,model,local and non-local semantic sentence interactions,problem of,redundancy and information novelty,local and non-local semantic sentence interactions problem of redundancy and information novelty,0.7305364012718201
translation,219,50,model,model,present,general crf based framework,model present general crf based framework,0.618446409702301
translation,219,50,model,model,incorporate,four different contextual factors,model incorporate four different contextual factors,0.6650921106338501
translation,219,52,model,group l 1 - regularization approach,in,crf model,group l 1 - regularization approach in crf model,0.5102599859237671
translation,219,52,model,crf model,for,automatic optimal feature learning,crf model for automatic optimal feature learning,0.6243454217910767
translation,219,52,model,automatic optimal feature learning,to unleash,potential,automatic optimal feature learning to unleash potential,0.6673277616500854
translation,219,52,model,automatic optimal feature learning,enhance,performance,automatic optimal feature learning enhance performance,0.6145137548446655
translation,219,52,model,performance,of,answer summarization,performance of answer summarization,0.5872815251350403
translation,219,52,model,model,propose,group l 1 - regularization approach,model propose group l 1 - regularization approach,0.6652643084526062
translation,220,159,baselines,supervised clustering,original implementation of,latent svm struct 5 - lssvm,supervised clustering original implementation of latent svm struct 5 - lssvm,0.73875892162323
translation,220,159,baselines,supervised clustering,our implementation of,lsp algorithm,supervised clustering our implementation of lsp algorithm,0.7590591311454773
translation,220,159,baselines,lsp algorithm,based on,same clustering inference,lsp algorithm based on same clustering inference,0.6965427398681641
translation,220,159,baselines,same clustering inference,on,undirected graphs,same clustering inference on undirected graphs,0.5608920454978943
translation,220,159,baselines,same clustering inference,using,kruskal 's spanning algorithm,same clustering inference using kruskal 's spanning algorithm,0.6976187825202942
translation,220,159,baselines,undirected graphs,using,kruskal 's spanning algorithm,undirected graphs using kruskal 's spanning algorithm,0.5197066068649292
translation,220,159,baselines,baselines,To perform,supervised clustering,baselines To perform supervised clustering,0.6321035027503967
translation,220,160,baselines,spectral clustering,employ,implementation,spectral clustering employ implementation,0.5977156162261963
translation,220,160,baselines,relational k-means,has,"szalkai , 2013 )","relational k-means has szalkai , 2013 )",0.5887469053268433
translation,220,5,model,model,for,automatically clustering questions,model for automatically clustering questions,0.6362420916557312
translation,220,5,model,automatically clustering questions,into,user intents,automatically clustering questions into user intents,0.5751208662986755
translation,220,5,model,user intents,to help,design tasks,user intents to help design tasks,0.6035338044166565
translation,220,5,model,model,propose,model,model propose model,0.6740307211875916
translation,220,5,model,model,for,automatically clustering questions,model for automatically clustering questions,0.6362420916557312
translation,220,7,model,powerful semantic classifiers,from,question duplicate / matching research,powerful semantic classifiers from question duplicate / matching research,0.5196901559829712
translation,220,7,model,powerful semantic classifiers,along with,novel idea of supervised clustering methods,powerful semantic classifiers along with novel idea of supervised clustering methods,0.5997375249862671
translation,220,7,model,novel idea of supervised clustering methods,based on,structured output,novel idea of supervised clustering methods based on structured output,0.6729190349578857
translation,220,7,model,model,using,powerful semantic classifiers,model using powerful semantic classifiers,0.6406516432762146
translation,220,22,model,automatically clustering questions,into,user intent categories,automatically clustering questions into user intent categories,0.5577676892280579
translation,220,27,model,state - of - the - art methods,for,question similarity / paraphrasing,state - of - the - art methods for question similarity / paraphrasing,0.5576141476631165
translation,220,36,results,our new structured output method,for,question clustering,our new structured output method for question clustering,0.6417017579078674
translation,220,36,results,our new structured output method,provides,clustering accuracy,our new structured output method provides clustering accuracy,0.661067008972168
translation,220,36,results,our new structured output method,provides,still valuable accuracy,our new structured output method provides still valuable accuracy,0.6689860820770264
translation,220,36,results,all its competitors,with respect to,standard clustering accuracy / purity,all its competitors with respect to standard clustering accuracy / purity,0.6351964473724365
translation,220,36,results,measures,defined in,cr domain,measures defined in cr domain,0.6490419507026672
translation,220,36,results,clustering accuracy,of,80 %,clustering accuracy of 80 %,0.6064974069595337
translation,220,36,results,80 %,with respect to,original quora annotation,80 % with respect to original quora annotation,0.6487783789634705
translation,220,36,results,still valuable accuracy,of,65 %,still valuable accuracy of 65 %,0.5789902806282043
translation,220,36,results,65 %,with respect to,intent classes,65 % with respect to intent classes,0.6689539551734924
translation,220,36,results,intent classes,designed by,expert in dialog modeling,intent classes designed by expert in dialog modeling,0.628240168094635
translation,220,36,results,our new structured output method,has,highly outperforms,our new structured output method has highly outperforms,0.6241648197174072
translation,220,36,results,highly outperforms,has,all its competitors,highly outperforms has all its competitors,0.5869295597076416
translation,220,36,results,results,show,our new structured output method,results show our new structured output method,0.6331354379653931
translation,220,187,results,clustering accuracy,has,lssvm approach,clustering accuracy has lssvm approach,0.5564610362052917
translation,220,187,results,lssvm approach,has,outperforms,lssvm approach has outperforms,0.6279394030570984
translation,220,187,results,outperforms,has,all the clustering baselines,outperforms has all the clustering baselines,0.5894452929496765
translation,220,187,results,about 10 points,has,highest baseline model,about 10 points has highest baseline model,0.5682539939880371
translation,220,187,results,results,In terms of,clustering accuracy,results In terms of clustering accuracy,0.7114176750183105
translation,220,190,results,all the clustering models,show,high accuracy,all the clustering models show high accuracy,0.6009531021118164
translation,220,190,results,high accuracy,superior to,svm classifier,high accuracy superior to svm classifier,0.7399221062660217
translation,220,190,results,results,has,all the clustering models,results has all the clustering models,0.5424372553825378
translation,220,191,results,lssvm,approaches,classifier,lssvm approaches classifier,0.6556494832038879
translation,220,191,results,classifier,in terms of,classification f1,classifier in terms of classification f1,0.6914867162704468
translation,220,191,results,results,has,lssvm,results has lssvm,0.5517383813858032
translation,220,200,results,lsp model,scored,best,lsp model scored best,0.7550984621047974
translation,220,200,results,best,with respect to,new annotation,best with respect to new annotation,0.6741296052932739
translation,220,200,results,results,has,lsp model,results has lsp model,0.5469768047332764
translation,220,212,results,approaches,based on,spectral and k-means clustering,approaches based on spectral and k-means clustering,0.6993123292922974
translation,220,212,results,spectral and k-means clustering,on,quora dataset,spectral and k-means clustering on quora dataset,0.5628851056098938
translation,220,212,results,structural output model,has,consistently outperforms,structural output model has consistently outperforms,0.6200724244117737
translation,220,212,results,consistently outperforms,has,approaches,consistently outperforms has approaches,0.6483997702598572
translation,220,213,results,most prominent improvement,comes from,singleton clusters,most prominent improvement comes from singleton clusters,0.7020764946937561
translation,220,213,results,questions,not,duplicate,questions not duplicate,0.7070212960243225
translation,220,213,results,duplicate,with,any other entries,duplicate with any other entries,0.6483938097953796
translation,220,213,results,singleton clusters,has,questions,singleton clusters has questions,0.6598063111305237
translation,220,213,results,results,has,most prominent improvement,results has most prominent improvement,0.5222278833389282
translation,220,215,results,lssvm,correctly recovers,71 %,lssvm correctly recovers 71 %,0.7883632779121399
translation,220,215,results,71 %,of,singleton clusters,71 % of singleton clusters,0.5775080919265747
translation,220,215,results,other methods,perform,much worse ( 5 - 30 % ),other methods perform much worse ( 5 - 30 % ),0.5682622194290161
translation,220,215,results,results,has,lssvm,results has lssvm,0.5517383813858032
translation,220,218,results,problematic,for,all the compared methods,problematic for all the compared methods,0.6554593443870544
translation,220,218,results,results,has,larger clusters,results has larger clusters,0.5700570344924927
translation,220,219,results,structural clustering,doing,better job,structural clustering doing better job,0.6735230684280396
translation,220,219,results,better job,at recovering,non-singleton clusters,better job at recovering non-singleton clusters,0.7198605537414551
translation,220,219,results,ceaf score,has,structural clustering,ceaf score has structural clustering,0.5913485288619995
translation,221,31,model,local faithfulness,align,gradient - based visual explanations,local faithfulness align gradient - based visual explanations,0.6359310746192932
translation,221,31,model,gradient - based visual explanations,generated by,textual explanation module,gradient - based visual explanations generated by textual explanation module,0.6826916933059692
translation,221,31,model,gradient - based visual explanations,generated by,the vqa module,gradient - based visual explanations generated by the vqa module,0.6685249209403992
translation,221,31,model,the vqa module,during,training,the vqa module during training,0.653308093547821
translation,221,31,model,model,To enforce,local faithfulness,model To enforce local faithfulness,0.7845275402069092
translation,222,153,baselines,baseline models,test on,curated development set,baseline models test on curated development set,0.7471646666526794
translation,222,153,baselines,blanc,on,squad1.1,blanc on squad1.1,0.547125518321991
translation,222,153,baselines,curated development set,of,hotpotqa dataset,curated development set of hotpotqa dataset,0.5270549654960632
translation,222,153,baselines,baselines,train,baseline models,baselines train baseline models,0.7132120728492737
translation,222,24,experiments,context prediction task,to predict,softlabels,context prediction task to predict softlabels,0.6622596383094788
translation,222,24,experiments,softlabels,generated from,given answerspans,softlabels generated from given answerspans,0.6301013231277466
translation,222,30,experiments,blanc and baseline models,on,squad1.1,blanc and baseline models on squad1.1,0.531650960445404
translation,222,88,experiments,zero-shot,train,qa models,zero-shot train qa models,0.6659365892410278
translation,222,88,experiments,supporting facts prediction,train,qa models,supporting facts prediction train qa models,0.7214770913124084
translation,222,88,experiments,qa models,on,squad,qa models on squad,0.5973185896873474
translation,222,88,experiments,supporting facts ( supporting sentences ),of,answers,supporting facts ( supporting sentences ) of answers,0.5701804161071777
translation,222,88,experiments,answers,in,hotpotqa,answers in hotpotqa,0.5549865365028381
translation,222,88,experiments,zero-shot,has,supporting facts prediction,zero-shot has supporting facts prediction,0.5673223733901978
translation,222,116,hyperparameters,max sequence length,of,transformer encoder,max sequence length of transformer encoder,0.5862281322479248
translation,222,116,hyperparameters,transformer encoder,to,384,transformer encoder to 384,0.5874478816986084
translation,222,121,hyperparameters,hyperparameters,has,q,hyperparameters has q,0.5346919298171997
translation,222,121,hyperparameters,hyperparameters,has,window-size,hyperparameters has window-size,0.5174991488456726
translation,222,7,model,context prediction,as,auxiliary task,context prediction as auxiliary task,0.49162495136260986
translation,222,7,model,auxiliary task,in,multi-task learning manner,auxiliary task in multi-task learning manner,0.5265036821365356
translation,222,7,model,block attention method,learns,context prediction task,block attention method learns context prediction task,0.6898147463798523
translation,222,7,model,model,propose,blanc,model propose blanc,0.6856090426445007
translation,222,46,model,softlabeling method,for,context prediction,softlabeling method for context prediction,0.5376617908477783
translation,222,46,model,block attention method,predicts,soft-labels,block attention method predicts soft-labels,0.7127701044082642
translation,222,46,model,model,propose,blanc,model propose blanc,0.6856090426445007
translation,222,111,model,model,use,12 - layer spanbert - base,model use 12 - layer spanbert - base,0.6602223515510559
translation,222,26,results,blanc,Adding,context prediction and block attention,blanc Adding context prediction and block attention,0.6405383944511414
translation,222,26,results,blanc,to correctly identify,context,blanc to correctly identify context,0.6719650030136108
translation,222,26,results,context prediction and block attention,enhances,blanc,context prediction and block attention enhances blanc,0.6621172428131104
translation,222,26,results,blanc,to correctly identify,context,blanc to correctly identify context,0.6719650030136108
translation,222,26,results,context,related to,given question,context related to given question,0.7437187433242798
translation,222,26,results,results,Adding,context prediction and block attention,results Adding context prediction and block attention,0.5961965322494507
translation,222,129,results,all comparison models,including,roberta and spanbert,all comparison models including roberta and spanbert,0.709527850151062
translation,222,129,results,blanc,has,consistently outperforms,blanc has consistently outperforms,0.6204781532287598
translation,222,129,results,consistently outperforms,has,all comparison models,consistently outperforms has all comparison models,0.5695726275444031
translation,222,129,results,results,shows,blanc,results shows blanc,0.6636252403259277
translation,222,131,results,outperforms,by,1.86,outperforms by 1.86,0.6055997610092163
translation,222,131,results,spanbert,by,1.86,spanbert by 1.86,0.5576640963554382
translation,222,131,results,naturalq,has,blanc,naturalq has blanc,0.6559835076332092
translation,222,131,results,blanc,has,outperforms,blanc has outperforms,0.6296398043632507
translation,222,131,results,outperforms,has,spanbert,outperforms has spanbert,0.662398099899292
translation,222,131,results,results,On,naturalq,results On naturalq,0.5434762835502625
translation,222,132,results,blanc,outperforms by,2.56,blanc outperforms by 2.56,0.7151105999946594
translation,222,132,results,newsqa,has,blanc,newsqa has blanc,0.6691011190414429
translation,222,132,results,results,On,newsqa,results On newsqa,0.5673846006393433
translation,222,135,results,multi-mentioned answer,is,smallest,multi-mentioned answer is smallest,0.6184689998626709
translation,222,135,results,multi-mentioned answer,is,largest,multi-mentioned answer is largest,0.6294766068458557
translation,222,135,results,smallest,in,squad,smallest in squad,0.5339812636375427
translation,222,135,results,smallest,in,newsqa - mrqa,smallest in newsqa - mrqa,0.574631929397583
translation,222,135,results,medium,for,naturalq - mrqa,medium for naturalq - mrqa,0.6797752976417542
translation,222,135,results,largest,in,newsqa - mrqa,largest in newsqa - mrqa,0.5525103807449341
translation,222,135,results,results,proportion of,multi-mentioned answer,results proportion of multi-mentioned answer,0.7140370607376099
translation,222,136,results,reading comprehension results,show,performance gap,reading comprehension results show performance gap,0.5818239450454712
translation,222,136,results,performance gap,of,blanc and spanbert,performance gap of blanc and spanbert,0.5747441053390503
translation,222,136,results,blanc and spanbert,has,increases,blanc and spanbert has increases,0.6381357908248901
translation,222,136,results,results,has,reading comprehension results,results has reading comprehension results,0.49134841561317444
translation,222,142,results,blanc,show that,performance gain,blanc show that performance gain,0.5464625954627991
translation,222,142,results,spanbert and bert,across,all subsets,spanbert and bert across all subsets,0.7151312232017517
translation,222,142,results,blanc,has,outperforms,blanc has outperforms,0.6296398043632507
translation,222,142,results,outperforms,has,spanbert and bert,outperforms has spanbert and bert,0.6671551465988159
translation,222,142,results,performance gain,has,increases,performance gain has increases,0.5932958126068115
translation,222,142,results,results,has,blanc,results has blanc,0.5666577816009521
translation,222,143,results,reading comprehension performance,of,blanc,reading comprehension performance of blanc,0.5258169770240784
translation,222,143,results,blanc,on,question - answer pairs,blanc on question - answer pairs,0.5910654664039612
translation,222,143,results,question - answer pairs,of,passages,question - answer pairs of passages,0.6137946844100952
translation,222,143,results,passages,with,n,passages with n,0.6489284634590149
translation,222,143,results,block attention method,increases,context - aware performance,block attention method increases context - aware performance,0.6950204968452454
translation,222,143,results,context - aware performance,of,spanbert,context - aware performance of spanbert,0.5809611082077026
translation,222,143,results,spanbert,by,3.6,spanbert by 3.6,0.5759047269821167
translation,222,143,results,spanbert,by,3.8,spanbert by 3.8,0.5818707942962646
translation,222,143,results,3.6,with,span - f1,3.6 with span - f1,0.6224002242088318
translation,222,143,results,3.8,with,span -em,3.8 with span -em,0.6504849195480347
translation,222,143,results,results,show,reading comprehension performance,results show reading comprehension performance,0.4683324992656708
translation,222,154,results,blanc,sentence relevant to,given question,blanc sentence relevant to given question,0.7085167765617371
translation,222,154,results,results,shows,blanc,results shows blanc,0.6636252403259277
translation,222,159,results,context word prediction task,increases,reading comprehension performance,context word prediction task increases reading comprehension performance,0.6012868285179138
translation,222,159,results,results,Leveraging,context word prediction task,results Leveraging context word prediction task,0.5814616084098816
translation,223,71,ablation-analysis,subtask a,has,"all the features ( e.g. , wm , tmb , md , ci and lss )","subtask a has all the features ( e.g. , wm , tmb , md , ci and lss )",0.5505720376968384
translation,223,71,ablation-analysis,ablation analysis,For,subtask a,ablation analysis For subtask a,0.6201931238174438
translation,223,10,experiments,subtask a,built,convolutional neural network ( cnn ) model,subtask a built convolutional neural network ( cnn ) model,0.6317922472953796
translation,223,10,experiments,convolutional neural network ( cnn ) model,to learn,joint representation,convolutional neural network ( cnn ) model to learn joint representation,0.593125581741333
translation,223,10,experiments,joint representation,for,question -comment ( q- c ) pair,joint representation for question -comment ( q- c ) pair,0.6247062087059021
translation,223,36,model,lexical semantic similarity feature ( lss ),included,lexical semantic similarity feature,lexical semantic similarity feature ( lss ) included lexical semantic similarity feature,0.5526102185249329
translation,223,36,model,lexical semantic similarity feature,in,our model,lexical semantic similarity feature in our model,0.49103817343711853
translation,223,36,model,model,has,lexical semantic similarity feature ( lss ),model has lexical semantic similarity feature ( lss ),0.5731220841407776
translation,223,57,model,softmax output,of,good classes,softmax output of good classes,0.544641375541687
translation,223,57,model,good classes,regarded as,ranking score,good classes regarded as ranking score,0.5405521988868713
translation,223,57,model,ranking score,between,question and comment,ranking score between question and comment,0.6151330471038818
translation,223,57,model,question and comment,by,simple hidden layer,question and comment by simple hidden layer,0.581860363483429
translation,223,57,model,simple hidden layer,building on,concatenation,simple hidden layer building on concatenation,0.7215718626976013
translation,223,57,model,simple hidden layer,building on,softmax operation,simple hidden layer building on softmax operation,0.7077591419219971
translation,223,57,model,concatenation,of,two feature vectors,concatenation of two feature vectors,0.5815221667289734
translation,223,57,model,concatenation,of,softmax operation,concatenation of softmax operation,0.5470796823501587
translation,223,57,model,model,has,softmax output,model has softmax output,0.5729582905769348
translation,223,72,results,cnn based model,achieves,comparable performance,cnn based model achieves comparable performance,0.676281750202179
translation,223,72,results,cnn based model,with,average value of scores,cnn based model with average value of scores,0.6265460252761841
translation,223,72,results,comparable performance,with,traditional method,comparable performance with traditional method,0.6536206007003784
translation,223,72,results,results,has,cnn based model,results has cnn based model,0.5846958160400391
translation,223,73,results,three algorithms,such as,logistic regression,three algorithms such as logistic regression,0.6504650712013245
translation,223,73,results,three algorithms,such as,adaboost,three algorithms such as adaboost,0.5603752732276917
translation,223,73,results,three algorithms,such as,random forest,three algorithms such as random forest,0.5479035377502441
translation,223,73,results,three algorithms,achieve,comparable results,three algorithms achieve comparable results,0.5755093097686768
translation,223,73,results,comparable results,with,traditional nlp features,comparable results with traditional nlp features,0.6584922075271606
translation,223,73,results,subtask b,has,three algorithms,subtask b has three algorithms,0.6039949655532837
translation,223,73,results,results,For,subtask b,results For subtask b,0.5699940323829651
translation,223,74,results,lr,with,all features,lr with all features,0.6650501489639282
translation,223,74,results,lr,achieve,best performance,lr achieve best performance,0.6485790014266968
translation,223,74,results,all features,achieve,best performance,all features achieve best performance,0.6340962052345276
translation,223,74,results,results,has,lr,results has lr,0.47720572352409363
translation,223,75,results,adaboost,with,all features ( excluding md feature ),adaboost with all features ( excluding md feature ),0.5986830592155457
translation,223,75,results,adaboost,makes,best result,adaboost makes best result,0.6217761635780334
translation,223,75,results,best result,compared with,random forest and logistic regression,best result compared with random forest and logistic regression,0.6601002812385559
translation,223,75,results,subtask c,has,adaboost,subtask c has adaboost,0.5718128085136414
translation,223,75,results,results,For,subtask c,results For subtask c,0.5909541249275208
translation,224,183,ablation-analysis,distractor setting,to,full wiki setting,distractor setting to full wiki setting,0.5233713388442993
translation,224,183,ablation-analysis,distractor setting,expanding,scope,distractor setting expanding scope,0.7649906873703003
translation,224,183,ablation-analysis,scope,of,context,scope of context,0.5926679968833923
translation,224,183,ablation-analysis,ablation analysis,From,distractor setting,ablation analysis From distractor setting,0.5367690324783325
translation,224,191,ablation-analysis,performance,of,bridge entity questions,performance of bridge entity questions,0.5807387828826904
translation,224,191,ablation-analysis,performance,of,comparison questions,performance of comparison questions,0.573462963104248
translation,224,191,ablation-analysis,full wiki setting,has,performance,full wiki setting has performance,0.5601704120635986
translation,224,191,ablation-analysis,bridge entity questions,has,drops significantly,bridge entity questions has drops significantly,0.6338798999786377
translation,224,191,ablation-analysis,comparison questions,has,decreases,comparison questions has decreases,0.6107689738273621
translation,224,191,ablation-analysis,decreases,has,only marginally,decreases has only marginally,0.6175348162651062
translation,224,191,ablation-analysis,ablation analysis,In,full wiki setting,ablation analysis In full wiki setting,0.47684043645858765
translation,224,194,ablation-analysis,ablation study,in,distractor setting,ablation study in distractor setting,0.512902557849884
translation,224,194,ablation-analysis,ablation analysis,perform,ablation study,ablation analysis perform ablation study,0.62177574634552
translation,224,194,ablation-analysis,ablation analysis,in,distractor setting,ablation analysis in distractor setting,0.5087473392486572
translation,224,195,ablation-analysis,selfattention and character - level models,contribute notably,final performance,selfattention and character - level models contribute notably final performance,0.6415902376174927
translation,224,195,ablation-analysis,ablation analysis,has,selfattention and character - level models,ablation analysis has selfattention and character - level models,0.5474549531936646
translation,224,197,ablation-analysis,strong supervision,over,supporting facts,strong supervision over supporting facts,0.6816391944885254
translation,224,197,ablation-analysis,supporting facts,has,decreases,supporting facts has decreases,0.6060301065444946
translation,224,197,ablation-analysis,decreases,has,performance,decreases has performance,0.5981842875480652
translation,224,197,ablation-analysis,ablation analysis,removing,strong supervision,ablation analysis removing strong supervision,0.6588284969329834
translation,224,212,ablation-analysis,distractor paragraphs,are,performance gap,distractor paragraphs are performance gap,0.5541980266571045
translation,224,212,ablation-analysis,distractor paragraphs,present,performance gap,distractor paragraphs present performance gap,0.6998472213745117
translation,224,212,ablation-analysis,performance gap,between,baseline model and the crowd worker,performance gap between baseline model and the crowd worker,0.6100124716758728
translation,224,212,ablation-analysis,baseline model and the crowd worker,on,both tasks,baseline model and the crowd worker on both tasks,0.5241646766662598
translation,224,212,ablation-analysis,?30 %,for,em and f 1,?30 % for em and f 1,0.7043935656547546
translation,224,212,ablation-analysis,distractor paragraphs,has,performance gap,distractor paragraphs has performance gap,0.56361985206604
translation,224,212,ablation-analysis,ablation analysis,When,distractor paragraphs,ablation analysis When distractor paragraphs,0.6475396752357483
translation,224,36,experiments,hotpotqa,collected by,crowdsourcing,hotpotqa collected by crowdsourcing,0.6638162732124329
translation,224,36,experiments,crowdsourcing,based on,wikipedia articles,crowdsourcing based on wikipedia articles,0.6458718776702881
translation,224,36,experiments,wikipedia articles,where,crowd workers,wikipedia articles where crowd workers,0.5394521951675415
translation,224,36,experiments,asked explicitly,to come up with,questions,asked explicitly to come up with questions,0.6931899189949036
translation,224,36,experiments,questions,requiring,reasoning,questions requiring reasoning,0.7250780463218689
translation,224,36,experiments,reasoning,about,all of the documents,reasoning about all of the documents,0.6478838920593262
translation,224,233,experiments,hotpotqa,has,large-scale question answering,hotpotqa has large-scale question answering,0.4711304008960724
translation,224,5,model,sentence - level supporting facts,required for,reasoning,sentence - level supporting facts required for reasoning,0.6772413849830627
translation,224,251,model,parlai,collecting,question answer pairs,parlai collecting question answer pairs,0.717540979385376
translation,224,251,model,parlai,by converting,collection workflow,parlai by converting collection workflow,0.6791623830795288
translation,224,251,model,question answer pairs,by converting,collection workflow,question answer pairs by converting collection workflow,0.6813824772834778
translation,224,251,model,collection workflow,into,systemoriented dialog,collection workflow into systemoriented dialog,0.5863215327262878
translation,224,251,model,model,adapt,parlai,model adapt parlai,0.7720003128051758
translation,224,35,results,hotpotqa,has,large-scale dataset,hotpotqa has large-scale dataset,0.5227396488189697
translation,224,35,results,results,present,hotpotqa,results present hotpotqa,0.622154951095581
translation,224,153,results,our implementation,without,weight averaging,our implementation without weight averaging,0.6871203184127808
translation,224,153,results,our implementation,achieves,performance,our implementation achieves performance,0.6747048497200012
translation,224,153,results,performance,very close to,squad,performance very close to squad,0.7204465866088867
translation,224,153,results,results,note,our implementation,results note our implementation,0.5733577013015747
translation,224,184,results,performance,in,full wiki setting,performance in full wiki setting,0.517808198928833
translation,224,184,results,performance,is,substantially lower,performance is substantially lower,0.5943140387535095
translation,224,184,results,full wiki setting,is,substantially lower,full wiki setting is substantially lower,0.5676923394203186
translation,224,184,results,results,has,performance,results has performance,0.5972660779953003
translation,224,185,results,model performance,in,all settings,model performance in all settings,0.5252467393875122
translation,224,185,results,significantly lower,than,human performance,significantly lower than human performance,0.6210573315620422
translation,224,185,results,results,has,model performance,results has model performance,0.5472817420959473
translation,224,187,results,our model,achieves,60 + supporting fact prediction f 1,our model achieves 60 + supporting fact prediction f 1,0.6721637845039368
translation,224,187,results,results,has,our model,results has our model,0.5871725678443909
translation,224,193,results,results,deterioration in,full wiki setting,results deterioration in full wiki setting,0.6184462904930115
translation,224,204,results,supporting facts supervision,is,most likely suboptimal,supporting facts supervision is most likely suboptimal,0.5445067882537842
translation,224,205,results,"all data splits ( train - easy , train-medium , and train- hard )",yields,best performance,"all data splits ( train - easy , train-medium , and train- hard ) yields best performance",0.6936073899269104
translation,224,205,results,best performance,adopted as,default setting,best performance adopted as default setting,0.632506787776947
translation,224,205,results,results,combining,"all data splits ( train - easy , train-medium , and train- hard )","results combining all data splits ( train - easy , train-medium , and train- hard )",0.6725188493728638
translation,224,210,results,original crowd worker,achieves,very high performance,original crowd worker achieves very high performance,0.6468686461448669
translation,224,210,results,very high performance,in,finding,very high performance in finding,0.5573768615722656
translation,224,210,results,very high performance,in,answering,very high performance in answering,0.5657474994659424
translation,224,210,results,finding,has,supporting facts,finding has supporting facts,0.6073393821716309
translation,224,210,results,answering,has,question,answering has question,0.638900101184845
translation,224,210,results,answering,has,correctly,answering has correctly,0.6048641204833984
translation,224,210,results,question,has,correctly,question has correctly,0.6134266257286072
translation,225,7,model,adversarial writing setting,where,humans,adversarial writing setting where humans,0.5371387600898743
translation,225,7,model,humans,interact with,trained models,humans interact with trained models,0.6665739417076111
translation,225,7,model,model,develop,adversarial writing setting,model develop adversarial writing setting,0.5298879742622375
translation,225,18,model,user interface,that allows,question writers,user interface that allows question writers,0.6634148359298706
translation,225,18,model,question writers,to adversarially craft,questions,question writers to adversarially craft questions,0.6200689673423767
translation,225,18,model,model,develop,user interface,model develop user interface,0.6459497213363647
translation,225,98,model,model,consider,two neural models,model consider two neural models,0.6982512474060059
translation,225,99,model,answer entities,has,question is about,answer entities has question is about,0.661018967628479
translation,225,107,results,buzz position,of,all models,buzz position of all models,0.6023766994476318
translation,225,107,results,significantly degrades,on,challenge set,significantly degrades on challenge set,0.5476115345954895
translation,225,107,results,all models,has,significantly degrades,all models has significantly degrades,0.5795802474021912
translation,225,107,results,results,has,buzz position,results has buzz position,0.5553109645843506
translation,225,112,results,decreased more in absolute accuracy,than,ir system,decreased more in absolute accuracy than ir system,0.6191169619560242
translation,225,112,results,two neural models,has,decreased more in absolute accuracy,two neural models has decreased more in absolute accuracy,0.5753808617591858
translation,225,113,results,largest absolute accuracy decrease,from,54.1 %,largest absolute accuracy decrease from 54.1 %,0.500657856464386
translation,225,113,results,54.1 %,to 32.4 %,full question,54.1 % to 32.4 % full question,0.778917133808136
translation,225,113,results,dan model,has,largest absolute accuracy decrease,dan model has largest absolute accuracy decrease,0.5497944355010986
translation,225,113,results,results,has,dan model,results has dan model,0.5482413172721863
translation,226,6,experiments,pragmatic questions,use,reinforcement learning,pragmatic questions use reinforcement learning,0.5633063316345215
translation,226,6,experiments,reinforcement learning,to optimize,informativeness metric,reinforcement learning to optimize informativeness metric,0.643403947353363
translation,226,6,experiments,informativeness metric,combined with,reward function,informativeness metric combined with reward function,0.650076150894165
translation,226,6,experiments,reward function,designed to promote,more specific questions,reward function designed to promote more specific questions,0.6722784042358398
translation,226,135,hyperparameters,adam optimizer,with,default hyperparameters,adam optimizer with default hyperparameters,0.5535434484481812
translation,226,135,hyperparameters,adam optimizer,anneal,learning rate,adam optimizer anneal learning rate,0.6712791919708252
translation,226,135,hyperparameters,default hyperparameters,to train and finetune,our question generator,default hyperparameters to train and finetune our question generator,0.7384634017944336
translation,226,135,hyperparameters,learning rate,by,0.5,learning rate by 0.5,0.5889546871185303
translation,226,135,hyperparameters,learning rate,whenever,dev performance,learning rate whenever dev performance,0.6170015335083008
translation,226,135,hyperparameters,0.5,whenever,dev performance,0.5 whenever dev performance,0.6212207674980164
translation,226,135,hyperparameters,does not improve,for,more than 3 consecutive epochs,does not improve for more than 3 consecutive epochs,0.6353325247764587
translation,226,135,hyperparameters,dev performance,has,does not improve,dev performance has does not improve,0.6500357389450073
translation,226,135,hyperparameters,hyperparameters,use,adam optimizer,hyperparameters use adam optimizer,0.6090166568756104
translation,226,17,model,question generator,generates,questions,question generator generates questions,0.7071086764335632
translation,226,17,model,questions,to gather,new information,questions to gather new information,0.6880342364311218
translation,226,17,model,new information,in,conversation,new information in conversation,0.549842357635498
translation,226,17,model,reasons,has,pragmatically,reasons has pragmatically,0.5676814317703247
translation,226,17,model,model,build,question generator,model build question generator,0.7651869654655457
translation,226,17,model,model,generates,questions,model generates questions,0.6617658734321594
translation,226,19,model,baseline question generation model,that generates,questions,baseline question generation model that generates questions,0.6247699856758118
translation,226,19,model,questions,in,conversation,questions in conversation,0.54798424243927
translation,226,19,model,questions,without conditioning on,unseen knowledge,questions without conditioning on unseen knowledge,0.7946875691413879
translation,226,64,model,two automatic reward functions,evaluate,informativeness and specificity,two automatic reward functions evaluate informativeness and specificity,0.5927361249923706
translation,226,64,model,two automatic reward functions,optimizing them with,reinforcement learning,two automatic reward functions optimizing them with reinforcement learning,0.7070972919464111
translation,226,64,model,informativeness and specificity,of,questions,informativeness and specificity of questions,0.5962883830070496
translation,226,64,model,model,proposing,two automatic reward functions,model proposing two automatic reward functions,0.6610706448554993
translation,226,145,results,baseline model and our pragmatically finetuned model,achieve,comparable performance,baseline model and our pragmatically finetuned model achieve comparable performance,0.6057994961738586
translation,226,145,results,comparable performance,evaluated against,reference question,comparable performance evaluated against reference question,0.7663273215293884
translation,226,145,results,comparable performance,evaluated against,reference question,comparable performance evaluated against reference question,0.7663273215293884
translation,226,145,results,reference question,using,n-gram overlap metrics ( rouge - l ),reference question using n-gram overlap metrics ( rouge - l ),0.6417427062988281
translation,226,145,results,perplexity,of,reference question,perplexity of reference question,0.5677586793899536
translation,226,147,results,finetuned model,improves upon,baseline model,finetuned model improves upon baseline model,0.71879643201828
translation,226,147,results,baseline model,on,informativeness and specificity,baseline model on informativeness and specificity,0.5063066482543945
translation,226,147,results,baseline model,both,informativeness and specificity,baseline model both informativeness and specificity,0.6504630446434021
translation,226,147,results,results,see,finetuned model,results see finetuned model,0.6023865342140198
translation,226,148,results,reference questions,are,only about as informative,reference questions are only about as informative,0.5979776978492737
translation,226,148,results,only about as informative,as,our baseline questions on average,only about as informative as our baseline questions on average,0.5346699357032776
translation,226,152,results,our model,sees,substantial improvements,our model sees substantial improvements,0.6474709510803223
translation,226,152,results,substantial improvements,on,proposed informativeness and specificity metrics,substantial improvements on proposed informativeness and specificity metrics,0.516204833984375
translation,226,152,results,results,show,our model,results show our model,0.6888449192047119
translation,227,151,baselines,paralex,using,only the initial seed lexicon,paralex using only the initial seed lexicon,0.6757538318634033
translation,227,178,hyperparameters,parameter learning,use,initial weight vector ? 0 = 0,parameter learning use initial weight vector ? 0 = 0,0.6107017993927002
translation,227,178,hyperparameters,parameter learning,shard,training data,parameter learning shard training data,0.6968048810958862
translation,227,178,hyperparameters,initial weight vector ? 0 = 0,use,t = 20 iterations,initial weight vector ? 0 = 0 use t = 20 iterations,0.574576199054718
translation,227,178,hyperparameters,training data,into,k = 10 pieces,training data into k = 10 pieces,0.5681709051132202
translation,227,178,hyperparameters,hyperparameters,For,parameter learning,hyperparameters For parameter learning,0.570834219455719
translation,227,5,model,learn,has,semantic lexicon,learn has semantic lexicon,0.5402593612670898
translation,227,5,model,learn,has,linear ranking function,learn has linear ranking function,0.5184623003005981
translation,227,5,model,linear ranking function,has,without manually annotating questions,linear ranking function has without manually annotating questions,0.5818130373954773
translation,227,12,model,"large , noisy , questionparaphrase corpus",where,question clusters,"large , noisy , questionparaphrase corpus where question clusters",0.5641555190086365
translation,227,12,model,question clusters,have,common but unknown query,question clusters have common but unknown query,0.6045414209365845
translation,227,20,model,lexical structures,combined to build,queries,lexical structures combined to build queries,0.6734049320220947
translation,227,20,model,queries,for,unseen questions,queries for unseen questions,0.6410393714904785
translation,227,24,model,learned word alignments,to aggressively generalize,seeds,learned word alignments to aggressively generalize seeds,0.7061198353767395
translation,227,24,model,learned word alignments,producing,large set of possible lexical equivalences,learned word alignments producing large set of possible lexical equivalences,0.6460767388343811
translation,227,24,model,seeds,producing,large set of possible lexical equivalences,seeds producing large set of possible lexical equivalences,0.7160617113113403
translation,227,25,model,linear ranking model,to filter,learned lexical equivalences,linear ranking model to filter learned lexical equivalences,0.6132911443710327
translation,227,25,model,model,learn,linear ranking model,model learn linear ranking model,0.638979971408844
translation,227,188,results,outperforms,in terms of both,f1 and map,outperforms in terms of both f1 and map,0.7150481343269348
translation,227,188,results,baseline systems,in terms of both,f1 and map,baseline systems in terms of both f1 and map,0.6774404644966125
translation,227,188,results,paralex,has,outperforms,paralex has outperforms,0.6487653255462646
translation,227,188,results,outperforms,has,baseline systems,outperforms has baseline systems,0.5976791977882385
translation,227,188,results,results,has,paralex,results has paralex,0.569782555103302
translation,227,189,results,lexicon- learning algorithm,boosts,recall,lexicon- learning algorithm boosts recall,0.7293082475662231
translation,227,189,results,recall,by,factor of 4,recall by factor of 4,0.6103642582893372
translation,227,189,results,factor of 4,over,initial lexicon,factor of 4 over initial lexicon,0.694299578666687
translation,227,189,results,results,has,lexicon- learning algorithm,results has lexicon- learning algorithm,0.565049946308136
translation,227,190,results,parameter - learning algorithm,results in,large gain,parameter - learning algorithm results in large gain,0.6082682013511658
translation,227,190,results,large gain,in,precision and recall,large gain in precision and recall,0.5587195754051208
translation,227,190,results,large gain,both,precision and recall,large gain both precision and recall,0.6818123459815979
translation,227,190,results,inducelex,generates,noisy set of patterns,inducelex generates noisy set of patterns,0.6738062500953674
translation,227,190,results,results,has,parameter - learning algorithm,results has parameter - learning algorithm,0.5612843036651611
translation,227,193,results,recall,of,system,recall of system,0.6522735357284546
translation,227,193,results,learned 2argument question templates,has,significantly increase,learned 2argument question templates has significantly increase,0.5618614554405212
translation,227,193,results,significantly increase,has,recall,significantly increase has recall,0.5792211294174194
translation,227,193,results,results,has,learned 2argument question templates,results has learned 2argument question templates,0.5893520712852478
translation,228,234,ablation-analysis,each component,in,our model,each component in our model,0.5248529314994812
translation,228,234,ablation-analysis,each component,plays,important part,each component plays important part,0.6598408818244934
translation,228,234,ablation-analysis,ablation analysis,has,each component,ablation analysis has each component,0.5230991840362549
translation,228,238,ablation-analysis,architecture,of,non-auto model,architecture of non-auto model,0.5595002174377441
translation,228,238,ablation-analysis,performance,has,drops significantly,performance has drops significantly,0.611855685710907
translation,228,188,baselines,"seq2seq ( du et al. , 2017 ) model",pioneered,nn - based qg,"seq2seq ( du et al. , 2017 ) model pioneered nn - based qg",0.7030718326568604
translation,228,188,baselines,"copy - net ( see et al. , 2017 ) model",introduced,pointer mechanism,"copy - net ( see et al. , 2017 ) model introduced pointer mechanism",0.6231308579444885
translation,228,188,baselines,"copy - net ( see et al. , 2017 ) model",introduced,copy mechanism,"copy - net ( see et al. , 2017 ) model introduced copy mechanism",0.6296337842941284
translation,228,188,baselines,"corenqg ( du and cardie , 2018 )",used,"hybrid features ( word , answer and coreference embeddings )","corenqg ( du and cardie , 2018 ) used hybrid features ( word , answer and coreference embeddings )",0.507664144039154
translation,228,188,baselines,"hybrid features ( word , answer and coreference embeddings )",for,encoder,"hybrid features ( word , answer and coreference embeddings ) for encoder",0.5840182900428772
translation,228,188,baselines,copy mechanism,for,decoder,copy mechanism for decoder,0.6608333587646484
translation,228,55,experiments,https,:,github.com,https : github.com,0.6102899312973022
translation,228,55,experiments,https,:,chaiz-pku/sequential,https : chaiz-pku/sequential,0.7200927138328552
translation,228,55,experiments,https,//,github.com,https // github.com,0.6462398171424866
translation,228,55,experiments,https,//,chaiz-pku/sequential,https // chaiz-pku/sequential,0.7455282211303711
translation,228,55,experiments,https,/,chaiz-pku/sequential,https / chaiz-pku/sequential,0.767478883266449
translation,228,55,experiments,github.com,/,chaiz-pku/sequential,github.com / chaiz-pku/sequential,0.6498515009880066
translation,228,55,experiments,chaiz-pku/sequential,has,-qg.,chaiz-pku/sequential has -qg.,0.6304133534431458
translation,228,10,model,questions,into,different groups,questions into different groups,0.5838528871536255
translation,228,10,model,each group,in,parallel,each group in parallel,0.5844091773033142
translation,228,10,model,model,divides,questions,model divides questions,0.7002782821655273
translation,228,10,model,model,generates,each group,model generates each group,0.6969581246376038
translation,228,11,model,two graphs,focusing on,information,two graphs focusing on information,0.7042801976203918
translation,228,11,model,two graphs,performs,dual- graph interaction,two graphs performs dual- graph interaction,0.5714862942695618
translation,228,11,model,information,from,"passages , answers","information from passages , answers",0.5556559562683105
translation,228,11,model,information,for,generation,information for generation,0.6364208459854126
translation,228,11,model,dual- graph interaction,to get,information,dual- graph interaction to get information,0.657252848148346
translation,228,11,model,information,for,generation,information for generation,0.6364208459854126
translation,228,11,model,model,builds,two graphs,model builds two graphs,0.640965461730957
translation,228,12,model,model,design,answer -aware attention mechanism,model design answer -aware attention mechanism,0.5856298208236694
translation,228,224,model,attention - aware heads,in,rationale encoder,attention - aware heads in rationale encoder,0.5216543674468994
translation,228,224,model,attention - aware heads,to get,uni-heads model,attention - aware heads to get uni-heads model,0.6030610203742981
translation,228,224,model,model,discard,attention - aware heads,model discard attention - aware heads,0.7335599064826965
translation,228,237,model,coarse- to-fine scenario,helps to better deal with,dependencies,coarse- to-fine scenario helps to better deal with dependencies,0.7360053658485413
translation,228,237,model,dependencies,between,questions,dependencies between questions,0.695801317691803
translation,228,237,model,model,has,coarse- to-fine scenario,model has coarse- to-fine scenario,0.5788937211036682
translation,228,199,results,our semi-autoregressive model,has,outperformed,our semi-autoregressive model has outperformed,0.5809358954429626
translation,228,199,results,outperformed,has,other methods,outperformed has other methods,0.615522027015686
translation,228,199,results,other methods,has,substantially,other methods has substantially,0.5453775525093079
translation,228,199,results,results,has,our semi-autoregressive model,results has our semi-autoregressive model,0.5691143274307251
translation,228,200,results,second and third groups,find that,models,second and third groups find that models,0.6804451942443848
translation,228,200,results,sqg,as,multi-turn dialog generation tasks,sqg as multi-turn dialog generation tasks,0.5085517764091492
translation,228,200,results,results,focus on,second and third groups,results focus on second and third groups,0.6539008617401123
translation,228,203,results,first and third groups,of,baselines,first and third groups of baselines,0.5463560819625854
translation,228,203,results,sqg models,show,more advantages,sqg models show more advantages,0.6769882440567017
translation,228,203,results,more advantages,than,tqg models,more advantages than tqg models,0.5798183083534241
translation,228,203,results,results,compare,first and third groups,results compare first and third groups,0.6083611845970154
translation,228,204,results,corefnet,gets,better performance,corefnet gets better performance,0.6385148167610168
translation,228,204,results,better performance,among,all baselines,better performance among all baselines,0.5655410289764404
translation,228,204,results,better performance,especially,redr,better performance especially redr,0.6602020263671875
translation,228,204,results,results,has,corefnet,results has corefnet,0.5567245483398438
translation,228,205,results,target answers,as,inputs,target answers as inputs,0.5192643404006958
translation,228,205,results,results,comparing with,implicitly performing,results comparing with implicitly performing,0.727501392364502
translation,228,206,results,performance,between,sqg task,performance between sqg task,0.5764159560203552
translation,228,206,results,performance,between,tqg task,performance between tqg task,0.6119703650474548
translation,228,206,results,performance,under,same model,performance under same model,0.7080623507499695
translation,228,206,results,tqg task,under,same model,tqg task under same model,0.6565194129943848
translation,228,206,results,evaluation scores,for,tqg tasks,evaluation scores for tqg tasks,0.5644065737724304
translation,228,206,results,tqg tasks,are,much higher,tqg tasks are much higher,0.6079243421554565
translation,228,206,results,performance,has,evaluation scores,performance has evaluation scores,0.5355499982833862
translation,228,206,results,results,compare,performance,results compare performance,0.7135117650032043
translation,228,235,results,more interacted,by,our dual- graph interaction scenario,more interacted by our dual- graph interaction scenario,0.5990525484085083
translation,228,235,results,more interacted,is,more powerful,more interacted is more powerful,0.5444971323013306
translation,228,235,results,independently updating,has,passage - info graph,independently updating has passage - info graph,0.5554555654525757
translation,228,235,results,independently updating,has,answer - info graph,independently updating has answer - info graph,0.5792894959449768
translation,228,235,results,results,for,no interact model,results for no interact model,0.6332437992095947
translation,228,236,results,uni-graph model,removing,passage encoder,uni-graph model removing passage encoder,0.6809990406036377
translation,228,236,results,uni-heads model,discarding,our answer -aware attention mechanism,uni-heads model discarding our answer -aware attention mechanism,0.7464083433151245
translation,228,236,results,significant worse performance,compared with,our full model,significant worse performance compared with our full model,0.6728829145431519
translation,228,236,results,results,has,uni-graph model,results has uni-graph model,0.5422334671020508
translation,228,239,results,dualgraph interaction,makes,performance,dualgraph interaction makes performance,0.5940082669258118
translation,228,239,results,better,than,seq2seq and copynet,better than seq2seq and copynet,0.5902263522148132
translation,228,239,results,performance,has,better,performance has better,0.6199294924736023
translation,228,239,results,results,has,dualgraph interaction,results has dualgraph interaction,0.5017957091331482
translation,229,82,baselines,paragraph ranker + full agg.,aggregates,answers,paragraph ranker + full agg. aggregates answers,0.7017895579338074
translation,229,82,baselines,baselines,has,paragraph ranker + full agg.,baselines has paragraph ranker + full agg.,0.5729441046714783
translation,229,73,experimental-setup,paragraph ranker,uses,3 - layer bi-lstm networks,paragraph ranker uses 3 - layer bi-lstm networks,0.5325748920440674
translation,229,73,experimental-setup,3 - layer bi-lstm networks,with,128 hidden units,3 - layer bi-lstm networks with 128 hidden units,0.5907941460609436
translation,229,76,experimental-setup,n = 20,for,number of documents,n = 20 for number of documents,0.5615279078483582
translation,229,76,experimental-setup,number of documents,to,retrieve,number of documents to retrieve,0.5394012928009033
translation,229,76,experimental-setup,m = 200,for,number of paragraphs,m = 200 for number of paragraphs,0.5941348075866699
translation,229,76,experimental-setup,number of paragraphs,to read,all the four datasets,number of paragraphs to read all the four datasets,0.5565184354782104
translation,229,77,experimental-setup,"adamax ( kingma and ba , 2014 )",as,optimization algorithm,"adamax ( kingma and ba , 2014 ) as optimization algorithm",0.5385408401489258
translation,229,77,experimental-setup,experimental setup,use,"adamax ( kingma and ba , 2014 )","experimental setup use adamax ( kingma and ba , 2014 )",0.5996699929237366
translation,229,78,experimental-setup,dropout,applied to,lstms and embeddings,dropout applied to lstms and embeddings,0.6351618766784668
translation,229,78,experimental-setup,lstms and embeddings,with,p = 0.4,lstms and embeddings with p = 0.4,0.6323010921478271
translation,229,78,experimental-setup,experimental setup,has,dropout,experimental setup has dropout,0.5067690014839172
translation,229,71,experiments,document reader and paragraph ranker,on,squad training set,document reader and paragraph ranker on squad training set,0.5288202166557312
translation,229,8,model,paragraph ranker,ranks,paragraphs,paragraph ranker ranks paragraphs,0.7817030549049377
translation,229,8,model,paragraphs,of,retrieved documents,paragraphs of retrieved documents,0.5562923550605774
translation,229,8,model,paragraphs,for,higher answer recall,paragraphs for higher answer recall,0.5927445888519287
translation,229,8,model,higher answer recall,with,less noise,higher answer recall with less noise,0.6315293312072754
translation,229,8,model,model,introduce,paragraph ranker,model introduce paragraph ranker,0.5998377799987793
translation,229,9,results,paragraphs and aggregating answers,using,paragraph ranker,paragraphs and aggregating answers using paragraph ranker,0.6702491641044617
translation,229,9,results,paragraph ranker,improves,performance,paragraph ranker improves performance,0.673884391784668
translation,229,9,results,performance,of,open-domain qa pipeline,performance of open-domain qa pipeline,0.5831469297409058
translation,229,9,results,performance,by,7.8 %,performance by 7.8 %,0.5764674544334412
translation,229,9,results,open-domain qa pipeline,on,four opendomain qa datasets,open-domain qa pipeline on four opendomain qa datasets,0.5115175843238831
translation,229,9,results,four opendomain qa datasets,by,7.8 %,four opendomain qa datasets by 7.8 %,0.5689077377319336
translation,229,9,results,ranking,has,paragraphs and aggregating answers,ranking has paragraphs and aggregating answers,0.5572176575660706
translation,229,9,results,results,show,ranking,results show ranking,0.6620067954063416
translation,229,18,results,our qa pipeline,considers,more documents,our qa pipeline considers more documents,0.6633158326148987
translation,229,18,results,our qa pipeline,ranks,paragraphs,our qa pipeline ranks paragraphs,0.7272271513938904
translation,229,18,results,more documents,for,high answer recall,more documents for high answer recall,0.5706493258476257
translation,229,18,results,paragraphs,to read,most relevant ones,paragraphs to read most relevant ones,0.6418206095695496
translation,229,18,results,our simple but efficient paragraph ranker,has,our qa pipeline,our simple but efficient paragraph ranker has our qa pipeline,0.6037980914115906
translation,229,18,results,results,using,our simple but efficient paragraph ranker,results using our simple but efficient paragraph ranker,0.6620112061500549
translation,230,83,ablation-analysis,performance decrease,when removing,each of the three matching strategies,performance decrease when removing each of the three matching strategies,0.683215856552124
translation,230,84,ablation-analysis,w/o max-attentive - matching,shows,least performance decrease,w/o max-attentive - matching shows least performance decrease,0.6726282238960266
translation,230,84,ablation-analysis,ablation analysis,has,w/o max-attentive - matching,ablation analysis has w/o max-attentive - matching,0.5681115984916687
translation,230,86,ablation-analysis,w/ o full-matching,shows,more performance decrease,w/ o full-matching shows more performance decrease,0.6752669215202332
translation,230,86,ablation-analysis,more performance decrease,than,w/o attentive -matching,more performance decrease than w/o attentive -matching,0.5934133529663086
translation,230,86,ablation-analysis,ablation analysis,has,w/ o full-matching,ablation analysis has w/ o full-matching,0.5518817901611328
translation,230,98,ablation-analysis,rich features,play,similar role,rich features play similar role,0.7688653469085693
translation,230,98,ablation-analysis,similar role,of leveraging,more information,similar role of leveraging more information,0.6690890192985535
translation,230,98,ablation-analysis,more information,than,answer position information,more information than answer position information,0.5334742069244385
translation,230,98,ablation-analysis,ablation analysis,has,multi-perspective matching,ablation analysis has multi-perspective matching,0.5303443670272827
translation,230,25,baselines,multiperspective context matching ( mpcm ) algorithm,takes,two texts,multiperspective context matching ( mpcm ) algorithm takes two texts,0.6602314710617065
translation,230,25,baselines,two texts,as,input,two texts as input,0.54634690284729
translation,230,25,baselines,input,before producing,vector of numbers,input before producing vector of numbers,0.7314144372940063
translation,230,25,baselines,vector of numbers,representing,similarity,vector of numbers representing similarity,0.6684027314186096
translation,230,25,baselines,similarity,under,different perspectives,similarity under different perspectives,0.6364925503730774
translation,230,30,baselines,sequence - to-sequence model,with,copy mechanism,sequence - to-sequence model with copy mechanism,0.6582381725311279
translation,230,93,baselines,s2s - ans,encodes,passage,s2s - ans encodes passage,0.7537710070610046
translation,230,93,baselines,s2s - ans,does not use,answer position information,s2s - ans does not use answer position information,0.7322996258735657
translation,230,93,baselines,baselines,has,s2s - ans,baselines has s2s - ans,0.5971710681915283
translation,230,94,baselines,s2s + cp +f,uses,answer position and rich features ( ne and pos tags ),s2s + cp +f uses answer position and rich features ( ne and pos tags ),0.5850568413734436
translation,230,94,baselines,s2s + cp +f,by concatenating,embeddings,s2s + cp +f by concatenating embeddings,0.6800611019134521
translation,230,94,baselines,s2s + cp +f,adopting,copy mechanism,s2s + cp +f adopting copy mechanism,0.75677090883255
translation,230,94,baselines,answer position and rich features ( ne and pos tags ),by concatenating,embeddings,answer position and rich features ( ne and pos tags ) by concatenating embeddings,0.7207314372062683
translation,230,94,baselines,embeddings,with,word embedding,embeddings with word embedding,0.6068796515464783
translation,230,94,baselines,word embedding,on,encoder side,word embedding on encoder side,0.534710705280304
translation,230,94,baselines,copy mechanism,for,decoder,copy mechanism for decoder,0.6608333587646484
translation,230,94,baselines,baselines,has,s2s + cp +f,baselines has s2s + cp +f,0.5671557784080505
translation,230,95,baselines,s2s + cp,is,our sequence - to-sequence baseline,s2s + cp is our sequence - to-sequence baseline,0.565251886844635
translation,230,95,baselines,our sequence - to-sequence baseline,with,copy mechanism,our sequence - to-sequence baseline with copy mechanism,0.6913470029830933
translation,230,95,baselines,our model,uses,multi-perspective encoder,our model uses multi-perspective encoder,0.5804621577262878
translation,230,95,baselines,baselines,has,s2s + cp,baselines has s2s + cp,0.5665140748023987
translation,230,78,hyperparameters,baseline and our model,trained with,cross-entropy loss,baseline and our model trained with cross-entropy loss,0.6834378242492676
translation,230,78,hyperparameters,hyperparameters,Both,baseline and our model,hyperparameters Both baseline and our model,0.6216896176338196
translation,230,78,hyperparameters,hyperparameters,has,baseline and our model,hyperparameters has baseline and our model,0.539922833442688
translation,230,24,model,matching,used between,target answer and the passage,matching used between target answer and the passage,0.7009579539299011
translation,230,24,model,matching,for collecting,relevant contextual information,matching for collecting relevant contextual information,0.6482458114624023
translation,230,24,model,target answer and the passage,for collecting,relevant contextual information,target answer and the passage for collecting relevant contextual information,0.6438125967979431
translation,230,24,model,model,has,matching,model has matching,0.587853729724884
translation,230,96,results,outperforms,on,both data splits,outperforms on both data splits,0.509166419506073
translation,230,96,results,s2s + cp,on,both data splits,s2s + cp on both data splits,0.5900611281394958
translation,230,96,results,m2s + cp,has,outperforms,m2s + cp has outperforms,0.6602010726928711
translation,230,96,results,outperforms,has,s2s + cp,outperforms has s2s + cp,0.6545443534851074
translation,230,96,results,results,has,m2s + cp,results has m2s + cp,0.5418201088905334
translation,230,97,results,m2s + cp,shows,better performance,m2s + cp shows better performance,0.6924290060997009
translation,230,97,results,better performance,than,s2s + cp +f,better performance than s2s + cp +f,0.5722286105155945
translation,230,97,results,word embedding features,has,m2s + cp,word embedding features has m2s + cp,0.5323436856269836
translation,230,97,results,results,taking,word embedding features,results taking word embedding features,0.5741181969642639
translation,230,113,results,question generation,for,extractive qa m2s + cp,question generation for extractive qa m2s + cp,0.6065563559532166
translation,230,113,results,extractive qa m2s + cp,is,consistently better,extractive qa m2s + cp is consistently better,0.6039670705795288
translation,230,113,results,consistently better,than,s2s + cp,consistently better than s2s + cp,0.625114381313324
translation,230,113,results,consistently better,both under,f1 score and exact match,consistently better both under f1 score and exact match,0.6714861989021301
translation,230,113,results,results,has,question generation,results has question generation,0.5214330554008484
translation,230,114,results,automatically generated questions,from,m2s + cp,automatically generated questions from m2s + cp,0.5539228320121765
translation,230,114,results,automatically generated questions,reach,better performance,automatically generated questions reach better performance,0.7342034578323364
translation,230,114,results,better performance,than,only 20 % gold data,better performance than only 20 % gold data,0.5380539894104004
translation,230,114,results,better performance,than,11 points better,better performance than 11 points better,0.5730788707733154
translation,230,114,results,better performance,using,only 20 % gold data,better performance using only 20 % gold data,0.6170969605445862
translation,230,114,results,better performance,using,only 10 % gold data,better performance using only 10 % gold data,0.619390070438385
translation,230,114,results,11 points better,than,only 10 % gold data,11 points better than only 10 % gold data,0.6052049994468689
translation,230,114,results,10 % gold data,has,automatically generated questions,10 % gold data has automatically generated questions,0.6029478311538696
translation,230,114,results,results,using,10 % gold data,results using 10 % gold data,0.6681538820266724
translation,231,177,baselines,simple sequence - to-sequence model,with,attention mechanism,simple sequence - to-sequence model with attention mechanism,0.6030029058456421
translation,231,177,baselines,baselines seq2seq,has,simple sequence - to-sequence model,baselines seq2seq has simple sequence - to-sequence model,0.5307056307792664
translation,231,177,baselines,baselines,has,baselines seq2seq,baselines has baselines seq2seq,0.5495138168334961
translation,231,178,baselines,conditional variational autoencoders,with,auxiliary bag-of-words loss,conditional variational autoencoders with auxiliary bag-of-words loss,0.5718927979469299
translation,231,178,baselines,auxiliary bag-of-words loss,to generate,diverse responses,auxiliary bag-of-words loss to generate diverse responses,0.6466461420059204
translation,231,178,baselines,cvae,has,conditional variational autoencoders,cvae has conditional variational autoencoders,0.46856874227523804
translation,231,178,baselines,baselines,has,cvae,baselines has cvae,0.5393452048301697
translation,231,179,baselines,std&htd,uses,soft typed decoder,std&htd uses soft typed decoder,0.6091951131820679
translation,231,179,baselines,std&htd,uses,hard typed decoder,std&htd uses hard typed decoder,0.6113843321800232
translation,231,179,baselines,std&htd,uses,hard typed decoder,std&htd uses hard typed decoder,0.6113843321800232
translation,231,179,baselines,std,uses,soft typed decoder,std uses soft typed decoder,0.6134217381477356
translation,231,179,baselines,soft typed decoder,by estimating,words ' distribution,soft typed decoder by estimating words ' distribution,0.7562479972839355
translation,231,179,baselines,soft typed decoder,uses,hard typed decoder,soft typed decoder uses hard typed decoder,0.5848162174224854
translation,231,179,baselines,words ' distribution,over,word types,words ' distribution over word types,0.6294316053390503
translation,231,179,baselines,htd,uses,hard typed decoder,htd uses hard typed decoder,0.6030738949775696
translation,231,179,baselines,hard typed decoder,by specifying,type of each word explicitly,hard typed decoder by specifying type of each word explicitly,0.7574027180671692
translation,231,179,baselines,std&htd,has,std,std&htd has std,0.6178103685379028
translation,231,179,baselines,std&htd,has,htd,std&htd has htd,0.6373191475868225
translation,231,179,baselines,baselines,has,std&htd,baselines has std&htd,0.5860261917114258
translation,231,206,baselines,matching score,use,pretrained gru - matchpyramid model,matching score use pretrained gru - matchpyramid model,0.577236533164978
translation,231,206,baselines,pretrained gru - matchpyramid model,to calculate,semantic coherence,pretrained gru - matchpyramid model to calculate semantic coherence,0.6503123044967651
translation,231,206,baselines,higher scores,imply,better semantic coherence,higher scores imply better semantic coherence,0.5709707140922546
translation,231,206,baselines,baselines,has,matching score,baselines has matching score,0.5688493251800537
translation,231,162,hyperparameters,"top 40,000 frequent words",as,vocabulary,"top 40,000 frequent words as vocabulary",0.5029767751693726
translation,231,162,hyperparameters,hyperparameters,choose,"top 40,000 frequent words","hyperparameters choose top 40,000 frequent words",0.6336137652397156
translation,231,163,hyperparameters,200d glove embedding,pretrained on,wikipedia 2014 dataset,200d glove embedding pretrained on wikipedia 2014 dataset,0.7713496685028076
translation,231,163,hyperparameters,wikipedia 2014 dataset,as,word embedding,wikipedia 2014 dataset as word embedding,0.46928277611732483
translation,231,163,hyperparameters,hyperparameters,use,200d glove embedding,hyperparameters use 200d glove embedding,0.5612857937812805
translation,231,164,hyperparameters,size of encoder state,set to,300,size of encoder state set to 300,0.7297111749649048
translation,231,164,hyperparameters,size of decoder state,set to,400,size of decoder state set to 400,0.7252687811851501
translation,231,164,hyperparameters,cvae,has,size of encoder state,cvae has size of encoder state,0.5238432884216309
translation,231,164,hyperparameters,cvae,has,size of decoder state,cvae has size of decoder state,0.5332090854644775
translation,231,164,hyperparameters,hyperparameters,In,cvae,hyperparameters In cvae,0.5298740267753601
translation,231,165,hyperparameters,latent gaussian variable,to,200,latent gaussian variable to 200,0.6347336173057556
translation,231,165,hyperparameters,question type,to,100,question type to 100,0.6022709608078003
translation,231,165,hyperparameters,mini-batch,to,30,mini-batch to 30,0.6282524466514587
translation,231,165,hyperparameters,hyperparameters,size of,question type,hyperparameters size of question type,0.6869847774505615
translation,231,165,hyperparameters,hyperparameters,size of,mini-batch,hyperparameters size of mini-batch,0.6402444839477539
translation,231,166,hyperparameters,prior network and question type prediction network,have,hidden layer,prior network and question type prediction network have hidden layer,0.5599721074104309
translation,231,166,hyperparameters,hidden layer,whose,dimension,hidden layer whose dimension,0.671592652797699
translation,231,166,hyperparameters,dimension,is,400,dimension is 400,0.6541664600372314
translation,231,166,hyperparameters,hyperparameters,has,prior network and question type prediction network,hyperparameters has prior network and question type prediction network,0.537750244140625
translation,231,167,hyperparameters,weights,initialized by,xavier method,weights initialized by xavier method,0.704289972782135
translation,231,167,hyperparameters,hyperparameters,has,weights,hyperparameters has weights,0.5201958417892456
translation,231,168,hyperparameters,tokenized,using,nltk tokenizer,tokenized using nltk tokenizer,0.6775268912315369
translation,231,168,hyperparameters,hyperparameters,has,dataset,hyperparameters has dataset,0.5238766074180603
translation,231,169,hyperparameters,our model,using,"adam ( kingma and ba , 2014 )","our model using adam ( kingma and ba , 2014 )",0.6679904460906982
translation,231,169,hyperparameters,"adam ( kingma and ba , 2014 )",with,learning state,"adam ( kingma and ba , 2014 ) with learning state",0.6277332305908203
translation,231,169,hyperparameters,"adam ( kingma and ba , 2014 )",with,gradient clipping,"adam ( kingma and ba , 2014 ) with gradient clipping",0.6111595630645752
translation,231,169,hyperparameters,learning state,of,0.001,learning state of 0.001,0.568476140499115
translation,231,169,hyperparameters,gradient clipping,at,5,gradient clipping at 5,0.5210050940513611
translation,231,169,hyperparameters,hyperparameters,optimize,our model,hyperparameters optimize our model,0.6838966608047485
translation,231,170,hyperparameters,kl - annealing strategy,with,temperature,kl - annealing strategy with temperature,0.6400696039199829
translation,231,170,hyperparameters,temperature,varying from,0 to 1,temperature varying from 0 to 1,0.7523611187934875
translation,231,170,hyperparameters,1/60k,after,each iteration,1/60k after each iteration,0.6757367253303528
translation,231,170,hyperparameters,each iteration,of,batch update,each iteration of batch update,0.5924258232116699
translation,231,170,hyperparameters,hyperparameters,use,kl - annealing strategy,hyperparameters use kl - annealing strategy,0.662860095500946
translation,231,171,hyperparameters,bidirectional gru model,for encoding,question and answer,bidirectional gru model for encoding question and answer,0.746277391910553
translation,231,171,hyperparameters,bidirectional gru model,has,hidden state,bidirectional gru model has hidden state,0.5815609097480774
translation,231,171,hyperparameters,question and answer,has,hidden state,question and answer has hidden state,0.6207299828529358
translation,231,171,hyperparameters,hidden state,of,300 in size,hidden state of 300 in size,0.5793587565422058
translation,231,171,hyperparameters,gru - matchpyramid model,has,bidirectional gru model,gru - matchpyramid model has bidirectional gru model,0.6022316217422485
translation,231,171,hyperparameters,question and answer,has,hidden state,question and answer has hidden state,0.6207299828529358
translation,231,171,hyperparameters,hyperparameters,For,gru - matchpyramid model,hyperparameters For gru - matchpyramid model,0.5656531453132629
translation,231,172,hyperparameters,kernel size,set to,"( 3,3 )","kernel size set to ( 3,3 )",0.6984695792198181
translation,231,172,hyperparameters,stride size,to,2,stride size to 2,0.5636417269706726
translation,231,172,hyperparameters,convolutional operation,has,kernel size,convolutional operation has kernel size,0.5265219211578369
translation,231,172,hyperparameters,hyperparameters,In,convolutional operation,hyperparameters In convolutional operation,0.49416324496269226
translation,231,173,hyperparameters,rl - cvae,set,threshold,rl - cvae set threshold,0.7098908424377441
translation,231,173,hyperparameters,rl - cvae,set,number n,rl - cvae set number n,0.6892306804656982
translation,231,173,hyperparameters,threshold,to,1,threshold to 1,0.6391687393188477
translation,231,173,hyperparameters,1,in,gru - matchpyramid 's training,1 in gru - matchpyramid 's training,0.5351411700248718
translation,231,173,hyperparameters,number n,to,100,number n to 100,0.6195023655891418
translation,231,173,hyperparameters,number n,in,min-max scaling,number n in min-max scaling,0.5500407814979553
translation,231,173,hyperparameters,hyperparameters,In,rl - cvae,hyperparameters In rl - cvae,0.516045331954956
translation,231,174,hyperparameters,pretrained cvae model,obtained by,bag-of-words loss,pretrained cvae model obtained by bag-of-words loss,0.5366661548614502
translation,231,174,hyperparameters,bag-of-words loss,along with,kl annealing,bag-of-words loss along with kl annealing,0.5840232372283936
translation,231,174,hyperparameters,kl annealing,to,0.5,kl annealing to 0.5,0.6296767592430115
translation,231,174,hyperparameters,0.5,after training,30 k batches,0.5 after training 30 k batches,0.7389353513717651
translation,231,174,hyperparameters,hyperparameters,has,pretrained cvae model,hyperparameters has pretrained cvae model,0.48107776045799255
translation,231,175,hyperparameters,policy loss,to obtain,final model,policy loss to obtain final model,0.5981456637382507
translation,231,175,hyperparameters,final model,with,kl weight,final model with kl weight,0.615676760673523
translation,231,175,hyperparameters,kl weight,kept at,0.5,kl weight kept at 0.5,0.6545775532722473
translation,231,175,hyperparameters,hyperparameters,use,policy loss,hyperparameters use policy loss,0.6051246523857117
translation,231,176,hyperparameters,a - cvae,set,balancing parameter,a - cvae set balancing parameter,0.6696400046348572
translation,231,176,hyperparameters,a - cvae,train,discriminator,a - cvae train discriminator,0.679651141166687
translation,231,176,hyperparameters,balancing parameter,in,generator 's loss,balancing parameter in generator 's loss,0.5319758653640747
translation,231,176,hyperparameters,balancing parameter,in,generator,balancing parameter in generator,0.5558984279632568
translation,231,176,hyperparameters,generator 's loss,to,0.1,generator 's loss to 0.1,0.5233925580978394
translation,231,176,hyperparameters,one time,for,every five times,one time for every five times,0.6360185742378235
translation,231,176,hyperparameters,training,of,generator,training of generator,0.6018707156181335
translation,231,176,hyperparameters,discriminator,has,one time,discriminator has one time,0.5627164244651794
translation,231,176,hyperparameters,every five times,has,training,every five times has training,0.6125593185424805
translation,231,176,hyperparameters,hyperparameters,In,a - cvae,hyperparameters In a - cvae,0.53077232837677
translation,231,7,model,two methods,to further enhance,semantic coherence,two methods to further enhance semantic coherence,0.6549618244171143
translation,231,7,model,semantic coherence,between,post and question,semantic coherence between post and question,0.6569176912307739
translation,231,7,model,post and question,under,guidance,post and question under guidance,0.7024990916252136
translation,231,7,model,guidance,of,answer,guidance of answer,0.614039421081543
translation,231,7,model,model,devise,two methods,model devise two methods,0.7674161195755005
translation,231,8,model,coherence score,between,generated question and answer,coherence score between generated question and answer,0.64203941822052
translation,231,8,model,coherence score,used as,reward function,coherence score used as reward function,0.58330237865448
translation,231,8,model,generated question and answer,used as,reward function,generated question and answer used as reward function,0.6245330572128296
translation,231,8,model,reward function,in,reinforcement learning framework,reward function in reinforcement learning framework,0.4890030324459076
translation,231,8,model,reward function,to encourage,cases,reward function to encourage cases,0.7228644490242004
translation,231,8,model,cases,in,semantic,cases in semantic,0.6313613057136536
translation,231,8,model,consistent,with,answer,consistent with answer,0.6759606599807739
translation,231,8,model,answer,in,semantic,answer in semantic,0.5281304121017456
translation,231,8,model,model,has,coherence score,model has coherence score,0.5530822277069092
translation,231,9,model,adversarial training,to explicitly control,question generation,adversarial training to explicitly control question generation,0.65456223487854
translation,231,9,model,question generation,in the direction of,question - answer coherence,question generation in the direction of question - answer coherence,0.6647135615348816
translation,231,9,model,model,incorporate,adversarial training,model incorporate adversarial training,0.7013677358627319
translation,231,37,model,two separate learning frameworks,based on,reinforcement learning ( rl ),two separate learning frameworks based on reinforcement learning ( rl ),0.6674525737762451
translation,231,37,model,two separate learning frameworks,based on,generative adversarial network ( gan ),two separate learning frameworks based on generative adversarial network ( gan ),0.6476286053657532
translation,231,37,model,model,propose,two separate learning frameworks,model propose two separate learning frameworks,0.6736567616462708
translation,231,38,model,rl framework,utilize,semantic coherence,rl framework utilize semantic coherence,0.5147943496704102
translation,231,38,model,semantic coherence,estimation of,question quality,semantic coherence estimation of question quality,0.6653399467468262
translation,231,38,model,model,For,rl framework,model For rl framework,0.6114222407341003
translation,231,39,model,gru - matchpyramid model,incorporates,bi-directional gru,gru - matchpyramid model incorporates bi-directional gru,0.7159799337387085
translation,231,39,model,bi-directional gru,into,matchpyramid model,bi-directional gru into matchpyramid model,0.5914382934570312
translation,231,39,model,matchpyramid model,to capture,higher level 's word semantic,matchpyramid model to capture higher level 's word semantic,0.7028409838676453
translation,231,39,model,model,propose,gru - matchpyramid model,model propose gru - matchpyramid model,0.6537590026855469
translation,231,40,model,pretrained gru - matchpyramid,to measure,coherence,pretrained gru - matchpyramid to measure coherence,0.6735851764678955
translation,231,40,model,coherence,between,question,coherence between question,0.6525307893753052
translation,231,40,model,coherence,between,cor-responding answer,coherence between cor-responding answer,0.6879531741142273
translation,231,40,model,model,employ,pretrained gru - matchpyramid,model employ pretrained gru - matchpyramid,0.5339733958244324
translation,231,43,model,gan framework,incorporate,adversarial training,gan framework incorporate adversarial training,0.6777764558792114
translation,231,43,model,adversarial training,to explicitly improve,coherence,adversarial training to explicitly improve coherence,0.7423545122146606
translation,231,43,model,coherence,between,questions and answers,coherence between questions and answers,0.6598768830299377
translation,231,43,model,model,For,gan framework,model For gan framework,0.5998435616493225
translation,231,44,model,question generator and a discriminator ( i.e. gru - match pyramid model ),measures,semantic coherence,question generator and a discriminator ( i.e. gru - match pyramid model ) measures semantic coherence,0.5258661508560181
translation,231,44,model,semantic coherence,generated questions ( as well as,ground - truth questions ) and answers,semantic coherence generated questions ( as well as ground - truth questions ) and answers,0.7040583491325378
translation,231,44,model,model,jointly train,question generator and a discriminator ( i.e. gru - match pyramid model ),model jointly train question generator and a discriminator ( i.e. gru - match pyramid model ),0.6969382762908936
translation,231,47,model,prior knowledge,of,question type,prior knowledge of question type,0.5573650002479553
translation,231,47,model,question type,to generate,questions,question type to generate questions,0.6794916391372681
translation,231,47,model,questions,with,reasonable type,questions with reasonable type,0.6015106439590454
translation,231,47,model,model,incorporate,prior knowledge,model incorporate prior knowledge,0.7195051312446594
translation,231,48,model,cvae,into,rl framework and gan frameworks,cvae into rl framework and gan frameworks,0.5610640048980713
translation,231,48,model,two novel cqg models,dubbed as,rl - cvae,two novel cqg models dubbed as rl - cvae,0.6654047966003418
translation,231,48,model,two novel cqg models,dubbed as,a- cvae,two novel cqg models dubbed as a- cvae,0.668498158454895
translation,231,48,model,model,integrate,cvae,model integrate cvae,0.6811272501945496
translation,231,48,model,model,propose,two novel cqg models,model propose two novel cqg models,0.6947841048240662
translation,231,51,model,answer information,exploited with,reinforcement learning and adversarial learning,answer information exploited with reinforcement learning and adversarial learning,0.7023975849151611
translation,231,51,model,model,has,answer information,model has answer information,0.6017977595329285
translation,231,191,results,cvae ( qt ),incorporates,question type information,cvae ( qt ) incorporates question type information,0.6800752878189087
translation,231,191,results,performance,compared with,cvae,performance compared with cvae,0.6936139464378357
translation,231,191,results,slightly improves,has,performance,slightly improves has performance,0.5808963179588318
translation,231,191,results,results,has,cvae ( qt ),results has cvae ( qt ),0.5423284769058228
translation,231,193,results,rl - cvae and a - cvae,perform,fairly well,rl - cvae and a - cvae perform fairly well,0.588337242603302
translation,231,193,results,fairly well,with,lower perplexities,fairly well with lower perplexities,0.6186127066612244
translation,231,193,results,results,has,rl - cvae and a - cvae,results has rl - cvae and a - cvae,0.5342262387275696
translation,231,194,results,higher rubg and ruba scores,taking advantage of,coherence,higher rubg and ruba scores taking advantage of coherence,0.6654137969017029
translation,231,194,results,coherence,between,question and answer,coherence between question and answer,0.644659161567688
translation,231,194,results,coherence,could further enhance,semantic coherence,coherence could further enhance semantic coherence,0.6473584175109863
translation,231,194,results,semantic coherence,between,post and question,semantic coherence between post and question,0.6569176912307739
translation,231,196,results,rl - cvae,performs,better,rl - cvae performs better,0.6843665242195129
translation,231,196,results,better,than,a- cvae,better than a- cvae,0.6596639752388
translation,231,196,results,question generation evaluation,has,rl - cvae,question generation evaluation has rl - cvae,0.5556957125663757
translation,231,196,results,results,has,question generation evaluation,results has question generation evaluation,0.5175259113311768
translation,231,202,results,gru - matchpyramid model,performs,better,gru - matchpyramid model performs better,0.6472013592720032
translation,231,202,results,better,than,matchpyramid ( dubbed as mp ) model,better than matchpyramid ( dubbed as mp ) model,0.5868580341339111
translation,231,202,results,matchpyramid ( dubbed as mp ) model,in,large scale conversational data,matchpyramid ( dubbed as mp ) model in large scale conversational data,0.48952820897102356
translation,231,202,results,results,has,gru - matchpyramid model,results has gru - matchpyramid model,0.5427723526954651
translation,231,207,results,our proposed models,could generate,questions,our proposed models could generate questions,0.7707042098045349
translation,231,207,results,questions,coherent to,answers,questions coherent to answers,0.7207496166229248
translation,231,207,results,answers,in,inference process,answers in inference process,0.5253668427467346
translation,231,208,results,coherence,of,question and answer,coherence of question and answer,0.586532711982727
translation,231,208,results,coherence,improved,overall performance,coherence improved overall performance,0.7616308331489563
translation,231,208,results,question and answer,improved,overall performance,question and answer improved overall performance,0.7832900285720825
translation,231,208,results,overall performance,by,large margin,overall performance by large margin,0.5720058083534241
translation,231,208,results,results,has,coherence,results has coherence,0.45612597465515137
translation,231,221,results,rl - cvae and a - cvae,consistently in line with,human perspective,rl - cvae and a - cvae consistently in line with human perspective,0.6889715790748596
translation,231,221,results,human perspective,especially in,semantic coherence,human perspective especially in semantic coherence,0.5311274528503418
translation,231,221,results,human perspective,especially in,willing to answer criteria,human perspective especially in willing to answer criteria,0.609143853187561
translation,231,221,results,results,has,rl - cvae and a - cvae,results has rl - cvae and a - cvae,0.5342262387275696
translation,231,223,results,our proposed models,effectively alleviate,dullness and deviation issues,our proposed models effectively alleviate dullness and deviation issues,0.7302188277244568
translation,231,223,results,state- ofthe- art model htd,has,our proposed models,state- ofthe- art model htd has our proposed models,0.5502486228942871
translation,231,223,results,results,Compared with,state- ofthe- art model htd,results Compared with state- ofthe- art model htd,0.6529552936553955
translation,232,153,baselines,prompt question,without,any context,prompt question without any context,0.7471824884414673
translation,232,153,baselines,any context,from,plausible answers or reference passages,any context from plausible answers or reference passages,0.5773972868919373
translation,232,205,baselines,four nq - open models,consisting of,diverse approaches,four nq - open models consisting of diverse approaches,0.6939957141876221
translation,232,205,baselines,baselines,include,four nq - open models,baselines include four nq - open models,0.5760117173194885
translation,232,2,experiments,ambigqa,has,answering ambiguous open-domain questions,ambigqa has answering ambiguous open-domain questions,0.6065314412117004
translation,232,6,experiments,dataset,covering,"14,042 questions","dataset covering 14,042 questions",0.7221132516860962
translation,232,6,experiments,"14,042 questions",from,nq - open,"14,042 questions from nq - open",0.5513828992843628
translation,232,6,experiments,ambignq,has,dataset,ambignq has dataset,0.5880363583564758
translation,232,32,experiments,dataset,with,"14,042 annotations","dataset with 14,042 annotations",0.6022878289222717
translation,232,32,experiments,"14,042 annotations",on,nq - open questions,"14,042 annotations on nq - open questions",0.5383484959602356
translation,232,32,experiments,ambignq,has,dataset,ambignq has dataset,0.5880363583564758
translation,232,79,experiments,amazon mechanical turk,for,crowdsourcing,amazon mechanical turk for crowdsourcing,0.5144479274749756
translation,232,219,experiments,ambignq,with,"14,042 annotations","ambignq with 14,042 annotations",0.6652892231941223
translation,232,219,experiments,"14,042 annotations",on,nq - open questions,"14,042 annotations on nq - open questions",0.5383484959602356
translation,232,28,model,qa model,with,three,qa model with three,0.7131934762001038
translation,232,28,model,set-based question answering,with,sequence - tosequence model,set-based question answering with sequence - tosequence model,0.6279246807098389
translation,232,28,model,set-based question answering,with,question disambiguation model,set-based question answering with question disambiguation model,0.5976102352142334
translation,232,28,model,modification,to,democratic cotraining,modification to democratic cotraining,0.5960986614227295
translation,232,28,model,democratic cotraining,leverages,partial supervision,democratic cotraining leverages partial supervision,0.7218050360679626
translation,232,28,model,partial supervision,available in,nq - open dataset,partial supervision available in nq - open dataset,0.5869295597076416
translation,232,8,results,strong baseline models,for,ambigqa,strong baseline models for ambigqa,0.6079869270324707
translation,232,8,results,strong baseline models,show benefit from,weakly supervised learning,strong baseline models show benefit from weakly supervised learning,0.6573144197463989
translation,232,8,results,ambigqa,show benefit from,weakly supervised learning,ambigqa show benefit from weakly supervised learning,0.6707087755203247
translation,232,8,results,weakly supervised learning,that incorporates,nq - open,weakly supervised learning that incorporates nq - open,0.6211028695106506
translation,232,164,results,disambig -first,is,significantly worse,disambig -first is significantly worse,0.5906879901885986
translation,232,164,results,significantly worse,than,other models,significantly worse than other models,0.5717747807502747
translation,232,165,results,classification accuracy,on,prompt question,classification accuracy on prompt question,0.538723886013031
translation,232,165,results,prompt question,is,ambiguous,prompt question is ambiguous,0.6081058382987976
translation,232,165,results,ambiguous,is,67 %,ambiguous is 67 %,0.6173059940338135
translation,232,165,results,67 %,close to,majority baseline ( 60 % ),67 % close to majority baseline ( 60 % ),0.713996171951294
translation,232,165,results,results,has,classification accuracy,results has classification accuracy,0.540421724319458
translation,232,174,results,spanseqgen,achieves,reasonable f1 ans scores,spanseqgen achieves reasonable f1 ans scores,0.684309184551239
translation,232,174,results,results,has,spanseqgen,results has spanseqgen,0.5405837893486023
translation,232,175,results,f1 ans,examples with,multiple question - answer pairs ( multi ),f1 ans examples with multiple question - answer pairs ( multi ),0.7840484976768494
translation,232,175,results,multiple question - answer pairs ( multi ),are,lower,multiple question - answer pairs ( multi ) are lower,0.6078897714614868
translation,232,175,results,results,has,f1 ans,results has f1 ans,0.5406737923622131
translation,232,176,results,spanseqgen,obtains,best performance,spanseqgen obtains best performance,0.6256175637245178
translation,232,176,results,best performance,in,f1 bleu and f1 edit - f1,best performance in f1 bleu and f1 edit - f1,0.5301184058189392
translation,232,176,results,results,has,spanseqgen,results has spanseqgen,0.5405837893486023
translation,232,180,results,gains,from,ensembling,gains from ensembling,0.6467568874359131
translation,232,180,results,gains,ensemble trained with,co-training method,gains ensemble trained with co-training method,0.7797103524208069
translation,232,180,results,ensembling,ensemble trained with,co-training method,ensembling ensemble trained with co-training method,0.7986763119697571
translation,232,180,results,co-training method,achieves,best performance,co-training method achieves best performance,0.6624174118041992
translation,232,180,results,best performance,on,all metrics,best performance on all metrics,0.5029283165931702
translation,232,180,results,results,see,gains,results see gains,0.5693391561508179
translation,233,115,baselines,baselines,has,question & description dual encoder,baselines has question & description dual encoder,0.5447177886962891
translation,233,114,hyperparameters,lstm cells,contained,300 hidden units,lstm cells contained 300 hidden units,0.560824453830719
translation,233,114,hyperparameters,lstm cells,contained,2 layers,lstm cells contained 2 layers,0.5834863185882568
translation,233,114,hyperparameters,lstm cells,optimized,binary crossentropy loss function,lstm cells optimized binary crossentropy loss function,0.6715604066848755
translation,233,114,hyperparameters,hyperparameters,has,lstm cells,hyperparameters has lstm cells,0.5413953065872192
translation,233,139,hyperparameters,lstm cells,contained,512 hidden units,lstm cells contained 512 hidden units,0.5455741286277771
translation,233,139,hyperparameters,lstm cells,contained,2 layers,lstm cells contained 2 layers,0.5834863185882568
translation,233,139,hyperparameters,hyperparameters,has,lstm cells,hyperparameters has lstm cells,0.5413953065872192
translation,233,140,hyperparameters,"dropout ( srivastava et al , 2014 ) probability",of,0.2,"dropout ( srivastava et al , 2014 ) probability of 0.2",0.5792203545570374
translation,233,140,hyperparameters,"dropout ( srivastava et al , 2014 ) probability",of,3.6,"dropout ( srivastava et al , 2014 ) probability of 3.6",0.5869811773300171
translation,233,140,hyperparameters,"gradient normalization ( pascanu et al. , 2013 )",of,3.6,"gradient normalization ( pascanu et al. , 2013 ) of 3.6",0.5686423182487488
translation,233,140,hyperparameters,hyperparameters,used,"dropout ( srivastava et al , 2014 ) probability","hyperparameters used dropout ( srivastava et al , 2014 ) probability",0.5606738924980164
translation,233,140,hyperparameters,hyperparameters,used,"gradient normalization ( pascanu et al. , 2013 )","hyperparameters used gradient normalization ( pascanu et al. , 2013 )",0.5264764428138733
translation,233,141,hyperparameters,length,of,175,length of 175,0.5876583456993103
translation,233,141,hyperparameters,175,equally split into,smaller chunks,175 equally split into smaller chunks,0.7844113707542419
translation,233,141,hyperparameters,smaller chunks,of,size,smaller chunks of size,0.609542727470398
translation,233,141,hyperparameters,size,has,12 increments,size has 12 increments,0.6378727555274963
translation,233,19,model,community - based question answering forums,of,stack exchange,community - based question answering forums of stack exchange,0.5528995394706726
translation,233,19,model,model,leverage,promising source,model leverage promising source,0.7723469138145447
translation,233,26,model,model,build,tutorial question - answering system,model build tutorial question - answering system,0.6579982042312622
translation,233,27,model,answers,by predicting,most relevant answer,answers by predicting most relevant answer,0.5830672979354858
translation,233,27,model,user 's question,by predicting,most relevant answer,user 's question by predicting most relevant answer,0.6359807848930359
translation,233,27,model,most relevant answer,from,set of predefined answers,most relevant answer from set of predefined answers,0.5163512229919434
translation,233,27,model,retrieval - based model,has,answers,retrieval - based model has answers,0.5973559021949768
translation,233,27,model,answers,has,user 's question,answers has user 's question,0.5823949575424194
translation,233,27,model,model,has,retrieval - based model,model has retrieval - based model,0.5546078085899353
translation,233,30,model,hybrid approach,involving,combination of the retrieval - based and generative models,hybrid approach involving combination of the retrieval - based and generative models,0.5612476468086243
translation,233,30,model,model,usefulness of,hybrid approach,model usefulness of hybrid approach,0.6981733441352844
translation,233,98,model,three overarching approaches,of,retrievalbased models,three overarching approaches of retrievalbased models,0.5740378499031067
translation,233,98,model,three overarching approaches,of,generative models,three overarching approaches of generative models,0.5817038416862488
translation,233,98,model,three overarching approaches,of,hybrid models,three overarching approaches of hybrid models,0.5958531498908997
translation,233,98,model,hybrid models,for,java programming - based tutorial question answering,hybrid models for java programming - based tutorial question answering,0.5726811289787292
translation,233,98,model,model,explore,three overarching approaches,model explore three overarching approaches,0.6760302186012268
translation,233,138,model,decoder,is,standard recurrent neural network,decoder is standard recurrent neural network,0.5822525024414062
translation,233,138,model,standard recurrent neural network,with,lstm cells,standard recurrent neural network with lstm cells,0.6255834698677063
translation,233,138,model,model,has,decoder,model has decoder,0.6226420402526855
translation,233,166,results,tf -idf,able to,slightly outperform,tf -idf able to slightly outperform,0.661899209022522
translation,233,166,results,tf -idf,for,recall@2 and recall@5,tf -idf for recall@2 and recall@5,0.6015996336936951
translation,233,166,results,dual encoder,at,recall@1 scores,dual encoder at recall@1 scores,0.535097062587738
translation,233,166,results,tf - idf,for,recall@2 and recall@5,tf - idf for recall@2 and recall@5,0.6015996336936951
translation,233,166,results,dade,has,tf -idf,dade has tf -idf,0.6078420877456665
translation,233,166,results,slightly outperform,has,dual encoder,slightly outperform has dual encoder,0.598401665687561
translation,233,166,results,dual encoder,has,outperforms,dual encoder has outperforms,0.6258973479270935
translation,233,166,results,outperforms,has,tf - idf,outperforms has tf - idf,0.6042096018791199
translation,233,166,results,results,see that,dade,results see that dade,0.6944723129272461
translation,233,166,results,results,for,dade,results for dade,0.6552020907402039
translation,233,167,results,qdde,outperformed by,tf -idf,qdde outperformed by tf -idf,0.7637422680854797
translation,233,167,results,tf -idf,in,recall@1 and recall@2,tf -idf in recall@1 and recall@2,0.5236505270004272
translation,233,167,results,results,see that,qdde,results see that qdde,0.650084376335144
translation,233,176,results,dev and test perplexity scores,better than,previous model,dev and test perplexity scores better than previous model,0.6942313313484192
translation,233,176,results,previous model,with,scores,previous model with scores,0.6186313033103943
translation,233,176,results,scores,of,68.91 and 70.15,scores of 68.91 and 70.15,0.5547084808349609
translation,233,176,results,results,has,dev and test perplexity scores,results has dev and test perplexity scores,0.5023580193519592
translation,235,18,baselines,hierarchical and overlapping structures,in,question categories,hierarchical and overlapping structures in question categories,0.5359541177749634
translation,235,6,model,answer information,into,question modeling,answer information into question modeling,0.6164669990539551
translation,235,6,model,answer information,introduce,novel group sparse autoencoders,answer information introduce novel group sparse autoencoders,0.5900750160217285
translation,235,6,model,novel group sparse autoencoders,refine,question representation,novel group sparse autoencoders refine question representation,0.6569673418998718
translation,235,6,model,question representation,by utilizing,group information,question representation by utilizing group information,0.6476290225982666
translation,235,6,model,group information,in,answer set,group information in answer set,0.49051451683044434
translation,235,6,model,model,to consider,answer information,model to consider answer information,0.7253862023353577
translation,235,6,model,model,introduce,novel group sparse autoencoders,model introduce novel group sparse autoencoders,0.6538200378417969
translation,235,7,model,novel group sparse cnns,naturally learn,question representation,novel group sparse cnns naturally learn question representation,0.7239970564842224
translation,235,7,model,question representation,with respect to,answers,question representation with respect to answers,0.7071447968482971
translation,235,7,model,question representation,by implanting,group sparse autoencoders,question representation by implanting group sparse autoencoders,0.6612842082977295
translation,235,7,model,group sparse autoencoders,into,traditional cnns,group sparse autoencoders into traditional cnns,0.5859618186950684
translation,235,7,model,model,propose,novel group sparse cnns,model propose novel group sparse cnns,0.6832632422447205
translation,235,19,model,learning procedure,first builds,dictionary,learning procedure first builds dictionary,0.771736741065979
translation,235,19,model,dictionary,with,series of grouped bases,dictionary with series of grouped bases,0.6649528741836548
translation,235,19,model,model,has,learning procedure,model has learning procedure,0.5516252517700195
translation,235,21,model,dictionary learning,to,cnn,dictionary learning to cnn,0.5793269276618958
translation,235,21,model,dictionary learning,first develop,neural version,dictionary learning first develop neural version,0.6752148270606995
translation,235,21,model,neural version,of,sgl,neural version of sgl,0.6088177561759949
translation,235,21,model,neural version,is,first full neural model,neural version is first full neural model,0.5458701252937317
translation,235,21,model,model,To apply,dictionary learning,model To apply dictionary learning,0.6773906946182251
translation,235,26,model,questions,with,sparsity,questions with sparsity,0.6583042144775391
translation,235,26,model,novel group sparse convolutional neural networks ( gscnns ),by implanting,gsa,novel group sparse convolutional neural networks ( gscnns ) by implanting gsa,0.7555358409881592
translation,235,26,model,gsa,onto,cnns,gsa onto cnns,0.6958144307136536
translation,235,26,model,gsa,enforcing,group sparsity,gsa enforcing group sparsity,0.7876449227333069
translation,235,26,model,group sparsity,between,convolutional and classification layers,group sparsity between convolutional and classification layers,0.6330364942550659
translation,235,26,model,model,to model,questions,model to model questions,0.8027057647705078
translation,235,26,model,model,with,sparsity,model with sparsity,0.6517093181610107
translation,235,111,model,question sentences,with,answer sets and group structure,question sentences with answer sets and group structure,0.6316536068916321
translation,235,111,model,neural version,of,dictionary learning,neural version of dictionary learning,0.59147709608078
translation,235,111,model,novel gsa framework,has,neural version,novel gsa framework has neural version,0.567377507686615
translation,235,111,model,model,to better represent,question sentences,model to better represent question sentences,0.6551108360290527
translation,235,111,model,model,presented,novel gsa framework,model presented novel gsa framework,0.6917980313301086
translation,235,96,results,improvements,are,substantial,improvements are substantial,0.6304974555969238
translation,235,96,results,improvements,are,not as significant,improvements are not as significant,0.5871109366416931
translation,235,96,results,substantial,for,in - surance and dmv,substantial for in - surance and dmv,0.6867035031318665
translation,235,96,results,not as significant,for,yahoo and trec,not as significant for yahoo and trec,0.6503605842590332
translation,235,96,results,results,has,improvements,results has improvements,0.615561842918396
translation,236,79,baselines,gru,has,"cho et al. , 2014 )","gru has cho et al. , 2014 )",0.5777172446250916
translation,236,80,experimental-setup,neural network model,implemented using,"keras ( chollet , 2015 )","neural network model implemented using keras ( chollet , 2015 )",0.6769968271255493
translation,236,80,experimental-setup,experimental setup,has,neural network model,experimental setup has neural network model,0.5400120615959167
translation,236,5,model,syntactic and semantic information,to build,single and meaningful embedding space,syntactic and semantic information to build single and meaningful embedding space,0.6663259267807007
translation,236,5,model,model,exploits,syntactic and semantic information,model exploits syntactic and semantic information,0.6935235857963562
translation,237,180,ablation-analysis,"all of unans , ans , and ans + unans augmented datasets",further improve,performance,"all of unans , ans , and ans + unans augmented datasets further improve performance",0.6937480568885803
translation,237,180,ablation-analysis,ablation analysis,has,"all of unans , ans , and ans + unans augmented datasets","ablation analysis has all of unans , ans , and ans + unans augmented datasets",0.521796703338623
translation,237,194,ablation-analysis,crqda,increases,accuracy,crqda increases accuracy,0.7337947487831116
translation,237,194,ablation-analysis,accuracy,of,bert large model,accuracy of bert large model,0.6186099052429199
translation,237,194,ablation-analysis,bert large model,by,0.7 %,bert large model by 0.7 %,0.6131762862205505
translation,237,194,ablation-analysis,ablation analysis,has,crqda,ablation analysis has crqda,0.5455304980278015
translation,237,6,baselines,crqda,utilizes,transformer autoencoder,crqda utilizes transformer autoencoder,0.5928064584732056
translation,237,6,baselines,transformer autoencoder,to map,original discrete question,transformer autoencoder to map original discrete question,0.6971691250801086
translation,237,6,baselines,original discrete question,into,continuous embedding space,original discrete question into continuous embedding space,0.6072868704795837
translation,237,6,baselines,baselines,has,crqda,baselines has crqda,0.5788395404815674
translation,237,42,baselines,cbert,retrofits,"bert ( devlin et al. , 2018 )","cbert retrofits bert ( devlin et al. , 2018 )",0.7093388438224792
translation,237,42,baselines,cbert,retrofits,conditional bert,cbert retrofits conditional bert,0.7113751173019409
translation,237,42,baselines,conditional bert,to predict,masked tokens,conditional bert to predict masked tokens,0.7388029098510742
translation,237,42,baselines,baselines,has,cbert,baselines has cbert,0.5764039158821106
translation,237,125,baselines,"eda ( wei and zou , 2019 )",augments,question data,"eda ( wei and zou , 2019 ) augments question data",0.6980677247047424
translation,237,125,baselines,question data,by performing,synonym replacement,question data by performing synonym replacement,0.6739793419837952
translation,237,125,baselines,question data,by performing,random insertion,question data by performing random insertion,0.6755749583244324
translation,237,125,baselines,question data,by performing,random swap,question data by performing random swap,0.659397304058075
translation,237,125,baselines,question data,by performing,random deletion operation,question data by performing random deletion operation,0.7043436169624329
translation,237,126,baselines,eda,to synthesize,new question data,eda to synthesize new question data,0.6150481700897217
translation,237,126,baselines,new question data,for,each question,new question data for each question,0.6362982988357544
translation,237,126,baselines,each question,of,squad 2.0,each question of squad 2.0,0.6023641228675842
translation,237,126,baselines,back - translation,uses,machine translation model,back - translation uses machine translation model,0.5514034032821655
translation,237,126,baselines,machine translation model,to translate,questions,machine translation model to translate questions,0.6679651737213135
translation,237,126,baselines,questions,into,french and back into english,questions into french and back into english,0.6181508302688599
translation,237,127,baselines,back - translation,based on,source code,back - translation based on source code,0.6593612432479858
translation,237,127,baselines,back - translation,to generate,new question data,back - translation to generate new question data,0.6885754466056824
translation,237,127,baselines,new question data,for,each original question,new question data for each original question,0.6191630959510803
translation,237,127,baselines,new question data,for,each question,new question data for each question,0.6362982988357544
translation,237,127,baselines,new question data,for,each question,new question data for each question,0.6362982988357544
translation,237,127,baselines,text-vae,uses,rnnbased vae,text-vae uses rnnbased vae,0.6723818778991699
translation,237,127,baselines,rnnbased vae,to generate,new question data,rnnbased vae to generate new question data,0.7146404981613159
translation,237,127,baselines,new question data,for,each question,new question data for each question,0.6362982988357544
translation,237,127,baselines,new question data,of,squad 2.0,new question data of squad 2.0,0.5863431096076965
translation,237,127,baselines,each question,of,squad 2.0,each question of squad 2.0,0.6023641228675842
translation,237,196,baselines,crqda,treats,question data augmentation task,crqda treats question data augmentation task,0.634545087814331
translation,237,196,baselines,question data augmentation task,as,constrained question rewriting problem,question data augmentation task as constrained question rewriting problem,0.4512856602668762
translation,237,196,baselines,baselines,has,crqda,baselines has crqda,0.5788395404815674
translation,237,120,experiments,autoencoder,trains on,"bookcorpus ( zhu et al. , 2015 )","autoencoder trains on bookcorpus ( zhu et al. , 2015 )",0.6501244306564331
translation,237,120,experiments,autoencoder,trains on,"english wikipedia ( devlin et al. , 2018 )","autoencoder trains on english wikipedia ( devlin et al. , 2018 )",0.644933819770813
translation,237,145,experiments,large-scale corpora,to train,autoencoder,large-scale corpora to train autoencoder,0.6616171002388
translation,237,145,experiments,large-scale corpora,has,english wikipedia and bookcorpus,large-scale corpora has english wikipedia and bookcorpus,0.5319744944572449
translation,237,146,experiments,autoencoder,on,english wikipedia and bookcorpus,autoencoder on english wikipedia and bookcorpus,0.5044287443161011
translation,237,121,hyperparameters,training steps,set to,"64 , 256 , 5e - 5 and 100,000","training steps set to 64 , 256 , 5e - 5 and 100,000",0.6982350945472717
translation,237,121,hyperparameters,hyperparameters,has,"sequence length , batch size","hyperparameters has sequence length , batch size",0.5149966478347778
translation,237,122,hyperparameters,each original answerable data,use,crqda,each original answerable data use crqda,0.628223180770874
translation,237,122,hyperparameters,crqda,to generate,new unanswerable question data,crqda to generate new unanswerable question data,0.7051777839660645
translation,237,122,hyperparameters,new unanswerable question data,resulting in,about 220 k data samples,new unanswerable question data resulting in about 220 k data samples,0.6437080502510071
translation,237,122,hyperparameters,hyperparameters,For,each original answerable data,hyperparameters For each original answerable data,0.5559402704238892
translation,237,123,hyperparameters,max-step,set to,"0.9 , 0.5 , 0.5 , 0.99 , and 5","max-step set to 0.9 , 0.5 , 0.5 , 0.99 , and 5",0.7230896353721619
translation,237,123,hyperparameters,hyperparameters,has,hyperparameter,hyperparameters has hyperparameter,0.48939356207847595
translation,237,147,hyperparameters,15 % tokens,of,encoder inputs,15 % tokens of encoder inputs,0.5865704417228699
translation,237,147,hyperparameters,hyperparameters,randomly mask,15 % tokens,hyperparameters randomly mask 15 % tokens,0.7518253922462463
translation,237,4,model,novel data augmentation method,referred to,controllable rewriting based question data augmentation ( crqda ),novel data augmentation method referred to controllable rewriting based question data augmentation ( crqda ),0.5581039190292358
translation,237,4,model,novel data augmentation method,referred to,machine reading comprehension ( mrc ),novel data augmentation method referred to machine reading comprehension ( mrc ),0.5901080965995789
translation,237,4,model,novel data augmentation method,referred to,question generation,novel data augmentation method referred to question generation,0.5837875008583069
translation,237,4,model,novel data augmentation method,referred to,question - answering natural language inference tasks,novel data augmentation method referred to question - answering natural language inference tasks,0.5203654766082764
translation,237,4,model,novel data augmentation method,for,machine reading comprehension ( mrc ),novel data augmentation method for machine reading comprehension ( mrc ),0.5826217532157898
translation,237,4,model,novel data augmentation method,for,question generation,novel data augmentation method for question generation,0.57937091588974
translation,237,4,model,novel data augmentation method,for,question - answering natural language inference tasks,novel data augmentation method for question - answering natural language inference tasks,0.5032445192337036
translation,237,4,model,model,propose,novel data augmentation method,model propose novel data augmentation method,0.6812246441841125
translation,237,5,model,question data augmentation task,as,constrained question rewriting problem,question data augmentation task as constrained question rewriting problem,0.4512856602668762
translation,237,5,model,constrained question rewriting problem,to generate,"context-relevant , high-quality , and diverse question data samples","constrained question rewriting problem to generate context-relevant , high-quality , and diverse question data samples",0.6553513407707214
translation,237,5,model,model,treat,question data augmentation task,model treat question data augmentation task,0.5791735649108887
translation,237,7,model,pre-trained mrc model,to revise,question representation,pre-trained mrc model to revise question representation,0.6287438869476318
translation,237,7,model,question representation,with,gradientbased optimization,question representation with gradientbased optimization,0.5771912336349487
translation,237,7,model,iteratively,with,gradientbased optimization,iteratively with gradientbased optimization,0.6300278902053833
translation,237,7,model,question representation,has,iteratively,question representation has iteratively,0.608900785446167
translation,237,7,model,model,uses,pre-trained mrc model,model uses pre-trained mrc model,0.5767772793769836
translation,237,24,model,qda method,called,controllable rewriting based question data augmentation ( crqda ),qda method called controllable rewriting based question data augmentation ( crqda ),0.6476569771766663
translation,237,24,model,qda method,generate,new context-relevant answerable questions,qda method generate new context-relevant answerable questions,0.6344570517539978
translation,237,24,model,qda method,generate,unanswerable questions,qda method generate unanswerable questions,0.6660922169685364
translation,237,24,model,model,propose,qda method,model propose qda method,0.6667240262031555
translation,237,25,model,crqda,treat,qda task,crqda treat qda task,0.619377076625824
translation,237,25,model,qda task,as,constrained question rewriting problem,qda task as constrained question rewriting problem,0.4581623077392578
translation,237,25,model,model,treat,qda task,model treat qda task,0.6226631999015808
translation,237,26,model,crqda,revise,original questions,crqda revise original questions,0.6690124869346619
translation,237,26,model,original questions,in,continuous embedding space,original questions in continuous embedding space,0.5260201692581177
translation,237,27,model,two components,of,crqda,two components of crqda,0.5957508683204651
translation,237,27,model,transformer - based autoencoder,whose,encoder,transformer - based autoencoder whose encoder,0.6080787777900696
translation,237,27,model,encoder,maps,question,encoder maps question,0.7551298141479492
translation,237,27,model,question,into,latent representation,question into latent representation,0.6157558560371399
translation,237,55,model,original answerable question,to,relevant unanswerable question,original answerable question to relevant unanswerable question,0.539455235004425
translation,237,55,model,relevant unanswerable question,in,unsupervised paradigm,relevant unanswerable question in unsupervised paradigm,0.5060932040214539
translation,237,55,model,original answerable question,to,new relevant answerable question,original answerable question to new relevant answerable question,0.557878851890564
translation,237,119,model,encoder and decoder,consist of,6 - layer transformers,encoder and decoder consist of 6 - layer transformers,0.6988130807876587
translation,237,119,model,6 - layer transformers,where,inner dimension,6 - layer transformers where inner dimension,0.6530666947364807
translation,237,119,model,inner dimension,of,feed -forward networks ( ffn ),inner dimension of feed -forward networks ( ffn ),0.5727778673171997
translation,237,119,model,inner dimension,of,hidden state size,inner dimension of hidden state size,0.5528548359870911
translation,237,119,model,inner dimension,of,number of attention head,inner dimension of number of attention head,0.5823279023170471
translation,237,119,model,number of attention head,set to,"4096 , 1024 , and 16","number of attention head set to 4096 , 1024 , and 16",0.6730321049690247
translation,237,195,model,novel question data augmentation method,called,crqda,novel question data augmentation method called crqda,0.6391265988349915
translation,237,195,model,novel question data augmentation method,for,contextrelevant answerable and unanswerable question generation,novel question data augmentation method for contextrelevant answerable and unanswerable question generation,0.5570569634437561
translation,237,195,model,model,present,novel question data augmentation method,model present novel question data augmentation method,0.6516604423522949
translation,237,197,model,original question,decoded back to,discrete space,original question decoded back to discrete space,0.6859996914863586
translation,237,197,model,revised,in,continuous embedding space,revised in continuous embedding space,0.5579026937484741
translation,237,197,model,continuous embedding space,with,gradient - based optimization,continuous embedding space with gradient - based optimization,0.6101817488670349
translation,237,197,model,discrete space,as,new question data sample,discrete space as new question data sample,0.5549936294555664
translation,237,197,model,pre-trained mrc model,has,original question,pre-trained mrc model has original question,0.5961613655090332
translation,237,197,model,model,guidance of,pre-trained mrc model,model guidance of pre-trained mrc model,0.6370723247528076
translation,237,35,results,state - of - the - art textual da methods,on,squad 2.0 dataset,state - of - the - art textual da methods on squad 2.0 dataset,0.4572618305683136
translation,237,35,results,crqda,has,outperforms,crqda has outperforms,0.6371902823448181
translation,237,35,results,outperforms,has,all those strong baselines,outperforms has all those strong baselines,0.5929060578346252
translation,237,35,results,results,compare,proposed crqda,results compare proposed crqda,0.6710947751998901
translation,237,134,results,popular textual da methods,including,eda,popular textual da methods including eda,0.6833329200744629
translation,237,134,results,popular textual da methods,including,back- translation,popular textual da methods including back- translation,0.681934118270874
translation,237,134,results,popular textual da methods,including,text - vae,popular textual da methods including text - vae,0.6856817603111267
translation,237,134,results,popular textual da methods,including,ae with noised,popular textual da methods including ae with noised,0.6998215317726135
translation,237,134,results,popular textual da methods,do not improve,performance,popular textual da methods do not improve performance,0.6396510004997253
translation,237,134,results,performance,of,mrc model,performance of mrc model,0.5902660489082336
translation,237,134,results,results,has,popular textual da methods,results has popular textual da methods,0.49444150924682617
translation,237,136,results,qda methods,including,3 m synth,qda methods including 3 m synth,0.7392380237579346
translation,237,136,results,qda methods,including,unansq,qda methods including unansq,0.6940624117851257
translation,237,136,results,qda methods,including,crqda,qda methods including crqda,0.6814618706703186
translation,237,136,results,qda methods,improve,model performance,qda methods improve model performance,0.6675356030464172
translation,237,136,results,sharp contrast,has,qda methods,sharp contrast has qda methods,0.5525375604629517
translation,237,136,results,results,In,sharp contrast,results In sharp contrast,0.5936019420623779
translation,237,137,results,crqda,brings,1.5 f1 score improvement,crqda brings 1.5 f1 score improvement,0.5682199597358704
translation,237,137,results,outperforms,brings,1.5 f1 score improvement,outperforms brings 1.5 f1 score improvement,0.6065094470977783
translation,237,137,results,all the strong baselines,brings,1.9 absolute em score,all the strong baselines brings 1.9 absolute em score,0.6077643036842346
translation,237,137,results,all the strong baselines,brings,1.5 f1 score improvement,all the strong baselines brings 1.5 f1 score improvement,0.5711824893951416
translation,237,137,results,1.5 f1 score improvement,based on,bert large,1.5 f1 score improvement based on bert large,0.6609107851982117
translation,237,137,results,crqda,has,outperforms,crqda has outperforms,0.6371902823448181
translation,237,137,results,outperforms,has,all the strong baselines,outperforms has all the strong baselines,0.6019706130027771
translation,237,137,results,results,has,crqda,results has crqda,0.5196598172187805
translation,237,142,results,crqda,improve,performance,crqda improve performance,0.6462624073028564
translation,237,142,results,performance,of,each mrc model,performance of each mrc model,0.6041358113288879
translation,237,142,results,performance,yielding,2.4 absolute f1 improvement,performance yielding 2.4 absolute f1 improvement,0.5818049907684326
translation,237,142,results,performance,yielding,1.5 absolute f1 improvement,performance yielding 1.5 absolute f1 improvement,0.5874800086021423
translation,237,142,results,2.4 absolute f1 improvement,with,bert base model,2.4 absolute f1 improvement with bert base model,0.5938876867294312
translation,237,142,results,1.5 absolute f1 improvement,with,roberta base,1.5 absolute f1 improvement with roberta base,0.6258505582809448
translation,237,142,results,results,see that,crqda,results see that crqda,0.648131251335144
translation,237,143,results,roberta large model,to guide,rewriting,roberta large model to guide rewriting,0.6672672629356384
translation,237,143,results,rewriting,of,question data,rewriting of question data,0.5794032216072083
translation,237,143,results,augmented dataset,can further improve,performance,augmented dataset can further improve performance,0.69294273853302
translation,237,143,results,roberta large model,has,augmented dataset,roberta large model has augmented dataset,0.5621419548988342
translation,237,143,results,results,use,roberta large model,results use roberta large model,0.6505929231643677
translation,237,153,results,reconstruction performance,of,autoencoder,reconstruction performance of autoencoder,0.5780004858970642
translation,237,153,results,reconstruction performance,is,better,reconstruction performance is better,0.6021655201911926
translation,237,153,results,more training data,has,reconstruction performance,more training data has reconstruction performance,0.5341347455978394
translation,237,153,results,results,with,more training data,results with more training data,0.6035581827163696
translation,237,154,results,performance,of,finetuned bert base model,performance of finetuned bert base model,0.6100313067436218
translation,237,154,results,finetuned bert base model,is,better,finetuned bert base model is better,0.5981746912002563
translation,237,154,results,results,has,performance,results has performance,0.5972660779953003
translation,237,155,results,autoencoders,can reconstruct,almost all questions,autoencoders can reconstruct almost all questions,0.6684619188308716
translation,237,155,results,wiki and wiki + mask,has,autoencoders,wiki and wiki + mask has autoencoders,0.6164950728416443
translation,237,155,results,almost all questions,has,well,almost all questions has well,0.5770686268806458
translation,237,156,results,reconstruction performance,of,model,reconstruction performance of model,0.5570881366729736
translation,237,156,results,reconstruction performance,performs,best,reconstruction performance performs best,0.6537692546844482
translation,237,156,results,model,trained with,wiki + mask,model trained with wiki + mask,0.7730097770690918
translation,237,156,results,results,has,reconstruction performance,results has reconstruction performance,0.5450562834739685
translation,237,157,results,fine- tuned bert base model,with,autoencoder,fine- tuned bert base model with autoencoder,0.631269633769989
translation,237,157,results,fine- tuned bert base model,performs,better,fine- tuned bert base model performs better,0.639327347278595
translation,237,157,results,autoencoder,trained on,wiki,autoencoder trained on wiki,0.753854513168335
translation,237,157,results,better,trained on,wiki + mask,better trained on wiki + mask,0.7480251789093018
translation,237,157,results,results,has,fine- tuned bert base model,results has fine- tuned bert base model,0.5725985169410706
translation,237,179,results,mrc,achieves,best performance,mrc achieves best performance,0.7368338108062744
translation,237,179,results,best performance,when,? a = 0.5,best performance when ? a = 0.5,0.6373602151870728
translation,237,179,results,results,observed,mrc,results observed mrc,0.5056904554367065
translation,237,181,results,roberta large model,fine- tuned on,ans + unans,roberta large model fine- tuned on ans + unans,0.7088478207588196
translation,237,181,results,roberta large model,performs,worse,roberta large model performs worse,0.6536650657653809
translation,237,181,results,worse,fine-tuned on,unans,worse fine-tuned on unans,0.733462393283844
translation,237,181,results,results,find that,roberta large model,results find that roberta large model,0.6759124398231506
translation,237,182,results,result,is,mixed,result is mixed,0.613936185836792
translation,237,182,results,mixed,using,more augmented data,mixed using more augmented data,0.7354126572608948
translation,237,182,results,mixed,is,not always beneficial,mixed is not always beneficial,0.5740910768508911
translation,237,182,results,more augmented data,is,not always beneficial,more augmented data is not always beneficial,0.5814049243927002
translation,237,182,results,results,is,mixed,results is mixed,0.5619810223579407
translation,237,182,results,results,using,more augmented data,results using more augmented data,0.7006195187568665
translation,237,182,results,results,has,result,results has result,0.5303357243537903
translation,238,126,ablation-analysis,gold segmentation knowledge,increases,performance,gold segmentation knowledge increases performance,0.7000768780708313
translation,238,126,ablation-analysis,performance,of,parsers,performance of parsers,0.6048592329025269
translation,238,126,ablation-analysis,parsers,by,5.3- 5.9 %,parsers by 5.3- 5.9 %,0.5817461013793945
translation,238,126,ablation-analysis,ablation analysis,injecting,gold segmentation knowledge,ablation analysis injecting gold segmentation knowledge,0.6509459614753723
translation,238,232,ablation-analysis,belonging to either of the two subsets ( or both ),account for,50.2 %,belonging to either of the two subsets ( or both ) account for 50.2 %,0.7077766060829163
translation,238,232,ablation-analysis,50.2 %,of,full set,50.2 % of full set,0.5863558650016785
translation,238,232,ablation-analysis,benefit,of developing,more sophisticated ensemble approaches,benefit of developing more sophisticated ensemble approaches,0.6603133082389832
translation,238,232,ablation-analysis,queries,has,belonging to either of the two subsets ( or both ),queries has belonging to either of the two subsets ( or both ),0.6140824556350708
translation,238,232,ablation-analysis,ablation analysis,has,queries,ablation analysis has queries,0.5778759717941284
translation,238,35,baselines,distant supervision,in the form of,training set,distant supervision in the form of training set,0.653207004070282
translation,238,35,baselines,training set,consisting of,"millions of ( query , title ) pairs","training set consisting of millions of ( query , title ) pairs",0.6939800977706909
translation,238,8,model,dependency grammar,to account for,segments - independent syntactic units,dependency grammar to account for segments - independent syntactic units,0.6526804566383362
translation,238,8,model,segments - independent syntactic units,within,potentially larger syntactic structure,segments - independent syntactic units within potentially larger syntactic structure,0.6435559988021851
translation,238,8,model,model,extend,dependency grammar,model extend dependency grammar,0.6880468726158142
translation,238,9,model,two distant supervision approaches,for,task,two distant supervision approaches for task,0.6103790998458862
translation,238,9,model,model,propose,two distant supervision approaches,model propose two distant supervision approaches,0.6607508063316345
translation,238,11,model,"millions of ( query , page title ) pairs",from,community question answering ( cqa ) domain,"millions of ( query , page title ) pairs from community question answering ( cqa ) domain",0.5108746886253357
translation,238,11,model,community question answering ( cqa ) domain,where,cqa page,community question answering ( cqa ) domain where cqa page,0.6167986392974854
translation,238,11,model,model,trained on,"millions of ( query , page title ) pairs","model trained on millions of ( query , page title ) pairs",0.729439914226532
translation,238,28,model,standard dependency grammar,to de- scribe,syntax,standard dependency grammar to de- scribe syntax,0.6504169702529907
translation,238,28,model,syntax,of,queries,syntax of queries,0.5774703025817871
translation,238,28,model,queries,with,question intent,queries with question intent,0.561738908290863
translation,238,28,model,model,extend,standard dependency grammar,model extend standard dependency grammar,0.664498507976532
translation,238,132,model,parser 's output,aligned against,segmentation,parser 's output aligned against segmentation,0.7423197627067566
translation,238,132,model,segmentation,such that,dependency edges,segmentation such that dependency edges,0.5772542357444763
translation,238,132,model,dependency edges,which cross,segment boundaries,dependency edges which cross segment boundaries,0.692590594291687
translation,238,7,results,standard dependency grammar,does not account for,full range of syntactic structures,standard dependency grammar does not account for full range of syntactic structures,0.6237179636955261
translation,238,7,results,full range of syntactic structures,manifested by,queries,full range of syntactic structures manifested by queries,0.695682168006897
translation,238,7,results,queries,with,question intent,queries with question intent,0.561738908290863
translation,238,7,results,results,show,standard dependency grammar,results show standard dependency grammar,0.5871785879135132
translation,238,41,results,our joint model,for,subset of single-segment queries,our joint model for subset of single-segment queries,0.6208735108375549
translation,238,41,results,outperformed,on,uas,outperformed on uas,0.6010569930076599
translation,238,41,results,alternatives,on,uas,alternatives on uas,0.6482166647911072
translation,238,41,results,uas,for,full test set,uas for full test set,0.6379007697105408
translation,238,41,results,our new treebank,has,our joint model,our new treebank has our joint model,0.5656874179840088
translation,238,41,results,our joint model,has,outperformed,our joint model has outperformed,0.5864173769950867
translation,238,41,results,outperformed,has,alternatives,outperformed has alternatives,0.6404951214790344
translation,238,43,results,all other models,on,subset of multi-segment queries,all other models on subset of multi-segment queries,0.5202426314353943
translation,238,43,results,beat,has,all other models,beat has all other models,0.605745792388916
translation,238,43,results,results,has,beat,results has beat,0.4631986618041992
translation,238,127,results,ranking,of,parsers,ranking of parsers,0.582522988319397
translation,238,127,results,ranking,with respect to,accuracy,ranking with respect to accuracy,0.658350944519043
translation,238,127,results,ranking,with respect to,accuracy,ranking with respect to accuracy,0.658350944519043
translation,238,127,results,parsers,with respect to,accuracy,parsers with respect to accuracy,0.6350980401039124
translation,238,127,results,parsers,with respect to,accuracy,parsers with respect to accuracy,0.6350980401039124
translation,238,127,results,parsers,differs from,ranking,parsers differs from ranking,0.6291894912719727
translation,238,127,results,accuracy,on,queries,accuracy on queries,0.52599036693573
translation,238,127,results,accuracy,on,ontonotes,accuracy on ontonotes,0.5757758617401123
translation,238,127,results,accuracy,differs from,ranking,accuracy differs from ranking,0.6158067584037781
translation,238,127,results,accuracy,on,ontonotes,accuracy on ontonotes,0.5757758617401123
translation,238,127,results,ranking,with respect to,accuracy,ranking with respect to accuracy,0.658350944519043
translation,238,127,results,accuracy,on,ontonotes,accuracy on ontonotes,0.5757758617401123
translation,238,127,results,redshift,to,first place,redshift to first place,0.6431365013122559
translation,238,127,results,rbg,to,last place,rbg to last place,0.6015239953994751
translation,238,127,results,results,has,ranking,results has ranking,0.5808752179145813
translation,238,213,results,distant and manual supervision,superior to,baseline methods,distant and manual supervision superior to baseline methods,0.7044973373413086
translation,238,213,results,baseline methods,in,both measures,baseline methods in both measures,0.43429458141326904
translation,238,214,results,q2 qproj model,performs,best,q2 qproj model performs best,0.6275414824485779
translation,238,214,results,best,in terms of,uas ( 77.1 % ),best in terms of uas ( 77.1 % ),0.6829826235771179
translation,238,214,results,results,has,q2 qproj model,results has q2 qproj model,0.5587518811225891
translation,238,215,results,distant - supervised and supervised models,performance of,upper bound gold standard segmentation,distant - supervised and supervised models performance of upper bound gold standard segmentation,0.6388007998466492
translation,238,215,results,uas,has,distant - supervised and supervised models,uas has distant - supervised and supervised models,0.5770787596702576
translation,238,215,results,results,In terms of,uas,results In terms of uas,0.5341484546661377
translation,238,219,results,lm baseline,scores,substantially lower,lm baseline scores substantially lower,0.7271353006362915
translation,238,219,results,substantially lower,than,other models,substantially lower than other models,0.5918112993240356
translation,238,219,results,other models,in,both measures,other models in both measures,0.46408265829086304
translation,238,219,results,results,has,lm baseline,results has lm baseline,0.5558016300201416
translation,238,229,results,distpipe,improves over,sup,distpipe improves over sup,0.7528732419013977
translation,238,229,results,sup,by,5.8 segmentation f1 points,sup by 5.8 segmentation f1 points,0.6273804306983948
translation,238,229,results,sup,by,1.9 %,sup by 1.9 %,0.6395035982131958
translation,238,229,results,1.9 %,in,uas,1.9 % in uas,0.5724365711212158
translation,238,229,results,results,has,distpipe,results has distpipe,0.550476610660553
translation,238,230,results,q2 qproj,performs,lower,q2 qproj performs lower,0.7110973000526428
translation,238,230,results,lower,than,pipeline models,lower than pipeline models,0.5886203646659851
translation,238,230,results,pipeline models,on,both subsets,pipeline models on both subsets,0.5404496788978577
translation,238,230,results,results,has,q2 qproj,results has q2 qproj,0.5798121690750122
translation,238,234,results,pipeline model,achieving,segmentation f1,pipeline model achieving segmentation f1,0.6342435479164124
translation,238,234,results,distpipe,achieving,segmentation f1,distpipe achieving segmentation f1,0.6133164763450623
translation,238,234,results,excels,on,multi-segment subset,excels on multi-segment subset,0.5930814743041992
translation,238,234,results,segmentation f1,of,42.2,segmentation f1 of 42.2,0.5739405155181885
translation,238,234,results,uas,of,67.5 %,uas of 67.5 %,0.5846564769744873
translation,238,234,results,67.5 %,compared to,23.5 and 64.3 %,67.5 % compared to 23.5 and 64.3 %,0.6923387050628662
translation,238,234,results,pipeline model,has,excels,pipeline model has excels,0.5860891938209534
translation,238,234,results,distpipe,has,excels,distpipe has excels,0.6337530612945557
translation,238,234,results,results,has,pipeline model,results has pipeline model,0.5633829832077026
translation,238,234,results,results,has,distpipe,results has distpipe,0.550476610660553
translation,238,235,results,joint model q2 qproj,achieves,uas score,joint model q2 qproj achieves uas score,0.6802874207496643
translation,238,235,results,uas score,similar to,supervised model,uas score similar to supervised model,0.6325253248214722
translation,238,235,results,results,has,joint model q2 qproj,results has joint model q2 qproj,0.5800414681434631
translation,238,236,results,ensemble model ens,provides,additional improvement,ensemble model ens provides additional improvement,0.6791258454322815
translation,238,236,results,additional improvement,hinting that,sup and distpipe models,additional improvement hinting that sup and distpipe models,0.7168154120445251
translation,238,236,results,results,has,ensemble model ens,results has ensemble model ens,0.5767142176628113
translation,238,237,results,segmentation f1,of,clear,segmentation f1 of clear,0.6169619560241699
translation,238,237,results,clear,as low as,2.2,clear as low as 2.2,0.5300189256668091
translation,238,237,results,results,has,segmentation f1,results has segmentation f1,0.5721861124038696
translation,238,240,results,single-segment queries,has,our joint model,single-segment queries has our joint model,0.5696179270744324
translation,238,240,results,our joint model,has,q2 qproj,our joint model has q2 qproj,0.6278990507125854
translation,238,240,results,results,On,single-segment queries,results On single-segment queries,0.5570809245109558
translation,238,242,results,q2 qproj and distpipe models,compared to,near perfect single-segment detection,q2 qproj and distpipe models compared to near perfect single-segment detection,0.6519878506660461
translation,238,242,results,near perfect single-segment detection,of,supervised sup model ( 96.2 ),near perfect single-segment detection of supervised sup model ( 96.2 ),0.5428148508071899
translation,238,242,results,q2 qproj and distpipe models,has,over-segment ( 86.5 and 86.8 f1 ),q2 qproj and distpipe models has over-segment ( 86.5 and 86.8 f1 ),0.5694491863250732
translation,238,242,results,results,Both,q2 qproj and distpipe models,results Both q2 qproj and distpipe models,0.6794261932373047
translation,239,211,ablation-analysis,full model,achieves,best overall performance,full model achieves best overall performance,0.6896765828132629
translation,239,211,ablation-analysis,full model,removing,relevance regularization ( - rr ),full model removing relevance regularization ( - rr ),0.6915860176086426
translation,239,211,ablation-analysis,relevance regularization ( - rr ),from,qc model,relevance regularization ( - rr ) from qc model,0.5320237874984741
translation,239,211,ablation-analysis,relevance regularization ( - rr ),leads to,performance drop,relevance regularization ( - rr ) leads to performance drop,0.6777827143669128
translation,239,211,ablation-analysis,ablation analysis,has,full model,ablation analysis has full model,0.526444137096405
translation,239,212,ablation-analysis,performance,on,sql dataset,performance on sql dataset,0.532957911491394
translation,239,212,ablation-analysis,adversarial sampling,has,- as ),adversarial sampling has - as ),0.5955405235290527
translation,239,212,ablation-analysis,adversarial sampling,has,hurts,adversarial sampling has hurts,0.5420214533805847
translation,239,212,ablation-analysis,- as ),has,hurts,- as ) has hurts,0.683072030544281
translation,239,212,ablation-analysis,hurts,has,performance,hurts has performance,0.6043882966041565
translation,239,212,ablation-analysis,ablation analysis,removing,adversarial sampling,ablation analysis removing adversarial sampling,0.6868446469306946
translation,239,214,ablation-analysis,qc,as,pretraining ( - pretrain ),qc as pretraining ( - pretrain ),0.533723771572113
translation,239,214,ablation-analysis,qc,has,greatly hurts,qc has greatly hurts,0.6056787967681885
translation,239,214,ablation-analysis,pretraining ( - pretrain ),has,greatly hurts,pretraining ( - pretrain ) has greatly hurts,0.5959357619285583
translation,239,214,ablation-analysis,greatly hurts,has,performance,greatly hurts has performance,0.5966358184814453
translation,239,214,ablation-analysis,ablation analysis,removing,qc,ablation analysis removing qc,0.7296139597892761
translation,239,192,baselines,dcs,is,strong baseline,dcs is strong baseline,0.6191957592964172
translation,239,192,baselines,decatt,that uses,more complex attention mechanism,decatt that uses more complex attention mechanism,0.7103188633918762
translation,239,192,baselines,outperforms,has,decatt,outperforms has decatt,0.731158435344696
translation,239,192,baselines,baselines,has,dcs,baselines has dcs,0.6005768775939941
translation,239,195,baselines,  - rr   variant,apply,adversarial sampling,  - rr   variant apply adversarial sampling,0.6632537245750427
translation,239,195,baselines,adversarial sampling,without,qd relevance regularization,adversarial sampling without qd relevance regularization,0.6009411215782166
translation,239,195,baselines,baselines,has,  - rr   variant,baselines has   - rr   variant,0.6075536012649536
translation,239,176,hyperparameters,vocabulary embedding size,for,natural language and programming language,vocabulary embedding size for natural language and programming language,0.5577409267425537
translation,239,176,hyperparameters,vocabulary embedding size,set at,200,vocabulary embedding size set at 200,0.615066647529602
translation,239,176,hyperparameters,hyperparameters,has,vocabulary embedding size,hyperparameters has vocabulary embedding size,0.4784645140171051
translation,239,177,hyperparameters,lstm hidden size,is,400,lstm hidden size is 400,0.5766904950141907
translation,239,177,hyperparameters,hyperparameters,has,lstm hidden size,hyperparameters has lstm hidden size,0.49869272112846375
translation,239,178,hyperparameters,margin,in,hinge loss,margin in hinge loss,0.5059742331504822
translation,239,178,hyperparameters,hinge loss,is,0.05,hinge loss is 0.05,0.5666218400001526
translation,239,178,hyperparameters,hyperparameters,has,margin,hyperparameters has margin,0.5714497566223145
translation,239,179,hyperparameters,trained dcs model,used as,pre-training,trained dcs model used as pre-training,0.6058382391929626
translation,239,179,hyperparameters,pre-training,for,our models,pre-training for our models,0.5863898992538452
translation,239,179,hyperparameters,hyperparameters,has,trained dcs model,hyperparameters has trained dcs model,0.53076171875
translation,239,180,hyperparameters,learning rate,set at,1e - 4,learning rate set at 1e - 4,0.6137527227401733
translation,239,180,hyperparameters,dropout rate,set at,0.25,dropout rate set at 0.25,0.5988520979881287
translation,239,180,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,239,180,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,239,181,hyperparameters,adversarial training,set,0.2,adversarial training set 0.2,0.6108595728874207
translation,239,181,hyperparameters,adversarial training,to,0.2,adversarial training to 0.2,0.5291103720664978
translation,239,181,hyperparameters,adversarial training,limit,maximum number of epochs,adversarial training limit maximum number of epochs,0.6339312791824341
translation,239,181,hyperparameters,maximum number of epochs,to,300,maximum number of epochs to 300,0.5812228918075562
translation,239,181,hyperparameters,hyperparameters,For,adversarial training,hyperparameters For adversarial training,0.555512011051178
translation,239,182,hyperparameters,standard l2 - regularization,used on,all the models,standard l2 - regularization used on all the models,0.6426644921302795
translation,239,182,hyperparameters,hyperparameters,has,standard l2 - regularization,hyperparameters has standard l2 - regularization,0.4239281415939331
translation,239,185,hyperparameters,code preprocessing steps,for,python,code preprocessing steps for python,0.5675402283668518
translation,239,185,hyperparameters,code preprocessing steps,for,sql,code preprocessing steps for sql,0.6158179640769958
translation,239,185,hyperparameters,hyperparameters,follow,code preprocessing steps,hyperparameters follow code preprocessing steps,0.5790742635726929
translation,239,185,hyperparameters,hyperparameters,for,sql,hyperparameters for sql,0.644481897354126
translation,239,5,model,adversarial learning,for,code retrieval,adversarial learning for code retrieval,0.5996567606925964
translation,239,5,model,adversarial learning,regularized by,questiondescription relevance,adversarial learning regularized by questiondescription relevance,0.690110445022583
translation,239,5,model,model,propose,adversarial learning,model propose adversarial learning,0.6682887077331543
translation,239,6,model,simple adversarial learning technique,to generate,difficult code snippets,simple adversarial learning technique to generate difficult code snippets,0.6922887563705444
translation,239,6,model,difficult code snippets,given,input question,difficult code snippets given input question,0.6773184537887573
translation,239,6,model,model,adapt,simple adversarial learning technique,model adapt simple adversarial learning technique,0.7150468826293945
translation,239,7,model,question - description relevance,to regularize,adversarial learning,question - description relevance to regularize adversarial learning,0.6622414588928223
translation,239,7,model,adversarial learning,such that,generated code snippet,adversarial learning such that generated code snippet,0.5762819051742554
translation,239,7,model,generated code snippet,contribute,more,generated code snippet contribute more,0.6367679238319397
translation,239,7,model,more,to,code retrieval training loss,more to code retrieval training loss,0.47790420055389404
translation,239,7,model,model,leverage,question - description relevance,model leverage question - description relevance,0.7836926579475403
translation,239,32,model,adversarial training,to,code retrieval,adversarial training to code retrieval,0.5446545481681824
translation,239,32,model,model,introduce,adversarial training,model introduce adversarial training,0.6563392877578735
translation,239,34,model,model,adapt,generative adversarial sampling technique,model adapt generative adversarial sampling technique,0.6893288493156433
translation,239,35,model,question - description ( qd ) relevance,as,complementary uni-modal view,question - description ( qd ) relevance as complementary uni-modal view,0.4864170551300049
translation,239,35,model,complementary uni-modal view,to reweight,adversarial training samples,complementary uni-modal view to reweight adversarial training samples,0.600565493106842
translation,239,35,model,model,employ,question - description ( qd ) relevance,model employ question - description ( qd ) relevance,0.565528929233551
translation,239,52,model,adversarial learning,to alleviate,bi-modal learning challenges,adversarial learning to alleviate bi-modal learning challenges,0.5874722003936768
translation,239,52,model,model,introduce,adversarial learning,model introduce adversarial learning,0.652680516242981
translation,239,53,model,adversarial qc generator,selects,unpaired code snippets,adversarial qc generator selects unpaired code snippets,0.7029613852500916
translation,239,53,model,adversarial qc generator,employ,question - description ( qd ) relevance model,adversarial qc generator employ question - description ( qd ) relevance model,0.5146480202674866
translation,239,53,model,difficult,for,qc model,difficult for qc model,0.7102354168891907
translation,239,53,model,ability,to distinguish,top-ranked positive and negative samples,ability to distinguish top-ranked positive and negative samples,0.7317307591438293
translation,239,53,model,question - description ( qd ) relevance model,to provide,secondary view,question - description ( qd ) relevance model to provide secondary view,0.6789025664329529
translation,239,53,model,model,employ,question - description ( qd ) relevance model,model employ question - description ( qd ) relevance model,0.536460280418396
translation,239,194,results,our proposed learning algorithm,improve,qc performance,our proposed learning algorithm improve qc performance,0.6417365074157715
translation,239,194,results,qc performance,compared to,all the baselines,qc performance compared to all the baselines,0.6547293663024902
translation,239,194,results,results,has,our proposed learning algorithm,results has our proposed learning algorithm,0.5650600790977478
translation,239,198,results,qc learning curves,on,dev set,qc learning curves on dev set,0.5449800491333008
translation,239,204,results,outperform,has,python sql map ndcg map ndcg mtl -mlp,outperform has python sql map ndcg map ndcg mtl -mlp,0.585430920124054
translation,239,204,results,outperform,has,) mtl baselines,outperform has ) mtl baselines,0.5852042436599731
translation,239,210,results,variants,has,outperform,variants has outperform,0.6309973001480103
translation,239,210,results,outperform,has,qd baselines editdist,outperform has qd baselines editdist,0.5736548900604248
translation,239,210,results,results,has,our method,results has our method,0.5589964985847473
translation,239,216,results,qc performance,using,fixed qd model,qc performance using fixed qd model,0.6811848282814026
translation,239,216,results,fixed qd model,for,relevance regularization,fixed qd model for relevance regularization,0.5648235082626343
translation,239,216,results,ndcg=0.7205,for,python,ndcg=0.7205 for python,0.620803952217102
translation,239,216,results,ndcg=0.6398,for,sql,ndcg=0.6398 for sql,0.6247170567512512
translation,239,216,results,relevance regularization,has,map=0.6371,relevance regularization has map=0.6371,0.5459493398666382
translation,239,216,results,relevance regularization,has,ndcg=0.7205,relevance regularization has ndcg=0.7205,0.5281683206558228
translation,239,216,results,results,report,qc performance,results report qc performance,0.6673276424407959
translation,239,217,results,qd model,to update,consistently improves,qd model to update consistently improves,0.8151634335517883
translation,239,217,results,consistently improves,has,qc performance,consistently improves has qc performance,0.5953397154808044
translation,239,217,results,results,allowing,qd model,results allowing qd model,0.6629761457443237
translation,240,104,ablation-analysis,contribute significantly,to,performance,contribute significantly to performance,0.6050375699996948
translation,240,104,ablation-analysis,performance,of,qsbp,performance of qsbp,0.5984715223312378
translation,240,104,ablation-analysis,minimum dependency distance,has,query snowball,minimum dependency distance has query snowball,0.5671706199645996
translation,240,104,ablation-analysis,ablation analysis,has,minimum dependency distance,ablation analysis has minimum dependency distance,0.5384554266929626
translation,240,121,experiments,qsbp,is,top performer,qsbp is top performer,0.5632376074790955
translation,240,121,experiments,qsbp,is,top performer,qsbp is top performer,0.5632376074790955
translation,240,121,experiments,qsbp,is,top performer,qsbp is top performer,0.5632376074790955
translation,240,121,experiments,top performer,for,"bio , def and rel questions","top performer for bio , def and rel questions",0.6201964020729065
translation,240,121,experiments,top performer,for,event and why questions,top performer for event and why questions,0.6199890375137329
translation,240,121,experiments,top performer,for,event and why questions,top performer for event and why questions,0.6199890375137329
translation,240,121,experiments,qsbp ( idf ),is,top performer,qsbp ( idf ) is top performer,0.5481076240539551
translation,240,121,experiments,top performer,for,event and why questions,top performer for event and why questions,0.6199890375137329
translation,240,5,model,information need representation,build,co-occurrence graph,information need representation build co-occurrence graph,0.6309981942176819
translation,240,5,model,co-occurrence graph,to obtain,words,co-occurrence graph to obtain words,0.5788739919662476
translation,240,5,model,words,augment,original query terms,words augment original query terms,0.5326933860778809
translation,240,5,model,model,To enrich,information need representation,model To enrich information need representation,0.68525630235672
translation,240,6,model,summarization problem,as,maximum coverage problem,summarization problem as maximum coverage problem,0.4484170079231262
translation,240,6,model,maximum coverage problem,with,knapsack constraints,maximum coverage problem with knapsack constraints,0.5434677600860596
translation,240,6,model,knapsack constraints,based on,word pairs,knapsack constraints based on word pairs,0.6277709007263184
translation,240,6,model,word pairs,rather than,single words,word pairs rather than single words,0.6786264181137085
translation,240,6,model,model,formulate,summarization problem,model formulate summarization problem,0.6572495102882385
translation,240,35,model,indirect relationships,between,query and words,indirect relationships between query and words,0.6596969962120056
translation,240,35,model,indirect relationships,to enrich,information need representation,indirect relationships to enrich information need representation,0.6869610548019409
translation,240,35,model,query and words,in,documents,query and words in documents,0.5467111468315125
translation,240,109,model,summarization problem,as,mckp,summarization problem as mckp,0.4968043267726898
translation,240,109,model,mckp,based on,word pairs,mckp based on word pairs,0.6531344056129456
translation,240,109,model,word pairs,rather than,single words,word pairs rather than single words,0.6786264181137085
translation,240,109,model,model,formulated,summarization problem,model formulated summarization problem,0.6200669407844543
translation,240,103,results,significantly outperforms,in terms of,all evaluation metrics,significantly outperforms in terms of all evaluation metrics,0.6755868196487427
translation,240,103,results,baseline,in terms of,all evaluation metrics,baseline in terms of all evaluation metrics,0.5788062810897827
translation,240,103,results,qsbp and qsbp ( idf ),has,significantly outperforms,qsbp and qsbp ( idf ) has significantly outperforms,0.5985578894615173
translation,240,103,results,significantly outperforms,has,qsbp ( nodist ),significantly outperforms has qsbp ( nodist ),0.6051198840141296
translation,240,103,results,significantly outperforms,has,qsb,significantly outperforms has qsb,0.600073516368866
translation,240,103,results,significantly outperforms,has,wp,significantly outperforms has wp,0.6295053958892822
translation,240,103,results,significantly outperforms,has,baseline,significantly outperforms has baseline,0.6019589304924011
translation,240,103,results,results,observed that,qsbp and qsbp ( idf ),results observed that qsbp and qsbp ( idf ),0.700921893119812
translation,240,106,results,qsbp and qsbp ( idf ),achieve,0.312 and 0.313,qsbp and qsbp ( idf ) achieve 0.312 and 0.313,0.6192709803581238
translation,240,106,results,0.312 and 0.313,in,f3 score,0.312 and 0.313 in f3 score,0.5379848480224609
translation,240,106,results,results,has,qsbp and qsbp ( idf ),results has qsbp and qsbp ( idf ),0.552821934223175
translation,241,132,ablation-analysis,our pruning,improve,our best system,our pruning improve our best system,0.6744385361671448
translation,241,132,ablation-analysis,our best system,from,46.29 to 47.10 map,our best system from 46.29 to 47.10 map,0.5401256084442139
translation,241,132,ablation-analysis,ablation analysis,has,our pruning,ablation analysis has our pruning,0.5892077684402466
translation,241,94,experimental-setup,kernel - based learning models,implemented in,kelp,kernel - based learning models implemented in kelp,0.7262144088745117
translation,241,94,experimental-setup,experimental setup,has,kernel - based learning models,experimental setup has kernel - based learning models,0.5260736346244812
translation,241,6,model,system,is,refinement,system is refinement,0.6280415058135986
translation,241,6,model,refinement,of,kernel - based sentence pair,refinement of kernel - based sentence pair,0.5460026860237122
translation,241,6,model,model,has,system,model has system,0.6186560392379761
translation,241,8,results,primary submission,third in,subtasks b and c,primary submission third in subtasks b and c,0.6747514605522156
translation,241,8,results,results,has,primary submission,results has primary submission,0.4854085147380829
translation,241,21,results,primary submission,third in,subtasks b and c,primary submission third in subtasks b and c,0.6747514605522156
translation,241,21,results,results,has,primary submission,results has primary submission,0.4854085147380829
translation,241,107,results,excellent results,of,2016,excellent results of 2016,0.5727381706237793
translation,241,107,results,model,is,very accurate,model is very accurate,0.5741881132125854
translation,241,107,results,model,achieved,first position,model achieved first position,0.7308889031410217
translation,241,107,results,first position,in terms of,map,first position in terms of map,0.724517285823822
translation,241,107,results,excellent results,has,model,excellent results has model,0.5700657367706299
translation,241,107,results,2016,has,model,2016 has model,0.6018114686012268
translation,241,107,results,results,confirmed,excellent results,results confirmed excellent results,0.5807956457138062
translation,241,131,results,our best system,with or without,pruning,our best system with or without pruning,0.6832830309867859
translation,241,131,results,pruning,is,less accurate,pruning is less accurate,0.6031721234321594
translation,241,131,results,less accurate,than,submitted results,less accurate than submitted results,0.6029685139656067
translation,241,131,results,less accurate,producing,map,less accurate producing map,0.7980574369430542
translation,241,131,results,map,of,46.29,map of 46.29,0.605667233467102
translation,241,131,results,results,show,our best system,results show our best system,0.6367209553718567
translation,241,146,results,results,for,subtask c,results for subtask c,0.5909541249275208
translation,241,147,results,primary submission,achieved,third highest map,primary submission achieved third highest map,0.6867655515670776
translation,241,147,results,third highest map,among,5 systems,third highest map among 5 systems,0.6025142669677734
translation,241,147,results,results,has,primary submission,results has primary submission,0.4854085147380829
translation,241,156,results,our primary submission,achieved,third position,our primary submission achieved third position,0.6291493773460388
translation,241,156,results,third position,w.r.t.,map,third position w.r.t. map,0.6238667368888855
translation,241,156,results,third position,among,13 systems,third position among 13 systems,0.6330341696739197
translation,242,131,baselines,answer candidates,by,maximum tf.idf document retrieval score,answer candidates by maximum tf.idf document retrieval score,0.5310760736465454
translation,242,131,baselines,answer candidates,using,unboosted query,answer candidates using unboosted query,0.6627500653266907
translation,242,131,baselines,unboosted query,of,question and answer terms,unboosted query of question and answer terms,0.555187463760376
translation,242,133,baselines,same architecture,as,full model,same architecture as full model,0.5742858052253723
translation,242,141,experimental-setup,embeddings,size,50,embeddings size 50,0.6815401911735535
translation,242,141,experimental-setup,experimental setup,used,embeddings,experimental setup used embeddings,0.6404268145561218
translation,242,143,experimental-setup,neural model,implemented in,"keras ( chollet , 2015 )","neural model implemented in keras ( chollet , 2015 )",0.733224630355835
translation,242,143,experimental-setup,"keras ( chollet , 2015 )",using,"theano ( theano development team , 2016 ) backend","keras ( chollet , 2015 ) using theano ( theano development team , 2016 ) backend",0.697080135345459
translation,242,143,experimental-setup,experimental setup,has,neural model,experimental setup has neural model,0.5172881484031677
translation,242,144,experimental-setup,feedforward component,use,shallow neural network,feedforward component use shallow neural network,0.5947201251983643
translation,242,144,experimental-setup,shallow neural network,lightly tuned to have,single fullyconnected layer,shallow neural network lightly tuned to have single fullyconnected layer,0.6516208648681641
translation,242,144,experimental-setup,shallow neural network,lightly tuned to have,glorot uniform initialization,shallow neural network lightly tuned to have glorot uniform initialization,0.6340553760528564
translation,242,144,experimental-setup,shallow neural network,lightly tuned to have,tanh activation,shallow neural network lightly tuned to have tanh activation,0.6773751378059387
translation,242,144,experimental-setup,shallow neural network,lightly tuned to have,l2regularization,shallow neural network lightly tuned to have l2regularization,0.6426227688789368
translation,242,144,experimental-setup,single fullyconnected layer,containing,10 nodes,single fullyconnected layer containing 10 nodes,0.662514865398407
translation,242,144,experimental-setup,single fullyconnected layer,containing,tanh activation,single fullyconnected layer containing tanh activation,0.6540515422821045
translation,242,144,experimental-setup,experimental setup,For,feedforward component,experimental setup For feedforward component,0.5948288440704346
translation,242,145,experimental-setup,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,242,145,experimental-setup,learning rate,of,100 epochs,learning rate of 100 epochs,0.5740818381309509
translation,242,145,experimental-setup,batch size,of,32,batch size of 32,0.6741614937782288
translation,242,145,experimental-setup,early stopping,with,patience,early stopping with patience,0.6632037162780762
translation,242,145,experimental-setup,patience,of,5 epochs,patience of 5 epochs,0.6565783619880676
translation,242,145,experimental-setup,rm - sprop optimizer,has,learning rate,rm - sprop optimizer has learning rate,0.5136327743530273
translation,242,145,experimental-setup,rm - sprop optimizer,has,batch size,rm - sprop optimizer has batch size,0.5393846035003662
translation,242,145,experimental-setup,experimental setup,trained with,rm - sprop optimizer,experimental setup trained with rm - sprop optimizer,0.7267051339149475
translation,242,146,experimental-setup,loss function,used,margin,loss function used margin,0.6198410391807556
translation,242,146,experimental-setup,margin,of,1.0,margin of 1.0,0.6274542808532715
translation,242,146,experimental-setup,experimental setup,has,loss function,experimental setup has loss function,0.48357120156288147
translation,242,6,model,answer ranking,as,distant supervision,answer ranking as distant supervision,0.4670744836330414
translation,242,6,model,informative justifications,where,justifications,informative justifications where justifications,0.6098356246948242
translation,242,7,model,neural network architecture,for,qa,neural network architecture for qa,0.6626331806182861
translation,242,7,model,reranks,as,intermediate,reranks as intermediate,0.5704993605613708
translation,242,7,model,answer justifications,as,intermediate,answer justifications as intermediate,0.5502824783325195
translation,242,7,model,reranks,has,answer justifications,reranks has answer justifications,0.6160130500793457
translation,242,7,model,model,propose,neural network architecture,model propose neural network architecture,0.679180383682251
translation,242,9,results,end-to - end approach,able to,significantly improve,end-to - end approach able to significantly improve,0.6758477091789246
translation,242,9,results,significantly improve,upon,strong ir baseline,significantly improve upon strong ir baseline,0.6254662275314331
translation,242,9,results,strong ir baseline,in,justification ranking,strong ir baseline in justification ranking,0.4705043435096741
translation,242,9,results,strong ir baseline,in,answer selection,strong ir baseline in answer selection,0.4714373052120209
translation,242,9,results,justification ranking,has,+ 9 % rated highly relevant,justification ranking has + 9 % rated highly relevant,0.5453755855560303
translation,242,9,results,answer selection,has,+ 6 % p@1 ),answer selection has + 6 % p@1 ),0.6312430500984192
translation,242,9,results,results,show,end-to - end approach,results show end-to - end approach,0.6282833218574524
translation,242,9,results,results,with,end-to - end approach,results with end-to - end approach,0.6465063095092773
translation,242,39,results,large ( + 9 % ) improvement,in,generating,large ( + 9 % ) improvement in generating,0.5621592402458191
translation,242,39,results,high-quality justifications,over,strong information retrieval ( ir ) baseline,high-quality justifications over strong information retrieval ( ir ) baseline,0.6511210799217224
translation,242,39,results,generating,has,high-quality justifications,generating has high-quality justifications,0.5082557201385498
translation,242,39,results,results,demonstrate,large ( + 9 % ) improvement,results demonstrate large ( + 9 % ) improvement,0.622516393661499
translation,242,158,results,full model,combines,"ir ++ , lexical overlap , discourse , and embeddings - based features","full model combines ir ++ , lexical overlap , discourse , and embeddings - based features",0.6949446797370911
translation,242,158,results,p@1,of,53.3 %,p@1 of 53.3 %,0.5895817875862122
translation,242,158,results,absolute gain,of,6.3 %,absolute gain of 6.3 %,0.5585235357284546
translation,242,158,results,absolute gain,over,strong ir baseline,absolute gain over strong ir baseline,0.6561254858970642
translation,242,158,results,"ir ++ , lexical overlap , discourse , and embeddings - based features",has,p@1,"ir ++ , lexical overlap , discourse , and embeddings - based features has p@1",0.6174345016479492
translation,242,158,results,53.3 %,has,absolute gain,53.3 % has absolute gain,0.5571878552436829
translation,242,158,results,results,has,full model,results has full model,0.523858904838562
translation,242,159,results,results,has,comparison to previous work,results has comparison to previous work,0.541883111000061
translation,242,164,results,loose comparison,has,our model,loose comparison has our model,0.5557830333709717
translation,242,164,results,our model,has,approximately 5 % higher performance,our model has approximately 5 % higher performance,0.5593339204788208
translation,242,172,results,competed,in,kaggle challenge,competed in kaggle challenge,0.5984787344932556
translation,242,172,results,our system,comes in in,7th place,our system comes in in 7th place,0.7205156683921814
translation,242,172,results,7th place,out of,170 competitors,7th place out of 170 competitors,0.6955475807189941
translation,242,172,results,kaggle challenge,has,our system,kaggle challenge has our system,0.6431456804275513
translation,242,172,results,results,In comparison to,other systems,results In comparison to other systems,0.6567578315734863
translation,242,194,results,61 %,of,top-ranked justifications,61 % of top-ranked justifications,0.569320023059845
translation,242,194,results,top-ranked justifications,from,our system,top-ranked justifications from our system,0.5403568744659424
translation,242,194,results,top-ranked justifications,rated as,good,top-ranked justifications rated as good,0.7077376246452332
translation,242,194,results,good,compared to,52 %,good compared to 52 %,0.6487273573875427
translation,242,194,results,52 %,from,ir baseline,52 % from ir baseline,0.5883536338806152
translation,242,196,results,ndcg@5,of,0.62,ndcg@5 of 0.62,0.5891302824020386
translation,242,196,results,ir baseline,has,significantly lower ndcg@5,ir baseline has significantly lower ndcg@5,0.5831747651100159
translation,243,63,baselines,"hard form of std , hard typed decoder ( htd )",control,decoding process,"hard form of std , hard typed decoder ( htd ) control decoding process",0.7647885084152222
translation,243,63,baselines,decoding process,by approximating,operation of argmax,decoding process by approximating operation of argmax,0.7318652868270874
translation,243,63,baselines,operation of argmax,with,"gumbel-softmax ( jang et al. , 2016 )","operation of argmax with gumbel-softmax ( jang et al. , 2016 )",0.5948078632354736
translation,243,63,baselines,decoding process,has,explicitly,decoding process has explicitly,0.5908756256103516
translation,243,137,baselines,simple encoder-decoder,with,attention mechanisms,simple encoder-decoder with attention mechanisms,0.6365422010421753
translation,243,137,baselines,mechanism- aware ( ma ) model,applies,multiple responding mechanisms,mechanism- aware ( ma ) model applies multiple responding mechanisms,0.6225218772888184
translation,243,137,baselines,multiple responding mechanisms,represented by,real-valued vectors,multiple responding mechanisms represented by real-valued vectors,0.7296827435493469
translation,243,137,baselines,seq2seq,has,simple encoder-decoder,seq2seq has simple encoder-decoder,0.5651999711990356
translation,243,137,baselines,ma,has,mechanism- aware ( ma ) model,ma has mechanism- aware ( ma ) model,0.5636188983917236
translation,243,137,baselines,baselines,has,seq2seq,baselines has seq2seq,0.5571820139884949
translation,243,139,baselines,topic-aware ( ta ) model,generates,informative responses,topic-aware ( ta ) model generates informative responses,0.6422845125198364
translation,243,139,baselines,informative responses,by incorporating,topic words,informative responses by incorporating topic words,0.6255211234092712
translation,243,139,baselines,topic words,predicted from,input post,topic words predicted from input post,0.6992112398147583
translation,243,139,baselines,erm,adaptively selects,subset of responding mechanisms,erm adaptively selects subset of responding mechanisms,0.7933949828147888
translation,243,139,baselines,elastic responding machine ( erm ),adaptively selects,subset of responding mechanisms,elastic responding machine ( erm ) adaptively selects subset of responding mechanisms,0.7850080132484436
translation,243,139,baselines,subset of responding mechanisms,using,reinforcement learning,subset of responding mechanisms using reinforcement learning,0.6299499273300171
translation,243,139,baselines,ta,has,topic-aware ( ta ) model,ta has topic-aware ( ta ) model,0.5654245615005493
translation,243,139,baselines,erm,has,elastic responding machine ( erm ),erm has elastic responding machine ( erm ),0.5508731603622437
translation,243,139,baselines,baselines,has,ta,baselines has ta,0.6096329092979431
translation,243,138,experimental-setup,number of mechanisms,set to,4,number of mechanisms set to 4,0.7139313817024231
translation,243,138,experimental-setup,one response,from,generated responses,one response from generated responses,0.5672152638435364
translation,243,138,experimental-setup,generated responses,for,evaluation,generated responses for evaluation,0.6220561861991882
translation,243,138,experimental-setup,evaluation,to avoid,selection bias,evaluation to avoid selection bias,0.5538660287857056
translation,243,138,experimental-setup,experimental setup,randomly picked,one response,experimental setup randomly picked one response,0.7255756258964539
translation,243,138,experimental-setup,experimental setup,has,number of mechanisms,experimental setup has number of mechanisms,0.5140280723571777
translation,243,142,experimental-setup,vocabulary size,to,20,vocabulary size to 20,0.6413142085075378
translation,243,142,experimental-setup,vocabulary size,",",000,"vocabulary size , 000",0.6469714045524597
translation,243,142,experimental-setup,20,",",000,"20 , 000",0.6560043096542358
translation,243,142,experimental-setup,dimension,of,word vectors,dimension of word vectors,0.6073312163352966
translation,243,142,experimental-setup,word vectors,as,100,word vectors as 100,0.6047105193138123
translation,243,143,experimental-setup,word vectors,pretrained with,around 9 million post-response pairs,word vectors pretrained with around 9 million post-response pairs,0.7134320735931396
translation,243,143,experimental-setup,around 9 million post-response pairs,from,weibo,around 9 million post-response pairs from weibo,0.5946764945983887
translation,243,143,experimental-setup,updated,during,training,updated during training,0.7725804448127747
translation,243,143,experimental-setup,training,of,decoders,training of decoders,0.6270387172698975
translation,243,143,experimental-setup,experimental setup,has,word vectors,experimental setup has word vectors,0.5082172751426697
translation,243,144,experimental-setup,hidden states,have,512 dimensions,hidden states have 512 dimensions,0.5667721033096313
translation,243,144,experimental-setup,4 - layer gru units,has,hidden states,4 - layer gru units has hidden states,0.5323576927185059
translation,243,144,experimental-setup,experimental setup,applied,4 - layer gru units,experimental setup applied 4 - layer gru units,0.6589826345443726
translation,243,8,model,two typed decoders,in which,type distribution,two typed decoders in which type distribution,0.5683237314224243
translation,243,8,model,two typed decoders,used to modulate,final generation distribution,two typed decoders used to modulate final generation distribution,0.6891269087791443
translation,243,8,model,type distribution,over,three types,type distribution over three types,0.7063266634941101
translation,243,8,model,model,devise,two typed decoders,model devise two typed decoders,0.6953856348991394
translation,243,32,model,two decoders,for,question generation,two decoders for question generation,0.5960017442703247
translation,243,32,model,two decoders,deals with,word types,two decoders deals with word types,0.7133904099464417
translation,243,32,model,hard typed decoder ( htd ),deals with,word types,hard typed decoder ( htd ) deals with word types,0.6690434813499451
translation,243,32,model,question generation,in,conversational systems,question generation in conversational systems,0.4829399883747101
translation,243,32,model,std,deals with,word types,std deals with word types,0.7191507816314697
translation,243,32,model,model,devise,two decoders,model devise two decoders,0.742875874042511
translation,243,33,model,each decoding position,firstly estimate,type distribution,each decoding position firstly estimate type distribution,0.7510553598403931
translation,243,33,model,type distribution,over,word types,type distribution over word types,0.6138340830802917
translation,243,33,model,model,At,each decoding position,model At each decoding position,0.5476386547088623
translation,243,60,model,encoder-decoder framework,propose,two decoders,encoder-decoder framework propose two decoders,0.6769711375236511
translation,243,60,model,two decoders,to effectively use,word types,two decoders to effectively use word types,0.6990929841995239
translation,243,60,model,word types,in,question generation,word types in question generation,0.5047557353973389
translation,243,60,model,model,On top of,encoder-decoder framework,model On top of encoder-decoder framework,0.7045645117759705
translation,243,62,model,type distribution,over,word types,type distribution over word types,0.6138340830802917
translation,243,62,model,type distribution,obtains,mixture of type-specific distributions,type distribution obtains mixture of type-specific distributions,0.598688542842865
translation,243,62,model,three type-specific generation distributions,over,vocabulary,three type-specific generation distributions over vocabulary,0.687351644039154
translation,243,62,model,mixture of type-specific distributions,for,word generation,mixture of type-specific distributions for word generation,0.5722182393074036
translation,243,62,model,model,estimates,type distribution,model estimates type distribution,0.7490265369415283
translation,243,62,model,model,estimates,three type-specific generation distributions,model estimates three type-specific generation distributions,0.6366778612136841
translation,243,64,model,final generation probability,of,word,final generation probability of word,0.6130114197731018
translation,243,64,model,word,modulated by,word type,word modulated by word type,0.7165912985801697
translation,243,72,model,more meaningful questions,propose,soft typed decoder,more meaningful questions propose soft typed decoder,0.6528440713882446
translation,243,72,model,model,to generate,more meaningful questions,model to generate more meaningful questions,0.7135140895843506
translation,243,73,model,latent type,among,set,latent type among set,0.6206572651863098
translation,243,73,model,each word,has,latent type,each word has latent type,0.5771781206130981
translation,243,73,model,set,has,"interrogative , topic word , ordinary word}","set has interrogative , topic word , ordinary word}",0.5986425876617432
translation,243,73,model,model,assumes,each word,model assumes each word,0.7199499011039734
translation,243,237,model,two typed decoders,to generate,questions,two typed decoders to generate questions,0.6759099960327148
translation,243,237,model,questions,in,open-domain conversational systems,questions in open-domain conversational systems,0.5044004321098328
translation,243,237,model,model,present,two typed decoders,model present two typed decoders,0.6257432103157043
translation,243,238,model,decoders,firstly estimate,type distribution,decoders firstly estimate type distribution,0.7326980233192444
translation,243,238,model,decoders,use,type distribution,decoders use type distribution,0.6794055700302124
translation,243,238,model,type distribution,over,word types,type distribution over word types,0.6138340830802917
translation,243,238,model,type distribution,to modulate,final word generation distribution,type distribution to modulate final word generation distribution,0.668640673160553
translation,243,238,model,type distribution,to modulate,final word generation distribution,type distribution to modulate final word generation distribution,0.668640673160553
translation,243,238,model,model,has,decoders,model has decoders,0.6080074906349182
translation,243,164,results,std and htd,perform,fairly well,std and htd perform fairly well,0.605510413646698
translation,243,164,results,fairly well,with,lower perplexities,fairly well with lower perplexities,0.6186127066612244
translation,243,164,results,fairly well,with,higher,fairly well with higher,0.6947625279426575
translation,243,164,results,fairly well,with,remarkably better topical response ratio ( trr ),fairly well with remarkably better topical response ratio ( trr ),0.61148601770401
translation,243,164,results,higher,has,distinct - 1 and distinct - 2 scores,higher has distinct - 1 and distinct - 2 scores,0.5404848456382751
translation,243,166,results,our decoders,have,better distinct - 1 and distinct - 2 scores,our decoders have better distinct - 1 and distinct - 2 scores,0.5557606816291809
translation,243,166,results,better distinct - 1 and distinct - 2 scores,than,baselines do,better distinct - 1 and distinct - 2 scores than baselines do,0.6170387864112854
translation,243,166,results,htd,performs,much better,htd performs much better,0.6417660713195801
translation,243,166,results,much better,than,strongest baseline ta,much better than strongest baseline ta,0.5787606835365295
translation,243,166,results,results,has,our decoders,results has our decoders,0.5671433210372925
translation,243,186,results,outperform,in terms of,all the metrics,outperform in terms of all the metrics,0.6853294968605042
translation,243,186,results,all the baselines,in terms of,all the metrics,all the baselines in terms of all the metrics,0.621505618095398
translation,243,186,results,std and htd,has,outperform,std and htd has outperform,0.6486071348190308
translation,243,186,results,outperform,has,all the baselines,outperform has all the baselines,0.6026061177253723
translation,243,186,results,results,indicate,std and htd,results indicate std and htd,0.5652080178260803
translation,243,188,results,results,has,our decoders,results has our decoders,0.5671433210372925
translation,243,189,results,htd,has,outperforms,htd has outperforms,0.6481802463531494
translation,243,189,results,outperforms,has,std,outperforms has std,0.626272976398468
translation,243,189,results,std,has,significantly,std has significantly,0.6640932559967041
translation,243,189,results,results,has,htd,results has htd,0.5847278237342834
translation,243,190,results,std,has,outperforms,std has outperforms,0.6407841444015503
translation,243,190,results,outperforms,has,seq2seq and ta,outperforms has seq2seq and ta,0.6031057238578796
translation,243,190,results,results,observed,std,results observed std,0.6224825382232666
translation,243,199,results,pattern distribution,by,our model,pattern distribution by our model,0.5934819579124451
translation,243,199,results,our model,closer to,humanwritten responses,our model closer to humanwritten responses,0.6913951635360718
translation,243,199,results,results,show that,pattern distribution,results show that pattern distribution,0.4865296483039856
translation,244,161,ablation-analysis,model performance,suffers,great decrease,model performance suffers great decrease,0.7044011354446411
translation,244,161,ablation-analysis,great decrease,from discarding,multi-hop inference module,great decrease from discarding multi-hop inference module,0.7652654051780701
translation,244,161,ablation-analysis,multi-hop inference module,on,two datasets,multi-hop inference module on two datasets,0.5257222056388855
translation,244,161,ablation-analysis,ablation analysis,has,model performance,ablation analysis has model performance,0.5138536095619202
translation,244,165,ablation-analysis,all the components,contribute to,final performance,all the components contribute to final performance,0.6760620474815369
translation,244,165,ablation-analysis,final performance,to,certain extent,final performance to certain extent,0.5697872042655945
translation,244,165,ablation-analysis,ablation analysis,has,all the components,ablation analysis has all the components,0.5586082935333252
translation,244,168,ablation-analysis,question pointer,casts,noticeably greater decrease,question pointer casts noticeably greater decrease,0.6510922908782959
translation,244,168,ablation-analysis,noticeably greater decrease,on,pubmedqa,noticeably greater decrease on pubmedqa,0.5959616899490356
translation,244,168,ablation-analysis,pubmedqa,than,wikihow,pubmedqa than wikihow,0.5832544565200806
translation,244,168,ablation-analysis,ablation analysis,Discarding,question pointer,ablation analysis Discarding question pointer,0.764037013053894
translation,244,171,ablation-analysis,multi-view coverage ( mvc ) loss,makes,great contribution,multi-view coverage ( mvc ) loss makes great contribution,0.6506497263908386
translation,244,171,ablation-analysis,great contribution,to,performance,great contribution to performance,0.5454780459403992
translation,244,171,ablation-analysis,great contribution,by alleviating,severe repetition problem,great contribution by alleviating severe repetition problem,0.6959770321846008
translation,244,171,ablation-analysis,severe repetition problem,along with,multi-view pgn,severe repetition problem along with multi-view pgn,0.6002918481826782
translation,244,171,ablation-analysis,ablation analysis,has,multi-view coverage ( mvc ) loss,ablation analysis has multi-view coverage ( mvc ) loss,0.5381014943122864
translation,244,193,ablation-analysis,qs and sd 2,fail to capture,key information,qs and sd 2 fail to capture key information,0.6912826895713806
translation,244,193,ablation-analysis,qs and sd 2,generating,irrelevant summaries,qs and sd 2 generating irrelevant summaries,0.7465242147445679
translation,244,193,ablation-analysis,irrelevant summaries,to,given question,irrelevant summaries to given question,0.5613706707954407
translation,244,193,ablation-analysis,irrelevant summaries,producing,some general sentences,irrelevant summaries producing some general sentences,0.7234277129173279
translation,244,193,ablation-analysis,some general sentences,due to,data-driven learning,some general sentences due to data-driven learning,0.6470521688461304
translation,244,193,ablation-analysis,ablation analysis,has,qs and sd 2,ablation analysis has qs and sd 2,0.5237666368484497
translation,244,134,baselines,four widely - adopted summarization baseline methods,including,two unsupervised extractive methods,four widely - adopted summarization baseline methods including two unsupervised extractive methods,0.6124863028526306
translation,244,134,baselines,four widely - adopted summarization baseline methods,including,lead3 and mmr,four widely - adopted summarization baseline methods including lead3 and mmr,0.6199963688850403
translation,244,134,baselines,four widely - adopted summarization baseline methods,including,two abstractive methods,four widely - adopted summarization baseline methods including two abstractive methods,0.6116843819618225
translation,244,134,baselines,four widely - adopted summarization baseline methods,including,"s2sa ( bahdanau et al. , 2015 )","four widely - adopted summarization baseline methods including s2sa ( bahdanau et al. , 2015 )",0.574499249458313
translation,244,134,baselines,four widely - adopted summarization baseline methods,including,pgn,four widely - adopted summarization baseline methods including pgn,0.6337733268737793
translation,244,135,baselines,"sd 2 ( nema et al. , 2017 )",is,sequence - tosequence model,"sd 2 ( nema et al. , 2017 ) is sequence - tosequence model",0.5820274949073792
translation,244,135,baselines,sequence - tosequence model,with,query attention,sequence - tosequence model with query attention,0.6217085719108582
translation,244,135,baselines,"qs ( hasselqvist et al. , 2017 )",incorporates,question information,"qs ( hasselqvist et al. , 2017 ) incorporates question information",0.691904604434967
translation,244,135,baselines,question information,into,pointer - generator network,question information into pointer - generator network,0.5684762597084045
translation,244,135,baselines,pointer - generator network,with,vanilla attention mechanism,pointer - generator network with vanilla attention mechanism,0.6279462575912476
translation,244,136,baselines,"s2s -mt ( fan et al. , 2019 )",uses,multi-task seq2seq model,"s2s -mt ( fan et al. , 2019 ) uses multi-task seq2seq model",0.5441955924034119
translation,244,136,baselines,multi-task seq2seq model,with,concatenation,multi-task seq2seq model with concatenation,0.5951271653175354
translation,244,136,baselines,concatenation,of,question and support document,concatenation of question and support document,0.6170458793640137
translation,244,136,baselines,concatenation,of,question and document,concatenation of question and document,0.6467686295509338
translation,244,136,baselines,"qpgn ( deng et al. , 2020a )",is,question - driven pointer - generator network,"qpgn ( deng et al. , 2020a ) is question - driven pointer - generator network",0.5555103421211243
translation,244,136,baselines,question - driven pointer - generator network,with,co-attention,question - driven pointer - generator network with co-attention,0.6210458278656006
translation,244,136,baselines,co-attention,between,question and document,co-attention between question and document,0.6422607898712158
translation,244,192,baselines,qpgn,considers,semantic relevance,qpgn considers semantic relevance,0.6536698341369629
translation,244,192,baselines,qpgn,leading to,incomplete summary,qpgn leading to incomplete summary,0.7192206978797913
translation,244,192,baselines,semantic relevance,to,given question,semantic relevance to given question,0.5312739610671997
translation,244,192,baselines,semantic relevance,leading to,incomplete summary,semantic relevance leading to incomplete summary,0.6765580773353577
translation,244,192,baselines,incomplete summary,lack of,necessary content,incomplete summary lack of necessary content,0.6527226567268372
translation,244,192,baselines,baselines,has,qpgn,baselines has qpgn,0.5967488288879395
translation,244,137,hyperparameters,all the models,with,pre-trained glove embeddings,all the models with pre-trained glove embeddings,0.5900019407272339
translation,244,137,hyperparameters,all the models,set,vocabulary size,all the models set vocabulary size,0.6143758296966553
translation,244,137,hyperparameters,vocabulary size,to,50k,vocabulary size to 50k,0.5763840675354004
translation,244,137,hyperparameters,hyperparameters,train,all the models,hyperparameters train all the models,0.6497803926467896
translation,244,138,hyperparameters,training and testing procedure,restrict,length,training and testing procedure restrict length,0.6623615622520447
translation,244,138,hyperparameters,length,of,generated summaries,length of generated summaries,0.5995470285415649
translation,244,138,hyperparameters,generated summaries,within,50 words,generated summaries within 50 words,0.6372316479682922
translation,244,138,hyperparameters,hyperparameters,During,training and testing procedure,hyperparameters During training and testing procedure,0.6287717819213867
translation,244,139,hyperparameters,learning rate,of,0.15,learning rate of 0.15,0.6145498156547546
translation,244,139,hyperparameters,initial accumulator value,of,0.1,initial accumulator value of 0.1,0.6011174917221069
translation,244,139,hyperparameters,hyperparameters,train with,learning rate,hyperparameters train with learning rate,0.6631114482879639
translation,244,139,hyperparameters,hyperparameters,train with,initial accumulator value,hyperparameters train with initial accumulator value,0.6653825640678406
translation,244,140,hyperparameters,dropout rate,set to,0.5,dropout rate set to 0.5,0.6910414695739746
translation,244,140,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,244,141,hyperparameters,hidden unit sizes,of,bilstm encoder and the lstm decoder,hidden unit sizes of bilstm encoder and the lstm decoder,0.5441784262657166
translation,244,141,hyperparameters,hidden unit sizes,set to,256,hidden unit sizes set to 256,0.6892455220222473
translation,244,141,hyperparameters,bilstm encoder and the lstm decoder,set to,256,bilstm encoder and the lstm decoder set to 256,0.6558308601379395
translation,244,141,hyperparameters,hyperparameters,has,hidden unit sizes,hyperparameters has hidden unit sizes,0.5064014792442322
translation,244,142,hyperparameters,models,with,batch size,models with batch size,0.6254024505615234
translation,244,142,hyperparameters,batch size,of,32,batch size of 32,0.6741614937782288
translation,244,142,hyperparameters,hyperparameters,train,models,hyperparameters train models,0.6666569709777832
translation,244,143,hyperparameters,other parameters,randomly initialized from,"[ - 0.05 , 0.05","other parameters randomly initialized from [ - 0.05 , 0.05",0.5873122811317444
translation,244,143,hyperparameters,hyperparameters,has,other parameters,hyperparameters has other parameters,0.4744566082954407
translation,244,5,model,question - driven abstractive summarization method,incorporate,multi-hop reasoning,question - driven abstractive summarization method incorporate multi-hop reasoning,0.6133286356925964
translation,244,5,model,multi-hop reasoning,into,question - driven summarization,multi-hop reasoning into question - driven summarization,0.5496866703033447
translation,244,5,model,model,propose,question - driven abstractive summarization method,model propose question - driven abstractive summarization method,0.6741693019866943
translation,244,6,model,relevance,to,question,relevance to question,0.5386306643486023
translation,244,6,model,interrelation,among,different sentences,interrelation among different sentences,0.588222324848175
translation,244,6,model,different sentences,via,human-like multi-hop inference module,different sentences via human-like multi-hop inference module,0.6551358103752136
translation,244,6,model,human-like multi-hop inference module,captures,important sentences,human-like multi-hop inference module captures important sentences,0.6958609223365784
translation,244,6,model,important sentences,for justifying,summarized answer,important sentences for justifying summarized answer,0.7143493294715881
translation,244,6,model,model,jointly model,relevance,model jointly model relevance,0.7753543257713318
translation,244,7,model,gated selective pointer generator network,with,multi-view coverage mechanism,gated selective pointer generator network with multi-view coverage mechanism,0.5949917435646057
translation,244,7,model,diverse information,from,different perspectives,diverse information from different perspectives,0.5299769043922424
translation,244,7,model,model,has,gated selective pointer generator network,model has gated selective pointer generator network,0.5660182237625122
translation,244,42,model,document,regarded as,hierarchical text structure,document regarded as hierarchical text structure,0.5805482268333435
translation,244,42,model,hierarchical text structure,to be assessed with,importance degree,hierarchical text structure to be assessed with importance degree,0.6359115242958069
translation,244,42,model,importance degree,in both,word - and sentencelevel,importance degree in both word - and sentencelevel,0.599412739276886
translation,244,42,model,word - and sentencelevel,for,content selection,word - and sentencelevel for content selection,0.5753504037857056
translation,244,42,model,model,has,document,model has document,0.6103928089141846
translation,244,43,model,multi-hop inference module,to enable,human-like multi-hop reasoning,multi-hop inference module to enable human-like multi-hop reasoning,0.7029687166213989
translation,244,43,model,human-like multi-hop reasoning,in,question - driven summarization,human-like multi-hop reasoning in question - driven summarization,0.5026982426643372
translation,244,43,model,human-like multi-hop reasoning,considers,semantic relevance,human-like multi-hop reasoning considers semantic relevance,0.5988897085189819
translation,244,43,model,semantic relevance,to,question,semantic relevance to question,0.5405464172363281
translation,244,43,model,information consistency,among,different sentences,information consistency among different sentences,0.5634624361991882
translation,244,43,model,model,develop,multi-hop inference module,model develop multi-hop inference module,0.637420654296875
translation,244,189,model,msg ( 2- hop ),effectively summarizes,1sthop and 2nd - hop inference content,msg ( 2- hop ) effectively summarizes 1sthop and 2nd - hop inference content,0.7335329055786133
translation,244,189,model,1sthop and 2nd - hop inference content,in,document,1sthop and 2nd - hop inference content in document,0.5316683650016785
translation,244,189,model,model,has,msg ( 2- hop ),model has msg ( 2- hop ),0.5817366242408752
translation,244,147,results,question information,not fully exploited for,summarization,question information not fully exploited for summarization,0.68177729845047
translation,244,147,results,outperforms,with,noticeable margin,outperforms with noticeable margin,0.7069945335388184
translation,244,147,results,all these methods,with,noticeable margin,all these methods with noticeable margin,0.6863080263137817
translation,244,147,results,msg,has,outperforms,msg has outperforms,0.6595916152000427
translation,244,147,results,outperforms,has,all these methods,outperforms has all these methods,0.5673355460166931
translation,244,148,results,pubmedqa,observe,qpgn,pubmedqa observe qpgn,0.6806204915046692
translation,244,148,results,qpgn,employs,special design,qpgn employs special design,0.6069476008415222
translation,244,148,results,qpgn,achieves,relatively better performance,qpgn achieves relatively better performance,0.7160108089447021
translation,244,148,results,special design,for modeling,interaction,special design for modeling interaction,0.7746697068214417
translation,244,148,results,interaction,between,question and document,interaction between question and document,0.6602423191070557
translation,244,148,results,relatively better performance,than,other summarization methods,relatively better performance than other summarization methods,0.5338301062583923
translation,244,149,results,msg,raises,state- of- the - art result,msg raises state- of- the - art result,0.6492778658866882
translation,244,149,results,state- of- the - art result,has,by about 3 %,state- of- the - art result has by about 3 %,0.5208531022071838
translation,244,149,results,results,Favorably,msg,results Favorably msg,0.6293559670448303
translation,244,150,results,msg,achieves,promising improvements,msg achieves promising improvements,0.6864316463470459
translation,244,150,results,promising improvements,via,multi-hop inference,promising improvements via multi-hop inference,0.682173490524292
translation,244,150,results,results,has,msg,results has msg,0.5388910174369812
translation,244,154,results,existing querybased summarization methods,in,all aspects,existing querybased summarization methods in all aspects,0.48143821954727173
translation,244,154,results,msg,has,consistently and substantially outperforms,msg has consistently and substantially outperforms,0.6062892079353333
translation,244,154,results,consistently and substantially outperforms,has,existing querybased summarization methods,consistently and substantially outperforms has existing querybased summarization methods,0.5441696047782898
translation,244,154,results,results,observe,msg,results observe msg,0.5840908885002136
translation,244,155,results,msg,effectively generates,concise but also informative answers,msg effectively generates concise but also informative answers,0.699699342250824
translation,244,155,results,results,show,msg,results show msg,0.6198960542678833
translation,244,162,results,fusion,of,selective sentence representations,fusion of selective sentence representations,0.5924887657165527
translation,244,162,results,selective sentence representations,from,all hops,selective sentence representations from all hops,0.5866629481315613
translation,244,162,results,selective sentence representations,brings,performance improvement,selective sentence representations brings performance improvement,0.5777061581611633
translation,244,162,results,performance improvement,including,aggre,performance improvement including aggre,0.6462284922599792
translation,244,162,results,performance improvement,applying,attention,performance improvement applying attention,0.678824245929718
translation,244,162,results,aggre,- gating,all the hops,aggre - gating all the hops,0.75328129529953
translation,244,162,results,attention,to weight,importance,attention to weight importance,0.7664709687232971
translation,244,162,results,importance,of,each hop,importance of each hop,0.6122172474861145
translation,244,162,results,results,has,fusion,results has fusion,0.5455302596092224
translation,244,163,results,better performance,to apply,proposed mar unit,better performance to apply proposed mar unit,0.6994568109512329
translation,244,163,results,proposed mar unit,as,multi-hop unit,proposed mar unit as multi-hop unit,0.5822205543518066
translation,244,167,results,msg,achieves,better performance,msg achieves better performance,0.6872944831848145
translation,244,167,results,better performance,by employing,gated attention,better performance by employing gated attention,0.6144141554832458
translation,244,167,results,gated attention,to distinguish,salient justification sentences,gated attention to distinguish salient justification sentences,0.6573970317840576
translation,244,167,results,salient justification sentences,for generating,summaries,salient justification sentences for generating summaries,0.7175309658050537
translation,244,167,results,results,shows,msg,results shows msg,0.6590100526809692
translation,244,173,results,msg ( 3 - hop ),outperforms,msg ( 1- hop ),msg ( 3 - hop ) outperforms msg ( 1- hop ),0.765613853931427
translation,244,173,results,msg ( 1- hop ),by,0.5 % and 0.7 %,msg ( 1- hop ) by 0.5 % and 0.7 %,0.590171217918396
translation,244,173,results,0.5 % and 0.7 %,on,wikihow,0.5 % and 0.7 % on wikihow,0.5792632102966309
translation,244,188,results,msg ( 3hop ),successfully summarizes,source document,msg ( 3hop ) successfully summarizes source document,0.7178409695625305
translation,244,188,results,source document,with,all the necessary and correct information,source document with all the necessary and correct information,0.5728747248649597
translation,244,188,results,generated summaries,has,msg ( 3hop ),generated summaries has msg ( 3hop ),0.6250357627868652
translation,245,9,results,scoring function,to capture,answerability,scoring function to capture answerability,0.6806824207305908
translation,245,9,results,scoring function,when,scoring function,scoring function when scoring function,0.6542936563491821
translation,245,9,results,scoring function,integrated with,existing metrics,scoring function integrated with existing metrics,0.6752153038978577
translation,245,9,results,scoring function,correlate,significantly better,scoring function correlate significantly better,0.668248176574707
translation,245,9,results,scoring function,integrated with,existing metrics,scoring function integrated with existing metrics,0.6752153038978577
translation,245,9,results,scoring function,correlate,significantly better,scoring function correlate significantly better,0.668248176574707
translation,245,9,results,significantly better,with,human judgments,significantly better with human judgments,0.649761438369751
translation,245,9,results,results,introduce,scoring function,results introduce scoring function,0.5684599280357361
translation,246,18,experiments,geosqa - an sqa dataset,in,geography domain,geosqa - an sqa dataset in geography domain,0.512981653213501
translation,246,18,experiments,geography domain,consisting of,"1,981 scenarios","geography domain consisting of 1,981 scenarios",0.7068235278129578
translation,246,18,experiments,geography domain,consisting of,"4,110 multiple -choice questions","geography domain consisting of 4,110 multiple -choice questions",0.6370997428894043
translation,246,18,experiments,"4,110 multiple -choice questions",at,high school level,"4,110 multiple -choice questions at high school level",0.4827452003955841
translation,246,19,experiments,highquality natural language description,of,content,highquality natural language description of content,0.5209176540374756
translation,246,113,experiments,bimpm,has,"wang et al. , 2017 )","bimpm has wang et al. , 2017 )",0.5867927670478821
translation,246,65,model,our method,to,entire dataset,our method to entire dataset,0.5183202624320984
translation,246,65,model,model,apply,our method,model apply our method,0.6677295565605164
translation,246,64,results,our method,achieves,accuracy,our method achieves accuracy,0.6433743238449097
translation,246,64,results,accuracy,of,95.3 %,accuracy of 95.3 %,0.5538517832756042
translation,246,64,results,accuracy,showing,satisfying performance,accuracy showing satisfying performance,0.7339761853218079
translation,246,64,results,95.3 %,on,test set,95.3 % on test set,0.5364962816238403
translation,246,64,results,results,has,our method,results has our method,0.5589964985847473
translation,247,118,ablation-analysis,and query,using OR only in case of,failure,and query using OR only in case of failure,0.7270560264587402
translation,247,118,ablation-analysis,failure,leads to,improvement,failure leads to improvement,0.6884558796882629
translation,247,118,ablation-analysis,improvement,of,expected article ranking position,improvement of expected article ranking position,0.5493829250335693
translation,247,118,ablation-analysis,recall ratio,has,drops significantly,recall ratio has drops significantly,0.6422143578529358
translation,247,118,ablation-analysis,ablation analysis,Starting with,and query,ablation analysis Starting with and query,0.6740965843200684
translation,247,119,ablation-analysis,removal,of,question focus,removal of question focus,0.6141223907470703
translation,247,119,ablation-analysis,question focus,from,list of keywords,question focus from list of keywords,0.547956645488739
translation,247,119,ablation-analysis,negative impact,on,performance,negative impact on performance,0.5604664087295532
translation,247,119,ablation-analysis,ablation analysis,has,removal,ablation analysis has removal,0.5308048129081726
translation,247,112,results,pattern matching stage,accepts,small part of questions,pattern matching stage accepts small part of questions,0.6731632947921753
translation,247,112,results,pattern matching stage,yields,high precision,pattern matching stage yields high precision,0.7037686109542847
translation,247,112,results,results,has,pattern matching stage,results has pattern matching stage,0.5736916661262512
translation,247,117,results,basic technique,yields,best result,basic technique yields best result,0.6779452562332153
translation,247,121,results,inflection,is,necessary,inflection is necessary,0.6499611735343933
translation,247,121,results,fuzzy queries,provide,more accurate re-sults,fuzzy queries provide more accurate re-sults,0.586796760559082
translation,247,121,results,results,taking into account,inflection,results taking into account inflection,0.7201005816459656
translation,248,199,ablation-analysis,removal,of,kb structure information,removal of kb structure information,0.6029174327850342
translation,248,199,ablation-analysis,removal,results in,biggest performance drop,removal results in biggest performance drop,0.634995698928833
translation,248,199,ablation-analysis,kb structure information,encoded in,entity and relation embeddings,kb structure information encoded in entity and relation embeddings,0.6773747801780701
translation,248,199,ablation-analysis,kb structure information,results in,biggest performance drop,kb structure information results in biggest performance drop,0.5695209503173828
translation,248,199,ablation-analysis,biggest performance drop,of,almost 10 percentage points,biggest performance drop of almost 10 percentage points,0.5426204204559326
translation,248,199,ablation-analysis,ablation analysis,see that,removal,ablation analysis see that removal,0.6496924757957458
translation,248,200,ablation-analysis,character - level information,proves to be,highly important,character - level information proves to be highly important,0.6791738271713257
translation,248,200,ablation-analysis,highly important,for,final state - of - the - art performance,highly important for final state - of - the - art performance,0.5792825818061829
translation,248,200,ablation-analysis,ablation analysis,has,character - level information,ablation analysis has character - level information,0.5076029896736145
translation,248,202,ablation-analysis,lexical information,about,related kb relations,lexical information about related kb relations,0.6251018047332764
translation,248,202,ablation-analysis,decrease,has,results,decrease has results,0.6160310506820679
translation,248,202,ablation-analysis,ablation analysis,excluding,token - level input,ablation analysis excluding token - level input,0.6965245008468628
translation,248,108,experimental-setup,50 - dimensional glove embeddings,pre-trained on,6 billion tokens corpus,50 - dimensional glove embeddings pre-trained on 6 billion tokens corpus,0.7336342334747314
translation,248,108,experimental-setup,experimental setup,use,50 - dimensional glove embeddings,experimental setup use 50 - dimensional glove embeddings,0.5974327921867371
translation,248,144,experiments,two new datasets,for,entity linking,two new datasets for entity linking,0.5648412704467773
translation,248,144,experiments,entity linking,on,questions,entity linking on questions,0.536190390586853
translation,248,144,experiments,graphquestions,has,"su et al. , 2016 )","graphquestions has su et al. , 2016 )",0.5608298778533936
translation,248,191,experiments,graphquestions,provides,much more difficult benchmark,graphquestions provides much more difficult benchmark,0.6012630462646484
translation,248,191,experiments,much more difficult benchmark,for,el,much more difficult benchmark for el,0.6656472086906433
translation,248,22,model,entity mention detection and entity disambiguation,jointly in,single neural model,entity mention detection and entity disambiguation jointly in single neural model,0.6334246397018433
translation,248,22,model,single neural model,makes,whole process,single neural model makes whole process,0.6823881268501282
translation,248,22,model,whole process,has,end-to - end differentiable,whole process has end-to - end differentiable,0.5617998838424683
translation,248,22,model,model,perform,entity mention detection and entity disambiguation,model perform entity mention detection and entity disambiguation,0.5817050933837891
translation,248,24,model,noise,automatically learn,features,noise automatically learn features,0.6903682351112366
translation,248,24,model,features,over,set of contexts,features over set of contexts,0.6544437408447266
translation,248,24,model,set of contexts,of,different granularity levels,set of contexts of different granularity levels,0.5692338943481445
translation,248,24,model,model,To overcome,noise,model To overcome noise,0.7213871479034424
translation,248,25,model,each level of granularity,handled by,separate component of the model,each level of granularity handled by separate component of the model,0.6978459358215332
translation,248,25,model,model,has,each level of granularity,model has each level of granularity,0.5954501032829285
translation,248,192,results,vcg model,shows,overall f-score result,vcg model shows overall f-score result,0.6604268550872803
translation,248,192,results,overall f-score result,better than,dbpedia spotlight baseline,overall f-score result better than dbpedia spotlight baseline,0.7027934193611145
translation,248,192,results,dbpedia spotlight baseline,by,wide margin,dbpedia spotlight baseline by wide margin,0.5572130680084229
translation,248,192,results,results,has,vcg model,results has vcg model,0.5408251285552979
translation,248,193,results,our model,achieves,higher precision values,our model achieves higher precision values,0.6366658806800842
translation,248,193,results,our model,manages to keep,satisfactory level of recall,our model manages to keep satisfactory level of recall,0.6477380394935608
translation,248,193,results,higher precision values,compared to,other approaches,higher precision values compared to other approaches,0.6357280015945435
translation,248,207,results,different types of context,helps to achieve,better performance,different types of context helps to achieve better performance,0.7424905896186829
translation,248,207,results,better performance,across,various entity classes,better performance across various entity classes,0.6470772624015808
translation,248,207,results,results,modeling,different types of context,results modeling different types of context,0.7284992933273315
translation,248,227,results,simplified vcg model,produces,results,simplified vcg model produces results,0.584360659122467
translation,248,227,results,better,than,dbpedia spot- light,better than dbpedia spot- light,0.6175963878631592
translation,248,227,results,better,than,s-mart,better than s-mart,0.6405162215232849
translation,248,227,results,results,has,better,results has better,0.6256940364837646
translation,248,228,results,vcg model,delivers,best f-score,vcg model delivers best f-score,0.5935755372047424
translation,248,228,results,best f-score,across,all setups,best f-score across all setups,0.6358065009117126
translation,248,228,results,results,has,vcg model,results has vcg model,0.5408251285552979
translation,248,229,results,our model,achieves,most gains,our model achieves most gains,0.6771031618118286
translation,248,229,results,most gains,in,precision,most gains in precision,0.5786152482032776
translation,248,229,results,precision,compared to,baselines and the previous state - of - the - art,precision compared to baselines and the previous state - of - the - art,0.5906233787536621
translation,248,229,results,baselines and the previous state - of - the - art,for,qa data,baselines and the previous state - of - the - art for qa data,0.5889362096786499
translation,248,229,results,results,observe,our model,results observe our model,0.6353915333747864
translation,248,230,results,simplified vcg baseline,uses,manually defined fea- tures,simplified vcg baseline uses manually defined fea- tures,0.6194975972175598
translation,248,230,results,vcg,has,constantly outperforms,vcg has constantly outperforms,0.6389341950416565
translation,248,230,results,constantly outperforms,has,simplified vcg baseline,constantly outperforms has simplified vcg baseline,0.5730495452880859
translation,248,230,results,results,has,vcg,results has vcg,0.5697683095932007
translation,248,232,results,vcg model,achieves,best macro result,vcg model achieves best macro result,0.6898607015609741
translation,248,232,results,results,has,vcg model,results has vcg model,0.5408251285552979
translation,249,145,baselines,baselines,reimplemented,two state - of - the - art question generation models,baselines reimplemented two state - of - the - art question generation models,0.7186691761016846
translation,249,6,model,holistic and novel generator -evaluator framework,directly optimizes,objectives,holistic and novel generator -evaluator framework directly optimizes objectives,0.6928631067276001
translation,249,6,model,objectives,reward,semantics and structure,objectives reward semantics and structure,0.7257170677185059
translation,249,7,model,generator,is,sequence - to-sequence model,generator is sequence - to-sequence model,0.5968993902206421
translation,249,7,model,sequence - to-sequence model,incorporates,structure and semantics,sequence - to-sequence model incorporates structure and semantics,0.6947774887084961
translation,249,7,model,structure and semantics,of,question being generated,structure and semantics of question being generated,0.6069045066833496
translation,249,7,model,model,has,generator,model has generator,0.5837957262992859
translation,249,10,model,evaluator model,assigns,reward,evaluator model assigns reward,0.6401707530021667
translation,249,10,model,reward,to,each predicted question,reward to each predicted question,0.6000797748565674
translation,249,10,model,each predicted question,based on,conformity,each predicted question based on conformity,0.7124608159065247
translation,249,10,model,conformity,to,structure,conformity to structure,0.6148514747619629
translation,249,10,model,structure,of,ground -truth questions,structure of ground -truth questions,0.5659105777740479
translation,249,10,model,model,has,evaluator model,model has evaluator model,0.5426477789878845
translation,249,11,model,two novel qg - specific reward functions,for,text conformity,two novel qg - specific reward functions for text conformity,0.6023948192596436
translation,249,11,model,two novel qg - specific reward functions,for,answer conformity,two novel qg - specific reward functions for answer conformity,0.606801450252533
translation,249,11,model,answer conformity,of,generated question,answer conformity of generated question,0.5727909803390503
translation,249,11,model,model,propose,two novel qg - specific reward functions,model propose two novel qg - specific reward functions,0.6710071563720703
translation,249,12,model,structure -sensitive rewards,based on,evaluation measures,structure -sensitive rewards based on evaluation measures,0.6526358127593994
translation,249,12,model,evaluation measures,such as,bleu,evaluation measures such as bleu,0.5208759307861328
translation,249,12,model,evaluation measures,such as,gleu,evaluation measures such as gleu,0.670509934425354
translation,249,12,model,evaluation measures,such as,rouge -l,evaluation measures such as rouge -l,0.5882413387298584
translation,249,12,model,rouge -l,suitable for,qg,rouge -l suitable for qg,0.7615442872047424
translation,249,24,model,framework,in which,generator mechanism ( the horse ),framework in which generator mechanism ( the horse ),0.6388860940933228
translation,249,24,model,generator mechanism ( the horse ),employed for,generating,generator mechanism ( the horse ) employed for generating,0.7324572801589966
translation,249,24,model,question - answer pair,invokes or pulls,evaluator mechanism ( the cart ),question - answer pair invokes or pulls evaluator mechanism ( the cart ),0.7551600337028503
translation,249,24,model,evaluator mechanism ( the cart ),employed for,evaluating,evaluator mechanism ( the cart ) employed for evaluating,0.7485265731811523
translation,249,24,model,generating,has,question - answer pair,generating has question - answer pair,0.5903164148330688
translation,249,24,model,evaluating,has,generated pair,evaluating has generated pair,0.6201945543289185
translation,249,24,model,model,present,framework,model present framework,0.646798312664032
translation,249,27,model,candidate answer generation,using,pointer networks,candidate answer generation using pointer networks,0.6474610567092896
translation,249,27,model,candidate answer generation,alongside,qg,candidate answer generation alongside qg,0.6662834286689758
translation,249,27,model,model,incorporate,candidate answer generation,model incorporate candidate answer generation,0.6972894668579102
translation,249,165,results,drop,in,every evaluation measure,drop in every evaluation measure,0.5432987809181213
translation,249,165,results,drop,with,bleu - 4,drop with bleu - 4,0.7175320982933044
translation,249,165,results,bleu - 4,registering,largest drop,bleu - 4 registering largest drop,0.6003424525260925
translation,249,165,results,largest drop,of,13.8 %,largest drop of 13.8 %,0.5162478685379028
translation,249,165,results,13.8 %,against,"13.4 % , 6.9 % and 4.7 %","13.8 % against 13.4 % , 6.9 % and 4.7 %",0.619084358215332
translation,249,165,results,"13.4 % , 6.9 % and 4.7 %",in,"bleu -3 , bleu - 2 and bleu - 1","13.4 % , 6.9 % and 4.7 % in bleu -3 , bleu - 2 and bleu - 1",0.532450258731842
translation,249,165,results,results,Without,copy mechanism,results Without copy mechanism,0.7591173052787781
translation,249,166,results,coverage mechanism,see,consistent but sufficiently lower drop ( 1 - 2 % ),coverage mechanism see consistent but sufficiently lower drop ( 1 - 2 % ),0.6050974130630493
translation,249,166,results,consistent but sufficiently lower drop ( 1 - 2 % ),in,each evaluation measure,consistent but sufficiently lower drop ( 1 - 2 % ) in each evaluation measure,0.5266709327697754
translation,249,166,results,each evaluation measure,for,ge rouge,each evaluation measure for ge rouge,0.5933932662010193
translation,249,166,results,results,without,coverage mechanism,results without coverage mechanism,0.7336364388465881
translation,249,171,results,autoqg,on,all evaluation metrics,autoqg on all evaluation metrics,0.48877590894699097
translation,249,171,results,all our eight models,has,outperform,all our eight models has outperform,0.604468584060669
translation,249,171,results,outperform,has,l2a,outperform has l2a,0.6802484393119812
translation,249,171,results,outperform,has,autoqg,outperform has autoqg,0.6468297839164734
translation,249,171,results,results,has,all our eight models,results has all our eight models,0.5263175368309021
translation,249,172,results,two of our models,has,outperform,two of our models has outperform,0.591746985912323
translation,249,172,results,outperform,has,nqg lc,outperform has nqg lc,0.6917464733123779
translation,249,172,results,results,has,two of our models,results has two of our models,0.5183857679367065
translation,249,173,results,evaluation metrics,as,reward function,evaluation metrics as reward function,0.4910569190979004
translation,249,173,results,reward function,during,reinforcement based learning,reward function during reinforcement based learning,0.6547964811325073
translation,249,173,results,reinforcement based learning,improves,performance,reinforcement based learning improves performance,0.7223203182220459
translation,249,173,results,performance,for,all metrics,performance for all metrics,0.570069432258606
translation,249,173,results,results,using,evaluation metrics,results using evaluation metrics,0.5912701487541199
translation,249,174,results,reward function,in combination with,qg quality specific rewards,reward function in combination with qg quality specific rewards,0.5817638039588928
translation,249,174,results,reward function,is,best performing model,reward function is best performing model,0.5273412466049194
translation,249,174,results,qg quality specific rewards,has,),qg quality specific rewards has ),0.5860013365745544
translation,249,174,results,outperforming,has,existing baselines,outperforming has existing baselines,0.5976287722587585
translation,249,174,results,results,observe,ge rouge + qss + anss,results observe ge rouge + qss + anss,0.6006240248680115
translation,249,175,results,autoqg,on,bleu - 4,autoqg on bleu - 4,0.6077194809913635
translation,249,175,results,bleu - 4,by,29.98 %,bleu - 4 by 29.98 %,0.5729548335075378
translation,249,175,results,meteor,by,13.15 %,meteor by 13.15 %,0.6394360661506653
translation,249,175,results,rouge -l,by,8.67 %,rouge -l by 8.67 %,0.6004121899604797
translation,249,175,results,results,improves over,autoqg,results improves over autoqg,0.6872962713241577
translation,249,177,results,ge das + qss + anss,being,best model,ge das + qss + anss being best model,0.6195130348205566
translation,249,177,results,best model,on,syntactic correctness and semantic correctness quality metrics,best model on syntactic correctness and semantic correctness quality metrics,0.4722769558429718
translation,249,177,results,all the other models,by,large margin,all the other models by large margin,0.576673150062561
translation,249,177,results,seven of our eight models,has,outperform,seven of our eight models has outperform,0.5930420160293579
translation,249,177,results,outperform,has,two baselines,outperform has two baselines,0.6084616184234619
translation,249,177,results,outperforming,has,all the other models,outperforming has all the other models,0.5863047242164612
translation,249,178,results,model ge bleu + qss + anss,generates,highly relevant questions,model ge bleu + qss + anss generates highly relevant questions,0.6913594603538513
translation,249,178,results,model ge bleu + qss + anss,is,best model,model ge bleu + qss + anss is best model,0.576134979724884
translation,249,178,results,best model,on,relevance metrics,best model on relevance metrics,0.515839159488678
translation,249,178,results,results,has,model ge bleu + qss + anss,results has model ge bleu + qss + anss,0.595211386680603
translation,249,179,results,each of our models,adding,qg -specific rewards ( e.g. ge bleu + qss + anss ),each of our models adding qg -specific rewards ( e.g. ge bleu + qss + anss ),0.6541696786880493
translation,249,179,results,each of our models,has,significantly improves,each of our models has significantly improves,0.5887095928192139
translation,249,179,results,qg -specific rewards ( e.g. ge bleu + qss + anss ),has,significantly improves,qg -specific rewards ( e.g. ge bleu + qss + anss ) has significantly improves,0.5779333114624023
translation,249,179,results,significantly improves,has,question quality,significantly improves has question quality,0.5814634561538696
translation,249,179,results,results,for,each of our models,results for each of our models,0.5874907970428467
translation,249,188,results,rouge,in conjunction with,cross entropy loss,rouge in conjunction with cross entropy loss,0.5845171213150024
translation,249,188,results,cross entropy loss,improves on,recall,cross entropy loss improves on recall,0.6995822787284851
translation,249,188,results,recall,as well as,precision,recall as well as precision,0.5984897017478943
translation,249,188,results,results,has,rouge,results has rouge,0.624253511428833
translation,250,220,experiments,1,set to,"1,000 annotated questions","1 set to 1,000 annotated questions",0.6766305565834045
translation,250,219,hyperparameters,batch size,for,updates,batch size for updates,0.6571369171142578
translation,250,219,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,250,6,model,novel framework,for,annotating,novel framework for annotating,0.6055861711502075
translation,250,6,model,annotating,has,qa datasets,annotating has qa datasets,0.5400583148002625
translation,250,6,model,learning,has,cost-effective annotation policy,learning has cost-effective annotation policy,0.5312981009483337
translation,250,6,model,model,propose,novel framework,model propose novel framework,0.720393180847168
translation,250,27,model,novel annotation framework,learns,cost-effective policy,novel annotation framework learns cost-effective policy,0.6427451372146606
translation,250,27,model,cost-effective policy,for choosing between,different annotation schemes,cost-effective policy for choosing between different annotation schemes,0.7180126309394836
translation,250,27,model,different annotation schemes,namely,conventional manual annotation scheme ( man ),different annotation schemes namely conventional manual annotation scheme ( man ),0.6837282180786133
translation,250,27,model,different annotation schemes,namely,semi-supervised annotation scheme ( sem ),different annotation schemes namely semi-supervised annotation scheme ( sem ),0.6769758462905884
translation,250,27,model,model,propose,novel annotation framework,model propose novel annotation framework,0.6703408360481262
translation,250,210,model,simple tf - idf - based information retrieval module,with,bert,simple tf - idf - based information retrieval module with bert,0.6419811844825745
translation,250,210,model,bert,as a module,machine comprehension,bert as a module machine comprehension,0.7878128886222839
translation,250,210,model,model,combines,simple tf - idf - based information retrieval module,model combines simple tf - idf - based information retrieval module,0.702219545841217
translation,250,218,model,policy networks,decide upon,annotation method,policy networks decide upon annotation method,0.7059904336929321
translation,250,218,model,annotation method,based on,features,annotation method based on features,0.6180322170257568
translation,250,218,model,features,of,2n highest - ranked candidates,features of 2n highest - ranked candidates,0.5715329051017761
translation,250,218,model,2n highest - ranked candidates,i.e.,top - 10,2n highest - ranked candidates i.e. top - 10,0.6695519089698792
translation,250,218,model,model,has,policy networks,model has policy networks,0.5386264324188232
translation,250,48,results,annotation costs,range of,4.1 % to 21.1 %,annotation costs range of 4.1 % to 21.1 %,0.7021017670631409
translation,250,48,results,our framework,has,outperforms,our framework has outperforms,0.6379533410072327
translation,250,48,results,outperforms,has,traditional manual annotation,outperforms has traditional manual annotation,0.5848654508590698
translation,250,48,results,reduces,has,annotation costs,reduces has annotation costs,0.5563269853591919
translation,250,48,results,results,has,our framework,results has our framework,0.6097875237464905
translation,250,50,results,question,has,answering,question has answering,0.6137614250183105
translation,250,232,results,our framework,successfully reduces,annotation cost,our framework successfully reduces annotation cost,0.6315829157829285
translation,250,232,results,annotation cost,by around 15 %,after only 20 batches,annotation cost by around 15 % after only 20 batches,0.7328548431396484
translation,250,232,results,"conventional , manual annotation",has,our framework,"conventional , manual annotation has our framework",0.5626298785209656
translation,250,232,results,results,Compared to,"conventional , manual annotation","results Compared to conventional , manual annotation",0.6286845207214355
translation,250,266,results,relative cost ratio c,able to,save,relative cost ratio c able to save,0.6387066841125488
translation,250,266,results,between 4.1 % and 21.1 % percent,of,overall annotation cost,between 4.1 % and 21.1 % percent of overall annotation cost,0.5751767754554749
translation,250,266,results,save,has,between 4.1 % and 21.1 % percent,save has between 4.1 % and 21.1 % percent,0.5861195921897888
translation,250,266,results,results,Depending on,relative cost ratio c,results Depending on relative cost ratio c,0.7435603141784668
translation,251,22,baselines,different nn - based approaches,including,cnns and lstms,different nn - based approaches including cnns and lstms,0.6491677165031433
translation,251,22,baselines,different nn - based approaches,to compute,representations,different nn - based approaches to compute representations,0.7243691682815552
translation,251,22,baselines,representations,of,q&as,representations of q&as,0.7083687782287598
translation,251,22,baselines,baselines,present,different nn - based approaches,baselines present different nn - based approaches,0.6490061283111572
translation,251,27,baselines,nn - based approaches,LSTM with,"attention , cnn and rcnn )","nn - based approaches LSTM with attention , cnn and rcnn )",0.7960528135299683
translation,251,27,baselines,nn - based approaches,for learning,vector representations,nn - based approaches for learning vector representations,0.7494871020317078
translation,251,27,baselines,vector representations,of,questions and answers,vector representations of questions and answers,0.5948068499565125
translation,251,27,baselines,vector representations,to be used for capturing,semantic similarity,vector representations to be used for capturing semantic similarity,0.6707202792167664
translation,251,27,baselines,baselines,present,nn - based approaches,baselines present nn - based approaches,0.6476490497589111
translation,251,70,experimental-setup,each document,with,"mada 3.1 ( habash et al. , 2009 )","each document with mada 3.1 ( habash et al. , 2009 )",0.5877319574356079
translation,251,70,experimental-setup,each document,with,context-sensitive lemmatizer,each document with context-sensitive lemmatizer,0.6398454904556274
translation,251,70,experimental-setup,"mada 3.1 ( habash et al. , 2009 )",for finding,word lemmas and part-ofspeech tags,"mada 3.1 ( habash et al. , 2009 ) for finding word lemmas and part-ofspeech tags",0.6105143427848816
translation,251,70,experimental-setup,"mada 3.1 ( habash et al. , 2009 )",has,context-sensitive lemmatizer,"mada 3.1 ( habash et al. , 2009 ) has context-sensitive lemmatizer",0.5491562485694885
translation,251,70,experimental-setup,experimental setup,preprocess,each document,experimental setup preprocess each document,0.6917333602905273
translation,251,13,experiments,question-external question -comment pair similarity,rerank,30 question - comment pairs,question-external question -comment pair similarity rerank 30 question - comment pairs,0.625808596611023
translation,251,13,experiments,30 question - comment pairs,according to,relevance,30 question - comment pairs according to relevance,0.6634039282798767
translation,251,13,experiments,relevance,with respect to,original question,relevance with respect to original question,0.5828179717063904
translation,251,103,model,rcnns,extract and aggregate,all possible n-grams,rcnns extract and aggregate all possible n-grams,0.7000427842140198
translation,251,103,model,all possible n-grams,within,input sequence,all possible n-grams within input sequence,0.6012001037597656
translation,251,103,model,all possible n-grams,including ones that are,consecutive,all possible n-grams including ones that are consecutive,0.766048789024353
translation,251,123,results,significantly higher,than,baselines,significantly higher than baselines,0.6176013350486755
translation,251,123,results,comparable,with,best results,comparable with best results,0.6343085169792175
translation,251,123,results,best results,over,all performance metrics,best results over all performance metrics,0.6499176025390625
translation,251,130,results,our results,are,significantly better,our results are significantly better,0.5901349782943726
translation,251,130,results,significantly better,than,baselines,significantly better than baselines,0.6141255497932434
translation,251,130,results,results,are,significantly better,results are significantly better,0.5535706281661987
translation,251,131,results,no significant difference,between,our contrastive1 and contrastive2 results,no significant difference between our contrastive1 and contrastive2 results,0.6245514750480652
translation,251,131,results,our contrastive1 and contrastive2 results,with,best result,our contrastive1 and contrastive2 results with best result,0.6395374536514282
translation,251,131,results,second best semeval result,on,map,second best semeval result on map,0.5538977384567261
translation,251,131,results,highest result,obtained with,our primary result,highest result obtained with our primary result,0.6284054517745972
translation,251,132,results,"combination of bov , lstm and rcnn",achieves,highest result,"combination of bov , lstm and rcnn achieves highest result",0.6667693257331848
translation,251,132,results,highest result,on,map,highest result on map,0.5397356152534485
translation,251,132,results,highest result,is,best,highest result is best,0.5575038194656372
translation,251,132,results,combination of bov and rcnn,is,best,combination of bov and rcnn is best,0.5740243196487427
translation,251,132,results,best,on,f1,best on f1,0.5707892179489136
translation,251,136,results,results,lower than,best se-meval results,results lower than best se-meval results,0.7055658102035522
translation,251,136,results,significantly higher,than,baselines,significantly higher than baselines,0.6176013350486755
translation,251,136,results,results,has,results,results has results,0.48582205176353455
translation,251,150,results,our primary submission,ranks,first,our primary submission ranks first,0.6929823756217957
translation,251,150,results,our primary submission,ranks,f1,our primary submission ranks f1,0.7642341256141663
translation,251,150,results,first,on,all ranking metrics,first on all ranking metrics,0.5026248693466187
translation,251,150,results,first,on,f1,first on f1,0.6165491342544556
translation,251,151,results,contrastive submissions,are,very competitive,contrastive submissions are very competitive,0.5452444553375244
translation,251,151,results,results,has,contrastive submissions,results has contrastive submissions,0.5375832915306091
translation,251,153,results,general domain vectors,were,useful,general domain vectors were useful,0.6246976256370544
translation,251,153,results,useful,with,no shrinking,useful with no shrinking,0.7016586661338806
translation,251,153,results,domain-specific ones,were,more beneficial,domain-specific ones were more beneficial,0.5515900254249573
translation,251,153,results,more beneficial,with,shrinking,more beneficial with shrinking,0.6895430088043213
translation,251,153,results,mixed results,has,general domain vectors,mixed results has general domain vectors,0.5904263257980347
translation,251,153,results,results,found,mixed results,results found mixed results,0.6842554211616516
translation,251,153,results,results,found,general domain vectors,results found general domain vectors,0.6633168458938599
translation,251,154,results,word vectors,trained on,combined corpus,word vectors trained on combined corpus,0.7245166301727295
translation,251,154,results,word vectors,result in,additional improvement,word vectors result in additional improvement,0.6524134278297424
translation,251,154,results,combined corpus,has,from both raw datasets,combined corpus has from both raw datasets,0.5761245489120483
translation,251,154,results,results,Using,word vectors,results Using word vectors,0.6304824948310852
translation,252,140,ablation-analysis,input paragraph,with,answer sentence,input paragraph with answer sentence,0.6027974486351013
translation,252,140,ablation-analysis,answer sentence,has,hurts,answer sentence has hurts,0.646458625793457
translation,252,140,ablation-analysis,hurts,has,model performance,hurts has model performance,0.5826900005340576
translation,252,140,ablation-analysis,ablation analysis,replacing,input paragraph,ablation analysis replacing input paragraph,0.6559373736381531
translation,252,143,ablation-analysis,performance,compared with,input paragraph,performance compared with input paragraph,0.6950685381889343
translation,252,143,ablation-analysis,ablation analysis,demonstrate,input answerable question,ablation analysis demonstrate input answerable question,0.6362897157669067
translation,252,115,experimental-setup,corpus,with,spacy toolkit,corpus with spacy toolkit,0.6509197950363159
translation,252,115,experimental-setup,spacy toolkit,for,tokenization and sentence segmentation,spacy toolkit for tokenization and sentence segmentation,0.5934964418411255
translation,252,115,experimental-setup,experimental setup,preprocess,corpus,experimental setup preprocess corpus,0.728046715259552
translation,252,116,experimental-setup,lowercase tokens,build,vocabulary,lowercase tokens build vocabulary,0.7028168439865112
translation,252,116,experimental-setup,vocabulary,on,squad 2.0 training set,vocabulary on squad 2.0 training set,0.514976441860199
translation,252,116,experimental-setup,squad 2.0 training set,with,word frequency threshold,squad 2.0 training set with word frequency threshold,0.6336513161659241
translation,252,116,experimental-setup,word frequency threshold,of,9,word frequency threshold of 9,0.6722789406776428
translation,252,116,experimental-setup,word frequency threshold,to remove,most noisy tokens,word frequency threshold to remove most noisy tokens,0.6335941553115845
translation,252,116,experimental-setup,experimental setup,has,lowercase tokens,experimental setup has lowercase tokens,0.5606316328048706
translation,252,117,experimental-setup,"word , character and token type embeddings dimension",to,300,"word , character and token type embeddings dimension to 300",0.5556675791740417
translation,252,117,experimental-setup,experimental setup,set,"word , character and token type embeddings dimension","experimental setup set word , character and token type embeddings dimension",0.6178188920021057
translation,252,118,experimental-setup,experimental setup,use,glove,experimental setup use glove,0.6229132413864136
translation,252,119,experimental-setup,840b.300d,to initialize,word embeddings,840b.300d to initialize word embeddings,0.6836392879486084
translation,252,119,experimental-setup,840b.300d,do,further updates,840b.300d do further updates,0.5012930631637573
translation,252,119,experimental-setup,pre-trained embeddings,to initialize,word embeddings,pre-trained embeddings to initialize word embeddings,0.6424980163574219
translation,252,119,experimental-setup,further updates,during,training,further updates during training,0.7601144313812256
translation,252,119,experimental-setup,840b.300d,has,pre-trained embeddings,840b.300d has pre-trained embeddings,0.5673120021820068
translation,252,119,experimental-setup,experimental setup,has,840b.300d,experimental setup has 840b.300d,0.5236150622367859
translation,252,121,experimental-setup,hidden state size,of,lstm network,hidden state size of lstm network,0.561460018157959
translation,252,121,experimental-setup,hidden state size,is,150,hidden state size is 150,0.6180418133735657
translation,252,121,experimental-setup,experimental setup,has,hidden state size,experimental setup has hidden state size,0.5191042423248291
translation,252,122,experimental-setup,dropout probability,set to,0.2,dropout probability set to 0.2,0.7079029083251953
translation,252,122,experimental-setup,experimental setup,has,dropout probability,experimental setup has dropout probability,0.5022250413894653
translation,252,123,experimental-setup,data,shuffled and split into,mini-batches,data shuffled and split into mini-batches,0.7520028948783875
translation,252,123,experimental-setup,mini-batches,of size,32,mini-batches of size 32,0.7157005667686462
translation,252,123,experimental-setup,32,for,training,32 for training,0.6913132667541504
translation,252,123,experimental-setup,experimental setup,has,data,experimental setup has data,0.5174660086631775
translation,252,124,experimental-setup,model,optimized with,"adagrad ( duchi et al. , 2011 )","model optimized with adagrad ( duchi et al. , 2011 )",0.6324284076690674
translation,252,124,experimental-setup,"adagrad ( duchi et al. , 2011 )",with,initial learning rate,"adagrad ( duchi et al. , 2011 ) with initial learning rate",0.5280276536941528
translation,252,124,experimental-setup,initial learning rate,of,0.15,initial learning rate of 0.15,0.5833383202552795
translation,252,124,experimental-setup,experimental setup,optimized with,"adagrad ( duchi et al. , 2011 )","experimental setup optimized with adagrad ( duchi et al. , 2011 )",0.6329172253608704
translation,252,124,experimental-setup,experimental setup,has,model,experimental setup has model,0.5338840484619141
translation,252,125,experimental-setup,beam size,is,5,beam size is 5,0.657706618309021
translation,252,125,experimental-setup,inference,has,beam size,inference has beam size,0.5730389356613159
translation,252,125,experimental-setup,experimental setup,During,inference,experimental setup During inference,0.6632732152938843
translation,252,176,experimental-setup,"uncased version of bert ( devlin et al. , 2019 )",for,fine-tuning,"uncased version of bert ( devlin et al. , 2019 ) for fine-tuning",0.5739331245422363
translation,252,176,experimental-setup,experimental setup,adopt,"uncased version of bert ( devlin et al. , 2019 )","experimental setup adopt uncased version of bert ( devlin et al. , 2019 )",0.5816214084625244
translation,252,177,experimental-setup,batch sizes,of,bert - base and bert - large,batch sizes of bert - base and bert - large,0.6366722583770752
translation,252,177,experimental-setup,batch sizes,set to,12 and 24,batch sizes set to 12 and 24,0.7218316197395325
translation,252,177,experimental-setup,bert - base and bert - large,set to,12 and 24,bert - base and bert - large set to 12 and 24,0.6820493340492249
translation,252,177,experimental-setup,experimental setup,has,batch sizes,experimental setup has batch sizes,0.5236980319023132
translation,252,5,model,data augmentation technique,by automatically generating,relevant unanswerable questions,data augmentation technique by automatically generating relevant unanswerable questions,0.7348809838294983
translation,252,5,model,relevant unanswerable questions,according to,answerable question,relevant unanswerable questions according to answerable question,0.6631033420562744
translation,252,5,model,answerable question,paired with,corresponding paragraph,answerable question paired with corresponding paragraph,0.6528949737548828
translation,252,5,model,corresponding paragraph,contains,answer,corresponding paragraph contains answer,0.6715684533119202
translation,252,5,model,model,propose,data augmentation technique,model propose data augmentation technique,0.6613809466362
translation,252,6,model,pair-to-sequence model,for,unanswerable question generation,pair-to-sequence model for unanswerable question generation,0.5884049534797668
translation,252,6,model,model,introduce,pair-to-sequence model,model introduce pair-to-sequence model,0.6276729702949524
translation,252,29,model,interactions,between,questions and paragraphs,interactions between questions and paragraphs,0.6468521356582642
translation,252,29,model,model,introduce,pair-to-sequence model,model introduce pair-to-sequence model,0.6276729702949524
translation,252,30,model,attention - based matching,aware of,each other,attention - based matching aware of each other,0.6905567646026611
translation,252,31,model,context - aware representations,to generate,outputs,context - aware representations to generate outputs,0.7117389440536499
translation,252,31,model,model,has,context - aware representations,model has context - aware representations,0.5391252040863037
translation,252,32,model,context words,during,generation process,context words during generation process,0.6275715827941895
translation,252,32,model,context words,incorporate,copy mechanism,context words incorporate copy mechanism,0.6487530469894409
translation,252,32,model,model,To facilitate,context words,model To facilitate context words,0.6425880193710327
translation,252,32,model,model,incorporate,copy mechanism,model incorporate copy mechanism,0.7351308465003967
translation,252,66,model,pair-to-sequence model,to capture,interactions,pair-to-sequence model to capture interactions,0.7189960479736328
translation,252,66,model,interactions,be-tween,inputs,interactions be-tween inputs,0.6687438488006592
translation,252,66,model,model,has,pair-to-sequence model,model has pair-to-sequence model,0.5666984915733337
translation,252,120,model,both encoder and decoder,share,same vocabulary and word embeddings,both encoder and decoder share same vocabulary and word embeddings,0.652630090713501
translation,252,120,model,model,has,both encoder and decoder,model has both encoder and decoder,0.5757463574409485
translation,252,33,results,unanswerable question generation task,shows,pair-tosequence model,unanswerable question generation task shows pair-tosequence model,0.582256019115448
translation,252,33,results,pair-tosequence model,generates,consistently better results,pair-tosequence model generates consistently better results,0.6582279801368713
translation,252,33,results,pair-tosequence model,performs,better,pair-tosequence model performs better,0.6401119232177734
translation,252,33,results,consistently better results,over,sequence - to-sequence baseline,consistently better results over sequence - to-sequence baseline,0.6876218318939209
translation,252,33,results,better,with,long paragraphs,better with long paragraphs,0.6548280715942383
translation,252,33,results,long paragraphs,than with,short answer sentences,long paragraphs than with short answer sentences,0.6517776846885681
translation,252,33,results,results,on,unanswerable question generation task,results on unanswerable question generation task,0.5062012076377869
translation,252,35,results,bert fine - tuning,as,reading comprehension model,bert fine - tuning as reading comprehension model,0.5767141580581665
translation,252,35,results,bert fine - tuning,obtain,1.9 % absolute improvement,bert fine - tuning obtain 1.9 % absolute improvement,0.5611727833747864
translation,252,35,results,1.9 % absolute improvement,of,f1 score,1.9 % absolute improvement of f1 score,0.5524769425392151
translation,252,35,results,f1 score,with,bert - base model,f1 score with bert - base model,0.624797523021698
translation,252,35,results,1.7 % absolute f1 improvement,with,bert - large model,1.7 % absolute f1 improvement with bert - large model,0.622044026851654
translation,252,35,results,results,using,bert fine - tuning,results using bert fine - tuning,0.6223839521408081
translation,252,139,results,proposed pair-tosequence model,that captures,interactions,proposed pair-tosequence model that captures interactions,0.742886483669281
translation,252,139,results,interactions,between,paragraph and question,interactions between paragraph and question,0.6635352969169617
translation,252,139,results,interactions,performs,consistently better,interactions performs consistently better,0.6516709923744202
translation,252,139,results,consistently better,than,sequence - to-sequence model,consistently better than sequence - to-sequence model,0.5814804434776306
translation,252,139,results,results,find that,proposed pair-tosequence model,results find that proposed pair-tosequence model,0.6515733599662781
translation,252,142,results,both ablation models,obtain,worse performance,both ablation models obtain worse performance,0.5767633318901062
translation,252,142,results,worse performance,compared with,full model,worse performance compared with full model,0.71638023853302
translation,252,142,results,results,has,both ablation models,results has both ablation models,0.44657623767852783
translation,252,150,results,sequence - to-sequence model,in terms of,all three metrics,sequence - to-sequence model in terms of all three metrics,0.6723446846008301
translation,252,150,results,outperforms,has,sequence - to-sequence model,outperforms has sequence - to-sequence model,0.5925841331481934
translation,252,150,results,results,has,pair-to-sequence model,results has pair-to-sequence model,0.5293745994567871
translation,252,192,results,generated unanswerable questions,improve,specifically designed reading comprehension models,generated unanswerable questions improve specifically designed reading comprehension models,0.5936146378517151
translation,252,192,results,generated unanswerable questions,improve,strong bert fine - tuning models,generated unanswerable questions improve strong bert fine - tuning models,0.5988485217094421
translation,252,192,results,generated unanswerable questions,improve,1.7 absolute f1 improvement,generated unanswerable questions improve 1.7 absolute f1 improvement,0.6316734552383423
translation,252,192,results,strong bert fine - tuning models,yielding,1.7 absolute f1 improvement,strong bert fine - tuning models yielding 1.7 absolute f1 improvement,0.5846552848815918
translation,252,192,results,1.9 absolute f1 improvement,with,bertbase model,1.9 absolute f1 improvement with bertbase model,0.628831684589386
translation,252,192,results,1.9 absolute f1 improvement,with,bert - large model,1.9 absolute f1 improvement with bert - large model,0.625639021396637
translation,252,192,results,1.7 absolute f1 improvement,with,bert - large model,1.7 absolute f1 improvement with bert - large model,0.6202608346939087
translation,252,192,results,results,see that,generated unanswerable questions,results see that generated unanswerable questions,0.6683439016342163
translation,252,193,results,submitted model,obtains,em score,submitted model obtains em score,0.6515541076660156
translation,252,193,results,submitted model,obtains,f1 score,submitted model obtains f1 score,0.580876350402832
translation,252,193,results,submitted model,on,hidden test set,submitted model on hidden test set,0.5754624605178833
translation,252,193,results,em score,of,80.75,em score of 80.75,0.5262594223022461
translation,252,193,results,f1 score,of,83.85,f1 score of 83.85,0.5126219987869263
translation,252,193,results,83.85,on,hidden test set,83.85 on hidden test set,0.5305376052856445
translation,252,193,results,results,has,submitted model,results has submitted model,0.5483712553977966
translation,252,194,results,pair-to-sequence model,proves to be,better option,pair-to-sequence model proves to be better option,0.6969038248062134
translation,252,194,results,better option,for generating,augmentation data,better option for generating augmentation data,0.7019392251968384
translation,252,194,results,augmentation data,than,other three methods,augmentation data than other three methods,0.5221197605133057
translation,252,194,results,results,has,pair-to-sequence model,results has pair-to-sequence model,0.5293745994567871
translation,252,198,results,pair-to-sequence model,yield,largest improvement,pair-to-sequence model yield largest improvement,0.7368825078010559
translation,252,199,results,enlarging,size of,augmentation data,enlarging size of augmentation data,0.6930088400840759
translation,252,199,results,model performance,especially with,bertbase model,model performance especially with bertbase model,0.6600928902626038
translation,252,199,results,augmentation data,has,further improve,augmentation data has further improve,0.596462070941925
translation,252,199,results,further improve,has,model performance,further improve has model performance,0.5723913908004761
translation,252,199,results,results,show,enlarging,results show enlarging,0.6665883660316467
translation,252,205,results,relevant unanswerable questions,by editing,answerable questions,relevant unanswerable questions by editing answerable questions,0.6651148200035095
translation,252,205,results,relevant unanswerable questions,conditioning on,corresponding paragraph,relevant unanswerable questions conditioning on corresponding paragraph,0.6882417798042297
translation,252,205,results,results,produce,relevant unanswerable questions,results produce relevant unanswerable questions,0.5963442325592041
translation,253,5,model,task,with,global inference process,task with global inference process,0.6099621653556824
translation,253,5,model,global inference process,to exploit,information,global inference process to exploit information,0.7107022404670715
translation,253,5,model,information,of,all comments,information of all comments,0.5483351945877075
translation,253,5,model,information,in the form of,fully connected graph,information in the form of fully connected graph,0.656144380569458
translation,253,5,model,all comments,in,answer -thread,all comments in answer -thread,0.5686245560646057
translation,253,5,model,model,approach,task,model approach task,0.7379855513572693
translation,253,6,model,inference,within,learning,inference within learning,0.6554978489875793
translation,253,6,model,model,comprises,two novel joint learning models,model comprises two novel joint learning models,0.6401665806770325
translation,253,7,model,first one,jointly learns,two node- and edge-level maxent classifiers,first one jointly learns two node- and edge-level maxent classifiers,0.7397932410240173
translation,253,7,model,first one,integrates,inference step,first one integrates inference step,0.6873698830604553
translation,253,7,model,two node- and edge-level maxent classifiers,with,stochastic gradient descent,two node- and edge-level maxent classifiers with stochastic gradient descent,0.5686882734298706
translation,253,7,model,inference step,with,loopy belief propagation,inference step with loopy belief propagation,0.6342148184776306
translation,253,7,model,model,has,first one,model has first one,0.5855533480644226
translation,253,42,model,classification decisions,level of,nodes and edges,classification decisions level of nodes and edges,0.42805278301239014
translation,253,42,model,global inference,to get,best label assignment,global inference to get best label assignment,0.5851789712905884
translation,253,42,model,best label assignment,to,all comments,best label assignment to all comments,0.5495772957801819
translation,253,42,model,model,has,classification decisions,model has classification decisions,0.5600196123123169
translation,253,43,model,online models,for learning,decisions jointly,online models for learning decisions jointly,0.724018394947052
translation,253,43,model,online models,incorporating,inference,online models incorporating inference,0.7280334234237671
translation,253,43,model,inference,inside,joint learning algorithm,inference inside joint learning algorithm,0.6871297359466553
translation,253,43,model,model,propose,online models,model propose online models,0.7165131568908691
translation,253,44,model,joint learning,of,two maxent classifiers,joint learning of two maxent classifiers,0.5024563670158386
translation,253,44,model,joint learning,integrating,global inference,joint learning integrating global inference,0.7000083923339844
translation,253,44,model,two maxent classifiers,with,stochastic gradient descent,two maxent classifiers with stochastic gradient descent,0.5845409631729126
translation,253,44,model,two maxent classifiers,integrating,global inference,two maxent classifiers integrating global inference,0.6831918358802795
translation,253,44,model,global inference,based on,loopy belief propagation,global inference based on loopy belief propagation,0.6465150117874146
translation,253,44,model,model,propose,joint learning,model propose joint learning,0.6796736717224121
translation,253,45,model,joint model,with,global normalization,joint model with global normalization,0.6303520798683167
translation,253,45,model,global normalization,instance of,fully connected conditional random fields,global normalization instance of fully connected conditional random fields,0.6606532335281372
translation,253,45,model,model,propose,joint model,model propose joint model,0.6510351896286011
translation,253,9,results,fccrf model,yields,best results,fccrf model yields best results,0.7033753395080566
translation,253,9,results,best results,on,task to date,best results on task to date,0.5420993566513062
translation,253,9,results,fccrf model,has,significantly outperforms,fccrf model has significantly outperforms,0.6060575842857361
translation,253,9,results,significantly outperforms,has,all other approaches,significantly outperforms has all other approaches,0.5666882395744324
translation,253,9,results,results,has,fccrf model,results has fccrf model,0.5319578647613525
translation,253,46,results,previous state of the art,for,comment classification problem,previous state of the art for comment classification problem,0.5863558053970337
translation,253,46,results,results,compare,joint models,results compare joint models,0.5719584822654724
translation,253,47,results,coupled learning - and - inference model,is,not competitive,coupled learning - and - inference model is not competitive,0.5961399674415588
translation,253,47,results,results,find that,coupled learning - and - inference model,results find that coupled learning - and - inference model,0.6098113656044006
translation,254,197,ablation-analysis,q/a type,decreased,performance,q/a type decreased performance,0.7357454895973206
translation,254,197,ablation-analysis,q/a type,decreased,performance,q/a type decreased performance,0.7357454895973206
translation,254,197,ablation-analysis,performance,deleting,least,performance deleting least,0.8152838945388794
translation,254,197,ablation-analysis,performance,has,most,performance has most,0.6291474103927612
translation,254,197,ablation-analysis,explanation features,has,decreased,explanation features has decreased,0.6133257746696472
translation,254,197,ablation-analysis,decreased,has,performance,decreased has performance,0.5981152057647705
translation,254,197,ablation-analysis,performance,has,least,performance has least,0.6453250646591187
translation,254,197,ablation-analysis,ablation analysis,deleting,q/a type,ablation analysis deleting q/a type,0.7834439277648926
translation,254,197,ablation-analysis,ablation analysis,deleting,explanation features,ablation analysis deleting explanation features,0.7873877882957458
translation,254,200,ablation-analysis,swaf ablation results,indicate,advantage,swaf ablation results indicate advantage,0.5810943841934204
translation,254,200,ablation-analysis,advantage,to using,each type of auxiliary feature,advantage to using each type of auxiliary feature,0.7153659462928772
translation,254,200,ablation-analysis,ablation analysis,has,swaf ablation results,ablation analysis has swaf ablation results,0.44916775822639465
translation,254,201,ablation-analysis,auxiliary feature sets,contributes to,final ensemble 's performance,auxiliary feature sets contributes to final ensemble 's performance,0.6554246544837952
translation,254,201,ablation-analysis,ablation analysis,Each of,auxiliary feature sets,ablation analysis Each of auxiliary feature sets,0.6123446822166443
translation,254,212,ablation-analysis,difference,in,performance,difference in performance,0.548568844795227
translation,254,212,ablation-analysis,performance,obtained when,explanation features,performance obtained when explanation features,0.6658897995948792
translation,254,212,ablation-analysis,explanation features,calculated using,emd,explanation features calculated using emd,0.6630290746688843
translation,254,212,ablation-analysis,explanation features,calculated using,rank-order correlation,explanation features calculated using rank-order correlation,0.6325389742851257
translation,254,212,ablation-analysis,explanation features,ablated from,final ensemble,explanation features ablated from final ensemble,0.6801480650901794
translation,254,212,ablation-analysis,rank-order correlation,ablated from,final ensemble,rank-order correlation ablated from final ensemble,0.7327064871788025
translation,254,212,ablation-analysis,ablation analysis,shows,difference,ablation analysis shows difference,0.6783558130264282
translation,254,73,baselines,total of four different categories,of,auxiliary features,total of four different categories of auxiliary features,0.5774657726287842
translation,254,73,baselines,auxiliary features,for,vqa,auxiliary features for vqa,0.6272662878036499
translation,254,137,baselines,baselines,has,long short -term memory ( lstm ),baselines has long short -term memory ( lstm ),0.5554016828536987
translation,254,174,baselines,neural module networks ( nmns ),has,"andreas et al. , 2016","neural module networks ( nmns ) has andreas et al. , 2016",0.549009382724762
translation,254,174,baselines,baselines,compare against,other state - of - the - art vqa systems,baselines compare against other state - of - the - art vqa systems,0.6452000737190247
translation,254,175,baselines,ibowimg,concatenates,image features,ibowimg concatenates image features,0.7445632815361023
translation,254,175,baselines,ibowimg,feeds them into,softmax classifier,ibowimg feeds them into softmax classifier,0.686958909034729
translation,254,175,baselines,image features,with,bag-of- word question embedding,image features with bag-of- word question embedding,0.5676239132881165
translation,254,175,baselines,softmax classifier,to predict,answer,softmax classifier to predict answer,0.7414696216583252
translation,254,175,baselines,softmax classifier,resulting in,performance,softmax classifier resulting in performance,0.645078718662262
translation,254,175,baselines,performance,comparable to,other models,performance comparable to other models,0.696509599685669
translation,254,175,baselines,baselines,has,ibowimg,baselines has ibowimg,0.5746508240699768
translation,254,140,experimental-setup,lstm,with,two hidden layers,lstm with two hidden layers,0.5861539244651794
translation,254,140,experimental-setup,two hidden layers,to obtain,2,two hidden layers to obtain 2,0.6163972020149231
translation,254,140,experimental-setup,two hidden layers,to obtain,048dimensional embedding,two hidden layers to obtain 048dimensional embedding,0.6377911567687988
translation,254,140,experimental-setup,2,",",048dimensional embedding,"2 , 048dimensional embedding",0.6458496451377869
translation,254,140,experimental-setup,2,followed by,fully - connected layer,2 followed by fully - connected layer,0.6483348608016968
translation,254,140,experimental-setup,048dimensional embedding,of,question,048dimensional embedding of question,0.614160418510437
translation,254,140,experimental-setup,048dimensional embedding,followed by,fully - connected layer,048dimensional embedding followed by fully - connected layer,0.6748116612434387
translation,254,140,experimental-setup,fully - connected layer,with,tanh non-linearity,fully - connected layer with tanh non-linearity,0.6681267023086548
translation,254,140,experimental-setup,tanh non-linearity,to transform,embedding,tanh non-linearity to transform embedding,0.7050675749778748
translation,254,140,experimental-setup,embedding,to,1,embedding to 1,0.6661238670349121
translation,254,140,experimental-setup,embedding,to,024 dimensions,embedding to 024 dimensions,0.6252379417419434
translation,254,140,experimental-setup,1,",",024 dimensions,"1 , 024 dimensions",0.6770056486129761
translation,254,140,experimental-setup,2,has,048dimensional embedding,2 has 048dimensional embedding,0.6458227634429932
translation,254,140,experimental-setup,experimental setup,has,lstm,experimental setup has lstm,0.5441007018089294
translation,254,172,experimental-setup,keras,with,tensorflow back - end,keras with tensorflow back - end,0.6672548055648804
translation,254,172,experimental-setup,experimental setup,used,keras,experimental setup used keras,0.6045212149620056
translation,254,9,model,four categories,of,auxiliary features,four categories of auxiliary features,0.596423864364624
translation,254,9,model,four categories,for,ensembling,four categories for ensembling,0.6968990564346313
translation,254,9,model,auxiliary features,for,ensembling,auxiliary features for ensembling,0.6544402241706848
translation,254,9,model,ensembling,for,vqa,ensembling for vqa,0.6594401597976685
translation,254,9,model,model,propose,four categories,model propose four categories,0.6883640885353088
translation,254,31,model,swaf,to more effectively combine,several vqa models,swaf to more effectively combine several vqa models,0.6864883899688721
translation,254,31,model,model,use,swaf,model use swaf,0.6868912577629089
translation,254,37,model,visual explanations,from,various deep learning models,visual explanations from various deep learning models,0.5310531854629517
translation,254,37,model,visual explanations,use,auxiliary features,visual explanations use auxiliary features,0.6448672413825989
translation,254,37,model,visual explanations,as,auxiliary features,visual explanations as auxiliary features,0.49222666025161743
translation,254,37,model,auxiliary features,for,swaf,auxiliary features for swaf,0.6119507551193237
translation,254,37,model,model,extract,visual explanations,model extract visual explanations,0.7275999784469604
translation,254,56,model,visual explanations,for,three different vqa models,visual explanations for three different vqa models,0.5798547863960266
translation,254,56,model,visual explanations,use,explanations,visual explanations use explanations,0.6682285070419312
translation,254,56,model,visual explanations,to develop,auxiliary features,visual explanations to develop auxiliary features,0.6658356785774231
translation,254,56,model,explanations,to develop,auxiliary features,explanations to develop auxiliary features,0.6859893798828125
translation,254,56,model,auxiliary features,aid in,effectively ensembling,auxiliary features aid in effectively ensembling,0.6992878317832947
translation,254,56,model,effectively ensembling,has,vqa systems,effectively ensembling has vqa systems,0.5621082186698914
translation,254,56,model,model,generate,visual explanations,model generate visual explanations,0.7043160200119019
translation,254,56,model,model,use,explanations,model use explanations,0.6950954794883728
translation,254,158,results,best mcb model,won,vqa 2016 challenge,best mcb model won vqa 2016 challenge,0.6984238028526306
translation,254,158,results,best mcb model,by obtaining,best performance,best mcb model by obtaining best performance,0.5785151720046997
translation,254,158,results,vqa 2016 challenge,by obtaining,best performance,vqa 2016 challenge by obtaining best performance,0.5389454364776611
translation,254,158,results,best performance,on,test set,best performance on test set,0.5610934495925903
translation,254,158,results,results,has,best mcb model,results has best mcb model,0.580163836479187
translation,254,168,results,"question , image , and explanation features",found that,neural network,"question , image , and explanation features found that neural network",0.5809581875801086
translation,254,168,results,neural network,with,two hidden layers,neural network with two hidden layers,0.6112470626831055
translation,254,168,results,two hidden layers,works,best,two hidden layers works best,0.6918804049491882
translation,254,168,results,results,For,"question , image , and explanation features","results For question , image , and explanation features",0.5737544894218445
translation,254,176,results,ibowimg,beats,most vqa models,ibowimg beats most vqa models,0.7699485421180725
translation,254,176,results,results,has,ibowimg,results has ibowimg,0.567710816860199
translation,254,192,results,swaf approach,obtains,new state - of- theart result,swaf approach obtains new state - of- theart result,0.6380531787872314
translation,254,192,results,new state - of- theart result,on,vqa task,new state - of- theart result on vqa task,0.5619899034500122
translation,254,192,results,results,has,swaf approach,results has swaf approach,0.5566889643669128
translation,254,193,results,vanilla stacking approach,beats,best individual model,vanilla stacking approach beats best individual model,0.7030484676361084
translation,254,193,results,vanilla stacking approach,adding,auxiliary features,vanilla stacking approach adding auxiliary features,0.6749181747436523
translation,254,193,results,auxiliary features,boosts,performance,auxiliary features boosts performance,0.739719808101654
translation,254,193,results,results,has,vanilla stacking approach,results has vanilla stacking approach,0.5659772157669067
translation,254,194,results,swaf model,uses,all three sets of auxiliary features,swaf model uses all three sets of auxiliary features,0.6346254348754883
translation,254,194,results,all three sets of auxiliary features,related to,iq pairs,all three sets of auxiliary features related to iq pairs,0.6880665421485901
translation,254,194,results,all three sets of auxiliary features,does,particularly well,all three sets of auxiliary features does particularly well,0.27218523621559143
translation,254,194,results,particularly well,on,more difficult   other   answer category,particularly well on more difficult   other   answer category,0.5175820589065552
translation,254,194,results,results,has,swaf model,results has swaf model,0.5248497128486633
translation,254,199,results,voting baseline,does not perform,very well,voting baseline does not perform very well,0.6958289742469788
translation,254,199,results,results,has,voting baseline,results has voting baseline,0.6110708713531494
translation,254,202,results,voting and the   vanilla stacking   ensembles,not perform,as well,voting and the   vanilla stacking   ensembles not perform as well,0.6305071115493774
translation,254,202,results,voting and the   vanilla stacking   ensembles,not perform,swaf,voting and the   vanilla stacking   ensembles not perform swaf,0.6321422457695007
translation,254,202,results,as well,as,swaf,as well as swaf,0.7488155961036682
translation,254,202,results,results,has,voting and the   vanilla stacking   ensembles,results has voting and the   vanilla stacking   ensembles,0.556036114692688
translation,254,204,results,explanations,generated by,various deep learning models,explanations generated by various deep learning models,0.6336240172386169
translation,254,204,results,various deep learning models,as,auxiliary features,various deep learning models as auxiliary features,0.47598499059677124
translation,254,204,results,auxiliary features,improved,performance,auxiliary features improved performance,0.7669676542282104
translation,254,204,results,results,using,explanations,results using explanations,0.6582204103469849
translation,254,213,results,emd,to compare,explanation maps,emd to compare explanation maps,0.7122888565063477
translation,254,213,results,explanation maps,has,more impact,explanation maps has more impact,0.596545934677124
translation,254,213,results,more impact,on,system 's accuracy,more impact on system 's accuracy,0.5650894641876221
translation,254,213,results,explanation maps,has,more impact,explanation maps has more impact,0.596545934677124
translation,254,213,results,results,using,emd,results using emd,0.6629889607429504
translation,254,214,results,our results,confirm,emd,our results confirm emd,0.6147687435150146
translation,254,214,results,emd,provides,finer-grained comparison,emd provides finer-grained comparison,0.6528177857398987
translation,254,214,results,finer-grained comparison,between,localization maps,finer-grained comparison between localization maps,0.68765789270401
translation,254,214,results,results,confirm,emd,results confirm emd,0.554426908493042
translation,255,36,model,constraint - driven learning ( codl ),to cope with,partially annotated structures,constraint - driven learning ( codl ) to cope with partially annotated structures,0.5668318867683411
translation,255,36,model,model,extend,constraint - driven learning ( codl ),model extend constraint - driven learning ( codl ),0.7270516753196716
translation,255,185,results,increases,for,schemes i and ii,increases for schemes i and ii,0.7216952443122864
translation,255,185,results,schemes i and ii,in,all three tasks,schemes i and ii in all three tasks,0.5151309370994568
translation,255,185,results,budget,has,increases,budget has increases,0.6031976342201233
translation,255,185,results,budget,has,system f 1,budget has system f 1,0.6299117207527161
translation,255,185,results,increases,has,system f 1,increases has system f 1,0.6320184469223022
translation,255,185,results,system f 1,has,increases,system f 1 has increases,0.6236201524734497
translation,255,186,results,budget,is,100 %,budget is 100 %,0.6014436483383179
translation,255,186,results,budget,not large enough to cover,entire u 0,budget not large enough to cover entire u 0,0.7006905674934387
translation,255,186,results,schemes i and ii,have,negligible differences,schemes i and ii have negligible differences,0.5454638004302979
translation,255,186,results,budget,not large enough to cover,entire u 0,budget not large enough to cover entire u 0,0.7006905674934387
translation,255,186,results,scheme ii,is,consistently better,scheme ii is consistently better,0.6033689379692078
translation,255,186,results,consistently better,than,i,consistently better than i,0.6648181676864624
translation,255,186,results,consistently better,in,all tasks,consistently better in all tasks,0.5204980373382568
translation,255,186,results,i,in,all tasks,i in all tasks,0.5769438147544861
translation,255,186,results,budget,has,schemes i and ii,budget has schemes i and ii,0.591485857963562
translation,255,186,results,budget,has,scheme ii,budget has scheme ii,0.6123695373535156
translation,255,186,results,100 %,has,schemes i and ii,100 % has schemes i and ii,0.620890200138092
translation,255,186,results,budget,has,scheme ii,budget has scheme ii,0.6123695373535156
translation,255,186,results,results,When,budget,results When budget,0.6087545156478882
translation,256,177,baselines,paraphrase relations,from,wordnet,paraphrase relations from wordnet,0.4804323613643646
translation,256,177,baselines,wordnet,has,paraphrase relations,wordnet has paraphrase relations,0.564764142036438
translation,256,177,baselines,baselines,has,wordnet,baselines has wordnet,0.5642151832580566
translation,256,183,baselines,paraphrase relations,from,ppdb,paraphrase relations from ppdb,0.5455614328384399
translation,256,183,baselines,derived,by,aligning,derived by aligning,0.6333212852478027
translation,256,183,baselines,ppdb,has,paraphrase relations,ppdb has paraphrase relations,0.5554253458976746
translation,256,183,baselines,aligning,has,bilingual parallel texts,aligning has bilingual parallel texts,0.4847818613052368
translation,256,183,baselines,baselines,has,ppdb,baselines has ppdb,0.6042618155479431
translation,256,190,baselines,answered,using,relations,answered using relations,0.7532569766044617
translation,256,190,baselines,relations,pooled from,all dialogs,relations pooled from all dialogs,0.7028858065605164
translation,256,190,baselines,all dialogs,about,all questions,all dialogs about all questions,0.6800281405448914
translation,256,190,baselines,knowbot,has,each question,knowbot has each question,0.6427314877510071
translation,256,190,baselines,baselines,has,knowbot,baselines has knowbot,0.5915399193763733
translation,256,5,model,concepts,in,science questions,concepts in science questions,0.48311713337898254
translation,256,5,model,concepts,to,propositions,concepts to propositions,0.5726118683815002
translation,256,5,model,propositions,in,fact corpus,propositions in fact corpus,0.4991496205329895
translation,256,5,model,new concepts and relations,in,knowledge graph ( kg ),new concepts and relations in knowledge graph ( kg ),0.5130825638771057
translation,256,5,model,new concepts and relations,uses,graph,new concepts and relations uses graph,0.6196119785308838
translation,256,5,model,graph,to solve,questions,graph to solve questions,0.6483033895492554
translation,256,6,model,knowledge,for,question - answering,knowledge for question - answering,0.6267643570899963
translation,256,6,model,knowledge,from,"open , natural language dialogs","knowledge from open , natural language dialogs",0.5707261562347412
translation,256,6,model,question - answering,from,"open , natural language dialogs","question - answering from open , natural language dialogs",0.536750853061676
translation,256,6,model,model,acquire,knowledge,model acquire knowledge,0.6432448029518127
translation,256,7,results,our relation - based strategies,complete,more successful dialogs,our relation - based strategies complete more successful dialogs,0.7642542719841003
translation,256,7,results,more successful dialogs,than,query expansion baseline,more successful dialogs than query expansion baseline,0.5662309527397156
translation,256,7,results,our taskdriven relations,are,more effective,our taskdriven relations are more effective,0.5771822333335876
translation,256,7,results,more effective,for solving,science questions,more effective for solving science questions,0.6079481840133667
translation,256,7,results,science questions,than,relations,science questions than relations,0.6612395644187927
translation,256,7,results,relations,from,general knowledge sources,relations from general knowledge sources,0.5235484838485718
translation,256,7,results,our method,is,practical enough to generalize,our method is practical enough to generalize,0.5886589288711548
translation,256,7,results,results,has,our relation - based strategies,results has our relation - based strategies,0.5425173044204712
translation,256,35,results,paraphrase relations,from,general knowledge bases,paraphrase relations from general knowledge bases,0.5201424956321716
translation,256,35,results,relations,acquired by,our method,relations acquired by our method,0.7389519810676575
translation,256,35,results,relations,are,more effective,relations are more effective,0.6010684967041016
translation,256,35,results,more effective,as,domain knowledge,more effective as domain knowledge,0.4988923966884613
translation,256,35,results,paraphrase relations,has,relations,paraphrase relations has relations,0.6020251512527466
translation,256,35,results,general knowledge bases,has,relations,general knowledge bases has relations,0.5935149192810059
translation,256,35,results,results,In comparison to,paraphrase relations,results In comparison to paraphrase relations,0.6308324337005615
translation,256,162,results,m.i.,cuts,knowledge acquisition rate,m.i. cuts knowledge acquisition rate,0.6549913287162781
translation,256,162,results,nearly in half,compared to,u.i,nearly in half compared to u.i,0.7005025148391724
translation,256,162,results,7.4 novel relations per dialog,to,13.5,7.4 novel relations per dialog to 13.5,0.5705811381340027
translation,256,162,results,knowledge acquisition rate,has,nearly in half,knowledge acquisition rate has nearly in half,0.5732957124710083
translation,256,162,results,u.i,has,7.4 novel relations per dialog,u.i has 7.4 novel relations per dialog,0.6055728793144226
translation,256,162,results,results,has,m.i.,results has m.i.,0.5113751292228699
translation,256,163,results,m.i.,learns,fewer new relations,m.i. learns fewer new relations,0.6577917337417603
translation,256,163,results,fewer new relations,per,dialog,fewer new relations per dialog,0.6628673672676086
translation,256,163,results,dialog,with,comparable task success,dialog with comparable task success,0.6448054909706116
translation,256,163,results,results,has,m.i.,results has m.i.,0.5113751292228699
translation,256,199,results,wordnet,works,surprisingly poorly,wordnet works surprisingly poorly,0.6103663444519043
translation,256,199,results,results,has,wordnet,results has wordnet,0.5814017653465271
translation,256,201,results,ppdb,performed,better,ppdb performed better,0.23771220445632935
translation,256,201,results,underperformed,has,identity,underperformed has identity,0.5919696688652039
translation,256,201,results,results,has,ppdb,results has ppdb,0.6044244170188904
translation,256,203,results,knowbot,achieves,accuracy,knowbot achieves accuracy,0.708690881729126
translation,256,203,results,accuracy,of,57 %,accuracy of 57 %,0.6052657961845398
translation,256,203,results,dramatic improvement,over,both baselines,dramatic improvement over both baselines,0.7401716709136963
translation,256,203,results,57 %,has,dramatic improvement,57 % has dramatic improvement,0.5933487415313721
translation,256,203,results,results,has,knowbot,results has knowbot,0.6168328523635864
translation,257,199,ablation-analysis,attention components,contribute to,performance,attention components contribute to performance,0.651013195514679
translation,257,199,ablation-analysis,performance,of,et - rr,performance of et - rr,0.5982586741447449
translation,257,199,ablation-analysis,ablation analysis,demonstrates,attention components,ablation analysis demonstrates attention components,0.6181647777557373
translation,257,201,ablation-analysis,performance,of,et - rr,performance of et - rr,0.5982586741447449
translation,257,201,ablation-analysis,et - rr,without,passage-question attention,et - rr without passage-question attention,0.7404370903968811
translation,257,201,ablation-analysis,drops,out of,all the components,drops out of all the components,0.6685166954994202
translation,257,201,ablation-analysis,most significantly,out of,all the components,most significantly out of all the components,0.6808172464370728
translation,257,201,ablation-analysis,passage-question attention,has,drops,passage-question attention has drops,0.6004927754402161
translation,257,201,ablation-analysis,drops,has,most significantly,drops has most significantly,0.6314013600349426
translation,257,201,ablation-analysis,ablation analysis,has,performance,ablation analysis has performance,0.5053174495697021
translation,257,202,ablation-analysis,choice interaction layer,gives,further 0.24 % boost,choice interaction layer gives further 0.24 % boost,0.613735020160675
translation,257,202,ablation-analysis,further 0.24 % boost,on,test accuracy,further 0.24 % boost on test accuracy,0.5432774424552917
translation,257,202,ablation-analysis,ablation analysis,worth noting,choice interaction layer,ablation analysis worth noting choice interaction layer,0.6274817585945129
translation,257,208,ablation-analysis,tf - idf,to select,essential terms,tf - idf to select essential terms,0.6984497308731079
translation,257,208,ablation-analysis,essential terms,in,question,essential terms in question,0.503013551235199
translation,257,208,ablation-analysis,essential terms,is,not effective,essential terms is not effective,0.5561785101890564
translation,257,208,ablation-analysis,ablation analysis,using,tf - idf,ablation analysis using tf - idf,0.7017695307731628
translation,257,155,baselines,second best model ( et classifier ),is,svm - based model,second best model ( et classifier ) is svm - based model,0.5566301345825195
translation,257,155,baselines,svm - based model,from,khashabi et al . ( 2017 ),svm - based model from khashabi et al . ( 2017 ),0.5101382732391357
translation,257,155,baselines,svm - based model,requiring,over 100 handcrafted features,svm - based model requiring over 100 handcrafted features,0.6012473702430725
translation,257,155,baselines,baselines,has,second best model ( et classifier ),baselines has second best model ( et classifier ),0.5549194812774658
translation,257,165,baselines,race - open,adapted,race dataset,race - open adapted race dataset,0.7099702954292297
translation,257,165,baselines,race dataset,to,open-domain setting,race dataset to open-domain setting,0.5019478797912598
translation,257,165,baselines,baselines,has,race - open,baselines has race - open,0.548235297203064
translation,257,170,baselines,"mcscript ( ostermann et al. , 2018 ) dataset",adapted to,open-domain setting,"mcscript ( ostermann et al. , 2018 ) dataset adapted to open-domain setting",0.6047364473342896
translation,257,170,baselines,mcscript-open,has,"mcscript ( ostermann et al. , 2018 ) dataset","mcscript-open has mcscript ( ostermann et al. , 2018 ) dataset",0.5590100288391113
translation,257,170,baselines,baselines,has,mcscript-open,baselines has mcscript-open,0.5695258378982544
translation,257,189,baselines,choice,as,query,choice as query,0.5519803166389465
translation,257,189,baselines,query,for,each choice,query for each choice,0.65468829870224
translation,257,143,experimental-setup,et - net and et - rr models,use,96 - dimensional hidden states,et - net and et - rr models use 96 - dimensional hidden states,0.6266685128211975
translation,257,143,experimental-setup,et - net and et - rr models,use,1 - layer bilstms,et - net and et - rr models use 1 - layer bilstms,0.5778844952583313
translation,257,143,experimental-setup,1 - layer bilstms,in,sequence modeling layer,1 - layer bilstms in sequence modeling layer,0.45989829301834106
translation,257,143,experimental-setup,experimental setup,For,et - net and et - rr models,experimental setup For et - net and et - rr models,0.6058715581893921
translation,257,143,experimental-setup,experimental setup,both,et - net and et - rr models,experimental setup both et - net and et - rr models,0.6820152401924133
translation,257,144,experimental-setup,dropout rate,of,0.4,dropout rate of 0.4,0.5964629054069519
translation,257,144,experimental-setup,0.4,applied for,embedding layer,0.4 applied for embedding layer,0.6469605565071106
translation,257,144,experimental-setup,0.4,applied for,bilstms ' output layer,0.4 applied for bilstms ' output layer,0.6179314255714417
translation,257,144,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,257,145,experimental-setup,"adamax ( kingma and ba , 2014 )",with,learning rate,"adamax ( kingma and ba , 2014 ) with learning rate",0.6055169105529785
translation,257,145,experimental-setup,"adamax ( kingma and ba , 2014 )",with,batch size,"adamax ( kingma and ba , 2014 ) with batch size",0.6251144409179688
translation,257,145,experimental-setup,learning rate,of,0.02,learning rate of 0.02,0.6127474308013916
translation,257,145,experimental-setup,batch size,of,32,batch size of 32,0.6741614937782288
translation,257,145,experimental-setup,experimental setup,use,"adamax ( kingma and ba , 2014 )","experimental setup use adamax ( kingma and ba , 2014 )",0.5996699929237366
translation,257,146,experimental-setup,model,trained for,100 epochs,model trained for 100 epochs,0.7761797308921814
translation,257,146,experimental-setup,experimental setup,trained for,100 epochs,experimental setup trained for 100 epochs,0.7018767595291138
translation,257,146,experimental-setup,experimental setup,has,model,experimental setup has model,0.5338840484619141
translation,257,181,experimental-setup,k = 10,for,all experiments,k = 10 for all experiments,0.596473217010498
translation,257,181,experimental-setup,experimental setup,set,k = 10,experimental setup set k = 10,0.6892520785331726
translation,257,7,model,retriever - reader model,learns to,attend,retriever - reader model learns to attend,0.689149022102356
translation,257,7,model,attend,on,essential terms,attend on essential terms,0.5167638659477234
translation,257,7,model,essential terms,during,question answering process,essential terms during question answering process,0.5088786482810974
translation,257,7,model,model,propose,retriever - reader model,model propose retriever - reader model,0.6850383877754211
translation,257,8,model,essential term selector,first identifies,most important words,essential term selector first identifies most important words,0.6998262405395508
translation,257,8,model,essential term selector,reformulates,query,essential term selector reformulates query,0.7100249528884888
translation,257,8,model,essential term selector,searches for,related evidence,essential term selector searches for related evidence,0.6532370448112488
translation,257,8,model,most important words,in,question,most important words in question,0.5394420027732849
translation,257,8,model,query,searches for,related evidence,query searches for related evidence,0.7484363317489624
translation,257,8,model,enhanced reader,distinguishes between,essential terms,enhanced reader distinguishes between essential terms,0.6884667277336121
translation,257,8,model,enhanced reader,distinguishes between,distracting words,enhanced reader distinguishes between distracting words,0.7375558018684387
translation,257,8,model,distracting words,to predict,answer,distracting words to predict answer,0.7495643496513367
translation,257,8,model,model,build,essential term selector,model build essential term selector,0.7166025638580322
translation,257,33,model,two -stage method,with,essential term selector,two -stage method with essential term selector,0.5844023823738098
translation,257,33,model,essential term selector,followed by,attention - enhanced reader,essential term selector followed by attention - enhanced reader,0.6909042596817017
translation,257,33,model,model,develop,two -stage method,model develop two -stage method,0.6022488474845886
translation,257,35,model,et - net,is,recurrent neural network,et - net is recurrent neural network,0.5969598293304443
translation,257,35,model,recurrent neural network,seeks,understand,recurrent neural network seeks understand,0.6793432235717773
translation,257,35,model,recurrent neural network,select,"essential terms , i.e. , key words","recurrent neural network select essential terms , i.e. , key words",0.6037545204162598
translation,257,35,model,understand,from,question,understand from question,0.6209885478019714
translation,257,35,model,"essential terms , i.e. , key words",from,question,"essential terms , i.e. , key words from question",0.5304101705551147
translation,257,35,model,understand,has,question,understand has question,0.6264404654502869
translation,257,35,model,model,has,et - net,model has et - net,0.6524900197982788
translation,257,42,model,choice-interaction module,to handle,semantic relations and differences,choice-interaction module to handle semantic relations and differences,0.703201174736023
translation,257,42,model,semantic relations and differences,between,answer choices,semantic relations and differences between answer choices,0.6655547618865967
translation,257,42,model,model,add,choice-interaction module,model add choice-interaction module,0.6754699349403381
translation,257,156,results,et - net,achieves,comparable result,et - net achieves comparable result,0.7053582668304443
translation,257,156,results,comparable result,with,et classifier,comparable result with et classifier,0.6976335048675537
translation,257,156,results,results,has,et - net,results has et - net,0.5439422130584717
translation,257,183,results,et - rr,achieves,relative 8.1 % improvement,et - rr achieves relative 8.1 % improvement,0.6491386294364929
translation,257,183,results,all previous models,without using,pre-trained models,all previous models without using pre-trained models,0.6914284229278564
translation,257,183,results,relative 8.1 % improvement,over,second best bilstm max-out method,relative 8.1 % improvement over second best bilstm max-out method,0.6364875435829163
translation,257,183,results,arc dataset,has,et - rr,arc dataset has et - rr,0.6124156713485718
translation,257,183,results,et - rr,has,outperforms,et - rr has outperforms,0.6585054397583008
translation,257,183,results,outperforms,has,all previous models,outperforms has all previous models,0.5923312306404114
translation,257,183,results,results,on,arc dataset,results on arc dataset,0.5663174390792847
translation,257,188,results,et - rr,increases,accuracy,et - rr increases accuracy,0.7554967403411865
translation,257,188,results,accuracy,by,10.33 %,accuracy by 10.33 %,0.5779559016227722
translation,257,188,results,10.33 %,compared to,state - of- the - art model moqa,10.33 % compared to state - of- the - art model moqa,0.6271735429763794
translation,257,188,results,results,has,et - rr,results has et - rr,0.5547820925712585
translation,257,190,results,outperforms,shows,effectiveness,outperforms shows effectiveness,0.7284584045410156
translation,257,190,results,et - rr,shows,effectiveness,et - rr shows effectiveness,0.6788962483406067
translation,257,190,results,concat ),shows,effectiveness,concat ) shows effectiveness,0.7094904184341431
translation,257,190,results,effectiveness,of,our essential - term-aware retriever,effectiveness of our essential - term-aware retriever,0.5551961660385132
translation,257,190,results,all datasets,has,et - rr,all datasets has et - rr,0.597501814365387
translation,257,190,results,et - rr,has,outperforms,et - rr has outperforms,0.6585054397583008
translation,257,190,results,outperforms,has,et - rr,outperforms has et - rr,0.6600948572158813
translation,257,190,results,outperforms,has,concat ),outperforms has concat ),0.7066173553466797
translation,257,190,results,et - rr,has,concat ),et - rr has concat ),0.6571969389915466
translation,257,190,results,results,Among,all datasets,results Among all datasets,0.5659634470939636
translation,257,196,results,recently proposed reading strategies and openai gpt models,finetune,generative pre-trained models,recently proposed reading strategies and openai gpt models finetune generative pre-trained models,0.6989001035690308
translation,257,196,results,generative pre-trained models,achieve,highest scores,generative pre-trained models achieve highest scores,0.5829592347145081
translation,257,197,results,relative improvement,of,3.8 %,relative improvement of 3.8 %,0.5568822622299194
translation,257,197,results,nonpre-trained models,has,our reader,nonpre-trained models has our reader,0.5962834358215332
translation,257,197,results,our reader,has,outperforms,our reader has outperforms,0.6325355172157288
translation,257,197,results,outperforms,has,other baselines,outperforms has other baselines,0.5879674553871155
translation,257,197,results,hierarchical co-matching,has,"wang et al. , 2018a","hierarchical co-matching has wang et al. , 2018a",0.529904305934906
translation,257,197,results,results,Among,nonpre-trained models,results Among nonpre-trained models,0.5957952737808228
translation,257,200,results,et - rr,with,all attention components,et - rr with all attention components,0.6072235107421875
translation,257,200,results,et - rr,performs,best,et - rr performs best,0.6499643921852112
translation,257,200,results,best,on,arc test set,best on arc test set,0.5615639686584473
translation,257,200,results,results,has,et - rr,results has et - rr,0.5547820925712585
translation,257,207,results,model,consistently outperforms,other baselines,model consistently outperforms other baselines,0.7877969145774841
translation,257,207,results,other baselines,given,different numbers of retrievals k.,other baselines given different numbers of retrievals k.,0.6642182469367981
translation,257,207,results,performance,for,all models,performance for all models,0.5856176614761353
translation,257,207,results,all models,is,best,all models is best,0.5319570302963257
translation,257,207,results,best,when,k = 10,best when k = 10,0.6777010560035706
translation,257,207,results,essential term selector et - net,has,model,essential term selector et - net has model,0.5565013885498047
translation,257,207,results,different numbers of retrievals k.,has,performance,different numbers of retrievals k. has performance,0.5505775809288025
translation,258,198,ablation-analysis,paragraph,causes,performance,paragraph causes performance,0.7316968441009521
translation,258,198,ablation-analysis,performance,to,drop a little,performance to drop a little,0.5812679529190063
translation,258,198,ablation-analysis,ablation analysis,encoding,paragraph,ablation analysis encoding paragraph,0.8464441895484924
translation,258,162,baselines,"seq2seq ( sutskever et al. , 2014 )",is,basic encoder-decoder sequence learning system,"seq2seq ( sutskever et al. , 2014 ) is basic encoder-decoder sequence learning system",0.5196771025657654
translation,258,162,baselines,basic encoder-decoder sequence learning system,for,machine translation,basic encoder-decoder sequence learning system for machine translation,0.5705737471580505
translation,258,162,baselines,baselines,has,"seq2seq ( sutskever et al. , 2014 )","baselines has seq2seq ( sutskever et al. , 2014 )",0.4700168967247009
translation,258,123,experimental-setup,our models,in,torch7,our models in torch7,0.6090019345283508
translation,258,123,experimental-setup,our models,on top of,newly released opennmt system,our models on top of newly released opennmt system,0.6809802651405334
translation,258,123,experimental-setup,experimental setup,implement,our models,experimental setup implement our models,0.6547874808311462
translation,258,127,experimental-setup,word embedding,of,300 dimensions,word embedding of 300 dimensions,0.5602567195892334
translation,258,127,experimental-setup,experimental setup,choose,word embedding,experimental setup choose word embedding,0.6294605135917664
translation,258,128,experimental-setup,840b.300d pre-trained embeddings,for,initialization,840b.300d pre-trained embeddings for initialization,0.5906303524971008
translation,258,128,experimental-setup,experimental setup,has,840b.300d pre-trained embeddings,experimental setup has 840b.300d pre-trained embeddings,0.5147464871406555
translation,258,129,experimental-setup,word representations,during,training,word representations during training,0.646007239818573
translation,258,129,experimental-setup,experimental setup,fix,word representations,experimental setup fix word representations,0.6215817332267761
translation,258,130,experimental-setup,lstm hidden unit size,to,600,lstm hidden unit size to 600,0.5671337842941284
translation,258,130,experimental-setup,number of layers,of,lstms,number of layers of lstms,0.584155797958374
translation,258,130,experimental-setup,lstms,to,2,lstms to 2,0.5824994444847107
translation,258,130,experimental-setup,2,in,both the encoder and the decoder,2 in both the encoder and the decoder,0.523154079914093
translation,258,130,experimental-setup,experimental setup,set,lstm hidden unit size,experimental setup set lstm hidden unit size,0.618138313293457
translation,258,130,experimental-setup,experimental setup,set,number of layers,experimental setup set number of layers,0.6420333981513977
translation,258,130,experimental-setup,experimental setup,set,number of layers,experimental setup set number of layers,0.6420333981513977
translation,258,131,experimental-setup,optimization,performed using,stochastic gradient descent ( sgd ),optimization performed using stochastic gradient descent ( sgd ),0.5894094705581665
translation,258,131,experimental-setup,stochastic gradient descent ( sgd ),with,initial learning rate,stochastic gradient descent ( sgd ) with initial learning rate,0.6076837182044983
translation,258,131,experimental-setup,initial learning rate,of,1.0,initial learning rate of 1.0,0.5808937549591064
translation,258,131,experimental-setup,experimental setup,has,optimization,experimental setup has optimization,0.5174741744995117
translation,258,132,experimental-setup,learning rate,at,epoch 8,learning rate at epoch 8,0.6083745956420898
translation,258,132,experimental-setup,experimental setup,start halving,learning rate,experimental setup start halving learning rate,0.7213693857192993
translation,258,136,experimental-setup,0.3,applied between,vertical lstm stacks,0.3 applied between vertical lstm stacks,0.6759962439537048
translation,258,136,experimental-setup,experimental setup,has,0.3,experimental setup has 0.3,0.5190725326538086
translation,258,137,experimental-setup,gradient,when,its norm,gradient when its norm,0.6894142627716064
translation,258,137,experimental-setup,its norm,exceeds,5,its norm exceeds 5,0.6607523560523987
translation,258,137,experimental-setup,experimental setup,clip,gradient,experimental setup clip gradient,0.687055766582489
translation,258,139,experimental-setup,training,for,up to 15 epochs,training for up to 15 epochs,0.6083508729934692
translation,258,139,experimental-setup,training,takes,approximately 2 hours,training takes approximately 2 hours,0.625450849533081
translation,258,139,experimental-setup,up to 15 epochs,takes,approximately 2 hours,up to 15 epochs takes approximately 2 hours,0.6411888003349304
translation,258,139,experimental-setup,experimental setup,run,training,experimental setup run training,0.694412350654602
translation,258,154,experiments,tri-gram language model,on,target side texts,tri-gram language model on target side texts,0.5043047666549683
translation,258,154,experiments,tri-gram language model,tune,system,tri-gram language model tune system,0.670335054397583
translation,258,154,experiments,target side texts,with,kenlm,target side texts with kenlm,0.6693891286849976
translation,258,154,experiments,system,with,mert,system with mert,0.7160578966140747
translation,258,154,experiments,mert,on,dev set,mert on dev set,0.628948986530304
translation,258,154,experiments,kenlm,has,"heafield et al. , 2013 )","kenlm has heafield et al. , 2013 )",0.580944299697876
translation,258,5,model,attention - based sequence learning model,investigate,effect,attention - based sequence learning model investigate effect,0.5638514161109924
translation,258,5,model,model,introduce,attention - based sequence learning model,model introduce attention - based sequence learning model,0.599532425403595
translation,258,5,model,model,investigate,effect,model investigate effect,0.6073026657104492
translation,258,30,model,paragraph - rather than sentence - level information,from,reading passage,paragraph - rather than sentence - level information from reading passage,0.5681824684143066
translation,258,30,model,importance,of,pre-trained vs. learned word embeddings,importance of pre-trained vs. learned word embeddings,0.5723118185997009
translation,258,66,model,conditional probability,using,rnn encoder-decoder architecture,conditional probability using rnn encoder-decoder architecture,0.6644671559333801
translation,258,66,model,conditional probability,adopt,global attention mechanism,conditional probability adopt global attention mechanism,0.6278880834579468
translation,258,66,model,global attention mechanism,to make,model,global attention mechanism to make model,0.6035929322242737
translation,258,66,model,model,focus on,certain elements,model focus on certain elements,0.719330906867981
translation,258,66,model,certain elements,of,input,certain elements of input,0.6099767684936523
translation,258,66,model,certain elements,when generating,each word,certain elements when generating each word,0.7180194854736328
translation,258,66,model,input,when generating,each word,input when generating each word,0.6849720478057861
translation,258,66,model,each word,during,decoding,each word during decoding,0.7375367283821106
translation,258,66,model,model,model,conditional probability,model model conditional probability,0.8523901700973511
translation,258,135,results,our system,encodes,only sentence,our system encodes only sentence,0.7230833768844604
translation,258,135,results,only sentence,with,pre-trained word embeddings,only sentence with pre-trained word embeddings,0.6301247477531433
translation,258,135,results,only sentence,achieves,best performance,only sentence achieves best performance,0.6778372526168823
translation,258,135,results,best performance,across,all the metrics,best performance across all the metrics,0.655470073223114
translation,258,135,results,results,has,our system,results has our system,0.5954442024230957
translation,258,194,results,ir,performs,poorly,ir performs poorly,0.6809287071228027
translation,258,194,results,results,note,ir,results note ir,0.5664592981338501
translation,258,195,results,baseline directin,performs,pretty well,baseline directin performs pretty well,0.5977574586868286
translation,258,195,results,pretty well,on,bleu and meteor,pretty well on bleu and meteor,0.5639564990997314
translation,258,195,results,results,has,baseline directin,results has baseline directin,0.5579527020454407
translation,258,196,results,on a par,with,directin's,on a par with directin's,0.7031935453414917
translation,258,196,results,results,has,h&s system 's performance,results has h&s system 's performance,0.5728962421417236
translation,258,197,results,performance,of,three models,performance of three models,0.6432158350944519
translation,258,197,results,three models,adding,pre-trained embeddings,three models adding pre-trained embeddings,0.5880973935127258
translation,258,197,results,pre-trained embeddings,has,helps,pre-trained embeddings has helps,0.5323379039764404
translation,258,197,results,results,Looking at,performance,results Looking at performance,0.6216241121292114
translation,258,200,results,outperforms,in,all modalities,outperforms in all modalities,0.5179200172424316
translation,258,200,results,h&s,in,all modalities,h&s in all modalities,0.5526149272918701
translation,258,200,results,our system,has,outperforms,our system has outperforms,0.6423544883728027
translation,258,200,results,outperforms,has,h&s,outperforms has h&s,0.6769624352455139
translation,258,200,results,results,see that,our system,results see that our system,0.6429948806762695
translation,258,201,results,ranked best,in,38.4 %,ranked best in 38.4 %,0.5376607775688171
translation,258,201,results,ranked best,with,average ranking,ranked best with average ranking,0.6390511989593506
translation,258,201,results,38.4 %,of,evaluations,38.4 % of evaluations,0.569484531879425
translation,258,201,results,average ranking,of,1.94,average ranking of 1.94,0.5442506074905396
translation,258,201,results,results,has,our system,results has our system,0.5954442024230957
translation,258,202,results,inter-rater agreement,of,krippendorff 's alpha,inter-rater agreement of krippendorff 's alpha,0.47091785073280334
translation,258,202,results,krippendorff 's alpha,of,0.236,krippendorff 's alpha of 0.236,0.48594388365745544
translation,258,202,results,0.236,achieved for,overall ranking,0.236 achieved for overall ranking,0.6260964274406433
translation,258,202,results,results,has,inter-rater agreement,results has inter-rater agreement,0.4259409010410309
translation,258,220,results,our model,encodes,paragraph information,our model encodes paragraph information,0.7337552309036255
translation,258,220,results,paragraph information,achieves,best performance,paragraph information achieves best performance,0.6750749349594116
translation,258,220,results,best performance,on,questions of   w/ paragraph   category,best performance on questions of   w/ paragraph   category,0.5228322744369507
translation,258,220,results,results,has,our model,results has our model,0.5871725678443909
translation,259,112,ablation-analysis,aoa layer,improves,performance,aoa layer improves performance,0.6881532669067383
translation,259,112,ablation-analysis,aoa layer,fails to improve,generalization performance,aoa layer fails to improve generalization performance,0.6651038527488708
translation,259,112,ablation-analysis,performance,of,bert,performance of bert,0.6690750122070312
translation,259,112,ablation-analysis,bert,on,squad dataset,bert on squad dataset,0.5268247127532959
translation,259,112,ablation-analysis,generalization performance,on,xlnet,generalization performance on xlnet,0.5386524796485901
translation,259,54,experimental-setup,sequence length,set as,340,sequence length set as 340,0.6274587512016296
translation,259,54,experimental-setup,340,when,fine-tuning,340 when fine-tuning,0.6762149333953857
translation,259,54,experimental-setup,fine-tuning,on,gpu,fine-tuning on gpu,0.5419071316719055
translation,259,54,experimental-setup,512,on,tensor processing unit ( tpu ),512 on tensor processing unit ( tpu ),0.5108515024185181
translation,259,54,experimental-setup,experimental setup,has,sequence length,experimental setup has sequence length,0.4895257353782654
translation,259,55,experimental-setup,datasets,tokenized with,"sentencepiece ( kudo and richardson , 2018 )","datasets tokenized with sentencepiece ( kudo and richardson , 2018 )",0.7722290754318237
translation,259,55,experimental-setup,datasets,uniformed in,lower cases,datasets uniformed in lower cases,0.741245687007904
translation,259,55,experimental-setup,experimental setup,has,datasets,experimental setup has datasets,0.47152194380760193
translation,259,6,model,generalization ability,propose,multi-task learning framework,generalization ability propose multi-task learning framework,0.6005110740661621
translation,259,6,model,multi-task learning framework,that learns,shared representation,multi-task learning framework that learns shared representation,0.5977737307548523
translation,259,6,model,shared representation,across,different tasks,shared representation across different tasks,0.7277781367301941
translation,259,6,model,model,To enhance,generalization ability,model To enhance generalization ability,0.697718620300293
translation,259,104,model,fine-tuned,with,additional mlp,fine-tuned with additional mlp,0.6773192286491394
translation,259,104,model,additional mlp,on,single gpu,additional mlp on single gpu,0.5596544742584229
translation,259,104,model,single gpu,based on,xlnet-large,single gpu based on xlnet-large,0.6222063302993774
translation,259,106,results,outperforms,in which,datasets,outperforms in which datasets,0.5993351936340332
translation,259,106,results,method,in which,datasets,method in which datasets,0.5435081124305725
translation,259,106,results,multi-task data feeding method,has,outperforms,multi-task data feeding method has outperforms,0.6069633960723877
translation,259,106,results,outperforms,has,method,outperforms has method,0.6658044457435608
translation,259,106,results,fed,has,one after another,fed has one after another,0.6357674598693848
translation,259,106,results,results,has,multi-task data feeding method,results has multi-task data feeding method,0.5315844416618347
translation,259,107,results,multitask learning,tends to enable,model,multitask learning tends to enable model,0.6972180604934692
translation,259,107,results,model,to achieve,uniform generalization performance,model to achieve uniform generalization performance,0.6391920447349548
translation,259,107,results,uniform generalization performance,on,unseen datasets,uniform generalization performance on unseen datasets,0.536225438117981
translation,259,107,results,single - task feeding method,better benefits,tasks,single - task feeding method better benefits tasks,0.6834203600883484
translation,259,114,results,xlnet model finetuned,with,mlp,xlnet model finetuned with mlp,0.6481791138648987
translation,259,114,results,mlp,on,tpu,mlp on tpu,0.5702344179153442
translation,259,114,results,mlp,achieves,best performance,mlp achieves best performance,0.7135545611381531
translation,259,114,results,results,has,xlnet model finetuned,results has xlnet model finetuned,0.574127733707428
translation,259,116,results,tpu,shows,effectiveness,tpu shows effectiveness,0.691119909286499
translation,259,116,results,effectiveness,on,training,effectiveness on training,0.5875539183616638
translation,259,116,results,training,with,ability,training with ability,0.6088686585426331
translation,259,116,results,ability,to afford,larger model,ability to afford larger model,0.6911555528640747
translation,259,116,results,ability,to afford,batch size,ability to afford batch size,0.6934088468551636
translation,259,116,results,ability,to afford,sequence length,ability to afford sequence length,0.6720808744430542
translation,259,116,results,results,has,tpu,results has tpu,0.5887479782104492
translation,259,118,results,larger linear layer,on,tpu,larger linear layer on tpu,0.5790171027183533
translation,259,118,results,bert - large baseline,by,huge margin,bert - large baseline by huge margin,0.5643002986907959
translation,259,118,results,outperforms,has,bert - large baseline,outperforms has bert - large baseline,0.6092385053634644
translation,259,119,results,our xlnet based model,fine-tuned under,multi-task learning setting,our xlnet based model fine-tuned under multi-task learning setting,0.6839978098869324
translation,259,119,results,our xlnet based model,shows,robust generalization and transferring ability,our xlnet based model shows robust generalization and transferring ability,0.6187095046043396
translation,259,119,results,robust generalization and transferring ability,over,baseline,robust generalization and transferring ability over baseline,0.6849960684776306
translation,259,119,results,test set,has,our xlnet based model,test set has our xlnet based model,0.5952655673027039
translation,259,119,results,results,On,test set,results On test set,0.582119882106781
translation,260,52,hyperparameters,glove,for,word embeddings,glove for word embeddings,0.6049095988273621
translation,260,52,hyperparameters,),for,word embeddings,) for word embeddings,0.6458534002304077
translation,260,52,hyperparameters,glove,has,),glove has ),0.6335808038711548
translation,260,52,hyperparameters,hyperparameters,use,glove,hyperparameters use glove,0.6525245904922485
translation,260,99,hyperparameters,size,of,visual feature,size of visual feature,0.5710943937301636
translation,260,99,hyperparameters,visual feature,of,each object,visual feature of each object,0.5627806186676025
translation,260,99,hyperparameters,visual feature,set to,2048,visual feature set to 2048,0.7128210067749023
translation,260,99,hyperparameters,dimension,of,hidden layer,dimension of hidden layer,0.6008988618850708
translation,260,99,hyperparameters,hidden layer,of,question encoder and caption encoder,hidden layer of question encoder and caption encoder,0.5746489763259888
translation,260,99,hyperparameters,question encoder and caption encoder,are,1024 and 2048,question encoder and caption encoder are 1024 and 2048,0.5922583937644958
translation,260,99,hyperparameters,hyperparameters,has,size,hyperparameters has size,0.5313372611999512
translation,260,100,hyperparameters,"adamax ( kingma and ba , 2014 )",for,optimizer,"adamax ( kingma and ba , 2014 ) for optimizer",0.5850836634635925
translation,260,100,hyperparameters,"adamax ( kingma and ba , 2014 )",for,learning rate,"adamax ( kingma and ba , 2014 ) for learning rate",0.570099413394928
translation,260,100,hyperparameters,learning rate,of,0.002,learning rate of 0.002,0.5967087745666504
translation,260,101,hyperparameters,final credit,added to,final logit,final credit added to final logit,0.7506395578384399
translation,260,101,hyperparameters,final credit,by multiplying,scalar value c,final credit by multiplying scalar value c,0.6726414561271667
translation,260,101,hyperparameters,hyperparameters,modulate,final credit,hyperparameters modulate final credit,0.6191834211349487
translation,260,7,model,combined visual and textual question answering ( vtqa ) model,takes as input,paragraph caption,combined visual and textual question answering ( vtqa ) model takes as input paragraph caption,0.6749347448348999
translation,260,7,model,combined visual and textual question answering ( vtqa ) model,takes as input,answers,combined visual and textual question answering ( vtqa ) model takes as input answers,0.6959171295166016
translation,260,7,model,paragraph caption,as well as,corresponding image,paragraph caption as well as corresponding image,0.6120363473892212
translation,260,7,model,given question,based on,both inputs,given question based on both inputs,0.6847198009490967
translation,260,7,model,answers,has,given question,answers has given question,0.6240186095237732
translation,260,7,model,model,propose,combined visual and textual question answering ( vtqa ) model,model propose combined visual and textual question answering ( vtqa ) model,0.6219754815101624
translation,260,8,model,inputs,fused to extract,related information,inputs fused to extract related information,0.7466427087783813
translation,260,8,model,related information,by,cross-attention,related information by cross-attention,0.5151079893112183
translation,260,8,model,fused again,in the form of,consensus,fused again in the form of consensus,0.7241209745407104
translation,260,8,model,expected answers,given,extra score,expected answers given extra score,0.6577005982398987
translation,260,8,model,extra score,to enhance,chance of selection,extra score to enhance chance of selection,0.7106374502182007
translation,260,8,model,cross-attention,has,early fusion,cross-attention has early fusion,0.5693433880805969
translation,260,8,model,consensus,has,late fusion ),consensus has late fusion ),0.594545304775238
translation,260,8,model,chance of selection,has,later fusion,chance of selection has later fusion,0.5620195865631104
translation,260,26,model,visual and textual features,combined via,crossattention,visual and textual features combined via crossattention,0.7626463174819946
translation,260,26,model,crossattention,to extract,related information,crossattention to extract related information,0.7228986024856567
translation,260,26,model,early fusion,has,visual and textual features,early fusion has visual and textual features,0.5626998543739319
translation,260,26,model,model,With,early fusion,model With early fusion,0.6537460088729858
translation,260,10,results,joint model,trained on,visual genome dataset,joint model trained on visual genome dataset,0.7383838295936584
translation,260,10,results,vqa performance,over,strong baseline model,vqa performance over strong baseline model,0.6065743565559387
translation,260,10,results,joint model,has,significantly improves,joint model has significantly improves,0.6109838485717773
translation,260,10,results,significantly improves,has,vqa performance,significantly improves has vqa performance,0.5344516038894653
translation,260,10,results,results,has,joint model,results has joint model,0.5378851294517517
translation,261,171,ablation-analysis,weights,of,scqa and bm 25 scores,weights of scqa and bm 25 scores,0.6074261665344238
translation,261,171,ablation-analysis,map,has,changes significantly,map has changes significantly,0.5830963253974915
translation,261,171,ablation-analysis,ablation analysis,by varying,weights,ablation analysis by varying weights,0.7508115172386169
translation,261,179,ablation-analysis,deep learning based solution,with,parameter sharing,deep learning based solution with parameter sharing,0.6403273940086365
translation,261,179,ablation-analysis,parameter sharing,is,more helpful,parameter sharing is more helpful,0.5501425266265869
translation,261,179,ablation-analysis,more helpful,than,parameter sharing,more helpful than parameter sharing,0.5477668046951294
translation,261,179,ablation-analysis,more helpful,without,parameter sharing,more helpful without parameter sharing,0.7190727591514587
translation,261,179,ablation-analysis,ablation analysis,observed that,deep learning based solution,ablation analysis observed that deep learning based solution,0.6262244582176208
translation,261,9,baselines,scqa,consist of,twin convolutional neural networks,scqa consist of twin convolutional neural networks,0.6065358519554138
translation,261,9,baselines,scqa,consist of,contrastive loss function,scqa consist of contrastive loss function,0.65301114320755
translation,261,9,baselines,twin convolutional neural networks,with,shared parameters,twin convolutional neural networks with shared parameters,0.6071350574493408
translation,261,9,baselines,baselines,has,scqa,baselines has scqa,0.588856041431427
translation,261,10,baselines,scqa,learns,similarity metric,scqa learns similarity metric,0.6288517117500305
translation,261,10,baselines,similarity metric,for,question -question pairs,similarity metric for question -question pairs,0.6193829774856567
translation,261,10,baselines,similarity metric,by leveraging,question - answer pairs,similarity metric by leveraging question - answer pairs,0.7067996263504028
translation,261,10,baselines,question - answer pairs,available in,cqa forum archives,question - answer pairs available in cqa forum archives,0.6847355365753174
translation,261,10,baselines,baselines,has,scqa,baselines has scqa,0.588856041431427
translation,261,30,baselines,baselines,has,topic models,baselines has topic models,0.5404506921768188
translation,261,197,baselines,scqa,employs,twin convolutional neural networks,scqa employs twin convolutional neural networks,0.5085093379020691
translation,261,197,baselines,twin convolutional neural networks,with,shared parameters,twin convolutional neural networks with shared parameters,0.6071350574493408
translation,261,197,baselines,shared parameters,to learn,semantic similarity,shared parameters to learn semantic similarity,0.6134999990463257
translation,261,197,baselines,semantic similarity,be-tween,question and answer pairs,semantic similarity be-tween question and answer pairs,0.679979145526886
translation,261,197,baselines,baselines,has,scqa,baselines has scqa,0.588856041431427
translation,261,213,experimental-setup,giza ++ 12 tool,with,question and best answer pair,giza ++ 12 tool with question and best answer pair,0.6670089364051819
translation,261,213,experimental-setup,question and best answer pair,as,parallel corpus,question and best answer pair as parallel corpus,0.5220847129821777
translation,261,213,experimental-setup,experimental setup,trained using,giza ++ 12 tool,experimental setup trained using giza ++ 12 tool,0.70719975233078
translation,261,237,experimental-setup,batch size,has,100,batch size has 100,0.6466267704963684
translation,261,237,experimental-setup,batch size,has,depth of cnn,batch size has depth of cnn,0.544312596321106
translation,261,237,experimental-setup,100,has,depth of cnn,100 has depth of cnn,0.5778652429580688
translation,261,237,experimental-setup,depth of cnn,has,3,depth of cnn has 3,0.5989834666252136
translation,261,237,experimental-setup,depth of cnn,has,learning rate,depth of cnn has learning rate,0.5433833599090576
translation,261,237,experimental-setup,3,has,learning rate,3 has learning rate,0.5544971823692322
translation,261,237,experimental-setup,learning rate,has,0.01,learning rate has 0.01,0.5422797799110413
translation,261,237,experimental-setup,0.01,has,momentum,0.01 has momentum,0.5673873424530029
translation,261,237,experimental-setup,momentum,has,0.05,momentum has 0.05,0.5595241785049438
translation,261,237,experimental-setup,maxpooling,has,100,maxpooling has 100,0.6237436532974243
translation,261,237,experimental-setup,100,has,length of semantic vector,100 has length of semantic vector,0.5496495366096497
translation,261,237,experimental-setup,length of semantic vector,has,128,length of semantic vector has 128,0.5746107697486877
translation,261,237,experimental-setup,experimental setup,Kernel width of Convolution,10,experimental setup Kernel width of Convolution 10,0.6528246998786926
translation,261,237,experimental-setup,experimental setup,Kernel width of Convolution,length of semantic vector,experimental setup Kernel width of Convolution length of semantic vector,0.8043820858001709
translation,261,237,experimental-setup,experimental setup,Kernel width of,maxpooling,experimental setup Kernel width of maxpooling,0.7518224120140076
translation,261,237,experimental-setup,experimental setup,has,hyperparameter,experimental setup has hyperparameter,0.4927394390106201
translation,261,191,experiments,q3,compare,performance,q3 compare performance,0.6977412700653076
translation,261,191,experiments,performance,of,t-scqa,performance of t-scqa,0.587977945804596
translation,261,191,experiments,t-scqa,with,supervised topic model,t-scqa with supervised topic model,0.5846036076545715
translation,261,214,experiments,topic based q-a topic model and q-a topic model ( s ),implemented,models qatm - pr ( question - answer topic model ),topic based q-a topic model and q-a topic model ( s ) implemented models qatm - pr ( question - answer topic model ),0.6481817364692688
translation,261,214,experiments,topic based q-a topic model and q-a topic model ( s ),implemented,t blm sqat m ?v,topic based q-a topic model and q-a topic model ( s ) implemented t blm sqat m ?v,0.6574457883834839
translation,261,214,experiments,supervised question - answer topic model,with,user votes as supervision,supervised question - answer topic model with user votes as supervision,0.6212157607078552
translation,261,214,experiments,t blm sqat m ?v,has,supervised question - answer topic model,t blm sqat m ?v has supervised question - answer topic model,0.580872118473053
translation,261,8,model,novel approach,to find,semantic similarity,novel approach to find semantic similarity,0.6172416806221008
translation,261,8,model,network,for,cqa ( scqa ),network for cqa ( scqa ),0.6668674945831299
translation,261,8,model,semantic similarity,between,current and the archived questions,semantic similarity between current and the archived questions,0.649682343006134
translation,261,8,model,model,propose,novel approach,model propose novel approach,0.7168048620223999
translation,261,41,model,novel unified model,called,siamese convolutional neural network,novel unified model called siamese convolutional neural network,0.6120700240135193
translation,261,41,model,novel unified model,for,cqa,novel unified model for cqa,0.6497714519500732
translation,261,41,model,siamese convolutional neural network,for,cqa,siamese convolutional neural network for cqa,0.6336299180984497
translation,261,41,model,model,propose,novel unified model,model propose novel unified model,0.5894982218742371
translation,261,42,model,scqa architecture,contain,deep convolutional neural networks,scqa architecture contain deep convolutional neural networks,0.5311462879180908
translation,261,42,model,deep convolutional neural networks,as,twin networks,deep convolutional neural networks as twin networks,0.516010046005249
translation,261,42,model,twin networks,with,contrastive energy function,twin networks with contrastive energy function,0.5827890634536743
translation,261,42,model,contrastive energy function,at,top,contrastive energy function at top,0.6009471416473389
translation,261,42,model,model,has,scqa architecture,model has scqa architecture,0.5581167936325073
translation,261,168,results,comparison,of,results,comparison of results,0.6241233944892883
translation,261,168,results,results,between,t-dsqa,results between t-dsqa,0.5639368295669556
translation,261,168,results,results,between,t-scqa,results between t-scqa,0.5624810457229614
translation,261,168,results,similar question retrieval,in,cqa forums,similar question retrieval in cqa forums,0.5268508195877075
translation,261,168,results,parameter sharing,has,definitely,parameter sharing has definitely,0.5906928777694702
translation,261,168,results,results,clear,comparison,results clear comparison,0.6988279819488525
translation,261,168,results,results,from,comparison,results from comparison,0.5769689679145813
translation,261,168,results,results,between,t-dsqa,results between t-dsqa,0.5639368295669556
translation,261,168,results,results,between,t-scqa,results between t-scqa,0.5624810457229614
translation,261,169,results,t-scqa,has,outperforms,t-scqa has outperforms,0.6260015368461609
translation,261,169,results,outperforms,has,all the previous approaches,outperforms has all the previous approaches,0.6048992276191711
translation,261,169,results,all the previous approaches,has,significantly,all the previous approaches has significantly,0.5730574131011963
translation,261,169,results,results,has,t-scqa,results has t-scqa,0.5435330867767334
translation,261,178,results,translation,has,models,translation has models,0.5903944373130798
translation,261,178,results,models,has,outperform,models has outperform,0.6452599167823792
translation,261,178,results,outperform,has,baseline methods,outperform has baseline methods,0.5856127142906189
translation,261,178,results,topic based approaches,has,outperform,topic based approaches has outperform,0.6059800982475281
translation,261,178,results,outperform,has,translational methods,outperform has translational methods,0.5894252061843872
translation,261,178,results,results,says,translation,results says translation,0.6246955990791321
translation,261,178,results,results,says,topic based approaches,results says topic based approaches,0.6490903496742249
translation,261,198,results,bm 25 scores,into,model t-scqa,bm 25 scores into model t-scqa,0.5911526083946228
translation,261,198,results,bm 25 scores,results in,improved matching performance,bm 25 scores results in improved matching performance,0.7112561464309692
translation,261,198,results,improved matching performance,for,textual and semantic matching,improved matching performance for textual and semantic matching,0.5820503234863281
translation,261,198,results,results,Interpolating,bm 25 scores,results Interpolating bm 25 scores,0.6139969229698181
translation,261,212,results,translation based methods,has,outperforms,translation based methods has outperforms,0.6172143220901489
translation,261,212,results,outperforms,has,baseline,outperforms has baseline,0.6131853461265564
translation,261,212,results,baseline,has,significantly,baseline has significantly,0.5920042991638184
translation,261,257,results,queries q1 -4 t-scqa,show,better performance,queries q1 -4 t-scqa show better performance,0.6520870923995972
translation,261,257,results,better performance,than,previous models,better performance than previous models,0.5755504369735718
translation,261,257,results,results,For,queries q1 -4 t-scqa,results For queries q1 -4 t-scqa,0.6447879076004028
translation,262,205,ablation-analysis,size,of,synthetic data,size of synthetic data,0.5901308059692383
translation,262,205,ablation-analysis,synthetic data,from,h1 to h10,synthetic data from h1 to h10,0.6136703491210938
translation,262,205,ablation-analysis,saturates,around,h2 - h4,saturates around h2 - h4,0.7610009908676147
translation,262,205,ablation-analysis,size,has,performance,size has performance,0.6035383939743042
translation,262,205,ablation-analysis,synthetic data,has,performance,synthetic data has performance,0.5760239958763123
translation,262,205,ablation-analysis,h1 to h10,has,performance,h1 to h10 has performance,0.5855609774589539
translation,262,205,ablation-analysis,performance,has,saturates,performance has saturates,0.5915381908416748
translation,262,205,ablation-analysis,ablation analysis,increasing,size,ablation analysis increasing size,0.7123610377311707
translation,262,44,baselines,two empirically effective strategies,one is,data filter,two empirically effective strategies one is data filter,0.7318183183670044
translation,262,44,baselines,data filter,based on,qap,data filter based on qap,0.6941848397254944
translation,262,44,baselines,data filter,to filter out,examples,data filter to filter out examples,0.7645934224128723
translation,262,44,baselines,examples,that have,low probabilities,examples that have low probabilities,0.6549980044364929
translation,262,44,baselines,low probabilities,to be,correctly answered,low probabilities to be correctly answered,0.5912401080131531
translation,262,44,baselines,mixing mini- batch training,has,strategy,mixing mini- batch training has strategy,0.5787756443023682
translation,262,44,baselines,baselines,introduce,two empirically effective strategies,baselines introduce two empirically effective strategies,0.6191258430480957
translation,262,165,experiments,semisupervised qa,use,"squadv1.1 ( rajpurkar et al. , 2016 )","semisupervised qa use squadv1.1 ( rajpurkar et al. , 2016 )",0.5301206707954407
translation,262,165,experiments,"squadv1.1 ( rajpurkar et al. , 2016 )",as,base qa task,"squadv1.1 ( rajpurkar et al. , 2016 ) as base qa task",0.46864351630210876
translation,262,165,experiments,original development set,in,half,original development set in half,0.54167240858078
translation,262,165,experiments,half,as,our development and test set,half as our development and test set,0.5320913791656494
translation,262,6,model,two semantics - enhanced rewards,obtained from,downstream question paraphrasing and question answering tasks,two semantics - enhanced rewards obtained from downstream question paraphrasing and question answering tasks,0.5140827298164368
translation,262,6,model,two semantics - enhanced rewards,to regularize,qg model,two semantics - enhanced rewards to regularize qg model,0.6835803985595703
translation,262,6,model,qg model,to generate,semantically valid questions,qg model to generate semantically valid questions,0.6871684789657593
translation,262,6,model,model,propose,two semantics - enhanced rewards,model propose two semantics - enhanced rewards,0.7017061114311218
translation,262,7,model,qa - based evaluation method,measures,qg model 's ability,qa - based evaluation method measures qg model 's ability,0.5289379358291626
translation,262,7,model,qg model 's ability,to mimic,human annotators,qg model 's ability to mimic human annotators,0.6112532019615173
translation,262,7,model,model,propose,qa - based evaluation method,model propose qa - based evaluation method,0.6073270440101624
translation,262,67,model,data filter,to remove,poorly generated examples,data filter to remove poorly generated examples,0.688525378704071
translation,262,67,model,mixing mini-batch training strategy,to more effectively use,synthetic data,mixing mini-batch training strategy to more effectively use synthetic data,0.7051956057548523
translation,262,67,model,model,introduce,data filter,model introduce data filter,0.6955313086509705
translation,262,67,model,model,introduce,mixing mini-batch training strategy,model introduce mixing mini-batch training strategy,0.6333394050598145
translation,262,177,results,our baseline qg model,obtains,non-trivial improvement,our baseline qg model obtains non-trivial improvement,0.5494136214256287
translation,262,177,results,our baseline qg model,tie,output projection matrix,our baseline qg model tie output projection matrix,0.48280471563339233
translation,262,177,results,non-trivial improvement,over,previous best qg system,non-trivial improvement over previous best qg system,0.6688830852508545
translation,262,177,results,output projection matrix,with,non-trainable word embedding matrix,output projection matrix with non-trainable word embedding matrix,0.5941879153251648
translation,262,179,results,metrics,are,significantly 4 ( p < 0.001 ) improved,metrics are significantly 4 ( p < 0.001 ) improved,0.5932294726371765
translation,262,179,results,significantly 4 ( p < 0.001 ) improved,except,qpp,significantly 4 ( p < 0.001 ) improved except qpp,0.6953819394111633
translation,262,179,results,results,has,metrics,results has metrics,0.5256302952766418
translation,262,184,results,significantly ( p < 0.001 ) improved,over,our baseline,significantly ( p < 0.001 ) improved over our baseline,0.6591794490814209
translation,262,184,results,all metrics,except,rouge -l,all metrics except rouge -l,0.6462424397468567
translation,262,184,results,significantly ( p < 0.05 ) improved,over,models,significantly ( p < 0.05 ) improved over models,0.6677484512329102
translation,262,184,results,models,using,traditional metrics,models using traditional metrics,0.653781533241272
translation,262,184,results,traditional metrics,as,rewards,traditional metrics as rewards,0.533089280128479
translation,262,184,results,qap and qqp,has,all metrics,qap and qqp has all metrics,0.5701138377189636
translation,262,184,results,results,using,qap and qqp,results using qap and qqp,0.6501932740211487
translation,262,185,results,our model,performs,consistently best,our model performs consistently best,0.6427621245384216
translation,262,185,results,consistently best,on,bleu4 / meteor / rouge -l,consistently best on bleu4 / meteor / rouge -l,0.5060133337974548
translation,262,185,results,consistently best,on,q-bleu1,consistently best on q-bleu1,0.523028552532196
translation,262,185,results,multi-reward optimization,has,our model,multi-reward optimization has our model,0.5521973371505737
translation,262,185,results,results,After applying,multi-reward optimization,results After applying multi-reward optimization,0.7042916417121887
translation,262,192,results,baseline and our best qg model,has,significantly improve,baseline and our best qg model has significantly improve,0.5607918500900269
translation,262,192,results,significantly improve,has,synthetic data 's qa performance,significantly improve has synthetic data 's qa performance,0.5462700724601746
translation,262,192,results,results,both,baseline and our best qg model,results both baseline and our best qg model,0.6285089254379272
translation,262,193,results,minor improvement,over,our baseline model,minor improvement over our baseline model,0.6520880460739136
translation,262,193,results,results,has,best qg model,results has best qg model,0.5847119688987732
translation,262,194,results,results,has,semi-supervised question answering,results has semi-supervised question answering,0.5348341464996338
translation,262,195,results,synthetic data only,adding,data filter,synthetic data only adding data filter,0.7511609792709351
translation,262,195,results,significantly improve,has,qa performance,significantly improve has qa performance,0.567300021648407
translation,262,195,results,results,when using,synthetic data only,results when using synthetic data only,0.7103464007377625
translation,262,196,results,improvement,is,relatively smaller,improvement is relatively smaller,0.5781981348991394
translation,262,196,results,relatively smaller,due to,regularization,relatively smaller due to regularization,0.7214248180389404
translation,262,196,results,regularization,from,ground - truth data,regularization from ground - truth data,0.5468923449516296
translation,262,196,results,semi-supervised qa,has,improvement,semi-supervised qa has improvement,0.554036557674408
translation,262,196,results,results,In terms of,semi-supervised qa,results In terms of semi-supervised qa,0.632736325263977
translation,262,200,results,beam search,with,beam size 10 ( + beam10 ),beam search with beam size 10 ( + beam10 ),0.6681736707687378
translation,262,200,results,beam size 10 ( + beam10 ),improves,bidaf - qa baseline,beam size 10 ( + beam10 ) improves bidaf - qa baseline,0.7134888172149658
translation,262,200,results,bidaf - qa baseline,by,1.51/1.13 absolute points,bidaf - qa baseline by 1.51/1.13 absolute points,0.5613555312156677
translation,262,200,results,1.51/1.13 absolute points,on,testing set,1.51/1.13 absolute points on testing set,0.5286891460418701
translation,262,200,results,results,seen that,beam search,results seen that beam search,0.7041889429092407
translation,262,200,results,results,using,beam search,results using beam search,0.6458462476730347
translation,262,201,results,best performance ( + h8 ),improves,bidaf - qa baseline,best performance ( + h8 ) improves bidaf - qa baseline,0.7133901119232178
translation,262,201,results,bidaf - qa baseline,by,1.69/1.27 absolute points,bidaf - qa baseline by 1.69/1.27 absolute points,0.5592736601829529
translation,262,201,results,1.69/1.27 absolute points,on,testing set,1.69/1.27 absolute points on testing set,0.5405718088150024
translation,262,201,results,new articles,has,best performance ( + h8 ),new articles has best performance ( + h8 ),0.5827745199203491
translation,262,206,results,converges,even before,all examples,converges even before all examples,0.6824691891670227
translation,262,206,results,big synthetic data,has,model,big synthetic data has model,0.4920438826084137
translation,262,206,results,h10,has,model,h10 has model,0.5797130465507507
translation,262,206,results,model,has,converges,model has converges,0.6297324895858765
translation,262,206,results,results,when using,big synthetic data,results when using big synthetic data,0.6899943351745605
translation,262,211,results,our methods,achieve,larger improvements,our methods achieve larger improvements,0.5738101601600647
translation,262,211,results,larger improvements,over,stronger baseline,larger improvements over stronger baseline,0.6332071423530579
translation,262,211,results,stronger baseline,than,their method,stronger baseline than their method,0.5707266926765442
translation,262,211,results,no or less new data injection,has,our methods,no or less new data injection has our methods,0.5728833675384521
translation,262,212,results,qg and qa results,with,bert,qg and qa results with bert,0.6861411929130554
translation,262,212,results,results,has,qg and qa results,results has qg and qa results,0.5794134736061096
translation,263,49,ablation-analysis,target distributions,at,every decoding time,target distributions at every decoding time,0.5506030321121216
translation,263,49,ablation-analysis,common topic,decoding,repetitive topic words,common topic decoding repetitive topic words,0.7045861482620239
translation,263,49,ablation-analysis,ablation analysis,naively averaging,target distributions,ablation analysis naively averaging target distributions,0.7145737409591675
translation,263,46,baselines,msqg max,takes,maximum probability,msqg max takes maximum probability,0.6413109302520752
translation,263,46,baselines,msqg max,normalizes them into,single distribution,msqg max normalizes them into single distribution,0.6646451950073242
translation,263,46,baselines,maximum probability,of,each word,maximum probability of each word,0.6047739386558533
translation,263,46,baselines,maximum probability,normalizes them into,single distribution,maximum probability normalizes them into single distribution,0.6405325531959534
translation,263,46,baselines,each word,across,n distributions,each word across n distributions,0.711708128452301
translation,263,46,baselines,baselines,has,msqg max,baselines has msqg max,0.5455074310302734
translation,263,59,baselines,training method,uses,"standard lstmbased ( hochreiter and schmidhuber , 1997 ) s2s","training method uses standard lstmbased ( hochreiter and schmidhuber , 1997 ) s2s",0.578774631023407
translation,263,59,baselines,"standard lstmbased ( hochreiter and schmidhuber , 1997 ) s2s",with,bi-linear attention,"standard lstmbased ( hochreiter and schmidhuber , 1997 ) s2s with bi-linear attention",0.5742972493171692
translation,263,59,baselines,baselines,has,training method,baselines has training method,0.5817532539367676
translation,263,62,baselines,s2s,is,bi-directional,s2s is bi-directional,0.6171640157699585
translation,263,62,baselines,bi-directional,with,256 - dim bi-linear attention,bi-directional with 256 - dim bi-linear attention,0.6272541880607605
translation,263,62,baselines,256 - dim bi-linear attention,in,each direction,256 - dim bi-linear attention in each direction,0.509845495223999
translation,263,62,baselines,each direction,with,"relu ( nair and hinton , 2010 )","each direction with relu ( nair and hinton , 2010 )",0.6115983724594116
translation,263,62,baselines,baselines,has,s2s,baselines has s2s,0.5941101908683777
translation,263,70,baselines,multi-encoder single- decoder ( mesd ) baseline,where,documents,multi-encoder single- decoder ( mesd ) baseline where documents,0.6337940692901611
translation,263,70,baselines,documents,encoded individually into,{ v i } n i=1,documents encoded individually into { v i } n i=1,0.6408063173294067
translation,263,18,hyperparameters,training time,train,standard sequenceto-sequence model,training time train standard sequenceto-sequence model,0.7085134983062744
translation,263,18,hyperparameters,standard sequenceto-sequence model,with,"large number of ( single document , question ) pairs","standard sequenceto-sequence model with large number of ( single document , question ) pairs",0.584196150302887
translation,263,18,hyperparameters,"large number of ( single document , question ) pairs",to generate,relevant question,"large number of ( single document , question ) pairs to generate relevant question",0.6367847919464111
translation,263,18,hyperparameters,relevant question,given,single document,relevant question given single document,0.7203541994094849
translation,263,18,hyperparameters,hyperparameters,At,training time,hyperparameters At training time,0.49539634585380554
translation,263,60,hyperparameters,encoder,concatenation of,100 - dim glove,encoder concatenation of 100 - dim glove,0.7035742402076721
translation,263,60,hyperparameters,encoder,concatenation of,100 - dim predicate location vector,encoder concatenation of 100 - dim predicate location vector,0.7297837138175964
translation,263,60,hyperparameters,encoder,concatenation of,"1024 - dim elmo ( peters et al. , 2018 ) vector","encoder concatenation of 1024 - dim elmo ( peters et al. , 2018 ) vector",0.6970443725585938
translation,263,60,hyperparameters,hyperparameters,input to,encoder,hyperparameters input to encoder,0.7708861827850342
translation,263,61,hyperparameters,targets,embedded into,100 - dim vectors,targets embedded into 100 - dim vectors,0.7452266216278076
translation,263,61,hyperparameters,hyperparameters,has,targets,hyperparameters has targets,0.5480541586875916
translation,263,63,hyperparameters,encoder,use,"adam ( kingma and ba , 2014 )","encoder use adam ( kingma and ba , 2014 )",0.5787331461906433
translation,263,63,hyperparameters,"adam ( kingma and ba , 2014 )",with,learning rate,"adam ( kingma and ba , 2014 ) with learning rate",0.5989180207252502
translation,263,63,hyperparameters,learning rate,of,2 ? 10 ?5,learning rate of 2 ? 10 ?5,0.6528677344322205
translation,263,63,hyperparameters,encoder,has,two layers,encoder has two layers,0.6066858768463135
translation,263,63,hyperparameters,hyperparameters,has,encoder,hyperparameters has encoder,0.5368890762329102
translation,263,68,hyperparameters,beam size,set to,5,beam size set to 5,0.7865644693374634
translation,263,68,hyperparameters,hyperparameters,has,beam size,hyperparameters has beam size,0.516274631023407
translation,263,8,model,rnn - based single encoder-decoder generator,from,"( single document , question ) pairs","rnn - based single encoder-decoder generator from ( single document , question ) pairs",0.572405219078064
translation,263,8,model,model,train,rnn - based single encoder-decoder generator,model train rnn - based single encoder-decoder generator,0.7138781547546387
translation,263,20,model,n input documents separately,using,trained encoder,n input documents separately using trained encoder,0.5744546055793762
translation,263,20,model,model,encode,n input documents separately,model encode n input documents separately,0.714566171169281
translation,263,24,results,our model,has,significantly outperforms,our model has significantly outperforms,0.6134821176528931
translation,263,24,results,significantly outperforms,has,multiple baselines,significantly outperforms has multiple baselines,0.6000661849975586
translation,263,24,results,results,has,our model,results has our model,0.5871725678443909
translation,263,96,results,our proposed msqg models,are,more effective,our proposed msqg models are more effective,0.6017751097679138
translation,263,96,results,more effective,in terms of,retrieving,more effective in terms of retrieving,0.7655057311058044
translation,263,96,results,retrieving,has,source 10 - passage sets,retrieving has source 10 - passage sets,0.5407586097717285
translation,263,96,results,results,Notice,our proposed msqg models,results Notice our proposed msqg models,0.7205913662910461
translation,263,97,results,baselines,in,all metrics,baselines in all metrics,0.4532690942287445
translation,263,97,results,msqg sharedh,has,rmrep,msqg sharedh has rmrep,0.6368992328643799
translation,263,97,results,rmrep,has,outperforms,rmrep has outperforms,0.6568160057067871
translation,263,97,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
translation,263,97,results,results,has,msqg sharedh,results has msqg sharedh,0.541820228099823
translation,263,97,results,results,has,rmrep,results has rmrep,0.5388773083686829
translation,263,105,results,strong baseline,by,large margin,strong baseline by large margin,0.5881295204162598
translation,263,105,results,comparison tasks,has,proposed model,comparison tasks has proposed model,0.5805578231811523
translation,263,105,results,proposed model,has,significantly outperforms,proposed model has significantly outperforms,0.6167242527008057
translation,263,105,results,significantly outperforms,has,strong baseline,significantly outperforms has strong baseline,0.6076291799545288
translation,263,105,results,results,In,comparison tasks,results In comparison tasks,0.5385218858718872
translation,264,55,baselines,multico,adapts,query - based contextualized sentence selection approach,multico adapts query - based contextualized sentence selection approach,0.6778310537338257
translation,264,55,baselines,multico,combined with,sparse self-attention mechanism,multico combined with sparse self-attention mechanism,0.72728031873703
translation,264,55,baselines,query - based contextualized sentence selection approach,combined with,sparse self-attention mechanism,query - based contextualized sentence selection approach combined with sparse self-attention mechanism,0.6830955147743225
translation,264,55,baselines,baselines,has,multico,baselines has multico,0.6428300142288208
translation,264,167,baselines,"tanda ( garg et al. , 2020 )",utilizes,bert - based architecture,"tanda ( garg et al. , 2020 ) utilizes bert - based architecture",0.6256955862045288
translation,264,167,baselines,to answer questions,using,"pairwise ( query , sentence ) classification approach","to answer questions using pairwise ( query , sentence ) classification approach",0.6896253824234009
translation,264,167,baselines,bert - based architecture,has,to answer questions,bert - based architecture has to answer questions,0.6039429306983948
translation,264,179,baselines,drqa reader,uses,rnn - based architecture,drqa reader uses rnn - based architecture,0.6073350310325623
translation,264,179,baselines,rnn - based architecture,along with,context- toquery attention,rnn - based architecture along with context- toquery attention,0.5888327360153198
translation,264,179,baselines,rnn - based architecture,to compute,answer,rnn - based architecture to compute answer,0.7699450254440308
translation,264,181,baselines,bidirectional attention ( query-to- context and context- to- query ),for,answer span prediction,bidirectional attention ( query-to- context and context- to- query ) for answer span prediction,0.5813249945640564
translation,264,182,baselines,qa versions,of,bert,qa versions of bert,0.685849666595459
translation,264,182,baselines,qa versions,of,"spanbert ( joshi et al. , 2020 )","qa versions of spanbert ( joshi et al. , 2020 )",0.6038185358047485
translation,264,182,baselines,qa versions,of,xl - net,qa versions of xl - net,0.6416275501251221
translation,264,182,baselines,baselines,use,qa versions,baselines use qa versions,0.6895906329154968
translation,264,153,experimental-setup,"adam optimizer ( kingma and ba , 2015 )",with,learning rate,"adam optimizer ( kingma and ba , 2015 ) with learning rate",0.5968794822692871
translation,264,153,experimental-setup,learning rate,of,2 ? 10 ?5,learning rate of 2 ? 10 ?5,0.6528677344322205
translation,264,153,experimental-setup,experimental setup,trained using,"adam optimizer ( kingma and ba , 2015 )","experimental setup trained using adam optimizer ( kingma and ba , 2015 )",0.746996283531189
translation,264,154,experimental-setup,maximum length,for,query and context sentences,maximum length for query and context sentences,0.5952162742614746
translation,264,154,experimental-setup,query and context sentences,set to,64 and 32 tokens,query and context sentences set to 64 and 32 tokens,0.655289888381958
translation,264,154,experimental-setup,maximum number of sentences,in,one segment,maximum number of sentences in one segment,0.5126084685325623
translation,264,154,experimental-setup,one segment,set to,13,one segment set to 13,0.7365384101867676
translation,264,154,experimental-setup,experimental setup,has,maximum length,experimental setup has maximum length,0.4922432005405426
translation,264,154,experimental-setup,experimental setup,has,maximum number of sentences,experimental setup has maximum number of sentences,0.5295649170875549
translation,264,157,experimental-setup,pre-trained version,of,"xlnet ( 24 layers , 340m parameters )","pre-trained version of xlnet ( 24 layers , 340m parameters )",0.5139099955558777
translation,264,157,experimental-setup,experimental setup,used,pre-trained version,experimental setup used pre-trained version,0.6147629618644714
translation,264,158,experimental-setup,servers,with,single tesla k80 gpus,servers with single tesla k80 gpus,0.59961998462677
translation,264,6,experiments,multiple answer spans healthcare question answering dataset,from,consumer health domain,multiple answer spans healthcare question answering dataset from consumer health domain,0.5233623385429382
translation,264,6,experiments,mash - qa,has,multiple answer spans healthcare question answering dataset,mash - qa has multiple answer spans healthcare question answering dataset,0.5674909949302673
translation,264,40,experiments,mash - qa,based on,questions and knowledge articles,mash - qa based on questions and knowledge articles,0.6847088932991028
translation,264,40,experiments,questions and knowledge articles,from,consumer health domain,questions and knowledge articles from consumer health domain,0.5394754409790039
translation,264,7,model,neural architecture,able to capture,relevance,neural architecture able to capture relevance,0.7345691919326782
translation,264,7,model,relevance,among,multiple answer spans,relevance among multiple answer spans,0.5452967882156372
translation,264,7,model,relevance,by using,query - based contextualized sentence selection approach,relevance by using query - based contextualized sentence selection approach,0.64835524559021
translation,264,7,model,query - based contextualized sentence selection approach,for forming,answer,query - based contextualized sentence selection approach for forming answer,0.7170644998550415
translation,264,7,model,answer,to,given question,answer to given question,0.6287168860435486
translation,264,7,model,multico,has,neural architecture,multico has neural architecture,0.5824267268180847
translation,264,7,model,model,propose,multico,model propose multico,0.6914223432540894
translation,264,50,model,multico,has,novel neural architecture,multico has novel neural architecture,0.5973209738731384
translation,264,50,model,model,propose,multico,model propose multico,0.6914223432540894
translation,264,51,model,xlnet,incorporates,transformer - xl units,xlnet incorporates transformer - xl units,0.7378267645835876
translation,264,51,model,transformer - xl units,to give,semantic representations,transformer - xl units to give semantic representations,0.6506950259208679
translation,264,51,model,semantic representations,that capture,long-range dependencies,semantic representations that capture long-range dependencies,0.6530057787895203
translation,264,51,model,long-range dependencies,existing in,long document context,long-range dependencies existing in long document context,0.6055192351341248
translation,264,51,model,model,utilizes,xlnet,model utilizes xlnet,0.6765906810760498
translation,264,187,results,span prediction task,on,single-span mash - qa,span prediction task on single-span mash - qa,0.5273585319519043
translation,264,187,results,results,for,span prediction task,results for span prediction task,0.523147463798523
translation,264,188,results,all the other baselines,by,wide margin,all the other baselines by wide margin,0.580040693283081
translation,264,188,results,multico,has,outperforms,multico has outperforms,0.6428431868553162
translation,264,188,results,outperforms,has,all the other baselines,outperforms has all the other baselines,0.5866766571998596
translation,265,114,ablation-analysis,degrades significantly,drops to,40.73 f1,degrades significantly drops to 40.73 f1,0.7655859589576721
translation,265,114,ablation-analysis,original model 's accuracy,has,degrades significantly,original model 's accuracy has degrades significantly,0.5982183814048767
translation,265,70,hyperparameters,single- paragraph bert model,trained in,distractor setting,single- paragraph bert model trained in distractor setting,0.7121766805648804
translation,265,70,hyperparameters,hyperparameters,use,single- paragraph bert model,hyperparameters use single- paragraph bert model,0.5897908210754395
translation,265,158,hyperparameters,"adam ( kingma and ba , 2015 )",with,learning rate 5 ? 10 ?5,"adam ( kingma and ba , 2015 ) with learning rate 5 ? 10 ?5",0.6578719615936279
translation,265,158,hyperparameters,hyperparameters,use,"adam ( kingma and ba , 2015 )","hyperparameters use adam ( kingma and ba , 2015 )",0.5931493639945984
translation,265,159,hyperparameters,maximum sequence length | s |,to,300,maximum sequence length | s | to 300,0.573369562625885
translation,265,159,hyperparameters,hyperparameters,lowercase,input,hyperparameters lowercase input,0.7558025121688843
translation,265,8,model,single- hop bert - based rc model,achieves,67 f1,single- hop bert - based rc model achieves 67 f1,0.6964635848999023
translation,265,8,model,comparable,to,state - of- theart multi-hop models,comparable to state - of- theart multi-hop models,0.5491098165512085
translation,265,8,model,67 f1,has,comparable,67 f1 has comparable,0.6386875510215759
translation,265,8,model,model,introduce,single- hop bert - based rc model,model introduce single- hop bert - based rc model,0.6191418170928955
translation,265,28,model,single - hop qa model,based on,"bert ( devlin et al. , 2018 )","single - hop qa model based on bert ( devlin et al. , 2018 )",0.6680194139480591
translation,265,28,model,single - hop qa model,achieves,performance competitive,single - hop qa model achieves performance competitive,0.7411537766456604
translation,265,28,model,model,design,single - hop qa model,model design single - hop qa model,0.6103670001029968
translation,265,64,results,single- paragraph bert model,achieves,67.08 f1,single- paragraph bert model achieves 67.08 f1,0.5679329037666321
translation,265,64,results,67.08 f1,comparable to,state- of- the- art,67.08 f1 comparable to state- of- the- art,0.6475902199745178
translation,265,64,results,results,has,single- paragraph bert model,results has single- paragraph bert model,0.47382622957229614
translation,265,73,results,single-paragraph bert,achieves,38.06 f1,single-paragraph bert achieves 38.06 f1,0.6183764934539795
translation,265,73,results,38.06 f1,in,open-domain setting,38.06 f1 in open-domain setting,0.5060003399848938
translation,265,73,results,results,has,single-paragraph bert,results has single-paragraph bert,0.5270103812217712
translation,265,115,results,model,trained on,adversarially selected distractors,model trained on adversarially selected distractors,0.7310513854026794
translation,265,115,results,original accuracy,increases to,58.42 f1,original accuracy increases to 58.42 f1,0.6562358736991882
translation,265,117,results,original accuracy,re-trained on,distractors,original accuracy re-trained on distractors,0.6537117958068848
translation,265,117,results,distractors,from,new distribution,distractors from new distribution,0.5662506222724915
translation,265,117,results,somewhat recover,has,original accuracy,somewhat recover has original accuracy,0.577803909778595
translation,265,117,results,results,has,model,results has model,0.5339115858078003
translation,265,123,results,struggles,in,open-domain setting,struggles in open-domain setting,0.5638418793678284
translation,265,123,results,our single - hop model,has,struggles,our single - hop model has struggles,0.5737571120262146
translation,265,123,results,results,has,our single - hop model,results has our single - hop model,0.5561375021934509
translation,267,24,ablation-analysis,our ablation studies,show that,our sub-questions,our ablation studies show that our sub-questions,0.5273609757423401
translation,267,24,ablation-analysis,our sub-questions,with,400 supervised examples,our sub-questions with 400 supervised examples,0.6179682612419128
translation,267,24,ablation-analysis,400 supervised examples,of,decompositions,400 supervised examples of decompositions,0.5838251709938049
translation,267,24,ablation-analysis,our answer - aware rescoring method,has,significantly improves,our answer - aware rescoring method has significantly improves,0.580568790435791
translation,267,24,ablation-analysis,significantly improves,has,performance,significantly improves has performance,0.5962982177734375
translation,267,24,ablation-analysis,ablation analysis,show that,our sub-questions,ablation analysis show that our sub-questions,0.5511587858200073
translation,267,24,ablation-analysis,ablation analysis,has,our ablation studies,ablation analysis has our ablation studies,0.5270883440971375
translation,267,5,model,compositional question,into,simpler sub-questions,compositional question into simpler sub-questions,0.6285014152526855
translation,267,5,model,simpler sub-questions,answered by,off - the-shelf single - hop rc models,simpler sub-questions answered by off - the-shelf single - hop rc models,0.6565443277359009
translation,267,7,model,new global rescoring approach,considers,each decomposition,new global rescoring approach considers each decomposition,0.646064043045044
translation,267,7,model,each decomposition,to select,best final answer,each decomposition to select best final answer,0.7135183811187744
translation,267,7,model,model,introduce,new global rescoring approach,model introduce new global rescoring approach,0.6374844312667847
translation,267,11,model,decomprc,learns to break,compositional multi-hop questions,decomprc learns to break compositional multi-hop questions,0.7462794780731201
translation,267,11,model,compositional multi-hop questions,into,"simpler , singlehop sub-questions","compositional multi-hop questions into simpler , singlehop sub-questions",0.5866101980209351
translation,267,11,model,"simpler , singlehop sub-questions",using,spans,"simpler , singlehop sub-questions using spans",0.70635986328125
translation,267,11,model,spans,from,original question,spans from original question,0.5748751163482666
translation,267,11,model,model,propose,decomprc,model propose decomprc,0.7159582376480103
translation,267,73,model,method,to create,subquestions,method to create subquestions,0.6746186017990112
translation,267,73,model,subquestions,using,span prediction,subquestions using span prediction,0.7142056822776794
translation,267,73,model,span prediction,over,question,span prediction over question,0.6483396887779236
translation,267,73,model,model,propose,method,model propose method,0.6280754208564758
translation,267,22,results,decomprc,providing,explainable evidence,decomprc providing explainable evidence,0.691390872001648
translation,267,22,results,outperforms,on,hot - potqa,outperforms on hot - potqa,0.5278060436248779
translation,267,22,results,other published methods,on,hot - potqa,other published methods on hot - potqa,0.5062229037284851
translation,267,22,results,explainable evidence,in the form of,sub-questions,explainable evidence in the form of sub-questions,0.7051790356636047
translation,267,22,results,decomprc,has,outperforms,decomprc has outperforms,0.6365375518798828
translation,267,22,results,outperforms,has,other published methods,outperforms has other published methods,0.5576883554458618
translation,267,148,results,results,of,decomprc,results of decomprc,0.6202285885810852
translation,267,149,results,all baselines,in,distractor and full wiki settings,all baselines in distractor and full wiki settings,0.49689146876335144
translation,267,149,results,previous published result,by,large margin,previous published result by large margin,0.5506190657615662
translation,267,149,results,decomprc,has,outperforms,decomprc has outperforms,0.6365375518798828
translation,267,149,results,outperforms,has,all baselines,outperforms has all baselines,0.5888515114784241
translation,267,149,results,outperforming,has,previous published result,outperforming has previous published result,0.6108282804489136
translation,267,149,results,results,observe,decomprc,results observe decomprc,0.624558687210083
translation,267,150,results,decomprc,not trained on,multi-hop qa pairs,decomprc not trained on multi-hop qa pairs,0.7169270515441895
translation,267,150,results,decomprc,shows,reasonable performance,decomprc shows reasonable performance,0.6702688932418823
translation,267,150,results,reasonable performance,across,all data splits,reasonable performance across all data splits,0.687594473361969
translation,267,150,results,results,has,interesting observation,results has interesting observation,0.5519416928291321
translation,267,151,results,bert,trained on,singlehop rc,bert trained on singlehop rc,0.6933252215385437
translation,267,151,results,singlehop rc,achieves,high f1 score,singlehop rc achieves high f1 score,0.6603690981864929
translation,267,151,results,results,observe,bert,results observe bert,0.4689315855503082
translation,267,154,results,outperforms,by,large margin,outperforms by large margin,0.6290680766105652
translation,267,154,results,bert,by,large margin,bert by large margin,0.6704505681991577
translation,267,154,results,large margin,in,single - hop non-solvable ( multi ) examples,large margin in single - hop non-solvable ( multi ) examples,0.5242247581481934
translation,267,154,results,decomprc,has,outperforms,decomprc has outperforms,0.6365375518798828
translation,267,154,results,outperforms,has,bert,outperforms has bert,0.7186265587806702
translation,267,154,results,results,observe,decomprc,results observe decomprc,0.624558687210083
translation,267,159,results,decomprc,achieves,best result,decomprc achieves best result,0.6870163679122925
translation,267,159,results,best result,out of,models,best result out of models,0.6613429188728333
translation,267,159,results,results,has,decomprc,results has decomprc,0.602719783782959
translation,267,167,results,more robust,to,change in distractors,more robust to change in distractors,0.5529233813285828
translation,267,167,results,both methods,has,degrade,both methods has degrade,0.5861204266548157
translation,267,167,results,results,has,performance,results has performance,0.5972660779953003
translation,267,168,results,decomprc - 1hop train,degrades,much less ( only 3.41 f1 ),decomprc - 1hop train degrades much less ( only 3.41 f1 ),0.6552903056144714
translation,267,168,results,much less ( only 3.41 f1 ),compared to,other approaches,much less ( only 3.41 f1 ) compared to other approaches,0.6782957315444946
translation,267,168,results,results,has,decomprc - 1hop train,results has decomprc - 1hop train,0.608181357383728
translation,268,131,baselines,second best system,is,hitsz - icrc,second best system is hitsz - icrc,0.6186996102333069
translation,268,131,baselines,hitsz - icrc,used,ensemble of classifiers,hitsz - icrc used ensemble of classifiers,0.6076065301895142
translation,268,131,baselines,baselines,has,second best system,baselines has second best system,0.5471576452255249
translation,268,10,experiments,37,has,contrastive,37 has contrastive,0.632865309715271
translation,268,39,experiments,semeval task,attracted,13 teams,semeval task attracted 13 teams,0.6504938006401062
translation,268,44,experiments,best accuracy,is,72 %,best accuracy is 72 %,0.5695372223854065
translation,268,44,experiments,extreme summarization task,has,best accuracy,extreme summarization task has best accuracy,0.5502410531044006
translation,268,127,experiments,18,has,contrastive,18 has contrastive,0.6403812170028687
translation,268,130,experiments,best system,ranks,first,best system ranks first,0.7857705950737
translation,268,130,experiments,best system,used,supervised featurerich approach,best system used supervised featurerich approach,0.5967649221420288
translation,268,130,experiments,jaist,ranks,first,jaist ranks first,0.7580372095108032
translation,268,130,experiments,jaist,used,supervised featurerich approach,jaist used supervised featurerich approach,0.5737599730491638
translation,268,130,experiments,first,in,official macro f 1 score ( 57.19 ),first in official macro f 1 score ( 57.19 ),0.4823615550994873
translation,268,130,experiments,first,in,accuracy ( 72.52 ),first in accuracy ( 72.52 ),0.47220808267593384
translation,268,130,experiments,supervised featurerich approach,includes,topic models and word vector representation,supervised featurerich approach includes topic models and word vector representation,0.5730503797531128
translation,268,130,experiments,supervised featurerich approach,includes,svm classifier,supervised featurerich approach includes svm classifier,0.6122179627418518
translation,268,130,experiments,supervised featurerich approach,with,svm classifier,supervised featurerich approach with svm classifier,0.6667481064796448
translation,268,130,experiments,topic models and word vector representation,with,svm classifier,topic models and word vector representation with svm classifier,0.612663209438324
translation,268,132,experiments,second,in terms of,macro f 1 ( 56.41 ),second in terms of macro f 1 ( 56.41 ),0.6472386121749878
translation,268,132,experiments,fifth,on,accuracy ( 68.67 ),fifth on accuracy ( 68.67 ),0.48381471633911133
translation,268,144,experiments,7,has,contrastive,7 has contrastive,0.6486235857009888
translation,268,179,experiments,top-performing system,in both,macro f 1 ( 63.7 ) and accuracy ( 72 ),top-performing system in both macro f 1 ( 63.7 ) and accuracy ( 72 ),0.607013463973999
translation,268,179,experiments,top-performing system,in both,vectorslu,top-performing system in both vectorslu,0.6256577372550964
translation,268,179,experiments,top-performing system,is,vectorslu,top-performing system is vectorslu,0.5752121210098267
translation,268,42,results,baselines,using,official f 1 - based score,baselines using official f 1 - based score,0.6583536267280579
translation,268,42,results,almost all submissions,has,managed to outperform,almost all submissions has managed to outperform,0.5840581655502319
translation,268,42,results,managed to outperform,has,baselines,managed to outperform has baselines,0.6420572400093079
translation,268,42,results,results,has,almost all submissions,results has almost all submissions,0.5077201128005981
translation,268,43,results,best system,detect,correct answer,best system detect correct answer,0.6450138688087463
translation,268,43,results,correct answer,with,accuracy,correct answer with accuracy,0.5952845215797424
translation,268,43,results,accuracy,of,73 %,accuracy of 73 %,0.6110997796058655
translation,268,43,results,accuracy,of,83 %,accuracy of 83 %,0.6133682727813721
translation,268,43,results,accuracy,about,73 %,accuracy about 73 %,0.6203616857528687
translation,268,43,results,accuracy,about,83 %,accuracy about 83 %,0.6237300634384155
translation,268,43,results,73 %,in,english task,73 % in english task,0.4868345558643341
translation,268,43,results,83 %,in,easier arabic task,83 % in easier arabic task,0.48268789052963257
translation,268,43,results,results,has,best system,results has best system,0.5931360125541687
translation,268,128,results,outperform,in terms of,macro f 1,outperform in terms of macro f 1,0.7284440994262695
translation,268,128,results,all submissions,has,outperform,all submissions has outperform,0.5971746444702148
translation,268,128,results,outperform,has,majority class baseline,outperform has majority class baseline,0.5917549729347229
translation,268,128,results,results,see that,all submissions,results see that all submissions,0.6223095655441284
translation,268,145,results,all teams,performed,well above,all teams performed well above,0.2700256407260895
translation,268,145,results,well above,predicts,irrelevant,well above predicts irrelevant,0.8198863863945007
translation,268,145,results,majority class baseline,predicts,irrelevant,majority class baseline predicts irrelevant,0.7945717573165894
translation,268,145,results,well above,has,majority class baseline,well above has majority class baseline,0.5857031941413879
translation,268,145,results,results,has,all teams,results has all teams,0.5129958987236023
translation,268,157,results,all submitted runs,identifying,irrelevant answers,all submitted runs identifying irrelevant answers,0.6119824051856995
translation,268,157,results,irrelevant answers,was,easiest,irrelevant answers was easiest,0.6147679686546326
translation,268,157,results,easiest,with,f 1,easiest with f 1,0.7022818326950073
translation,268,157,results,results,For,all submitted runs,results For all submitted runs,0.5626835227012634
translation,268,165,results,vectorslu,relies heavily on,word overlap and similarity,vectorslu relies heavily on word overlap and similarity,0.7289478182792664
translation,268,165,results,word overlap and similarity,between,question and the answer,word overlap and similarity between question and the answer,0.6447533965110779
translation,268,165,results,word overlap and similarity,experienced,relatively higher drop,word overlap and similarity experienced relatively higher drop,0.7021144032478333
translation,268,165,results,relatively higher drop,in,performance,relatively higher drop in performance,0.5773095488548279
translation,268,165,results,performance,compared to,rest,performance compared to rest,0.698719322681427
translation,268,165,results,results,has,vectorslu,results has vectorslu,0.5895588397979736
translation,268,169,results,outperformed,always predicts,yes,outperformed always predicts yes,0.8257423043251038
translation,268,169,results,majority class baseline,always predicts,yes,majority class baseline always predicts yes,0.7998546361923218
translation,268,169,results,subtask a,has,all submissions,subtask a has all submissions,0.5637609362602234
translation,268,169,results,all submissions,has,outperformed,all submissions has outperformed,0.608756422996521
translation,268,169,results,outperformed,has,majority class baseline,outperformed has majority class baseline,0.602397084236145
translation,268,202,results,outperform,on,official macro f 1 metric,outperform on official macro f 1 metric,0.5588590502738953
translation,268,202,results,majority class baseline,for,both subtasks,majority class baseline for both subtasks,0.5207083225250244
translation,268,202,results,majority class baseline,for,both languages,majority class baseline for both languages,0.5593371987342834
translation,268,202,results,outperform,has,majority class baseline,outperform has majority class baseline,0.5917549729347229
translation,268,202,results,official macro f 1 metric,has,majority class baseline,official macro f 1 metric has majority class baseline,0.5361931920051575
translation,268,204,results,results,for,arabic,results for arabic,0.5326715707778931
translation,268,204,results,arabic,higher than,english,arabic higher than english,0.7052118182182312
translation,268,204,results,english,for,subtask a,english for subtask a,0.626409113407135
translation,268,204,results,results,for,arabic,results for arabic,0.5326715707778931
translation,268,204,results,results,has,results,results has results,0.48582205176353455
translation,268,207,results,difference,between,top systems,difference between top systems,0.7570785284042358
translation,268,207,results,top systems,for,arabic and english,top systems for arabic and english,0.5759813785552979
translation,268,207,results,top systems,is,10 points ( 82.02 vs. 72.52 ),top systems is 10 points ( 82.02 vs. 72.52 ),0.5354017615318298
translation,268,207,results,arabic and english,is,10 points ( 82.02 vs. 72.52 ),arabic and english is 10 points ( 82.02 vs. 72.52 ),0.518779993057251
translation,268,207,results,accuracy,has,difference,accuracy has difference,0.5579516291618347
translation,268,207,results,results,looking at,accuracy,results looking at accuracy,0.6115431189537048
translation,269,83,experimental-setup,"liblinear ( fan et al. , 2008 )",to train,svm classifier,"liblinear ( fan et al. , 2008 ) to train svm classifier",0.6634452939033508
translation,269,83,experimental-setup,experimental setup,to train,svm classifier,experimental setup to train svm classifier,0.6851738095283508
translation,269,83,experimental-setup,experimental setup,has,"libsvm ( chang and lin , 2011 )","experimental setup has libsvm ( chang and lin , 2011 )",0.5143595933914185
translation,269,112,experiments,three submissions,submitted for,english subtask b,three submissions submitted for english subtask b,0.6784704923629761
translation,269,112,experiments,english subtask b,including,primary submission,english subtask b including primary submission,0.7115370631217957
translation,269,112,experiments,english subtask b,including,contrastive2 submission,english subtask b including contrastive2 submission,0.6640365123748779
translation,269,136,experiments,performance,of,submission,performance of submission,0.6176324486732483
translation,269,136,experiments,english subtask a,has,performance,english subtask a has performance,0.5955034494400024
translation,269,136,experiments,performance,has,hierarchical classification method result,performance has hierarchical classification method result,0.5575342774391174
translation,269,136,experiments,submission,has,contrastive1,submission has contrastive1,0.6176340579986572
translation,269,136,experiments,submission,has,hierarchical classification method result,submission has hierarchical classification method result,0.5556785464286804
translation,269,142,experiments,performance,on,primary submission,performance on primary submission,0.5397914052009583
translation,269,142,experiments,performance,was,much better,performance was much better,0.6322904825210571
translation,269,142,experiments,performance,result of,one-step classification method,performance result of one-step classification method,0.7551046013832092
translation,269,142,experiments,one-step classification method,on,all answers,one-step classification method on all answers,0.5363845825195312
translation,269,142,experiments,all answers,of,yes_no question,all answers of yes_no question,0.5440202355384827
translation,269,142,experiments,much better,than,other submissions,much better than other submissions,0.5735791921615601
translation,269,142,experiments,other submissions,results of,two -step classification method,other submissions results of two -step classification method,0.7005389332771301
translation,269,142,experiments,english subtask b,has,performance,english subtask b has performance,0.5691071152687073
translation,269,137,results,performance,of,hierarchical classification method,performance of hierarchical classification method,0.6161789894104004
translation,269,137,results,better,than,other submission,better than other submission,0.5680657029151917
translation,269,137,results,other submission,in,arabic task,other submission in arabic task,0.5212476849555969
translation,269,137,results,results,has,performance,results has performance,0.5972660779953003
translation,270,5,model,syntactic differences,between,questions and relevant text passages,syntactic differences between questions and relevant text passages,0.6301871538162231
translation,270,5,model,model,focus on,syntactic differences,model focus on syntactic differences,0.7132922410964966
translation,270,6,model,novel algorithm,analyzes,dependency structures,novel algorithm analyzes dependency structures,0.6481781005859375
translation,270,6,model,novel algorithm,acquires,transformational patterns,novel algorithm acquires transformational patterns,0.6010549664497375
translation,270,6,model,dependency structures,of,queries and known relevant text passages,dependency structures of queries and known relevant text passages,0.5176773071289062
translation,270,6,model,transformational patterns,to retrieve,relevant textual content,transformational patterns to retrieve relevant textual content,0.7304208874702454
translation,270,6,model,model,propose,novel algorithm,model propose novel algorithm,0.7439810037612915
translation,270,12,model,novel algorithm,analyses,de-pendency structures,novel algorithm analyses de-pendency structures,0.5666542053222656
translation,270,12,model,de-pendency structures,of,known valid answer sentence,de-pendency structures of known valid answer sentence,0.5516341924667358
translation,270,12,model,relevant text passages,from,underlying document collection,relevant text passages from underlying document collection,0.5321214199066162
translation,270,12,model,model,present,novel algorithm,model present novel algorithm,0.7301753759384155
translation,270,13,model,position,of,key phrases,position of key phrases,0.5595201253890991
translation,270,13,model,position,analyzed and linked to,certain syntactic question type,position analyzed and linked to certain syntactic question type,0.5720764398574829
translation,270,13,model,key phrases,in,answer sentence,key phrases in answer sentence,0.4344644546508789
translation,270,13,model,key phrases,relative to,answer,key phrases relative to answer,0.5557220578193665
translation,270,13,model,answer sentence,relative to,answer,answer sentence relative to answer,0.6869421005249023
translation,270,15,results,valid dependency structures,from,known answer sentences,valid dependency structures from known answer sentences,0.5154842138290405
translation,270,15,results,valid dependency structures,able to,link,valid dependency structures able to link,0.6460004448890686
translation,270,15,results,much wider spectrum,of,answer sentences,much wider spectrum of answer sentences,0.5886992812156677
translation,270,15,results,answer sentences,to,question,answer sentences to question,0.6187059283256531
translation,270,15,results,link,has,much wider spectrum,link has much wider spectrum,0.6123024821281433
translation,270,15,results,results,learn,valid dependency structures,results learn valid dependency structures,0.570094645023346
translation,271,95,experiments,domain adaptation,from,quora,domain adaptation from quora,0.5783589482307434
translation,271,95,experiments,domain adaptation,performs,worst,domain adaptation performs worst,0.675336480140686
translation,271,95,experiments,quora,performs,worst,quora performs worst,0.7180793881416321
translation,271,95,experiments,worst,with,almost no improvement,worst with almost no improvement,0.6525402069091797
translation,271,95,experiments,almost no improvement,over,direct transfer,almost no improvement over direct transfer,0.6838625073432922
translation,271,65,hyperparameters,"bi-lstm ( hochreiter and schmidhuber , 1997 ) encoder",operates on,300 dimensional glove word embeddings,"bi-lstm ( hochreiter and schmidhuber , 1997 ) encoder operates on 300 dimensional glove word embeddings",0.6487557888031006
translation,271,65,hyperparameters,300 dimensional glove word embeddings,train on,combined data,300 dimensional glove word embeddings train on combined data,0.7213417887687683
translation,271,65,hyperparameters,combined data,from,all domains,combined data from all domains,0.5654701590538025
translation,271,65,hyperparameters,hyperparameters,use,"bi-lstm ( hochreiter and schmidhuber , 1997 ) encoder","hyperparameters use bi-lstm ( hochreiter and schmidhuber , 1997 ) encoder",0.5485730171203613
translation,271,66,hyperparameters,word embeddings,fixed in,our experiments,word embeddings fixed in our experiments,0.5783593058586121
translation,271,66,hyperparameters,hyperparameters,keep,word embeddings,hyperparameters keep word embeddings,0.5392046570777893
translation,271,23,model,adversarial domain adaptation ( ada ),to effectively use,labeled data,adversarial domain adaptation ( ada ) to effectively use labeled data,0.6669864058494568
translation,271,23,model,labeled data,from,another forum,labeled data from another forum,0.5695403218269348
translation,271,83,results,cosine similarity,with,hinge loss,cosine similarity with hinge loss,0.5983524322509766
translation,271,83,results,cosine similarity,yields,much better results,cosine similarity yields much better results,0.6936547756195068
translation,271,83,results,much better results,than using,cross-entropy loss,much better results than using cross-entropy loss,0.640313982963562
translation,271,83,results,results,using,cosine similarity,results using cosine similarity,0.6640024781227112
translation,271,88,results,wasserstein and the classification - based methods,perform,very similarly,wasserstein and the classification - based methods perform very similarly,0.587932288646698
translation,271,88,results,very similarly,after,proper hyper-parameter tuning,very similarly after proper hyper-parameter tuning,0.7008539438247681
translation,271,88,results,results,see that,wasserstein and the classification - based methods,results see that wasserstein and the classification - based methods,0.646372377872467
translation,271,92,results,almost all source-target domain pairs,from,stackexchange family,almost all source-target domain pairs from stackexchange family,0.5466768741607666
translation,271,92,results,domain adaptation,improves over,both baselines,domain adaptation improves over both baselines,0.7324749231338501
translation,271,92,results,both baselines,with,average relative improvement,both baselines with average relative improvement,0.6082957983016968
translation,271,92,results,average relative improvement,of,5.6 %,average relative improvement of 5.6 %,0.5566513538360596
translation,271,92,results,almost all source-target domain pairs,has,domain adaptation,almost all source-target domain pairs has domain adaptation,0.5653047561645508
translation,271,92,results,results,For,almost all source-target domain pairs,results For almost all source-target domain pairs,0.5875720977783203
translation,271,93,results,improvement,goes up to,14 %,improvement goes up to 14 %,0.70497727394104
translation,271,93,results,14 %,for,askubuntu -android source - target domain pair,14 % for askubuntu -android source - target domain pair,0.5995465517044067
translation,271,93,results,results,has,improvement,results has improvement,0.6248279809951782
translation,271,94,results,domain adaptation,on,sprint dataset,domain adaptation on sprint dataset,0.5104609727859497
translation,271,94,results,domain adaptation,performs,better,domain adaptation performs better,0.6165781021118164
translation,271,94,results,better,than,direct transfer,better than direct transfer,0.6157660484313965
translation,271,94,results,worse,than,bm25,worse than bm25,0.618742823600769
translation,271,94,results,results,has,domain adaptation,results has domain adaptation,0.5254684686660767
translation,272,134,ablation-analysis,performance,of,our model,performance of our model,0.5847885608673096
translation,272,134,ablation-analysis,obviously drops,increase of,answer length,obviously drops increase of answer length,0.7345945835113525
translation,272,134,ablation-analysis,our model,has,obviously drops,our model has obviously drops,0.6223689317703247
translation,272,99,experimental-setup,tokenizer,from,stanford corenlp,tokenizer from stanford corenlp,0.5250846147537231
translation,272,99,experimental-setup,tokenizer,to preprocess,each passage and question,tokenizer to preprocess each passage and question,0.7333182692527771
translation,272,99,experimental-setup,experimental setup,use,tokenizer,experimental setup use tokenizer,0.6232596039772034
translation,272,100,experimental-setup,gated recurrent unit variant,of,lstm,gated recurrent unit variant of lstm,0.580686092376709
translation,272,100,experimental-setup,gated recurrent unit variant,used throughout,our model,gated recurrent unit variant used throughout our model,0.666583240032196
translation,272,100,experimental-setup,experimental setup,has,gated recurrent unit variant,experimental setup has gated recurrent unit variant,0.5503972172737122
translation,272,101,experimental-setup,word embedding,use,pretrained case-sensitive glove embeddings,word embedding use pretrained case-sensitive glove embeddings,0.534946084022522
translation,272,101,experimental-setup,word embedding,use,zero vectors,word embedding use zero vectors,0.6477743983268738
translation,272,101,experimental-setup,pretrained case-sensitive glove embeddings,for,questions and passages,pretrained case-sensitive glove embeddings for questions and passages,0.5776599049568176
translation,272,101,experimental-setup,fixed,during,training,fixed during training,0.7956002354621887
translation,272,101,experimental-setup,zero vectors,to represent,all out - of- vocab words,zero vectors to represent all out - of- vocab words,0.6338635087013245
translation,272,101,experimental-setup,pretrained case-sensitive glove embeddings,has,fixed,pretrained case-sensitive glove embeddings has fixed,0.5515900254249573
translation,272,101,experimental-setup,experimental setup,For,word embedding,experimental setup For word embedding,0.541065514087677
translation,272,103,experimental-setup,hidden vector length,set to,75,hidden vector length set to 75,0.7232177257537842
translation,272,103,experimental-setup,75,for,all layers,75 for all layers,0.6385263800621033
translation,272,103,experimental-setup,experimental setup,has,hidden vector length,experimental setup has hidden vector length,0.525132417678833
translation,272,104,experimental-setup,hidden size,to compute,attention scores,hidden size to compute attention scores,0.7518976330757141
translation,272,104,experimental-setup,attention scores,is,75,attention scores is 75,0.5851427316665649
translation,272,104,experimental-setup,experimental setup,has,hidden size,experimental setup has hidden size,0.5616794228553772
translation,272,105,experimental-setup,"dropout ( srivastava et al. , 2014 )",between,layers,"dropout ( srivastava et al. , 2014 ) between layers",0.6070454120635986
translation,272,105,experimental-setup,"dropout ( srivastava et al. , 2014 )",with,dropout rate,"dropout ( srivastava et al. , 2014 ) with dropout rate",0.531439483165741
translation,272,105,experimental-setup,dropout rate,of,0.2,dropout rate of 0.2,0.5832480192184448
translation,272,105,experimental-setup,experimental setup,apply,"dropout ( srivastava et al. , 2014 )","experimental setup apply dropout ( srivastava et al. , 2014 )",0.5641402006149292
translation,272,106,experimental-setup,"adadelta ( zeiler , 2012 )",with,initial learning rate,"adadelta ( zeiler , 2012 ) with initial learning rate",0.588582456111908
translation,272,106,experimental-setup,initial learning rate,of,1,initial learning rate of 1,0.5969468951225281
translation,272,106,experimental-setup,experimental setup,optimized with,"adadelta ( zeiler , 2012 )","experimental setup optimized with adadelta ( zeiler , 2012 )",0.7056093215942383
translation,272,5,model,question and passage,with,gated attention - based recurrent networks,question and passage with gated attention - based recurrent networks,0.6349449753761292
translation,272,5,model,gated attention - based recurrent networks,to obtain,question - aware passage representation,gated attention - based recurrent networks to obtain question - aware passage representation,0.5406053066253662
translation,272,5,model,model,match,question and passage,model match question and passage,0.7744337916374207
translation,272,6,model,self-matching attention mechanism,to refine,representation,self-matching attention mechanism to refine representation,0.6733013987541199
translation,272,6,model,self-matching attention mechanism,effectively encodes,information,self-matching attention mechanism effectively encodes information,0.7327488660812378
translation,272,6,model,representation,by matching,passage,representation by matching passage,0.8155702948570251
translation,272,6,model,information,from,whole passage,information from whole passage,0.5657477974891663
translation,272,6,model,model,propose,self-matching attention mechanism,model propose self-matching attention mechanism,0.6062133312225342
translation,272,23,model,end-to - end neural network,for,reading comprehension and question answering,end-to - end neural network for reading comprehension and question answering,0.5545275211334229
translation,272,23,model,gated self-matching network,has,end-to - end neural network,gated self-matching network has end-to - end neural network,0.5548875331878662
translation,272,23,model,model,introduce,gated self-matching network,model introduce gated self-matching network,0.6360217928886414
translation,272,24,model,recurrent network encoder,to build,representation,recurrent network encoder to build representation,0.6888498067855835
translation,272,24,model,representation,for,questions and passages,representation for questions and passages,0.6505308151245117
translation,272,24,model,gated matching layer,to match,question and passage,gated matching layer to match question and passage,0.7305933833122253
translation,272,24,model,self-matching layer,to aggregate,information,self-matching layer to aggregate information,0.7613351941108704
translation,272,24,model,information,from,whole passage,information from whole passage,0.5657477974891663
translation,272,24,model,model,consists of,four parts,model consists of four parts,0.7259754538536072
translation,272,26,model,gated attention - based recurrent network,adds,additional gate,gated attention - based recurrent network adds additional gate,0.6028457283973694
translation,272,26,model,additional gate,to,attention - based recurrent networks,additional gate to attention - based recurrent networks,0.5581396222114563
translation,272,26,model,model,propose,gated attention - based recurrent network,model propose gated attention - based recurrent network,0.6380494236946106
translation,272,28,model,gated attention - based recurrent network,assigns,different levels of importance,gated attention - based recurrent network assigns different levels of importance,0.6374515891075134
translation,272,28,model,different levels of importance,to,passage parts,different levels of importance to passage parts,0.5786367654800415
translation,272,28,model,different levels of importance,masking out,irrelevant passage parts,different levels of importance masking out irrelevant passage parts,0.7568690180778503
translation,272,28,model,passage parts,depending on,relevance,passage parts depending on relevance,0.730719268321991
translation,272,28,model,relevance,to,question,relevance to question,0.5386306643486023
translation,272,28,model,gating mechanism,has,gated attention - based recurrent network,gating mechanism has gated attention - based recurrent network,0.5375680327415466
translation,272,28,model,model,introducing,gating mechanism,model introducing gating mechanism,0.7639573216438293
translation,272,29,model,self-matching mechanism,can effectively aggregate,evidence,self-matching mechanism can effectively aggregate evidence,0.7538970112800598
translation,272,29,model,evidence,from,whole passage,evidence from whole passage,0.6459497213363647
translation,272,29,model,evidence,to infer,answer,evidence to infer answer,0.8282018303871155
translation,272,29,model,model,introduce,self-matching mechanism,model introduce self-matching mechanism,0.6328525543212891
translation,272,33,model,self-matching layer,to dynamically refine,passage representation,self-matching layer to dynamically refine passage representation,0.7383524179458618
translation,272,33,model,passage representation,with,information,passage representation with information,0.603728711605072
translation,272,33,model,information,from,whole passage,information from whole passage,0.5657477974891663
translation,272,33,model,model,propose,self-matching layer,model propose self-matching layer,0.6816613674163818
translation,272,34,model,gated attention - based recurrent networks,on,passage,gated attention - based recurrent networks on passage,0.5654522776603699
translation,272,34,model,passage,against,passage itself,passage against passage itself,0.6998235583305359
translation,272,34,model,current passage word,from,every word,current passage word from every word,0.5520815849304199
translation,272,34,model,every word,in,passage,every word in passage,0.5442224740982056
translation,272,102,model,1 layer of bi-directional gru,to compute,character - level embeddings,1 layer of bi-directional gru to compute character - level embeddings,0.66706782579422
translation,272,102,model,3 layers of bi-directional gru,to encode,questions and passages,3 layers of bi-directional gru to encode questions and passages,0.6328538656234741
translation,272,102,model,gated attention - based recurrent network,for,question and passage matching,gated attention - based recurrent network for question and passage matching,0.5780174136161804
translation,272,102,model,gated attention - based recurrent network,encoded,bidirectionally,gated attention - based recurrent network encoded bidirectionally,0.7432621717453003
translation,272,102,model,model,utilize,1 layer of bi-directional gru,model utilize 1 layer of bi-directional gru,0.589992344379425
translation,272,102,model,model,utilize,3 layers of bi-directional gru,model utilize 3 layers of bi-directional gru,0.6120098829269409
translation,272,102,model,model,utilize,gated attention - based recurrent network,model utilize gated attention - based recurrent network,0.5777027010917664
translation,272,174,model,self-matching,based on,question - aware representation,self-matching based on question - aware representation,0.6643564701080322
translation,272,174,model,self-matching,based on,gated attention - based recurrent networks,self-matching based on gated attention - based recurrent networks,0.6509943604469299
translation,272,174,model,model,apply,self-matching,model apply self-matching,0.6926573514938354
translation,272,9,results,single model,achieves,71.3 %,single model achieves 71.3 %,0.6145920753479004
translation,272,9,results,71.3 %,on,evaluation metrics,71.3 % on evaluation metrics,0.48665839433670044
translation,272,9,results,71.3 %,on,hidden test set,71.3 % on hidden test set,0.5405858159065247
translation,272,9,results,evaluation metrics,of,exact match,evaluation metrics of exact match,0.5040953159332275
translation,272,9,results,ensemble model,boosts,results,ensemble model boosts results,0.7247231006622314
translation,272,9,results,results,to,75.9 %,results to 75.9 %,0.4947461187839508
translation,272,9,results,results,to,75.9 %,results to 75.9 %,0.4947461187839508
translation,272,9,results,results,has,single model,results has single model,0.5429759621620178
translation,272,37,results,our single model,achieves,71.3 % exact match accuracy,our single model achieves 71.3 % exact match accuracy,0.6554388403892517
translation,272,37,results,71.3 % exact match accuracy,on,hidden squad test set,71.3 % exact match accuracy on hidden squad test set,0.5216060876846313
translation,272,37,results,ensemble model,further boosts,result,ensemble model further boosts result,0.7166429162025452
translation,272,37,results,result,to,75.9 %,result to 75.9 %,0.5189229846000671
translation,272,37,results,results,has,our single model,results has our single model,0.5605776309967041
translation,273,145,hyperparameters,statistical models,use,stochastic adaptive subgradient algorithm,statistical models use stochastic adaptive subgradient algorithm,0.5697752833366394
translation,273,145,hyperparameters,stochastic adaptive subgradient algorithm,called,adagrad,stochastic adaptive subgradient algorithm called adagrad,0.5833510756492615
translation,273,145,hyperparameters,per-coordinate learning rates,to exploit,rarely seen features,per-coordinate learning rates to exploit rarely seen features,0.7105159759521484
translation,273,145,hyperparameters,per-coordinate learning rates,to exploit,remaining scalable,per-coordinate learning rates to exploit remaining scalable,0.7088024616241455
translation,273,145,hyperparameters,hyperparameters,To build,statistical models,hyperparameters To build statistical models,0.6388217210769653
translation,273,147,hyperparameters,adagrad,in,clearnlp,adagrad in clearnlp,0.6041148900985718
translation,273,147,hyperparameters,clearnlp,using,hinge-loss,clearnlp using hinge-loss,0.6470381021499634
translation,273,147,hyperparameters,default hyper-parameters,has,learning rate,default hyper-parameters has learning rate,0.4934147000312805
translation,273,147,hyperparameters,learning rate,has,a,learning rate has a,0.5678636431694031
translation,273,147,hyperparameters,learning rate,has,= 0.01,learning rate has = 0.01,0.5676239728927612
translation,273,147,hyperparameters,a,has,= 0.01,a has = 0.01,0.5855022668838501
translation,273,147,hyperparameters,termination criterion,has,r = 0.1,termination criterion has r = 0.1,0.5158934593200684
translation,273,147,hyperparameters,hyperparameters,implementation of,adagrad,hyperparameters implementation of adagrad,0.5570290088653564
translation,273,147,hyperparameters,hyperparameters,take,default hyper-parameters,hyperparameters take default hyper-parameters,0.5709553360939026
translation,273,4,model,model,suggests,architectural approach,model suggests architectural approach,0.640811562538147
translation,273,5,model,four kinds of entity relations,added to,knowledge graph,four kinds of entity relations added to knowledge graph,0.6130759119987488
translation,273,22,model,model,suggests,architectural approach,model suggests architectural approach,0.640811562538147
translation,273,23,model,systematic way,of building,graph,systematic way of building graph,0.7540106773376465
translation,273,23,model,graph,by merging,four kinds of information,graph by merging four kinds of information,0.8044627904891968
translation,273,23,model,four kinds of information,generated by,existing tools,four kinds of information generated by existing tools,0.6408377289772034
translation,273,23,model,model,present,systematic way,model present systematic way,0.720842719078064
translation,273,150,results,cross-validation score,is,71.75 %,cross-validation score is 71.75 %,0.5310612320899963
translation,273,150,results,results,has,cross-validation score,results has cross-validation score,0.5529488921165466
translation,274,6,experiments,questions,to,answers,questions to answers,0.5958693623542786
translation,274,6,experiments,questions,based on,cyk parsing,questions based on cyk parsing,0.6402262449264526
translation,274,6,experiments,answers,based on,cyk parsing,answers based on cyk parsing,0.6808121204376221
translation,274,14,experiments,qa task,as,translation procedure,qa task as translation procedure,0.5255600214004517
translation,274,172,experiments,qp,perform,worse ( 11.8 % ),qp perform worse ( 11.8 % ),0.6124433279037476
translation,274,172,experiments,worse ( 11.8 % ),than,re,worse ( 11.8 % ) than re,0.591351330280304
translation,274,5,model,model,present,translation - based approach,model present translation - based approach,0.6666224002838135
translation,274,13,model,semantic parsing,into,question answering procedure,semantic parsing into question answering procedure,0.5422002077102661
translation,274,94,model,dependency tree - based method,to handle,multiple -constraint questions,dependency tree - based method to handle multiple -constraint questions,0.7319508194923401
translation,274,94,model,dependency tree - based method,decomposing,original question,dependency tree - based method decomposing original question,0.7297943830490112
translation,274,94,model,original question,into,set of sub-questions,original question into set of sub-questions,0.5769804120063782
translation,274,94,model,set of sub-questions,using,syntax - based patterns,set of sub-questions using syntax - based patterns,0.6391373872756958
translation,274,94,model,answers,of,all sub-questions,answers of all sub-questions,0.5779004693031311
translation,274,94,model,all sub-questions,as,final answers,all sub-questions as final answers,0.5000929236412048
translation,274,94,model,final answers,of,original question,final answers of original question,0.5561830401420593
translation,274,94,model,intersecting,has,answers,intersecting has answers,0.5968388319015503
translation,274,94,model,model,propose,dependency tree - based method,model propose dependency tree - based method,0.6859559416770935
translation,274,165,results,outperforms,on,dev and test,outperforms on dev and test,0.565814733505249
translation,274,165,results,baseline,on,dev and test,baseline on dev and test,0.6046220660209656
translation,274,165,results,kb - qa method,has,outperforms,kb - qa method has outperforms,0.6389850378036499
translation,274,165,results,outperforms,has,baseline,outperforms has baseline,0.6131853461265564
translation,274,169,results,accuracy,of,re,accuracy of re,0.6843714714050293
translation,274,169,results,accuracy,of,slightly better,accuracy of slightly better,0.587256133556366
translation,274,169,results,accuracy,is,slightly better,accuracy is slightly better,0.5838770270347595
translation,274,169,results,re,on,test ( 32.5 % ),re on test ( 32.5 % ),0.6000248789787292
translation,274,169,results,slightly better,than,baseline 's result ( 31.4 % ),slightly better than baseline 's result ( 31.4 % ),0.5264390110969543
translation,274,169,results,results,see that,accuracy,results see that accuracy,0.6826137900352478
translation,274,171,results,quality,of,relation expressions,quality of relation expressions,0.5825538635253906
translation,274,171,results,quality,of,lexicon entries,quality of lexicon entries,0.6239305138587952
translation,274,171,results,quality,use,extraction - related statistics,quality use extraction - related statistics,0.6622868180274963
translation,274,171,results,relation expressions,quality of,lexicon entries,relation expressions quality of lexicon entries,0.5431091785430908
translation,274,171,results,relation expressions,use,extraction - related statistics,relation expressions use extraction - related statistics,0.6588534116744995
translation,274,171,results,relation expressions,as,features,relation expressions as features,0.49430978298187256
translation,274,171,results,lexicon entries,used in,baseline,lexicon entries used in baseline,0.6799399852752686
translation,274,171,results,extraction - related statistics,of,relation expressions,extraction - related statistics of relation expressions,0.5576658248901367
translation,274,171,results,extraction - related statistics,brings,more information,extraction - related statistics brings more information,0.6202852725982666
translation,274,171,results,relation expressions,as,features,relation expressions as features,0.49430978298187256
translation,274,171,results,more information,to measure,confidence,more information to measure confidence,0.6890996694564819
translation,274,171,results,confidence,of,mapping,confidence of mapping,0.6527857780456543
translation,274,171,results,results,use,extraction - related statistics,results use extraction - related statistics,0.618044376373291
translation,274,171,results,results,has,quality,results has quality,0.38800710439682007
translation,274,173,results,qp only ( 97.5 % ),has,outperforms,qp only ( 97.5 % ) has outperforms,0.6092841029167175
translation,274,173,results,outperforms,has,re only ( 73.2 % ),outperforms has re only ( 73.2 % ),0.6173612475395203
translation,274,173,results,outperforms,has,significantly,outperforms has significantly,0.6533594727516174
translation,274,173,results,re only ( 73.2 % ),has,significantly,re only ( 73.2 % ) has significantly,0.5870267152786255
translation,275,165,ablation-analysis,attention mechanism,replaced by,unweighted average,attention mechanism replaced by unweighted average,0.7019860148429871
translation,275,165,ablation-analysis,attention mechanism,has,accuracy,attention mechanism has accuracy,0.5115496516227722
translation,275,165,ablation-analysis,unweighted average,has,accuracy,unweighted average has accuracy,0.6001502275466919
translation,275,165,ablation-analysis,accuracy,has,declined,accuracy has declined,0.5948014855384827
translation,275,165,ablation-analysis,declined,has,approximately 3 %,declined has approximately 3 %,0.6424490809440613
translation,275,165,ablation-analysis,ablation analysis,removing,attention mechanism,ablation analysis removing attention mechanism,0.7049257159233093
translation,275,173,ablation-analysis,precision,for,first output,precision for first output,0.637007474899292
translation,275,173,ablation-analysis,first output,declined,11.4 %,first output declined 11.4 %,0.6521769165992737
translation,275,173,ablation-analysis,85 % cases,generate,correct answer,85 % cases generate correct answer,0.6707078814506531
translation,275,173,ablation-analysis,correct answer,in,top - 5,correct answer in top - 5,0.5106227993965149
translation,275,183,ablation-analysis,merging module,impaired,overall precision,merging module impaired overall precision,0.7589265704154968
translation,275,183,ablation-analysis,merging module,shows,bigger improvement,merging module shows bigger improvement,0.6619401574134827
translation,275,183,ablation-analysis,bigger improvement,on,recall,bigger improvement on recall,0.5321186184883118
translation,275,183,ablation-analysis,overall precision,has,a little bit,overall precision has a little bit,0.5527995228767395
translation,275,17,baselines,approaches,based on,semantic parsing and neural networks,approaches based on semantic parsing and neural networks,0.6411147713661194
translation,275,17,baselines,semantic parsing and neural networks,learn,entire representations,semantic parsing and neural networks learn entire representations,0.6588823199272156
translation,275,17,baselines,entire representations,for,questions,entire representations for questions,0.6810398697853088
translation,275,17,baselines,entire representations,by using,neural network,entire representations by using neural network,0.6420485973358154
translation,275,17,baselines,questions,with,different query structures,questions with different query structures,0.6329260468482971
translation,275,17,baselines,neural network,following,encode- and - compare framework,neural network following encode- and - compare framework,0.688201904296875
translation,275,17,baselines,baselines,has,approaches,baselines has approaches,0.6255190372467041
translation,275,138,baselines,sqg,firstly generates,candidate queries,sqg firstly generates candidate queries,0.7297254204750061
translation,275,138,baselines,candidate queries,by finding,valid walks,candidate queries by finding valid walks,0.6994315981864929
translation,275,138,baselines,valid walks,containing,all of entities and properties,valid walks containing all of entities and properties,0.6463040113449097
translation,275,138,baselines,baselines,has,sqg,baselines has sqg,0.5728135704994202
translation,275,139,baselines,compqa,is,kbqa system,compqa is kbqa system,0.5987797975540161
translation,275,139,baselines,baselines,has,compqa,baselines has compqa,0.6240342855453491
translation,275,130,experimental-setup,machine,with,intel xeon e3- 1225 3.2 ghz processor,machine with intel xeon e3- 1225 3.2 ghz processor,0.5404333472251892
translation,275,130,experimental-setup,machine,with,32 gb,machine with 32 gb,0.6606667637825012
translation,275,130,experimental-setup,machine,with,nvidia gtx1080 ti gpu,machine with nvidia gtx1080 ti gpu,0.568146824836731
translation,275,131,experimental-setup,embedding layer,used,random embedding,embedding layer used random embedding,0.5573320388793945
translation,275,131,experimental-setup,experimental setup,For,embedding layer,experimental setup For embedding layer,0.5711873769760132
translation,275,133,experimental-setup,threshold,for,frequent query substructures,threshold for frequent query substructures,0.5945949554443359
translation,275,133,experimental-setup,frequent query substructures,set to,30,frequent query substructures set to 30,0.5756285786628723
translation,275,133,experimental-setup,maximum iteration number k,for,merging,maximum iteration number k for merging,0.6461281776428223
translation,275,133,experimental-setup,merging,set to,2,merging set to 2,0.7931824326515198
translation,275,133,experimental-setup,maximum triple number,for,merged results,maximum triple number for merged results,0.6131361722946167
translation,275,133,experimental-setup,merged results,set to,5,merged results set to 5,0.7254831790924072
translation,275,133,experimental-setup,maximum aggregation number,set to,2,maximum aggregation number set to 2,0.723279595375061
translation,275,133,experimental-setup,experimental setup,has,threshold,experimental setup has threshold,0.5392109155654907
translation,275,133,experimental-setup,experimental setup,has,maximum aggregation number,experimental setup has maximum aggregation number,0.5463895797729492
translation,275,6,model,new query generation approach,based on,frequent query substructures,new query generation approach based on frequent query substructures,0.6041856408119202
translation,275,6,model,subqg,has,new query generation approach,subqg has new query generation approach,0.5855440497398376
translation,275,6,model,model,propose,subqg,model propose subqg,0.7262535691261292
translation,275,23,model,multiple neural networks,to predict,query substructures,multiple neural networks to predict query substructures,0.6616144180297852
translation,275,23,model,query substructures,contained in,question,query substructures contained in question,0.6315189599990845
translation,275,23,model,model,employ,multiple neural networks,model employ multiple neural networks,0.5840308666229248
translation,275,24,model,existing query structure,for,input question,existing query structure for input question,0.5971554517745972
translation,275,24,model,input question,by using,combinational ranking function,input question by using combinational ranking function,0.6536622643470764
translation,275,24,model,model,select,existing query structure,model select existing query structure,0.6942418217658997
translation,275,143,results,all the comparative approaches,on,both datasets,all the comparative approaches on both datasets,0.4819517433643341
translation,275,143,results,our approach subqg,has,outperformed,our approach subqg has outperformed,0.6106167435646057
translation,275,143,results,outperformed,has,all the comparative approaches,outperformed has all the comparative approaches,0.6063697338104248
translation,275,143,results,results,has,our approach subqg,results has our approach subqg,0.6021326184272766
translation,275,150,results,results,on,qald - 5 dataset,results on qald - 5 dataset,0.5631780028343201
translation,275,150,results,qald - 5 dataset,is,not as high,qald - 5 dataset is not as high,0.5935072302818298
translation,275,150,results,not as high,as,result,not as high as result,0.6232335567474365
translation,275,150,results,result,on,lc - quad,result on lc - quad,0.5664250254631042
translation,275,150,results,results,on,qald - 5 dataset,results on qald - 5 dataset,0.5631780028343201
translation,275,150,results,results,on,lc - quad,results on lc - quad,0.5117660760879517
translation,275,150,results,results,has,results,results has results,0.48582205176353455
translation,275,166,results,additional part,of,speech tag sequence,additional part of speech tag sequence,0.571483314037323
translation,275,166,results,additional part,gained,no significant improvement,additional part gained no significant improvement,0.6653297543525696
translation,275,166,results,speech tag sequence,of,input question,speech tag sequence of input question,0.5527951121330261
translation,275,166,results,results,Adding,additional part,results Adding additional part,0.6957597732543945
translation,275,181,results,results,on,lc - quad dataset,results on lc - quad dataset,0.5636740922927856
translation,275,182,results,our query substructure based approaches,obtained,stable improvements,our query substructure based approaches obtained stable improvements,0.5905658602714539
translation,275,182,results,stable improvements,on,precision and recall,stable improvements on precision and recall,0.5166525840759277
translation,275,182,results,more training data,has,our query substructure based approaches,more training data has our query substructure based approaches,0.5626978278160095
translation,275,182,results,results,With,more training data,results With more training data,0.6035581827163696
translation,275,184,results,merging module,our,substructure based query generation approach,merging module our substructure based query generation approach,0.629473865032196
translation,275,184,results,substructure based query generation approach,showed,best performance,substructure based query generation approach showed best performance,0.6796272397041321
translation,275,184,results,merging module,has,substructure based query generation approach,merging module has substructure based query generation approach,0.5631640553474426
translation,276,224,ablation-analysis,test time,choosing,outputs,test time choosing outputs,0.6897769570350647
translation,276,224,ablation-analysis,outputs,of,last hop,outputs of last hop,0.5892409086227417
translation,276,224,ablation-analysis,last hop,from,sentence representation layer,last hop from sentence representation layer,0.5323179364204407
translation,276,224,ablation-analysis,sentence representation layer,as,sentence vectors,sentence representation layer as sentence vectors,0.5099153518676758
translation,276,224,ablation-analysis,better results,on,p@1 and mrr,better results on p@1 and mrr,0.5765336155891418
translation,276,224,ablation-analysis,ablation analysis,At,test time,ablation analysis At test time,0.5376085042953491
translation,276,156,baselines,margin based ranking loss,with,ranknet,margin based ranking loss with ranknet,0.5948257446289062
translation,276,156,baselines,ranknet,has,"burges et al. , 2005 )","ranknet has burges et al. , 2005 )",0.5882726311683655
translation,276,160,baselines,arc - ii,learns,hierarchical pattern,arc - ii learns hierarchical pattern,0.6688629984855652
translation,276,160,baselines,hierarchical pattern,based on,arc - i,hierarchical pattern based on arc - i,0.6802659630775452
translation,276,160,baselines,skip- thoughts model,trains,encoder-decoder model,skip- thoughts model trains encoder-decoder model,0.7351505756378174
translation,276,160,baselines,encoder-decoder model,to construct,sentence vectors,encoder-decoder model to construct sentence vectors,0.6726523041725159
translation,276,161,baselines,"compare-aggregate ( wang and jiang , 2017 )",are,attention - based models,"compare-aggregate ( wang and jiang , 2017 ) are attention - based models",0.5626232624053955
translation,276,161,baselines,rewrite + rank,based on,generative adversarial network,rewrite + rank based on generative adversarial network,0.6417065262794495
translation,276,161,baselines,attentive lstm,has,abcnn,attentive lstm has abcnn,0.5868204236030579
translation,276,161,baselines,baselines,has,attentive lstm,baselines has attentive lstm,0.5500566363334656
translation,276,32,experiments,sentences,similar to,human reading behavior,sentences similar to human reading behavior,0.6455411314964294
translation,276,126,hyperparameters,hidden size,to,300,hidden size to 300,0.6187703609466553
translation,276,126,hyperparameters,numbers of gru layers,for modeling,questions and answers,numbers of gru layers for modeling questions and answers,0.6968572735786438
translation,276,126,hyperparameters,questions and answers,are,1,questions and answers are 1,0.6004907488822937
translation,276,127,hyperparameters,dropout,is,0.5,dropout is 0.5,0.5335426330566406
translation,276,127,hyperparameters,word embedding,pre-trained by,skip-gram model,word embedding pre-trained by skip-gram model,0.7556093335151672
translation,276,127,hyperparameters,hyperparameters,has,dropout,hyperparameters has dropout,0.5324090719223022
translation,276,127,hyperparameters,hyperparameters,has,word embedding,hyperparameters has word embedding,0.4999687969684601
translation,276,128,hyperparameters,number of hops,is,3,number of hops is 3,0.6224575042724609
translation,276,128,hyperparameters,sequential extension layer,has,number of hops,sequential extension layer has number of hops,0.5804790258407593
translation,276,128,hyperparameters,hyperparameters,For,sequential extension layer,hyperparameters For sequential extension layer,0.5297824740409851
translation,276,129,hyperparameters,margin,is,0.1,margin is 0.1,0.5679431557655334
translation,276,129,hyperparameters,weights,for,all hops,weights for all hops,0.6365680694580078
translation,276,129,hyperparameters,all hops,set as,"( 0.2 , 0.3 , 0.5 )","all hops set as ( 0.2 , 0.3 , 0.5 )",0.6365503072738647
translation,276,129,hyperparameters,similarity calculation,has,margin,similarity calculation has margin,0.547055721282959
translation,276,129,hyperparameters,similarity calculation,has,weights,similarity calculation has weights,0.5555098056793213
translation,276,129,hyperparameters,hyperparameters,For,similarity calculation,hyperparameters For similarity calculation,0.5914867520332336
translation,276,130,hyperparameters,weights,set to,constants,weights set to constants,0.6816582083702087
translation,276,130,hyperparameters,constants,keep,reasonable tolerance,constants keep reasonable tolerance,0.6627094149589539
translation,276,130,hyperparameters,hyperparameters,keep,reasonable tolerance,hyperparameters keep reasonable tolerance,0.6202975511550903
translation,276,130,hyperparameters,hyperparameters,has,weights,hyperparameters has weights,0.5201958417892456
translation,276,131,hyperparameters,batch size,is,20,batch size is 20,0.6585116386413574
translation,276,131,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,276,132,hyperparameters,parameters,optimized by,back propagation and momentum,parameters optimized by back propagation and momentum,0.7489204406738281
translation,276,157,hyperparameters,jieba 4 toolkits,for,word segmentation,jieba 4 toolkits for word segmentation,0.5944312810897827
translation,276,157,hyperparameters,jieba 4 toolkits,tune,hidden size,jieba 4 toolkits tune hidden size,0.7511024475097656
translation,276,157,hyperparameters,hidden size,to,200,hidden size to 200,0.6244208812713623
translation,276,157,hyperparameters,numbers of gru layers,for modeling,questions and answers,numbers of gru layers for modeling questions and answers,0.6968572735786438
translation,276,157,hyperparameters,hyperparameters,use,jieba 4 toolkits,hyperparameters use jieba 4 toolkits,0.6080572605133057
translation,276,157,hyperparameters,hyperparameters,tune,hidden size,hyperparameters tune hidden size,0.7449708580970764
translation,276,157,hyperparameters,hyperparameters,tune,numbers of gru layers,hyperparameters tune numbers of gru layers,0.6684607863426208
translation,276,6,model,keyword mask model,for,cqa,keyword mask model for cqa,0.6468091011047363
translation,276,6,model,sakm ),for,cqa,sakm ) for cqa,0.7181829214096069
translation,276,6,model,cqa,to imitate,human reading behavior,cqa to imitate human reading behavior,0.6526094079017639
translation,276,6,model,keyword mask model,has,sakm ),keyword mask model has sakm ),0.617251455783844
translation,276,7,model,question and answer text,regard,each other,question and answer text regard each other,0.6709185242652893
translation,276,7,model,question and answer text,repeat,multiple times ( hops ),question and answer text repeat multiple times ( hops ),0.6989974975585938
translation,276,7,model,each other,as,context,each other as context,0.5383629202842712
translation,276,7,model,each other,within,keyword - mask attention,each other within keyword - mask attention,0.7102532386779785
translation,276,7,model,keyword - mask attention,when,encoding,keyword - mask attention when encoding,0.6535293459892273
translation,276,7,model,multiple times ( hops ),in,sequential style,multiple times ( hops ) in sequential style,0.5398955941200256
translation,276,7,model,encoding,has,representations,encoding has representations,0.6218299865722656
translation,276,7,model,model,has,question and answer text,model has question and answer text,0.5702863931655884
translation,276,13,model,question,choose,most matching one,question choose most matching one,0.7189818024635315
translation,276,13,model,model,Given,question,model Given question,0.7526277899742126
translation,276,31,model,sequential attention with keyword mask ( sakm ) model,for,answer selection task,sequential attention with keyword mask ( sakm ) model for answer selection task,0.5540796518325806
translation,276,31,model,model,propose,sequential attention with keyword mask ( sakm ) model,model propose sequential attention with keyword mask ( sakm ) model,0.6529126167297363
translation,276,40,model,keywords,in,long context,keywords in long context,0.4548336863517761
translation,276,40,model,model,has,keywords,model has keywords,0.5036013126373291
translation,276,140,results,most latent representation models,obtain,better results,most latent representation models obtain better results,0.5421003103256226
translation,276,140,results,better results,than,interaction focused models,better results than interaction focused models,0.5936843752861023
translation,276,140,results,results,has,most latent representation models,results has most latent representation models,0.5257344841957092
translation,276,141,results,proposed sakm model,achieves,best results,proposed sakm model achieves best results,0.6798420548439026
translation,276,141,results,best results,on,p@1 and mrr,best results on p@1 and mrr,0.5626727938652039
translation,276,141,results,results,has,proposed sakm model,results has proposed sakm model,0.5921362042427063
translation,276,142,results,two -step attention model,by,3.8 %,two -step attention model by 3.8 %,0.5490230321884155
translation,276,142,results,two -step attention model,by,1.8 %,two -step attention model by 1.8 %,0.5486933588981628
translation,276,142,results,two -step attention model,shows that,our sequential extension structure,two -step attention model shows that our sequential extension structure,0.6594406366348267
translation,276,142,results,3.8 %,in terms of,p@1,3.8 % in terms of p@1,0.6868960857391357
translation,276,142,results,1.8 %,in terms of,mrr,1.8 % in terms of mrr,0.7281903028488159
translation,276,142,results,our sequential extension structure,is,effective,our sequential extension structure is effective,0.6209784150123596
translation,276,142,results,our basic sa model,has,outperforms,our basic sa model has outperforms,0.6091119050979614
translation,276,142,results,outperforms,has,two -step attention model,outperforms has two -step attention model,0.572283923625946
translation,276,142,results,results,has,our basic sa model,results has our basic sa model,0.550277829170227
translation,276,143,results,outperforms,by,1.0 %,outperforms by 1.0 %,0.6205192804336548
translation,276,143,results,hy - perqa model,by,1.0 %,hy - perqa model by 1.0 %,0.5968831777572632
translation,276,143,results,1.0 %,in terms of,p@1 and 1.3 %,1.0 % in terms of p@1 and 1.3 %,0.744500458240509
translation,276,143,results,sakm model,has,outperforms,sakm model has outperforms,0.6372246742248535
translation,276,143,results,outperforms,has,hy - perqa model,outperforms has hy - perqa model,0.5870024561882019
translation,276,143,results,results,has,sakm model,results has sakm model,0.5357551574707031
translation,276,168,results,perform better,than,other baselines,perform better than other baselines,0.5497822761535645
translation,276,168,results,yahoocqa,has,attentionbased models,yahoocqa has attentionbased models,0.558834433555603
translation,276,168,results,attentionbased models,has,perform better,attentionbased models has perform better,0.5761444568634033
translation,276,169,results,outperforms,by,3.2 %,outperforms by 3.2 %,0.6225284934043884
translation,276,169,results,rewrite,by,3.2 %,rewrite by 3.2 %,0.6317532658576965
translation,276,169,results,+ rank,by,3.6 %,+ rank by 3.6 %,0.6052672863006592
translation,276,169,results,+ rank,by,3.2 %,+ rank by 3.2 %,0.6064160466194153
translation,276,169,results,+ rank,in terms of,err,+ rank in terms of err,0.7353666424751282
translation,276,169,results,3.6 %,in terms of,ndcg,3.6 % in terms of ndcg,0.6910260915756226
translation,276,169,results,sa model,has,outperforms,sa model has outperforms,0.6347360610961914
translation,276,169,results,outperforms,has,rewrite,outperforms has rewrite,0.6736056208610535
translation,276,169,results,rewrite,has,+ rank,rewrite has + rank,0.6406061053276062
translation,276,169,results,results,has,sa model,results has sa model,0.5408997535705566
translation,276,170,results,sakm model,achieves,slightly improvement,sakm model achieves slightly improvement,0.6661673784255981
translation,276,170,results,slightly improvement,compared to,sa version,slightly improvement compared to sa version,0.6629289388656616
translation,276,170,results,results,has,sakm model,results has sakm model,0.5357551574707031
translation,276,225,results,outperforms,by,more than 3 %,outperforms by more than 3 %,0.6160065531730652
translation,276,225,results,sa model,by,more than 3 %,sa model by more than 3 %,0.6059773564338684
translation,276,225,results,sa model,on,mrr,sa model on mrr,0.5852656960487366
translation,276,225,results,sa model,on,mrr,sa model on mrr,0.5852656960487366
translation,276,225,results,more than 3 %,on,p@1 and 2 %,more than 3 % on p@1 and 2 %,0.6265227794647217
translation,276,225,results,more than 3 %,on,mrr,more than 3 % on mrr,0.5877280235290527
translation,276,225,results,sakm model,has,outperforms,sakm model has outperforms,0.6372246742248535
translation,276,225,results,outperforms,has,sa model,outperforms has sa model,0.5997357964515686
translation,276,225,results,results,has,sakm model,results has sakm model,0.5357551574707031
translation,276,227,results,first hop,of,sa model,first hop of sa model,0.583522379398346
translation,276,227,results,significant,compared to,two -way attention model,significant compared to two -way attention model,0.6476922035217285
translation,276,227,results,first hop,has,gains,first hop has gains,0.5902168154716492
translation,276,227,results,sa model,has,gains,sa model has gains,0.5933318734169006
translation,276,227,results,results,choose,first hop,results choose first hop,0.6848853230476379
translation,277,177,ablation-analysis,significant performance drop,on,simplequestions ( 91.2 % to 88.9 % ),significant performance drop on simplequestions ( 91.2 % to 88.9 % ),0.4918862581253052
translation,277,180,ablation-analysis,ablation analysis,of,proposed hr - bilstm,ablation analysis of proposed hr - bilstm,0.5966745615005493
translation,277,186,ablation-analysis,simplequestions,removing,deep layers,simplequestions removing deep layers,0.7135891318321228
translation,277,186,ablation-analysis,deep layers,causes,small drop,deep layers causes small drop,0.6796183586120605
translation,277,186,ablation-analysis,small drop,in,performance,small drop in performance,0.5669881105422974
translation,277,186,ablation-analysis,ablation analysis,On,simplequestions,ablation analysis On simplequestions,0.5634985566139221
translation,277,188,ablation-analysis,webqsp,replacing,bilstm,webqsp replacing bilstm,0.6715225577354431
translation,277,188,ablation-analysis,bilstm,with,cnn,bilstm with cnn,0.6286547780036926
translation,277,188,ablation-analysis,bilstm,results in,large performance drop,bilstm results in large performance drop,0.5509027242660522
translation,277,188,ablation-analysis,cnn,in,our hierarchical matching framework,cnn in our hierarchical matching framework,0.531029224395752
translation,277,188,ablation-analysis,ablation analysis,on,webqsp,ablation analysis on webqsp,0.5706066489219666
translation,277,212,ablation-analysis,entity re-ranking step,has,significantly decreases,entity re-ranking step has significantly decreases,0.5934052467346191
translation,277,212,ablation-analysis,significantly decreases,has,scores,significantly decreases has scores,0.5891715884208679
translation,277,219,ablation-analysis,crucial,for,our system,crucial for our system,0.7024480104446411
translation,277,207,baselines,stateof - the- art,on,webqsp,stateof - the- art on webqsp,0.55128413438797
translation,277,207,baselines,state- of- the - art,on,simplequestions,state- of- the - art on simplequestions,0.5517095327377319
translation,277,168,hyperparameters,size of hidden states,for,lstms,size of hidden states for lstms,0.5755562782287598
translation,277,168,hyperparameters,lstms,has,"{ 50 , 100 , 200 , 400 } )","lstms has { 50 , 100 , 200 , 400 } )",0.6057682633399963
translation,277,168,hyperparameters,learning rate,has,"{ 0.1 , 0.5 , 1.0 , 2.0}","learning rate has { 0.1 , 0.5 , 1.0 , 2.0}",0.5081822276115417
translation,277,168,hyperparameters,hyperparameters,tune,size of hidden states,hyperparameters tune size of hidden states,0.6714459657669067
translation,277,168,hyperparameters,hyperparameters,tune,learning rate,hyperparameters tune learning rate,0.7057108283042908
translation,277,170,hyperparameters,word vectors,initialized with,300 -d pretrained word embeddings,word vectors initialized with 300 -d pretrained word embeddings,0.6811296939849854
translation,277,170,hyperparameters,hyperparameters,has,word vectors,hyperparameters has word vectors,0.5067952871322632
translation,277,171,hyperparameters,embeddings,of,relation names,embeddings of relation names,0.5332297682762146
translation,277,171,hyperparameters,relation names,are,randomly initialized,relation names are randomly initialized,0.5677092671394348
translation,277,171,hyperparameters,hyperparameters,has,embeddings,hyperparameters has embeddings,0.548468291759491
translation,277,5,model,hierarchical recurrent neural network,enhanced by,residual learning,hierarchical recurrent neural network enhanced by residual learning,0.7151879668235779
translation,277,5,model,residual learning,detects,kb relations,residual learning detects kb relations,0.6612314581871033
translation,277,5,model,kb relations,given,input question,kb relations given input question,0.7033658027648926
translation,277,5,model,model,propose,hierarchical recurrent neural network,model propose hierarchical recurrent neural network,0.682123064994812
translation,277,6,model,deep residual bidirectional lstms,to compare,questions and relation names,deep residual bidirectional lstms to compare questions and relation names,0.6283161640167236
translation,277,6,model,questions and relation names,via,different levels of abstraction,questions and relation names via different levels of abstraction,0.6163152456283569
translation,277,7,model,simple kbqa system,integrates,entity linking,simple kbqa system integrates entity linking,0.679493248462677
translation,277,7,model,simple kbqa system,integrates,our proposed relation detector,simple kbqa system integrates our proposed relation detector,0.6582725048065186
translation,277,7,model,our proposed relation detector,to make,two components,our proposed relation detector to make two components,0.61871737241745
translation,277,7,model,model,propose,simple kbqa system,model propose simple kbqa system,0.6954519152641296
translation,277,23,model,model,improves,kb relation detection,model improves kb relation detection,0.663308322429657
translation,277,29,model,candidate entities,retrieved by,entity linker,candidate entities retrieved by entity linker,0.5551006197929382
translation,277,29,model,entity linker,based on,question,entity linker based on question,0.6548705697059631
translation,277,29,model,proposed relation detection model,Re-ranking,entity candidates,proposed relation detection model Re-ranking entity candidates,0.7772987484931946
translation,277,29,model,key role,in,kbqa process,key role in kbqa process,0.5217117071151733
translation,277,29,model,key role,Re-ranking,entity candidates,key role Re-ranking entity candidates,0.7838678359985352
translation,277,29,model,confident relations,detected from,raw question text,confident relations detected from raw question text,0.7506980895996094
translation,277,29,model,model,Given,input question,model Given input question,0.7358803153038025
translation,277,29,model,model,Re-ranking,entity candidates,model Re-ranking entity candidates,0.7861582636833191
translation,277,181,results,hierarchical matching,between,questions and both relation names and relation words,hierarchical matching between questions and both relation names and relation words,0.6325205564498901
translation,277,181,results,hierarchical matching,yields,improvement,hierarchical matching yields improvement,0.6765428185462952
translation,277,181,results,questions and both relation names and relation words,yields,improvement,questions and both relation names and relation words yields improvement,0.7021291255950928
translation,277,181,results,improvement,on,both datasets,improvement on both datasets,0.5129412412643433
translation,277,181,results,both datasets,especially for,simplequestions,both datasets especially for simplequestions,0.637631356716156
translation,277,181,results,simplequestions,has,93.3 % vs. 91.2/88.8 %,simplequestions has 93.3 % vs. 91.2/88.8 %,0.5621166825294495
translation,277,181,results,results,has,hierarchical matching,results has hierarchical matching,0.5201614499092102
translation,277,182,results,residual learning,helps,hierarchical matching,residual learning helps hierarchical matching,0.6262352466583252
translation,277,182,results,hierarchical matching,compared to,weighted - sum and attention - based baselines,hierarchical matching compared to weighted - sum and attention - based baselines,0.6134455800056458
translation,277,182,results,results,has,residual learning,results has residual learning,0.5485007166862488
translation,277,183,results,attention - based baseline,tried,model,attention - based baseline tried model,0.6578437685966492
translation,277,183,results,model,lead to,better results,model lead to better results,0.7079017758369446
translation,277,183,results,significantly helps,on,webqsp,significantly helps on webqsp,0.55079585313797
translation,277,183,results,better results,compared to,hr - bilstm,better results compared to hr - bilstm,0.6503145694732666
translation,277,183,results,residual learning,has,significantly helps,residual learning has significantly helps,0.6027358770370483
translation,277,183,results,results,For,attention - based baseline,results For attention - based baseline,0.5996503233909607
translation,277,187,results,webqsp,benefits more from,residual and deeper architecture,webqsp benefits more from residual and deeper architecture,0.7149646282196045
translation,277,187,results,results,has,webqsp,results has webqsp,0.5659517049789429
translation,277,199,results,two -layer bilstm,converges to,94.99 %,two -layer bilstm converges to 94.99 %,0.6950048208236694
translation,277,204,results,similar training accuracy,compared to,hr - bilstm,similar training accuracy compared to hr - bilstm,0.620512843132019
translation,277,204,results,similar training accuracy,indicating,more serious over-fitting problem,similar training accuracy indicating more serious over-fitting problem,0.6705052852630615
translation,277,204,results,results,gives,similar training accuracy,results gives similar training accuracy,0.6246504187583923
translation,277,210,results,our method,includes,improved relation detector ( hr - bilstm ),our method includes improved relation detector ( hr - bilstm ),0.5775399208068848
translation,277,210,results,our method,improves,kbqa end task,our method improves kbqa end task,0.6723453402519226
translation,277,210,results,kbqa end task,by,2 - 3 %,kbqa end task by 2 - 3 %,0.6169142127037048
translation,277,211,results,our system,does not use,joint - inference,our system does not use joint - inference,0.6986597180366516
translation,277,211,results,our system,does not use,feature - based re-ranking step,our system does not use feature - based re-ranking step,0.7037143111228943
translation,277,211,results,our system,achieves,better or comparable results,our system achieves better or comparable results,0.6199272871017456
translation,277,211,results,better or comparable results,to,state - of - the- art,better or comparable results to state - of - the- art,0.5478088855743408
translation,278,28,baselines,heterogeneous qa dataset,present,hybridqa,heterogeneous qa dataset present hybridqa,0.6644682884216309
translation,278,28,baselines,hybridqa,collected by,crowdsourcing,hybridqa collected by crowdsourcing,0.7350763082504272
translation,278,28,baselines,crowdsourcing,based on,wikipedia tables,crowdsourcing based on wikipedia tables,0.641920804977417
translation,278,121,baselines,passage retriever,aims to,link,passage retriever aims to link,0.4305490553379059
translation,278,121,baselines,link,implicitly mentioned by,question,link implicitly mentioned by question,0.6975123286247253
translation,278,121,baselines,cells,implicitly mentioned by,question,cells implicitly mentioned by question,0.7323681712150574
translation,278,121,baselines,cells,through,hyperlinked passage,cells through hyperlinked passage,0.6169906854629517
translation,278,121,baselines,link,has,cells,link has cells,0.6407343745231628
translation,278,121,baselines,baselines,has,passage retriever,baselines has passage retriever,0.5540831089019775
translation,278,129,baselines,baselines,has,ranking model,baselines has ranking model,0.5493344068527222
translation,278,156,baselines,four bert variants,provided by,huggingface library,four bert variants provided by huggingface library,0.6706494092941284
translation,278,156,baselines,huggingface library,namely,baseuncased,huggingface library namely baseuncased,0.6456986665725708
translation,278,158,experiments,"filtering , hop , and rc models",use,"adamw ( loshchilov and hutter , 2017 ) optimizer","filtering , hop , and rc models use adamw ( loshchilov and hutter , 2017 ) optimizer",0.6120964288711548
translation,278,158,experiments,"adamw ( loshchilov and hutter , 2017 ) optimizer",with,learning rates,"adamw ( loshchilov and hutter , 2017 ) optimizer with learning rates",0.5964840054512024
translation,278,158,experiments,learning rates,of,"2e - 6 , 5e - 6 , and 3e - 5","learning rates of 2e - 6 , 5e - 6 , and 3e - 5",0.6187145113945007
translation,278,157,hyperparameters,modules,for,3.0 epochs,modules for 3.0 epochs,0.6490655541419983
translation,278,157,hyperparameters,modules,save,checkpoint file,modules save checkpoint file,0.7058347463607788
translation,278,157,hyperparameters,checkpoint file,end of,each epoch,checkpoint file end of each epoch,0.6803413033485413
translation,278,157,hyperparameters,hyperparameters,train,modules,hyperparameters train modules,0.7118221521377563
translation,278,6,model,model,present,hybridqa,model present hybridqa,0.7510132193565369
translation,278,11,model,hybrid model,combines,heterogeneous information,hybrid model combines heterogeneous information,0.7666643261909485
translation,278,11,model,heterogeneous information,to find,answer,heterogeneous information to find answer,0.6193009614944458
translation,278,12,results,em scores,obtained by,two baselines,em scores obtained by two baselines,0.6148005723953247
translation,278,12,results,two baselines,are,below 20 %,two baselines are below 20 %,0.6165440082550049
translation,278,12,results,hybrid model,achieve,em,hybrid model achieve em,0.7021870613098145
translation,278,12,results,em,has,over 40 %,em has over 40 %,0.6502441167831421
translation,278,12,results,results,show that,em scores,results show that em scores,0.4504867196083069
translation,278,40,results,our experiments,show,two homogeneous models,our experiments show two homogeneous models,0.6360040903091431
translation,278,40,results,two homogeneous models,achieve,em,two homogeneous models achieve em,0.6330859065055847
translation,278,40,results,hybrider,achieve,em,hybrider achieve em,0.692683756351471
translation,278,40,results,em,has,lower than 20 %,em has lower than 20 %,0.6210538148880005
translation,278,40,results,em,has,over 40 %,em has over 40 %,0.6502441167831421
translation,278,165,results,estimated accuracy,of,em=88.2 and f1=93.5,estimated accuracy of em=88.2 and f1=93.5,0.5652920603752136
translation,278,165,results,em=88.2 and f1=93.5,higher than,hotpotqa,em=88.2 and f1=93.5 higher than hotpotqa,0.7203741669654846
translation,278,165,results,results,obtain,estimated accuracy,results obtain estimated accuracy,0.6169835329055786
translation,278,169,results,in-table questions,are,remarkably simpler,in-table questions are remarkably simpler,0.5899760723114014
translation,278,169,results,remarkably simpler,than,in - passage question,remarkably simpler than in - passage question,0.5895252227783203
translation,278,169,results,overall accuracy,is,roughly 8 - 10 % higher,overall accuracy is roughly 8 - 10 % higher,0.5627701878547668
translation,278,169,results,results,has,in-table questions,results has in-table questions,0.5737292766571045
translation,278,170,results,best accuracy,achieved with,bert - large - uncased as backend,best accuracy achieved with bert - large - uncased as backend,0.6986132264137268
translation,278,170,results,bert - large - uncased as backend,can beat,bert - base-uncased,bert - large - uncased as backend can beat bert - base-uncased,0.6506309509277344
translation,278,170,results,bert - base-uncased,by,roughly 2 %,bert - base-uncased by roughly 2 %,0.6470839977264404
translation,278,170,results,experimented model variants,has,best accuracy,experimented model variants has best accuracy,0.5414586067199707
translation,278,170,results,results,With,experimented model variants,results With experimented model variants,0.6195365190505981
translation,279,123,ablation-analysis,weight vector,find that,grammar features,weight vector find that grammar features,0.6452648043632507
translation,279,123,ablation-analysis,grammar features,"number of prepositions , determiners , and "" to "" s in",response,"grammar features number of prepositions , determiners , and "" to "" s in response",0.7339887022972107
translation,279,123,ablation-analysis,ablation analysis,inspecting,weight vector,ablation analysis inspecting weight vector,0.6160919070243835
translation,279,131,baselines,best neural model,in terms of,p@1,best neural model in terms of p@1,0.7177940607070923
translation,279,131,baselines,p@1,is,bert model,p@1 is bert model,0.649146318435669
translation,279,131,baselines,fine-tuned,with,softmax loss,fine-tuned with softmax loss,0.6248010993003845
translation,279,131,baselines,bert model,has,fine-tuned,bert model has fine-tuned,0.6138871908187866
translation,279,131,baselines,baselines,has,best neural model,baselines has best neural model,0.5482536554336548
translation,279,141,baselines,pointer generator networks ( pgn ),to produce,fluent answer response,pointer generator networks ( pgn ) to produce fluent answer response,0.6989160776138306
translation,279,141,baselines,their variants,to produce,fluent answer response,their variants to produce fluent answer response,0.697198748588562
translation,279,145,baselines,pgn,are,widely used seq2seq models,pgn are widely used seq2seq models,0.5897657871246338
translation,279,145,baselines,widely used seq2seq models,equipped with,copy-attention mechanism,widely used seq2seq models equipped with copy-attention mechanism,0.6583892703056335
translation,279,145,baselines,copy-attention mechanism,capable of copying,any word,copy-attention mechanism capable of copying any word,0.8277426362037659
translation,279,145,baselines,any word,input directly into,generated output,any word input directly into generated output,0.8072941303253174
translation,279,145,baselines,baselines,has,pgn,baselines has pgn,0.5789557695388794
translation,279,147,baselines,pgns,with,pre-training information,pgns with pre-training information,0.5989482402801514
translation,279,147,baselines,pre-training information,by initializing,embedding layer,pre-training information by initializing embedding layer,0.7074643969535828
translation,279,147,baselines,embedding layer,with,glove vectors,embedding layer with glove vectors,0.6390811800956726
translation,279,147,baselines,"q , r pairs",from,questions -only subset,"q , r pairs from questions -only subset",0.5776051878929138
translation,279,147,baselines,questions -only subset,of,opensubtitles corpus,questions -only subset of opensubtitles corpus,0.5408262014389038
translation,279,151,baselines,tunable automatic conversation model,trained on,147m reddit conversationlike exchanges,tunable automatic conversation model trained on 147m reddit conversationlike exchanges,0.6836827993392944
translation,279,151,baselines,147m reddit conversationlike exchanges,using,gpt - 2 model architecture,147m reddit conversationlike exchanges using gpt - 2 model architecture,0.6576236486434937
translation,279,151,baselines,baselines,has,d-gpt,baselines has d-gpt,0.5542179346084595
translation,279,156,baselines,baselines,has,coqa,baselines has coqa,0.6032889485359192
translation,279,161,baselines,baselines,has,quac,baselines has quac,0.6291012763977051
translation,279,77,experimental-setup,unsuitable responses,are used at least once,training,unsuitable responses are used at least once training,0.7064947485923767
translation,279,77,experimental-setup,unsuitable responses,create,smaller batches,unsuitable responses create smaller batches,0.6254550814628601
translation,279,77,experimental-setup,smaller batches,by taking,strides,smaller batches by taking strides,0.6822235584259033
translation,279,77,experimental-setup,strides,of,k ? 1 size,strides of k ? 1 size,0.6056141257286072
translation,279,77,experimental-setup,experimental setup,To ensure,unsuitable responses,experimental setup To ensure unsuitable responses,0.6931991577148438
translation,279,78,experimental-setup,k = 150,for,da+elmo,k = 150 for da+elmo,0.6614469289779663
translation,279,78,experimental-setup,k = 50,for,bert,k = 50 for bert,0.7069515585899353
translation,279,78,experimental-setup,bert,when,training,bert when training,0.7123651504516602
translation,279,78,experimental-setup,training,with,softmax loss,training with softmax loss,0.608198881149292
translation,279,146,experimental-setup,2 - layer stacked bi-lstm pgn,using,opennmt toolkit,2 - layer stacked bi-lstm pgn using opennmt toolkit,0.6186383962631226
translation,279,146,experimental-setup,opennmt toolkit,on,ss and ss + data,opennmt toolkit on ss and ss + data,0.5567070245742798
translation,279,146,experimental-setup,experimental setup,train,2 - layer stacked bi-lstm pgn,experimental setup train 2 - layer stacked bi-lstm pgn,0.6428077816963196
translation,279,69,experiments,"contextualized elmo ( peters et al. , 2018 ) embedding",with,da model,"contextualized elmo ( peters et al. , 2018 ) embedding with da model",0.5904845595359802
translation,279,69,experiments,bert,use,"bert - base , uncased model","bert use bert - base , uncased model",0.6425265669822693
translation,279,69,experiments,"bert - base , uncased model",for,sentence pair classification,"bert - base , uncased model for sentence pair classification",0.6031419634819031
translation,279,152,experiments,d- gpt,on,our task,d- gpt on our task,0.5644219517707825
translation,279,152,experiments,d- gpt,using,ss and ss + datasets,d- gpt using ss and ss + datasets,0.6839593648910522
translation,279,152,experiments,our task,using,ss and ss + datasets,our task using ss and ss + datasets,0.6496564149856567
translation,279,6,model,method,for situating,qa responses,method for situating qa responses,0.7086030840873718
translation,279,6,model,qa responses,within,seq2seq nlg approach,qa responses within seq2seq nlg approach,0.720618724822998
translation,279,6,model,qa responses,to generate,fluent grammatical answer responses,qa responses to generate fluent grammatical answer responses,0.6286958456039429
translation,279,6,model,model,propose,method,model propose method,0.6280754208564758
translation,279,7,model,data augmentation,to generate,training data,data augmentation to generate training data,0.7213322520256042
translation,279,7,model,training data,for,end-to - end system,training data for end-to - end system,0.6373870968818665
translation,279,8,model,syntactic transformations ( sts ),to produce,question -specific candidate answer responses,syntactic transformations ( sts ) to produce question -specific candidate answer responses,0.6678175330162048
translation,279,8,model,syntactic transformations ( sts ),rank them using,bert - based classifier,syntactic transformations ( sts ) rank them using bert - based classifier,0.6437230110168457
translation,279,8,model,model,develop,syntactic transformations ( sts ),model develop syntactic transformations ( sts ),0.646790623664856
translation,279,18,model,seq2seq models,that generate,fluent and informative answer responses,seq2seq models that generate fluent and informative answer responses,0.6476808190345764
translation,279,18,model,fluent and informative answer responses,to,conversational questions,fluent and informative answer responses to conversational questions,0.5658479332923889
translation,279,18,model,model,develop,seq2seq models,model develop seq2seq models,0.6496421098709106
translation,279,19,model,answers,from,existing qa dataset,answers from existing qa dataset,0.5513972043991089
translation,279,19,model,answers,into,fluent responses,answers into fluent responses,0.5696308612823486
translation,279,19,model,fluent responses,via,data augmentation,fluent responses via data augmentation,0.7066848278045654
translation,279,20,model,supervised training data,by converting,questions and associated extractive answers,supervised training data by converting questions and associated extractive answers,0.6438596248626709
translation,279,20,model,questions and associated extractive answers,from,squadlike qa dataset,questions and associated extractive answers from squadlike qa dataset,0.5461761355400085
translation,279,20,model,fluent responses,via,syntactic transformations ( sts ),fluent responses via syntactic transformations ( sts ),0.6281400322914124
translation,279,20,model,model,synthetically generate,supervised training data,model synthetically generate supervised training data,0.6591631174087524
translation,279,119,results,shortest response baseline ( shortresp ),performs,worse,shortest response baseline ( shortresp ) performs worse,0.6185258626937866
translation,279,119,results,worse,than,ml models,worse than ml models,0.5632532835006714
translation,279,119,results,ml models,has,0.14 to 0.51 absolute p@1 difference,ml models has 0.14 to 0.51 absolute p@1 difference,0.5015690326690674
translation,279,119,results,results,show,shortest response baseline ( shortresp ),results show shortest response baseline ( shortresp ),0.6182224750518799
translation,279,121,results,language model baseline ( langmodel ),performs,even worse,language model baseline ( langmodel ) performs even worse,0.5518231987953186
translation,279,121,results,even worse,has,0.41 to 0.78 absolute difference,even worse has 0.41 to 0.78 absolute difference,0.47698384523391724
translation,279,121,results,results,has,language model baseline ( langmodel ),results has language model baseline ( langmodel ),0.5347785353660583
translation,279,122,results,feature - based linear model,shows,good performance,feature - based linear model shows good performance,0.6428653597831726
translation,279,122,results,good performance,when trained with,softmax loss,good performance when trained with softmax loss,0.624379575252533
translation,279,122,results,softmax loss,beating,many of the neural models,softmax loss beating many of the neural models,0.5976924300193787
translation,279,122,results,many of the neural models,in terms of,pr - auc and max -f1,many of the neural models in terms of pr - auc and max -f1,0.7439641356468201
translation,279,122,results,results,has,feature - based linear model,results has feature - based linear model,0.5347316861152649
translation,279,130,results,da model,with,elmo embeddings,da model with elmo embeddings,0.6252902150154114
translation,279,130,results,better able,to predict,right pronouns,better able to predict right pronouns,0.6865450143814087
translation,279,130,results,right pronouns,for,named entities,right pronouns for named entities,0.5314096808433533
translation,279,130,results,results,find that,da model,results find that da model,0.6648610830307007
translation,279,198,results,almost all models,perform,better,almost all models perform better,0.598285973072052
translation,279,198,results,better,trained with,ss + data,better trained with ss + data,0.7624099850654602
translation,279,198,results,results,has,almost all models,results has almost all models,0.5374234914779663
translation,279,199,results,pgn model,trained on,ss data,pgn model trained on ss data,0.7616217136383057
translation,279,199,results,perform better,than,sts + bert baseline,perform better than sts + bert baseline,0.5875020623207092
translation,279,199,results,sts + bert baseline,in terms of,option e.,sts + bert baseline in terms of option e.,0.7303119897842407
translation,279,199,results,pgn model,has,all other variants,pgn model has all other variants,0.5630855560302734
translation,279,199,results,ss data,has,all other variants,ss data has all other variants,0.553726315498352
translation,279,199,results,all other variants,has,perform better,all other variants has perform better,0.6008703112602234
translation,279,199,results,results,Except for,pgn model,results Except for pgn model,0.7021344304084778
translation,279,200,results,gpt - 2 model,trained on,ss data from scratch,gpt - 2 model trained on ss data from scratch,0.7905540466308594
translation,279,200,results,gpt - 2 model,does not perform,very well,gpt - 2 model does not perform very well,0.74037766456604
translation,279,200,results,results,has,gpt - 2 model,results has gpt - 2 model,0.5124862790107727
translation,279,202,results,best model,is,d - gpt,best model is d - gpt,0.5741158723831177
translation,279,202,results,d - gpt,when finetuned with,ss + dataset,d - gpt when finetuned with ss + dataset,0.7438037991523743
translation,279,202,results,results,has,best model,results has best model,0.5634682774543762
translation,280,193,ablation-analysis,performance,of,g a,performance of g a,0.6611686944961548
translation,280,193,ablation-analysis,performance,is,very poor,performance is very poor,0.5857824087142944
translation,280,193,ablation-analysis,g a,is,very poor,g a is very poor,0.6067782640457153
translation,280,193,ablation-analysis,very poor,due to,missing of attention mechanism,very poor due to missing of attention mechanism,0.6881546378135681
translation,280,193,ablation-analysis,ablation analysis,has,performance,ablation analysis has performance,0.5053174495697021
translation,280,198,ablation-analysis,effectiveness,of,aspect feature and aspect pointer network,effectiveness of aspect feature and aspect pointer network,0.5942617654800415
translation,280,198,ablation-analysis,aspect feature and aspect pointer network,illustrated via,slight but stable improvement,aspect feature and aspect pointer network illustrated via slight but stable improvement,0.6975712180137634
translation,280,198,ablation-analysis,aspect feature and aspect pointer network,illustrated via,performance drop,aspect feature and aspect pointer network illustrated via performance drop,0.7349548935890198
translation,280,198,ablation-analysis,slight but stable improvement,of,g p n a + aspect,slight but stable improvement of g p n a + aspect,0.5755000114440918
translation,280,198,ablation-analysis,g p n a + aspect,over,g p n a,g p n a + aspect over g p n a,0.6321401000022888
translation,280,198,ablation-analysis,performance drop,of,aita - aspect,performance drop of aita - aspect,0.5878641605377197
translation,280,198,ablation-analysis,aita - aspect,on,all the categories,aita - aspect on all the categories,0.5601285696029663
translation,280,179,baselines,sentence - based seq2seq generation model,trained with,user-written answerquestion pairs,sentence - based seq2seq generation model trained with user-written answerquestion pairs,0.7311436533927917
translation,280,183,baselines,a + aspect,has,aspect,a + aspect has aspect,0.5831716656684875
translation,280,183,baselines,"et al. , 2018 )",has,aspect,"et al. , 2018 ) has aspect",0.5089999437332153
translation,280,172,experiments,all text,including,"question , answer and review","all text including question , answer and review",0.7253474593162537
translation,280,172,experiments,all text,utilize,stanfordnlp,all text utilize stanfordnlp,0.630840539932251
translation,280,172,experiments,stanfordnlp,for,tokenizing,stanfordnlp for tokenizing,0.6293770670890808
translation,280,172,experiments,stanfordnlp,for,lower casing,stanfordnlp for lower casing,0.6674917340278625
translation,280,171,hyperparameters,fixed 300 dimension glove word embeddings,used as,basic word vectors,fixed 300 dimension glove word embeddings used as basic word vectors,0.5612033009529114
translation,280,171,hyperparameters,hyperparameters,has,fixed 300 dimension glove word embeddings,hyperparameters has fixed 300 dimension glove word embeddings,0.4852995276451111
translation,280,173,hyperparameters,dimension,of,aspect distribution,dimension of aspect distribution,0.5987192988395691
translation,280,173,hyperparameters,aspect distribution,set to,20,aspect distribution set to 20,0.6797041296958923
translation,280,173,hyperparameters,final loss function,set to,0.8,final loss function set to 0.8,0.6639376282691956
translation,280,173,hyperparameters,ranker,has,dimension,ranker has dimension,0.6108098030090332
translation,280,173,hyperparameters,hyperparameters,In,ranker,hyperparameters In ranker,0.5119226574897766
translation,280,174,hyperparameters,head number,set to,3,head number set to 3,0.5927662253379822
translation,280,174,hyperparameters,dimension,for,"q , k , v","dimension for q , k , v",0.6189368367195129
translation,280,174,hyperparameters,"q , k , v",is,300,"q , k , v is 300",0.6503875851631165
translation,280,174,hyperparameters,multi-head self-attention,has,head number,multi-head self-attention has head number,0.5561661124229431
translation,280,174,hyperparameters,multi-head self-attention,has,dimension,multi-head self-attention has dimension,0.5619238018989563
translation,280,174,hyperparameters,hyperparameters,In,multi-head self-attention,hyperparameters In multi-head self-attention,0.4879608750343323
translation,280,176,hyperparameters,hidden dimension,in,generator,hidden dimension in generator,0.555627703666687
translation,280,176,hyperparameters,generator,set to,200,generator set to 200,0.742721676826477
translation,280,176,hyperparameters,hyperparameters,has,hidden dimension,hyperparameters has hidden dimension,0.528815507888794
translation,280,177,hyperparameters,iterative learning algorithm,set,epoch number,iterative learning algorithm set epoch number,0.6659463047981262
translation,280,177,hyperparameters,iterative learning algorithm,set,updating instance number,iterative learning algorithm set updating instance number,0.6659407615661621
translation,280,177,hyperparameters,epoch number,to,10,epoch number to 10,0.6300293207168579
translation,280,177,hyperparameters,updating instance number,to,0.05 ? | s qa |.,updating instance number to 0.05 ? | s qa |.,0.5728321671485901
translation,280,177,hyperparameters,hyperparameters,In,iterative learning algorithm,hyperparameters In iterative learning algorithm,0.48136410117149353
translation,280,7,model,proper training instances,for,generation model,proper training instances for generation model,0.5847603678703308
translation,280,7,model,proper training instances,propose,iterative learning framework,proper training instances propose iterative learning framework,0.6368029117584229
translation,280,7,model,iterative learning framework,with,adaptive instance transfer and augmentation,iterative learning framework with adaptive instance transfer and augmentation,0.6076315641403198
translation,280,7,model,model,To obtain,proper training instances,model To obtain proper training instances,0.6044912338256836
translation,280,15,model,model,utilize,question generation ( qg ),model utilize question generation ( qg ),0.6504152417182922
translation,280,43,model,learning framework,with,adaptive instance transfer and augmentation,learning framework with adaptive instance transfer and augmentation,0.6113712787628174
translation,280,43,model,model,propose,learning framework,model propose learning framework,0.7003335356712341
translation,280,44,model,pre-trained generation model,based on,user-posed answer-question pairs,pre-trained generation model based on user-posed answer-question pairs,0.6142590641975403
translation,280,44,model,pre-trained generation model,utilized as,initial question generator,pre-trained generation model utilized as initial question generator,0.6013977527618408
translation,280,55,model,novel adaptive instance transfer and augmentation framework,for handling,data lacking challenge,novel adaptive instance transfer and augmentation framework for handling data lacking challenge,0.6954454183578491
translation,280,55,model,data lacking challenge,in,task,data lacking challenge in task,0.5265242457389832
translation,280,55,model,model,has,novel adaptive instance transfer and augmentation framework,model has novel adaptive instance transfer and augmentation framework,0.543703556060791
translation,280,56,model,review - based question generation,conducted on,e-commerce,review - based question generation conducted on e-commerce,0.5943871736526489
translation,280,56,model,model,has,review - based question generation,model has review - based question generation,0.536088228225708
translation,280,180,model,pointer network,incorporated in,seq2seq decoding,pointer network incorporated in seq2seq decoding,0.6823709011077881
translation,280,180,model,g p n a,has,pointer network,g p n a has pointer network,0.5884434580802917
translation,280,180,model,model,has,g p n a,model has g p n a,0.599093496799469
translation,280,181,model,review data,incorporated via,retrieval - based method,review data incorporated via retrieval - based method,0.7286595702171326
translation,280,181,model,g p n ar,has,review data,g p n ar has review data,0.5895030498504639
translation,280,181,model,model,has,g p n ar,model has g p n ar,0.5274884700775146
translation,280,197,model,aita,adapts and augments,qa set,aita adapts and augments qa set,0.7326101660728455
translation,280,197,model,qa set,to select,suitable review-question pairs,qa set to select suitable review-question pairs,0.7389931082725525
translation,280,197,model,suitable review-question pairs,considering,aspect and generation suitability,suitable review-question pairs considering aspect and generation suitability,0.6944217085838318
translation,280,197,model,suitable review-question pairs,resulting in,better generator,suitable review-question pairs resulting in better generator,0.7073577642440796
translation,280,197,model,model,has,aita,model has aita,0.6484912037849426
translation,280,191,results,aita,achieves,best performance,aita achieves best performance,0.709181010723114
translation,280,191,results,best performance,on,all product categories,best performance on all product categories,0.48191770911216736
translation,280,191,results,best performance,regarding,different evaluation metrics,best performance regarding different evaluation metrics,0.6040294766426086
translation,280,192,results,significant improvements,over,other models,significant improvements over other models,0.6477917432785034
translation,280,192,results,other models,demonstrate,our instance transfer and augmentation method,other models demonstrate our instance transfer and augmentation method,0.5714084506034851
translation,280,192,results,our instance transfer and augmentation method,reduce,inappropriate answerquestion pairs,our instance transfer and augmentation method reduce inappropriate answerquestion pairs,0.6980699300765991
translation,280,192,results,results,has,significant improvements,results has significant improvements,0.5530977845191956
translation,280,194,results,worse performance,than,ours,worse performance than ours,0.5696640610694885
translation,280,194,results,results,has,both g p n a and g p n a + aspect,results has both g p n a and g p n a + aspect,0.49417465925216675
translation,280,196,results,g p n ar,performs,much worse,g p n ar performs much worse,0.6170691251754761
translation,280,196,results,much worse,than,g p n a,much worse than g p n a,0.5816775560379028
translation,280,196,results,g p n a,proves,simple retrieval method,g p n a proves simple retrieval method,0.6698465943336487
translation,280,196,results,not effective,for,merging,not effective for merging,0.642784595489502
translation,280,196,results,results,has,g p n ar,results has g p n ar,0.5499117374420166
translation,280,206,results,average scores,shows,our framework,average scores shows our framework,0.6752167344093323
translation,280,206,results,our framework,achieves,best performance,our framework achieves best performance,0.6905233263969421
translation,280,206,results,best performance,regarding,all the metrics,best performance regarding all the metrics,0.5892792344093323
translation,280,206,results,results,has,average scores,results has average scores,0.5333548784255981
translation,280,207,results,significantly outperform,regarding,aspect and relevance,significantly outperform regarding aspect and relevance,0.6328307390213013
translation,280,207,results,aita and g p n a + aspect,has,significantly outperform,aita and g p n a + aspect has significantly outperform,0.5944101810455322
translation,280,207,results,significantly outperform,has,g p n a,significantly outperform has g p n a,0.5808123350143433
translation,280,207,results,results,incorporation of,implicit aspect information,results incorporation of implicit aspect information,0.5808354616165161
translation,280,208,results,g p n ar,with,simple retrieval method,g p n ar with simple retrieval method,0.6178141236305237
translation,280,208,results,simple retrieval method,for augmenting,training instances,simple retrieval method for augmenting training instances,0.6990841627120972
translation,280,208,results,results,has,g p n ar,results has g p n ar,0.5499117374420166
translation,281,69,baselines,soco,uses,elastic search ( es ),soco uses elastic search ( es ),0.6261453628540039
translation,281,69,baselines,elastic search ( es ),as,index backbone,elastic search ( es ) as index backbone,0.5314862132072449
translation,281,69,baselines,baselines,has,soco,baselines has soco,0.6050229072570801
translation,281,121,baselines,ms marco,has,"nguyen et al. , 2016 )","ms marco has nguyen et al. , 2016 )",0.5779443979263306
translation,281,68,experimental-setup,soco python package ( soco-core-python ),installed as,python package,soco python package ( soco-core-python ) installed as python package,0.6870284080505371
translation,281,68,experimental-setup,python package,by running,pip3,python package by running pip3,0.7002235054969788
translation,281,68,experimental-setup,pip3,install,soco-core-python,pip3 install soco-core-python,0.7364001870155334
translation,281,68,experimental-setup,experimental setup,has,soco python package ( soco-core-python ),experimental setup has soco python package ( soco-core-python ),0.5544295310974121
translation,281,77,experimental-setup,soco-core- python,to index,frames,soco-core- python to index frames,0.7321127653121948
translation,281,77,experimental-setup,soco-core- python,to index,query,soco-core- python to index query,0.6883887648582458
translation,281,77,experimental-setup,query,for,answers,query for answers,0.6583672761917114
translation,281,77,experimental-setup,query,via,restful api endpoint,query via restful api endpoint,0.6813572645187378
translation,281,77,experimental-setup,answers,via,restful api endpoint,answers via restful api endpoint,0.6704350709915161
translation,281,6,results,large improvement,over,classic search engine baseline,large improvement over classic search engine baseline,0.6724575161933899
translation,281,6,results,large improvement,on,several standard qa datasets,large improvement on several standard qa datasets,0.5031872987747192
translation,281,6,results,classic search engine baseline,on,several standard qa datasets,classic search engine baseline on several standard qa datasets,0.4691210687160492
translation,281,6,results,first natural language processing research qa dataset,via,community effort,first natural language processing research qa dataset via community effort,0.6016940474510193
translation,281,6,results,results,present,large improvement,results present large improvement,0.7257163524627686
translation,281,116,results,results,for,soco - qa performance,results for soco - qa performance,0.6163024306297302
translation,281,125,results,proposed soco - qa model,able to,significantly outperform,proposed soco - qa model able to significantly outperform,0.6842992305755615
translation,281,125,results,baseline bm25,on,all datasets,baseline bm25 on all datasets,0.4692346155643463
translation,281,125,results,significantly outperform,has,baseline bm25,significantly outperform has baseline bm25,0.5753545761108398
translation,281,125,results,results,has,proposed soco - qa model,results has proposed soco - qa model,0.6093622446060181
translation,281,127,results,striking 251 % and 253.6 % relative mrr improvement,on,nq and marco dataset,striking 251 % and 253.6 % relative mrr improvement on nq and marco dataset,0.5562669038772583
translation,281,127,results,results,shows,striking 251 % and 253.6 % relative mrr improvement,results shows striking 251 % and 253.6 % relative mrr improvement,0.6584994792938232
translation,281,128,results,soco,able to beat,bm25,soco able to beat bm25,0.7072279453277588
translation,281,128,results,bm25,on,squad and trivia dataset,bm25 on squad and trivia dataset,0.5183425545692444
translation,281,128,results,results,has,soco,results has soco,0.597816526889801
translation,281,132,results,bert model,is,best performing model,bert model is best performing model,0.5832182765007019
translation,281,132,results,bert model,consistently,best performing model,bert model consistently best performing model,0.7348459959030151
translation,281,132,results,best performing model,for,all distance pairs,best performing model for all distance pairs,0.564536452293396
translation,281,132,results,results,find that,bert model,results find that bert model,0.6439866423606873
translation,281,133,results,models,achieve,higher prediction performance,models achieve higher prediction performance,0.6286280155181885
translation,281,133,results,distance,has,models,distance has models,0.5986207127571106
translation,282,10,experiments,science questions,from,standardized tests,science questions from standardized tests,0.520112156867981
translation,282,10,experiments,science questions,separated into,easy set and a challenge set,science questions separated into easy set and a challenge set,0.6458301544189453
translation,282,10,experiments,standardized tests,separated into,easy set and a challenge set,standardized tests separated into easy set and a challenge set,0.6346675753593445
translation,282,126,experiments,approximately 200 questions,from,arc challenge set,approximately 200 questions from arc challenge set,0.574118971824646
translation,282,126,experiments,approximately 200 questions,shared by,ai2,approximately 200 questions shared by ai2,0.7191421389579773
translation,282,126,experiments,ai2,with,types of knowledge and reasoning,ai2 with types of knowledge and reasoning,0.61122727394104
translation,282,126,experiments,types of knowledge and reasoning,required to answer,respective questions,types of knowledge and reasoning required to answer respective questions,0.7466026544570923
translation,282,7,model,novel interface,human annotation of,science question - answer pairs,novel interface human annotation of science question - answer pairs,0.7819079756736755
translation,282,7,model,model,introduce,novel interface,model introduce novel interface,0.7038742303848267
translation,282,125,model,annotation instructions,for,knowledge and reasoning type labels,annotation instructions for knowledge and reasoning type labels,0.5794389843940735
translation,282,125,model,knowledge and reasoning type labels,used for,question analysis,knowledge and reasoning type labels used for question analysis,0.6966885328292847
translation,282,125,model,question analysis,for,standardized tests,question analysis for standardized tests,0.6157540082931519
translation,282,125,model,model,introduce,novel annotation interface,model introduce novel annotation interface,0.6441113948822021
translation,282,125,model,model,define,annotation instructions,model define annotation instructions,0.6666333079338074
translation,282,121,results,performance,increased,27 correctly answered questions,performance increased 27 correctly answered questions,0.6613916158676147
translation,282,121,results,performance,increased,42 % increase,performance increased 42 % increase,0.7130786180496216
translation,282,121,results,42 % increase,in,accuracy,42 % increase in accuracy,0.535575270652771
translation,282,121,results,annotated context,has,performance,annotated context has performance,0.5857705473899841
translation,282,121,results,27 correctly answered questions,has,42 % increase,27 correctly answered questions has 42 % increase,0.565905749797821
translation,282,121,results,results,With,annotated context,results With annotated context,0.6594075560569763
translation,283,50,baselines,strong abstractive baseline,by training,seq2seq model,strong abstractive baseline by training seq2seq model,0.7579765915870667
translation,283,50,baselines,seq2seq model,on,multiple tasks,seq2seq model on multiple tasks,0.5587751269340515
translation,283,50,baselines,multiple tasks,over,same data,multiple tasks over same data,0.713438093662262
translation,283,190,experimental-setup,vocabulary,apply,byte-pair encoding,vocabulary apply byte-pair encoding,0.6449638605117798
translation,283,190,experimental-setup,byte-pair encoding,to generate,40 k codes,byte-pair encoding to generate 40 k codes,0.6849112510681152
translation,283,190,experimental-setup,40 k codes,applied to,all datasets,40 k codes applied to all datasets,0.6303099393844604
translation,283,190,experimental-setup,byte-pair encoding,has,", 2016 )","byte-pair encoding has , 2016 )",0.5639854669570923
translation,283,190,experimental-setup,experimental setup,To reduce,vocabulary,experimental setup To reduce vocabulary,0.6641970276832581
translation,283,191,experimental-setup,vocabulary,of,"52,863 tokens","vocabulary of 52,863 tokens",0.5758816599845886
translation,283,191,experimental-setup,vocabulary,for,answer generation,vocabulary for answer generation,0.5922349691390991
translation,283,191,experimental-setup,"52,863 tokens",for,answer generation,"52,863 tokens for answer generation",0.5595085620880127
translation,283,191,experimental-setup,experimental setup,model,vocabulary,experimental setup model vocabulary,0.701456606388092
translation,283,192,experimental-setup,transformer implementation,of,fairseq - py,transformer implementation of fairseq - py,0.5294058918952942
translation,283,192,experimental-setup,transformer implementation,train with,big architecture,transformer implementation train with big architecture,0.6848857998847961
translation,283,192,experimental-setup,fairseq - py,has,),fairseq - py has ),0.6781717538833618
translation,283,192,experimental-setup,experimental setup,use,transformer implementation,experimental setup use transformer implementation,0.5939902067184448
translation,283,192,experimental-setup,experimental setup,train with,big architecture,experimental setup train with big architecture,0.6713075041770935
translation,283,193,experimental-setup,data length,train with,large batch size,data length train with large batch size,0.7103881239891052
translation,283,193,experimental-setup,large batch size,by delaying,gradient updates,large batch size by delaying gradient updates,0.6652703285217285
translation,283,193,experimental-setup,gradient updates,until,sufficient number of examples have been seen,gradient updates until sufficient number of examples have been seen,0.6892432570457458
translation,283,193,experimental-setup,experimental setup,Given,data length,experimental setup Given data length,0.6927714943885803
translation,283,195,experimental-setup,repeated trigrams,to prevent,repetition,repeated trigrams to prevent repetition,0.6355932950973511
translation,283,195,experimental-setup,experimental setup,disallow,repeated trigrams,experimental setup disallow repeated trigrams,0.7656841278076172
translation,283,5,experiments,270 k threads,from,reddit forum,270 k threads from reddit forum,0.6266167163848877
translation,283,5,experiments,reddit forum,Explain Like I'm Five,( eli5 ),reddit forum Explain Like I'm Five ( eli5 ),0.7222595810890198
translation,283,5,experiments,( eli5 ),where,online community,( eli5 ) where online community,0.6316623091697693
translation,283,5,experiments,online community,provides,answers,online community provides answers,0.6953328251838684
translation,283,5,experiments,answers,to,questions,answers to questions,0.5806413292884827
translation,283,5,experiments,comprehensible,by,five year olds,comprehensible by five year olds,0.6058284044265747
translation,283,194,experiments,abstractive models,using,beam search,abstractive models using beam search,0.6158685684204102
translation,283,194,experiments,beam search,with,beam 5,beam search with beam 5,0.7028772234916687
translation,283,210,experiments,rouge - 20 %,shows,much starker contrast,rouge - 20 % shows much starker contrast,0.7076890468597412
translation,283,210,experiments,much starker contrast,between,language modeling and seq2seq,much starker contrast between language modeling and seq2seq,0.6710412502288818
translation,283,210,experiments,much starker contrast,between,standard seq2seq and multi-task training,much starker contrast between standard seq2seq and multi-task training,0.6288585066795349
translation,283,210,experiments,much starker contrast,between,standard seq2seq and multi-task training,much starker contrast between standard seq2seq and multi-task training,0.6288585066795349
translation,283,203,results,abstractive methods,achieve,higher rouge,abstractive methods achieve higher rouge,0.645791232585907
translation,283,203,results,results,has,abstractive methods,results has abstractive methods,0.44091805815696716
translation,283,212,results,language model,still,better,language model still better,0.6952097415924072
translation,283,212,results,better,at,fill -1,better at fill -1,0.5834335684776306
translation,283,217,results,multi-task model,performs,similarly,multi-task model performs similarly,0.6547650694847107
translation,283,217,results,similarly,to,extractive model,similarly to extractive model,0.5677317380905151
translation,283,217,results,results,has,multi-task model,results has multi-task model,0.546853244304657
translation,283,221,results,large gap,between,human performance and all models,large gap between human performance and all models,0.664742648601532
translation,283,221,results,results,In,answer accuracy,results In answer accuracy,0.5063225626945496
translation,283,228,results,multi-task model,better at generating,relevant answers,multi-task model better at generating relevant answers,0.7114391326904297
translation,283,228,results,84 % relevancy,compared to,68 %,84 % relevancy compared to 68 %,0.6655095219612122
translation,283,228,results,relevant answers,has,84 % relevancy,relevant answers has 84 % relevancy,0.5372717380523682
translation,283,228,results,results,has,multi-task model,results has multi-task model,0.546853244304657
translation,283,230,results,reference answer,preferred over,output,reference answer preferred over output,0.7502182722091675
translation,283,230,results,output,of,all of our trained models,output of all of our trained models,0.5798866748809814
translation,283,230,results,all of our trained models,in,at least 85.5 % of cases,all of our trained models in at least 85.5 % of cases,0.5392882227897644
translation,283,230,results,results,has,reference answer,results has reference answer,0.5401437878608704
translation,283,237,results,full answer,discriminates,reasonably well,full answer discriminates reasonably well,0.7010341286659241
translation,283,237,results,reasonably well,between,models,reasonably well between models,0.6949804425239563
translation,283,237,results,models,with,same general architecture,models with same general architecture,0.6511715054512024
translation,283,237,results,abstractive system,against,extractive one,abstractive system against extractive one,0.6857380867004395
translation,283,237,results,results,has,full answer,results has full answer,0.5778666138648987
translation,284,138,ablation-analysis,bidaf,demonstrated,weakest resilience,bidaf demonstrated weakest resilience,0.6793400049209595
translation,284,138,ablation-analysis,weakest resilience,to,deliberate attack,weakest resilience to deliberate attack,0.5490344762802124
translation,284,138,ablation-analysis,deliberate attack,with,decrease,deliberate attack with decrease,0.6843697428703308
translation,284,138,ablation-analysis,decrease,of,43.25 f1,decrease of 43.25 f1,0.5778976678848267
translation,284,138,ablation-analysis,decrease,of,32.08 f1,decrease of 32.08 f1,0.5468230247497559
translation,284,138,ablation-analysis,bert and drqa,suffered,decrease,bert and drqa suffered decrease,0.7378261089324951
translation,284,138,ablation-analysis,decrease,of,26.13 f1,decrease of 26.13 f1,0.5793644189834595
translation,284,138,ablation-analysis,decrease,of,32.08 f1,decrease of 32.08 f1,0.5468230247497559
translation,284,138,ablation-analysis,ablation analysis,has,bidaf,ablation analysis has bidaf,0.5121188759803772
translation,284,153,ablation-analysis,re-training,causes,only a negligible drop,re-training causes only a negligible drop,0.622988760471344
translation,284,153,ablation-analysis,only a negligible drop,to,performance,only a negligible drop to performance,0.5989901423454285
translation,284,153,ablation-analysis,performance,of,qa models,performance of qa models,0.6271283626556396
translation,284,153,ablation-analysis,qa models,on,original development set,qa models on original development set,0.5595176219940186
translation,284,153,ablation-analysis,ablation analysis,has,re-training,ablation analysis has re-training,0.5206686854362488
translation,284,51,baselines,transformer model,from,vaswani et al . ( 2017 ),transformer model from vaswani et al . ( 2017 ),0.53044193983078
translation,284,51,baselines,transformer model,from,encoder-decoder architecture,transformer model from encoder-decoder architecture,0.5801280736923218
translation,284,51,baselines,transformer model,which is,encoder-decoder architecture,transformer model which is encoder-decoder architecture,0.6293697953224182
translation,284,51,baselines,encoder-decoder architecture,relies mainly on,self-attention mechanism,encoder-decoder architecture relies mainly on self-attention mechanism,0.7034038305282593
translation,284,52,model,decoder,using,copy mechanism,decoder using copy mechanism,0.735127866268158
translation,284,52,model,copy mechanism,allows,tokens,copy mechanism allows tokens,0.7530638575553894
translation,284,52,model,tokens,to be copied from,source question,tokens to be copied from source question,0.5300672650337219
translation,284,52,model,model,extend,decoder,model extend decoder,0.7774487137794495
translation,284,53,model,probability distribution,of,output vocabulary,probability distribution of output vocabulary,0.6107104420661926
translation,284,53,model,output vocabulary,to include,tokens,output vocabulary to include tokens,0.6353821158409119
translation,284,53,model,tokens,from,source question,tokens from source question,0.5216626524925232
translation,284,53,model,model,augmenting,probability distribution,model augmenting probability distribution,0.6530081033706665
translation,284,40,results,outperforms,have,worse performance,outperforms have worse performance,0.6168345212936401
translation,284,40,results,human,have,worse performance,human have worse performance,0.5843263268470764
translation,284,40,results,worse performance,on,nonadversarial paraphrased test set,worse performance on nonadversarial paraphrased test set,0.512157142162323
translation,284,40,results,outperforms,has,human,outperforms has human,0.6239288449287415
translation,284,133,results,all three models,suffer,significant drop,all three models suffer significant drop,0.6443643569946289
translation,284,133,results,significant drop,in,performance,significant drop in performance,0.5765949487686157
translation,284,136,results,performance,of,qa models,performance of qa models,0.6271283626556396
translation,284,136,results,qa models,on,original and paraphrased questions,qa models on original and paraphrased questions,0.5352417230606079
translation,284,136,results,qa models,for,adversarial paraphrased test set,qa models for adversarial paraphrased test set,0.5730717778205872
translation,284,136,results,original and paraphrased questions,for,adversarial paraphrased test set,original and paraphrased questions for adversarial paraphrased test set,0.5750967860221863
translation,284,137,results,adversarial paraphrased test set,able to exploit,reliance,adversarial paraphrased test set able to exploit reliance,0.6791361570358276
translation,284,137,results,reliance,of,qa models,reliance of qa models,0.6424009203910828
translation,284,137,results,reliance,to cause,drastic decrease,reliance to cause drastic decrease,0.7604342103004456
translation,284,137,results,qa models,on,string matching,qa models on string matching,0.5222516059875488
translation,284,137,results,drastic decrease,in,models ' performance,drastic decrease in models ' performance,0.5362391471862793
translation,284,137,results,results,has,adversarial paraphrased test set,results has adversarial paraphrased test set,0.5308164954185486
translation,284,152,results,augmented training dataset,is,noisy,augmented training dataset is noisy,0.5687883496284485
translation,284,152,results,all qa models,show,improvement,all qa models show improvement,0.7053422331809998
translation,284,152,results,improvement,on,paraphrased test set,improvement on paraphrased test set,0.5416362881660461
translation,284,152,results,improvement,after,retraining,improvement after retraining,0.6972728967666626
translation,284,165,results,re-training,leads to,significant improvement,re-training leads to significant improvement,0.6400481462478638
translation,284,165,results,significant improvement,in,performance,significant improvement in performance,0.5336322784423828
translation,284,165,results,performance,of,bert and bidaf,performance of bert and bidaf,0.6507631540298462
translation,284,165,results,performance,on,adversarial paraphrased test set,performance on adversarial paraphrased test set,0.50101637840271
translation,284,165,results,bert and bidaf,on,adversarial paraphrased test set,bert and bidaf on adversarial paraphrased test set,0.5351168513298035
translation,284,165,results,results,see that,re-training,results see that re-training,0.6633239984512329
translation,284,166,results,re-training,able to improve,drqa 's performance,re-training able to improve drqa 's performance,0.7293841242790222
translation,284,166,results,drqa 's performance,has,slightly,drqa 's performance has slightly,0.5820934176445007
translation,284,166,results,results,has,re-training,results has re-training,0.5483691096305847
translation,284,167,results,re-training,causes,slight decrease,re-training causes slight decrease,0.7147315740585327
translation,284,167,results,slight decrease,in,performance,slight decrease in performance,0.5411154627799988
translation,284,167,results,performance,on,original squad development set,performance on original squad development set,0.5336534976959229
translation,284,167,results,results,has,re-training,results has re-training,0.5483691096305847
translation,285,7,experiments,qa,with respect to,openbookqa dataset,qa with respect to openbookqa dataset,0.6372905969619751
translation,285,7,experiments,state of the art language models,with,abductive information retrieval ( ir ),state of the art language models with abductive information retrieval ( ir ),0.5822981595993042
translation,285,7,experiments,state of the art language models,with,information gain based re-ranking,state of the art language models with information gain based re-ranking,0.5799421072006226
translation,285,7,experiments,state of the art language models,with,passage selection,state of the art language models with passage selection,0.5506635308265686
translation,285,7,experiments,state of the art language models,with,weighted scoring,state of the art language models with weighted scoring,0.5813400745391846
translation,285,7,experiments,weighted scoring,to achieve,72.0 % accuracy,weighted scoring to achieve 72.0 % accuracy,0.6212784051895142
translation,285,7,experiments,11.6 % improvement,over,current state of the art,11.6 % improvement over current state of the art,0.6060072779655457
translation,285,7,experiments,72.0 % accuracy,has,11.6 % improvement,72.0 % accuracy has 11.6 % improvement,0.5703675746917725
translation,285,178,model,model,has,passage selection,model has passage selection,0.5567793846130371
translation,285,36,results,knowledge selection and retrieval techniques,achieves,accuracy,knowledge selection and retrieval techniques achieves accuracy,0.663510262966156
translation,285,36,results,accuracy,of,72 %,accuracy of 72 %,0.611503541469574
translation,285,36,results,72 %,with,margin,72 % with margin,0.7237178683280945
translation,285,36,results,margin,of,11.6 %,margin of 11.6 %,0.5804978609085083
translation,285,36,results,11.6 %,on,current state of the art,11.6 % on current state of the art,0.49762776494026184
translation,285,36,results,results,Our,knowledge selection and retrieval techniques,results Our knowledge selection and retrieval techniques,0.6099146008491516
translation,285,36,results,results,has,knowledge selection and retrieval techniques,results has knowledge selection and retrieval techniques,0.5482556819915771
translation,285,168,results,best performance,of,bert qa model,best performance of bert qa model,0.6516943573951721
translation,285,168,results,bert qa model,seen to be,66.2 %,bert qa model seen to be 66.2 %,0.6365998983383179
translation,285,168,results,66.2 %,using,openbook facts,66.2 % using openbook facts,0.646355152130127
translation,285,168,results,results,has,best performance,results has best performance,0.5759831070899963
translation,285,172,results,trained models,performed,poorly,trained models performed poorly,0.295133501291275
translation,285,172,results,poorly,compared to,baselines,poorly compared to baselines,0.7315481901168823
translation,285,172,results,results,noticed that,trained models,results noticed that trained models,0.6502577662467957
translation,285,173,results,word symmetric difference model,performs,better,word symmetric difference model performs better,0.6077775955200195
translation,285,173,results,better,indicating,abductive,better indicating abductive,0.731144905090332
translation,285,173,results,results,has,word symmetric difference model,results has word symmetric difference model,0.5515434741973877
translation,285,177,results,question answering,shows,incremental improvement,question answering shows incremental improvement,0.6310561299324036
translation,285,177,results,incremental improvement,on,baselines,incremental improvement on baselines,0.543975830078125
translation,285,177,results,incremental improvement,inclusion of,carefully selected knowledge,incremental improvement inclusion of carefully selected knowledge,0.5967145562171936
translation,285,177,results,results,has,question answering,results has question answering,0.5357730984687805
translation,286,18,ablation-analysis,ablation analysis,has,question difficulty estimation,ablation analysis has question difficulty estimation,0.5413655638694763
translation,286,5,model,competition - based model,for estimating,question difficulty,competition - based model for estimating question difficulty,0.7345753312110901
translation,286,5,model,question difficulty,by leveraging,pairwise comparisons,question difficulty by leveraging pairwise comparisons,0.6761329174041748
translation,286,5,model,pairwise comparisons,between,questions and users,pairwise comparisons between questions and users,0.6449558734893799
translation,286,5,model,model,propose,competition - based model,model propose competition - based model,0.653414249420166
translation,286,25,model,competition - based approach,jointly models,question difficulty,competition - based approach jointly models question difficulty,0.7519237995147705
translation,286,25,model,competition - based approach,jointly models,user expertise level,competition - based approach jointly models user expertise level,0.7443901300430298
translation,286,25,model,model,propose,competition - based approach,model propose competition - based approach,0.7006199359893799
translation,286,78,results,significantly outperformed,on,both data sets,significantly outperformed on both data sets,0.5553302764892578
translation,286,78,results,pagerank - based approach,on,both data sets,pagerank - based approach on both data sets,0.5415800213813782
translation,286,78,results,pagerank - based approach,achieved,similar performance,pagerank - based approach achieved similar performance,0.6997905373573303
translation,286,78,results,similar performance,as,randomly guessing,similar performance as randomly guessing,0.5638349056243896
translation,286,78,results,proposed competition - based approach,has,significantly outperformed,proposed competition - based approach has significantly outperformed,0.6149048209190369
translation,286,78,results,significantly outperformed,has,pagerank - based approach,significantly outperformed has pagerank - based approach,0.6082460880279541
translation,286,78,results,significantly outperformed,has,pagerank - based approach,significantly outperformed has pagerank - based approach,0.6082460880279541
translation,286,78,results,results,see that,proposed competition - based approach,results see that proposed competition - based approach,0.6824594736099243
translation,287,19,baselines,two adaptive computation methods,for,odqa,two adaptive computation methods for odqa,0.6350526809692383
translation,287,19,baselines,two adaptive computation methods,for,skylinebuilder,two adaptive computation methods for skylinebuilder,0.5477229952812195
translation,287,19,baselines,baselines,introduce,two adaptive computation methods,baselines introduce two adaptive computation methods,0.6326578259468079
translation,287,23,experiments,our experiments,on,squad - open dataset,our experiments on squad - open dataset,0.5313233733177185
translation,287,23,experiments,squad - open dataset,show,our methods,squad - open dataset show our methods,0.5933582186698914
translation,287,23,experiments,our methods,are,very effective,our methods are very effective,0.5606176853179932
translation,287,23,experiments,very effective,at reducing,computational footprint,very effective at reducing computational footprint,0.67746901512146
translation,287,23,experiments,computational footprint,of,odqa models,computational footprint of odqa models,0.6026395559310913
translation,287,8,model,technique,operating on,individual passages,technique operating on individual passages,0.7390133738517761
translation,287,8,model,individual passages,in isolation,per-layer estimation,individual passages in isolation per-layer estimation,0.7477827072143555
translation,287,8,model,individual passages,relies on,anytime prediction,individual passages relies on anytime prediction,0.7436909675598145
translation,287,8,model,per-layer estimation,of,early exit probability,per-layer estimation of early exit probability,0.5723016858100891
translation,287,8,model,model,introduce,technique,model introduce technique,0.711319088935852
translation,287,9,model,passage,to allocate computation,each step,passage to allocate computation each step,0.8418881297111511
translation,287,9,model,resource allocation policy,trained via,reinforcement learning,resource allocation policy trained via reinforcement learning,0.6569544076919556
translation,287,9,model,model,introduce,sky -linebuilder,model introduce sky -linebuilder,0.6541644334793091
translation,287,21,model,construction,of,multiple towers,construction of multiple towers,0.6037299633026123
translation,287,21,model,construction,introduce,global method,construction introduce global method,0.6542592644691467
translation,287,21,model,multiple towers,in,parallel,multiple towers in parallel,0.5610808730125427
translation,287,21,model,policy,to decide,which tower,policy to decide which tower,0.7656635046005249
translation,287,21,model,which tower,extend,one more layer next,which tower extend one more layer next,0.7512305378913879
translation,287,21,model,global method,has,skylinebuilder,global method has skylinebuilder,0.5277695059776306
translation,287,21,model,multiple towers,has,one layer at a time,multiple towers has one layer at a time,0.6120067834854126
translation,287,21,model,model,for coordinating,construction,model for coordinating construction,0.6636333465576172
translation,287,21,model,model,introduce,global method,model introduce global method,0.6732155680656433
translation,287,22,model,single transformer towers,constructs,skyline of towers,single transformer towers constructs skyline of towers,0.6462005376815796
translation,287,22,model,skyline of towers,with,different heights,skyline of towers with different heights,0.606664776802063
translation,287,22,model,model,Rather than building,single transformer towers,model Rather than building single transformer towers,0.6353852152824402
translation,287,22,model,model,constructs,skyline of towers,model constructs skyline of towers,0.6721677780151367
translation,287,20,results,towerbuilder,builds,tower,towerbuilder builds tower,0.7029554843902588
translation,287,20,results,towerbuilder,composition of,transformer layers,towerbuilder composition of transformer layers,0.6923994421958923
translation,287,20,results,tower,composition of,transformer layers,tower composition of transformer layers,0.7026434540748596
translation,287,20,results,transformer layers,on,single passage,transformer layers on single passage,0.5750333070755005
translation,287,20,results,early stopping condition,is,met-we,early stopping condition is met-we,0.6280171275138855
translation,287,20,results,reading,has,retrieved passages,reading has retrieved passages,0.625119686126709
translation,287,20,results,results,has,towerbuilder,results has towerbuilder,0.53775554895401
translation,287,24,results,skylinebuilder,retains,95 %,skylinebuilder retains 95 %,0.6910566091537476
translation,287,24,results,95 %,of,accuracy,95 % of accuracy,0.6150429248809814
translation,287,24,results,accuracy,of,24 - layer model,accuracy of 24 - layer model,0.6193423867225647
translation,287,24,results,24 - layer model,using,only 5.6 layers on average,24 - layer model using only 5.6 layers on average,0.6492244601249695
translation,287,24,results,results,find that,skylinebuilder,results find that skylinebuilder,0.6012022495269775
translation,288,17,baselines,quint,gives,step-by-step explanations,quint gives step-by-step explanations,0.641068160533905
translation,288,17,baselines,step-by-step explanations,derives,answers,step-by-step explanations derives answers,0.6716522574424744
translation,288,17,baselines,answers,for,questions,answers for questions,0.628709077835083
translation,288,17,baselines,baselines,demonstrate,quint,baselines demonstrate quint,0.6435152292251587
translation,288,5,experiments,quint,automatically learns,role-aligned utterance -query templates,quint automatically learns role-aligned utterance -query templates,0.7340731024742126
translation,288,5,experiments,role-aligned utterance -query templates,from,user questions,role-aligned utterance -query templates from user questions,0.5832322835922241
translation,288,5,experiments,user questions,paired with,answers,user questions paired with answers,0.6632443070411682
translation,288,33,model,embedding,map,"questions , kb entities , and subgraphs","embedding map questions , kb entities , and subgraphs",0.6692125797271729
translation,288,33,model,shared space,for,kb - qa,shared space for kb - qa,0.6664137244224548
translation,288,33,model,shared space,without explicitly generating,semantic representation,shared space without explicitly generating semantic representation,0.7398338913917542
translation,288,33,model,kb - qa,without explicitly generating,semantic representation,kb - qa without explicitly generating semantic representation,0.7017465233802795
translation,288,33,model,model,has,embedding,model has embedding,0.587139904499054
translation,288,37,model,live online kb - qa system,visualizes,derivation steps,live online kb - qa system visualizes derivation steps,0.6953833699226379
translation,288,37,model,live online kb - qa system,takes,first steps,live online kb - qa system takes first steps,0.685710072517395
translation,288,37,model,derivation steps,for generating,answer,derivation steps for generating answer,0.6931056380271912
translation,288,37,model,first steps,towards,explainable question - answering,first steps towards explainable question - answering,0.6605703234672546
translation,289,9,model,method,exploits,commonalities in experiences,method exploits commonalities in experiences,0.6236355900764465
translation,289,9,model,commonalities in experiences,to automatically extract,pairs,commonalities in experiences to automatically extract pairs,0.7310580015182495
translation,289,9,model,of questions,appropriate candidates for,cloze task,of questions appropriate candidates for cloze task,0.5775254368782043
translation,289,9,model,commonalities in experiences,has,people share online,commonalities in experiences has people share online,0.5218038558959961
translation,289,9,model,pairs,has,of questions,pairs has of questions,0.6261269450187683
translation,289,9,model,model,devise,method,model devise method,0.7383982539176941
translation,290,147,experimental-setup,number of epochs,set to,3,number of epochs set to 3,0.7306022047996521
translation,290,147,experimental-setup,3,with,learning rate,3 with learning rate,0.6402537822723389
translation,290,147,experimental-setup,learning rate,equal to,3.0 ? 10 ?5,learning rate equal to 3.0 ? 10 ?5,0.6873602271080017
translation,290,147,experimental-setup,experimental setup,has,number of epochs,experimental setup has number of epochs,0.5322787761688232
translation,290,148,experimental-setup,learning rate,scheduled according to,warm - up linear scheduler,learning rate scheduled according to warm - up linear scheduler,0.7062532305717468
translation,290,148,experimental-setup,warm - up linear scheduler,where,percentage ratio,warm - up linear scheduler where percentage ratio,0.6041089296340942
translation,290,148,experimental-setup,percentage ratio,for,warm - up,percentage ratio for warm - up,0.6692084670066833
translation,290,148,experimental-setup,warm - up,consistently set to,6 %,warm - up consistently set to 6 %,0.6936821341514587
translation,290,148,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,290,149,experimental-setup,batch size,kept,constant,batch size kept constant,0.728512167930603
translation,290,149,experimental-setup,batch size,equal to,8,batch size equal to 8,0.7336986660957336
translation,290,149,experimental-setup,batch size,equal to,4,batch size equal to 4,0.7330234050750732
translation,290,149,experimental-setup,constant,across,training,constant across training,0.7475759983062744
translation,290,149,experimental-setup,8,for,base models,8 for base models,0.6108140349388123
translation,290,149,experimental-setup,4,for,large ones,4 for large ones,0.6054236888885498
translation,290,149,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,290,150,experimental-setup,optimizer,is,adamw,optimizer is adamw,0.49692919850349426
translation,290,150,experimental-setup,adamw,with,default parameters,adamw with default parameters,0.5585182905197144
translation,290,150,experimental-setup,experimental setup,has,optimizer,experimental setup has optimizer,0.5528271794319153
translation,290,151,experimental-setup,experiments,carried out,"huggingface transformers library ( wolf et al. , 2019 )","experiments carried out huggingface transformers library ( wolf et al. , 2019 )",0.5645248889923096
translation,290,151,experimental-setup,experiments,with,"huggingface transformers library ( wolf et al. , 2019 )","experiments with huggingface transformers library ( wolf et al. , 2019 )",0.6055783033370972
translation,290,151,experimental-setup,"huggingface transformers library ( wolf et al. , 2019 )",on,single v100 gpu,"huggingface transformers library ( wolf et al. , 2019 ) on single v100 gpu",0.49377211928367615
translation,290,151,experimental-setup,experimental setup,has,experiments,experimental setup has experiments,0.5502888560295105
translation,290,181,experiments,squad1.1,reaches,81.4 % f1,squad1.1 reaches 81.4 % f1,0.6728512048721313
translation,290,181,experiments,squad1.1,reaches,68.4 % em,squad1.1 reaches 68.4 % em,0.7092153429985046
translation,290,181,experiments,squad1.1,on,fquad1.1 dev set,squad1.1 on fquad1.1 dev set,0.5709375739097595
translation,290,181,experiments,68.4 % em,on,fquad1.1 dev set,68.4 % em on fquad1.1 dev set,0.5712250471115112
translation,290,176,results,native and the translated dataset,into,augmented dataset,native and the translated dataset into augmented dataset,0.5740207433700562
translation,290,176,results,native and the translated dataset,call,augmented dataset,native and the translated dataset call augmented dataset,0.5793182849884033
translation,290,176,results,native and the translated dataset,observe,significant performance improvement,native and the translated dataset observe significant performance improvement,0.5766950249671936
translation,290,176,results,results,merge,native and the translated dataset,results merge native and the translated dataset,0.6519320607185364
translation,290,182,results,xlm -r large,reaches,88.8 % f1,xlm -r large reaches 88.8 % f1,0.6862691640853882
translation,290,182,results,xlm -r large,reaches,79.5 %,xlm -r large reaches 79.5 %,0.6957886219024658
translation,290,182,results,xlm -r large,reaches,73.2 % em,xlm -r large reaches 73.2 % em,0.7019013166427612
translation,290,182,results,79.5 %,on,squad1.1 dev set,79.5 % on squad1.1 dev set,0.5453312993049622
translation,290,182,results,73.2 % em,on,fquad1.1 dev set,73.2 % em on fquad1.1 dev set,0.5701741576194763
translation,290,182,results,73.2 % em,trained on,squad1.1,73.2 % em trained on squad1.1,0.7058424353599548
translation,290,182,results,results,find that,xlm -r large,results find that xlm -r large,0.6750255227088928
translation,290,183,results,models,perform,very well,models perform very well,0.6222718358039856
translation,290,183,results,models,trained on,native french and native english datasets,models trained on native french and native english datasets,0.69863361120224
translation,290,183,results,very well,compared to,results,very well compared to results,0.6708822250366211
translation,290,183,results,results,trained on,native french and native english datasets,results trained on native french and native english datasets,0.6481326222419739
translation,290,183,results,results,show,models,results show models,0.6068246960639954
translation,290,188,results,camembert base and camembert large models,reach,very promising baseline,camembert base and camembert large models reach very promising baseline,0.6622516512870789
translation,290,188,results,human performance,across,development and test datasets,human performance across development and test datasets,0.6811513900756836
translation,290,188,results,consistently,across,development and test datasets,consistently across development and test datasets,0.7152889966964722
translation,290,188,results,large model,has,outperforms,large model has outperforms,0.6229472160339355
translation,290,188,results,outperforms,has,human performance,outperforms has human performance,0.5948210954666138
translation,290,188,results,human performance,has,consistently,human performance has consistently,0.602949857711792
translation,290,188,results,results,has,camembert base and camembert large models,results has camembert base and camembert large models,0.5551894903182983
translation,290,189,results,comparable model sizes,find that,monolingual models,comparable model sizes find that monolingual models,0.6183594465255737
translation,290,189,results,multilingual models,on,reading comprehension task,multilingual models on reading comprehension task,0.5128061771392822
translation,290,189,results,monolingual models,has,outperform,monolingual models has outperform,0.5937855839729309
translation,290,189,results,outperform,has,multilingual models,outperform has multilingual models,0.5663485527038574
translation,290,189,results,results,For,comparable model sizes,results For comparable model sizes,0.5870972275733948
translation,290,191,results,xlm -r large,performs,consistently better,xlm -r large performs consistently better,0.6457818150520325
translation,290,191,results,consistently better,than,monolingual model camembert base,consistently better than monolingual model camembert base,0.5668055415153503
translation,290,191,results,monolingual model camembert base,on,development and test sets,monolingual model camembert base on development and test sets,0.5371967554092407
translation,290,191,results,development and test sets,of,fquad1.1,development and test sets of fquad1.1,0.5871544480323792
translation,290,191,results,results,find that,xlm -r large,results find that xlm -r large,0.6750255227088928
translation,290,192,results,xlm -r large,reaches,79 % em,xlm -r large reaches 79 % em,0.7480596899986267
translation,290,192,results,79 % em,on,fquadtest,79 % em on fquadtest,0.5949124693870544
translation,290,192,results,fquadtest,better than,human performance,fquadtest better than human performance,0.7296637892723083
translation,290,192,results,results,highlight,xlm -r large,results highlight xlm -r large,0.6462080478668213
translation,290,202,results,native french training data,with,translated samples,native french training data with translated samples,0.625998854637146
translation,290,202,results,performances,on,native evaluation set,performances on native evaluation set,0.5458682179450989
translation,290,202,results,results,enriching,native french training data,results enriching native french training data,0.5589278340339661
translation,290,203,results,significant gap,between,native french and the french translated data,significant gap between native french and the french translated data,0.6378528475761414
translation,290,203,results,native french and the french translated data,in terms on,quality,native french and the french translated data in terms on quality,0.6464958786964417
translation,291,6,baselines,straightforward bert employment,reveals,defects,straightforward bert employment reveals defects,0.7217087745666504
translation,291,29,baselines,straightforward bert employment,reveals,defects,straightforward bert employment reveals defects,0.7217087745666504
translation,291,4,model,pre-trained bert language model,to tackle,question generation tasks,pre-trained bert language model to tackle question generation tasks,0.5686314702033997
translation,291,4,model,model,employment of,pre-trained bert language model,model employment of pre-trained bert language model,0.563887894153595
translation,291,5,model,three neural architectures,built on top of,bert,three neural architectures built on top of bert,0.6930902004241943
translation,291,5,model,bert,for,question generation tasks,bert for question generation tasks,0.5944377779960632
translation,291,5,model,model,introduce,three neural architectures,model introduce three neural architectures,0.6735557913780212
translation,291,7,model,our bert employment,into,sequential manner,our bert employment into sequential manner,0.683497428894043
translation,291,7,model,sequential manner,for taking,information,sequential manner for taking information,0.7637082934379578
translation,291,7,model,information,from,previous decoded results,information from previous decoded results,0.5179957151412964
translation,291,7,model,model,by restructuring,our bert employment,model by restructuring our bert employment,0.7494003176689148
translation,291,27,model,pre-trained bert language model,to tackle,question generation tasks,pre-trained bert language model to tackle question generation tasks,0.5686314702033997
translation,291,27,model,model,employment of,pre-trained bert language model,model employment of pre-trained bert language model,0.563887894153595
translation,291,28,model,three neural architectures,built on top of,bert,three neural architectures built on top of bert,0.6930902004241943
translation,291,28,model,bert,for,question generation tasks,bert for question generation tasks,0.5944377779960632
translation,291,28,model,model,introduce,three neural architectures,model introduce three neural architectures,0.6735557913780212
translation,291,32,model,sequential question generation model,based on,bert,sequential question generation model based on bert,0.6427417993545532
translation,291,32,model,bert - sqg ( bert - sequential question generation ),for taking,information,bert - sqg ( bert - sequential question generation ) for taking information,0.7612309455871582
translation,291,32,model,information,from,previous decoded results,information from previous decoded results,0.5179957151412964
translation,291,32,model,model,propose,sequential question generation model,model propose sequential question generation model,0.6759948134422302
translation,291,38,model,two sequential question generation models,based on,bert,two sequential question generation models based on bert,0.6530174016952515
translation,291,38,model,model,propose,two sequential question generation models,model propose two sequential question generation models,0.6666192412376404
translation,291,39,model,simple but effective input encoding scheme,inserts,special highlighting tokens [ hl ],simple but effective input encoding scheme inserts special highlighting tokens [ hl ],0.6223686933517456
translation,291,39,model,special highlighting tokens [ hl ],before and after,given answer span,special highlighting tokens [ hl ] before and after given answer span,0.5278185606002808
translation,291,39,model,given answer span,to address,ambiguity issue,given answer span to address ambiguity issue,0.6678889989852905
translation,291,39,model,ambiguity issue,when,answer phase,ambiguity issue when answer phase,0.678337812423706
translation,291,24,results,results,employing,pre-training,results employing pre-training,0.5815622210502625
translation,291,33,results,bert - sqg model,propose,augmented model,bert - sqg model propose augmented model,0.6231312155723572
translation,291,33,results,exiting best model,by advancing,state - of- the - art results,exiting best model by advancing state - of- the - art results,0.7065206170082092
translation,291,33,results,state - of- the - art results,from,16.85 to 21.04,state - of- the - art results from 16.85 to 21.04,0.5117699503898621
translation,291,33,results,augmented model,called,bert - hlsqg ( highlight sequential question generation ),augmented model called bert - hlsqg ( highlight sequential question generation ),0.6688295602798462
translation,291,33,results,performance,of,bert - sqg,performance of bert - sqg,0.619159996509552
translation,291,33,results,bert - sqg model,has,outperforms,bert - sqg model has outperforms,0.613012433052063
translation,291,33,results,outperforms,has,exiting best model,outperforms has exiting best model,0.63478022813797
translation,291,33,results,results,propose,augmented model,results propose augmented model,0.6382795572280884
translation,292,257,ablation-analysis,root - type,can achieve,high qa accuracy,root - type can achieve high qa accuracy,0.7105847001075745
translation,292,257,ablation-analysis,high qa accuracy,when using,notext input,high qa accuracy when using notext input,0.6998982429504395
translation,292,257,ablation-analysis,ablation analysis,Regarding,different types of qa pairs,ablation analysis Regarding different types of qa pairs,0.6186235547065735
translation,292,203,baselines,non-neural approaches,extract,sentences,non-neural approaches extract sentences,0.6668779850006104
translation,292,203,baselines,sentences,from,source article,sentences from source article,0.5841383934020996
translation,292,203,baselines,sentences,form,summary,sentences form summary,0.7175372838973999
translation,292,211,baselines,summarunner,presents,autoregressive sequence labeling method,summarunner presents autoregressive sequence labeling method,0.6640098690986633
translation,292,211,baselines,autoregressive sequence labeling method,based on,recurrent neural networks,autoregressive sequence labeling method based on recurrent neural networks,0.6413489580154419
translation,292,211,baselines,baselines,has,summarunner,baselines has summarunner,0.5826749801635742
translation,292,214,baselines,distraction -m3,trains,summarization system,distraction -m3 trains summarization system,0.7421886920928955
translation,292,214,baselines,summarization system,to distract,attention,summarization system to distract attention,0.6688007116317749
translation,292,214,baselines,attention,to traverse,different regions,attention to traverse different regions,0.7874721884727478
translation,292,214,baselines,different regions,of,source article,different regions of source article,0.5691860318183899
translation,292,214,baselines,baselines,has,distraction -m3,baselines has distraction -m3,0.5976544618606567
translation,292,267,baselines,baselines,has,cnn,baselines has cnn,0.5907226204872131
translation,292,187,hyperparameters,hidden state dimension,of,lstm,hidden state dimension of lstm,0.5725870728492737
translation,292,187,hyperparameters,hidden state dimension,to be,256,hidden state dimension to be 256,0.5605120658874512
translation,292,187,hyperparameters,lstm,to be,256,lstm to be 256,0.5123810172080994
translation,292,187,hyperparameters,hyperparameters,set,hidden state dimension,hyperparameters set hidden state dimension,0.6264743208885193
translation,292,193,hyperparameters,sinusoidal positional encodings,of,30 dimensions,sinusoidal positional encodings of 30 dimensions,0.586737871170044
translation,292,193,hyperparameters,"et al. , 2017 )",of,30 dimensions,"et al. , 2017 ) of 30 dimensions",0.5759252905845642
translation,292,193,hyperparameters,hyperparameters,use,100 - dimensional word embeddings,hyperparameters use 100 - dimensional word embeddings,0.5437304973602295
translation,292,194,hyperparameters,maximum article length,set to,400 words,maximum article length set to 400 words,0.5853229761123657
translation,292,194,hyperparameters,hyperparameters,has,maximum article length,hyperparameters has maximum article length,0.47672945261001587
translation,292,196,hyperparameters,at most 10 qa pairs ( k=10 ),to guide,extraction,at most 10 qa pairs ( k=10 ) to guide extraction,0.6398481726646423
translation,292,196,hyperparameters,extraction,of,summary segments,extraction of summary segments,0.6170009970664978
translation,292,197,hyperparameters,mini-batch training,with,"adam optimizer ( kingma and ba , 2014 )","mini-batch training with adam optimizer ( kingma and ba , 2014 )",0.6231120228767395
translation,292,197,hyperparameters,"adam optimizer ( kingma and ba , 2014 )",where,mini-batch,"adam optimizer ( kingma and ba , 2014 ) where mini-batch",0.5925194025039673
translation,292,197,hyperparameters,mini-batch,contains,128 articles,mini-batch contains 128 articles,0.6938420534133911
translation,292,197,hyperparameters,mini-batch,contains,qa pairs,mini-batch contains qa pairs,0.654100775718689
translation,292,197,hyperparameters,hyperparameters,apply,mini-batch training,hyperparameters apply mini-batch training,0.5783742666244507
translation,292,198,hyperparameters,summary ratio,set to,0.15,summary ratio set to 0.15,0.6933051943778992
translation,292,198,hyperparameters,0.15,yielding,extractive summaries,0.15 yielding extractive summaries,0.6322953701019287
translation,292,198,hyperparameters,extractive summaries,has,of about 60 words,extractive summaries has of about 60 words,0.5868194699287415
translation,292,198,hyperparameters,hyperparameters,has,summary ratio,hyperparameters has summary ratio,0.5305749773979187
translation,292,35,model,human abstracts,to guide,extraction of summary text units,human abstracts to guide extraction of summary text units,0.6433182954788208
translation,292,41,model,novel reinforcement learning framework,to explore,space of possible extractive summaries,novel reinforcement learning framework to explore space of possible extractive summaries,0.6464291214942932
translation,292,41,model,novel reinforcement learning framework,assess,each summary,novel reinforcement learning framework assess each summary,0.6407633423805237
translation,292,41,model,each summary,using,novel reward function,each summary using novel reward function,0.6255669593811035
translation,292,41,model,novel reward function,judging,summary 's,novel reward function judging summary 's,0.6644484400749207
translation,292,41,model,novel reward function,judging,"fluency , length","novel reward function judging fluency , length",0.58107990026474
translation,292,41,model,novel reward function,judging,competency,novel reward function judging competency,0.6696186065673828
translation,292,41,model,competency,to answer,important questions,competency to answer important questions,0.6990929841995239
translation,292,41,model,summary 's,has,adequacy,summary 's has adequacy,0.5762854814529419
translation,292,41,model,model,utilize,novel reinforcement learning framework,model utilize novel reinforcement learning framework,0.5321887731552124
translation,292,71,model,novel supervised framework,encouraging,selection of consecutive sequences of words,novel supervised framework encouraging selection of consecutive sequences of words,0.6365816593170166
translation,292,71,model,selection of consecutive sequences of words,to form,extractive summary,selection of consecutive sequences of words to form extractive summary,0.6400794982910156
translation,292,71,model,model,present,novel supervised framework,model present novel supervised framework,0.6468057036399841
translation,292,72,model,reinforcement learning,to explore,space,reinforcement learning to explore space,0.6723940372467041
translation,292,72,model,reinforcement learning,promote,"fluent , adequate , and competent","reinforcement learning promote fluent , adequate , and competent",0.597131609916687
translation,292,72,model,space,of,possible extractive summaries,space of possible extractive summaries,0.560110867023468
translation,292,72,model,"fluent , adequate , and competent",in,question answering,"fluent , adequate , and competent in question answering",0.45087096095085144
translation,292,72,model,model,leverage,reinforcement learning,model leverage reinforcement learning,0.7140412926673889
translation,292,215,model,graph attention,introduces,graph- based attention mechanism,graph attention introduces graph- based attention mechanism,0.609276533126831
translation,292,215,model,graph- based attention mechanism,to enhance,encoderdecoder framework,graph- based attention mechanism to enhance encoderdecoder framework,0.672297477722168
translation,292,215,model,model,has,graph attention,model has graph attention,0.5476001501083374
translation,292,42,results,extractive summaries,yielding,highest expected rewards,extractive summaries yielding highest expected rewards,0.6134320497512817
translation,292,228,results,our qasumm methods,focusing on,chunk extraction,our qasumm methods focusing on chunk extraction,0.7338860630989075
translation,292,228,results,chunk extraction,perform,on par,chunk extraction perform on par,0.5600650310516357
translation,292,228,results,on par,with,competitive systems,on par with competitive systems,0.7219261527061462
translation,292,228,results,results,has,our qasumm methods,results has our qasumm methods,0.5411455631256104
translation,292,237,results,variants,of,qasumm method,variants of qasumm method,0.6211110353469849
translation,292,237,results,variants,find that,qasumm + root,variants find that qasumm + root,0.6538283824920654
translation,292,237,results,qasumm method,find that,qasumm + root,qasumm method find that qasumm + root,0.6512143015861511
translation,292,237,results,qasumm + root,achieves,highest scores,qasumm + root achieves highest scores,0.6712303161621094
translation,292,237,results,highest scores,on,dm dataset,highest scores on dm dataset,0.5106605887413025
translation,292,237,results,results,Among,variants,results Among variants,0.5687075853347778
translation,292,238,results,qasumm +ner,performs,consistently well,qasumm +ner performs consistently well,0.6261714100837708
translation,292,238,results,consistently well,on,cnn and dm datasets,consistently well on cnn and dm datasets,0.5295956134796143
translation,292,238,results,results,has,qasumm +ner,results has qasumm +ner,0.5592830777168274
translation,292,252,results,question - answering with goldsumm,performs,best,question - answering with goldsumm performs best,0.6088256239891052
translation,292,252,results,best,for,all qa types,best for all qa types,0.6185498237609863
translation,292,252,results,results,observe,question - answering with goldsumm,results observe question - answering with goldsumm,0.560562789440155
translation,292,253,results,outperforms,using,full - text,outperforms using full - text,0.6408496499061584
translation,292,253,results,scenarios,using,full - text,scenarios using full - text,0.6195931434631348
translation,292,253,results,full - text,as,source input,full - text as source input,0.5124569535255432
translation,292,253,results,outperforms,has,scenarios,outperforms has scenarios,0.6612445116043091
translation,292,253,results,results,has,outperforms,results has outperforms,0.6657275557518005
translation,292,255,results,performance,of,qa - summ + noq,performance of qa - summ + noq,0.6138985753059387
translation,292,255,results,qa - summ + noq,in between,notext and goldsumm,qa - summ + noq in between notext and goldsumm,0.7383760213851929
translation,292,255,results,notext and goldsumm,for,all answer types,notext and goldsumm for all answer types,0.6152620315551758
translation,292,255,results,results,observe that,performance,results observe that performance,0.6149371862411499
translation,292,259,results,ner - type qa pairs,work,best,ner - type qa pairs work best,0.6408628821372986
translation,292,259,results,best,for,goldsumm and full - text,best for goldsumm and full - text,0.576815128326416
translation,292,259,results,results,has,ner - type qa pairs,results has ner - type qa pairs,0.543272852897644
translation,292,260,results,subj / obj - type qa pairs,have,smallest gap,subj / obj - type qa pairs have smallest gap,0.5550492405891418
translation,292,260,results,smallest gap,between,train / dev accuracies,smallest gap between train / dev accuracies,0.6113104224205017
translation,292,260,results,results,find,subj / obj - type qa pairs,results find subj / obj - type qa pairs,0.5708459615707397
translation,292,270,results,extracting chunks,performs,superior,extracting chunks performs superior,0.6177071928977966
translation,292,270,results,extracting chunks,combining,chunks,extracting chunks combining chunks,0.768095850944519
translation,292,270,results,chunks,with,lstm representations,chunks with lstm representations,0.6277991533279419
translation,292,270,results,lstm representations,yield,highest scores,lstm representations yield highest scores,0.7207468748092651
translation,292,270,results,results,find that,extracting chunks,results find that extracting chunks,0.6608311533927917
translation,292,270,results,results,combining,chunks,results combining chunks,0.6124908328056335
translation,292,283,results,all systems,resulted in,similar performance times,all systems resulted in similar performance times,0.6270067095756531
translation,292,283,results,human abstracts,has,all systems,human abstracts has all systems,0.5943697094917297
translation,292,284,results,large margin,in,qa accuracy,large margin in qa accuracy,0.563387930393219
translation,292,284,results,large margin,compared to,abstractive and our supervised approach,large margin compared to abstractive and our supervised approach,0.6266524195671082
translation,292,284,results,qa accuracy,in,our full system,qa accuracy in our full system,0.5048866868019104
translation,292,284,results,results,observe,large margin,results observe large margin,0.6539188027381897
translation,292,285,results,informativeness,of,summaries,informativeness of summaries,0.6002655625343323
translation,292,285,results,informativeness,yielded,higher performance,informativeness yielded higher performance,0.6097134947776794
translation,292,285,results,summaries,to be,same,summaries to be same,0.6581137180328369
translation,292,285,results,summaries,to be,our systems,summaries to be our systems,0.5651859641075134
translation,292,285,results,same,has,our systems,same has our systems,0.6465820074081421
translation,293,147,hyperparameters,3 epochs,on,triviaqa,3 epochs on triviaqa,0.5541521906852722
translation,293,147,hyperparameters,2 epochs,on,nar-rativeqa,2 epochs on nar-rativeqa,0.5018743872642517
translation,293,147,hyperparameters,hyperparameters,fine-tune,3 epochs,hyperparameters fine-tune 3 epochs,0.7064989805221558
translation,293,147,hyperparameters,hyperparameters,fine-tune,2 epochs,hyperparameters fine-tune 2 epochs,0.7131550312042236
translation,293,179,hyperparameters,external clean supervision,from,squad 2.0,external clean supervision from squad 2.0,0.5377803444862366
translation,293,179,hyperparameters,squad 2.0,to train,bert - based qa model,squad 2.0 to train bert - based qa model,0.6882404685020447
translation,293,179,hyperparameters,bert - based qa model,for,2 epochs,bert - based qa model for 2 epochs,0.6492520570755005
translation,293,30,model,framework,for understanding,different distant supervision assumptions,framework for understanding different distant supervision assumptions,0.6889934539794922
translation,293,30,model,framework,corresponding,trade - off,framework corresponding trade - off,0.6278981566429138
translation,293,30,model,different distant supervision assumptions,corresponding,trade - off,different distant supervision assumptions corresponding trade - off,0.5896555781364441
translation,293,30,model,trade - off,among,"coverage , quality and strength of distant supervision signal","trade - off among coverage , quality and strength of distant supervision signal",0.5941274166107178
translation,293,30,model,model,corresponding,trade - off,model corresponding trade - off,0.6494710445404053
translation,293,37,model,model,formalize,paragraph - level and document- level models,model formalize paragraph - level and document- level models,0.7198533415794373
translation,293,37,model,model,both,paragraph - level and document- level models,model both paragraph - level and document- level models,0.6901861429214478
translation,293,175,model,two distant supervision objectives,in,multitask manner,two distant supervision objectives in multitask manner,0.5258556604385376
translation,293,175,model,two distant supervision objectives,i.e.,h2,two distant supervision objectives i.e. h2,0.6625574827194214
translation,293,175,model,two distant supervision objectives,i.e.,h2,two distant supervision objectives i.e. h2,0.6625574827194214
translation,293,175,model,h2,-,p and h3 - d,h2 - p and h3 - d,0.7084730863571167
translation,293,175,model,h2,-,- p and h2 -d,h2 - - p and h2 -d,0.6905627846717834
translation,293,175,model,p and h3 - d,for,triviaqa,p and h3 - d for triviaqa,0.7247187495231628
translation,293,175,model,p and h3 - d,for,narrativeqa,p and h3 - d for narrativeqa,0.7058702707290649
translation,293,175,model,- p and h2 -d,for,narrativeqa,- p and h2 -d for narrativeqa,0.7123565673828125
translation,293,175,model,h2,has,p and h3 - d,h2 has p and h3 - d,0.6289199590682983
translation,293,175,model,h2,has,- p and h2 -d,h2 has - p and h2 -d,0.6294689178466797
translation,293,175,model,model,combine,two distant supervision objectives,model combine two distant supervision objectives,0.703586757183075
translation,293,32,results,our study,show that,model,our study show that model,0.5694843530654907
translation,293,32,results,model,with,most suitable probabilistic treatment,model with most suitable probabilistic treatment,0.6103951930999756
translation,293,32,results,model,achieves,large improvements,model achieves large improvements,0.6794090867042542
translation,293,32,results,large improvements,of,4.6 f1,large improvements of 4.6 f1,0.5707442164421082
translation,293,32,results,large improvements,of,1.7 rouge -l,large improvements of 1.7 rouge -l,0.5846375823020935
translation,293,32,results,large improvements,on,narra-tiveqa,large improvements on narra-tiveqa,0.6381446123123169
translation,293,32,results,4.6 f1,on,triviaqa,4.6 f1 on triviaqa,0.5338981747627258
translation,293,32,results,1.7 rouge -l,on,narra-tiveqa,1.7 rouge -l on narra-tiveqa,0.5516826510429382
translation,293,32,results,results,show that,model,results show that model,0.4897858202457428
translation,293,33,results,efficient multi-loss objective,combine,benefits,efficient multi-loss objective combine benefits,0.6898677349090576
translation,293,33,results,benefits,of,different formulations,benefits of different formulations,0.5581457614898682
translation,293,33,results,significant improvements,in,accuracy,significant improvements in accuracy,0.5446901321411133
translation,293,33,results,significant improvements,surpassing,best previously reported results,significant improvements surpassing best previously reported results,0.7117748856544495
translation,293,33,results,best previously reported results,on,two studied tasks,best previously reported results on two studied tasks,0.45316413044929504
translation,293,33,results,results,design,efficient multi-loss objective,results design efficient multi-loss objective,0.637965977191925
translation,293,106,results,position - based weak supervision,is,more effective,position - based weak supervision is more effective,0.5653036832809448
translation,293,106,results,more effective,than,span-based,more effective than span-based,0.5652466416358948
translation,293,106,results,span-based,for,our model,span-based for our model,0.6479345560073853
translation,293,106,results,majority of settings,has,position - based weak supervision,majority of settings has position - based weak supervision,0.554037868976593
translation,293,151,results,inference,with,max,inference with max,0.6923673748970032
translation,293,151,results,max,is,better,max is better,0.6772061586380005
translation,293,151,results,narrativeqa,has,inference,narrativeqa has inference,0.6133561730384827
translation,293,151,results,results,On,narrativeqa,results On narrativeqa,0.5006263852119446
translation,293,155,results,span- based mml,performs,consistently better,span- based mml performs consistently better,0.6124217510223389
translation,293,155,results,consistently better,than,span-based hardem,consistently better than span-based hardem,0.5828575491905212
translation,293,155,results,h2 -d and h3 -d,has,span- based mml,h2 -d and h3 -d has span- based mml,0.5931088328361511
translation,293,155,results,results,under,h2 -d and h3 -d,results under h2 -d and h3 -d,0.5722677111625671
translation,293,156,results,mml,consistently better than,hardem,mml consistently better than hardem,0.7334006428718567
translation,293,156,results,position - based objectives,has,mml,position - based objectives has mml,0.5735778212547302
translation,293,156,results,results,For,position - based objectives,results For position - based objectives,0.5740070343017578
translation,293,157,results,each distant supervision hypothesis / probability space combination,has,positionbased mml,each distant supervision hypothesis / probability space combination has positionbased mml,0.5407590270042419
translation,293,165,results,h3,achieves,significantly bet- ter results,h3 achieves significantly bet- ter results,0.6598826050758362
translation,293,165,results,significantly bet- ter results,than,other formulations,significantly bet- ter results than other formulations,0.5960686206817627
translation,293,165,results,triviaqa,has,h3,triviaqa has h3,0.662614107131958
translation,293,165,results,triviaqa,has,- d,triviaqa has - d,0.6944252848625183
translation,293,165,results,h3,has,- d,h3 has - d,0.6938508152961731
translation,293,165,results,results,For,triviaqa,results For triviaqa,0.6138095855712891
translation,293,167,results,paragraph - level models h1 - p and h2 - p,has,outperform,paragraph - level models h1 - p and h2 - p has outperform,0.555940568447113
translation,293,167,results,outperform,has,corresponding document- level counterparts h1 - d and h2 -d,outperform has corresponding document- level counterparts h1 - d and h2 -d,0.6230823993682861
translation,293,167,results,results,has,paragraph - level models h1 - p and h2 - p,results has paragraph - level models h1 - p and h2 - p,0.4959222674369812
translation,293,169,results,h2 - d models,achieve,best performance,h2 - d models achieve best performance,0.6699621677398682
translation,293,169,results,best performance,for,narrativeqa,best performance for narrativeqa,0.6576070785522461
translation,293,183,results,external clean supervision,improves,model performance,external clean supervision improves model performance,0.711025595664978
translation,293,183,results,results,using,external clean supervision,results using external clean supervision,0.6116665005683899
translation,293,185,results,performance,on,triviaqa,performance on triviaqa,0.5598219037055969
translation,293,185,results,performance,on,narrativeqa,performance on narrativeqa,0.6089268922805786
translation,293,185,results,single-objective components,has,multi-objective formulations,single-objective components has multi-objective formulations,0.5718455910682678
translation,293,185,results,multi-objective formulations,has,improve,multi-objective formulations has improve,0.5366442799568176
translation,293,185,results,improve,has,performance,improve has performance,0.5578044652938843
translation,293,185,results,results,Compared with,single-objective components,results Compared with single-objective components,0.6419341564178467
translation,293,189,results,our best models,achieve,4.9 f1 and 5.5 em improvement,our best models achieve 4.9 f1 and 5.5 em improvement,0.6195948123931885
translation,293,189,results,our best models,achieve,6.8 f1 and 7.4 em improvement,our best models achieve 6.8 f1 and 7.4 em improvement,0.6169324517250061
translation,293,189,results,4.9 f1 and 5.5 em improvement,on,full test set,4.9 f1 and 5.5 em improvement on full test set,0.5249661207199097
translation,293,189,results,6.8 f1 and 7.4 em improvement,on,verified subset,6.8 f1 and 7.4 em improvement on verified subset,0.5778656601905823
translation,293,189,results,recent triviaqa sota,has,our best models,recent triviaqa sota has our best models,0.5908494591712952
translation,293,189,results,results,Compared to,recent triviaqa sota,results Compared to recent triviaqa sota,0.6951289176940918
translation,293,191,results,large improvement,without,additional fully labeled data,large improvement without additional fully labeled data,0.7037800550460815
translation,293,191,results,results,has,large improvement,results has large improvement,0.6200464963912964
translation,293,192,results,external fully labeled data,to initialize,model,external fully labeled data to initialize model,0.7248029708862305
translation,293,192,results,external fully labeled data,has,performance,external fully labeled data has performance,0.5214439034461975
translation,293,192,results,model,has,performance,model has performance,0.5451148748397827
translation,293,192,results,results,With,external fully labeled data,results With external fully labeled data,0.6106134653091431
translation,294,26,baselines,transfer learning ( tl ),utilize,source domain,transfer learning ( tl ) utilize source domain,0.5822106003761292
translation,294,26,baselines,adequate labeling,help,target domain,adequate labeling help target domain,0.7532036900520325
translation,294,42,baselines,baselines,has,multi-turn hcnn ( mt - hcnn ),baselines has multi-turn hcnn ( mt - hcnn ),0.5546274185180664
translation,294,91,baselines,baselines,compared,multiturn model ( mt - hcnn ),baselines compared multiturn model ( mt - hcnn ),0.6874958872795105
translation,294,113,baselines,src-only,uses,only source data,src-only uses only source data,0.662278950214386
translation,294,113,baselines,tgt-only,uses,only target data,tgt-only uses only target data,0.6701059937477112
translation,294,113,baselines,tl - s,uses,both source and target data,tl - s uses both source and target data,0.6865574717521667
translation,294,113,baselines,both source and target data,with,adversarial training,both source and target data with adversarial training,0.6119436621665955
translation,294,113,baselines,baselines,has,src-only,baselines has src-only,0.57950359582901
translation,294,120,baselines,bot,uses,tf - idf model,bot uses tf - idf model,0.6122426390647888
translation,294,120,baselines,bot,uses,our model,bot uses our model,0.6879837512969971
translation,294,120,baselines,bot,uses,our model,bot uses our model,0.6879837512969971
translation,294,120,baselines,tf - idf model,in,lucene,tf - idf model in lucene,0.528239905834198
translation,294,120,baselines,tf - idf model,to return,set of candidates,tf - idf model to return set of candidates,0.6840746402740479
translation,294,120,baselines,our model,to rerank,all the candidates,our model to rerank all the candidates,0.6975500583648682
translation,294,120,baselines,our model,returns,top,our model returns top,0.7409812808036804
translation,294,120,baselines,each query,has,bot,each query has bot,0.6258047819137573
translation,294,123,baselines,current query,to retrieve,candidate,current query to retrieve candidate,0.7621681094169617
translation,294,123,baselines,online model,has,degenerated version,online model has degenerated version,0.598865270614624
translation,294,93,experimental-setup,experimental setup,implemented with,tensorflow,experimental setup implemented with tensorflow,0.6839593052864075
translation,294,96,experimental-setup,cnn,set,window size,cnn set window size,0.6681555509567261
translation,294,96,experimental-setup,cnn,set,stride,cnn set stride,0.6770319938659668
translation,294,96,experimental-setup,window size,of,convolution layer,window size of convolution layer,0.5626088380813599
translation,294,96,experimental-setup,convolution layer,as,2,convolution layer as 2,0.5297484993934631
translation,294,96,experimental-setup,relu,as,activation function,relu as activation function,0.5622032880783081
translation,294,96,experimental-setup,stride,of,max-pooling layer,stride of max-pooling layer,0.5705248117446899
translation,294,96,experimental-setup,max-pooling layer,as,2,max-pooling layer as 2,0.4779531955718994
translation,294,96,experimental-setup,experimental setup,For,cnn,experimental setup For cnn,0.5924025177955627
translation,294,97,experimental-setup,hidden node size,of,fully - connected layer,hidden node size of fully - connected layer,0.5439120531082153
translation,294,97,experimental-setup,hidden node size,set as,128,hidden node size set as 128,0.6821447610855103
translation,294,97,experimental-setup,fully - connected layer,set as,128,fully - connected layer set as 128,0.5853459239006042
translation,294,97,experimental-setup,experimental setup,has,hidden node size,experimental setup has hidden node size,0.5258879661560059
translation,294,98,experimental-setup,adadelta,to train,our model,adadelta to train our model,0.7072427272796631
translation,294,98,experimental-setup,our model,with,initial learning rate,our model with initial learning rate,0.6006351709365845
translation,294,98,experimental-setup,initial learning rate,of,0.08,initial learning rate of 0.08,0.5806505084037781
translation,294,98,experimental-setup,experimental setup,has,adadelta,experimental setup has adadelta,0.5753169655799866
translation,294,121,experimental-setup,candidate size,as,15,candidate size as 15,0.593728244304657
translation,294,121,experimental-setup,context length,as,3,context length as 3,0.552129328250885
translation,294,121,experimental-setup,experimental setup,set,candidate size,experimental setup set candidate size,0.6552694439888
translation,294,121,experimental-setup,experimental setup,set,context length,experimental setup set context length,0.6301607489585876
translation,294,7,experiments,transfer learning,for,multi-turn information seeking conversations,transfer learning for multi-turn information seeking conversations,0.5551085472106934
translation,294,71,experiments,ubuntu dialog corpus,contains,multiturn technical support conversation data,ubuntu dialog corpus contains multiturn technical support conversation data,0.5771183371543884
translation,294,71,experiments,multiturn technical support conversation data,collected from,chat logs,multiturn technical support conversation data collected from chat logs,0.6778952479362488
translation,294,71,experiments,chat logs,of,freenode internet relay chat ( irc ) network,chat logs of freenode internet relay chat ( irc ) network,0.560703456401825
translation,294,119,experiments,our model,in,alime assist bot,our model in alime assist bot,0.6028280258178711
translation,294,119,experiments,online,in,alime assist bot,online in alime assist bot,0.6314504742622375
translation,294,119,experiments,our model,has,online,our model has online,0.6258986592292786
translation,294,8,model,efficient and effective multiturn conversation model,based on,convolutional neural networks,efficient and effective multiturn conversation model based on convolutional neural networks,0.588594377040863
translation,294,8,model,model,propose,efficient and effective multiturn conversation model,model propose efficient and effective multiturn conversation model,0.632962703704834
translation,294,22,model,rnn layers,of,inputs,rnn layers of inputs,0.5616828203201294
translation,294,22,model,inputs,from,model,inputs from model,0.6106885671615601
translation,294,22,model,smn,uses,sentence interaction based ( si - based ) pyramid model,smn uses sentence interaction based ( si - based ) pyramid model,0.5579801201820374
translation,294,22,model,sentence interaction based ( si - based ) pyramid model,to model,each utterance and response pair,sentence interaction based ( si - based ) pyramid model to model each utterance and response pair,0.7148128151893616
translation,294,22,model,model,remove,rnn layers,model remove rnn layers,0.6509079933166504
translation,294,24,model,component,to incorporate,se - based bcnn model,component to incorporate se - based bcnn model,0.659050703048706
translation,294,24,model,se - based bcnn model,resulting in,hybrid cnn ( hcnn ),se - based bcnn model resulting in hybrid cnn ( hcnn ),0.6376455426216125
translation,294,24,model,model,extend,component,model extend component,0.687926173210144
translation,294,32,model,domain discriminators,on,both source and target features,domain discriminators on both source and target features,0.4818776547908783
translation,294,32,model,both source and target features,derived by,domainspecific nns,both source and target features derived by domainspecific nns,0.6046385169029236
translation,294,32,model,domainspecific nns,to help learn,domain-specific features,domainspecific nns to help learn domain-specific features,0.5681618452072144
translation,294,32,model,model,use,domain discriminators,model use domain discriminators,0.6831408143043518
translation,294,122,model,computation,bundle,15 candidates,computation bundle 15 candidates,0.7801715135574341
translation,294,122,model,15 candidates,into,mini-batch,15 candidates into mini-batch,0.5746559500694275
translation,294,122,model,mini-batch,to feed into,our model,mini-batch to feed into our model,0.6936562657356262
translation,294,122,model,model,To accelerate,computation,model To accelerate computation,0.744620680809021
translation,294,103,results,rnn based methods,like,mv - lstm and smn,rnn based methods like mv - lstm and smn,0.6036047339439392
translation,294,103,results,mv - lstm and smn,have,clear advantages,mv - lstm and smn have clear advantages,0.5641480684280396
translation,294,103,results,mv - lstm and smn,better or comparable with,state- ofthe- art cnn - based models,mv - lstm and smn better or comparable with state- ofthe- art cnn - based models,0.6000166535377502
translation,294,103,results,clear advantages,over,two cnn - based approaches,clear advantages over two cnn - based approaches,0.6559125185012817
translation,294,103,results,two cnn - based approaches,like,arc - i and arc - ii,two cnn - based approaches like arc - i and arc - ii,0.5772697925567627
translation,294,103,results,state- ofthe- art cnn - based models,like,pyramid and duet,state- ofthe- art cnn - based models like pyramid and duet,0.6225509643554688
translation,294,103,results,mt - hcnnd,shows,benefits,mt - hcnnd shows benefits,0.6983293890953064
translation,294,103,results,benefits,of adding,convolutional layer,benefits of adding convolutional layer,0.6687180995941162
translation,294,103,results,convolutional layer,to,output representations,convolutional layer to output representations,0.5314584374427795
translation,294,103,results,output representations,of,all the utterances,output representations of all the utterances,0.5790714621543884
translation,294,103,results,smn,not perform,well,smn not perform well,0.7371314764022827
translation,294,103,results,well,in,alimedata,well in alimedata,0.5996373891830444
translation,294,103,results,alimedata,compared to,udc,alimedata compared to udc,0.7345520257949829
translation,294,103,results,our mt -hcnn,has,outperforms,our mt -hcnn has outperforms,0.6253333687782288
translation,294,103,results,outperforms,has,mt - hcnnd,outperforms has mt - hcnnd,0.6380242705345154
translation,294,103,results,results,has,rnn based methods,results has rnn based methods,0.5367080569267273
translation,294,106,results,comparable or better results,compared with,smn,comparable or better results compared with smn,0.6708129644393921
translation,294,106,results,comparable or better results,compared with,smn,comparable or better results compared with smn,0.6708129644393921
translation,294,106,results,much more efficient,than,smn,much more efficient than smn,0.5941070318222046
translation,294,106,results,mt -hcnn,has,comparable or better results,mt -hcnn has comparable or better results,0.5566105842590332
translation,294,106,results,results,has,mt -hcnn,results has mt -hcnn,0.5248314142227173
translation,294,108,results,mt -hcnn module,able to support,peak qps,mt -hcnn module able to support peak qps,0.6028167605400085
translation,294,108,results,peak qps,on,cluster of 2 service instances,peak qps on cluster of 2 service instances,0.5768312811851501
translation,294,108,results,11 of 40,on,cluster of 2 service instances,11 of 40 on cluster of 2 service instances,0.5434924364089966
translation,294,108,results,4g memory,on,intel xeon e5 - 2430 machine,4g memory on intel xeon e5 - 2430 machine,0.41369956731796265
translation,294,108,results,peak qps,has,11 of 40,peak qps has 11 of 40,0.5587759017944336
translation,294,108,results,results,has,mt -hcnn module,results has mt -hcnn module,0.5462992191314697
translation,294,110,results,proposed mt - hcnn,shown to be both efficient and effective,question matching,proposed mt - hcnn shown to be both efficient and effective question matching,0.6549711227416992
translation,294,110,results,question matching,in,multi-turn conversations,question matching in multi-turn conversations,0.511039674282074
translation,294,110,results,results,has,proposed mt - hcnn,results has proposed mt - hcnn,0.5786219835281372
translation,294,115,results,src-only,performs,worse,src-only performs worse,0.634080171585083
translation,294,115,results,worse,than,tgt-only,worse than tgt-only,0.6245913505554199
translation,294,115,results,results,has,src-only,results has src-only,0.5410274267196655
translation,294,117,results,tl -s,able to leverage,knowledge,tl -s able to leverage knowledge,0.7417514324188232
translation,294,117,results,tl -s,boost,performance,tl -s boost performance,0.7254241108894348
translation,294,117,results,knowledge,from,source domain,knowledge from source domain,0.5072470307350159
translation,294,117,results,our model,shows,better performance,our model shows better performance,0.6902104020118713
translation,294,117,results,better performance,than,tl -s,better performance than tl -s,0.6105397343635559
translation,294,125,results,our method,yielding,5 % ? 10 % improvement,our method yielding 5 % ? 10 % improvement,0.6223031282424927
translation,294,125,results,consistently outperforms,yielding,5 % ? 10 % improvement,consistently outperforms yielding 5 % ? 10 % improvement,0.6872050762176514
translation,294,125,results,our method,has,consistently outperforms,our method has consistently outperforms,0.6093489527702332
translation,294,125,results,consistently outperforms,has,online model,consistently outperforms has online model,0.6021913290023804
translation,294,125,results,results,has,our method,results has our method,0.5589964985847473
translation,294,128,results,our model,built on,cnn based modules,our model built on cnn based modules,0.6756584048271179
translation,294,128,results,cnn based modules,yields,comparable results,cnn based modules yields comparable results,0.6893870830535889
translation,294,128,results,cnn based modules,with,better efficiency,cnn based modules with better efficiency,0.635138750076294
translation,294,128,results,smn,has,our model,smn has our model,0.6069572567939758
translation,294,128,results,results,Different from,smn,results Different from smn,0.6198097467422485
translation,295,72,ablation-analysis,well - formed question,gets,54.9 %,well - formed question gets 54.9 %,0.5582408308982849
translation,295,66,baselines,majority class baseline,is,61.5 %,majority class baseline is 61.5 %,0.5547107458114624
translation,295,66,baselines,majority class baseline,corresponds to,all queries being classified non-wellformed,majority class baseline corresponds to all queries being classified non-wellformed,0.6635491847991943
translation,295,66,baselines,baselines,has,majority class baseline,baselines has majority class baseline,0.5729553699493408
translation,295,20,experiments,"dataset of 25,100 queries",annotated with,probability,"dataset of 25,100 queries annotated with probability",0.7254675030708313
translation,295,20,experiments,probability,of being,well - formed natural language question,probability of being well - formed natural language question,0.6365759968757629
translation,295,62,hyperparameters,size,of,first and second hidden layers,size of first and second hidden layers,0.5838748216629028
translation,295,62,hyperparameters,first and second hidden layers,to be,128 and 64,first and second hidden layers to be 128 and 64,0.5510984659194946
translation,295,62,hyperparameters,hyperparameters,fix,size,hyperparameters fix size,0.7027605175971985
translation,295,63,hyperparameters,character n-gram embeddings,of,length,character n-gram embeddings of length,0.5449633598327637
translation,295,63,hyperparameters,length,has,16,length has 16,0.5646985173225403
translation,295,63,hyperparameters,hyperparameters,has,character n-gram embeddings,hyperparameters has character n-gram embeddings,0.4966577887535095
translation,295,63,hyperparameters,hyperparameters,has,other feature embeddings,hyperparameters has other feature embeddings,0.49320849776268005
translation,295,64,hyperparameters,stochastic gradient descent,with,momentum,stochastic gradient descent with momentum,0.6327335834503174
translation,295,64,hyperparameters,momentum,for,optimization,momentum for optimization,0.6519289016723633
translation,295,64,hyperparameters,momentum,with,learning rate,momentum with learning rate,0.6444010734558105
translation,295,64,hyperparameters,momentum,with,batch size,momentum with batch size,0.6645553112030029
translation,295,64,hyperparameters,learning rate,tuned over,[ 0.001 ? 0.3 ],learning rate tuned over [ 0.001 ? 0.3 ],0.7202625870704651
translation,295,64,hyperparameters,learning rate,tuned over,batch size,learning rate tuned over batch size,0.7473925948143005
translation,295,64,hyperparameters,learning rate,tuned over,32,learning rate tuned over 32,0.7251895666122437
translation,295,64,hyperparameters,learning rate,tuned over,50000 training steps,learning rate tuned over 50000 training steps,0.7213997840881348
translation,295,64,hyperparameters,batch size,of,32,batch size of 32,0.6741614937782288
translation,295,64,hyperparameters,hyperparameters,use,stochastic gradient descent,hyperparameters use stochastic gradient descent,0.5965404510498047
translation,295,22,results,test set,of,"3,850 queries","test set of 3,850 queries",0.5602376461029053
translation,295,22,results,test set,report,accuracy,test set report accuracy,0.6472522616386414
translation,295,22,results,"3,850 queries",report,accuracy,"3,850 queries report accuracy",0.6170644164085388
translation,295,22,results,accuracy,of,70.1 %,accuracy of 70.1 %,0.5611398816108704
translation,295,22,results,70.1 %,on,binary classification task,70.1 % on binary classification task,0.49749380350112915
translation,295,22,results,results,On,test set,results On test set,0.582119882106781
translation,295,76,results,best performance,obtained,70.7 %,best performance obtained 70.7 %,0.6031404137611389
translation,295,76,results,70.7 %,while using,"word - 1 , 2 - grams","70.7 % while using word - 1 , 2 - grams",0.6689947843551636
translation,295,76,results,70.7 %,while using,"pos -1 , 2 , 3 - grams","70.7 % while using pos -1 , 2 , 3 - grams",0.6657804846763611
translation,295,76,results,results,has,best performance,results has best performance,0.5759831070899963
translation,295,77,results,pos n-grams,gave,strong boost,pos n-grams gave strong boost,0.6074517369270325
translation,295,77,results,strong boost,of,5.2 points,strong boost of 5.2 points,0.5526065826416016
translation,295,77,results,5.2 points,over,word unigrams and bigrams,5.2 points over word unigrams and bigrams,0.5632442831993103
translation,295,77,results,results,Using,pos n-grams,results Using pos n-grams,0.5958268046379089
translation,295,78,results,"character - 3 , 4 grams",gave improvement over,word unigrams and bigrams,"character - 3 , 4 grams gave improvement over word unigrams and bigrams",0.6939037442207336
translation,295,78,results,"character - 3 , 4 grams",combined with,pos tags,"character - 3 , 4 grams combined with pos tags",0.6577760577201843
translation,295,78,results,performance,not,sustain,performance not sustain,0.7463476657867432
translation,295,78,results,sustain,combined with,pos tags,sustain combined with pos tags,0.7138901352882385
translation,295,78,results,results,has,"character - 3 , 4 grams","results has character - 3 , 4 grams",0.5491309762001038
translation,296,181,baselines,linear structure,of,language,linear structure of language,0.5849550366401672
translation,296,181,baselines,question,using,linear rnns - lstm ( no kg ),question using linear rnns - lstm ( no kg ),0.6948581337928772
translation,296,181,baselines,question,using,"relation -network ( santoro et al. , 2017 ) augmented model","question using relation -network ( santoro et al. , 2017 ) augmented model",0.6610234379768372
translation,296,183,baselines,two variants,of,tree-structured lstms,two variants of tree-structured lstms,0.5624426603317261
translation,296,183,baselines,two variants,operates on,preparsed questions,two variants operates on preparsed questions,0.7236040830612183
translation,296,183,baselines,tree - lstm,operates on,preparsed questions,tree - lstm operates on preparsed questions,0.7686131000518799
translation,296,183,baselines,unsupervised tree-lstm model,learns,jointly parse and represent,unsupervised tree-lstm model learns jointly parse and represent,0.7389156818389893
translation,296,183,baselines,tree - lstm ( unsup . ),has,unsupervised tree-lstm model,tree - lstm ( unsup . ) has unsupervised tree-lstm model,0.5774148106575012
translation,296,183,baselines,jointly parse and represent,has,sentence,jointly parse and represent has sentence,0.5638540983200073
translation,296,183,baselines,baselines,compare,two variants,baselines compare two variants,0.6878613233566284
translation,296,228,baselines,dynamic neural module networks ( d- nmn ),selecting from,small set,dynamic neural module networks ( d- nmn ) selecting from small set,0.6828649044036865
translation,296,228,baselines,baselines,has,dynamic neural module networks ( d- nmn ),baselines has dynamic neural module networks ( d- nmn ),0.56834876537323
translation,296,152,experiments,clevrgen,generate,dataset,clevrgen generate dataset,0.665219783782959
translation,296,152,experiments,dataset,of,question and answers,dataset of question and answers,0.5857078433036804
translation,296,152,experiments,question and answers,based on,clevr dataset,question and answers based on clevr dataset,0.5859942436218262
translation,296,152,experiments,knowledge graphs,containing,attribute information,knowledge graphs containing attribute information,0.6110489368438721
translation,296,152,experiments,attribute information,of,objects and relations between them,attribute information of objects and relations between them,0.5158990621566772
translation,296,184,experiments,genx,use,end-to - end semantic parsing model,genx use end-to - end semantic parsing model,0.6069179773330688
translation,296,184,experiments,end-to - end semantic parsing model,from,pasupat and liang ( 2015 ),end-to - end semantic parsing model from pasupat and liang ( 2015 ),0.5602108240127563
translation,296,5,model,end-to- end differentiable model,interpreting questions about,knowledge graph ( kg ),end-to- end differentiable model interpreting questions about knowledge graph ( kg ),0.6138558387756348
translation,296,6,model,each span,of,text,each span of text,0.5993877649307251
translation,296,6,model,text,represented by,denotation,text represented by denotation,0.7305794358253479
translation,296,6,model,vector,captures,ungrounded aspects of meaning,vector captures ungrounded aspects of meaning,0.7405745387077332
translation,296,6,model,denotation,has,in a kg,denotation has in a kg,0.596345067024231
translation,296,6,model,model,has,each span,model has each span,0.5947824716567993
translation,296,10,model,variety of challenging semantic operators,such as,quantifiers,variety of challenging semantic operators such as quantifiers,0.6308974027633667
translation,296,10,model,variety of challenging semantic operators,such as,disjunctions,variety of challenging semantic operators such as disjunctions,0.6350115537643433
translation,296,10,model,variety of challenging semantic operators,such as,composed relations,variety of challenging semantic operators such as composed relations,0.6026803255081177
translation,296,10,model,variety of challenging semantic operators,infers,latent syntactic structure,variety of challenging semantic operators infers latent syntactic structure,0.6739268898963928
translation,296,10,model,model,learns,variety of challenging semantic operators,model learns variety of challenging semantic operators,0.6893499493598938
translation,296,17,model,latent tree,of,interpretable expressions,latent tree of interpretable expressions,0.5436157584190369
translation,296,17,model,latent tree,recursively combining,constituents,latent tree recursively combining constituents,0.7879166007041931
translation,296,17,model,interpretable expressions,over,sentence,interpretable expressions over sentence,0.6762501001358032
translation,296,17,model,constituents,using,small set of neural modules,constituents using small set of neural modules,0.664025604724884
translation,296,36,model,independence assumptions,give,linguistically motivated inductive bias,independence assumptions give linguistically motivated inductive bias,0.6064703464508057
translation,296,41,model,end-to - end differentiable model,jointly learns,structures and modules,end-to - end differentiable model jointly learns structures and modules,0.702384352684021
translation,296,41,model,structures and modules,by,gradient descent,structures and modules by gradient descent,0.5499860644340515
translation,296,41,model,model,has,end-to - end differentiable model,model has end-to - end differentiable model,0.5253418684005737
translation,296,42,model,model,is,new combination of classical and neural methods,model is new combination of classical and neural methods,0.5880946516990662
translation,296,42,model,model,maintains,interpretability and generalization behaviour,model maintains interpretability and generalization behaviour,0.7038094997406006
translation,296,42,model,new combination of classical and neural methods,maintains,interpretability and generalization behaviour,new combination of classical and neural methods maintains interpretability and generalization behaviour,0.6189582943916321
translation,296,42,model,interpretability and generalization behaviour,of,semantic parsing,interpretability and generalization behaviour of semantic parsing,0.5694897174835205
translation,296,42,model,interpretability and generalization behaviour,while being,end-to - end differentiable,interpretability and generalization behaviour while being end-to - end differentiable,0.6314592957496643
translation,296,42,model,model,is,new combination of classical and neural methods,model is new combination of classical and neural methods,0.5880946516990662
translation,296,42,model,model,has,model,model has model,0.5623406171798706
translation,296,203,model,operators,not considered during,development,operators not considered during development,0.6886476278305054
translation,296,203,model,model,learns to,operators,model learns to operators,0.6563255786895752
translation,296,203,model,model,interpret,operators,model interpret operators,0.7256428003311157
translation,296,18,results,rnn encoders,particularly when,test questions,rnn encoders particularly when test questions,0.5895404815673828
translation,296,18,results,test questions,are,longer,test questions are longer,0.6215423941612244
translation,296,18,results,longer,than,training questions,longer than training questions,0.5683031678199768
translation,296,18,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,296,18,results,outperforms,has,rnn encoders,outperforms has rnn encoders,0.6075160503387451
translation,296,18,results,results,has,our model,results has our model,0.5871725678443909
translation,296,177,results,lstm ( no kg ),has,accuracy,lstm ( no kg ) has accuracy,0.5678354501724243
translation,296,177,results,accuracy,close to,chance,accuracy close to chance,0.7400156259536743
translation,296,177,results,lstm ( no kg ),has,accuracy,lstm ( no kg ) has accuracy,0.5678354501724243
translation,296,177,results,results,has,lstm ( no kg ),results has lstm ( no kg ),0.5352078676223755
translation,296,178,results,almost perfectly solves,showing,ability,almost perfectly solves showing ability,0.7141125202178955
translation,296,178,results,all questions,showing,ability,all questions showing ability,0.7436114549636841
translation,296,178,results,ability,to learn,challenging,ability to learn challenging,0.6466420292854309
translation,296,178,results,ability,to learn,semantic operators,ability to learn semantic operators,0.6031790971755981
translation,296,178,results,parse questions,using,weak end-to - end supervision,parse questions using weak end-to - end supervision,0.627589225769043
translation,296,178,results,our model,has,almost perfectly solves,our model has almost perfectly solves,0.6135132312774658
translation,296,178,results,almost perfectly solves,has,all questions,almost perfectly solves has all questions,0.6159771084785461
translation,296,178,results,challenging,has,semantic operators,challenging has semantic operators,0.588709831237793
translation,296,178,results,results,has,our model,results has our model,0.5871725678443909
translation,296,189,results,all baseline models,fail to,generalize,all baseline models fail to generalize,0.7240300178527832
translation,296,189,results,generalize,to,questions,generalize to questions,0.6278966069221497
translation,296,189,results,well,to,questions,well to questions,0.6159866452217102
translation,296,189,results,questions,requiring,longer chains of reasoning,questions requiring longer chains of reasoning,0.6740319132804871
translation,296,189,results,longer chains of reasoning,than,training,longer chains of reasoning than training,0.5860751867294312
translation,296,189,results,generalize,has,well,generalize has well,0.6266511678695679
translation,296,189,results,results,has,all baseline models,results has all baseline models,0.4813043773174286
translation,296,190,results,substantially outperforms,showing,ability,substantially outperforms showing ability,0.7398938536643982
translation,296,190,results,ability,to perform,complex multi-hop reasoning,ability to perform complex multi-hop reasoning,0.6904181838035583
translation,296,190,results,model,has,substantially outperforms,model has substantially outperforms,0.6249649524688721
translation,296,190,results,substantially outperforms,has,baselines,substantially outperforms has baselines,0.6038005948066711
translation,296,190,results,results,has,model,results has model,0.5339115858078003
translation,296,196,results,all baselines,achieve,close to random performance,all baselines achieve close to random performance,0.5956427454948425
translation,296,196,results,results,has,all baselines,results has all baselines,0.526987612247467
translation,296,199,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,296,199,results,outperforms,has,tree - lstm ( unsup . ),outperforms has tree - lstm ( unsup . ),0.608921468257904
translation,296,199,results,outperforms,has,version of our model,outperforms has version of our model,0.6597055792808533
translation,296,199,results,results,has,our model,results has our model,0.5871725678443909
translation,296,201,results,outperforms,has,strong lstm and semantic parsing baselines,outperforms has strong lstm and semantic parsing baselines,0.57341068983078
translation,297,225,ablation-analysis,lm filtering,leads to,improved f1 / em metrics,lm filtering leads to improved f1 / em metrics,0.6543494462966919
translation,297,225,ablation-analysis,ablation analysis,in,majority of the experiments,ablation analysis in majority of the experiments,0.5119002461433411
translation,297,30,baselines,aqgen,generates,answer,aqgen generates answer,0.7833185791969299
translation,297,30,baselines,answer,then,question,answer then question,0.6182786226272583
translation,297,30,baselines,question,generates,answer,question generates answer,0.6737707257270813
translation,297,30,baselines,question,concatenates,answer,question concatenates answer,0.7207133769989014
translation,297,30,baselines,question,generates,answer,question generates answer,0.6737707257270813
translation,297,30,baselines,qagen two-step ( 2s ),generates,answer,qagen two-step ( 2s ) generates answer,0.7202156782150269
translation,297,30,baselines,answer,in,second pass,answer in second pass,0.5449609160423279
translation,297,30,baselines,second pass,through,same encoder-decoder,second pass through same encoder-decoder,0.6864049434661865
translation,297,57,baselines,qagen,generates,question and answer jointly,qagen generates question and answer jointly,0.7178874015808105
translation,297,57,baselines,question and answer jointly,given,input passage,question and answer jointly given input passage,0.7268070578575134
translation,297,57,baselines,baselines,has,qagen,baselines has qagen,0.5761241316795349
translation,297,60,baselines,question generation and answer generation,in,two separate passes,question generation and answer generation in two separate passes,0.5170577764511108
translation,297,60,baselines,two separate passes,over,enc- dec lm,two separate passes over enc- dec lm,0.7565651535987854
translation,297,60,baselines,qagen,has,two -step ( 2s ),qagen has two -step ( 2s ),0.5770913362503052
translation,297,60,baselines,baselines,has,qagen,baselines has qagen,0.5761241316795349
translation,297,144,experimental-setup,generative models,trained on,squad 1.1,generative models trained on squad 1.1,0.6997168064117432
translation,297,144,experimental-setup,squad 1.1,for,5 epochs,squad 1.1 for 5 epochs,0.6679895520210266
translation,297,144,experimental-setup,best model,selected based on,cross entropy loss,best model selected based on cross entropy loss,0.5859201550483704
translation,297,144,experimental-setup,cross entropy loss,on,squad dev set,cross entropy loss on squad dev set,0.5190501809120178
translation,297,144,experimental-setup,experimental setup,has,generative models,experimental setup has generative models,0.5311566591262817
translation,297,145,experimental-setup,"adamw ( loshchilov and hutter , 2017 ) optimizer",with,learning rate,"adamw ( loshchilov and hutter , 2017 ) optimizer with learning rate",0.598147988319397
translation,297,145,experimental-setup,learning rate,of,3 ? 10 ?5,learning rate of 3 ? 10 ?5,0.652306854724884
translation,297,145,experimental-setup,experimental setup,has,"adamw ( loshchilov and hutter , 2017 ) optimizer","experimental setup has adamw ( loshchilov and hutter , 2017 ) optimizer",0.5110814571380615
translation,297,146,experimental-setup,rc model training,use,bert- base -uncased model,rc model training use bert- base -uncased model,0.612288236618042
translation,297,146,experimental-setup,experimental setup,For,rc model training,experimental setup For rc model training,0.5749032497406006
translation,297,148,experimental-setup,384,with,document stride,384 with document stride,0.6707057356834412
translation,297,148,experimental-setup,maximum sequence length,has,384,maximum sequence length has 384,0.5749037861824036
translation,297,148,experimental-setup,document stride,has,128,document stride has 128,0.5968621373176575
translation,297,148,experimental-setup,experimental setup,set,maximum sequence length,experimental setup set maximum sequence length,0.653292715549469
translation,297,319,experimental-setup,warmup,set to,10 %,warmup set to 10 %,0.7254105806350708
translation,297,319,experimental-setup,10 %,of,total training steps,10 % of total training steps,0.6046215295791626
translation,297,319,experimental-setup,generative models,has,warmup,generative models has warmup,0.5548216104507446
translation,297,319,experimental-setup,experimental setup,training of,generative models,experimental setup training of generative models,0.6651387810707092
translation,297,320,experimental-setup,batch size,of,24,batch size of 24,0.6606289148330688
translation,297,321,experimental-setup,2 to 3 hours,on,3,2 to 3 hours on 3,0.6324649453163147
translation,297,321,experimental-setup,2 to 3 hours,on,gpus,2 to 3 hours on gpus,0.5285305976867676
translation,297,321,experimental-setup,3,has,gpus,3 has gpus,0.525983452796936
translation,297,324,experimental-setup,each epoch,of,training,each epoch of training,0.613933265209198
translation,297,324,experimental-setup,training,took,2 to 12 hours,training took 2 to 12 hours,0.6251894235610962
translation,297,324,experimental-setup,experimental setup,has,each epoch,experimental setup has each epoch,0.5484756827354431
translation,297,325,experimental-setup,hyperparameters,of,generative and rc downstream models,hyperparameters of generative and rc downstream models,0.537424623966217
translation,297,325,experimental-setup,generative and rc downstream models,were,fixed,generative and rc downstream models were fixed,0.6394132971763611
translation,297,325,experimental-setup,experimental setup,has,hyperparameters,experimental setup has hyperparameters,0.4710215926170349
translation,297,137,experiments,bioasq,employed,mrqa shared task version,bioasq employed mrqa shared task version,0.6855790019035339
translation,297,137,experiments,", 2015 )",employed,mrqa shared task version,", 2015 ) employed mrqa shared task version",0.5784395337104797
translation,297,137,experiments,mrqa shared task version,of,bioasq,mrqa shared task version of bioasq,0.6171397566795349
translation,297,137,experiments,mrqa shared task version,consists of,dev set,mrqa shared task version consists of dev set,0.6562930941581726
translation,297,137,experiments,dev set,with,"1,504 samples","dev set with 1,504 samples",0.6355818510055542
translation,297,147,experiments,adamw optimizer,used with,learning rate,adamw optimizer used with learning rate,0.646440863609314
translation,297,147,experiments,adamw optimizer,used with,batch size,adamw optimizer used with batch size,0.6370018124580383
translation,297,147,experiments,adamw optimizer,without,linear warmup,adamw optimizer without linear warmup,0.6691107153892517
translation,297,147,experiments,learning rate,of,3 ? 10 ?5,learning rate of 3 ? 10 ?5,0.652306854724884
translation,297,147,experiments,24,for,2 epochs,24 for 2 epochs,0.6790614724159241
translation,297,147,experiments,24,without,linear warmup,24 without linear warmup,0.6981218457221985
translation,297,147,experiments,2 epochs,without,linear warmup,2 epochs without linear warmup,0.7352616786956787
translation,297,147,experiments,batch size,has,24,batch size has 24,0.6131199598312378
translation,297,157,experiments,bart and gpt2,achieved,21.29 and 18.31 bleu,bart and gpt2 achieved 21.29 and 18.31 bleu,0.6963515281677246
translation,297,198,experiments,rc model,with,synthetic data,rc model with synthetic data,0.6154100894927979
translation,297,198,experiments,synthetic data,from,all the four domains,synthetic data from all the four domains,0.5518640875816345
translation,297,208,experiments,qagen2s,with,lm filtering,qagen2s with lm filtering,0.688121497631073
translation,297,208,experiments,lm filtering,used as,generator,lm filtering used as generator,0.6182811856269836
translation,297,8,hyperparameters,generator,trained by,finetuning,generator trained by finetuning,0.748985767364502
translation,297,8,hyperparameters,pretrained lm,using,maximum likelihood estimation,pretrained lm using maximum likelihood estimation,0.6480427980422974
translation,297,8,hyperparameters,finetuning,has,pretrained lm,finetuning has pretrained lm,0.5676189661026001
translation,297,8,hyperparameters,hyperparameters,has,generator,hyperparameters has generator,0.5314127802848816
translation,297,29,hyperparameters,bart,as,pretrained lm,bart as pretrained lm,0.5914961099624634
translation,297,29,hyperparameters,hyperparameters,use,bart,hyperparameters use bart,0.6282802224159241
translation,297,4,model,end-to- end approach,for,synthetic qa data generation,end-to- end approach for synthetic qa data generation,0.6065845489501953
translation,297,4,model,model,propose,end-to- end approach,model propose end-to- end approach,0.717731773853302
translation,297,5,model,single transformer - based encoderdecoder network,trained,end-to -end,single transformer - based encoderdecoder network trained end-to -end,0.6947198510169983
translation,297,5,model,end-to -end,to generate,answers and questions,end-to -end to generate answers and questions,0.6882420778274536
translation,297,5,model,model,comprises,single transformer - based encoderdecoder network,model comprises single transformer - based encoderdecoder network,0.6410121321678162
translation,297,6,model,passage,to,encoder,passage to encoder,0.6148511171340942
translation,297,6,model,decoder,to generate,question,decoder to generate question,0.7217492461204529
translation,297,6,model,decoder,to generate,answer,decoder to generate answer,0.7186844348907471
translation,297,6,model,answer,has,token - by- token,answer has token - by- token,0.6343357563018799
translation,297,6,model,model,feed,passage,model feed passage,0.7225550413131714
translation,297,24,model,end-to - end approach,for,synthetic qa data generation,end-to - end approach for synthetic qa data generation,0.6065845489501953
translation,297,24,model,model,propose,end-to - end approach,model propose end-to - end approach,0.717731773853302
translation,297,25,model,single transformer - based encoder- decoder network,trained,end-to -end,single transformer - based encoder- decoder network trained end-to -end,0.7039260864257812
translation,297,25,model,end-to -end,to generate,answer and the question,end-to -end to generate answer and the question,0.7193565368652344
translation,297,25,model,model,comprises,single transformer - based encoder- decoder network,model comprises single transformer - based encoder- decoder network,0.6502694487571716
translation,297,26,model,passage,to,encoder,passage to encoder,0.6148511171340942
translation,297,26,model,decoder,to generate,question,decoder to generate question,0.7217492461204529
translation,297,26,model,decoder,to generate,answer token - bytoken,decoder to generate answer token - bytoken,0.6730366349220276
translation,297,26,model,model,feed,passage,model feed passage,0.7225550413131714
translation,297,34,results,qagen,performs,better,qagen performs better,0.703192949295044
translation,297,34,results,qagen,performs,baselines,qagen performs baselines,0.713287889957428
translation,297,34,results,better,than,aqgen,better than aqgen,0.6828954219818115
translation,297,34,results,better,than,baselines,better than baselines,0.6307952404022217
translation,297,34,results,baselines,for,all datasets,baselines for all datasets,0.5390927791595459
translation,297,34,results,qagen2s,provides,best results,qagen2s provides best results,0.6652030944824219
translation,297,34,results,results,has,qagen,results has qagen,0.5340508222579956
translation,297,35,results,qagen2s,improves,squad baseline,qagen2s improves squad baseline,0.743947446346283
translation,297,35,results,squad baseline,by,more than 8 points,squad baseline by more than 8 points,0.5793983936309814
translation,297,35,results,squad baseline,by,more than 7 points,squad baseline by more than 7 points,0.579841136932373
translation,297,35,results,more than 8 points,in,em,more than 8 points in em,0.6184510588645935
translation,297,35,results,more than 7 points,in,f1,more than 7 points in f1,0.5613644123077393
translation,297,35,results,nq dataset,has,qagen2s,nq dataset has qagen2s,0.5860518217086792
translation,297,35,results,results,For,nq dataset,results For nq dataset,0.6370930671691895
translation,297,36,results,newsqa and bioasq,gains in,em,newsqa and bioasq gains in em,0.7662534117698669
translation,297,36,results,results,For,newsqa and bioasq,results For newsqa and bioasq,0.6195140480995178
translation,297,37,results,synthetically generated data,by,qagen2s,synthetically generated data by qagen2s,0.6360810399055481
translation,297,37,results,in-domain performance,of,small and large rc models,in-domain performance of small and large rc models,0.5903998017311096
translation,297,37,results,in-domain performance,both,small and large rc models,in-domain performance both small and large rc models,0.6579715013504028
translation,297,37,results,in-domain performance,leading to,f1,in-domain performance leading to f1,0.6869566440582275
translation,297,37,results,/ em improvements,of,1/0.5 and 3.1/2.2,/ em improvements of 1/0.5 and 3.1/2.2,0.6692400574684143
translation,297,37,results,/ em improvements,of,bert- base -uncased trained rc models,/ em improvements of bert- base -uncased trained rc models,0.6208221316337585
translation,297,37,results,1/0.5 and 3.1/2.2,on,roberta - large,1/0.5 and 3.1/2.2 on roberta - large,0.5820353031158447
translation,297,37,results,bert- base -uncased trained rc models,on,squad dev,bert- base -uncased trained rc models on squad dev,0.571949303150177
translation,297,37,results,f1,has,/ em improvements,f1 has / em improvements,0.6164173483848572
translation,297,37,results,results,demonstrate,synthetically generated data,results demonstrate synthetically generated data,0.5876376032829285
translation,297,174,results,results,has,domain adaptation results,results has domain adaptation results,0.5414679646492004
translation,297,180,results,outperform,by,wide margins,outperform by wide margins,0.6592182517051697
translation,297,180,results,baseline models,trained on,squad 1.1 only,baseline models trained on squad 1.1 only,0.7359094619750977
translation,297,180,results,qagen and qa - gen2s models,has,outperform,qagen and qa - gen2s models has outperform,0.6332995295524597
translation,297,180,results,wide margins,has,baseline models,wide margins has baseline models,0.6096761226654053
translation,297,180,results,results,Our,qagen and qa - gen2s models,results Our qagen and qa - gen2s models,0.6383936405181885
translation,297,180,results,results,has,qagen and qa - gen2s models,results has qagen and qa - gen2s models,0.5216103792190552
translation,297,181,results,qagen and qagen2s,has,significantly outperforms,qagen and qagen2s has significantly outperforms,0.6229503750801086
translation,297,181,results,significantly outperforms,has,qgen,significantly outperforms has qgen,0.6011164784431458
translation,297,181,results,results,has,qagen and qagen2s,results has qagen and qagen2s,0.5369936227798462
translation,297,183,results,lm and round-trip filtering,applied to,best performing model,lm and round-trip filtering applied to best performing model,0.7116430997848511
translation,297,183,results,lm and round-trip filtering,observe that,lm filtering approach,lm and round-trip filtering observe that lm filtering approach,0.6226223707199097
translation,297,183,results,lm and round-trip filtering,observe that,more effective,lm and round-trip filtering observe that more effective,0.600564181804657
translation,297,183,results,lm filtering approach,is,more effective,lm filtering approach is more effective,0.5907832980155945
translation,297,183,results,more effective,than,round-trip filtering,more effective than round-trip filtering,0.5705180764198303
translation,297,183,results,round-trip filtering,in,bioasq and duorc target domains,round-trip filtering in bioasq and duorc target domains,0.5361213684082031
translation,297,183,results,results,Comparing,lm and round-trip filtering,results Comparing lm and round-trip filtering,0.6484416723251343
translation,297,184,results,barely underperforms ( ? 1 point ),in,f1 and em,barely underperforms ( ? 1 point ) in f1 and em,0.5638799071311951
translation,297,184,results,f1 and em,in,other two domains,f1 and em in other two domains,0.562478244304657
translation,297,184,results,results,has,barely underperforms ( ? 1 point ),results has barely underperforms ( ? 1 point ),0.585171103477478
translation,297,186,results,highest ( em / f1 ) domain adaptation gains,seen with,bioasq ( 4/2.2 ) and duorc ( 1.2/1.1 ),highest ( em / f1 ) domain adaptation gains seen with bioasq ( 4/2.2 ) and duorc ( 1.2/1.1 ),0.6875136494636536
translation,297,186,results,results,has,highest ( em / f1 ) domain adaptation gains,results has highest ( em / f1 ) domain adaptation gains,0.5445118546485901
translation,297,189,results,supervised target domain training,of,duorc,supervised target domain training of duorc,0.5788356065750122
translation,297,189,results,supervised target domain training,of,duorc training set,supervised target domain training of duorc training set,0.5478050112724304
translation,297,189,results,synthetic data,outperforms,duorc training set,synthetic data outperforms duorc training set,0.7356780171394348
translation,297,189,results,duorc training set,consists of,39144 pairs,duorc training set consists of 39144 pairs,0.6219041347503662
translation,297,189,results,results,Comparing,supervised target domain training,results Comparing supervised target domain training,0.5737183094024658
translation,297,189,results,results,with,supervised target domain training,results with supervised target domain training,0.5916712284088135
translation,297,190,results,our domain adaptation methods,show,substantial gains,our domain adaptation methods show substantial gains,0.6167131662368774
translation,297,190,results,substantial gains,with,newsqa and natural questions domain,substantial gains with newsqa and natural questions domain,0.628986656665802
translation,297,190,results,improvements,to match,performance,improvements to match performance,0.702247142791748
translation,297,190,results,performance,of,supervised target domain training,performance of supervised target domain training,0.5775957703590393
translation,297,190,results,results,has,our domain adaptation methods,results has our domain adaptation methods,0.5017521381378174
translation,297,192,results,generating synthetic qa data,from,target domain text,generating synthetic qa data from target domain text,0.5219152569770813
translation,297,192,results,generating synthetic qa data,leads to,significant gains,generating synthetic qa data leads to significant gains,0.6650912165641785
translation,297,192,results,significant gains,on,target domain dev set,significant gains on target domain dev set,0.5567097663879395
translation,297,192,results,results,suggest,generating synthetic qa data,results suggest generating synthetic qa data,0.5823063850402832
translation,297,201,results,em / f1 scores,of,cross-domain rc models,em / f1 scores of cross-domain rc models,0.5711385011672974
translation,297,201,results,cross-domain rc models,on,squad 1.1 dev set,cross-domain rc models on squad 1.1 dev set,0.4956350028514862
translation,297,202,results,synthetic data,from,any of the four domains,synthetic data from any of the four domains,0.5488404631614685
translation,297,202,results,performance,for,squad,performance for squad,0.6903296709060669
translation,297,202,results,synthetic data,has,significantly improved,synthetic data has significantly improved,0.5898053050041199
translation,297,202,results,any of the four domains,has,significantly improved,any of the four domains has significantly improved,0.5267420411109924
translation,297,202,results,significantly improved,has,performance,significantly improved has performance,0.5962094068527222
translation,297,202,results,results,see that,synthetic data,results see that synthetic data,0.6100749969482422
translation,297,202,results,results,using,synthetic data,results using synthetic data,0.606222927570343
translation,297,203,results,rc model,with,data,rc model with data,0.6530122756958008
translation,297,203,results,data,from,all domains + squad training data,data from all domains + squad training data,0.5189383029937744
translation,297,203,results,large gain,in both,em ( 3.8 ) and f1 ( 2.7 ),large gain in both em ( 3.8 ) and f1 ( 2.7 ),0.6682361364364624
translation,297,203,results,training,has,rc model,training has rc model,0.5778767466545105
translation,297,203,results,results,when,training,results when training,0.6565081477165222
translation,297,207,results,qagen2s,has,outperforms,qagen2s has outperforms,0.6656522154808044
translation,297,207,results,outperforms,has,qagen,outperforms has qagen,0.6364363431930542
translation,297,207,results,results,has,qagen2s,results has qagen2s,0.5477055311203003
translation,297,213,results,beam search,has,underperforms,beam search has underperforms,0.6007521152496338
translation,297,213,results,underperforms,has,topk + nucleus,underperforms has topk + nucleus,0.6354735493659973
translation,297,213,results,results,indicate,beam search,results indicate beam search,0.5794569849967957
translation,297,218,results,10 pairs per document,leads to,best em / f1,10 pairs per document leads to best em / f1,0.6047978401184082
translation,297,218,results,best em / f1,on,target domain,best em / f1 on target domain,0.5387842059135437
translation,297,218,results,results,sampling,10 pairs per document,results sampling 10 pairs per document,0.612487256526947
translation,297,226,results,aqgen,benefits the most,lm filtering,aqgen benefits the most lm filtering,0.6160633563995361
translation,297,226,results,lower quality,than,other two models,lower quality than other two models,0.6079647541046143
translation,297,226,results,results,has,aqgen,results has aqgen,0.5843507647514343
translation,297,241,results,increasing,has,"number of generated ( q , a ) pairs ( 5 pairs per passage )","increasing has number of generated ( q , a ) pairs ( 5 pairs per passage )",0.5871099233627319
translation,297,241,results,"number of generated ( q , a ) pairs ( 5 pairs per passage )",has,rc model performance,"number of generated ( q , a ) pairs ( 5 pairs per passage ) has rc model performance",0.5551949143409729
translation,297,241,results,rc model performance,has,improves,rc model performance has improves,0.6146906018257141
translation,297,251,results,domain adaptation,on,nq domain,domain adaptation on nq domain,0.5663939714431763
translation,297,251,results,nq domain,using,qagen2s generated samples,nq domain using qagen2s generated samples,0.7288968563079834
translation,297,253,results,squad 1.1 baselines,significantly higher than,bert - base,squad 1.1 baselines significantly higher than bert - base,0.5699521899223328
translation,297,254,results,em / f1 gains,of,5.8/3.4,em / f1 gains of 5.8/3.4,0.5942298173904419
translation,297,254,results,5.8/3.4,achieved,target domain,5.8/3.4 achieved target domain,0.6787474155426025
translation,297,254,results,2,has,em / f1 gains,2 has em / f1 gains,0.6697074770927429
translation,297,254,results,results,has,2,results has 2,0.48405981063842773
translation,297,254,results,results,has,em / f1 gains,results has em / f1 gains,0.5668339729309082
translation,297,255,results,1/0.5 gains,in,em / f1,1/0.5 gains in em / f1,0.6140280961990356
translation,297,255,results,1/0.5 gains,observed in,squad 1.1 dev set,1/0.5 gains observed in squad 1.1 dev set,0.6802921891212463
translation,297,255,results,results,has,1/0.5 gains,results has 1/0.5 gains,0.5660483241081238
translation,297,283,results,summation,has,outperforms,summation has outperforms,0.6377787590026855
translation,297,283,results,outperforms,has,averaging,outperforms has averaging,0.6256771683692932
translation,297,283,results,results,observe,summation,results observe summation,0.5585875511169434
translation,297,283,results,results,using,summation,results using summation,0.616241991519928
translation,297,294,results,generated samples,using,aqgen,generated samples using aqgen,0.708573579788208
translation,297,294,results,aqgen,have,lower quality,aqgen have lower quality,0.5847058892250061
translation,297,294,results,lower quality,than,other two models,lower quality than other two models,0.6079647541046143
translation,297,294,results,results,observe,generated samples,results observe generated samples,0.6022340655326843
translation,298,160,experiments,trefl,mainly focused on,improving recall,trefl mainly focused on improving recall,0.6034152507781982
translation,298,167,experiments,hlt-fbk - ev2 - trel2 + terfl,was,system answering,hlt-fbk - ev2 - trel2 + terfl was system answering,0.6691576242446899
translation,298,167,experiments,hlt-fbk - ev2 - trel2 + terfl,for,blogs,hlt-fbk - ev2 - trel2 + terfl for blogs,0.6648635268211365
translation,298,167,experiments,system answering,correctly,more questions,system answering correctly more questions,0.762208878993988
translation,298,167,experiments,more questions,about,distant entities,more questions about distant entities,0.621290922164917
translation,298,167,experiments,news and wiki,has,hlt-fbk - ev2 - trel2 + terfl,news and wiki has hlt-fbk - ev2 - trel2 + terfl,0.6300642490386963
translation,298,143,results,all the others,by,significant margin,all the others by significant margin,0.6288103461265564
translation,298,143,results,participant system hlt-fbk -ev2 - trel2 system (,has,.30 r ),participant system hlt-fbk -ev2 - trel2 system ( has .30 r ),0.6053327918052673
translation,298,143,results,participant system hlt-fbk -ev2 - trel2 system (,has,outperformed,participant system hlt-fbk -ev2 - trel2 system ( has outperformed,0.6017282009124756
translation,298,143,results,.30 r ),has,outperformed,.30 r ) has outperformed,0.6270902752876282
translation,298,143,results,outperformed,has,all the others,outperformed has all the others,0.625552237033844
translation,298,143,results,results,has,participant system hlt-fbk -ev2 - trel2 system (,results has participant system hlt-fbk -ev2 - trel2 system (,0.5231893062591553
translation,298,144,results,caevo,performed,best,caevo performed best,0.2508608102798462
translation,298,144,results,caevo,behind,winning participant recall,caevo behind winning participant recall,0.6698421835899353
translation,298,144,results,best,among,off-theshelf systems,best among off-theshelf systems,0.6115595102310181
translation,298,144,results,winning participant recall,by,13 % absolute,winning participant recall by 13 % absolute,0.5956438779830933
translation,298,144,results,results,has,caevo,results has caevo,0.5739958882331848
translation,298,150,results,best overall sys - tem,maintained,top position,best overall sys - tem maintained top position,0.6524454355239868
translation,298,150,results,hlt -fbk - ev2 - trel2,maintained,top position,hlt -fbk - ev2 - trel2 maintained top position,0.6343207359313965
translation,298,150,results,results,has,best overall sys - tem,results has best overall sys - tem,0.5791294574737549
translation,298,157,results,went up,on,all systems,went up on all systems,0.569950520992279
translation,298,157,results,all systems,by,49 %,all systems by 49 %,0.6028155088424683
translation,298,157,results,recall,has,went up,recall has went up,0.6630489230155945
translation,298,157,results,results,has,recall,results has recall,0.6362076997756958
translation,298,158,results,top system ( hlt - fbk-ev2 - trel2 ),improved,4 % absolute ( 13 % relative ),top system ( hlt - fbk-ev2 - trel2 ) improved 4 % absolute ( 13 % relative ),0.7406390905380249
translation,298,159,results,largest gain,with,tipsem,largest gain with tipsem,0.6978579759597778
translation,298,159,results,results,has,largest gain,results has largest gain,0.5784004926681519
translation,298,166,results,genre,on average,trefl,genre on average trefl,0.6349561810493469
translation,298,166,results,trefl,improved,systems ' relative recall,trefl improved systems ' relative recall,0.6771467924118042
translation,298,166,results,systems ' relative recall,by,60 % ( news ),systems ' relative recall by 60 % ( news ),0.5711184740066528
translation,298,166,results,systems ' relative recall,by,48 % ( wiki ),systems ' relative recall by 48 % ( wiki ),0.5760273337364197
translation,298,166,results,systems ' relative recall,by,47 % ( news ),systems ' relative recall by 47 % ( news ),0.5731624364852905
translation,298,166,results,results,By,genre,results By genre,0.4621726870536804
translation,298,168,results,hlt-fbk - ev2 - trel2 + terfl,answers,more questions,hlt-fbk - ev2 - trel2 + terfl answers more questions,0.7869346737861633
translation,298,168,results,results,found that,hlt-fbk - ev2 - trel2 + terfl,results found that hlt-fbk - ev2 - trel2 + terfl,0.601840078830719
translation,298,183,results,best obtained recalls,are,.52,best obtained recalls are .52,0.5703873634338379
translation,298,183,results,best obtained recalls,are,.39,best obtained recalls are .39,0.5767301321029663
translation,298,183,results,best obtained recalls,are,.42,best obtained recalls are .42,0.5666322112083435
translation,298,183,results,.52,in,news,.52 in news,0.5785635113716125
translation,298,183,results,.39,in,wiki,.39 in wiki,0.5319475531578064
translation,298,183,results,.42,in,blogs,.42 in blogs,0.5418620705604553
translation,298,183,results,.42,compared to,".38 , .36 and .22",".42 compared to .38 , .36 and .22",0.6141485571861267
translation,298,183,results,".38 , .36 and .22",obtained for,yes -questions,".38 , .36 and .22 obtained for yes -questions",0.6062548160552979
translation,298,183,results,results,has,best obtained recalls,results has best obtained recalls,0.562208354473114
translation,298,191,results,best overall recall,was,30 %,best overall recall was 30 %,0.6110948324203491
translation,298,191,results,30 %,has,34 %,30 % has 34 %,0.6344707012176514
translation,298,191,results,results,has,best overall recall,results has best overall recall,0.5610206723213196
translation,298,192,results,top result,higher than,best off - the-shelf system,top result higher than best off - the-shelf system,0.6806614995002747
translation,298,192,results,best off - the-shelf system,has,17 %,best off - the-shelf system has 17 %,0.5604618787765503
translation,298,192,results,best off - the-shelf system,has,27 %,best off - the-shelf system has 27 %,0.5584220886230469
translation,298,192,results,17 %,has,27 %,17 % has 27 %,0.6499349474906921
translation,298,192,results,results,has,top result,results has top result,0.606375515460968
translation,298,193,results,only system,using,event co-reference,only system using event co-reference,0.6926446557044983
translation,298,193,results,event co-reference,obtained,best results,event co-reference obtained best results,0.7020017504692078
translation,298,193,results,best results,adding,event coref,best results adding event coref,0.730015754699707
translation,298,193,results,event coref,help,other systems,event coref help other systems,0.7633978128433228
translation,298,194,results,trefl,improved,qa recall,trefl improved qa recall,0.7033265233039856
translation,298,194,results,qa recall,of,all systems,qa recall of all systems,0.6010857820510864
translation,298,194,results,qa recall,ranging from,3 % to 12 % absolute ( 13 % to 80 % relative ),qa recall ranging from 3 % to 12 % absolute ( 13 % to 80 % relative ),0.608654260635376
translation,298,194,results,results,Adding,trefl,results Adding trefl,0.6671702861785889
translation,299,84,baselines,memn2n,uses,end-to - end memory networks,memn2n uses end-to - end memory networks,0.6392621397972107
translation,299,84,baselines,end-to - end memory networks,with,static set,end-to - end memory networks with static set,0.6361871361732483
translation,299,84,baselines,static set,of,initial memory slots,static set of initial memory slots,0.5817446112632751
translation,299,88,baselines,memexnet,uses,textual representation,memexnet uses textual representation,0.5976018905639648
translation,299,88,baselines,memexnet,frames,problem,memexnet frames problem,0.739788830280304
translation,299,88,baselines,textual representation,for,multi-modal attributes,textual representation for multi-modal attributes,0.5719480514526367
translation,299,88,baselines,problem,into,classification problem,problem into classification problem,0.6401354074478149
translation,299,88,baselines,classification problem,via,text kernel match approaches,classification problem via text kernel match approaches,0.6281332969665527
translation,299,88,baselines,text kernel match approaches,to predict,approximate answers,text kernel match approaches to predict approximate answers,0.6325829029083252
translation,299,88,baselines,baselines,has,memexnet,baselines has memexnet,0.5485280752182007
translation,299,92,baselines,any of the module networks,predicts,mgn graph output,any of the module networks predicts mgn graph output,0.7177280783653259
translation,299,92,baselines,mgn graph output,as,answers,mgn graph output as answers,0.5232268571853638
translation,299,87,hyperparameters,memory slot size,tuned as,hyperparameter,memory slot size tuned as hyperparameter,0.6384355425834656
translation,299,87,hyperparameters,hyperparameters,has,memory slot size,hyperparameters has memory slot size,0.5031461715698242
translation,299,94,hyperparameters,bi-lstm hidden states,for,language model,bi-lstm hidden states for language model,0.5763647556304932
translation,299,94,hyperparameters,max memory slots,has,"{ 1 , 5 , 10 , 20 , 40 , 80 }","max memory slots has { 1 , 5 , 10 , 20 , 40 , 80 }",0.5757169723510742
translation,299,94,hyperparameters,hyperparameters,tune,parameters,hyperparameters tune parameters,0.6951712965965271
translation,299,95,hyperparameters,parameters,with,"adagrad ( duchi et al. , 2011 )","parameters with adagrad ( duchi et al. , 2011 )",0.5000483989715576
translation,299,95,hyperparameters,"adagrad ( duchi et al. , 2011 )",with,batch size,"adagrad ( duchi et al. , 2011 ) with batch size",0.5456205606460571
translation,299,95,hyperparameters,"adagrad ( duchi et al. , 2011 )",with,learning rate,"adagrad ( duchi et al. , 2011 ) with learning rate",0.53758704662323
translation,299,95,hyperparameters,"adagrad ( duchi et al. , 2011 )",with,epsilon 10 ?8,"adagrad ( duchi et al. , 2011 ) with epsilon 10 ?8",0.6299458742141724
translation,299,95,hyperparameters,"adagrad ( duchi et al. , 2011 )",with,decay,"adagrad ( duchi et al. , 2011 ) with decay",0.5897694230079651
translation,299,95,hyperparameters,batch size,has,10,batch size has 10,0.6518703699111938
translation,299,95,hyperparameters,learning rate,has,0.01,learning rate has 0.01,0.5422797799110413
translation,299,95,hyperparameters,decay,has,0.1,decay has 0.1,0.49961456656455994
translation,299,95,hyperparameters,hyperparameters,optimize,parameters,hyperparameters optimize parameters,0.6659795045852661
translation,299,6,model,dynamic expansion,of,memory slots,dynamic expansion of memory slots,0.6032209396362305
translation,299,6,model,memory slots,through,graph traversals,memory slots through graph traversals,0.6534209251403809
translation,299,6,model,graph traversals,able to answer,queries,graph traversals able to answer queries,0.6554667949676514
translation,299,6,model,queries,in,contexts,queries in contexts,0.5222949385643005
translation,299,6,model,queries,in,external knowledge,queries in external knowledge,0.4944939613342285
translation,299,6,model,contexts,from,external knowledge,contexts from external knowledge,0.5507460832595825
translation,299,6,model,model,propose,memory graph networks ( mgn ),model propose memory graph networks ( mgn ),0.6484002470970154
translation,299,7,model,episodic memory qa net,with,multiple module networks,episodic memory qa net with multiple module networks,0.6011242866516113
translation,299,7,model,episodic memory qa net,to effectively handle,various question types,episodic memory qa net to effectively handle various question types,0.642058789730072
translation,299,7,model,model,propose,episodic memory qa net,model propose episodic memory qa net,0.669765830039978
translation,299,12,model,personal and retrospective questions,based on,memory graphs ( mg ),personal and retrospective questions based on memory graphs ( mg ),0.6281214952468872
translation,299,12,model,memory graphs ( mg ),where,each episodic memory,memory graphs ( mg ) where each episodic memory,0.5814399719238281
translation,299,12,model,answers,has,personal and retrospective questions,answers has personal and retrospective questions,0.5766115784645081
translation,299,13,model,memory graph network,walks from,initial nodes,memory graph network walks from initial nodes,0.675934910774231
translation,299,13,model,memory graph network,expands,memory slots,memory graph network expands memory slots,0.6810830235481262
translation,299,13,model,initial nodes,to attend to,relevant contexts,initial nodes to attend to relevant contexts,0.6532309651374817
translation,299,13,model,model,has,memory graph network,model has memory graph network,0.5366699695587158
translation,299,24,model,model,called,memory graph network ( mgn ),model called memory graph network ( mgn ),0.6794703006744385
translation,299,24,model,model,propose,model,model propose model,0.6740307211875916
translation,299,24,model,model,called,memory graph network ( mgn ),model called memory graph network ( mgn ),0.6794703006744385
translation,299,27,model,main episodic memory qa network,with,multiple module networks,main episodic memory qa network with multiple module networks,0.6083179712295532
translation,299,27,model,multiple module networks,such as,"choose , count , etc.","multiple module networks such as choose , count , etc.",0.6457500457763672
translation,299,27,model,model,implement,main episodic memory qa network,model implement main episodic memory qa network,0.6411774158477783
translation,299,28,model,large-scale dataset collection,for,episodic memory qa,large-scale dataset collection for episodic memory qa,0.564481258392334
translation,299,28,model,large-scale dataset collection,build,synthetic memory graph generator,large-scale dataset collection build synthetic memory graph generator,0.6848013997077942
translation,299,28,model,synthetic memory graph generator,creates,multiple episodic memory graph nodes,synthetic memory graph generator creates multiple episodic memory graph nodes,0.6597414016723633
translation,299,28,model,multiple episodic memory graph nodes,connected with,"real entities ( e.g. locations , events , public entities )","multiple episodic memory graph nodes connected with real entities ( e.g. locations , events , public entities )",0.6794903874397278
translation,299,28,model,"real entities ( e.g. locations , events , public entities )",appearing on,common-fact kgs,"real entities ( e.g. locations , events , public entities ) appearing on common-fact kgs",0.6263383626937866
translation,299,28,model,model,To bootstrap,large-scale dataset collection,model To bootstrap large-scale dataset collection,0.6596921682357788
translation,299,85,model,memory slot,represented with,bag-of-symbols,memory slot represented with bag-of-symbols,0.6735774278640747
translation,299,85,model,bag-of-symbols,for,surrounding attributes and nodes,bag-of-symbols for surrounding attributes and nodes,0.626594603061676
translation,299,85,model,model,has,memory slot,model has memory slot,0.5879961252212524
translation,299,86,model,single softmax layer,for,answer classification,single softmax layer for answer classification,0.596743106842041
translation,299,86,model,model,use,single softmax layer,model use single softmax layer,0.6008889675140381
translation,299,128,model,memory graph networks,extends,conventional memory networks,memory graph networks extends conventional memory networks,0.6722038388252258
translation,299,128,model,mgn ),extends,conventional memory networks,mgn ) extends conventional memory networks,0.7104139924049377
translation,299,128,model,conventional memory networks,by enabling,dynamic expansion,conventional memory networks by enabling dynamic expansion,0.7253149747848511
translation,299,128,model,dynamic expansion,of,memory slots,dynamic expansion of memory slots,0.6032209396362305
translation,299,128,model,memory slots,through,graph traversals,memory slots through graph traversals,0.6534209251403809
translation,299,128,model,memory graph networks,has,mgn ),memory graph networks has mgn ),0.5894579291343689
translation,299,129,model,several neural module networks,proposed for,proposed task,several neural module networks proposed for proposed task,0.7360889911651611
translation,299,129,model,several neural module networks,takes,queries and memory graphs,several neural module networks takes queries and memory graphs,0.6702205538749695
translation,299,129,model,queries and memory graphs,as,input,queries and memory graphs as input,0.4764542579650879
translation,299,129,model,input,to infer,answers,input to infer answers,0.7192698121070862
translation,299,129,model,model,has,several neural module networks,model has several neural module networks,0.5702353119850159
translation,299,130,model,main episodic memory qa net,aggregates,answer prediction,main episodic memory qa net aggregates answer prediction,0.6949906945228577
translation,299,130,model,answer prediction,from,each neural module,answer prediction from each neural module,0.5453020930290222
translation,299,98,results,outperforms,for,precision,outperforms for precision,0.6575056314468384
translation,299,98,results,other qa baselines,for,precision,other qa baselines for precision,0.6156683564186096
translation,299,98,results,precision,at,all ks,precision at all ks,0.5783255100250244
translation,299,98,results,proposed memory qa model,has,outperforms,proposed memory qa model has outperforms,0.6336991190910339
translation,299,98,results,outperforms,has,other qa baselines,outperforms has other qa baselines,0.5838125944137573
translation,299,99,results,memqa model,learns to condition,walk path,memqa model learns to condition walk path,0.790878415107727
translation,299,99,results,memqa model,attend,expand memory nodes,memqa model attend expand memory nodes,0.7292460799217224
translation,299,99,results,walk path,on,query contexts,walk path on query contexts,0.5567193627357483
translation,299,99,results,baseline models,rely on,initial memory slots,baseline models rely on initial memory slots,0.6867473125457764
translation,299,99,results,mgn walker model,has,memqa model,mgn walker model has memqa model,0.556259036064148
translation,299,99,results,outperforming,has,baseline models,outperforming has baseline models,0.5904433727264404
translation,299,99,results,results,with,mgn walker model,results with mgn walker model,0.6044459342956543
translation,299,102,results,answers,with,modules,answers with modules,0.6784452795982361
translation,299,102,results,modules,specifically designed for,various types of questions,modules specifically designed for various types of questions,0.7072529792785645
translation,299,102,results,neural module components ( g+ n and g + n+e ),has,greatly outperform,neural module components ( g+ n and g + n+e ) has greatly outperform,0.546916663646698
translation,299,102,results,greatly outperform,has,ablation model ( g ),greatly outperform has ablation model ( g ),0.576958954334259
translation,299,104,results,graph embeddings ( g + n +e ),improve,performance,graph embeddings ( g + n +e ) improve performance,0.6495950222015381
translation,299,104,results,performance,over,ablation model,performance over ablation model,0.665520966053009
translation,299,104,results,results,Note,graph embeddings ( g + n +e ),results Note graph embeddings ( g + n +e ),0.5845937728881836
translation,300,43,ablation-analysis,accuracy,of,biobert,accuracy of biobert,0.6666336059570312
translation,300,43,ablation-analysis,biobert,on,chq answering,biobert on chq answering,0.6608105897903442
translation,300,43,ablation-analysis,our method,superior to,mlm,our method superior to mlm,0.7146413326263428
translation,300,43,ablation-analysis,mlm,for,infusing,mlm for infusing,0.6740683913230896
translation,300,43,ablation-analysis,infusing,has,disease knowledge,infusing has disease knowledge,0.5632166266441345
translation,300,43,ablation-analysis,ablation analysis,has,accuracy,ablation analysis has accuracy,0.4860230088233948
translation,300,217,ablation-analysis,disease prediction,in,auxiliary sentence,disease prediction in auxiliary sentence,0.4868279695510864
translation,300,217,ablation-analysis,disease prediction,leads to,much lower performance,disease prediction leads to much lower performance,0.6189201474189758
translation,300,217,ablation-analysis,ablation analysis,remove,aspect prediction,ablation analysis remove aspect prediction,0.7005293965339661
translation,300,217,ablation-analysis,ablation analysis,removing,disease prediction,ablation analysis removing disease prediction,0.7337889671325684
translation,300,158,baselines,biomedical corpus,of,pubmed abstracts and clinical notes,biomedical corpus of pubmed abstracts and clinical notes,0.5030995011329651
translation,300,158,baselines,"scibert ( beltagy et al. , 2019 )",is,bert - base ( 108 m parameters ) model,"scibert ( beltagy et al. , 2019 ) is bert - base ( 108 m parameters ) model",0.5707085728645325
translation,300,158,baselines,bert - base ( 108 m parameters ) model,pre-trained on,random sample,bert - base ( 108 m parameters ) model pre-trained on random sample,0.7726759314537048
translation,300,158,baselines,random sample,of,full text,random sample of full text,0.5232340693473816
translation,300,158,baselines,full text,of,1.14 m papers,full text of 1.14 m papers,0.5198619961738586
translation,300,158,baselines,1.14 m papers,from,"semantic scholar ( ammar et al. , 2018 ) papers","1.14 m papers from semantic scholar ( ammar et al. , 2018 ) papers",0.4881511926651001
translation,300,158,baselines,"semantic scholar ( ammar et al. , 2018 ) papers",from,computer science domain,"semantic scholar ( ammar et al. , 2018 ) papers from computer science domain",0.4935109615325928
translation,300,158,baselines,82 %,from,biomedical domain,82 % from biomedical domain,0.5056877136230469
translation,300,136,experimental-setup,widely used pytorch implementation,of,bert,widely used pytorch implementation of bert,0.5725795030593872
translation,300,136,experimental-setup,adam,as,optimizer,adam as optimizer,0.5250696539878845
translation,300,136,experimental-setup,experimental setup,use,widely used pytorch implementation,experimental setup use widely used pytorch implementation,0.6156225800514221
translation,300,137,experimental-setup,batch size,as,16 and as 10,batch size as 16 and as 10,0.6558012962341309
translation,300,137,experimental-setup,experimental setup,empirically set,learning rate,experimental setup empirically set learning rate,0.7496674656867981
translation,300,137,experimental-setup,experimental setup,empirically set,batch size,experimental setup empirically set batch size,0.7574515342712402
translation,300,156,experimental-setup,best performing version,of,clinicalbert ( 108 m parameters ),best performing version of clinicalbert ( 108 m parameters ),0.5620286464691162
translation,300,156,experimental-setup,best performing version,based on,discharge summaries,best performing version based on discharge summaries,0.6251341700553894
translation,300,156,experimental-setup,discharge summaries,of,clinical notes,discharge summaries of clinical notes,0.49158504605293274
translation,300,156,experimental-setup,experimental setup,adopt,best performing version,experimental setup adopt best performing version,0.6340051293373108
translation,300,162,experimental-setup,fine- tuning,of,bert and its variants,fine- tuning of bert and its variants,0.551846981048584
translation,300,162,experimental-setup,batch size,selected from,"[ 16 , 32 ]","batch size selected from [ 16 , 32 ]",0.6082620024681091
translation,300,162,experimental-setup,batch size,selected from,learning rate,batch size selected from learning rate,0.5539740920066833
translation,300,162,experimental-setup,learning rate,selected from,"[ 1e - 5 , 2e -5 , 3e -5 , 4e- 5 , 5e - 5 ]","learning rate selected from [ 1e - 5 , 2e -5 , 3e -5 , 4e- 5 , 5e - 5 ]",0.5673366785049438
translation,300,162,experimental-setup,fine- tuning,has,batch size,fine- tuning has batch size,0.5576867461204529
translation,300,162,experimental-setup,bert and its variants,has,batch size,bert and its variants has batch size,0.6144022941589355
translation,300,162,experimental-setup,experimental setup,For,fine- tuning,experimental setup For fine- tuning,0.6026358008384705
translation,300,174,experimental-setup,mse,adopted as,loss,mse adopted as loss,0.5975359678268433
translation,300,174,experimental-setup,mse,use,adam,mse use adam,0.632480800151825
translation,300,174,experimental-setup,adam,as,optimizer,adam as optimizer,0.5250696539878845
translation,300,174,experimental-setup,experimental setup,use,adam,experimental setup use adam,0.5593148469924927
translation,300,174,experimental-setup,experimental setup,has,mse,experimental setup has mse,0.5144648551940918
translation,300,185,experimental-setup,cross-entropy,adopted as,loss function,cross-entropy adopted as loss function,0.601344883441925
translation,300,185,experimental-setup,cross-entropy,use,adam,cross-entropy use adam,0.6601848602294922
translation,300,185,experimental-setup,adam,as,optimizer,adam as optimizer,0.5250696539878845
translation,300,185,experimental-setup,experimental setup,use,adam,experimental setup use adam,0.5593148469924927
translation,300,185,experimental-setup,experimental setup,has,cross-entropy,experimental setup has cross-entropy,0.5549614429473877
translation,300,186,experimental-setup,hyper- parameters,tuned on,validation set,hyper- parameters tuned on validation set,0.7546796202659607
translation,300,186,experimental-setup,validation set,in terms of,f1,validation set in terms of f1,0.7391937971115112
translation,300,186,experimental-setup,validation set,set,batch size,validation set set batch size,0.740488588809967
translation,300,186,experimental-setup,validation set,set,learning rate,validation set set learning rate,0.695380449295044
translation,300,186,experimental-setup,batch size,as,32,batch size as 32,0.5678326487541199
translation,300,186,experimental-setup,learning rate,as,5e - 5,learning rate as 5e - 5,0.5931772589683533
translation,300,186,experimental-setup,experimental setup,has,hyper- parameters,experimental setup has hyper- parameters,0.5121466517448425
translation,300,41,experiments,bert models,including,bert,bert models including bert,0.7085515856742859
translation,300,41,experiments,bert models,including,bluebert,bert models including bluebert,0.6775892376899719
translation,300,41,experiments,bert models,including,clinical - bert,bert models including clinical - bert,0.7186415195465088
translation,300,41,experiments,bert models,including,scibert,bert models including scibert,0.653982400894165
translation,300,41,experiments,bert models,including,biobert,bert models including biobert,0.6997208595275879
translation,300,41,experiments,bert models,including,albert,bert models including albert,0.7090708613395691
translation,300,41,experiments,bert models,including,consumer health question ( chq ) answering,bert models including consumer health question ( chq ) answering,0.6931262016296387
translation,300,41,experiments,bert models,including,medical language inference,bert models including medical language inference,0.6293657422065735
translation,300,41,experiments,bert models,including,disease name recognition,bert models including disease name recognition,0.628954291343689
translation,300,160,experiments,disease knowledge infusion,over,three biomedical nlp tasks,disease knowledge infusion over three biomedical nlp tasks,0.6197173595428467
translation,300,260,hyperparameters,cross - entropy loss function,use,adam,cross - entropy loss function use adam,0.589186429977417
translation,300,260,hyperparameters,adam,as,optimizer,adam as optimizer,0.5250696539878845
translation,300,260,hyperparameters,hyperparameters,use,adam,hyperparameters use adam,0.6479569673538208
translation,300,260,hyperparameters,hyperparameters,has,cross - entropy loss function,hyperparameters has cross - entropy loss function,0.4946978986263275
translation,300,8,model,new disease knowledge infusion training procedure,evaluate it on,suite of bert models,new disease knowledge infusion training procedure evaluate it on suite of bert models,0.7152609825134277
translation,300,8,model,suite of bert models,including,bert,suite of bert models including bert,0.7108482718467712
translation,300,8,model,suite of bert models,including,biobert,suite of bert models including biobert,0.7541435360908508
translation,300,8,model,suite of bert models,including,scibert,suite of bert models including scibert,0.7318978309631348
translation,300,8,model,suite of bert models,including,clinical - bert,suite of bert models including clinical - bert,0.736598551273346
translation,300,8,model,suite of bert models,including,bluebert,suite of bert models including bluebert,0.7223753333091736
translation,300,8,model,suite of bert models,including,albert,suite of bert models including albert,0.7144892811775208
translation,300,8,model,model,propose,new disease knowledge infusion training procedure,model propose new disease knowledge infusion training procedure,0.6774076223373413
translation,300,8,model,model,evaluate it on,suite of bert models,model evaluate it on suite of bert models,0.6937554478645325
translation,300,34,model,new disease knowledge infusion training procedure,to explicitly augment,bert - like models,new disease knowledge infusion training procedure to explicitly augment bert - like models,0.7524763345718384
translation,300,34,model,bert - like models,with,disease knowledge,bert - like models with disease knowledge,0.6383057832717896
translation,300,34,model,model,propose,new disease knowledge infusion training procedure,model propose new disease knowledge infusion training procedure,0.6774076223373413
translation,300,35,model,bert,to infer,corresponding disease and aspect,bert to infer corresponding disease and aspect,0.8178678750991821
translation,300,35,model,corresponding disease and aspect,from,diseasedescriptive text,corresponding disease and aspect from diseasedescriptive text,0.6242247819900513
translation,300,35,model,corresponding disease and aspect,enabled by,weakly - supervised signals,corresponding disease and aspect enabled by weakly - supervised signals,0.7380226850509644
translation,300,35,model,weakly - supervised signals,from,wikipedia,weakly - supervised signals from wikipedia,0.4942172169685364
translation,300,35,model,model,train,bert,model train bert,0.7097170352935791
translation,300,229,model,cross-entropy loss,used by,disease knowledge infusion,cross-entropy loss used by disease knowledge infusion,0.7102622389793396
translation,300,229,model,cross-entropy loss,enhanced,adding the term,cross-entropy loss enhanced adding the term,0.6827758550643921
translation,300,229,model,maximizing,has,raw logits,maximizing has raw logits,0.5742449760437012
translation,300,229,model,model,has,cross-entropy loss,model has cross-entropy loss,0.5217790007591248
translation,300,231,model,new disease infusion training procedure,to augment,bert - like pre-trained language models,new disease infusion training procedure to augment bert - like pre-trained language models,0.6932322978973389
translation,300,231,model,bert - like pre-trained language models,with,disease knowledge,bert - like pre-trained language models with disease knowledge,0.6197054982185364
translation,300,231,model,model,propose,new disease infusion training procedure,model propose new disease infusion training procedure,0.6538140773773193
translation,300,44,results,new sota results,observed in,two datasets,new sota results observed in two datasets,0.6570537686347961
translation,300,44,results,results,has,new sota results,results has new sota results,0.576847493648529
translation,300,197,results,disease knowledge,via,our new training regimen,disease knowledge via our new training regimen,0.6894963979721069
translation,300,197,results,our new training regimen,see,significant improvement,our new training regimen see significant improvement,0.578379213809967
translation,300,197,results,significant improvement,in,nearly all cases,significant improvement in nearly all cases,0.5549877882003784
translation,300,197,results,results,by infusing,disease knowledge,results by infusing disease knowledge,0.6161906123161316
translation,300,210,results,high- capacity model,like,albert,high- capacity model like albert,0.5362627506256104
translation,300,210,results,high- capacity model,can achieve,similar performance,high- capacity model can achieve similar performance,0.7227028012275696
translation,300,210,results,similar performance,as,biomedical bert models,similar performance as biomedical bert models,0.5434489250183105
translation,300,210,results,biomedical bert models,on,"trecqa - 2017 , bc5cdr","biomedical bert models on trecqa - 2017 , bc5cdr",0.5247100591659546
translation,300,210,results,biomedical bert models,on,ncbi,biomedical bert models on ncbi,0.5596239566802979
translation,300,210,results,even better performance,on,mediqa - 2019 and mednli,even better performance on mediqa - 2019 and mednli,0.5489831566810608
translation,300,210,results,results,find that,high- capacity model,results find that high- capacity model,0.6775851249694824
translation,300,216,results,worse results,in terms of,accuracy and precision,worse results in terms of accuracy and precision,0.6996144652366638
translation,300,216,results,worse results,shows,auxiliary sentence,worse results shows auxiliary sentence,0.6677553057670593
translation,300,216,results,accuracy and precision,shows,auxiliary sentence,accuracy and precision shows auxiliary sentence,0.669576108455658
translation,300,216,results,auxiliary sentence,is,effective remedy,auxiliary sentence is effective remedy,0.5633323192596436
translation,300,216,results,effective remedy,for,problem,effective remedy for problem,0.6470613479614258
translation,300,216,results,problem,that,some passages do not mention their disease and aspects,problem that some passages do not mention their disease and aspects,0.6984449625015259
translation,300,216,results,results,observe,worse results,results observe worse results,0.5875658392906189
translation,300,221,results,default masked language model,in,bert,default masked language model in bert,0.5772085189819336
translation,300,221,results,our proposed disease infusion training task,has,outperforms,our proposed disease infusion training task has outperforms,0.6061629056930542
translation,300,221,results,outperforms,has,default masked language model,outperforms has default masked language model,0.6026526689529419
translation,300,222,results,disease knowledge infusion,adding,more data,disease knowledge infusion adding more data,0.7923055291175842
translation,300,222,results,works better,adding,more data,works better adding more data,0.7468761801719666
translation,300,222,results,more data,to,training process,more data to training process,0.5323852300643921
translation,300,222,results,disease knowledge infusion,has,works better,disease knowledge infusion has works better,0.6194114089012146
translation,300,222,results,results,shows that,our approach,results shows that our approach,0.7036905288696289
translation,300,228,results,accuracy,observe,disease knowledge infusion,accuracy observe disease knowledge infusion,0.6233900189399719
translation,300,228,results,disease knowledge infusion,takes,only three epochs,disease knowledge infusion takes only three epochs,0.6451807618141174
translation,300,228,results,only three epochs,to achieve,optimal performance,only three epochs to achieve optimal performance,0.6939182281494141
translation,300,228,results,optimal performance,on,biobert,optimal performance on biobert,0.6615296602249146
translation,300,228,results,biobert,over,chq answering task,biobert over chq answering task,0.6621943712234497
translation,300,228,results,results,in terms of,accuracy,results in terms of accuracy,0.7184101343154907
translation,300,228,results,results,observe,disease knowledge infusion,results observe disease knowledge infusion,0.5740803480148315
translation,301,270,ablation-analysis,refinements to lxmert,critical to produce,high quality images,refinements to lxmert critical to produce high quality images,0.7345691323280334
translation,301,270,ablation-analysis,ablation analysis,has,refinements to lxmert,ablation analysis has refinements to lxmert,0.6106256246566772
translation,301,271,ablation-analysis,updating pre-training data,for,ccc objective,updating pre-training data for ccc objective,0.5846951603889465
translation,301,271,ablation-analysis,updating pre-training data,is,less critical,updating pre-training data is less critical,0.5998439788818359
translation,301,271,ablation-analysis,third refinement,has,updating pre-training data,third refinement has updating pre-training data,0.5668832659721375
translation,301,271,ablation-analysis,ablation analysis,has,third refinement,ablation analysis has third refinement,0.574260413646698
translation,301,162,baselines,x-lxmert with lxmert,state- of- theart,text-to- image generation methods,x-lxmert with lxmert state- of- theart text-to- image generation methods,0.7005797028541565
translation,301,185,baselines,novel metric,to test,semantic consistency,novel metric to test semantic consistency,0.7062286734580994
translation,301,185,baselines,semantic consistency,between,caption and image,semantic consistency between caption and image,0.617797315120697
translation,301,185,baselines,semantic consistency,Measuring,semantics,semantic consistency Measuring semantics,0.6810206174850464
translation,301,185,baselines,caption and image,inspired by,masked token modeling,caption and image inspired by masked token modeling,0.6396291255950928
translation,301,185,baselines,semantics,Using,masking,semantics Using masking,0.760692834854126
translation,301,66,experiments,reconstructing randomly masked words and regions,given,remaining inputs,reconstructing randomly masked words and regions given remaining inputs,0.6816084384918213
translation,301,66,experiments,object classification,on,masked image regions,object classification on masked image regions,0.5394432544708252
translation,301,66,experiments,masked object classification ( moc ),has,object classification,masked object classification ( moc ) has object classification,0.5702382326126099
translation,301,143,hyperparameters,generator and discriminator,jointly trained with,4 losses,generator and discriminator jointly trained with 4 losses,0.6838554739952087
translation,301,143,hyperparameters,ac,-,gan loss,ac - gan loss,0.6781827211380005
translation,301,143,hyperparameters,ac,has,gan loss,ac has gan loss,0.6103782057762146
translation,301,144,hyperparameters,coefficients,for,different loss,coefficients for different loss,0.6402804851531982
translation,301,144,hyperparameters,different loss,are,"( 1 , 1 , 10 , 10 )","different loss are ( 1 , 1 , 10 , 10 )",0.5976409912109375
translation,301,144,hyperparameters,hyperparameters,has,coefficients,hyperparameters has coefficients,0.5175530314445496
translation,301,145,hyperparameters,hyperparameters,has,perceptual loss,hyperparameters has perceptual loss,0.5228661894798279
translation,301,146,hyperparameters,adam optimizer,with,"( ? 1 , ? 2 ) = ( 0 , 0.999 )","adam optimizer with ( ? 1 , ? 2 ) = ( 0 , 0.999 )",0.5973895192146301
translation,301,146,hyperparameters,adam optimizer,with,two -time update rule,adam optimizer with two -time update rule,0.6227478384971619
translation,301,146,hyperparameters,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,301,146,hyperparameters,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,301,146,hyperparameters,two -time update rule,with,learning rate,two -time update rule with learning rate,0.5773537158966064
translation,301,146,hyperparameters,learning rate,of,0.0004 and 0.0001,learning rate of 0.0004 and 0.0001,0.6246747374534607
translation,301,146,hyperparameters,0.0004 and 0.0001,for,generator and discriminator,0.0004 and 0.0001 for generator and discriminator,0.657599687576294
translation,301,146,hyperparameters,hyperparameters,use,adam optimizer,hyperparameters use adam optimizer,0.6090166568756104
translation,301,147,hyperparameters,generator,with,batch size,generator with batch size,0.6292926669120789
translation,301,147,hyperparameters,batch size,for,60 epochs,batch size for 60 epochs,0.5428292155265808
translation,301,147,hyperparameters,96,for,60 epochs,96 for 60 epochs,0.6330499053001404
translation,301,147,hyperparameters,batch size,has,96,batch size has 96,0.6077668070793152
translation,301,147,hyperparameters,hyperparameters,train,generator,hyperparameters train generator,0.6964189410209656
translation,301,150,hyperparameters,pre-training,Following,"lxmert ( tan and bansal , 2019 )","pre-training Following lxmert ( tan and bansal , 2019 )",0.6537256836891174
translation,301,150,hyperparameters,pre-training,use,"adamw optimizer ( loshchilov and hutter , 2019 )","pre-training use adamw optimizer ( loshchilov and hutter , 2019 )",0.5953086018562317
translation,301,150,hyperparameters,"lxmert ( tan and bansal , 2019 )",use,"adamw optimizer ( loshchilov and hutter , 2019 )","lxmert ( tan and bansal , 2019 ) use adamw optimizer ( loshchilov and hutter , 2019 )",0.6233823895454407
translation,301,150,hyperparameters,"adamw optimizer ( loshchilov and hutter , 2019 )",with,"( ? 1 , ? 2 ) = ( 0.9 , 0.999 )","adamw optimizer ( loshchilov and hutter , 2019 ) with ( ? 1 , ? 2 ) = ( 0.9 , 0.999 )",0.5718382000923157
translation,301,150,hyperparameters,learning rate,with,5 % linear warmup schedule,learning rate with 5 % linear warmup schedule,0.6570395827293396
translation,301,150,hyperparameters,1e - 5,with,5 % linear warmup schedule,1e - 5 with 5 % linear warmup schedule,0.6675621271133423
translation,301,150,hyperparameters,learning rate,has,1e - 5,learning rate has 1e - 5,0.5607917308807373
translation,301,150,hyperparameters,hyperparameters,use,"adamw optimizer ( loshchilov and hutter , 2019 )","hyperparameters use adamw optimizer ( loshchilov and hutter , 2019 )",0.5921003222465515
translation,301,150,hyperparameters,hyperparameters,has,pre-training,hyperparameters has pre-training,0.5166658163070679
translation,301,151,hyperparameters,batch size 920,for,20 epochs,batch size 920 for 20 epochs,0.5415749549865723
translation,301,151,hyperparameters,hyperparameters,train,x-lxmert,hyperparameters train x-lxmert,0.6860362887382507
translation,301,7,model,uniform masking,with,large range of masking ratios,uniform masking with large range of masking ratios,0.6553910374641418
translation,301,7,model,right pre-training datasets,to,right objectives,right pre-training datasets to right objectives,0.5699757933616638
translation,301,7,model,x - lxmert,has,extension,x - lxmert has extension,0.623974621295929
translation,301,7,model,model,introduce,x - lxmert,model introduce x - lxmert,0.6611611843109131
translation,301,27,model,x-lxmert,builds upon,lxmert,x-lxmert builds upon lxmert,0.5801027417182922
translation,301,27,model,x-lxmert,effectively perform,discriminative as well as generative tasks,x-lxmert effectively perform discriminative as well as generative tasks,0.746394693851471
translation,301,27,model,model,introduce,x-lxmert,model introduce x-lxmert,0.6611611843109131
translation,301,34,model,x - lxmert,generate,captions and images,x - lxmert generate captions and images,0.6118066310882568
translation,301,34,model,unified multimodal transformer model,can,answer questions,unified multimodal transformer model can answer questions,0.6269607543945312
translation,301,34,model,x - lxmert,has,unified multimodal transformer model,x - lxmert has unified multimodal transformer model,0.5397951602935791
translation,301,34,model,model,present,x - lxmert,model present x - lxmert,0.6974828243255615
translation,301,104,model,new cluster-centroid classification objective ( ccc ),to replace,previous regression objective,new cluster-centroid classification objective ( ccc ) to replace previous regression objective,0.704940676689148
translation,301,104,model,previous regression objective,with,high cardinality classification objective,previous regression objective with high cardinality classification objective,0.5509530901908875
translation,301,104,model,model,has,new cluster-centroid classification objective ( ccc ),model has new cluster-centroid classification objective ( ccc ),0.5610029101371765
translation,301,29,results,x - lxmert,able to generate,rich imagery,x - lxmert able to generate rich imagery,0.7543935775756836
translation,301,29,results,rich imagery,that is,semantically consistent,rich imagery that is semantically consistent,0.6111837029457092
translation,301,29,results,semantically consistent,with,input captions,semantically consistent with input captions,0.6138959527015686
translation,301,29,results,proposed image generator,has,x - lxmert,proposed image generator has x - lxmert,0.6031099557876587
translation,301,29,results,results,coupled with,proposed image generator,results coupled with proposed image generator,0.6737843751907349
translation,301,142,results,uniform masking,aligns well with,linear decay,uniform masking aligns well with linear decay,0.6561930775642395
translation,301,142,results,uniform masking,makes,model,uniform masking makes model,0.6194526553153992
translation,301,142,results,linear decay,of,mask - predict,linear decay of mask - predict,0.6300820708274841
translation,301,142,results,robust,to,varied number of masked locations,robust to varied number of masked locations,0.5616422295570374
translation,301,142,results,model,has,robust,model has robust,0.5682940483093262
translation,301,142,results,results,has,uniform masking,results has uniform masking,0.532414972782135
translation,301,207,results,lxmert,across,all generation metrics,lxmert across all generation metrics,0.7222391366958618
translation,301,207,results,x - lxmert,has,significantly outperforms,x - lxmert has significantly outperforms,0.6182868480682373
translation,301,207,results,significantly outperforms,has,lxmert,significantly outperforms has lxmert,0.6072580218315125
translation,301,207,results,results,has,x - lxmert,results has x - lxmert,0.568388819694519
translation,301,208,results,two specialized generation models,comparable to,attngan and controlgan,two specialized generation models comparable to attngan and controlgan,0.6590393781661987
translation,301,208,results,x-lxmert,has,outperforms,x-lxmert has outperforms,0.6436635851860046
translation,301,208,results,outperforms,has,two specialized generation models,outperforms has two specialized generation models,0.595380961894989
translation,301,208,results,results,has,x-lxmert,results has x-lxmert,0.568388819694519
translation,301,222,results,semantics,generated by,x-lxmert,semantics generated by x-lxmert,0.6819463968276978
translation,301,222,results,x-lxmert,on par with,dm - gan,x-lxmert on par with dm - gan,0.7146221399307251
translation,301,222,results,x-lxmert,significantly better than,lxmert,x-lxmert significantly better than lxmert,0.6825891137123108
translation,301,222,results,results,see that,semantics,results see that semantics,0.6335304379463196
translation,301,229,results,x - lxmert 's generation capabilities,rival,state of the art specialized generation models,x - lxmert 's generation capabilities rival state of the art specialized generation models,0.7423505187034607
translation,301,229,results,results,has,x - lxmert 's generation capabilities,results has x - lxmert 's generation capabilities,0.5157673358917236
translation,301,237,results,x-lxmert,shows,roughly 2 % drop,x-lxmert shows roughly 2 % drop,0.7377920746803284
translation,301,237,results,x-lxmert,retains,massive jumps,x-lxmert retains massive jumps,0.7902721762657166
translation,301,237,results,massive jumps,obtained by,lxmert,massive jumps obtained by lxmert,0.6961738467216492
translation,301,237,results,lxmert,on,nlvr,lxmert on nlvr,0.6498640179634094
translation,301,237,results,nlvr,compared to,previous generation of models,nlvr compared to previous generation of models,0.6720520853996277
translation,301,237,results,results,has,x-lxmert,results has x-lxmert,0.568388819694519
translation,301,272,results,x-lxmert,is,fairly robust,x-lxmert is fairly robust,0.5986914038658142
translation,301,272,results,fairly robust,to,sampling strategy,fairly robust to sampling strategy,0.5841020941734314
translation,301,272,results,sampling strategy,particularly for,image semantics,sampling strategy particularly for image semantics,0.6243598461151123
translation,301,272,results,results,shows,x-lxmert,results shows x-lxmert,0.692406415939331
translation,302,184,ablation-analysis,idf weighting,seems to,degrade,idf weighting seems to degrade,0.7170659303665161
translation,302,184,ablation-analysis,question demoting,causes,correlation,question demoting causes correlation,0.6958123445510864
translation,302,184,ablation-analysis,correlation,with,grader,correlation with grader,0.6491529941558838
translation,302,184,ablation-analysis,grader,to,increase,grader to increase,0.5899214148521423
translation,302,184,ablation-analysis,degrade,has,performance,degrade has performance,0.5986349582672119
translation,302,184,ablation-analysis,increasing,has,rmse,increasing has rmse,0.6303237676620483
translation,302,184,ablation-analysis,ablation analysis,introduction of,idf weighting,ablation analysis introduction of idf weighting,0.7098554968833923
translation,302,184,ablation-analysis,ablation analysis,introducing,question demoting,ablation analysis introducing question demoting,0.7390360832214355
translation,302,214,ablation-analysis,bow - only svm model,for,svr,bow - only svm model for svr,0.5861502289772034
translation,302,214,ablation-analysis,bow - only svm model,reduces,rmse,bow - only svm model reduces rmse,0.6403385996818542
translation,302,214,ablation-analysis,rmse,by,.022,rmse by .022,0.5776600241661072
translation,302,214,ablation-analysis,.022,compared to,best bow feature,.022 compared to best bow feature,0.6438559293746948
translation,302,214,ablation-analysis,ablation analysis,using,bow - only svm model,ablation analysis using bow - only svm model,0.6403220295906067
translation,302,16,model,improving,to,short answer grading,improving to short answer grading,0.5312458872795105
translation,302,16,model,existing bag-of-words ( bow ) approaches,to,short answer grading,existing bag-of-words ( bow ) approaches to short answer grading,0.4867614209651947
translation,302,16,model,short answer grading,utilizing,machine learning techniques,short answer grading utilizing machine learning techniques,0.597989022731781
translation,302,5,results,several graph alignment features,with,lexical semantic similarity measures,several graph alignment features with lexical semantic similarity measures,0.5742810368537903
translation,302,5,results,several graph alignment features,show,student answers,several graph alignment features show student answers,0.5706510543823242
translation,302,5,results,lexical semantic similarity measures,using,machine learning techniques,lexical semantic similarity measures using machine learning techniques,0.588483452796936
translation,302,5,results,can be more accurately graded,than,semantic measures,can be more accurately graded than semantic measures,0.5988323092460632
translation,302,5,results,student answers,has,can be more accurately graded,student answers has can be more accurately graded,0.5620635747909546
translation,302,5,results,results,combine,several graph alignment features,results combine several graph alignment features,0.6277428865432739
translation,302,175,results,average correlation,between,bow methods ' sim-ilarity scores,average correlation between bow methods ' sim-ilarity scores,0.6405267119407654
translation,302,175,results,average correlation,between,student grades,average correlation between student grades,0.6590768694877625
translation,302,175,results,up to 0.046,with,average improvement,up to 0.046 with average improvement,0.6503106355667114
translation,302,175,results,average improvement,of,0.019,average improvement of 0.019,0.5841534733772278
translation,302,175,results,0.019,across,all eleven semantic features,0.019 across all eleven semantic features,0.6453098058700562
translation,302,175,results,relatively minor change,has,average correlation,relatively minor change has average correlation,0.55536288022995
translation,302,175,results,results,With,relatively minor change,results With relatively minor change,0.6487832069396973
translation,302,176,results,results,applying,question demoting,results applying question demoting,0.711806058883667
translation,302,176,results,question demoting,to,our semantic features,question demoting to our semantic features,0.6055253744125366
translation,302,176,results,results,of,question demoting,results of question demoting,0.5472737550735474
translation,302,176,results,results,applying,question demoting,results applying question demoting,0.711806058883667
translation,302,177,results,scores,using,rmse,scores using rmse,0.7006524801254272
translation,302,177,results,difference,is,less consistent,difference is less consistent,0.6089447140693665
translation,302,177,results,less consistent,yielding,average improvement,less consistent yielding average improvement,0.6422708034515381
translation,302,177,results,average improvement,of,0.002,average improvement of 0.002,0.5594584941864014
translation,302,177,results,scores,has,difference,scores has difference,0.5714523196220398
translation,302,177,results,rmse,has,difference,rmse has difference,0.5523698329925537
translation,302,177,results,results,comparing,scores,results comparing scores,0.6109444499015808
translation,302,203,results,several systems,appear,better,several systems appear better,0.6676558256149292
translation,302,203,results,better,when evaluating on,correlation measure,better when evaluating on correlation measure,0.6853945851325989
translation,302,203,results,correlation measure,like,pearson 's ?,correlation measure like pearson 's ?,0.5847108960151672
translation,302,203,results,others,appear,better,others appear better,0.6737967729568481
translation,302,203,results,better,when analyzing,error rate,better when analyzing error rate,0.6396796107292175
translation,302,203,results,results,results that,several systems,results results that several systems,0.7852734327316284
translation,302,204,results,seemed to outperform,when measuring,correlation,seemed to outperform when measuring correlation,0.6822053790092468
translation,302,204,results,svr system,when measuring,correlation,svr system when measuring correlation,0.6907016038894653
translation,302,204,results,svm - rank system,has,seemed to outperform,svm - rank system has seemed to outperform,0.6322411894798279
translation,302,204,results,seemed to outperform,has,svr system,seemed to outperform has svr system,0.6045263409614563
translation,302,204,results,results,has,svm - rank system,results has svm - rank system,0.5898480415344238
translation,302,208,results,correlative measure,yields,predictably poor results,correlative measure yields predictably poor results,0.7163090705871582
translation,302,208,results,error rate,comparable to ( or better than,more intelligent bow metrics,error rate comparable to ( or better than more intelligent bow metrics,0.5438507199287415
translation,302,208,results,results,Evaluating with,correlative measure,results Evaluating with correlative measure,0.7317506074905396
translation,302,213,results,correlation,for,bow - only svm model,correlation for bow - only svm model,0.58663010597229
translation,302,213,results,bow - only svm model,for,svmrank,bow - only svm model for svmrank,0.520057201385498
translation,302,213,results,svmrank,improved upon,best bow feature,svmrank improved upon best bow feature,0.5963470339775085
translation,302,213,results,results,has,correlation,results has correlation,0.5354241132736206
translation,304,12,results,best systems,achieved,official score ( map ),best systems achieved official score ( map ),0.7078936100006104
translation,304,12,results,official score ( map ),of,"88.43 , 47.22 , 15.46 , and 61.16","official score ( map ) of 88.43 , 47.22 , 15.46 , and 61.16",0.528319239616394
translation,304,12,results,"88.43 , 47.22 , 15.46 , and 61.16",in,"subtasks a , b , c , and d","88.43 , 47.22 , 15.46 , and 61.16 in subtasks a , b , c , and d",0.5035066604614258
translation,304,12,results,results,has,best systems,results has best systems,0.5547273755073547
translation,305,5,experimental-setup,data set,generated by,matching,data set generated by matching,0.6534936428070068
translation,305,5,experimental-setup,trivia-type question - answer pairs,with,subject - predicateobject triples,trivia-type question - answer pairs with subject - predicateobject triples,0.5951895713806152
translation,305,5,experimental-setup,subject - predicateobject triples,in,freebase,subject - predicateobject triples in freebase,0.4570329189300537
translation,305,5,experimental-setup,matching,has,trivia-type question - answer pairs,matching has trivia-type question - answer pairs,0.5609948635101318
translation,305,5,experimental-setup,experimental setup,has,data set,experimental setup has data set,0.5154800415039062
translation,305,29,results,freebaseqa,contains,over 54 k matches,freebaseqa contains over 54 k matches,0.6489498019218445
translation,305,29,results,over 54 k matches,from,28 k unique questions,over 54 k matches from 28 k unique questions,0.5619941353797913
translation,305,29,results,over 54 k matches,about,28 k unique questions,over 54 k matches about 28 k unique questions,0.621131420135498
translation,305,29,results,results,has,freebaseqa,results has freebaseqa,0.5580099821090698
translation,306,129,ablation-analysis,concatenation,of,nodes and queries,concatenation of nodes and queries,0.5862852931022644
translation,306,129,ablation-analysis,nodes and queries,directly into,output layer,nodes and queries directly into output layer,0.6531239151954651
translation,306,129,ablation-analysis,significant performance drop,with,more than 3 %,significant performance drop with more than 3 %,0.6650535464286804
translation,306,129,ablation-analysis,ablation analysis,remove,bi-directional attention,ablation analysis remove bi-directional attention,0.6262635588645935
translation,306,134,ablation-analysis,removal,of,semantic-level features,removal of semantic-level features,0.5972930192947388
translation,306,134,ablation-analysis,removal,causes,slight decline,removal causes slight decline,0.7180812358856201
translation,306,134,ablation-analysis,semantic-level features,causes,slight decline,semantic-level features causes slight decline,0.6946353912353516
translation,306,134,ablation-analysis,slight decline,on,performance,slight decline on performance,0.5678781867027283
translation,306,134,ablation-analysis,performance,including,ner and pos features,performance including ner and pos features,0.6400043368339539
translation,306,134,ablation-analysis,ablation analysis,has,removal,ablation analysis has removal,0.5308048129081726
translation,306,135,ablation-analysis,elmo feature,causes,dramatical drop,elmo feature causes dramatical drop,0.7058807015419006
translation,306,135,ablation-analysis,dramatical drop,reflects,insufficiency,dramatical drop reflects insufficiency,0.7670125365257263
translation,306,135,ablation-analysis,insufficiency,of only using,word embeddings,insufficiency of only using word embeddings,0.6874508261680603
translation,306,135,ablation-analysis,ablation analysis,Further removal of,elmo feature,ablation analysis Further removal of elmo feature,0.6775045990943909
translation,306,113,baselines,entity-gcn,has,"de cao et al. , 2018 )","entity-gcn has de cao et al. , 2018 )",0.5945395827293396
translation,306,112,experiments,50 - epoch training,on,two gtx1080 ti gpus,50 - epoch training on two gtx1080 ti gpus,0.5030831098556519
translation,306,112,experiments,two gtx1080 ti gpus,using,pre-built and pre-processed graph data,two gtx1080 ti gpus using pre-built and pre-processed graph data,0.6336352229118347
translation,306,112,experiments,pre-built and pre-processed graph data,generated from,original corpus,pre-built and pre-processed graph data generated from original corpus,0.667023241519928
translation,306,102,hyperparameters,300 - dimension glove pre-trained embeddings,from,840b web crawl data,300 - dimension glove pre-trained embeddings from 840b web crawl data,0.5457565188407898
translation,306,102,hyperparameters,300 - dimension glove pre-trained embeddings,used as,token - level features,300 - dimension glove pre-trained embeddings used as token - level features,0.5553908348083496
translation,306,102,hyperparameters,hyperparameters,has,300 - dimension glove pre-trained embeddings,hyperparameters has 300 - dimension glove pre-trained embeddings,0.49939271807670593
translation,306,103,hyperparameters,spacy,to provide,additional 8 - dimension ner and pos features,spacy to provide additional 8 - dimension ner and pos features,0.6446548104286194
translation,306,103,hyperparameters,hyperparameters,used,spacy,hyperparameters used spacy,0.6120096445083618
translation,306,104,hyperparameters,dimension,of,1 - layer linear network,dimension of 1 - layer linear network,0.5825074315071106
translation,306,104,hyperparameters,1 - layer linear network,for,nodes,1 - layer linear network for nodes,0.6234526038169861
translation,306,104,hyperparameters,nodes,in,multi-level feature module,nodes in multi-level feature module,0.5380349159240723
translation,306,104,hyperparameters,multi-level feature module,was,512,multi-level feature module was 512,0.5674879550933838
translation,306,104,hyperparameters,512,with,tanh as activation function,512 with tanh as activation function,0.6660793423652649
translation,306,104,hyperparameters,hyperparameters,has,dimension,hyperparameters has dimension,0.514893651008606
translation,306,105,hyperparameters,2 - layer bi-lstm,employed for,queries,2 - layer bi-lstm employed for queries,0.6659834384918213
translation,306,105,hyperparameters,queries,whose,hidden state size,queries whose hidden state size,0.635482668876648
translation,306,105,hyperparameters,hidden state size,is,256,hidden state size is 256,0.5838548541069031
translation,306,105,hyperparameters,hyperparameters,has,2 - layer bi-lstm,hyperparameters has 2 - layer bi-lstm,0.5277990102767944
translation,306,107,hyperparameters,gcn layer number l,set as,5,gcn layer number l set as 5,0.7070386409759521
translation,306,107,hyperparameters,hyperparameters,has,gcn layer number l,hyperparameters has gcn layer number l,0.4829254150390625
translation,306,108,hyperparameters,unit number,of,intermediate layers,unit number of intermediate layers,0.6324767470359802
translation,306,108,hyperparameters,intermediate layers,in,output layer,intermediate layers in output layer,0.49623480439186096
translation,306,108,hyperparameters,output layer,was,256,output layer was 256,0.5654640197753906
translation,306,108,hyperparameters,hyperparameters,has,unit number,hyperparameters has unit number,0.5212448835372925
translation,306,109,hyperparameters,number of nodes and the query length,truncated as,500 and 25,number of nodes and the query length truncated as 500 and 25,0.5685989856719971
translation,306,109,hyperparameters,500 and 25,for,normalized computation,500 and 25 for normalized computation,0.6765499711036682
translation,306,109,hyperparameters,hyperparameters,has,number of nodes and the query length,hyperparameters has number of nodes and the query length,0.532767653465271
translation,306,110,hyperparameters,dropout,with,rate,dropout with rate,0.5557408928871155
translation,306,110,hyperparameters,rate,applied before,gcn layer,rate applied before gcn layer,0.6323267817497253
translation,306,110,hyperparameters,0.2,applied before,gcn layer,0.2 applied before gcn layer,0.6717610955238342
translation,306,110,hyperparameters,rate,has,0.2,rate has 0.2,0.5200834274291992
translation,306,110,hyperparameters,hyperparameters,has,dropout,hyperparameters has dropout,0.5324090719223022
translation,306,111,hyperparameters,adam optimizer,employed with,initial learning rate 2 ? 10 ?4,adam optimizer employed with initial learning rate 2 ? 10 ?4,0.7033290863037109
translation,306,111,hyperparameters,batch size,has,32,batch size has 32,0.630264163017273
translation,306,111,hyperparameters,hyperparameters,has,adam optimizer,hyperparameters has adam optimizer,0.5012347102165222
translation,306,5,model,bi-directional attention entity graph convolutional network ( bag ),leveraging,relationships,bi-directional attention entity graph convolutional network ( bag ) leveraging relationships,0.6625683307647705
translation,306,5,model,bi-directional attention entity graph convolutional network ( bag ),leveraging,attention information,bi-directional attention entity graph convolutional network ( bag ) leveraging attention information,0.657461941242218
translation,306,5,model,relationships,between,nodes,relationships between nodes,0.6330342292785645
translation,306,5,model,nodes,in,entity graph,nodes in entity graph,0.5266223549842834
translation,306,5,model,model,propose,bi-directional attention entity graph convolutional network ( bag ),model propose bi-directional attention entity graph convolutional network ( bag ),0.6386594772338867
translation,306,6,model,graph convolutional networks,to obtain,relation - aware representation,graph convolutional networks to obtain relation - aware representation,0.5367520451545715
translation,306,6,model,relation - aware representation,of,nodes,relation - aware representation of nodes,0.5879244804382324
translation,306,6,model,nodes,for,entity graphs,nodes for entity graphs,0.569808304309845
translation,306,6,model,entity graphs,built from,documents,entity graphs built from documents,0.651787519454956
translation,306,6,model,documents,with,multi-level features,documents with multi-level features,0.6249523162841797
translation,306,6,model,model,has,graph convolutional networks,model has graph convolutional networks,0.5049984455108643
translation,306,7,model,bidirectional attention,applied on,graphs and queries,bidirectional attention applied on graphs and queries,0.6463687419891357
translation,306,7,model,graphs and queries,to generate,query - aware nodes representation,graphs and queries to generate query - aware nodes representation,0.6391997933387756
translation,306,7,model,query - aware nodes representation,used for,final prediction,query - aware nodes representation used for final prediction,0.6479983329772949
translation,306,7,model,model,has,bidirectional attention,model has bidirectional attention,0.5291703343391418
translation,306,22,model,new graph - based qa model,named,bi-directional attention entity graph convolutional network ( bag ),new graph - based qa model named bi-directional attention entity graph convolutional network ( bag ),0.7088678479194641
translation,306,22,model,model,propose,new graph - based qa model,model propose new graph - based qa model,0.6549562215805054
translation,306,23,model,documents,transformed into,graph,documents transformed into graph,0.6677688956260681
translation,306,23,model,graph,in which,nodes,graph in which nodes,0.6216940879821777
translation,306,23,model,graph,in which,edges,graph in which edges,0.6248834729194641
translation,306,23,model,nodes,are,entities,nodes are entities,0.5929450988769531
translation,306,23,model,model,has,documents,model has documents,0.6377290487289429
translation,306,24,model,graph convolutional networks ( gcns ),to learn,relation - aware representation,graph convolutional networks ( gcns ) to learn relation - aware representation,0.6242998242378235
translation,306,24,model,relation - aware representation,of,nodes,relation - aware representation of nodes,0.5879244804382324
translation,306,25,model,new bi-directional attention,between,graph and a query,new bi-directional attention between graph and a query,0.5488431453704834
translation,306,25,model,new bi-directional attention,to derive,mutual information,new bi-directional attention to derive mutual information,0.6295171976089478
translation,306,25,model,graph and a query,with,multi-level features,graph and a query with multi-level features,0.6427280306816101
translation,306,25,model,mutual information,for,final prediction,mutual information for final prediction,0.6200076937675476
translation,306,25,model,model,introduce,new bi-directional attention,model introduce new bi-directional attention,0.6289054155349731
translation,306,46,model,entity graph,based on,entity - gcn,entity graph based on entity - gcn,0.6454191207885742
translation,306,46,model,model,construct,entity graph,model construct entity graph,0.705711305141449
translation,306,120,results,our bag model,achieves,state - of - the art per-formance,our bag model achieves state - of - the art per-formance,0.6582465767860413
translation,306,120,results,state - of - the art per-formance,on,unmasked and masked data,state - of - the art per-formance on unmasked and masked data,0.5566575527191162
translation,306,120,results,state - of - the art per-formance,with,accuracy,state - of - the art per-formance with accuracy,0.6265297532081604
translation,306,120,results,69.0 %,on,test set,69.0 % on test set,0.5325026512145996
translation,306,120,results,accuracy,has,69.0 %,accuracy has 69.0 %,0.5668678283691406
translation,306,120,results,results,has,our bag model,results has our bag model,0.5634267926216125
translation,306,121,results,significant superior,than,fastqa and bidaf,significant superior than fastqa and bidaf,0.599061906337738
translation,306,121,results,significant superior,due to,leveraging,significant superior due to leveraging,0.7094763517379761
translation,306,121,results,leveraging,of,relationship information,leveraging of relationship information,0.5141616463661194
translation,306,121,results,relationship information,given by,graph,relationship information given by graph,0.6736592054367065
translation,306,125,results,bag model,achieves,best performance,bag model achieves best performance,0.6762738823890686
translation,306,125,results,best performance,under,all data configurations,best performance under all data configurations,0.6558107137680054
translation,306,125,results,results,has,bag model,results has bag model,0.5954388976097107
translation,306,126,results,bag,gets,small promotion,bag gets small promotion,0.6555540561676025
translation,306,126,results,small promotion,on,masked data,small promotion on masked data,0.5870025157928467
translation,306,126,results,results,noticed that,bag,results noticed that bag,0.616984486579895
translation,307,80,baselines,vcg,", 2018 )",current state - of - the - art entity linking system,"vcg , 2018 ) current state - of - the - art entity linking system",0.625599205493927
translation,307,80,baselines,current state - of - the - art entity linking system,on,webqsp,current state - of - the - art entity linking system on webqsp,0.5223322510719299
translation,307,25,experiments,entity disambiguation annotations,from,sorokin and gurevych ( 2018 ),entity disambiguation annotations from sorokin and gurevych ( 2018 ),0.5486862659454346
translation,307,25,experiments,entity disambiguation annotations,to create,endto-end question entity linking benchmark,entity disambiguation annotations to create endto-end question entity linking benchmark,0.5839962959289551
translation,307,4,model,fast end-to - end entity linking,uses,biencoder,fast end-to - end entity linking uses biencoder,0.5412926077842712
translation,307,4,model,biencoder,to jointly perform,mention detection and linking,biencoder to jointly perform mention detection and linking,0.65884929895401
translation,307,4,model,mention detection and linking,in,one pass,mention detection and linking in one pass,0.520232081413269
translation,307,4,model,elq,has,fast end-to - end entity linking,elq has fast end-to - end entity linking,0.526759147644043
translation,307,4,model,model,present,elq,model present elq,0.7363800406455994
translation,307,14,model,question representations,to jointly detect,mentions,question representations to jointly detect mentions,0.7090544700622559
translation,307,14,model,question representations,score,candidate entities,question representations score candidate entities,0.722832202911377
translation,307,14,model,candidate entities,through,innerproduct,candidate entities through innerproduct,0.6718907356262207
translation,307,14,model,innerproduct,with,entity vector,innerproduct with entity vector,0.6192001700401306
translation,307,14,model,model,use,question representations,model use question representations,0.6917343735694885
translation,307,17,model,fast and accurate entity linking system,specifically targets,questions,fast and accurate entity linking system specifically targets questions,0.6266118884086609
translation,307,17,model,elq,has,fast and accurate entity linking system,elq has fast and accurate entity linking system,0.5339900851249695
translation,307,17,model,model,propose,elq,model propose elq,0.6981790065765381
translation,307,21,model,question encoder,derives,token - level embeddings,question encoder derives token - level embeddings,0.5618820190429688
translation,307,21,model,token - level embeddings,for,input question,token - level embeddings for input question,0.5612955689430237
translation,307,21,model,model,has,question encoder,model has question encoder,0.5963594913482666
translation,307,81,model,blink,requires,pre-specified mention boundaries,blink requires pre-specified mention boundaries,0.6698578596115112
translation,307,81,model,pre-specified mention boundaries,as,input,pre-specified mention boundaries as input,0.5054055452346802
translation,307,81,model,input,train,"separate , bert - based span extraction model","input train separate , bert - based span extraction model",0.6703321933746338
translation,307,81,model,"separate , bert - based span extraction model",on,webqsp,"separate , bert - based span extraction model on webqsp",0.5292150974273682
translation,307,81,model,"separate , bert - based span extraction model",to predict,mention boundaries,"separate , bert - based span extraction model to predict mention boundaries",0.6913403868675232
translation,307,81,model,model,has,blink,model has blink,0.5481157898902893
translation,307,26,results,previous methods,in both,accuracy and run-time,previous methods in both accuracy and run-time,0.637618899345398
translation,307,27,results,much faster end-to - end inference time,than,any other neural baseline,much faster end-to - end inference time than any other neural baseline,0.5203772187232971
translation,307,27,results,elq,has,much faster end-to - end inference time,elq has much faster end-to - end inference time,0.5460492372512817
translation,307,27,results,results,has,elq,results has elq,0.5820263028144836
translation,307,84,results,stateof - the- art ( vcg ),on,both datasets,stateof - the- art ( vcg ) on both datasets,0.4982846975326538
translation,307,84,results,bertbased biencoder models,has,far outperform,bertbased biencoder models has far outperform,0.6113478541374207
translation,307,84,results,far outperform,has,stateof - the- art ( vcg ),far outperform has stateof - the- art ( vcg ),0.5976336598396301
translation,307,84,results,results,find that,bertbased biencoder models,results find that bertbased biencoder models,0.5353975296020508
translation,307,85,results,elq,is,much more efficient,elq is much more efficient,0.5790420770645142
translation,307,85,results,all other models,trained in,comparable setting,all other models trained in comparable setting,0.7410005927085876
translation,307,85,results,much more efficient,than,every other neural baseline ( vcg and blink ),much more efficient than every other neural baseline ( vcg and blink ),0.5369378924369812
translation,307,85,results,elq,has,outperforms,elq has outperforms,0.6402539610862732
translation,307,85,results,outperforms,has,all other models,outperforms has all other models,0.5782700181007385
translation,307,85,results,results,has,elq,results has elq,0.5820263028144836
translation,307,86,results,elq,up to,2.3 ? better,elq up to 2.3 ? better,0.6498182415962219
translation,307,86,results,2.3 ? better,than,tagme,2.3 ? better than tagme,0.5646796226501465
translation,307,86,results,results,has,elq,results has elq,0.5820263028144836
translation,307,102,results,tagme,with,elq,tagme with elq,0.7351061105728149
translation,307,102,results,performance,including,5.9 % and 3.9 % absolute improvements,performance including 5.9 % and 3.9 % absolute improvements,0.6898443698883057
translation,307,102,results,5.9 % and 3.9 % absolute improvements,on,wq and nq,5.9 % and 3.9 % absolute improvements on wq and nq,0.5512666702270508
translation,307,102,results,elq,has,significantly improves,elq has significantly improves,0.6303016543388367
translation,307,102,results,significantly improves,has,performance,significantly improves has performance,0.5962982177734375
translation,307,102,results,results,replacing,tagme,results replacing tagme,0.6832590103149414
translation,307,103,results,elq,trained on,wikipedia,elq trained on wikipedia,0.8048608303070068
translation,307,103,results,elq,achieves,good results,elq achieves good results,0.670051097869873
translation,307,103,results,further fine-tuning,on,webqsp,further fine-tuning on webqsp,0.5912522673606873
translation,307,103,results,webqsp,gives,extra gains,webqsp gives extra gains,0.6716805100440979
translation,307,103,results,extra gains,on,wq,extra gains on wq,0.5593627095222473
translation,308,7,model,uima framework,to distribute,computation of cqa tasks,uima framework to distribute computation of cqa tasks,0.46629437804222107
translation,308,7,model,computation of cqa tasks,over,computer clusters,computation of cqa tasks over computer clusters,0.6456733345985413
translation,308,7,model,model,present,uima framework,model present uima framework,0.6609743237495422
translation,308,23,model,uima framework,to manage,computation distribution,uima framework to manage computation distribution,0.6443300843238831
translation,308,23,model,computation distribution,of,complicated processing pipelines,computation distribution of complicated processing pipelines,0.5606434345245361
translation,308,23,model,complicated processing pipelines,involved in,cqa systems,complicated processing pipelines involved in cqa systems,0.6921665668487549
translation,308,23,model,model,propose,uima framework,model propose uima framework,0.6762543320655823
translation,308,24,model,computation,of,standard linguistic processing components,computation of standard linguistic processing components,0.5279392600059509
translation,308,24,model,computation,of,feature / structure extractors,computation of feature / structure extractors,0.5737828612327576
translation,308,24,model,computation,of,classification or learning phases,computation of classification or learning phases,0.5911440253257751
translation,308,24,model,standard linguistic processing components,has,feature / structure extractors,standard linguistic processing components has feature / structure extractors,0.5295670032501221
translation,308,24,model,classification or learning phases,has,scalable,classification or learning phases has scalable,0.5933030843734741
translation,308,24,model,model,make,computation,model make computation,0.7100147604942322
translation,308,25,results,state - of - the - art accuracy,of,tree kernelbased rerankers,state - of - the - art accuracy of tree kernelbased rerankers,0.5430075526237488
translation,309,6,ablation-analysis,novel multitask architectures,with,high- level ( semantic ) layer -specific sharing,novel multitask architectures with high- level ( semantic ) layer -specific sharing,0.6105353236198425
translation,309,6,ablation-analysis,ablation analysis,propose,novel multitask architectures,ablation analysis propose novel multitask architectures,0.640119731426239
translation,309,175,ablation-analysis,2 - way mtl model,with,entailment generation,2 - way mtl model with entailment generation,0.5686314105987549
translation,309,175,ablation-analysis,entailment generation,reduces,extraneous count,entailment generation reduces extraneous count,0.6515757441520691
translation,309,175,ablation-analysis,extraneous count,by,17.2 %,extraneous count by 17.2 %,0.5325686931610107
translation,309,175,ablation-analysis,17.2 %,w.r.t.,baseline,17.2 % w.r.t. baseline,0.5527365803718567
translation,309,175,ablation-analysis,ablation analysis,found that,2 - way mtl model,ablation analysis found that 2 - way mtl model,0.6408352255821228
translation,309,45,baselines,baselines,has,baseline pointer + coverage model,baselines has baseline pointer + coverage model,0.5264441967010498
translation,309,22,model,novel multi-task learning architectures,based on,multi-layered encoder and decoder models,novel multi-task learning architectures based on multi-layered encoder and decoder models,0.6175107359886169
translation,309,22,model,novel multi-task learning architectures,empirically show,substantially better,novel multi-task learning architectures empirically show substantially better,0.7384781241416931
translation,309,22,model,substantially better,to share,higherlevel semantic layers,substantially better to share higherlevel semantic layers,0.6583468914031982
translation,309,22,model,higherlevel semantic layers,between,three aforementioned tasks,higherlevel semantic layers between three aforementioned tasks,0.6272236704826355
translation,309,22,model,higherlevel semantic layers,keeping,lower - level ( lexico-syntactic ) layers unshared,higherlevel semantic layers keeping lower - level ( lexico-syntactic ) layers unshared,0.6683128476142883
translation,309,22,model,model,present,novel multi-task learning architectures,model present novel multi-task learning architectures,0.6056849360466003
translation,309,7,results,statistically significant improvements,over,state-ofthe- art,statistically significant improvements over state-ofthe- art,0.6395524144172668
translation,309,7,results,state-ofthe- art,on,cnn / dailymail and gigaword datasets,state-ofthe- art on cnn / dailymail and gigaword datasets,0.4564495086669922
translation,309,24,results,"soft , layer -specific sharing model",with,question and entailment generation auxiliary tasks,"soft , layer -specific sharing model with question and entailment generation auxiliary tasks",0.6045814156532288
translation,309,24,results,"soft , layer -specific sharing model",achieves,statistically significant improvements,"soft , layer -specific sharing model achieves statistically significant improvements",0.674355685710907
translation,309,24,results,statistically significant improvements,over,state - of- the - art,statistically significant improvements over state - of- the - art,0.642184853553772
translation,309,24,results,statistically significant improvements,on,cnn / dailymail and gigaword datasets,statistically significant improvements on cnn / dailymail and gigaword datasets,0.48684871196746826
translation,309,24,results,state - of- the - art,on,cnn / dailymail and gigaword datasets,state - of- the - art on cnn / dailymail and gigaword datasets,0.45976904034614563
translation,309,24,results,results,has,"soft , layer -specific sharing model","results has soft , layer -specific sharing model",0.5474333167076111
translation,309,25,results,significantly better,on,duc - 2002 transfer setup,significantly better on duc - 2002 transfer setup,0.516511857509613
translation,309,38,results,our new soft sharing parameter approach,gives,stat,our new soft sharing parameter approach gives stat,0.6439568996429443
translation,309,38,results,results,has,our new soft sharing parameter approach,results has our new soft sharing parameter approach,0.5623038411140442
translation,309,122,results,results,shows,our baseline model,results shows our baseline model,0.6813945174217224
translation,309,123,results,performs better,than,all previous works,performs better than all previous works,0.5429725646972656
translation,309,123,results,gigaword dataset,has,our baseline model,gigaword dataset has our baseline model,0.5414698123931885
translation,309,123,results,our baseline model,has,performs better,our baseline model has performs better,0.5884690880775452
translation,309,123,results,results,On,gigaword dataset,results On gigaword dataset,0.48238715529441833
translation,309,126,results,multi-task setting,better than,our strong baseline models,multi-task setting better than our strong baseline models,0.7063779830932617
translation,309,128,results,statistically significant,for,cnn / dailymail,statistically significant for cnn / dailymail,0.6315647959709167
translation,309,128,results,statistically significant,for,gigaword,statistically significant for gigaword,0.6660758852958679
translation,309,128,results,statistically significant,for,gigaword,statistically significant for gigaword,0.6660758852958679
translation,309,128,results,meteor ( p < 0.01 ),for,cnn / dailymail,meteor ( p < 0.01 ) for cnn / dailymail,0.6385622024536133
translation,309,128,results,meteor ( p < 0.01 ),for,gigaword,meteor ( p < 0.01 ) for gigaword,0.6713833808898926
translation,309,128,results,all metrics ( p < 0.01 ),for,gigaword,all metrics ( p < 0.01 ) for gigaword,0.6237583160400391
translation,309,128,results,multi-task learning with question generation,has,improvements,multi-task learning with question generation has improvements,0.5291669368743896
translation,309,128,results,results,For,multi-task learning with question generation,results For multi-task learning with question generation,0.5358951687812805
translation,309,135,results,2 - way multi-task models,perform,sig- multi-task with entailment,2 - way multi-task models perform sig- multi-task with entailment,0.5819026827812195
translation,309,135,results,2 - way multi-task models,perform,multi-task learning,2 - way multi-task models perform multi-task learning,0.5608952045440674
translation,309,135,results,multi-task learning,with,all three tasks together,multi-task learning with all three tasks together,0.6001851558685303
translation,309,135,results,results,perform,multi-task learning,results perform multi-task learning,0.5503182411193848
translation,309,136,results,our full multi-task model,achieves,best scores,our full multi-task model achieves best scores,0.6629871129989624
translation,309,136,results,best scores,on,cnn / dailymail and gigaword datasets,best scores on cnn / dailymail and gigaword datasets,0.46041393280029297
translation,309,136,results,results,show,our full multi-task model,results show our full multi-task model,0.5881885290145874
translation,309,137,results,3 - way multi-task model,with both,entailment and question generation,3 - way multi-task model with both entailment and question generation,0.6298240423202515
translation,309,137,results,outperforms,with,stat,outperforms with stat,0.7333526015281677
translation,309,137,results,publicly - available pretrained result,of,previous sota,publicly - available pretrained result of previous sota,0.5026894211769104
translation,309,137,results,publicly - available pretrained result,with,stat,publicly - available pretrained result with stat,0.6595594882965088
translation,309,137,results,),of,previous sota,) of previous sota,0.636786162853241
translation,309,137,results,3 - way multi-task model,has,outperforms,3 - way multi-task model has outperforms,0.574936032295227
translation,309,137,results,outperforms,has,publicly - available pretrained result,outperforms has publicly - available pretrained result,0.547378659248352
translation,309,137,results,publicly - available pretrained result,has,),publicly - available pretrained result has ),0.5815847516059875
translation,309,137,results,results,has,3 - way multi-task model,results has 3 - way multi-task model,0.5207887887954712
translation,309,143,results,our mtl model,is,better,our mtl model is better,0.6036057472229004
translation,309,143,results,better,than,our state - of - theart baseline,better than our state - of - theart baseline,0.5425453186035156
translation,309,143,results,better,on,relevance and readability,better on relevance and readability,0.49409928917884827
translation,309,150,results,multitask model,achieves,statistically significant improvements,multitask model achieves statistically significant improvements,0.6532428860664368
translation,309,150,results,statistically significant improvements,over,strong baseline,statistically significant improvements over strong baseline,0.6640254855155945
translation,309,150,results,statistically significant improvements,over,pointercoverage model,statistically significant improvements over pointercoverage model,0.661251962184906
translation,309,173,results,our multi-task model,improves upon,baseline,our multi-task model improves upon baseline,0.74957674741745
translation,309,173,results,baseline,in the aspect of being entailed by,source document,baseline in the aspect of being entailed by source document,0.6783629059791565
translation,310,164,ablation-analysis,first stage,of,training,first stage of training,0.604648768901825
translation,310,164,ablation-analysis,rs,prevents,excessive focus,rs prevents excessive focus,0.7364476323127747
translation,310,164,ablation-analysis,excessive focus,on,positive paragraphs,excessive focus on positive paragraphs,0.5504392981529236
translation,310,164,ablation-analysis,first stage,has,rs,first stage has rs,0.6066097617149353
translation,310,164,ablation-analysis,training,has,rs,training has rs,0.6617156267166138
translation,310,164,ablation-analysis,ablation analysis,In,first stage,ablation analysis In first stage,0.5396569967269897
translation,310,139,baselines,traditional rc models,i.e.,"bidaf ( seo et al. , 2016 )","traditional rc models i.e. bidaf ( seo et al. , 2016 )",0.6861738562583923
translation,310,139,baselines,single- paragraph models,i.e.,"r3 ( wang et al. , 2018a )","single- paragraph models i.e. r3 ( wang et al. , 2018a )",0.683476448059082
translation,310,166,experiments,sum method,helps,our model,sum method helps our model,0.6664426326751709
translation,310,166,experiments,our model,better extract,correct answer,our model better extract correct answer,0.7747812867164612
translation,310,142,hyperparameters,ranker,use,300 - dimensional word embeddings,ranker use 300 - dimensional word embeddings,0.5686383247375488
translation,310,142,hyperparameters,300 - dimensional word embeddings,pre-trained by,glove,300 - dimensional word embeddings pre-trained by glove,0.7620936632156372
translation,310,142,hyperparameters,20 - dimensional character embeddings,as,learnable parameters,20 - dimensional character embeddings as learnable parameters,0.47118079662323
translation,310,142,hyperparameters,hyperparameters,For,ranker,hyperparameters For ranker,0.5933512449264526
translation,310,143,hyperparameters,common word feature,mapped into,4 - dimensional vector,common word feature mapped into 4 - dimensional vector,0.6844922304153442
translation,310,143,hyperparameters,updated,during,training,updated during training,0.7725804448127747
translation,310,143,hyperparameters,hyperparameters,has,common word feature,hyperparameters has common word feature,0.49620866775512695
translation,310,144,hyperparameters,hidden size,of,lstm,hidden size of lstm,0.5805742740631104
translation,310,144,hyperparameters,lstm,to,150,lstm to 150,0.6207871437072754
translation,310,144,hyperparameters,number of lstm layers,to,1,number of lstm layers to 1,0.5339049696922302
translation,310,144,hyperparameters,hyperparameters,set,hidden size,hyperparameters set hidden size,0.6568804979324341
translation,310,144,hyperparameters,hyperparameters,set,number of lstm layers,hyperparameters set number of lstm layers,0.5872710347175598
translation,310,145,hyperparameters,adam,with,learning rate,adam with learning rate,0.6478732824325562
translation,310,145,hyperparameters,learning rate,to optimize,model,learning rate to optimize model,0.7232510447502136
translation,310,145,hyperparameters,5e - 4,to optimize,model,5e - 4 to optimize model,0.7474564909934998
translation,310,145,hyperparameters,learning rate,has,5e - 4,learning rate has 5e - 4,0.5730709433555603
translation,310,146,hyperparameters,batch size,set to,8,batch size set to 8,0.770182192325592
translation,310,146,hyperparameters,dropout,applied to,outputs,dropout applied to outputs,0.6944172382354736
translation,310,146,hyperparameters,dropout,at,rate,dropout at rate,0.5147415399551392
translation,310,146,hyperparameters,outputs,of,all lstm layers,outputs of all lstm layers,0.5237430930137634
translation,310,146,hyperparameters,rate,of,0.2,rate of 0.2,0.6393477320671082
translation,310,146,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,310,146,hyperparameters,hyperparameters,has,dropout,hyperparameters has dropout,0.5324090719223022
translation,310,9,model,ranking model,leveraging,paragraph -question and the paragraph - paragraph relevance,ranking model leveraging paragraph -question and the paragraph - paragraph relevance,0.6520883440971375
translation,310,9,model,ranking model,to compute,confidence score,ranking model to compute confidence score,0.6746835708618164
translation,310,9,model,confidence score,for,each paragraph,confidence score for each paragraph,0.6140657067298889
translation,310,9,model,model,introduce,ranking model,model introduce ranking model,0.5961945652961731
translation,310,10,model,modified weighted sampling strategy,for,training,modified weighted sampling strategy for training,0.6144231557846069
translation,310,10,model,influence,of,noisy and distracting paragraphs,influence of noisy and distracting paragraphs,0.5754234790802002
translation,310,33,model,word-level and sentence - level information,between,paragraph and the question,word-level and sentence - level information between paragraph and the question,0.6118844747543335
translation,310,33,model,model,Through,multi-level attention mechanism,model Through multi-level attention mechanism,0.6171321868896484
translation,310,35,model,confidence scores,design,modified weighted sampling strategy,confidence scores design modified weighted sampling strategy,0.5773029923439026
translation,310,35,model,modified weighted sampling strategy,to select,training paragraphs,modified weighted sampling strategy to select training paragraphs,0.7380709052085876
translation,310,35,model,training paragraphs,simultaneously ameliorate,influence,training paragraphs simultaneously ameliorate influence,0.7248873710632324
translation,310,35,model,influence,of,distracting and noisy paragraphs,influence of distracting and noisy paragraphs,0.5641961097717285
translation,310,35,model,model,based on,confidence scores,model based on confidence scores,0.668637752532959
translation,310,59,model,characters,in,word,characters in word,0.5253772735595703
translation,310,59,model,characters,in,word,characters in word,0.5253772735595703
translation,310,59,model,characters,of,word,characters of word,0.5847733020782471
translation,310,59,model,20 - dimensional vectors,passed to,convolutional layer,20 - dimensional vectors passed to convolutional layer,0.6825066804885864
translation,310,59,model,20 - dimensional vectors,passed to,max pooling layer,20 - dimensional vectors passed to max pooling layer,0.5991882681846619
translation,310,59,model,max pooling layer,to obtain,fixed - size vector,max pooling layer to obtain fixed - size vector,0.5389898419380188
translation,310,59,model,fixed - size vector,of,word,fixed - size vector of word,0.598459780216217
translation,310,59,model,model,map,characters,model map characters,0.8185694217681885
translation,310,158,results,"our ( rs , rk , max ) model",achieves,better results,"our ( rs , rk , max ) model achieves better results",0.6441057324409485
translation,310,158,results,better results,on,most of the datasets,better results on most of the datasets,0.4881395399570465
translation,310,158,results,better results,compared to,baselines,better results compared to baselines,0.6713672876358032
translation,310,158,results,results,has,"our ( rs , rk , max ) model","results has our ( rs , rk , max ) model",0.5338417887687683
translation,310,162,results,consistently outperforms,across,three datasets,consistently outperforms across three datasets,0.6935528516769409
translation,310,162,results,"our ( rs , rk , max ) model",across,three datasets,"our ( rs , rk , max ) model across three datasets",0.727289080619812
translation,310,162,results,"our ( rs?ws , rk , max ) model",has,consistently outperforms,"our ( rs?ws , rk , max ) model has consistently outperforms",0.5813161134719849
translation,310,162,results,consistently outperforms,has,"our ( rs , rk , max ) model","consistently outperforms has our ( rs , rk , max ) model",0.598731279373169
translation,310,162,results,results,has,"our ( rs?ws , rk , max ) model","results has our ( rs?ws , rk , max ) model",0.5447164177894592
translation,310,169,results,"our ( rs?ws , rk , max ) model",achieves,close performance,"our ( rs?ws , rk , max ) model achieves close performance",0.6730117201805115
translation,310,169,results,close performance,compared to,has - qa model,close performance compared to has - qa model,0.7219242453575134
translation,310,169,results,searchqa dataset,has,"our ( rs?ws , rk , max ) model","searchqa dataset has our ( rs?ws , rk , max ) model",0.5790396928787231
translation,310,169,results,results,On,searchqa dataset,results On searchqa dataset,0.5662329792976379
translation,310,172,results,our model,better at filtering out,irrelevant data,our model better at filtering out irrelevant data,0.8119311928749084
translation,310,172,results,improvement,in,searchqa,improvement in searchqa,0.5549055337905884
translation,310,172,results,not as significant,as in,other datasets,not as significant as in other datasets,0.5997627973556519
translation,310,172,results,results,has,our model,results has our model,0.5871725678443909
translation,310,173,results,"our ( rs?ws , rk , sum ) model",achieves,sota result,"our ( rs?ws , rk , sum ) model achieves sota result",0.686843752861023
translation,310,173,results,sota result,by adopting,sum method,sota result by adopting sum method,0.7357995510101318
translation,310,173,results,sota result,adopts,similar method,sota result adopts similar method,0.6201712489128113
translation,310,173,results,results,has,"our ( rs?ws , rk , sum ) model","results has our ( rs?ws , rk , sum ) model",0.5424889922142029
translation,310,181,results,our ranker,surpasses,ir model,our ranker surpasses ir model,0.6111636757850647
translation,310,181,results,ir model,with,large margin,ir model with large margin,0.6423776149749756
translation,310,183,results,fse and msr,adopt,neural network,fse and msr adopt neural network,0.6310535669326782
translation,310,183,results,neural network,to select,paragraphs,neural network to select paragraphs,0.7433861494064331
translation,310,183,results,our model,works,better,our model works better,0.6327770352363586
translation,310,183,results,r3,has,ds - qa,r3 has ds - qa,0.6631409525871277
translation,310,183,results,r3,has,our model,r3 has our model,0.6100741028785706
translation,310,183,results,ds - qa,has,fse and msr,ds - qa has fse and msr,0.5945190191268921
translation,310,183,results,results,Compared with,r3,results Compared with r3,0.6689147353172302
translation,310,191,results,ws,leads to,slight improvement,ws leads to slight improvement,0.7117704749107361
translation,310,191,results,slight improvement,thanks to,denoising effect,slight improvement thanks to denoising effect,0.45228639245033264
translation,310,191,results,rs,has,ws,rs has ws,0.6905159950256348
translation,310,191,results,results,Compared with,rs,results Compared with rs,0.7256646156311035
translation,310,193,results,rs ? ws,gains,best score,rs ? ws gains best score,0.7547033429145813
translation,310,193,results,best score,on,three datasets,best score on three datasets,0.5004531741142273
translation,310,193,results,results,see that,rs ? ws,results see that rs ? ws,0.6583521962165833
translation,311,118,baselines,three variants,with,information retrieval baseline,three variants with information retrieval baseline,0.6229166984558105
translation,311,118,baselines,gan - utility,is,our full model,gan - utility is our full model,0.5291433930397034
translation,311,118,baselines,our full model,which is,util -ity calculator based gan training,our full model which is util -ity calculator based gan training,0.5780065059661865
translation,311,119,baselines,max - utility,is,reinforcement learning baseline,max - utility is reinforcement learning baseline,0.5201144814491272
translation,311,119,baselines,reinforcement learning baseline,where,pretrained question generator model,reinforcement learning baseline where pretrained question generator model,0.5191439986228943
translation,311,119,baselines,pretrained question generator model,is,further trained,pretrained question generator model is further trained,0.507088303565979
translation,311,119,baselines,further trained,to optimize,utility reward ( ?2.2 ),further trained to optimize utility reward ( ?2.2 ),0.7138491272926331
translation,311,119,baselines,utility reward ( ?2.2 ),without,adversarial training,utility reward ( ?2.2 ) without adversarial training,0.6726571321487427
translation,311,119,baselines,baselines,has,max - utility,baselines has max - utility,0.547218382358551
translation,311,120,baselines,mle,is,question generator model,mle is question generator model,0.566860556602478
translation,311,120,baselines,question generator model,pretrained on,"context , question pairs","question generator model pretrained on context , question pairs",0.6869571208953857
translation,311,120,baselines,"context , question pairs",using,maximum likelihood objective ( ?2.1 ),"context , question pairs using maximum likelihood objective ( ?2.1 )",0.6407540440559387
translation,311,120,baselines,baselines,has,mle,baselines has mle,0.6115690469741821
translation,311,6,model,generative adversarial network ( gan ),where,generator,generative adversarial network ( gan ) where generator,0.5947293043136597
translation,311,6,model,generator,is,sequence - to-sequence model,generator is sequence - to-sequence model,0.5968993902206421
translation,311,6,model,discriminator,is,utility function,discriminator is utility function,0.525762140750885
translation,311,6,model,utility function,models,value,utility function models value,0.6975125074386597
translation,311,6,model,value,of updating,context,value of updating context,0.7400918006896973
translation,311,6,model,context,answer to,clarification question,context answer to clarification question,0.7056412100791931
translation,311,6,model,model,develop,generative adversarial network ( gan ),model develop generative adversarial network ( gan ),0.6134535074234009
translation,311,12,model,clarification question generation model,builds on,sequence - tosequence approach,clarification question generation model builds on sequence - tosequence approach,0.6799299716949463
translation,311,12,model,model,has,clarification question generation model,model has clarification question generation model,0.547620952129364
translation,311,146,results,all ablations,on,diversity,all ablations on diversity,0.6026339530944824
translation,311,146,results,amazon dataset,has,gan - utility,amazon dataset has gan - utility,0.5732336044311523
translation,311,146,results,gan - utility,has,outperforms,gan - utility has outperforms,0.6366574168205261
translation,311,146,results,outperforms,has,all ablations,outperforms has all ablations,0.5847799181938171
translation,311,146,results,results,In,amazon dataset,results In amazon dataset,0.5397416353225708
translation,311,153,results,all ablations,on,bleu,all ablations on bleu,0.6546648740768433
translation,311,153,results,all ablations,on,me-teor,all ablations on me-teor,0.6433497667312622
translation,311,153,results,stackexchange dataset,has,gan - utility,stackexchange dataset has gan - utility,0.5593165755271912
translation,311,153,results,gan - utility,has,outperforms,gan - utility has outperforms,0.6366574168205261
translation,311,153,results,outperforms,has,all ablations,outperforms has all ablations,0.5847799181938171
translation,311,153,results,results,In,stackexchange dataset,results In stackexchange dataset,0.5165035128593445
translation,311,154,results,gan - utility,in,bleu,gan - utility in bleu,0.594782829284668
translation,311,154,results,mle,has,does not outperform,mle has does not outperform,0.6290498971939087
translation,311,154,results,does not outperform,has,gan - utility,does not outperform has gan - utility,0.634378969669342
translation,311,156,results,mle,on,diversity,mle on diversity,0.5264232754707336
translation,311,156,results,gan - utility,has,outperforms,gan - utility has outperforms,0.6366574168205261
translation,311,156,results,outperforms,has,mle,outperforms has mle,0.6274203658103943
translation,311,162,results,all models,weaker than,lucene,all models weaker than lucene,0.7744080424308777
translation,311,162,results,equally good,at,seeking new information,equally good at seeking new information,0.5369893312454224
translation,311,162,results,equally good,at,seeking new information,equally good at seeking new information,0.5369893312454224
translation,311,162,results,equally good,at,seeking new information,equally good at seeking new information,0.5369893312454224
translation,311,162,results,better,at,seeking new information,better at seeking new information,0.5555582046508789
translation,311,162,results,better,cost of,lower specificity,better cost of lower specificity,0.6380833983421326
translation,311,162,results,better,cost of,lower usefulness,better cost of lower usefulness,0.6091250777244568
translation,311,162,results,results,has,all models,results has all models,0.5029959678649902
translation,311,163,results,our full model,performs,significantly better,our full model performs significantly better,0.6124597787857056
translation,311,163,results,significantly better,at,usefulness criteria,significantly better at usefulness criteria,0.4895905554294586
translation,311,163,results,our full model,has,gan - utility,our full model has gan - utility,0.5897469520568848
translation,311,163,results,results,has,our full model,results has our full model,0.5411279201507568
translation,311,163,results,results,has,gan - utility,results has gan - utility,0.5534266829490662
translation,311,164,results,all our models,produce,questions,all our models produce questions,0.7097426056861877
translation,311,164,results,questions,that are,more useful,questions that are more useful,0.608745813369751
translation,311,164,results,more useful,than,lucene and reference,more useful than lucene and reference,0.5778924226760864
translation,311,164,results,results,has,all our models,results has all our models,0.546053409576416
translation,311,165,results,gan - utility,performs,significantly better,gan - utility performs significantly better,0.6041226387023926
translation,311,165,results,at generating questions,more specific to,product,at generating questions more specific to product,0.7675615549087524
translation,311,165,results,significantly better,has,at generating questions,significantly better has at generating questions,0.5502648949623108
translation,311,165,results,results,has,gan - utility,results has gan - utility,0.5534266829490662
translation,312,235,ablation-analysis,number of edges,has,increases,number of edges has increases,0.6126993894577026
translation,312,235,ablation-analysis,increases,has,f1 score,increases has f1 score,0.5979864597320557
translation,312,235,ablation-analysis,f1 score,has,decreases,f1 score has decreases,0.6175783276557922
translation,312,211,experiments,parasempre,follow,semantic parsing paradigm,parasempre follow semantic parsing paradigm,0.5896888971328735
translation,312,211,experiments,parasempre,has,"berant and liang , 2014 )","parasempre has berant and liang , 2014 )",0.5975359678268433
translation,312,261,experiments,f1 score,of,"sem - pre , parasempre , and jacana","f1 score of sem - pre , parasempre , and jacana",0.567745566368103
translation,312,261,experiments,"sem - pre , parasempre , and jacana",are,"37.65 % , 53.2 % , and 36.2 %","sem - pre , parasempre , and jacana are 37.65 % , 53.2 % , and 36.2 %",0.5307953357696533
translation,312,261,experiments,"sem - pre , parasempre , and jacana",only,"37.65 % , 53.2 % , and 36.2 %","sem - pre , parasempre , and jacana only 37.65 % , 53.2 % , and 36.2 %",0.6851882338523865
translation,312,261,experiments,"37.65 % , 53.2 % , and 36.2 %",of,score,"37.65 % , 53.2 % , and 36.2 % of score",0.5473872423171997
translation,312,261,experiments,score,on,top - 1 paraphrases,score on top - 1 paraphrases,0.49606284499168396
translation,312,261,experiments,fourth- ranked paraphrases,has,f1 score,fourth- ranked paraphrases has f1 score,0.5366158485412598
translation,312,265,experiments,sempre and jacana,get,significantly lower f1 score,sempre and jacana get significantly lower f1 score,0.606117308139801
translation,312,265,experiments,significantly lower f1 score,on,multi-answer questions,significantly lower f1 score on multi-answer questions,0.5150993466377258
translation,312,265,experiments,),on,multi-answer questions,) on multi-answer questions,0.5389951467514038
translation,312,265,experiments,significantly lower f1 score,has,),significantly lower f1 score has ),0.5817427039146423
translation,312,5,model,graph-structured logical forms,from,knowledge base,graph-structured logical forms from knowledge base,0.5537576079368591
translation,312,7,model,qa dataset,with,"over 5,000","qa dataset with over 5,000",0.647456169128418
translation,312,7,model,"over 5,000",associated with,answers,"over 5,000 associated with answers",0.6593040823936462
translation,312,7,model,logical form-question pairs,associated with,answers,logical form-question pairs associated with answers,0.6579119563102722
translation,312,7,model,answers,from,knowledge base,answers from knowledge base,0.5553200840950012
translation,312,7,model,"over 5,000",has,logical form-question pairs,"over 5,000 has logical form-question pairs",0.5867197513580322
translation,312,7,model,model,construct,qa dataset,model construct qa dataset,0.7982030510902405
translation,312,18,model,model,examine,impact,model examine impact,0.674853503704071
translation,312,19,model,semi-automated framework,to construct,qa datasets,semi-automated framework to construct qa datasets,0.6742052435874939
translation,312,19,model,qa datasets,with,characteristic specification,qa datasets with characteristic specification,0.5836014151573181
translation,312,19,model,characteristic specification,from,knowledge base,characteristic specification from knowledge base,0.5144128799438477
translation,312,19,model,model,present,semi-automated framework,model present semi-automated framework,0.6637831926345825
translation,312,21,model,graph queries,from,knowledge base,graph queries from knowledge base,0.5202201008796692
translation,312,21,model,graph queries,into,questions,graph queries into questions,0.5548317432403564
translation,312,21,model,human annotators,to convert,graph queries,human annotators to convert graph queries,0.5746561288833618
translation,312,21,model,graph queries,into,questions,graph queries into questions,0.5548317432403564
translation,312,21,model,model,automatically generate,graph queries,model automatically generate graph queries,0.6705710291862488
translation,312,21,model,model,employ,human annotators,model employ human annotators,0.5517920255661011
translation,312,24,model,redundant components,in,graph query,redundant components in graph query,0.5297378301620483
translation,312,24,model,model,identify,redundant components,model identify redundant components,0.7144636511802673
translation,312,24,model,model,develop,techniques,model develop techniques,0.6671661138534546
translation,312,25,model,"frequency of entities , classes , and relations",mined from,web,"frequency of entities , classes , and relations mined from web",0.626569926738739
translation,312,25,model,"frequency of entities , classes , and relations",quantify,commonness,"frequency of entities , classes , and relations quantify commonness",0.656150221824646
translation,312,25,model,"frequency of entities , classes , and relations",filter out,too rare ones,"frequency of entities , classes , and relations filter out too rare ones",0.7635310888290405
translation,312,25,model,commonness,of,graph query,commonness of graph query,0.5451335906982422
translation,312,25,model,model,based on,"frequency of entities , classes , and relations","model based on frequency of entities , classes , and relations",0.6097351312637329
translation,312,25,model,model,quantify,commonness,model quantify commonness,0.7225983142852783
translation,312,212,model,sempre,conducts,bottom - up beambased parsing,sempre conducts bottom - up beambased parsing,0.6529843807220459
translation,312,212,model,bottom - up beambased parsing,on,questions,bottom - up beambased parsing on questions,0.5372844934463501
translation,312,212,model,questions,to find,best logical form,questions to find best logical form,0.6415818929672241
translation,312,212,model,model,has,sempre,model has sempre,0.5881115794181824
translation,312,219,results,overall evaluation,Compared with,scores,overall evaluation Compared with scores,0.6091337203979492
translation,312,219,results,overall evaluation,scores on,webquestions ( 30 % - 40 % ),overall evaluation scores on webquestions ( 30 % - 40 % ),0.6259003281593323
translation,312,219,results,scores,on,graphquestions,scores on graphquestions,0.5311026573181152
translation,312,219,results,graphquestions,are,lower,graphquestions are lower,0.5987673401832581
translation,312,219,results,results,has,overall evaluation,results has overall evaluation,0.5780411958694458
translation,312,223,results,graphquestions,has,sempre and parasempre,graphquestions has sempre and parasempre,0.6119261980056763
translation,312,223,results,sempre and parasempre,has,significantly outperform,sempre and parasempre has significantly outperform,0.5903737545013428
translation,312,223,results,significantly outperform,has,jacana,significantly outperform has jacana,0.6076927185058594
translation,312,223,results,results,On,graphquestions,results On graphquestions,0.5404661297798157
translation,312,248,results,sem - pre 's performance,gets,worse,sem - pre 's performance gets worse,0.6723045110702515
translation,312,248,results,worse,on,most common questions,worse on most common questions,0.5210828185081482
translation,312,248,results,results,has,interesting observation,results has interesting observation,0.5519416928291321
translation,312,262,results,parasempre,seem to be,more robust,parasempre seem to be more robust,0.6848646998405457
translation,312,262,results,paraphrasing,has,parasempre,paraphrasing has parasempre,0.6262631416320801
translation,313,64,baselines,bert,for,q-to -a similarity,bert for q-to -a similarity,0.6803374886512756
translation,313,64,baselines,candidate faq pairs,according to,similarity,candidate faq pairs according to similarity,0.6525282859802246
translation,313,64,baselines,reranking,has,candidate faq pairs,reranking has candidate faq pairs,0.5306981801986694
translation,313,64,baselines,baselines,has,bert,baselines has bert,0.5950736403465271
translation,313,105,baselines,first one,calculates,combined score,first one calculates combined score,0.6606932878494263
translation,313,105,baselines,"combsum ( kurland and culpepper , 2018 )",calculates,combined score,"combsum ( kurland and culpepper , 2018 ) calculates combined score",0.6485852599143982
translation,313,105,baselines,combined score,by summing for,each candidate pair,combined score by summing for each candidate pair,0.6560168862342834
translation,313,105,baselines,scores,assigned to,three re-ranking methods,scores assigned to three re-ranking methods,0.6978409886360168
translation,313,105,baselines,first one,has,"combsum ( kurland and culpepper , 2018 )","first one has combsum ( kurland and culpepper , 2018 )",0.5274957418441772
translation,313,105,baselines,each candidate pair,has,scores,each candidate pair has scores,0.5849658250808716
translation,313,105,baselines,baselines,has,first one,baselines has first one,0.6238251328468323
translation,313,105,baselines,baselines,has,"combsum ( kurland and culpepper , 2018 )","baselines has combsum ( kurland and culpepper , 2018 )",0.518787682056427
translation,313,107,baselines,poolrank,ranks,candidate pairs,poolrank ranks candidate pairs,0.770248293876648
translation,313,107,baselines,candidate pairs,using,combsum,candidate pairs using combsum,0.6973496675491333
translation,313,107,baselines,baselines,has,poolrank,baselines has poolrank,0.5741909146308899
translation,313,118,baselines,two ( supervised ) learningto -rank methods,trained over,diverse set of text similarity features,two ( supervised ) learningto -rank methods trained over diverse set of text similarity features,0.7329069375991821
translation,313,118,baselines,( supervised ) learning - to - rank approach,based on,convolutional neural network ( cnn ),( supervised ) learning - to - rank approach based on convolutional neural network ( cnn ),0.6378313302993774
translation,313,118,baselines,rc,has,"ensemble of three unsupervised methods ( bm25 , vector-space and word-embeddings )","rc has ensemble of three unsupervised methods ( bm25 , vector-space and word-embeddings )",0.5649381875991821
translation,313,118,baselines,listnet and lambdamart,has,two ( supervised ) learningto -rank methods,listnet and lambdamart has two ( supervised ) learningto -rank methods,0.5309804081916809
translation,313,118,baselines,cnn - rank,has,( supervised ) learning - to - rank approach,cnn - rank has ( supervised ) learning - to - rank approach,0.5833925604820251
translation,313,120,baselines,search engine,for,q-to -q matching,search engine for q-to -q matching,0.5938301682472229
translation,313,120,baselines,supervised fine - tuned bert model,for,q-to -a matching,supervised fine - tuned bert model for q-to -a matching,0.5949437022209167
translation,313,120,baselines,two methods,has,search engine,two methods has search engine,0.546868085861206
translation,313,120,baselines,"tsubaki ( shinzato et al. , 2008 )",has,search engine,"tsubaki ( shinzato et al. , 2008 ) has search engine",0.5482113361358643
translation,313,138,baselines,base bm25 retrieval ( bm25 ( q + a ) ),has,our three proposed unsupervised re-ranking methods,base bm25 retrieval ( bm25 ( q + a ) ) has our three proposed unsupervised re-ranking methods,0.5600059032440186
translation,313,138,baselines,baselines,compare,base bm25 retrieval ( bm25 ( q + a ) ),baselines compare base bm25 retrieval ( bm25 ( q + a ) ),0.6755838990211487
translation,313,139,baselines,fine-tuned,on,union,fine-tuned on union,0.549420952796936
translation,313,139,baselines,union,of,respective training sets,union of respective training sets,0.6244689226150513
translation,313,139,baselines,respective training sets,of,faqir and stackfaq datasets,respective training sets of faqir and stackfaq datasets,0.5801998972892761
translation,313,139,baselines,baselines,compare to,poolrank +,baselines compare to poolrank +,0.6433323621749878
translation,313,155,baselines,first one,based on,ir passage retrieval approach,first one based on ir passage retrieval approach,0.6669864058494568
translation,313,155,baselines,fine-tuned,to predict,query - to - answer and query - to-question matching,fine-tuned to predict query - to - answer and query - to-question matching,0.660232663154602
translation,313,155,baselines,baselines,has,first one,baselines has first one,0.6238251328468323
translation,313,141,experiments,faqir,achieved,"0.67 , 0.61 and 0.90","faqir achieved 0.67 , 0.61 and 0.90",0.6173750162124634
translation,313,141,experiments,"0.67 , 0.61 and 0.90",for,"p@5 , map and mrr","0.67 , 0.61 and 0.90 for p@5 , map and mrr",0.640981137752533
translation,313,149,experiments,all other methods,except,"supervised tsubaki + bert ( sakata et al. , 2019 )","all other methods except supervised tsubaki + bert ( sakata et al. , 2019 )",0.632345974445343
translation,313,149,experiments,stack - faq,has,poolrank,stack - faq has poolrank,0.618605375289917
translation,313,149,experiments,poolrank,has,outperformed,poolrank has outperformed,0.638979971408844
translation,313,149,experiments,outperformed,has,all other methods,outperformed has all other methods,0.6067802309989929
translation,313,133,hyperparameters,fine-tuning,done with,learning rate,fine-tuning done with learning rate,0.6648560166358948
translation,313,133,hyperparameters,fine-tuning,done with,3 training epochs,fine-tuning done with 3 training epochs,0.6820341944694519
translation,313,133,hyperparameters,learning rate,of,2e - 5,learning rate of 2e - 5,0.6410878896713257
translation,313,133,hyperparameters,learning rate,of,3 training epochs,learning rate of 3 training epochs,0.5623351335525513
translation,313,133,hyperparameters,hyperparameters,has,fine-tuning,hyperparameters has fine-tuning,0.5253989100456238
translation,313,21,model,unsupervised gap,by using,distant supervision,unsupervised gap by using distant supervision,0.6493126749992371
translation,313,21,model,distant supervision,to train,neural models,distant supervision to train neural models,0.6885073781013489
translation,313,21,model,model,overcome,unsupervised gap,model overcome unsupervised gap,0.7619401812553406
translation,313,26,model,novel weaksupervision approach,using,automatically generated question paraphrases,novel weaksupervision approach using automatically generated question paraphrases,0.5805524587631226
translation,313,26,model,automatically generated question paraphrases,coupled with,smart filtering,automatically generated question paraphrases coupled with smart filtering,0.658189058303833
translation,313,26,model,smart filtering,to ensure,high-quality paraphrases,smart filtering to ensure high-quality paraphrases,0.651544451713562
translation,313,26,model,model,implement,novel weaksupervision approach,model implement novel weaksupervision approach,0.6470715403556824
translation,313,81,model,generative pre-training ( gpt - 2 ) neural network model,for generating,question paraphrases,generative pre-training ( gpt - 2 ) neural network model for generating question paraphrases,0.6764084100723267
translation,313,81,model,model,fine- tune,generative pre-training ( gpt - 2 ) neural network model,model fine- tune generative pre-training ( gpt - 2 ) neural network model,0.7172113060951233
translation,313,82,model,gpt - 2,pre-trained on,huge bodies of text,gpt - 2 pre-trained on huge bodies of text,0.791093111038208
translation,313,82,model,gpt - 2,producing,deeply coherent text paragraphs,gpt - 2 producing deeply coherent text paragraphs,0.7303763628005981
translation,313,82,model,huge bodies of text,capturing,natural language structure,huge bodies of text capturing natural language structure,0.7037741541862488
translation,313,82,model,huge bodies of text,producing,deeply coherent text paragraphs,huge bodies of text producing deeply coherent text paragraphs,0.6991261839866638
translation,313,82,model,model,has,gpt - 2,model has gpt - 2,0.6007091999053955
translation,313,140,results,bert - q-q,was,best,bert - q-q was best,0.6851404309272766
translation,313,140,results,three re-rankers,has,bert - q-q,three re-rankers has bert - q-q,0.66933673620224
translation,313,140,results,results,observe,three re-rankers,results observe three re-rankers,0.579262375831604
translation,313,140,results,results,among,three re-rankers,results among three re-rankers,0.5762031674385071
translation,313,144,results,fusion methods,achieved,better results,fusion methods achieved better results,0.7270780205726624
translation,313,144,results,better results,than,individual re-rankers,better results than individual re-rankers,0.6270674467086792
translation,313,144,results,better results,with,better performance,better results with better performance,0.6223106980323792
translation,313,144,results,better performance,by,poolrank variants over combosum,better performance by poolrank variants over combosum,0.5672012567520142
translation,313,144,results,both datasets,has,fusion methods,both datasets has fusion methods,0.5630003213882446
translation,313,144,results,results,on,both datasets,results on both datasets,0.49870386719703674
translation,313,146,results,significantly better performance,on,faqir,significantly better performance on faqir,0.5257486701011658
translation,313,146,results,faqir,than,other two individual rankers,faqir than other two individual rankers,0.634764552116394
translation,313,146,results,bert - q-q,has,significantly better performance,bert - q-q has significantly better performance,0.6155014634132385
translation,313,146,results,results,has,bert - q-q,results has bert - q-q,0.6237596869468689
translation,313,147,results,poolrank,uses,relevance model,poolrank uses relevance model,0.5892308354377747
translation,313,147,results,poolrank,is,better approach,poolrank is better approach,0.5784953236579895
translation,313,147,results,poolrank,gives,better fusion results,poolrank gives better fusion results,0.6357372403144836
translation,313,147,results,relevance model,is,better approach,relevance model is better approach,0.5521567463874817
translation,313,147,results,results,has,poolrank,results has poolrank,0.5937986969947815
translation,313,148,results,baselines,see that,faqir,baselines see that faqir,0.6218191981315613
translation,313,148,results,baselines,see that,our unsupervised poolrank,baselines see that our unsupervised poolrank,0.5742298364639282
translation,313,148,results,baselines,on,faqir,baselines on faqir,0.49121150374412537
translation,313,148,results,all other methods,including,supervised methods,all other methods including supervised methods,0.6223498582839966
translation,313,148,results,baselines,has,our unsupervised poolrank,baselines has our unsupervised poolrank,0.561931848526001
translation,313,148,results,faqir,has,our unsupervised poolrank,faqir has our unsupervised poolrank,0.5902568697929382
translation,313,148,results,our unsupervised poolrank,has,outperformed,our unsupervised poolrank has outperformed,0.5871337056159973
translation,313,148,results,outperformed,has,all other methods,outperformed has all other methods,0.6067802309989929
translation,313,148,results,results,comparing with,baselines,results comparing with baselines,0.6670688390731812
translation,313,148,results,results,on,faqir,results on faqir,0.5056973695755005
translation,313,150,results,poolrank +,achieved,"( 0.75 , 0.88 and 0.90","poolrank + achieved ( 0.75 , 0.88 and 0.90",0.6580893993377686
translation,313,150,results,"( 0.75 , 0.88 and 0.90",for,"p@5 , map and mrr","( 0.75 , 0.88 and 0.90 for p@5 , map and mrr",0.6685047149658203
translation,313,150,results,faqir initial retrieval,done against,subset of 789 faq pairs,faqir initial retrieval done against subset of 789 faq pairs,0.6822944283485413
translation,313,150,results,subset of 789 faq pairs,relevant to,at least one user query,subset of 789 faq pairs relevant to at least one user query,0.6450423002243042
translation,313,150,results,unsupervised results,has,poolrank +,unsupervised results has poolrank +,0.6003363728523254
translation,314,101,ablation-analysis,passage initialization,with,question entity linking,passage initialization with question entity linking,0.5989832878112793
translation,314,101,ablation-analysis,passage initialization,observed,significant performance boost,passage initialization observed significant performance boost,0.6519026160240173
translation,314,101,ablation-analysis,ablation analysis,investigate,passage initialization,ablation analysis investigate passage initialization,0.6648372411727905
translation,314,108,ablation-analysis,dev set,where,entity linking and the auxiliary objective,dev set where entity linking and the auxiliary objective,0.6360026597976685
translation,314,108,ablation-analysis,entity linking and the auxiliary objective,has,slightly improve,entity linking and the auxiliary objective has slightly improve,0.5807880759239197
translation,314,108,ablation-analysis,slightly improve,has,performance,slightly improve has performance,0.5619211196899414
translation,314,108,ablation-analysis,ablation analysis,on,dev set,ablation analysis on dev set,0.6199683547019958
translation,314,35,baselines,baseline qa model,from,yang et al . ( 2018 ),baseline qa model from yang et al . ( 2018 ),0.46336859464645386
translation,314,8,model,bridge reasoner,trained with,weakly supervised signal,bridge reasoner trained with weakly supervised signal,0.6965227127075195
translation,314,8,model,bridge reasoner,produces,candidate answer passages,bridge reasoner produces candidate answer passages,0.6406528949737549
translation,314,8,model,candidate answer passages,for,passage reader,candidate answer passages for passage reader,0.6260876655578613
translation,314,8,model,passage reader,to extract,answer,passage reader to extract answer,0.6915810108184814
translation,314,54,model,shared bidirectional gru,to encode,question and the passages,shared bidirectional gru to encode question and the passages,0.7367005348205566
translation,314,54,model,shared bidirectional gru,has,),shared bidirectional gru has ),0.6246610879898071
translation,314,54,model,model,uses,shared bidirectional gru,model uses shared bidirectional gru,0.6346225142478943
translation,314,58,model,bridge reasoner,integrates,multiple types of evidence,bridge reasoner integrates multiple types of evidence,0.664350152015686
translation,314,58,model,multiple types of evidence,to predict,bridge entities,multiple types of evidence to predict bridge entities,0.7322481274604797
translation,314,58,model,bridge entities,link to,potential answer passages,bridge entities link to potential answer passages,0.599609375
translation,314,58,model,model,has,bridge reasoner,model has bridge reasoner,0.5774787664413452
translation,314,9,results,full- wiki hotpotqa benchmark,significantly improve,baseline method,full- wiki hotpotqa benchmark significantly improve baseline method,0.5897514820098877
translation,314,9,results,baseline method,by,14 point f1,baseline method by 14 point f1,0.5652961730957031
translation,314,9,results,results,On,full- wiki hotpotqa benchmark,results On full- wiki hotpotqa benchmark,0.4748017489910126
translation,314,10,results,competitive,with,state - of - the - art,competitive with state - of - the - art,0.6227762699127197
translation,314,10,results,state - of - the - art,that applies,bert,state - of - the - art that applies bert,0.6339384317398071
translation,314,10,results,bert,in,multiple modules,bert in multiple modules,0.6326802372932434
translation,314,10,results,memoryinefficient contextual embeddings,has,our result,memoryinefficient contextual embeddings has our result,0.5463928580284119
translation,314,10,results,results,Without using,memoryinefficient contextual embeddings,results Without using memoryinefficient contextual embeddings,0.6757997870445251
translation,314,36,results,result,shows that,full access,result shows that full access,0.6300244927406311
translation,314,36,results,full access,gives,marginal improvements,full access gives marginal improvements,0.6536211967468262
translation,314,36,results,results,shows that,full access,results shows that full access,0.6244450807571411
translation,314,36,results,results,has,result,results has result,0.5303357243537903
translation,314,90,results,bridge reasoner,retrieves,answer passage,bridge reasoner retrieves answer passage,0.7267408967018127
translation,314,90,results,answer passage,with,significantly higher accuracy,answer passage with significantly higher accuracy,0.6216096878051758
translation,314,90,results,significantly higher accuracy,than,hotpotqa 's ir method,significantly higher accuracy than hotpotqa 's ir method,0.5758314728736877
translation,314,90,results,results,has,bridge reasoner,results has bridge reasoner,0.570164680480957
translation,314,91,results,local context evidence,is,more effective,local context evidence is more effective,0.5644717812538147
translation,314,91,results,more effective,than,passage content evidence,more effective than passage content evidence,0.5431416630744934
translation,314,91,results,passage content evidence,for,answer passage prediction,passage content evidence for answer passage prediction,0.6070091128349304
translation,314,91,results,results,see that,local context evidence,results see that local context evidence,0.6173851490020752
translation,314,92,results,context,of,start passages,context of start passages,0.5481514930725098
translation,314,92,results,reading,has,context,reading has context,0.5735974907875061
translation,314,98,results,our approach,shows,huge advantage,our approach shows huge advantage,0.6001724004745483
translation,314,98,results,is about 10 % higher,in terms of,em and f1,is about 10 % higher in terms of em and f1,0.7780507802963257
translation,314,98,results,is about 10 % higher,compared to,current known best system w/o bert ( grn ),is about 10 % higher compared to current known best system w/o bert ( grn ),0.7070241570472717
translation,314,98,results,without bert,has,our approach,without bert has our approach,0.64949631690979
translation,314,98,results,results,Among,without bert,results Among without bert,0.6324068903923035
translation,314,100,results,models w/ bert,i.e.,cogqa,models w/ bert i.e. cogqa,0.6907650232315063
translation,314,100,results,models w/ bert,i.e.,our result,models w/ bert i.e. our result,0.6600387692451477
translation,314,100,results,our result,still,competitive,our result still competitive,0.6896607279777527
translation,314,100,results,models w/ bert,has,our result,models w/ bert has our result,0.6168705821037292
translation,314,100,results,cogqa,has,our result,cogqa has our result,0.6369257569313049
translation,314,100,results,results,compared to,models w/ bert,results compared to models w/ bert,0.7097242474555969
translation,314,105,results,our method,on par with,cogqa,our method on par with cogqa,0.7186501026153564
translation,314,105,results,35.0 v.s. 34.6,for,em,35.0 v.s. 34.6 for em,0.6316795349121094
translation,314,105,results,35.0 v.s. 34.6,for,both 46.2 for f1,35.0 v.s. 34.6 for both 46.2 for f1,0.6463406085968018
translation,314,105,results,cogqa,has,35.0 v.s. 34.6,cogqa has 35.0 v.s. 34.6,0.5656195282936096
translation,315,86,baselines,information retrieval scores,has,score of top retrieved sentence matching question plus answer,information retrieval scores has score of top retrieved sentence matching question plus answer,0.5571794509887695
translation,315,8,model,simple   knowledge graph   representation,of,question,simple   knowledge graph   representation of question,0.5471057295799255
translation,315,8,model,simple   knowledge graph   representation,leverage,several large-scale linguistic resources,simple   knowledge graph   representation leverage several large-scale linguistic resources,0.6561407446861267
translation,315,8,model,several large-scale linguistic resources,to provide,missing background knowledge,several large-scale linguistic resources to provide missing background knowledge,0.640620768070221
translation,315,8,model,model,by using,simple   knowledge graph   representation,model by using simple   knowledge graph   representation,0.626518726348877
translation,315,8,model,model,leverage,several large-scale linguistic resources,model leverage several large-scale linguistic resources,0.6224661469459534
translation,315,98,results,two most important algorithmic features,adding,concepts,two most important algorithmic features adding concepts,0.6307273507118225
translation,315,98,results,concepts,implied but not explicitly stated in,text,concepts implied but not explicitly stated in text,0.641185462474823
translation,315,98,results,implied information,that is,low relevance,implied information that is low relevance,0.5976433157920837
translation,315,98,results,implied information,of,low relevance,implied information of low relevance,0.5684226155281067
translation,315,98,results,low relevance,to,answer,low relevance to answer,0.5366857051849365
translation,315,98,results,answering,has,questions,answering has questions,0.6159771084785461
translation,315,98,results,questions,has,correctly,questions has correctly,0.6111969947814941
translation,315,98,results,results,suggest,two most important algorithmic features,results suggest two most important algorithmic features,0.589862048625946
translation,316,8,model,question classification paradigm,includes,question taxonomy,question classification paradigm includes question taxonomy,0.617350697517395
translation,316,8,model,question classification paradigm,includes,question classifier,question classification paradigm includes question classifier,0.6125914454460144
translation,316,8,model,question taxonomy,suitable to,general qa,question taxonomy suitable to general qa,0.7098918557167053
translation,316,8,model,question classifier,based on,mln ( markov logic network ),question classifier based on mln ( markov logic network ),0.6501869559288025
translation,316,8,model,question classifier,where,statistical methods,question classifier where statistical methods,0.5834837555885315
translation,316,8,model,question classifier,rule- based methods and,statistical methods,question classifier rule- based methods and statistical methods,0.701358437538147
translation,316,8,model,mln ( markov logic network ),where,statistical methods,mln ( markov logic network ) where statistical methods,0.6224570870399475
translation,316,8,model,statistical methods,unified into,single framework,statistical methods unified into single framework,0.7416126728057861
translation,316,8,model,single framework,in,fuzzy discriminative learning approach,single framework in fuzzy discriminative learning approach,0.5446634888648987
translation,316,8,model,model,propose,question classification paradigm,model propose question classification paradigm,0.691616952419281
translation,316,35,model,fuzzy discriminative weight learning,of,markov logic network,fuzzy discriminative weight learning of markov logic network,0.5153884291648865
translation,316,35,model,model,propose,fuzzy discriminative weight learning,model propose fuzzy discriminative weight learning,0.6429594159126282
translation,316,130,model,fuzzy discriminative weight learning,take,prior confidence,fuzzy discriminative weight learning take prior confidence,0.5588635802268982
translation,316,130,model,prior confidence,of,each evidence atom,prior confidence of each evidence atom,0.564298152923584
translation,316,130,model,each evidence atom,into,account,each evidence atom into account,0.6181942820549011
translation,316,130,model,model,propose,fuzzy discriminative weight learning,model propose fuzzy discriminative weight learning,0.6429594159126282
translation,317,146,ablation-analysis,approach,without,position embedding,approach without position embedding,0.7235006093978882
translation,317,146,ablation-analysis,position embedding,has,slight performance decay,position embedding has slight performance decay,0.5347505211830139
translation,317,146,ablation-analysis,ablation analysis,observe that,approach,ablation analysis observe that approach,0.6417133212089539
translation,317,147,ablation-analysis,severe performance drop,when removing,questionfocused dual attention,severe performance drop when removing questionfocused dual attention,0.6820005774497986
translation,317,147,ablation-analysis,ablation analysis,notice,severe performance drop,ablation analysis notice severe performance drop,0.7525133490562439
translation,317,134,baselines,five query - based methods,has,"bert ( devlin et al. , 2018 )","five query - based methods has bert ( devlin et al. , 2018 )",0.5460765957832336
translation,317,164,baselines,abstractive method,generates,answer summary,abstractive method generates answer summary,0.6184734106063843
translation,317,164,baselines,answer summary,from,vocabulary and the original answer,answer summary from vocabulary and the original answer,0.5454789400100708
translation,317,164,baselines,baselines,has,abstractive method,baselines has abstractive method,0.531877875328064
translation,317,128,experiments,"stanford corenlp 3 and tex-trank ( mihalcea and tarau , 2004 )",for,wiki-howqa dataset,"stanford corenlp 3 and tex-trank ( mihalcea and tarau , 2004 ) for wiki-howqa dataset",0.5833433866500854
translation,317,135,experiments,bert / xlnet,utilize,abstractive summarization schema,bert / xlnet utilize abstractive summarization schema,0.6298156976699829
translation,317,135,experiments,abstractive summarization schema,as,encoder part,abstractive summarization schema as encoder part,0.5291932821273804
translation,317,135,experiments,encoder part,replaced with,bert / xlnet encoder ( question&answer ),encoder part replaced with bert / xlnet encoder ( question&answer ),0.7497762441635132
translation,317,135,experiments,decoder,trained from,scratch,decoder trained from scratch,0.7589792013168335
translation,317,174,experiments,wikihowqa,observe,our approach,wikihowqa observe our approach,0.6059924364089966
translation,317,174,experiments,our approach,performs,better,our approach performs better,0.6449240446090698
translation,317,174,experiments,better,than,all baselines,better than all baselines,0.6078218221664429
translation,317,126,hyperparameters,hyperparameters,utilize,100 - dimension pre-trained glove embeddings,hyperparameters utilize 100 - dimension pre-trained glove embeddings,0.5319476127624512
translation,317,130,hyperparameters,dropout rate,of,0.2,dropout rate of 0.2,0.5832480192184448
translation,317,131,hyperparameters,adam optimizer,to train,parameters,adam optimizer to train parameters,0.68204265832901
translation,317,131,hyperparameters,parameters,with,initial learning rate,parameters with initial learning rate,0.5836310386657715
translation,317,131,hyperparameters,initial learning rate,of,0.0005,initial learning rate of 0.0005,0.5819736123085022
translation,317,131,hyperparameters,hyperparameters,utilize,adam optimizer,hyperparameters utilize adam optimizer,0.5550990104675293
translation,317,132,hyperparameters,our approach,with,four epochs,our approach with four epochs,0.6653261184692383
translation,317,132,hyperparameters,hyperparameters,train,our approach,hyperparameters train our approach,0.6410592198371887
translation,317,7,model,question - focused dual attention,for,chinese medical answer summarization,question - focused dual attention for chinese medical answer summarization,0.5422250032424927
translation,317,8,model,original long answer text,into,medical concept graph,original long answer text into medical concept graph,0.5404407382011414
translation,317,8,model,medical concept graph,with,graph convolution networks,medical concept graph with graph convolution networks,0.5144029259681702
translation,317,8,model,model,organize,original long answer text,model organize original long answer text,0.635358989238739
translation,317,26,model,graph convolution network,with,question - focused dual attention ( q- gcn ) model,graph convolution network with question - focused dual attention ( q- gcn ) model,0.5998514890670776
translation,317,26,model,question - focused dual attention ( q- gcn ) model,to generate,summaries,question - focused dual attention ( q- gcn ) model to generate summaries,0.7208405137062073
translation,317,26,model,model,propose,graph convolution network,model propose graph convolution network,0.6518986821174622
translation,317,27,model,motivation,is,graph - based structure,motivation is graph - based structure,0.533200204372406
translation,317,27,model,graph - based structure,better represent,correlation,graph - based structure better represent correlation,0.6659793853759766
translation,317,27,model,correlation,between,diverse concepts,correlation between diverse concepts,0.7059688568115234
translation,317,27,model,correlation,capture,plot,correlation capture plot,0.7824381589889526
translation,317,27,model,diverse concepts,in,answer,diverse concepts in answer,0.5808449983596802
translation,317,27,model,plot,of,whole text,plot of whole text,0.5956551432609558
translation,317,27,model,model,has,motivation,model has motivation,0.5275270342826843
translation,317,28,model,long answer text,into,several entities / keywords,long answer text into several entities / keywords,0.5522497296333313
translation,317,28,model,long answer text,represent,answer,long answer text represent answer,0.5618824362754822
translation,317,28,model,centered clusters,of,texts,centered clusters of texts,0.6282767653465271
translation,317,28,model,answer,with,medical concept graph,answer with medical concept graph,0.5904031991958618
translation,317,28,model,several entities / keywords,has,centered clusters,several entities / keywords has centered clusters,0.5890435576438904
translation,317,28,model,model,decompose,long answer text,model decompose long answer text,0.7392362952232361
translation,317,29,model,each vertex,of,graph,each vertex of graph,0.617770254611969
translation,317,29,model,concept clusters,regarding,entities / keywords,concept clusters regarding entities / keywords,0.6001431345939636
translation,317,29,model,model,has,each vertex,model has each vertex,0.5801244974136353
translation,317,30,model,edge,between,vertices,edge between vertices,0.6742068529129028
translation,317,30,model,edge,between,vertices,edge between vertices,0.6742068529129028
translation,317,30,model,edge,via,semantic relations,edge via semantic relations,0.6776500344276428
translation,317,30,model,semantic relations,between,vertices,semantic relations between vertices,0.6396699547767639
translation,317,30,model,model,calculate,edge,model calculate edge,0.6682816743850708
translation,317,81,model,vertex encoder,consists of,two modules,vertex encoder consists of two modules,0.6672286987304688
translation,317,81,model,two modules,namely,embedding module,two modules namely embedding module,0.7097190022468567
translation,317,81,model,two modules,namely,self-attention module,two modules namely self-attention module,0.6786317825317383
translation,317,81,model,model,has,vertex encoder,model has vertex encoder,0.5403741598129272
translation,317,82,model,regular word embedding,of,both words and concepts,regular word embedding of both words and concepts,0.5554706454277039
translation,317,82,model,regular word embedding,via,sharing embedding lookup table,regular word embedding via sharing embedding lookup table,0.6639471650123596
translation,317,82,model,sharing embedding lookup table,to represent,word information,sharing embedding lookup table to represent word information,0.6579990386962891
translation,317,82,model,model,adopt,regular word embedding,model adopt regular word embedding,0.6150808334350586
translation,317,129,model,one layer gcn,to ease,over-smoothing problem,one layer gcn to ease over-smoothing problem,0.6605808138847351
translation,317,129,model,model,utilize,one layer gcn,model utilize one layer gcn,0.5822820067405701
translation,317,167,model,information,provided by,question and critical component,information provided by question and critical component,0.7332017421722412
translation,317,167,model,question and critical component,from,medical concept graph,question and critical component from medical concept graph,0.5537967681884766
translation,317,167,model,medical concept graph,with,gcn,medical concept graph with gcn,0.5801480412483215
translation,317,204,model,approach,of,graph convolution network,approach of graph convolution network,0.5679442882537842
translation,317,204,model,graph convolution network,with,question - focused dual attention,graph convolution network with question - focused dual attention,0.597030520439148
translation,317,204,model,question - focused dual attention,to generate,chinese answer summaries,question - focused dual attention to generate chinese answer summaries,0.6415960192680359
translation,317,204,model,model,propose,approach,model propose approach,0.6953083872795105
translation,317,127,results,performance ( f1 ),of,medical ner and keyword extraction,performance ( f1 ) of medical ner and keyword extraction,0.5531646609306335
translation,317,127,results,medical ner and keyword extraction,is,0.91 and 0.89,medical ner and keyword extraction is 0.91 and 0.89,0.5473752021789551
translation,317,127,results,results,has,performance ( f1 ),results has performance ( f1 ),0.5681300163269043
translation,317,141,results,xlnet,achieves,higher rouge score,xlnet achieves higher rouge score,0.6637521982192993
translation,317,141,results,higher rouge score,than,bert,higher rouge score than bert,0.612504243850708
translation,317,141,results,results,notice,xlnet,results notice xlnet,0.7172968983650208
translation,317,142,results,pgn,has,outperforms,pgn has outperforms,0.6582108736038208
translation,317,142,results,outperforms,has,xlnet,outperforms has xlnet,0.6197744607925415
translation,317,142,results,results,has,pgn,results has pgn,0.5731626749038696
translation,317,143,results,question - enhanced approaches,has,outperform,question - enhanced approaches has outperform,0.6036889553070068
translation,317,143,results,outperform,has,all the state - of - the - art methods,outperform has all the state - of - the - art methods,0.5497617125511169
translation,317,143,results,results,observe,question - enhanced approaches,results observe question - enhanced approaches,0.5700497627258301
translation,317,144,results,answer text,into,concept graph,answer text into concept graph,0.5776560306549072
translation,317,144,results,our approach,further improves,results,our approach further improves results,0.7471879720687866
translation,317,144,results,results,by,noticeable margin,results by noticeable margin,0.6546564698219299
translation,317,144,results,answer text,has,our approach,answer text has our approach,0.6394842863082886
translation,317,144,results,results,by,noticeable margin,results by noticeable margin,0.6546564698219299
translation,317,148,results,performance decay,without,gcn,performance decay without gcn,0.6997320652008057
translation,317,156,results,human evaluation,shows,our approach,human evaluation shows our approach,0.6080134510993958
translation,317,156,results,other methods,in,all aspects,other methods in all aspects,0.4966100752353668
translation,317,156,results,our approach,has,consistently outperforms,our approach has consistently outperforms,0.6119859218597412
translation,317,156,results,consistently outperforms,has,other methods,consistently outperforms has other methods,0.5590806603431702
translation,317,157,results,bert and xlnet,achieve,relatively low scores,bert and xlnet achieve relatively low scores,0.6358816027641296
translation,317,157,results,relatively low scores,in,informativity and conciseness,relatively low scores in informativity and conciseness,0.5049301385879517
translation,317,157,results,results,has,bert and xlnet,results has bert and xlnet,0.590635359287262
translation,317,158,results,bert and xlnet,generate,more fluent summaries,bert and xlnet generate more fluent summaries,0.6700007319450378
translation,317,158,results,more fluent summaries,with,higher readability scores,more fluent summaries with higher readability scores,0.5844995379447937
translation,317,158,results,more fluent summaries,take advantage of,pre-trained language model,more fluent summaries take advantage of pre-trained language model,0.6208134889602661
translation,317,158,results,results,has,bert and xlnet,results has bert and xlnet,0.590635359287262
translation,317,168,results,our model,learns,well,our model learns well,0.7183654308319092
translation,317,168,results,well,to generate,answer summaries,well to generate answer summaries,0.6944625973701477
translation,317,168,results,answer summaries,highly related to,given questions,answer summaries highly related to given questions,0.687315821647644
translation,317,168,results,results,has,our model,results has our model,0.5871725678443909
translation,317,180,results,our approach,is,more efficient,our approach is more efficient,0.5499119758605957
translation,317,180,results,more efficient,especially for,long answers,more efficient especially for long answers,0.6468648910522461
translation,317,180,results,results,observe,our approach,results observe our approach,0.6077187061309814
translation,317,181,results,answers,shorter than,100 words,answers shorter than 100 words,0.7525535821914673
translation,317,181,results,seq2seq and neusum,are,marginally better,seq2seq and neusum are marginally better,0.5760636925697327
translation,317,181,results,marginally better,than,our approach,marginally better than our approach,0.5296868681907654
translation,317,181,results,answers,has,seq2seq and neusum,answers has seq2seq and neusum,0.6403719782829285
translation,317,181,results,results,For,answers,results For answers,0.5908249616622925
translation,317,189,results,our approach,without,question - focused dual attention,our approach without question - focused dual attention,0.7176170945167542
translation,317,189,results,trivial summaries,include,redundant information,trivial summaries include redundant information,0.6452749967575073
translation,317,189,results,trivial summaries,miss,key points,trivial summaries miss key points,0.672048032283783
translation,317,189,results,key points,relevant to,question,key points relevant to question,0.734272301197052
translation,317,189,results,results,observe,our approach,results observe our approach,0.6077187061309814
translation,318,202,ablation-analysis,ablation,on,all evaluated components,ablation on all evaluated components,0.5757443308830261
translation,318,202,ablation-analysis,all evaluated components,led to,significant performance drop,all evaluated components led to significant performance drop,0.7052807211875916
translation,318,202,ablation-analysis,ablation analysis,on,all evaluated components,ablation analysis on all evaluated components,0.520658552646637
translation,318,202,ablation-analysis,ablation analysis,has,ablation,ablation analysis has ablation,0.511965274810791
translation,318,203,ablation-analysis,reasoning chain,crucial for,multi-hop qg,reasoning chain crucial for multi-hop qg,0.6980764865875244
translation,318,203,ablation-analysis,multi-hop qg,guidance of,logical correlations,multi-hop qg guidance of logical correlations,0.7393669486045837
translation,318,203,ablation-analysis,ablation analysis,infer,reasoning chain,ablation analysis infer reasoning chain,0.694010317325592
translation,318,214,ablation-analysis,even a small amount of unlabeled data,play,decisive role,even a small amount of unlabeled data play decisive role,0.700084924697876
translation,318,214,ablation-analysis,decisive role,in,improving,decisive role in improving,0.5715084075927734
translation,318,214,ablation-analysis,performance,in terms of,three metrics,performance in terms of three metrics,0.7123782634735107
translation,318,214,ablation-analysis,improving,has,performance,improving has performance,0.5706568360328674
translation,318,193,baselines,basic model,with,copy mechanism,basic model with copy mechanism,0.6163710951805115
translation,318,193,baselines,copy mechanism,i.e.,nqg ++,copy mechanism i.e. nqg ++,0.6885719299316406
translation,318,193,baselines,"ass2s ( kim et al. , 2019 )",encoded,answer,"ass2s ( kim et al. , 2019 ) encoded answer",0.6747162342071533
translation,318,193,baselines,answer,to form,answerfocused questions,answer to form answerfocused questions,0.6630440950393677
translation,318,193,baselines,"corefnqg ( du and cardie , 2018 )",incorporated,linguistic features,"corefnqg ( du and cardie , 2018 ) incorporated linguistic features",0.6291467547416687
translation,318,193,baselines,linguistic features,to represent,inputs better,linguistic features to represent inputs better,0.6851710081100464
translation,318,193,baselines,maxpointer,using,gated self-attention,maxpointer using gated self-attention,0.6959497332572937
translation,318,193,baselines,"zhao et al. , 2018 )",using,gated self-attention,"zhao et al. , 2018 ) using gated self-attention",0.6296422481536865
translation,318,193,baselines,gated self-attention,to form,questions,gated self-attention to form questions,0.6491492390632629
translation,318,193,baselines,questions,for,long text inputs,questions for long text inputs,0.6175504326820374
translation,318,193,baselines,mpqg +r,captured,broader context,mpqg +r captured broader context,0.7807859182357788
translation,318,193,baselines,mpqg +r,captured,text,mpqg +r captured text,0.7622731328010559
translation,318,193,baselines,maxpointer,has,"zhao et al. , 2018 )","maxpointer has zhao et al. , 2018 )",0.534349262714386
translation,318,193,baselines,mpqg +r,has,"song et al. , 2018 )","mpqg +r has song et al. , 2018 )",0.581814169883728
translation,318,194,baselines,effect of unlabeled data,examined,two variants,effect of unlabeled data examined two variants,0.6528515219688416
translation,318,186,experiments,pattern learning,parsed,every question,pattern learning parsed every question,0.7603176236152649
translation,318,186,experiments,every question,by,stanford corenlp toolkit,every question by stanford corenlp toolkit,0.5476223826408386
translation,318,180,hyperparameters,768 - dimension pre-trained vectors,from,uncased bert,768 - dimension pre-trained vectors from uncased bert,0.5857847929000854
translation,318,180,hyperparameters,hyperparameters,leveraged,768 - dimension pre-trained vectors,hyperparameters leveraged 768 - dimension pre-trained vectors,0.6429260969161987
translation,318,181,hyperparameters,number of states k and emissions l,in,semi-markov model,number of states k and emissions l in semi-markov model,0.5453142523765564
translation,318,181,hyperparameters,number of states k and emissions l,set to,"50 , 4","number of states k and emissions l set to 50 , 4",0.7060389518737793
translation,318,181,hyperparameters,semi-markov model,set to,"50 , 4","semi-markov model set to 50 , 4",0.7023229002952576
translation,318,181,hyperparameters,hyperparameters,has,number of states k and emissions l,hyperparameters has number of states k and emissions l,0.5223017334938049
translation,318,182,hyperparameters,size,of,hidden units,size of hidden units,0.6125550866127014
translation,318,182,hyperparameters,hidden units,in,both encoder and decoder,hidden units in both encoder and decoder,0.4960777759552002
translation,318,182,hyperparameters,hidden units,was,300,hidden units was 300,0.6254392266273499
translation,318,182,hyperparameters,hyperparameters,has,size,hyperparameters has size,0.5313372611999512
translation,318,183,hyperparameters,recurrent weights,initialized by,uniform distribution,recurrent weights initialized by uniform distribution,0.7480449676513672
translation,318,183,hyperparameters,uniform distribution,between,?0.01 and 0.01,uniform distribution between ?0.01 and 0.01,0.6518419981002808
translation,318,183,hyperparameters,uniform distribution,updated with,stochastic gradient descent,uniform distribution updated with stochastic gradient descent,0.6597465872764587
translation,318,183,hyperparameters,hyperparameters,has,recurrent weights,hyperparameters has recurrent weights,0.5275602340698242
translation,318,184,hyperparameters,"adam ( kingma and ba , 2015 )",as,optimizer,"adam ( kingma and ba , 2015 ) as optimizer",0.5069490075111389
translation,318,184,hyperparameters,optimizer,with,learning rate,optimizer with learning rate,0.6228271722793579
translation,318,184,hyperparameters,learning rate,of,10 ?3,learning rate of 10 ?3,0.6398048996925354
translation,318,184,hyperparameters,hyperparameters,used,"adam ( kingma and ba , 2015 )","hyperparameters used adam ( kingma and ba , 2015 )",0.5725852847099304
translation,318,185,hyperparameters,trade- off parameter,set to,0.4,trade- off parameter set to 0.4,0.6962618231773376
translation,318,185,hyperparameters,hyperparameters,has,trade- off parameter,hyperparameters has trade- off parameter,0.5158633589744568
translation,318,7,model,model,with the help of,large scale of unlabeled data,model with the help of large scale of unlabeled data,0.6128499507904053
translation,318,7,model,large scale of unlabeled data,that is,much easier to obtain,large scale of unlabeled data that is much easier to obtain,0.6001656651496887
translation,318,7,model,model,learn,model,model learn model,0.6338090896606445
translation,318,10,model,latent patterns,as,prior,latent patterns as prior,0.55534428358078
translation,318,10,model,latent patterns,regularize,generation model,latent patterns regularize generation model,0.7154363393783569
translation,318,10,model,latent patterns,produce,optimal results,latent patterns produce optimal results,0.6446194648742676
translation,318,10,model,model,With,latent patterns,model With latent patterns,0.6439430713653564
translation,318,39,model,practical two -stage approach,to learn,multihop qg model,practical two -stage approach to learn multihop qg model,0.532739520072937
translation,318,39,model,multihop qg model,from both,small -scale labeled data,multihop qg model from both small -scale labeled data,0.577197790145874
translation,318,39,model,multihop qg model,from both,a large-size unlabeled corpus,multihop qg model from both a large-size unlabeled corpus,0.5493717789649963
translation,318,39,model,model,propose,practical two -stage approach,model propose practical two -stage approach,0.6468926668167114
translation,318,40,model,neural hidden semi-markov model,to parameterize,sophisticated structural patterns,neural hidden semi-markov model to parameterize sophisticated structural patterns,0.7174996733665466
translation,318,40,model,sophisticated structural patterns,on,questions,sophisticated structural patterns on questions,0.595427930355072
translation,318,40,model,sophisticated structural patterns,by,latent variables,sophisticated structural patterns by latent variables,0.5602131485939026
translation,318,40,model,questions,by,latent variables,questions by latent variables,0.5850162506103516
translation,318,40,model,model,exploit,neural hidden semi-markov model,model exploit neural hidden semi-markov model,0.6581546068191528
translation,318,41,model,variables,estimated by,maximizing,variables estimated by maximizing,0.726542592048645
translation,318,41,model,domain knowledge,has,variables,domain knowledge has variables,0.5554075241088867
translation,318,41,model,maximizing,has,likelihood of the unlabeled data,maximizing has likelihood of the unlabeled data,0.5040110349655151
translation,318,41,model,model,Without,domain knowledge,model Without domain knowledge,0.6783398389816284
translation,318,48,model,usage of prior patterns,optimize,model,usage of prior patterns optimize model,0.75343918800354
translation,318,48,model,model,by,reinforcement learning,model by reinforcement learning,0.5473924875259399
translation,318,48,model,reinforcement learning,with,augmented evaluated loss,reinforcement learning with augmented evaluated loss,0.6355536580085754
translation,318,48,model,model,to better balance,supervision,model to better balance supervision,0.7399032711982727
translation,318,48,model,model,optimize,model,model optimize model,0.72225421667099
translation,318,48,model,model,by,reinforcement learning,model by reinforcement learning,0.5473924875259399
translation,318,104,model,guidance,of,reasoning chain,guidance of reasoning chain,0.5417966842651367
translation,318,104,model,guidance,of,prior patterns,guidance of prior patterns,0.6274791359901428
translation,318,104,model,guidance,build,multi-hop qg model,guidance build multi-hop qg model,0.6074187755584717
translation,318,104,model,prior patterns,build,multi-hop qg model,prior patterns build multi-hop qg model,0.7009169459342957
translation,318,104,model,multi-hop qg model,on,extracted contents,multi-hop qg model on extracted contents,0.5180869102478027
translation,318,104,model,extracted contents,by,sequenceto-sequence framework,extracted contents by sequenceto-sequence framework,0.5831692218780518
translation,318,104,model,model,Under,guidance,model Under guidance,0.6837241649627686
translation,318,104,model,model,build,multi-hop qg model,model build multi-hop qg model,0.6963874101638794
translation,318,197,results,our approach,achieved,best performance,our approach achieved best performance,0.7143116593360901
translation,318,198,results,"best baseline ( i.e. , maxpointer )",by over,"11.6 % , 10.5 % , 11.4 %","best baseline ( i.e. , maxpointer ) by over 11.6 % , 10.5 % , 11.4 %",0.6214534044265747
translation,318,198,results,"11.6 % , 10.5 % , 11.4 %",in terms of,"bleu -4 , meteor , and rouge -l","11.6 % , 10.5 % , 11.4 % in terms of bleu -4 , meteor , and rouge -l",0.615483283996582
translation,318,198,results,significantly outperformed,has,"best baseline ( i.e. , maxpointer )","significantly outperformed has best baseline ( i.e. , maxpointer )",0.5657801628112793
translation,318,199,results,ours - pattn,found that,performance,ours - pattn found that performance,0.6556720733642578
translation,318,199,results,improves,with,more unlabeled data,improves with more unlabeled data,0.6392200589179993
translation,318,199,results,performance,has,improves,performance has improves,0.6178221106529236
translation,318,207,results,all baselines,in terms of,three metrics,all baselines in terms of three metrics,0.6067853569984436
translation,318,207,results,improvement,on,rationality metric,improvement on rationality metric,0.49427148699760437
translation,318,207,results,improvement,was,largest,improvement was largest,0.6693429350852966
translation,318,207,results,rationality metric,was,largest,rationality metric was largest,0.5960779786109924
translation,318,207,results,our model,has,significantly outperformed,our model has significantly outperformed,0.5656611323356628
translation,318,207,results,significantly outperformed,has,all baselines,significantly outperformed has all baselines,0.6105693578720093
translation,318,215,results,higher,when,scale of the labeled data,higher when scale of the labeled data,0.6521691083908081
translation,318,215,results,scale of the labeled data,was,small,scale of the labeled data was small,0.6064953207969666
translation,318,215,results,results,has,ratio of improvement,results has ratio of improvement,0.5871521830558777
translation,318,216,results,unlabeled data,on learning,qg model,unlabeled data on learning qg model,0.643611490726471
translation,318,216,results,qg model,with,low labeled resource,qg model with low labeled resource,0.5939247012138367
translation,318,216,results,results,usefulness of,unlabeled data,results usefulness of unlabeled data,0.644375741481781
translation,318,228,results,qa model,trained on,30 % labeled data,qa model trained on 30 % labeled data,0.7297781109809875
translation,318,228,results,30 % labeled data,obtained,competitive performance,30 % labeled data obtained competitive performance,0.5878483057022095
translation,318,228,results,competitive performance,against,qa model,competitive performance against qa model,0.7197918891906738
translation,318,228,results,qa model,learned on,100 % labeled data,qa model learned on 100 % labeled data,0.6994081139564514
translation,318,228,results,results,has,qa model,results has qa model,0.5610512495040894
translation,319,150,baselines,triplet loss,has,with random sampling,triplet loss has with random sampling,0.5525475740432739
translation,319,130,experiments,vocabulary,for,quora dataset,vocabulary for quora dataset,0.5704410076141357
translation,319,130,experiments,vocabulary,for,bigger open-domain qa dataset,vocabulary for bigger open-domain qa dataset,0.5734294652938843
translation,319,130,experiments,vocabulary,fixed at,"50,000","vocabulary fixed at 50,000",0.6692466139793396
translation,319,130,experiments,vocabulary,of,size,vocabulary of size,0.6203036308288574
translation,319,130,experiments,bigger open-domain qa dataset,used,vocabulary,bigger open-domain qa dataset used vocabulary,0.6260503530502319
translation,319,130,experiments,vocabulary,of,size,vocabulary of size,0.6203036308288574
translation,319,130,experiments,size,has,"100,000","size has 100,000",0.5907235145568848
translation,319,131,hyperparameters,rare words,use,hashing trick,rare words use hashing trick,0.6195150017738342
translation,319,131,hyperparameters,hashing trick,with,"5,000 bins","hashing trick with 5,000 bins",0.6569614410400391
translation,319,131,hyperparameters,hashing trick,with,"10,000 bins","hashing trick with 10,000 bins",0.6601616144180298
translation,319,131,hyperparameters,"5,000 bins",for,quora dataset,"5,000 bins for quora dataset",0.5742326974868774
translation,319,131,hyperparameters,"5,000 bins",for,qa dataset,"5,000 bins for qa dataset",0.6058766841888428
translation,319,131,hyperparameters,"5,000 bins",for,qa dataset,"5,000 bins for qa dataset",0.6058766841888428
translation,319,131,hyperparameters,"10,000 bins",for,qa dataset,"10,000 bins for qa dataset",0.6016910076141357
translation,319,131,hyperparameters,hyperparameters,To map,rare words,hyperparameters To map rare words,0.6472620964050293
translation,319,132,hyperparameters,dimensionality,of,word embeddings,dimensionality of word embeddings,0.5177170038223267
translation,319,132,hyperparameters,word embeddings,at,"300 ( i.e. , e dim = 300 )","word embeddings at 300 ( i.e. , e dim = 300 )",0.5083125829696655
translation,319,132,hyperparameters,convolutional layer,uses,window size,convolutional layer uses window size,0.5936943888664246
translation,319,132,hyperparameters,window size,of,"5 ( i.e. , win = 5 )","window size of 5 ( i.e. , win = 5 )",0.5974473357200623
translation,319,132,hyperparameters,encoder,outputs,vector,encoder outputs vector,0.7555533051490784
translation,319,132,hyperparameters,size,has,n = 300,size has n = 300,0.587363600730896
translation,319,132,hyperparameters,hyperparameters,set,dimensionality,hyperparameters set dimensionality,0.6572375893592834
translation,319,132,hyperparameters,hyperparameters,set,convolutional layer,hyperparameters set convolutional layer,0.6075060963630676
translation,319,133,hyperparameters,network,trained with,margin ? = 0.5,network trained with margin ? = 0.5,0.7384340167045593
translation,319,133,hyperparameters,triplet loss,has,network,triplet loss has network,0.562318742275238
translation,319,133,hyperparameters,hyperparameters,For,triplet loss,hyperparameters For triplet loss,0.523138165473938
translation,319,134,hyperparameters,default batch size,for,all the experiments,default batch size for all the experiments,0.5691177248954773
translation,319,134,hyperparameters,default batch size,is,"512 ( i.e. , n = 512 )","default batch size is 512 ( i.e. , n = 512 )",0.5294196605682373
translation,319,134,hyperparameters,all the experiments,is,"512 ( i.e. , n = 512 )","all the experiments is 512 ( i.e. , n = 512 )",0.5734108090400696
translation,319,134,hyperparameters,smoothing factor,for,sdml,smoothing factor for sdml,0.5854479074478149
translation,319,134,hyperparameters,smoothing factor,is,0.3,smoothing factor is 0.3,0.5642966628074646
translation,319,134,hyperparameters,hyperparameters,has,default batch size,hyperparameters has default batch size,0.49267709255218506
translation,319,135,hyperparameters,training,performed using,adam optimizer,training performed using adam optimizer,0.6298501491546631
translation,319,135,hyperparameters,adam optimizer,with,learning rate ? = 0.001,adam optimizer with learning rate ? = 0.001,0.6134544014930725
translation,319,135,hyperparameters,learning rate ? = 0.001,until,model,learning rate ? = 0.001 until model,0.6871553063392639
translation,319,135,hyperparameters,model,stops,improving,model stops improving,0.7292760610580444
translation,319,135,hyperparameters,model,using,early stopping,model using early stopping,0.6742904782295227
translation,319,135,hyperparameters,improving,on,validation test,improving on validation test,0.6026756763458252
translation,319,135,hyperparameters,improving,using,early stopping,improving using early stopping,0.6601768732070923
translation,319,135,hyperparameters,early stopping,on,roc auc metric,early stopping on roc auc metric,0.5312708020210266
translation,319,7,model,qpr system,implemented as,neural information retrieval ( nir ) system,qpr system implemented as neural information retrieval ( nir ) system,0.6482590436935425
translation,319,7,model,neural information retrieval ( nir ) system,consisting of,neural network sentence encoder,neural information retrieval ( nir ) system consisting of neural network sentence encoder,0.7027783989906311
translation,319,7,model,neural information retrieval ( nir ) system,consisting of,approximate k-nearest neighbour index,neural information retrieval ( nir ) system consisting of approximate k-nearest neighbour index,0.7198540568351746
translation,319,7,model,approximate k-nearest neighbour index,for,efficient vector retrieval,approximate k-nearest neighbour index for efficient vector retrieval,0.5727607011795044
translation,319,7,model,model,describe,qpr system,model describe qpr system,0.6820995807647705
translation,319,12,model,industrial scale,when trained on,noisy training data,industrial scale when trained on noisy training data,0.6784255504608154
translation,319,12,model,model,propose,question paraphrase retrieval ( qpr ),model propose question paraphrase retrieval ( qpr ),0.6585301160812378
translation,319,49,model,loss function,minimizes,effect of false negatives,loss function minimizes effect of false negatives,0.6993657946586609
translation,319,49,model,effect of false negatives,in,training data,effect of false negatives in training data,0.5328381657600403
translation,319,49,model,model,propose,loss function,model propose loss function,0.6510262489318848
translation,319,50,model,proposed loss function,trains,model,proposed loss function trains model,0.643949031829834
translation,319,50,model,proposed loss function,uses,label smoothing,proposed loss function uses label smoothing,0.5565120577812195
translation,319,50,model,model,to identify,valid paraphrase,model to identify valid paraphrase,0.691971480846405
translation,319,50,model,valid paraphrase,in,set of randomly sampled questions,valid paraphrase in set of randomly sampled questions,0.5021363496780396
translation,319,50,model,label smoothing,to assign,some probability mass,label smoothing to assign some probability mass,0.706926167011261
translation,319,50,model,some probability mass,to,negative examples,some probability mass to negative examples,0.5424723029136658
translation,319,50,model,some probability mass,mitigating,impact,some probability mass mitigating impact,0.6945065855979919
translation,319,50,model,model,has,proposed loss function,model has proposed loss function,0.5459903478622437
translation,319,153,results,our approach,improves over,original triplet loss,our approach improves over original triplet loss,0.7161921262741089
translation,319,153,results,original triplet loss,on,both datasets,original triplet loss on both datasets,0.46312442421913147
translation,319,153,results,original triplet loss,has,considerably,original triplet loss has considerably,0.5770847797393799
translation,319,153,results,results,has,our approach,results has our approach,0.6050099730491638
translation,319,154,results,outperforms,of,loss,outperforms of loss,0.6407197117805481
translation,319,154,results,euc implementation,of,loss,euc implementation of loss,0.5564455389976501
translation,319,154,results,ssd distance,has,outperforms,ssd distance has outperforms,0.6261681914329529
translation,319,154,results,outperforms,has,euc implementation,outperforms has euc implementation,0.5969560742378235
translation,319,154,results,results,has,ssd distance,results has ssd distance,0.5536950826644897
translation,319,155,results,results,on,open domain qa dataset validation and test set,results on open domain qa dataset validation and test set,0.5452844500541687
translation,320,55,hyperparameters,feature - rich encoder,consists of,2 layers bil -stm,feature - rich encoder consists of 2 layers bil -stm,0.5990632772445679
translation,320,55,hyperparameters,hidden size,has,encoders,hidden size has encoders,0.6191847324371338
translation,320,55,hyperparameters,hyperparameters,has,feature - rich encoder,hyperparameters has feature - rich encoder,0.5083388686180115
translation,320,56,hyperparameters,cutoff length,of,input sequences,cutoff length of input sequences,0.5713894367218018
translation,320,56,hyperparameters,cutoff length,set to,10,cutoff length set to 10,0.7891322374343872
translation,320,56,hyperparameters,input sequences,set to,10,input sequences set to 10,0.7302322387695312
translation,320,56,hyperparameters,hyperparameters,has,cutoff length,hyperparameters has cutoff length,0.4999757707118988
translation,320,57,hyperparameters,beam search,with,beam size,beam search with beam size,0.6320963501930237
translation,320,57,hyperparameters,beam size,of,12,beam size of 12,0.6878777742385864
translation,320,64,results,performance,gets,3.41 and 3.32 points increment,performance gets 3.41 and 3.32 points increment,0.6110527515411377
translation,320,64,results,3.41 and 3.32 points increment,on,squad and marco,3.41 and 3.32 points increment on squad and marco,0.560981035232544
translation,320,64,results,squad and marco,is,large margin,squad and marco is large margin,0.59670490026474
translation,320,64,results,baseline model,has,performance,baseline model has performance,0.5428252816200256
translation,320,64,results,results,comparing with,baseline model,results comparing with baseline model,0.6776164174079895
translation,321,108,baselines,baselines,for,tqa dataset,baselines for tqa dataset,0.6029233932495117
translation,321,111,baselines,baseline models,are,random model,baseline models are random model,0.5814805030822754
translation,321,111,baselines,baseline models,are,text-only model,baseline models are text-only model,0.5131482481956482
translation,321,111,baselines,baseline models,are,bidaf model,baseline models are bidaf model,0.5608519315719604
translation,321,111,baselines,baseline models,are,),baseline models are ),0.6297158002853394
translation,321,111,baselines,baselines,has,baseline models,baselines has baseline models,0.5690722465515137
translation,321,112,baselines,text - only model,variant of,memory network,text - only model variant of memory network,0.6954389214515686
translation,321,112,baselines,memory network,where,"paragraph , question and options","memory network where paragraph , question and options",0.6216574311256409
translation,321,112,baselines,embedded separately,using,lstm,embedded separately using lstm,0.7123518586158752
translation,321,112,baselines,lstm,followed by,attention mechanism,lstm followed by attention mechanism,0.6697189807891846
translation,321,112,baselines,baselines,has,text - only model,baselines has text - only model,0.5212336182594299
translation,321,43,hyperparameters,"word embeddings ( mikolov et al. , 2013 )",to encode,words,"word embeddings ( mikolov et al. , 2013 ) to encode words",0.6826170086860657
translation,321,43,hyperparameters,words,present in,question,words present in question,0.6771911978721619
translation,321,43,hyperparameters,words,present in,option and the most relevant paragraph,words present in option and the most relevant paragraph,0.6633288264274597
translation,321,43,hyperparameters,hyperparameters,use,"word embeddings ( mikolov et al. , 2013 )","hyperparameters use word embeddings ( mikolov et al. , 2013 )",0.5259324312210083
translation,321,6,model,our model,assigns,score,our model assigns score,0.7278351187705994
translation,321,6,model,our model,chooses,final option,our model chooses final option,0.7529316544532776
translation,321,6,model,score,to,each question-option tuple,score to each question-option tuple,0.5980975031852722
translation,321,6,model,article,has,our model,article has our model,0.5934900641441345
translation,321,6,model,multiple choice question,has,our model,multiple choice question has our model,0.5645614862442017
translation,321,6,model,model,Given,article,model Given article,0.7312644720077515
translation,321,22,model,cnn based model,for,multiple choice question answering,cnn based model for multiple choice question answering,0.5362058281898499
translation,321,22,model,model,build,cnn based model,model build cnn based model,0.7190799713134766
translation,321,110,model,baseline models,rely on,word-level attention,baseline models rely on word-level attention,0.6650014519691467
translation,321,110,model,baseline models,rely on,encoding question and options,baseline models rely on encoding question and options,0.7060101628303528
translation,321,110,model,model,has,baseline models,model has baseline models,0.549335777759552
translation,321,113,model,character and word level embedding,to encode,question and the text,character and word level embedding to encode question and the text,0.7327154278755188
translation,321,113,model,question and the text,followed by,bidirectional attention mechanism,question and the text followed by bidirectional attention mechanism,0.6720924973487854
translation,321,113,model,bidaf model,has,character and word level embedding,bidaf model has character and word level embedding,0.5242364406585693
translation,321,113,model,model,In,bidaf model,model In bidaf model,0.5870464444160461
translation,321,128,model,cnn based model,for,multiple choice question answering,cnn based model for multiple choice question answering,0.5362058281898499
translation,321,128,model,model,proposed,cnn based model,model proposed cnn based model,0.7419626712799072
translation,321,8,results,outperforms,on,two datasets,outperforms on two datasets,0.5213328003883362
translation,321,8,results,several lstm - based baseline models,on,two datasets,several lstm - based baseline models on two datasets,0.4908781349658966
translation,321,8,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,321,8,results,outperforms,has,several lstm - based baseline models,outperforms has several lstm - based baseline models,0.5463469624519348
translation,321,8,results,results,has,our model,results has our model,0.5871725678443909
translation,321,103,results,highly overfits,on,sciq dataset,highly overfits on sciq dataset,0.5672950744628906
translation,321,103,results,highly overfits,shows that,cnn - based models,highly overfits shows that cnn - based models,0.6831421256065369
translation,321,103,results,gru bl,has,highly overfits,gru bl has highly overfits,0.6333327889442444
translation,321,103,results,cnn - based models,has,work better,cnn - based models has work better,0.5641539692878723
translation,321,103,results,results,Note,gru bl,results Note gru bl,0.6134741306304932
translation,321,119,results,"cn n 2,3,4 model",shows,significant improvement,"cn n 2,3,4 model shows significant improvement",0.686870813369751
translation,321,119,results,significant improvement,over,baseline models,significant improvement over baseline models,0.6524149775505066
translation,322,120,ablation-analysis,bilstm,with,crf,bilstm with crf,0.748549222946167
translation,322,120,ablation-analysis,crf,for,entity detection / linking,crf for entity detection / linking,0.5917001366615295
translation,322,120,ablation-analysis,crf,yields,73.7,crf yields 73.7,0.6611659526824951
translation,322,120,ablation-analysis,entity detection / linking,yields,73.7,entity detection / linking yields 73.7,0.6477835178375244
translation,322,120,ablation-analysis,73.7,is,1.2 absolute decrease,73.7 is 1.2 absolute decrease,0.5519585013389587
translation,322,120,ablation-analysis,1.2 absolute decrease,in,end-to - end accuracy,1.2 absolute decrease in end-to - end accuracy,0.501524806022644
translation,322,120,ablation-analysis,ablation analysis,Replacing,bilstm,ablation analysis Replacing bilstm,0.7096441984176636
translation,322,89,experimental-setup,models,implemented in,pytorch v0.2.0,models implemented in pytorch v0.2.0,0.6974404454231262
translation,322,89,experimental-setup,pytorch v0.2.0,with,cuda 8.0,pytorch v0.2.0 with cuda 8.0,0.5995586514472961
translation,322,89,experimental-setup,cuda 8.0,running on,nvidia geforce gtx 1080 gpu,cuda 8.0 running on nvidia geforce gtx 1080 gpu,0.7246723175048828
translation,322,89,experimental-setup,experimental setup,implemented in,pytorch v0.2.0,experimental setup implemented in pytorch v0.2.0,0.6707190275192261
translation,322,89,experimental-setup,experimental setup,has,models,experimental setup has models,0.5060054659843445
translation,322,90,experimental-setup,"glove embeddings ( pennington et al. , 2014 )",of size,300,"glove embeddings ( pennington et al. , 2014 ) of size 300",0.6918931007385254
translation,322,90,experimental-setup,"glove embeddings ( pennington et al. , 2014 )",served as,input,"glove embeddings ( pennington et al. , 2014 ) served as input",0.550099790096283
translation,322,90,experimental-setup,input,to,our models,input to our models,0.5870118737220764
translation,322,90,experimental-setup,experimental setup,has,"glove embeddings ( pennington et al. , 2014 )","experimental setup has glove embeddings ( pennington et al. , 2014 )",0.5136281251907349
translation,322,91,experimental-setup,negative log likelihood loss,to optimize,model parameters,negative log likelihood loss to optimize model parameters,0.6959913372993469
translation,322,91,experimental-setup,model parameters,using,adam,model parameters using adam,0.7006658315658569
translation,322,91,experimental-setup,adam,with,initial learning rate,adam with initial learning rate,0.6089812517166138
translation,322,91,experimental-setup,initial learning rate,of,0.0001,initial learning rate of 0.0001,0.5696305632591248
translation,322,91,experimental-setup,experimental setup,used,negative log likelihood loss,experimental setup used negative log likelihood loss,0.5877711176872253
translation,322,93,experimental-setup,mlp hidden sizes,set to,300,mlp hidden sizes set to 300,0.7230181097984314
translation,322,94,experimental-setup,cnns,used,size 300 output channel,cnns used size 300 output channel,0.5721078515052795
translation,322,94,experimental-setup,experimental setup,For,cnns,experimental setup For cnns,0.5687901377677917
translation,322,95,experimental-setup,dropout rate,for,cnns,dropout rate for cnns,0.5703630447387695
translation,322,95,experimental-setup,cnns,was,0.5 and 0.3,cnns was 0.5 and 0.3,0.6199078559875488
translation,322,95,experimental-setup,0.5 and 0.3,for,rnns,0.5 and 0.3 for rnns,0.6623519659042358
translation,322,95,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,322,96,experimental-setup,crf implementation,used,stanford ner tagger,crf implementation used stanford ner tagger,0.519841194152832
translation,322,96,experimental-setup,experimental setup,For,crf implementation,experimental setup For crf implementation,0.5331495404243469
translation,322,97,experimental-setup,lr,used,scikit-learn package,lr used scikit-learn package,0.594636857509613
translation,322,97,experimental-setup,scikit-learn package,in,python,scikit-learn package in python,0.47841590642929077
translation,322,97,experimental-setup,experimental setup,For,lr,experimental setup For lr,0.6013345718383789
translation,322,6,results,simplequestions dataset,find that,basic lstms and grus plus a few heuristics,simplequestions dataset find that basic lstms and grus plus a few heuristics,0.645808219909668
translation,322,6,results,basic lstms and grus plus a few heuristics,yield,accuracies,basic lstms and grus plus a few heuristics yield accuracies,0.6233764886856079
translation,322,6,results,accuracies,that approach,state of the art,accuracies that approach state of the art,0.609221339225769
translation,322,16,results,simplequestions dataset,find that,baseline nn architectures plus simple heuristics,simplequestions dataset find that baseline nn architectures plus simple heuristics,0.5907797813415527
translation,322,16,results,baseline nn architectures plus simple heuristics,yield,accuracies,baseline nn architectures plus simple heuristics yield accuracies,0.6299599409103394
translation,322,16,results,accuracies,that approach,state of the art,accuracies that approach state of the art,0.609221339225769
translation,322,16,results,results,On,simplequestions dataset,results On simplequestions dataset,0.5304885506629944
translation,322,17,results,combination of simple techniques,that do not involve,neural networks,combination of simple techniques that do not involve neural networks,0.6319968700408936
translation,322,17,results,combination of simple techniques,achieve,reasonable accuracy,combination of simple techniques achieve reasonable accuracy,0.5895301699638367
translation,322,17,results,results,show that,combination of simple techniques,results show that combination of simple techniques,0.4964515268802643
translation,322,104,results,entity detection,on,validation set,entity detection on validation set,0.5402504205703735
translation,322,104,results,bilstm,outperforms,bigru,bilstm outperforms bigru,0.7332698106765747
translation,322,104,results,bilstm,achieves,93,bilstm achieves 93,0.6991330981254578
translation,322,104,results,entity detection,has,bilstm,entity detection has bilstm,0.55336594581604
translation,322,104,results,validation set,has,bilstm,validation set has bilstm,0.6145210266113281
translation,322,104,results,results,For,entity detection,results For entity detection,0.5852258801460266
translation,322,107,results,entity linking,using,crf,entity linking using crf,0.6738879680633545
translation,322,107,results,entity linking,achieves,comparable accuracy,entity linking achieves comparable accuracy,0.6416702270507812
translation,322,107,results,results,see that,entity linking,results see that entity linking,0.6177487373352051
translation,322,115,results,50 candidate entities,with,five candidate relations,50 candidate entities with five candidate relations,0.6338503360748291
translation,322,115,results,results,crossing,50 candidate entities,results crossing 50 candidate entities,0.7134720683097839
translation,322,117,results,best model combination,is,bilstm,best model combination is bilstm,0.5361819267272949
translation,322,117,results,best model combination,is,bigru,best model combination is bigru,0.5776917338371277
translation,322,117,results,best model combination,is,competitive,best model combination is competitive,0.5747970342636108
translation,322,117,results,best model combination,achieves,accuracy,best model combination achieves accuracy,0.6594008207321167
translation,322,117,results,bilstm,for,entity detection / linking,bilstm for entity detection / linking,0.5594121217727661
translation,322,117,results,bigru,for,relation prediction,bigru for relation prediction,0.598541259765625
translation,322,117,results,accuracy,of,74.9,accuracy of 74.9,0.560747504234314
translation,322,117,results,competitive,with,cluster of recent top results,competitive with cluster of recent top results,0.6704052090644836
translation,322,117,results,results,has,best model combination,results has best model combination,0.5419034957885742
translation,322,121,results,bigru,with,cnn,bigru with cnn,0.6902254223823547
translation,322,121,results,bigru,has,only a tiny effect,bigru has only a tiny effect,0.6051326394081116
translation,322,121,results,cnn,for,relation prediction,cnn for relation prediction,0.5958094596862793
translation,322,121,results,only a tiny effect,on,accuracy,only a tiny effect on accuracy,0.6075853109359741
translation,322,121,results,relation prediction,has,only a tiny effect,relation prediction has only a tiny effect,0.5732917785644531
translation,322,121,results,accuracy,has,0.2 decrease at most,accuracy has 0.2 decrease at most,0.5340318083763123
translation,322,121,results,results,Replacing,bigru,results Replacing bigru,0.6885741353034973
translation,322,122,results,that do n't use neural networks ( crf + lr ),perform,surprisingly well,that do n't use neural networks ( crf + lr ) perform surprisingly well,0.6143891215324402
translation,322,122,results,surprisingly well,combining,lr ( glove+rel ) or lr ( td- idf ),surprisingly well combining lr ( glove+rel ) or lr ( td- idf ),0.6968656182289124
translation,322,122,results,lr ( glove+rel ) or lr ( td- idf ),for,relation prediction,lr ( glove+rel ) or lr ( td- idf ) for relation prediction,0.6422032713890076
translation,322,122,results,relation prediction,with,crfs,relation prediction with crfs,0.6240517497062683
translation,322,122,results,crfs,for,entity detection / linking,crfs for entity detection / linking,0.5613830089569092
translation,322,122,results,crfs,achieves,69.9 and 67.3,crfs achieves 69.9 and 67.3,0.6493103504180908
translation,322,122,results,entity detection / linking,achieves,69.9 and 67.3,entity detection / linking achieves 69.9 and 67.3,0.6263167262077332
translation,322,122,results,baselines,has,that do n't use neural networks ( crf + lr ),baselines has that do n't use neural networks ( crf + lr ),0.5649585723876953
translation,322,122,results,results,show,baselines,results show baselines,0.639954686164856
translation,322,122,results,results,combining,lr ( glove+rel ) or lr ( td- idf ),results combining lr ( glove+rel ) or lr ( td- idf ),0.6834126114845276
translation,323,65,ablation-analysis,parameters,of,pre-trained model,parameters of pre-trained model,0.5729337930679321
translation,323,65,ablation-analysis,significantly degrade,by,9 %,significantly degrade by 9 %,0.6872879266738892
translation,323,65,ablation-analysis,significantly degrade,by,10 %,significantly degrade by 10 %,0.6801359057426453
translation,323,65,ablation-analysis,9 %,for,em,9 % for em,0.675087034702301
translation,323,65,ablation-analysis,10 %,for,f1,10 % for f1,0.6773692965507507
translation,323,65,ablation-analysis,ablation analysis,fix,parameters,ablation analysis fix parameters,0.6637821793556213
translation,323,66,ablation-analysis,graph fusion block,both,em and f1,graph fusion block both em and f1,0.6716113686561584
translation,323,66,ablation-analysis,drop,about,4 %,drop about 4 %,0.7063284516334534
translation,323,66,ablation-analysis,em and f1,has,drop,em and f1 has drop,0.6543604731559753
translation,323,66,ablation-analysis,ablation analysis,remove,graph fusion block,ablation analysis remove graph fusion block,0.7101809978485107
translation,323,67,ablation-analysis,pre-trained models,used in,feature - based approach,pre-trained models used in feature - based approach,0.6941384673118591
translation,323,67,ablation-analysis,graph neural networks,can play,important role,graph neural networks can play important role,0.6850395798683167
translation,323,67,ablation-analysis,pre-trained models,has,graph neural networks,pre-trained models has graph neural networks,0.5704603791236877
translation,323,67,ablation-analysis,feature - based approach,has,graph neural networks,feature - based approach has graph neural networks,0.5761328339576721
translation,323,67,ablation-analysis,ablation analysis,only when,pre-trained models,ablation analysis only when pre-trained models,0.6008313894271851
translation,323,101,ablation-analysis,entity graph,with,self-attention,entity graph with self-attention,0.6273480653762817
translation,323,101,ablation-analysis,self-attention,to,baseline model,self-attention to baseline model,0.5324013233184814
translation,323,101,ablation-analysis,entity graph,has,final results,entity graph has final results,0.5804792046546936
translation,323,101,ablation-analysis,final results,has,significantly improved,final results has significantly improved,0.5525354743003845
translation,323,101,ablation-analysis,ablation analysis,add,entity graph,ablation analysis add entity graph,0.56755530834198
translation,323,108,ablation-analysis,transformers,show,powerful reasoning ability,transformers show powerful reasoning ability,0.5815858840942383
translation,323,40,baselines,roberta large model,to calculate,relevant score,roberta large model to calculate relevant score,0.7026504278182983
translation,323,40,baselines,relevant score,between,query,relevant score between query,0.6152652502059937
translation,323,40,baselines,relevant score,between,each candidate paragraphs,relevant score between each candidate paragraphs,0.6134970784187317
translation,323,111,hyperparameters,number of layers,of,different modules,number of layers of different modules,0.5977078080177307
translation,323,111,hyperparameters,different modules,is,two,different modules is two,0.6457759737968445
translation,323,111,hyperparameters,hidden dimensions,set to,300,hidden dimensions set to 300,0.7263270020484924
translation,323,111,hyperparameters,hyperparameters,has,hidden dimensions,hyperparameters has hidden dimensions,0.5240819454193115
translation,323,112,hyperparameters,all models,trained for,30 epochs,all models trained for 30 epochs,0.7603457570075989
translation,323,112,hyperparameters,30 epochs,with,batch size,30 epochs with batch size,0.6014043092727661
translation,323,112,hyperparameters,batch size,of,24,batch size of 24,0.6606289148330688
translation,323,112,hyperparameters,feature - based setting,has,all models,feature - based setting has all models,0.5633277893066406
translation,323,112,hyperparameters,hyperparameters,In,feature - based setting,hyperparameters In feature - based setting,0.47554218769073486
translation,323,113,hyperparameters,models,trained for,3 epochs,models trained for 3 epochs,0.8019023537635803
translation,323,113,hyperparameters,3 epochs,with,batch size,3 epochs with batch size,0.6305070519447327
translation,323,113,hyperparameters,batch size,of,8,batch size of 8,0.6920418739318848
translation,323,113,hyperparameters,fine- tuning setting,has,models,fine- tuning setting has models,0.5570545792579651
translation,323,113,hyperparameters,hyperparameters,In,fine- tuning setting,hyperparameters In fine- tuning setting,0.4574764668941498
translation,323,114,hyperparameters,initial learning rate,is,2e - 4 and 3e - 5,initial learning rate is 2e - 4 and 3e - 5,0.5787886381149292
translation,323,114,hyperparameters,2e - 4 and 3e - 5,in,feature - based setting and fine-tuning setting,2e - 4 and 3e - 5 in feature - based setting and fine-tuning setting,0.554134726524353
translation,323,114,hyperparameters,hyperparameters,has,initial learning rate,hyperparameters has initial learning rate,0.4469831883907318
translation,323,4,model,graph structure,necessary for,multi-hop question answering,graph structure necessary for multi-hop question answering,0.6801387071609497
translation,323,4,model,model,investigate,graph structure,model investigate graph structure,0.6463640332221985
translation,323,115,model,entity -centered attention pattern,leverage,approximate method,entity -centered attention pattern leverage approximate method,0.7154063582420349
translation,323,115,model,approximate method,attention head contains,entity -centered attention patterns,approximate method attention head contains entity -centered attention patterns,0.7740442752838135
translation,323,115,model,model,leverage,approximate method,model leverage approximate method,0.7360563278198242
translation,323,115,model,model,has,entity -centered attention pattern,model has entity -centered attention pattern,0.5129346251487732
translation,323,59,results,our strong baseline model,achieves,stateof - the - art results,our strong baseline model achieves stateof - the - art results,0.6248565912246704
translation,323,59,results,stateof - the - art results,on,official leaderboard,stateof - the - art results on official leaderboard,0.53248530626297
translation,323,59,results,results,has,our strong baseline model,results has our strong baseline model,0.5670273900032043
translation,323,64,results,model,with and without,graph fusion block,model with and without graph fusion block,0.6498896479606628
translation,323,64,results,model,reach,almost equal results,model reach almost equal results,0.764238715171814
translation,323,64,results,fine-tuning approach,has,model,fine-tuning approach has model,0.5712066292762756
translation,323,64,results,results,using,fine-tuning approach,results using fine-tuning approach,0.6673012971878052
translation,323,100,results,model,with,graph fusion block,model with graph fusion block,0.6366569399833679
translation,323,100,results,graph fusion block,obtains,significant advantage,graph fusion block obtains significant advantage,0.6042406558990479
translation,323,100,results,baseline,has,model,baseline has model,0.550536036491394
translation,323,100,results,results,Compared with,baseline,results Compared with baseline,0.695019543170929
translation,323,102,results,graphattention,does not show,clear advantage,graphattention does not show clear advantage,0.7221740484237671
translation,323,102,results,self-attention,has,graphattention,self-attention has graphattention,0.5682407021522522
translation,323,102,results,results,Compared with,self-attention,results Compared with self-attention,0.5859836339950562
translation,323,105,results,graphattention,consistently achieves,similar results,graphattention consistently achieves similar results,0.6796558499336243
translation,323,105,results,similar results,as,self-attention,similar results as self-attention,0.5217186212539673
translation,323,109,results,two layers,of,transformer,two layers of transformer,0.6374204754829407
translation,323,109,results,two layers,achieve,comparable results,two layers achieve comparable results,0.6023061871528625
translation,323,109,results,comparable results,as,dfgn,comparable results as dfgn,0.597107470035553
translation,323,109,results,results,stacking,two layers,results stacking two layers,0.6812588572502136
translation,324,203,baselines,offthe-shelf mrc model,trained with,negative log-likelihood loss,offthe-shelf mrc model trained with negative log-likelihood loss,0.7291848659515381
translation,324,265,experimental-setup,word embeddings,initialized by,300 - dimensional fasttext,word embeddings initialized by 300 - dimensional fasttext,0.6564280986785889
translation,324,265,experimental-setup,word vectors,trained on,common crawl ( 600b tokens ),word vectors trained on common crawl ( 600b tokens ),0.7183578014373779
translation,324,265,experimental-setup,word vectors,fixed during,training,word vectors fixed during training,0.684945285320282
translation,324,265,experimental-setup,300 - dimensional fasttext,has,word vectors,300 - dimensional fasttext has word vectors,0.5603192448616028
translation,324,265,experimental-setup,experimental setup,has,word embeddings,experimental setup has word embeddings,0.4932878315448761
translation,324,266,experimental-setup,character level embedding vectors,initialized with,32 - dimensional random vectors,character level embedding vectors initialized with 32 - dimensional random vectors,0.7555169463157654
translation,324,266,experimental-setup,experimental setup,has,character level embedding vectors,experimental setup has character level embedding vectors,0.5044592022895813
translation,324,268,experimental-setup,max pooling layer,on,character dimension,max pooling layer on character dimension,0.51028972864151
translation,324,268,experimental-setup,multi-layer perceptron ( mlp ),of,output size 64,multi-layer perceptron ( mlp ) of output size 64,0.5811620950698853
translation,324,268,experimental-setup,multi-layer perceptron ( mlp ),to aggregate,concatenation,multi-layer perceptron ( mlp ) to aggregate concatenation,0.7512001991271973
translation,324,268,experimental-setup,output size 64,to aggregate,concatenation,output size 64 to aggregate concatenation,0.7053893804550171
translation,324,268,experimental-setup,concatenation,of,word - and character - level representations,concatenation of word - and character - level representations,0.6050525903701782
translation,324,268,experimental-setup,experimental setup,use,max pooling layer,experimental setup use max pooling layer,0.5525079369544983
translation,324,273,experimental-setup,self-attention layer,use,block hidden size,self-attention layer use block hidden size,0.6007696390151978
translation,324,273,experimental-setup,self-attention layer,use,single head attention mechanism,self-attention layer use single head attention mechanism,0.6184796690940857
translation,324,273,experimental-setup,block hidden size,of,64,block hidden size of 64,0.6393693089485168
translation,324,273,experimental-setup,experimental setup,In,self-attention layer,experimental setup In self-attention layer,0.4973124861717224
translation,324,274,experimental-setup,layernorm and dropout,applied after,each component,layernorm and dropout applied after each component,0.6694498658180237
translation,324,274,experimental-setup,each component,inside,block,each component inside block,0.729282557964325
translation,324,274,experimental-setup,experimental setup,has,layernorm and dropout,experimental setup has layernorm and dropout,0.5064420104026794
translation,324,319,experimental-setup,training,with,vanilla dqn,training with vanilla dqn,0.6938180923461914
translation,324,319,experimental-setup,vanilla dqn,use,replay memory,vanilla dqn use replay memory,0.6384181976318359
translation,324,319,experimental-setup,replay memory,size,"500,000","replay memory size 500,000",0.7831981182098389
translation,324,320,experimental-setup,?- greedy,value of ?,anneals,?- greedy value of ? anneals,0.7370489239692688
translation,324,320,experimental-setup,anneals,from,1.0 to 0.1,anneals from 1.0 to 0.1,0.5760093331336975
translation,324,320,experimental-setup,1.0 to 0.1,within,"100,000 episodes","1.0 to 0.1 within 100,000 episodes",0.6267889738082886
translation,324,322,experimental-setup,our network,after,every 20 game steps,our network after every 20 game steps,0.6971519589424133
translation,324,322,experimental-setup,experimental setup,update,our network,experimental setup update our network,0.7310760617256165
translation,324,323,experimental-setup,updating,use,mini-batch,updating use mini-batch,0.6165975332260132
translation,324,323,experimental-setup,mini-batch,of size,64,mini-batch of size 64,0.7458136081695557
translation,324,323,experimental-setup,experimental setup,During,updating,experimental setup During updating,0.604584276676178
translation,324,324,experimental-setup,"adam ( kingma and ba , 2014 )",as,step rule,"adam ( kingma and ba , 2014 ) as step rule",0.5086702108383179
translation,324,324,experimental-setup,"adam ( kingma and ba , 2014 )",as,learning rate,"adam ( kingma and ba , 2014 ) as learning rate",0.4865206778049469
translation,324,324,experimental-setup,step rule,for,optimization,step rule for optimization,0.6427600979804993
translation,324,324,experimental-setup,learning rate,set to,0.00025,learning rate set to 0.00025,0.6881960034370422
translation,324,325,experimental-setup,agent,trained with,rainbow algorithm,agent trained with rainbow algorithm,0.7120234966278076
translation,324,327,experimental-setup,experimental setup,implemented using,pytorch,experimental setup implemented using pytorch,0.6359934210777283
translation,324,177,experiments,strong,at,memorizing,strong at memorizing,0.6040273308753967
translation,324,177,experiments,memorizing,has,training games,memorizing has training games,0.5712064504623413
translation,324,264,model,aggregate both word - and character - level information,to produce,vector,aggregate both word - and character - level information to produce vector,0.6994717717170715
translation,324,264,model,vector,each token in,text,vector each token in text,0.6904533505439758
translation,324,264,model,model,In,embedding layer,model In embedding layer,0.5142669677734375
translation,324,267,model,convolutional layer,with,64 kernels,convolutional layer with 64 kernels,0.6032187342643738
translation,324,267,model,64 kernels,of size,5,64 kernels of size 5,0.719193696975708
translation,324,267,model,64 kernels,to,aggregate,64 kernels to aggregate,0.5579960346221924
translation,324,267,model,aggregate,has,sequence of characters,aggregate has sequence of characters,0.6005088090896606
translation,324,267,model,model,has,convolutional layer,model has convolutional layer,0.5413889288902283
translation,324,272,model,each kernel 's size,is,7,each kernel 's size is 7,0.6255565881729126
translation,324,272,model,convolutional layers,share,weights,convolutional layers share weights,0.6915834546089172
translation,324,272,model,each convolutional layer,has,64 filters,each convolutional layer has 64 filters,0.5406447052955627
translation,324,318,model,two extra rewards,added onto,sufficient information bonus,two extra rewards added onto sufficient information bonus,0.6103324890136719
translation,324,318,model,sufficient information bonus,for,attribute question,sufficient information bonus for attribute question,0.6271283626556396
translation,324,318,model,sufficient information bonus,with,coefficient,sufficient information bonus with coefficient,0.6040300726890564
translation,324,318,model,coefficient,of,0.1,coefficient of 0.1,0.5974176526069641
translation,324,318,model,model,has,two extra rewards,model has two extra rewards,0.5608798861503601
translation,324,176,results,training data size,is,"small ( e.g. , 10 games )","training data size is small ( e.g. , 10 games )",0.5029768943786621
translation,324,176,results,our baseline agent,trained with,all the three rl methods,our baseline agent trained with all the three rl methods,0.7213947176933289
translation,324,176,results,all the three rl methods,successfully master,training games,all the three rl methods successfully master training games,0.698249101638794
translation,324,176,results,training data size,has,our baseline agent,training data size has our baseline agent,0.5159443020820618
translation,324,176,results,"small ( e.g. , 10 games )",has,our baseline agent,"small ( e.g. , 10 games ) has our baseline agent",0.5395544171333313
translation,324,176,results,results,when,training data size,results when training data size,0.6487857699394226
translation,324,180,results,our agents,fail to,generalize,our agents fail to generalize,0.7926994562149048
translation,324,180,results,generalize,on,attribute questions,generalize on attribute questions,0.53936368227005
translation,324,180,results,results,observe,our agents,results observe our agents,0.6298028826713562
translation,324,181,results,all three agents,produce,accuracy,all three agents produce accuracy,0.6552321910858154
translation,324,181,results,accuracy,of,0.5,accuracy of 0.5,0.6153009533882141
translation,324,181,results,no agent,performs,significantly better,no agent performs significantly better,0.6110547184944153
translation,324,181,results,significantly better,than,random,significantly better than random,0.6052526235580444
translation,324,181,results,unlimited games setting,has,all three agents,unlimited games setting has all three agents,0.5549940466880798
translation,324,181,results,zero-shot test,has,no agent,zero-shot test has no agent,0.6062446236610413
translation,324,181,results,results,In,unlimited games setting,results In unlimited games setting,0.5506379008293152
translation,324,196,results,objects being asked about,in,current observation,objects being asked about in current observation,0.49660202860832214
translation,324,196,results,random baseline 's performance,goes up,significantly,random baseline 's performance goes up significantly,0.7222093343734741
translation,324,196,results,objects being asked about,has,random baseline 's performance,objects being asked about has random baseline 's performance,0.5995127558708191
translation,324,204,results,match -lstm,does,fairly well,match -lstm does fairly well,0.26672953367233276
translation,324,204,results,fairly well,on,all 3 question types,fairly well on all 3 question types,0.4695175886154175
translation,324,204,results,"86.4 , 89.9 and 93.2 test accuracy",on,"location , existence , and attribute questions","86.4 , 89.9 and 93.2 test accuracy on location , existence , and attribute questions",0.46983015537261963
translation,324,204,results,all 3 question types,has,"86.4 , 89.9 and 93.2 test accuracy","all 3 question types has 86.4 , 89.9 and 93.2 test accuracy",0.5461998581886292
translation,324,204,results,results,has,match -lstm,results has match -lstm,0.5814555883407593
translation,325,164,ablation-analysis,fine-tuned 12 - layer bert model,with,"bert - of-theseus ( xu et al. , 2020 )","fine-tuned 12 - layer bert model with bert - of-theseus ( xu et al. , 2020 )",0.6083709597587585
translation,325,164,ablation-analysis,fine-tuned 12 - layer bert model,obtain,performance,fine-tuned 12 - layer bert model obtain performance,0.5262231230735779
translation,325,164,ablation-analysis,performance,of,6 - layer model,performance of 6 - layer model,0.593063473701477
translation,325,164,ablation-analysis,ablation analysis,compress,fine-tuned 12 - layer bert model,ablation analysis compress fine-tuned 12 - layer bert model,0.6506058573722839
translation,325,46,baselines,baselines,has,multi-task applicable,baselines has multi-task applicable,0.5445358157157898
translation,325,48,baselines,multi-task field - shared sequence to sequence ( mtf - s2s ),achieves,better performance,multi-task field - shared sequence to sequence ( mtf - s2s ) achieves better performance,0.6823172569274902
translation,325,48,baselines,straightforward yet effective model,achieves,better performance,straightforward yet effective model achieves better performance,0.6434603333473206
translation,325,48,baselines,better performance,on,all three tasks,better performance on all three tasks,0.47258245944976807
translation,325,48,baselines,better performance,compared to,singletask counterparts,better performance compared to singletask counterparts,0.7024063467979431
translation,325,48,baselines,multi-task field - shared sequence to sequence ( mtf - s2s ),has,straightforward yet effective model,multi-task field - shared sequence to sequence ( mtf - s2s ) has straightforward yet effective model,0.5341946482658386
translation,325,48,baselines,baselines,present,multi-task field - shared sequence to sequence ( mtf - s2s ),baselines present multi-task field - shared sequence to sequence ( mtf - s2s ),0.6433030366897583
translation,325,152,baselines,mtf - s2s,on,each task,mtf - s2s on each task,0.5758542418479919
translation,325,152,baselines,mtf - s2s,to provide,single - task baseline,mtf - s2s to provide single - task baseline,0.595133364200592
translation,325,153,baselines,both mtf - s2s and seq2seq baselines,are,character - based,both mtf - s2s and seq2seq baselines are character - based,0.5946839451789856
translation,325,153,baselines,embeddings,initialized with,tencent ai lab embedding,embeddings initialized with tencent ai lab embedding,0.706073522567749
translation,325,153,baselines,baselines,has,both mtf - s2s and seq2seq baselines,baselines has both mtf - s2s and seq2seq baselines,0.5561704635620117
translation,325,154,baselines,both mtf - s2s and seq2seq baselines,use,"beam search ( wiseman and rush , 2016 )","both mtf - s2s and seq2seq baselines use beam search ( wiseman and rush , 2016 )",0.6043394207954407
translation,325,154,baselines,"beam search ( wiseman and rush , 2016 )",when,decoding,"beam search ( wiseman and rush , 2016 ) when decoding",0.656682550907135
translation,325,154,baselines,baselines,For,both mtf - s2s and seq2seq baselines,baselines For both mtf - s2s and seq2seq baselines,0.5834733843803406
translation,325,157,baselines,statistical baselines,extract,character - based unigram and bigram features,statistical baselines extract character - based unigram and bigram features,0.6719874739646912
translation,325,157,baselines,statistical baselines,use,logistic classifier,statistical baselines use logistic classifier,0.6009821891784668
translation,325,157,baselines,logistic classifier,to predict,classes,logistic classifier to predict classes,0.7660362720489502
translation,325,169,baselines,generation - based baselines,use,character - based seq2seq,generation - based baselines use character - based seq2seq,0.6299313902854919
translation,325,169,baselines,generation - based baselines,use,seq2seq,generation - based baselines use seq2seq,0.6223224997520447
translation,325,169,baselines,seq2seq,with,attention,seq2seq with attention,0.6781299710273743
translation,325,169,baselines,baselines,For,generation - based baselines,baselines For generation - based baselines,0.6334996223449707
translation,325,173,baselines,extractive methods,choose,two widely used classical methods,extractive methods choose two widely used classical methods,0.6497295498847961
translation,325,173,baselines,extractive methods,choose,lexrank,extractive methods choose lexrank,0.6590303182601929
translation,325,173,baselines,extractive methods,choose,"dpcnn ( johnson and zhang , 2017 ) radev","extractive methods choose dpcnn ( johnson and zhang , 2017 ) radev",0.7087169289588928
translation,325,173,baselines,lexrank,has,"erkan and text cnn ( kim , 2014 )","lexrank has erkan and text cnn ( kim , 2014 )",0.5920969843864441
translation,325,173,baselines,baselines,For,extractive methods,baselines For extractive methods,0.581946849822998
translation,325,174,baselines,abstractive methods,use,wean and global encoding,abstractive methods use wean and global encoding,0.6233871579170227
translation,325,174,baselines,wean and global encoding,along with,seq2seq,wean and global encoding along with seq2seq,0.6509782671928406
translation,325,174,baselines,wean and global encoding,as,baselines,wean and global encoding as baselines,0.5323567986488342
translation,325,174,baselines,baselines,For,abstractive methods,baselines For abstractive methods,0.5074514746665955
translation,325,175,baselines,"bertabs ( liu and lapata , 2019 )",has,bert - based summarization model,"bertabs ( liu and lapata , 2019 ) has bert - based summarization model",0.5657497644424438
translation,325,7,experiments,mat -inf,contains,1.07 million,mat -inf contains 1.07 million,0.6772519946098328
translation,325,7,experiments,question - answer pairs,with,human-labeled categories,question - answer pairs with human-labeled categories,0.580299437046051
translation,325,7,experiments,question - answer pairs,with,usergenerated question descriptions,question - answer pairs with usergenerated question descriptions,0.581619381904602
translation,325,7,experiments,1.07 million,has,question - answer pairs,1.07 million has question - answer pairs,0.5664623379707336
translation,325,23,experiments,matinf,consists of,question answering data,matinf consists of question answering data,0.6508010029792786
translation,325,23,experiments,question answering data,crawled from,large chinese maternity and baby caring qa site,question answering data crawled from large chinese maternity and baby caring qa site,0.7551038265228271
translation,325,47,experiments,mat -inf,is,first dataset,mat -inf is first dataset,0.5763067007064819
translation,325,47,experiments,first dataset,simultaneously contains,ground truths,first dataset simultaneously contains ground truths,0.6362221240997314
translation,325,47,experiments,ground truths,for,three major nlp tasks,ground truths for three major nlp tasks,0.5299996733665466
translation,325,47,experiments,ground truths,could facilitate,new multi-task learning methods,ground truths could facilitate new multi-task learning methods,0.633611798286438
translation,325,148,experiments,mtf - s2s,set,all ? i = 0.25,mtf - s2s set all ? i = 0.25,0.6686503291130066
translation,325,148,experiments,mtf - s2s,use,"adam ( kingma and ba , 2015 ) optimizer","mtf - s2s use adam ( kingma and ba , 2015 ) optimizer",0.6136380434036255
translation,325,148,experiments,"adam ( kingma and ba , 2015 ) optimizer",to co-train,model,"adam ( kingma and ba , 2015 ) optimizer to co-train model",0.7069347500801086
translation,325,148,experiments,model,for,one epoch,model for one epoch,0.6177431344985962
translation,325,148,experiments,one epoch,with,batch sizes,one epoch with batch sizes,0.6623514294624329
translation,325,148,experiments,one epoch,with,learning rate,one epoch with learning rate,0.6672982573509216
translation,325,148,experiments,one epoch,with,learning rate,one epoch with learning rate,0.6672982573509216
translation,325,148,experiments,batch sizes,of,"64 , 64 , 12 and 52","batch sizes of 64 , 64 , 12 and 52",0.6398622989654541
translation,325,148,experiments,"64 , 64 , 12 and 52",for,"bs summ , bs qa , bs ct opic , and bs cage","64 , 64 , 12 and 52 for bs summ , bs qa , bs ct opic , and bs cage",0.6558094620704651
translation,325,148,experiments,"bs summ , bs qa , bs ct opic , and bs cage",with,learning rate,"bs summ , bs qa , bs ct opic , and bs cage with learning rate",0.6371941566467285
translation,325,148,experiments,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,325,158,experiments,neural networks,choose,"fasttext ( grave et al. , 2017 )","neural networks choose fasttext ( grave et al. , 2017 )",0.6799436807632446
translation,325,158,experiments,neural networks,choose,"text cnn ( kim , 2014 )","neural networks choose text cnn ( kim , 2014 )",0.6953911185264587
translation,325,158,experiments,neural networks,choose,"dcnn ( kalchbrenner et al. , 2014 )","neural networks choose dcnn ( kalchbrenner et al. , 2014 )",0.612150251865387
translation,325,158,experiments,neural networks,choose,"rcnn ( lai et al. , 2015 )","neural networks choose rcnn ( lai et al. , 2015 )",0.6215212941169739
translation,325,158,experiments,neural networks,choose,"dpcnn ( johnson and zhang , 2017 )","neural networks choose dpcnn ( johnson and zhang , 2017 )",0.6705675721168518
translation,325,162,experiments,language models,fine- tune,"bert ( devlin et al. , 2019 )","language models fine- tune bert ( devlin et al. , 2019 )",0.6694621443748474
translation,325,162,experiments,language models,fine- tune,ernie,language models fine- tune ernie,0.6489564180374146
translation,325,180,experiments,tougher matinf -c-topic,has,language models,tougher matinf -c-topic has language models,0.5768499970436096
translation,325,180,experiments,language models,has,prominently outperform,language models has prominently outperform,0.5881555080413818
translation,325,180,experiments,prominently outperform,has,other baselines,prominently outperform has other baselines,0.5699324607849121
translation,325,183,experiments,language models,with,accuracy,language models with accuracy,0.6087610721588135
translation,325,183,experiments,accuracy,of,91.02,accuracy of 91.02,0.5568457245826721
translation,325,199,experiments,single - task counterpart,by,4.73,single - task counterpart by 4.73,0.5340232253074646
translation,325,199,experiments,4.73,on,rouge -l.,4.73 on rouge -l.,0.6146253943443298
translation,325,199,experiments,outperforms,has,single - task counterpart,outperforms has single - task counterpart,0.5856255888938904
translation,325,149,hyperparameters,model,for,each task,model for each task,0.6081339716911316
translation,325,149,hyperparameters,model,with,learning rate,model with learning rate,0.6086345314979553
translation,325,149,hyperparameters,learning rate,of,5 ? 10 ?5,learning rate of 5 ? 10 ?5,0.6658074855804443
translation,325,149,hyperparameters,hyperparameters,fine- tune,model,hyperparameters fine- tune model,0.7390584349632263
translation,325,151,hyperparameters,hidden size,of,all lstm encoders / decoders and attentions,hidden size of all lstm encoders / decoders and attentions,0.5590137243270874
translation,325,151,hyperparameters,all lstm encoders / decoders and attentions,is,200,all lstm encoders / decoders and attentions is 200,0.5634708404541016
translation,325,151,hyperparameters,hyperparameters,has,hidden size,hyperparameters has hidden size,0.535134494304657
translation,325,159,hyperparameters,sentences,into,words,sentences into words,0.5743101239204407
translation,325,159,hyperparameters,words,with,jieba,words with jieba,0.6950194835662842
translation,325,160,hyperparameters,word embedding,with,pretrained tencent ai lab embedding,word embedding with pretrained tencent ai lab embedding,0.5702677369117737
translation,325,160,hyperparameters,pretrained tencent ai lab embedding,except for,fasttext,pretrained tencent ai lab embedding except for fasttext,0.5860962271690369
translation,325,160,hyperparameters,hyperparameters,initialize,word embedding,hyperparameters initialize word embedding,0.7091864347457886
translation,325,161,hyperparameters,cross-entropy,with,"adam ( kingma and ba , 2015 ) optimizer","cross-entropy with adam ( kingma and ba , 2015 ) optimizer",0.6072732210159302
translation,325,161,hyperparameters,"adam ( kingma and ba , 2015 ) optimizer",with,learning rate,"adam ( kingma and ba , 2015 ) optimizer with learning rate",0.6114457249641418
translation,325,161,hyperparameters,"adam ( kingma and ba , 2015 ) optimizer",apply,early stopping,"adam ( kingma and ba , 2015 ) optimizer apply early stopping",0.6030715107917786
translation,325,161,hyperparameters,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,325,161,hyperparameters,hyperparameters,minimize,cross-entropy,hyperparameters minimize cross-entropy,0.7335095405578613
translation,325,163,hyperparameters,learning rate,for,fine- tuning,learning rate for fine- tuning,0.6222808957099915
translation,325,163,hyperparameters,learning rate,apply,early stopping,learning rate apply early stopping,0.6021116971969604
translation,325,163,hyperparameters,fine- tuning,to,5 ? 10 ?5,fine- tuning to 5 ? 10 ?5,0.6243557333946228
translation,325,163,hyperparameters,hyperparameters,set,learning rate,hyperparameters set learning rate,0.5994082689285278
translation,325,163,hyperparameters,hyperparameters,apply,early stopping,hyperparameters apply early stopping,0.6104239225387573
translation,325,6,model,first jointly labeled large-scale dataset,for,classification,first jointly labeled large-scale dataset for classification,0.5444111227989197
translation,325,6,model,first jointly labeled large-scale dataset,for,question answering and summarization,first jointly labeled large-scale dataset for question answering and summarization,0.5656916499137878
translation,325,6,model,matinf,has,first jointly labeled large-scale dataset,matinf has first jointly labeled large-scale dataset,0.5018057823181152
translation,325,6,model,model,propose,matinf,model propose matinf,0.6745692491531372
translation,325,181,results,other baselines,with,considerable margin,other baselines with considerable margin,0.6373078227043152
translation,325,181,results,-lm neural networks,has,"dpcnn ( johnson and zhang , 2017 )","-lm neural networks has dpcnn ( johnson and zhang , 2017 )",0.5615208148956299
translation,325,181,results,"dpcnn ( johnson and zhang , 2017 )",has,outperforms,"dpcnn ( johnson and zhang , 2017 ) has outperforms",0.5717357397079468
translation,325,181,results,outperforms,has,other baselines,outperforms has other baselines,0.5879674553871155
translation,325,181,results,results,Among,-lm neural networks,results Among -lm neural networks,0.5844607353210449
translation,325,181,results,results,non,-lm neural networks,results non -lm neural networks,0.5944481492042542
translation,325,182,results,all other baselines,including,cnn / dm,all other baselines including cnn / dm,0.678593099117279
translation,325,182,results,all other baselines,including,lcsts,all other baselines including lcsts,0.7175990343093872
translation,325,182,results,all other baselines,including,matinf - summ,all other baselines including matinf - summ,0.6595508456230164
translation,325,182,results,matinf -c-age,has,dpcnn,matinf -c-age has dpcnn,0.6855394244194031
translation,325,182,results,dpcnn,has,outperforms,dpcnn has outperforms,0.6520873308181763
translation,325,182,results,outperforms,has,all other baselines,outperforms has all other baselines,0.5747320652008057
translation,325,182,results,cnn / dm,has,lcsts,cnn / dm has lcsts,0.6396871209144592
translation,325,182,results,lcsts,has,matinf -summ,lcsts has matinf -summ,0.6652388572692871
translation,325,182,results,results,On,matinf -c-age,results On matinf -c-age,0.5291755795478821
translation,325,186,results,mtf - s2s,shows,satisfying performance,mtf - s2s shows satisfying performance,0.6210175156593323
translation,325,186,results,satisfying performance,on,matinf -c-age and matinf - c-topic,satisfying performance on matinf -c-age and matinf - c-topic,0.550715982913971
translation,325,186,results,satisfying performance,on,outperforming,satisfying performance on outperforming,0.5908790230751038
translation,325,186,results,single task,by,0.14 and 0.19,single task by 0.14 and 0.19,0.5741496086120605
translation,325,186,results,single task,in terms of,accuracy,single task in terms of accuracy,0.6466346383094788
translation,325,186,results,0.14 and 0.19,in terms of,accuracy,0.14 and 0.19 in terms of accuracy,0.7329379320144653
translation,325,186,results,multi-task baseline,has,mtf - s2s,multi-task baseline has mtf - s2s,0.5579202771186829
translation,325,186,results,outperforming,has,same model,outperforming has same model,0.6406975388526917
translation,325,186,results,results,For,multi-task baseline,results For multi-task baseline,0.5656383037567139
translation,325,191,results,outperforms,by,margin,outperforms by margin,0.6504319310188293
translation,325,191,results,retrieval - based baseline,by,margin,retrieval - based baseline by margin,0.573697566986084
translation,325,191,results,margin,of,2.56,margin of 2.56,0.6066352725028992
translation,325,191,results,seq2seq with attention,has,outperforms,seq2seq with attention has outperforms,0.6124613285064697
translation,325,191,results,outperforms,has,retrieval - based baseline,outperforms has retrieval - based baseline,0.5845874547958374
translation,325,191,results,results,has,seq2seq with attention,results has seq2seq with attention,0.5524299740791321
translation,325,195,results,mtf - s2s,is,effective,mtf - s2s is effective,0.6827504634857178
translation,325,195,results,effective,on,qa task,effective on qa task,0.5876505374908447
translation,325,195,results,outperforms,by,0.74,outperforms by 0.74,0.6156151294708252
translation,325,195,results,single - task version,by,0.74,single - task version by 0.74,0.5316451191902161
translation,325,195,results,0.74,on,rouge - l.,0.74 on rouge - l.,0.5962496995925903
translation,325,195,results,outperforms,has,single - task version,outperforms has single - task version,0.5825417637825012
translation,325,195,results,results,has,mtf - s2s,results has mtf - s2s,0.5368064641952515
translation,325,197,results,performance,of,two basic baselines,performance of two basic baselines,0.5813236832618713
translation,325,197,results,performance,see,obvious difference,performance see obvious difference,0.5827008485794067
translation,325,197,results,two basic baselines,see,obvious difference,two basic baselines see obvious difference,0.6069635152816772
translation,325,197,results,"seq2seq + att ( luong et al. , 2015 )",see,obvious difference,"seq2seq + att ( luong et al. , 2015 ) see obvious difference",0.5564640760421753
translation,325,197,results,obvious difference,in,performance,obvious difference in performance,0.49978312849998474
translation,325,197,results,performance,between,extractive and abstractive methods,performance between extractive and abstractive methods,0.622796356678009
translation,325,197,results,extractive and abstractive methods,on,datasets,extractive and abstractive methods on datasets,0.5151613354682922
translation,325,197,results,results,comparing,performance,results comparing performance,0.7179481983184814
translation,325,198,results,all other baselines,on,matinf - summ,all other baselines on matinf - summ,0.5400347113609314
translation,325,198,results,"bertabs ( liu and lapata , 2019 )",has,significantly outperforms,"bertabs ( liu and lapata , 2019 ) has significantly outperforms",0.5960744023323059
translation,325,198,results,significantly outperforms,has,all other baselines,significantly outperforms has all other baselines,0.5790193676948547
translation,325,198,results,results,has,"bertabs ( liu and lapata , 2019 )","results has bertabs ( liu and lapata , 2019 )",0.5673883557319641
translation,326,5,model,syn - qg,set of,transparent syntactic rules,syn - qg set of transparent syntactic rules,0.6489165425300598
translation,326,5,model,transparent syntactic rules,leveraging,universal dependencies,transparent syntactic rules leveraging universal dependencies,0.6486765146255493
translation,326,5,model,transparent syntactic rules,leveraging,shallow semantic parsing,transparent syntactic rules leveraging shallow semantic parsing,0.6782198548316956
translation,326,5,model,transparent syntactic rules,leveraging,lexical resources,transparent syntactic rules leveraging lexical resources,0.6648839712142944
translation,326,5,model,transparent syntactic rules,leveraging,custom rules,transparent syntactic rules leveraging custom rules,0.6650962233543396
translation,326,5,model,custom rules,transform,declarative sentences,custom rules transform declarative sentences,0.7393649220466614
translation,326,5,model,declarative sentences,into,questionanswer pairs,declarative sentences into questionanswer pairs,0.5209051966667175
translation,326,25,results,back - translated questions,are,grammatically superior,back - translated questions are grammatically superior,0.5650955438613892
translation,326,25,results,sometimes slightly irrelevant,compared to,original counterparts,sometimes slightly irrelevant compared to original counterparts,0.6281269192695618
translation,326,25,results,results,notice,back - translated questions,results notice back - translated questions,0.6147363185882568
translation,327,83,hyperparameters,max-pooling method,to deal with,variable feature size,max-pooling method to deal with variable feature size,0.6816871762275696
translation,327,83,hyperparameters,hyperparameters,choose,max-pooling method,hyperparameters choose max-pooling method,0.6389296650886536
translation,327,93,hyperparameters,keyword extraction algorithm,for,each sub-sentence,keyword extraction algorithm for each sub-sentence,0.5928545594215393
translation,327,93,hyperparameters,each sub-sentence,extract,1/3,each sub-sentence extract 1/3,0.7145940661430359
translation,327,93,hyperparameters,each sub-sentence,set,"b = 1.4 , d = 0.8","each sub-sentence set b = 1.4 , d = 0.8",0.683427631855011
translation,327,93,hyperparameters,1/3,of,words,1/3 of words,0.6233720183372498
translation,327,93,hyperparameters,words,as,keywords,words as keywords,0.49560117721557617
translation,327,93,hyperparameters,hyperparameters,use,keyword extraction algorithm,hyperparameters use keyword extraction algorithm,0.6515509486198425
translation,327,94,hyperparameters,cnn model,set up,filter shape,cnn model set up filter shape,0.5979188084602356
translation,327,94,hyperparameters,filter shape,is,3*300,filter shape is 3*300,0.5966902375221252
translation,327,94,hyperparameters,hyperparameters,In,cnn model,hyperparameters In cnn model,0.44801566004753113
translation,327,95,hyperparameters,number of filters,is,500,number of filters is 500,0.6061723828315735
translation,327,95,hyperparameters,hyperparameters,has,number of filters,hyperparameters has number of filters,0.5513906478881836
translation,327,28,model,unsupervised keyword extraction method,based on,dependency analysis,unsupervised keyword extraction method based on dependency analysis,0.639232337474823
translation,327,28,model,model,use,unsupervised keyword extraction method,model use unsupervised keyword extraction method,0.6125787496566772
translation,327,84,model,model,has,last layer,model has last layer,0.5881316661834717
translation,327,101,results,our results,better than,baseline,our results better than baseline,0.7036615014076233
translation,327,101,results,test set,has,our results,test set has our results,0.5929542779922485
translation,327,101,results,results,In,test set,results In test set,0.5512292981147766
translation,327,102,results,our result,is,all not so good,our result is all not so good,0.6172170639038086
translation,327,102,results,development set,has,our result,development set has our result,0.6344768404960632
translation,327,102,results,results,In,development set,results In development set,0.5407683253288269
translation,328,23,baselines,two methods,of,discourse parsing,two methods of discourse parsing,0.5267491340637207
translation,328,23,baselines,deep model,based on,rhetorical structure theory ( rst ),deep model based on rhetorical structure theory ( rst ),0.7001953721046448
translation,328,71,hyperparameters,rnnlm,learned,word embeddings,rnnlm learned word embeddings,0.6533200740814209
translation,328,71,hyperparameters,rnnlm,include,cosine similarity - based features,rnnlm include cosine similarity - based features,0.5848391652107239
translation,328,71,hyperparameters,word embeddings,using,word2vec rnnlm,word embeddings using word2vec rnnlm,0.5469812750816345
translation,328,71,hyperparameters,hyperparameters,has,rnnlm,hyperparameters has rnnlm,0.5468769073486328
translation,328,6,model,two inexpensive methods,for training,alignment models,two inexpensive methods for training alignment models,0.7480052709579468
translation,328,6,model,two inexpensive methods,by generating,artificial question - answer pairs,two inexpensive methods by generating artificial question - answer pairs,0.7124210596084595
translation,328,6,model,alignment models,solely using,free text,alignment models solely using free text,0.6447678208351135
translation,328,6,model,artificial question - answer pairs,from,discourse structures,artificial question - answer pairs from discourse structures,0.5017678737640381
translation,328,6,model,model,propose,two inexpensive methods,model propose two inexpensive methods,0.6716076731681824
translation,328,29,model,method,to train,alignment model,method to train alignment model,0.7029926180839539
translation,328,29,model,alignment model,over,free text,alignment model over free text,0.6388089656829834
translation,328,29,model,alignment model,by making use of,discourse structures,alignment model by making use of discourse structures,0.6652957201004028
translation,328,64,model,reranked,using,more expressive model,reranked using more expressive model,0.6959483027458191
translation,328,64,model,more expressive model,incorporates,alignment features,more expressive model incorporates alignment features,0.709676206111908
translation,328,64,model,alignment features,alongside,cr score,alignment features alongside cr score,0.6965453028678894
translation,328,64,model,model,has,answers,model has answers,0.6534027457237244
translation,328,58,results,all relations,slightly improves,performance,all relations slightly improves performance,0.7379172444343567
translation,328,58,results,performance,by,0.3 % p@1,performance by 0.3 % p@1,0.6117293834686279
translation,328,58,results,results,Using,all relations,results Using all relations,0.6528851985931396
translation,328,96,results,rst and seq models,Comparing,two principal alignment models,rst and seq models Comparing two principal alignment models,0.6817857027053833
translation,328,96,results,by about 0.5 % p@1,in,both domains,by about 0.5 % p@1 in both domains,0.5754875540733337
translation,328,96,results,two principal alignment models,has,rst - based model,two principal alignment models has rst - based model,0.5439168214797974
translation,328,96,results,rst - based model,has,significantly outperforms,rst - based model has significantly outperforms,0.6011853814125061
translation,328,96,results,significantly outperforms,has,seq model,significantly outperforms has seq model,0.60187828540802
translation,328,96,results,significantly outperforms,has,by about 0.5 % p@1,significantly outperforms has by about 0.5 % p@1,0.6153459548950195
translation,328,96,results,seq model,has,by about 0.5 % p@1,seq model has by about 0.5 % p@1,0.5902865529060364
translation,328,96,results,results,Comparing,two principal alignment models,results Comparing two principal alignment models,0.5507727861404419
translation,328,99,results,performance,RST model compare to,model,performance RST model compare to model,0.7214196920394897
translation,328,99,results,performance,RST model compare to,alignment model,performance RST model compare to alignment model,0.7188103199005127
translation,328,99,results,model,trained on,in- domain pairs,model trained on in- domain pairs,0.7733469009399414
translation,328,99,results,model,trained on,explicit in-domain question - answer pairs,model trained on explicit in-domain question - answer pairs,0.763347864151001
translation,328,99,results,and seq results,for,ya,and seq results for ya,0.6953973174095154
translation,328,99,results,and seq results,higher than,alignment model,and seq results higher than alignment model,0.6782923936843872
translation,328,99,results,ya,higher than,alignment model,ya higher than alignment model,0.7139906287193298
translation,328,99,results,alignment model,trained on,explicit in-domain question - answer pairs,alignment model trained on explicit in-domain question - answer pairs,0.7422489523887634
translation,328,99,results,results,How does,performance,results How does performance,0.6373046040534973
translation,328,103,results,cr + rnnlm model,peaks at,26.6 % p@1,cr + rnnlm model peaks at 26.6 % p@1,0.6826767325401306
translation,328,103,results,cr + rnnlm model,peaks at,31.7 %,cr + rnnlm model peaks at 31.7 %,0.6758837699890137
translation,328,103,results,26.6 % p@1,for,ya,26.6 % p@1 for ya,0.6806020140647888
translation,328,103,results,31.7 %,for,bio why,31.7 % for bio why,0.658698320388794
translation,328,103,results,discourse - based alignment models,has,outperform,discourse - based alignment models has outperform,0.5905711054801941
translation,328,103,results,outperform,has,cr + rnnlm model,outperform has cr + rnnlm model,0.5831740498542786
translation,328,103,results,results,has,discourse - based alignment models,results has discourse - based alignment models,0.5078637003898621
translation,328,107,results,and seq alignment models,has,significantly outperform,and seq alignment models has significantly outperform,0.5983247756958008
translation,328,107,results,significantly outperform,has,rnnlm and cr baselines,significantly outperform has rnnlm and cr baselines,0.5666471123695374
translation,328,108,results,ya,has,rst and seq models,ya has rst and seq models,0.6194502115249634
translation,328,108,results,rst and seq models,has,significantly outperform,rst and seq models has significantly outperform,0.5977550745010376
translation,328,108,results,significantly outperform,has,cr baseline ( p < 0.001 ),significantly outperform has cr baseline ( p < 0.001 ),0.575402021408081
translation,329,98,ablation-analysis,length constraint,in,n-gram,length constraint in n-gram,0.5130403637886047
translation,329,98,ablation-analysis,n-gram,from,coala,n-gram from coala,0.6057121157646179
translation,329,98,ablation-analysis,relative gain,in,insur-anceqa dataset,relative gain in insur-anceqa dataset,0.5461169481277466
translation,329,98,ablation-analysis,ablation analysis,relax,length constraint,ablation analysis relax length constraint,0.7576965689659119
translation,329,98,ablation-analysis,ablation analysis,achieve,relative gain,ablation analysis achieve relative gain,0.6531844735145569
translation,329,85,experiments,unigram cnn,uses,1:1 word matching,unigram cnn uses 1:1 word matching,0.5526105165481567
translation,329,85,experiments,unigram cnn,able to utilize,word - based signals,unigram cnn able to utilize word - based signals,0.6156433820724487
translation,329,85,experiments,word - based signals,as,query term weighting value,word - based signals as query term weighting value,0.5076806545257568
translation,329,87,hyperparameters,word embeddings,use,300d pre-trained glove,word embeddings use 300d pre-trained glove,0.5505344271659851
translation,329,87,hyperparameters,hyperparameters,For,word embeddings,hyperparameters For word embeddings,0.49829164147377014
translation,329,88,hyperparameters,sequence length,of,passage,sequence length of passage,0.5957733988761902
translation,329,88,hyperparameters,sequence length,are,all different,sequence length are all different,0.6016437411308289
translation,329,88,hyperparameters,all different,for,each dataset,all different for each dataset,0.6140570640563965
translation,329,88,hyperparameters,400 tokens,for,wikipassageqa,400 tokens for wikipassageqa,0.6188087463378906
translation,329,88,hyperparameters,200 tokens,for,insuranceqa,200 tokens for insuranceqa,0.659632682800293
translation,329,88,hyperparameters,each dataset,has,400 tokens,each dataset has 400 tokens,0.5849205851554871
translation,329,88,hyperparameters,each dataset,has,200 tokens,each dataset has 200 tokens,0.5913793444633484
translation,329,88,hyperparameters,hyperparameters,has,sequence length,hyperparameters has sequence length,0.521059513092041
translation,329,89,hyperparameters,dropout,applied after,every layers,dropout applied after every layers,0.6706175208091736
translation,329,89,hyperparameters,every layers,with,keep rate,every layers with keep rate,0.696435809135437
translation,329,89,hyperparameters,keep rate,of,0.7,keep rate of 0.7,0.6258217692375183
translation,329,89,hyperparameters,hyperparameters,has,dropout,hyperparameters has dropout,0.5324090719223022
translation,329,90,hyperparameters,weights,except,embedding matrices,weights except embedding matrices,0.6381560564041138
translation,329,90,hyperparameters,weights,constrained by,l2 regularization,weights constrained by l2 regularization,0.7315985560417175
translation,329,90,hyperparameters,l2 regularization,with,constant values,l2 regularization with constant values,0.6187070608139038
translation,329,90,hyperparameters,constant values,of,10 ?7 and 10 ?5,constant values of 10 ?7 and 10 ?5,0.6197466850280762
translation,329,90,hyperparameters,10 ?7 and 10 ?5,for,wikipassageqa and insur-anceqa,10 ?7 and 10 ?5 for wikipassageqa and insur-anceqa,0.7002341747283936
translation,329,90,hyperparameters,hyperparameters,has,weights,hyperparameters has weights,0.5201958417892456
translation,329,91,hyperparameters,adam optimizer,with,initial learning rate,adam optimizer with initial learning rate,0.5838022828102112
translation,329,91,hyperparameters,initial learning rate,of,10 ?6 and 10 ?4,initial learning rate of 10 ?6 and 10 ?4,0.6368908882141113
translation,329,91,hyperparameters,hyperparameters,use,adam optimizer,hyperparameters use adam optimizer,0.6090166568756104
translation,329,6,model,complementary strength,by,hybrid approach,complementary strength by hybrid approach,0.5989322662353516
translation,329,6,model,hybrid approach,allowing,multi-granular interactions,hybrid approach allowing multi-granular interactions,0.7114658355712891
translation,329,6,model,multi-granular interactions,represented at,word level,multi-granular interactions represented at word level,0.5908210277557373
translation,329,6,model,easy integration,with,strong word - level signals,easy integration with strong word - level signals,0.659278392791748
translation,329,7,model,micron : multigranular interaction,for,contextualizing representation,micron : multigranular interaction for contextualizing representation,0.6219081282615662
translation,329,7,model,micron : multigranular interaction,has,novel approach,micron : multigranular interaction has novel approach,0.6130593419075012
translation,329,7,model,contextualizing representation,has,novel approach,contextualizing representation has novel approach,0.6051401495933533
translation,329,7,model,model,propose,micron : multigranular interaction,model propose micron : multigranular interaction,0.6782974600791931
translation,329,10,model,word representation,with,surrounding n-grams,word representation with surrounding n-grams,0.6424969434738159
translation,329,10,model,word - based signals,for,query term weighting,word - based signals for query term weighting,0.5831061005592346
translation,329,10,model,model,by contextualizing,word representation,model by contextualizing word representation,0.7086398005485535
translation,329,44,model,multigranular interaction,to extract,important information,multigranular interaction to extract important information,0.721158504486084
translation,329,44,model,important information,from,question / passage matching,important information from question / passage matching,0.5483794212341309
translation,329,44,model,question / passage matching,by proposing,mi,question / passage matching by proposing mi,0.711323082447052
translation,329,44,model,mi,-,cron : multigranular interaction,mi - cron : multigranular interaction,0.6384260654449463
translation,329,44,model,cron : multigranular interaction,for,contextualizing representation,cron : multigranular interaction for contextualizing representation,0.6460052132606506
translation,329,44,model,mi,has,cron : multigranular interaction,mi has cron : multigranular interaction,0.6131042838096619
translation,329,44,model,model,utilize,multigranular interaction,model utilize multigranular interaction,0.6320026516914368
translation,329,45,model,model,leverage,strong word - level signals,model leverage strong word - level signals,0.7316182255744934
translation,329,94,results,results,on,wikipassageqa and insuranceqa datasets,results on wikipassageqa and insuranceqa datasets,0.49338623881340027
translation,329,95,results,proposed approach,named,micron,proposed approach named micron,0.5813739895820618
translation,329,95,results,significantly outperforms,achieving,best performance,significantly outperforms achieving best performance,0.6707302927970886
translation,329,95,results,representation - focused and interaction - focused baselines,in,various evaluation metrics,representation - focused and interaction - focused baselines in various evaluation metrics,0.4764445424079895
translation,329,95,results,representation - focused and interaction - focused baselines,achieving,best performance,representation - focused and interaction - focused baselines achieving best performance,0.6544378995895386
translation,329,95,results,best performance,in,both datasets,best performance in both datasets,0.4641151428222656
translation,329,95,results,proposed approach,has,significantly outperforms,proposed approach has significantly outperforms,0.6191551685333252
translation,329,95,results,micron,has,significantly outperforms,micron has significantly outperforms,0.6409887075424194
translation,329,95,results,significantly outperforms,has,representation - focused and interaction - focused baselines,significantly outperforms has representation - focused and interaction - focused baselines,0.6089528799057007
translation,329,95,results,results,observe,proposed approach,results observe proposed approach,0.6161628365516663
translation,329,97,results,mi - cron,allows,matching,mi - cron allows matching,0.7106437087059021
translation,329,97,results,mi - cron,achieves,improvement,mi - cron achieves improvement,0.7154392004013062
translation,329,97,results,matching,between,"different n-grams ( e.g. , 2:3 , 3:5 )","matching between different n-grams ( e.g. , 2:3 , 3:5 )",0.6259453892707825
translation,329,97,results,improvement,on,both datasets,improvement on both datasets,0.5129412412643433
translation,329,97,results,improvement,by,"4.0 % point accuracy , 2.57 % point map","improvement by 4.0 % point accuracy , 2.57 % point map",0.5693409442901611
translation,329,97,results,n-gram cnn,has,mi - cron,n-gram cnn has mi - cron,0.5777342915534973
translation,329,97,results,results,Compared to,n-gram cnn,results Compared to n-gram cnn,0.6393795013427734
translation,330,8,baselines,baselines,demonstrate,paraqg,baselines demonstrate paraqg,0.6353117823600769
translation,330,142,baselines,express,is,web application framework,express is web application framework,0.527554988861084
translation,330,142,baselines,web application framework,used for,server-side,web application framework used for server-side,0.610010027885437
translation,330,142,baselines,server-side,on top of,node.js,server-side on top of node.js,0.6766890287399292
translation,330,142,baselines,baselines,has,express,baselines has express,0.6270390748977661
translation,330,131,experimental-setup,paraqg,comprises,frontend user interface,paraqg comprises frontend user interface,0.6903668642044067
translation,330,131,experimental-setup,paraqg,comprises,backend question generator,paraqg comprises backend question generator,0.7061061859130859
translation,330,131,experimental-setup,paraqg,comprises,bert - based question filtering module,paraqg comprises bert - based question filtering module,0.6534063816070557
translation,330,132,experimental-setup,question generator model,implemented using,pytorch 2 framework,question generator model implemented using pytorch 2 framework,0.6357813477516174
translation,330,132,experimental-setup,experimental setup,has,question generator model,experimental setup has question generator model,0.5366770029067993
translation,330,133,experimental-setup,question generator model,on,"squad 1.0 ( rajpurkar et al. , 2016 ) dataset","question generator model on squad 1.0 ( rajpurkar et al. , 2016 ) dataset",0.4854624569416046
translation,330,133,experimental-setup,experimental setup,trained,question generator model,experimental setup trained question generator model,0.6945704817771912
translation,330,134,experimental-setup,pre-trained glove word vectors,of,300 dimension,pre-trained glove word vectors of 300 dimension,0.557948648929596
translation,330,134,experimental-setup,fix them,during,training,fix them during training,0.6705366969108582
translation,330,134,experimental-setup,experimental setup,use,pre-trained glove word vectors,experimental setup use pre-trained glove word vectors,0.5494672060012817
translation,330,135,experimental-setup,single - layer bi-lstm decoder,of,hidden size 600,single - layer bi-lstm decoder of hidden size 600,0.5526971817016602
translation,330,135,experimental-setup,experimental setup,employ,2 - layer bi-lstm encoder,experimental setup employ 2 - layer bi-lstm encoder,0.5250049233436584
translation,330,135,experimental-setup,experimental setup,employ,single - layer bi-lstm decoder,experimental setup employ single - layer bi-lstm decoder,0.5233239531517029
translation,330,136,experimental-setup,optimization,use,sgd,optimization use sgd,0.6316283941268921
translation,330,136,experimental-setup,sgd,with,annealing,sgd with annealing,0.7121179699897766
translation,330,136,experimental-setup,experimental setup,For,optimization,experimental setup For optimization,0.5891668200492859
translation,330,137,experimental-setup,initial learning rate,to,0.1,initial learning rate to 0.1,0.5299986600875854
translation,330,137,experimental-setup,experimental setup,set,initial learning rate,experimental setup set initial learning rate,0.634415328502655
translation,330,138,experimental-setup,model,for,20 epochs,model for 20 epochs,0.6305356025695801
translation,330,138,experimental-setup,model,with,batch size 64,model with batch size 64,0.6863330602645874
translation,330,138,experimental-setup,20 epochs,with,batch size 64,20 epochs with batch size 64,0.6123945116996765
translation,330,138,experimental-setup,experimental setup,train,model,experimental setup train model,0.6514950394630432
translation,330,139,experimental-setup,dropout,of,0.3,dropout of 0.3,0.6232042908668518
translation,330,139,experimental-setup,0.3,applied between,vertical bi-lstm stacks,0.3 applied between vertical bi-lstm stacks,0.682647168636322
translation,330,139,experimental-setup,experimental setup,has,dropout,experimental setup has dropout,0.5067690014839172
translation,330,143,experimental-setup,bert - based filtering module,finetune,bert base model,bert - based filtering module finetune bert base model,0.7525586485862732
translation,330,143,experimental-setup,bert - based filtering module,set,learning rate,bert - based filtering module set learning rate,0.6080262660980225
translation,330,143,experimental-setup,bert - based filtering module,set,batch size,bert - based filtering module set batch size,0.6413223743438721
translation,330,143,experimental-setup,bert base model,on,squad 2.0,bert base model on squad 2.0,0.5382779240608215
translation,330,143,experimental-setup,squad 2.0,for,3 epochs,squad 2.0 for 3 epochs,0.6775153875350952
translation,330,143,experimental-setup,experimental setup,For,bert - based filtering module,experimental setup For bert - based filtering module,0.6019608974456787
translation,330,9,model,paraqg,incorporates,number of novel functionalities,paraqg incorporates number of novel functionalities,0.6655856966972351
translation,330,9,model,number of novel functionalities,to make,question generation process,number of novel functionalities to make question generation process,0.571759045124054
translation,330,9,model,question generation process,has,user-friendly,question generation process has user-friendly,0.5514729022979736
translation,330,9,model,model,has,paraqg,model has paraqg,0.6355413198471069
translation,330,29,model,passage,of,text,passage of text,0.5964770317077637
translation,330,29,model,passage,as,input,passage as input,0.5894505977630615
translation,330,29,model,text,as,input,text as input,0.5475685596466064
translation,330,29,model,model,Given,passage,model Given passage,0.756255030632019
translation,330,35,model,paraqg,detects and presents,generated questions,paraqg detects and presents generated questions,0.701472282409668
translation,330,35,model,generated questions,based on,grouped / faceted view,generated questions based on grouped / faceted view,0.7017238736152649
translation,330,35,model,grouped / faceted view,of,similar answer spans,grouped / faceted view of similar answer spans,0.5926724672317505
translation,330,35,model,model,has,paraqg,model has paraqg,0.6355413198471069
translation,330,36,model,novel question filtering,based on,"bert ( devlin et al. , 2018 )","novel question filtering based on bert ( devlin et al. , 2018 )",0.6744870543479919
translation,330,36,model,unanswerable questions,from,text,unanswerable questions from text,0.5827617645263672
translation,330,57,model,paraqg,uses,novel four-stage procedure,paraqg uses novel four-stage procedure,0.6535584330558777
translation,330,57,model,automatic question generation,pertaining to,selected answer,automatic question generation pertaining to selected answer,0.6649392247200012
translation,330,57,model,filtering and grouping questions,based on,confidence scores and different facets,filtering and grouping questions based on confidence scores and different facets,0.677396297454834
translation,330,57,model,model,has,paraqg,model has paraqg,0.6355413198471069
translation,330,77,model,paragraph input,as,single fixed - length continuous vector,paragraph input as single fixed - length continuous vector,0.5568972229957581
translation,330,147,results,paraqg,filters out,unanswerble question,paraqg filters out unanswerble question,0.7977287769317627
translation,330,147,results,unanswerble question,using,bert - based model,unanswerble question using bert - based model,0.6843869090080261
translation,330,147,results,results,has,paraqg,results has paraqg,0.5724308490753174
translation,331,147,ablation-analysis,joint f1,increases by,0.30 points,joint f1 increases by 0.30 points,0.7021831274032593
translation,331,147,ablation-analysis,entity nodes,has,joint f1,entity nodes has joint f1,0.5904207825660706
translation,331,147,ablation-analysis,ablation analysis,adding,entity nodes,ablation analysis adding entity nodes,0.6807847023010254
translation,331,42,baselines,"decomprc ( min et al. , 2019",decomposes,compositional question,"decomprc ( min et al. , 2019 decomposes compositional question",0.7604649066925049
translation,331,42,baselines,"decomprc ( min et al. , 2019",leverages,single - hop mrc models,"decomprc ( min et al. , 2019 leverages single - hop mrc models",0.7001087665557861
translation,331,42,baselines,compositional question,into,simpler sub-questions,compositional question into simpler sub-questions,0.6285014152526855
translation,331,42,baselines,single - hop mrc models,to answer,sub-questions,single - hop mrc models to answer sub-questions,0.7256885766983032
translation,331,42,baselines,baselines,has,"decomprc ( min et al. , 2019","baselines has decomprc ( min et al. , 2019",0.5668476819992065
translation,331,4,model,hierarchical graph network ( hgn ),for,multi-hop question answering,hierarchical graph network ( hgn ) for multi-hop question answering,0.6080989837646484
translation,331,4,model,model,present,hierarchical graph network ( hgn ),model present hierarchical graph network ( hgn ),0.6550559997558594
translation,331,5,model,clues,from,scattered texts,clues from scattered texts,0.6332306265830994
translation,331,5,model,scattered texts,across,multiple paragraphs,scattered texts across multiple paragraphs,0.6533873081207275
translation,331,5,model,hierarchical graph,created by constructing,nodes,hierarchical graph created by constructing nodes,0.7574673891067505
translation,331,5,model,nodes,on,"different levels of granularity ( questions , paragraphs , sentences , entities )","nodes on different levels of granularity ( questions , paragraphs , sentences , entities )",0.5462849140167236
translation,331,5,model,representations,initialized with,pre-trained contextual encoders,representations initialized with pre-trained contextual encoders,0.7351062297821045
translation,331,5,model,clues,has,hierarchical graph,clues has hierarchical graph,0.6051030158996582
translation,331,5,model,scattered texts,has,hierarchical graph,scattered texts has hierarchical graph,0.5924433469772339
translation,331,5,model,multiple paragraphs,has,hierarchical graph,multiple paragraphs has hierarchical graph,0.5951949954032898
translation,331,5,model,model,To aggregate,clues,model To aggregate clues,0.8283565640449524
translation,331,6,model,initial node representations,updated,graph propagation,initial node representations updated graph propagation,0.6510364413261414
translation,331,6,model,multihop reasoning,performed via,traversing,multihop reasoning performed via traversing,0.6140229105949402
translation,331,6,model,traversing,through,graph edges,traversing through graph edges,0.6666699051856995
translation,331,6,model,graph edges,for,each subsequent sub-task,graph edges for each subsequent sub-task,0.5835229158401489
translation,331,6,model,hierarchical graph,has,initial node representations,hierarchical graph has initial node representations,0.5506391525268555
translation,331,6,model,model,Given,hierarchical graph,model Given hierarchical graph,0.6778920292854309
translation,331,7,model,heterogeneous nodes,into,integral unified graph,heterogeneous nodes into integral unified graph,0.5963730216026306
translation,331,7,model,hierarchical differentiation,of,node granularity,hierarchical differentiation of node granularity,0.5929773449897766
translation,331,7,model,node granularity,enables,hgn,node granularity enables hgn,0.7018861770629883
translation,331,7,model,hgn,to support,different question answering sub-tasks simultaneously,hgn to support different question answering sub-tasks simultaneously,0.6457307934761047
translation,331,7,model,heterogeneous nodes,has,hierarchical differentiation,heterogeneous nodes has hierarchical differentiation,0.582841157913208
translation,331,7,model,model,By weaving,heterogeneous nodes,model By weaving heterogeneous nodes,0.7360612750053406
translation,331,27,model,hierarchical graph network ( hgn ),for,multi-hop question answering,hierarchical graph network ( hgn ) for multi-hop question answering,0.6080989837646484
translation,331,27,model,hierarchical graph network ( hgn ),empowers,joint answer / evidence prediction,hierarchical graph network ( hgn ) empowers joint answer / evidence prediction,0.674990713596344
translation,331,27,model,joint answer / evidence prediction,via,multi-level fine - grained graphs,joint answer / evidence prediction via multi-level fine - grained graphs,0.6183712482452393
translation,331,27,model,multi-level fine - grained graphs,in,hierarchical framework,multi-level fine - grained graphs in hierarchical framework,0.5330153107643127
translation,331,27,model,model,propose,hierarchical graph network ( hgn ),model propose hierarchical graph network ( hgn ),0.6586160659790039
translation,331,28,model,each question,construct,hierarchical graph,each question construct hierarchical graph,0.7688964605331421
translation,331,28,model,hierarchical graph,to capture,clues,hierarchical graph to capture clues,0.651872992515564
translation,331,28,model,clues,from,sources,clues from sources,0.6052544116973877
translation,331,33,model,span prediction module,introduced,final answer prediction,span prediction module introduced final answer prediction,0.6597421169281006
translation,331,33,model,span prediction module,for,final answer prediction,span prediction module for final answer prediction,0.6031683683395386
translation,331,48,model,hde - graph,constructs,dynamic entity graph,hde - graph constructs dynamic entity graph,0.5959663987159729
translation,331,48,model,dynamic entity graph,where,each reasoning step,dynamic entity graph where each reasoning step,0.6001954078674316
translation,331,48,model,dynamic entity graph,in,each reasoning step,dynamic entity graph in each reasoning step,0.5038047432899475
translation,331,48,model,fusion module,designed to improve,interaction,fusion module designed to improve interaction,0.7583601474761963
translation,331,48,model,interaction,between,entity graph and documents,interaction between entity graph and documents,0.6529974341392517
translation,331,48,model,each reasoning step,has,irrelevant entities,each reasoning step has irrelevant entities,0.5678306818008423
translation,331,48,model,model,has,hde - graph,model has hde - graph,0.6155665516853333
translation,331,60,model,hierarchical graph network ( hgn ),consists of,four main components,hierarchical graph network ( hgn ) consists of four main components,0.6716008186340332
translation,331,60,model,graph construction module,through which,hierarchical graph,graph construction module through which hierarchical graph,0.5708512663841248
translation,331,60,model,hierarchical graph,constructed to connect,clues,hierarchical graph constructed to connect clues,0.720100998878479
translation,331,60,model,clues,from,different sources,clues from different sources,0.6283280253410339
translation,331,60,model,context encoding module,where,initial representations,context encoding module where initial representations,0.5909610390663147
translation,331,60,model,initial representations,of,graph nodes,initial representations of graph nodes,0.5668836236000061
translation,331,60,model,initial representations,obtained via,roberta - based encoder,initial representations obtained via roberta - based encoder,0.6299852728843689
translation,331,60,model,graph nodes,obtained via,roberta - based encoder,graph nodes obtained via roberta - based encoder,0.6607323288917542
translation,331,60,model,graph reasoning module,where,graph-attention - based message passing algorithm,graph reasoning module where graph-attention - based message passing algorithm,0.6413818001747131
translation,331,60,model,graph-attention - based message passing algorithm,applied to,jointly update,graph-attention - based message passing algorithm applied to jointly update,0.6976038217544556
translation,331,60,model,jointly update,has,node representations,jointly update has node representations,0.522881269454956
translation,331,60,model,model,proposed,hierarchical graph network ( hgn ),model proposed hierarchical graph network ( hgn ),0.712939977645874
translation,331,37,results,proposed model,achieves,new state of the art,proposed model achieves new state of the art,0.6358587145805359
translation,331,37,results,new state of the art,in,distractor and fullwiki settings,new state of the art in distractor and fullwiki settings,0.5122618675231934
translation,331,37,results,hotpotqa benchmark,has,proposed model,hotpotqa benchmark has proposed model,0.5560697913169861
translation,331,37,results,results,On,hotpotqa benchmark,results On hotpotqa benchmark,0.4860924780368805
translation,331,146,results,ps graph,improves,joint f1 score,ps graph improves joint f1 score,0.6838708519935608
translation,331,146,results,joint f1 score,over,plain roberta model,joint f1 score over plain roberta model,0.6567346453666687
translation,331,146,results,plain roberta model,by,2.81 points,plain roberta model by 2.81 points,0.5685914158821106
translation,331,146,results,results,use of,ps graph,results use of ps graph,0.6899065971374512
translation,331,149,results,edges,among,sentences and paragraphs,edges among sentences and paragraphs,0.6164716482162476
translation,331,149,results,final hierarchical graph,provides,additional improvement,final hierarchical graph provides additional improvement,0.6673720479011536
translation,331,149,results,additional improvement,of,0.24 points,additional improvement of 0.24 points,0.49958235025405884
translation,331,149,results,edges,has,final hierarchical graph,edges has final hierarchical graph,0.6028169989585876
translation,331,149,results,sentences and paragraphs,has,final hierarchical graph,sentences and paragraphs has final hierarchical graph,0.5949659943580627
translation,331,149,results,results,including,edges,results including edges,0.6131569147109985
translation,331,153,results,hgn variants,has,outperform,hgn variants has outperform,0.5785132050514221
translation,331,153,results,outperform,has,dfgn,outperform has dfgn,0.6366431713104248
translation,331,153,results,outperform,has,eps,outperform has eps,0.6565321087837219
translation,331,153,results,outperform,has,sae,outperform has sae,0.6262737512588501
translation,331,153,results,results,show,hgn variants,results show hgn variants,0.5329310894012451
translation,332,168,ablation-analysis,some performance boost,especially,aspect-enhanced module,some performance boost especially aspect-enhanced module,0.6388445496559143
translation,332,132,baselines,question answering models,including,"cnn ( severyn and moschitti , 2015a )","question answering models including cnn ( severyn and moschitti , 2015a )",0.6478924751281738
translation,332,132,baselines,question answering models,including,"qa -lstm ( tan et al. , 2016 )","question answering models including qa -lstm ( tan et al. , 2016 )",0.5944592952728271
translation,332,132,baselines,hcan,has,"rao et al. , 2019 )","hcan has rao et al. , 2019 )",0.5668800473213196
translation,332,134,baselines,baselines,has,retrieval - based unsupervised models,baselines has retrieval - based unsupervised models,0.5481900572776794
translation,332,136,baselines,qcem,is,unsupervised method,qcem is unsupervised method,0.5828345417976379
translation,332,136,baselines,question candidate embedding matching,is,unsupervised method,question candidate embedding matching is unsupervised method,0.5128412842750549
translation,332,136,baselines,unsupervised method,sums,word vectors,unsupervised method sums word vectors,0.7325771450996399
translation,332,136,baselines,word vectors,of,each sentence,word vectors of each sentence,0.5767431259155273
translation,332,136,baselines,word vectors,to measure,performance,word vectors to measure performance,0.6579660177230835
translation,332,136,baselines,each sentence,For,evaluation metrics,each sentence For evaluation metrics,0.5343798995018005
translation,332,136,baselines,precision at n ( p@n ),to measure,performance,precision at n ( p@n ) to measure performance,0.7279687523841858
translation,332,136,baselines,qcem,has,question candidate embedding matching,qcem has question candidate embedding matching,0.5462011694908142
translation,332,136,baselines,baselines,has,qcem,baselines has qcem,0.6077637076377869
translation,332,143,hyperparameters,sizes,of,cnn filters,sizes of cnn filters,0.5774455070495605
translation,332,143,hyperparameters,sizes,with,75 filters,sizes with 75 filters,0.6779496669769287
translation,332,143,hyperparameters,cnn filters,in,character - level embedding,cnn filters in character - level embedding,0.46699628233909607
translation,332,143,hyperparameters,cnn filters,with,75 filters,cnn filters with 75 filters,0.6684271097183228
translation,332,143,hyperparameters,character - level embedding,set to,"[ 2 , 3 , 4 , 5 ]","character - level embedding set to [ 2 , 3 , 4 , 5 ]",0.6323335766792297
translation,332,143,hyperparameters,character - level embedding,with,75 filters,character - level embedding with 75 filters,0.6380197405815125
translation,332,143,hyperparameters,"[ 2 , 3 , 4 , 5 ]",with,75 filters,"[ 2 , 3 , 4 , 5 ] with 75 filters",0.65574711561203
translation,332,143,hyperparameters,75 filters,resulting in,300d character - level embedding,75 filters resulting in 300d character - level embedding,0.6629657745361328
translation,332,143,hyperparameters,300d character - level embedding,for,each word,300d character - level embedding for each word,0.5882205963134766
translation,332,143,hyperparameters,hyperparameters,has,sizes,hyperparameters has sizes,0.5322444438934326
translation,332,144,hyperparameters,hidden dimension,of,contextaware bi-lstm encoder,hidden dimension of contextaware bi-lstm encoder,0.5391784906387329
translation,332,144,hyperparameters,contextaware bi-lstm encoder,set to,150,contextaware bi-lstm encoder set to 150,0.643010675907135
translation,332,144,hyperparameters,150,with,dropout rate,150 with dropout rate,0.5900102257728577
translation,332,144,hyperparameters,dropout rate,being,0.3,dropout rate being 0.3,0.5430317521095276
translation,332,144,hyperparameters,hyperparameters,has,hidden dimension,hyperparameters has hidden dimension,0.528815507888794
translation,332,145,hyperparameters,hidden dimension,of,sentence encoder,hidden dimension of sentence encoder,0.5761686563491821
translation,332,145,hyperparameters,sentence encoder,set to,64,sentence encoder set to 64,0.6394971013069153
translation,332,145,hyperparameters,dropout rate,being,0.3,dropout rate being 0.3,0.5430317521095276
translation,332,145,hyperparameters,hyperparameters,has,hidden dimension,hyperparameters has hidden dimension,0.528815507888794
translation,332,146,hyperparameters,hidden dimensions,of,mlp layer,hidden dimensions of mlp layer,0.5903465747833252
translation,332,146,hyperparameters,hidden dimensions,set to,300 and 100,hidden dimensions set to 300 and 100,0.7518149018287659
translation,332,146,hyperparameters,mlp layer,in,final prediction layer,mlp layer in final prediction layer,0.528154730796814
translation,332,146,hyperparameters,relu,as,activation function,relu as activation function,0.5622032880783081
translation,332,146,hyperparameters,hyperparameters,has,hidden dimensions,hyperparameters has hidden dimensions,0.5240819454193115
translation,332,147,hyperparameters,batch size,of,100,batch size of 100,0.6914901733398438
translation,332,6,model,novel framework,to tackle,pqa task,novel framework to tackle pqa task,0.6919727325439453
translation,332,6,model,pqa task,via exploiting,heterogeneous information,pqa task via exploiting heterogeneous information,0.7220777869224548
translation,332,6,model,heterogeneous information,including,natural language text and attribute - value pairs,heterogeneous information including natural language text and attribute - value pairs,0.6403705477714539
translation,332,6,model,natural language text and attribute - value pairs,from,two information sources,natural language text and attribute - value pairs from two information sources,0.5328429341316223
translation,332,6,model,two information sources,of,concerned product,two information sources of concerned product,0.5804624557495117
translation,332,6,model,two information sources,namely,product details,two information sources namely product details,0.628362774848938
translation,332,6,model,model,propose,novel framework,model propose novel framework,0.720393180847168
translation,332,7,model,heterogeneous information encoding component,obtaining,unified representations of information,heterogeneous information encoding component obtaining unified representations of information,0.6457189321517944
translation,332,7,model,unified representations of information,with,different formats,unified representations of information with different formats,0.6218827366828918
translation,332,7,model,model,has,heterogeneous information encoding component,model has heterogeneous information encoding component,0.5173975229263306
translation,332,35,model,novel framework,for,pqa task,novel framework for pqa task,0.6149401664733887
translation,332,35,model,pqa task,using,heterogenous information,pqa task using heterogenous information,0.6697965860366821
translation,332,35,model,heterogenous information,via,weak supervision paradigm ( hiws ),heterogenous information via weak supervision paradigm ( hiws ),0.6579747796058655
translation,332,35,model,model,propose,novel framework,model propose novel framework,0.720393180847168
translation,332,36,model,hiws,exploits,corresponding product details,hiws exploits corresponding product details,0.7524064183235168
translation,332,36,model,hiws,exploits,user reviews,hiws exploits user reviews,0.7565227150917053
translation,332,36,model,user reviews,to return,ranked snippet list,user reviews to return ranked snippet list,0.614819347858429
translation,332,36,model,ranked snippet list,serving as,response,ranked snippet list serving as response,0.6334097981452942
translation,332,36,model,product-related question,has,hiws,product-related question has hiws,0.6153892874717712
translation,332,36,model,model,Given,product-related question,model Given product-related question,0.7048633098602295
translation,332,40,model,weak supervision paradigm,making use of,original user-posted answers,weak supervision paradigm making use of original user-posted answers,0.5997214913368225
translation,332,40,model,original user-posted answers,during,training,original user-posted answers during training,0.6332252621650696
translation,332,40,model,model,develop,weak supervision paradigm,model develop weak supervision paradigm,0.6035056114196777
translation,332,167,model,important components,in,hiws,important components in hiws,0.553226113319397
translation,332,167,model,hiws,are,aspect-enhanced representations,hiws are aspect-enhanced representations,0.5819997191429138
translation,332,167,model,hiws,are,question intent matching,hiws are question intent matching,0.539193332195282
translation,332,150,results,our proposed hiws,achieves,best performance,our proposed hiws achieves best performance,0.728813648223877
translation,332,150,results,best performance,among,all evaluation metrics,best performance among all evaluation metrics,0.5533827543258667
translation,332,150,results,best performance,compared with,supervised qa matching methods,best performance compared with supervised qa matching methods,0.6615096926689148
translation,332,150,results,results,demonstrates,our proposed hiws,results demonstrates our proposed hiws,0.7043652534484863
translation,332,151,results,some simple qa models,such as,qa - lstm,some simple qa models such as qa - lstm,0.6274213790893555
translation,332,151,results,unsupervised models,such as,qcem,unsupervised models such as qcem,0.6304110884666443
translation,332,151,results,unsupervised models,achieve,reasonable performance,unsupervised models achieve reasonable performance,0.5756792426109314
translation,332,151,results,results,observe,some simple qa models,results observe some simple qa models,0.62457674741745
translation,332,151,results,results,observe,unsupervised models,results observe unsupervised models,0.5922848582267761
translation,332,152,results,state - of- theart models,such as,bimpm and hcan,state - of- theart models such as bimpm and hcan,0.6463950276374817
translation,332,152,results,state - of- theart models,do not perform,promising,state - of- theart models do not perform promising,0.672746479511261
translation,332,152,results,results,For,state - of- theart models,results For state - of- theart models,0.5755234360694885
translation,332,160,results,model variants,observe that,models,model variants observe that models,0.654029369354248
translation,332,160,results,models,trained with,original qa pairs,models trained with original qa pairs,0.7541993260383606
translation,332,160,results,models,perform,quite worse,models perform quite worse,0.5766614079475403
translation,332,160,results,results,Comparing,model variants,results Comparing model variants,0.6333330869674683
translation,332,161,results,outperform,models with,qa,outperform models with qa,0.7185080647468567
translation,332,161,results,qa,via,fine-tuning,qa via fine-tuning,0.7231696844100952
translation,332,161,results,fine-tuning,with,proper data,fine-tuning with proper data,0.6747554540634155
translation,332,161,results,sqs,has,outperform,sqs has outperform,0.6817633509635925
translation,332,162,results,performance,for,all models,performance for all models,0.5856176614761353
translation,332,162,results,improved,with,our proposed weak supervision paradigm,improved with our proposed weak supervision paradigm,0.611534059047699
translation,332,162,results,improved,demonstrating,effectiveness,improved demonstrating effectiveness,0.7525179386138916
translation,332,162,results,results,has,performance,results has performance,0.5972660779953003
translation,333,179,baselines,context- independent embeddings,from,feed-forward lms,context- independent embeddings from feed-forward lms,0.551545262336731
translation,333,179,baselines,elmo,has,context-dependent bidirectional lstm,elmo has context-dependent bidirectional lstm,0.553492546081543
translation,333,160,experimental-setup,default optimizer,from,bert,default optimizer from bert,0.5804546475410461
translation,333,160,experimental-setup,experimental setup,use,default optimizer,experimental setup use default optimizer,0.6302392482757568
translation,333,161,experimental-setup,retriever,with,ict,retriever with ict,0.7498871088027954
translation,333,161,experimental-setup,retriever,use,learning rate,retriever use learning rate,0.6432222723960876
translation,333,161,experimental-setup,retriever,use,batch size,retriever use batch size,0.6591309309005737
translation,333,161,experimental-setup,learning rate,of,10 ?4,learning rate of 10 ?4,0.6496500372886658
translation,333,161,experimental-setup,batch size,of,4096,batch size of 4096,0.6386147141456604
translation,333,161,experimental-setup,4096,on,google cloud tpus,4096 on google cloud tpus,0.5590006709098816
translation,333,161,experimental-setup,google cloud tpus,for,100k steps,google cloud tpus for 100k steps,0.6430670619010925
translation,333,161,experimental-setup,experimental setup,pre-training,retriever,experimental setup pre-training retriever,0.6969509124755859
translation,333,162,experimental-setup,finetuning,use,learning rate,finetuning use learning rate,0.6465399861335754
translation,333,162,experimental-setup,finetuning,use,batch size,finetuning use batch size,0.6874542832374573
translation,333,162,experimental-setup,learning rate,of,10 ?5,learning rate of 10 ?5,0.6583234071731567
translation,333,162,experimental-setup,batch size,of,1,batch size of 1,0.6655300855636597
translation,333,162,experimental-setup,1,on,single machine,1 on single machine,0.5627434849739075
translation,333,162,experimental-setup,single machine,with,12gb gpu,single machine with 12gb gpu,0.5876254439353943
translation,333,162,experimental-setup,experimental setup,When,finetuning,experimental setup When finetuning,0.656631588935852
translation,333,163,experimental-setup,answer spans,limited to,10 tokens,answer spans limited to 10 tokens,0.6628985404968262
translation,333,163,experimental-setup,experimental setup,has,answer spans,experimental setup has answer spans,0.6043761968612671
translation,333,24,experiments,orqa,retrieve,evidence,orqa retrieve evidence,0.6859499216079712
translation,333,24,experiments,orqa,supervised only by,questionanswer string pairs,orqa supervised only by questionanswer string pairs,0.6688019633293152
translation,333,24,experiments,evidence,from,open corpus,evidence from open corpus,0.5967952609062195
translation,333,178,experiments,unsupervised pooled representations,from,neural language models ( lm ),unsupervised pooled representations from neural language models ( lm ),0.5443626642227173
translation,333,7,model,evidence retrieval,from,all of wikipedia,evidence retrieval from all of wikipedia,0.607232391834259
translation,333,7,model,evidence retrieval,treated as,latent variable,evidence retrieval treated as latent variable,0.6475176811218262
translation,333,84,model,pre-trained retriever,pre-encode,all evidence blocks,pre-trained retriever pre-encode all evidence blocks,0.8138866424560547
translation,333,84,model,all evidence blocks,from,wikipedia,all evidence blocks from wikipedia,0.6274431347846985
translation,333,84,model,all evidence blocks,enabling,dynamic yet fast top-k retrieval,all evidence blocks enabling dynamic yet fast top-k retrieval,0.7294402122497559
translation,333,84,model,dynamic yet fast top-k retrieval,during,fine-tuning,dynamic yet fast top-k retrieval during fine-tuning,0.6410916447639465
translation,333,84,model,bias,away from,supportive evidence,bias away from supportive evidence,0.37676823139190674
translation,333,84,model,bias,towards,supportive evidence,bias towards supportive evidence,0.3466759920120239
translation,333,84,model,retrieval,away from,spurious ambiguities,retrieval away from spurious ambiguities,0.6885720491409302
translation,333,84,model,retrieval,away from,supportive evidence,retrieval away from supportive evidence,0.6891871094703674
translation,333,84,model,retrieval,towards,supportive evidence,retrieval towards supportive evidence,0.6660928130149841
translation,333,84,model,bias,has,retrieval,bias has retrieval,0.5768917202949524
translation,333,84,model,model,has,pre-trained retriever,model has pre-trained retriever,0.5897770524024963
translation,333,184,results,first result,to note,bm25,first result to note bm25,0.5699332356452942
translation,333,184,results,bm25,is,powerful retrieval system,bm25 is powerful retrieval system,0.5362030863761902
translation,333,184,results,results,has,first result,results has first result,0.554772675037384
translation,334,222,baselines,kullback leibler divergence,between,summary and input,kullback leibler divergence between summary and input,0.6145874261856079
translation,334,222,baselines,kls -i,has,kullback leibler divergence,kls -i has kullback leibler divergence,0.5606276988983154
translation,334,222,baselines,baselines,has,kls -i,baselines has kls -i,0.6019243597984314
translation,334,223,baselines,jensen shannon divergence,between,input and summary,jensen shannon divergence between input and summary,0.6252150535583496
translation,334,223,baselines,unjsd,has,jensen shannon divergence,unjsd has jensen shannon divergence,0.567655622959137
translation,334,223,baselines,baselines,has,unjsd,baselines has unjsd,0.6183047294616699
translation,334,5,experiments,automatically annotated,using,coresc scheme,automatically annotated using coresc scheme,0.6542304754257202
translation,334,5,experiments,coresc scheme,captures,11 contentbased concepts,coresc scheme captures 11 contentbased concepts,0.747110903263092
translation,334,5,experiments,11 contentbased concepts,such as,hypothesis,11 contentbased concepts such as hypothesis,0.6554808020591736
translation,334,5,experiments,11 contentbased concepts,such as,"result , conclusion","11 contentbased concepts such as result , conclusion",0.5921162962913513
translation,334,5,experiments,11 contentbased concepts,such as,sentence level,11 contentbased concepts such as sentence level,0.5920062065124512
translation,334,5,experiments,11 contentbased concepts,at,sentence level,11 contentbased concepts at sentence level,0.5184655785560608
translation,334,27,experiments,scientific discourse,to create,content model,scientific discourse to create content model,0.7350559234619141
translation,334,27,experiments,content model,for,extractive summarisation,content model for extractive summarisation,0.5583218932151794
translation,334,27,experiments,content,of,full paper,content of full paper,0.5848463177680969
translation,334,27,experiments,cohesion,of,narrative,cohesion of narrative,0.5952351093292236
translation,334,4,model,automatically generated scientific discourse annotations,to create,content model,automatically generated scientific discourse annotations to create content model,0.6281008124351501
translation,334,4,model,content model,has,for the summarisation of scientific articles,content model has for the summarisation of scientific articles,0.5317137837409973
translation,334,28,model,articles,with,scheme,articles with scheme,0.6723880171775818
translation,334,28,model,scheme,captures,fine- grained aspects,scheme captures fine- grained aspects,0.7802704572677612
translation,334,28,model,fine- grained aspects,of,content and conceptual structure,fine- grained aspects of content and conceptual structure,0.5741667747497559
translation,334,28,model,content and conceptual structure,of,papers,content and conceptual structure of papers,0.5530386567115784
translation,334,28,model,papers,namely,core scientific concepts ( coresc ) scheme,papers namely core scientific concepts ( coresc ) scheme,0.74228835105896
translation,334,28,model,model,automatically annotate,articles,model automatically annotate articles,0.7377313375473022
translation,334,37,results,answer,with,75 % precision,answer with 75 % precision,0.6605761647224426
translation,334,37,results,66 %,of,complex questions,66 % of complex questions,0.5359504222869873
translation,334,37,results,66 %,with,75 % precision,66 % with 75 % precision,0.6681197285652161
translation,334,37,results,complex questions,with,75 % precision,complex questions with 75 % precision,0.6290932297706604
translation,334,37,results,baseline,of,microsoft autosummarise summaries,baseline of microsoft autosummarise summaries,0.5644779205322266
translation,334,37,results,automatically generated coresc summaries,has,answer,automatically generated coresc summaries has answer,0.6114581227302551
translation,334,37,results,answer,has,66 %,answer has 66 %,0.5753762125968933
translation,334,37,results,outperforming,has,baseline,outperforming has baseline,0.6440464854240417
translation,334,37,results,results,show,automatically generated coresc summaries,results show automatically generated coresc summaries,0.6089633107185364
translation,334,186,results,rouge,showed,summaries c,rouge showed summaries c,0.761225163936615
translation,334,186,results,slightly higher rouge - 1 measure,than,summaries b ( 0.75 vs 0.73 ),slightly higher rouge - 1 measure than summaries b ( 0.75 vs 0.73 ),0.5584531426429749
translation,334,186,results,slightly higher rouge - 1 measure,with respect to,abstracts,slightly higher rouge - 1 measure with respect to abstracts,0.7235901951789856
translation,334,186,results,same,for,two ( 0.70 ),same for two ( 0.70 ),0.60194331407547
translation,334,200,results,automated corescs,are,informative,automated corescs are informative,0.6614927053451538
translation,334,200,results,automated corescs,enable,experts,automated corescs enable experts,0.7056661248207092
translation,334,200,results,answer,with,precision,answer with precision,0.6356071829795837
translation,334,200,results,66 %,of,complex questions,66 % of complex questions,0.5359504222869873
translation,334,200,results,66 %,with,precision,66 % with precision,0.6825393438339233
translation,334,200,results,precision,of,75 %,precision of 75 %,0.5993586182594299
translation,334,200,results,outperform,has,microsoft autosummarise summaries,outperform has microsoft autosummarise summaries,0.5917282700538635
translation,334,200,results,answer,has,66 %,answer has 66 %,0.5753762125968933
translation,335,224,ablation-analysis,type vector based representation,leads to,better performance,type vector based representation leads to better performance,0.6606484055519104
translation,335,224,ablation-analysis,better performance,especially when,n,better performance especially when n,0.6661965250968933
translation,335,224,ablation-analysis,n,-,gram pruning,n - gram pruning,0.608296811580658
translation,335,224,ablation-analysis,n,has,gram pruning,n has gram pruning,0.579030454158783
translation,335,189,baselines,prune,has,search space,prune has search space,0.5652196407318115
translation,335,189,baselines,baselines,consider,two baseline methods,baselines consider two baseline methods,0.6626171469688416
translation,335,190,baselines,first baseline,is,n-,first baseline is n-,0.6060510277748108
translation,335,190,baselines,gram pruning method,establishes,candidate pool,gram pruning method establishes candidate pool,0.6122226715087891
translation,335,190,baselines,candidate pool,by retaining,subject-relation pairs,candidate pool by retaining subject-relation pairs,0.6444864273071289
translation,335,190,baselines,n-,has,gram pruning method,n- has gram pruning method,0.561366856098175
translation,335,190,baselines,baselines,has,first baseline,baselines has first baseline,0.595583438873291
translation,335,191,baselines,n,-,gram +,n - gram +,0.6899389624595642
translation,335,191,baselines,n,has,gram +,n has gram +,0.6313521265983582
translation,335,196,baselines,first baseline,is,embedding average model ( embed - avg ),first baseline is embedding average model ( embed - avg ),0.5149657130241394
translation,335,196,baselines,embedding average model ( embed - avg ),takes,element- wise average,embedding average model ( embed - avg ) takes element- wise average,0.6050935983657837
translation,335,196,baselines,element- wise average,of,word embeddings,element- wise average of word embeddings,0.5667338967323303
translation,335,196,baselines,word embeddings,of,question,word embeddings of question,0.5739884972572327
translation,335,196,baselines,word embeddings,to be,question representation,word embeddings to be question representation,0.49529144167900085
translation,335,196,baselines,question,to be,question representation,question to be question representation,0.582125186920166
translation,335,196,baselines,baselines,has,first baseline,baselines has first baseline,0.595583438873291
translation,335,2,experiments,cfo,has,conditional focused neural question answering,cfo has conditional focused neural question answering,0.5483044385910034
translation,335,206,hyperparameters,experiment setting,During,training,experiment setting During training,0.6668798327445984
translation,335,206,hyperparameters,all word embeddings,initialized using,pretrained glove,all word embeddings initialized using pretrained glove,0.7135255932807922
translation,335,206,hyperparameters,all word embeddings,fine tuned in,subsequent training,all word embeddings fine tuned in subsequent training,0.6832926273345947
translation,335,206,hyperparameters,experiment setting,has,all word embeddings,experiment setting has all word embeddings,0.4971931278705597
translation,335,206,hyperparameters,training,has,all word embeddings,training has all word embeddings,0.5496209263801575
translation,335,206,hyperparameters,hyperparameters,has,experiment setting,hyperparameters has experiment setting,0.488952100276947
translation,335,207,hyperparameters,word embedding dimension,set to,300,word embedding dimension set to 300,0.7002131342887878
translation,335,207,hyperparameters,bigru hidden size,has,256,bigru hidden size has 256,0.6365854144096375
translation,335,207,hyperparameters,hyperparameters,has,word embedding dimension,hyperparameters has word embedding dimension,0.4810734987258911
translation,335,207,hyperparameters,hyperparameters,has,bigru hidden size,hyperparameters has bigru hidden size,0.5414251685142517
translation,335,208,hyperparameters,entity embeddings,using,transe,entity embeddings using transe,0.6938348412513733
translation,335,208,hyperparameters,triples,included in,fb5 m,triples included in fb5 m,0.7258710861206055
translation,335,208,hyperparameters,pretraining,has,entity embeddings,pretraining has entity embeddings,0.4935023784637451
translation,335,208,hyperparameters,hyperparameters,For,pretraining,hyperparameters For pretraining,0.518390417098999
translation,335,209,hyperparameters,other parameters,randomly initialized uniformly from,"[?0.08 , 0.08 ]","other parameters randomly initialized uniformly from [?0.08 , 0.08 ]",0.6653892993927002
translation,335,209,hyperparameters,hyperparameters,has,other parameters,hyperparameters has other parameters,0.4744566082954407
translation,335,210,hyperparameters,hinge loss margins,set to,0.1,hinge loss margins set to 0.1,0.6537840366363525
translation,335,210,hyperparameters,hyperparameters,has,hinge loss margins,hyperparameters has hinge loss margins,0.4859668016433716
translation,335,211,hyperparameters,hyperparameters,has,negative sampling sizes m s and m r,hyperparameters has negative sampling sizes m s and m r,0.5295796990394592
translation,335,212,hyperparameters,parameters,trained using,mini-,parameters trained using mini-,0.6950954794883728
translation,335,212,hyperparameters,batch adagrad,),"momentum ( pham et al. , 2015 )","batch adagrad ) momentum ( pham et al. , 2015 )",0.5168526768684387
translation,335,212,hyperparameters,batch adagrad,with,"momentum ( pham et al. , 2015 )","batch adagrad with momentum ( pham et al. , 2015 )",0.5504564046859741
translation,335,212,hyperparameters,optimization,has,parameters,optimization has parameters,0.49972158670425415
translation,335,212,hyperparameters,mini-,has,batch adagrad,mini- has batch adagrad,0.5586113929748535
translation,335,212,hyperparameters,hyperparameters,For,optimization,hyperparameters For optimization,0.5834490656852722
translation,335,213,hyperparameters,learning rates,tuned to be,0.001,learning rates tuned to be 0.001,0.7046268582344055
translation,335,213,hyperparameters,learning rates,tuned to be,0.03,learning rates tuned to be 0.03,0.7011922001838684
translation,335,213,hyperparameters,learning rates,tuned to be,0.02,learning rates tuned to be 0.02,0.6913477778434753
translation,335,213,hyperparameters,0.001,for,question embedding,0.001 for question embedding,0.6140847206115723
translation,335,213,hyperparameters,question embedding,with,type vector,question embedding with type vector,0.6100177764892578
translation,335,213,hyperparameters,0.03,for,ltg - cnn methods,0.03 for ltg - cnn methods,0.5664470791816711
translation,335,213,hyperparameters,0.02,for,rest of the models,0.02 for rest of the models,0.6102377772331238
translation,335,213,hyperparameters,hyperparameters,has,learning rates,hyperparameters has learning rates,0.48709791898727417
translation,335,214,hyperparameters,momentum rate,set to,0.9,momentum rate set to 0.9,0.6969946622848511
translation,335,214,hyperparameters,0.9,for,all models,0.9 for all models,0.5777945518493652
translation,335,214,hyperparameters,mini-batch size,is,256,mini-batch size is 256,0.5855526328086853
translation,335,214,hyperparameters,hyperparameters,has,momentum rate,hyperparameters has momentum rate,0.49931901693344116
translation,335,214,hyperparameters,hyperparameters,has,mini-batch size,hyperparameters has mini-batch size,0.4961773455142975
translation,335,215,hyperparameters,vertical dropout,to regularize,all bi-grus,vertical dropout to regularize all bi-grus,0.738726019859314
translation,335,215,hyperparameters,hyperparameters,has,vertical dropout,hyperparameters has vertical dropout,0.5277743339538574
translation,335,34,model,cfo,has,novel method,cfo has novel method,0.6352782249450684
translation,335,34,model,novel method,has,to answer single -fact questions,novel method has to answer single -fact questions,0.5416966080665588
translation,335,34,model,model,propose,cfo,model propose cfo,0.7188754081726074
translation,335,192,model,heuristics,related to,overlapping,heuristics related to overlapping,0.7679812908172607
translation,335,192,model,heuristics,exploited to further shrink,n-gram pool,heuristics exploited to further shrink n-gram pool,0.7386383414268494
translation,335,220,results,cfo ( focused pruning + bigru + type vector ),achieves,best performance,cfo ( focused pruning + bigru + type vector ) achieves best performance,0.6443052887916565
translation,335,220,results,all other methods,by,substantial margins,all other methods by substantial margins,0.5711597204208374
translation,335,220,results,outperforming,has,all other methods,outperforming has all other methods,0.5718973278999329
translation,335,220,results,results,has,cfo ( focused pruning + bigru + type vector ),results has cfo ( focused pruning + bigru + type vector ),0.5453320741653442
translation,335,221,results,bigru based relation scoring network,boosts,accuracy,bigru based relation scoring network boosts accuracy,0.6757237315177917
translation,335,221,results,accuracy,by,3.5 % to 4.8 %,accuracy by 3.5 % to 4.8 %,0.5719230771064758
translation,335,221,results,accuracy,compared to,second best alternative,accuracy compared to second best alternative,0.7049799561500549
translation,335,223,results,embed - avg,achieves,better performance,embed - avg achieves better performance,0.7128713130950928
translation,335,223,results,better performance,than,more complex ltg - cnn,better performance than more complex ltg - cnn,0.6050155758857727
translation,335,227,results,all systems,using,proposed focused pruning method,all systems using proposed focused pruning method,0.7334486246109009
translation,335,227,results,outperform,counterparts with,alternative pruning methods,outperform counterparts with alternative pruning methods,0.7331401705741882
translation,335,227,results,proposed focused pruning method,has,outperform,proposed focused pruning method has outperform,0.6158680319786072
translation,335,227,results,results,has,all systems,results has all systems,0.5592814683914185
translation,335,228,results,cfo,already better than,memory network ensembles,cfo already better than memory network ensembles,0.6441698670387268
translation,335,228,results,memory network ensembles,by,11.8 %,memory network ensembles by 11.8 %,0.5624407529830933
translation,335,228,results,ensembles,has,cfo,ensembles has cfo,0.6240893602371216
translation,335,228,results,results,Without using,ensembles,results Without using ensembles,0.6217004656791687
translation,335,235,results,focused pruning,achieves,comparable recall rate,focused pruning achieves comparable recall rate,0.6538405418395996
translation,335,235,results,comparable recall rate,to,n,comparable recall rate to n,0.54392009973526
translation,335,235,results,n,-,gram pruning,n - gram pruning,0.608296811580658
translation,335,235,results,results,has,focused pruning,results has focused pruning,0.5779368877410889
translation,335,253,results,rnn - crf,achieves,accuracy,rnn - crf achieves accuracy,0.6985914707183838
translation,335,253,results,rnn - crf,achieves,accuracy,rnn - crf achieves accuracy,0.6985914707183838
translation,335,253,results,accuracy,of,95.5 %,accuracy of 95.5 %,0.5564337372779846
translation,335,253,results,accuracy,of,feature based crf,accuracy of feature based crf,0.578170657157898
translation,335,253,results,accuracy,of,feature based crf,accuracy of feature based crf,0.578170657157898
translation,335,253,results,feature based crf,is,91.2 %,feature based crf is 91.2 %,0.518415629863739
translation,335,253,results,results,turns out,rnn - crf,results turns out rnn - crf,0.6028493046760559
translation,335,261,results,bigru,employed as,subject network,bigru employed as subject network,0.6754389405250549
translation,335,261,results,accuracy,is,consistently higher,accuracy is consistently higher,0.604989767074585
translation,335,261,results,consistently higher,regardless of,relation network structures,consistently higher regardless of relation network structures,0.7071380615234375
translation,335,261,results,bigru,has,accuracy,bigru has accuracy,0.6037951707839966
translation,336,186,ablation-analysis,textrank algorithm,for,parallel corpus preprocessing,textrank algorithm for parallel corpus preprocessing,0.5643489360809326
translation,336,186,ablation-analysis,average number of translations per word,reduced from,69 to 24,average number of translations per word reduced from 69 to 24,0.6762126684188843
translation,336,186,ablation-analysis,performance,of,question retrieval,performance of question retrieval,0.595305323600769
translation,336,186,ablation-analysis,performance,is,significantly improved,performance is significantly improved,0.611599326133728
translation,336,186,ablation-analysis,textrank algorithm,has,average number of translations per word,textrank algorithm has average number of translations per word,0.5418914556503296
translation,336,186,ablation-analysis,parallel corpus preprocessing,has,average number of translations per word,parallel corpus preprocessing has average number of translations per word,0.5543307662010193
translation,336,186,ablation-analysis,ablation analysis,using,textrank algorithm,ablation analysis using textrank algorithm,0.65867018699646
translation,336,7,model,novel phrase - based translation model,for,question retrieval,novel phrase - based translation model for question retrieval,0.548186719417572
translation,336,7,model,model,propose,novel phrase - based translation model,model propose novel phrase - based translation model,0.5980086326599121
translation,336,27,model,phrasebased translation model ( p- trans ),for,question retrieval,phrasebased translation model ( p- trans ) for question retrieval,0.5659911632537842
translation,336,27,model,model,propose,phrasebased translation model ( p- trans ),model propose phrasebased translation model ( p- trans ),0.640925943851471
translation,336,196,model,novel phrase - based translation model,for,question retrieval,novel phrase - based translation model for question retrieval,0.548186719417572
translation,336,196,model,model,propose,novel phrase - based translation model,model propose novel phrase - based translation model,0.5980086326599121
translation,336,8,results,phrasebased translation model,is,more effective,phrasebased translation model is more effective,0.4901358187198639
translation,336,172,results,word - based translation language model ( translm ),has,significantly outperforms,word - based translation language model ( translm ) has significantly outperforms,0.21591311693191528
translation,336,172,results,significantly outperforms,has,word - based translation model,significantly outperforms has word - based translation model,0.5516760349273682
translation,336,173,results,answer part,into,models,answer part into models,0.6563791632652283
translation,336,173,results,answer part,either,word - based or phrase - based,answer part either word - based or phrase - based,0.7790465950965881
translation,336,173,results,models,either,word - based or phrase - based,models either word - based or phrase - based,0.759295642375946
translation,336,173,results,performance,of,question retrieval,performance of question retrieval,0.595305323600769
translation,336,173,results,significantly improve,has,performance,significantly improve has performance,0.5721634030342102
translation,336,173,results,results,Incorporating,answer part,results Incorporating answer part,0.6801358461380005
translation,336,174,results,proposed phrase - based translation model ( p- trans ),has,significantly outperforms,proposed phrase - based translation model ( p- trans ) has significantly outperforms,0.5833465456962585
translation,336,174,results,significantly outperforms,has,state - of- theart word - based translation models,significantly outperforms has state - of- theart word - based translation models,0.5467074513435364
translation,336,174,results,results,has,proposed phrase - based translation model ( p- trans ),results has proposed phrase - based translation model ( p- trans ),0.5764061212539673
translation,336,176,results,proposed phrase - based translation model,is,more effective,proposed phrase - based translation model is more effective,0.5011022686958313
translation,336,176,results,more effective,than,state - of - the - art word - based translation models,more effective than state - of - the - art word - based translation models,0.5128955245018005
translation,336,176,results,results,has,proposed phrase - based translation model,results has proposed phrase - based translation model,0.5643675327301025
translation,336,178,results,longer phrases,up to,maximum length of five,longer phrases up to maximum length of five,0.6422960758209229
translation,336,178,results,longer phrases,consistently improve,retrieval performance,longer phrases consistently improve retrieval performance,0.6980580687522888
translation,336,178,results,results,using,longer phrases,results using longer phrases,0.6541345119476318
translation,336,194,results,two baseline methods,for,question retrieval,two baseline methods for question retrieval,0.5629897117614746
translation,336,194,results,pooling strategy,has,significantly outperforms,pooling strategy has significantly outperforms,0.6103456616401672
translation,336,194,results,significantly outperforms,has,two baseline methods,significantly outperforms has two baseline methods,0.5793110728263855
translation,336,194,results,results,see that,pooling strategy,results see that pooling strategy,0.5858340859413147
translation,336,197,results,proposed approach,is,more effective,proposed approach is more effective,0.5907676219940186
translation,336,197,results,more effective,capture,contextual information,more effective capture contextual information,0.7226186990737915
translation,336,197,results,contextual information,instead of translating,single words,contextual information instead of translating single words,0.6798234581947327
translation,337,66,baselines,two baselines,selects,answer randomly,two baselines selects answer randomly,0.7508476376533508
translation,337,66,baselines,two baselines,selects,candidate retrieval ( cr ) baseline,two baselines selects candidate retrieval ( cr ) baseline,0.6908023357391357
translation,337,66,baselines,baseline,selects,answer randomly,baseline selects answer randomly,0.7777416110038757
translation,337,68,experimental-setup,gensim 7 implementation,of,dbow and dm paragraph vector models,gensim 7 implementation of dbow and dm paragraph vector models,0.580856442451477
translation,337,68,experimental-setup,experimental setup,use,gensim 7 implementation,experimental setup use gensim 7 implementation,0.5849611163139343
translation,337,69,experimental-setup,word embeddings,for,skipavg model,word embeddings for skipavg model,0.5302181839942932
translation,337,69,experimental-setup,word embeddings,obtained with,word2vec,word embeddings obtained with word2vec,0.5299643278121948
translation,337,69,experimental-setup,experimental setup,has,word embeddings,experimental setup has word embeddings,0.4932878315448761
translation,337,71,experimental-setup,tokenized,with,stanford tokenizer,tokenized with stanford tokenizer,0.6313706040382385
translation,337,71,experimental-setup,tokenized,with,lowercased,tokenized with lowercased,0.7134596705436707
translation,337,71,experimental-setup,experimental setup,has,data,experimental setup has data,0.5174660086631775
translation,337,36,hyperparameters,stochastic gradient descent,to minimize,loss,stochastic gradient descent to minimize loss,0.6975611448287964
translation,337,36,hyperparameters,loss,over,training set,loss over training set,0.6531999111175537
translation,337,36,hyperparameters,hyperparameters,use,stochastic gradient descent,hyperparameters use stochastic gradient descent,0.5965404510498047
translation,337,4,model,non-factoid answer reranking,whereby,question - answer pairs,non-factoid answer reranking whereby question - answer pairs,0.5983400344848633
translation,337,4,model,question - answer pairs,represented by,concatenated distributed representation vectors,question - answer pairs represented by concatenated distributed representation vectors,0.7300758361816406
translation,337,4,model,multilayer perceptron,to compute,score,multilayer perceptron to compute score,0.7561169266700745
translation,337,4,model,score,for,answer,score for answer,0.5824374556541443
translation,337,22,results,state - of- theart performance,has,37.17 p@1,state - of- theart performance has 37.17 p@1,0.5530608892440796
translation,337,22,results,results,achieve,state - of- theart performance,results achieve state - of- theart performance,0.61161208152771
translation,337,75,results,best development p@1 and mrr,of,multilayer perceptron,best development p@1 and mrr of multilayer perceptron,0.6012207269668579
translation,337,75,results,multilayer perceptron,trained on,yahoo,multilayer perceptron trained on yahoo,0.7129916548728943
translation,337,75,results,results,report,best development p@1 and mrr,results report best development p@1 and mrr,0.6795879006385803
translation,337,78,results,distributed representations,including,skipavg model,distributed representations including skipavg model,0.6975660920143127
translation,337,78,results,distributed representations,beat,random and candidate retrieval baselines,distributed representations beat random and candidate retrieval baselines,0.6841695308685303
translation,337,78,results,random and candidate retrieval baselines,by,large and statistically significant margin,random and candidate retrieval baselines by large and statistically significant margin,0.5758645534515381
translation,337,78,results,results,has,distributed representations,results has distributed representations,0.5272979140281677
translation,337,79,results,multilayer perceptron with dbow and dm representations,has,significantly outperform,multilayer perceptron with dbow and dm representations has significantly outperform,0.5842267870903015
translation,337,79,results,significantly outperform,has,skipavg representations,significantly outperform has skipavg representations,0.596255898475647
translation,337,79,results,results,has,multilayer perceptron with dbow and dm representations,results has multilayer perceptron with dbow and dm representations,0.5674872398376465
translation,337,80,results,outperforming,by,small margin,outperforming by small margin,0.6272038221359253
translation,337,80,results,rest,by,small margin,rest by small margin,0.5733758211135864
translation,337,80,results,both paragraph vector representations,has,outperforming,both paragraph vector representations has outperforming,0.5976696014404297
translation,337,80,results,outperforming,has,rest,outperforming has rest,0.5940765142440796
translation,337,80,results,results,has,both paragraph vector representations,results has both paragraph vector representations,0.49989452958106995
translation,337,81,results,slightly higher p@1 and mrr,on,development set,slightly higher p@1 and mrr on development set,0.5915064811706543
translation,337,81,results,results,has,multilayer perceptron,results has multilayer perceptron,0.5506366491317749
translation,337,85,results,outperforms,by,statistically significant margin,outperforms by statistically significant margin,0.6818264722824097
translation,337,85,results,baselines,by,statistically significant margin,baselines by statistically significant margin,0.6017391681671143
translation,337,85,results,dbow,has,outperforms,dbow has outperforms,0.6632428765296936
translation,337,85,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
translation,337,85,results,results,has,dbow,results has dbow,0.646782636642456
translation,337,86,results,"combination of the dbow , dm and skipavg models",provides,slightly better results,"combination of the dbow , dm and skipavg models provides slightly better results",0.6031246781349182
translation,337,86,results,improvement,over,dbow,improvement over dbow,0.6863924860954285
translation,337,86,results,results,has,"combination of the dbow , dm and skipavg models","results has combination of the dbow , dm and skipavg models",0.5267909169197083
translation,337,93,results,answers data,are,more beneficial,answers data are more beneficial,0.6003938913345337
translation,337,93,results,more beneficial,than using,much larger gigaword dataset,more beneficial than using much larger gigaword dataset,0.6961492300033569
translation,337,93,results,results,has,answers data,results has answers data,0.5187066793441772
translation,338,147,baselines,baselines,has,rule based method,baselines has rule based method,0.6023309230804443
translation,338,151,baselines,machine learning method,trained from,rule- based features,machine learning method trained from rule- based features,0.7538739442825317
translation,338,151,baselines,machine learning method,trained from,facts,machine learning method trained from facts,0.8030590415000916
translation,338,151,baselines,facts,from,common-sense knowledge base,facts from common-sense knowledge base,0.5253739356994629
translation,338,151,baselines,rb +cb +ml,has,machine learning method,rb +cb +ml has machine learning method,0.5837507843971252
translation,338,151,baselines,baselines,has,rb +cb +ml,baselines has rb +cb +ml,0.5845585465431213
translation,338,154,baselines,svm,using,features,svm using features,0.668384850025177
translation,338,154,baselines,svm,is,svm classifier,svm is svm classifier,0.5926549434661865
translation,338,154,baselines,features,extracted from,rules,features extracted from rules,0.5776174664497375
translation,338,154,baselines,features,extracted from,chinese emotion cognition lexicon,features extracted from chinese emotion cognition lexicon,0.47500839829444885
translation,338,154,baselines,svm,is,svm classifier,svm is svm classifier,0.5926549434661865
translation,338,154,baselines,svm classifier,using,"unigram , bigram and trigram features","svm classifier using unigram , bigram and trigram features",0.6396039724349976
translation,338,160,baselines,cnn,for,sentence classification,cnn for sentence classification,0.5805941224098206
translation,338,160,baselines,convolutional neural network,for,sentence classification,convolutional neural network for sentence classification,0.5432462692260742
translation,338,160,baselines,cnn,has,convolutional neural network,cnn has convolutional neural network,0.528005063533783
translation,338,160,baselines,baselines,has,cnn,baselines has cnn,0.5907226204872131
translation,338,163,baselines,number of hops,set to,3,number of hops set to 3,0.7145438194274902
translation,338,163,baselines,baselines,has,number of hops,baselines has number of hops,0.5641397833824158
translation,338,172,baselines,rb +cb + ml,uses,rules and common-sense knowledge,rb +cb + ml uses rules and common-sense knowledge,0.6195661425590515
translation,338,172,baselines,rules and common-sense knowledge,as,features,rules and common-sense knowledge as features,0.5661811232566833
translation,338,172,baselines,rules and common-sense knowledge,to train,machine learning classifier,rules and common-sense knowledge to train machine learning classifier,0.6731811165809631
translation,338,172,baselines,machine learning methods,has,rb +cb + ml,machine learning methods has rb +cb + ml,0.5566920042037964
translation,338,174,experiments,svm and word2vec,are,word feature based methods,svm and word2vec are word feature based methods,0.581875205039978
translation,338,174,experiments,svm and word2vec,have,similar performance,svm and word2vec have similar performance,0.5537654161453247
translation,338,127,hyperparameters,word embeddings,learned using,skip-gram model,word embeddings learned using skip-gram model,0.727151095867157
translation,338,127,hyperparameters,hyperparameters,has,word embeddings,hyperparameters has word embeddings,0.4784160554409027
translation,338,128,hyperparameters,word embedding,is,20,word embedding is 20,0.5587344169616699
translation,338,128,hyperparameters,hyperparameters,size of,word embedding,hyperparameters size of word embedding,0.6156876683235168
translation,338,129,hyperparameters,dropout,set to,0.4,dropout set to 0.4,0.5996330380439758
translation,338,129,hyperparameters,hyperparameters,has,dropout,hyperparameters has dropout,0.5324090719223022
translation,338,162,hyperparameters,word embeddings,pre-trained by,skip-grams,word embeddings pre-trained by skip-grams,0.7150793671607971
translation,338,162,hyperparameters,hyperparameters,has,word embeddings,hyperparameters has word embeddings,0.4784160554409027
translation,338,164,hyperparameters,hyperparameters,has,convolutional multiple -slot deep memory network,hyperparameters has convolutional multiple -slot deep memory network,0.5063865184783936
translation,338,165,hyperparameters,word embeddings,pre-trained by,skip-grams,word embeddings pre-trained by skip-grams,0.7150793671607971
translation,338,165,hyperparameters,hyperparameters,has,word embeddings,hyperparameters has word embeddings,0.4784160554409027
translation,338,166,hyperparameters,number of hops,is,3,number of hops is 3,0.6224575042724609
translation,338,166,hyperparameters,hyperparameters,has,number of hops,hyperparameters has number of hops,0.5435999631881714
translation,338,7,model,new mechanism,to store,relevant context,new mechanism to store relevant context,0.7216894030570984
translation,338,7,model,relevant context,in,different memory slots,relevant context in different memory slots,0.4974452555179596
translation,338,7,model,relevant context,to model,context information,relevant context to model context information,0.6531672477722168
translation,338,7,model,model,propose,new mechanism,model propose new mechanism,0.7242430448532104
translation,338,34,model,recurrent structure,to mine,deep relation,recurrent structure to mine deep relation,0.6605897545814514
translation,338,34,model,deep relation,between,query and a text,deep relation between query and a text,0.6029577851295471
translation,338,34,model,model,has,recurrent structure,model has recurrent structure,0.5595463514328003
translation,338,40,model,new deep memory network architecture,to model,context of each word simultaneously,new deep memory network architecture to model context of each word simultaneously,0.6858927607536316
translation,338,40,model,context of each word simultaneously,by,multiple memory slots,context of each word simultaneously by multiple memory slots,0.5388622283935547
translation,338,40,model,multiple memory slots,capture,sequential information,multiple memory slots capture sequential information,0.7615839242935181
translation,338,40,model,sequential information,using,convolutional operations,sequential information using convolutional operations,0.6981886029243469
translation,338,40,model,model,propose,new deep memory network architecture,model propose new deep memory network architecture,0.7027125954627991
translation,338,156,model,svm classifier,using,word representations,svm classifier using word representations,0.6476976275444031
translation,338,156,model,word representations,learned by,"word2vec ( mikolov et al. , 2013 )","word representations learned by word2vec ( mikolov et al. , 2013 )",0.5982446670532227
translation,338,156,model,model,is,svm classifier,model is svm classifier,0.5743134021759033
translation,338,177,model,text,by,syntactic tree,text by syntactic tree,0.5493335723876953
translation,338,177,model,model,models,text,model models text,0.8720391988754272
translation,338,246,model,model,treat,emotion cause extraction,model treat emotion cause extraction,0.5764816403388977
translation,338,246,model,model,propose,model,model propose model,0.6740307211875916
translation,338,248,model,our new memory network architecture,able to store,context,our new memory network architecture able to store context,0.7572416067123413
translation,338,248,model,context,in,different memory slots,context in different memory slots,0.5319587588310242
translation,338,248,model,context information,in,proper sequence,context information in proper sequence,0.49101656675338745
translation,338,248,model,proper sequence,by,convolutional operation,proper sequence by convolutional operation,0.5958003997802734
translation,338,248,model,model,has,our new memory network architecture,model has our new memory network architecture,0.5604310631752014
translation,338,168,results,rule based rb,gives,fairly high precision,rule based rb gives fairly high precision,0.6143723130226135
translation,338,168,results,rule based rb,with,low recall,rule based rb with low recall,0.6145695447921753
translation,338,168,results,fairly high precision,with,low recall,fairly high precision with low recall,0.6410316824913025
translation,338,168,results,results,has,rule based rb,results has rule based rb,0.5714926719665527
translation,338,169,results,cb,achieves,highest recall,cb achieves highest recall,0.6900184750556946
translation,338,169,results,cb,has,common-sense based method,cb has common-sense based method,0.5847505331039429
translation,338,169,results,results,has,cb,results has cb,0.47837144136428833
translation,338,169,results,results,has,common-sense based method,results has common-sense based method,0.5491436719894409
translation,338,171,results,rb + cb,combination of,rb and cb,rb + cb combination of rb and cb,0.6584101319313049
translation,338,171,results,rb + cb,combination of,rb,rb + cb combination of rb,0.5842921137809753
translation,338,171,results,rb and cb,gives,higher,rb and cb gives higher,0.7083357572555542
translation,338,171,results,improvement,of,1.27 %,improvement of 1.27 %,0.5632216930389404
translation,338,171,results,marginal,compared to,rb,marginal compared to rb,0.6801453828811646
translation,338,171,results,higher,has,f-measure,higher has f-measure,0.5963128209114075
translation,338,171,results,higher,has,improvement,higher has improvement,0.6073253750801086
translation,338,171,results,results,has,rb + cb,results has rb + cb,0.5434946417808533
translation,338,173,results,f-measure,of,0.5597,f-measure of 0.5597,0.5641831159591675
translation,338,173,results,outperforming,has,rb + cb,outperforming has rb + cb,0.6235676407814026
translation,338,175,results,word2vec,performs,worse,word2vec performs worse,0.6575015783309937
translation,338,175,results,worse,than,svm,worse than svm,0.6057107448577881
translation,338,175,results,results,For,word2vec,results For word2vec,0.5231895446777344
translation,338,180,results,proposed convms - memnet architecture,boost,performance,proposed convms - memnet architecture boost performance,0.6358503103256226
translation,338,180,results,performance,by,11.54 %,performance by 11.54 %,0.5655352473258972
translation,338,180,results,performance,by,4.84 %,performance by 4.84 %,0.5626046061515808
translation,338,180,results,performance,by,8.24 %,performance by 8.24 %,0.5735135078430176
translation,338,180,results,performance,compared to,memnet,performance compared to memnet,0.6250518560409546
translation,338,180,results,11.54 %,in,precision,11.54 % in precision,0.5388351678848267
translation,338,180,results,4.84 %,in,recall,4.84 % in recall,0.5685756206512451
translation,338,180,results,8.24 %,in,fmeasure,8.24 % in fmeasure,0.587279200553894
translation,338,180,results,results,using,proposed convms - memnet architecture,results using proposed convms - memnet architecture,0.645271897315979
translation,338,182,results,previous best- performing method,by,3.01 %,previous best- performing method by 3.01 %,0.5286758542060852
translation,338,182,results,convms - memnet,has,outperforms,convms - memnet has outperforms,0.6195700168609619
translation,338,182,results,outperforms,has,previous best- performing method,outperforms has previous best- performing method,0.5755261778831482
translation,338,182,results,previous best- performing method,has,multi-kernel,previous best- performing method has multi-kernel,0.551161527633667
translation,338,182,results,3.01 %,has,in f-measure,3.01 % has in f-measure,0.5969014763832092
translation,338,182,results,results,has,convms - memnet,results has convms - memnet,0.5611494183540344
translation,338,194,results,pre-trained word embedding,gives,2.59 % higher f-measure,pre-trained word embedding gives 2.59 % higher f-measure,0.546329915523529
translation,338,194,results,2.59 % higher f-measure,compared to,random initialization,2.59 % higher f-measure compared to random initialization,0.6566069722175598
translation,338,194,results,results,observed,pre-trained word embedding,results observed pre-trained word embedding,0.5757125020027161
translation,338,196,results,word embedding,trained from,other much larger corpus,word embedding trained from other much larger corpus,0.7317691445350647
translation,338,196,results,word embedding,gives,better results,word embedding gives better results,0.6362079381942749
translation,338,196,results,results,using,word embedding,results using word embedding,0.6253795623779297
translation,338,203,results,single layer network,achieved,competitive performance,single layer network achieved competitive performance,0.6928054094314575
translation,338,203,results,results,has,single layer network,results has single layer network,0.5651297569274902
translation,338,204,results,increasing number of hops,has,performance,increasing number of hops has performance,0.618215799331665
translation,338,204,results,performance,has,improves,performance has improves,0.6178221106529236
translation,338,204,results,results,With,increasing number of hops,results With increasing number of hops,0.6730948686599731
translation,338,216,results,better results,obtained using,deep memory network,better results obtained using deep memory network,0.6025167107582092
translation,338,216,results,deep memory network,trained with,at least 3 hops,deep memory network trained with at least 3 hops,0.710552453994751
translation,338,216,results,results,has,better results,results has better results,0.5785699486732483
translation,338,223,results,outperforms,by,5.6 %,outperforms by 5.6 %,0.6204336881637573
translation,338,223,results,memnet,by,5.6 %,memnet by 5.6 %,0.5609607696533203
translation,338,223,results,memnet,in,f-measure,memnet in f-measure,0.49527284502983093
translation,338,223,results,5.6 %,in,f-measure,5.6 % in f-measure,0.5233644247055054
translation,338,223,results,proposed convms - memnet,has,outperforms,proposed convms - memnet has outperforms,0.6052260994911194
translation,338,223,results,outperforms,has,memnet,outperforms has memnet,0.6151848435401917
translation,338,224,results,convms - memnet,able to identify,word level emotion,convms - memnet able to identify word level emotion,0.6198787093162537
translation,338,224,results,word level emotion,cause,better,word level emotion cause better,0.741301953792572
translation,338,224,results,better,compare to,memnet,better compare to memnet,0.7245739698410034
translation,338,224,results,context features,has,convms - memnet,context features has convms - memnet,0.520486056804657
translation,339,157,hyperparameters,100k iterations,with,batch size,100k iterations with batch size,0.6020987629890442
translation,339,157,hyperparameters,batch size,of,64,batch size of 64,0.6741159558296204
translation,339,157,hyperparameters,hyperparameters,trained for,100k iterations,hyperparameters trained for 100k iterations,0.6477328538894653
translation,339,163,results,two models,learn to solve,task,two models learn to solve task,0.7573188543319702
translation,339,163,results,final accuracy,of,98.9 % and 99.4 %,final accuracy of 98.9 % and 99.4 %,0.5537040829658508
translation,339,163,results,task,has,quasi-perfectly,task has quasi-perfectly,0.6409170627593994
translation,339,163,results,results,has,two models,results has two models,0.5340345501899719
translation,339,183,results,clear negative effect,of,partitioned configuration,clear negative effect of partitioned configuration,0.6276836395263672
translation,339,183,results,partitioned configuration,on,performance,partitioned configuration on performance,0.5831641554832458
translation,339,183,results,performance,for,model,performance for model,0.6508644819259644
translation,339,183,results,model,trained on,qfull,model trained on qfull,0.7859346270561218
translation,339,186,results,no clear preference,for,perfectly clustered partitioned,no clear preference for perfectly clustered partitioned,0.6339319944381714
translation,339,186,results,no clear preference,for,the perfectly mixed paired arrangement,no clear preference for the perfectly mixed paired arrangement,0.636843204498291
translation,339,186,results,results,has,no clear preference,results has no clear preference,0.5395433306694031
translation,339,189,results,results,has,performance,results has performance,0.5972660779953003
translation,340,60,ablation-analysis,blurred image with answer,are,more informative,blurred image with answer are more informative,0.6131407022476196
translation,340,60,ablation-analysis,more informative,in terms of,human vqa accuracy,more informative in terms of human vqa accuracy,0.6391652226448059
translation,340,60,ablation-analysis,ablation analysis,has,attention maps,ablation analysis has attention maps,0.5304310321807861
translation,340,23,experiments,game - inspired novel interfaces,for collecting,human attention maps,game - inspired novel interfaces for collecting human attention maps,0.5819688439369202
translation,340,23,experiments,human attention maps,of,humans choose to look to answer questions,human attention maps of humans choose to look to answer questions,0.5332916975021362
translation,340,23,experiments,humans choose to look to answer questions,from,large-scale vqa dataset,humans choose to look to answer questions from large-scale vqa dataset,0.5384969711303711
translation,340,23,experiments,qualitative and quantitative comparison,of,task - independent saliency baseline,qualitative and quantitative comparison of task - independent saliency baseline,0.5608456134796143
translation,340,24,results,machine - generated attention maps,from,most accurate vqa model,machine - generated attention maps from most accurate vqa model,0.5068055391311646
translation,340,24,results,machine - generated attention maps,have,mean rank -correlation,machine - generated attention maps have mean rank -correlation,0.5440786480903625
translation,340,24,results,mean rank -correlation,of,0.26,mean rank -correlation of 0.26,0.5513581037521362
translation,340,24,results,0.26,with,human attention maps,0.26 with human attention maps,0.5949977040290833
translation,340,24,results,0.26,worse than,task - independent saliency maps,0.26 worse than task - independent saliency maps,0.6662629246711731
translation,340,24,results,human attention maps,worse than,task - independent saliency maps,human attention maps worse than task - independent saliency maps,0.6622536182403564
translation,340,24,results,mean rank-correlation,of,0.49,mean rank-correlation of 0.49,0.5459538102149963
translation,340,24,results,results,find that,machine - generated attention maps,results find that machine - generated attention maps,0.5814512372016907
translation,340,26,results,center bias,find that,correlation,center bias find that correlation,0.6791896820068359
translation,340,26,results,correlation,of,task - independent saliency,correlation of task - independent saliency,0.5578079223632812
translation,340,26,results,task - independent saliency,is,poor,task - independent saliency is poor,0.5674923658370972
translation,340,26,results,trends,for,machine - generated vqa - attention maps,trends for machine - generated vqa - attention maps,0.6012658476829529
translation,340,26,results,machine - generated vqa - attention maps,remain,same,machine - generated vqa - attention maps remain same,0.6634925007820129
translation,340,26,results,results,control for,center bias,results control for center bias,0.6480139493942261
translation,340,74,results,san - 2 and hiecoatt attention maps,positively correlated with,human attention maps,san - 2 and hiecoatt attention maps positively correlated with human attention maps,0.6969754099845886
translation,340,74,results,san - 2 and hiecoatt attention maps,not as strongly as,task - independent judd saliency maps,san - 2 and hiecoatt attention maps not as strongly as task - independent judd saliency maps,0.671120822429657
translation,340,74,results,results,has,san - 2 and hiecoatt attention maps,results has san - 2 and hiecoatt attention maps,0.49777519702911377
translation,341,137,ablation-analysis,features,contribute to,performance,features contribute to performance,0.6973457336425781
translation,341,137,ablation-analysis,performance,of,our final system,performance of our final system,0.5824689269065857
translation,341,26,baselines,module,using,tf - idf matching,module using tf - idf matching,0.6901460289955139
translation,341,26,baselines,model,trained to detect,answer spans,model trained to detect answer spans,0.7534635066986084
translation,341,26,baselines,answer spans,in,few returned documents,answer spans in few returned documents,0.5060937404632568
translation,341,26,baselines,document retriever,has,module,document retriever has module,0.5936124324798584
translation,341,60,baselines,best performing system,uses,bigram counts,best performing system uses bigram counts,0.6622776389122009
translation,341,60,baselines,bigram counts,while preserving,speed and memory efficiency,bigram counts while preserving speed and memory efficiency,0.7013292908668518
translation,341,60,baselines,speed and memory efficiency,by using,"hashing of ( weinberger et al. , 2009 )","speed and memory efficiency by using hashing of ( weinberger et al. , 2009 )",0.666500985622406
translation,341,60,baselines,"hashing of ( weinberger et al. , 2009 )",to map,bigrams,"hashing of ( weinberger et al. , 2009 ) to map bigrams",0.6764851808547974
translation,341,60,baselines,bigrams,to,2 24 bins,bigrams to 2 24 bins,0.5743897557258606
translation,341,60,baselines,bigrams,with,unsigned murmur3 hash,bigrams with unsigned murmur3 hash,0.6597990989685059
translation,341,60,baselines,baselines,has,best performing system,baselines has best performing system,0.5688688158988953
translation,341,143,baselines,three versions of drqa,evaluate,impact,three versions of drqa evaluate impact,0.6574262380599976
translation,341,143,baselines,impact,of using,distant supervision,impact of using distant supervision,0.6786856055259705
translation,341,143,baselines,sources,provided to,document reader,sources provided to document reader,0.7028728127479553
translation,341,143,baselines,single document reader model,trained on,squad training set,single document reader model trained on squad training set,0.7167181372642517
translation,341,143,baselines,single document reader model,used on,all evaluation sets,single document reader model used on all evaluation sets,0.6685177683830261
translation,341,143,baselines,squad,has,single document reader model,squad has single document reader model,0.5744699239730835
translation,341,144,baselines,document reader model,pre-trained on,squad,document reader model pre-trained on squad,0.7244415879249573
translation,341,144,baselines,document reader model,fine-tuned for,each dataset independently,document reader model fine-tuned for each dataset independently,0.7221560478210449
translation,341,144,baselines,each dataset independently,using,distant supervision ( ds ) training set,each dataset independently using distant supervision ( ds ) training set,0.6472519040107727
translation,341,144,baselines,fine-tune ( ds ),has,document reader model,fine-tune ( ds ) has document reader model,0.5642406344413757
translation,341,144,baselines,baselines,has,fine-tune ( ds ),baselines has fine-tune ( ds ),0.5649312734603882
translation,341,145,baselines,single document reader model,jointly trained on,squad training set,single document reader model jointly trained on squad training set,0.7621451616287231
translation,341,145,baselines,single document reader model,jointly trained on,all the ds sources,single document reader model jointly trained on all the ds sources,0.724902331829071
translation,341,145,baselines,multitask ( ds ),has,single document reader model,multitask ( ds ) has single document reader model,0.567093551158905
translation,341,145,baselines,baselines,has,multitask ( ds ),baselines has multitask ( ds ),0.5655684471130371
translation,341,146,baselines,full wikipedia setting,use,streamlined model,full wikipedia setting use streamlined model,0.6141484975814819
translation,341,146,baselines,streamlined model,that does not use,corenlp parsed f token features,streamlined model that does not use corenlp parsed f token features,0.6813682913780212
translation,341,146,baselines,streamlined model,that does not use,lemmas,streamlined model that does not use lemmas,0.6853023171424866
translation,341,146,baselines,lemmas,for,f exact match,lemmas for f exact match,0.5951289534568787
translation,341,127,experimental-setup,3 - layer bidirectional lstms,with,h = 128 hidden units,3 - layer bidirectional lstms with h = 128 hidden units,0.5593356490135193
translation,341,127,experimental-setup,h = 128 hidden units,for,paragraph and question encoding,h = 128 hidden units for paragraph and question encoding,0.5685473084449768
translation,341,127,experimental-setup,h = 128 hidden units,both,paragraph and question encoding,h = 128 hidden units both paragraph and question encoding,0.6317239999771118
translation,341,127,experimental-setup,experimental setup,use,3 - layer bidirectional lstms,experimental setup use 3 - layer bidirectional lstms,0.5769892930984497
translation,341,128,experimental-setup,stanford corenlp toolkit,for,tokenization,stanford corenlp toolkit for tokenization,0.5797368288040161
translation,341,128,experimental-setup,stanford corenlp toolkit,generating,partof-speech,stanford corenlp toolkit generating partof-speech,0.623599112033844
translation,341,128,experimental-setup,stanford corenlp toolkit,generating,named entity tags,stanford corenlp toolkit generating named entity tags,0.5976753234863281
translation,341,128,experimental-setup,experimental setup,apply,stanford corenlp toolkit,experimental setup apply stanford corenlp toolkit,0.5846158266067505
translation,341,129,experimental-setup,training examples,sorted by,length,training examples sorted by length,0.6869457960128784
translation,341,129,experimental-setup,training examples,divided into,minibatches,training examples divided into minibatches,0.5956485867500305
translation,341,129,experimental-setup,length,of,paragraph,length of paragraph,0.5935345888137817
translation,341,129,experimental-setup,minibatches,of,32 examples each,minibatches of 32 examples each,0.5371328592300415
translation,341,129,experimental-setup,experimental setup,has,training examples,experimental setup has training examples,0.5167377591133118
translation,341,130,experimental-setup,adamax,for,optimization,adamax for optimization,0.6393342614173889
translation,341,131,experimental-setup,dropout,with,p = 0.3,dropout with p = 0.3,0.6808434128761292
translation,341,131,experimental-setup,p = 0.3,applied to,word embeddings,p = 0.3 applied to word embeddings,0.6542688012123108
translation,341,131,experimental-setup,p = 0.3,applied to,all the hidden units,p = 0.3 applied to all the hidden units,0.6821873188018799
translation,341,131,experimental-setup,all the hidden units,of,lstms,all the hidden units of lstms,0.5534578561782837
translation,341,131,experimental-setup,experimental setup,has,dropout,experimental setup has dropout,0.5067690014839172
translation,341,6,model,search component,based on,bigram hashing,search component based on bigram hashing,0.6402387022972107
translation,341,6,model,search component,based on,tf - idf matching,search component based on tf - idf matching,0.6382916569709778
translation,341,6,model,tf - idf matching,with,multi-layer recurrent neural network model,tf - idf matching with multi-layer recurrent neural network model,0.5834466814994812
translation,341,6,model,multi-layer recurrent neural network model,trained to detect,answers,multi-layer recurrent neural network model trained to detect answers,0.694253146648407
translation,341,6,model,answers,in,wikipedia paragraphs,answers in wikipedia paragraphs,0.512421190738678
translation,341,25,model,mrs,requiring,open-domain system,mrs requiring open-domain system,0.7132119536399841
translation,341,25,model,evaluate,has,mrs,evaluate has mrs,0.6013189554214478
translation,341,25,model,model,show,multiple existing qa datasets,model show multiple existing qa datasets,0.663821816444397
translation,341,59,results,our system,by taking,local word order,our system by taking local word order,0.5651758909225464
translation,341,59,results,local word order,into,account,local word order into account,0.5797669291496277
translation,341,59,results,local word order,with,n-gram features,local word order with n-gram features,0.6288476586341858
translation,341,59,results,account,with,n-gram features,account with n-gram features,0.6583914160728455
translation,341,59,results,results,improve,our system,results improve our system,0.6569348573684692
translation,341,123,results,all datasets,indicate,our simple approach,all datasets indicate our simple approach,0.5844476819038391
translation,341,123,results,outperforms,especially with,bigram hashing,outperforms especially with bigram hashing,0.6461012959480286
translation,341,123,results,wikipedia search,especially with,bigram hashing,wikipedia search especially with bigram hashing,0.5486482381820679
translation,341,123,results,our simple approach,has,outperforms,our simple approach has outperforms,0.6350935697555542
translation,341,123,results,outperforms,has,wikipedia search,outperforms has wikipedia search,0.6095978617668152
translation,341,134,results,our system ( single model ),achieve,70.0 % exact match,our system ( single model ) achieve 70.0 % exact match,0.6117669939994812
translation,341,134,results,our system ( single model ),achieve,79.0 % f1 scores,our system ( single model ) achieve 79.0 % f1 scores,0.5866430997848511
translation,341,134,results,our system ( single model ),on,test set,our system ( single model ) on test set,0.5357226133346558
translation,341,134,results,our system ( single model ),match,top performance,our system ( single model ) match top performance,0.7439539432525635
translation,341,134,results,79.0 % f1 scores,on,test set,79.0 % f1 scores on test set,0.5190798044204712
translation,341,134,results,results,has,our system ( single model ),results has our system ( single model ),0.5580957531929016
translation,341,138,results,our system,able to achieve,f1,our system able to achieve f1,0.6300980448722839
translation,341,138,results,f1,over,77 %,f1 over 77 %,0.6962659358978271
translation,341,138,results,aligned question embedding feature,has,our system,aligned question embedding feature has our system,0.5671501755714417
translation,341,138,results,few manual features,has,our system,few manual features has our system,0.5710113048553467
translation,341,138,results,results,Without,aligned question embedding feature,results Without aligned question embedding feature,0.6873493790626526
translation,341,153,results,drqa,provides,reasonable performance,drqa provides reasonable performance,0.6539055109024048
translation,341,155,results,single model,trained only on,squad,single model trained only on squad,0.7576228380203247
translation,341,155,results,outperformed,on,all four of the datasets,outperformed on all four of the datasets,0.538938581943512
translation,341,155,results,outperformed,by,multitask model,outperformed by multitask model,0.6071705222129822
translation,341,155,results,multitask model,that uses,distant supervision,multitask model that uses distant supervision,0.5522639155387878
translation,341,155,results,results,has,single model,results has single model,0.5429759621620178
translation,341,156,results,performance,training on,squad alone,performance training on squad alone,0.7552482485771179
translation,341,156,results,squad alone,is,not far behind,squad alone is not far behind,0.6059628129005432
translation,341,156,results,results,has,performance,results has performance,0.5972660779953003
translation,341,157,results,improvement,from,squad to multitask ( ds ),improvement from squad to multitask ( ds ),0.5667929649353027
translation,341,157,results,improvement,not from,task transfer,improvement not from task transfer,0.5956947207450867
translation,341,157,results,squad to multitask ( ds ),not from,task transfer,squad to multitask ( ds ) not from task transfer,0.6459498405456543
translation,341,157,results,each dataset,using,ds,each dataset using ds,0.6741527915000916
translation,341,157,results,ds,gives,improvements,ds gives improvements,0.6729122400283813
translation,341,157,results,results,majority of,improvement,results majority of improvement,0.7366255521774292
translation,341,159,results,unconstrained qa system,using,redundant resources,unconstrained qa system using redundant resources,0.6952860355377197
translation,341,159,results,results,compare to,unconstrained qa system,results compare to unconstrained qa system,0.6645339727401733
translation,341,160,results,our performance,is,not too far behind,our performance is not too far behind,0.5976038575172424
translation,341,160,results,not too far behind,on,curatedtrec,not too far behind on curatedtrec,0.6481958031654358
translation,342,133,ablation-analysis,additional ptweet information,improved,svm performance,additional ptweet information improved svm performance,0.7442137598991394
translation,342,133,ablation-analysis,svm performance,from,73.9,svm performance from 73.9,0.5034778118133545
translation,342,133,ablation-analysis,svm performance,from,74.5,svm performance from 74.5,0.5043604373931885
translation,342,133,ablation-analysis,73.9,to,74.5,73.9 to 74.5,0.5788912773132324
translation,342,133,ablation-analysis,ablation analysis,has,additional ptweet information,ablation analysis has additional ptweet information,0.49604663252830505
translation,342,96,baselines,baselines,has,nouns embedding ( nounembed ),baselines has nouns embedding ( nounembed ),0.5709019303321838
translation,342,102,baselines,baselines,has,latent dirichlet allocation ( lda ),baselines has latent dirichlet allocation ( lda ),0.5514474511146545
translation,342,105,baselines,google topic categories ( gtopic ),labels,text,google topic categories ( gtopic ) labels text,0.7443991303443909
translation,342,105,baselines,google 's content classifier,labels,text,google 's content classifier labels text,0.7257193922996521
translation,342,105,baselines,text,with respect to,700 + topic categories,text with respect to 700 + topic categories,0.6533552408218384
translation,342,105,baselines,700 + topic categories,in,content hierarchy,700 + topic categories in content hierarchy,0.4969090521335602
translation,342,105,baselines,google topic categories ( gtopic ),has,google 's content classifier,google topic categories ( gtopic ) has google 's content classifier,0.5935820937156677
translation,342,105,baselines,baselines,has,google topic categories ( gtopic ),baselines has google topic categories ( gtopic ),0.5535270571708679
translation,342,74,hyperparameters,embedding features,used,glove vectors,embedding features used glove vectors,0.6128190755844116
translation,342,74,hyperparameters,glove vectors,pretrained on,2b tweets,glove vectors pretrained on 2b tweets,0.750190019607544
translation,342,74,hyperparameters,hyperparameters,For,embedding features,hyperparameters For embedding features,0.5451770424842834
translation,342,116,hyperparameters,linear svm,with,c = 0.1,linear svm with c = 0.1,0.6051397323608398
translation,342,116,hyperparameters,4 - layer bilstm,implemented using,pytorch,4 - layer bilstm implemented using pytorch,0.6609651446342468
translation,342,116,hyperparameters,4 - layer bilstm,with,hidden size,4 - layer bilstm with hidden size,0.5896190404891968
translation,342,116,hyperparameters,hidden size,of,100,hidden size of 100,0.6616772413253784
translation,342,116,hyperparameters,hyperparameters,created,two types of classifiers,hyperparameters created two types of classifiers,0.5328769683837891
translation,342,117,hyperparameters,learning rate,of,bilstm,learning rate of bilstm,0.5671712756156921
translation,342,117,hyperparameters,bilstm,to,0.0001,bilstm to 0.0001,0.567185640335083
translation,342,117,hyperparameters,0.0001,with,dropout rate,0.0001 with dropout rate,0.5362054705619812
translation,342,117,hyperparameters,dropout rate,of,0.1,dropout rate of 0.1,0.5755324363708496
translation,342,117,hyperparameters,hyperparameters,set,learning rate,hyperparameters set learning rate,0.5994082689285278
translation,342,118,hyperparameters,glove embeddings,pre-trained on,2b tweets,glove embeddings pre-trained on 2b tweets,0.8021616339683533
translation,342,118,hyperparameters,2b tweets,of size,25 or 100 dimensions,2b tweets of size 25 or 100 dimensions,0.7526032328605652
translation,342,118,hyperparameters,hyperparameters,use,glove embeddings,hyperparameters use glove embeddings,0.5820891261100769
translation,342,7,model,questions,in,tweets,questions in tweets,0.5273674130439758
translation,342,7,model,questions,paired with,prior tweets,questions paired with prior tweets,0.6349064111709595
translation,342,7,model,tweets,paired with,prior tweets,tweets paired with prior tweets,0.6278672218322754
translation,342,7,model,prior tweets,to provide,context,prior tweets to provide context,0.5762890577316284
translation,342,24,model,classification models,exploit,question and the initial tweet,classification models exploit question and the initial tweet,0.6893797516822815
translation,342,24,model,classification models,both,question and the initial tweet,classification models both question and the initial tweet,0.6431007981300354
translation,342,24,model,question and the initial tweet,prior to,question,question and the initial tweet prior to question,0.6812145709991455
translation,342,24,model,model,present,classification models,model present classification models,0.7357016801834106
translation,342,125,results,bilstm,achieving,f1 score,bilstm achieving f1 score,0.6756659150123596
translation,342,125,results,f1 score,of,70.8,f1 score of 70.8,0.5428763031959534
translation,342,125,results,bilstm,has,outperforms,bilstm has outperforms,0.6106641888618469
translation,342,125,results,outperforms,has,svms,outperforms has svms,0.6074677109718323
translation,342,141,results,recall,for,rhetorical questions,recall for rhetorical questions,0.6432252526283264
translation,342,141,results,recall,increases,about 1 %,recall increases about 1 %,0.7635508179664612
translation,342,141,results,rhetorical questions,increases,about 1 %,rhetorical questions increases about 1 %,0.7650701403617859
translation,342,141,results,precision,for,information - seeking questions,precision for information - seeking questions,0.6439258456230164
translation,342,141,results,precision,goes,up,precision goes up,0.7118597626686096
translation,342,141,results,information - seeking questions,goes,up,information - seeking questions goes up,0.6006119251251221
translation,342,143,results,improves,for,both categories,improves for both categories,0.6252277493476868
translation,342,143,results,both categories,as,topic features,both categories as topic features,0.48911264538764954
translation,342,143,results,recall and precision,has,improves,recall and precision has improves,0.6618871092796326
translation,342,143,results,results,has,recall and precision,results has recall and precision,0.5866001844406128
translation,343,186,baselines,best baselines,are,bert - large and gpt,best baselines are bert - large and gpt,0.5618192553520203
translation,343,186,baselines,bert - large and gpt,with,accuracy,bert - large and gpt with accuracy,0.7077556848526001
translation,343,186,baselines,accuracy,of,55.9 % and 45.5 %,accuracy of 55.9 % and 45.5 %,0.5779070258140564
translation,343,186,baselines,55.9 % and 45.5 %,on,random split,55.9 % and 45.5 % on random split,0.5737302303314209
translation,343,186,baselines,55.9 % and 45.5 %,on,question concept split,55.9 % and 45.5 % on question concept split,0.5138124227523804
translation,343,186,baselines,baselines,has,best baselines,baselines has best baselines,0.5512824058532715
translation,343,8,experiments,multiple -choice questions,that mention,source concept,multiple -choice questions that mention source concept,0.6685351133346558
translation,343,8,experiments,discriminate,in turn between,target concepts,discriminate in turn between target concepts,0.7173396348953247
translation,343,8,experiments,author,has,multiple -choice questions,author has multiple -choice questions,0.5449545979499817
translation,343,24,experiments,commonsense question answering,based on,knowledge,commonsense question answering based on knowledge,0.6223593950271606
translation,343,24,experiments,knowledge,encoded in,conceptnet,knowledge encoded in conceptnet,0.7471497654914856
translation,343,152,experiments,output layer size,number of,candidate answers,output layer size number of candidate answers,0.7177777886390686
translation,343,152,experiments,softmax,to train,cross-entropy loss,softmax to train cross-entropy loss,0.6379033923149109
translation,343,152,experiments,esim,has,strong nli model,esim has strong nli model,0.5601935386657715
translation,343,178,hyperparameters,pre-trained word embeddings,consider,300d glove embeddings,pre-trained word embeddings consider 300d glove embeddings,0.556823194026947
translation,343,178,hyperparameters,pre-trained word embeddings,consider,300d numberbatch conceptnet node embeddings,pre-trained word embeddings consider 300d numberbatch conceptnet node embeddings,0.5710995197296143
translation,343,178,hyperparameters,300d numberbatch conceptnet node embeddings,kept,fixed,300d numberbatch conceptnet node embeddings kept fixed,0.6621765494346619
translation,343,178,hyperparameters,fixed,at,training time,fixed at training time,0.5772721171379089
translation,343,178,hyperparameters,hyperparameters,For,pre-trained word embeddings,hyperparameters For pre-trained word embeddings,0.46875226497650146
translation,343,7,model,common sense beyond associations,extract from,"con -ceptnet ( speer et al. , 2017 )","common sense beyond associations extract from con -ceptnet ( speer et al. , 2017 )",0.5333874225616455
translation,343,7,model,multiple target concepts,that have,same semantic relation,multiple target concepts that have same semantic relation,0.6118766069412231
translation,343,7,model,same semantic relation,to,single source concept,same semantic relation to single source concept,0.5546393394470215
translation,343,7,model,"con -ceptnet ( speer et al. , 2017 )",has,multiple target concepts,"con -ceptnet ( speer et al. , 2017 ) has multiple target concepts",0.5482462644577026
translation,343,25,model,method,for generating,commonsense questions,method for generating commonsense questions,0.6280725598335266
translation,343,25,model,commonsense questions,at,scale,commonsense questions at scale,0.5257495641708374
translation,343,25,model,commonsense questions,by asking,crowd workers,commonsense questions by asking crowd workers,0.7254557609558105
translation,343,25,model,crowd workers,to author,questions,crowd workers to author questions,0.6660071015357971
translation,343,25,model,questions,describe,relation,questions describe relation,0.7117757797241211
translation,343,25,model,relation,between,concepts,relation between concepts,0.632919430732727
translation,343,25,model,concepts,from,conceptnet,concepts from conceptnet,0.5447629690170288
translation,343,25,model,model,propose,method,model propose method,0.6280754208564758
translation,343,155,model,"bidaf ( seo et al. , 2016 )",with,self-attention layer,"bidaf ( seo et al. , 2016 ) with self-attention layer",0.5878257155418396
translation,343,155,model,"bidaf ( seo et al. , 2016 )",with,elmo representa-tions,"bidaf ( seo et al. , 2016 ) with elmo representa-tions",0.6109414100646973
translation,343,155,model,model,augment,"bidaf ( seo et al. , 2016 )","model augment bidaf ( seo et al. , 2016 )",0.6697009205818176
translation,343,179,model,esim,with,1024d elmo contextual representations,esim with 1024d elmo contextual representations,0.6441057920455933
translation,343,179,model,1024d elmo contextual representations,fixed during,training,1024d elmo contextual representations fixed during training,0.6636043787002563
translation,343,179,model,model,combine,esim,model combine esim,0.6716979146003723
translation,343,193,results,performance,compared to,glove embeddings,performance compared to glove embeddings,0.6378664374351501
translation,343,193,results,not improve,has,performance,not improve has performance,0.587424099445343
translation,343,193,results,results,note,elmo representations,results note elmo representations,0.5791754126548767
translation,343,194,results,bidaf ++,uses,web snippets,bidaf ++ uses web snippets,0.6437426209449768
translation,343,194,results,web snippets,as,context,web snippets as context,0.49180033802986145
translation,343,194,results,results,for,bidaf ++,results for bidaf ++,0.634178102016449
translation,343,195,results,snippets,lead to,high performance,snippets lead to high performance,0.6850632429122925
translation,343,196,results,performance,on,random split,performance on random split,0.5644312500953674
translation,343,196,results,random split,is,five points lower,random split is five points lower,0.5891048908233643
translation,343,196,results,five points lower,than,question concept split,five points lower than question concept split,0.6070396900177002
translation,343,196,results,question concept split,on,average,question concept split on average,0.5605500340461731
translation,343,196,results,question concept split,across,all trained models,question concept split across all trained models,0.6739693880081177
translation,343,196,results,results,has,performance,results has performance,0.5972660779953003
translation,343,198,results,all sanity models,trained on,commonsenseqa,all sanity models trained on commonsenseqa,0.730355441570282
translation,343,198,results,commonsenseqa,achieve,very high performance,commonsenseqa achieve very high performance,0.6323277354240417
translation,343,198,results,92 %,for,bert - large,92 % for bert - large,0.6472004652023315
translation,343,198,results,very high performance,has,92 %,very high performance has 92 %,0.6017080545425415
translation,343,198,results,results,has,all sanity models,results has all sanity models,0.5025061368942261
translation,344,25,model,phonesthemes,by analyzing,orthographic correlates,phonesthemes by analyzing orthographic correlates,0.6382637023925781
translation,344,25,model,orthographic correlates,in,large corpus of written english,orthographic correlates in large corpus of written english,0.4867294132709503
translation,344,25,model,orthographic correlates,leveraging,word embeddings,orthographic correlates leveraging word embeddings,0.6920272707939148
translation,344,25,model,word embeddings,constructed with,word2vec,word embeddings constructed with word2vec,0.6130821108818054
translation,344,25,model,model,investigate,phonesthemes,model investigate phonesthemes,0.6137616038322449
translation,344,113,results,correlation,between,average number of successful t-tests,correlation between average number of successful t-tests,0.6473667621612549
translation,344,113,results,correlation,between,average similarity,correlation between average similarity,0.655044436454773
translation,344,113,results,average similarity,is,high ( r = .93 ),average similarity is high ( r = .93 ),0.47937723994255066
translation,344,113,results,results,has,correlation,results has correlation,0.5354241132736206
translation,344,114,results,cluster size,negatively correlated with,all semantic similarity measures,cluster size negatively correlated with all semantic similarity measures,0.7391442060470581
translation,344,114,results,results,has,cluster size,results has cluster size,0.5433667898178101
translation,344,158,results,first label evaluation test,has,centroid,first label evaluation test has centroid,0.5619706511497498
translation,344,158,results,centroid,has,overwhelmingly outperforms,centroid has overwhelmingly outperforms,0.6131340265274048
translation,344,158,results,overwhelmingly outperforms,has,wordnet,overwhelmingly outperforms has wordnet,0.5974534153938293
translation,344,158,results,results,Regarding,first label evaluation test,results Regarding first label evaluation test,0.5848763585090637
translation,345,25,ablation-analysis,competition,introduced,some minor improvements and extra features,competition introduced some minor improvements and extra features,0.6703738570213318
translation,345,25,ablation-analysis,some minor improvements and extra features,without changing,fundamental architecture,some minor improvements and extra features without changing fundamental architecture,0.7415681481361389
translation,345,25,ablation-analysis,some minor improvements and extra features,improved,map result,some minor improvements and extra features improved map result,0.7386160492897034
translation,345,25,ablation-analysis,fundamental architecture,of,network,fundamental architecture of network,0.5999553799629211
translation,345,25,ablation-analysis,map result,by,almost two points,map result by almost two points,0.6251809597015381
translation,345,25,ablation-analysis,ablation analysis,after,competition,ablation analysis after competition,0.7327958941459656
translation,345,27,ablation-analysis,every single piece,contributes,important information,every single piece contributes important information,0.680421769618988
translation,345,27,ablation-analysis,important information,to achieve,final performance,important information to achieve final performance,0.6719496846199036
translation,345,27,ablation-analysis,ablation analysis,observed,every single piece,ablation analysis observed every single piece,0.662037193775177
translation,345,125,ablation-analysis,most important,turn out to be,task feats,most important turn out to be task feats,0.5777425169944763
translation,345,125,ablation-analysis,task feats,contributing,over 5 map points,task feats contributing over 5 map points,0.723609209060669
translation,345,125,ablation-analysis,task feats,handle,important information sources,task feats handle important information sources,0.6882272958755493
translation,345,125,ablation-analysis,1.60,has,points,1.60 has points,0.5938659906387329
translation,345,125,ablation-analysis,ablation analysis,has,most important,ablation analysis has most important,0.5005125999450684
translation,345,128,ablation-analysis,ablation analysis,has,mte - motivated syntax vec vectors,ablation analysis has mte - motivated syntax vec vectors,0.5236210227012634
translation,345,129,ablation-analysis,vectors,is,not enough,vectors is not enough,0.6475702524185181
translation,345,129,ablation-analysis,vectors,adding,cosines,vectors adding cosines,0.7207556962966919
translation,345,129,ablation-analysis,cosines,as,pairwise features,cosines as pairwise features,0.49767690896987915
translation,345,129,ablation-analysis,ablation analysis,adding,cosines,ablation analysis adding cosines,0.7534382939338684
translation,345,132,ablation-analysis,two mte features,contribute,0.8 map points,two mte features contribute 0.8 map points,0.6603532433509827
translation,345,6,experiments,good results,on,subtask c,good results on subtask c,0.5471745729446411
translation,345,126,experiments,ql vec,trained on,text,ql vec trained on text,0.7564998865127563
translation,345,126,experiments,text,from,target forum,text from target forum,0.5787146687507629
translation,345,126,experiments,importance,has,ql vec,importance has ql vec,0.5996677279472351
translation,345,126,experiments,word embeddings,has,ql vec,word embeddings has ql vec,0.557871401309967
translation,345,127,experiments,google vec,trained on,100 billion words,google vec trained on 100 billion words,0.7197352051734924
translation,345,127,experiments,google vec,still,useful,google vec still useful,0.696491539478302
translation,345,127,experiments,useful,presence of,domain-specific ql vec,useful presence of domain-specific ql vec,0.6690568923950195
translation,345,127,experiments,useful,trained on,four orders of magnitude less data,useful trained on four orders of magnitude less data,0.7551668882369995
translation,345,127,experiments,domain-specific ql vec,trained on,four orders of magnitude less data,domain-specific ql vec trained on four orders of magnitude less data,0.7796040773391724
translation,345,91,hyperparameters,our model,on,train - part1,our model on train - part1,0.5398736000061035
translation,345,91,hyperparameters,our model,with,minibatches,our model with minibatches,0.6520516872406006
translation,345,91,hyperparameters,our model,with,regularization,our model with regularization,0.6744288206100464
translation,345,91,hyperparameters,our model,with,decay,our model with decay,0.6980666518211365
translation,345,91,hyperparameters,our model,using,stochastic gradient descent,our model using stochastic gradient descent,0.6470151543617249
translation,345,91,hyperparameters,train - part1,with,hidden layers,train - part1 with hidden layers,0.6158812642097473
translation,345,91,hyperparameters,train - part1,with,minibatches,train - part1 with minibatches,0.6495296359062195
translation,345,91,hyperparameters,train - part1,with,minibatches,train - part1 with minibatches,0.6495296359062195
translation,345,91,hyperparameters,train - part1,using,stochastic gradient descent,train - part1 using stochastic gradient descent,0.6512229442596436
translation,345,91,hyperparameters,hidden layers,of size,3,hidden layers of size 3,0.7335939407348633
translation,345,91,hyperparameters,hidden layers,for,63 epochs,hidden layers for 63 epochs,0.5697591304779053
translation,345,91,hyperparameters,3,for,63 epochs,3 for 63 epochs,0.6734676957130432
translation,345,91,hyperparameters,minibatches,of size,30,minibatches of size 30,0.688652753829956
translation,345,91,hyperparameters,regularization,of,0.0015,regularization of 0.0015,0.5881431102752686
translation,345,91,hyperparameters,decay,of,0.0001,decay of 0.0001,0.5845822095870972
translation,345,91,hyperparameters,0.0001,using,stochastic gradient descent,0.0001 using stochastic gradient descent,0.6239730715751648
translation,345,91,hyperparameters,stochastic gradient descent,with,adagrad,stochastic gradient descent with adagrad,0.6186466217041016
translation,345,91,hyperparameters,hyperparameters,train,our model,hyperparameters train our model,0.6687315702438354
translation,345,92,hyperparameters,input feature values,to,[ ?1 ; 1 ] interval,input feature values to [ ?1 ; 1 ] interval,0.5030277371406555
translation,345,92,hyperparameters,input feature values,to,uniform distribution,input feature values to uniform distribution,0.5345566868782043
translation,345,92,hyperparameters,input feature values,using,minmax,input feature values using minmax,0.6314250826835632
translation,345,92,hyperparameters,[ ?1 ; 1 ] interval,using,minmax,[ ?1 ; 1 ] interval using minmax,0.5504364967346191
translation,345,92,hyperparameters,hyperparameters,normalize,input feature values,hyperparameters normalize input feature values,0.6330884099006653
translation,345,13,model,model,extend,mte neural network framework,model extend mte neural network framework,0.6936022639274597
translation,345,5,results,lightweight task-specific features,obtained,very encouraging experimental results,lightweight task-specific features obtained very encouraging experimental results,0.5789885520935059
translation,345,5,results,very encouraging experimental results,with,sizeable contributions,very encouraging experimental results with sizeable contributions,0.5883928537368774
translation,345,5,results,sizeable contributions,from both,mte features,sizeable contributions from both mte features,0.66021728515625
translation,345,5,results,sizeable contributions,from both,pairwise network architecture,sizeable contributions from both pairwise network architecture,0.6259403824806213
translation,345,5,results,results,addition of,lightweight task-specific features,results addition of lightweight task-specific features,0.5747672915458679
translation,345,111,results,out of twelve teams,on,map,out of twelve teams on map,0.6017805933952332
translation,345,111,results,ranked sixth,has,out of twelve teams,ranked sixth has out of twelve teams,0.584234893321991
translation,345,111,results,results,has,primary submission,results has primary submission,0.4854085147380829
translation,345,117,results,4th place,with,map,4th place with map,0.676887571811676
translation,345,117,results,map,of,49.38,map of 49.38,0.5880076885223389
translation,345,117,results,baseline,of,40.36,baseline of 40.36,0.5034634470939636
translation,345,117,results,results,achieved,4th place,results achieved 4th place,0.6997692584991455
translation,345,121,results,mte - nn - improved,showed,notable improvements,mte - nn - improved showed notable improvements,0.7129040360450745
translation,345,121,results,notable improvements,on,dev set,notable improvements on dev set,0.6265445351600647
translation,345,121,results,dev set,over,our primary submission,dev set over our primary submission,0.6500829458236694
translation,345,121,results,results,has,mte - nn - improved,results has mte - nn - improved,0.5834389925003052
translation,346,95,ablation-analysis,more incomplete,more obvious,performance improvement,more incomplete more obvious performance improvement,0.826698899269104
translation,346,95,ablation-analysis,performance improvement,of,qa,performance improvement of qa,0.6333118677139282
translation,346,95,ablation-analysis,document information,to complete and enhance,kb,document information to complete and enhance kb,0.6583622097969055
translation,346,95,ablation-analysis,performance,of,qa,performance of qa,0.6746920347213745
translation,346,95,ablation-analysis,more incomplete,has,kb,more incomplete has kb,0.623562753200531
translation,346,95,ablation-analysis,ablation analysis,has,more incomplete,ablation analysis has more incomplete,0.5396009087562561
translation,346,98,ablation-analysis,text information,improves,performance,text information improves performance,0.7009743452072144
translation,346,98,ablation-analysis,text information,cause,extra interference,text information cause extra interference,0.7299811840057373
translation,346,98,ablation-analysis,performance,in the case of,incomplete kb,performance in the case of incomplete kb,0.666816234588623
translation,346,98,ablation-analysis,incomplete kb,is sufficient to support,answering questions,incomplete kb is sufficient to support answering questions,0.740886390209198
translation,346,98,ablation-analysis,extra interference,when,kb,extra interference when kb,0.6734657883644104
translation,346,98,ablation-analysis,extra interference,lead to,performance degradation,extra interference lead to performance degradation,0.6015394926071167
translation,346,98,ablation-analysis,kb,is sufficient to support,answering questions,kb is sufficient to support answering questions,0.7441298961639404
translation,346,98,ablation-analysis,ablation analysis,notice,text information,ablation analysis notice text information,0.7218643426895142
translation,346,102,ablation-analysis,attention mechanism,adopted by,model,attention mechanism adopted by model,0.6749500632286072
translation,346,102,ablation-analysis,attention mechanism,is,effective,attention mechanism is effective,0.5358812808990479
translation,346,102,ablation-analysis,model,is,effective,model is effective,0.6228228211402893
translation,346,102,ablation-analysis,effective,especially,dual-step attention,effective especially dual-step attention,0.6409673094749451
translation,346,102,ablation-analysis,dual-step attention,proposed at,hgcn layer,dual-step attention proposed at hgcn layer,0.6786367297172546
translation,346,102,ablation-analysis,dual-step attention,brings,1.2 % improvement,dual-step attention brings 1.2 % improvement,0.572092592716217
translation,346,102,ablation-analysis,1.2 % improvement,of,hits@1,1.2 % improvement of hits@1,0.5760854482650757
translation,346,103,ablation-analysis,strategy,of,entity - enriched kb,strategy of entity - enriched kb,0.5410991907119751
translation,346,103,ablation-analysis,strategy,increases,hits@1,strategy increases hits@1,0.7632352113723755
translation,346,103,ablation-analysis,entity - enriched kb,increases,hits@1,entity - enriched kb increases hits@1,0.6895099878311157
translation,346,103,ablation-analysis,hits@1,by,0.9 %,hits@1 by 0.9 %,0.6085237264633179
translation,346,103,ablation-analysis,ablation analysis,has,strategy,ablation analysis has strategy,0.489571213722229
translation,346,68,baselines,kvmemnet,is,end-toend memory network,kvmemnet is end-toend memory network,0.5887422561645508
translation,346,68,baselines,end-toend memory network,stores,kb facts and text,end-toend memory network stores kb facts and text,0.7728058099746704
translation,346,68,baselines,kb facts and text,into,key -value pairs,kb facts and text into key -value pairs,0.5449547171592712
translation,346,69,baselines,kb and text,with,early fusion strategy,kb and text with early fusion strategy,0.6810042262077332
translation,346,73,experimental-setup,experimental setup,implemented in,"pytorch ( paszke et al. , 2019 )","experimental setup implemented in pytorch ( paszke et al. , 2019 )",0.6700041890144348
translation,346,73,experimental-setup,experimental setup,trained on,one nvidia tesla p40 gpu,experimental setup trained on one nvidia tesla p40 gpu,0.707068920135498
translation,346,74,experimental-setup,100 - dimensional transe embeddings,for,entities and relations,100 - dimensional transe embeddings for entities and relations,0.6266329288482666
translation,346,74,experimental-setup,),for,entities and relations,) for entities and relations,0.6302844285964966
translation,346,74,experimental-setup,"300 - dimensional glove embeddings ( pennington et al. , 2014 )",for,question and text words,"300 - dimensional glove embeddings ( pennington et al. , 2014 ) for question and text words",0.569007396697998
translation,346,74,experimental-setup,100 - dimensional transe embeddings,has,),100 - dimensional transe embeddings has ),0.6222997307777405
translation,346,74,experimental-setup,experimental setup,apply,100 - dimensional transe embeddings,experimental setup apply 100 - dimensional transe embeddings,0.5881854891777039
translation,346,74,experimental-setup,experimental setup,apply,"300 - dimensional glove embeddings ( pennington et al. , 2014 )","experimental setup apply 300 - dimensional glove embeddings ( pennington et al. , 2014 )",0.5509886741638184
translation,346,75,experimental-setup,word numbers,of,questions and documents,word numbers of questions and documents,0.5870936512947083
translation,346,75,experimental-setup,word numbers,limited to be,10 and 50,word numbers limited to be 10 and 50,0.7183960676193237
translation,346,75,experimental-setup,questions and documents,limited to be,10 and 50,questions and documents limited to be 10 and 50,0.7202211022377014
translation,346,75,experimental-setup,experimental setup,has,word numbers,experimental setup has word numbers,0.5218406915664673
translation,346,76,experimental-setup,hidden size,set to,100,hidden size set to 100,0.7469037175178528
translation,346,76,experimental-setup,experimental setup,has,hidden size,experimental setup has hidden size,0.5616794228553772
translation,346,78,experimental-setup,dropout,is,0.2,dropout is 0.2,0.541784942150116
translation,346,78,experimental-setup,batch size,is,8,batch size is 8,0.655981183052063
translation,346,78,experimental-setup,experimental setup,has,dropout,experimental setup has dropout,0.5067690014839172
translation,346,78,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,346,81,experimental-setup,number of parameters,is,69 million,number of parameters is 69 million,0.6015756726264954
translation,346,81,experimental-setup,experimental setup,has,number of parameters,experimental setup has number of parameters,0.5264257788658142
translation,346,82,experimental-setup,adam optimizer,applied to minimize,binary cross-entropy loss,adam optimizer applied to minimize binary cross-entropy loss,0.6495945453643799
translation,346,82,experimental-setup,binary cross-entropy loss,with,learning rate,binary cross-entropy loss with learning rate,0.5686848759651184
translation,346,82,experimental-setup,learning rate,of,0.0005,learning rate of 0.0005,0.6050455570220947
translation,346,82,experimental-setup,experimental setup,has,adam optimizer,experimental setup has adam optimizer,0.5293667316436768
translation,346,80,experiments,average runtime,for,one epoch,average runtime for one epoch,0.5513083934783936
translation,346,80,experiments,one epoch,is,5 minutes,one epoch is 5 minutes,0.6097164750099182
translation,346,80,experiments,max number of epochs,to,200,max number of epochs to 200,0.5950810313224792
translation,346,5,model,qa method,leveraging,text information,qa method leveraging text information,0.6555419564247131
translation,346,5,model,text information,to enhance,incomplete kb,text information to enhance incomplete kb,0.692889392375946
translation,346,5,model,model,proposes,qa method,model proposes qa method,0.7656636238098145
translation,346,6,model,entity representation,through,semantic information,entity representation through semantic information,0.5408827662467957
translation,346,6,model,semantic information,contained in,text,semantic information contained in text,0.6246965527534485
translation,346,6,model,graph convolutional networks,to update,entity status,graph convolutional networks to update entity status,0.6993737816810608
translation,346,6,model,model,enriches,entity representation,model enriches entity representation,0.6778078079223633
translation,346,6,model,model,employs,graph convolutional networks,model employs graph convolutional networks,0.5010080337524414
translation,346,7,model,latent structural information,of,text,latent structural information of text,0.5875880122184753
translation,346,7,model,latent structural information,of,text,latent structural information of text,0.5875880122184753
translation,346,7,model,latent structural information,treat,text,latent structural information treat text,0.5943851470947266
translation,346,7,model,text,as,hyperedges,text as hyperedges,0.5424433946609497
translation,346,7,model,hyperedges,connecting,entities,hyperedges connecting entities,0.7092463970184326
translation,346,7,model,entities,to complement,deficient relations,entities to complement deficient relations,0.6362839341163635
translation,346,7,model,deficient relations,in,kb,deficient relations in kb,0.5755984783172607
translation,346,7,model,hypergraph convolutional networks,applied to,reason,hypergraph convolutional networks applied to reason,0.669411301612854
translation,346,7,model,reason,on,hypergraph - formed text,reason on hypergraph - formed text,0.5809766054153442
translation,346,7,model,model,to exploit,latent structural information,model to exploit latent structural information,0.7199831008911133
translation,346,20,model,novel qa model,based on,text enhanced knowledge graph,novel qa model based on text enhanced knowledge graph,0.6301563382148743
translation,346,20,model,entity representation,by,text semantic information,entity representation by text semantic information,0.49898943305015564
translation,346,20,model,entity representation,complements,relations,entity representation complements relations,0.7223864793777466
translation,346,20,model,relations,through,structural information,relations through structural information,0.6240708827972412
translation,346,20,model,relations,has,in kb,relations has in kb,0.6157453656196594
translation,346,20,model,structural information,has,of the text,structural information has of the text,0.5836318731307983
translation,346,20,model,model,propose,novel qa model,model propose novel qa model,0.7111940979957581
translation,346,21,model,entities,in,kb,entities in kb,0.5280968546867371
translation,346,21,model,entities,in,kb,entities in kb,0.5280968546867371
translation,346,21,model,entities,combining,text information,entities combining text information,0.6835177540779114
translation,346,21,model,model,firstly encodes,entities,model firstly encodes entities,0.697302520275116
translation,346,21,model,model,applies,graph convolutional networks ( gcn ),model applies graph convolutional networks ( gcn ),0.5763562321662903
translation,346,79,model,gcn layer l 1 and hgcn layer l 2,are,1 and 2,gcn layer l 1 and hgcn layer l 2 are 1 and 2,0.6015598773956299
translation,346,79,model,model,has,gcn layer l 1 and hgcn layer l 2,model has gcn layer l 1 and hgcn layer l 2,0.5336973071098328
translation,346,26,results,extensive experiments,conducted on,"widely used webquestionssp ( yih et al. , 2016 )","extensive experiments conducted on widely used webquestionssp ( yih et al. , 2016 )",0.6551522016525269
translation,346,87,results,our model,gets,competitive performance,our model gets competitive performance,0.6074209213256836
translation,346,87,results,our model,achieves,best results,our model achieves best results,0.6756261587142944
translation,346,87,results,competitive performance,in,kb -only setting,competitive performance in kb -only setting,0.5617392063140869
translation,346,87,results,best results,in,other two settings,best results in other two settings,0.5144108533859253
translation,346,87,results,results,has,our model,results has our model,0.5871725678443909
translation,346,89,results,our method,achieves,best performance,our method achieves best performance,0.6578624248504639
translation,346,89,results,best performance,proving,our proposed enhancement strategy,best performance proving our proposed enhancement strategy,0.6942529678344727
translation,346,89,results,our proposed enhancement strategy,effectively enhance,incomplete kb,our proposed enhancement strategy effectively enhance incomplete kb,0.6578383445739746
translation,346,89,results,incomplete kb,by fully introducing,semantic and structural information,incomplete kb by fully introducing semantic and structural information,0.6881491541862488
translation,346,89,results,semantic and structural information,implied in,text,semantic and structural information implied in text,0.6800134181976318
translation,346,89,results,kb + text 's setting,has,our method,kb + text 's setting has our method,0.594887912273407
translation,346,92,results,our model,achieves,best results,our model achieves best results,0.6756261587142944
translation,346,92,results,best results,compared with,baseline methods,best results compared with baseline methods,0.6376683115959167
translation,346,92,results,text,has,our model,text has our model,0.6233155727386475
translation,346,92,results,results,After combining,text,results After combining text,0.6732202768325806
translation,346,93,results,performance,in,kb +,performance in kb +,0.5483080744743347
translation,346,94,results,significantly improved,over,kb -only setting,significantly improved over kb -only setting,0.675409197807312
translation,346,94,results,results,has,text setting,results has text setting,0.4566328823566437
translation,346,97,results,our method,achieves,largest or almost the largest increment,our method achieves largest or almost the largest increment,0.6309967041015625
translation,346,97,results,results,observe,our method,results observe our method,0.5780253410339355
translation,346,101,results,results,under,10 % kb setting,results under 10 % kb setting,0.6373268961906433
translation,347,8,model,cqa retrieval technique,embeds,question - answer pairs,cqa retrieval technique embeds question - answer pairs,0.6543213129043579
translation,347,8,model,question - answer pairs,within,unified latent space,question - answer pairs within unified latent space,0.6442828178405762
translation,347,8,model,local neighborhood structure,of,question and answer spaces,local neighborhood structure of question and answer spaces,0.5783470273017883
translation,347,8,model,cqa retrieval technique,has,laser - qa,cqa retrieval technique has laser - qa,0.5796622037887573
translation,347,8,model,model,devise,cqa retrieval technique,model devise cqa retrieval technique,0.7204152941703796
translation,348,154,ablation-analysis,copy mechanism,aid,question generation,copy mechanism aid question generation,0.6694293022155762
translation,348,154,ablation-analysis,coreference resolution,aid,question generation,coreference resolution aid question generation,0.5979649424552917
translation,348,154,ablation-analysis,copy mechanism,has,answer features,copy mechanism has answer features,0.6041068434715271
translation,348,154,ablation-analysis,ablation analysis,shows,copy mechanism,ablation analysis shows copy mechanism,0.6396424174308777
translation,348,159,ablation-analysis,performance drop,of,corefnqg - gating,performance drop of corefnqg - gating,0.6100779175758362
translation,348,159,ablation-analysis,corefnqg - gating,shows that,gating network,corefnqg - gating shows that gating network,0.6613810658454895
translation,348,159,ablation-analysis,gating network,playing,important role,gating network playing important role,0.6955496668815613
translation,348,159,ablation-analysis,important role,for getting,refined coreference position feature embedding,important role for getting refined coreference position feature embedding,0.5985295176506042
translation,348,159,ablation-analysis,refined coreference position feature embedding,helps,model,refined coreference position feature embedding helps model,0.5317380428314209
translation,348,159,ablation-analysis,model,learn,importance,model learn importance,0.6098143458366394
translation,348,159,ablation-analysis,importance,of,antecedent,importance of antecedent,0.5911726951599121
translation,348,160,ablation-analysis,performance drop,of,corefnqg -mention - pair score,performance drop of corefnqg -mention - pair score,0.5195853114128113
translation,348,160,ablation-analysis,corefnqg -mention - pair score,shows,mention,corefnqg -mention - pair score shows mention,0.6226599216461182
translation,348,160,ablation-analysis,neural network,better encode,coreference knowledge,neural network better encode coreference knowledge,0.7178846597671509
translation,348,160,ablation-analysis,ablation analysis,has,performance drop,ablation analysis has performance drop,0.5563539266586304
translation,348,143,baselines,seq2seq + copy w/ answer,is,attention - based sequence - to-sequence model,seq2seq + copy w/ answer is attention - based sequence - to-sequence model,0.5641170144081116
translation,348,143,baselines,attention - based sequence - to-sequence model,augmented with,copy mechanism,attention - based sequence - to-sequence model augmented with copy mechanism,0.7163982391357422
translation,348,143,baselines,copy mechanism,with,answer features,copy mechanism with answer features,0.6540347933769226
translation,348,143,baselines,answer features,concatenated with,word embeddings,answer features concatenated with word embeddings,0.6265602707862854
translation,348,143,baselines,word embeddings,during,encoding,word embeddings during encoding,0.6839070320129395
translation,348,143,baselines,baselines,has,seq2seq + copy w/ answer,baselines has seq2seq + copy w/ answer,0.5623293519020081
translation,348,7,experiments,our system,composed of,answer span extraction system,our system composed of answer span extraction system,0.6179471611976624
translation,348,7,experiments,our system,composed of,passage - level qg system,our system composed of passage - level qg system,0.6702107787132263
translation,348,7,experiments,our system,to,"10,000 top-ranking wikipedia articles","our system to 10,000 top-ranking wikipedia articles",0.5142177939414978
translation,348,7,experiments,our system,create,corpus,our system create corpus,0.6463557481765747
translation,348,7,experiments,corpus,of,over one million questionanswer pairs,corpus of over one million questionanswer pairs,0.5643686056137085
translation,348,212,experiments,docreader,on,training set,docreader on training set,0.5572063326835632
translation,348,212,experiments,models ' performance,on,original dev set,models ' performance on original dev set,0.5189489722251892
translation,348,212,experiments,original dev set,of,squad,original dev set of squad,0.6017104983329773
translation,348,212,experiments,performance,around,45.2 %,performance around 45.2 %,0.6597932577133179
translation,348,212,experiments,performance,around,56.7 %,performance around 56.7 %,0.6549005508422852
translation,348,212,experiments,45.2 %,on,em,45.2 % on em,0.6614478230476379
translation,348,212,experiments,56.7 %,on,f - 1 metric,56.7 % on f - 1 metric,0.5652588605880737
translation,348,213,experiments,docreader,trained on,original squad training set,docreader trained on original squad training set,0.7476483583450317
translation,348,213,experiments,original squad training set,achieves,69.5 % em,original squad training set achieves 69.5 % em,0.6689249873161316
translation,348,213,experiments,original squad training set,achieves,78.8 % f - 1,original squad training set achieves 78.8 % f - 1,0.6369580030441284
translation,348,5,model,neural network approach,incorporates,coreference knowledge,neural network approach incorporates coreference knowledge,0.6613301634788513
translation,348,5,model,coreference knowledge,via,novel gating mechanism,coreference knowledge via novel gating mechanism,0.6794293522834778
translation,348,5,model,model,propose,neural network approach,model propose neural network approach,0.7160876393318176
translation,348,34,model,gated coreference knowledge,for,neural question generation ( corefnqg ),gated coreference knowledge for neural question generation ( corefnqg ),0.6183874607086182
translation,348,34,model,neural sequence model,with,novel gating mechanism,neural sequence model with novel gating mechanism,0.6259137988090515
translation,348,34,model,novel gating mechanism,leverages,continuous representations,novel gating mechanism leverages continuous representations,0.7227758169174194
translation,348,34,model,continuous representations,of,coreference clusters,continuous representations of coreference clusters,0.593059241771698
translation,348,34,model,gated coreference knowledge,has,neural sequence model,gated coreference knowledge has neural sequence model,0.5404708981513977
translation,348,34,model,neural question generation ( corefnqg ),has,neural sequence model,neural question generation ( corefnqg ) has neural sequence model,0.5771622657775879
translation,348,34,model,model,propose,gated coreference knowledge,model propose gated coreference knowledge,0.6499495506286621
translation,348,6,results,linguistic knowledge,introduced by,coreference representation,linguistic knowledge introduced by coreference representation,0.6450244784355164
translation,348,6,results,coreference representation,has,aids,coreference representation has aids,0.5692227482795715
translation,348,6,results,aids,has,question generation,aids has question generation,0.5514081120491028
translation,348,6,results,question generation,has,significantly,question generation has significantly,0.602264940738678
translation,348,35,results,squad dataset,find that,corefnqg,squad dataset find that corefnqg,0.5884838700294495
translation,348,35,results,corefnqg,enables,better question generation,corefnqg enables better question generation,0.6865167021751404
translation,348,36,results,baseline neural sequence models,that encode,information,baseline neural sequence models that encode information,0.7085179686546326
translation,348,36,results,information,from,single sentence,information from single sentence,0.5484383702278137
translation,348,36,results,outperforms significantly,has,baseline neural sequence models,outperforms significantly has baseline neural sequence models,0.5933505296707153
translation,348,36,results,results,has,outperforms significantly,results has outperforms significantly,0.6500969529151917
translation,348,153,results,seq2seq baseline,by,large margin,seq2seq baseline by large margin,0.5399442315101624
translation,348,153,results,corefnqg,has,outperforms,corefnqg has outperforms,0.6492885947227478
translation,348,153,results,outperforms,has,seq2seq baseline,outperforms has seq2seq baseline,0.5656357407569885
translation,348,153,results,results,has,corefnqg,results has corefnqg,0.5627557039260864
translation,348,155,results,corefnqg,has,outperforms,corefnqg has outperforms,0.6492885947227478
translation,348,155,results,outperforms,has,both seq2seq + copy models,outperforms has both seq2seq + copy models,0.5705274939537048
translation,348,155,results,both seq2seq + copy models,has,significantly,both seq2seq + copy models has significantly,0.597347617149353
translation,348,155,results,results,has,corefnqg,results has corefnqg,0.5627557039260864
translation,348,162,results,gaps,of,performance,gaps of performance,0.6274337768554688
translation,348,162,results,performance,between,our model and the baseline models,performance between our model and the baseline models,0.6013702154159546
translation,348,162,results,performance,still,significant,performance still significant,0.706364631652832
translation,348,162,results,our model and the baseline models,are,significant,our model and the baseline models are significant,0.5965478420257568
translation,348,162,results,our model and the baseline models,still,significant,our model and the baseline models still significant,0.6958138346672058
translation,348,162,results,results,has,gaps,results has gaps,0.5407499074935913
translation,348,163,results,results,see that,all three systems ' performance drop,results see that all three systems ' performance drop,0.6316314935684204
translation,348,170,results,baseline models,across,all metrics,baseline models across all metrics,0.6676902174949646
translation,348,170,results,outperforms,has,baseline models,outperforms has baseline models,0.5940393209457397
translation,348,170,results,results,see that,corefnqg,results see that corefnqg,0.6393178105354309
translation,348,173,results,human-generated questions,are,more natural and varied in form,human-generated questions are more natural and varied in form,0.5586301684379578
translation,348,173,results,more natural and varied in form,with,better paraphrasing,more natural and varied in form with better paraphrasing,0.6276653409004211
translation,348,173,results,results,see that,human-generated questions,results see that human-generated questions,0.6078569293022156
translation,348,175,results,all variants of bilstm models,has,outperform,all variants of bilstm models has outperform,0.5834471583366394
translation,348,175,results,outperform,has,off- the-shelf ner system,outperform has off- the-shelf ner system,0.6001238226890564
translation,348,175,results,results,see that,all variants of bilstm models,results see that all variants of bilstm models,0.5560181736946106
translation,348,176,results,bilstm - crf,encodes,character - level and ner features,bilstm - crf encodes character - level and ner features,0.6687062978744507
translation,348,176,results,bilstm - crf,performs,best,bilstm - crf performs best,0.5900893807411194
translation,348,176,results,character - level and ner features,for,each token,character - level and ner features for each token,0.5638564229011536
translation,348,176,results,best,in terms of,f-measure,best in terms of f-measure,0.681032121181488
translation,348,176,results,results,has,bilstm - crf,results has bilstm - crf,0.5153601765632629
translation,348,195,results,neural models,do,quite well,neural models do quite well,0.4816638231277466
translation,348,195,results,grammaticality,has,neural models,grammaticality has neural models,0.518625020980835
translation,348,195,results,results,In terms of,grammaticality,results In terms of grammaticality,0.6321412324905396
translation,348,196,results,our method ( corefnqg ),performs,statistically significantly better,our method ( corefnqg ) performs statistically significantly better,0.6265690326690674
translation,348,196,results,statistically significantly better,across,all metrics,statistically significantly better across all metrics,0.6755262613296509
translation,348,196,results,statistically significantly better,in comparison to,baseline model ( contextnqg ),statistically significantly better in comparison to baseline model ( contextnqg ),0.6804686784744263
translation,348,196,results,results,see that,our method ( corefnqg ),results see that our method ( corefnqg ),0.6383662819862366
translation,349,22,model,novel model,for,relation extraction from cqa data,novel model for relation extraction from cqa data,0.556489884853363
translation,349,22,model,facts,between,entities,facts between entities,0.6448839902877808
translation,349,22,model,facts,entities mentioned in,answer sentences,facts entities mentioned in answer sentences,0.7202354073524475
translation,349,22,model,entities,mentioned in,question,entities mentioned in question,0.6789332628250122
translation,349,22,model,entities,mentioned in,answer sentences,entities mentioned in answer sentences,0.6013520956039429
translation,349,22,model,model,propose,novel model,model propose novel model,0.6891457438468933
translation,349,80,results,precision - recall curves,for,qnabased and sentence - based baseline models,precision - recall curves for qnabased and sentence - based baseline models,0.5608907341957092
translation,349,83,results,20.5 % to 39.4 %,of,correct triples,20.5 % to 39.4 % of correct triples,0.5863131880760193
translation,349,83,results,20.5 % to 39.4 %,not extracted by,baseline model,20.5 % to 39.4 % not extracted by baseline model,0.65584397315979
translation,349,83,results,qna - based model,not extracted by,baseline model,qna - based model not extracted by baseline model,0.6509379744529724
translation,349,83,results,combination of both models,able to achieve,higher precision and recall,combination of both models able to achieve higher precision and recall,0.6525639891624451
translation,349,83,results,results,from,20.5 % to 39.4 %,results from 20.5 % to 39.4 %,0.49901559948921204
translation,350,98,ablation-analysis,initial question ( pos1 ),behind,dynsp,initial question ( pos1 ) behind dynsp,0.6659342050552368
translation,350,98,ablation-analysis,dynsp,by,3.7 %,dynsp by 3.7 %,0.6041857600212097
translation,350,98,ablation-analysis,ablation analysis,For,initial question ( pos1 ),ablation analysis For initial question ( pos1 ),0.6239165663719177
translation,350,102,ablation-analysis,average question accuracy,jumps to,61.7 %,average question accuracy jumps to 61.7 %,0.6679832935333252
translation,350,102,ablation-analysis,61.7 %,showing,6.6,61.7 % showing 6.6,0.6399582624435425
translation,350,102,ablation-analysis,previous reference answers,has,average question accuracy,previous reference answers has average question accuracy,0.5346751809120178
translation,350,102,ablation-analysis,ablation analysis,provide,previous reference answers,ablation analysis provide previous reference answers,0.6045807600021362
translation,350,105,ablation-analysis,decreases,from,55.1 %,decreases from 55.1 %,0.6032599806785583
translation,350,105,ablation-analysis,overall accuracy,has,decreases,overall accuracy has decreases,0.5960311889648438
translation,350,105,ablation-analysis,55.1 %,has,to 51.5 %,55.1 % has to 51.5 %,0.5334533452987671
translation,350,107,ablation-analysis,model,without,special number handling,model without special number handling,0.7612072229385376
translation,350,107,ablation-analysis,model,out -performs,previous sota camp,model out -performs previous sota camp,0.7684974670410156
translation,350,107,ablation-analysis,previous sota camp,by,more than 5 points,previous sota camp by more than 5 points,0.6233824491500854
translation,350,107,ablation-analysis,more than 5 points,has,45.6 % vs 55.1 % ),more than 5 points has 45.6 % vs 55.1 % ),0.5358726978302002
translation,350,107,ablation-analysis,ablation analysis,call out,model,ablation analysis call out model,0.6634171009063721
translation,350,93,experimental-setup,adam optimizer,for,optimization,adam optimizer for optimization,0.5982468128204346
translation,350,93,experimental-setup,tune hyperparameters,with,"google vizier ( golovin et al. , 2017 )","tune hyperparameters with google vizier ( golovin et al. , 2017 )",0.5532094240188599
translation,350,93,experimental-setup,experimental setup,use,adam optimizer,experimental setup use adam optimizer,0.5987385511398315
translation,350,93,experimental-setup,experimental setup,use,tune hyperparameters,experimental setup use tune hyperparameters,0.6024694442749023
translation,350,5,model,tables,as,graphs,tables as graphs,0.5596903562545776
translation,350,5,model,tables,using,graph neural network model,tables using graph neural network model,0.6946754455566406
translation,350,5,model,graphs,using,graph neural network model,graphs using graph neural network model,0.6389872431755066
translation,350,5,model,graph neural network model,based on,transformer architecture,graph neural network model based on transformer architecture,0.6871213912963867
translation,350,5,model,model,encode,tables,model encode tables,0.6885556578636169
translation,350,17,model,novel approach,encodes,structured resources,novel approach encodes structured resources,0.7076823711395264
translation,350,17,model,structured resources,along with,questions and answers,structured resources along with questions and answers,0.5900729298591614
translation,350,17,model,structured resources,has,i.e. tables,structured resources has i.e. tables,0.5511706471443176
translation,350,17,model,model,propose,novel approach,model propose novel approach,0.7168048620223999
translation,350,18,model,conversational context,without,vocabulary,conversational context without vocabulary,0.690722644329071
translation,350,18,model,conversational context,definition of,detailed operations,conversational context definition of detailed operations,0.6976197957992554
translation,350,18,model,conversational context,definition of,vocabulary,conversational context definition of vocabulary,0.6950383186340332
translation,350,18,model,vocabulary,dependent on,logical form formalism,vocabulary dependent on logical form formalism,0.6467729210853577
translation,350,18,model,logical form formalism,required in,weakly supervised semantic parsing,logical form formalism required in weakly supervised semantic parsing,0.5821102261543274
translation,350,21,model,qa model,for,sequence of questions,qa model for sequence of questions,0.6206265687942505
translation,350,21,model,model,build,qa model,model build qa model,0.7679067254066467
translation,350,61,model,graph neural network ( gnn ) encoder,based on,"transformer ( vaswani et al. , 2017 )","graph neural network ( gnn ) encoder based on transformer ( vaswani et al. , 2017 )",0.6595017910003662
translation,350,61,model,model,use,graph neural network ( gnn ) encoder,model use graph neural network ( gnn ) encoder,0.6474657654762268
translation,350,70,model,transformer decoder,to include,copy mechanism,transformer decoder to include copy mechanism,0.6692085862159729
translation,350,70,model,copy mechanism,based on,pointer network,copy mechanism based on pointer network,0.6412960290908813
translation,350,70,model,model,extend,transformer decoder,model extend transformer decoder,0.7713286876678467
translation,350,97,results,our model,improves,sota,our model improves sota,0.7549527287483215
translation,350,97,results,sota,from,45.6 %,sota from 45.6 %,0.5498830676078796
translation,350,97,results,sota,reducing,relative error rate,sota reducing relative error rate,0.6572880148887634
translation,350,97,results,45.6 %,by,camp,45.6 % by camp,0.6200376152992249
translation,350,97,results,camp,to,55.1 %,camp to 55.1 %,0.5975608825683594
translation,350,97,results,55.1 %,in,question accuracy ( all ),55.1 % in question accuracy ( all ),0.48988214135169983
translation,350,97,results,relative error rate,by,18 %,relative error rate by 18 %,0.6031906008720398
translation,350,97,results,dynsp,has,"iyyer et al. , 2017 )","dynsp has iyyer et al. , 2017 )",0.5877079963684082
translation,350,97,results,results,observe,our model,results observe our model,0.6353915333747864
translation,350,99,results,our model,handles,follow up questions,our model handles follow up questions,0.7435810565948486
translation,350,99,results,previously best model fp,by,20 %,previously best model fp by 20 %,0.5875058174133301
translation,350,99,results,20 %,on,pos3,20 % on pos3,0.6707882881164551
translation,350,99,results,outperforming,has,previously best model fp,outperforming has previously best model fp,0.6086657643318176
translation,350,99,results,results,has,our model,results has our model,0.5871725678443909
translation,350,101,results,our model,effectively leverages,context information,our model effectively leverages context information,0.6892278790473938
translation,350,101,results,context information,by improving,average question accuracy,context information by improving average question accuracy,0.6461140513420105
translation,350,101,results,average question accuracy,from,45.1 % to 55.1 %,average question accuracy from 45.1 % to 55.1 %,0.5194012522697449
translation,350,101,results,45.1 % to 55.1 %,in comparison to,use of context,45.1 % to 55.1 % in comparison to use of context,0.5991823673248291
translation,350,101,results,use of context,in,dynsp,use of context in dynsp,0.5912878513336182
translation,350,101,results,use of context,yielding,2.7 % improvement,use of context yielding 2.7 % improvement,0.6251600980758667
translation,350,101,results,results,observe,our model,results observe our model,0.6353915333747864
translation,351,107,experimental-setup,barqa,for,five epochs,barqa for five epochs,0.6559141874313354
translation,351,107,experimental-setup,barqa,with,learning rate,barqa with learning rate,0.5935499668121338
translation,351,107,experimental-setup,barqa,with,batch size,barqa with batch size,0.6658052206039429
translation,351,107,experimental-setup,learning rate,of,3e - 5,learning rate of 3e - 5,0.6390295624732971
translation,351,107,experimental-setup,batch size,of,24,batch size of 24,0.6606289148330688
translation,351,7,model,model,present,question answering framework ( barqa ),model present question answering framework ( barqa ),0.6417734622955322
translation,351,8,model,model,propose,novel method,model propose novel method,0.7230806350708008
translation,351,35,model,new method,to generate,large amount of   quasi- bridging   training data,new method to generate large amount of   quasi- bridging   training data,0.6859092712402344
translation,351,35,model,large amount of   quasi- bridging   training data,from,automatically parsed gigaword corpus,large amount of   quasi- bridging   training data from automatically parsed gigaword corpus,0.4853452146053314
translation,351,35,model,model,propose,new method,model propose new method,0.675626814365387
translation,351,218,model,model,propose,new method,model propose new method,0.675626814365387
translation,351,36,results,our   quasi- bridging   training data,is,better pre-training choice,our   quasi- bridging   training data is better pre-training choice,0.5450119376182556
translation,351,36,results,better pre-training choice,for,bridging anaphora resolution,better pre-training choice for bridging anaphora resolution,0.5869584083557129
translation,351,36,results,bridging anaphora resolution,compared to,squad corpus,bridging anaphora resolution compared to squad corpus,0.5724098682403564
translation,351,36,results,results,demonstrate,our   quasi- bridging   training data,results demonstrate our   quasi- bridging   training data,0.5759792327880859
translation,351,167,results,isnotes,find that,barqa,isnotes find that barqa,0.730945885181427
translation,351,167,results,barqa,trained on,small number of in- domain dataset ( bashi ),barqa trained on small number of in- domain dataset ( bashi ),0.7243353724479675
translation,351,167,results,barqa,achieves,accuracy,barqa achieves accuracy,0.7018792629241943
translation,351,167,results,accuracy,of,38.16 %,accuracy of 38.16 %,0.5607311725616455
translation,351,167,results,accuracy,better than,model,accuracy better than model,0.7299808263778687
translation,351,167,results,38.16 %,on,isnotes,38.16 % on isnotes,0.5724250078201294
translation,351,167,results,38.16 %,better than,model,38.16 % better than model,0.7593395709991455
translation,351,167,results,model,trained on,other two large-scale datasets,model trained on other two large-scale datasets,0.7289859652519226
translation,351,167,results,results,on,isnotes,results on isnotes,0.5901305079460144
translation,351,168,results,better results,compared to using,bashi,better results compared to using bashi,0.6686053276062012
translation,351,168,results,bashi,as,only training dataset,bashi as only training dataset,0.48529425263404846
translation,351,170,results,performance,of using,quasibridging alone,performance of using quasibridging alone,0.7365514039993286
translation,351,170,results,quasibridging alone,worse than,squad 1.1 only,quasibridging alone worse than squad 1.1 only,0.7521640658378601
translation,351,170,results,results,notice that,performance,results notice that performance,0.6314935088157654
translation,351,171,results,qua-sibridging and bashi,achieves,best result,qua-sibridging and bashi achieves best result,0.6909083724021912
translation,351,171,results,best result,on,isnotes,best result on isnotes,0.5785597562789917
translation,351,171,results,best result,with,accuracy,best result with accuracy,0.6465001106262207
translation,351,171,results,accuracy,of,47.21 %,accuracy of 47.21 %,0.5536697506904602
translation,351,171,results,results,combining,qua-sibridging and bashi,results combining qua-sibridging and bashi,0.7272334694862366
translation,351,174,results,model,on,quasibridging,model on quasibridging,0.5924827456474304
translation,351,174,results,fine-tuning,on,isnotes,fine-tuning on isnotes,0.5812971591949463
translation,351,174,results,best result,with,accuracy,best result with accuracy,0.6465001106262207
translation,351,174,results,accuracy,of,37.79 %,accuracy of 37.79 %,0.5546749830245972
translation,351,174,results,results,Pre-training,model,results Pre-training model,0.6919490694999695
translation,351,187,results,our system barqa,when using,gold mentions,our system barqa when using gold mentions,0.6331415176391602
translation,351,187,results,our system barqa,achieves,new state - of - the - art result,our system barqa achieves new state - of - the - art result,0.6709029078483582
translation,351,187,results,gold mentions,together with,semantic information,gold mentions together with semantic information,0.6293346285820007
translation,351,187,results,semantic information,to further prune,span predictions,semantic information to further prune span predictions,0.7036164402961731
translation,351,187,results,new state - of - the - art result,on,isnotes,new state - of - the - art result on isnotes,0.6075223684310913
translation,351,187,results,new state - of - the - art result,with,strict accuracy,new state - of - the - art result with strict accuracy,0.6450106501579285
translation,351,187,results,strict accuracy,of,50.08 %,strict accuracy of 50.08 %,0.5588484406471252
translation,351,187,results,results,has,our system barqa,results has our system barqa,0.5599598288536072
translation,351,192,results,our system barqa,without,mention / semantic information,our system barqa without mention / semantic information,0.6574628353118896
translation,351,192,results,our system barqa,any,mention / semantic information,our system barqa any mention / semantic information,0.6431082487106323
translation,351,192,results,our system barqa,achieves,accuracy,our system barqa achieves accuracy,0.6808068156242371
translation,351,192,results,accuracy,of,32.27 %,accuracy of 32.27 %,0.5605531334877014
translation,351,192,results,results,has,our system barqa,results has our system barqa,0.5599598288536072
translation,351,193,results,result,of,barqa,result of barqa,0.654107391834259
translation,351,193,results,barqa,further improved,accuracy,barqa further improved accuracy,0.7145817875862122
translation,351,193,results,accuracy,of,38.66 %,accuracy of 38.66 %,0.5586462020874023
translation,351,193,results,38.66 %,integrate,mention / semantic information,38.66 % integrate mention / semantic information,0.6109134554862976
translation,351,193,results,mention / semantic information,into,model,mention / semantic information into model,0.5759233832359314
translation,351,193,results,results,of,barqa,results of barqa,0.5951626300811768
translation,351,193,results,results,has,result,results has result,0.5303357243537903
translation,351,215,results,our barqa model,better at resolving,context-dependent bridging anaphors,our barqa model better at resolving context-dependent bridging anaphors,0.6509594917297363
translation,351,215,results,results,seems that,our barqa model,results seems that our barqa model,0.6972383856773376
translation,352,7,baselines,standard machine learning methods,such as,support vector machines ( svm ),standard machine learning methods such as support vector machines ( svm ),0.6664463877677917
translation,352,7,baselines,standard machine learning methods,with,convolutional neural network ( cnn ),standard machine learning methods with convolutional neural network ( cnn ),0.6013117432594299
translation,352,7,baselines,support vector machines ( svm ),with,convolutional neural network ( cnn ),support vector machines ( svm ) with convolutional neural network ( cnn ),0.6256447434425354
translation,352,7,baselines,baselines,compare,standard machine learning methods,baselines compare standard machine learning methods,0.6553323268890381
translation,352,149,baselines,method,based on,jaccard coefficient,method based on jaccard coefficient,0.6560505032539368
translation,352,149,baselines,three main baselines,has,method,three main baselines has method,0.542158842086792
translation,352,149,baselines,baselines,explore,three main baselines,baselines explore three main baselines,0.6653535962104797
translation,352,194,baselines,svm,with,four additional shingling features,svm with four additional shingling features,0.6617770195007324
translation,352,194,baselines,found best,among,baselines,found best among baselines,0.6605368852615356
translation,352,194,baselines,baselines,has,svm,baselines has svm,0.5812874436378479
translation,352,205,experiments,word embeddings,trained on,two different corpora,word embeddings trained on two different corpora,0.6707955002784729
translation,352,157,hyperparameters,frequency threshold,to reduce,number of features,frequency threshold to reduce number of features,0.6728654503822327
translation,352,161,hyperparameters,word embeddings,used in,our experiments,word embeddings used in our experiments,0.5775914192199707
translation,352,161,hyperparameters,initialized,by means of,unsupervised pre-training,initialized by means of unsupervised pre-training,0.6520559787750244
translation,352,161,hyperparameters,hyperparameters,has,word embeddings,hyperparameters has word embeddings,0.4784160554409027
translation,352,162,hyperparameters,pre-training,using,skip-gram nn architecture,pre-training using skip-gram nn architecture,0.6623663902282715
translation,352,162,hyperparameters,skip-gram nn architecture,available in,word2vec 13 tool,skip-gram nn architecture available in word2vec 13 tool,0.6404871344566345
translation,352,162,hyperparameters,hyperparameters,perform,pre-training,hyperparameters perform pre-training,0.5585601925849915
translation,352,206,hyperparameters,both word embeddings,have,200 dimensions,both word embeddings have 200 dimensions,0.5595737099647522
translation,352,206,hyperparameters,hyperparameters,has,both word embeddings,hyperparameters has both word embeddings,0.4857286810874939
translation,352,24,model,convolutional neural network architecture,to detect,semantically equivalent questions,convolutional neural network architecture to detect semantically equivalent questions,0.6499602198600769
translation,352,24,model,model,propose,convolutional neural network architecture,model propose convolutional neural network architecture,0.6117611527442932
translation,352,25,model,words,into,word embeddings,words into word embeddings,0.5141636729240417
translation,352,25,model,words,using,large collection of unlabeled data,words using large collection of unlabeled data,0.6099722385406494
translation,352,25,model,word embeddings,using,large collection of unlabeled data,word embeddings using large collection of unlabeled data,0.5559451580047607
translation,352,25,model,convolutional network,to build,distributed vector representations,convolutional network to build distributed vector representations,0.6279609799385071
translation,352,25,model,distributed vector representations,for,pairs of questions,distributed vector representations for pairs of questions,0.6179836988449097
translation,352,229,model,method,identifying,semantically equivalent questions,method identifying semantically equivalent questions,0.5832070708274841
translation,352,229,model,semantically equivalent questions,based on,convolutional neural network,semantically equivalent questions based on convolutional neural network,0.5422096252441406
translation,352,229,model,model,propose,method,model propose method,0.6280754208564758
translation,352,45,results,word embeddings,pre-trained on,domain-specific data,word embeddings pre-trained on domain-specific data,0.7500993609428406
translation,352,45,results,word embeddings,achieve,very high performance,word embeddings achieve very high performance,0.5582097172737122
translation,352,45,results,bigger word embeddings,obtain,higher accuracy,bigger word embeddings obtain higher accuracy,0.5581785440444946
translation,352,45,results,in - domain word embeddings,provide,better performance,in - domain word embeddings provide better performance,0.5584956407546997
translation,352,45,results,better performance,independent of,training set size,better performance independent of training set size,0.7076927423477173
translation,352,45,results,in - domain word embeddings,achieve,relatively high accuracy,in - domain word embeddings achieve relatively high accuracy,0.572056233882904
translation,352,45,results,relatively high accuracy,even using,out - of- domain training data,relatively high accuracy even using out - of- domain training data,0.7011268734931946
translation,352,189,results,outperforms,by,significant margin,outperforms by significant margin,0.6588555574417114
translation,352,189,results,baselines,by,significant margin,baselines by significant margin,0.6298518180847168
translation,352,189,results,cnn with word embeddings,has,outperforms,cnn with word embeddings has outperforms,0.3651382625102997
translation,352,189,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
translation,352,189,results,results,has,cnn with word embeddings,results has cnn with word embeddings,0.5767918825149536
translation,352,190,results,algorithms,do not benefit from,code,algorithms do not benefit from code,0.6694468855857849
translation,352,190,results,algorithms,including,code,algorithms including code,0.7523717880249023
translation,352,190,results,results,indicate,algorithms,results indicate algorithms,0.6039696335792542
translation,352,195,results,best baseline,by,significant margin,best baseline by significant margin,0.5726962089538574
translation,352,195,results,cnn with word embeddings,has,outperforms,cnn with word embeddings has outperforms,0.3651382625102997
translation,352,195,results,outperforms,has,best baseline,outperforms has best baseline,0.6073977947235107
translation,352,195,results,results,has,cnn with word embeddings,results has cnn with word embeddings,0.5767918825149536
translation,352,207,results,in- domain data,is,more beneficial,in- domain data is more beneficial,0.5676066875457764
translation,352,207,results,more beneficial,for,network,more beneficial for network,0.6584454774856567
translation,352,207,results,results,training on,in- domain data,results training on in- domain data,0.7239258885383606
translation,352,214,results,accuracy,training with,full 24 k training set and 4 k subset,accuracy training with full 24 k training set and 4 k subset,0.7359954714775085
translation,352,214,results,full 24 k training set and 4 k subset,is,about 9 %,full 24 k training set and 4 k subset is about 9 %,0.5649543404579163
translation,352,214,results,full 24 k training set and 4 k subset,is,only about 1 %,full 24 k training set and 4 k subset is only about 1 %,0.5469656586647034
translation,352,214,results,about 9 %,for,svm,about 9 % for svm,0.7238537669181824
translation,352,214,results,only about 1 %,for,cnn,only about 1 % for cnn,0.7044010162353516
translation,352,214,results,results,difference in,accuracy,results difference in accuracy,0.6975123286247253
translation,352,218,results,cnn accuracy,on,meta test data ( 92.68 % ),cnn accuracy on meta test data ( 92.68 % ),0.5084294676780701
translation,352,218,results,results,see that,cnn accuracy,results see that cnn accuracy,0.6077084541320801
translation,352,221,results,indomain word embeddings,helps to achieve,relatively high accuracy,indomain word embeddings helps to achieve relatively high accuracy,0.628226101398468
translation,352,221,results,relatively high accuracy,on,test set,relatively high accuracy on test set,0.5285804271697998
translation,352,221,results,83.35 %,on,test set,83.35 % on test set,0.5214207172393799
translation,352,221,results,relatively high accuracy,has,83.35 %,relatively high accuracy has 83.35 %,0.5531719923019409
translation,352,221,results,results,using,indomain word embeddings,results using indomain word embeddings,0.6137592792510986
translation,352,223,results,cnn,with,in- domain word vectors,cnn with in- domain word vectors,0.6138691902160645
translation,352,223,results,outperforms,in identifying,semantically equivalent questions,outperforms in identifying semantically equivalent questions,0.6485459208488464
translation,352,223,results,vocabulary - based baselines,in identifying,semantically equivalent questions,vocabulary - based baselines in identifying semantically equivalent questions,0.6457653045654297
translation,352,223,results,in- domain word vectors,has,outperforms,in- domain word vectors has outperforms,0.606433093547821
translation,352,223,results,outperforms,has,vocabulary - based baselines,outperforms has vocabulary - based baselines,0.5892243981361389
translation,353,178,ablation-analysis,effectiveness,of,each component,effectiveness of each component,0.5642370581626892
translation,353,178,ablation-analysis,each component,of,our architecture,each component of our architecture,0.5783185958862305
translation,353,178,ablation-analysis,of adding commonsense information,on,narrativeqa validation set,of adding commonsense information on narrativeqa validation set,0.5941051244735718
translation,353,178,ablation-analysis,effectiveness,has,of adding commonsense information,effectiveness has of adding commonsense information,0.5880913138389587
translation,353,178,ablation-analysis,ablation analysis,tested,effectiveness,ablation analysis tested effectiveness,0.7521469593048096
translation,353,7,model,strong generative baseline,uses,multi-attention mechanism,strong generative baseline uses multi-attention mechanism,0.5464933514595032
translation,353,7,model,multi-attention mechanism,to perform,multiple hops of reasoning,multi-attention mechanism to perform multiple hops of reasoning,0.7012849450111389
translation,353,7,model,multi-attention mechanism,to perform,pointer - generator decoder,multi-attention mechanism to perform pointer - generator decoder,0.660544216632843
translation,353,7,model,pointer - generator decoder,to synthesize,answer,pointer - generator decoder to synthesize answer,0.6407467126846313
translation,353,7,model,model,present,strong generative baseline,model present strong generative baseline,0.6369480490684509
translation,353,9,model,novel system,selecting,grounded multi-hop relational commonsense information,novel system selecting grounded multi-hop relational commonsense information,0.7078456878662109
translation,353,9,model,grounded multi-hop relational commonsense information,from,con-ceptnet,grounded multi-hop relational commonsense information from con-ceptnet,0.5490317940711975
translation,353,9,model,grounded multi-hop relational commonsense information,via,term-frequency based scoring function,grounded multi-hop relational commonsense information via term-frequency based scoring function,0.6509107947349548
translation,353,9,model,model,introduce,novel system,model introduce novel system,0.6879348158836365
translation,353,10,model,extracted commonsense information,to fill in,gaps of reasoning,extracted commonsense information to fill in gaps of reasoning,0.7251980304718018
translation,353,10,model,gaps of reasoning,between,context hops,gaps of reasoning between context hops,0.6436094641685486
translation,353,10,model,gaps of reasoning,using,selectivelygated attention mechanism,gaps of reasoning using selectivelygated attention mechanism,0.6831544041633606
translation,353,10,model,model,use,extracted commonsense information,model use extracted commonsense information,0.6873727440834045
translation,353,26,model,strong baseline model,uses,multiple hops,strong baseline model uses multiple hops,0.612897515296936
translation,353,26,model,multiple hops,of,bidirectional attention,multiple hops of bidirectional attention,0.5925903916358948
translation,353,26,model,multiple hops,of,self-attention,multiple hops of self-attention,0.598964512348175
translation,353,26,model,multiple hops,of,pointer - generator decoder,multiple hops of pointer - generator decoder,0.597308337688446
translation,353,26,model,multiple hops,to effectively,read and reason,multiple hops to effectively read and reason,0.5855823159217834
translation,353,26,model,multiple hops,to effectively,synthesize,multiple hops to effectively synthesize,0.6077587008476257
translation,353,26,model,read and reason,within,long passage,read and reason within long passage,0.6351416707038879
translation,353,26,model,multi-hop pointer - generator model ( mhpgm ),has,strong baseline model,multi-hop pointer - generator model ( mhpgm ) has strong baseline model,0.5166460871696472
translation,353,26,model,synthesize,has,coherent response,synthesize has coherent response,0.5634093284606934
translation,353,26,model,model,first propose,multi-hop pointer - generator model ( mhpgm ),model first propose multi-hop pointer - generator model ( mhpgm ),0.6812554001808167
translation,353,29,model,novel method,of,inserting,novel method of inserting,0.6105448007583618
translation,353,29,model,selected commonsense paths,between,hops,selected commonsense paths between hops,0.6900529861450195
translation,353,29,model,selected commonsense paths,via,necessary and optional information cell ( noic ),selected commonsense paths via necessary and optional information cell ( noic ),0.6628089547157288
translation,353,29,model,hops,of,document-context reasoning,hops of document-context reasoning,0.5878605842590332
translation,353,29,model,document-context reasoning,within,our model,document-context reasoning within our model,0.6449522376060486
translation,353,29,model,necessary and optional information cell ( noic ),employs,selectivelygated attention mechanism,necessary and optional information cell ( noic ) employs selectivelygated attention mechanism,0.5453317165374756
translation,353,29,model,selectivelygated attention mechanism,utilizes,commonsense information,selectivelygated attention mechanism utilizes commonsense information,0.6163184642791748
translation,353,29,model,commonsense information,to effectively fill in,gaps of inference,commonsense information to effectively fill in gaps of inference,0.7215059399604797
translation,353,29,model,inserting,has,selected commonsense paths,inserting has selected commonsense paths,0.5871154069900513
translation,353,41,model,gated attention network,to create,baseline model,gated attention network to create baseline model,0.630581796169281
translation,353,41,model,baseline model,better suited for,complex mrc datasets,baseline model better suited for complex mrc datasets,0.7081596255302429
translation,353,41,model,baseline model,by improving,attention and gating mechanisms,baseline model by improving attention and gating mechanisms,0.6762405633926392
translation,353,41,model,baseline model,expanding,generation capabilities,baseline model expanding generation capabilities,0.735557496547699
translation,353,41,model,baseline model,allowing access to,external commonsense,baseline model allowing access to external commonsense,0.6833088397979736
translation,353,41,model,complex mrc datasets,such as,narrativeqa,complex mrc datasets such as narrativeqa,0.6799084544181824
translation,353,41,model,attention and gating mechanisms,expanding,generation capabilities,attention and gating mechanisms expanding generation capabilities,0.7228081226348877
translation,353,41,model,external commonsense,for connecting,implicit relations,external commonsense for connecting implicit relations,0.7573052048683167
translation,353,41,model,model,expand upon,gated attention network,model expand upon gated attention network,0.6616487503051758
translation,353,54,model,model,employ,multi-hop commonsense paths,model employ multi-hop commonsense paths,0.5679136514663696
translation,353,55,model,our multi-hop reasoning architecture,to incorporate,different aspects,our multi-hop reasoning architecture to incorporate different aspects,0.6849057078361511
translation,353,55,model,different aspects,of,commonsense relationship path,different aspects of commonsense relationship path,0.5574929118156433
translation,353,55,model,different aspects,to bridge,different inference gaps,different aspects to bridge different inference gaps,0.6217346787452698
translation,353,55,model,commonsense relationship path,at,each hop,commonsense relationship path at each hop,0.5460803508758545
translation,353,55,model,model,in tandem with,our multi-hop reasoning architecture,model in tandem with our multi-hop reasoning architecture,0.5796397924423218
translation,353,27,results,our model,achieves,41.49 rouge -l and 17.33 meteor,our model achieves 41.49 rouge -l and 17.33 meteor,0.659351110458374
translation,353,27,results,our model,substantially better than,performance,our model substantially better than performance,0.7350464463233948
translation,353,27,results,41.49 rouge -l and 17.33 meteor,on,summary subtask of narrativeqa,41.49 rouge -l and 17.33 meteor on summary subtask of narrativeqa,0.5436490178108215
translation,353,27,results,41.49 rouge -l and 17.33 meteor,substantially better than,performance,41.49 rouge -l and 17.33 meteor substantially better than performance,0.7506844997406006
translation,353,27,results,results,has,our model,results has our model,0.5871725678443909
translation,353,33,results,our background commonsense knowledge enhanced model,achieved,1.5 % higher accuracy,our background commonsense knowledge enhanced model achieved 1.5 % higher accuracy,0.6593384146690369
translation,353,33,results,1.5 % higher accuracy,than,our strong baseline,1.5 % higher accuracy than our strong baseline,0.5236043930053711
translation,353,33,results,results,found that,our background commonsense knowledge enhanced model,results found that our background commonsense knowledge enhanced model,0.6191519498825073
translation,353,173,results,all generative models,on,narrativeqa,all generative models on narrativeqa,0.595329761505127
translation,353,173,results,competitive,with,top span prediction models,competitive with top span prediction models,0.6287623047828674
translation,353,173,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,353,173,results,outperforms,has,all generative models,outperforms has all generative models,0.5693139433860779
translation,353,173,results,results,see empirically that,our model,results see empirically that our model,0.7530694603919983
translation,353,174,results,noic commonsense integration,able to,further improve,noic commonsense integration able to further improve,0.6723120808601379
translation,353,174,results,performance,establishing,new state - of - the - art,performance establishing new state - of - the - art,0.6409862041473389
translation,353,174,results,new state - of - the - art,for,task,new state - of - the - art for task,0.5847323536872864
translation,353,174,results,further improve,has,performance,further improve has performance,0.5871225595474243
translation,353,174,results,results,with,noic commonsense integration,results with noic commonsense integration,0.6196984052658081
translation,353,175,results,our model,performs,well,our model performs well,0.6747565865516663
translation,353,175,results,well,on,wikihop,well on wikihop,0.6183934211730957
translation,353,175,results,improved,via,addition of commonsense,improved via addition of commonsense,0.7179652452468872
translation,353,175,results,addition of commonsense,has,),addition of commonsense has ),0.6185432076454163
translation,353,175,results,results,see that,our model,results see that our model,0.6820751428604126
translation,353,179,results,performance,on top of,our strong baseline,performance on top of our strong baseline,0.6492576599121094
translation,353,179,results,effectively introducing,has,external knowledge,effectively introducing has external knowledge,0.5621079802513123
translation,353,179,results,improve,has,performance,improve has performance,0.5578044652938843
translation,353,179,results,performance,has,further,performance has further,0.6324895024299622
translation,353,179,results,results,see that,effectively introducing,results see that effectively introducing,0.7459695935249329
translation,353,186,results,performance significantly,across,all metrics,performance significantly across all metrics,0.6741037964820862
translation,353,186,results,our commonsense selection and incorporation mechanism,has,improves,our commonsense selection and incorporation mechanism has improves,0.6163651943206787
translation,353,186,results,improves,has,performance significantly,improves has performance significantly,0.5617428421974182
translation,353,193,results,our algorithm,able to provide,useful commonsense,our algorithm able to provide useful commonsense,0.5908026099205017
translation,353,193,results,useful commonsense,has,48 % of the time,useful commonsense has 48 % of the time,0.5806075930595398
translation,353,197,results,human evaluation results,in agreement with,automatic evaluation metrics,human evaluation results in agreement with automatic evaluation metrics,0.5143792629241943
translation,353,197,results,reasonable impact,on,overall correctness of the model,reasonable impact on overall correctness of the model,0.5390734672546387
translation,353,197,results,automatic evaluation metrics,has,our commonsense incorporation,automatic evaluation metrics has our commonsense incorporation,0.5920727849006653
translation,353,197,results,our commonsense incorporation,has,reasonable impact,our commonsense incorporation has reasonable impact,0.5941062569618225
translation,353,197,results,results,see that,human evaluation results,results see that human evaluation results,0.5404163002967834
translation,354,6,model,different approach,present,information retrieval style solution,different approach present information retrieval style solution,0.6410315632820129
translation,354,6,model,model,propose,different approach,model propose different approach,0.7351981401443481
translation,354,6,model,model,present,information retrieval style solution,model present information retrieval style solution,0.6330574750900269
translation,354,7,model,candidate re-ranking,to,answer questions,candidate re-ranking to answer questions,0.554101288318634
translation,354,7,model,model,adopt,two - phase approach,model adopt two - phase approach,0.70379638671875
translation,354,8,model,triplet- siamese-hybrid cnn ( tshcnn ),to re-rank,candidate answers,triplet- siamese-hybrid cnn ( tshcnn ) to re-rank candidate answers,0.7414745688438416
translation,354,8,model,model,propose,triplet- siamese-hybrid cnn ( tshcnn ),model propose triplet- siamese-hybrid cnn ( tshcnn ),0.6289767622947693
translation,354,14,model,information retrieval ( ir ) style approach,to,qa task,information retrieval ( ir ) style approach to qa task,0.5319879651069641
translation,354,14,model,triplet- siamese -hybrid convolutional neural network ( tshcnn ),jointly learns to rank,candidate answers,triplet- siamese -hybrid convolutional neural network ( tshcnn ) jointly learns to rank candidate answers,0.7857968211174011
translation,354,14,model,model,introduce,information retrieval ( ir ) style approach,model introduce information retrieval ( ir ) style approach,0.624325692653656
translation,354,14,model,model,propose,triplet- siamese -hybrid convolutional neural network ( tshcnn ),model propose triplet- siamese -hybrid convolutional neural network ( tshcnn ),0.6468589305877686
translation,354,36,model,convolutional neural networks ( cnn ),to learn,semantic representation,convolutional neural networks ( cnn ) to learn semantic representation,0.6197885274887085
translation,354,36,model,semantic representation,for,input text,semantic representation for input text,0.550881028175354
translation,354,36,model,order,in,short phrases,order in short phrases,0.5161330699920654
translation,354,36,model,model,use,convolutional neural networks ( cnn ),model use convolutional neural networks ( cnn ),0.6243861317634583
translation,354,55,model,custom negative sample generation method,that generates,negative samples,custom negative sample generation method that generates negative samples,0.6796378493309021
translation,354,55,model,negative samples,similar to,actual answer,negative samples similar to actual answer,0.687005341053009
translation,354,55,model,negative samples,helps further increase,discriminatory ability,negative samples helps further increase discriminatory ability,0.7300013899803162
translation,354,55,model,discriminatory ability,of,our network,discriminatory ability of our network,0.5430224537849426
translation,354,55,model,model,develop,custom negative sample generation method,model develop custom negative sample generation method,0.6517448425292969
translation,354,69,results,recall,increase,k,recall increase k,0.756338357925415
translation,354,69,results,increases,increase,k,increases increase k,0.8228411674499512
translation,354,69,results,recall,has,increases,recall has increases,0.6245496273040771
translation,354,69,results,results,has,recall,results has recall,0.6362076997756958
translation,354,73,results,significant improvement,of,17 %,significant improvement of 17 %,0.5816689133644104
translation,354,73,results,17 %,in,our accuracy,17 % in our accuracy,0.5482193231582642
translation,354,73,results,17 %,after,candidate re-ranking,17 % after candidate re-ranking,0.6879141330718994
translation,354,73,results,our accuracy,after,candidate re-ranking,our accuracy after candidate re-ranking,0.676288902759552
translation,354,77,results,scores,obtained using,custom negative sample generation method,scores obtained using custom negative sample generation method,0.6652217507362366
translation,354,77,results,custom negative sample generation method,were,17.3 % and 41.8 % higher,custom negative sample generation method were 17.3 % and 41.8 % higher,0.5864037275314331
translation,354,77,results,17.3 % and 41.8 % higher,compared to,only 10 negative samples,17.3 % and 41.8 % higher compared to only 10 negative samples,0.6677219271659851
translation,354,77,results,results,has,scores,results has scores,0.5219217538833618
translation,354,81,results,our model,without,additional inputs,our model without additional inputs,0.6882038712501526
translation,354,81,results,our model,obtain,improvement,our model obtain improvement,0.5901011824607849
translation,354,81,results,improvement,of,14.9 % and 38.9 %,improvement of 14.9 % and 38.9 %,0.5801113843917847
translation,354,81,results,14.9 % and 38.9 %,in,our scores,14.9 % and 38.9 % in our scores,0.5571534633636475
translation,354,81,results,14.9 % and 38.9 %,when,additional inputs,14.9 % and 38.9 % when additional inputs,0.6272507905960083
translation,354,81,results,14.9 % and 38.9 %,provide,additional inputs,14.9 % and 38.9 % provide additional inputs,0.5932318568229675
translation,354,81,results,additional inputs,in the form of,concatenated question and tuple,additional inputs in the form of concatenated question and tuple,0.7250032424926758
translation,354,81,results,additional inputs,with and without,our custom negative sampling approach,additional inputs with and without our custom negative sampling approach,0.687030017375946
translation,354,81,results,results,Compared to,our model,results Compared to our model,0.6829187870025635
translation,354,86,results,impressive 62.9 %,in,scores,impressive 62.9 % in scores,0.5600490570068359
translation,354,86,results,impressive 62.9 %,compared to,our model,impressive 62.9 % compared to our model,0.6386275887489319
translation,354,86,results,our model,has,without neither of these techniques,our model has without neither of these techniques,0.5882144570350647
translation,354,87,results,accuracy,of,80 %,accuracy of 80 %,0.6205134987831116
translation,354,87,results,80 %,has,new stateof - the - art,80 % has new stateof - the - art,0.5743081569671631
translation,355,20,baselines,two methods,for selecting,relevant sentences,two methods for selecting relevant sentences,0.7312469482421875
translation,355,20,baselines,relevant sentences,from,reviews,relevant sentences from reviews,0.5672900080680847
translation,355,20,baselines,various representations,for encoding,questions and reviews,various representations for encoding questions and reviews,0.762714684009552
translation,355,20,baselines,questions and reviews,including,bag-ofwords,questions and reviews including bag-ofwords,0.6873385310173035
translation,355,95,experimental-setup,experimental setup,use,pre-trained google news word2vec model,experimental setup use pre-trained google news word2vec model,0.5569366216659546
translation,355,105,experiments,home and kitchen domain,shows,better performance,home and kitchen domain shows better performance,0.6693896651268005
translation,355,105,experiments,better performance,with,question + review systems,better performance with question + review systems,0.6523699164390564
translation,355,21,results,our systems,tend to outperform,chance baseline,our systems tend to outperform chance baseline,0.7799187898635864
translation,355,21,results,our systems,not by,large margin,our systems not by large margin,0.6824663281440735
translation,355,21,results,development set,has,our systems,development set has our systems,0.6272360682487488
translation,355,21,results,results,On,development set,results On development set,0.588634192943573
translation,355,22,results,three domains,find that,question -only systems,three domains find that question -only systems,0.6390764117240906
translation,355,22,results,question -only systems,tend to perform,as well,question -only systems tend to perform as well,0.7251433730125427
translation,355,22,results,sometimes outperform,use,reviews,sometimes outperform use reviews,0.6788830757141113
translation,355,22,results,results,Over,three domains,results Over three domains,0.5721495747566223
translation,355,104,results,questions,provide,some information,questions provide some information,0.6716353893280029
translation,355,104,results,questions,help find,correct answer,questions help find correct answer,0.6402360200881958
translation,355,104,results,two out of three domains,show,best performance,two out of three domains show best performance,0.6114181280136108
translation,355,104,results,best performance,using,question,best performance using question,0.7007433176040649
translation,355,104,results,results,has,questions,results has questions,0.5194433331489563
translation,355,106,results,sentences,from,reviews,sentences from reviews,0.5913431644439697
translation,355,106,results,no clear winner,between,intersection and union methods,no clear winner between intersection and union methods,0.6476343274116516
translation,355,106,results,results,When selecting,sentences,results When selecting sentences,0.6054823398590088
translation,355,108,results,bert,perform,better,bert perform better,0.6379222273826599
translation,355,108,results,better,then,logistic regression,better then logistic regression,0.564224898815155
translation,355,108,results,logistic regression,on,development set,logistic regression on development set,0.5919537544250488
translation,355,108,results,best question-only and ques-tion + review models,to,test set,best question-only and ques-tion + review models to test set,0.5106319785118103
translation,355,108,results,bert,has,base and large models,bert has base and large models,0.6101135611534119
translation,355,108,results,results,apply,best question-only and ques-tion + review models,results apply best question-only and ques-tion + review models,0.6056099534034729
translation,355,108,results,results,has,bert,results has bert,0.43097156286239624
translation,355,108,results,results,has,base and large models,results has base and large models,0.5622804760932922
translation,356,4,model,generating,has,"deep ( i.e , high- level ) comprehension questions from novel text","generating has deep ( i.e , high- level ) comprehension questions from novel text",0.48169875144958496
translation,356,5,model,task,into,ontologycrowd - relevance workflow,task into ontologycrowd - relevance workflow,0.5519120693206787
translation,356,5,model,ontologycrowd - relevance workflow,first representing,original text,ontologycrowd - relevance workflow first representing original text,0.6598421931266785
translation,356,5,model,ontologycrowd - relevance workflow,crowdsourcing,candidate question templates,ontologycrowd - relevance workflow crowdsourcing candidate question templates,0.7819933295249939
translation,356,5,model,ontologycrowd - relevance workflow,ranking,potentially relevant templates,ontologycrowd - relevance workflow ranking potentially relevant templates,0.7366301417350769
translation,356,5,model,original text,in,low-dimensional ontology,original text in low-dimensional ontology,0.4750709533691406
translation,356,5,model,candidate question templates,aligned with,space,candidate question templates aligned with space,0.7002633810043335
translation,356,5,model,potentially relevant templates,for,novel region of text,potentially relevant templates for novel region of text,0.6031215786933899
translation,356,5,model,model,decomposing,task,model decomposing task,0.7573506236076355
translation,357,134,ablation-analysis,doc2vec,used,"doc2vec ( le and mikolov , 2014 )","doc2vec used doc2vec ( le and mikolov , 2014 )",0.5598265528678894
translation,357,134,ablation-analysis,doc2vec,to generate,sentence vectors,doc2vec to generate sentence vectors,0.6671525239944458
translation,357,134,ablation-analysis,"doc2vec ( le and mikolov , 2014 )",to generate,sentence vectors,"doc2vec ( le and mikolov , 2014 ) to generate sentence vectors",0.6261493563652039
translation,357,134,ablation-analysis,ablation analysis,has,doc2vec,ablation analysis has doc2vec,0.556117594242096
translation,357,76,experiments,best performance,with,20 topics,best performance with 20 topics,0.6481408476829529
translation,357,135,experiments,primary submission,for,subtasks a and b,primary submission for subtasks a and b,0.5734067559242249
translation,357,135,experiments,primary submission,uses,svm,primary submission uses svm,0.6532256007194519
translation,357,135,experiments,subtasks a and b,uses,svm,subtasks a and b uses svm,0.6576941609382629
translation,357,135,experiments,svm,with,rbf kernel,svm with rbf kernel,0.647553026676178
translation,357,135,experiments,rbf kernel,for,classification,rbf kernel for classification,0.6425283551216125
translation,357,5,model,support vector machine ( svm ) based system,makes use of,"textual , domain-specific , wordembedding and topic-modeling features","support vector machine ( svm ) based system makes use of textual , domain-specific , wordembedding and topic-modeling features",0.6631850004196167
translation,357,5,model,model,develop,support vector machine ( svm ) based system,model develop support vector machine ( svm ) based system,0.6077802181243896
translation,357,6,model,novel method,for,dialogue chain identification,novel method for dialogue chain identification,0.6249048113822937
translation,357,6,model,dialogue chain identification,in,comment threads,dialogue chain identification in comment threads,0.5203891396522522
translation,357,6,model,model,propose,novel method,model propose novel method,0.7230806350708008
translation,357,13,model,model,propose,rich feature - based system,model propose rich feature - based system,0.6640385985374451
translation,357,14,model,architecture,integrates,"textual , semantic and domain-specific features","architecture integrates textual , semantic and domain-specific features",0.6319026947021484
translation,357,14,model,"textual , semantic and domain-specific features",to achieve,good results,"textual , semantic and domain-specific features to achieve good results",0.6123014688491821
translation,357,14,model,model,create,architecture,model create architecture,0.6587472558021545
translation,357,15,model,customized preprocessing pipeline,rather than using,standard tools,customized preprocessing pipeline rather than using standard tools,0.634846568107605
translation,357,15,model,model,develop,customized preprocessing pipeline,model develop customized preprocessing pipeline,0.6770055294036865
translation,357,19,model,novel method,identification of,dialogue groups,novel method identification of dialogue groups,0.6714997291564941
translation,357,19,model,dialogue groups,in,comment thread,dialogue groups in comment thread,0.5273541212081909
translation,357,19,model,dialogue groups,by constructing,user interaction graph,dialogue groups by constructing user interaction graph,0.7319076061248779
translation,357,19,model,model,propose,novel method,model propose novel method,0.7230806350708008
translation,357,118,results,comparable re-sults,with,linear kernel,comparable re-sults with linear kernel,0.6240846514701843
translation,357,118,results,comparable re-sults,with,l2 - regularized logistic regression,comparable re-sults with l2 - regularized logistic regression,0.630090057849884
translation,357,118,results,results,achieve,comparable re-sults,results achieve comparable re-sults,0.5776261687278748
translation,358,7,model,integer linear program,to solve,several disambiguation tasks jointly,integer linear program to solve several disambiguation tasks jointly,0.6760467886924744
translation,358,7,model,model,based on,integer linear program,model based on integer linear program,0.6369960308074951
translation,358,8,model,rich type system,provided by,knowledge bases,rich type system provided by knowledge bases,0.6808885335922241
translation,358,8,model,rich type system,to constrain,semantic -coherence objective function,rich type system to constrain semantic -coherence objective function,0.6859921813011169
translation,358,8,model,knowledge bases,in,web of linked data,knowledge bases in web of linked data,0.47759678959846497
translation,358,42,model,new elements,towards making,translation,new elements towards making translation,0.6553468704223633
translation,358,42,model,translation,of,questions,translation of questions,0.580058217048645
translation,358,42,model,questions,into,sparql triple patterns,questions into sparql triple patterns,0.5767400860786438
translation,358,43,model,disambiguation and mapping tasks jointly,by encoding them into,comprehensive integer linear program ( ilp ),disambiguation and mapping tasks jointly by encoding them into comprehensive integer linear program ( ilp ),0.6980077624320984
translation,358,43,model,comprehensive integer linear program ( ilp ),mapping of,phrases,comprehensive integer linear program ( ilp ) mapping of phrases,0.7282824516296387
translation,358,43,model,comprehensive integer linear program ( ilp ),construction of,sparql triple patterns,comprehensive integer linear program ( ilp ) construction of sparql triple patterns,0.6278768181800842
translation,358,43,model,segmentation,of,questions,segmentation of questions,0.6083178520202637
translation,358,43,model,questions,into,meaningful phrases,questions into meaningful phrases,0.5598950982093811
translation,358,43,model,phrases,to,"semantic entities , classes , and relations","phrases to semantic entities , classes , and relations",0.47295665740966797
translation,358,43,model,phrases,construction of,sparql triple patterns,phrases construction of sparql triple patterns,0.5942753553390503
translation,358,43,model,comprehensive integer linear program ( ilp ),has,segmentation,comprehensive integer linear program ( ilp ) has segmentation,0.5783243179321289
translation,358,43,model,model,solve,disambiguation and mapping tasks jointly,model solve disambiguation and mapping tasks jointly,0.6429901719093323
translation,358,51,model,framework,comprises,full suite of components,framework comprises full suite of components,0.7240422368049622
translation,358,51,model,full suite of components,for,question decomposition,full suite of components for question decomposition,0.6177759766578674
translation,358,51,model,full suite of components,mapping,constituents,full suite of components mapping constituents,0.7092187404632568
translation,358,51,model,constituents,into,semantic concept space,constituents into semantic concept space,0.601539671421051
translation,358,51,model,constituents,generating,alternative candidate mappings,constituents generating alternative candidate mappings,0.742487907409668
translation,358,51,model,constituents,into,set of sparql triple patterns,constituents into set of sparql triple patterns,0.5560906529426575
translation,358,51,model,coherent mapping,of,all constituents,coherent mapping of all constituents,0.6091854572296143
translation,358,51,model,all constituents,into,set of sparql triple patterns,all constituents into set of sparql triple patterns,0.5690762400627136
translation,358,51,model,model,developed,framework,model developed framework,0.6242915987968445
translation,358,51,model,model,computing,coherent mapping,model computing coherent mapping,0.8036497235298157
translation,359,114,experiments,validation set,for,vqa v2,validation set for vqa v2,0.6336309313774109
translation,359,114,experiments,vqa v2,to tune,initial learning rate and the number of epochs,vqa v2 to tune initial learning rate and the number of epochs,0.700848400592804
translation,359,114,experiments,vqa v2,yielding,highest overall vqa score,vqa v2 yielding highest overall vqa score,0.6752398610115051
translation,359,114,experiments,initial learning rate and the number of epochs,yielding,highest overall vqa score,initial learning rate and the number of epochs yielding highest overall vqa score,0.6263725161552429
translation,359,70,hyperparameters,faster r-cnn head,in conjunction with,resnet - 101 base network,faster r-cnn head in conjunction with resnet - 101 base network,0.6517566442489624
translation,359,70,hyperparameters,faster r-cnn head,as,detection module,faster r-cnn head as detection module,0.508604109287262
translation,359,70,hyperparameters,hyperparameters,use,faster r-cnn head,hyperparameters use faster r-cnn head,0.6164808869361877
translation,359,113,hyperparameters,joint model,using,adamax optimizer,joint model using adamax optimizer,0.66845703125
translation,359,113,hyperparameters,adamax optimizer,),learning rate,adamax optimizer ) learning rate,0.5818130373954773
translation,359,113,hyperparameters,adamax optimizer,with,batch size,adamax optimizer with batch size,0.611325204372406
translation,359,113,hyperparameters,adamax optimizer,with,learning rate,adamax optimizer with learning rate,0.6312124133110046
translation,359,113,hyperparameters,batch size,of,384,batch size of 384,0.6493436694145203
translation,359,113,hyperparameters,learning rate,of,0.002,learning rate of 0.002,0.5967087745666504
translation,359,113,hyperparameters,hyperparameters,train,joint model,hyperparameters train joint model,0.6302191615104675
translation,359,115,hyperparameters,1,",",280,"1 , 280",0.7201765179634094
translation,359,115,hyperparameters,hidden units,in,question embedding and attention model,hidden units in question embedding and attention model,0.5016447305679321
translation,359,115,hyperparameters,question embedding and attention model,in,vqa module,question embedding and attention model in vqa module,0.4969777464866638
translation,359,115,hyperparameters,vqa module,with,36 object detection features,vqa module with 36 object detection features,0.6224343180656433
translation,359,115,hyperparameters,36 object detection features,for,each image,36 object detection features for each image,0.5943898558616638
translation,359,115,hyperparameters,1,has,280,1 has 280,0.6366977095603943
translation,359,115,hyperparameters,1,has,hidden units,1 has hidden units,0.5788763165473938
translation,359,115,hyperparameters,280,has,hidden units,280 has hidden units,0.5721479654312134
translation,359,115,hyperparameters,hyperparameters,use,1,hyperparameters use 1,0.6232619285583496
translation,359,115,hyperparameters,hyperparameters,use,280,hyperparameters use 280,0.6628930568695068
translation,359,115,hyperparameters,hyperparameters,",",280,"hyperparameters , 280",0.5792660713195801
translation,359,116,hyperparameters,dimension,of,lstm hidden state,dimension of lstm hidden state,0.5352127552032471
translation,359,116,hyperparameters,dimension,of,image feature embedding,dimension of image feature embedding,0.5235633254051208
translation,359,116,hyperparameters,word embedding,set to,512,word embedding set to 512,0.6496448516845703
translation,359,116,hyperparameters,captioning models,has,dimension,captioning models has dimension,0.580051064491272
translation,359,116,hyperparameters,hyperparameters,For,captioning models,hyperparameters For captioning models,0.5605060458183289
translation,359,117,hyperparameters,glove vectors,to initialize,word embedding matrix,glove vectors to initialize word embedding matrix,0.6733264923095703
translation,359,117,hyperparameters,word embedding matrix,in,caption embedding module,word embedding matrix in caption embedding module,0.48000872135162354
translation,359,118,hyperparameters,training process,with,human annotated captions,training process with human annotated captions,0.5749146938323975
translation,359,118,hyperparameters,human annotated captions,from,coco dataset,human annotated captions from coco dataset,0.4969758987426758
translation,359,118,hyperparameters,human annotated captions,pre-train,vqa and captiongeneration modules,human annotated captions pre-train vqa and captiongeneration modules,0.6986514925956726
translation,359,118,hyperparameters,vqa and captiongeneration modules,for,20 epochs,vqa and captiongeneration modules for 20 epochs,0.5921863317489624
translation,359,118,hyperparameters,vqa and captiongeneration modules,with,final joint loss,vqa and captiongeneration modules with final joint loss,0.6150104403495789
translation,359,118,hyperparameters,hyperparameters,initialize,training process,hyperparameters initialize training process,0.768352746963501
translation,359,118,hyperparameters,hyperparameters,pre-train,vqa and captiongeneration modules,hyperparameters pre-train vqa and captiongeneration modules,0.690123975276947
translation,359,121,hyperparameters,our model,using,generated captions,our model using generated captions,0.6753969788551331
translation,359,121,hyperparameters,generated captions,with,0.25 ? learning - rate,generated captions with 0.25 ? learning - rate,0.6008250713348389
translation,359,121,hyperparameters,0.25 ? learning - rate,for,another 10 epochs,0.25 ? learning - rate for another 10 epochs,0.6048080921173096
translation,359,121,hyperparameters,hyperparameters,fine- tune,our model,hyperparameters fine- tune our model,0.7147842645645142
translation,359,5,model,novel approach,to improve,vqa performance,novel approach to improve vqa performance,0.6232389211654663
translation,359,5,model,captions,targeted to help,answer,captions targeted to help answer,0.6853548288345337
translation,359,5,model,answer,has,specific visual question,answer has specific visual question,0.5971226692199707
translation,359,5,model,model,present,novel approach,model present novel approach,0.6989931464195251
translation,359,31,model,caption information,propose,novel caption embedding module,caption information propose novel caption embedding module,0.6154212355613708
translation,359,31,model,novel caption embedding module,produces,caption embedding,novel caption embedding module produces caption embedding,0.5505546927452087
translation,359,31,model,important words,in,caption,important words in caption,0.48375967144966125
translation,359,31,model,caption embedding,tailored for,answer prediction,caption embedding tailored for answer prediction,0.6825313568115234
translation,359,31,model,model,to incorporate,caption information,model to incorporate caption information,0.6924155950546265
translation,359,32,model,caption embeddings,to adjust,visual top-down attention weights,caption embeddings to adjust visual top-down attention weights,0.6219700574874878
translation,359,32,model,visual top-down attention weights,for,each object,visual top-down attention weights for each object,0.562999427318573
translation,359,32,model,model,has,caption embeddings,model has caption embeddings,0.4965437650680542
translation,359,69,model,object detection,as,bottom - up attention,object detection as bottom - up attention,0.4792138636112213
translation,359,69,model,salient image regions,with,clear boundaries,salient image regions with clear boundaries,0.5987055897712708
translation,359,69,model,model,use,object detection,model use object detection,0.5973528623580933
translation,359,29,results,helpfulness,measured using,inner-product,helpfulness measured using inner-product,0.6018276214599609
translation,359,29,results,inner-product,of,gradients,inner-product of gradients,0.5818801522254944
translation,359,29,results,gradients,from,caption generation loss,gradients from caption generation loss,0.5475181937217712
translation,359,29,results,gradients,from,vqa answer prediction loss,gradients from vqa answer prediction loss,0.47777870297431946
translation,359,29,results,results,has,helpfulness,results has helpfulness,0.5384415984153748
translation,359,138,results,other state - of - the - art single models,by,clear margin,other state - of - the - art single models by clear margin,0.5535410046577454
translation,359,138,results,our single model,has,outperforms,our single model has outperforms,0.6204248070716858
translation,359,138,results,outperforms,has,other state - of - the - art single models,outperforms has other state - of - the - art single models,0.5410073399543762
translation,359,138,results,clear margin,has,i.e. 2.06 %,clear margin has i.e. 2.06 %,0.5584809184074402
translation,359,138,results,results,has,our single model,results has our single model,0.5605776309967041
translation,359,139,results,other methods,especially in,' num ' and ' other ' categories,other methods especially in ' num ' and ' other ' categories,0.6306986212730408
translation,359,139,results,our single model,has,outperforms,our single model has outperforms,0.6204248070716858
translation,359,139,results,outperforms,has,other methods,outperforms has other methods,0.5689877271652222
translation,359,139,results,results,observe that,our single model,results observe that our single model,0.6196622252464294
translation,359,141,results,with different initialization seeds,results in,score,with different initialization seeds results in score,0.6241748332977295
translation,359,141,results,score,of,69.7 %,score of 69.7 %,0.5307173728942871
translation,359,141,results,ensemble of 10 models,has,with different initialization seeds,ensemble of 10 models has with different initialization seeds,0.5154422521591187
translation,359,153,results,our generated question - relevant captions,trained with,our caption selection strategy,our generated question - relevant captions trained with our caption selection strategy,0.684959888458252
translation,359,153,results,our caption selection strategy,provide,more helpful clues,our caption selection strategy provide more helpful clues,0.6251806020736694
translation,359,153,results,more helpful clues,for,vqa process,more helpful clues for vqa process,0.6259591579437256
translation,359,153,results,more helpful clues,than,question - agnostic up - down captions,more helpful clues than question - agnostic up - down captions,0.5804924964904785
translation,359,153,results,captions,by,1.2 %,captions by 1.2 %,0.5592582821846008
translation,359,153,results,outperforming,has,captions,outperforming has captions,0.5682371258735657
translation,359,153,results,results,observe,our generated question - relevant captions,results observe our generated question - relevant captions,0.53988116979599
translation,359,157,results,improvement,when using,caption features,improvement when using caption features,0.663901150226593
translation,359,157,results,caption features,to adjust,attention weights,caption features to adjust attention weights,0.642223596572876
translation,359,157,results,results,observe,improvement,results observe improvement,0.6390148997306824
translation,359,172,results,caption attention adjustment,improves,match,caption attention adjustment improves match,0.6892999410629272
translation,359,172,results,match,between,automated attention and human-annotated attention,match between automated attention and human-annotated attention,0.6624715924263
translation,359,172,results,results,indicate,caption attention adjustment,results indicate caption attention adjustment,0.571988582611084
translation,359,173,results,human captions,provide,a bit more improvement,human captions provide a bit more improvement,0.5872815251350403
translation,359,173,results,a bit more improvement,than,automatically generated ones,a bit more improvement than automatically generated ones,0.6202177405357361
translation,359,173,results,results,has,human captions,results has human captions,0.5041254162788391
translation,360,6,baselines,quase,learns,representations,quase learns representations,0.7062214612960815
translation,360,6,baselines,representations,from,qa data,representations from qa data,0.5979710221290588
translation,360,6,baselines,representations,using,bert or other state- ofthe - art contextual language models,representations using bert or other state- ofthe - art contextual language models,0.6483266949653625
translation,360,6,baselines,baselines,has,quase,baselines has quase,0.6255627274513245
translation,360,124,baselines,bert,on,qamr,bert on qamr,0.657738983631134
translation,360,124,baselines,bert,on,two single-sentence tasks ( srl and re ),bert on two single-sentence tasks ( srl and re ),0.5400581359863281
translation,360,124,baselines,bert,on,two paired - sentence tasks ( te and mrc ),bert on two paired - sentence tasks ( te and mrc ),0.5326412320137024
translation,360,124,baselines,bert,on,two single-sentence tasks ( srl and re ),bert on two single-sentence tasks ( srl and re ),0.5400581359863281
translation,360,124,baselines,bert,on,two paired - sentence tasks ( te and mrc ),bert on two paired - sentence tasks ( te and mrc ),0.5326412320137024
translation,360,124,baselines,baselines,compare,bert,baselines compare bert,0.7444093227386475
translation,360,117,hyperparameters,deep neural model,for,srl,deep neural model for srl,0.5858454704284668
translation,360,117,hyperparameters,end-to - end neural model,for,coref,end-to - end neural model for coref,0.6163182854652405
translation,360,33,results,s-quase,for,singlesentence tasks,s-quase for singlesentence tasks,0.6504417657852173
translation,360,33,results,s-quase,for,paired - sentence tasks,s-quase for paired - sentence tasks,0.5882618427276611
translation,360,33,results,s-quase,for,paired - sentence tasks,s-quase for paired - sentence tasks,0.5882618427276611
translation,360,33,results,p-quase,for,paired - sentence tasks,p-quase for paired - sentence tasks,0.5945982336997986
translation,360,33,results,quase qamr,improves,all 7 tasks,quase qamr improves all 7 tasks,0.6959039568901062
translation,360,33,results,all 7 tasks,in,low resource settings,all 7 tasks in low resource settings,0.5265949368476868
translation,360,33,results,average error reduction rate,of,7.1 %,average error reduction rate of 7.1 %,0.579742968082428
translation,360,33,results,7.1 %,compared to,bert,7.1 % compared to bert,0.659797191619873
translation,360,33,results,results,use,s-quase,results use s-quase,0.616766631603241
translation,360,33,results,results,use,p-quase,results use p-quase,0.6263900399208069
translation,360,138,results,great advantage,over,p-quase,great advantage over p-quase,0.7078569531440735
translation,360,138,results,great advantage,over,p-quase,great advantage over p-quase,0.7078569531440735
translation,360,138,results,p-quase,on,single-sentence tasks,p-quase on single-sentence tasks,0.519196629524231
translation,360,138,results,p-quase,on,paired - sentence tasks,p-quase on paired - sentence tasks,0.5221017003059387
translation,360,138,results,p-quase,on,paired - sentence tasks,p-quase on paired - sentence tasks,0.5221017003059387
translation,360,138,results,p-quase,better than,s-quase,p-quase better than s-quase,0.7946117520332336
translation,360,138,results,s-quase,on,paired - sentence tasks,s-quase on paired - sentence tasks,0.5206435918807983
translation,360,138,results,s-quase,has,great advantage,s-quase has great advantage,0.6130749583244324
translation,360,138,results,results,find that,s-quase,results find that s-quase,0.6595536470413208
translation,360,138,results,results,find that,p-quase,results find that p-quase,0.6757527589797974
translation,360,147,results,good furtherpre -training choice,for,quase,good furtherpre -training choice for quase,0.7188841700553894
translation,360,147,results,results,find that,qamr,results find that qamr,0.6547175049781799
translation,360,171,results,p-quase qam r,achieve,76.91 f1,p-quase qam r achieve 76.91 f1,0.6398204565048218
translation,360,171,results,76.91 f1,on,ptb set,76.91 f1 on ptb set,0.5770524144172668
translation,360,171,results,ptb set,of,qa - srl,ptb set of qa - srl,0.623910665512085
translation,360,171,results,results,find that,p-quase qam r,results find that p-quase qam r,0.6614975333213806
translation,361,135,ablation-analysis,md,leads to,performance drop,md leads to performance drop,0.6710073947906494
translation,361,135,ablation-analysis,performance drop,on,arc - easy,performance drop on arc - easy,0.5803655385971069
translation,361,135,ablation-analysis,ablation analysis,has,md,ablation analysis has md,0.5327683091163635
translation,361,138,ablation-analysis,fine - turning,on,external in - domain data,fine - turning on external in - domain data,0.5832981467247009
translation,361,138,ablation-analysis,external in - domain data,has,hurts,external in - domain data has hurts,0.5758771300315857
translation,361,138,ablation-analysis,hurts,has,performance,hurts has performance,0.6043882966041565
translation,361,105,baselines,gpt ii,based on,fine-tuning,gpt ii based on fine-tuning,0.6881741881370544
translation,361,105,baselines,generative pre-trained transformer ( gpt ) language model,instead of,bert,generative pre-trained transformer ( gpt ) language model instead of bert,0.6597505211830139
translation,361,105,baselines,rs ii,Based on,gpt,rs ii Based on gpt,0.7122036814689636
translation,361,105,baselines,rs ii,Based on,general reading strategies ( rs ),rs ii Based on general reading strategies ( rs ),0.6292069554328918
translation,361,105,baselines,general reading strategies ( rs ),applied during,finetuning stage,general reading strategies ( rs ) applied during finetuning stage,0.7169566750526428
translation,361,105,baselines,finetuning stage,adding,trainable embedding,finetuning stage adding trainable embedding,0.649989128112793
translation,361,105,baselines,trainable embedding,into,text embedding,trainable embedding into text embedding,0.5814198851585388
translation,361,105,baselines,text embedding,of,tokens,text embedding of tokens,0.540472149848938
translation,361,105,baselines,tokens,relevant to,candidate answer options,tokens relevant to candidate answer options,0.6679501533508301
translation,361,105,baselines,fine-tuning,has,generative pre-trained transformer ( gpt ) language model,fine-tuning has generative pre-trained transformer ( gpt ) language model,0.575599193572998
translation,361,105,baselines,fine-tuning,has,general reading strategies ( rs ),fine-tuning has general reading strategies ( rs ),0.5636332631111145
translation,361,105,baselines,gpt,has,general reading strategies ( rs ),gpt has general reading strategies ( rs ),0.5271999835968018
translation,361,105,baselines,baselines,has,gpt ii,baselines has gpt ii,0.5901699066162109
translation,361,106,baselines,bert ii,Based on,bert,bert ii Based on bert,0.6282302737236023
translation,361,106,baselines,baselines,has,bert ii,baselines has bert ii,0.6393000483512878
translation,361,93,experiments,information retrieval,use,version 7.4.0,information retrieval use version 7.4.0,0.5959744453430176
translation,361,93,experiments,information retrieval,set,maximum number,information retrieval set maximum number,0.6347960829734802
translation,361,93,experiments,version 7.4.0,of,lucene,version 7.4.0 of lucene,0.5476087927818298
translation,361,93,experiments,maximum number,of,retrieved sentences k,maximum number of retrieved sentences k,0.5757551193237305
translation,361,93,experiments,retrieved sentences k,to,50,retrieved sentences k to 50,0.6066206693649292
translation,361,87,hyperparameters,two-step fine-tuning framework,use,uncased bert large,two-step fine-tuning framework use uncased bert large,0.6185213923454285
translation,361,87,hyperparameters,uncased bert large,as,pre-trained language model,uncased bert large as pre-trained language model,0.5345478653907776
translation,361,87,hyperparameters,hyperparameters,For,two-step fine-tuning framework,hyperparameters For two-step fine-tuning framework,0.5878244042396545
translation,361,92,hyperparameters,noun phrase chunker,in,spacy,noun phrase chunker in spacy,0.4985848069190979
translation,361,92,hyperparameters,noun phrase chunker,to extract,concept mentions,noun phrase chunker to extract concept mentions,0.652107298374176
translation,361,92,hyperparameters,hyperparameters,use,noun phrase chunker,hyperparameters use noun phrase chunker,0.593582272529602
translation,361,5,model,simple yet effective methods,for exploiting,two sources of external knowledge,simple yet effective methods for exploiting two sources of external knowledge,0.678328275680542
translation,361,5,model,two sources of external knowledge,for,subject - area qa,two sources of external knowledge for subject - area qa,0.6106265187263489
translation,361,5,model,model,explore,simple yet effective methods,model explore simple yet effective methods,0.6810101866722107
translation,361,6,model,first,enriches,original subject - area reference corpus,first enriches original subject - area reference corpus,0.6984134912490845
translation,361,6,model,original subject - area reference corpus,with,relevant text snippets,original subject - area reference corpus with relevant text snippets,0.5768184065818787
translation,361,6,model,relevant text snippets,extracted from,"open-domain resource ( i.e. , wikipedia )","relevant text snippets extracted from open-domain resource ( i.e. , wikipedia )",0.5256885886192322
translation,361,6,model,"open-domain resource ( i.e. , wikipedia )",that cover,potentially ambiguous concepts,"open-domain resource ( i.e. , wikipedia ) that cover potentially ambiguous concepts",0.669368326663971
translation,361,6,model,potentially ambiguous concepts,in,question and answer options,potentially ambiguous concepts in question and answer options,0.4955264627933502
translation,361,6,model,model,enriches,original subject - area reference corpus,model enriches original subject - area reference corpus,0.7015381455421448
translation,361,6,model,model,has,first,model has first,0.6180936098098755
translation,361,26,model,two sources of external knowledge,i.e.,open-domain and in- domain ),two sources of external knowledge i.e. open-domain and in- domain ),0.6847889423370361
translation,361,26,model,two sources of external knowledge,incorporating them into,pre-trained language model,two sources of external knowledge incorporating them into pre-trained language model,0.6749878525733948
translation,361,26,model,pre-trained language model,during,fine-tuning stage,pre-trained language model during fine-tuning stage,0.6051260232925415
translation,361,26,model,model,investigate,two sources of external knowledge,model investigate two sources of external knowledge,0.6130349636077881
translation,361,27,model,concepts,in,question and answer options,concepts in question and answer options,0.5099396705627441
translation,361,27,model,concepts,to,open-domain resource,concepts to open-domain resource,0.5190998911857605
translation,361,27,model,potentially ambiguous concepts,to,open-domain resource,potentially ambiguous concepts to open-domain resource,0.5214781761169434
translation,361,27,model,open-domain resource,provides,unstructured background information,open-domain resource provides unstructured background information,0.6256143450737
translation,361,27,model,unstructured background information,relevant to,concepts,unstructured background information relevant to concepts,0.6350281238555908
translation,361,27,model,unstructured background information,used to enrich,original reference corpus,unstructured background information used to enrich original reference corpus,0.6892704963684082
translation,361,27,model,model,identify,concepts,model identify concepts,0.654569685459137
translation,361,175,model,external knowledge,into,pre-trained model,external knowledge into pre-trained model,0.584696888923645
translation,361,175,model,pre-trained model,improve,subjectarea qa tasks,pre-trained model improve subjectarea qa tasks,0.6633085608482361
translation,361,175,model,model,incorporate,external knowledge,model incorporate external knowledge,0.7065366506576538
translation,361,9,results,additional qa training instances,is,not uniformly helpful,additional qa training instances is not uniformly helpful,0.5411730408668518
translation,361,9,results,degrades,when,added instances,degrades when added instances,0.6964178681373596
translation,361,9,results,added instances,exhibit,higher level of difficulty,added instances exhibit higher level of difficulty,0.6593892574310303
translation,361,9,results,higher level of difficulty,than,original training data,higher level of difficulty than original training data,0.5655339956283569
translation,361,9,results,performance,has,degrades,performance has degrades,0.5837839841842651
translation,361,35,results,consistent gains,by introducing,knowledge,consistent gains by introducing knowledge,0.6863129138946533
translation,361,35,results,consistent gains,employing,additional indomain training data,consistent gains employing additional indomain training data,0.614151120185852
translation,361,35,results,knowledge,from,wikipedia,knowledge from wikipedia,0.6168104410171509
translation,361,35,results,additional indomain training data,is,uniformly helpful,additional indomain training data is uniformly helpful,0.5477451086044312
translation,361,35,results,additional indomain training data,not,uniformly helpful,additional indomain training data not uniformly helpful,0.6957420706748962
translation,361,35,results,degrades,when,added data,degrades when added data,0.7208776473999023
translation,361,35,results,added data,exhibit,higher level of difficulty,added data exhibit higher level of difficulty,0.6522006988525391
translation,361,35,results,higher level of difficulty,than,original training data,higher level of difficulty than original training data,0.5655339956283569
translation,361,35,results,uniformly helpful,has,performance,uniformly helpful has performance,0.6068612933158875
translation,361,35,results,performance,has,degrades,performance has degrades,0.5837839841842651
translation,361,35,results,results,observe,consistent gains,results observe consistent gains,0.6185363531112671
translation,361,108,results,consistent improvements,in,accuracy,consistent improvements in accuracy,0.531859815120697
translation,361,108,results,consistent improvements,enrich,reference corpus,consistent improvements enrich reference corpus,0.66962730884552
translation,361,108,results,accuracy,across,all tasks,accuracy across all tasks,0.6518583297729492
translation,361,108,results,accuracy,enrich,reference corpus,accuracy enrich reference corpus,0.6126625537872314
translation,361,108,results,reference corpus,with,relevant texts,reference corpus with relevant texts,0.5969324111938477
translation,361,108,results,relevant texts,from,wikipedia,relevant texts from wikipedia,0.5820444822311401
translation,361,108,results,wikipedia,to form,new reference documents,wikipedia to form new reference documents,0.5826454162597656
translation,361,108,results,results,see,consistent improvements,results see consistent improvements,0.6187942624092102
translation,361,109,results,extracted external corpus,to perform,information retrieval,extracted external corpus to perform information retrieval,0.6509948372840881
translation,361,109,results,extracted external corpus,achieve,reasonable performance,extracted external corpus achieve reasonable performance,0.595184326171875
translation,361,109,results,information retrieval,for,reference document generation,information retrieval for reference document generation,0.5321901440620422
translation,361,109,results,reasonable performance,compared to using,original reference corpus,reasonable performance compared to using original reference corpus,0.6924206018447876
translation,361,109,results,original reference corpus,especially on,openbookqa dataset,original reference corpus especially on openbookqa dataset,0.5439212322235107
translation,361,109,results,openbookqa dataset,has,62.2 %,openbookqa dataset has 62.2 %,0.563115119934082
translation,361,109,results,results,using,extracted external corpus,results using extracted external corpus,0.6419568061828613
translation,361,111,results,integrated corpus,boosts,performance,integrated corpus boosts performance,0.7096991539001465
translation,361,111,results,results,using,integrated corpus,results using integrated corpus,0.6446954607963562
translation,361,114,results,similar gain,on,arc - easy,similar gain on arc - easy,0.5735703706741333
translation,361,114,results,results,not see,similar gain,results not see similar gain,0.6647965908050537
translation,361,116,results,best models,i.e.,irc + iec,best models i.e. irc + iec,0.6235362887382507
translation,361,116,results,best models,already beats,previous stateof - the- art model,best models already beats previous stateof - the- art model,0.6053801774978638
translation,361,116,results,irc + iec,for,arc - bookqa,irc + iec for arc - bookqa,0.6950681805610657
translation,361,116,results,results,has,best models,results has best models,0.5325852632522583
translation,361,134,results,dramatic gain,on,arc - challenge dataset,dramatic gain on arc - challenge dataset,0.5295939445495605
translation,361,134,results,dramatic gain,from,46.1 % to 53.7 %,dramatic gain from 46.1 % to 53.7 %,0.5166399478912354
translation,361,134,results,results,see,dramatic gain,results see dramatic gain,0.6550201773643494
translation,361,149,results,external knowledge,from,indomain datasets ( instances and reference corpora ) and open-domain texts,external knowledge from indomain datasets ( instances and reference corpora ) and open-domain texts,0.5323370099067688
translation,361,149,results,external knowledge,observe,consistent improvements,external knowledge observe consistent improvements,0.6176553964614868
translation,361,149,results,consistent improvements,on,most of the categories,consistent improvements on most of the categories,0.5385150909423828
translation,361,149,results,results,by leveraging,external knowledge,results by leveraging external knowledge,0.6218175292015076
translation,362,153,ablation-analysis,apparent decrease,in,our model,apparent decrease in our model,0.5516534447669983
translation,362,153,ablation-analysis,apparent decrease,when,any part of modules,apparent decrease when any part of modules,0.6808339357376099
translation,362,153,ablation-analysis,our model,when,any part of modules,our model when any part of modules,0.6363910436630249
translation,362,153,ablation-analysis,any part of modules,is,elimi,any part of modules is elimi,0.655726969242096
translation,362,153,ablation-analysis,ablation analysis,observe,apparent decrease,ablation analysis observe apparent decrease,0.6162210702896118
translation,362,158,ablation-analysis,degrade,as ablating,occurrence flags,degrade as ablating occurrence flags,0.7346643209457397
translation,362,158,ablation-analysis,significantly,as ablating,occurrence flags,significantly as ablating occurrence flags,0.7448563575744629
translation,362,158,ablation-analysis,all types,has,degrade,all types has degrade,0.638181746006012
translation,362,158,ablation-analysis,degrade,has,significantly,degrade has significantly,0.589924693107605
translation,362,158,ablation-analysis,ablation analysis,Results of,all types,ablation analysis Results of all types,0.7154587507247925
translation,362,159,ablation-analysis,accuracy,about,7 %,accuracy about 7 %,0.6355146765708923
translation,362,159,ablation-analysis,almost 4 times higher,than,decrease,almost 4 times higher than decrease,0.6344963908195496
translation,362,159,ablation-analysis,decrease,due to,eliminating f-flag,decrease due to eliminating f-flag,0.732573926448822
translation,362,159,ablation-analysis,a-flag,has,drops,a-flag has drops,0.6537013053894043
translation,362,159,ablation-analysis,drops,has,accuracy,drops has accuracy,0.6373558044433594
translation,362,159,ablation-analysis,ablation analysis,eliminating,a-flag,ablation analysis eliminating a-flag,0.6954523324966431
translation,362,131,baselines,vqa approaches,for,diagram questions,vqa approaches for diagram questions,0.6492041945457458
translation,362,131,baselines,diagram parse graph ( dpg ),as,context graph,diagram parse graph ( dpg ) as context graph,0.48642343282699585
translation,362,131,baselines,context graph,on,diagrams,context graph on diagrams,0.573206901550293
translation,362,131,baselines,diagrams,built by,dsdp - net,diagrams built by dsdp - net,0.6972957253456116
translation,362,7,model,visual features,establish,context graph,visual features establish context graph,0.5422459244728088
translation,362,7,model,context graph,from,texts and images,context graph from texts and images,0.5819928646087646
translation,362,7,model,new module f-gcn,based on,graph convolutional networks ( gcn ),new module f-gcn based on graph convolutional networks ( gcn ),0.6657265424728394
translation,362,7,model,model,establish,context graph,model establish context graph,0.5955106616020203
translation,362,7,model,model,propose,new module f-gcn,model propose new module f-gcn,0.6799443364143372
translation,362,9,model,novel self-supervised open -set learning process,without,any annotations,novel self-supervised open -set learning process without any annotations,0.7246733903884888
translation,362,9,model,model,introduce,novel self-supervised open -set learning process,model introduce novel self-supervised open -set learning process,0.6084399819374084
translation,362,28,model,novel module,based on,graph convolution networks ( gcn ),novel module based on graph convolution networks ( gcn ),0.683149516582489
translation,362,28,model,novel module,to extract,proper knowledge,novel module to extract proper knowledge,0.6733714938163757
translation,362,28,model,proper knowledge,for,solving,proper knowledge for solving,0.6685970425605774
translation,362,28,model,solving,has,questions,solving has questions,0.5981201529502869
translation,362,28,model,model,establish,multi-modal context graph,model establish multi-modal context graph,0.5958028435707092
translation,362,28,model,model,propose,novel module,model propose novel module,0.7295136451721191
translation,362,130,model,memory networks,to embed,texts,memory networks to embed texts,0.6829367280006409
translation,362,130,model,texts,in,lessons and questions,texts in lessons and questions,0.5651463270187378
translation,362,130,model,model,exploits,memory networks,model exploits memory networks,0.6592065095901489
translation,362,132,model,machine comprehension model,exploits,bidirectional attention mechanism,machine comprehension model exploits bidirectional attention mechanism,0.6156439185142517
translation,362,132,model,bidirectional attention mechanism,to capture,dependencies,bidirectional attention mechanism to capture dependencies,0.6935287117958069
translation,362,132,model,dependencies,between,question and corresponding context paragraph,dependencies between question and corresponding context paragraph,0.6363340020179749
translation,362,132,model,bidaf ( bidirectional attention flow network ),has,machine comprehension model,bidaf ( bidirectional attention flow network ) has machine comprehension model,0.5309966206550598
translation,362,132,model,model,incorporates,bidaf ( bidirectional attention flow network ),model incorporates bidaf ( bidirectional attention flow network ),0.7008764743804932
translation,362,138,results,results,on,tqa dataset,results on tqa dataset,0.5683961510658264
translation,362,139,results,all variants,of,our model,all variants of our model,0.5964832305908203
translation,362,139,results,other recent models,in,all type of question,other recent models in all type of question,0.5023082494735718
translation,362,139,results,our model,has,outperform,our model has outperform,0.6410396099090576
translation,362,139,results,outperform,has,other recent models,outperform has other recent models,0.5934943556785583
translation,362,139,results,results,show,all variants,results show all variants,0.6044794321060181
translation,362,140,results,our best model,shows,about 4 % higher,our best model shows about 4 % higher,0.6863065958023071
translation,362,140,results,about 4 % higher,than,state - of - the - art model,about 4 % higher than state - of - the - art model,0.5387274026870728
translation,362,140,results,results,has,our best model,results has our best model,0.5419765710830688
translation,362,141,results,accuracy,in,text question,accuracy in text question,0.5154002904891968
translation,362,141,results,other results,with,about 8 % margin,other results with about 8 % margin,0.6649230718612671
translation,362,141,results,text question,has,significantly outperforms,text question has significantly outperforms,0.6431794166564941
translation,362,141,results,significantly outperforms,has,other results,significantly outperforms has other results,0.5899722576141357
translation,362,142,results,result,on,diagram questions,result on diagram questions,0.5568203926086426
translation,362,142,results,result,shows,more than 1 % increase,result shows more than 1 % increase,0.7148796916007996
translation,362,142,results,diagram questions,shows,more than 1 % increase,diagram questions shows more than 1 % increase,0.71686851978302
translation,362,142,results,more than 1 % increase,over,previous best model,more than 1 % increase over previous best model,0.6964460611343384
translation,362,142,results,results,on,diagram questions,results on diagram questions,0.5466101169586182
translation,362,142,results,results,has,result,results has result,0.5303357243537903
translation,362,144,results,model w/o visual context,uses,one - layer gcn,model w/o visual context uses one - layer gcn,0.5084933638572693
translation,362,144,results,model w/o visual context,shows,better result,model w/o visual context shows better result,0.6775947213172913
translation,362,144,results,one - layer gcn,for,textual context,one - layer gcn for textual context,0.5774194002151489
translation,362,144,results,better result,compared to,memn+ vqa,better result compared to memn+ vqa,0.6431700587272644
translation,362,144,results,better result,compared to,memn + dpg,better result compared to memn + dpg,0.6424569487571716
translation,362,144,results,better result,compared to,igmn,better result compared to igmn,0.6818140149116516
translation,362,144,results,memn + dpg,with,large margin,memn + dpg with large margin,0.6505678296089172
translation,362,144,results,igmn,with,about 3 % margin,igmn with about 3 % margin,0.6989346146583557
translation,362,145,results,igmn,exploits,graph module,igmn exploits graph module,0.7389088869094849
translation,362,145,results,graph module,of,contraction,graph module of contraction,0.5917136073112488
translation,362,145,results,outperforms,especially in,text problems,outperforms especially in text problems,0.6368642449378967
translation,362,145,results,outperforms,especially in,mc,outperforms especially in mc,0.633739709854126
translation,362,145,results,mc,with,over 5 % margin,mc with over 5 % margin,0.7141660451889038
translation,362,145,results,results,has,igmn,results has igmn,0.5843418836593628
translation,362,147,results,our models,with,multi-modal contexts,our models with multi-modal contexts,0.5996161103248596
translation,362,147,results,multi-modal contexts,show,significantly better results,multi-modal contexts show significantly better results,0.6112065315246582
translation,362,147,results,significantly better results,on,text and diagram questions,significantly better results on text and diagram questions,0.5265794396400452
translation,362,147,results,results,has,our models,results has our models,0.5733726620674133
translation,362,148,results,results,of,diagram question,results of diagram question,0.5846322178840637
translation,362,148,results,outperform,over,1 %,outperform over 1 %,0.7653912901878357
translation,362,148,results,1 %,rather than,our model w/o visual context,1 % rather than our model w/o visual context,0.6114560961723328
translation,362,148,results,diagram question,has,outperform,diagram question has outperform,0.6450816988945007
translation,362,148,results,results,of,diagram question,results of diagram question,0.5846322178840637
translation,362,148,results,results,has,results,results has results,0.48582205176353455
translation,362,149,results,f-gcn,sufficiently exploits,visual contexts,f-gcn sufficiently exploits visual contexts,0.7353412508964539
translation,362,149,results,visual contexts,to solve,diagram questions,visual contexts to solve diagram questions,0.59767746925354
translation,362,149,results,results,indicate,f-gcn,results indicate f-gcn,0.5449603796005249
translation,362,161,results,our model,without,both flags,our model without both flags,0.7495864033699036
translation,362,161,results,our model,shows,lowest results,our model shows lowest results,0.7473588585853577
translation,362,161,results,lowest results,due to,loss of representational power,lowest results due to loss of representational power,0.6827563643455505
translation,362,161,results,results,has,our model,results has our model,0.5871725678443909
translation,363,327,ablation-analysis,40.8 % and 0.23 %,of,cases,40.8 % and 0.23 % of cases,0.5846648216247559
translation,363,327,ablation-analysis,40.8 % and 0.23 %,not show,any change,40.8 % and 0.23 % not show any change,0.713744044303894
translation,363,327,ablation-analysis,any change,in,score,any change in score,0.5163858532905579
translation,363,327,ablation-analysis,score,for,bert - grc and bert - grc,score for bert - grc and bert - grc,0.6844714879989624
translation,363,327,ablation-analysis,ablation analysis,for,40.8 % and 0.23 %,ablation analysis for 40.8 % and 0.23 %,0.5785311460494995
translation,363,37,baselines,eqasc - perturbed,contains,semantically invariant perturbations,eqasc - perturbed contains semantically invariant perturbations,0.6915367245674133
translation,363,37,baselines,semantically invariant perturbations,of,subset of qasc explanations,semantically invariant perturbations of subset of qasc explanations,0.5816279649734497
translation,363,37,baselines,baselines,has,eqasc - perturbed,baselines has eqasc - perturbed,0.6147180795669556
translation,363,38,baselines,eobqa,adds,adding explanation annotations,eobqa adds adding explanation annotations,0.6370328664779663
translation,363,38,baselines,adding explanation annotations,to,obqa test set,adding explanation annotations to obqa test set,0.5382484793663025
translation,363,38,baselines,baselines,has,eobqa,baselines has eobqa,0.5885084271430969
translation,363,148,experimental-setup,allennlp toolkit,to code,our models,allennlp toolkit to code our models,0.698383092880249
translation,363,148,experimental-setup,experimental setup,use,allennlp toolkit,experimental setup use allennlp toolkit,0.6093052625656128
translation,363,6,experiments,over 98 k explanation annotations,for,multihop question answering dataset qasc,over 98 k explanation annotations for multihop question answering dataset qasc,0.5775080323219299
translation,363,36,experiments,eqasc,containing,annotations,eqasc containing annotations,0.7364524602890015
translation,363,36,experiments,annotations,on,over 98 k candidate explanations,annotations on over 98 k candidate explanations,0.5334241390228271
translation,363,36,experiments,over 98 k candidate explanations,for,qasc dataset,over 98 k candidate explanations for qasc dataset,0.6177550554275513
translation,363,36,experiments,over 98 k candidate explanations,including on,multiple ( typically 10 ) possible explanations,over 98 k candidate explanations including on multiple ( typically 10 ) possible explanations,0.6730993986129761
translation,363,36,experiments,multiple ( typically 10 ) possible explanations,for,each answer,multiple ( typically 10 ) possible explanations for each answer,0.591272234916687
translation,363,36,experiments,multiple ( typically 10 ) possible explanations,including,valid and invalid explanations,multiple ( typically 10 ) possible explanations including valid and invalid explanations,0.6929591298103333
translation,363,144,results,generalized chains ( bert - grc ),performs,similarly,generalized chains ( bert - grc ) performs similarly,0.6021668910980225
translation,363,144,results,similarly,to,bert - chain,similarly to bert - chain,0.6628589034080505
translation,363,144,results,results,Using,generalized chains ( bert - grc ),results Using generalized chains ( bert - grc ),0.6702516674995422
translation,363,178,results,performance,on,eqasc,performance on eqasc,0.5552114844322205
translation,363,180,results,best performing versions,of,bert - chain and bert - grc,best performing versions of bert - chain and bert - grc,0.618032693862915
translation,363,180,results,best performing versions,has,significantly outperforms,best performing versions has significantly outperforms,0.6063816547393799
translation,363,180,results,bert - chain and bert - grc,has,significantly outperforms,bert - chain and bert - grc has significantly outperforms,0.6137973070144653
translation,363,180,results,significantly outperforms,has,baselines,significantly outperforms has baselines,0.6014122366905212
translation,363,181,results,auc - roc,is,11 % higher,auc - roc is 11 % higher,0.5894597172737122
translation,363,181,results,ndcg,rises from,0.60 to 0.64,ndcg rises from 0.60 to 0.64,0.6843328475952148
translation,363,181,results,p@1,rises from,0.47 to 0.54,p@1 rises from 0.47 to 0.54,0.6985832452774048
translation,363,181,results,0.47 to 0.54,for,bert - grc,0.47 to 0.54 for bert - grc,0.6216082572937012
translation,363,181,results,11 % higher,has,absolute ),11 % higher has absolute ),0.5924817323684692
translation,363,181,results,results,has,auc - roc,results has auc - roc,0.5549678206443787
translation,363,182,results,generalized chain representation,lead to,significant reduction ( nor gain ),generalized chain representation lead to significant reduction ( nor gain ),0.6614287495613098
translation,363,182,results,significant reduction ( nor gain ),in,performance,significant reduction ( nor gain ) in performance,0.5474491119384766
translation,363,182,results,lexical details,through,variabilization,lexical details through variabilization,0.681391716003418
translation,363,182,results,results,has,generalized chain representation,results has generalized chain representation,0.5868598818778992
translation,363,184,results,bert - qa baseline,scores,surprisingly low,bert - qa baseline scores surprisingly low,0.7320882678031921
translation,363,184,results,results,has,bert - qa baseline,results has bert - qa baseline,0.5650545358657837
translation,363,194,results,generalized chain representation,exhibits,no change,generalized chain representation exhibits no change,0.6655100584030151
translation,363,194,results,large fraction of the instances,has,generalized chain representation,large fraction of the instances has generalized chain representation,0.547819197177887
translation,363,194,results,results,In,large fraction of the instances,results In large fraction of the instances,0.5469915270805359
translation,363,201,results,bert trained models,continue to,significantly outperform,bert trained models continue to significantly outperform,0.6862251162528992
translation,363,201,results,significantly outperform,has,retrieval baseline,significantly outperform has retrieval baseline,0.5876144766807556
translation,363,201,results,results,illustrate,bert trained models,results illustrate bert trained models,0.609988272190094
translation,363,328,results,grcs,improve,prediction consistency,grcs improve prediction consistency,0.6871885061264038
translation,363,328,results,results,suggest,grcs,results suggest grcs,0.5778119564056396
translation,364,148,baselines,cm - squad,generate,cmqa dataset,cm - squad generate cmqa dataset,0.6598613858222961
translation,364,148,baselines,cm - squad,use,our approach of cmqg,cm - squad use our approach of cmqg,0.64720219373703
translation,364,148,baselines,cmqa dataset,from,"portion of squad ( rajpurkar et al. , 2016 ) questions","cmqa dataset from portion of squad ( rajpurkar et al. , 2016 ) questions",0.5242019295692444
translation,364,148,baselines,"portion of squad ( rajpurkar et al. , 2016 ) questions",into,hindi,"portion of squad ( rajpurkar et al. , 2016 ) questions into hindi",0.5617811679840088
translation,364,148,baselines,"portion of squad ( rajpurkar et al. , 2016 ) questions",into,hindi questions,"portion of squad ( rajpurkar et al. , 2016 ) questions into hindi questions",0.5356278419494629
translation,364,148,baselines,our approach of cmqg,to transform,hindi questions,our approach of cmqg to transform hindi questions,0.7164322137832642
translation,364,148,baselines,hindi questions,into,code-mixed questions,hindi questions into code-mixed questions,0.5900194644927979
translation,364,148,baselines,baselines,has,datasets ( cmqa,baselines has datasets ( cmqa,0.5974307656288147
translation,364,184,baselines,baseline,our implementation of,"webshodh ( chandu et al. , 2017 )","baseline our implementation of webshodh ( chandu et al. , 2017 )",0.6469601392745972
translation,364,184,baselines,"webshodh ( chandu et al. , 2017 )",improvements in,some existing components,"webshodh ( chandu et al. , 2017 ) improvements in some existing components",0.6927882432937622
translation,364,184,baselines,baselines,has,baseline,baselines has baseline,0.6124745607376099
translation,364,185,baselines,webshodh 's support vector machine based ( svm ) based question classification,with,recurrent cnn based answer-type detection network,webshodh 's support vector machine based ( svm ) based question classification with recurrent cnn based answer-type detection network,0.6529242396354675
translation,364,167,experimental-setup,english,use,"fast - text ( bojanowski et al. , 2016 ) word embedding","english use fast - text ( bojanowski et al. , 2016 ) word embedding",0.5895635485649109
translation,364,167,experimental-setup,"fast - text ( bojanowski et al. , 2016 ) word embedding",dimension,300,"fast - text ( bojanowski et al. , 2016 ) word embedding dimension 300",0.6760414838790894
translation,364,167,experimental-setup,experimental setup,For,english,experimental setup For english,0.6364397406578064
translation,364,169,experimental-setup,word embeddings,dimension,300,word embeddings dimension 300,0.6829888224601746
translation,364,169,experimental-setup,word embeddings,by,word embedding algorithm,word embeddings by word embedding algorithm,0.5280638933181763
translation,364,169,experimental-setup,300,by,word embedding algorithm,300 by word embedding algorithm,0.5699132084846497
translation,364,171,experimental-setup,hyper-parameters,set to,character embedding dimension,hyper-parameters set to character embedding dimension,0.6452846527099609
translation,364,171,experimental-setup,hyper-parameters,set to,gru hidden unit size,hyper-parameters set to gru hidden unit size,0.6789127588272095
translation,364,171,experimental-setup,hyper-parameters,set to,cnn filter size,hyper-parameters set to cnn filter size,0.6600740551948547
translation,364,171,experimental-setup,hyper-parameters,set to,"filter size=3 , 4","hyper-parameters set to filter size=3 , 4",0.6956680417060852
translation,364,171,experimental-setup,hyper-parameters,set to,batch size,hyper-parameters set to batch size,0.7046759724617004
translation,364,171,experimental-setup,hyper-parameters,set to,# of epochs,hyper-parameters set to # of epochs,0.6652227640151978
translation,364,171,experimental-setup,hyper-parameters,set to,initial learning rate,hyper-parameters set to initial learning rate,0.6567577123641968
translation,364,171,experimental-setup,gru hidden unit size,=,150,gru hidden unit size = 150,0.7065389156341553
translation,364,171,experimental-setup,gru hidden unit size,=,150,gru hidden unit size = 150,0.7065389156341553
translation,364,171,experimental-setup,gru hidden unit size,=,150,gru hidden unit size = 150,0.7065389156341553
translation,364,171,experimental-setup,cnn filter size,=,150,cnn filter size = 150,0.666559100151062
translation,364,171,experimental-setup,# of epochs,=,100,# of epochs = 100,0.7014289498329163
translation,364,171,experimental-setup,character embedding dimension,has,=50,character embedding dimension has =50,0.6136066913604736
translation,364,171,experimental-setup,batch size,has,=60,batch size has =60,0.6424779891967773
translation,364,171,experimental-setup,initial learning rate,has,=0.001,initial learning rate has =0.001,0.5322429537773132
translation,364,172,experimental-setup,hyperparameters,decided based on,model performance,hyperparameters decided based on model performance,0.6585168242454529
translation,364,172,experimental-setup,model performance,on,development set,model performance on development set,0.5744194388389587
translation,364,172,experimental-setup,development set,of,cm - squad dataset,development set of cm - squad dataset,0.5377079844474792
translation,364,172,experimental-setup,experimental setup,Optimal values of,hyperparameters,experimental setup Optimal values of hyperparameters,0.6558497548103333
translation,364,173,experimental-setup,"adam optimizer ( kingma and ba , 2014 )",to optimize,weights,"adam optimizer ( kingma and ba , 2014 ) to optimize weights",0.7143373489379883
translation,364,173,experimental-setup,weights,during,training,weights during training,0.6922373175621033
translation,364,173,experimental-setup,experimental setup,has,"adam optimizer ( kingma and ba , 2014 )","experimental setup has adam optimizer ( kingma and ba , 2014 )",0.535233736038208
translation,364,145,experiments,higher cmi score,compared to,fire 7 2015 ( cmi = 11.65 ),higher cmi score compared to fire 7 2015 ( cmi = 11.65 ),0.6697940230369568
translation,364,145,experiments,hinglish que dataset,has,higher cmi score,hinglish que dataset has higher cmi score,0.5537248253822327
translation,364,168,experiments,hindi sentences,from,bojar et al . ( 2014 ),hindi sentences from bojar et al . ( 2014 ),0.4962472915649414
translation,364,27,model,linguistically motivated technique,for generating,code-mixed questions,linguistically motivated technique for generating code-mixed questions,0.7362278699874878
translation,364,27,model,model,propose,linguistically motivated technique,model propose linguistically motivated technique,0.6982609629631042
translation,364,29,model,effective framework,based on,deep neural network,effective framework based on deep neural network,0.6226648688316345
translation,364,29,model,deep neural network,for,code-mixed question answering ( cmqa ),deep neural network for code-mixed question answering ( cmqa ),0.6064940690994263
translation,364,29,model,model,propose,effective framework,model propose effective framework,0.6704967617988586
translation,364,30,model,cmqa technique,use,multiple attention based recurrent units,cmqa technique use multiple attention based recurrent units,0.5599040985107422
translation,364,30,model,multiple attention based recurrent units,to represent,code-mixed questions,multiple attention based recurrent units to represent code-mixed questions,0.68686443567276
translation,364,30,model,multiple attention based recurrent units,to represent,english passages,multiple attention based recurrent units to represent english passages,0.6528113484382629
translation,364,30,model,model,In,cmqa technique,model In cmqa technique,0.5622547268867493
translation,364,34,model,bilinear attention and answer-type focused neural framework,to deal with,cmqa,bilinear attention and answer-type focused neural framework to deal with cmqa,0.6792290210723877
translation,364,34,model,model,propose,bilinear attention and answer-type focused neural framework,model propose bilinear attention and answer-type focused neural framework,0.6390087604522705
translation,364,170,model,monolingual vectors,of,english and roman words,monolingual vectors of english and roman words,0.5877305865287781
translation,364,170,model,english and roman words,in,unified vector space,english and roman words in unified vector space,0.5329192280769348
translation,364,170,model,unified vector space,using,linear transformation matrix,unified vector space using linear transformation matrix,0.6478161215782166
translation,364,170,model,model,align,monolingual vectors,model align monolingual vectors,0.6484025716781616
translation,364,147,results,cmi score,of,system generated codemixed questions,cmi score of system generated codemixed questions,0.5558673143386841
translation,364,147,results,system generated codemixed questions,is,37.22,system generated codemixed questions is 37.22,0.5706431269645691
translation,364,147,results,results,has,cmi score,results has cmi score,0.49514299631118774
translation,364,202,results,performance,of,ir based baseline,performance of ir based baseline,0.5567577481269836
translation,364,202,results,performance,are,poor,performance are poor,0.6042965650558472
translation,364,202,results,results,of,cmqa,results of cmqa,0.568769097328186
translation,364,207,results,performance,of,cm - mmqa,performance of cm - mmqa,0.6142795085906982
translation,364,207,results,cm - mmqa,better than,cm - squad,cm - mmqa better than cm - squad,0.7957097887992859
translation,364,225,results,performance,on,cm - mmqa dataset,performance on cm - mmqa dataset,0.5116066932678223
translation,364,225,results,cm - mmqa dataset,obtain,em and f1 scores,cm - mmqa dataset obtain em and f1 scores,0.5422499775886536
translation,364,225,results,em and f1 scores,of,40.50 % and 53.73 %,em and f1 scores of 40.50 % and 53.73 %,0.5461376905441284
translation,364,225,results,results,evaluate,performance,results evaluate performance,0.6956482529640198
translation,365,85,experiments,significantly outperform,on,percentile rank,significantly outperform on percentile rank,0.5856755375862122
translation,365,85,experiments,percentile rank,of,ground - truth image,percentile rank of ground - truth image,0.5824500322341919
translation,365,85,experiments,diverse-q-bot + a-bot ( sl and rl ),has,significantly outperform,diverse-q-bot + a-bot ( sl and rl ) has significantly outperform,0.5846474170684814
translation,365,85,experiments,significantly outperform,has,baseline,significantly outperform has baseline,0.6015630960464478
translation,365,76,hyperparameters,dropout rate,of,0.5,dropout rate of 0.5,0.6072384119033813
translation,365,76,hyperparameters,0.5,for,all sl - pretraining experiments,0.5 for all sl - pretraining experiments,0.606684684753418
translation,365,76,hyperparameters,no dropout,for,rl - finetuning,no dropout for rl - finetuning,0.6343466639518738
translation,365,77,hyperparameters,"adam ( kingma and ba , 2015 )",with,learning rate,"adam ( kingma and ba , 2015 ) with learning rate",0.5996310710906982
translation,365,77,hyperparameters,learning rate,of,10 ?3,learning rate of 10 ?3,0.6398048996925354
translation,365,77,hyperparameters,10 ?3,decayed by,0.25 every epoch,10 ?3 decayed by 0.25 every epoch,0.7616208791732788
translation,365,77,hyperparameters,hyperparameters,used,"adam ( kingma and ba , 2015 )","hyperparameters used adam ( kingma and ba , 2015 )",0.5725852847099304
translation,365,78,hyperparameters,objective,for training,diverse -q- bot,objective for training diverse -q- bot,0.7789468765258789
translation,365,78,hyperparameters,diverse -q- bot,sum of,smooth - l1 penalty,diverse -q- bot sum of smooth - l1 penalty,0.661933422088623
translation,365,78,hyperparameters,hyperparameters,has,objective,hyperparameters has objective,0.49555033445358276
translation,365,7,model,simple auxiliary objective,incentivizes,q-bot,simple auxiliary objective incentivizes q-bot,0.6658967733383179
translation,365,7,model,q-bot,to ask,diverse questions,q-bot to ask diverse questions,0.7077510356903076
translation,365,7,model,diverse questions,reducing,repetitions,diverse questions reducing repetitions,0.7496290802955627
translation,365,7,model,larger state space,during,rl,larger state space during rl,0.6698451042175293
translation,365,7,model,model,devise,simple auxiliary objective,model devise simple auxiliary objective,0.7815765142440796
translation,365,30,model,smooth - l1 penalty,penalizes,similarity,smooth - l1 penalty penalizes similarity,0.7799352407455444
translation,365,30,model,similarity,in,successive state vectors,similarity in successive state vectors,0.5553390979766846
translation,365,30,model,model,devise,smooth - l1 penalty,model devise smooth - l1 penalty,0.737582802772522
translation,365,36,results,diverse-q- bot -a- bot dialog,after,rl,diverse-q- bot -a- bot dialog after rl,0.6707560420036316
translation,365,36,results,rl,is,detailed,rl is detailed,0.6885678172111511
translation,365,36,results,results,find that,diverse-q- bot -a- bot dialog,results find that diverse-q- bot -a- bot dialog,0.6448102593421936
translation,365,80,results,training,for,large number of epochs (   80 ),training for large number of epochs (   80 ),0.6146986484527588
translation,365,80,results,large number of epochs (   80 ),with,mentioned range of coefficient values,large number of epochs (   80 ) with mentioned range of coefficient values,0.6331390142440796
translation,365,80,results,mentioned range of coefficient values,led to,best results,mentioned range of coefficient values led to best results,0.660733699798584
translation,365,80,results,results,observed,training,results observed training,0.6312442421913147
translation,365,82,results,questionrepetition penalty,consistently increases,diversity,questionrepetition penalty consistently increases diversity,0.7875838875770569
translation,365,82,results,diversity,over,baseline,diversity over baseline,0.7117322087287903
translation,365,82,results,diversity,has,in both sl and rl,diversity has in both sl and rl,0.5744887590408325
translation,365,82,results,results,has,questionrepetition penalty,results has questionrepetition penalty,0.5430468320846558
translation,365,83,results,rl,asks,~ 1.5 more unique questions,rl asks ~ 1.5 more unique questions,0.7134560346603394
translation,365,83,results,rl,asks,~ 6.3x more novel questions,rl asks ~ 6.3x more novel questions,0.7473645210266113
translation,365,83,results,diverse -q- bot,asks,~ 1.5 more unique questions,diverse -q- bot asks ~ 1.5 more unique questions,0.7085104584693909
translation,365,83,results,~ 1.5 more unique questions,on average,das,~ 1.5 more unique questions on average das,0.6416041851043701
translation,365,83,results,~ 1.5 more unique questions,on average,~ 6.3x more novel questions,~ 1.5 more unique questions on average ~ 6.3x more novel questions,0.6422274708747864
translation,365,83,results,~ 1.5 more unique questions,than,das,~ 1.5 more unique questions than das,0.5801702737808228
translation,365,83,results,~ 1.5 more unique questions,for,every 10 - round dialog,~ 1.5 more unique questions for every 10 - round dialog,0.6466062664985657
translation,365,83,results,fraction and entropy,of,unique generated n-grams,fraction and entropy of unique generated n-grams,0.6143688559532166
translation,365,83,results,rl,has,diverse -q- bot,rl has diverse -q- bot,0.6592782139778137
translation,365,83,results,higher,has,fraction and entropy,higher has fraction and entropy,0.5937506556510925
translation,365,83,results,results,has,rl,results has rl,0.4445461630821228
translation,365,84,results,does not statistically improve,over,baseline,does not statistically improve over baseline,0.6769936680793762
translation,365,84,results,human questions,from,visdial,human questions from visdial,0.6126165390014648
translation,365,84,results,outperforms,has,sl,outperforms has sl,0.6629595160484314
translation,365,84,results,outperforms,has,does not statistically improve,outperforms has does not statistically improve,0.6587845683097839
translation,365,84,results,sl,has,does not statistically improve,sl has does not statistically improve,0.6112876534461975
translation,366,75,ablation-analysis,strong regularization,such as,dropout,strong regularization such as dropout,0.6198052763938904
translation,366,75,ablation-analysis,strong regularization,helps,our nn models,strong regularization helps our nn models,0.6137149333953857
translation,366,75,ablation-analysis,dropout,with,high dropping probability,dropout with high dropping probability,0.6324123740196228
translation,366,75,ablation-analysis,high dropping probability,reduces,overfitting,high dropping probability reduces overfitting,0.6747872233390808
translation,366,75,ablation-analysis,our nn models,achieve,high performance,our nn models achieve high performance,0.6606828570365906
translation,366,75,ablation-analysis,ablation analysis,Applying,strong regularization,ablation analysis Applying strong regularization,0.6969633102416992
translation,366,52,experimental-setup,pre-training,using,skip-gram nn architecture,pre-training using skip-gram nn architecture,0.6623663902282715
translation,366,52,experimental-setup,skip-gram nn architecture,available in,gensim word2vec tool,skip-gram nn architecture available in gensim word2vec tool,0.6393870115280151
translation,366,52,experimental-setup,skip-gram nn architecture,use,in- domain word embeddings vectors,skip-gram nn architecture use in- domain word embeddings vectors,0.5671813488006592
translation,366,52,experimental-setup,in- domain word embeddings vectors,trained on,community question answering dataset,in- domain word embeddings vectors trained on community question answering dataset,0.6877133846282959
translation,366,52,experimental-setup,community question answering dataset,provided by,semeval - 2016,community question answering dataset provided by semeval - 2016,0.5895773768424988
translation,366,52,experimental-setup,experimental setup,perform,pre-training,experimental setup perform pre-training,0.5816611051559448
translation,366,52,experimental-setup,experimental setup,use,in- domain word embeddings vectors,experimental setup use in- domain word embeddings vectors,0.5736044049263
translation,366,53,experimental-setup,maximum distance,between,current and predicted word,maximum distance between current and predicted word,0.6425862908363342
translation,366,53,experimental-setup,current and predicted word,within,sentence,current and predicted word within sentence,0.6374234557151794
translation,366,53,experimental-setup,all words,with,total frequency,all words with total frequency,0.5839800834655762
translation,366,53,experimental-setup,total frequency,lower than,5,total frequency lower than 5,0.7951833009719849
translation,366,53,experimental-setup,5 negative samples,drawn for,negative sampling,5 negative samples drawn for negative sampling,0.7323128581047058
translation,366,53,experimental-setup,number of iterations,set to,30,number of iterations set to 30,0.6980058550834656
translation,366,53,experimental-setup,each positive sample,has,5 negative samples,each positive sample has 5 negative samples,0.6053515076637268
translation,366,58,experimental-setup,two -layer feedforward neural network,with,64 sigmoid activation hidden units,two -layer feedforward neural network with 64 sigmoid activation hidden units,0.6285855174064636
translation,366,58,experimental-setup,64 sigmoid activation hidden units,as,our classifier,64 sigmoid activation hidden units as our classifier,0.5261111855506897
translation,366,58,experimental-setup,experimental setup,consider,two -layer feedforward neural network,experimental setup consider two -layer feedforward neural network,0.6501135230064392
translation,366,59,experimental-setup,weights,initialized with,random orthogonal conditions,weights initialized with random orthogonal conditions,0.7697848081588745
translation,366,59,experimental-setup,experimental setup,has,weights,experimental setup has weights,0.5104771852493286
translation,366,60,experimental-setup,"dropout ( srivastava et al. , 2014 )",with,fixed dropping probability,"dropout ( srivastava et al. , 2014 ) with fixed dropping probability",0.5906038880348206
translation,366,60,experimental-setup,fixed dropping probability,of,0.9,fixed dropping probability of 0.9,0.6030632257461548
translation,366,60,experimental-setup,fixed dropping probability,of,0.5,fixed dropping probability of 0.5,0.6116273403167725
translation,366,60,experimental-setup,0.9,for,input layer,0.9 for input layer,0.5964117050170898
translation,366,60,experimental-setup,0.9,for,hidden layer,0.9 for hidden layer,0.6137796640396118
translation,366,60,experimental-setup,0.9,for,hidden layer,0.9 for hidden layer,0.6137796640396118
translation,366,60,experimental-setup,0.5,for,hidden layer,0.5 for hidden layer,0.620051383972168
translation,366,60,experimental-setup,experimental setup,apply,"dropout ( srivastava et al. , 2014 )","experimental setup apply dropout ( srivastava et al. , 2014 )",0.5641402006149292
translation,366,61,experimental-setup,minimizing,using,"adam ( kingma and ba , 2014 )","minimizing using adam ( kingma and ba , 2014 )",0.6868366599082947
translation,366,61,experimental-setup,categorical cross entropy error,over,training set,categorical cross entropy error over training set,0.651133120059967
translation,366,61,experimental-setup,minimizing,has,categorical cross entropy error,minimizing has categorical cross entropy error,0.5342553853988647
translation,366,61,experimental-setup,"adam ( kingma and ba , 2014 )",has,first-order gradient - based optimization method,"adam ( kingma and ba , 2014 ) has first-order gradient - based optimization method",0.5524736046791077
translation,366,62,experimental-setup,backpropagation algorithm,to compute,gradients,backpropagation algorithm to compute gradients,0.7180043458938599
translation,366,62,experimental-setup,gradients,of,network,gradients of network,0.5913940072059631
translation,366,62,experimental-setup,experimental setup,use,backpropagation algorithm,experimental setup use backpropagation algorithm,0.6072379350662231
translation,366,63,experimental-setup,nn and the backpropagation algorithm,using,keras,nn and the backpropagation algorithm using keras,0.717714786529541
translation,366,63,experimental-setup,experimental setup,implement,nn and the backpropagation algorithm,experimental setup implement nn and the backpropagation algorithm,0.6537396907806396
translation,366,71,results,outperform,models using,naive word embedding averages,outperform models using naive word embedding averages,0.6634815335273743
translation,366,71,results,models with idf weighting scheme,has,outperform,models with idf weighting scheme has outperform,0.6365097165107727
translation,366,71,results,results,show,models with idf weighting scheme,results show models with idf weighting scheme,0.6663897037506104
translation,366,72,results,performance differences,between,different models,performance differences between different models,0.6803839802742004
translation,366,72,results,different models,using,same weighting scheme,different models using same weighting scheme,0.7158870697021484
translation,366,72,results,different models,using,same word embedding dimension,different models using same word embedding dimension,0.6398353576660156
translation,366,72,results,same word embedding dimension,are,not significant,same word embedding dimension are not significant,0.5915810465812683
translation,366,72,results,results,has,performance differences,results has performance differences,0.5273621082305908
translation,366,73,results,rf and nn,perform,slightly better,rf and nn perform slightly better,0.5574714541435242
translation,366,73,results,slightly better,than,svm,slightly better than svm,0.6039022207260132
translation,366,73,results,results,find that,rf and nn,results find that rf and nn,0.6218554973602295
translation,366,74,results,large word embedding dimension ( ? 80 ),results in,severe overfitting,large word embedding dimension ( ? 80 ) results in severe overfitting,0.5572535395622253
translation,366,74,results,severe overfitting,on,development dataset,severe overfitting on development dataset,0.5099771022796631
translation,366,74,results,results,observe,large word embedding dimension ( ? 80 ),results observe large word embedding dimension ( ? 80 ),0.618502140045166
translation,366,74,results,results,using,large word embedding dimension ( ? 80 ),results using large word embedding dimension ( ? 80 ),0.662505567073822
translation,366,76,results,best classifier ( nn - idf - 100 ),achieves,highest mean average precision,best classifier ( nn - idf - 100 ) achieves highest mean average precision,0.661336362361908
translation,366,76,results,highest mean average precision,among,our classifiers,highest mean average precision among our classifiers,0.5682038068771362
translation,366,76,results,results,has,best classifier ( nn - idf - 100 ),results has best classifier ( nn - idf - 100 ),0.5558309555053711
translation,367,232,baselines,bm25,as,baseline,bm25 as baseline,0.5477827191352844
translation,367,232,baselines,bm25,incorporate,one of the five joint models,bm25 incorporate one of the five joint models,0.6565061211585999
translation,367,233,experimental-setup,lambdamart,to train,combined ranker,lambdamart to train combined ranker,0.6782263517379761
translation,367,233,experimental-setup,experimental setup,has,lambdamart,experimental setup has lambdamart,0.5929619669914246
translation,367,5,model,model,give,systematic study,model give systematic study,0.6308386921882629
translation,367,6,model,training algorithm,generalizes,generative adversarial network ( gan ),training algorithm generalizes generative adversarial network ( gan ),0.7103153467178345
translation,367,6,model,training algorithm,generalizes,generative domain-adaptive nets ( gdan ),training algorithm generalizes generative domain-adaptive nets ( gdan ),0.7394112348556519
translation,367,6,model,training algorithm,generalizes,question answering scenario,training algorithm generalizes question answering scenario,0.7101724743843079
translation,367,6,model,generative domain-adaptive nets ( gdan ),under,question answering scenario,generative domain-adaptive nets ( gdan ) under question answering scenario,0.6034789681434631
translation,367,6,model,model,present,training algorithm,model present training algorithm,0.7228260636329651
translation,367,19,model,qg,as,generation problem,qg as generation problem,0.5431106686592102
translation,367,19,model,qg,exploit,sequence - to-sequence learning ( seq2seq ),qg exploit sequence - to-sequence learning ( seq2seq ),0.7173375487327576
translation,367,19,model,model,consider,qg,model consider qg,0.7386745810508728
translation,367,19,model,model,exploit,sequence - to-sequence learning ( seq2seq ),model exploit sequence - to-sequence learning ( seq2seq ),0.7281051278114319
translation,367,210,model,each table,represented as,flattened vector,each table represented as flattened vector,0.6417827606201172
translation,367,210,model,similarity,between,query,similarity between query,0.655769407749176
translation,367,210,model,similarity,between,table,similarity between table,0.6401615142822266
translation,367,210,model,bm25,has,each table,bm25 has each table,0.6360375881195068
translation,367,210,model,model,In,bm25,model In bm25,0.5870752334594727
translation,367,10,results,performance,of,qg model,performance of qg model,0.5996718406677246
translation,367,10,results,qg model,in terms of,bleu score,qg model in terms of bleu score,0.5966878533363342
translation,367,10,results,qg model,could be easily improved,qa model,qg model could be easily improved qa model,0.7360584139823914
translation,367,10,results,qa model,via,policy gradient,qa model via policy gradient,0.6594153642654419
translation,367,10,results,results,has,performance,results has performance,0.5972660779953003
translation,367,44,results,performance,of,qg model,performance of qg model,0.5996718406677246
translation,367,44,results,qg model,in terms of,bleu score,qg model in terms of bleu score,0.5966878533363342
translation,367,44,results,qg model,could be,consistently improved,qg model could be consistently improved,0.7299724817276001
translation,367,44,results,consistently improved,by,qa model,consistently improved by qa model,0.6299350261688232
translation,367,44,results,consistently improved,via,policy gradient,consistently improved via policy gradient,0.6488767266273499
translation,367,44,results,qa model,via,policy gradient,qa model via policy gradient,0.6594153642654419
translation,367,44,results,results,show that,performance,results show that performance,0.510168194770813
translation,367,223,results,generated questions,as,positive ones,generated questions as positive ones,0.5269294381141663
translation,367,223,results,positive ones,improves,qa model,positive ones improves qa model,0.753632664680481
translation,367,223,results,positive ones,has,collaborative,positive ones has collaborative,0.6023666858673096
translation,367,223,results,results,regarding,generated questions,results regarding generated questions,0.5916840434074402
translation,367,224,results,our algorithm ( gcn ),has,significantly improves,our algorithm ( gcn ) has significantly improves,0.5870330929756165
translation,367,224,results,significantly improves,has,tqnn model,significantly improves has tqnn model,0.5773128867149353
translation,367,224,results,results,has,our algorithm ( gcn ),results has our algorithm ( gcn ),0.5590822100639343
translation,367,235,results,baseline system,could be,dramatically improved,baseline system could be dramatically improved,0.6652838587760925
translation,367,235,results,dramatically improved,by,our system,dramatically improved by our system,0.605698823928833
translation,367,235,results,results,see that,baseline system,results see that baseline system,0.6487388014793396
translation,367,238,results,competitive,performs,better,competitive performs better,0.6381790637969971
translation,367,238,results,better,than,collaborative,better than collaborative,0.6340155601501465
translation,367,238,results,collaborative,on,qg,collaborative on qg,0.6192308664321899
translation,367,238,results,results,in terms of,bleu - 4,results in terms of bleu - 4,0.6086405515670776
translation,367,243,results,gcn approach,obtains,significant improvement,gcn approach obtains significant improvement,0.5994470715522766
translation,367,243,results,significant improvement,over,baseline model,significant improvement over baseline model,0.6503056883811951
translation,367,243,results,results,has,gcn approach,results has gcn approach,0.5694804191589355
translation,367,257,results,results,are,almost consistent,results are almost consistent,0.5851383209228516
translation,367,257,results,results,see that,results,results see that results,0.5610302686691284
translation,367,257,results,results,are,almost consistent,results are almost consistent,0.5851383209228516
translation,367,257,results,results,on,table - based qa and qg tasks,results on table - based qa and qg tasks,0.5173332691192627
translation,367,258,results,our gcn algorithms,achieves,promising performances,our gcn algorithms achieves promising performances,0.643477201461792
translation,367,258,results,promising performances,compared to,strong baseline methods,promising performances compared to strong baseline methods,0.5804553031921387
translation,367,258,results,results,has,our gcn algorithms,results has our gcn algorithms,0.5654740929603577
translation,368,151,ablation-analysis,model scale,further to,1.2 billion parameters,model scale further to 1.2 billion parameters,0.6024190783500671
translation,368,151,ablation-analysis,model scale,improves,answer generation quality f1,model scale improves answer generation quality f1,0.6874242424964905
translation,368,151,ablation-analysis,1.2 billion parameters,improves,answer generation quality f1,1.2 billion parameters improves answer generation quality f1,0.6526508927345276
translation,368,151,ablation-analysis,answer generation quality f1,by,0.4,answer generation quality f1 by 0.4,0.5516582727432251
translation,368,151,ablation-analysis,em,improves by,0.1,em improves by 0.1,0.5945749282836914
translation,368,151,ablation-analysis,ablation analysis,find,model scale,ablation analysis find model scale,0.5836995840072632
translation,368,151,ablation-analysis,ablation analysis,increasing,model scale,ablation analysis increasing model scale,0.7457748055458069
translation,368,81,experimental-setup,bert models,for,filtration,bert models for filtration,0.7136543989181519
translation,368,81,experimental-setup,bert models,for,question answering,bert models for question answering,0.6111419796943665
translation,368,81,experimental-setup,bert models,done with,learning rate,bert models done with learning rate,0.6644358038902283
translation,368,81,experimental-setup,bert models,done with,cosine decay,bert models done with cosine decay,0.6497277021408081
translation,368,81,experimental-setup,learning rate,of,1e - 5,learning rate of 1e - 5,0.6323861479759216
translation,368,81,experimental-setup,cosine decay,scheduled over,2 epochs,cosine decay scheduled over 2 epochs,0.7802923917770386
translation,368,81,experimental-setup,2 epochs,of,training data,2 epochs of training data,0.57123202085495
translation,368,81,experimental-setup,experimental setup,Finetuning,bert models,experimental setup Finetuning bert models,0.7332741618156433
translation,368,204,experimental-setup,overgeneration,sample,one question,overgeneration sample one question,0.7489253282546997
translation,368,204,experimental-setup,overgeneration,one with,top -p ( p = 0.9 ) nucleus sampling,overgeneration one with top -p ( p = 0.9 ) nucleus sampling,0.2774689197540283
translation,368,204,experimental-setup,one question,with,top -k ( k = 40 ) sampling,one question with top -k ( k = 40 ) sampling,0.6686967611312866
translation,368,204,experimental-setup,one question,with,top -p ( p = 0.9 ) nucleus sampling,one question with top -p ( p = 0.9 ) nucleus sampling,0.637792706489563
translation,368,204,experimental-setup,experimental setup,To perform,overgeneration,experimental setup To perform overgeneration,0.6885853409767151
translation,368,249,experimental-setup,gpt - 2 models,trained at,batch size,gpt - 2 models trained at batch size,0.7010563611984253
translation,368,249,experimental-setup,gpt - 2 models,trained at,"adamw ( loshchilov and hutter , 2018 )","gpt - 2 models trained at adamw ( loshchilov and hutter , 2018 )",0.6986464262008667
translation,368,249,experimental-setup,gpt - 2 models,trained at,global gradient norm clipping,gpt - 2 models trained at global gradient norm clipping,0.6814786195755005
translation,368,249,experimental-setup,gpt - 2 models,trained at,normal initialization,gpt - 2 models trained at normal initialization,0.6738288402557373
translation,368,249,experimental-setup,batch size,of,512,batch size of 512,0.6329059600830078
translation,368,249,experimental-setup,batch size,for,300k iterations,batch size for 300k iterations,0.568132758140564
translation,368,249,experimental-setup,512,for,300k iterations,512 for 300k iterations,0.5779311060905457
translation,368,249,experimental-setup,300k iterations,with,3 k iterations,300k iterations with 3 k iterations,0.6769000291824341
translation,368,249,experimental-setup,3 k iterations,of,warmup,3 k iterations of warmup,0.5973308086395264
translation,368,249,experimental-setup,"adamw ( loshchilov and hutter , 2018 )",for,optimization,"adamw ( loshchilov and hutter , 2018 ) for optimization",0.6080144047737122
translation,368,249,experimental-setup,learning rate,of,1.5e - 4,learning rate of 1.5e - 4,0.5943935513496399
translation,368,249,experimental-setup,learning rate,decaying,linearly,learning rate decaying linearly,0.7937746644020081
translation,368,249,experimental-setup,1.5e - 4,decaying,linearly,1.5e - 4 decaying linearly,0.6516156792640686
translation,368,249,experimental-setup,linearly,to,1e - 5,linearly to 1e - 5,0.652428925037384
translation,368,249,experimental-setup,weight decay,of,0.01,weight decay of 0.01,0.603249192237854
translation,368,249,experimental-setup,global gradient norm clipping,of,1.0,global gradient norm clipping of 1.0,0.5541549921035767
translation,368,249,experimental-setup,normal initialization,of,"? ? n ( 0 , 0.02 )","normal initialization of ? ? n ( 0 , 0.02 )",0.5586292743682861
translation,368,249,experimental-setup,optimization,has,learning rate,optimization has learning rate,0.4759896993637085
translation,368,249,experimental-setup,experimental setup,has,gpt - 2 models,experimental setup has gpt - 2 models,0.5567178130149841
translation,368,258,experimental-setup,mixed precision training,on,nvidia v100 gpus,mixed precision training on nvidia v100 gpus,0.48828524351119995
translation,368,258,experimental-setup,experimental setup,trained with,mixed precision training,experimental setup trained with mixed precision training,0.7407509088516235
translation,368,30,model,question generators,on,labeled data,question generators on labeled data,0.5434548258781433
translation,368,30,model,question generators,uses,pretrained gpt - 2 decoder models,question generators uses pretrained gpt - 2 decoder models,0.5310307741165161
translation,368,30,model,concatenation,of,"context , answer , and question tokens","concatenation of context , answer , and question tokens",0.5512832403182983
translation,368,32,model,overgenerate and filter approach,to further improve,question filtration,overgenerate and filter approach to further improve question filtration,0.7100217342376709
translation,368,32,model,model,propose,overgenerate and filter approach,model propose overgenerate and filter approach,0.7165493965148926
translation,368,7,results,squad1.1 question answering,achieve,higher accuracy,squad1.1 question answering achieve higher accuracy,0.6146108508110046
translation,368,7,results,higher accuracy,using,synthetic questions and answers,higher accuracy using synthetic questions and answers,0.6607744693756104
translation,368,9,results,our methodology,to,squad2.0,our methodology to squad2.0,0.556906521320343
translation,368,9,results,squad2.0,show,2.8 absolute gain,squad2.0 show 2.8 absolute gain,0.6377492547035217
translation,368,9,results,2.8 absolute gain,on,em score,2.8 absolute gain on em score,0.5445185899734497
translation,368,9,results,2.8 absolute gain,compared to,prior work,2.8 absolute gain compared to prior work,0.5605018138885498
translation,368,9,results,prior work,using,synthetic data,prior work using synthetic data,0.6283978819847107
translation,368,9,results,results,apply,our methodology,results apply our methodology,0.581484854221344
translation,368,31,results,large generative transformer models,up to,8.3b parameters,large generative transformer models up to 8.3b parameters,0.5648163557052612
translation,368,31,results,large generative transformer models,improves,quality of generated questions,large generative transformer models improves quality of generated questions,0.6446401476860046
translation,368,31,results,8.3b parameters,improves,quality of generated questions,8.3b parameters improves quality of generated questions,0.6368361711502075
translation,368,31,results,results,pretraining,large generative transformer models,results pretraining large generative transformer models,0.6535381078720093
translation,368,36,results,answerable squad1.1 questions,recover,100.8 %,answerable squad1.1 questions recover 100.8 %,0.6443352699279785
translation,368,36,results,answerable squad1.1 questions,recover,100.1 %,answerable squad1.1 questions recover 100.1 %,0.6526693105697632
translation,368,36,results,answerable squad1.1 questions,when training on,purely synthetic questions and answers,answerable squad1.1 questions when training on purely synthetic questions and answers,0.670061469078064
translation,368,36,results,100.8 %,of,fully supervised em,100.8 % of fully supervised em,0.5232648849487305
translation,368,36,results,100.1 %,of,fully supervised f1 scores,100.1 % of fully supervised f1 scores,0.5075626373291016
translation,368,36,results,100.1 %,when training on,purely synthetic questions and answers,100.1 % when training on purely synthetic questions and answers,0.693021297454834
translation,368,36,results,purely synthetic questions and answers,generated from,unlabeled data,purely synthetic questions and answers generated from unlabeled data,0.6391341090202332
translation,368,36,results,results,For,answerable squad1.1 questions,results For answerable squad1.1 questions,0.5755917429924011
translation,368,38,results,resulting model,on,real squad1.1 data,resulting model on real squad1.1 data,0.5375103950500488
translation,368,38,results,real squad1.1 data,reaches,89.4 em,real squad1.1 data reaches 89.4 em,0.7166900038719177
translation,368,38,results,real squad1.1 data,reaches,95.1 f1 score,real squad1.1 data reaches 95.1 f1 score,0.6773371696472168
translation,368,38,results,real squad1.1 data,higher than,any prior bert - based approach,real squad1.1 data higher than any prior bert - based approach,0.6551947593688965
translation,368,38,results,95.1 f1 score,higher than,any prior bert - based approach,95.1 f1 score higher than any prior bert - based approach,0.666341245174408
translation,368,38,results,results,Finetuning,resulting model,results Finetuning resulting model,0.6363301873207092
translation,368,105,results,performance,of,human-labeled data,performance of human-labeled data,0.5523435473442078
translation,368,105,results,human-labeled data,by only using,synthetic data,human-labeled data by only using synthetic data,0.6832868456840515
translation,368,105,results,synthetic data,generated from,synthetic corpus,synthetic data generated from synthetic corpus,0.6427855491638184
translation,368,105,results,recover and surpass,has,performance,recover and surpass has performance,0.6045291423797607
translation,368,123,results,outperform,has,existing methods,outperform has existing methods,0.5829497575759888
translation,368,150,results,answer generation quality,by,1.4 em and 0.3 f1,answer generation quality by 1.4 em and 0.3 f1,0.5686770081520081
translation,368,150,results,1.4 em and 0.3 f1,between,bert - large and our 345 million parameter answer generation model,1.4 em and 0.3 f1 between bert - large and our 345 million parameter answer generation model,0.6081883907318115
translation,368,150,results,pretraining,has,dramatically improve,pretraining has dramatically improve,0.5848662257194519
translation,368,150,results,dramatically improve,has,answer generation quality,dramatically improve has answer generation quality,0.5532169342041016
translation,368,150,results,results,find that,improvements,results find that improvements,0.6680544018745422
translation,368,227,results,large question answering datasets,from,unlabeled wikipedia documents,large question answering datasets from unlabeled wikipedia documents,0.46254947781562805
translation,368,227,results,345 million parameter bert - style model,achieving,88.4 em score,345 million parameter bert - style model achieving 88.4 em score,0.605568528175354
translation,368,228,results,resulting model,on,real squad1.1 data,resulting model on real squad1.1 data,0.5375103950500488
translation,368,228,results,real squad1.1 data,boosts,em score,real squad1.1 data boosts em score,0.7340232729911804
translation,368,228,results,em score,to,89.4,em score to 89.4,0.550352156162262
translation,368,228,results,results,Finetuning,resulting model,results Finetuning resulting model,0.6363301873207092
translation,368,229,results,1.7 point improvement,over,our fully supervised baseline,1.7 point improvement over our fully supervised baseline,0.6211940050125122
translation,368,319,results,question generation,allow for,improved squad2.0 score,question generation allow for improved squad2.0 score,0.6786630749702454
translation,368,319,results,results,Improvements in,question generation,results Improvements in question generation,0.6527759432792664
translation,369,86,ablation-analysis,significant improvements,when introducing,relation matrix m,significant improvements when introducing relation matrix m,0.7205267548561096
translation,369,86,ablation-analysis,relation matrix m,in,soft-cosine metric ( cos m ),relation matrix m in soft-cosine metric ( cos m ),0.5114395618438721
translation,369,86,ablation-analysis,ablation analysis,reveal,significant improvements,ablation analysis reveal significant improvements,0.7049139738082886
translation,369,44,experimental-setup,word2vec toolkit,in,cbow configuration,word2vec toolkit in cbow configuration,0.49088793992996216
translation,369,44,experimental-setup,one,estimated on,english wikipedia,one estimated on english wikipedia,0.7049633860588074
translation,369,44,experimental-setup,one,estimated using,unannotated corpus of questions and comments,one estimated using unannotated corpus of questions and comments,0.7253116369247437
translation,369,44,experimental-setup,other,estimated using,unannotated corpus of questions and comments,other estimated using unannotated corpus of questions and comments,0.7077335715293884
translation,369,44,experimental-setup,unannotated corpus of questions and comments,on,qatar - living forum,unannotated corpus of questions and comments on qatar - living forum,0.5441499948501587
translation,369,44,experimental-setup,2 distributed representations,has,of words,2 distributed representations has of words,0.5065291523933411
translation,369,44,experimental-setup,cbow configuration,has,one,cbow configuration has one,0.6424157619476318
translation,369,28,model,robust and simple supervised framework,has,logistic regression ),robust and simple supervised framework has logistic regression ),0.5722468495368958
translation,369,85,results,baseline pp cos tfidf,show,influence,baseline pp cos tfidf show influence,0.6564534306526184
translation,369,85,results,influence,of,appropriate term weighting,influence of appropriate term weighting,0.6164547204971313
translation,369,85,results,appropriate term weighting,over,simple binary coefficients,appropriate term weighting over simple binary coefficients,0.70328688621521
translation,369,85,results,results,has,baseline pp cos tfidf,results has baseline pp cos tfidf,0.551742434501648
translation,369,89,results,levenshtein- based m matrix,performs,best,levenshtein- based m matrix performs best,0.6369041204452515
translation,369,89,results,best,on,test2017,best on test2017,0.6104076504707336
translation,369,89,results,results,has,levenshtein- based m matrix,results has levenshtein- based m matrix,0.5404772162437439
translation,369,90,results,carefully chosen relation matrix m,in,cosine - based similarity measure,carefully chosen relation matrix m in cosine - based similarity measure,0.49250683188438416
translation,369,90,results,carefully chosen relation matrix m,improves,performances,carefully chosen relation matrix m improves performances,0.6900578737258911
translation,369,91,results,cosine,between,tf - idf weighted average word2vec,cosine between tf - idf weighted average word2vec,0.5924808979034424
translation,369,91,results,cosine,performs,well,cosine performs well,0.6536903381347656
translation,369,91,results,tf - idf weighted average word2vec,is,less effective,tf - idf weighted average word2vec is less effective,0.5134935975074768
translation,369,91,results,less effective,on,dev and test2016,less effective on dev and test2016,0.5736231803894043
translation,369,91,results,well,on,test2017,well on test2017,0.5985900163650513
translation,369,91,results,results,has,cosine,results has cosine,0.42653194069862366
translation,370,31,baselines,nl2psql,propose,end-to- end natural nl2 psql model,nl2psql propose end-to- end natural nl2 psql model,0.6820136904716492
translation,370,31,baselines,end-to- end natural nl2 psql model,without the restriction of,executable sql query,end-to- end natural nl2 psql model without the restriction of executable sql query,0.5346013903617859
translation,370,31,baselines,baselines,has,nl2psql,baselines has nl2psql,0.5772620439529419
translation,370,150,baselines,seq2seq,encodes,questions,seq2seq encodes questions,0.6904703974723816
translation,370,150,baselines,seq2seq,decodes into,queries,seq2seq decodes into queries,0.7057477235794067
translation,370,150,baselines,queries,using,recurrent neural networks ( rnns ),queries using recurrent neural networks ( rnns ),0.7409027814865112
translation,370,160,baselines,copynet,"to selectively "" copy",some input sequence,"copynet to selectively "" copy some input sequence",0.7307949066162109
translation,370,160,baselines,some input sequence,into,output,some input sequence into output,0.6113286018371582
translation,370,160,baselines,baselines,has,copynet,baselines has copynet,0.5773433446884155
translation,370,151,experimental-setup,encoder dimension,is,200,encoder dimension is 200,0.6169257164001465
translation,370,151,experimental-setup,200,by using,bi-directional rnn,200 by using bi-directional rnn,0.6394071578979492
translation,370,151,experimental-setup,bi-directional rnn,with,concatenation,bi-directional rnn with concatenation,0.5674084424972534
translation,370,151,experimental-setup,experimental setup,has,encoder dimension,experimental setup has encoder dimension,0.5270550847053528
translation,370,152,experimental-setup,dimension,of,decoder rnn,dimension of decoder rnn,0.611032247543335
translation,370,152,experimental-setup,decoder rnn,set as,100,decoder rnn set as 100,0.6761260628700256
translation,370,152,experimental-setup,experimental setup,has,dimension,experimental setup has dimension,0.4825572967529297
translation,370,153,experimental-setup,objective function,with,"adam ( kingma and ba , 2015 )","objective function with adam ( kingma and ba , 2015 )",0.5966670513153076
translation,370,153,experimental-setup,"adam ( kingma and ba , 2015 )",using,default momentum hyperparameters,"adam ( kingma and ba , 2015 ) using default momentum hyperparameters",0.6006975173950195
translation,370,153,experimental-setup,experimental setup,optimized,objective function,experimental setup optimized objective function,0.7072893381118774
translation,370,154,experimental-setup,initial learning rate,as,0.0001,initial learning rate as 0.0001,0.5298184156417847
translation,370,154,experimental-setup,mini-batch size,as,32,mini-batch size as 32,0.5737630724906921
translation,370,154,experimental-setup,experimental setup,set,initial learning rate,experimental setup set initial learning rate,0.634415328502655
translation,370,154,experimental-setup,experimental setup,set,mini-batch size,experimental setup set mini-batch size,0.6688211560249329
translation,370,155,experimental-setup,"variational rnn dropout ( gal and ghahramani , 2016 )",with,dropout rate,"variational rnn dropout ( gal and ghahramani , 2016 ) with dropout rate",0.580355703830719
translation,370,155,experimental-setup,dropout rate,of,0.4,dropout rate of 0.4,0.5964629054069519
translation,370,156,experimental-setup,beam-search decoding,set,beam size,beam-search decoding set beam size,0.6550856232643127
translation,370,156,experimental-setup,beam-search decoding,ran for,100 epochs,beam-search decoding ran for 100 epochs,0.6587489247322083
translation,370,156,experimental-setup,beam size,as,100,beam size as 100,0.6114119291305542
translation,370,156,experimental-setup,100 epochs,to train,model,100 epochs to train model,0.7057757377624512
translation,370,156,experimental-setup,experimental setup,During,beam-search decoding,experimental setup During beam-search decoding,0.6632477045059204
translation,370,156,experimental-setup,experimental setup,ran for,100 epochs,experimental setup ran for 100 epochs,0.6038010716438293
translation,370,33,model,two-step translation,by cascading,two sequence - to-sequence models,two-step translation by cascading two sequence - to-sequence models,0.7535821795463562
translation,370,33,model,two sequence - to-sequence models,for generating,initial sql statements,two sequence - to-sequence models for generating initial sql statements,0.7167686820030212
translation,370,33,model,initial sql statements,from,natural language input,initial sql statements from natural language input,0.5317000150680542
translation,370,33,model,other,to fix,ill-formed sql statements,other to fix ill-formed sql statements,0.6099516153335571
translation,370,33,model,model,performs,two-step translation,model performs two-step translation,0.6418271660804749
translation,370,35,model,novel metrics,allowing,mismatches,novel metrics allowing mismatches,0.7346373200416565
translation,370,35,model,novel metrics,allowing,matches,novel metrics allowing matches,0.7239965796470642
translation,370,35,model,novel metrics,requiring,matches,novel metrics requiring matches,0.6999551057815552
translation,370,35,model,mismatches,for,non-essential terms,mismatches for non-essential terms,0.6621596813201904
translation,370,35,model,matches,for,important sql keyword,matches for important sql keyword,0.5789037942886353
translation,370,35,model,canonical - bleu,abstracts,identifier names,canonical - bleu abstracts identifier names,0.6598062515258789
translation,370,35,model,identifier names,of,sql statements,identifier names of sql statements,0.4966314136981964
translation,370,35,model,identifier names,before calculating,bleu scores,identifier names before calculating bleu scores,0.6106839776039124
translation,370,35,model,identifier names,before calculating,bleu scores,identifier names before calculating bleu scores,0.6106839776039124
translation,370,35,model,bleu scores,using,sql keywords,bleu scores using sql keywords,0.5780545473098755
translation,370,35,model,sql - bleu,calculates,bleu scores,sql - bleu calculates bleu scores,0.5669212937355042
translation,370,35,model,bleu scores,using,sql keywords,bleu scores using sql keywords,0.5780545473098755
translation,370,35,model,model,propose,novel metrics,model propose novel metrics,0.707027792930603
translation,370,180,results,token level approaches,mostly yield,higher sql - bleu scores,token level approaches mostly yield higher sql - bleu scores,0.7842382788658142
translation,370,180,results,ir approach,has,token level approaches,ir approach has token level approaches,0.5880855321884155
translation,370,180,results,results,Compared to,ir approach,results Compared to ir approach,0.6902674436569214
translation,370,186,results,ir approach,observed,most of the generative approaches,ir approach observed most of the generative approaches,0.6496412754058838
translation,370,186,results,most of the generative approaches,yield,higher canonical - bleu scores,most of the generative approaches yield higher canonical - bleu scores,0.6958815455436707
translation,370,186,results,results,Compared to,ir approach,results Compared to ir approach,0.6902674436569214
translation,371,6,model,focus,of,question,focus of question,0.5926834940910339
translation,371,6,model,question,in,text passage,question in text passage,0.5098888278007507
translation,371,6,model,question pattern,leads,sequential generation,question pattern leads sequential generation,0.6784941554069519
translation,371,6,model,sequential generation,of,words,sequential generation of words,0.6240991353988647
translation,371,6,model,words,in,question,words in question,0.5680095553398132
translation,371,6,model,model,To properly generate,question,model To properly generate question,0.702903687953949
translation,371,169,model,framework,for,answer - unaware cqg,framework for answer - unaware cqg,0.6309194564819336
translation,371,169,model,model,propose,framework,model propose framework,0.666053295135498
translation,371,139,results,qualities,of,generated questions,qualities of generated questions,0.6031109690666199
translation,371,139,results,generated questions,were,slightly better,generated questions were slightly better,0.6400818228721619
translation,371,139,results,slightly better,than,random choice,slightly better than random choice,0.6300843954086304
translation,371,139,results,random choice,of,chunk,random choice of chunk,0.6269219517707825
translation,371,139,results,chunk,as,question focus,chunk as question focus,0.5548551082611084
translation,371,144,results,our method,with,ground -truth question foci and question patterns,our method with ground -truth question foci and question patterns,0.581832766532898
translation,371,144,results,ground -truth question foci and question patterns,has,largely outperformed,ground -truth question foci and question patterns has largely outperformed,0.5922457575798035
translation,371,144,results,largely outperformed,has,comparing systems,largely outperformed has comparing systems,0.6291252970695496
translation,371,144,results,results,shows that,our method,results shows that our method,0.6864691376686096
translation,372,41,ablation-analysis,sub-questions,improve,multi-hop qa,sub-questions improve multi-hop qa,0.6860607862472534
translation,372,41,ablation-analysis,multi-hop qa,by using,single - hop qa model,multi-hop qa by using single - hop qa model,0.6980035305023193
translation,372,41,ablation-analysis,single - hop qa model,to retrieve,question - relevant text,single - hop qa model to retrieve question - relevant text,0.7535384297370911
translation,372,41,ablation-analysis,ablation analysis,shows,sub-questions,ablation analysis shows sub-questions,0.6666319966316223
translation,372,155,ablation-analysis,onus training,contribute to,decomposition quality,onus training contribute to decomposition quality,0.6583474278450012
translation,372,155,ablation-analysis,ablation analysis,has,pseudo-decomposition,ablation analysis has pseudo-decomposition,0.5675844550132751
translation,372,178,ablation-analysis,sub-answers,crucial to,improving qa,sub-answers crucial to improving qa,0.7310456037521362
translation,372,178,ablation-analysis,sub-questions,with,random answers,sub-questions with random answers,0.6188990473747253
translation,372,178,ablation-analysis,ablation analysis,has,sub-answers,ablation analysis has sub-answers,0.5820162296295166
translation,372,113,hyperparameters,mlm finetuning,for,one epoch,mlm finetuning for one epoch,0.5929654836654663
translation,372,113,hyperparameters,mlm finetuning,for,pseudodecompositions d,mlm finetuning for pseudodecompositions d,0.6298300623893738
translation,372,113,hyperparameters,one epoch,on,q,one epoch on q,0.639026403427124
translation,372,113,hyperparameters,pseudodecompositions d,formed via,random retrieval,pseudodecompositions d formed via random retrieval,0.5736814141273499
translation,372,113,hyperparameters,final weights,to initialize,pretrained encoder-decoder,final weights to initialize pretrained encoder-decoder,0.6985769867897034
translation,372,113,hyperparameters,hyperparameters,do,mlm finetuning,hyperparameters do mlm finetuning,0.44713422656059265
translation,372,5,model,unsupervised approach,to produce,sub-questions,unsupervised approach to produce sub-questions,0.6960999965667725
translation,372,5,model,model,take,unsupervised approach,model take unsupervised approach,0.6739809513092041
translation,372,6,model,algorithm,for,one-to-n unsupervised sequence transduction ( onus ),algorithm for one-to-n unsupervised sequence transduction ( onus ),0.6034544110298157
translation,372,6,model,one-to-n unsupervised sequence transduction ( onus ),map,"one hard , multi-hop question","one-to-n unsupervised sequence transduction ( onus ) map one hard , multi-hop question",0.6820281147956848
translation,372,6,model,model,propose,algorithm,model propose algorithm,0.729954719543457
translation,372,7,model,sub-questions,with,off-the-shelf qa model,sub-questions with off-the-shelf qa model,0.6474747657775879
translation,372,7,model,resulting answers,to,recomposition model,resulting answers to recomposition model,0.5517975091934204
translation,372,7,model,model,answer,sub-questions,model answer sub-questions,0.7429100871086121
translation,372,7,model,model,give,resulting answers,model give resulting answers,0.7106748223304749
translation,372,27,model,supervision,decompose,questions,supervision decompose questions,0.7286940813064575
translation,372,27,model,questions,in,fully unsupervised way,questions in fully unsupervised way,0.5458129048347473
translation,372,27,model,model,of using,supervision,model of using supervision,0.6997890472412109
translation,372,27,model,model,decompose,questions,model decompose questions,0.7440528273582458
translation,372,28,model,algorithm,for,one-to-n unsupervised sequence transduction ( onus ),algorithm for one-to-n unsupervised sequence transduction ( onus ),0.6034544110298157
translation,372,29,model,noisy   pseudo- decomposition,for,each hard question,noisy   pseudo- decomposition for each hard question,0.6216583251953125
translation,372,29,model,each hard question,by using,embedding similarity,each hard question by using embedding similarity,0.6319626569747925
translation,372,29,model,embedding similarity,to retrieve,sub-question candidates,embedding similarity to retrieve sub-question candidates,0.7519567012786865
translation,372,29,model,model,automatically create,noisy   pseudo- decomposition,model automatically create noisy   pseudo- decomposition,0.6277581453323364
translation,372,31,model,decomposition model,on,mined data,decomposition model on mined data,0.5762861371040344
translation,372,31,model,mined data,with,unsupervised sequence - to-sequence learning,mined data with unsupervised sequence - to-sequence learning,0.6302646994590759
translation,372,31,model,model,train,decomposition model,model train decomposition model,0.7005957961082458
translation,372,8,results,large qa improvements,on,hotpotqa,large qa improvements on hotpotqa,0.5611992478370667
translation,372,8,results,large qa improvements,over,strong baseline,large qa improvements over strong baseline,0.6790213584899902
translation,372,8,results,strong baseline,on,"original , out -ofdomain , and multi-hop dev sets","strong baseline on original , out -ofdomain , and multi-hop dev sets",0.4774262011051178
translation,372,8,results,results,show,large qa improvements,results show large qa improvements,0.6698454022407532
translation,372,9,results,onus,automatically learns to decompose,different kinds of questions,onus automatically learns to decompose different kinds of questions,0.757112443447113
translation,372,9,results,results,has,onus,results has onus,0.6266642808914185
translation,372,30,results,over 10 m possible sub-questions,from,common crawl,over 10 m possible sub-questions from common crawl,0.6032231450080872
translation,372,30,results,common crawl,with,classifier,common crawl with classifier,0.670843243598938
translation,372,30,results,results,mine,over 10 m possible sub-questions,results mine over 10 m possible sub-questions,0.6565589904785156
translation,372,39,results,qa models,that use,decompositions,qa models that use decompositions,0.7228699326515198
translation,372,39,results,"strong roberta baseline min et al. , 2019a )",by,3.1 points,"strong roberta baseline min et al. , 2019a ) by 3.1 points",0.5718943476676941
translation,372,39,results,"strong roberta baseline min et al. , 2019a )",by,10 points,"strong roberta baseline min et al. , 2019a ) by 10 points",0.5734378695487976
translation,372,39,results,"strong roberta baseline min et al. , 2019a )",by,11 points,"strong roberta baseline min et al. , 2019a ) by 11 points",0.5710932016372681
translation,372,39,results,3.1 points,on,original dev set,3.1 points on original dev set,0.49094894528388977
translation,372,39,results,in f1,on,original dev set,in f1 on original dev set,0.6328585743904114
translation,372,39,results,10 points,on,out-of- domain dev set,10 points on out-of- domain dev set,0.5634039640426636
translation,372,39,results,out-of- domain dev set,from,min et al . ( 2019 b ),out-of- domain dev set from min et al . ( 2019 b ),0.5392927527427673
translation,372,39,results,11 points,on,multi-hop dev set,11 points on multi-hop dev set,0.5619973540306091
translation,372,39,results,multi-hop dev set,from,jiang and bansal ( 2019a ),multi-hop dev set from jiang and bansal ( 2019a ),0.5760127902030945
translation,372,39,results,decompositions,has,outperform,decompositions has outperform,0.6228777170181274
translation,372,39,results,outperform,has,"strong roberta baseline min et al. , 2019a )","outperform has strong roberta baseline min et al. , 2019a )",0.5902580618858337
translation,372,39,results,3.1 points,has,in f1,3.1 points has in f1,0.5217028260231018
translation,372,39,results,results,has,qa models,results has qa models,0.5290708541870117
translation,372,40,results,our method,is,competitive,our method is competitive,0.6158542037010193
translation,372,40,results,competitive,with,state - of- the - art methods,competitive with state - of- the - art methods,0.6108415126800537
translation,372,40,results,"hgn ( fang et al. , 2019 )",that use,"additional , strong supervision","hgn ( fang et al. , 2019 ) that use additional , strong supervision",0.6922739148139954
translation,372,40,results,"additional , strong supervision",on,sentences,"additional , strong supervision on sentences",0.5094398856163025
translation,372,40,results,sentences,relevant to,question,sentences relevant to question,0.6499513387680054
translation,372,40,results,state - of- the - art methods,has,"sae ( tu et al. , 2020 )","state - of- the - art methods has sae ( tu et al. , 2020 )",0.5276542901992798
translation,372,40,results,results,has,our method,results has our method,0.5589964985847473
translation,372,43,results,onus,automatically learns to generate,useful decompositions,onus automatically learns to generate useful decompositions,0.7239086031913757
translation,372,43,results,useful decompositions,for,all four question types,useful decompositions for all four question types,0.5813309550285339
translation,372,43,results,all four question types,in,hotpotqa,all four question types in hotpotqa,0.5298270583152771
translation,372,43,results,results,has,onus,results has onus,0.6266642808914185
translation,372,44,results,trained onus model,decompose,some questions,trained onus model decompose some questions,0.7618789076805115
translation,372,44,results,some questions,in,visual qa,some questions in visual qa,0.5387416481971741
translation,372,44,results,some questions,in,knowledge - base qa,some questions in knowledge - base qa,0.5343233942985535
translation,372,44,results,finetuning,has,trained onus model,finetuning has trained onus model,0.6155363321304321
translation,372,44,results,results,Without,finetuning,results Without finetuning,0.7093496918678284
translation,372,151,results,roberta baseline,does,quite well,roberta baseline does quite well,0.28426408767700195
translation,372,151,results,quite well,on,hotpotqa ( 77.0 f1 ),quite well on hotpotqa ( 77.0 f1 ),0.5400211811065674
translation,372,151,results,results,has,roberta baseline,results has roberta baseline,0.5533122420310974
translation,372,152,results,large gains,over,roberta baseline,large gains over roberta baseline,0.6889090538024902
translation,372,152,results,large gains,by simply adding,sub-questions and sub-answers,large gains by simply adding sub-questions and sub-answers,0.6930581331253052
translation,372,152,results,sub-questions and sub-answers,to,input,sub-questions and sub-answers to input,0.5940174460411072
translation,372,152,results,results,achieve,large gains,results achieve large gains,0.665022075176239
translation,372,153,results,decompositions,from,onus,decompositions from onus,0.6373392343521118
translation,372,153,results,decompositions,find,gain,decompositions find gain,0.653278648853302
translation,372,153,results,gain,of,3.1 f1,gain of 3.1 f1,0.6049901843070984
translation,372,153,results,gain,of,11 f1,gain of 11 f1,0.6455387473106384
translation,372,153,results,gain,of,10 f1,gain of 10 f1,0.6641408205032349
translation,372,153,results,3.1 f1,on,original dev set,3.1 f1 on original dev set,0.49970462918281555
translation,372,153,results,11 f1,on,multi-hop dev,11 f1 on multi-hop dev,0.6019366383552551
translation,372,153,results,10 f1,on,ood dev,10 f1 on ood dev,0.6665273904800415
translation,372,153,results,results,Using,decompositions,results Using decompositions,0.6558257341384888
translation,372,154,results,onus decompositions,match,performance,onus decompositions match performance,0.7658980488777161
translation,372,154,results,performance,of using,supervised and heuristic decompositions,performance of using supervised and heuristic decompositions,0.6950585246086121
translation,372,154,results,supervised and heuristic decompositions,from,decomprc,supervised and heuristic decompositions from decomprc,0.601982831954956
translation,372,154,results,results,has,onus decompositions,results has onus decompositions,0.6003716588020325
translation,372,156,results,fasttext pseudo-decompositions,provide,improvement,fasttext pseudo-decompositions provide improvement,0.5424238443374634
translation,372,156,results,fasttext pseudo-decompositions,over,random pseudo-decompositions,fasttext pseudo-decompositions over random pseudo-decompositions,0.6552495956420898
translation,372,156,results,improvement,in,qa,improvement in qa,0.5922157764434814
translation,372,156,results,qa,over,baseline,qa over baseline,0.7215273976325989
translation,372,156,results,random pseudo-decompositions,has,f1 ),random pseudo-decompositions has f1 ),0.6238258481025696
translation,372,156,results,results,has,fasttext pseudo-decompositions,results has fasttext pseudo-decompositions,0.5650413036346436
translation,372,157,results,seq2seq,trained on,fasttext pseudo-decompositions,seq2seq trained on fasttext pseudo-decompositions,0.762951135635376
translation,372,157,results,seq2seq,trained on,fasttext pseudo-decompositions,seq2seq trained on fasttext pseudo-decompositions,0.762951135635376
translation,372,157,results,seq2seq,achieves,comparable gains,seq2seq achieves comparable gains,0.7068168520927429
translation,372,157,results,comparable gains,to,fasttext pseudo-decompositions,comparable gains to fasttext pseudo-decompositions,0.5838711857795715
translation,372,157,results,results,has,seq2seq,results has seq2seq,0.5235289335250854
translation,372,158,results,onus,improves over,pseudod and seq2seq,onus improves over pseudod and seq2seq,0.7305237054824829
translation,372,158,results,hard questions and pseudodecompositions,ignoring,noisy pairing,hard questions and pseudodecompositions ignoring noisy pairing,0.6204495429992676
translation,372,158,results,results,has,onus,results has onus,0.6266642808914185
translation,372,162,results,test f1,of,79.34,test f1 of 79.34,0.5329063534736633
translation,372,162,results,exact match ( em ),of,66.33,exact match ( em ) of 66.33,0.5496721863746643
translation,372,162,results,results,achieved,test f1,results achieved test f1,0.6978640556335449
translation,372,162,results,results,achieved,exact match ( em ),results achieved exact match ( em ),0.7275872826576233
translation,372,173,results,onus decompositions,improve,qa,onus decompositions improve qa,0.6677398085594177
translation,372,173,results,qa,across,all types,qa across all types,0.7433081269264221
translation,372,173,results,results,has,onus decompositions,results has onus decompositions,0.6003716588020325
translation,372,175,results,our qa approach,falling back to,single - hop qa model,our qa approach falling back to single - hop qa model,0.6219838857650757
translation,372,175,results,our qa approach,learns to leverage,decompositions,our qa approach learns to leverage decompositions,0.7501052618026733
translation,372,175,results,single - hop questions,has,our qa approach,single - hop questions has our qa approach,0.597070038318634
translation,372,175,results,results,For,single - hop questions,results For single - hop questions,0.58652263879776
translation,372,179,results,sub-answers,see,improved qa,sub-answers see improved qa,0.6294956803321838
translation,372,179,results,improved qa,with or without,sub-questions ( 80.1 and 80.2 f1,improved qa with or without sub-questions ( 80.1 and 80.2 f1,0.6901330351829529
translation,372,179,results,results,when,sub-answers,results when sub-answers,0.6398486495018005
translation,372,180,results,important,provide,sentence,important provide sentence,0.6178915500640869
translation,372,180,results,sentence,containing,predicted answer span,sentence containing predicted answer span,0.6421862244606018
translation,372,180,results,predicted answer span,instead of,answer span alone,predicted answer span instead of answer span alone,0.5687835812568665
translation,372,180,results,answer span alone,has,80.1 vs. 77.8 f1,answer span alone has 80.1 vs. 77.8 f1,0.5423184037208557
translation,372,180,results,results,provide,sentence,results provide sentence,0.5432335734367371
translation,372,205,results,qa accuracy,use,lower probability decompositions,qa accuracy use lower probability decompositions,0.6396319270133972
translation,372,205,results,decreases,use,lower probability decompositions,decreases use lower probability decompositions,0.7055626511573792
translation,372,205,results,accuracy,remains,relatively robust,accuracy remains relatively robust,0.6326666474342346
translation,372,205,results,qa accuracy,has,decreases,qa accuracy has decreases,0.6193403601646423
translation,372,205,results,results,has,qa accuracy,results has qa accuracy,0.5655482411384583
translation,373,39,ablation-analysis,out - of- domain data,improve,selective prediction,out - of- domain data improve selective prediction,0.7026523947715759
translation,373,39,ablation-analysis,selective prediction,under,domain shift,selective prediction under domain shift,0.6655453443527222
translation,373,39,ablation-analysis,domain shift,when used to train,calibrator,domain shift when used to train calibrator,0.656090497970581
translation,373,39,ablation-analysis,ablation analysis,show that,out - of- domain data,ablation analysis show that out - of- domain data,0.5224838256835938
translation,373,23,experiments,qa model,on,data,qa model on data,0.5934243202209473
translation,373,23,experiments,data,from,source distribution,data from source distribution,0.5447176694869995
translation,373,23,experiments,data,from,source distribution,data from source distribution,0.5447176694869995
translation,373,23,experiments,samples,from,source distribution,samples from source distribution,0.5540568232536316
translation,373,23,experiments,samples,from,unknown ood distribution,samples from unknown ood distribution,0.6131312251091003
translation,373,6,model,mixture,of,in- domain and out - of- domain data,mixture of in- domain and out - of- domain data,0.5932880640029907
translation,373,21,model,selective prediction,abstains on,inputs,selective prediction abstains on inputs,0.7232272624969482
translation,373,21,model,inputs,where,confidence,inputs where confidence,0.6506827473640442
translation,373,21,model,confidence,is,low,confidence is low,0.5956851243972778
translation,373,21,model,model,setting of,selective prediction,model setting of selective prediction,0.5991652011871338
translation,373,7,results,abstention policies,based solely on,model 's softmax probabilities,abstention policies based solely on model 's softmax probabilities,0.6180852651596069
translation,373,7,results,model 's softmax probabilities,fare,poorly,model 's softmax probabilities fare poorly,0.6876903772354126
translation,373,7,results,results,has,abstention policies,results has abstention policies,0.46662452816963196
translation,373,272,results,outperforms,when,d test,outperforms when d test,0.6885578036308289
translation,373,272,results,max - prob,when,d test,max - prob when d test,0.6851949095726013
translation,373,272,results,most,when,d test,most when d test,0.7351027727127075
translation,373,272,results,d test,mixture of,p source and q unk,d test mixture of p source and q unk,0.7239952087402344
translation,373,272,results,calibrator,has,outperforms,calibrator has outperforms,0.6306052803993225
translation,373,272,results,outperforms,has,max - prob,outperforms has max - prob,0.6504538655281067
translation,373,272,results,max - prob,has,most,max - prob has most,0.657433032989502
translation,373,272,results,results,has,calibrator,results has calibrator,0.5952214002609253
translation,373,278,results,calibrator,with access to,q known,calibrator with access to q known,0.6837125420570374
translation,373,278,results,q known,has,outperforms,q known has outperforms,0.6552132964134216
translation,373,278,results,outperforms,has,all other methods,outperforms has all other methods,0.5550965666770935
translation,373,278,results,results,has,calibrator,results has calibrator,0.5952214002609253
translation,374,5,experiments,three language modeling tasks,pre-train,transformers,three language modeling tasks pre-train transformers,0.5972342491149902
translation,374,5,experiments,three language modeling tasks,pre-train,token - and utterance - level language modeling,three language modeling tasks pre-train token - and utterance - level language modeling,0.6491159200668335
translation,374,5,experiments,three language modeling tasks,pre-train,utterance order prediction,three language modeling tasks pre-train utterance order prediction,0.6562805771827698
translation,374,5,experiments,utterance order prediction,that learn,token and utterance embeddings,utterance order prediction that learn token and utterance embeddings,0.5994850397109985
translation,374,5,experiments,token and utterance embeddings,for better understanding,dialogue contexts,token and utterance embeddings for better understanding dialogue contexts,0.654779314994812
translation,374,6,model,multitask learning,between,utterance prediction and the token span prediction,multitask learning between utterance prediction and the token span prediction,0.556099534034729
translation,374,6,model,utterance prediction and the token span prediction,applied to,finetune,utterance prediction and the token span prediction applied to finetune,0.7010753154754639
translation,374,6,model,finetune,for,span-based question answering ( qa ),finetune for span-based question answering ( qa ),0.6103769540786743
translation,374,6,model,model,has,multitask learning,model has multitask learning,0.5049652457237244
translation,374,14,model,fine- tuning,for,span- based qa,fine- tuning for span- based qa,0.5890229344367981
translation,374,14,model,every utterance,separated encoded,multi-head attentions,every utterance separated encoded multi-head attentions,0.8185956478118896
translation,374,14,model,additional transformers,built on,token and utterance embeddings,additional transformers built on token and utterance embeddings,0.7156162858009338
translation,374,14,model,fine- tuning,has,every utterance,fine- tuning has every utterance,0.6019938588142395
translation,374,14,model,span- based qa,has,every utterance,span- based qa has every utterance,0.5995338559150696
translation,374,76,results,* pre models,show,marginal improvement,* pre models show marginal improvement,0.6858673095703125
translation,374,76,results,marginal improvement,over,base models,marginal improvement over base models,0.7137187719345093
translation,374,76,results,results,has,* pre models,results has * pre models,0.5600688457489014
translation,374,77,results,models,using,our approach,models using our approach,0.6676508784294128
translation,374,77,results,models,perform,noticeably better,models perform noticeably better,0.5571387410163879
translation,374,77,results,noticeably better,than,baseline models,noticeably better than baseline models,0.5705884695053101
translation,374,77,results,noticeably better,showing,3.8 % and 1.4 % improvements,noticeably better showing 3.8 % and 1.4 % improvements,0.7169328331947327
translation,374,77,results,3.8 % and 1.4 % improvements,on,sm,3.8 % and 1.4 % improvements on sm,0.5375638008117676
translation,374,77,results,sm,from,bert and roberta,sm from bert and roberta,0.6359761953353882
translation,374,77,results,results,has,models,results has models,0.5335168838500977
translation,374,78,results,two dialogue-specific lm approaches,give,very marginal improvement,two dialogue-specific lm approaches give very marginal improvement,0.561398446559906
translation,374,78,results,very marginal improvement,over,baseline models,very marginal improvement over baseline models,0.668452262878418
translation,374,78,results,results,has,two dialogue-specific lm approaches,results has two dialogue-specific lm approaches,0.4709396958351135
translation,374,81,results,improvement,on,um,improvement on um,0.659523606300354
translation,374,81,results,um,giving,2 % and 1 % boosts,um giving 2 % and 1 % boosts,0.747188925743103
translation,374,81,results,2 % and 1 % boosts,to,bert pre and roberta pre,2 % and 1 % boosts to bert pre and roberta pre,0.641488790512085
translation,374,81,results,results,has,improvement,results has improvement,0.6248279809951782
translation,375,160,ablation-analysis,appropriateness interactions,between,c 1 and q 1,appropriateness interactions between c 1 and q 1,0.6919143199920654
translation,375,160,ablation-analysis,appropriateness interactions,between,c 2 and q 2,appropriateness interactions between c 2 and q 2,0.6786694526672363
translation,375,160,ablation-analysis,c 2 and q 2,improves,map,c 2 and q 2 improves map,0.704839289188385
translation,375,160,ablation-analysis,map,by,?9 points,map by ?9 points,0.6299126148223877
translation,375,160,ablation-analysis,ablation analysis,Adding,appropriateness interactions,ablation analysis Adding appropriateness interactions,0.7440246343612671
translation,375,163,ablation-analysis,relatedness interactions and features,between,q and q 1,relatedness interactions and features between q and q 1,0.6718065738677979
translation,375,163,ablation-analysis,relatedness interactions and features,between,q and q 2,relatedness interactions and features between q and q 2,0.6651138067245483
translation,375,163,ablation-analysis,relatedness interactions and features,to be,crucial,relatedness interactions and features to be crucial,0.5953680872917175
translation,375,163,ablation-analysis,ablation analysis,Adding,relatedness interactions and features,ablation analysis Adding relatedness interactions and features,0.675102174282074
translation,375,165,ablation-analysis,question -question similarity,plays,important role,question -question similarity plays important role,0.7232701778411865
translation,375,165,ablation-analysis,important role,in,solving,important role in solving,0.5279502272605896
translation,375,165,ablation-analysis,solving,has,cqa task,solving has cqa task,0.5496099591255188
translation,375,165,ablation-analysis,ablation analysis,shows,question -question similarity,ablation analysis shows question -question similarity,0.6468235850334167
translation,375,167,ablation-analysis,full network,Adding,appropriateness and relatedness interactions,full network Adding appropriateness and relatedness interactions,0.7261200547218323
translation,375,167,ablation-analysis,appropriateness and relatedness interactions,yields,improvement,appropriateness and relatedness interactions yields improvement,0.7385290265083313
translation,375,167,ablation-analysis,improvement,of,another two map points absolute,improvement of another two map points absolute,0.6183145642280579
translation,375,167,ablation-analysis,another two map points absolute,shows,appropriateness features,another two map points absolute shows appropriateness features,0.6906402707099915
translation,375,167,ablation-analysis,to 54.51 ),shows,appropriateness features,to 54.51 ) shows appropriateness features,0.6471127271652222
translation,375,167,ablation-analysis,appropriateness features,encode,information,appropriateness features encode information,0.7806259989738464
translation,375,167,ablation-analysis,information,modeled by,relevance and relatedness,information modeled by relevance and relatedness,0.6497860550880432
translation,375,167,ablation-analysis,another two map points absolute,has,to 54.51 ),another two map points absolute has to 54.51 ),0.602094292640686
translation,375,167,ablation-analysis,ablation analysis,has,full network,ablation analysis has full network,0.5316458940505981
translation,375,172,ablation-analysis,lexical similarity features,have,large impact,lexical similarity features have large impact,0.5228562355041504
translation,375,172,ablation-analysis,large impact,excluding them,network,large impact excluding them network,0.6940388083457947
translation,375,172,ablation-analysis,large impact,yields,decrease,large impact yields decrease,0.7690770030021667
translation,375,172,ablation-analysis,network,yields,decrease,network yields decrease,0.7482267618179321
translation,375,172,ablation-analysis,decrease,of,over eight map points,decrease of over eight map points,0.6469255685806274
translation,375,172,ablation-analysis,ablation analysis,see that,lexical similarity features,ablation analysis see that lexical similarity features,0.5772901773452759
translation,375,177,ablation-analysis,performance,by,six map points absolute,performance by six map points absolute,0.6253003478050232
translation,375,177,ablation-analysis,domain-specific features,has,hurts,domain-specific features has hurts,0.588776171207428
translation,375,177,ablation-analysis,hurts,has,performance,hurts has performance,0.6043882966041565
translation,375,177,ablation-analysis,performance,has,greatly,performance has greatly,0.6084486842155457
translation,375,177,ablation-analysis,ablation analysis,eliminating,domain-specific features,ablation analysis eliminating domain-specific features,0.7080803513526917
translation,375,178,ablation-analysis,eliminating the use of distributed representation,has,lesser impact,eliminating the use of distributed representation has lesser impact,0.5718743205070496
translation,375,178,ablation-analysis,lesser impact,has,3.3,lesser impact has 3.3,0.5915220975875854
translation,375,178,ablation-analysis,lesser impact,has,map points absolute,lesser impact has map points absolute,0.5863742828369141
translation,375,178,ablation-analysis,3.3,has,map points absolute,3.3 has map points absolute,0.5373924970626831
translation,375,178,ablation-analysis,ablation analysis,has,eliminating the use of distributed representation,ablation analysis has eliminating the use of distributed representation,0.5795750617980957
translation,375,101,experimental-setup,in- domain word embeddings,using,word2vec,in- domain word embeddings using word2vec,0.6028265357017517
translation,375,101,experimental-setup,word2vec,on,all available qatarliving data,word2vec on all available qatarliving data,0.5119014382362366
translation,375,101,experimental-setup,experimental setup,train,in- domain word embeddings,experimental setup train in- domain word embeddings,0.6303668022155762
translation,375,135,experimental-setup,theano,to train,our model,theano to train our model,0.7677107453346252
translation,375,135,experimental-setup,theano,using,stochastic gradient descent,theano using stochastic gradient descent,0.6375710964202881
translation,375,135,experimental-setup,our model,on,train - part1,our model on train - part1,0.5398736000061035
translation,375,135,experimental-setup,our model,with,minibatches,our model with minibatches,0.6520516872406006
translation,375,135,experimental-setup,our model,with,minibatches,our model with minibatches,0.6520516872406006
translation,375,135,experimental-setup,our model,with,learning rate,our model with learning rate,0.6211487054824829
translation,375,135,experimental-setup,our model,using,stochastic gradient descent,our model using stochastic gradient descent,0.6470151543617249
translation,375,135,experimental-setup,train - part1,with,hidden layers,train - part1 with hidden layers,0.6158812642097473
translation,375,135,experimental-setup,train - part1,with,minibatches,train - part1 with minibatches,0.6495296359062195
translation,375,135,experimental-setup,train - part1,with,minibatches,train - part1 with minibatches,0.6495296359062195
translation,375,135,experimental-setup,train - part1,with,regularization,train - part1 with regularization,0.6298948526382446
translation,375,135,experimental-setup,train - part1,with,learning rate,train - part1 with learning rate,0.6506302952766418
translation,375,135,experimental-setup,train - part1,using,stochastic gradient descent,train - part1 using stochastic gradient descent,0.6512229442596436
translation,375,135,experimental-setup,hidden layers,of size,3,hidden layers of size 3,0.7335939407348633
translation,375,135,experimental-setup,hidden layers,for,100 epochs,hidden layers for 100 epochs,0.5454974174499512
translation,375,135,experimental-setup,hidden layers,with,minibatches,hidden layers with minibatches,0.5902681946754456
translation,375,135,experimental-setup,3,for,100 epochs,3 for 100 epochs,0.615050733089447
translation,375,135,experimental-setup,minibatches,of size,30,minibatches of size 30,0.688652753829956
translation,375,135,experimental-setup,regularization,of,0.05,regularization of 0.05,0.6283608675003052
translation,375,135,experimental-setup,learning rate,of,0.01,learning rate of 0.01,0.6152973175048828
translation,375,135,experimental-setup,stochastic gradient descent,with,adagrad,stochastic gradient descent with adagrad,0.6186466217041016
translation,375,135,experimental-setup,experimental setup,use,theano,experimental setup use theano,0.6059675216674805
translation,375,136,experimental-setup,input feature values,to,[ ?1 ; 1 ] interval,input feature values to [ ?1 ; 1 ] interval,0.5030277371406555
translation,375,136,experimental-setup,input feature values,to,nn weights,input feature values to nn weights,0.5187686085700989
translation,375,136,experimental-setup,input feature values,to,uniform distribution,input feature values to uniform distribution,0.5345566868782043
translation,375,136,experimental-setup,input feature values,using,minmax,input feature values using minmax,0.6314250826835632
translation,375,136,experimental-setup,[ ?1 ; 1 ] interval,using,minmax,[ ?1 ; 1 ] interval using minmax,0.5504364967346191
translation,375,136,experimental-setup,experimental setup,normalize,input feature values,experimental setup normalize input feature values,0.6782572269439697
translation,375,6,model,task,as,answer ranking problem,task as answer ranking problem,0.502177357673645
translation,375,6,model,task,adopting,pairwise neural network architecture,task adopting pairwise neural network architecture,0.6944432854652405
translation,375,6,model,pairwise neural network architecture,selects,which of two competing answers is better,pairwise neural network architecture selects which of two competing answers is better,0.7170045375823975
translation,375,6,model,model,approach,task,model approach task,0.7379855513572693
translation,375,8,model,proposed neural network,models,interactions,proposed neural network models interactions,0.7513819932937622
translation,375,8,model,interactions,among,all input components,interactions among all input components,0.5861935615539551
translation,375,8,model,interactions,using,lexical matching,interactions using lexical matching,0.6644014716148376
translation,375,8,model,interactions,using,domain-specific features,interactions using domain-specific features,0.6258183717727661
translation,375,8,model,all input components,using,lexical matching,all input components using lexical matching,0.6671155691146851
translation,375,8,model,model,has,proposed neural network,model has proposed neural network,0.56941157579422
translation,375,22,model,pairwise deep neural network,to rank,comments,pairwise deep neural network to rank comments,0.7350980639457703
translation,375,22,model,comments,retrieved from,different question - comment threads,comments retrieved from different question - comment threads,0.5152371525764465
translation,375,22,model,different question - comment threads,according to,relevance,different question - comment threads according to relevance,0.6411550641059875
translation,375,22,model,relevance,as,answers,relevance as answers,0.5318832397460938
translation,375,22,model,answers,to,original question being asked,answers to original question being asked,0.5340781211853027
translation,375,22,model,model,use,pairwise deep neural network,model use pairwise deep neural network,0.6101728081703186
translation,375,23,model,contribution,of,edges,contribution of edges,0.5848862528800964
translation,375,23,model,edges,in,triangle,edges in triangle,0.5426846742630005
translation,375,23,model,triangle,formed by,pairwise interactions,triangle formed by pairwise interactions,0.6830369234085083
translation,375,23,model,pairwise interactions,between,original question,pairwise interactions between original question,0.6606238484382629
translation,375,23,model,pairwise interactions,between,related question,pairwise interactions between related question,0.6734893321990967
translation,375,23,model,pairwise interactions,between,related comments,pairwise interactions between related comments,0.6551438570022583
translation,375,23,model,pairwise interactions,to rank,comments,pairwise interactions to rank comments,0.7219868898391724
translation,375,23,model,related comments,to rank,comments,related comments to rank comments,0.6298797726631165
translation,375,23,model,comments,in,unified fashion,comments in unified fashion,0.5594537258148193
translation,375,23,model,model,investigate,contribution,model investigate contribution,0.5729866623878479
translation,375,47,model,model,solve,cqa task problem,model solve cqa task problem,0.6711724400520325
translation,375,99,model,vector representation,of,text,vector representation of text,0.5824713706970215
translation,375,99,model,vector representation,by simply,averaging,vector representation by simply averaging,0.642490804195404
translation,375,99,model,averaging,over,embeddings,averaging over embeddings,0.6698052287101746
translation,375,99,model,embeddings,has,of all words in the text,embeddings has of all words in the text,0.5716884732246399
translation,375,99,model,model,compute,vector representation,model compute vector representation,0.7280775904655457
translation,375,164,results,map score,jumps to,52.43,map score jumps to 52.43,0.6631728410720825
translation,375,164,results,52.43,significantly above,ir baseline,52.43 significantly above ir baseline,0.6781380772590637
translation,375,164,results,  relevance only   basic system,has,map score,  relevance only   basic system has map score,0.57403165102005
translation,375,166,results,answer ranking task,of,full nn,answer ranking task of full nn,0.5244950652122498
translation,375,166,results,results,on,answer ranking task,results on answer ranking task,0.5263282060623169
translation,375,182,results,distributed representations,are,0.7 map points,distributed representations are 0.7 map points,0.5635600090026855
translation,375,182,results,more informative,than,lexical similarities,more informative than lexical similarities,0.585921585559845
translation,375,182,results,relevance,has,distributed representations,relevance has distributed representations,0.5752503275871277
translation,375,182,results,0.7 map points,has,more informative,0.7 map points has more informative,0.5500717759132385
translation,375,182,results,results,for,relevance,results for relevance,0.5329238176345825
translation,375,197,results,pre-trained network,to incorporate,subtask a predictions,pre-trained network to incorporate subtask a predictions,0.6603904962539673
translation,375,197,results,pre-trained network,yields,another sizable improvement,pre-trained network yields another sizable improvement,0.6725040674209595
translation,375,197,results,subtask a predictions,as,features,subtask a predictions as features,0.5398672819137573
translation,375,197,results,another sizable improvement,to,final map,another sizable improvement to final map,0.5661520957946777
translation,375,197,results,final map,of,55.82,final map of 55.82,0.5650032162666321
translation,375,197,results,results,using,pre-trained network,results using pre-trained network,0.6593978404998779
translation,375,198,results,results,compare them to,state of the art,results compare them to state of the art,0.5832971334457397
translation,375,201,results,all systems,in,competition,all systems in competition,0.614362895488739
translation,375,201,results,competition,performed over,ir baseline,competition performed over ir baseline,0.6995611786842346
translation,375,201,results,ir baseline,with,map scores,ir baseline with map scores,0.6294592022895813
translation,375,201,results,map scores,ranging from,43.20 to 55.41,map scores ranging from 43.20 to 55.41,0.5846895575523376
translation,375,201,results,results,see that,all systems,results see that all systems,0.6336653232574463
translation,375,202,results,our full network,with,subtask a predictions,our full network with subtask a predictions,0.6928580403327942
translation,375,202,results,our full network,achieves,best results,our full network achieves best results,0.6745103001594543
translation,375,202,results,best results,with,55.82 map,best results with 55.82 map,0.6026984453201294
translation,375,202,results,results,see that,our full network,results see that our full network,0.6574010848999023
translation,375,204,results,our pairwise neural network,produces,results,our pairwise neural network produces results,0.6209284067153931
translation,375,204,results,on par,with,state of the art,on par with state of the art,0.6623167991638184
translation,375,204,results,subtask a predictions,has,our pairwise neural network,subtask a predictions has our pairwise neural network,0.6132710576057434
translation,375,204,results,results,even without,subtask a predictions,results even without subtask a predictions,0.6727768182754517
translation,376,5,experiments,our model,for,singlerelation question answering,our model for singlerelation question answering,0.5867913961410522
translation,376,137,hyperparameters,hyperparameters,used,2 layered - lstms and cnns,hyperparameters used 2 layered - lstms and cnns,0.6001462936401367
translation,376,138,hyperparameters,hyperparameters,trained for,14 epochs and 3 days,hyperparameters trained for 14 epochs and 3 days,0.6927260756492615
translation,376,16,model,character - level encoder - decoder framework,providing,much more compact model,character - level encoder - decoder framework providing much more compact model,0.6171145439147949
translation,376,16,model,performance,over,state - of - the - art word - level neural models,performance over state - of - the - art word - level neural models,0.6207836866378784
translation,376,16,model,significantly improves,has,performance,significantly improves has performance,0.5962982177734375
translation,376,16,model,model,with,character - level encoder - decoder framework,model with character - level encoder - decoder framework,0.6044277548789978
translation,376,17,model,encoder,to embed,question,encoder to embed question,0.7194393277168274
translation,376,17,model,long short -term memory ( lstm ),has,encoder,long short -term memory ( lstm ) has encoder,0.5357109904289246
translation,376,17,model,model,use,long short -term memory ( lstm ),model use long short -term memory ( lstm ),0.6554040908813477
translation,376,20,model,millions of entities and thousands of predicates,in,kb,millions of entities and thousands of predicates in kb,0.5383734703063965
translation,376,20,model,large output layer,in,decoder,large output layer in decoder,0.5648492574691772
translation,376,20,model,general interaction function,between,question embeddings and kb embeddings,general interaction function between question embeddings and kb embeddings,0.6695347428321838
translation,376,20,model,general interaction function,that measures,semantic relevance,general interaction function that measures semantic relevance,0.6677094101905823
translation,376,20,model,question embeddings and kb embeddings,that measures,semantic relevance,question embeddings and kb embeddings that measures semantic relevance,0.6214370727539062
translation,376,20,model,semantic relevance,to determine,output,semantic relevance to determine output,0.6126890778541565
translation,376,20,model,model,use,general interaction function,model use general interaction function,0.6840817928314209
translation,376,21,model,semantic relevance function,successfully produce,likelihood scores,semantic relevance function successfully produce likelihood scores,0.6586398482322693
translation,376,21,model,likelihood scores,for,kb entries,likelihood scores for kb entries,0.6206211447715759
translation,376,21,model,kb entries,that are not present in,our vocabulary,kb entries that are not present in our vocabulary,0.6452124118804932
translation,376,21,model,model,combined use of,character - level modeling,model combined use of character - level modeling,0.6133903861045837
translation,376,22,model,"our novel , character - level encoder- decoder model",is,compact,"our novel , character - level encoder- decoder model is compact",0.5663183927536011
translation,376,22,model,"our novel , character - level encoder- decoder model",requires,significantly less data,"our novel , character - level encoder- decoder model requires significantly less data",0.6238529086112976
translation,376,22,model,"our novel , character - level encoder- decoder model",able to,generalize,"our novel , character - level encoder- decoder model able to generalize",0.6537580490112305
translation,376,22,model,compact,requires,significantly less data,compact requires significantly less data,0.7234570384025574
translation,376,22,model,significantly less data,to,train,significantly less data to train,0.6186162829399109
translation,376,22,model,train,than,previous work,train than previous work,0.550369381904602
translation,376,22,model,well,to,unseen entities,well to unseen entities,0.5998398065567017
translation,376,22,model,unseen entities,in,test time,unseen entities in test time,0.5059632062911987
translation,376,22,model,generalize,has,well,generalize has well,0.6266511678695679
translation,376,22,model,model,has,"our novel , character - level encoder- decoder model","model has our novel , character - level encoder- decoder model",0.5185919404029846
translation,376,23,results,ensembles,achieve,70.9 % accuracy,ensembles achieve 70.9 % accuracy,0.6274958848953247
translation,376,23,results,ensembles,achieve,70.3 % accuracy,ensembles achieve 70.3 % accuracy,0.6287286281585693
translation,376,23,results,ensembles,achieve,outperforming,ensembles achieve outperforming,0.6282050609588623
translation,376,23,results,70.9 % accuracy,in,freebase2 m setting,70.9 % accuracy in freebase2 m setting,0.5484071373939514
translation,376,23,results,70.9 % accuracy,in,freebase5 m setting,70.9 % accuracy in freebase5 m setting,0.5533431768417358
translation,376,23,results,70.3 % accuracy,in,freebase5 m setting,70.3 % accuracy in freebase5 m setting,0.5500873327255249
translation,376,23,results,70.3 % accuracy,on,simplequestions dataset,70.3 % accuracy on simplequestions dataset,0.49992141127586365
translation,376,23,results,previous state - of- arts,of,62.7 % and 63.9 %,previous state - of- arts of 62.7 % and 63.9 %,0.5454691052436829
translation,376,23,results,outperforming,has,previous state - of- arts,outperforming has previous state - of- arts,0.551729142665863
translation,376,23,results,results,without use of,ensembles,results without use of ensembles,0.5220853686332703
translation,376,113,results,end-to-end results,on,simplequestions,end-to-end results on simplequestions,0.5483242273330688
translation,376,113,results,simplequestions dataset,in terms of,sq accuracy,simplequestions dataset in terms of sq accuracy,0.6312402486801147
translation,376,113,results,simplequestions dataset,for,fb2m and fb5m settings,simplequestions dataset for fb2m and fb5m settings,0.5915641188621521
translation,376,113,results,results,on,simplequestions,results on simplequestions,0.5083195567131042
translation,376,113,results,results,on,simplequestions dataset,results on simplequestions dataset,0.5304885506629944
translation,376,113,results,results,has,end-to-end results,results has end-to-end results,0.5268822312355042
translation,376,115,results,our single character - level model,achieves,sq accuracies,our single character - level model achieves sq accuracies,0.7056713104248047
translation,376,115,results,sq accuracies,of,70.9 % and 70.3 %,sq accuracies of 70.9 % and 70.3 %,0.5826359391212463
translation,376,115,results,70.9 % and 70.3 %,on,fb2m and fb5m settings,70.9 % and 70.3 % on fb2m and fb5m settings,0.563413143157959
translation,376,115,results,previous state - of - art results,by,8.2 % and 6.4 %,previous state - of - art results by 8.2 % and 6.4 %,0.5497414469718933
translation,376,115,results,outperforming,has,previous state - of - art results,outperforming has previous state - of - art results,0.5482785105705261
translation,376,115,results,results,has,our single character - level model,results has our single character - level model,0.5226197242736816
translation,376,116,results,our word- level model,has,19.9 m parameters,our word- level model has 19.9 m parameters,0.5264729261398315
translation,376,116,results,our word- level model,achieves,best sq accuracy,our word- level model achieves best sq accuracy,0.6639803647994995
translation,376,116,results,best sq accuracy,of,53.9 %,best sq accuracy of 53.9 %,0.5403624773025513
translation,376,116,results,character - level model,has,our word- level model,character - level model has our word- level model,0.5661300420761108
translation,376,116,results,our word- level model,has,19.9 m parameters,our word- level model has 19.9 m parameters,0.5264729261398315
translation,376,116,results,results,Compared to,character - level model,results Compared to character - level model,0.6383830308914185
translation,376,122,results,results,has,character - level vs. word -level models,results has character - level vs. word -level models,0.48082035779953003
translation,376,124,results,word-level and character - level models,perform,comparably well,word-level and character - level models perform comparably well,0.5752652883529663
translation,376,124,results,comparably well,when predicting,predicate,comparably well when predicting predicate,0.7245349884033203
translation,376,124,results,comparably well,reaching,accuracy,comparably well reaching accuracy,0.7601996660232544
translation,376,124,results,accuracy,of,around 80 %,accuracy of around 80 %,0.5794417262077332
translation,376,124,results,results,has,word-level and character - level models,results has word-level and character - level models,0.46517226099967957
translation,376,127,results,character - level models,achieve,96.6 % accuracy,character - level models achieve 96.6 % accuracy,0.5918493866920471
translation,376,127,results,96.6 % accuracy,in predicting,correct entity,96.6 % accuracy in predicting correct entity,0.6866591572761536
translation,376,127,results,correct entity,on,mixed set,correct entity on mixed set,0.5487900972366333
translation,376,127,results,results,has,character - level models,results has character - level models,0.46649184823036194
translation,376,133,results,two -layer lstm,boosts,joint accuracy,two -layer lstm boosts joint accuracy,0.6761536002159119
translation,376,133,results,joint accuracy,by,over 6 %,joint accuracy by over 6 %,0.5861142873764038
translation,376,133,results,results,find that,two -layer lstm,results find that two -layer lstm,0.6417154669761658
translation,376,134,results,accuracy gains,result of,improved predicate predictions,accuracy gains result of improved predicate predictions,0.7078298926353455
translation,377,61,ablation-analysis,placeholding,allows for,significant reduction,placeholding allows for significant reduction,0.7376775741577148
translation,377,61,ablation-analysis,significant reduction,of,vocabulary size ( ? 30 % ),significant reduction of vocabulary size ( ? 30 % ),0.552937924861908
translation,377,61,ablation-analysis,ablation analysis,has,placeholding,ablation analysis has placeholding,0.5454837679862976
translation,377,29,model,base architecture,with,copying mechanism,base architecture with copying mechanism,0.6163608431816101
translation,377,29,model,base architecture,with,placeholders,base architecture with placeholders,0.6422033309936523
translation,377,29,model,base architecture,with,contextual word embeddings,base architecture with contextual word embeddings,0.5967833399772644
translation,377,74,results,contextualized embeddings,allow to,further improve,contextualized embeddings allow to further improve,0.6439757347106934
translation,377,74,results,contextualized embeddings,obtaining,bleu4 score,contextualized embeddings obtaining bleu4 score,0.5535688400268555
translation,377,74,results,bleu4 score,of,13.23,bleu4 score of 13.23,0.5276908874511719
translation,377,74,results,almost one absolute point,above,current state - of - the - art,almost one absolute point above current state - of - the - art,0.7299370169639587
translation,377,74,results,further improve,has,performances,further improve has performances,0.5848833322525024
translation,377,85,results,best performance,when using,three mechanisms,best performance when using three mechanisms,0.6985181570053101
translation,377,85,results,best performance,reaching,improvement,best performance reaching improvement,0.7256023287773132
translation,377,85,results,three mechanisms,reaching,improvement,three mechanisms reaching improvement,0.8073986768722534
translation,377,85,results,improvement,of,almost one bleu4 point,improvement of almost one bleu4 point,0.5362083315849304
translation,377,85,results,almost one bleu4 point,over,current state - of - the - art approaches,almost one bleu4 point over current state - of - the - art approaches,0.6690502166748047
translation,377,85,results,0.5,for,rouge - l,0.5 for rouge - l,0.6584183573722839
translation,377,85,results,results,has,best performance,results has best performance,0.5759831070899963
translation,378,104,baselines,cnn,utilize,cnn neural encoder,cnn utilize cnn neural encoder,0.5547885298728943
translation,378,104,baselines,cnn neural encoder,for,answerability prediction,cnn neural encoder for answerability prediction,0.5647495985031128
translation,378,104,baselines,baselines,has,cnn,baselines has cnn,0.5907226204872131
translation,378,106,baselines,bert,is,bidirectional transformer - based language -model,bert is bidirectional transformer - based language -model,0.5449507236480713
translation,378,106,baselines,baselines,has,bert,baselines has bert,0.5950736403465271
translation,378,117,baselines,most of the questions,receive,difficult to answer,most of the questions receive difficult to answer,0.6156488656997681
translation,378,117,baselines,difficult to answer,in,legally - sound way,difficult to answer in legally - sound way,0.4886120557785034
translation,378,117,baselines,legally - sound way,on the basis of,information,legally - sound way on the basis of information,0.6520302295684814
translation,378,117,baselines,information,present in,privacy policy,information present in privacy policy,0.55748051404953
translation,378,117,baselines,no - answer baseline ( na ),has,most of the questions,no - answer baseline ( na ) has most of the questions,0.5577980279922485
translation,378,117,baselines,baselines,has,no - answer baseline ( na ),baselines has no - answer baseline ( na ),0.5563880205154419
translation,378,122,baselines,bert,implement,two bert - based baselines,bert implement two bert - based baselines,0.6864399909973145
translation,378,122,baselines,two bert - based baselines,for,evidence identification,two bert - based baselines for evidence identification,0.5996506214141846
translation,378,122,baselines,baselines,has,bert,baselines has bert,0.5950736403465271
translation,378,6,experiments,priva,-,cyqa,priva - cyqa,0.7276619672775269
translation,378,6,experiments,corpus,consisting of,1750 questions,corpus consisting of 1750 questions,0.6999667882919312
translation,378,6,experiments,corpus,consisting of,over 3500 expert annotations,corpus consisting of over 3500 expert annotations,0.7525627017021179
translation,378,6,experiments,1750 questions,about,privacy policies,1750 questions about privacy policies,0.6498509645462036
translation,378,6,experiments,over 3500 expert annotations,of,relevant answers,over 3500 expert annotations of relevant answers,0.5068632960319519
translation,378,6,experiments,priva,has,cyqa,priva has cyqa,0.6958143711090088
translation,378,6,experiments,priva,has,corpus,priva has corpus,0.655809223651886
translation,378,6,experiments,cyqa,has,corpus,cyqa has corpus,0.5821053981781006
translation,378,22,experiments,corpus,consisting of,1750 questions,corpus consisting of 1750 questions,0.6999667882919312
translation,378,22,experiments,1750 questions,contents of,privacy policies,1750 questions contents of privacy policies,0.6658678650856018
translation,378,22,experiments,pri - vacyqa,has,corpus,pri - vacyqa has corpus,0.569717526435852
translation,378,98,hyperparameters,svm,define,3 sets of features,svm define 3 sets of features,0.6289724707603455
translation,378,98,hyperparameters,3 sets of features,to characterize,each question,3 sets of features to characterize each question,0.683391273021698
translation,378,98,hyperparameters,hyperparameters,has,svm,hyperparameters has svm,0.5564075112342834
translation,378,105,hyperparameters,filter size,of,5,filter size of 5,0.687269389629364
translation,378,105,hyperparameters,5,with,64 filters,5 with 64 filters,0.6718122959136963
translation,378,105,hyperparameters,64 filters,to encode,questions,64 filters to encode questions,0.7083501219749451
translation,378,105,hyperparameters,hyperparameters,use,glove word embeddings,hyperparameters use glove word embeddings,0.5590203404426575
translation,378,105,hyperparameters,hyperparameters,use,filter size,hyperparameters use filter size,0.6474370360374451
translation,378,107,hyperparameters,bert - base,on,binary answerability identification task,bert - base on binary answerability identification task,0.5458493828773499
translation,378,107,hyperparameters,binary answerability identification task,with,learning rate,binary answerability identification task with learning rate,0.5787811875343323
translation,378,107,hyperparameters,binary answerability identification task,with,maximum sequence length,binary answerability identification task with maximum sequence length,0.6204950213432312
translation,378,107,hyperparameters,learning rate,of,2e - 5,learning rate of 2e - 5,0.6410878896713257
translation,378,107,hyperparameters,2e - 5,for,3 epochs,2e - 5 for 3 epochs,0.616808295249939
translation,378,107,hyperparameters,2e - 5,with,maximum sequence length,2e - 5 with maximum sequence length,0.6504948139190674
translation,378,107,hyperparameters,maximum sequence length,of,128,maximum sequence length of 128,0.6129645705223083
translation,378,107,hyperparameters,hyperparameters,fine- tune,bert - base,hyperparameters fine- tune bert - base,0.7275826930999756
translation,378,123,model,bert,on,each query - policy sentence pair,bert on each query - policy sentence pair,0.5913293361663818
translation,378,123,model,each query - policy sentence pair,as,binary classification task,each query - policy sentence pair as binary classification task,0.5000790357589722
translation,378,123,model,binary classification task,to identify,sentence,binary classification task to identify sentence,0.6226516366004944
translation,378,123,model,sentence,is evidence for,question or not,sentence is evidence for question or not,0.7048833966255188
translation,378,123,model,model,train,bert,model train bert,0.7097170352935791
translation,378,7,results,human performance,by,almost 0.3 f1,human performance by almost 0.3 f1,0.5661925673484802
translation,378,7,results,almost 0.3 f1,on,privacyqa,almost 0.3 f1 on privacyqa,0.5652020573616028
translation,378,7,results,strong neural baseline,has,underperforms,strong neural baseline has underperforms,0.5472571849822998
translation,378,7,results,underperforms,has,human performance,underperforms has human performance,0.5747115612030029
translation,378,7,results,results,observe,strong neural baseline,results observe strong neural baseline,0.639855146408081
translation,378,132,results,bert,exhibits,best performance,bert exhibits best performance,0.7321372628211975
translation,378,132,results,best performance,on,binary answerability identification task,best performance on binary answerability identification task,0.5271672010421753
translation,378,132,results,results,observe,bert,results observe bert,0.4689315855503082
translation,378,133,results,performance,of,majority -class baseline,performance of majority -class baseline,0.5407179594039917
translation,378,133,results,most baselines,has,considerably exceed,most baselines has considerably exceed,0.5913338661193848
translation,378,133,results,considerably exceed,has,performance,considerably exceed has performance,0.5909502506256104
translation,378,133,results,results,has,most baselines,results has most baselines,0.5433915853500366
translation,378,137,results,no-answer ( na ) baseline,performs,28 f1,no-answer ( na ) baseline performs 28 f1,0.6129357218742371
translation,378,137,results,results,has,no-answer ( na ) baseline,results has no-answer ( na ) baseline,0.5321546196937561
translation,378,138,results,bert + unanswerable,achieves,f1,bert + unanswerable achieves f1,0.6805692315101624
translation,378,138,results,f1,of,39.8,f1 of 39.8,0.5866617560386658
translation,378,138,results,best-performing baseline,has,bert + unanswerable,best-performing baseline has bert + unanswerable,0.5865377187728882
translation,378,138,results,results,observe,best-performing baseline,results observe best-performing baseline,0.6081966757774353
translation,378,144,results,significant room,for,improvement,significant room for improvement,0.6541754603385925
translation,378,144,results,significant room,especially for,"first party , third party and data retention categories","significant room especially for first party , third party and data retention categories",0.6766802668571472
translation,378,144,results,improvement,across,all categories of questions,improvement across all categories of questions,0.6804260611534119
translation,378,144,results,results,observe,significant room,results observe significant room,0.5892863869667053
translation,379,6,model,question query,encodes,important information,question query encodes important information,0.740809977054596
translation,379,7,results,ace and the conll04 corpora,demonstrate,proposed paradigm,ace and the conll04 corpora demonstrate proposed paradigm,0.5872364044189453
translation,379,7,results,proposed paradigm,has,significantly outperforms,proposed paradigm has significantly outperforms,0.6194567084312439
translation,379,7,results,significantly outperforms,has,previous best models,significantly outperforms has previous best models,0.5908374786376953
translation,379,8,results,stateof - the - art results,on,"all of the ace04 , ace05 and conll04 datasets","stateof - the - art results on all of the ace04 , ace05 and conll04 datasets",0.4745531678199768
translation,379,8,results,stateof - the - art results,increasing,sota results,stateof - the - art results increasing sota results,0.6516090631484985
translation,379,8,results,sota results,on,three datasets,sota results on three datasets,0.514430046081543
translation,379,10,results,proposed multi-turn qa model,achieves,best performance,proposed multi-turn qa model achieves best performance,0.7222787141799927
translation,379,10,results,best performance,on,resume dataset,best performance on resume dataset,0.555876612663269
translation,379,10,results,results,has,proposed multi-turn qa model,results has proposed multi-turn qa model,0.592018187046051
translation,379,39,results,proposed paradigm,transforms,entity -relation extraction task,proposed paradigm transforms entity -relation extraction task,0.6748534440994263
translation,379,39,results,proposed paradigm,introduces,significant performance boost,proposed paradigm introduces significant performance boost,0.6912328004837036
translation,379,39,results,entity -relation extraction task,to,multi-turn qa task,entity -relation extraction task to multi-turn qa task,0.49819353222846985
translation,379,39,results,significant performance boost,over,existing systems,significant performance boost over existing systems,0.6621378660202026
translation,380,198,ablation-analysis,proposed mdn method,works,way better,proposed mdn method works way better,0.6329653263092041
translation,380,198,ablation-analysis,way better,than,other variants,way better than other variants,0.6138295531272888
translation,380,198,ablation-analysis,way better,in terms of,"bleu , meteor and rouge metrics","way better in terms of bleu , meteor and rouge metrics",0.6481165289878845
translation,380,198,ablation-analysis,way better,achieving,improvement,way better achieving improvement,0.6712266802787781
translation,380,198,ablation-analysis,improvement,of,"6 % , 12 % and 18 %","improvement of 6 % , 12 % and 18 %",0.6123258471488953
translation,380,198,ablation-analysis,"6 % , 12 % and 18 %",in,scores,"6 % , 12 % and 18 % in scores",0.5790851712226868
translation,380,198,ablation-analysis,"6 % , 12 % and 18 %",over,best other variant,"6 % , 12 % and 18 % over best other variant",0.6820570826530457
translation,380,150,baselines,place net,obtaining,embeddings,place net obtaining embeddings,0.6916543245315552
translation,380,150,baselines,embeddings,based on,visual scene understanding,embeddings based on visual scene understanding,0.667876660823822
translation,380,150,baselines,baselines,has,place net,baselines has place net,0.6212972402572632
translation,380,153,baselines,baselines,has,differential image network,baselines has differential image network,0.5651335120201111
translation,380,200,baselines,baselines,has,baseline and state- of- the - art,baselines has baseline and state- of- the - art,0.5714438557624817
translation,380,98,experiments,itml based metric learning,for,image features,itml based metric learning for image features,0.5817297101020813
translation,380,168,experiments,model,separately for,vqg - coco and vqa dataset,model separately for vqg - coco and vqa dataset,0.7331404685974121
translation,380,24,model,differential context,obtained through,supporting and contrasting exemplars,differential context obtained through supporting and contrasting exemplars,0.565138041973114
translation,380,24,model,supporting and contrasting exemplars,to obtain,differential embedding,supporting and contrasting exemplars to obtain differential embedding,0.5561783313751221
translation,380,24,model,model,uses,differential context,model uses differential context,0.6222226023674011
translation,380,33,model,multimodal differential network,to solve,task,multimodal differential network to solve task,0.6512594819068909
translation,380,33,model,task,of,visual question generation,task of visual question generation,0.5510983467102051
translation,380,33,model,model,propose,multimodal differential network,model propose multimodal differential network,0.6642084717750549
translation,380,99,results,knn - based approach,has,outperforms,knn - based approach has outperforms,0.621076226234436
translation,380,99,results,outperforms,has,latter one,outperforms has latter one,0.6410365700721741
translation,380,99,results,results,has,knn - based approach,results has knn - based approach,0.5681735277175903
translation,380,167,results,good results,on,vqa dataset,good results on vqa dataset,0.49004191160202026
translation,380,167,results,results,get,good results,results get good results,0.5651708841323853
translation,380,190,results,proposed mdn,provides,improved embeddings,proposed mdn provides improved embeddings,0.6165177822113037
translation,380,190,results,improved embeddings,to,decoder,improved embeddings to decoder,0.6087720990180969
translation,380,190,results,results,observe,proposed mdn,results observe proposed mdn,0.6224868893623352
translation,380,197,results,joint method ( jm ),of,combining the embeddings,joint method ( jm ) of combining the embeddings,0.5715506672859192
translation,380,197,results,combining the embeddings,works,best,combining the embeddings works best,0.650920033454895
translation,380,197,results,best,in,all cases,best in all cases,0.5550767183303833
translation,380,197,results,all cases,except,tag embeddings,all cases except tag embeddings,0.6855341792106628
translation,380,197,results,results,observe,joint method ( jm ),results observe joint method ( jm ),0.5932673215866089
translation,380,204,results,vqa dataset,achieve,improvement,vqa dataset achieve improvement,0.6335468888282776
translation,380,204,results,improvement,of,8 %,improvement of 8 %,0.6366114616394043
translation,380,204,results,improvement,of,7 %,improvement of 7 %,0.6378616094589233
translation,380,204,results,8 %,in,bleu,8 % in bleu,0.5529053807258606
translation,380,204,results,8 %,in,meteor metric scores,8 % in meteor metric scores,0.5402257442474365
translation,380,204,results,7 %,in,meteor metric scores,7 % in meteor metric scores,0.5412898659706116
translation,380,204,results,7 %,over,baselines,7 % over baselines,0.7131712436676025
translation,380,204,results,vqg - coco dataset,is,15 %,vqg - coco dataset is 15 %,0.5677544474601746
translation,380,204,results,results,for,vqa dataset,results for vqa dataset,0.5706924796104431
translation,380,204,results,results,for,vqg - coco dataset,results for vqg - coco dataset,0.5941422581672668
translation,380,205,results,previous state - of- the - art,for,vqa dataset,previous state - of- the - art for vqa dataset,0.5611132383346558
translation,380,205,results,vqa dataset,by around,10 %,vqa dataset by around 10 %,0.7138007879257202
translation,380,205,results,6 %,in,bleu score,6 % in bleu score,0.527431845664978
translation,380,205,results,10 %,in,meteor score,10 % in meteor score,0.5463504195213318
translation,380,206,results,vqg - coco dataset,improve over,"mostafazadeh et al. , 2016 )","vqg - coco dataset improve over mostafazadeh et al. , 2016 )",0.6676886081695557
translation,380,206,results,vqg - coco dataset,improve over,"jain et al. , 2017 )","vqg - coco dataset improve over jain et al. , 2017 )",0.6630030274391174
translation,380,206,results,"mostafazadeh et al. , 2016 )",by,3.7 %,"mostafazadeh et al. , 2016 ) by 3.7 %",0.5204455852508545
translation,380,206,results,"mostafazadeh et al. , 2016 )",by,3.5 %,"mostafazadeh et al. , 2016 ) by 3.5 %",0.5295236706733704
translation,380,206,results,"mostafazadeh et al. , 2016 )",by,3.5 %,"mostafazadeh et al. , 2016 ) by 3.5 %",0.5295236706733704
translation,380,206,results,"jain et al. , 2017 )",by,3.5 %,"jain et al. , 2017 ) by 3.5 %",0.5496082901954651
translation,380,206,results,3.5 %,in terms of,meteor scores,3.5 % in terms of meteor scores,0.7203117609024048
translation,380,206,results,results,In,vqg - coco dataset,results In vqg - coco dataset,0.531051754951477
translation,381,138,baselines,ir,is,information retrieval baseline,ir is information retrieval baseline,0.5388609170913696
translation,381,138,baselines,baselines,has,ir,baselines has ir,0.6097233891487122
translation,381,145,baselines,encoder - decoder model,with,single placeholder,encoder - decoder model with single placeholder,0.6540478467941284
translation,381,146,experimental-setup,encoder,with,transe embeddings,encoder with transe embeddings,0.6840127110481262
translation,381,146,experimental-setup,encoder,with,glove word embeddings,encoder with glove word embeddings,0.6472824811935425
translation,381,146,experimental-setup,decoder,with,glove word embeddings,decoder with glove word embeddings,0.6290233731269836
translation,381,146,experimental-setup,experimental setup,initialize,encoder,experimental setup initialize encoder,0.6722666025161743
translation,381,146,experimental-setup,experimental setup,initialize,decoder,experimental setup initialize decoder,0.686920702457428
translation,381,152,experimental-setup,rmsprop optimization algorithm,with,decreasing learning rate,rmsprop optimization algorithm with decreasing learning rate,0.59553062915802
translation,381,152,experimental-setup,rmsprop optimization algorithm,with,clipping gradients,rmsprop optimization algorithm with clipping gradients,0.5717312097549438
translation,381,152,experimental-setup,decreasing learning rate,of,0.001,decreasing learning rate of 0.001,0.5884644985198975
translation,381,152,experimental-setup,batch size,=,200,batch size = 200,0.6971617341041565
translation,381,152,experimental-setup,clipping gradients,with,norms,clipping gradients with norms,0.604469895362854
translation,381,152,experimental-setup,norms,larger than,0.1,norms larger than 0.1,0.6420297622680664
translation,381,152,experimental-setup,experimental setup,use,rmsprop optimization algorithm,experimental setup use rmsprop optimization algorithm,0.6225349307060242
translation,381,155,experimental-setup,word embeddings,chose,"glove ( pennington et al. , 2014 ) pretrained embeddings","word embeddings chose glove ( pennington et al. , 2014 ) pretrained embeddings",0.48227494955062866
translation,381,155,experimental-setup,"glove ( pennington et al. , 2014 ) pretrained embeddings",of size,100,"glove ( pennington et al. , 2014 ) pretrained embeddings of size 100",0.6033888459205627
translation,381,155,experimental-setup,experimental setup,For,word embeddings,experimental setup For word embeddings,0.5087873935699463
translation,381,156,experimental-setup,transe embeddings,on,fb5m dataset,transe embeddings on fb5m dataset,0.5414552092552185
translation,381,156,experimental-setup,transe embeddings,using,transe model implementation,transe embeddings using transe model implementation,0.6844173073768616
translation,381,156,experimental-setup,transe embeddings,has,of size h k = 200,transe embeddings has of size h k = 200,0.5907096862792969
translation,381,156,experimental-setup,experimental setup,train,transe embeddings,experimental setup train transe embeddings,0.7135719656944275
translation,381,157,experimental-setup,gru hidden size,of,decoder,gru hidden size of decoder,0.6137145757675171
translation,381,157,experimental-setup,gru hidden size,of,textual encoder,gru hidden size of textual encoder,0.5834223031997681
translation,381,157,experimental-setup,decoder,to,h d = 500,decoder to h d = 500,0.5950915217399597
translation,381,157,experimental-setup,decoder,to,h c = 200,decoder to h c = 200,0.6436094045639038
translation,381,157,experimental-setup,experimental setup,set,gru hidden size,experimental setup set gru hidden size,0.654859185218811
translation,381,157,experimental-setup,experimental setup,set,textual encoder,experimental setup set textual encoder,0.6342364549636841
translation,381,158,experimental-setup,networks hyperparameters,set with respect to,final bleu - 4 score,networks hyperparameters set with respect to final bleu - 4 score,0.5828161835670471
translation,381,158,experimental-setup,final bleu - 4 score,over,validation set,final bleu - 4 score over validation set,0.6095808744430542
translation,381,158,experimental-setup,experimental setup,has,networks hyperparameters,experimental setup has networks hyperparameters,0.488271564245224
translation,381,159,experimental-setup,neural networks,implemented using,tensorflow,neural networks implemented using tensorflow,0.60506671667099
translation,381,159,experimental-setup,experimental setup,has,neural networks,experimental setup has neural networks,0.5093538165092468
translation,381,4,model,question generation,from,knowledge base triples,question generation from knowledge base triples,0.5278779864311218
translation,381,4,model,triples,containing,"predicates , subject types or object types","triples containing predicates , subject types or object types",0.6475515961647034
translation,381,5,model,triples occurrences,in,natural language corpus,triples occurrences in natural language corpus,0.4721389710903168
translation,381,5,model,natural language corpus,in,encoderdecoder architecture,natural language corpus in encoderdecoder architecture,0.4629238545894623
translation,381,5,model,original part- of-speech copy action mechanism,to generate,questions,original part- of-speech copy action mechanism to generate questions,0.6822194457054138
translation,381,5,model,model,leverages,triples occurrences,model leverages triples occurrences,0.741065502166748
translation,381,20,model,model,for,zero - shot question generation,model for zero - shot question generation,0.6121417284011841
translation,381,20,model,model,propose,model,model propose model,0.6740307211875916
translation,381,20,model,model,for,zero - shot question generation,model for zero - shot question generation,0.6121417284011841
translation,381,21,model,our model,with,set of textual contexts,our model with set of textual contexts,0.5944281816482544
translation,381,21,model,set of textual contexts,paired with,input kb triple,set of textual contexts paired with input kb triple,0.6077197194099426
translation,381,21,model,input kb triple,through,distant supervision,input kb triple through distant supervision,0.6476734280586243
translation,381,21,model,model,feed,our model,model feed our model,0.6391447186470032
translation,381,22,model,encoder-decoder architecture,in which,encoder,encoder-decoder architecture in which encoder,0.5916809439659119
translation,381,22,model,encoder,encodes,input kb triple,encoder encodes input kb triple,0.7663840055465698
translation,381,22,model,input kb triple,along with,set of textual contexts,input kb triple along with set of textual contexts,0.5598297715187073
translation,381,22,model,set of textual contexts,into,hidden representations,set of textual contexts into hidden representations,0.5411614775657654
translation,381,22,model,model,derives,encoder-decoder architecture,model derives encoder-decoder architecture,0.6014379262924194
translation,381,23,model,hidden representations,fed to,decoder,hidden representations fed to decoder,0.7502097487449646
translation,381,23,model,decoder,equipped with,attention mechanism,decoder equipped with attention mechanism,0.687572717666626
translation,381,23,model,attention mechanism,to generate,output question,attention mechanism to generate output question,0.6826354265213013
translation,381,23,model,model,has,hidden representations,model has hidden representations,0.5676955580711365
translation,381,29,results,our model,using,contexts,our model using contexts,0.7139838933944702
translation,381,29,results,contexts,through,distant supervision,contexts through distant supervision,0.6835343837738037
translation,381,29,results,strongest baseline,among,six ( + 2.04 bleu - 4 score ),strongest baseline among six ( + 2.04 bleu - 4 score ),0.5600372552871704
translation,381,29,results,distant supervision,has,significantly outperforms,distant supervision has significantly outperforms,0.15104259550571442
translation,381,29,results,significantly outperforms,has,strongest baseline,significantly outperforms has strongest baseline,0.5958999395370483
translation,381,30,results,our copy action mechanism,increases,improvement,our copy action mechanism increases improvement,0.7316245436668396
translation,381,30,results,improvement,has,+ 2.39 ),improvement has + 2.39 ),0.584432065486908
translation,381,30,results,results,Adding,our copy action mechanism,results Adding our copy action mechanism,0.6617388725280762
translation,381,176,results,our,encodes,kb fact and textual contexts,our encodes kb fact and textual contexts,0.6978930234909058
translation,381,176,results,our,achieves,significant enhancement,our achieves significant enhancement,0.7078843712806702
translation,381,176,results,kb fact and textual contexts,achieves,significant enhancement,kb fact and textual contexts achieves significant enhancement,0.654941976070404
translation,381,176,results,significant enhancement,over,all the baselines,significant enhancement over all the baselines,0.700562596321106
translation,381,176,results,significant enhancement,with,+ 2.04 bleu - 4 score,significant enhancement with + 2.04 bleu - 4 score,0.618184506893158
translation,381,176,results,all the baselines,in,all evaluation metrics,all the baselines in all evaluation metrics,0.477157860994339
translation,381,176,results,+ 2.04 bleu - 4 score,than,encoder - decoder baseline,+ 2.04 bleu - 4 score than encoder - decoder baseline,0.5026044249534607
translation,381,176,results,results,encodes,kb fact and textual contexts,results encodes kb fact and textual contexts,0.6164101958274841
translation,381,176,results,results,has,our,results has our,0.6219730973243713
translation,381,177,results,partof-speech copy actions,improves,enhancement,partof-speech copy actions improves enhancement,0.6720758676528931
translation,381,177,results,enhancement,to reach,+ 2.39 bleu - 4 points,enhancement to reach + 2.39 bleu - 4 points,0.6044536232948303
translation,381,177,results,results,Incorporating,partof-speech copy actions,results Incorporating partof-speech copy actions,0.6414451003074646
translation,381,178,results,encoder- decoder baseline and the r-transe baseline,performed,best,encoder- decoder baseline and the r-transe baseline performed best,0.2814995348453522
translation,381,178,results,all baselines,has,encoder- decoder baseline and the r-transe baseline,all baselines has encoder- decoder baseline and the r-transe baseline,0.5427337288856506
translation,381,178,results,results,Among,all baselines,results Among all baselines,0.5574625730514526
translation,381,184,results,different variations,of,our system,different variations of our system,0.571852445602417
translation,381,184,results,different variations,express,unseen predicate,different variations express unseen predicate,0.7452595233917236
translation,381,184,results,unseen predicate,in,target question,unseen predicate in target question,0.4989786148071289
translation,381,184,results,unseen predicate,with comparison to,encoder - decoder baseline,unseen predicate with comparison to encoder - decoder baseline,0.6604387164115906
translation,381,184,results,target question,with comparison to,encoder - decoder baseline,target question with comparison to encoder - decoder baseline,0.6543134450912476
translation,381,185,results,proposed copy actions,scored,significant enhancement,proposed copy actions scored significant enhancement,0.7422120571136475
translation,381,185,results,significant enhancement,in,identification of unseen predicates,significant enhancement in identification of unseen predicates,0.5488619208335876
translation,381,185,results,up to + 40 % more,than,best performing baseline,up to + 40 % more than best performing baseline,0.5794895887374878
translation,381,185,results,our model version,without,copy actions,our model version without copy actions,0.7090175151824951
translation,381,185,results,results,has,proposed copy actions,results has proposed copy actions,0.6063812375068665
translation,381,189,results,predicate context,to,our model,predicate context to our model,0.5133212208747864
translation,381,189,results,predicate context,enhanced,model performance,predicate context enhanced model performance,0.7143543362617493
translation,381,189,results,model performance,for expressing,unseen predicates,model performance for expressing unseen predicates,0.666083812713623
translation,381,189,results,unseen predicates,by,+ 9 %,unseen predicates by + 9 %,0.5951625108718872
translation,381,189,results,results,Adding,predicate context,results Adding predicate context,0.6798031330108643
translation,382,164,ablation-analysis,task -specific embeddings and character embeddings,contribute to,model performance,task -specific embeddings and character embeddings contribute to model performance,0.5958400368690491
translation,382,164,ablation-analysis,ablation analysis,see that,task -specific embeddings and character embeddings,ablation analysis see that task -specific embeddings and character embeddings,0.5577905774116516
translation,382,164,ablation-analysis,ablation analysis,using,task -specific embeddings and character embeddings,ablation analysis using task -specific embeddings and character embeddings,0.6176223158836365
translation,382,168,ablation-analysis,orthogonal decomposition,instead of,subject - body alignment,orthogonal decomposition instead of subject - body alignment,0.6151162385940552
translation,382,168,ablation-analysis,orthogonal decomposition,bring about,significant performance gain,orthogonal decomposition bring about significant performance gain,0.6485282182693481
translation,382,168,ablation-analysis,ablation analysis,Using,orthogonal decomposition,ablation analysis Using orthogonal decomposition,0.6910912990570068
translation,382,173,ablation-analysis,multi-dimensional attention,further boost,model performance,multi-dimensional attention further boost model performance,0.7208603024482727
translation,382,173,ablation-analysis,ablation analysis,Using,multi-dimensional attention,ablation analysis Using multi-dimensional attention,0.6502417325973511
translation,382,174,ablation-analysis,qcn,conclude that,both the subject and the body,qcn conclude that both the subject and the body,0.6825903654098511
translation,382,174,ablation-analysis,both the subject and the body,indispensable for,question representation,both the subject and the body indispensable for question representation,0.717664361000061
translation,382,174,ablation-analysis,ablation analysis,Comparing,qcn,ablation analysis Comparing qcn,0.6685468554496765
translation,382,175,ablation-analysis,ablation analysis,has,outperforms,ablation analysis has outperforms,0.6058512330055237
translation,382,121,baselines,baselines,has,cnn-lstm -crf,baselines has cnn-lstm -crf,0.5288237929344177
translation,382,128,baselines,top systems,from,semeval 2015,top systems from semeval 2015,0.5876075625419617
translation,382,128,baselines,systems,relying on,thread level information,systems relying on thread level information,0.6998370885848999
translation,382,128,baselines,thread level information,to make,global inference,thread level information to make global inference,0.5948726534843445
translation,382,128,baselines,baselines,include,top systems,baselines include top systems,0.5842745304107666
translation,382,138,baselines,baselines,has,beihang-msra,baselines has beihang-msra,0.5392928719520569
translation,382,143,baselines,lstm,is,simple neural network,lstm is simple neural network,0.566399335861206
translation,382,143,baselines,baselines,has,lstm,baselines has lstm,0.5395978093147278
translation,382,145,baselines,baselines,has,lstm - subject-body,baselines has lstm - subject-body,0.5528427958488464
translation,382,110,experimental-setup,tokenizer,from,"nltk ( bird , 2006 )","tokenizer from nltk ( bird , 2006 )",0.5440385341644287
translation,382,110,experimental-setup,tokenizer,to preprocess,each sentence,tokenizer to preprocess each sentence,0.7065628170967102
translation,382,110,experimental-setup,experimental setup,use,tokenizer,experimental setup use tokenizer,0.6232596039772034
translation,382,111,experimental-setup,word embeddings,in,sentence encoder layer,word embeddings in sentence encoder layer,0.47693783044815063
translation,382,111,experimental-setup,word embeddings,initialized with,300 - dimensional glove,word embeddings initialized with 300 - dimensional glove,0.7127784490585327
translation,382,111,experimental-setup,word vectors,trained on,domainspecific unannotated corpus,word vectors trained on domainspecific unannotated corpus,0.6930534839630127
translation,382,111,experimental-setup,300 - dimensional glove,has,word vectors,300 - dimensional glove has word vectors,0.5570069551467896
translation,382,111,experimental-setup,experimental setup,has,word embeddings,experimental setup has word embeddings,0.4932878315448761
translation,382,112,experimental-setup,adam optimizer,for,optimization,adam optimizer for optimization,0.5982468128204346
translation,382,112,experimental-setup,adam optimizer,with,first momentum coefficient,adam optimizer with first momentum coefficient,0.6035394072532654
translation,382,112,experimental-setup,adam optimizer,with,second momentum coefficient,adam optimizer with second momentum coefficient,0.6108108758926392
translation,382,112,experimental-setup,first momentum coefficient,of,0.9,first momentum coefficient of 0.9,0.5872665047645569
translation,382,112,experimental-setup,first momentum coefficient,of,0.999,first momentum coefficient of 0.999,0.5610658526420593
translation,382,112,experimental-setup,second momentum coefficient,of,0.999,second momentum coefficient of 0.999,0.5564921498298645
translation,382,112,experimental-setup,experimental setup,use,adam optimizer,experimental setup use adam optimizer,0.5987385511398315
translation,382,113,experimental-setup,small grid search,over,combinations,small grid search over combinations,0.6746228933334351
translation,382,113,experimental-setup,combinations,of,"initial learning rate [ 1 ? 10 ?6 , 3 ? 10 ?6 , 1 ? 10 ?5 ]","combinations of initial learning rate [ 1 ? 10 ?6 , 3 ? 10 ?6 , 1 ? 10 ?5 ]",0.6109337210655212
translation,382,113,experimental-setup,combinations,of,l2 regularization parameter,combinations of l2 regularization parameter,0.5759400129318237
translation,382,113,experimental-setup,combinations,of,"batch size [ 8 , 16 , 32 ]","combinations of batch size [ 8 , 16 , 32 ]",0.6065534353256226
translation,382,113,experimental-setup,l2 regularization parameter,has,"1 ? 10 ?7 , 3 ? 10 ?7 , 1 ? 10 ?6 ]","l2 regularization parameter has 1 ? 10 ?7 , 3 ? 10 ?7 , 1 ? 10 ?6 ]",0.529533326625824
translation,382,113,experimental-setup,experimental setup,perform,small grid search,experimental setup perform small grid search,0.5753313899040222
translation,382,7,model,question condensing networks ( qcn ),to make use of,subject - body relationship,question condensing networks ( qcn ) to make use of subject - body relationship,0.6865509748458862
translation,382,7,model,subject - body relationship,of,community questions,subject - body relationship of community questions,0.5707318782806396
translation,382,7,model,model,propose,question condensing networks ( qcn ),model propose question condensing networks ( qcn ),0.6297141909599304
translation,382,8,model,question subject,is,primary part,question subject is primary part,0.5582128167152405
translation,382,8,model,primary part,of,question representation,primary part of question representation,0.6032268404960632
translation,382,8,model,aggregated,based on,similarity and disparity,aggregated based on similarity and disparity,0.6763513088226318
translation,382,8,model,similarity and disparity,with,question subject,similarity and disparity with question subject,0.6360816359519958
translation,382,28,model,subject-body relationship,in,community questions,subject-body relationship in community questions,0.5124902129173279
translation,382,28,model,subject-body relationship,treat,question subject,subject-body relationship treat question subject,0.5917683839797974
translation,382,28,model,question subject,as,primary part,question subject as primary part,0.511009156703949
translation,382,28,model,primary part,of,question,primary part of question,0.5859127044677734
translation,382,28,model,question body information,based on,similarity and disparity,question body information based on similarity and disparity,0.6588655710220337
translation,382,28,model,similarity and disparity,with,question subject,similarity and disparity with question subject,0.6360816359519958
translation,382,28,model,model,to utilize,subject-body relationship,model to utilize subject-body relationship,0.7754836082458496
translation,382,28,model,model,treat,question subject,model treat question subject,0.62269127368927
translation,382,28,model,model,aggregate,question body information,model aggregate question body information,0.7925503253936768
translation,382,43,model,question - answer pairs,using,multi-dimensional attention mechanism,question - answer pairs using multi-dimensional attention mechanism,0.6470796465873718
translation,382,43,model,model,align,question - answer pairs,model align question - answer pairs,0.701612651348114
translation,382,122,model,question,encoded by,cnn,question encoded by cnn,0.7576958537101746
translation,382,122,model,answers,encoded by,cnn,answers encoded by cnn,0.8047740459442139
translation,382,122,model,linearly connected,in,sequence,linearly connected in sequence,0.559516966342926
translation,382,122,model,model,has,question,model has question,0.5776472091674805
translation,382,137,model,syntactic tree kernels,with,relational links,syntactic tree kernels with relational links,0.5751415491104126
translation,382,137,model,relational links,between,questions and answers,relational links between questions and answers,0.6827278733253479
translation,382,137,model,model,used,syntactic tree kernels,model used syntactic tree kernels,0.5633561015129089
translation,382,142,model,supervised model,using,traditional features,supervised model using traditional features,0.6626124978065491
translation,382,142,model,convolutional neural network,to represent,question - answer pair,convolutional neural network to represent question - answer pair,0.6716176271438599
translation,382,142,model,model,combined,supervised model,model combined supervised model,0.744979202747345
translation,382,144,model,lstm,to obtain,question and answer representation,lstm to obtain question and answer representation,0.5885718464851379
translation,382,147,model,lstm,applied on,question subject and body,lstm applied on question subject and body,0.6843814253807068
translation,382,147,model,concatenated,to form,question representation,concatenated to form question representation,0.6488496661186218
translation,382,147,model,model,has,lstm,model has lstm,0.5196293592453003
translation,382,129,results,proposed qcn,achieve,state - of - the - art performance,proposed qcn achieve state - of - the - art performance,0.608331024646759
translation,382,129,results,previous best model,by,1.7 %,previous best model by 1.7 %,0.5476381182670593
translation,382,129,results,previous best model,by,3.4 %,previous best model by 3.4 %,0.5492337346076965
translation,382,129,results,1.7 %,in terms of,f1,1.7 % in terms of f1,0.7178937792778015
translation,382,129,results,3.4 %,in terms of,accuracy,3.4 % in terms of accuracy,0.6762990355491638
translation,382,129,results,outperforming,has,previous best model,outperforming has previous best model,0.6299944519996643
translation,382,129,results,results,observe,proposed qcn,results observe proposed qcn,0.6225363612174988
translation,382,131,results,can outperform,relying on,thread level information,can outperform relying on thread level information,0.7431033849716187
translation,382,131,results,systems,relying on,thread level information,systems relying on thread level information,0.6998370885848999
translation,382,131,results,thread level information,to make,global inference,thread level information to make global inference,0.5948726534843445
translation,382,131,results,model,has,can outperform,model has can outperform,0.6425521373748779
translation,382,131,results,can outperform,has,systems,can outperform has systems,0.6441153287887573
translation,382,131,results,results,has,model,results has model,0.5339115858078003
translation,382,133,results,outperform,by,large margin,outperform by large margin,0.6385749578475952
translation,382,133,results,qcn,has,outperform,qcn has outperform,0.6850338578224182
translation,382,133,results,results,has,qcn,results has qcn,0.5938161015510559
translation,382,149,results,orthogonal decomposition,is,more effective,orthogonal decomposition is more effective,0.58742356300354
translation,382,149,results,more effective,than,simple concatenation,more effective than simple concatenation,0.5913823843002319
translation,382,171,results,outperforms,by,great margin,outperforms by great margin,0.6537057757377625
translation,382,171,results,qcn,has,outperforms,qcn has outperforms,0.645211935043335
translation,382,171,results,results,has,qcn,results has qcn,0.5938161015510559
translation,383,50,baselines,hotpotqa,is,multihop benchmark,hotpotqa is multihop benchmark,0.5356303453445435
translation,383,50,baselines,multihop benchmark,over,pairs,multihop benchmark over pairs,0.6818522214889526
translation,383,50,baselines,pairs,of,text paragraphs,pairs of text paragraphs,0.6303462386131287
translation,383,50,baselines,text paragraphs,from,wikipedia,text paragraphs from wikipedia,0.5520542860031128
translation,383,50,baselines,retrieval,from,fixed kb schemas,retrieval from fixed kb schemas,0.5928412079811096
translation,383,50,baselines,baselines,has,hotpotqa,baselines has hotpotqa,0.5762498378753662
translation,383,155,baselines,baselines,has,"question-only , passage-only and image-only baselines","baselines has question-only , passage-only and image-only baselines",0.5665473341941833
translation,383,156,baselines,three unimodal baselines,for,automated quality assurance,three unimodal baselines for automated quality assurance,0.6310029029846191
translation,383,156,baselines,automated quality assurance,of,vlqa data,automated quality assurance of vlqa data,0.587360680103302
translation,383,156,baselines,baselines,use,three unimodal baselines,baselines use three unimodal baselines,0.6097099184989929
translation,383,157,experimental-setup,"question-only , passage-only and image-only models",implemented using,"roberta ( liu et al. , 2019 )","question-only , passage-only and image-only models implemented using roberta ( liu et al. , 2019 )",0.5967214703559875
translation,383,157,experimental-setup,"roberta ( liu et al. , 2019 )",finetuned on,arc,"roberta ( liu et al. , 2019 ) finetuned on arc",0.6878188252449036
translation,383,157,experimental-setup,"roberta ( liu et al. , 2019 )","ALBERT ( Lan et al. , 2019 ) finetuned on",race,"roberta ( liu et al. , 2019 ) ALBERT ( Lan et al. , 2019 ) finetuned on race",0.8091984391212463
translation,383,157,experimental-setup,"roberta ( liu et al. , 2019 )","LXMERT ( Tan and Bansal , 2019 ) finetuned on","vqa ( antol et al. , 2015 )","roberta ( liu et al. , 2019 ) LXMERT ( Tan and Bansal , 2019 ) finetuned on vqa ( antol et al. , 2015 )",0.8237635493278503
translation,383,157,experimental-setup,experimental setup,"LXMERT ( Tan and Bansal , 2019 ) finetuned on","vqa ( antol et al. , 2015 )","experimental setup LXMERT ( Tan and Bansal , 2019 ) finetuned on vqa ( antol et al. , 2015 )",0.7993991374969482
translation,383,157,experimental-setup,experimental setup,has,"question-only , passage-only and image-only models","experimental setup has question-only , passage-only and image-only models",0.5242311954498291
translation,383,201,experiments,joint reasoning,over,image -text multimodal context,joint reasoning over image -text multimodal context,0.6599603295326233
translation,383,201,experiments,joint reasoning,developed,visuo-linguistic question answering ( vlqa ) dataset,joint reasoning developed visuo-linguistic question answering ( vlqa ) dataset,0.6044538021087646
translation,383,154,results,performance,of,random baseline,performance of random baseline,0.5675410628318787
translation,383,154,results,random baseline,is,31.36 %,random baseline is 31.36 %,0.5601614117622375
translation,383,208,results,pre-trained vision - language models,fail to solve,significant portion,pre-trained vision - language models fail to solve significant portion,0.7241396307945251
translation,383,208,results,significant portion,of,vlqa items,significant portion of vlqa items,0.5861148238182068
translation,383,208,results,results,observe that,pre-trained vision - language models,results observe that pre-trained vision - language models,0.5514588356018066
translation,383,209,results,proposed modular method hole,has,slightly outperforms,proposed modular method hole has slightly outperforms,0.6285924315452576
translation,383,209,results,results,has,proposed modular method hole,results has proposed modular method hole,0.6141878962516785
translation,383,212,results,human evaluation,of,vlqa test-set,human evaluation of vlqa test-set,0.5745015144348145
translation,383,212,results,reported accuracy,is,84.0 %,reported accuracy is 84.0 %,0.5176935791969299
translation,383,212,results,human evaluation,has,reported accuracy,human evaluation has reported accuracy,0.5282197594642639
translation,383,212,results,vlqa test-set,has,reported accuracy,vlqa test-set has reported accuracy,0.5313422679901123
translation,383,212,results,results,For,human evaluation,results For human evaluation,0.5413625836372375
translation,384,205,ablation-analysis,ablation analysis,observe,qa +,ablation analysis observe qa +,0.6581307053565979
translation,384,188,baselines,squad and ms marco datasets,use,four qa baselines,squad and ms marco datasets use four qa baselines,0.5892409682273865
translation,384,188,baselines,baselines,For,squad and ms marco datasets,baselines For squad and ms marco datasets,0.6018966436386108
translation,384,189,baselines,first two baselines,based on,simple word overlap,first two baselines based on simple word overlap,0.5696973204612732
translation,384,189,baselines,wordcnt and normwordcnt,based on,simple word overlap,wordcnt and normwordcnt based on simple word overlap,0.6489678025245667
translation,384,189,baselines,baselines,has,first two baselines,baselines has first two baselines,0.5843560099601746
translation,384,191,baselines,wordcnt,uses,unnormalized word co-occurrence,wordcnt uses unnormalized word co-occurrence,0.580413281917572
translation,384,191,baselines,wordcnt,uses,normalized word co-occurrence,wordcnt uses normalized word co-occurrence,0.5866763591766357
translation,384,191,baselines,wordcnt,uses,normalized word co-occurrence,wordcnt uses normalized word co-occurrence,0.5866763591766357
translation,384,191,baselines,normwordcnt,uses,normalized word co-occurrence,normwordcnt uses normalized word co-occurrence,0.5786575675010681
translation,384,191,baselines,baselines,has,wordcnt,baselines has wordcnt,0.5838298797607422
translation,384,192,baselines,"abcnn ( yin et al. , 2015 )",use,neural network approach,"abcnn ( yin et al. , 2015 ) use neural network approach",0.6110873818397522
translation,384,192,baselines,neural network approach,to model,semantic relatedness,neural network approach to model semantic relatedness,0.6851251125335693
translation,384,192,baselines,semantic relatedness,of,sentence pairs,semantic relatedness of sentence pairs,0.520715594291687
translation,384,193,baselines,baselines,For,wikiqa and trecqa dataset,baselines For wikiqa and trecqa dataset,0.6001313328742981
translation,384,194,baselines,first baseline,uses,"edit distance ( levenshtein , 1966 )","first baseline uses edit distance ( levenshtein , 1966 )",0.5506764650344849
translation,384,194,baselines,simple ir baselines,generates,questions,simple ir baselines generates questions,0.6671823263168335
translation,384,194,baselines,simple ir baselines,uses,"edit distance ( levenshtein , 1966 )","simple ir baselines uses edit distance ( levenshtein , 1966 )",0.5443601608276367
translation,384,194,baselines,"edit distance ( levenshtein , 1966 )",to calculate,distance,"edit distance ( levenshtein , 1966 ) to calculate distance",0.6281554698944092
translation,384,194,baselines,distance,between,question,distance between question,0.6887022852897644
translation,384,194,baselines,distance,between,input sentence,distance between input sentence,0.6306415796279907
translation,384,194,baselines,baselines,has,first baseline,baselines has first baseline,0.595583438873291
translation,384,195,baselines,mt system - moses,models,question generation,mt system - moses models question generation,0.7036876082420349
translation,384,196,baselines,directin,uses,longest sub-sentence,directin uses longest sub-sentence,0.5878223776817322
translation,384,196,baselines,longest sub-sentence,of,input sentence,longest sub-sentence of input sentence,0.5366247892379761
translation,384,196,baselines,longest sub-sentence,as,question,longest sub-sentence as question,0.5505670309066772
translation,384,197,baselines,h&s,is,rule- based overgenerate - and - rank system,h&s is rule- based overgenerate - and - rank system,0.5999341607093811
translation,384,31,experiments,seq2seq model,with,soft attention,seq2seq model with soft attention,0.6225751638412476
translation,384,31,experiments,soft attention,for,qg,soft attention for qg,0.628497302532196
translation,384,31,experiments,neural model,inspired from,attentive reader,neural model inspired from attentive reader,0.6871700882911682
translation,384,219,experiments,our approach,achieves,new state - of- the- art,our approach achieves new state - of- the- art,0.5970230102539062
translation,384,167,hyperparameters,our neural networks,keep,most frequent 50 k words,our neural networks keep most frequent 50 k words,0.6058075428009033
translation,384,167,hyperparameters,our neural networks,map,all other words,our neural networks map all other words,0.6550036072731018
translation,384,167,hyperparameters,most frequent 50 k words,including,entity and placeholder markers,most frequent 50 k words including entity and placeholder markers,0.704444408416748
translation,384,167,hyperparameters,all other words,to,special < unk > token,all other words to special < unk > token,0.5774443745613098
translation,384,167,hyperparameters,hyperparameters,For training,our neural networks,hyperparameters For training our neural networks,0.7163636684417725
translation,384,168,hyperparameters,100 - dimensional pretrained glove word embeddings,for,initialization,100 - dimensional pretrained glove word embeddings for initialization,0.5028587579727173
translation,384,168,hyperparameters,hyperparameters,choose,word embedding size d = 100,hyperparameters choose word embedding size d = 100,0.6020810008049011
translation,384,168,hyperparameters,hyperparameters,use,100 - dimensional pretrained glove word embeddings,hyperparameters use 100 - dimensional pretrained glove word embeddings,0.521602213382721
translation,384,169,hyperparameters,"k , m , k 1 and k 2",by,grid search,"k , m , k 1 and k 2 by grid search",0.613946795463562
translation,384,169,hyperparameters,grid search,on,held - out development set,grid search on held - out development set,0.5874890685081482
translation,384,169,hyperparameters,hyperparameters,set,"k , m , k 1 and k 2","hyperparameters set k , m , k 1 and k 2",0.6376670002937317
translation,384,169,hyperparameters,hyperparameters,for,self-training,hyperparameters for self-training,0.5579334497451782
translation,384,7,model,self-training method,for,jointly learning,self-training method for jointly learning,0.618197500705719
translation,384,7,model,jointly learning,to,ask,jointly learning to ask,0.5610631704330444
translation,384,7,model,unlabeled text,along with,labeled question answer pairs,unlabeled text along with labeled question answer pairs,0.6002737283706665
translation,384,7,model,labeled question answer pairs,for,learning,labeled question answer pairs for learning,0.5669775009155273
translation,384,7,model,model,propose,self-training method,model propose self-training method,0.6558359861373901
translation,384,87,model,qa and qg models,trained on,labeled corpus,qa and qg models trained on labeled corpus,0.724757730960846
translation,384,87,model,model,has,qa and qg models,model has qa and qg models,0.6139279007911682
translation,384,190,model,word co-occurrence,between,question sentence,word co-occurrence between question sentence,0.589705765247345
translation,384,190,model,word co-occurrence,between,candidate answer sentence,word co-occurrence between candidate answer sentence,0.5952996611595154
translation,384,190,model,model,compute,word co-occurrence,model compute word co-occurrence,0.6837790608406067
translation,384,30,results,questions,in,increasing order,questions in increasing order,0.5537760257720947
translation,384,30,results,questions,leads to,improvements,questions leads to improvements,0.688857913017273
translation,384,30,results,increasing order,of,hardness,increasing order of hardness,0.6340778470039368
translation,384,30,results,improvements,over,baseline,improvements over baseline,0.7402786016464233
translation,384,30,results,improvements,introduces,questions,improvements introduces questions,0.7040719985961914
translation,384,30,results,questions,has,randomly,questions has randomly,0.6206367015838623
translation,384,30,results,results,introducing,questions,results introducing questions,0.6466343402862549
translation,384,201,results,degrades,instead of,improving,degrades instead of improving,0.7109434604644775
translation,384,201,results,improving,with,time,improving with time,0.631691038608551
translation,384,201,results,map score,has,degrades,map score has degrades,0.6172304153442383
translation,384,201,results,results,observe,map score,results observe map score,0.6312945485115051
translation,384,204,results,degrades,by having,more and more unlabeled data,degrades by having more and more unlabeled data,0.6830020546913147
translation,384,204,results,curriculum learning based oracle,has,map score,curriculum learning based oracle has map score,0.5787686705589294
translation,384,204,results,map score,has,degrades,map score has degrades,0.6172304153442383
translation,384,206,results,qg oracle,performs,better,qg oracle performs better,0.6175633072853088
translation,384,206,results,better,than,qa and qg,better than qa and qg,0.5930010080337524
translation,384,206,results,results,has,qg oracle,results has qg oracle,0.5977954268455505
translation,384,213,results,qa evaluations,on,squad and ms marco datasets,qa evaluations on squad and ms marco datasets,0.5188886523246765
translation,384,214,results,our qa model,has,competetive or better performance,our qa model has competetive or better performance,0.605414867401123
translation,384,214,results,competetive or better performance,over,all the baselines,competetive or better performance over all the baselines,0.6723538041114807
translation,384,214,results,our qa model,has,competetive or better performance,our qa model has competetive or better performance,0.605414867401123
translation,384,214,results,results,observe,our qa model,results observe our qa model,0.6449999809265137
translation,384,215,results,ensembling or diversity,see,further improvement,ensembling or diversity see further improvement,0.5852692127227783
translation,384,215,results,further improvement,in,result,further improvement in result,0.5707936882972717
translation,384,215,results,results,incorporate,ensembling or diversity,results incorporate ensembling or diversity,0.6461445689201355
translation,384,217,results,our qa model,is,competitive,our qa model is competitive,0.6293150782585144
translation,384,217,results,competitive,to,all the baselines,competitive to all the baselines,0.5869916677474976
translation,384,217,results,results,observe,our qa model,results observe our qa model,0.6449999809265137
translation,384,218,results,ensembling and diversity,while jointly learning,qa and qg models,ensembling and diversity while jointly learning qa and qg models,0.7862080931663513
translation,384,218,results,qa and qg models,see,incremental improvements,qa and qg models see incremental improvements,0.6027622818946838
translation,384,218,results,results,introduce,ensembling and diversity,results introduce ensembling and diversity,0.5789698958396912
translation,384,221,results,qg,on,four datasets,qg on four datasets,0.5408737063407898
translation,384,222,results,qg model,performs,much better,qg model performs much better,0.6362079381942749
translation,384,222,results,much better,than,all the baselines,much better than all the baselines,0.5883567929267883
translation,384,222,results,results,observe that,qg model,results observe that qg model,0.6230306625366211
translation,384,223,results,self-training,jointly training,qa and qg models,self-training jointly training qa and qg models,0.8598070740699768
translation,384,223,results,qa and qg models,leads to,even better performance,qa and qg models leads to even better performance,0.7061062455177307
translation,384,223,results,results,observe,self-training,results observe self-training,0.570454478263855
translation,384,224,results,self-training and leveraging,relationship between,qa and qg,self-training and leveraging relationship between qa and qg,0.7449381947517395
translation,384,224,results,very useful,for boosting,performance,very useful for boosting performance,0.6986158490180969
translation,384,224,results,performance,of,qa and qg models,performance of qa and qg models,0.6292775869369507
translation,384,224,results,results,show,self-training and leveraging,results show self-training and leveraging,0.5603614449501038
translation,384,237,results,all the proposed heuristics,lead to,improvements,all the proposed heuristics lead to improvements,0.6868305802345276
translation,384,237,results,improvements,in,final performance,improvements in final performance,0.5090992450714111
translation,384,237,results,final performance,of,qa and qg,final performance of qa and qg,0.6108919382095337
translation,384,237,results,results,observe,all the proposed heuristics,results observe all the proposed heuristics,0.563621461391449
translation,384,239,results,lesser impact,on,performance,lesser impact on performance,0.5685864090919495
translation,384,239,results,lesser impact,see,much more significant performance gain,lesser impact see much more significant performance gain,0.5616053938865662
translation,384,239,results,much more significant performance gain,by,ensembling,much more significant performance gain by ensembling,0.6054191589355469
translation,384,239,results,ensembling,to combine,various heuristics,ensembling to combine various heuristics,0.7076166868209839
translation,384,239,results,e&e,to incorporate,diversity,e&e to incorporate diversity,0.7333881855010986
translation,384,239,results,results,see,much more significant performance gain,results see much more significant performance gain,0.5942445993423462
translation,384,245,results,significantly drop,in,performance,significantly drop in performance,0.5627492666244507
translation,384,245,results,performance,reduce,proportion of labeled training set,performance reduce proportion of labeled training set,0.6798206567764282
translation,384,245,results,all the baselines,has,significantly drop,all the baselines has significantly drop,0.581985354423523
translation,384,245,results,results,observe,all the baselines,results observe all the baselines,0.5937308669090271
translation,385,131,ablation-analysis,"and semantic features ( i.e. ,   netypes   )",complementary to,"corpusbased statistics features ( i.e. , average idf )","and semantic features ( i.e. ,   netypes   ) complementary to corpusbased statistics features ( i.e. , average idf )",0.612173318862915
translation,385,131,ablation-analysis,syntactical,has,"and semantic features ( i.e. ,   netypes   )","syntactical has and semantic features ( i.e. ,   netypes   )",0.5563188791275024
translation,385,131,ablation-analysis,ablation analysis,has,syntactical,ablation analysis has syntactical,0.5659196376800537
translation,385,133,ablation-analysis,feature group,characterize,position,feature group characterize position,0.7214534282684326
translation,385,133,ablation-analysis,position,of,candidate mmp,position of candidate mmp,0.6668192744255066
translation,385,133,ablation-analysis,position,has,large impact,position has large impact,0.5987998843193054
translation,385,133,ablation-analysis,candidate mmp,relative to,sentence,candidate mmp relative to sentence,0.7048360109329224
translation,385,133,ablation-analysis,candidate mmp,relative to,parse tree,candidate mmp relative to parse tree,0.6958109736442566
translation,385,133,ablation-analysis,candidate mmp,has,large impact,candidate mmp has large impact,0.5939686894416809
translation,385,133,ablation-analysis,large impact,improves,f-measure,large impact improves f-measure,0.736703634262085
translation,385,133,ablation-analysis,f-measure,by,5.6 points,f-measure by 5.6 points,0.560212254524231
translation,385,195,ablation-analysis,mmp features,are,very helpful,mmp features are very helpful,0.5535995960235596
translation,385,195,ablation-analysis,very helpful,to,relevance model,very helpful to relevance model,0.4834497272968292
translation,385,195,ablation-analysis,ablation analysis,show,mmp features,ablation analysis show mmp features,0.6315651535987854
translation,385,5,model,learnable system,extract and rank,terms and phrases,learnable system extract and rank terms and phrases,0.6458077430725098
translation,385,5,model,model,present,learnable system,model present learnable system,0.672804057598114
translation,385,14,model,important terms and phrases,in,natural language question,important terms and phrases in natural language question,0.38631969690322876
translation,385,14,model,model,identifies,important terms and phrases,model identifies important terms and phrases,0.6508787870407104
translation,385,21,model,deep syntactic and semantic analyses,of,questions,deep syntactic and semantic analyses of questions,0.5629250407218933
translation,385,21,model,questions,to determine and rank,mmps,questions to determine and rank mmps,0.6704710125923157
translation,385,21,model,model,explore,deep syntactic and semantic analyses,model explore deep syntactic and semantic analyses,0.6711432933807373
translation,385,238,model,framework,to select and rank mandatory matching phrases ( MMP ),question answering,framework to select and rank mandatory matching phrases ( MMP ) question answering,0.7159302830696106
translation,385,239,model,framework,full use of,"lexical , syntactic and semantic information","framework full use of lexical , syntactic and semantic information",0.5357518196105957
translation,385,239,model,"lexical , syntactic and semantic information",in,question,"lexical , syntactic and semantic information in question",0.536314845085144
translation,385,239,model,"lexical , syntactic and semantic information",does not require,answer data,"lexical , syntactic and semantic information does not require answer data",0.6750645637512207
translation,385,239,model,model,has,framework,model has framework,0.5441871285438538
translation,385,7,results,our proposed model,predict,mmps,our proposed model predict mmps,0.7529252171516418
translation,385,7,results,mmps,with,high accuracy,mmps with high accuracy,0.6612890362739563
translation,385,7,results,results,has,our proposed model,results has our proposed model,0.5871988534927368
translation,385,8,results,qa system,derived from,mmp model,qa system derived from mmp model,0.6779093742370605
translation,385,8,results,features,derived from,mmp model,features derived from mmp model,0.6874505281448364
translation,385,8,results,performance significantly,over,state - of - the - art baseline,performance significantly over state - of - the - art baseline,0.6321674585342407
translation,385,8,results,qa system,has,features,qa system has features,0.5709880590438843
translation,385,8,results,features,has,improve,features has improve,0.6091814637184143
translation,385,8,results,mmp model,has,improve,mmp model has improve,0.6256500482559204
translation,385,8,results,improve,has,performance significantly,improve has performance significantly,0.559346616268158
translation,385,125,results,mmp classifier,achieves,88.6 % f-measure,mmp classifier achieves 88.6 % f-measure,0.6618993878364563
translation,385,125,results,results,has,mmp classifier,results has mmp classifier,0.5729660391807556
translation,385,132,results,lexical features,addition of,case - features,lexical features addition of case - features,0.5785340666770935
translation,385,132,results,improve,addition of,case - features,improve addition of case - features,0.5762169361114502
translation,385,132,results,recall,addition of,case - features,recall addition of case - features,0.5675359964370728
translation,385,132,results,f-measure,by,4 points,f-measure by 4 points,0.6089413166046143
translation,385,132,results,improve,has,recall,improve has recall,0.563742995262146
translation,385,132,results,results,has,lexical features,results has lexical features,0.519563615322113
translation,385,194,results,logistic regression models,are,consistently better,logistic regression models are consistently better,0.5785456895828247
translation,385,194,results,consistently better,than,decision tree ones,consistently better than decision tree ones,0.6029549241065979
translation,385,194,results,two classifiers,has,logistic regression models,two classifiers has logistic regression models,0.5600828528404236
translation,385,194,results,results,Between,two classifiers,results Between two classifiers,0.5893656015396118
translation,385,217,results,mmp system,is,5 points better,mmp system is 5 points better,0.5793933272361755
translation,385,217,results,mmp system,about,5 points better,mmp system about 5 points better,0.649185836315155
translation,385,217,results,5 points better,than,baseline system,5 points better than baseline system,0.5792537927627563
translation,385,218,results,notably better,by,2 points,notably better by 2 points,0.6181389689445496
translation,385,218,results,far better,by,7.7 %,far better by 7.7 %,0.5837478041648865
translation,385,218,results,far better,than,baseline,far better than baseline,0.564407229423523
translation,385,218,results,results,has,precision,results has precision,0.592319667339325
translation,385,218,results,results,has,recall,results has recall,0.6362076997756958
translation,385,233,results,4 participating sites,has,our system,4 participating sites has our system,0.6018880605697632
translation,385,233,results,our system,has,highest performance,our system has highest performance,0.5840916037559509
translation,385,233,results,results,Among,4 participating sites,results Among 4 participating sites,0.5530803203582764
translation,386,125,hyperparameters,size,of,entity and relation representations d,size of entity and relation representations d,0.5908302664756775
translation,386,125,hyperparameters,size,of,hidden state,size of hidden state,0.5946854948997498
translation,386,125,hyperparameters,entity and relation representations d,at,100,entity and relation representations d at 100,0.574030876159668
translation,386,125,hyperparameters,hidden state,at,200,hidden state at 200,0.5986123085021973
translation,386,125,hyperparameters,hyperparameters,set,size,hyperparameters set size,0.6779580116271973
translation,386,125,hyperparameters,hyperparameters,set,hidden state,hyperparameters set hidden state,0.640205979347229
translation,386,126,hyperparameters,models,with,path length 3,models with path length 3,0.6669556498527527
translation,386,126,hyperparameters,hyperparameters,use,single layer lstm,hyperparameters use single layer lstm,0.5536746978759766
translation,386,126,hyperparameters,hyperparameters,train,models,hyperparameters train models,0.6666569709777832
translation,386,127,hyperparameters,neural network,using,"adam ( kingma and ba , 2015 )","neural network using adam ( kingma and ba , 2015 )",0.6725901365280151
translation,386,127,hyperparameters,"adam ( kingma and ba , 2015 )",with,learning rate 0.001,"adam ( kingma and ba , 2015 ) with learning rate 0.001",0.5885973572731018
translation,386,127,hyperparameters,"adam ( kingma and ba , 2015 )",with,mini-batches,"adam ( kingma and ba , 2015 ) with mini-batches",0.6121659874916077
translation,386,127,hyperparameters,mini-batches,of size,256,mini-batches of size 256,0.7383550405502319
translation,386,127,hyperparameters,256,with,20 rollouts,256 with 20 rollouts,0.6799039840698242
translation,386,127,hyperparameters,hyperparameters,optimize,neural network,hyperparameters optimize neural network,0.7013351917266846
translation,386,128,hyperparameters,test time,use,beam search,test time use beam search,0.6696777939796448
translation,386,128,hyperparameters,beam search,with,beam size,beam search with beam size,0.6320963501930237
translation,386,128,hyperparameters,beam size,of,100,beam size of 100,0.6846780180931091
translation,386,128,hyperparameters,hyperparameters,During,test time,hyperparameters During test time,0.6625650525093079
translation,386,149,hyperparameters,model,pretrained in,supervised way,model pretrained in supervised way,0.7911142110824585
translation,386,149,hyperparameters,hyperparameters,has,model,hyperparameters has model,0.5282720923423767
translation,386,150,hyperparameters,model,retrained using,rl algorithm,model retrained using rl algorithm,0.7110900282859802
translation,386,150,hyperparameters,rl algorithm,with,ternary reward structure,rl algorithm with ternary reward structure,0.6040480732917786
translation,386,150,hyperparameters,hyperparameters,retrained using,rl algorithm,hyperparameters retrained using rl algorithm,0.6643406748771667
translation,386,150,hyperparameters,hyperparameters,has,model,hyperparameters has model,0.5282720923423767
translation,386,34,model,depth - first - search ( dfs ) algorithm,to collect,paths,depth - first - search ( dfs ) algorithm to collect paths,0.6581788659095764
translation,386,34,model,paths,lead to,correct answer,paths lead to correct answer,0.7231074571609497
translation,386,34,model,model,investigate,depth - first - search ( dfs ) algorithm,model investigate depth - first - search ( dfs ) algorithm,0.6479527354240417
translation,386,136,results,models,trained using,supervised learning,models trained using supervised learning,0.7284979224205017
translation,386,136,results,models,score,lower,models score lower,0.7163445949554443
translation,386,136,results,supervised learning,score,lower,supervised learning score lower,0.6638301014900208
translation,386,136,results,lower,on,all metrics,lower on all metrics,0.5485875606536865
translation,386,136,results,lower,compared to,rl,lower compared to rl,0.694407045841217
translation,386,136,results,alexa69k - 378,has,models,alexa69k - 378 has models,0.6373822689056396
translation,386,136,results,results,On,alexa69k - 378,results On alexa69k - 378,0.49469536542892456
translation,386,137,results,supervised learning,with,rl,supervised learning with rl,0.6657332181930542
translation,386,137,results,supervised learning,has,overall performance,supervised learning has overall performance,0.5498802661895752
translation,386,137,results,rl,has,overall performance,rl has overall performance,0.6183133721351624
translation,386,137,results,overall performance,has,increases,overall performance has increases,0.5817619562149048
translation,386,137,results,results,When combining,supervised learning,results When combining supervised learning,0.6747438907623291
translation,386,139,results,rl system,with,our ternary reward structure ( no answer rl ),rl system with our ternary reward structure ( no answer rl ),0.6558737754821777
translation,386,139,results,increase significantly,on,both datasets,increase significantly on both datasets,0.5372359752655029
translation,386,139,results,rl system,has,precision and qa score,rl system has precision and qa score,0.5804867744445801
translation,386,139,results,our ternary reward structure ( no answer rl ),has,precision and qa score,our ternary reward structure ( no answer rl ) has precision and qa score,0.5879954099655151
translation,386,139,results,precision and qa score,has,increase significantly,precision and qa score has increase significantly,0.610900342464447
translation,386,139,results,results,train,rl system,results train rl system,0.6613969802856445
translation,386,140,results,no answer rl model,decided not to,answer,no answer rl model decided not to answer,0.6573204398155212
translation,386,140,results,answer,over,40 %,answer over 40 %,0.627362847328186
translation,386,140,results,40 %,of,questions,40 % of questions,0.6270124316215515
translation,386,140,results,absolute hits@1 reduction,of,only 1.3 %,absolute hits@1 reduction of only 1.3 %,0.5322647094726562
translation,386,140,results,only 1.3 %,over,standard rl,only 1.3 % over standard rl,0.6650344133377075
translation,386,140,results,fb15k - 237,has,no answer rl model,fb15k - 237 has no answer rl model,0.631370484828949
translation,386,140,results,results,For,fb15k - 237,results For fb15k - 237,0.5950779318809509
translation,386,141,results,answered correctly,compared to,24.75 %,answered correctly compared to 24.75 %,0.6229660511016846
translation,386,141,results,24.75 %,of,original question - answering system,24.75 % of original question - answering system,0.5255164504051208
translation,386,141,results,absolute improvement,of,over 15 %,absolute improvement of over 15 %,0.5857303142547607
translation,386,141,results,answered questions,has,40.11 %,answered questions has 40.11 %,0.5360061526298523
translation,386,141,results,original question - answering system,has,absolute improvement,original question - answering system has absolute improvement,0.5283772945404053
translation,386,141,results,results,of,answered questions,results of answered questions,0.5334933400154114
translation,386,152,results,qa score,of,52.16 %,qa score of 52.16 %,0.5319895148277283
translation,386,152,results,52.16 %,obtained,absolute improvement,52.16 % obtained absolute improvement,0.6104544401168823
translation,386,152,results,52.16 %,is,absolute improvement,52.16 % is absolute improvement,0.5529728531837463
translation,386,152,results,absolute improvement,of,4.58 %,absolute improvement of 4.58 %,0.5224546790122986
translation,386,152,results,absolute improvement,of,2.66 %,absolute improvement of 2.66 %,0.5195450186729431
translation,386,152,results,4.58 %,over,best individual model,4.58 % over best individual model,0.6437717080116272
translation,386,152,results,2.66 %,over,lin et al . ( 2018 ),2.66 % over lin et al . ( 2018 ),0.5441476106643677
translation,386,152,results,fb15k - 237,has,qa score,fb15k - 237 has qa score,0.6044686436653137
translation,386,152,results,results,On,fb15k - 237,results On fb15k - 237,0.5215417742729187
translation,387,77,hyperparameters,questions,trimmed to,maximum,questions trimmed to maximum,0.6624160408973694
translation,387,77,hyperparameters,maximum,has,of t = 14 words,maximum has of t = 14 words,0.6104185581207275
translation,387,77,hyperparameters,hyperparameters,has,questions,hyperparameters has questions,0.5738486051559448
translation,387,6,model,model,proposes,multi-grained attention method,model proposes multi-grained attention method,0.6820905208587646
translation,387,7,model,explicit wordobject correspondence,by,two types of wordlevel attention,explicit wordobject correspondence by two types of wordlevel attention,0.5013646483421326
translation,387,7,model,two types of wordlevel attention,complementary to,sentenceimage association,two types of wordlevel attention complementary to sentenceimage association,0.613350510597229
translation,387,7,model,model,learns,explicit wordobject correspondence,model learns explicit wordobject correspondence,0.710024893283844
translation,387,21,model,matching model,trained on,object-detection dataset,matching model trained on object-detection dataset,0.6834411025047302
translation,387,21,model,object-detection dataset,to learn,explicit correspondence,object-detection dataset to learn explicit correspondence,0.5927442908287048
translation,387,21,model,explicit correspondence,between,content words,explicit correspondence between content words,0.6475942134857178
translation,387,21,model,explicit correspondence,between,visual counterparts,explicit correspondence between visual counterparts,0.7090058326721191
translation,387,21,model,content words,in,question,content words in question,0.48172906041145325
translation,387,21,model,content words,in,visual counterparts,content words in visual counterparts,0.5135518312454224
translation,387,21,model,model,has,matching model,model has matching model,0.5754806399345398
translation,387,26,model,multi-grained attention mechanism,integrating,two types of object features,multi-grained attention mechanism integrating two types of object features,0.6605876088142395
translation,387,26,model,model,proposes,multi-grained attention mechanism,model proposes multi-grained attention mechanism,0.6963073015213013
translation,387,27,model,deep contextualized word representation elmo,adopted in,vqa task,deep contextualized word representation elmo adopted in vqa task,0.5953417420387268
translation,387,27,model,vqa task,to facilitate,better question encoding,vqa task to facilitate better question encoding,0.6330322027206421
translation,387,27,model,model,has,deep contextualized word representation elmo,model has deep contextualized word representation elmo,0.5630636811256409
translation,387,33,model,object- detection - based approach,to represent,input image,object- detection - based approach to represent input image,0.6716909408569336
translation,387,33,model,model,adopt,object- detection - based approach,model adopt object- detection - based approach,0.6739051342010498
translation,387,8,results,multi-grained attention model,achieves,competitive performance,multi-grained attention model achieves competitive performance,0.6559944748878479
translation,387,8,results,competitive performance,with,stateof - the - art models,competitive performance with stateof - the - art models,0.6368532180786133
translation,387,8,results,vqa benchmark,has,multi-grained attention model,vqa benchmark has multi-grained attention model,0.5008730292320251
translation,387,8,results,results,Evaluated on,vqa benchmark,results Evaluated on vqa benchmark,0.6730029582977295
translation,387,82,results,proposed two branches,of,fine- grained wl and wo attentions,proposed two branches of fine- grained wl and wo attentions,0.5867757797241211
translation,387,82,results,fine- grained wl and wo attentions,has,significantly improves,fine- grained wl and wo attentions has significantly improves,0.5829732418060303
translation,387,82,results,significantly improves,has,baseline performance,significantly improves has baseline performance,0.5950104594230652
translation,387,82,results,results,Adding,proposed two branches,results Adding proposed two branches,0.7633439302444458
translation,387,83,results,elmo embeddings,combined with,glove embeddings,elmo embeddings combined with glove embeddings,0.5781606435775757
translation,387,83,results,elmo embeddings,improves,overall performance,elmo embeddings improves overall performance,0.6385074257850647
translation,387,83,results,glove embeddings,provide,more sophisticated text representations,glove embeddings provide more sophisticated text representations,0.5579789280891418
translation,387,83,results,more sophisticated text representations,improves,overall performance,more sophisticated text representations improves overall performance,0.6428785920143127
translation,387,83,results,results,verifies that,elmo embeddings,results verifies that elmo embeddings,0.2969888746738434
translation,388,48,experimental-setup,arabic word vectors,use,word2vec,arabic word vectors use word2vec,0.5723728537559509
translation,388,48,experimental-setup,word2vec,to train,100 - dimensional vectors,word2vec to train 100 - dimensional vectors,0.6525174975395203
translation,388,48,experimental-setup,100 - dimensional vectors,with,default settings,100 - dimensional vectors with default settings,0.5886850953102112
translation,388,48,experimental-setup,100 - dimensional vectors,on,lemmatized version,100 - dimensional vectors on lemmatized version,0.5657466053962708
translation,388,48,experimental-setup,100 - dimensional vectors,obtaining,vocabulary,100 - dimensional vectors obtaining vocabulary,0.6703317761421204
translation,388,48,experimental-setup,default settings,on,lemmatized version,default settings on lemmatized version,0.5593500137329102
translation,388,48,experimental-setup,lemmatized version,of,arabic gigaword,lemmatized version of arabic gigaword,0.6085807085037231
translation,388,48,experimental-setup,vocabulary,of,"120,000 word lemmas","vocabulary of 120,000 word lemmas",0.5796833634376526
translation,388,48,experimental-setup,experimental setup,For,arabic word vectors,experimental setup For arabic word vectors,0.5578628778457642
translation,388,47,experiments,english word vectors,use,googlenews vectors dataset,english word vectors use googlenews vectors dataset,0.5735511183738708
translation,388,47,experiments,"3,000,000 word vocabulary",of,300 - dimensional word vectors,"3,000,000 word vocabulary of 300 - dimensional word vectors",0.5586281418800354
translation,388,47,experiments,"3,000,000 word vocabulary",trained on,100 billion words,"3,000,000 word vocabulary trained on 100 billion words",0.6938923001289368
translation,388,50,experiments,doc2 vec,provides,vectors,doc2 vec provides vectors,0.617805540561676
translation,388,50,experiments,vectors,for,text,vectors for text,0.6371566653251648
translation,388,50,experiments,text,of,arbitrary length,text of arbitrary length,0.5778489708900452
translation,388,90,experiments,our approach,for,answer selection task,our approach for answer selection task,0.5709662437438965
translation,388,90,experiments,answer selection task,in,english,answer selection task in english,0.4292442500591278
translation,388,90,experiments,english,ranked,6th out of 12 submissions,english ranked 6th out of 12 submissions,0.6846277713775635
translation,388,91,experiments,our approach,for,answer selection,our approach for answer selection,0.6158798336982727
translation,388,91,experiments,answer selection,in,arabic,answer selection in arabic,0.5379308462142944
translation,388,91,experiments,answer selection,ranked,2nd out of 4 submissions,answer selection ranked 2nd out of 4 submissions,0.7072893977165222
translation,388,107,experiments,indirect yes / no answer inference task,achieve,best performance,indirect yes / no answer inference task achieve best performance,0.6352007985115051
translation,388,107,experiments,indirect yes / no answer inference task,ranked 1st out of,8 submissions,indirect yes / no answer inference task ranked 1st out of 8 submissions,0.701508104801178
translation,388,9,model,simple features,derived from,vector similarity,simple features derived from vector similarity,0.6220720410346985
translation,389,157,ablation-analysis,embedding similarity,between,a and d,embedding similarity between a and d,0.6667559742927551
translation,389,157,ablation-analysis,a and d,is,most important feature,a and d is most important feature,0.556266188621521
translation,389,157,ablation-analysis,ablation analysis,find that,embedding similarity,ablation analysis find that embedding similarity,0.5925211310386658
translation,389,158,ablation-analysis,string similarities,such as,"token sim , ed , and suffix","string similarities such as token sim , ed , and suffix",0.6037046313285828
translation,389,158,ablation-analysis,string similarities,are,more important,string similarities are more important,0.5699162483215332
translation,389,158,ablation-analysis,"token sim , ed , and suffix",are,more important,"token sim , ed , and suffix are more important",0.5727367401123047
translation,389,158,ablation-analysis,more important,in,mcql,more important in mcql,0.5230957865715027
translation,389,158,ablation-analysis,mcql,than,sciq,mcql than sciq,0.6347401738166809
translation,389,158,ablation-analysis,ablation analysis,has,string similarities,ablation analysis has string similarities,0.563685417175293
translation,389,19,baselines,learning - based ranking models,to select,distractors,learning - based ranking models to select distractors,0.6859087944030762
translation,389,19,baselines,distractors,resemble,in actual exam mcqs,distractors resemble in actual exam mcqs,0.668153703212738
translation,389,75,baselines,logistic regression,has,efficient generalized linear classification model,logistic regression has efficient generalized linear classification model,0.5172661542892456
translation,389,75,baselines,baselines,study,three feature - based classifiers,baselines study three feature - based classifiers,0.5544602274894714
translation,389,107,baselines,unsupervised baselines,measure,similarities,unsupervised baselines measure similarities,0.6241707801818848
translation,389,107,baselines,similarities,between,key and distractors,similarities between key and distractors,0.7091668844223022
translation,389,107,baselines,pointwise mutual information ( pmi ),based on,co-occurrences,pointwise mutual information ( pmi ) based on co-occurrences,0.6348263621330261
translation,389,107,baselines,edit distance ( ed ),measures,spelling similarity,edit distance ( ed ) measures spelling similarity,0.5147603750228882
translation,389,133,baselines,"lambdamart ( burges , 2010 )",has,gradient boosted tree based learning - torank model,"lambdamart ( burges , 2010 ) has gradient boosted tree based learning - torank model",0.5632879137992859
translation,389,4,experiments,select useful distractors,for,multiple choice questions,select useful distractors for multiple choice questions,0.5975642204284668
translation,389,118,experiments,lambdamart experiments,use,xg - boost library,lambdamart experiments use xg - boost library,0.616850733757019
translation,389,103,hyperparameters,c,to,1,c to 1,0.6935829520225525
translation,389,103,hyperparameters,c,to,500 rounds of boosting,c to 500 rounds of boosting,0.5754903554916382
translation,389,103,hyperparameters,c,for,lr,c for lr,0.6707175970077515
translation,389,103,hyperparameters,c,use,500 trees,c use 500 trees,0.692970335483551
translation,389,103,hyperparameters,c,use,500 rounds of boosting,c use 500 rounds of boosting,0.6492411494255066
translation,389,103,hyperparameters,1,for,lr,1 for lr,0.7093673348426819
translation,389,103,hyperparameters,500 trees,for,rf,500 trees for rf,0.6685310006141663
translation,389,103,hyperparameters,500 rounds of boosting,for,lm,500 rounds of boosting for lm,0.5926660895347595
translation,389,103,hyperparameters,hyperparameters,set,c,hyperparameters set c,0.6763419508934021
translation,389,103,hyperparameters,hyperparameters,set,500 rounds of boosting,hyperparameters set 500 rounds of boosting,0.5983278155326843
translation,389,103,hyperparameters,hyperparameters,use,500 rounds of boosting,hyperparameters use 500 rounds of boosting,0.5904505252838135
translation,389,104,hyperparameters,number of negative samples,equal to,number of distractors,number of negative samples equal to number of distractors,0.6598399877548218
translation,389,104,hyperparameters,number of distractors,is,3,number of distractors is 3,0.5909173488616943
translation,389,104,hyperparameters,3,for,most questions,3 for most questions,0.6143339276313782
translation,389,104,hyperparameters,first stage training,has,number of negative samples,first stage training has number of negative samples,0.534529983997345
translation,389,104,hyperparameters,hyperparameters,For,first stage training,hyperparameters For first stage training,0.5261132717132568
translation,389,117,hyperparameters,"implementations of scikit-learn ( pedregosa et al. , 2011 )",for,logistic regression and random forest experiments,"implementations of scikit-learn ( pedregosa et al. , 2011 ) for logistic regression and random forest experiments",0.5545992255210876
translation,389,117,hyperparameters,hyperparameters,use,"implementations of scikit-learn ( pedregosa et al. , 2011 )","hyperparameters use implementations of scikit-learn ( pedregosa et al. , 2011 )",0.5719709396362305
translation,389,119,hyperparameters,sciq and mcql datasets,train with,500 rounds,sciq and mcql datasets train with 500 rounds,0.7423015236854553
translation,389,119,hyperparameters,sciq and mcql datasets,train with,step size shrinkage,sciq and mcql datasets train with step size shrinkage,0.7306105494499207
translation,389,119,hyperparameters,sciq and mcql datasets,train with,minimum loss reduction,sciq and mcql datasets train with minimum loss reduction,0.7067750096321106
translation,389,119,hyperparameters,500 rounds,of,boosting,500 rounds of boosting,0.6092914938926697
translation,389,119,hyperparameters,step size shrinkage,of,0.1,step size shrinkage of 0.1,0.6008502244949341
translation,389,119,hyperparameters,maximum depth,of,30,maximum depth of 30,0.6690465211868286
translation,389,119,hyperparameters,minimum child weight,of,0.1,minimum child weight of 0.1,0.5803463459014893
translation,389,119,hyperparameters,minimum loss reduction,of,1.0,minimum loss reduction of 1.0,0.5857948064804077
translation,389,119,hyperparameters,1.0,for,partition,1.0 for partition,0.6882707476615906
translation,389,119,hyperparameters,hyperparameters,For,sciq and mcql datasets,hyperparameters For sciq and mcql datasets,0.5764410495758057
translation,389,125,hyperparameters,nn - based models,implemented with,"tensorflow ( abadi et al. , 2016 )","nn - based models implemented with tensorflow ( abadi et al. , 2016 )",0.6240395307540894
translation,389,125,hyperparameters,hyperparameters,has,nn - based models,hyperparameters has nn - based models,0.5287964344024658
translation,389,126,hyperparameters,generator,uniformly select,k = 512 candidates,generator uniformly select k = 512 candidates,0.7748391628265381
translation,389,126,hyperparameters,generator,sample,16 distractors,generator sample 16 distractors,0.7056384682655334
translation,389,126,hyperparameters,hyperparameters,When training,generator,hyperparameters When training generator,0.713840126991272
translation,389,127,hyperparameters,temperature,set to,5,temperature set to 5,0.7689851522445679
translation,389,127,hyperparameters,hyperparameters,has,temperature,hyperparameters has temperature,0.5350812077522278
translation,389,129,hyperparameters,word embeddings,initialized using,pre-trained glove,word embeddings initialized using pre-trained glove,0.7382447719573975
translation,389,129,hyperparameters,embedding size,is,300,embedding size is 300,0.624373197555542
translation,389,129,hyperparameters,pre-trained glove,has,embedding size,pre-trained glove has embedding size,0.5565869808197021
translation,389,129,hyperparameters,hyperparameters,has,word embeddings,hyperparameters has word embeddings,0.4784160554409027
translation,389,129,hyperparameters,hyperparameters,has,embedding size,hyperparameters has embedding size,0.4976881444454193
translation,389,130,hyperparameters,"adam algorithm ( kingma and ba , 2015 )",with,learning rate,"adam algorithm ( kingma and ba , 2015 ) with learning rate",0.6075246930122375
translation,389,130,hyperparameters,"adam algorithm ( kingma and ba , 2015 )",with,weight decay,"adam algorithm ( kingma and ba , 2015 ) with weight decay",0.6154656410217285
translation,389,130,hyperparameters,learning rate,of,1e - 4,learning rate of 1e - 4,0.6311931014060974
translation,389,130,hyperparameters,weight decay,of,1e - 6,weight decay of 1e - 6,0.6190267205238342
translation,389,130,hyperparameters,hyperparameters,optimized using,"adam algorithm ( kingma and ba , 2015 )","hyperparameters optimized using adam algorithm ( kingma and ba , 2015 )",0.690205454826355
translation,389,20,model,two types of models,for,dg,two types of models for dg,0.646644115447998
translation,389,20,model,model,propose,two types of models,model propose two types of models,0.6582664847373962
translation,389,128,model,scoring functions,based on,decomposable attention model,scoring functions based on decomposable attention model,0.6424654722213745
translation,389,128,model,model,has,scoring functions,model has scoring functions,0.5317414999008179
translation,389,144,results,proposed ranking mod- els,perform better,"unsupervised similarity - based methods ( pmi , ed , and emb sim )","proposed ranking mod- els perform better unsupervised similarity - based methods ( pmi , ed , and emb sim )",0.6634206175804138
translation,389,144,results,"unsupervised similarity - based methods ( pmi , ed , and emb sim )",has,most of the time,"unsupervised similarity - based methods ( pmi , ed , and emb sim ) has most of the time",0.5670661330223083
translation,389,145,results,ensem - ble models - rf and lm,have,comparable perfor - mance,ensem - ble models - rf and lm have comparable perfor - mance,0.5494343042373657
translation,389,145,results,significantly better,than,other meth - ods,significantly better than other meth - ods,0.5911362767219543
translation,389,145,results,results,has,ensem - ble models - rf and lm,results has ensem - ble models - rf and lm,0.5355018377304077
translation,389,147,results,nn per- forms,worse than,feature - based models,nn per- forms worse than feature - based models,0.6906710863113403
translation,389,147,results,results,has,nn per- forms,results has nn per- forms,0.5722446441650391
translation,389,159,results,ed,has,relatively good performance,ed has relatively good performance,0.5961700081825256
translation,390,102,ablation-analysis,normalization,of,feature values,normalization of feature values,0.5677074193954468
translation,390,102,ablation-analysis,feature values,tendency of,enhancing,feature values tendency of enhancing,0.7447936534881592
translation,390,102,ablation-analysis,enhancing,has,achieved map scores,enhancing has achieved map scores,0.5933722257614136
translation,390,102,ablation-analysis,ablation analysis,has,normalization,ablation analysis has normalization,0.5343902707099915
translation,390,24,model,supervised learning approach,over,word2vec features,supervised learning approach over word2vec features,0.5942935943603516
translation,390,24,model,supervised learning approach,to build,ranking model,supervised learning approach to build ranking model,0.6580877304077148
translation,390,24,model,word2vec features,extracted from,collection of questions,word2vec features extracted from collection of questions,0.5511012673377991
translation,390,24,model,model,leveraged,supervised learning approach,model leveraged supervised learning approach,0.6560380458831787
translation,390,11,results,our primary submission,achieved,29.7 % improvement,our primary submission achieved 29.7 % improvement,0.6720201969146729
translation,390,11,results,29.7 % improvement,over,map score,29.7 % improvement over map score,0.6548834443092346
translation,390,11,results,map score,of,baseline,map score of baseline,0.5710239410400391
translation,390,11,results,results,has,our primary submission,results has our primary submission,0.5467225909233093
translation,390,13,results,covariance word embedding features,raised,improvement,covariance word embedding features raised improvement,0.6293792724609375
translation,390,13,results,improvement,over,baseline,improvement over baseline,0.7266895771026611
translation,390,13,results,baseline,to,37.9 %,baseline to 37.9 %,0.5149834156036377
translation,390,13,results,results,Integrating,covariance word embedding features,results Integrating covariance word embedding features,0.6332080960273743
translation,390,88,results,performed better,than,our primary and contrastive - 1 submissions,performed better than our primary and contrastive - 1 submissions,0.5695575475692749
translation,390,88,results,map score,of,39.07,map score of 39.07,0.548580527305603
translation,390,88,results,improvement,of,31.3 %,improvement of 31.3 %,0.5652587413787842
translation,390,88,results,31.3 %,over,baseline,31.3 % over baseline,0.6239098310470581
translation,391,165,ablation-analysis,striking a balance,between,two styles,striking a balance between two styles,0.6748019456863403
translation,391,165,ablation-analysis,two styles,in,generating tokens,two styles in generating tokens,0.5309213995933533
translation,391,165,ablation-analysis,generating tokens,at,each step,generating tokens at each step,0.5581526160240173
translation,391,124,baselines,qg models,based on,"bert - hlsqg ( chan and fan , 2019 )","qg models based on bert - hlsqg ( chan and fan , 2019 )",0.6963055729866028
translation,391,124,baselines,baselines,has,qg models,baselines has qg models,0.5626081824302673
translation,391,126,experimental-setup,qa model,based on,bert - base and bert - large,qa model based on bert - base and bert - large,0.6926988363265991
translation,391,126,experimental-setup,bert - base and bert - large,implemented by,"hugging face ( wolf et al. , 2019 )","bert - base and bert - large implemented by hugging face ( wolf et al. , 2019 )",0.6962584257125854
translation,391,126,experimental-setup,experimental setup,has,discriminator model,experimental setup has discriminator model,0.5238585472106934
translation,391,126,experimental-setup,experimental setup,has,qa model,experimental setup has qa model,0.575002133846283
translation,391,130,experimental-setup,copy-type and lm - type assistants,pre-trained on,10 k instances,copy-type and lm - type assistants pre-trained on 10 k instances,0.7781293392181396
translation,391,130,experimental-setup,experimental setup,has,copy-type and lm - type assistants,experimental setup has copy-type and lm - type assistants,0.5188995003700256
translation,391,131,experimental-setup,discriminator model,trained on,fully balanced training set,discriminator model trained on fully balanced training set,0.7467330098152161
translation,391,131,experimental-setup,fully balanced training set,of,70 k instances,fully balanced training set of 70 k instances,0.5526438355445862
translation,391,131,experimental-setup,experimental setup,has,discriminator model,experimental setup has discriminator model,0.5238585472106934
translation,391,133,experimental-setup,qa models,trained with,10 k instances,qa models trained with 10 k instances,0.7386345267295837
translation,391,133,experimental-setup,experimental setup,has,qa models,experimental setup has qa models,0.5579879879951477
translation,391,132,experiments,student qg model,trained on,10 k instances,student qg model trained on 10 k instances,0.7626017332077026
translation,391,132,experiments,10 k instances,generated by,teacher module,10 k instances generated by teacher module,0.6780614256858826
translation,391,7,model,novel regularization method,based on,teacher -student architecture,novel regularization method based on teacher -student architecture,0.6595255732536316
translation,391,7,model,novel regularization method,to avoid,bias,novel regularization method to avoid bias,0.6770532131195068
translation,391,7,model,bias,toward,particular question generation strategy,bias toward particular question generation strategy,0.6788960695266724
translation,391,7,model,model,propose,novel regularization method,model propose novel regularization method,0.6892793774604797
translation,391,33,model,teacher,employs,two qg models,teacher employs two qg models,0.6330136060714722
translation,391,33,model,teacher,adopts,semantic -level regularization process,teacher adopts semantic -level regularization process,0.6481642127037048
translation,391,33,model,semantic -level regularization process,to avoid,bias,semantic -level regularization process to avoid bias,0.6467339992523193
translation,391,33,model,bias,toward,particular question generation strategy,bias toward particular question generation strategy,0.6788960695266724
translation,391,33,model,two qg models,has,assistants ),two qg models has assistants ),0.6270051002502441
translation,391,33,model,two qg models,has,lm -type and copy-type,two qg models has lm -type and copy-type,0.6041908264160156
translation,391,33,model,model,has,teacher,model has teacher,0.6139839291572571
translation,391,34,model,student module,learns how to make,balance,student module learns how to make balance,0.6793810725212097
translation,391,34,model,balance,between,two extreme types of questions,balance between two extreme types of questions,0.6584423780441284
translation,391,34,model,two extreme types of questions,generated by,assistants,two extreme types of questions generated by assistants,0.6415835618972778
translation,391,34,model,model,has,student module,model has student module,0.5899577140808105
translation,391,35,model,regularization,helps suppress,copying behavior and semantic drifts,regularization helps suppress copying behavior and semantic drifts,0.7205145955085754
translation,391,35,model,regularization,generate,more versatile questions,regularization generate more versatile questions,0.6566606760025024
translation,391,35,model,copying behavior and semantic drifts,of,existing qg models,copying behavior and semantic drifts of existing qg models,0.5472535490989685
translation,391,35,model,more versatile questions,compatible with,given contexts,more versatile questions compatible with given contexts,0.7196294665336609
translation,391,35,model,model,has,regularization,model has regularization,0.5611564517021179
translation,391,36,model,novel generation method,based on,teacher -student architecture,novel generation method based on teacher -student architecture,0.6900330781936646
translation,391,36,model,novel generation method,that regularizes,two generation models,novel generation method that regularizes two generation models,0.7159841656684875
translation,391,36,model,two generation models,in,unsupervised setting,two generation models in unsupervised setting,0.5214296579360962
translation,391,94,model,assistant modules,pre-trained with,cross-entropy,assistant modules pre-trained with cross-entropy,0.7631317377090454
translation,391,94,model,assistant modules,source of,knowledge,assistant modules source of knowledge,0.5501025319099426
translation,391,94,model,cross-entropy,to implement,question generators,cross-entropy to implement question generators,0.7191261649131775
translation,391,94,model,knowledge,to,regularization module,knowledge to regularization module,0.5316475629806519
translation,391,94,model,model,has,assistant modules,model has assistant modules,0.5969175100326538
translation,391,201,model,teacher module,regularizes,two question generation models,teacher module regularizes two question generation models,0.7127387523651123
translation,391,201,model,model,devise,teacher module,model devise teacher module,0.7737690210342407
translation,391,204,model,teacher,composed of,two distinctive qg models,teacher composed of two distinctive qg models,0.6783962845802307
translation,391,204,model,teacher,composed of,regularization module,teacher composed of regularization module,0.6999576687812805
translation,391,204,model,two distinctive qg models,as,assistants,two distinctive qg models as assistants,0.5802574157714844
translation,391,204,model,regularization module,attempts to stay,unbiased,regularization module attempts to stay unbiased,0.6390270590782166
translation,391,204,model,unbiased,between,two styles of qg,unbiased between two styles of qg,0.6836845874786377
translation,391,204,model,model,has,teacher,model has teacher,0.6139839291572571
translation,391,142,results,baselines,on,all the qa datasets,baselines on all the qa datasets,0.49839428067207336
translation,391,142,results,our model,has,significantly outperforms,our model has significantly outperforms,0.6134821176528931
translation,391,142,results,significantly outperforms,has,baselines,significantly outperforms has baselines,0.6014122366905212
translation,391,153,results,squad,confirm that,token - level regularization ( 58.2 in f1 ),squad confirm that token - level regularization ( 58.2 in f1 ),0.5581862330436707
translation,391,153,results,token - level regularization ( 58.2 in f1 ),is,more effective,token - level regularization ( 58.2 in f1 ) is more effective,0.48385611176490784
translation,391,153,results,more effective,than,instance - level regularization,more effective than instance - level regularization,0.5220243334770203
translation,391,153,results,instance - level regularization,implemented by,  qg,instance - level regularization implemented by   qg,0.6941865682601929
translation,391,153,results,  qg,trained on,copy and lm,  qg trained on copy and lm,0.7990670800209045
translation,391,153,results,instance - level regularization,has,53.9 ),instance - level regularization has 53.9 ),0.5186841487884521
translation,391,153,results,copy and lm,has,model,copy and lm has model,0.597089409828186
translation,391,157,results,discriminator model,achieves,accuracy,discriminator model achieves accuracy,0.6562203764915466
translation,391,157,results,accuracy,of,95.53 %,accuracy of 95.53 %,0.5461848974227905
translation,391,157,results,95.53 %,on,10 k dev set,95.53 % on 10 k dev set,0.5465212464332581
translation,391,157,results,training,has,discriminator model,training has discriminator model,0.5365726351737976
translation,391,157,results,results,After,training,results After training,0.690481960773468
translation,391,157,results,results,has,discriminator model,results has discriminator model,0.5067229270935059
translation,391,161,results,discriminator model,has,outperforms,discriminator model has outperforms,0.6191484332084656
translation,391,161,results,outperforms,has,other two models,outperforms has other two models,0.5785214304924011
translation,391,164,results,frequency - based model,performs,better,frequency - based model performs better,0.6475360989570618
translation,391,164,results,better,than,random model,better than random model,0.6142482161521912
translation,391,173,results,uqg,is,remarkably useful,uqg is remarkably useful,0.5661465525627136
translation,391,173,results,remarkably useful,in,no-data or small - data regimes,remarkably useful in no-data or small - data regimes,0.5348218083381653
translation,391,173,results,results,has,uqg,results has uqg,0.6026840806007385
translation,391,174,results,proposed qg method,added to,bert - large qa model,proposed qg method added to bert - large qa model,0.6872106790542603
translation,391,174,results,performance boost,of,about 10 to 20 points,performance boost of about 10 to 20 points,0.5680698156356812
translation,391,174,results,results,has,proposed qg method,results has proposed qg method,0.5796610116958618
translation,392,14,baselines,two unsupervised domain adaptation methods,to incorporate,unlabeled mrda utterances,two unsupervised domain adaptation methods to incorporate unlabeled mrda utterances,0.677650511264801
translation,392,14,baselines,unlabeled mrda utterances,into,text - based question detector,unlabeled mrda utterances into text - based question detector,0.574253499507904
translation,392,14,baselines,baselines,experiment with,two unsupervised domain adaptation methods,baselines experiment with two unsupervised domain adaptation methods,0.6669801473617554
translation,392,31,baselines,two domain adaptation approaches,to utilize,unlabeled speech data,two domain adaptation approaches to utilize unlabeled speech data,0.6069799661636353
translation,392,31,baselines,blitzer et al . 's structural correspondence learning ( scl ),is,featurelearning method,blitzer et al . 's structural correspondence learning ( scl ) is featurelearning method,0.5429728031158447
translation,392,31,baselines,scl,is,featurelearning method,scl is featurelearning method,0.5530683398246765
translation,392,31,baselines,baselines,compare,two domain adaptation approaches,baselines compare two domain adaptation approaches,0.6403992176055908
translation,392,12,experiments,question detection,in,meeting recorder dialog act corpus ( mrda ),question detection in meeting recorder dialog act corpus ( mrda ),0.5215989947319031
translation,392,12,experiments,question detection,using,text sentences,question detection using text sentences,0.6205660700798035
translation,392,12,experiments,text sentences,with,question marks,text sentences with question marks,0.6291295886039734
translation,392,12,experiments,question marks,in,wikipedia   talk   pages,question marks in wikipedia   talk   pages,0.4989177882671356
translation,392,6,results,text-trained model,achieves,over 90 %,text-trained model achieves over 90 %,0.6552209258079529
translation,392,6,results,over 90 %,of,performance,over 90 % of performance,0.6059802174568176
translation,392,6,results,domain-matched model,including,prosodic features,domain-matched model including prosodic features,0.6863937377929688
translation,392,6,results,does especially poorly,on,declarative questions,does especially poorly on declarative questions,0.5324443578720093
translation,392,6,results,results,has,text-trained model,results has text-trained model,0.5569696426391602
translation,392,72,results,prosody,does best,yes -no and declarative,prosody does best yes -no and declarative,0.7115662693977356
translation,392,72,results,mrda - trained system,has,prosody,mrda - trained system has prosody,0.5622147917747498
translation,392,72,results,results,For,mrda - trained system,results For mrda - trained system,0.6139811277389526
translation,392,101,results,analysis,of,adapted systems,analysis of adapted systems,0.6154391169548035
translation,392,101,results,adapted systems,suggests,prosody features,adapted systems suggests prosody features,0.6088075637817383
translation,392,101,results,prosody features,to improve,performance,prosody features to improve performance,0.6218884587287903
translation,392,101,results,utilized,to improve,performance,utilized to improve performance,0.7172085046768188
translation,392,101,results,performance,in,both methods,performance in both methods,0.4739112854003906
translation,392,101,results,results,has,analysis,results has analysis,0.5002367496490479
translation,393,248,baselines,baselines,For,visual7w,baselines For visual7w,0.6541573405265808
translation,393,250,baselines,mlp method,uses,"( image , question , answer ) triplets","mlp method uses ( image , question , answer ) triplets",0.6055338978767395
translation,393,250,baselines,"( image , question , answer ) triplets",to score,answer choices,"( image , question , answer ) triplets to score answer choices",0.6824005842208862
translation,393,250,baselines,baselines,has,mlp method,baselines has mlp method,0.5615783333778381
translation,393,251,baselines,baselines,For,clevr,baselines For clevr,0.5996741652488708
translation,393,252,baselines,n2nmn,predict,layout,n2nmn predict layout,0.7203440070152283
translation,393,252,baselines,n2nmn,compose,network,n2nmn compose network,0.7395918369293213
translation,393,252,baselines,layout,based on,question,layout based on question,0.6754302382469177
translation,393,252,baselines,network,using,set of neural modules,network using set of neural modules,0.6993365287780762
translation,393,252,baselines,baselines,has,n2nmn,baselines has n2nmn,0.5747638940811157
translation,393,253,baselines,cnn +lstm + rn,learns to,infer,cnn +lstm + rn learns to infer,0.7035884261131287
translation,393,253,baselines,relation,using,neural network model,relation using neural network model,0.7074740529060364
translation,393,253,baselines,neural network model,called,relation networks,neural network model called relation networks,0.6247470378875732
translation,393,253,baselines,infer,has,relation,infer has relation,0.6197385191917419
translation,393,253,baselines,baselines,has,cnn +lstm + rn,baselines has cnn +lstm + rn,0.5268944501876831
translation,393,238,experimental-setup,object detector,with,96 classes,object detector with 96 classes,0.5859934687614441
translation,393,238,experimental-setup,object detector,trained using,all combinations,object detector trained using all combinations,0.7201225757598877
translation,393,238,experimental-setup,all combinations,by,tensorflow object detection api,all combinations by tensorflow object detection api,0.495660662651062
translation,393,238,experimental-setup,of the attributes,by,tensorflow object detection api,of the attributes by tensorflow object detection api,0.5632905960083008
translation,393,238,experimental-setup,all combinations,has,of the attributes,all combinations has of the attributes,0.5931857228279114
translation,393,238,experimental-setup,experimental setup,has,object detector,experimental setup has object detector,0.48774459958076477
translation,393,239,experiments,faster r-cnn nasnet,trained on,ms - coco dataset,faster r-cnn nasnet trained on ms - coco dataset,0.7331163883209229
translation,393,5,model,mn - gmn,uses,graph structure,mn - gmn uses graph structure,0.6123336553573608
translation,393,5,model,mn - gmn,applies,recently proposed powerful graph neural network model,mn - gmn applies recently proposed powerful graph neural network model,0.5544822216033936
translation,393,5,model,graph structure,with,different region features,graph structure with different region features,0.6083734631538391
translation,393,5,model,different region features,as,node attributes,different region features as node attributes,0.501642107963562
translation,393,5,model,recently proposed powerful graph neural network model,to reason about,objects and their interactions,recently proposed powerful graph neural network model to reason about objects and their interactions,0.7420352697372437
translation,393,5,model,objects and their interactions,in,image,objects and their interactions in image,0.5414377450942993
translation,393,5,model,model,has,mn - gmn,model has mn - gmn,0.6010311245918274
translation,393,6,model,input module,of,mn - gmn,input module of mn - gmn,0.5899909138679504
translation,393,6,model,mn - gmn,generates,set of visual features,mn - gmn generates set of visual features,0.6307017803192139
translation,393,6,model,set of visual features,set of,encoded region-grounded captions ( rgcs ),set of visual features set of encoded region-grounded captions ( rgcs ),0.6914145946502686
translation,393,6,model,encoded region-grounded captions ( rgcs ),for,image,encoded region-grounded captions ( rgcs ) for image,0.6198055744171143
translation,393,6,model,model,has,input module,model has input module,0.5717214941978455
translation,393,16,model,new neural network architecture,for,vqa,new neural network architecture for vqa,0.6406590342521667
translation,393,16,model,model,proposes,new neural network architecture,model proposes new neural network architecture,0.6977526545524597
translation,393,37,model,flexibility of gns,to combine,information,flexibility of gns to combine information,0.6949160695075989
translation,393,37,model,information,from,two different sources,information from two different sources,0.5699203610420227
translation,393,37,model,visual features,from,different image regions,visual features from different image regions,0.5311805605888367
translation,393,37,model,textual features,based on,region-grounded captions ( rgcs ),textual features based on region-grounded captions ( rgcs ),0.6155322194099426
translation,393,37,model,model,exploit,flexibility of gns,model exploit flexibility of gns,0.7466542720794678
translation,393,38,model,rgc detector,learned by,transfer learning,rgc detector learned by transfer learning,0.747611403465271
translation,393,38,model,transfer learning,from,dataset,transfer learning from dataset,0.5545569062232971
translation,393,38,model,dataset,with,region-grounded captions,dataset with region-grounded captions,0.5698163509368896
translation,393,38,model,model,has,rgc detector,model has rgc detector,0.5604956150054932
translation,393,249,model,mcb,leverages,visual genome qa pairs,mcb leverages visual genome qa pairs,0.7090578079223633
translation,393,249,model,mcb,leverages,152 - layer resnet,mcb leverages 152 - layer resnet,0.6650617122650146
translation,393,249,model,visual genome qa pairs,as,additional training data,visual genome qa pairs as additional training data,0.4735799729824066
translation,393,249,model,152 - layer resnet,as,pretrained model,152 - layer resnet as pretrained model,0.48256272077560425
translation,393,249,model,model,has,mcb,model has mcb,0.5816494822502136
translation,393,254,model,program - gen,exploits,supervision,program - gen exploits supervision,0.779977023601532
translation,393,254,model,supervision,from,functional programming,supervision from functional programming,0.5676451921463013
translation,393,254,model,supervision,to generate,clevr questions,supervision to generate clevr questions,0.6431071162223816
translation,393,254,model,model,has,program - gen,model has program - gen,0.606521487236023
translation,393,256,model,model,implement,several lesion architectures,model implement several lesion architectures,0.6893969774246216
translation,393,51,results,architecture,seen as,multimodal relational extension,architecture seen as multimodal relational extension,0.6140893697738647
translation,393,51,results,multimodal relational extension,to,dmn,multimodal relational extension to dmn,0.5540301203727722
translation,393,51,results,state - of - the - art,on,three vqa datasets,state - of - the - art on three vqa datasets,0.46777263283729553
translation,393,51,results,architecture,has,rivals,architecture has rivals,0.5892937779426575
translation,393,51,results,rivals,has,state - of - the - art,rivals has state - of - the - art,0.5653420090675354
translation,393,51,results,results,has,architecture,results has architecture,0.5793547034263611
translation,393,271,results,all question types,has,n - gmn,all question types has n - gmn,0.6351851224899292
translation,393,271,results,n - gmn,has,outperforms,n - gmn has outperforms,0.6432212591171265
translation,393,271,results,outperforms,has,mn + resnet,outperforms has mn + resnet,0.6108574867248535
translation,393,271,results,results,Across,all question types,results Across all question types,0.6257266402244568
translation,393,273,results,mn - gmn ?,has,outperforms,mn - gmn ? has outperforms,0.6740977168083191
translation,393,273,results,outperforms,has,n-gmn,outperforms has n-gmn,0.6237518787384033
translation,393,273,results,results,has,mn - gmn ?,results has mn - gmn ?,0.5692439675331116
translation,393,275,results,useful,for,answering,useful for answering,0.6791961193084717
translation,393,275,results,answering,has,other and yes / no question types,answering has other and yes / no question types,0.5993912816047668
translation,393,275,results,results,has,rgcs,results has rgcs,0.48511406779289246
translation,393,276,results,our full model mn - gmn,has,outperforms,our full model mn - gmn has outperforms,0.6138616800308228
translation,393,276,results,outperforms,has,mn - gmn,outperforms has mn - gmn,0.604472815990448
translation,393,276,results,results,has,our full model mn - gmn,results has our full model mn - gmn,0.541018545627594
translation,393,278,results,full model 's accuracy,higher than,baselines,full model 's accuracy higher than baselines,0.6740710139274597
translation,393,278,results,results,has,full model 's accuracy,results has full model 's accuracy,0.5180766582489014
translation,393,279,results,"n-gmn , mn - gmn ? , and mn - gmn",has,outperform,"n-gmn , mn - gmn ? , and mn - gmn has outperform",0.5743686556816101
translation,393,279,results,outperform,has,baselines,outperform has baselines,0.6363358497619629
translation,393,279,results,baselines,has,mlp,baselines has mlp,0.6334225535392761
translation,393,279,results,results,on,visual7w,results on visual7w,0.5321012735366821
translation,393,280,results,n- gmn +,on,clevr,n- gmn + on clevr,0.6374236345291138
translation,393,280,results,n- gmn +,are,competitive,n- gmn + are competitive,0.6679198741912842
translation,393,280,results,competitive,with,state- of- the- art ramen,competitive with state- of- the- art ramen,0.7106073498725891
translation,393,280,results,competitive,with,program - gen,competitive with program - gen,0.7163547873497009
translation,393,280,results,competitive,with,ns - vqa,competitive with ns - vqa,0.7076998949050903
translation,393,280,results,results,for,n- gmn +,results for n- gmn +,0.6008233428001404
translation,394,182,ablation-analysis,clear gap,between,top - 3/5 and top - 1 ds - qa performance ( 10 - 20 % ),clear gap between top - 3/5 and top - 1 ds - qa performance ( 10 - 20 % ),0.6470245718955994
translation,394,185,ablation-analysis,information,from,all informative paragraphs,information from all informative paragraphs,0.5309999585151672
translation,394,185,ablation-analysis,information,effectively enhance,our model,information effectively enhance our model,0.6256103515625
translation,394,185,ablation-analysis,our model,in,ds - qa,our model in ds - qa,0.5822492837905884
translation,394,185,ablation-analysis,ablation analysis,aggregating,information,ablation analysis aggregating information,0.6878432631492615
translation,394,117,baselines,reading comprehension model,performs,multiple hops,reading comprehension model performs multiple hops,0.5704654455184937
translation,394,117,baselines,multiple hops,over,paragraph,multiple hops over paragraph,0.7044238448143005
translation,394,117,baselines,multiple hops,with,gated attention mechanism,multiple hops with gated attention mechanism,0.6317112445831299
translation,394,117,baselines,paragraph,with,gated attention mechanism,paragraph with gated attention mechanism,0.6522203683853149
translation,394,117,baselines,reading comprehension model,with,bi-directional attention flow network,reading comprehension model with bi-directional attention flow network,0.5465443730354309
translation,394,117,baselines,"ga ( dhingra et al. , 2017a )",has,reading comprehension model,"ga ( dhingra et al. , 2017a ) has reading comprehension model",0.541019082069397
translation,394,117,baselines,"bidaf ( seo et al. , 2017 )",has,reading comprehension model,"bidaf ( seo et al. , 2017 ) has reading comprehension model",0.5389392375946045
translation,394,117,baselines,baselines,select,several public models,baselines select several public models,0.6211621165275574
translation,394,118,baselines,reinforced system learning,to re-write,questions,reinforced system learning to re-write questions,0.7677018642425537
translation,394,118,baselines,reinforced system learning,aggregate,answers,reinforced system learning aggregate answers,0.7951369881629944
translation,394,118,baselines,answers,generated by,re-written questions,answers generated by re-written questions,0.6783065795898438
translation,394,118,baselines,reinforced model,making use of,ranker,reinforced model making use of ranker,0.7497087717056274
translation,394,118,baselines,ranker,for selecting,most confident paragraph,ranker for selecting most confident paragraph,0.6952053308486938
translation,394,118,baselines,most confident paragraph,to train,reading comprehension model,most confident paragraph to train reading comprehension model,0.6933426856994629
translation,394,118,baselines,aqa,has,reinforced system learning,aqa has reinforced system learning,0.5712279677391052
translation,394,118,baselines,aqa,has,reinforced model,aqa has reinforced model,0.593508780002594
translation,394,118,baselines,r 3,has,"wang et al. , 2018a )","r 3 has wang et al. , 2018a )",0.5364477634429932
translation,394,118,baselines,r 3,has,reinforced model,r 3 has reinforced model,0.6063507199287415
translation,394,118,baselines,"wang et al. , 2018a )",has,reinforced model,"wang et al. , 2018a ) has reinforced model",0.5311964154243469
translation,394,118,baselines,baselines,has,aqa,baselines has aqa,0.5698035359382629
translation,394,119,baselines,uniform distribution,to,paragraph selection,uniform distribution to paragraph selection,0.5712358951568604
translation,394,149,experiments,or two retrieved paragraphs,actually contain,correct answer,or two retrieved paragraphs actually contain correct answer,0.6222431659698486
translation,394,123,hyperparameters,hidden size,of,"lstm n ? { 32 , 64 , 128 , ? ? ? , 512 }","hidden size of lstm n ? { 32 , 64 , 128 , ? ? ? , 512 }",0.5643895864486694
translation,394,123,hyperparameters,hidden size,of,lstm layers,hidden size of lstm layers,0.5470054149627686
translation,394,123,hyperparameters,hidden size,number of,lstm layers,hidden size number of lstm layers,0.6493631601333618
translation,394,123,hyperparameters,lstm layers,for,document and question encoder,lstm layers for document and question encoder,0.5417876839637756
translation,394,123,hyperparameters,document and question encoder,among,"{ 1 , 2 , 3 , 4 }","document and question encoder among { 1 , 2 , 3 , 4 }",0.5859023332595825
translation,394,123,hyperparameters,regularization weight,among,"0.1 , 0.5 , 1.0 , 2.0 }","regularization weight among 0.1 , 0.5 , 1.0 , 2.0 }",0.5451596975326538
translation,394,123,hyperparameters,regularization weight,{,"0.1 , 0.5 , 1.0 , 2.0 }","regularization weight { 0.1 , 0.5 , 1.0 , 2.0 }",0.7098000645637512
translation,394,123,hyperparameters,batch size,among,"{ 4 , 8 , 16 , 32 , 64 , 128 }","batch size among { 4 , 8 , 16 , 32 , 64 , 128 }",0.5807815194129944
translation,394,123,hyperparameters,hyperparameters,select,hidden size,hyperparameters select hidden size,0.6623082756996155
translation,394,123,hyperparameters,hyperparameters,select,regularization weight,hyperparameters select regularization weight,0.6252545118331909
translation,394,126,hyperparameters,our +full model,first initialized by,pre-training,our +full model first initialized by pre-training,0.6627314686775208
translation,394,126,hyperparameters,our +full model,set,iteration number,our +full model set iteration number,0.6843795776367188
translation,394,126,hyperparameters,pre-training,using,our +avg model,pre-training using our +avg model,0.6723651885986328
translation,394,126,hyperparameters,iteration number,over,all the training data,iteration number over all the training data,0.6429661512374878
translation,394,126,hyperparameters,all the training data,as,10,all the training data as 10,0.5961548686027527
translation,394,126,hyperparameters,hyperparameters,set,iteration number,hyperparameters set iteration number,0.6492393016815186
translation,394,127,hyperparameters,pre-trained word embeddings,use,300 - dimensional glove,pre-trained word embeddings use 300 - dimensional glove,0.5792343020439148
translation,394,127,hyperparameters,word embeddings,learned from,840b web crawl data,word embeddings learned from 840b web crawl data,0.6642526984214783
translation,394,127,hyperparameters,300 - dimensional glove,has,word embeddings,300 - dimensional glove has word embeddings,0.5366740822792053
translation,394,127,hyperparameters,hyperparameters,For,pre-trained word embeddings,hyperparameters For pre-trained word embeddings,0.46875226497650146
translation,394,8,model,novel ds - qa model,employs,paragraph selector,novel ds - qa model employs paragraph selector,0.6001224517822266
translation,394,8,model,paragraph selector,to filter out,noisy paragraphs,paragraph selector to filter out noisy paragraphs,0.7125136256217957
translation,394,8,model,paragraph reader,to extract,correct answer,paragraph reader to extract correct answer,0.678424596786499
translation,394,8,model,correct answer,from,denoised paragraphs,correct answer from denoised paragraphs,0.5469223260879517
translation,394,8,model,model,propose,novel ds - qa model,model propose novel ds - qa model,0.6892904043197632
translation,394,31,model,coarseto-fine denoising model,for,ds - qa,coarseto-fine denoising model for ds - qa,0.6124264597892761
translation,394,31,model,model,propose,coarseto-fine denoising model,model propose coarseto-fine denoising model,0.6366897225379944
translation,394,191,model,paragraph selector,to skim over,paragraphs,paragraph selector to skim over paragraphs,0.7075775265693665
translation,394,191,model,paragraph reader,to perform,intensive reading,paragraph reader to perform intensive reading,0.6694146394729614
translation,394,191,model,intensive reading,on,selected paragraphs,intensive reading on selected paragraphs,0.568444013595581
translation,394,197,model,background knowledge,such as,factual knowledge,background knowledge such as factual knowledge,0.5913172364234924
translation,394,197,model,background knowledge,has,common sense knowledge,background knowledge has common sense knowledge,0.5014818906784058
translation,394,197,model,model,has,background knowledge,model has background knowledge,0.5287304520606995
translation,394,130,results,our rnn paragraph selector,leads to,statistically significant improvements,our rnn paragraph selector leads to statistically significant improvements,0.6469181180000305
translation,394,130,results,statistically significant improvements,on,quasar -t and searchqa,statistically significant improvements on quasar -t and searchqa,0.5544240474700928
translation,394,130,results,results,has,our rnn paragraph selector,results has our rnn paragraph selector,0.5342130064964294
translation,394,131,results,our+full,which uses,mlp paragraph selector,our+full which uses mlp paragraph selector,0.7279065847396851
translation,394,131,results,our+full,performs,worse,our+full performs worse,0.6380226612091064
translation,394,131,results,worse,on,quasar -t dataset,worse on quasar -t dataset,0.5214857459068298
translation,394,131,results,worse,compared to,our+avg,worse compared to our+avg,0.6372907161712646
translation,394,131,results,results,Note,our+full,results Note our+full,0.5931037664413452
translation,394,136,results,all models,with,sum or max paragraph readers,all models with sum or max paragraph readers,0.6356546878814697
translation,394,136,results,sum or max paragraph readers,have,comparable performance,sum or max paragraph readers have comparable performance,0.5390179753303528
translation,394,136,results,comparable performance,in,most cases,comparable performance in most cases,0.5666742324829102
translation,394,136,results,our+avg with max reader,has,about 3 % increment,our+avg with max reader has about 3 % increment,0.6180021166801453
translation,394,136,results,about 3 % increment,compared to,with sum reader,about 3 % increment compared to with sum reader,0.7269781231880188
translation,394,136,results,with sum reader,on,searchqa dataset,with sum reader on searchqa dataset,0.5736331343650818
translation,394,136,results,our+avg with max reader,has,about 3 % increment,our+avg with max reader has about 3 % increment,0.6180021166801453
translation,394,145,results,rich information,of,all retrieved paragraphs,rich information of all retrieved paragraphs,0.5740411281585693
translation,394,145,results,answer,to,question,answer to question,0.6327336430549622
translation,394,145,results,results,incorporating,rich information,results incorporating rich information,0.6668947339057922
translation,394,146,results,all datasets,has,our+full model,all datasets has our+full model,0.5513297319412231
translation,394,146,results,our+full model,has,outperforms,our+full model has outperforms,0.6286240220069885
translation,394,146,results,outperforms,has,our +avg model,outperforms has our +avg model,0.5959116220474243
translation,394,146,results,our +avg model,has,significantly and consistently,our +avg model has significantly and consistently,0.5683375597000122
translation,394,146,results,results,On,all datasets,results On all datasets,0.4960552155971527
translation,394,148,results,worse performance,compared to,r 3 model,worse performance compared to r 3 model,0.6667345762252808
translation,394,148,results,triviaqa dataset,has,our +avg model,triviaqa dataset has our +avg model,0.5482525825500488
translation,394,148,results,our +avg model,has,worse performance,our +avg model has worse performance,0.5834515690803528
translation,394,148,results,results,On,triviaqa dataset,results On triviaqa dataset,0.5083121061325073
translation,394,151,results,our+full model,has,slight improvement,our+full model has slight improvement,0.5862616896629333
translation,394,151,results,slight improvement,by considering,confidence,slight improvement by considering confidence,0.7240035533905029
translation,394,151,results,confidence,of,each retrieved paragraph,confidence of each retrieved paragraph,0.6215118765830994
translation,394,151,results,results,has,our+full model,results has our+full model,0.5451327562332153
translation,394,152,results,slight improvement,compared to,r 3 model,slight improvement compared to r 3 model,0.6847319006919861
translation,394,152,results,curatedtrec and webquestions datasets,has,our model,curatedtrec and webquestions datasets has our model,0.5688276886940002
translation,394,152,results,results,On,curatedtrec and webquestions datasets,results On curatedtrec and webquestions datasets,0.5168081521987915
translation,394,163,results,traditional ir model,in selecting,informative paragraphs,traditional ir model in selecting informative paragraphs,0.6326573491096497
translation,394,163,results,significantly,in selecting,informative paragraphs,significantly in selecting informative paragraphs,0.7080481648445129
translation,394,163,results,our+indep and our+full,has,outperform,our+indep and our+full has outperform,0.6257637143135071
translation,394,163,results,outperform,has,traditional ir model,outperform has traditional ir model,0.5390679240226746
translation,394,163,results,traditional ir model,has,significantly,traditional ir model has significantly,0.5539274215698242
translation,394,165,results,similar performance,compare with,our+single,similar performance compare with our+single,0.6892672181129456
translation,394,165,results,our+single,from,hits@1 to hits@5,our+single from hits@1 to hits@5,0.6167829036712646
translation,394,165,results,our+single,to select,valid paragraphs,our+single to select valid paragraphs,0.7302246689796448
translation,394,165,results,our +full,has,similar performance,our +full has similar performance,0.5963069200515747
translation,394,165,results,results,has,our +full,results has our +full,0.5659291744232178
translation,394,169,results,our+full,performs,better,our+full performs better,0.6388322710990906
translation,394,169,results,better,in,answer extraction,better in answer extraction,0.5140790343284607
translation,394,169,results,better,compared to,our+single,better compared to our+single,0.7017755508422852
translation,394,184,results,r 3 model,in,"top - 1 , top - 3 and top - 5","r 3 model in top - 1 , top - 3 and top - 5",0.5622478127479553
translation,394,184,results,r 3 model,on,quasar -t and searchqa datasets,r 3 model on quasar -t and searchqa datasets,0.5224968791007996
translation,394,184,results,r 3 model,by,5 % to 7 %,r 3 model by 5 % to 7 %,0.6059076189994812
translation,394,184,results,our+full model,has,outperforms,our+full model has outperforms,0.6286240220069885
translation,394,184,results,outperforms,has,r 3 model,outperforms has r 3 model,0.6241031289100647
translation,394,184,results,results,has,our+full model,results has our+full model,0.5451327562332153
translation,395,209,ablation-analysis,refined data and the combination,observe that,threshold 0.15,refined data and the combination observe that threshold 0.15,0.5957010984420776
translation,395,209,ablation-analysis,threshold 0.15,achieves,better performance,threshold 0.15 achieves better performance,0.6324099898338318
translation,395,209,ablation-analysis,better performance,than,threshold 0.3,better performance than threshold 0.3,0.573462188243866
translation,395,209,ablation-analysis,greatly reduced,when,threshold,greatly reduced when threshold,0.715552806854248
translation,395,209,ablation-analysis,threshold,set to,0.0,threshold set to 0.0,0.701747715473175
translation,395,209,ablation-analysis,ablation analysis,For,refined data and the combination,ablation analysis For refined data and the combination,0.6297307014465332
translation,395,25,experiments,refqa dataset,harvests,lexically and syntactically divergent questions,refqa dataset harvests lexically and syntactically divergent questions,0.6566672325134277
translation,395,25,experiments,lexically and syntactically divergent questions,from,wikipedia,lexically and syntactically divergent questions from wikipedia,0.5728224515914917
translation,395,25,experiments,wikipedia,by using,cited documents,wikipedia by using cited documents,0.6219978928565979
translation,395,148,hyperparameters,"adam ( kingma and ba , 2015 )",as,our optimizer,"adam ( kingma and ba , 2015 ) as our optimizer",0.49293166399002075
translation,395,148,hyperparameters,"adam ( kingma and ba , 2015 )",with,learning rate,"adam ( kingma and ba , 2015 ) with learning rate",0.5996310710906982
translation,395,148,hyperparameters,"adam ( kingma and ba , 2015 )",with,batch size,"adam ( kingma and ba , 2015 ) with batch size",0.6160398721694946
translation,395,148,hyperparameters,our optimizer,with,learning rate,our optimizer with learning rate,0.6119175553321838
translation,395,148,hyperparameters,learning rate,of,3e - 5,learning rate of 3e - 5,0.6390295624732971
translation,395,148,hyperparameters,batch size,of,24,batch size of 24,0.6606289148330688
translation,395,148,hyperparameters,hyperparameters,use,"adam ( kingma and ba , 2015 )","hyperparameters use adam ( kingma and ba , 2015 )",0.5931493639945984
translation,395,149,hyperparameters,max sequence length,set to,384,max sequence length set to 384,0.7352433800697327
translation,395,149,hyperparameters,hyperparameters,has,max sequence length,hyperparameters has max sequence length,0.4825800061225891
translation,395,150,hyperparameters,long document,into,multiple windows,long document into multiple windows,0.5936292409896851
translation,395,150,hyperparameters,multiple windows,with,stride,multiple windows with stride,0.6941089630126953
translation,395,150,hyperparameters,stride,of,128,stride of 128,0.650640070438385
translation,395,150,hyperparameters,hyperparameters,split,long document,hyperparameters split long document,0.6898850798606873
translation,395,154,hyperparameters,300k data,from,refqa,300k data from refqa,0.5515210032463074
translation,395,154,hyperparameters,300k data,to train,initial qa model,300k data to train initial qa model,0.7194346785545349
translation,395,154,hyperparameters,hyperparameters,uniformly sample,300k data,hyperparameters uniformly sample 300k data,0.6789706349372864
translation,395,159,hyperparameters,threshold,set to,0.15,threshold set to 0.15,0.7117780447006226
translation,395,159,hyperparameters,0.15,for filtering,model predictions,0.15 for filtering model predictions,0.7032977938652039
translation,395,159,hyperparameters,hyperparameters,has,threshold,hyperparameters has threshold,0.557809591293335
translation,395,6,model,two approaches,to improve,unsupervised qa,two approaches to improve unsupervised qa,0.6899755001068115
translation,395,7,model,lexically and syntactically divergent questions,from,wikipedia,lexically and syntactically divergent questions from wikipedia,0.5728224515914917
translation,395,7,model,lexically and syntactically divergent questions,to automatically construct,corpus,lexically and syntactically divergent questions to automatically construct corpus,0.6946738958358765
translation,395,7,model,corpus,of,question - answer pairs,corpus of question - answer pairs,0.5697861909866333
translation,395,7,model,model,harvest,lexically and syntactically divergent questions,model harvest lexically and syntactically divergent questions,0.6433084607124329
translation,395,8,model,qa model,to extract,more appropriate answers,qa model to extract more appropriate answers,0.6587516665458679
translation,395,8,model,more appropriate answers,iteratively refines,data,more appropriate answers iteratively refines data,0.8008350133895874
translation,395,8,model,data,over,re - fqa,data over re - fqa,0.7585892081260681
translation,395,8,model,model,take advantage of,qa model,model take advantage of qa model,0.6743963360786438
translation,395,24,model,two approaches,to improve,quality,two approaches to improve quality,0.7146167755126953
translation,395,24,model,quality,of,synthetic context -questionanswer triples,quality of synthetic context -questionanswer triples,0.5617495179176331
translation,395,28,model,data,over,refqa,data over refqa,0.6570634841918945
translation,395,28,model,model,iteratively refine,data,model iteratively refine data,0.7464902400970459
translation,395,147,model,linear layer,to compute,probability,linear layer to compute probability,0.7495623230934143
translation,395,147,model,probability,of,each token,probability of each token,0.6089731454849243
translation,395,147,model,each token,being,start or end,each token being start or end,0.6665197610855103
translation,395,147,model,start or end,of,answer span,start or end of answer span,0.5682539343833923
translation,395,147,model,model,apply,linear layer,model apply linear layer,0.678641140460968
translation,395,32,results,iteratively refining the data,improves,model performance,iteratively refining the data improves model performance,0.6671658158302307
translation,395,32,results,results,show,iteratively refining the data,results show iteratively refining the data,0.618557870388031
translation,395,34,results,our method,yields,state - of - the - art results,our method yields state - of - the - art results,0.6455949544906616
translation,395,34,results,state - of - the - art results,against,strong baselines,state - of - the - art results against strong baselines,0.6375096440315247
translation,395,34,results,strong baselines,in,unsupervised setting,strong baselines in unsupervised setting,0.512259304523468
translation,395,34,results,results,has,our method,results has our method,0.5589964985847473
translation,395,171,results,qa model,on,refqa,qa model on refqa,0.5753936171531677
translation,395,171,results,qa model,on,outperforms,qa model on outperforms,0.6089369058609009
translation,395,171,results,previous methods,by,large margin,previous methods by large margin,0.5573447942733765
translation,395,171,results,refqa,has,outperforms,refqa has outperforms,0.6122265458106995
translation,395,171,results,outperforms,has,previous methods,outperforms has previous methods,0.5876122713088989
translation,395,171,results,results,Training,qa model,results Training qa model,0.7348981499671936
translation,395,172,results,our approach,achieves,new state - of - the - art results,our approach achieves new state - of - the - art results,0.6229143738746643
translation,395,172,results,new state - of - the - art results,in,unsupervised setting,new state - of - the - art results in unsupervised setting,0.5251502394676208
translation,395,172,results,iterative data refinement,has,our approach,iterative data refinement has our approach,0.5645492672920227
translation,395,172,results,results,Combining with,iterative data refinement,results Combining with iterative data refinement,0.6399857401847839
translation,395,173,results,qa model,attains,71.4 f1,qa model attains 71.4 f1,0.6500019431114197
translation,395,173,results,qa model,attains,45.1 f1,qa model attains 45.1 f1,0.6730078458786011
translation,395,173,results,qa model,attains,outperforming,qa model attains outperforming,0.6902955770492554
translation,395,173,results,71.4 f1,on,squad 1.1 test set,71.4 f1 on squad 1.1 test set,0.493874728679657
translation,395,173,results,45.1 f1,on,newsqa test set,45.1 f1 on newsqa test set,0.5164172649383545
translation,395,173,results,45.1 f1,without using,annotated data,45.1 f1 without using annotated data,0.6991686820983887
translation,395,173,results,newsqa test set,without using,annotated data,newsqa test set without using annotated data,0.6693404316902161
translation,395,173,results,outperforming,has,all of the previous unsupervised methods,outperforming has all of the previous unsupervised methods,0.5663438439369202
translation,395,173,results,results,has,qa model,results has qa model,0.5610512495040894
translation,395,185,results,our refqa,achieves,consistent gain,our refqa achieves consistent gain,0.7555503845214844
translation,395,185,results,consistent gain,over,all cloze translation methods,consistent gain over all cloze translation methods,0.6838430762290955
translation,395,185,results,results,training on,our refqa,results training on our refqa,0.7190601825714111
translation,395,186,results,dependency reconstruction method,is,favorable,dependency reconstruction method is favorable,0.6054701805114746
translation,395,186,results,favorable,compared with,  identity mapping   method,favorable compared with   identity mapping   method,0.7097427845001221
translation,395,186,results,results,has,dependency reconstruction method,results has dependency reconstruction method,0.5235607028007507
translation,395,187,results,improvement,of,drc,improvement of drc,0.5947660803794861
translation,395,187,results,drc,on,wiki,drc on wiki,0.5869917869567871
translation,395,187,results,drc,smaller than,refqa,drc smaller than refqa,0.7286916375160217
translation,395,187,results,results,has,improvement,results has improvement,0.6248279809951782
translation,395,200,results,refined and filtered data,is,more useful,refined and filtered data is more useful,0.5761476159095764
translation,395,200,results,more useful,than,only using one of them,more useful than only using one of them,0.622107744216919
translation,395,200,results,results,combination of,refined and filtered data,results combination of refined and filtered data,0.6376381516456604
translation,395,201,results,our combination approach,further improves,model performance,our combination approach further improves model performance,0.7286400198936462
translation,395,201,results,model performance,to,72.6 f1 ( 1.6 absolute improvement ),model performance to 72.6 f1 ( 1.6 absolute improvement ),0.5060126781463623
translation,395,201,results,iterative training,has,our combination approach,iterative training has our combination approach,0.6001438498497009
translation,395,201,results,results,Using,iterative training,results Using iterative training,0.6278882622718811
translation,395,202,results,refined data,contributes,further improvement,refined data contributes further improvement,0.6968640685081482
translation,395,202,results,further improvement,compared with,filtered data,further improvement compared with filtered data,0.7000569701194763
translation,395,202,results,results,using,refined data,results using refined data,0.6707068085670471
translation,395,206,results,threshold,of,0.15,threshold of 0.15,0.6128607988357544
translation,395,206,results,threshold,achieves,better performance,threshold achieves better performance,0.6911618113517761
translation,395,206,results,results,Using,threshold,results Using threshold,0.6717706322669983
translation,395,208,results,filtered data,using,higher confidence threshold,filtered data using higher confidence threshold,0.7282094955444336
translation,395,208,results,higher confidence threshold,achieves,better performance,higher confidence threshold achieves better performance,0.6924191117286682
translation,395,208,results,results,for,filtered data,results for filtered data,0.6490656733512878
translation,395,221,results,qa model,in,most types of refinement,qa model in most types of refinement,0.4836864769458771
translation,395,221,results,most types of refinement,except,oa ? pa,most types of refinement except oa ? pa,0.7227419018745422
translation,395,221,results,refined data,has,improves,refined data has improves,0.598281979560852
translation,395,221,results,improves,has,qa model,improves has qa model,0.5997719764709473
translation,395,233,results,our method,obtains,best performance,our method obtains best performance,0.5811209678649902
translation,395,233,results,best performance,in,restricted setting,best performance in restricted setting,0.5519087910652161
translation,395,233,results,best performance,compared with,previous state of the art,best performance compared with previous state of the art,0.5953327417373657
translation,395,233,results,best performance,compared with,directly fine-tuning,best performance compared with directly fine-tuning,0.6616763472557068
translation,395,233,results,directly fine-tuning,has,bert,directly fine-tuning has bert,0.621688187122345
translation,395,233,results,results,has,our method,results has our method,0.5589964985847473
translation,395,234,results,approach,achieves,79.4 f1,approach achieves 79.4 f1,0.6617279052734375
translation,395,234,results,79.4 f1,with,only 100 labeled examples,79.4 f1 with only 100 labeled examples,0.5615076422691345
translation,395,234,results,16.4 absolute gains,than,other models,16.4 absolute gains than other models,0.5522664189338684
translation,395,234,results,79.4 f1,has,16.4 absolute gains,79.4 f1 has 16.4 absolute gains,0.5746867060661316
translation,395,234,results,results,has,approach,results has approach,0.5518386363983154
translation,395,236,results,results,of,different methods,results of different methods,0.5607766509056091
translation,395,236,results,different methods,become,comparable,different methods become comparable,0.5737782120704651
translation,395,236,results,comparable,when,labeled data size,comparable when labeled data size,0.6757259964942932
translation,395,236,results,labeled data size,greater than,"10,000","labeled data size greater than 10,000",0.6910771727561951
translation,395,236,results,results,observe that,results,results observe that results,0.5468541383743286
translation,395,236,results,results,of,different methods,results of different methods,0.5607766509056091
translation,396,97,ablation-analysis,chain of documents,is,important,chain of documents is important,0.5992196202278137
translation,396,97,ablation-analysis,ablation analysis,modeling,chain of documents,ablation analysis modeling chain of documents,0.7983548641204834
translation,396,74,baselines,bert encoder,use,bert - base - uncased model,bert encoder use bert - base - uncased model,0.6239711046218872
translation,396,80,baselines,rocchio 's algorithm,on top of,tf - idf retrieval model ( prf - tfidf ),rocchio 's algorithm on top of tf - idf retrieval model ( prf - tfidf ),0.6793564558029175
translation,396,80,baselines,relevance model ( rm3 ),based on,language modeling framework,relevance model ( rm3 ) based on language modeling framework,0.6303321123123169
translation,396,80,baselines,language modeling framework,in,information retrieval ( prf - rm ),language modeling framework in information retrieval ( prf - rm ),0.5441338419914246
translation,396,80,baselines,two widely used prf models,has,rocchio 's algorithm,two widely used prf models has rocchio 's algorithm,0.5055268406867981
translation,396,80,baselines,baselines,compare with,two widely used prf models,baselines compare with two widely used prf models,0.6128552556037903
translation,396,118,baselines,baseline models,are,bm25 retriever,baseline models are bm25 retriever,0.5426188111305237
translation,396,118,baselines,baseline models,are,bert - re-ranker model,baseline models are bert - re-ranker model,0.5684804320335388
translation,396,118,baselines,bert - re-ranker model,of,"( nogueira and cho , 2019 )","bert - re-ranker model of ( nogueira and cho , 2019 )",0.5552152395248413
translation,396,118,baselines,bert - re-ranker model,ranks,candidate supporting paragraphs,bert - re-ranker model ranks candidate supporting paragraphs,0.7627518177032471
translation,396,118,baselines,candidate supporting paragraphs,for,question,candidate supporting paragraphs for question,0.6438404321670532
translation,396,118,baselines,baselines,has,baseline models,baselines has baseline models,0.5690722465515137
translation,396,81,experiments,query likelihood retrieval model,with,dirichlet prior smoothing,query likelihood retrieval model with dirichlet prior smoothing,0.5908244252204895
translation,396,16,model,entities,present in,initially retrieved paragraph,entities present in initially retrieved paragraph,0.6969499588012695
translation,396,16,model,initially retrieved paragraph,to jointly find,passage,initially retrieved paragraph to jointly find passage,0.6102137565612793
translation,396,16,model,relevant,to answer,multi-hop query,relevant to answer multi-hop query,0.6679778099060059
translation,396,17,model,re-ranker model,uses,contextualized entity representation,re-ranker model uses contextualized entity representation,0.5319105386734009
translation,396,17,model,contextualized entity representation,obtained from,"pre-trained bert ( devlin et al. , 2018 ) language model","contextualized entity representation obtained from pre-trained bert ( devlin et al. , 2018 ) language model",0.5105982422828674
translation,396,17,model,model,has,major component,model has major component,0.5433776378631592
translation,396,19,model,re-ranker,uses,representation,re-ranker uses representation,0.6432574987411499
translation,396,19,model,re-ranker,uses,representation,re-ranker uses representation,0.6432574987411499
translation,396,19,model,representation,of both,initial paragraph,representation of both initial paragraph,0.6963165402412415
translation,396,19,model,representation,of both,all the entities,representation of both all the entities,0.6771767735481262
translation,396,19,model,representation,of,all the entities,representation of all the entities,0.5665399432182312
translation,396,19,model,representation,to determine,evidence,representation to determine evidence,0.7122865319252014
translation,396,19,model,representation,of,all the entities,representation of all the entities,0.5665399432182312
translation,396,19,model,evidence,to,gather,evidence to gather,0.6304039359092712
translation,396,19,model,gather,has,next,gather has next,0.675591230392456
translation,396,19,model,model,has,re-ranker,model has re-ranker,0.609228789806366
translation,396,31,model,entitycentric ir approach,jointly performs,entity linking,entitycentric ir approach jointly performs entity linking,0.7024638652801514
translation,396,31,model,entitycentric ir approach,effectively finds,relevant evidence,entitycentric ir approach effectively finds relevant evidence,0.645002007484436
translation,396,31,model,relevant evidence,required for,questions,relevant evidence required for questions,0.6604561805725098
translation,396,31,model,from a large corpus,containing,millions of paragraphs,from a large corpus containing millions of paragraphs,0.6325441002845764
translation,396,31,model,multi-hop reasoning,has,from a large corpus,multi-hop reasoning has from a large corpus,0.4868096113204956
translation,396,31,model,model,presents,entitycentric ir approach,model presents entitycentric ir approach,0.6302408576011658
translation,396,26,results,outperforms,for,multi-hop qa,outperforms for multi-hop qa,0.657993495464325
translation,396,26,results,all of these methods,for,multi-hop qa,all of these methods for multi-hop qa,0.6466466188430786
translation,396,26,results,significantly,for,multi-hop qa,significantly for multi-hop qa,0.6935445070266724
translation,396,26,results,our method,has,outperforms,our method has outperforms,0.6322360634803772
translation,396,26,results,outperforms,has,all of these methods,outperforms has all of these methods,0.5636326670646667
translation,396,26,results,all of these methods,has,significantly,all of these methods has significantly,0.544260561466217
translation,396,26,results,results,has,our method,results has our method,0.5589964985847473
translation,396,88,results,absolute increase,of,26.5 % ( accuracy @10 ),absolute increase of 26.5 % ( accuracy @10 ),0.5668097734451294
translation,396,88,results,absolute increase,of,18.4 % ( map ),absolute increase of 18.4 % ( map ),0.537655770778656
translation,396,88,results,absolute increase,compared to,bert - re-ranker,absolute increase compared to bert - re-ranker,0.6871899366378784
translation,396,88,results,retrieval technique,has,vastly outperforms,retrieval technique has vastly outperforms,0.5997039079666138
translation,396,88,results,vastly outperforms,has,other existing retrieval systems,vastly outperforms has other existing retrieval systems,0.5672503113746643
translation,396,103,results,our model,performs,equally well,our model performs equally well,0.6311533451080322
translation,396,103,results,equally well,on,both type of queries,equally well on both type of queries,0.5553916096687317
translation,396,103,results,results,shows,our model,results shows our model,0.7287026643753052
translation,396,108,results,10.59 absolute increase,in,f1 score,10.59 absolute increase in f1 score,0.5323218703269958
translation,396,108,results,10.59 absolute increase,than,baseline,10.59 absolute increase than baseline,0.5384851098060608
translation,396,108,results,f1 score,than,baseline,f1 score than baseline,0.5661450624465942
translation,396,108,results,results,achieve,10.59 absolute increase,results achieve 10.59 absolute increase,0.6086464524269104
translation,396,110,results,results,show,retrieval,results show retrieval,0.6777358055114746
translation,396,119,results,outperforms,in,zero-shot setting,outperforms in zero-shot setting,0.5534459352493286
translation,396,119,results,both models,in,zero-shot setting,both models in zero-shot setting,0.5581381320953369
translation,396,119,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,396,119,results,outperforms,has,both models,outperforms has both models,0.6001972556114197
translation,397,193,baselines,question - to - choice ( q2 choice ),computes,attention,question - to - choice ( q2 choice ) computes attention,0.764144241809845
translation,397,193,baselines,attention,between,question and the answer choice,attention between question and the answer choice,0.6135298013687134
translation,397,193,baselines,esim + elmo,with,elmo,esim + elmo with elmo,0.6815752387046814
translation,397,193,baselines,esim + elmo,with,) embeddings,esim + elmo with ) embeddings,0.6963876485824585
translation,397,193,baselines,esim + elmo,to compute,questionchoice entailment,esim + elmo to compute questionchoice entailment,0.6981915831565857
translation,397,193,baselines,) embeddings,to compute,questionchoice entailment,) embeddings to compute questionchoice entailment,0.7998330593109131
translation,397,194,baselines,knowledge enhanced reader ( ker ),uses,core fact ( f ),knowledge enhanced reader ( ker ) uses core fact ( f ),0.6107279062271118
translation,397,194,baselines,knowledge enhanced reader ( ker ),uses,knowledge,knowledge enhanced reader ( ker ) uses knowledge,0.6134558320045471
translation,397,194,baselines,knowledge,retrieved from,conceptnet,knowledge retrieved from conceptnet,0.5991058349609375
translation,397,194,baselines,knowledge,to compute,cross-attentions,knowledge to compute cross-attentions,0.7354915142059326
translation,397,194,baselines,cross-attentions,between,"question , knowledge , and answer choices","cross-attentions between question , knowledge , and answer choices",0.5987377166748047
translation,397,237,baselines,baselines,has,no spans ( model ),baselines has no spans ( model ),0.5895524024963379
translation,397,142,hyperparameters,300 dimensional 840b glove embeddings,to embed,each word,300 dimensional 840b glove embeddings to embed each word,0.6377822160720825
translation,397,142,hyperparameters,each word,in,inputs,each word in inputs,0.5852435827255249
translation,397,142,hyperparameters,hyperparameters,use,300 dimensional 840b glove embeddings,hyperparameters use 300 dimensional 840b glove embeddings,0.5613533854484558
translation,397,143,hyperparameters,bi-lstm,with,100 - dimensional hidden states,bi-lstm with 100 - dimensional hidden states,0.6165770888328552
translation,397,143,hyperparameters,100 - dimensional hidden states,to compute,contextual encodings,100 - dimensional hidden states to compute contextual encodings,0.7252726554870605
translation,397,143,hyperparameters,contextual encodings,for,each string,contextual encodings for each string,0.6240069270133972
translation,397,143,hyperparameters,hyperparameters,use,bi-lstm,hyperparameters use bi-lstm,0.6121622323989868
translation,397,7,model,novel approach,explicitly identifies,knowledge gap,novel approach explicitly identifies knowledge gap,0.7964931130409241
translation,397,7,model,knowledge gap,between,key span,knowledge gap between key span,0.6506452560424805
translation,397,7,model,key span,in,provided knowledge,key span in provided knowledge,0.5313462018966675
translation,397,7,model,key span,in,answer choices,key span in answer choices,0.522743284702301
translation,397,7,model,model,develop,novel approach,model develop novel approach,0.6610212326049805
translation,397,8,model,gap,by determining,relationship,gap by determining relationship,0.7423169612884521
translation,397,8,model,relationship,between,span,relationship between span,0.6428462862968445
translation,397,8,model,relationship,between,answer choice,relationship between answer choice,0.6743528842926025
translation,397,8,model,relationship,based on,retrieved knowledge,relationship based on retrieved knowledge,0.6849446892738342
translation,397,8,model,retrieved knowledge,targeting,gap,retrieved knowledge targeting gap,0.6719560623168945
translation,397,8,model,model,has,gapqa,model has gapqa,0.6111138463020325
translation,397,9,model,model,to simultaneously fill,knowledge gap,model to simultaneously fill knowledge gap,0.677925705909729
translation,397,9,model,model,compose it with,provided partial knowledge,model compose it with provided partial knowledge,0.6093911528587341
translation,397,9,model,model,jointly training,model,model jointly training model,0.7179450988769531
translation,397,42,model,existing rc model and large-scale dataset,to train,span-prediction model,existing rc model and large-scale dataset to train span-prediction model,0.6502510905265808
translation,397,42,model,model,exploits,existing rc model and large-scale dataset,model exploits existing rc model and large-scale dataset,0.6939346194267273
translation,397,43,model,second,uses,multi-task learning,second uses multi-task learning,0.6124930381774902
translation,397,43,model,multi-task learning,to train,separate qa model,multi-task learning to train separate qa model,0.6944154500961304
translation,397,43,model,separate qa model,to jointly predict,relation,separate qa model to jointly predict relation,0.7611708045005798
translation,397,43,model,relation,representing,gap,relation representing gap,0.7184479236602783
translation,397,43,model,model,uses,multi-task learning,model uses multi-task learning,0.5744587779045105
translation,397,43,model,model,has,second,model has second,0.6080706119537354
translation,397,44,model,questions,without,labelled knowledge gaps,questions without labelled knowledge gaps,0.7352274656295776
translation,397,44,model,questions,has,qa model,questions has qa model,0.5897948741912842
translation,397,44,model,labelled knowledge gaps,has,qa model,labelled knowledge gaps has qa model,0.6217667460441589
translation,397,44,model,model,For,questions,model For questions,0.6588122844696045
translation,397,45,results,previous state- ofthe - art partial knowledge models,by,6.5 % ( 64.41 vs 57.93 ),previous state- ofthe - art partial knowledge models by 6.5 % ( 64.41 vs 57.93 ),0.503176748752594
translation,397,45,results,previous state- ofthe - art partial knowledge models,on,targeted subset of openbookqa,previous state- ofthe - art partial knowledge models on targeted subset of openbookqa,0.5208673477172852
translation,397,45,results,targeted subset of openbookqa,amenable to,gap-based reasoning,targeted subset of openbookqa amenable to gap-based reasoning,0.7899740934371948
translation,397,45,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,397,45,results,outperforms,has,previous state- ofthe - art partial knowledge models,outperforms has previous state- ofthe - art partial knowledge models,0.5553921461105347
translation,397,45,results,results,has,our model,results has our model,0.5871725678443909
translation,397,46,results,our model,with,simple heuristic,our model with simple heuristic,0.6737244129180908
translation,397,46,results,simple heuristic,to identify,missing gaps,simple heuristic to identify missing gaps,0.6877633333206177
translation,397,46,results,previous models,by,3,previous models by 3,0.628235399723053
translation,397,46,results,previous models,by,4 % ( 61.38 vs. 57.93 ),previous models by 4 % ( 61.38 vs. 57.93 ),0.49143439531326294
translation,397,46,results,3,.,4 % ( 61.38 vs. 57.93 ),3 . 4 % ( 61.38 vs. 57.93 ),0.5852771997451782
translation,397,46,results,fact annotations,has,our model,fact annotations has our model,0.6191296577453613
translation,397,46,results,outperforms,has,previous models,outperforms has previous models,0.6068373918533325
translation,397,46,results,results,without missing,fact annotations,results without missing fact annotations,0.7134217023849487
translation,397,188,results,bidaf model,trained on,squad dataset,bidaf model trained on squad dataset,0.7367753386497498
translation,397,188,results,bidaf model,performs,poorly,bidaf model performs poorly,0.7231794595718384
translation,397,188,results,poorly,on,our task,poorly on our task,0.6203168630599976
translation,397,189,results,kgd,to fine- tune,bidaf,kgd to fine- tune bidaf,0.7420788407325745
translation,397,189,results,bidaf,pretrained on,squad,bidaf pretrained on squad,0.7166398167610168
translation,397,189,results,squad,results in,best f1 ( 78.55 ) and em ( 63.99 ) scores,squad results in best f1 ( 78.55 ) and em ( 63.99 ) scores,0.6899793744087219
translation,397,189,results,best f1 ( 78.55 ) and em ( 63.99 ) scores,on,dev set,best f1 ( 78.55 ) and em ( 63.99 ) scores on dev set,0.5413833260536194
translation,397,189,results,kgd ( from scratch ),has,substantially improves,kgd ( from scratch ) has substantially improves,0.6095861792564392
translation,397,189,results,substantially improves,has,accuracy,substantially improves has accuracy,0.5874377489089966
translation,397,189,results,results,using,kgd,results using kgd,0.6549863815307617
translation,397,204,results,our proposed gapqa,improves,statistically significantly,our proposed gapqa improves statistically significantly,0.7246190309524536
translation,397,204,results,statistically significantly,over,partial knowledge baselines,statistically significantly over partial knowledge baselines,0.6602194309234619
translation,397,204,results,partial knowledge baselines,by,6.5 % to 14.4 %,partial knowledge baselines by 6.5 % to 14.4 %,0.5457133650779724
translation,397,204,results,targeted obqa - short subset,has,our proposed gapqa,targeted obqa - short subset has our proposed gapqa,0.5939077138900757
translation,397,204,results,results,On,targeted obqa - short subset,results On targeted obqa - short subset,0.5782896280288696
translation,397,205,results,improvement,of,3 + %,improvement of 3 + %,0.6292316317558289
translation,397,205,results,improvement,relative to,prior approaches,improvement relative to prior approaches,0.7110008597373962
translation,397,205,results,3 + %,relative to,prior approaches,3 + % relative to prior approaches,0.7031826376914978
translation,397,211,results,improvement,by,5.9 % and 11.3 %,improvement by 5.9 % and 11.3 %,0.6012842655181885
translation,397,211,results,5.9 % and 11.3 %,has,given only wordnet and omcs knowledge,5.9 % and 11.3 % has given only wordnet and omcs knowledge,0.55110102891922
translation,397,213,results,our model,able to exploit,additional knowledge,our model able to exploit additional knowledge,0.6886457800865173
translation,397,213,results,improve further,by,4 %,improve further by 4 %,0.679711103439331
translation,397,213,results,full con-ceptnet knowledge,has,our model,full con-ceptnet knowledge has our model,0.5892941355705261
translation,397,213,results,large-scale text corpora,has,our model,large-scale text corpora has our model,0.5148137807846069
translation,397,213,results,results,When provided with,full con-ceptnet knowledge,results When provided with full con-ceptnet knowledge,0.6817077994346619
translation,397,221,results,noisy spans,produced by,out-of- domain bidaf model,noisy spans produced by out-of- domain bidaf model,0.6540120244026184
translation,397,221,results,worse,that,full gapqa model,worse that full gapqa model,0.6665076613426208
translation,397,221,results,full gapqa model,by,5.5 %,full gapqa model by 5.5 %,0.5813368558883667
translation,397,242,results,results,has,no spans ( ir ),results has no spans ( ir ),0.5559636354446411
translation,398,83,baselines,siamese architecture,without,attention mechanism,siamese architecture without attention mechanism,0.6757263541221619
translation,398,28,hyperparameters,word embeddings,based on,"word2vec ( mikolov et al. , 2013 )","word embeddings based on word2vec ( mikolov et al. , 2013 )",0.5120846629142761
translation,398,28,hyperparameters,"word2vec ( mikolov et al. , 2013 )",as input to,convolutions,"word2vec ( mikolov et al. , 2013 ) as input to convolutions",0.6271238327026367
translation,398,28,hyperparameters,hyperparameters,use,word embeddings,hyperparameters use word embeddings,0.5600625872612
translation,398,31,hyperparameters,word embeddings,generated using,skip-gram model,word embeddings generated using skip-gram model,0.5862581729888916
translation,398,31,hyperparameters,word embeddings,generated using,dimensionality,word embeddings generated using dimensionality,0.5997626781463623
translation,398,31,hyperparameters,skip-gram model,setting,context window,skip-gram model setting context window,0.3999813497066498
translation,398,31,hyperparameters,context window,to,5,context window to 5,0.6250700950622559
translation,398,31,hyperparameters,dimensionality,to,d = 200,dimensionality to d = 200,0.579563319683075
translation,398,31,hyperparameters,hyperparameters,has,word embeddings,hyperparameters has word embeddings,0.4784160554409027
translation,398,86,hyperparameters,"adadelta ( zeiler , 2012 )",as,optimizer,"adadelta ( zeiler , 2012 ) as optimizer",0.5067919492721558
translation,398,86,hyperparameters,l 2 regularization,to avoid,overfitting,l 2 regularization to avoid overfitting,0.6454142332077026
translation,398,86,hyperparameters,hyperparameters,employ,"adadelta ( zeiler , 2012 )","hyperparameters employ adadelta ( zeiler , 2012 )",0.5104125142097473
translation,398,6,model,siamese cnn architecture,extended by,two attention mechanisms,siamese cnn architecture extended by two attention mechanisms,0.6131327152252197
translation,398,12,results,our system,ranked,7 th place,our system ranked 7 th place,0.7605888843536377
translation,398,12,results,our system,achieving,map score,our system achieving map score,0.6720139980316162
translation,398,12,results,map score,of,86.2,map score of 86.2,0.5601167678833008
translation,398,12,results,86.2,outperformed by,2 points,86.2 outperformed by 2 points,0.6635381579399109
translation,398,12,results,2 points,by,1 st ranked system,2 points by 1 st ranked system,0.6505708694458008
translation,398,12,results,results,has,our system,results has our system,0.5954442024230957
translation,398,93,results,results,obtained,systems,results obtained systems,0.6707159280776978
translation,398,93,results,results,obtained,systems,results obtained systems,0.6707159280776978
translation,398,94,results,attention based mechanism,boosts,map score,attention based mechanism boosts map score,0.7099272608757019
translation,398,94,results,attention based mechanism,boosts,avgrec,attention based mechanism boosts avgrec,0.723393976688385
translation,398,94,results,attention based mechanism,boosts,mrr score,attention based mechanism boosts mrr score,0.723747968673706
translation,398,94,results,map score,by,3 - 4 points,map score by 3 - 4 points,0.5571389198303223
translation,398,94,results,avgrec,by,3 points,avgrec by 3 points,0.6291699409484863
translation,398,94,results,mrr score,by,3 - 4 points,mrr score by 3 - 4 points,0.5158451199531555
translation,398,94,results,results,show,attention based mechanism,results show attention based mechanism,0.6710109114646912
translation,398,95,results,outperforms,by,1 point,outperforms by 1 point,0.6787850260734558
translation,398,95,results,abcnn1,by,1 point,abcnn1 by 1 point,0.5979335904121399
translation,398,95,results,abcnn2,has,outperforms,abcnn2 has outperforms,0.6768016219139099
translation,398,95,results,outperforms,has,abcnn1,outperforms has abcnn1,0.6164144277572632
translation,398,95,results,results,observe,abcnn2,results observe abcnn2,0.536990761756897
translation,398,100,results,our system,outperformed by,2 points,our system outperformed by 2 points,0.7201664447784424
translation,398,100,results,our system,outperformed by,only 0.6 points,our system outperformed by only 0.6 points,0.7272029519081116
translation,398,100,results,our system,by,only 0.6 points,our system by only 0.6 points,0.5969952344894409
translation,398,100,results,2 points,by,kelp and the beihang - msra submission,2 points by kelp and the beihang - msra submission,0.5546004176139832
translation,398,100,results,only 0.6 points,by,iit - uhh submission,only 0.6 points by iit - uhh submission,0.645479142665863
translation,398,100,results,results,has,our system,results has our system,0.5954442024230957
translation,399,113,ablation-analysis,interaction layer and augmented features,contribute to,behavior of neural networks,interaction layer and augmented features contribute to behavior of neural networks,0.6670558452606201
translation,399,113,ablation-analysis,ablation analysis,clear,interaction layer and augmented features,ablation analysis clear interaction layer and augmented features,0.7225486040115356
translation,399,98,experimental-setup,experimental setup,implemented with,tensorflow v1.0,experimental setup implemented with tensorflow v1.0,0.6515279412269592
translation,399,99,experimental-setup,texts,from,questions and comments,texts from questions and comments,0.6124555468559265
translation,399,99,experimental-setup,texts,used,first,texts used first,0.6495624780654907
translation,399,99,experimental-setup,texts,at,first,texts at first,0.6027310490608215
translation,399,99,experimental-setup,texts,to train,word2 vec vectors,texts to train word2 vec vectors,0.6621784567832947
translation,399,99,experimental-setup,first,to train,word2 vec vectors,first to train word2 vec vectors,0.6485047340393066
translation,399,99,experimental-setup,word2 vec vectors,by,python package gensim,word2 vec vectors by python package gensim,0.5367038249969482
translation,399,99,experimental-setup,length,fixed to,100,length fixed to 100,0.7694923877716064
translation,399,99,experimental-setup,experimental setup,has,texts,experimental setup has texts,0.49664267897605896
translation,399,100,experimental-setup,max sequence length,of,question and the comment,max sequence length of question and the comment,0.6261554956436157
translation,399,100,experimental-setup,max sequence length,fixed to,200,max sequence length fixed to 200,0.764226496219635
translation,399,100,experimental-setup,experimental setup,has,max sequence length,experimental setup has max sequence length,0.5051833987236023
translation,399,101,experimental-setup,paddings,if,sequence,paddings if sequence,0.6734793186187744
translation,399,101,experimental-setup,sequence,is,short,sequence is short,0.6269439458847046
translation,399,101,experimental-setup,truncate,has,excess,truncate has excess,0.6352554559707642
translation,399,101,experimental-setup,experimental setup,add,paddings,experimental setup add paddings,0.6361442804336548
translation,399,102,experimental-setup,single- type cnn networks,have,filter size,single- type cnn networks have filter size,0.5300525426864624
translation,399,102,experimental-setup,single- type cnn networks,have,800 feature maps,single- type cnn networks have 800 feature maps,0.5505954027175903
translation,399,102,experimental-setup,single- type cnn networks,have,filter sizes,single- type cnn networks have filter sizes,0.5503230094909668
translation,399,102,experimental-setup,filter size,of,3,filter size of 3,0.6739966869354248
translation,399,102,experimental-setup,filter size,of,800 feature maps,filter size of 800 feature maps,0.5942482352256775
translation,399,102,experimental-setup,filter size,of,"1,2,3 and 5","filter size of 1,2,3 and 5",0.6537438035011292
translation,399,102,experimental-setup,multitype cnn networks,have,filter sizes,multitype cnn networks have filter sizes,0.5594363808631897
translation,399,102,experimental-setup,filter sizes,of,"1,2,3 and 5","filter sizes of 1,2,3 and 5",0.6540601849555969
translation,399,102,experimental-setup,filter sizes,with,800 feature maps each,filter sizes with 800 feature maps each,0.6437059640884399
translation,399,102,experimental-setup,"1,2,3 and 5",with,800 feature maps each,"1,2,3 and 5 with 800 feature maps each",0.6406912207603455
translation,399,102,experimental-setup,experimental setup,has,single- type cnn networks,experimental setup has single- type cnn networks,0.5256574153900146
translation,399,103,experimental-setup,bi-lstms,have,output length,bi-lstms have output length,0.5502961277961731
translation,399,103,experimental-setup,bi-lstms,have,hidden states,bi-lstms have hidden states,0.5798302888870239
translation,399,103,experimental-setup,output length,of,400,output length of 400,0.6551467180252075
translation,399,103,experimental-setup,400,of,each direction,400 of each direction,0.6557663679122925
translation,399,103,experimental-setup,hidden states,outputted directly for,higher layer,hidden states outputted directly for higher layer,0.6835739612579346
translation,399,103,experimental-setup,experimental setup,has,bi-lstms,experimental setup has bi-lstms,0.5668566823005676
translation,399,103,experimental-setup,experimental setup,has,hidden states,experimental setup has hidden states,0.5453227758407593
translation,399,104,experimental-setup,number of nodes,in,hidden layer,number of nodes in hidden layer,0.5056959390640259
translation,399,104,experimental-setup,hidden layer,is,256,hidden layer is 256,0.5885547399520874
translation,399,104,experimental-setup,activation function,used in,fully connected neural networks,activation function used in fully connected neural networks,0.6664383411407471
translation,399,104,experimental-setup,activation function,is,relu,activation function is relu,0.6688944697380066
translation,399,104,experimental-setup,fully connected neural networks,is,relu,fully connected neural networks is relu,0.5711885094642639
translation,399,104,experimental-setup,experimental setup,has,number of nodes,experimental setup has number of nodes,0.5328719615936279
translation,399,104,experimental-setup,experimental setup,has,activation function,experimental setup has activation function,0.48881152272224426
translation,399,105,experimental-setup,optimizer,set to,adagradoptimizer,optimizer set to adagradoptimizer,0.602275550365448
translation,399,105,experimental-setup,learning rate,set to,0.01,learning rate set to 0.01,0.6997436881065369
translation,399,105,experimental-setup,0.01,has,initially,0.01 has initially,0.5649867057800293
translation,399,105,experimental-setup,experimental setup,has,optimizer,experimental setup has optimizer,0.5528271794319153
translation,399,105,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,399,6,model,bi-directional long-short term memory networks,applied in,our methods,bi-directional long-short term memory networks applied in our methods,0.6244547963142395
translation,399,6,model,bi-directional long-short term memory networks,to extract,semantic information,bi-directional long-short term memory networks to extract semantic information,0.661683976650238
translation,399,6,model,our methods,to extract,semantic information,our methods to extract semantic information,0.6861092448234558
translation,399,6,model,semantic information,from,questions and answers ( comments ),semantic information from questions and answers ( comments ),0.5489867329597473
translation,399,6,model,model,has,convolutional neural networks,model has convolutional neural networks,0.48890411853790283
translation,399,7,model,full advantage,of,question -comment semantic relevance,full advantage of question -comment semantic relevance,0.5549935698509216
translation,399,7,model,question -comment semantic relevance,deploy,interaction layer and augmented features,question -comment semantic relevance deploy interaction layer and augmented features,0.68565833568573
translation,399,7,model,interaction layer and augmented features,before calculating,similarity,interaction layer and augmented features before calculating similarity,0.7180250287055969
translation,399,7,model,model,to take,full advantage,model to take full advantage,0.6647300124168396
translation,399,26,model,deep neural networks framework,to measure,relevance,deep neural networks framework to measure relevance,0.6401432156562805
translation,399,26,model,deep neural networks framework,rank,candidates,deep neural networks framework rank candidates,0.697971761226654
translation,399,26,model,relevance,of,questions and comments,relevance of questions and comments,0.5660152435302734
translation,399,26,model,relevance,in,cqa task,relevance in cqa task,0.50271075963974
translation,399,26,model,questions and comments,in,cqa task,questions and comments in cqa task,0.4918641448020935
translation,399,26,model,candidates,according to,similarity,candidates according to similarity,0.680412769317627
translation,399,26,model,similarity,to,original question,similarity to original question,0.5836459994316101
translation,399,26,model,model,deploy,deep neural networks framework,model deploy deep neural networks framework,0.6572327613830566
translation,399,27,model,methods,based on,deep neural networks,methods based on deep neural networks,0.6292557120323181
translation,399,27,model,deep neural networks,extract,semantic features,deep neural networks extract semantic features,0.6404833197593689
translation,399,27,model,semantic features,from,question and the comment,semantic features from question and the comment,0.5618876218795776
translation,399,28,model,connection,between,question,connection between question,0.6743653416633606
translation,399,28,model,connection,between,comment,connection between comment,0.6641654372215271
translation,399,28,model,connection,apply,interaction layer,connection apply interaction layer,0.6359871625900269
translation,399,28,model,interaction layer,before calculating,similarity,interaction layer before calculating similarity,0.6923104524612427
translation,399,28,model,model,to increase,connection,model to increase connection,0.7672932744026184
translation,399,28,model,model,apply,interaction layer,model apply interaction layer,0.6676385998725891
translation,399,29,model,augmented features,to improve,performance,augmented features to improve performance,0.6716777086257935
translation,399,29,model,performance,of,deep neural networks,performance of deep neural networks,0.587662935256958
translation,399,29,model,model,add,augmented features,model add augmented features,0.6743091940879822
translation,399,110,results,significantly outperform,has,baselines,significantly outperform has baselines,0.5966368913650513
translation,399,111,results,neural network based methods,perform,quite stable,neural network based methods perform quite stable,0.5608327984809875
translation,399,111,results,results,has,neural network based methods,results has neural network based methods,0.5378287434577942
translation,399,112,results,multicnn,along with,augmented features and interaction layer,multicnn along with augmented features and interaction layer,0.5991557240486145
translation,399,112,results,multicnn,achieves,best map scores,multicnn achieves best map scores,0.6916202306747437
translation,399,112,results,best map scores,among,methods,best map scores among methods,0.5968676805496216
translation,399,112,results,results,has,multicnn,results has multicnn,0.5852648019790649
translation,399,116,results,our methods,surpass,baselines,our methods surpass baselines,0.6692047715187073
translation,399,116,results,best method,obtains,map,best method obtains map,0.6106534004211426
translation,399,116,results,map,of,13.55,map of 13.55,0.6131462454795837
translation,400,7,baselines,pairwise learning,to rank,methods,pairwise learning to rank methods,0.6607457399368286
translation,400,7,baselines,methods,on,rich set of hand designed and representation learning features,methods on rich set of hand designed and representation learning features,0.5177614092826843
translation,400,25,experimental-setup,"glove embeddings ( pennington et al. , 2014 )",pretrained using,6 billion tokens,"glove embeddings ( pennington et al. , 2014 ) pretrained using 6 billion tokens",0.7593283653259277
translation,400,25,experimental-setup,6 billion tokens,from,wikipedia - 2014 and gigaword dataset,6 billion tokens from wikipedia - 2014 and gigaword dataset,0.526282787322998
translation,400,25,experimental-setup,experimental setup,used,"glove embeddings ( pennington et al. , 2014 )","experimental setup used glove embeddings ( pennington et al. , 2014 )",0.6085965037345886
translation,400,42,experimental-setup,contrastive loss,as,loss function,contrastive loss as loss function,0.5339086651802063
translation,400,42,experimental-setup,loss function,to,siamese network,loss function to siamese network,0.5119329690933228
translation,400,43,experimental-setup,glove pretrained vectors ( 300 dimension ),fed as,input,glove pretrained vectors ( 300 dimension ) fed as input,0.6422342658042908
translation,400,43,experimental-setup,input,to,neural network,input to neural network,0.583171546459198
translation,400,43,experimental-setup,experimental setup,has,glove pretrained vectors ( 300 dimension ),experimental setup has glove pretrained vectors ( 300 dimension ),0.5303559899330139
translation,400,60,experimental-setup,final hidden embedding size,is,256 dimension,final hidden embedding size is 256 dimension,0.5615029335021973
translation,400,60,experimental-setup,256 dimension,for,our bi-gru network,256 dimension for our bi-gru network,0.5848875045776367
translation,400,60,experimental-setup,experimental setup,has,final hidden embedding size,experimental setup has final hidden embedding size,0.5329799652099609
translation,400,63,experimental-setup,1d - convolution,with,128 kernels,1d - convolution with 128 kernels,0.6315972805023193
translation,400,63,experimental-setup,stride,of,5,stride of 5,0.6592143774032593
translation,400,63,experimental-setup,stride,followed by,1d - max pool,stride followed by 1d - max pool,0.6800862550735474
translation,400,63,experimental-setup,5,followed by,1d - max pool,5 followed by 1d - max pool,0.6412257552146912
translation,400,63,experimental-setup,1d - max pool,with,pool-size,1d - max pool with pool-size,0.6547033190727234
translation,400,63,experimental-setup,pool-size,of,5,pool-size of 5,0.6905831694602966
translation,400,63,experimental-setup,dense layer,to create,128 dimension vector,dense layer to create 128 dimension vector,0.660060703754425
translation,400,63,experimental-setup,1d - convolution,has,stride,1d - convolution has stride,0.5908869504928589
translation,400,63,experimental-setup,experimental setup,use,1d - convolution,experimental setup use 1d - convolution,0.5787869691848755
translation,400,65,experimental-setup,keras 1 library,with,theano,keras 1 library with theano,0.6801908612251282
translation,400,65,experimental-setup,) backend,to train,above 3 models,) backend to train above 3 models,0.7460930943489075
translation,400,65,experimental-setup,theano,has,) backend,theano has ) backend,0.6343435049057007
translation,400,65,experimental-setup,experimental setup,use,keras 1 library,experimental setup use keras 1 library,0.6080055236816406
translation,400,66,experimental-setup,batch size,set to,64,batch size set to 64,0.7451491355895996
translation,400,66,experimental-setup,dropout rate,is,0.25,dropout rate is 0.25,0.550621747970581
translation,400,66,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,400,66,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,400,67,experimental-setup,experimental setup,run,25 epochs,experimental setup run 25 epochs,0.7399520874023438
translation,400,120,experiments,pointwise learning,to,rank method,pointwise learning to rank method,0.5455440282821655
translation,400,120,experiments,pointwise learning,got,inferior results,pointwise learning got inferior results,0.620653510093689
translation,400,44,model,final neural embeddings,generated by,various architectures,final neural embeddings generated by various architectures,0.5935073494911194
translation,400,44,model,model,has,final neural embeddings,model has final neural embeddings,0.5544811487197876
translation,400,62,model,convolution networks,as,neural network architecture,convolution networks as neural network architecture,0.5510644316673279
translation,400,62,model,neural network architecture,to generate,embeddings,neural network architecture to generate embeddings,0.7001573443412781
translation,400,62,model,embeddings,inside,siamese network,embeddings inside siamese network,0.7330663800239563
translation,400,62,model,model,use,convolution networks,model use convolution networks,0.6223139762878418
translation,400,84,model,word pair similarity,function of,shortest path,word pair similarity function of shortest path,0.640685498714447
translation,400,84,model,shortest path,between,words,shortest path between words,0.6555629372596741
translation,400,84,model,shortest path,height of,lowest common subsumer ( lcs ),shortest path height of lowest common subsumer ( lcs ),0.7160674929618835
translation,400,84,model,model,has,word pair similarity,model has word pair similarity,0.5395379066467285
translation,400,116,results,highest mrr,amongst,all the primary submissions,highest mrr amongst all the primary submissions,0.6052142977714539
translation,400,116,results,results,achieved,highest mrr,results achieved highest mrr,0.7579705119132996
translation,400,117,results,dev and test set accuracy,for,our system,dev and test set accuracy for our system,0.5674041509628296
translation,400,117,results,our system,with,each feature,our system with each feature,0.6313529014587402
translation,400,117,results,each feature,applied,incrementally,each feature applied incrementally,0.7046124339103699
translation,400,118,results,our both contrastive submissions,trained on,svm,our both contrastive submissions trained on svm,0.7901173830032349
translation,400,118,results,our both contrastive submissions,achieved,better test accuracy,our both contrastive submissions achieved better test accuracy,0.694842517375946
translation,400,118,results,better test accuracy,training on,logistic regression,better test accuracy training on logistic regression,0.7314929366111755
translation,400,118,results,results,has,our both contrastive submissions,results has our both contrastive submissions,0.5553883910179138
translation,401,164,ablation-analysis,low-resource forums,omitting,f d,low-resource forums omitting f d,0.7047922015190125
translation,401,164,ablation-analysis,f d,has,beneficial effect,f d has beneficial effect,0.5943832397460938
translation,401,164,ablation-analysis,ablation analysis,On,low-resource forums,ablation analysis On low-resource forums,0.5611330270767212
translation,401,166,ablation-analysis,domain-specific embeddings,contribute,positively,domain-specific embeddings contribute positively,0.6584263443946838
translation,401,166,ablation-analysis,highresource cqadupstack forums,has,domain-specific embeddings,highresource cqadupstack forums has domain-specific embeddings,0.5513580441474915
translation,401,166,ablation-analysis,ablation analysis,On,highresource cqadupstack forums,ablation analysis On highresource cqadupstack forums,0.5758796334266663
translation,401,167,ablation-analysis,either f g or f d,from,ensemble,either f g or f d from ensemble,0.620741069316864
translation,401,167,ablation-analysis,either f g or f d,lead to,significant drop,either f g or f d lead to significant drop,0.7470903992652893
translation,401,167,ablation-analysis,significant drop,in,map,significant drop in map,0.586338460445404
translation,401,167,ablation-analysis,ablation analysis,omitting,either f g or f d,ablation analysis omitting either f g or f d,0.7167364358901978
translation,401,168,ablation-analysis,biggest positive effect,on,mv - dase,biggest positive effect on mv - dase,0.6002799868583679
translation,401,168,ablation-analysis,significantly more harmful,than,omitting,significantly more harmful than omitting,0.5974555015563965
translation,401,168,ablation-analysis,use,has,biggest positive effect,use has biggest positive effect,0.5687395334243774
translation,401,168,ablation-analysis,omitting,has,any other single view,omitting has any other single view,0.5896051526069641
translation,401,168,ablation-analysis,ablation analysis,has,use,ablation analysis has use,0.5268670320510864
translation,401,27,baselines,mv - dase,uses,unlabeled in- domain data,mv - dase uses unlabeled in- domain data,0.6254324316978455
translation,401,27,baselines,baselines,has,mv - dase,baselines has mv - dase,0.54388827085495
translation,401,111,baselines,ir baseline,is,"bm25 ( robertson et al. , 1995 )","ir baseline is bm25 ( robertson et al. , 1995 )",0.5603823661804199
translation,401,111,baselines,"bm25 ( robertson et al. , 1995 )",with,default parameters,"bm25 ( robertson et al. , 1995 ) with default parameters",0.5628818273544312
translation,401,111,baselines,"elasticsearch 6.5.4 ( gormley and tong , 2015 )",with,default parameters,"elasticsearch 6.5.4 ( gormley and tong , 2015 ) with default parameters",0.51423180103302
translation,401,111,baselines,baselines,has,ir baseline,baselines has ir baseline,0.6062028408050537
translation,401,113,baselines,unsupervised baselines,are,? elmo,unsupervised baselines are ? elmo,0.5914380550384521
translation,401,130,baselines,bm25,is,tough baseline,bm25 is tough baseline,0.6159351468086243
translation,401,130,baselines,tough baseline,for,dqd,tough baseline for dqd,0.670920729637146
translation,401,130,baselines,baselines,has,bm25,baselines has bm25,0.582451343536377
translation,401,136,experiments,mv - dase,has,outperforms,mv - dase has outperforms,0.6504305005073547
translation,401,136,experiments,outperforms,has,bm25,outperforms has bm25,0.5911413431167603
translation,401,136,experiments,outperforms,has,significantly and almost consistently,outperforms has significantly and almost consistently,0.626893162727356
translation,401,136,experiments,bm25,has,significantly and almost consistently,bm25 has significantly and almost consistently,0.6119256615638733
translation,401,136,experiments,significantly and almost consistently,has,19 out of 20 test forums,significantly and almost consistently has 19 out of 20 test forums,0.6260163187980652
translation,401,114,hyperparameters,elmo like bert,SIF - weight,all vectors,elmo like bert SIF - weight all vectors,0.8375760912895203
translation,401,114,hyperparameters,language model,on,in-domain corpus,language model on in-domain corpus,0.5173075795173645
translation,401,114,hyperparameters,all vectors,according to,in-domain word probability,all vectors according to in-domain word probability,0.5592749118804932
translation,401,114,hyperparameters,all vectors,average over,layers and tokens,all vectors average over layers and tokens,0.7965417504310608
translation,401,114,hyperparameters,hyperparameters,treat,elmo like bert,hyperparameters treat elmo like bert,0.5986377596855164
translation,401,5,model,multi-view framework mv - dase,combines,ensemble of sentence encoders,multi-view framework mv - dase combines ensemble of sentence encoders,0.6821491122245789
translation,401,5,model,ensemble of sentence encoders,via,generalized canonical correlation analysis,ensemble of sentence encoders via generalized canonical correlation analysis,0.6211222410202026
translation,401,5,model,generalized canonical correlation analysis,using,unlabeled data only,generalized canonical correlation analysis using unlabeled data only,0.7241994142532349
translation,401,5,model,model,has,multi-view framework mv - dase,model has multi-view framework mv - dase,0.5703924894332886
translation,401,6,model,ensemble,includes,generic and domain-specific averaged word embeddings,ensemble includes generic and domain-specific averaged word embeddings,0.5440274477005005
translation,401,6,model,ensemble,includes,domain-finetuned bert,ensemble includes domain-finetuned bert,0.6245614290237427
translation,401,6,model,ensemble,includes,universal sentence encoder,ensemble includes universal sentence encoder,0.6095805764198303
translation,401,26,model,multi-view representation learning,from,word,multi-view representation learning from word,0.5647422075271606
translation,401,26,model,multi-view representation learning,propose,mv - dase,multi-view representation learning propose mv - dase,0.6165233254432678
translation,401,26,model,mv - dase,combines,ensemble of sentence encoders,mv - dase combines ensemble of sentence encoders,0.6902215480804443
translation,401,26,model,ensemble of sentence encoders,via,generalized canonical correlation analysis,ensemble of sentence encoders via generalized canonical correlation analysis,0.6211222410202026
translation,401,26,model,mv - dase,has,multi-view domain adapted sentence embeddings ),mv - dase has multi-view domain adapted sentence embeddings ),0.5856415629386902
translation,401,26,model,model,take,multi-view representation learning,model take multi-view representation learning,0.5882165431976318
translation,401,26,model,model,propose,mv - dase,model propose mv - dase,0.6222351789474487
translation,401,119,model,gcca,with,naive view concatenation,gcca with naive view concatenation,0.5777660608291626
translation,401,119,model,gcca,with,view averaging,gcca with view averaging,0.6923463344573975
translation,401,119,model,model,replace,gcca,model replace gcca,0.6481299996376038
translation,401,139,results,views,that make up,ensemble,views that make up ensemble,0.674561083316803
translation,401,139,results,mv - dase,has,outperforms,mv - dase has outperforms,0.6504305005073547
translation,401,139,results,outperforms,has,views,outperforms has views,0.6218852996826172
translation,401,139,results,ensemble,has,significantly and almost consistently,ensemble has significantly and almost consistently,0.6065599322319031
translation,401,139,results,results,has,mv - dase,results has mv - dase,0.5539402961730957
translation,401,144,results,word- level cca baseline,has,outperforms,word- level cca baseline has outperforms,0.6072027683258057
translation,401,144,results,outperforms,has,f g and f d,outperforms has f g and f d,0.6841506958007812
translation,401,144,results,results,has,word- level cca baseline,results has word- level cca baseline,0.49502742290496826
translation,401,149,results,supervised domain-adversarial ada,performs,significantly worse,supervised domain-adversarial ada performs significantly worse,0.5826852917671204
translation,401,149,results,significantly worse,than,unsupervised mv - dase,significantly worse than unsupervised mv - dase,0.5995174050331116
translation,401,149,results,results,has,supervised domain-adversarial ada,results has supervised domain-adversarial ada,0.5459953546524048
translation,401,162,results,domain-finetuned elmo,performs,comparably,domain-finetuned elmo performs comparably,0.6029994487762451
translation,401,162,results,comparably,to,domain-finetuned bert,comparably to domain-finetuned bert,0.6263662576675415
translation,401,162,results,domain-finetuned bert,on,some forums,domain-finetuned bert on some forums,0.5761014819145203
translation,401,162,results,results,has,domain-finetuned elmo,results has domain-finetuned elmo,0.5570853352546692
translation,401,171,results,naive concatenation or averaging of views,is,significantly less effective,naive concatenation or averaging of views is significantly less effective,0.5114313960075378
translation,401,171,results,significantly less effective,than,view correlation,significantly less effective than view correlation,0.5900056958198547
translation,401,171,results,view correlation,by,gcca,view correlation by gcca,0.5684571862220764
translation,401,171,results,results,has,naive concatenation or averaging of views,results has naive concatenation or averaging of views,0.5559806823730469
translation,401,187,results,mv - dase,has,outperforms,mv - dase has outperforms,0.6504305005073547
translation,401,187,results,outperforms,has,individual views,outperforms has individual views,0.5918949842453003
translation,401,187,results,outperforms,has,concatenation and average,outperforms has concatenation and average,0.6181991696357727
translation,401,188,results,previous state of the art ( a supervised system ),by,margin,previous state of the art ( a supervised system ) by margin,0.5943723320960999
translation,401,188,results,margin,of,2.5 % map,margin of 2.5 % map,0.5997534394264221
translation,401,188,results,results,beats,previous state of the art ( a supervised system ),results beats previous state of the art ( a supervised system ),0.6894229650497437
translation,402,7,ablation-analysis,difficulty,best predicted using,signal,difficulty best predicted using signal,0.746135413646698
translation,402,7,ablation-analysis,signal,from,item stem,signal from item stem,0.6244267821311951
translation,402,7,ablation-analysis,item stem,has,description of the clinical case ),item stem has description of the clinical case ),0.5919780731201172
translation,402,7,ablation-analysis,ablation analysis,has,difficulty,ablation analysis has difficulty,0.5313053727149963
translation,402,68,baselines,first approach,used,pre-trained elmo parameters,first approach used pre-trained elmo parameters,0.5998910069465637
translation,402,68,baselines,first approach,trained on,mcq data,first approach trained on mcq data,0.791234016418457
translation,402,68,baselines,pre-trained elmo parameters,as,initialization,pre-trained elmo parameters as initialization,0.4428808391094208
translation,402,68,baselines,baselines,has,first approach,baselines has first approach,0.5953190326690674
translation,402,69,experimental-setup,nvidia tesla m60 gpu,to accelerate,model training,nvidia tesla m60 gpu to accelerate model training,0.664188027381897
translation,402,69,experimental-setup,experimental setup,has,nvidia tesla m60 gpu,experimental setup has nvidia tesla m60 gpu,0.5245811343193054
translation,402,21,experiments,difficulty and response time parameters,for,"set of ?18,000 multiplechoice questions ( mcqs )","difficulty and response time parameters for set of ?18,000 multiplechoice questions ( mcqs )",0.5948852300643921
translation,402,21,experiments,"set of ?18,000 multiplechoice questions ( mcqs )",from,united states medical licensing examination ( usmle r ),"set of ?18,000 multiplechoice questions ( mcqs ) from united states medical licensing examination ( usmle r )",0.5184507966041565
translation,402,21,experiments,transfer learning,has,tl ),transfer learning has tl ),0.6369304656982422
translation,402,78,results,models,achieved,slight but significant rmse decrease,models achieved slight but significant rmse decrease,0.72001051902771
translation,402,78,results,slight but significant rmse decrease,compared to,zeror baseline,slight but significant rmse decrease compared to zeror baseline,0.6696650981903076
translation,402,79,results,prediction,of,response time variable,prediction of response time variable,0.6072099804878235
translation,402,79,results,method 2,has,significantly improved,method 2 has significantly improved,0.5777087807655334
translation,402,79,results,significantly improved,has,prediction,significantly improved has prediction,0.5727071762084961
translation,402,79,results,results,has,method 2,results has method 2,0.501240611076355
translation,402,81,results,stem alone,provided,best results,stem alone provided best results,0.6735174059867859
translation,402,81,results,best results,for,p-value variable,best results for p-value variable,0.5771333575248718
translation,402,81,results,p-value variable,in,method 1 ( 23.32 ),p-value variable in method 1 ( 23.32 ),0.4344635605812073
translation,402,81,results,pvalue,used as,auxiliary task,pvalue used as auxiliary task,0.6181746125221252
translation,402,81,results,auxiliary task,for predicting,response time,auxiliary task for predicting response time,0.6781251430511475
translation,402,81,results,response time,in,method 2,response time in method 2,0.5378945469856262
translation,402,81,results,response time,has,0.31 ),response time has 0.31 ),0.5485052466392517
translation,402,81,results,results,Signal from,stem alone,results Signal from stem alone,0.6510148048400879
translation,402,82,results,signal,from,full item,signal from full item,0.5896306037902832
translation,402,82,results,response time,predicted using,method 1 ( 0.29 ),response time predicted using method 1 ( 0.29 ),0.643044650554657
translation,402,82,results,response time,used as,auxiliary task,response time used as auxiliary task,0.6127150654792786
translation,402,82,results,auxiliary task,for predicting,p-value ( 23.04 ),auxiliary task for predicting p-value ( 23.04 ),0.629382848739624
translation,402,82,results,full item,has,outperformed,full item has outperformed,0.6401748061180115
translation,402,82,results,outperformed,has,other configurations,outperformed has other configurations,0.6361366510391235
translation,402,82,results,results,has,signal,results has signal,0.4737548828125
translation,402,86,results,stem and options content,as,two predictors ( stem + options ),stem and options content as two predictors ( stem + options ),0.4963564872741699
translation,402,86,results,slightly more accurate results,than,single predictor ( full item ),slightly more accurate results than single predictor ( full item ),0.5403752326965332
translation,402,86,results,two predictors ( stem + options ),has,no significant effects,two predictors ( stem + options ) has no significant effects,0.5550705790519714
translation,402,86,results,results,Using,stem and options content,results Using stem and options content,0.6098756790161133
translation,403,55,baselines,baselines,has,implementation 1 ( drqa ),baselines has implementation 1 ( drqa ),0.5762977004051208
translation,403,170,baselines,rankqa,use,general model,rankqa use general model,0.6627342104911804
translation,403,170,baselines,general model,trained on,all four datasets simultaneously,general model trained on all four datasets simultaneously,0.745000422000885
translation,403,170,baselines,baselines,For,rankqa,baselines For rankqa,0.6200728416442871
translation,403,144,experimental-setup,training splits,of,squad,training splits of squad,0.6155168414115906
translation,403,144,experimental-setup,training splits,of,curatedtrec,training splits of curatedtrec,0.5857830047607422
translation,403,144,experimental-setup,training splits,of,wikimovies,training splits of wikimovies,0.5763657093048096
translation,403,144,experimental-setup,training splits,of,webquestions,training splits of webquestions,0.5537880659103394
translation,403,144,experimental-setup,webquestions,for,training and model selection,webquestions for training and model selection,0.5873567461967468
translation,403,144,experimental-setup,experimental setup,use,training splits,experimental setup use training splits,0.5849418640136719
translation,403,152,experimental-setup,training,use,"adam ( kingma and ba , 2014 )","training use adam ( kingma and ba , 2014 )",0.6256451606750488
translation,403,152,experimental-setup,"adam ( kingma and ba , 2014 )",with,learning rate,"adam ( kingma and ba , 2014 ) with learning rate",0.5989180207252502
translation,403,152,experimental-setup,"adam ( kingma and ba , 2014 )",with,batch size,"adam ( kingma and ba , 2014 ) with batch size",0.6181668639183044
translation,403,152,experimental-setup,learning rate,of,0.0005,learning rate of 0.0005,0.6050455570220947
translation,403,152,experimental-setup,batch size,of,256,batch size of 256,0.6323750615119934
translation,403,152,experimental-setup,experimental setup,During,training,experimental setup During training,0.6835477948188782
translation,403,153,experimental-setup,hidden layer,set to,m = 512 units,hidden layer set to m = 512 units,0.6329964995384216
translation,403,153,experimental-setup,experimental setup,has,hidden layer,experimental setup has hidden layer,0.5278815627098083
translation,403,154,experimental-setup,number of top -n documents,to,n = 10,number of top -n documents to n = 10,0.5425971746444702
translation,403,154,experimental-setup,initially generated,to,k = 40,initially generated to k = 40,0.5732865333557129
translation,403,154,experimental-setup,experimental setup,set,number of top -n documents,experimental setup set number of top -n documents,0.6068074107170105
translation,403,154,experimental-setup,experimental setup,set,number of top -k candidate answers,experimental setup set number of top -k candidate answers,0.6278022527694702
translation,403,140,experiments,rankqa,has,information retrieval module,rankqa has information retrieval module,0.5707591772079468
translation,403,32,model,conventional two -staged process,with,additional third stage,conventional two -staged process with additional third stage,0.6674904227256775
translation,403,32,model,additional third stage,for,efficient answer reranking,additional third stage for efficient answer reranking,0.6368231773376465
translation,403,32,model,model,augment,conventional two -staged process,model augment conventional two -staged process,0.6733855605125427
translation,403,60,model,second qa pipeline,where,document reader,second qa pipeline where document reader,0.6581525206565857
translation,403,60,model,document reader,from,drqa,document reader from drqa,0.6199037432670593
translation,403,60,model,document reader,replaced with,"bert ( devlin et al. , 2019 )","document reader replaced with bert ( devlin et al. , 2019 )",0.7386570572853088
translation,403,60,model,model,implement,second qa pipeline,model implement second qa pipeline,0.7014227509498596
translation,403,39,results,rankqa,achieves,state - of - the - art performance,rankqa achieves state - of - the - art performance,0.6691843271255493
translation,403,39,results,state - of - the - art performance,across,3 established benchmark datasets,state - of - the - art performance across 3 established benchmark datasets,0.6272326707839966
translation,403,172,results,direct comparison,between,drqa and rankqa,direct comparison between drqa and rankqa,0.6722375750541687
translation,403,172,results,drqa and rankqa,demonstrates,performance improvement,drqa and rankqa demonstrates performance improvement,0.6893730163574219
translation,403,172,results,performance improvement,from,up to 7.0 percentage points,performance improvement from up to 7.0 percentage points,0.5429839491844177
translation,403,172,results,performance improvement,with,average gain,performance improvement with average gain,0.6259965300559998
translation,403,172,results,up to 7.0 percentage points,when using,rankqa,up to 7.0 percentage points when using rankqa,0.7411448955535889
translation,403,172,results,average gain,of,4.9 percentage points,average gain of 4.9 percentage points,0.5639299750328064
translation,403,172,results,4.9 percentage points,over,all datasets,4.9 percentage points over all datasets,0.6578455567359924
translation,403,172,results,results,has,direct comparison,results has direct comparison,0.5469655990600586
translation,403,174,results,outperforms,by,notable margin,outperforms by notable margin,0.6289628148078918
translation,403,174,results,all other state - of - the - art qa systems,in,3 out of 4 datasets,all other state - of - the - art qa systems in 3 out of 4 datasets,0.4835924804210663
translation,403,174,results,all other state - of - the - art qa systems,by,notable margin,all other state - of - the - art qa systems by notable margin,0.5871327519416809
translation,403,174,results,rankqa,has,outperforms,rankqa has outperforms,0.5379952192306519
translation,403,174,results,outperforms,has,all other state - of - the - art qa systems,outperforms has all other state - of - the - art qa systems,0.5654955506324768
translation,403,174,results,results,has,rankqa,results has rankqa,0.5793048143386841
translation,403,177,results,performance improvements,of,up to 9.3 percentage points,performance improvements of up to 9.3 percentage points,0.5698418617248535
translation,403,177,results,performance improvements,with,average performance gain,performance improvements with average performance gain,0.6128612160682678
translation,403,177,results,average performance gain,of,5.8 percentage points,average performance gain of 5.8 percentage points,0.5615624785423279
translation,403,177,results,results,achieve,performance improvements,results achieve performance improvements,0.6524728536605835
translation,403,189,results,re-ranking module,in,qa pipeline,re-ranking module in qa pipeline,0.5050092935562134
translation,403,189,results,results,after integrating,re-ranking module,results after integrating re-ranking module,0.7360970973968506
translation,403,190,results,answer re-ranking,yields,significant performance improvements,answer re-ranking yields significant performance improvements,0.6752282977104187
translation,403,190,results,significant performance improvements,over,all four datasets,significant performance improvements over all four datasets,0.641270637512207
translation,403,190,results,significant performance improvements,ranging between,12.5 and 5.5 percentage points,significant performance improvements ranging between 12.5 and 5.5 percentage points,0.6849671602249146
translation,403,190,results,results,has,answer re-ranking,results has answer re-ranking,0.5873667597770691
translation,403,201,results,answer re-ranking,represents,efficient remedy,answer re-ranking represents efficient remedy,0.6378008127212524
translation,403,201,results,efficient remedy,against,noise- information tradeoff,efficient remedy against noise- information tradeoff,0.6415109634399414
translation,403,201,results,results,see that,answer re-ranking,results see that answer re-ranking,0.6655791401863098
translation,404,5,experiments,each question,into,entity mention and a relation pattern,each question into entity mention and a relation pattern,0.5543957352638245
translation,404,104,hyperparameters,five hundred neurons,used in,convolutional layer,five hundred neurons used in convolutional layer,0.6369971632957458
translation,404,104,hyperparameters,five hundred neurons,used in,max-pooling layer,five hundred neurons used in max-pooling layer,0.5965670943260193
translation,404,104,hyperparameters,five hundred neurons,used in,final semantic layer,five hundred neurons used in final semantic layer,0.6426031589508057
translation,404,104,hyperparameters,hyperparameters,has,five hundred neurons,hyperparameters has five hundred neurons,0.538524866104126
translation,404,105,hyperparameters,learning rate,of,0.002,learning rate of 0.002,0.5967087745666504
translation,404,105,hyperparameters,converged,after,150 iterations,converged after 150 iterations,0.6215254068374634
translation,404,105,hyperparameters,training,has,converged,training has converged,0.6140456199645996
translation,404,4,model,semantic parsing framework,based on,semantic similarity,semantic parsing framework based on semantic similarity,0.5651801228523254
translation,404,4,model,semantic similarity,for,open domain question answering ( qa ),semantic similarity for open domain question answering ( qa ),0.5917612910270691
translation,404,4,model,model,develop,semantic parsing framework,model develop semantic parsing framework,0.5728390216827393
translation,404,6,model,convolutional neural network models,measure,similarity,convolutional neural network models measure similarity,0.6230854988098145
translation,404,6,model,convolutional neural network models,measure,similarity,convolutional neural network models measure similarity,0.6230854988098145
translation,404,6,model,similarity,of,entity mentions with entities,similarity of entity mentions with entities,0.5511143803596497
translation,404,6,model,similarity,of,relation patterns and relations,similarity of relation patterns and relations,0.5938544273376465
translation,404,6,model,similarity,of,relation patterns and relations,similarity of relation patterns and relations,0.5938544273376465
translation,404,6,model,entity mentions with entities,in,knowledge base ( kb ),entity mentions with entities in knowledge base ( kb ),0.5047302842140198
translation,404,6,model,similarity,of,relation patterns and relations,similarity of relation patterns and relations,0.5938544273376465
translation,404,6,model,relation patterns and relations,in,kb,relation patterns and relations in kb,0.5331125259399414
translation,404,6,model,model,Using,convolutional neural network models,model Using convolutional neural network models,0.588952362537384
translation,404,16,model,semantic parsing framework,tailored to,single-relation questions,semantic parsing framework tailored to single-relation questions,0.6092798113822937
translation,404,16,model,model,propose,semantic parsing framework,model propose semantic parsing framework,0.6296065449714661
translation,404,17,model,novel semantic similarity model,using,convolutional neural networks,novel semantic similarity model using convolutional neural networks,0.5640242695808411
translation,404,57,model,convolutional neural network based semantic model,develop,new convolutional neural network ( cnn ) based semantic model ( cnnsm ),convolutional neural network based semantic model develop new convolutional neural network ( cnn ) based semantic model ( cnnsm ),0.6327933073043823
translation,404,57,model,new convolutional neural network ( cnn ) based semantic model ( cnnsm ),for,semantic parsing,new convolutional neural network ( cnn ) based semantic model ( cnnsm ) for semantic parsing,0.5995019674301147
translation,404,57,model,model,develop,new convolutional neural network ( cnn ) based semantic model ( cnnsm ),model develop new convolutional neural network ( cnn ) based semantic model ( cnnsm ),0.618098258972168
translation,404,57,model,model,has,convolutional neural network based semantic model,model has convolutional neural network based semantic model,0.5302718281745911
translation,404,59,model,cnnsm,uses,max pooling layer,cnnsm uses max pooling layer,0.5543192028999329
translation,404,59,model,max pooling layer,to extract,most salient local features,max pooling layer to extract most salient local features,0.657291829586029
translation,404,59,model,most salient local features,to form,fixed - length global feature vector,most salient local features to form fixed - length global feature vector,0.5771100521087646
translation,404,127,model,semantic parsing framework,for,single-relation questions,semantic parsing framework for single-relation questions,0.5189245343208313
translation,404,127,model,model,propose,semantic parsing framework,model propose semantic parsing framework,0.6296065449714661
translation,404,128,model,relation patterns and entity mentions,using,semantic similarity function,relation patterns and entity mentions using semantic similarity function,0.6530911922454834
translation,404,128,model,semantic similarity function,rather than,lexical rules,semantic similarity function rather than lexical rules,0.6441119313240051
translation,404,129,model,similarity model,trained using,convolutional neural networks,similarity model trained using convolutional neural networks,0.6777418255805969
translation,404,129,model,convolutional neural networks,with,letter-trigrams vectors,convolutional neural networks with letter-trigrams vectors,0.5967429876327515
translation,404,129,model,model,has,similarity model,model has similarity model,0.5632039904594421
translation,404,20,results,general semantic similarity model,to match,patterns and relations,general semantic similarity model to match patterns and relations,0.7376267313957214
translation,404,20,results,higher precision,at,all the recall points,higher precision at all the recall points,0.5433499217033386
translation,404,20,results,questions,in,same test set,questions in same test set,0.5400063991546631
translation,404,20,results,patterns and relations,has,as well as mentions and entities,patterns and relations has as well as mentions and entities,0.6003116369247437
translation,404,20,results,outperforms,has,existing rule learning system,outperforms has existing rule learning system,0.5941652059555054
translation,404,20,results,results,By using,general semantic similarity model,results By using general semantic similarity model,0.6925102472305298
translation,404,121,results,precision,of,our cnnsm pm system,precision of our cnnsm pm system,0.5799043774604797
translation,404,121,results,our cnnsm pm system,is,consistently higher,our cnnsm pm system is consistently higher,0.6246126890182495
translation,404,121,results,consistently higher,than,paralex,consistently higher than paralex,0.6369684934616089
translation,404,121,results,consistently higher,across,all recall regions,consistently higher across all recall regions,0.7432669997215271
translation,404,122,results,cnnsm m system,performs,similarly,cnnsm m system performs similarly,0.6931112408638
translation,404,122,results,cnnsm m system,performs,inferior,cnnsm m system performs inferior,0.6965769529342651
translation,404,122,results,similarly,to,cnnsm pm,similarly to cnnsm pm,0.6342145204544067
translation,404,122,results,cnnsm pm,in,high precision regime,cnnsm pm in high precision regime,0.5414990782737732
translation,404,122,results,inferior,when,recall,inferior when recall,0.6766409277915955
translation,404,122,results,results,has,cnnsm m system,results has cnnsm m system,0.5885279178619385
translation,404,131,results,our method,achieves,higher precision,our method achieves higher precision,0.6499314308166504
translation,404,131,results,higher precision,on,qa task,higher precision on qa task,0.5825188159942627
translation,404,131,results,qa task,than,"previous work , paralex","qa task than previous work , paralex",0.5629794001579285
translation,404,131,results,results,has,our method,results has our method,0.5589964985847473
translation,405,222,ablation-analysis,drops sharply,for,both neural network approaches,drops sharply for both neural network approaches,0.6557273864746094
translation,405,222,ablation-analysis,diversity,has,relative change in parameters,diversity has relative change in parameters,0.5539292693138123
translation,405,222,ablation-analysis,relative change in parameters,has,drops sharply,relative change in parameters has drops sharply,0.6005842685699463
translation,405,222,ablation-analysis,ablation analysis,bring in,diversity,ablation analysis bring in diversity,0.653860867023468
translation,405,232,ablation-analysis,sp - regularizer,is,important,sp - regularizer is important,0.5672149658203125
translation,405,232,ablation-analysis,ablation analysis,choice of,sp - regularizer,ablation analysis choice of sp - regularizer,0.6450265049934387
translation,405,241,ablation-analysis,ablation analysis,has,importance weighting ( iw ),ablation analysis has importance weighting ( iw ),0.5482170581817627
translation,405,245,ablation-analysis,strategy,is,more important,strategy is more important,0.5603851675987244
translation,405,245,ablation-analysis,strategy,leads to,improvements,strategy leads to improvements,0.7037306427955627
translation,405,245,ablation-analysis,more important,for,memory networks,more important for memory networks,0.6504594087600708
translation,405,245,ablation-analysis,more important,leads to,improvements,more important leads to improvements,0.7024019360542297
translation,405,245,ablation-analysis,improvements,on,all the tasks,improvements on all the tasks,0.5223153233528137
translation,405,245,ablation-analysis,ablation analysis,has,strategy,ablation analysis has strategy,0.489571213722229
translation,405,31,baselines,alignment - based approach,for,machine comprehension,alignment - based approach for machine comprehension,0.6029372215270996
translation,405,31,baselines,alignment - based approach,for,multiple -choice elementary science test,alignment - based approach for multiple -choice elementary science test,0.6191263794898987
translation,405,31,baselines,alignment - based approach,for,multiple -choice elementary science test,alignment - based approach for multiple -choice elementary science test,0.6191263794898987
translation,405,31,baselines,qanta,has,recursive neural network,qanta has recursive neural network,0.5851801037788391
translation,405,150,baselines,baselines,has,farthest from decision boundary ( ffdb ),baselines has farthest from decision boundary ( ffdb ),0.5491324067115784
translation,405,165,baselines,baselines,has,importance -weighting ( iw ),baselines has importance -weighting ( iw ),0.5600935220718384
translation,405,16,model,methods,for,learning,methods for learning,0.5613669157028198
translation,405,16,model,learning,in the context of,non-convex models,learning in the context of non-convex models,0.5325086116790771
translation,405,16,model,curriculum,in the context of,non-convex models,curriculum in the context of non-convex models,0.6424363851547241
translation,405,16,model,non-convex models,for,question answering,non-convex models for question answering,0.6012668609619141
translation,405,16,model,learning,has,curriculum,learning has curriculum,0.6104846000671387
translation,405,16,model,model,explore,methods,model explore methods,0.5866641998291016
translation,405,217,results,heuristics ( and improvements,lead to,improvements,heuristics ( and improvements lead to improvements,0.6838892698287964
translation,405,217,results,improvements,in,final test accuracy,improvements in final test accuracy,0.49879416823387146
translation,405,217,results,final test accuracy,for,alignment - based models,final test accuracy for alignment - based models,0.6036017537117004
translation,405,217,results,final test accuracy,for,qanta,final test accuracy for qanta,0.6178061366081238
translation,405,217,results,results,observe,variants of spl ( and e&e ),results observe variants of spl ( and e&e ),0.5928806066513062
translation,405,233,results,soft regularizers,perform,better,soft regularizers perform better,0.5695967078208923
translation,405,233,results,better,than,hard regularizer,better than hard regularizer,0.5896170735359192
translation,405,233,results,results,has,soft regularizers,results has soft regularizers,0.5470502972602844
translation,405,234,results,mixed regularizer,with,mixture weighting ),mixed regularizer with mixture weighting ),0.6713886260986328
translation,405,234,results,mixed regularizer,performs,even better,mixed regularizer performs even better,0.5761563777923584
translation,405,234,results,results,has,mixed regularizer,results has mixed regularizer,0.5709151029586792
translation,405,235,results,all the heuristics,work as well as,spl,all the heuristics work as well as spl,0.7149137258529663
translation,405,235,results,results,observe,all the heuristics,results observe all the heuristics,0.5896983742713928
translation,405,238,results,very similar performance,to,spl,very similar performance to spl,0.6091515421867371
translation,405,238,results,spl,with,hard sp - regularizer,spl with hard sp - regularizer,0.609376847743988
translation,405,238,results,ecio heuristic,has,very similar performance,ecio heuristic has very similar performance,0.5783904790878296
translation,405,238,results,results,has,ecio heuristic,results has ecio heuristic,0.5952163338661194
translation,405,240,results,ensemble,is,significant improvement,ensemble is significant improvement,0.5711342692375183
translation,405,240,results,significant improvement,over,individual heuristics,significant improvement over individual heuristics,0.6721328496932983
translation,405,240,results,results,has,ensemble,results has ensemble,0.5487152338027954
translation,405,244,results,e&e,provides,improvements,e&e provides improvements,0.7090736031532288
translation,405,244,results,improvements,across,all the experiments,improvements across all the experiments,0.6845147609710693
translation,405,244,results,improvements,for,all the spl experiments,improvements for all the spl experiments,0.6292072534561157
translation,405,244,results,all the experiments,for,all the spl experiments,all the experiments for all the spl experiments,0.6240480542182922
translation,405,244,results,results,has,e&e,results has e&e,0.543891429901123
translation,405,256,results,final epochs,of,curriculum learning,final epochs of curriculum learning,0.6112208366394043
translation,405,256,results,final epochs,of,curriculum learning,final epochs of curriculum learning,0.6112208366394043
translation,405,256,results,curriculum learning,see,greater gain,curriculum learning see greater gain,0.629602313041687
translation,405,256,results,greater gain,in,test accuracy,greater gain in test accuracy,0.5336636304855347
translation,405,256,results,test accuracy,for,grade 5 questions,test accuracy for grade 5 questions,0.5982020497322083
translation,405,256,results,results,has,final epochs,results has final epochs,0.5495871901512146
translation,406,210,ablation-analysis,both modules,contribute to,final veracity prediction performance,both modules contribute to final veracity prediction performance,0.6544845700263977
translation,406,210,ablation-analysis,final veracity prediction performance,in,either label setting,final veracity prediction performance in either label setting,0.48536422848701477
translation,406,210,ablation-analysis,ablation analysis,has,both modules,ablation analysis has both modules,0.5615182518959045
translation,406,211,ablation-analysis,multi-view agreement matching,with,single- view matching operation,multi-view agreement matching with single- view matching operation,0.5776917934417725
translation,406,211,ablation-analysis,multi-view agreement matching,leads to,inferior performance,multi-view agreement matching leads to inferior performance,0.636756956577301
translation,406,211,ablation-analysis,ablation analysis,replace,multi-view agreement matching,ablation analysis replace multi-view agreement matching,0.54556804895401
translation,406,174,baselines,baselines,has,"declare ( popat et al. , 2018 )","baselines has declare ( popat et al. , 2018 )",0.5101739764213562
translation,406,177,baselines,"nsmn ( nie et al. , 2019 )",has,pipeline - based system,"nsmn ( nie et al. , 2019 ) has pipeline - based system",0.5918210744857788
translation,406,177,baselines,baselines,has,"nsmn ( nie et al. , 2019 )","baselines has nsmn ( nie et al. , 2019 )",0.5189138650894165
translation,406,179,baselines,baselines,has,"mul-tifc ( augenstein et al. , 2019 )","baselines has mul-tifc ( augenstein et al. , 2019 )",0.5419350266456604
translation,406,159,experimental-setup,pre-trained 300d glove word vectors,to initialize,embedding matrix,pre-trained 300d glove word vectors to initialize embedding matrix,0.6903865337371826
translation,406,159,experimental-setup,experimental setup,utilize,pre-trained 300d glove word vectors,experimental setup utilize pre-trained 300d glove word vectors,0.5305356979370117
translation,406,160,experimental-setup,k,set to,5,k set to 5,0.7577354311943054
translation,406,160,experimental-setup,experimental setup,has,k,experimental setup has k,0.5740119814872742
translation,406,161,experimental-setup,hidden dimension,of,bi-gru,hidden dimension of bi-gru,0.5884360074996948
translation,406,161,experimental-setup,bi-gru,set to be,256,bi-gru set to be 256,0.7376946806907654
translation,406,161,experimental-setup,256,with,dropout,256 with dropout,0.7203097939491272
translation,406,161,experimental-setup,dropout,of,0.4,dropout of 0.4,0.6157568693161011
translation,406,161,experimental-setup,experimental setup,has,hidden dimension,experimental setup has hidden dimension,0.5457288026809692
translation,406,162,experimental-setup,evidence agreement matching module,perform,grid search,evidence agreement matching module perform grid search,0.5964937806129456
translation,406,162,experimental-setup,grid search,over,n a and d a,grid search over n a and d a,0.7079476118087769
translation,406,162,experimental-setup,experimental setup,For,evidence agreement matching module,experimental setup For evidence agreement matching module,0.5805114507675171
translation,406,163,experimental-setup,relu,used as,activation function,relu used as activation function,0.6731882095336914
translation,406,163,experimental-setup,activation function,in,mlp layer,activation function in mlp layer,0.4895295798778534
translation,406,163,experimental-setup,experimental setup,has,relu,experimental setup has relu,0.6043177843093872
translation,406,164,experimental-setup,batches,of,answers,batches of answers,0.640353262424469
translation,406,164,experimental-setup,answers,with,similar length,answers with similar length,0.6311929821968079
translation,406,164,experimental-setup,batch size,being,64,batch size being 64,0.630869448184967
translation,406,164,experimental-setup,experimental setup,assemble,batches,experimental setup assemble batches,0.6141065955162048
translation,406,165,experimental-setup,adam optimiser,with,learning rate,adam optimiser with learning rate,0.6170838475227356
translation,406,165,experimental-setup,adam optimiser,train,models,adam optimiser train models,0.6764038801193237
translation,406,165,experimental-setup,learning rate,of,0.0005,learning rate of 0.0005,0.6050455570220947
translation,406,165,experimental-setup,models,on,two tesla k80 gpus,models on two tesla k80 gpus,0.5328953862190247
translation,406,165,experimental-setup,experimental setup,use,adam optimiser,experimental setup use adam optimiser,0.5980468988418579
translation,406,166,experimental-setup,overfitting,conduct,early stopping,overfitting conduct early stopping,0.6428064107894897
translation,406,166,experimental-setup,overfitting,add,l2 regularization,overfitting add l2 regularization,0.5860273241996765
translation,406,166,experimental-setup,early stopping,on,validation set,early stopping on validation set,0.5784603357315063
translation,406,166,experimental-setup,validation set,with,patience,validation set with patience,0.662807822227478
translation,406,166,experimental-setup,patience,being,5,patience being 5,0.6849094033241272
translation,406,166,experimental-setup,l2 regularization,with,weight,l2 regularization with weight,0.5956174731254578
translation,406,166,experimental-setup,weight,of,0.002,weight of 0.002,0.5592438578605652
translation,406,166,experimental-setup,experimental setup,To avoid,overfitting,experimental setup To avoid overfitting,0.6404601335525513
translation,406,166,experimental-setup,experimental setup,add,l2 regularization,experimental setup add l2 regularization,0.5691570043563843
translation,406,178,experiments,claim verification module,for,our task,claim verification module for our task,0.5953365564346313
translation,406,186,experiments,de-clare model,obtains,similar performance,de-clare model obtains similar performance,0.6535745859146118
translation,406,186,experiments,similar performance,with,cnn - claim method,similar performance with cnn - claim method,0.6228389143943787
translation,406,6,model,large scale fact checking dataset,from,product question answering forums,large scale fact checking dataset from product question answering forums,0.5046672224998474
translation,406,6,model,answer - fact,has,large scale fact checking dataset,answer - fact has large scale fact checking dataset,0.5506607890129089
translation,406,6,model,model,introduce,answer - fact,model introduce answer - fact,0.6881264448165894
translation,406,38,model,large scale fact checking dataset,called,an-swerfact,large scale fact checking dataset called an-swerfact,0.6577134728431702
translation,406,38,model,an-swerfact,for investigating,answer veracity,an-swerfact for investigating answer veracity,0.7246291637420654
translation,406,38,model,answer veracity,in,product question answering forums,answer veracity in product question answering forums,0.5171242356300354
translation,406,38,model,model,introduce,large scale fact checking dataset,model introduce large scale fact checking dataset,0.5956317782402039
translation,406,45,model,answer veracity prediction model,with,tailored evidence ranking modules,answer veracity prediction model with tailored evidence ranking modules,0.6290521621704102
translation,406,45,model,tailored evidence ranking modules,to predict,answer veracity,tailored evidence ranking modules to predict answer veracity,0.7332329154014587
translation,406,45,model,answer veracity,in,pqa forums,answer veracity in pqa forums,0.5455244779586792
translation,406,46,model,aver,utilizes,information,aver utilizes information,0.7197762131690979
translation,406,46,model,information,from,question and answer text,information from question and answer text,0.513093888759613
translation,406,46,model,information,both,question and answer text,information both question and answer text,0.6148674488067627
translation,406,46,model,information,to rank,evidence sentences,information to rank evidence sentences,0.725673496723175
translation,406,46,model,evidence sentences,with,different gating mechanisms,evidence sentences with different gating mechanisms,0.5822778940200806
translation,406,46,model,model,has,aver,model has aver,0.6301775574684143
translation,406,172,model,two claim-focused fact checking models,based on,"cnn ( rashkin et al. , 2017 )","two claim-focused fact checking models based on cnn ( rashkin et al. , 2017 )",0.6621868014335632
translation,406,172,model,two claim-focused fact checking models,based on,"lstm ( rashkin et al. , 2017 )","two claim-focused fact checking models based on lstm ( rashkin et al. , 2017 )",0.6234238743782043
translation,406,172,model,"lstm ( rashkin et al. , 2017 )",for obtaining,claim representations,"lstm ( rashkin et al. , 2017 ) for obtaining claim representations",0.6348429322242737
translation,406,172,model,model,has,two claim-focused fact checking models,model has two claim-focused fact checking models,0.5700930953025818
translation,406,229,model,answerfact,has,evidence - based fact checking,answerfact has evidence - based fact checking,0.5782983303070068
translation,406,229,model,model,introduce,answerfact,model introduce answerfact,0.6613935828208923
translation,406,185,results,models,considering,evidence information,models considering evidence information,0.7100495100021362
translation,406,185,results,evidence information,e.g.,multifc and aver model,evidence information e.g. multifc and aver model,0.6925307512283325
translation,406,185,results,better results,those relying on,claim text only,better results those relying on claim text only,0.6673847436904907
translation,406,185,results,results,observed that,models,results observed that models,0.6753876209259033
translation,406,190,results,model,find that,aver without any gate,model find that aver without any gate,0.7254133820533752
translation,406,190,results,aver without any gate,achieve,better results,aver without any gate achieve better results,0.6310828328132629
translation,406,190,results,better results,than,most baseline models,better results than most baseline models,0.5164722204208374
translation,406,191,results,guide,from,question and answer information,guide from question and answer information,0.5656949877738953
translation,406,191,results,model,with,soft or hard gate mechanism,model with soft or hard gate mechanism,0.6216753721237183
translation,406,191,results,consistently outperforms,on,two label settings,consistently outperforms on two label settings,0.4971717596054077
translation,406,191,results,all baseline methods,on,two label settings,all baseline methods on two label settings,0.44290289282798767
translation,406,191,results,guide,has,model,guide has model,0.5959309935569763
translation,406,191,results,question and answer information,has,model,question and answer information has model,0.5783104300498962
translation,406,191,results,soft or hard gate mechanism,has,consistently outperforms,soft or hard gate mechanism has consistently outperforms,0.6086190938949585
translation,406,191,results,consistently outperforms,has,all baseline methods,consistently outperforms has all baseline methods,0.5647729635238647
translation,406,191,results,results,With,guide,results With guide,0.673009991645813
translation,406,193,results,model,with,soft gate,model with soft gate,0.6428617835044861
translation,406,193,results,model,with,hard gate,model with hard gate,0.6472430229187012
translation,406,193,results,model,with,hard gate,model with hard gate,0.6472430229187012
translation,406,193,results,better results,than,model,better results than model,0.6328511238098145
translation,406,193,results,model,with,hard gate,model with hard gate,0.6472430229187012
translation,406,193,results,results,notice that,model,results notice that model,0.6129556894302368
translation,406,202,results,our proposed model,is,superior,our proposed model is superior,0.6399104595184326
translation,406,202,results,superior,than,both alternatives,superior than both alternatives,0.618226945400238
translation,406,202,results,results,see that,our proposed model,results see that our proposed model,0.6661979556083679
translation,406,203,results,model,with,fully connected evidence ranking,model with fully connected evidence ranking,0.6377216577529907
translation,406,203,results,model,performs,even worse,model performs even worse,0.637363612651825
translation,406,203,results,even worse,than,averaging,even worse than averaging,0.6504612565040588
translation,406,203,results,averaging,has,evidence embeddings,averaging has evidence embeddings,0.5508081912994385
translation,406,203,results,results,noticed that,model,results noticed that model,0.6667574048042297
translation,407,28,ablation-analysis,both tasks,including,causal models,both tasks including causal models,0.6964484453201294
translation,407,28,ablation-analysis,causal models,has,significantly improves,causal models has significantly improves,0.5904732942581177
translation,407,28,ablation-analysis,significantly improves,has,performance,significantly improves has performance,0.5962982177734375
translation,407,28,ablation-analysis,ablation analysis,In,both tasks,ablation analysis In both tasks,0.5077076554298401
translation,407,28,ablation-analysis,ablation analysis,including,causal models,ablation analysis including causal models,0.704770028591156
translation,407,91,baselines,baselines,has,causal embedding model ( cembed ),baselines has causal embedding model ( cembed ),0.5800947546958923
translation,407,101,baselines,causal alignment model ( calign ),has,monolingual alignment ( or translation ),causal alignment model ( calign ) has monolingual alignment ( or translation ),0.5553185343742371
translation,407,101,baselines,baselines,has,causal alignment model ( calign ),baselines has causal alignment model ( calign ),0.59409099817276
translation,407,132,baselines,standard word2vec model,trained with,skip-gram algorithm,standard word2vec model trained with skip-gram algorithm,0.7105722427368164
translation,407,132,baselines,standard word2vec model,trained with,sliding window of 5,standard word2vec model trained with sliding window of 5,0.7153594493865967
translation,407,132,baselines,standard word2vec model,using,original texts,standard word2vec model using original texts,0.5946462750434875
translation,407,132,baselines,sliding window of 5,using,original texts,sliding window of 5 using original texts,0.6615864038467407
translation,407,132,baselines,vanilla embeddings model ( vembed ),has,standard word2vec model,vanilla embeddings model ( vembed ) has standard word2vec model,0.5228747129440308
translation,407,165,baselines,vanilla alignment model ( valign ),trained over,65 k question - answer pairs,vanilla alignment model ( valign ) trained over 65 k question - answer pairs,0.7679807543754578
translation,407,165,baselines,65 k question - answer pairs,from,yahoo,65 k question - answer pairs from yahoo,0.5377054214477539
translation,407,179,baselines,baselines,has,strongest baseline,baselines has strongest baseline,0.5783555507659912
translation,407,110,experimental-setup,network,using,binary cross entropy objective function,network using binary cross entropy objective function,0.6666548848152161
translation,407,110,experimental-setup,network,using,keras library,network using keras library,0.6843798756599426
translation,407,110,experimental-setup,network,using,keras library,network using keras library,0.6843798756599426
translation,407,110,experimental-setup,adam optimizer,using,keras library,adam optimizer using keras library,0.6286577582359314
translation,407,110,experimental-setup,experimental setup,train,network,experimental setup train network,0.6590225696563721
translation,407,111,experimental-setup,convolutional layer,contained,100 filters,convolutional layer contained 100 filters,0.5607073903083801
translation,407,111,experimental-setup,filter length,of,2,filter length of 2,0.6670847535133362
translation,407,111,experimental-setup,experimental setup,has,convolutional layer,experimental setup has convolutional layer,0.5196938514709473
translation,407,136,experimental-setup,pairs,were,randomly shuffled,pairs were randomly shuffled,0.7025207877159119
translation,407,136,experimental-setup,random,has,pairs,random has pairs,0.6294732093811035
translation,407,136,experimental-setup,experimental setup,has,random,experimental setup has random,0.5945391654968262
translation,407,136,experimental-setup,experimental setup,has,pairs,experimental setup has pairs,0.49610215425491333
translation,407,54,model,three steps,start by,bootstrapping,three steps start by bootstrapping,0.7046443819999695
translation,407,54,model,large number,of,cause-effect pairs,large number of cause-effect pairs,0.5760927796363831
translation,407,54,model,cause-effect pairs,from,free text,cause-effect pairs from free text,0.574047863483429
translation,407,54,model,bootstrapping,has,large number,bootstrapping has large number,0.5761488676071167
translation,407,54,model,model,start by,bootstrapping,model start by bootstrapping,0.721283495426178
translation,407,124,model,embedding models,rank,pairs,embedding models rank pairs,0.7257630825042725
translation,407,124,model,pairs,using,cosine similarity,pairs using cosine similarity,0.6760362386703491
translation,407,124,model,cosine similarity,between,target vector,cosine similarity between target vector,0.6148669123649597
translation,407,124,model,target vector,for,causal word,target vector for causal word,0.5913137793540955
translation,407,124,model,model,has,embedding models,model has embedding models,0.5956419706344604
translation,407,12,results,causality,improves,performance,causality improves performance,0.755664050579071
translation,407,12,results,performance,in,both tasks,performance in both tasks,0.468425989151001
translation,407,12,results,results,explicitly modeling,causality,results explicitly modeling causality,0.58045893907547
translation,407,139,results,causal models,better able to rank,causal pairs,causal models better able to rank causal pairs,0.7049254179000854
translation,407,139,results,causal pairs,than,vanilla embedding baseline ( vembed ),causal pairs than vanilla embedding baseline ( vembed ),0.5429831743240356
translation,407,139,results,outperforms,has,random baseline,outperforms has random baseline,0.5755338072776794
translation,407,139,results,results,has,causal models,results has causal models,0.4940863847732544
translation,407,140,results,our look - up baseline,ranks,pairs,our look - up baseline ranks pairs,0.7830305099487305
translation,407,140,results,pairs,by,frequency,pairs by frequency,0.6010490655899048
translation,407,140,results,pairs,shows,high precision,pairs shows high precision,0.7081807851791382
translation,407,140,results,frequency,in,our causal database,frequency in our causal database,0.5830895900726318
translation,407,140,results,coverage,for,only 35 %,coverage for only 35 %,0.6143642067909241
translation,407,140,results,only 35 %,of,causal semeval pairs,only 35 % of causal semeval pairs,0.620642364025116
translation,407,140,results,results,has,our look - up baseline,results has our look - up baseline,0.5675947666168213
translation,407,141,results,some models,perform,better,some models perform better,0.598842442035675
translation,407,141,results,better,on,low-recall portion of the curve,better on low-recall portion of the curve,0.5079424381256104
translation,407,141,results,embedding and alignment models,have,higher and more consistent performance,embedding and alignment models have higher and more consistent performance,0.5276669263839722
translation,407,141,results,higher and more consistent performance,across,pr curve,higher and more consistent performance across pr curve,0.6922706961631775
translation,407,141,results,results,has,some models,results has some models,0.5835086703300476
translation,407,150,results,best overall model,is,cembedbinoise,best overall model is cembedbinoise,0.5967605710029602
translation,407,150,results,best overall model,incorporates,noise handling approach,best overall model incorporates noise handling approach,0.7263268828392029
translation,407,150,results,cembedbinoise,is,bidirectional,cembedbinoise is bidirectional,0.6240735650062561
translation,407,150,results,results,has,best overall model,results has best overall model,0.554720938205719
translation,407,151,results,performance,low-recall portion of,curve,performance low-recall portion of curve,0.739739716053009
translation,407,151,results,substantially improves,has,performance,substantially improves has performance,0.5890060663223267
translation,407,184,results,no individual causal model,has,outperforms,no individual causal model has outperforms,0.6161693334579468
translation,407,184,results,outperforms,has,strong vanilla embedding baseline,outperforms has strong vanilla embedding baseline,0.5777221918106079
translation,407,184,results,results,has,no individual causal model,results has no individual causal model,0.5346183180809021
translation,407,189,results,lookup baseline ( lu ),to,bestperforming causal model,lookup baseline ( lu ) to bestperforming causal model,0.5436720252037048
translation,407,189,results,bestperforming causal model,has,does not improve,bestperforming causal model has does not improve,0.5906162261962891
translation,407,189,results,does not improve,has,performance,does not improve has performance,0.5884436368942261
translation,407,189,results,results,Adding in,lookup baseline ( lu ),results Adding in lookup baseline ( lu ),0.6930086612701416
translation,407,197,results,best performing overall model,combines,both variants,best performing overall model combines both variants,0.7238044142723083
translation,407,197,results,both variants,of,causal embedding model,both variants of causal embedding model,0.6149030327796936
translation,407,197,results,both variants,reaching,p@1,both variants reaching p@1,0.7070766091346741
translation,407,197,results,p@1,of,37.3 %,p@1 of 37.3 %,0.5877463817596436
translation,407,197,results,p@1,shows,7.7 % relative improvement,p@1 shows 7.7 % relative improvement,0.6865836977958679
translation,407,197,results,7.7 % relative improvement,over,strong cr + vembed baseline,7.7 % relative improvement over strong cr + vembed baseline,0.6489179134368896
translation,407,197,results,causal embedding model,has,cembedbi and cembedbinoise ),causal embedding model has cembedbi and cembedbinoise ),0.6148163676261902
translation,407,197,results,results,has,best performing overall model,results has best performing overall model,0.5481922030448914
translation,408,93,ablation-analysis,performance,of,tasebio,performance of tasebio,0.609801173210144
translation,408,93,ablation-analysis,decreases,as,number of gold spans,decreases as number of gold spans,0.5691891312599182
translation,408,93,ablation-analysis,only moder-ately,as,number of gold spans,only moder-ately as number of gold spans,0.5575297474861145
translation,408,93,ablation-analysis,both drop and quoref,has,performance,both drop and quoref has performance,0.6008564829826355
translation,408,93,ablation-analysis,performance,has,decreases,performance has decreases,0.599746823310852
translation,408,93,ablation-analysis,decreases,has,only moder-ately,decreases has only moder-ately,0.5975927710533142
translation,408,93,ablation-analysis,number of gold spans,has,increases,number of gold spans has increases,0.6296471953392029
translation,408,93,ablation-analysis,ablation analysis,shows,both drop and quoref,ablation analysis shows both drop and quoref,0.6938073635101318
translation,408,93,ablation-analysis,ablation analysis,in,both drop and quoref,ablation analysis in both drop and quoref,0.5826377868652344
translation,408,96,ablation-analysis,performance,of,p head module,performance of p head module,0.6645017266273499
translation,408,96,ablation-analysis,p head module,in,tasebio+sse,p head module in tasebio+sse,0.5540338158607483
translation,408,96,ablation-analysis,ablation analysis,analyzed,performance,ablation analysis analyzed performance,0.46258822083473206
translation,408,97,ablation-analysis,3.7 % and 7.2 %,of,multispan questions,3.7 % and 7.2 % of multispan questions,0.5827457308769226
translation,408,97,ablation-analysis,multispan questions,in,drop and quoref,multispan questions in drop and quoref,0.5528249144554138
translation,408,97,ablation-analysis,ablation analysis,has,non-multi-span head,ablation analysis has non-multi-span head,0.5614475011825562
translation,408,88,experiments,taseio,on,quoref test set,taseio on quoref test set,0.5901368856430054
translation,408,7,model,task,as,sequence tagging problem,task as sequence tagging problem,0.528242290019989
translation,408,20,model,simple and fully differentiable architecture,for handling,multi-span questions,simple and fully differentiable architecture for handling multi-span questions,0.722671627998352
translation,408,20,model,model,propose,simple and fully differentiable architecture,model propose simple and fully differentiable architecture,0.6622162461280823
translation,408,8,results,our model,substantially improves,performance,our model substantially improves performance,0.7795023918151855
translation,408,8,results,performance,on,span extraction questions,performance on span extraction questions,0.522278904914856
translation,408,8,results,span extraction questions,from,drop and quoref,span extraction questions from drop and quoref,0.575255811214447
translation,408,8,results,drop and quoref,by,9.9 and 5.5 em points,drop and quoref by 9.9 and 5.5 em points,0.6295984983444214
translation,408,8,results,results,has,our model,results has our model,0.5871725678443909
translation,408,24,results,single-span architecture,with,our multi-span approach,single-span architecture with our multi-span approach,0.6636485457420349
translation,408,24,results,our multi-span approach,improves,performance,our multi-span approach improves performance,0.6972823143005371
translation,408,24,results,performance,by,7.8 and 5.5 em points,performance by 7.8 and 5.5 em points,0.6039885878562927
translation,408,24,results,results,Replacing,single-span architecture,results Replacing single-span architecture,0.6447023153305054
translation,408,76,results,all prior models,that handle,multi-span questions,all prior models that handle multi-span questions,0.6424950957298279
translation,408,76,results,all prior models,improving by,at least 3.2 em points,all prior models improving by at least 3.2 em points,0.7113263607025146
translation,408,76,results,drop,has,tasebio+sse ( bert large ),drop has tasebio+sse ( bert large ),0.6393679976463318
translation,408,76,results,tasebio+sse ( bert large ),has,outperforms,tasebio+sse ( bert large ) has outperforms,0.6362186074256897
translation,408,76,results,outperforms,has,all prior models,outperforms has all prior models,0.5862451791763306
translation,408,76,results,results,On,drop,results On drop,0.530678391456604
translation,408,77,results,multi-span questions,obtaining,similar performance,multi-span questions obtaining similar performance,0.6276827454566956
translation,408,77,results,performance,over,bert - calc and mtmsn,performance over bert - calc and mtmsn,0.6880897879600525
translation,408,77,results,dramatically improve,has,performance,dramatically improve has performance,0.5738885998725891
translation,408,77,results,results,On,multi-span questions,results On multi-span questions,0.47172871232032776
translation,408,78,results,quoref,compared to,"corefroberta large ( ye et al. , 2020 )","quoref compared to corefroberta large ( ye et al. , 2020 )",0.6682606935501099
translation,408,78,results,"corefroberta large ( ye et al. , 2020 )",achieve,substantial improvement,"corefroberta large ( ye et al. , 2020 ) achieve substantial improvement",0.6282660365104675
translation,408,78,results,substantial improvement,of,over 20 em,substantial improvement of over 20 em,0.6648572087287903
translation,408,78,results,substantial improvement,of,4.5 em and 3.2 f1,substantial improvement of 4.5 em and 3.2 f1,0.5816698670387268
translation,408,78,results,over 20 em,on,multi-span questions,over 20 em on multi-span questions,0.5765475630760193
translation,408,78,results,improvement,of,4.5 em and 3.2 f1,improvement of 4.5 em and 3.2 f1,0.6125645637512207
translation,408,78,results,4.5 em and 3.2 f1,on,full development set,4.5 em and 3.2 f1 on full development set,0.5126854777336121
translation,408,78,results,results,On,quoref,results On quoref,0.618089497089386
translation,408,80,results,single-span extraction architecture,with,our multi-span extraction,single-span extraction architecture with our multi-span extraction,0.6330240368843079
translation,408,80,results,single-span extraction architecture,results in,dramatic improvement,single-span extraction architecture results in dramatic improvement,0.5971686244010925
translation,408,80,results,dramatic improvement,in,multi-span question performance,dramatic improvement in multi-span question performance,0.5222680568695068
translation,408,81,results,both architectures,tends to yield,best overall performance,both architectures tends to yield best overall performance,0.7333460450172424
translation,408,81,results,improvement,over using,only,improvement over using only,0.770609438419342
translation,408,81,results,improvement,over using,our multi-span architecture,improvement over using our multi-span architecture,0.750282347202301
translation,408,81,results,our multi-span architecture,is,not substantial,our multi-span architecture is not substantial,0.637201726436615
translation,408,81,results,only,has,our multi-span architecture,only has our multi-span architecture,0.6169626116752625
translation,408,81,results,results,combining,both architectures,results combining both architectures,0.7043119072914124
translation,408,89,results,our model,obtains,79.7 em and 86.1 f1,our model obtains 79.7 em and 86.1 f1,0.599154531955719
translation,408,89,results,improvement,of,3.9 em points,improvement of 3.9 em points,0.5610079169273376
translation,408,89,results,improvement,of,3.3 f1 points,improvement of 3.3 f1 points,0.5609222650527954
translation,408,89,results,improvement,over,state- of- the - art corefroberta large,improvement over state- of- the - art corefroberta large,0.6668199896812439
translation,408,89,results,3.3 f1 points,over,state- of- the - art corefroberta large,3.3 f1 points over state- of- the - art corefroberta large,0.6158729195594788
translation,408,89,results,results,has,our model,results has our model,0.5871725678443909
translation,408,90,results,taseio +sse model,achieves,80.4 em,taseio +sse model achieves 80.4 em,0.6666834354400635
translation,408,90,results,taseio +sse model,achieves,83.6 f1,taseio +sse model achieves 83.6 f1,0.6424315571784973
translation,408,90,results,83.6 f1,on,entire test set,83.6 f1 on entire test set,0.5114591121673584
translation,408,90,results,entire test set,including,nonspan questions,entire test set including nonspan questions,0.6816596984863281
translation,408,90,results,drop,has,taseio +sse model,drop has taseio +sse model,0.6160084009170532
translation,408,90,results,results,On,drop,results On drop,0.530678391456604
translation,408,95,results,our architecture,is,quite accurate,our architecture is quite accurate,0.5557222366333008
translation,408,95,results,quite accurate,in predicting,correct number of spans,quite accurate in predicting correct number of spans,0.6594995856285095
translation,408,95,results,results,see that,our architecture,results see that our architecture,0.6288131475448608
translation,409,81,ablation-analysis,question mark,at,end,question mark at end,0.5753747820854187
translation,409,81,ablation-analysis,question mark,results in,decreased performance,question mark results in decreased performance,0.647465705871582
translation,409,81,ablation-analysis,end,of,template,end of template,0.623953640460968
translation,409,81,ablation-analysis,not as large,removing,wh-component,not as large removing wh-component,0.7618201971054077
translation,409,81,ablation-analysis,ablation analysis,Removing,question mark,ablation analysis Removing question mark,0.770003080368042
translation,409,99,ablation-analysis,training,from,our synthetically generated template - based data,training from our synthetically generated template - based data,0.5371347665786743
translation,409,99,ablation-analysis,our synthetically generated template - based data,with,auxiliary matching,our synthetically generated template - based data with auxiliary matching,0.6786958575248718
translation,409,99,ablation-analysis,10 k datapoints,has,training,10 k datapoints has training,0.5973569750785828
translation,409,99,ablation-analysis,auxiliary matching,has,outperforms,auxiliary matching has outperforms,0.6321701407432556
translation,409,30,experiments,question,for,"all ( context , answer ) pairs","question for all ( context , answer ) pairs",0.6183357834815979
translation,409,30,experiments,question,fine - tune,pretrained bert model,question fine - tune pretrained bert model,0.7079935073852539
translation,409,30,experiments,pretrained bert model,on,data,pretrained bert model on data,0.5243090391159058
translation,409,7,model,unsupervised approach,to training,qa models,unsupervised approach to training qa models,0.6936401128768921
translation,409,7,model,qa models,with,generated pseudotraining data,qa models with generated pseudotraining data,0.5949215888977051
translation,409,7,model,model,propose,unsupervised approach,model propose unsupervised approach,0.7285106778144836
translation,409,8,results,generating questions,for,qa training,generating questions for qa training,0.6450124979019165
translation,409,8,results,generating questions,by applying,simple template,generating questions by applying simple template,0.6767575144767761
translation,409,8,results,simple template,on,"related , retrieved sentence","simple template on related , retrieved sentence",0.532203733921051
translation,409,8,results,"related , retrieved sentence",rather than,original context sentence,"related , retrieved sentence rather than original context sentence",0.6556950807571411
translation,409,8,results,downstream qa performance,by allowing,model,downstream qa performance by allowing model,0.6924452185630798
translation,409,8,results,model,to learn,more complex context-question relationships,model to learn more complex context-question relationships,0.6359806656837463
translation,409,8,results,improves,has,downstream qa performance,improves has downstream qa performance,0.5436958074569702
translation,409,8,results,results,show,generating questions,results show generating questions,0.5660315752029419
translation,409,9,results,qa model,gives,relative improvement,qa model gives relative improvement,0.6252341866493225
translation,409,9,results,relative improvement,over,previous unsupervised model,relative improvement over previous unsupervised model,0.6611842513084412
translation,409,9,results,relative improvement,about,20 %,relative improvement about 20 %,0.6512975096702576
translation,409,9,results,previous unsupervised model,in,f1 score,previous unsupervised model in f1 score,0.46960076689720154
translation,409,9,results,previous unsupervised model,on,squad dataset,previous unsupervised model on squad dataset,0.530788004398346
translation,409,9,results,20 %,when,answer,20 % when answer,0.7105634212493896
translation,409,9,results,answer,is,named entity,answer is named entity,0.5679850578308105
translation,409,9,results,results,Training,qa model,results Training qa model,0.7348981499671936
translation,409,64,results,retrieved sentences,improves over,original sentence,retrieved sentences improves over original sentence,0.6607508659362793
translation,409,64,results,retrieved sentences,using,original sentence,retrieved sentences using original sentence,0.6419645547866821
translation,409,64,results,results,using,retrieved sentences,results using retrieved sentences,0.6354130506515503
translation,409,76,results,improve largely,over,simple cloze templates,improve largely over simple cloze templates,0.7145456075668335
translation,409,76,results,wh template methods,has,improve largely,wh template methods has improve largely,0.6024128794670105
translation,409,76,results,results,has,wh template methods,results has wh template methods,0.5445035696029663
translation,409,77,results,wh + b + a + ?,performs,best,wh + b + a + ? performs best,0.622916042804718
translation,409,77,results,best,among,template - based methods,best among template - based methods,0.6326130032539368
translation,409,77,results,results,has,wh + b + a + ?,results has wh + b + a + ?,0.6049163937568665
translation,409,79,results,same data,as,wh + b + a + ?,same data as wh + b + a + ?,0.5949663519859314
translation,409,79,results,same data,removing,wh-component,same data removing wh-component,0.7803955674171448
translation,409,79,results,large decrease,in,performance,large decrease in performance,0.5610359311103821
translation,409,79,results,results,Using,same data,results Using same data,0.7081328630447388
translation,409,84,results,question quality,makes,difference,question quality makes difference,0.5847682952880859
translation,409,84,results,difference,in,performance,difference in performance,0.548568844795227
translation,409,84,results,difference,from,jump,difference from jump,0.6217479705810547
translation,409,84,results,jump,from,cloze-style questions,jump from cloze-style questions,0.5934311747550964
translation,409,84,results,results,Improving,question quality,results Improving question quality,0.5075576305389404
translation,409,88,results,most-common wh word,by grouping,named entities,most-common wh word by grouping named entities,0.6673194169998169
translation,409,88,results,most-common wh word,using,single wh-word ( what ),most-common wh word using single wh-word ( what ),0.6478097438812256
translation,409,88,results,named entities,into,5 categories,named entities into 5 categories,0.48186546564102173
translation,409,88,results,very close,to,best- performing wh n-gram prior method,very close to best- performing wh n-gram prior method,0.5646934509277344
translation,409,88,results,single wh-word ( what ),results in,significant decrease,single wh-word ( what ) results in significant decrease,0.6366142630577087
translation,409,88,results,significant decrease,in,performance,significant decrease in performance,0.5324122905731201
translation,409,88,results,results,Using,most-common wh word,results Using most-common wh word,0.5881266593933105
translation,409,93,results,auxillary matching,leads to,improvements,auxillary matching leads to improvements,0.6801023483276367
translation,409,93,results,improvements,over,no matching,improvements over no matching,0.7075291872024536
translation,409,93,results,improvements,with,best results,improvements with best results,0.574959397315979
translation,409,93,results,no matching,when using,template - based data,no matching when using template - based data,0.7395774126052856
translation,409,93,results,no matching,with,best results,no matching with best results,0.6202095150947571
translation,409,93,results,best results,using,matching,best results using matching,0.7080262899398804
translation,409,93,results,matching,with,query and context,matching with query and context,0.6558047533035278
translation,409,93,results,results,has,auxillary matching,results has auxillary matching,0.518376350402832
translation,409,100,results,data,from,our templatebased data,data from our templatebased data,0.5414069890975952
translation,409,100,results,data,has,consistently outperforms,data has consistently outperforms,0.6364240050315857
translation,409,100,results,our templatebased data,has,consistently outperforms,our templatebased data has consistently outperforms,0.5940004587173462
translation,409,100,results,results,Using,data,results Using data,0.582046627998352
translation,410,68,experimental-setup,back - end services,implemented in,python,back - end services implemented in python,0.7207823991775513
translation,410,68,experimental-setup,python,with,flask,python with flask,0.6756446361541748
translation,410,68,experimental-setup,experimental setup,has,back - end services,experimental setup has back - end services,0.5180346369743347
translation,410,30,model,modular debugging application,for,kb qa,modular debugging application for kb qa,0.6156476140022278
translation,410,30,model,model,present,modular debugging application,model present modular debugging application,0.6345546245574951
translation,411,143,ablation-analysis,constraints,from,search index,constraints from search index,0.5022175908088684
translation,411,143,ablation-analysis,asr accuracies,improved by,about 1 % absolute,asr accuracies improved by about 1 % absolute,0.7082723379135132
translation,411,143,ablation-analysis,constraints,has,asr accuracies,constraints has asr accuracies,0.5714261531829834
translation,411,143,ablation-analysis,search index,has,asr accuracies,search index has asr accuracies,0.5696223974227905
translation,411,143,ablation-analysis,ablation analysis,by integrating,constraints,ablation analysis by integrating constraints,0.6776286959648132
translation,411,5,baselines,qme !,retrieves,answers,qme ! retrieves answers,0.7842636108398438
translation,411,5,baselines,speech - based question - answering system,allows for,spoken queries,speech - based question - answering system allows for spoken queries,0.6650635004043579
translation,411,5,baselines,answers,to,questions,answers to questions,0.5806413292884827
translation,411,5,baselines,questions,instead of,web pages,questions instead of web pages,0.6232472062110901
translation,411,5,baselines,qme !,has,speech - based question - answering system,qme ! has speech - based question - answering system,0.5574339628219604
translation,411,127,experiments,unigram based tf .idf method,retrieved,93 %,unigram based tf .idf method retrieved 93 %,0.54567551612854
translation,411,127,experiments,unigram based tf .idf method,retrieved,97 %,unigram based tf .idf method retrieved 97 %,0.5357475876808167
translation,411,127,experiments,unigram based tf .idf method,retrieved,100 %,unigram based tf .idf method retrieved 100 %,0.5409442186355591
translation,411,127,experiments,93 %,of,user 's query,93 % of user 's query,0.5877740383148193
translation,411,127,experiments,93 %,in,first position,93 % in first position,0.530211329460144
translation,411,127,experiments,97 %,one of,top - 5 positions,97 % one of top - 5 positions,0.6126995086669922
translation,411,127,experiments,100 %,one of,top - 10 positions,100 % one of top - 10 positions,0.5983359217643738
translation,411,17,model,speech-driven question answering system,has,qme !,speech-driven question answering system has qme !,0.603097140789032
translation,411,18,model,natural input modality,for,users,natural input modality for users,0.5604962706565857
translation,411,18,model,users,to pose,information need,users to pose information need,0.6718705296516418
translation,411,18,model,collection of answers,potentially address,information need directly,collection of answers potentially address information need directly,0.6874279379844666
translation,411,18,model,natural input modality,has,spoken language input,natural input modality has spoken language input,0.5240465402603149
translation,411,6,results,bootstrap methods,to distinguish,dynamic questions,bootstrap methods to distinguish dynamic questions,0.690116286277771
translation,411,6,results,bootstrap methods,show,benefits,bootstrap methods show benefits,0.6502965092658997
translation,411,6,results,dynamic questions,from,static questions,dynamic questions from static questions,0.5693303346633911
translation,411,6,results,benefits,of,tight coupling,benefits of tight coupling,0.5727164149284363
translation,411,6,results,tight coupling,of,speech recognition and retrieval components,tight coupling of speech recognition and retrieval components,0.5766347646713257
translation,411,6,results,speech recognition and retrieval components,of,system,speech recognition and retrieval components of system,0.5930427312850952
translation,411,6,results,results,present,bootstrap methods,results present bootstrap methods,0.580454409122467
translation,411,6,results,results,show,benefits,results show benefits,0.6036745309829712
translation,411,126,results,retrieval accuracy scores,for,various retrieval techniques,retrieval accuracy scores for various retrieval techniques,0.5563895106315613
translation,411,126,results,retrieval accuracy scores,performed,exceedingly well,retrieval accuracy scores performed exceedingly well,0.25625118613243103
translation,411,126,results,results,On,seen set of queries,results On seen set of queries,0.5836296081542969
translation,412,157,ablation-analysis,kg embeddings,to,model,kg embeddings to model,0.5496215224266052
translation,412,157,ablation-analysis,kg embeddings,leads to,gain,kg embeddings leads to gain,0.6694708466529846
translation,412,157,ablation-analysis,model,leads to,gain,model leads to gain,0.7028193473815918
translation,412,157,ablation-analysis,gain,of,11.56 % and 7.19 %,gain of 11.56 % and 7.19 %,0.6074513792991638
translation,412,157,ablation-analysis,11.56 % and 7.19 %,in,vqa and ok - vqa datasets,11.56 % and 7.19 % in vqa and ok - vqa datasets,0.5238611698150635
translation,412,157,ablation-analysis,ablation analysis,Adding,kg embeddings,ablation analysis Adding kg embeddings,0.7291377782821655
translation,412,165,baselines,simple,create,embedding g,simple create embedding g,0.6447403430938721
translation,412,165,baselines,output,of,concept-language module,output of concept-language module,0.5883297324180603
translation,412,176,baselines,some articles,from,wikipedia,some articles from wikipedia,0.6062036156654358
translation,412,176,baselines,some articles,for,each question - image pair,some articles for each question - image pair,0.5976678729057312
translation,412,176,baselines,wikipedia,for,each question - image pair,wikipedia for each question - image pair,0.6481104493141174
translation,412,176,baselines,network,to predict,whether and where,network to predict whether and where,0.7679646611213684
translation,412,176,baselines,network,to predict,ground - truth answers,network to predict ground - truth answers,0.7036699056625366
translation,412,176,baselines,whether and where,appear in,each sentence,whether and where appear in each sentence,0.6136836409568787
translation,412,176,baselines,ground - truth answers,appear in,article,ground - truth answers appear in article,0.6311404705047607
translation,412,176,baselines,ground - truth answers,appear in,each sentence,ground - truth answers appear in each sentence,0.5535875558853149
translation,412,176,baselines,whether and where,has,ground - truth answers,whether and where has ground - truth answers,0.5705679655075073
translation,412,10,experimental-setup,https,:,github.com/,https : github.com/,0.6192581057548523
translation,412,10,experimental-setup,https,:,ziamaryam/conceptbert,https : ziamaryam/conceptbert,0.6812407374382019
translation,412,10,experimental-setup,https,//,github.com/,https // github.com/,0.7059125900268555
translation,412,10,experimental-setup,https,//,ziamaryam/conceptbert,https // ziamaryam/conceptbert,0.7049161195755005
translation,412,10,experimental-setup,github.com/,has,ziamaryam/conceptbert,github.com/ has ziamaryam/conceptbert,0.5901262164115906
translation,412,147,experimental-setup,linear decay learning rate schedule,with,warm up,linear decay learning rate schedule with warm up,0.6602243781089783
translation,412,147,experimental-setup,warm up,to train,model,warm up to train model,0.7214975953102112
translation,412,147,experimental-setup,experimental setup,has,linear decay learning rate schedule,experimental setup has linear decay learning rate schedule,0.5089420676231384
translation,412,210,experimental-setup,binary cross-entropy loss,with,batch size,binary cross-entropy loss with batch size,0.5985110402107239
translation,412,210,experimental-setup,binary cross-entropy loss,over,maximum of 20 epochs,binary cross-entropy loss over maximum of 20 epochs,0.6440647840499878
translation,412,210,experimental-setup,binary cross-entropy loss,on,8 tesla gpus,binary cross-entropy loss on 8 tesla gpus,0.4908270239830017
translation,412,210,experimental-setup,batch size,of,1024,batch size of 1024,0.6398507952690125
translation,412,210,experimental-setup,1024,over,maximum of 20 epochs,1024 over maximum of 20 epochs,0.6913039088249207
translation,412,210,experimental-setup,maximum of 20 epochs,on,8 tesla gpus,maximum of 20 epochs on 8 tesla gpus,0.48329025506973267
translation,412,210,experimental-setup,experimental setup,use,binary cross-entropy loss,experimental setup use binary cross-entropy loss,0.6059027314186096
translation,412,9,experiments,conceptnet kg,for encoding,common sense knowledge,conceptnet kg for encoding common sense knowledge,0.6814606189727783
translation,412,136,experiments,vilbert model,built on,conceptual captions dataset,vilbert model built on conceptual captions dataset,0.7060462832450867
translation,412,136,experiments,vilbert model,learn,interactions,vilbert model learn interactions,0.6754880547523499
translation,412,136,experiments,conceptual captions dataset,collection of,3.3 million image-caption pairs,conceptual captions dataset collection of 3.3 million image-caption pairs,0.5771836042404175
translation,412,136,experiments,3.3 million image-caption pairs,to capture,diversity,3.3 million image-caption pairs to capture diversity,0.6470215916633606
translation,412,136,experiments,interactions,between,images and text,interactions between images and text,0.7314090728759766
translation,412,136,experiments,diversity,has,of visual content,diversity has of visual content,0.5883365869522095
translation,412,6,model,concept-aware algorithm,for,questions which require common sense,concept-aware algorithm for questions which require common sense,0.6457792520523071
translation,412,6,model,basic factual knowledge,from,external structured content,basic factual knowledge from external structured content,0.5329717993736267
translation,412,6,model,concept-aware algorithm,has,conceptbert,concept-aware algorithm has conceptbert,0.5818390846252441
translation,412,6,model,model,present,concept-aware algorithm,model present concept-aware algorithm,0.6849381327629089
translation,412,7,model,conceptbert,requires,visual elements,conceptbert requires visual elements,0.6620966792106628
translation,412,7,model,conceptbert,requires,knowledge graph ( kg ),conceptbert requires knowledge graph ( kg ),0.6594903469085693
translation,412,7,model,visual elements,of,image,visual elements of image,0.6268765330314636
translation,412,7,model,knowledge graph ( kg ),to infer,correct answer,knowledge graph ( kg ) to infer correct answer,0.7509526610374451
translation,412,7,model,image,has,conceptbert,image has conceptbert,0.5505458116531372
translation,412,7,model,natural language,has,conceptbert,natural language has conceptbert,0.5563719868659973
translation,412,7,model,model,Given,image,model Given image,0.7798500061035156
translation,412,8,model,multi-modal representation,learns,joint concept-vision - language embedding,multi-modal representation learns joint concept-vision - language embedding,0.6800181269645691
translation,412,8,model,model,introduce,multi-modal representation,model introduce multi-modal representation,0.6175731420516968
translation,412,36,model,queried entities,into,questions,queried entities into questions,0.56199049949646
translation,412,36,model,model,injects,queried entities,model injects queried entities,0.6940171122550964
translation,412,37,model,model,jointly learns from,"visual , language , and kg embeddings","model jointly learns from visual , language , and kg embeddings",0.7888386249542236
translation,412,40,model,embedded inputs,passed through,two main modules,embedded inputs passed through two main modules,0.6916807293891907
translation,412,40,model,model,has,embedded inputs,model has embedded inputs,0.5793594717979431
translation,412,41,model,vision- language representation module,jointly enhances,image and question embeddings,vision- language representation module jointly enhances image and question embeddings,0.7042090892791748
translation,412,41,model,vision- language representation module,each improving,context representation,vision- language representation module each improving context representation,0.6468364596366882
translation,412,41,model,context representation,with,other one,context representation with other one,0.6470879912376404
translation,412,41,model,model,has,vision- language representation module,model has vision- language representation module,0.5557764172554016
translation,412,42,model,conceptlanguage representation,uses,kg embedding,conceptlanguage representation uses kg embedding,0.5313340425491333
translation,412,42,model,kg embedding,to incorporate,relevant external information,kg embedding to incorporate relevant external information,0.6812989115715027
translation,412,42,model,relevant external information,in,question embedding,relevant external information in question embedding,0.5081963539123535
translation,412,42,model,model,has,conceptlanguage representation,model has conceptlanguage representation,0.512437641620636
translation,412,43,model,aggregated,to represent,concept-visionlanguage embeddings,aggregated to represent concept-visionlanguage embeddings,0.6582954525947571
translation,412,43,model,classifier,to predict,answer,classifier to predict answer,0.7497855424880981
translation,412,43,model,model,has,outputs,model has outputs,0.5564337968826294
translation,412,44,model,pre-trained image and language features,fuse them with,kg embeddings,pre-trained image and language features fuse them with kg embeddings,0.6584370136260986
translation,412,44,model,kg embeddings,to incorporate,external knowledge,kg embeddings to incorporate external knowledge,0.6266478896141052
translation,412,44,model,external knowledge,into,vqa task,external knowledge into vqa task,0.5482439398765564
translation,412,44,model,model,use,pre-trained image and language features,model use pre-trained image and language features,0.5906867384910583
translation,412,71,model,graph convolutional encoder,takes,graph,graph convolutional encoder takes graph,0.5993349552154541
translation,412,71,model,graph convolutional encoder,encodes,each node,graph convolutional encoder encodes each node,0.7527831196784973
translation,412,71,model,graph,as,input,graph as input,0.5213677287101746
translation,412,71,model,model,has,graph convolutional encoder,model has graph convolutional encoder,0.5156949758529663
translation,412,88,model,attentional transformer layer,which is,multi-layer bidirectional transformer,attentional transformer layer which is multi-layer bidirectional transformer,0.6645683646202087
translation,412,88,model,multi-layer bidirectional transformer,using,encoder part,multi-layer bidirectional transformer using encoder part,0.5881322622299194
translation,412,88,model,encoder part,of,original transformer,encoder part of original transformer,0.5797426700592041
translation,412,88,model,model,use,attentional transformer layer,model use attentional transformer layer,0.6331881880760193
translation,412,137,model,vision- language module,includes,6 layers of transformer blocks,vision- language module includes 6 layers of transformer blocks,0.6388708353042603
translation,412,137,model,6 layers of transformer blocks,with,8 and 12 attention heads,6 layers of transformer blocks with 8 and 12 attention heads,0.6577451229095459
translation,412,137,model,8 and 12 attention heads,in,visual stream and linguistic streams,8 and 12 attention heads in visual stream and linguistic streams,0.5486947894096375
translation,412,137,model,model,has,vision- language module,model has vision- language module,0.5696302056312561
translation,412,163,model,performance,of,our complete model,performance of our complete model,0.5938906073570251
translation,412,163,model,our complete model,on,vqa 2.0 validation set,our complete model on vqa 2.0 validation set,0.5284984707832336
translation,412,163,model,vqa 2.0 validation set,compared with,existing models,vqa 2.0 validation set compared with existing models,0.6695621013641357
translation,412,163,model,existing models,combines,bottom - up and top-down attention mechanism,existing models combines bottom - up and top-down attention mechanism,0.6627163887023926
translation,412,163,model,up - down model,combines,bottom - up and top-down attention mechanism,up - down model combines bottom - up and top-down attention mechanism,0.6830421090126038
translation,412,163,model,bottom - up and top-down attention mechanism,enables,attention,bottom - up and top-down attention mechanism enables attention,0.6329461932182312
translation,412,163,model,model,has,performance,model has performance,0.5451148748397827
translation,412,166,model,g and the image embedding,feed them to,vision - language module,g and the image embedding feed them to vision - language module,0.6481513381004333
translation,412,166,model,g and the image embedding,send,output,g and the image embedding send output,0.6143143177032471
translation,412,166,model,output,to,classifier,output to classifier,0.5973520874977112
translation,412,166,model,output,check,answer,output check answer,0.7493738532066345
translation,412,166,model,classifier,check,answer,classifier check answer,0.7249457240104675
translation,412,166,model,model,use,g and the image embedding,model use g and the image embedding,0.592678427696228
translation,412,194,model,conceptaware end-to - end pipeline,for,questions which require knowledge from external structured content,conceptaware end-to - end pipeline for questions which require knowledge from external structured content,0.6199708580970764
translation,412,194,model,conceptbert,has,conceptaware end-to - end pipeline,conceptbert has conceptaware end-to - end pipeline,0.5930601954460144
translation,412,194,model,model,present,conceptbert,model present conceptbert,0.6594982743263245
translation,412,195,model,new representation of questions,enhanced with,external knowledge,new representation of questions enhanced with external knowledge,0.6959632635116577
translation,412,195,model,external knowledge,exploiting,transformer blocks,external knowledge exploiting transformer blocks,0.7503166198730469
translation,412,195,model,external knowledge,exploiting,knowledge graph embeddings,external knowledge exploiting knowledge graph embeddings,0.6576771140098572
translation,412,195,model,model,introduce,new representation of questions,model introduce new representation of questions,0.6777525544166565
translation,412,158,results,vl model,has,outperforms,vl model has outperforms,0.649949848651886
translation,412,158,results,outperforms,has,cl model,outperforms has cl model,0.6426010131835938
translation,412,158,results,results,note,vl model,results note vl model,0.6104632616043091
translation,412,161,results,cvl model,gives,highest accuracy,cvl model gives highest accuracy,0.6408426761627197
translation,412,161,results,highest accuracy,indicates,effectiveness,highest accuracy indicates effectiveness,0.6687394380569458
translation,412,161,results,effectiveness,of,joint concept-visionlanguage representation,effectiveness of joint concept-visionlanguage representation,0.5754114985466003
translation,412,161,results,vl and cl,has,cvl model,vl and cl has cvl model,0.5908623337745667
translation,412,161,results,results,Compared to,vl and cl,results Compared to vl and cl,0.6600483059883118
translation,412,162,results,results,on,vqa 2.0 dataset,results on vqa 2.0 dataset,0.5161681771278381
translation,412,170,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,412,170,results,outperforms,has,existing models,outperforms has existing models,0.5960175395011902
translation,412,170,results,results,show,our model,results show our model,0.6888449192047119
translation,412,172,results,results,on,ok - vqa dataset,results on ok - vqa dataset,0.5217442512512207
translation,412,177,results,our model,surpasses,baselines and sota models,our model surpasses baselines and sota models,0.6438446640968323
translation,412,177,results,baselines and sota models,in,almost every category,baselines and sota models in almost every category,0.5075286626815796
translation,412,178,results,con-ceptbert,performs,especially well,con-ceptbert performs especially well,0.7286648154258728
translation,412,178,results,especially well,in,  cooking and food   ( cf ),especially well in   cooking and food   ( cf ),0.551446795463562
translation,412,178,results,especially well,in,plants and animals,especially well in plants and animals,0.5800085067749023
translation,412,178,results,especially well,in,( pa ),especially well in ( pa ),0.5586889386177063
translation,412,178,results,especially well,in,science and technology   ( st ) categories,especially well in science and technology   ( st ) categories,0.577420175075531
translation,412,178,results,especially well,with,gain,especially well with gain,0.7568684220314026
translation,412,178,results,science and technology   ( st ) categories,with,gain,science and technology   ( st ) categories with gain,0.6727344989776611
translation,412,178,results,gain,larger than,3 %,gain larger than 3 %,0.762270987033844
translation,412,178,results,results,has,con-ceptbert,results has con-ceptbert,0.5727768540382385
translation,412,181,results,vilbert,performs,better,vilbert performs better,0.6441987752914429
translation,412,181,results,better,in,category,better in category,0.5252485871315002
translation,412,181,results,category,compared to,conceptbert,category compared to conceptbert,0.6685064435005188
translation,412,181,results,"geography , history , language and culture",has,( ghlc ),"geography , history , language and culture has ( ghlc )",0.603108286857605
translation,412,181,results,results,has,vilbert,results has vilbert,0.5752189755439758
translation,413,142,baselines,crossentropy loss,as,scoring function,crossentropy loss as scoring function,0.5137047171592712
translation,413,142,baselines,gpt -2 large with language modeling,has,crossentropy loss,gpt -2 large with language modeling has crossentropy loss,0.5610633492469788
translation,413,154,baselines,both the krl model ( 365 m ) and the smlm model ( 358 m ),uses,roberta -large ( 355 m ),both the krl model ( 365 m ) and the smlm model ( 358 m ) uses roberta -large ( 355 m ),0.5875958204269409
translation,413,154,baselines,roberta -large ( 355 m ),as,encoder,roberta -large ( 355 m ) as encoder,0.576565682888031
translation,413,154,baselines,baselines,has,both the krl model ( 365 m ) and the smlm model ( 358 m ),baselines has both the krl model ( 365 m ) and the smlm model ( 358 m ),0.5681856870651245
translation,413,155,experimental-setup,model,for,smlm,model for smlm,0.6313448548316956
translation,413,155,experimental-setup,model,for,krl,model for krl,0.6823999881744385
translation,413,155,experimental-setup,model,for,krl,model for krl,0.6823999881744385
translation,413,155,experimental-setup,three epochs,for,smlm,three epochs for smlm,0.6420925855636597
translation,413,155,experimental-setup,three epochs,for,krl,three epochs for krl,0.677457332611084
translation,413,155,experimental-setup,three epochs,for,krl,three epochs for krl,0.677457332611084
translation,413,155,experimental-setup,batch sizes,for,smlm,batch sizes for smlm,0.5942793488502502
translation,413,155,experimental-setup,batch sizes,for,krl,batch sizes for krl,0.6284725666046143
translation,413,155,experimental-setup,batch sizes,for,krl,batch sizes for krl,0.6284725666046143
translation,413,155,experimental-setup,"[ 512 , 1024 ]",for,smlm,"[ 512 , 1024 ] for smlm",0.6008734107017517
translation,413,155,experimental-setup,"[ 512 , 1024 ]",for,krl,"[ 512 , 1024 ] for krl",0.6276232600212097
translation,413,155,experimental-setup,"[ 512 , 1024 ]",for,krl,"[ 512 , 1024 ] for krl",0.6276232600212097
translation,413,155,experimental-setup,smlm,for,krl,smlm for krl,0.6606025099754333
translation,413,155,experimental-setup,"[ 32 , 64 ]",for,krl,"[ 32 , 64 ] for krl",0.6561264991760254
translation,413,155,experimental-setup,learning rate,in,range,learning rate in range,0.5099278688430786
translation,413,155,experimental-setup,batch sizes,has,"[ 512 , 1024 ]","batch sizes has [ 512 , 1024 ]",0.5515803694725037
translation,413,155,experimental-setup,batch sizes,has,"[ 32 , 64 ]","batch sizes has [ 32 , 64 ]",0.5313782691955566
translation,413,155,experimental-setup,batch sizes,has,learning rate,batch sizes has learning rate,0.5147282481193542
translation,413,155,experimental-setup,batch sizes,has,warm - up steps,batch sizes has warm - up steps,0.5701830983161926
translation,413,155,experimental-setup,experimental setup,train,model,experimental setup train model,0.6514950394630432
translation,413,188,experiments,model,is,more perplexed,model is more perplexed,0.6093307137489319
translation,413,188,experiments,more perplexed,to predict,question,more perplexed to predict question,0.7413799166679382
translation,413,188,experiments,more perplexed,leading to,higher accuracy,more perplexed leading to higher accuracy,0.6817265748977661
translation,413,188,experiments,question,when given,wrong answer option,question when given wrong answer option,0.7265672087669373
translation,413,188,experiments,wrong answer option,leading to,higher accuracy,wrong answer option leading to higher accuracy,0.6729313731193542
translation,413,188,experiments,higher accuracy,for,only question distance score,higher accuracy for only question distance score,0.5942391753196716
translation,413,188,experiments,openbookqa,has,three datasets,openbookqa has three datasets,0.5276801586151123
translation,413,188,experiments,openbookqa,has,model,openbookqa has model,0.6028868556022644
translation,413,188,experiments,commonsenseqa,has,three datasets,commonsenseqa has three datasets,0.5344259142875671
translation,413,188,experiments,three datasets,has,model,three datasets has model,0.6038299798965454
translation,413,190,experiments,model,is,more perplexed,model is more perplexed,0.6093307137489319
translation,413,190,experiments,more perplexed,when predicting,context,more perplexed when predicting context,0.733184278011322
translation,413,190,experiments,context,given,wrong answer option,context given wrong answer option,0.7213757634162903
translation,413,190,experiments,socialiqa,has,question,socialiqa has question,0.609720766544342
translation,413,190,experiments,question,has,least accuracy,question has least accuracy,0.585058331489563
translation,413,23,model,tasks which do not have appropriate knowledge graphs,propose,heuristics,tasks which do not have appropriate knowledge graphs propose heuristics,0.6308579444885254
translation,413,23,model,heuristics,to create,synthetic knowledge graphs,heuristics to create synthetic knowledge graphs,0.6257550120353699
translation,413,23,model,model,For,tasks which do not have appropriate knowledge graphs,model For tasks which do not have appropriate knowledge graphs,0.6083196401596069
translation,413,167,results,our ktl trained models,perform,statistically significantly better,our ktl trained models perform statistically significantly better,0.6160982251167297
translation,413,167,results,statistically significantly better,than,baselines,statistically significantly better than baselines,0.5955755710601807
translation,413,167,results,results,observe that,our ktl trained models,results observe that our ktl trained models,0.5675903558731079
translation,413,168,results,nceloss,with,cosine similarity,nceloss with cosine similarity,0.6591805219650269
translation,413,168,results,nceloss,performs,best,nceloss performs best,0.7107375264167786
translation,413,168,results,cosine similarity,performs,best,cosine similarity performs best,0.6015239953994751
translation,413,168,results,different krl models,has,nceloss,different krl models has nceloss,0.5999303460121155
translation,413,168,results,results,comparing,different krl models,results comparing different krl models,0.6423947215080261
translation,413,170,results,different ktl instantiations,see that,smlm model,different ktl instantiations see that smlm model,0.6000641584396362
translation,413,170,results,smlm model,performs,best,smlm model performs best,0.6147013902664185
translation,413,170,results,results,comparing,different ktl instantiations,results comparing different ktl instantiations,0.6585087180137634
translation,413,177,results,fact corpus,for,qasc,fact corpus for qasc,0.6172570586204529
translation,413,178,results,substantial improvement,in,socialiqa,substantial improvement in socialiqa,0.5296459794044495
translation,413,178,results,substantial improvement,in,anli,substantial improvement in anli,0.6743646264076233
translation,413,178,results,substantial improvement,in,qasc,substantial improvement in qasc,0.5719112753868103
translation,413,178,results,substantial improvement,in,com-monsenseqa,substantial improvement in com-monsenseqa,0.5562976002693176
translation,413,178,results,com-monsenseqa,as,respective ktl knowledge corpus,com-monsenseqa as respective ktl knowledge corpus,0.5275318026542664
translation,413,178,results,respective ktl knowledge corpus,contains,sufficient knowledge,respective ktl knowledge corpus contains sufficient knowledge,0.5759056806564331
translation,413,179,results,qasc,reduce,problem,qasc reduce problem,0.7225168943405151
translation,413,179,results,problem,from,eight - way,problem from eight - way,0.5674833059310913
translation,413,179,results,top - 4 accuracy,on,qasc,top - 4 accuracy on qasc,0.5476423501968384
translation,413,179,results,top - 4 accuracy,is,92 %,top - 4 accuracy is 92 %,0.5748524069786072
translation,413,179,results,top - 4 accuracy,above,92 %,top - 4 accuracy above 92 %,0.7259413003921509
translation,413,179,results,results,to note,qasc,results to note qasc,0.6134164333343506
translation,413,179,results,results,for,qasc,results for qasc,0.6657271385192871
translation,413,181,results,prior supervised approaches,like,"bidaf ( seo et al. , 2017 )","prior supervised approaches like bidaf ( seo et al. , 2017 )",0.5657623410224915
translation,413,181,results,results,approaches,prior supervised approaches,results approaches prior supervised approaches,0.7195991277694702
translation,413,185,results,our ktl pretrained encoders,perform,significantly better,our ktl pretrained encoders perform significantly better,0.5870547294616699
translation,413,185,results,our ktl pretrained encoders,perform,approach,our ktl pretrained encoders perform approach,0.5642210245132446
translation,413,185,results,significantly better,than,baselines,significantly better than baselines,0.6141255497932434
translation,413,185,results,fully supervised model,with,only 7.5 % percent,fully supervised model with only 7.5 % percent,0.5852265954017639
translation,413,185,results,only 7.5 % percent,behind,fully supervised model,only 7.5 % percent behind fully supervised model,0.6500142216682434
translation,413,185,results,fully supervised model,on,socialiqa,fully supervised model on socialiqa,0.5469461679458618
translation,413,185,results,approach,has,fully supervised model,approach has fully supervised model,0.5825932621955872
translation,413,185,results,results,observe,our ktl pretrained encoders,results observe our ktl pretrained encoders,0.5579559803009033
translation,413,189,results,all three distance scores,have,nearly equal performance,all three distance scores have nearly equal performance,0.5615028142929077
translation,413,189,results,anli,has,all three distance scores,anli has all three distance scores,0.5967051386833191
translation,413,189,results,results,in,anli,results in anli,0.640405535697937
translation,413,192,results,effect of hypothesis conversion,has,curriculum filtering,effect of hypothesis conversion has curriculum filtering,0.5766353607177734
translation,413,192,results,results,has,effect of hypothesis conversion,results has effect of hypothesis conversion,0.5628111958503723
translation,414,15,model,handling,of,self-joining queries,handling of self-joining queries,0.5465922951698303
translation,414,15,model,handling,of,aggregate operators,handling of aggregate operators,0.559053361415863
translation,414,15,model,handling,of,sub-queries,handling of sub-queries,0.5645325779914856
translation,414,15,model,theorem proving,for,consolidating,theorem proving for consolidating,0.6637187004089355
translation,414,15,model,consolidating,has,equivalent queries,consolidating has equivalent queries,0.5697813630104065
translation,414,15,model,model,explicitly describe,handling,model explicitly describe handling,0.7141359448432922
translation,414,15,model,model,employ,theorem proving,model employ theorem proving,0.5747395157814026
translation,415,244,ablation-analysis,any of the iou or qou columns,observe,substan - tial gains,any of the iou or qou columns observe substan - tial gains,0.6684402227401733
translation,415,244,ablation-analysis,substan - tial gains,when,complementary information,substan - tial gains when complementary information,0.6682047843933105
translation,415,244,ablation-analysis,complementary information,added to,model,complementary information added to model,0.7023869156837463
translation,415,244,ablation-analysis,ablation analysis,On,any of the iou or qou columns,ablation analysis On any of the iou or qou columns,0.5407500863075256
translation,415,255,ablation-analysis,shortcuts,on,combined iou + qou - decoys,shortcuts on combined iou + qou - decoys,0.5837161540985107
translation,415,255,ablation-analysis,advantage,of using,sophisticated models,advantage of using sophisticated models,0.6797686219215393
translation,415,255,ablation-analysis,sophisticated models,becomes,obvious,sophisticated models becomes obvious,0.6761958003044128
translation,415,255,ablation-analysis,shortcuts,has,advantage,shortcuts has advantage,0.5650792717933655
translation,415,255,ablation-analysis,ablation analysis,eliminating,shortcuts,ablation analysis eliminating shortcuts,0.7301786541938782
translation,415,209,experimental-setup,200 - layer resnet,to compute,visual features,200 - layer resnet to compute visual features,0.6785876750946045
translation,415,209,experimental-setup,visual features,which are,"2,048 - dimensional","visual features which are 2,048 - dimensional",0.6532647609710693
translation,415,209,experimental-setup,experimental setup,use,200 - layer resnet,experimental setup use 200 - layer resnet,0.5489736795425415
translation,415,210,experimental-setup,resnet,pre-trained on,imagenet,resnet pre-trained on imagenet,0.7303796410560608
translation,415,210,experimental-setup,imagenet,has,),imagenet has ),0.6459620594978333
translation,415,210,experimental-setup,experimental setup,has,resnet,experimental setup has resnet,0.558605432510376
translation,415,211,experimental-setup,word2vec feature,for,questions and answers,word2vec feature for questions and answers,0.5882457494735718
translation,415,211,experimental-setup,questions and answers,are,300 dimensional,questions and answers are 300 dimensional,0.6722255349159241
translation,415,211,experimental-setup,300 dimensional,pre-trained on,google news,300 dimensional pre-trained on google news,0.7424142360687256
translation,415,211,experimental-setup,experimental setup,has,word2vec feature,experimental setup has word2vec feature,0.4935302436351776
translation,415,72,model,automatic procedures,to revise,vqa and visual7w,automatic procedures to revise vqa and visual7w,0.6619676351547241
translation,415,72,model,model,propose,automatic procedures,model propose automatic procedures,0.6943332552909851
translation,415,212,model,parameters,of,mlp models,parameters of mlp models,0.5756556987762451
translation,415,212,model,mlp models,learned by minimizing,binary logistic loss,mlp models learned by minimizing binary logistic loss,0.7583041787147522
translation,415,212,model,binary logistic loss,of predicting,candidate answer,binary logistic loss of predicting candidate answer,0.7148557901382446
translation,415,212,model,candidate answer,target of,corresponding iqa triplet,candidate answer target of corresponding iqa triplet,0.6539455056190491
translation,415,212,model,model,has,parameters,model has parameters,0.49046650528907776
translation,415,102,results,learning model,performs,significantly better,learning model performs significantly better,0.6527448892593384
translation,415,102,results,model,performs,significantly better,model performs significantly better,0.650906503200531
translation,415,102,results,significantly better,than,random guessing,significantly better than random guessing,0.6273674368858337
translation,415,102,results,significantly better,than,humans ( 52.9 % vs. 25 % ),significantly better than humans ( 52.9 % vs. 25 % ),0.5459200143814087
translation,415,102,results,learning model,has,model,learning model has model,0.5785998702049255
translation,415,105,results,information,about,image,information about image,0.675778329372406
translation,415,105,results,machine,close to,performance,machine close to performance,0.7199320197105408
translation,415,105,results,performance,when,all information is used ( 62.4 % vs. 65.7 % ),performance when all information is used ( 62.4 % vs. 65.7 % ),0.6598548293113708
translation,415,105,results,information,has,machine,information has machine,0.5463083386421204
translation,415,105,results,image,has,machine,image has machine,0.6030316948890686
translation,415,105,results,machine,has,improves significantly,machine has improves significantly,0.640221118927002
translation,415,105,results,results,Adding,information,results Adding information,0.6040204763412476
translation,415,235,results,relatively small gain,from,mlp - ia,relatively small gain from mlp - ia,0.6058769226074219
translation,415,235,results,relatively small gain,suggests that,question information,relatively small gain suggests that question information,0.7145243883132935
translation,415,235,results,question information,can be,ignored,question information can be ignored,0.7139280438423157
translation,415,235,results,ignored,to attain,good performance,ignored to attain good performance,0.6228845715522766
translation,415,235,results,orig decoys,has,relatively small gain,orig decoys has relatively small gain,0.6313880681991577
translation,415,235,results,mlp - ia,has,to mlp - iqa,mlp - ia has to mlp - iqa,0.641928493976593
translation,415,235,results,results,With,orig decoys,results With orig decoys,0.6486818194389343
translation,415,236,results,iou - decoys,which,questions,iou - decoys which questions,0.6578904390335083
translation,415,236,results,iou - decoys,require,questions,iou - decoys require questions,0.711013674736023
translation,415,236,results,gain,is,substantial,gain is substantial,0.6608620285987854
translation,415,236,results,gain,from,27.3 % to 84.1 %,gain from 27.3 % to 84.1 %,0.5578928589820862
translation,415,236,results,substantial,from,27.3 % to 84.1 %,substantial from 27.3 % to 84.1 %,0.5397692918777466
translation,415,236,results,iou - decoys,has,gain,iou - decoys has gain,0.590315580368042
translation,415,236,results,results,with,iou - decoys,results with iou - decoys,0.6182061433792114
translation,415,237,results,qou-decoys,including,images information,qou-decoys including images information,0.6615412831306458
translation,415,237,results,improves,from,40.7 %,improves from 40.7 %,0.5881077647209167
translation,415,237,results,substantially,to,57.6 %,substantially to 57.6 %,0.5991339683532715
translation,415,237,results,images information,has,improves,images information has improves,0.6228829026222229
translation,415,237,results,40.7 %,has,mlp - qa ),40.7 % has mlp - qa ),0.5974293351173401
translation,415,237,results,mlp - qa ),has,substantially,mlp - qa ) has substantially,0.6356604099273682
translation,415,237,results,57.6 %,has,mlp - iqa ),57.6 % has mlp - iqa ),0.5876589417457581
translation,415,237,results,results,with,qou-decoys,results with qou-decoys,0.6327188014984131
translation,415,242,results,answer information only ( mlp - a ),attains,chance - level accuracy,answer information only ( mlp - a ) attains chance - level accuracy,0.6756028532981873
translation,415,242,results,results,using,answer information only ( mlp - a ),results using answer information only ( mlp - a ),0.6841657757759094
translation,415,243,results,results,On,vqa dataset,results On vqa dataset,0.5189293026924133
translation,415,246,results,improvements,from,mlp - qa,improvements from mlp - qa,0.584301769733429
translation,415,246,results,mlp - qa,to,mlp - iqa,mlp - qa to mlp - iqa,0.5918657779693604
translation,415,246,results,lower,when facing,iou - decoys,lower when facing iou - decoys,0.6780095100402832
translation,415,246,results,results,notice that,improvements,results notice that improvements,0.6438111066818237
translation,415,262,results,fine -tuning,has,largely improves,fine -tuning has largely improves,0.5808970332145691
translation,415,262,results,largely improves,has,performance,largely improves has performance,0.5754314661026001
translation,416,5,model,clustering algorithm,optimizes,qa accuracy,clustering algorithm optimizes qa accuracy,0.7263683676719666
translation,416,5,model,qa accuracy,by maximizing,log-likelihood,qa accuracy by maximizing log-likelihood,0.6476038694381714
translation,416,5,model,log-likelihood,of,set of question - and - answer pairs,log-likelihood of set of question - and - answer pairs,0.5976019501686096
translation,416,15,model,system,trained with,clusters,system trained with clusters,0.7280335426330566
translation,416,15,model,clusters,of,q-a pairs,clusters of q-a pairs,0.6435430645942688
translation,416,15,model,model,use,statistical qa framework,model use statistical qa framework,0.6484836935997009
translation,416,4,results,correlation,between,question answering ( qa ) accuracy,correlation between question answering ( qa ) accuracy,0.6062467098236084
translation,416,4,results,correlation,between,log-likelihood,correlation between log-likelihood,0.6521421074867249
translation,416,4,results,log-likelihood,of,answer typing component,log-likelihood of answer typing component,0.5820194482803345
translation,416,4,results,answer typing component,of,our statistical qa model,answer typing component of our statistical qa model,0.5560189485549927
translation,416,4,results,strong,has,correlation,strong has correlation,0.5862206220626831
translation,416,79,results,manual clusters ( baseline ),achieves,m rr eval,manual clusters ( baseline ) achieves m rr eval,0.6888765096664429
translation,416,79,results,m rr eval,of,0.262,m rr eval of 0.262,0.588199257850647
translation,416,79,results,m rr eval,of,0.281,m rr eval of 0.281,0.5808600783348083
translation,416,79,results,m rr eval,of,0.281,m rr eval of 0.281,0.5808600783348083
translation,416,79,results,clusters,resulting from,clustering algorithm,clusters resulting from clustering algorithm,0.7425442934036255
translation,416,79,results,clusters,give,m rr eval,clusters give m rr eval,0.685696005821228
translation,416,79,results,m rr eval,of,0.281,m rr eval of 0.281,0.5808600783348083
translation,416,79,results,m rr eval,is,relative improvement,m rr eval is relative improvement,0.6169230937957764
translation,416,79,results,0.281,is,relative improvement,0.281 is relative improvement,0.5226839184761047
translation,416,79,results,relative improvement,of,7 %,relative improvement of 7 %,0.6129486560821533
translation,416,79,results,results,see that,manual clusters ( baseline ),results see that manual clusters ( baseline ),0.6639915108680725
translation,416,81,results,one - in- each cluster configuration,achieves,m rr eval,one - in- each cluster configuration achieves m rr eval,0.7190759778022766
translation,416,81,results,m rr eval,of,0.263,m rr eval of 0.263,0.5858256816864014
translation,416,81,results,m rr eval,is,statistically significant improvement,m rr eval is statistically significant improvement,0.6043220162391663
translation,416,81,results,m rr eval,not,statistically significant improvement,m rr eval not statistically significant improvement,0.6829882860183716
translation,416,81,results,0.263,not,statistically significant improvement,0.263 not statistically significant improvement,0.6034354567527771
translation,416,81,results,statistically significant improvement,over,baseline,statistically significant improvement over baseline,0.6921539306640625
translation,416,81,results,results,has,one - in- each cluster configuration,results has one - in- each cluster configuration,0.5570257306098938
translation,416,82,results,lowest accuracy,with,m,lowest accuracy with m,0.6773095726966858
translation,416,82,results,all - in- one cluster configuration,has,lowest accuracy,all - in- one cluster configuration has lowest accuracy,0.5574468970298767
translation,416,82,results,results,has,all - in- one cluster configuration,results has all - in- one cluster configuration,0.5447781682014465
translation,417,150,results,beetle ii system,performs,only slightly better,beetle ii system performs only slightly better,0.612657904624939
translation,417,150,results,only slightly better,than,baseline,only slightly better than baseline,0.5667433738708496
translation,417,150,results,baseline,on,overall accuracy,baseline on overall accuracy,0.5013943910598755
translation,417,150,results,overall accuracy,has,0.44 vs. 0.42 micro-averaged recall,overall accuracy has 0.44 vs. 0.42 micro-averaged recall,0.5608028769493103
translation,417,150,results,results,has,beetle ii system,results has beetle ii system,0.5525521039962769
translation,417,151,results,macro- averaged f 1 score,of,beetle ii system,macro- averaged f 1 score of beetle ii system,0.5887582302093506
translation,417,151,results,results,has,macro- averaged f 1 score,results has macro- averaged f 1 score,0.5303022265434265
translation,417,153,results,beetle ii parser,to,our lexical similarity baseline,beetle ii parser to our lexical similarity baseline,0.5219637155532837
translation,417,153,results,beetle ii,performs,similarly,beetle ii performs similarly,0.6584193706512451
translation,417,153,results,similarly,on,micro- and macroaveraged scores,similarly on micro- and macroaveraged scores,0.5899605751037598
translation,417,153,results,beetle ii parser,has,beetle ii,beetle ii parser has beetle ii,0.6578831076622009
translation,417,153,results,our lexical similarity baseline,has,beetle ii,our lexical similarity baseline has beetle ii,0.609748899936676
translation,417,153,results,beetle ii,has,lower overall accuracy,beetle ii has lower overall accuracy,0.5652351975440979
translation,417,153,results,results,Comparing,beetle ii parser,results Comparing beetle ii parser,0.6316789388656616
translation,417,154,results,beetle ii precision,higher than,classifier,beetle ii precision higher than classifier,0.6670669317245483
translation,417,154,results,classifier,in,all cases,classifier in all cases,0.6227039098739624
translation,417,154,results,all cases,except for,binary decision,all cases except for binary decision,0.6546314358711243
translation,417,154,results,binary decision,as to,corrective feedback should be issued,binary decision as to corrective feedback should be issued,0.7181046009063721
translation,417,154,results,results,has,beetle ii precision,results has beetle ii precision,0.5286574363708496
translation,417,161,results,accuracy,of,majority class baseline,accuracy of majority class baseline,0.5635683536529541
translation,417,161,results,majority class baseline,is,40 %,majority class baseline is 40 %,0.6044756174087524
translation,417,161,results,40 %,for,sci-entsbank,40 % for sci-entsbank,0.6608343720436096
translation,417,161,results,results,has,accuracy,results has accuracy,0.5888755321502686
translation,417,162,results,10 - fold crossvalidation,for,our simple lexical similarity classifier,10 - fold crossvalidation for our simple lexical similarity classifier,0.5920218825340271
translation,417,163,results,majority class baseline,by,0.18 and 3 %,majority class baseline by 0.18 and 3 %,0.5826953053474426
translation,417,163,results,majority class baseline,on,macro- averaged f 1 - measure and accuracy,majority class baseline on macro- averaged f 1 - measure and accuracy,0.5010445713996887
translation,417,163,results,0.18 and 3 %,on,macro- averaged f 1 - measure and accuracy,0.18 and 3 % on macro- averaged f 1 - measure and accuracy,0.5493203997612
translation,417,163,results,lexical similarity based classifier,has,outperforms,lexical similarity based classifier has outperforms,0.6324225664138794
translation,417,163,results,outperforms,has,majority class baseline,outperforms has majority class baseline,0.5907526612281799
translation,417,163,results,results,has,lexical similarity based classifier,results has lexical similarity based classifier,0.5897037982940674
translation,417,164,results,f 1 measure,for,two -way classification,f 1 measure for two -way classification,0.6310608983039856
translation,417,164,results,two -way classification,detecting,answers,two -way classification detecting answers,0.6774205565452576
translation,417,164,results,answers,which need,corrective feedback,answers which need corrective feedback,0.7207905650138855
translation,417,164,results,corrective feedback,is,0.66,corrective feedback is 0.66,0.5719538331031799
translation,417,164,results,results,for,two -way classification,results for two -way classification,0.5702847838401794
translation,417,164,results,results,has,f 1 measure,results has f 1 measure,0.5563697814941406
translation,417,171,results,system,performs,significantly better,system performs significantly better,0.6508617997169495
translation,417,171,results,significantly better,than,majority class baseline,significantly better than majority class baseline,0.5674336552619934
translation,417,172,results,outperformed,by,13 % and 3 %,outperformed by 13 % and 3 %,0.65840744972229
translation,417,172,results,majority - class baseline,by,0.33,majority - class baseline by 0.33,0.573897123336792
translation,417,172,results,majority - class baseline,by,0.18,majority - class baseline by 0.18,0.5662763714790344
translation,417,172,results,majority - class baseline,by,13 % and 3 %,majority - class baseline by 13 % and 3 %,0.6049975752830505
translation,417,172,results,majority - class baseline,by,13 % and 3 %,majority - class baseline by 13 % and 3 %,0.6049975752830505
translation,417,172,results,0.33,on,beetle ),0.33 on beetle ),0.5385574698448181
translation,417,172,results,0.18,on,scientsbank ),0.18 on scientsbank ),0.5378397107124329
translation,417,172,results,13 % and 3 %,on,accuracy,13 % and 3 % on accuracy,0.592333197593689
translation,417,172,results,macro- averaged f 1 - measure,has,our lexical classifier,macro- averaged f 1 - measure has our lexical classifier,0.5809059739112854
translation,417,172,results,our lexical classifier,has,outperformed,our lexical classifier has outperformed,0.5867516398429871
translation,417,172,results,outperformed,has,majority - class baseline,outperformed has majority - class baseline,0.602641224861145
translation,417,172,results,results,On,macro- averaged f 1 - measure,results On macro- averaged f 1 - measure,0.5176442861557007
translation,417,174,results,system,specifically designed to parse,beetle corpus answers,system specifically designed to parse beetle corpus answers,0.7335084676742554
translation,417,174,results,macro- averaged f 1 - measure,was,just 0.46,macro- averaged f 1 - measure was just 0.46,0.6254690289497375
translation,417,174,results,macro- averaged f 1 - measure,on,binary decision,macro- averaged f 1 - measure on binary decision,0.5597088932991028
translation,417,174,results,binary decision,regarding,response,binary decision regarding response,0.6986467838287354
translation,417,174,results,system,has,macro- averaged f 1 - measure,system has macro- averaged f 1 - measure,0.5868933796882629
translation,417,174,results,beetle corpus answers,has,macro- averaged f 1 - measure,beetle corpus answers has macro- averaged f 1 - measure,0.5725910067558289
translation,417,174,results,results,With,system,results With system,0.6613344550132751
translation,418,55,hyperparameters,inputs,to,reader,inputs to reader,0.6275686025619507
translation,418,55,hyperparameters,reader,padded to,384 tokens,reader padded to 384 tokens,0.7795394062995911
translation,418,55,hyperparameters,learning rate,set to,3 ? 10 ?5,learning rate set to 3 ? 10 ?5,0.7429711222648621
translation,418,55,hyperparameters,hyperparameters,has,inputs,hyperparameters has inputs,0.5048274993896484
translation,418,55,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,418,4,model,end-to- end question answering,integrates,bert,end-to- end question answering integrates bert,0.6218776702880859
translation,418,9,model,bert,with,open-source anserini ir toolkit,bert with open-source anserini ir toolkit,0.644843578338623
translation,418,9,model,open-source anserini ir toolkit,to create,bertserini,open-source anserini ir toolkit to create bertserini,0.6384354829788208
translation,418,9,model,bertserini,has,end-to - end open-domain question answering ( qa ) system,bertserini has end-to - end open-domain question answering ( qa ) system,0.5552356839179993
translation,418,9,model,model,integrate,bert,model integrate bert,0.6852753758430481
translation,418,11,results,best practices,from,information retrieval community,best practices from information retrieval community,0.560500979423523
translation,418,11,results,best practices,with,bert,best practices with bert,0.6816526651382446
translation,418,11,results,best practices,experiments on,standard benchmark,best practices experiments on standard benchmark,0.6398413777351379
translation,418,11,results,bert,to produce,end-to - end system,bert to produce end-to - end system,0.7017142176628113
translation,418,11,results,results,integrate,best practices,results integrate best practices,0.5570142865180969
translation,418,73,results,underperforms,by,large margin,underperforms by large margin,0.5930063724517822
translation,418,73,results,article retrieval,has,underperforms,article retrieval has underperforms,0.5679722428321838
translation,418,73,results,underperforms,has,paragraph retrieval,underperforms has paragraph retrieval,0.540708065032959
translation,418,73,results,results,see that,article retrieval,results see that article retrieval,0.626526951789856
translation,418,74,results,sentences,perform,reasonably but not as well,sentences perform reasonably but not as well,0.5905402302742004
translation,418,74,results,reasonably but not as well,as,paragraphs,reasonably but not as well as paragraphs,0.6915433406829834
translation,418,74,results,results,has,sentences,results has sentences,0.4975784122943878
translation,419,31,ablation-analysis,every single piece,contributes,important information,every single piece contributes important information,0.680421769618988
translation,419,31,ablation-analysis,important information,to achieve,final performance,important information to achieve final performance,0.6719496846199036
translation,419,31,ablation-analysis,ablation analysis,observe,every single piece,ablation analysis observe every single piece,0.6057908535003662
translation,419,96,ablation-analysis,network,to do,classification,network to do classification,0.43863338232040405
translation,419,96,ablation-analysis,network,giving,question,network giving question,0.7287890911102295
translation,419,96,ablation-analysis,network,giving,single comment,network giving single comment,0.7144043445587158
translation,419,96,ablation-analysis,single comment,as,input,single comment as input,0.5332121253013611
translation,419,96,ablation-analysis,performance drop,of,0.6 map points,performance drop of 0.6 map points,0.5775836706161499
translation,419,96,ablation-analysis,0.6 map points,compared to,proposed pairwise learning model,0.6 map points compared to proposed pairwise learning model,0.6186803579330444
translation,419,96,ablation-analysis,classification,has,mte - cqa classif ication,classification has mte - cqa classif ication,0.5799431800842285
translation,419,96,ablation-analysis,ablation analysis,adapting,network,ablation analysis adapting network,0.7704930305480957
translation,419,101,ablation-analysis,most important,turn out to be,task features,most important turn out to be task features,0.560722291469574
translation,419,101,ablation-analysis,task features,contributing,five map points,task features contributing five map points,0.7289091348648071
translation,419,101,ablation-analysis,task features,handle,important information sources,task features handle important information sources,0.6466917395591736
translation,419,101,ablation-analysis,ablation analysis,has,most important,ablation analysis has most important,0.5005125999450684
translation,419,102,ablation-analysis,importance,come,word embeddings,importance come word embeddings,0.656605064868927
translation,419,102,ablation-analysis,ql vectors,trained on,text,ql vectors trained on text,0.7714913487434387
translation,419,102,ablation-analysis,text,from,target forum,text from target forum,0.5787146687507629
translation,419,104,ablation-analysis,mte - motivated syntax vectors,contribute,half a map point,mte - motivated syntax vectors contribute half a map point,0.6337695121765137
translation,419,104,ablation-analysis,ablation analysis,has,mte - motivated syntax vectors,ablation analysis has mte - motivated syntax vectors,0.5292436480522156
translation,419,105,ablation-analysis,other two mte features,contribute,0.8 map points,other two mte features contribute 0.8 map points,0.6475573182106018
translation,419,105,ablation-analysis,ablation analysis,has,other two mte features,ablation analysis has other two mte features,0.5316015481948853
translation,419,109,experiments,our system mte - cqa,rank,first,our system mte - cqa rank first,0.7672321796417236
translation,419,109,experiments,our system mte - cqa,rank,second,our system mte - cqa rank second,0.7743272185325623
translation,419,109,experiments,our system mte - cqa,rank,fourth,our system mte - cqa rank fourth,0.7485421895980835
translation,419,109,experiments,first,on,mrr,first on mrr,0.570044755935669
translation,419,109,experiments,second,on,map,second on map,0.5944998860359192
translation,419,109,experiments,fourth,on,avgrec,fourth on avgrec,0.6111083030700684
translation,419,109,experiments,avgrec,in,semeval - 2016 task 3 competition,avgrec in semeval - 2016 task 3 competition,0.516108512878418
translation,419,82,hyperparameters,our model,on,train - part1,our model on train - part1,0.5398736000061035
translation,419,82,hyperparameters,our model,with,minibatches,our model with minibatches,0.6520516872406006
translation,419,82,hyperparameters,our model,with,regularization,our model with regularization,0.6744288206100464
translation,419,82,hyperparameters,our model,with,decay,our model with decay,0.6980666518211365
translation,419,82,hyperparameters,our model,using,stochastic gradient descent,our model using stochastic gradient descent,0.6470151543617249
translation,419,82,hyperparameters,train - part1,with,hidden layers,train - part1 with hidden layers,0.6158812642097473
translation,419,82,hyperparameters,train - part1,with,minibatches,train - part1 with minibatches,0.6495296359062195
translation,419,82,hyperparameters,train - part1,with,minibatches,train - part1 with minibatches,0.6495296359062195
translation,419,82,hyperparameters,train - part1,using,stochastic gradient descent,train - part1 using stochastic gradient descent,0.6512229442596436
translation,419,82,hyperparameters,hidden layers,of size,3,hidden layers of size 3,0.7335939407348633
translation,419,82,hyperparameters,hidden layers,for,100 epochs,hidden layers for 100 epochs,0.5454974174499512
translation,419,82,hyperparameters,3,for,100 epochs,3 for 100 epochs,0.615050733089447
translation,419,82,hyperparameters,minibatches,of size,30,minibatches of size 30,0.688652753829956
translation,419,82,hyperparameters,regularization,of,0.005,regularization of 0.005,0.6134800910949707
translation,419,82,hyperparameters,decay,of,0.0001,decay of 0.0001,0.5845822095870972
translation,419,82,hyperparameters,0.0001,using,stochastic gradient descent,0.0001 using stochastic gradient descent,0.6239730715751648
translation,419,82,hyperparameters,stochastic gradient descent,with,adagrad,stochastic gradient descent with adagrad,0.6186466217041016
translation,419,82,hyperparameters,hyperparameters,train,our model,hyperparameters train our model,0.6687315702438354
translation,419,83,hyperparameters,input feature values,to,[ ?1 ; 1 ] interval,input feature values to [ ?1 ; 1 ] interval,0.5030277371406555
translation,419,83,hyperparameters,input feature values,to,uniform distribution,input feature values to uniform distribution,0.5345566868782043
translation,419,83,hyperparameters,input feature values,using,minmax,input feature values using minmax,0.6314250826835632
translation,419,83,hyperparameters,[ ?1 ; 1 ] interval,using,minmax,[ ?1 ; 1 ] interval using minmax,0.5504364967346191
translation,419,83,hyperparameters,hyperparameters,normalize,input feature values,hyperparameters normalize input feature values,0.6330884099006653
translation,419,14,model,model,extend,mte neural network framework,model extend mte neural network framework,0.6936022639274597
translation,419,103,model,google vectors,trained on,100 billion words,google vectors trained on 100 billion words,0.7078238725662231
translation,419,103,model,google vectors,still,useful,google vectors still useful,0.7362447381019592
translation,419,103,model,useful,presence of,domain-specific ql vectors,useful presence of domain-specific ql vectors,0.639000415802002
translation,419,103,model,useful,trained on,four orders of magnitude less data,useful trained on four orders of magnitude less data,0.7551668882369995
translation,419,28,results,na?ve application,of,mte architecture and features,na?ve application of mte architecture and features,0.6114046573638916
translation,419,28,results,na?ve application,yields,results,na?ve application yields results,0.7674708366394043
translation,419,28,results,mte architecture and features,on,cqa task,mte architecture and features on cqa task,0.5677325129508972
translation,419,28,results,results,that are,largely above,results that are largely above,0.6844075918197632
translation,419,28,results,largely above,has,task baselines,largely above has task baselines,0.5866568088531494
translation,419,28,results,results,show,na?ve application,results show na?ve application,0.6870973110198975
translation,419,29,results,models,with,indomain data,models with indomain data,0.6219963431358337
translation,419,29,results,lightweight task -specific features,to,boost,lightweight task -specific features to boost,0.5004202127456665
translation,419,29,results,boost,to reach,state - of - the - art performance,boost to reach state - of - the - art performance,0.6530038714408875
translation,419,29,results,our system,to reach,state - of - the - art performance,our system to reach state - of - the - art performance,0.5787293314933777
translation,419,29,results,boost,has,our system,boost has our system,0.598308265209198
translation,419,29,results,results,adapting,models,results adapting models,0.6823934316635132
translation,419,92,results,vanilla mte system ( mte vanilla ),uses,features,vanilla mte system ( mte vanilla ) uses features,0.6683647632598877
translation,419,92,results,vanilla mte system ( mte vanilla ),performs,surprisingly well,vanilla mte system ( mte vanilla ) performs surprisingly well,0.620500385761261
translation,419,92,results,results,see that,vanilla mte system ( mte vanilla ),results see that vanilla mte system ( mte vanilla ),0.6840739846229553
translation,419,93,results,chronological baseline,assumes,early comments,chronological baseline assumes early comments,0.683081328868866
translation,419,93,results,early comments,better than,later ones ( baseline time ),early comments better than later ones ( baseline time ),0.7130135893821716
translation,419,93,results,later ones ( baseline time ),by,large margins,later ones ( baseline time ) by large margins,0.5865874886512756
translation,419,93,results,large margins,by about,11 and 17 map points absolute,large margins by about 11 and 17 map points absolute,0.677284300327301
translation,419,93,results,outperforms,has,random baseline ( baseline rand ),outperforms has random baseline ( baseline rand ),0.5574299097061157
translation,419,93,results,outperforms,has,chronological baseline,outperforms has chronological baseline,0.6331819295883179
translation,419,93,results,results,has,outperforms,results has outperforms,0.6657275557518005
translation,419,95,results,task-specific features,in,mte - cqa pairwise,task-specific features in mte - cqa pairwise,0.5268456339836121
translation,419,95,results,task-specific features,improves,results,task-specific features improves results,0.6500812768936157
translation,419,95,results,results,by,another 8 map points absolute,results by another 8 map points absolute,0.6383992433547974
translation,419,95,results,results,adding,task-specific features,results adding task-specific features,0.5852477550506592
translation,419,95,results,results,by,another 8 map points absolute,results by another 8 map points absolute,0.6383992433547974
translation,419,97,results,pairwise training strategy,confirmed to be,better,pairwise training strategy confirmed to be better,0.6931062936782837
translation,419,97,results,better,for,ranking task,better for ranking task,0.6036894917488098
translation,419,97,results,better,not by,large margin,better not by large margin,0.6785715222358704
translation,419,97,results,results,has,pairwise training strategy,results has pairwise training strategy,0.5435266494750977
translation,419,111,results,results,are,5 and 16 points,results are 5 and 16 points,0.5678972601890564
translation,419,111,results,5 and 16 points,above,average and the worst systems,5 and 16 points above average and the worst systems,0.7015350461006165
translation,419,111,results,results,are,5 and 16 points,results are 5 and 16 points,0.5678972601890564
translation,419,111,results,results,has,results,results has results,0.48582205176353455
translation,420,5,model,unified question retrieval model,based on,latent semantic indexing,unified question retrieval model based on latent semantic indexing,0.5336713790893555
translation,420,5,model,unified question retrieval model,can capture,word associations,unified question retrieval model can capture word associations,0.6583960056304932
translation,420,5,model,latent semantic indexing,with,tensor analysis,latent semantic indexing with tensor analysis,0.5683514475822449
translation,420,5,model,word associations,among,different parts,word associations among different parts,0.5857206583023071
translation,420,5,model,different parts,of,cqa triples simultaneously,different parts of cqa triples simultaneously,0.6187229156494141
translation,420,5,model,model,propose,unified question retrieval model,model propose unified question retrieval model,0.6372292041778564
translation,420,21,model,novel unified retrieval model,for,cqa,novel unified retrieval model for cqa,0.6307147145271301
translation,420,21,model,model,propose,novel unified retrieval model,model propose novel unified retrieval model,0.6230499148368835
translation,420,22,model,lsti,integrate,two detached parts,lsti integrate two detached parts,0.6830692887306213
translation,420,22,model,two detached parts,modeling,word association and question retrieval,two detached parts modeling word association and question retrieval,0.6745680570602417
translation,420,22,model,two detached parts,into,single model,two detached parts into single model,0.6288275718688965
translation,420,113,results,others,on,both datasets,others on both datasets,0.4858146905899048
translation,420,113,results,our method,has,outperforms,our method has outperforms,0.6322360634803772
translation,420,113,results,outperforms,has,others,outperforms has others,0.6126620769500732
translation,421,111,baselines,avg,is,baseline model,avg is baseline model,0.5535690784454346
translation,421,111,baselines,avg,computes,averages,avg computes averages,0.7323694825172424
translation,421,111,baselines,averages,of,unigram word embeddings,averages of unigram word embeddings,0.5583998560905457
translation,421,111,baselines,baselines,has,avg,baselines has avg,0.5678225159645081
translation,421,28,experimental-setup,infoboxqa dataset infoboxes,designed to be,human-readable,infoboxqa dataset infoboxes designed to be human-readable,0.6461979746818542
translation,421,28,experimental-setup,experimental setup,has,infoboxqa dataset infoboxes,experimental setup has infoboxqa dataset infoboxes,0.5475504398345947
translation,421,74,experimental-setup,pre-trained glove 2 embeddings,with,d = 300,pre-trained glove 2 embeddings with d = 300,0.5997872948646545
translation,421,74,experimental-setup,experimental setup,use,pre-trained glove 2 embeddings,experimental setup use pre-trained glove 2 embeddings,0.5475609302520752
translation,421,76,experimental-setup,multi-channel cnn architecture,with,three weightsharing cnns,multi-channel cnn architecture with three weightsharing cnns,0.632405161857605
translation,421,76,experimental-setup,experimental setup,use,multi-channel cnn architecture,experimental setup use multi-channel cnn architecture,0.5967971682548523
translation,421,26,experiments,infoboxqa,targets,tabular data,infoboxqa targets tabular data,0.6994478106498718
translation,421,26,experiments,tabular data,not augmented with,value types,tabular data not augmented with value types,0.7618783116340637
translation,421,26,experiments,tabular data,linked to,ontology,tabular data linked to ontology,0.6970632076263428
translation,421,97,experiments,tri-cnn,in,dataset- sts 3 framework,tri-cnn in dataset- sts 3 framework,0.5291590094566345
translation,421,97,experiments,dataset- sts 3 framework,for,semantic text similarity,dataset- sts 3 framework for semantic text similarity,0.5656858086585999
translation,421,97,experiments,semantic text similarity,built on top of,keras deep learning library,semantic text similarity built on top of keras deep learning library,0.6976838707923889
translation,421,6,model,system,to access,data,system to access data,0.6618914008140564
translation,421,6,model,data,in,infoboxes,data in infoboxes,0.52524733543396
translation,421,6,model,infoboxes,in,structured manner,infoboxes in structured manner,0.5724458694458008
translation,421,6,model,model,build,system,model build system,0.7545252442359924
translation,421,27,model,multichannel convolutional neural network ( cnn ) model,achieves,best results,multichannel convolutional neural network ( cnn ) model achieves best results,0.6562780141830444
translation,421,27,model,best results,on,infoboxqa,best results on infoboxqa,0.5629737377166748
translation,421,27,model,best results,compared to,other neural network models,best results compared to other neural network models,0.6430637836456299
translation,421,27,model,other neural network models,in,answer selection task,other neural network models in answer selection task,0.4472109377384186
translation,421,27,model,model,built,multichannel convolutional neural network ( cnn ) model,model built multichannel convolutional neural network ( cnn ) model,0.6395968198776245
translation,421,65,model,architecture,with,three weightsharing cnns,architecture with three weightsharing cnns,0.6387621760368347
translation,421,65,model,model,propose,architecture,model propose architecture,0.697865903377533
translation,421,66,model,element- wise product merge layer,to compute,similarities,element- wise product merge layer to compute similarities,0.7439493536949158
translation,421,66,model,similarities,between,question and attribute,similarities between question and attribute,0.6785192489624023
translation,421,66,model,similarities,between,attribute and answer,similarities between attribute and answer,0.689586341381073
translation,421,66,model,similarities,between,attribute and answer,similarities between attribute and answer,0.689586341381073
translation,421,66,model,model,use,element- wise product merge layer,model use element- wise product merge layer,0.6609748601913452
translation,421,114,model,rnn,computes,summary embeddings,rnn computes summary embeddings,0.7140120267868042
translation,421,114,model,summary embeddings,of,question and answer,summary embeddings of question and answer,0.6149846315383911
translation,421,114,model,summary embeddings,using,bidirectional grus,summary embeddings using bidirectional grus,0.6736520528793335
translation,421,114,model,attn1511,feeds,outputs,attn1511 feeds outputs,0.7465768456459045
translation,421,114,model,outputs,of,bi-gru,outputs of bi-gru,0.6129348874092102
translation,421,114,model,bi-gru,into,convolution layer,bi-gru into convolution layer,0.5792753100395203
translation,421,114,model,model,has,rnn,model has rnn,0.6028110980987549
translation,421,119,results,results,on,infoboxqa,results on infoboxqa,0.576786994934082
translation,421,120,results,performance,of,baselines,performance of baselines,0.5870708227157593
translation,421,120,results,baselines,indicates,unigram bag-of-words models,baselines indicates unigram bag-of-words models,0.5953091979026794
translation,421,120,results,unigram bag-of-words models,are,not sufficiently expressive,unigram bag-of-words models are not sufficiently expressive,0.5667953491210938
translation,421,120,results,not sufficiently expressive,for,matching,not sufficiently expressive for matching,0.6218211054801941
translation,421,120,results,tri-cnn,makes use of,larger semantic units,tri-cnn makes use of larger semantic units,0.6268607378005981
translation,421,120,results,larger semantic units,through,multiple channels,larger semantic units through multiple channels,0.6154890656471252
translation,421,120,results,results,has,performance,results has performance,0.5972660779953003
translation,421,121,results,combination of an rnn and cnn,in,attn1511,combination of an rnn and cnn in attn1511,0.5723003149032593
translation,421,121,results,combination of an rnn and cnn,achieves,better results,combination of an rnn and cnn achieves better results,0.6186133623123169
translation,421,121,results,combination of an rnn and cnn,performs,slightly worse,combination of an rnn and cnn performs slightly worse,0.5987748503684998
translation,421,121,results,better results,than,rnn,better results than rnn,0.6191876530647278
translation,421,121,results,slightly worse,than,cnn model,slightly worse than cnn model,0.564891517162323
translation,421,121,results,cnn model,with,weightsharing,cnn model with weightsharing,0.6531288027763367
translation,421,121,results,results,has,attention mechanism,results has attention mechanism,0.5514436960220337
translation,421,124,results,tri-cnn model,built on top of,weight - sharing architecture,tri-cnn model built on top of weight - sharing architecture,0.6994951367378235
translation,421,124,results,tri-cnn model,achieves,best performance,tri-cnn model achieves best performance,0.6641209125518799
translation,421,124,results,results,has,tri-cnn model,results has tri-cnn model,0.542928159236908
translation,421,125,results,tri-cnn,computes,match,tri-cnn computes match,0.7388941049575806
translation,421,125,results,match,by comparing,similarities,match by comparing similarities,0.7557990550994873
translation,421,125,results,similarities,between,question - attribute and answer-attribute,similarities between question - attribute and answer-attribute,0.6674886345863342
translation,421,125,results,similarities,leads to,improved results,similarities leads to improved results,0.6895226240158081
translation,421,125,results,improved results,over,models,improved results over models,0.7026195526123047
translation,421,125,results,models,compare,question,models compare question,0.7640647888183594
translation,421,125,results,results,has,tri-cnn,results has tri-cnn,0.5484370589256287
translation,422,155,ablation-analysis,comparable performance,of,link prediction,comparable performance of link prediction,0.599632978439331
translation,422,155,ablation-analysis,link prediction,to,static kg methods,link prediction to static kg methods,0.5248771905899048
translation,422,155,ablation-analysis,link prediction,indicates,even predicting 1 - hop knowledge paths,link prediction indicates even predicting 1 - hop knowledge paths,0.6067400574684143
translation,422,155,ablation-analysis,static kg methods,indicates,even predicting 1 - hop knowledge paths,static kg methods indicates even predicting 1 - hop knowledge paths,0.6073513031005859
translation,422,155,ablation-analysis,even predicting 1 - hop knowledge paths,to address,kg sparsity,even predicting 1 - hop knowledge paths to address kg sparsity,0.6164765954017639
translation,422,155,ablation-analysis,ablation analysis,has,comparable performance,ablation analysis has comparable performance,0.5483081936836243
translation,422,180,ablation-analysis,global generator,has,higher novelty,global generator has higher novelty,0.55695641040802
translation,422,180,ablation-analysis,ablation analysis,has,global generator,ablation analysis has global generator,0.5057765245437622
translation,422,29,baselines,neural kg,due to,neural generalization,neural kg due to neural generalization,0.6579522490501404
translation,422,29,baselines,neural kg,term,static kg,neural kg term static kg,0.6887950897216797
translation,422,29,baselines,neural generalization,over,structured kgs,neural generalization over structured kgs,0.6999235153198242
translation,422,29,baselines,static kg,for,methods,static kg for methods,0.6364647746086121
translation,422,29,baselines,methods,which rely exclusively on,existing facts,methods which rely exclusively on existing facts,0.744265615940094
translation,422,136,baselines,1 - hop link predictor,on,question and the answer entities,1 - hop link predictor on question and the answer entities,0.575054943561554
translation,422,136,baselines,fine- tuned lm,has,static kg - augmented models,fine- tuned lm has static kg - augmented models,0.5890927314758301
translation,422,138,baselines,  fine- tuned lm   ablation,of,our qa framework,  fine- tuned lm   ablation of our qa framework,0.5424364805221558
translation,422,138,baselines,  fine- tuned lm   ablation,without,knowledge module,  fine- tuned lm   ablation without knowledge module,0.6877357959747314
translation,422,138,baselines,our qa framework,without,knowledge module,our qa framework without knowledge module,0.7100561857223511
translation,422,138,baselines,knowledge module,has,),knowledge module has ),0.6424310803413391
translation,422,140,baselines,three static kg variants,of,qa framework,three static kg variants of qa framework,0.5813047885894775
translation,422,140,baselines,three static kg variants,model,knowledge module,three static kg variants model knowledge module,0.6703793406486511
translation,422,140,baselines,knowledge module,with,path / graph encoders,knowledge module with path / graph encoders,0.599515974521637
translation,422,140,baselines,rn degenerate version,of,our system,rn degenerate version of our system,0.6183169484138489
translation,422,140,baselines,rn degenerate version,computes,knowledge embedding,rn degenerate version computes knowledge embedding,0.7091244459152222
translation,422,140,baselines,knowledge embedding,by,attention mechanism,knowledge embedding by attention mechanism,0.5135043859481812
translation,422,140,baselines,attention mechanism,over,retrieved paths,attention mechanism over retrieved paths,0.6646429300308228
translation,422,140,baselines,retrieved paths,for,each question - choice entity pair,retrieved paths for each question - choice entity pair,0.5803394317626953
translation,422,140,baselines,relational graph convolutional networks ( rgcn ),encode,local graphs,relational graph convolutional networks ( rgcn ) encode local graphs,0.7564046382904053
translation,422,140,baselines,local graphs,by using,graph convolutional networks,local graphs by using graph convolutional networks,0.5579081773757935
translation,422,140,baselines,graph convolutional networks,with,relation -specific weight matrices,graph convolutional networks with relation -specific weight matrices,0.6078243851661682
translation,422,140,baselines,gconattn,models,alignment,gconattn models alignment,0.7325400114059448
translation,422,140,baselines,alignment,between,entities,alignment between entities,0.7071263194084167
translation,422,140,baselines,alignment,via,attention and pools,alignment via attention and pools,0.7155913710594177
translation,422,140,baselines,gconattn,has,"wang et al. , 2019 )","gconattn has wang et al. , 2019 )",0.5972439646720886
translation,422,142,baselines,relation,between,question and answer entities,relation between question and answer entities,0.6606600880622864
translation,422,147,baselines,three variants,of,our method,three variants of our method,0.5668160319328308
translation,422,147,baselines,differ,in terms of,knowledge embedding,differ in terms of knowledge embedding,0.729052722454071
translation,422,147,baselines,pg - full,combination of,our global pg and a static rn,pg - full combination of our global pg and a static rn,0.6588212251663208
translation,422,147,baselines,local pg,trained on,local and global paths,local pg trained on local and global paths,0.7492974996566772
translation,422,147,baselines,"global , data- independent pg",trained on,global paths only,"global , data- independent pg trained on global paths only",0.7246356010437012
translation,422,147,baselines,pg - local,has,local pg,pg - local has local pg,0.6271722912788391
translation,422,147,baselines,pg - global,has,"global , data- independent pg","pg - global has global , data- independent pg",0.5589224100112915
translation,422,147,baselines,baselines,experiment with,three variants,baselines experiment with three variants,0.6652116179466248
translation,422,173,baselines,commonsense knowledge base completion model,has,"bilinear avg ( li et al. , 2016 )","commonsense knowledge base completion model has bilinear avg ( li et al. , 2016 )",0.5097360014915466
translation,422,150,experiments,several encoders,as,context module,several encoders as context module,0.49582409858703613
translation,422,150,experiments,several encoders,for,open-bookqa,several encoders for open-bookqa,0.6040525436401367
translation,422,150,experiments,roberta - large,for,commonsenseqa,roberta - large for commonsenseqa,0.6811316609382629
translation,422,150,experiments,roberta - large,for,open-bookqa,roberta - large for open-bookqa,0.6839712262153625
translation,422,150,experiments,"aristoroberta ( clark et al. , 2019 )",for,open-bookqa,"aristoroberta ( clark et al. , 2019 ) for open-bookqa",0.5981278419494629
translation,422,166,experiments,scratch variant,achieves,68.75 and 65.50 accuracy,scratch variant achieves 68.75 and 65.50 accuracy,0.6775727272033691
translation,422,166,experiments,68.75 and 65.50 accuracy,on,com-monsenseqa and openbookqa test sets,68.75 and 65.50 accuracy on com-monsenseqa and openbookqa test sets,0.5044629573822021
translation,422,166,experiments,68.75 and 65.50 accuracy,with,roberta - large,68.75 and 65.50 accuracy with roberta - large,0.6481676697731018
translation,422,166,experiments,roberta - large,as,text encoder,roberta - large as text encoder,0.5675980448722839
translation,422,7,model,general commonsense qa framework,with,knowledgeable path generator,general commonsense qa framework with knowledgeable path generator,0.5904055833816528
translation,422,7,model,model,augment,general commonsense qa framework,model augment general commonsense qa framework,0.667709231376648
translation,422,8,model,existing paths,in,kg,existing paths in kg,0.5334812998771667
translation,422,8,model,existing paths,with,state - of - the - art language model,existing paths with state - of - the - art language model,0.6202125549316406
translation,422,8,model,our generator,learns to connect,pair of entities,our generator learns to connect pair of entities,0.7553249597549438
translation,422,8,model,pair of entities,in,text,pair of entities in text,0.5329268574714661
translation,422,8,model,pair of entities,with,"dynamic , and potentially novel , multi-hop relational path","pair of entities with dynamic , and potentially novel , multi-hop relational path",0.6330678462982178
translation,422,8,model,existing paths,has,our generator,existing paths has our generator,0.6041942834854126
translation,422,8,model,state - of - the - art language model,has,our generator,state - of - the - art language model has our generator,0.5321671962738037
translation,422,28,model,knowledgeable path generator ( pg ),generalizes over,facts,knowledgeable path generator ( pg ) generalizes over facts,0.7974616289138794
translation,422,28,model,facts,stored in,kg,facts stored in kg,0.7258175611495972
translation,422,28,model,model,propose,knowledgeable path generator ( pg ),model propose knowledgeable path generator ( pg ),0.6783555746078491
translation,422,143,model,"transe ( bordes et al. , 2013 )",to learn,representation,"transe ( bordes et al. , 2013 ) to learn representation",0.6152161359786987
translation,422,143,model,representation,for,every entity and relation,representation for every entity and relation,0.6036311984062195
translation,422,143,model,every entity and relation,in,conceptnet,every entity and relation in conceptnet,0.5395100712776184
translation,422,143,model,leveraged,to predict,1,leveraged to predict 1,0.7659852504730225
translation,422,143,model,1,-,hop relation,1 - hop relation,0.6442745327949524
translation,422,143,model,hop relation,for,each pair,hop relation for each pair,0.6367827653884888
translation,422,143,model,each pair,of,question and answer entities,each pair of question and answer entities,0.6054419279098511
translation,422,143,model,model,employ,"transe ( bordes et al. , 2013 )","model employ transe ( bordes et al. , 2013 )",0.5600312948226929
translation,422,37,results,out method,performs,better,out method performs better,0.6383865475654602
translation,422,37,results,better,than,previous systems,better than previous systems,0.6103594303131104
translation,422,37,results,previous systems,augmented with,static kgs,previous systems augmented with static kgs,0.7282827496528625
translation,422,37,results,static kgs,by,up to 6 % in accuracy,static kgs by up to 6 % in accuracy,0.6361917853355408
translation,422,37,results,results,show,out method,results show out method,0.6985155940055847
translation,422,38,results,accuracy gain,over,baselines,accuracy gain over baselines,0.6868653297424316
translation,422,38,results,accuracy gain,grows as,training data,accuracy gain grows as training data,0.5710893869400024
translation,422,38,results,larger inductive bias,of,our generator,larger inductive bias of our generator,0.620519757270813
translation,422,38,results,low-resource setting,has,accuracy gain,low-resource setting has accuracy gain,0.5419884324073792
translation,422,38,results,training data,has,decreases,training data has decreases,0.5969269275665283
translation,422,38,results,results,In,low-resource setting,results In low-resource setting,0.5339930057525635
translation,422,133,results,global sampling,generates,"2,825,692 paths","global sampling generates 2,825,692 paths",0.6473071575164795
translation,422,133,results,local sampling,results in,"133,612 paths","local sampling results in 133,612 paths",0.6455143690109253
translation,422,133,results,local sampling,results in,"105,155","local sampling results in 105,155",0.6690978407859802
translation,422,133,results,"133,612 paths",for,common-senseqa,"133,612 paths for common-senseqa",0.6184349060058594
translation,422,133,results,"105,155",for,openbookqa,"105,155 for openbookqa",0.6300093531608582
translation,422,133,results,results,has,global sampling,results has global sampling,0.5181989669799805
translation,422,152,results,both datasets,observe,consistent improvements,both datasets observe consistent improvements,0.5830187797546387
translation,422,152,results,consistent improvements,brought by,our method,consistent improvements brought by our method,0.7132686972618103
translation,422,152,results,our method,with,different context encoders,our method with different context encoders,0.6098810434341431
translation,422,152,results,results,On,both datasets,results On both datasets,0.49870386719703674
translation,422,153,results,our full model,combines,generated and static knowledge,our full model combines generated and static knowledge,0.6622909307479858
translation,422,153,results,our full model,achieves,best performance,our full model achieves best performance,0.6880382895469666
translation,422,153,results,generated and static knowledge,achieves,best performance,generated and static knowledge achieves best performance,0.652837336063385
translation,422,153,results,results,has,our full model,results has our full model,0.5411279201507568
translation,422,154,results,local or global variant,yields,second best results,local or global variant yields second best results,0.6910563707351685
translation,422,158,results,full method,achieves,best performance,full method achieves best performance,0.6962085366249084
translation,422,158,results,best performance,on,both datasets,best performance on both datasets,0.45760658383369446
translation,422,161,results,our method ( with roberta ),performs,better or equal,our method ( with roberta ) performs better or equal,0.6276882290840149
translation,422,161,results,better or equal,to,baselines,better or equal to baselines,0.6008377075195312
translation,422,161,results,baselines,with,any amount of training data,baselines with any amount of training data,0.6131027936935425
translation,422,161,results,results,show,our method ( with roberta ),results show our method ( with roberta ),0.6185742020606995
translation,422,162,results,performance gain,brought by,our global or full model,performance gain brought by our global or full model,0.6567975282669067
translation,422,162,results,our global or full model,is,higher,our global or full model is higher,0.6653275489807129
translation,422,162,results,higher,when,less data,higher when less data,0.737515389919281
translation,422,162,results,results,has,performance gain,results has performance gain,0.5905830264091492
translation,422,177,results,our two generator variants,able to connect,vast majority,our two generator variants able to connect vast majority,0.6812868118286133
translation,422,177,results,of the entity pairs,with,valid path,of the entity pairs with valid path,0.6152821183204651
translation,422,179,results,novel paths,from,global generator,novel paths from global generator,0.6144914627075195
translation,422,179,results,novel paths,of,higher quality,novel paths of higher quality,0.577757716178894
translation,422,179,results,global generator,are,higher quality,global generator are higher quality,0.583094596862793
translation,422,179,results,results,has,novel paths,results has novel paths,0.5620261430740356
translation,422,187,results,our global generator,achieves,higher scores,our global generator achieves higher scores,0.6552277207374573
translation,422,187,results,both dimensions,has,our global generator,both dimensions has our global generator,0.6525751352310181
translation,422,187,results,results,For,both dimensions,results For both dimensions,0.588805615901947
translation,423,26,ablation-analysis,thread - level features,are,important,thread - level features are important,0.5266968011856079
translation,423,26,ablation-analysis,thread - level features,providing,consistent improvement,thread - level features providing consistent improvement,0.6713573336601257
translation,423,26,ablation-analysis,consistent improvement,for,all our learning models,consistent improvement for all our learning models,0.6138082146644592
translation,423,26,ablation-analysis,ablation analysis,show,thread - level features,ablation analysis show thread - level features,0.5932716131210327
translation,423,92,ablation-analysis,threadlevel features,helps,significantly,threadlevel features helps significantly,0.6305683255195618
translation,423,92,ablation-analysis,ordinal regression model,achieves,at least as good results,ordinal regression model achieves at least as good results,0.6179676651954651
translation,423,92,ablation-analysis,at least as good results,as,top system,at least as good results as top system,0.5545534491539001
translation,423,92,ablation-analysis,top system,at,semeval,top system at semeval,0.6023701429367065
translation,423,92,ablation-analysis,ablation analysis,using,threadlevel features,ablation analysis using threadlevel features,0.6359350085258484
translation,423,108,ablation-analysis,thread - level features,do not always improve,f 1,thread - level features do not always improve f 1,0.6548210978507996
translation,423,108,ablation-analysis,test dataset,has,thread - level features,test dataset has thread - level features,0.5195515751838684
translation,423,108,ablation-analysis,f 1,has,ta and a ta,f 1 has ta and a ta,0.669425368309021
translation,423,108,ablation-analysis,ablation analysis,on,test dataset,ablation analysis on test dataset,0.5591011047363281
translation,423,77,hyperparameters,local classifiers,are,support vector machines ( svm ),local classifiers are support vector machines ( svm ),0.5637662410736084
translation,423,77,hyperparameters,local classifiers,are,logistic regression,local classifiers are logistic regression,0.581341564655304
translation,423,77,hyperparameters,local classifiers,are,logistic ordinal regression,local classifiers are logistic ordinal regression,0.5611617565155029
translation,423,77,hyperparameters,local classifiers,with,logistic ordinal regression,local classifiers with logistic ordinal regression,0.6002687215805054
translation,423,77,hyperparameters,support vector machines ( svm ),with,"c = 1 ( joachims , 1999 )","support vector machines ( svm ) with c = 1 ( joachims , 1999 )",0.5997300148010254
translation,423,77,hyperparameters,support vector machines ( svm ),with,logistic ordinal regression,support vector machines ( svm ) with logistic ordinal regression,0.5986717939376831
translation,423,77,hyperparameters,logistic regression,with,gaussian prior,logistic regression with gaussian prior,0.6267935633659363
translation,423,77,hyperparameters,gaussian prior,with,variance 10,gaussian prior with variance 10,0.6975654363632202
translation,423,77,hyperparameters,logistic ordinal regression,has,"mccullagh , 1980 )","logistic ordinal regression has mccullagh , 1980 )",0.5303403735160828
translation,423,77,hyperparameters,hyperparameters,has,local classifiers,hyperparameters has local classifiers,0.5436237454414368
translation,423,78,hyperparameters,long- range sequential dependencies,use,second-order svm hmm,long- range sequential dependencies use second-order svm hmm,0.5794069170951843
translation,423,78,hyperparameters,long- range sequential dependencies,use,second-order linear-chain crf,long- range sequential dependencies use second-order linear-chain crf,0.5943745970726013
translation,423,78,hyperparameters,second-order svm hmm,with,c = 500 and epsilon = 0.01,second-order svm hmm with c = 500 and epsilon = 0.01,0.6349484920501709
translation,423,78,hyperparameters,second-order linear-chain crf,considers,dependencies,second-order linear-chain crf considers dependencies,0.5996987223625183
translation,423,78,hyperparameters,dependencies,between,three neighboring labels,dependencies between three neighboring labels,0.6767730712890625
translation,423,78,hyperparameters,three neighboring labels,in,sequence,three neighboring labels in sequence,0.5565255284309387
translation,423,78,hyperparameters,hyperparameters,to capture,long- range sequential dependencies,hyperparameters to capture long- range sequential dependencies,0.6471187472343445
translation,423,78,hyperparameters,hyperparameters,use,second-order svm hmm,hyperparameters use second-order svm hmm,0.6162133812904358
translation,423,6,model,specific features,looking globally at,thread,specific features looking globally at thread,0.6063048243522644
translation,423,6,model,model,explored,two ways,model explored two ways,0.7046580910682678
translation,423,27,results,linear-chain models,fail to exploit,sequential dependencies,linear-chain models fail to exploit sequential dependencies,0.7664313316345215
translation,423,27,results,sequential dependencies,between,nearby answer labels,sequential dependencies between nearby answer labels,0.6636500358581543
translation,423,27,results,sequential dependencies,to improve,results significantly,sequential dependencies to improve results significantly,0.6532333493232727
translation,423,27,results,results,has,linear-chain models,results has linear-chain models,0.489432692527771
translation,423,96,results,f 1 values,for,baseline features,f 1 values for baseline features,0.5911493897438049
translation,423,96,results,f 1 values,using,labels,f 1 values using labels,0.7132720947265625
translation,423,96,results,baseline features,suggest,labels,baseline features suggest labels,0.6223728656768799
translation,423,96,results,baseline features,using,labels,baseline features using labels,0.6371987462043762
translation,423,96,results,labels,in,thread sequence,labels in thread sequence,0.48642805218696594
translation,423,96,results,labels,yields,better performance,labels yields better performance,0.6990451216697693
translation,423,96,results,better performance,with,svm hmm and crf,better performance with svm hmm and crf,0.6655624508857727
translation,423,96,results,results,has,f 1 values,results has f 1 values,0.5135229825973511
translation,423,97,results,thread - level features,used,models,thread - level features used models,0.6124090552330017
translation,423,97,results,models,using,sequence labels,models using sequence labels,0.6163907647132874
translation,423,97,results,sequence labels,do not outperform,svm and logistic regression,sequence labels do not outperform svm and logistic regression,0.7469745874404907
translation,423,97,results,thread - level features,has,models,thread - level features has models,0.5405002236366272
translation,423,97,results,results,When,thread - level features,results When thread - level features,0.58827143907547
translation,423,98,results,posterior marginals maximization,is,slightly better,posterior marginals maximization is slightly better,0.5167282819747925
translation,423,98,results,maximizing,on,each comment,maximizing on each comment,0.5847639441490173
translation,423,98,results,each comment,pays more than,entire thread,each comment pays more than entire thread,0.6928720474243164
translation,423,98,results,two variations of crf,has,posterior marginals maximization,two variations of crf has posterior marginals maximization,0.5056834816932678
translation,423,98,results,results,Regarding,two variations of crf,results Regarding two variations of crf,0.5648341178894043
translation,423,105,results,performance,of,different models,performance of different models,0.6085451245307922
translation,423,105,results,no statistically significant differences,observed,"f 1 or f 1 , ta","no statistically significant differences observed f 1 or f 1 , ta",0.6019248962402344
translation,423,105,results,no statistically significant differences,on,"f 1 or f 1 , ta","no statistically significant differences on f 1 or f 1 , ta",0.5974844694137573
translation,423,105,results,performance,has,no statistically significant differences,performance has no statistically significant differences,0.5671237707138062
translation,423,105,results,different models,has,no statistically significant differences,different models has no statistically significant differences,0.5189931988716125
translation,423,105,results,results,looking at,performance,results looking at performance,0.6216241121292114
translation,423,107,results,performance,of,all the models,performance of all the models,0.5676261186599731
translation,423,107,results,improves,by,approximately two f 1 points absolute,improves by approximately two f 1 points absolute,0.6784303784370422
translation,423,107,results,thread - level features,has,performance,thread - level features has performance,0.5398769378662109
translation,423,107,results,all the models,has,improves,all the models has improves,0.6271371245384216
translation,424,185,ablation-analysis,all the feature sets,improved,performance,all the feature sets improved performance,0.718477725982666
translation,424,185,ablation-analysis,all the feature sets,got,best performance,all the feature sets got best performance,0.598983883857727
translation,424,185,ablation-analysis,ablation analysis,confirmed,all the feature sets,ablation analysis confirmed all the feature sets,0.6869879961013794
translation,424,185,ablation-analysis,ablation analysis,got,best performance,ablation analysis got best performance,0.6394666433334351
translation,424,213,ablation-analysis,our causal relation features,effective in improving,quality,our causal relation features effective in improving quality,0.7360104918479919
translation,424,213,ablation-analysis,quality,of,highly confident answers,quality of highly confident answers,0.5514774322509766
translation,424,221,ablation-analysis,intra-and inter-sentential causal relations,contributed to,performance improvement,intra-and inter-sentential causal relations contributed to performance improvement,0.4902018904685974
translation,424,221,ablation-analysis,ablation analysis,has,intra-and inter-sentential causal relations,ablation analysis has intra-and inter-sentential causal relations,0.5629118084907532
translation,424,222,ablation-analysis,three types of causal relation features,by,ablation tests,three types of causal relation features by ablation tests,0.5725722908973694
translation,424,222,ablation-analysis,ablation analysis,contributions of,three types of causal relation features,ablation analysis contributions of three types of causal relation features,0.6751686930656433
translation,424,193,baselines,oh + prevcf,is,system,oh + prevcf is system,0.6465066075325012
translation,424,193,baselines,system,with,re-ranker,system with re-ranker,0.6821504831314087
translation,424,193,baselines,re-ranker,trained with,features,re-ranker trained with features,0.7537524104118347
translation,424,193,baselines,re-ranker,trained with,causal relation feature,re-ranker trained with causal relation feature,0.7162768840789795
translation,424,193,baselines,re-ranker,with,causal relation feature,re-ranker with causal relation feature,0.6249597072601318
translation,424,193,baselines,features,used in,oh,features used in oh,0.7632516622543335
translation,424,193,baselines,baselines,has,oh + prevcf,baselines has oh + prevcf,0.609592616558075
translation,424,198,experimental-setup,tinysvm,with,linear kernel,tinysvm with linear kernel,0.5977038145065308
translation,424,198,experimental-setup,linear kernel,for training,re-rankers,linear kernel for training re-rankers,0.7076947689056396
translation,424,198,experimental-setup,re-rankers,in,ourcf,re-rankers in ourcf,0.6173615455627441
translation,424,198,experimental-setup,re-rankers,in,oh + prevcf,re-rankers in oh + prevcf,0.588101327419281
translation,424,198,experimental-setup,experimental setup,used,tinysvm,experimental setup used tinysvm,0.5740081071853638
translation,424,37,results,precision,by,4.4 %,precision by 4.4 %,0.5698404312133789
translation,424,37,results,4.4 %,against,all the questions,4.4 % against all the questions,0.6047215461730957
translation,424,37,results,4.4 %,over,current state - of - the - art system,4.4 % over current state - of - the - art system,0.6321533918380737
translation,424,37,results,all the questions,in,our test set,all the questions in our test set,0.4963839650154114
translation,424,37,results,current state - of - the - art system,of,japanese why - qa,current state - of - the - art system of japanese why - qa,0.5463522672653198
translation,424,181,results,our method,recognized,intra-and inter-sentential causal relations,our method recognized intra-and inter-sentential causal relations,0.6345987319946289
translation,424,181,results,intra-and inter-sentential causal relations,with,over 80 % precision,intra-and inter-sentential causal relations with over 80 % precision,0.6574804186820984
translation,424,181,results,significantly outperformed,in,precision and recall rates,significantly outperformed in precision and recall rates,0.5607129335403442
translation,424,181,results,our baseline system,in,precision and recall rates,our baseline system in precision and recall rates,0.48376521468162537
translation,424,181,results,our baseline system,both,precision and recall rates,our baseline system both precision and recall rates,0.6213077902793884
translation,424,181,results,significantly outperformed,has,our baseline system,significantly outperformed has our baseline system,0.6115074157714844
translation,424,181,results,results,confirmed,our method,results confirmed our method,0.6532680988311768
translation,424,204,results,proposed method,improved,p@1,proposed method improved p@1,0.7201053500175476
translation,424,204,results,p@1,by,4.4 %,p@1 by 4.4 %,0.6249020099639893
translation,424,204,results,4.4 %,over,oh,4.4 % over oh,0.6577616333961487
translation,424,204,results,proposed method,has,outperformed,proposed method has outperformed,0.630740761756897
translation,424,204,results,outperformed,has,other four systems,outperformed has other four systems,0.6203445196151733
translation,424,204,results,results,has,proposed method,results has proposed method,0.5845219492912292
translation,424,205,results,ourcf,showed,performance improvement,ourcf showed performance improvement,0.7032364010810852
translation,424,205,results,performance improvement,over,murata,performance improvement over murata,0.6761776208877563
translation,424,205,results,results,has,ourcf,results has ourcf,0.5987651944160461
translation,424,207,results,oh + prevcf,has,outperformed,oh + prevcf has outperformed,0.6748587489128113
translation,424,207,results,results,has,oh + prevcf,results has oh + prevcf,0.5764641761779785
translation,424,212,results,both systems,provided,top-answers,both systems provided top-answers,0.7136936187744141
translation,424,212,results,top-answers,for,25 %,top-answers for 25 %,0.5972114205360413
translation,424,212,results,25 %,of,all the questions,25 % of all the questions,0.5767748951911926
translation,424,212,results,our method,achieved,83.2 % precision,our method achieved 83.2 % precision,0.6528100371360779
translation,424,212,results,83.2 % precision,much higher than,oh 's,83.2 % precision much higher than oh 's,0.6495834589004517
translation,424,223,results,features,by,excitation polarity matching ( all -{ ef 1ef 4 } ),features by excitation polarity matching ( all -{ ef 1ef 4 } ),0.534579336643219
translation,424,223,results,performance,is,worst,performance is worst,0.6286083459854126
translation,424,223,results,features,has,performance,features has performance,0.5997235178947449
translation,424,223,results,excitation polarity matching ( all -{ ef 1ef 4 } ),has,performance,excitation polarity matching ( all -{ ef 1ef 4 } ) has performance,0.5707763433456421
translation,424,223,results,results,not use,features,results not use features,0.6314134001731873
translation,425,118,ablation-analysis,drop a lot,on,all the metrics,drop a lot on all the metrics,0.5303431153297424
translation,425,118,ablation-analysis,information,in,input text,information in input text,0.49761825799942017
translation,425,118,ablation-analysis,results,has,drop a lot,results has drop a lot,0.5634781122207642
translation,425,118,ablation-analysis,ablation analysis,has,results,ablation analysis has results,0.4875600337982178
translation,425,123,ablation-analysis,encoding representations,results are,greatly improved,encoding representations results are greatly improved,0.6689123511314392
translation,425,123,ablation-analysis,ablation analysis,add,decision maker,ablation analysis add decision maker,0.5831910371780396
translation,425,124,ablation-analysis,model,with,reinforcement learning framework,model with reinforcement learning framework,0.5566576719284058
translation,425,124,ablation-analysis,answer quality,as,reward,answer quality as reward,0.4981958568096161
translation,425,124,ablation-analysis,ablation analysis,fine - tune,model,ablation analysis fine - tune model,0.6870270371437073
translation,425,100,hyperparameters,word embeddings,initialized by,glove,word embeddings initialized by glove,0.6727533936500549
translation,425,100,hyperparameters,hyperparameters,has,word embeddings,hyperparameters has word embeddings,0.4784160554409027
translation,425,102,hyperparameters,lstm hidden unit size,to,500,lstm hidden unit size to 500,0.5608512163162231
translation,425,102,hyperparameters,number of layers,of,lstms,number of layers of lstms,0.584155797958374
translation,425,102,hyperparameters,lstms,to,2,lstms to 2,0.5824994444847107
translation,425,102,hyperparameters,2,in,both the encoder and the decoder,2 in both the encoder and the decoder,0.523154079914093
translation,425,102,hyperparameters,hyperparameters,set,lstm hidden unit size,hyperparameters set lstm hidden unit size,0.5915696620941162
translation,425,102,hyperparameters,hyperparameters,set,number of layers,hyperparameters set number of layers,0.6222911477088928
translation,425,102,hyperparameters,hyperparameters,set,number of layers,hyperparameters set number of layers,0.6222911477088928
translation,425,103,hyperparameters,optimization,performed using,stochastic gradient descent ( sgd ),optimization performed using stochastic gradient descent ( sgd ),0.5894094705581665
translation,425,103,hyperparameters,stochastic gradient descent ( sgd ),with,initial learning rate,stochastic gradient descent ( sgd ) with initial learning rate,0.6076837182044983
translation,425,103,hyperparameters,initial learning rate,of,1.0,initial learning rate of 1.0,0.5808937549591064
translation,425,103,hyperparameters,hyperparameters,has,optimization,hyperparameters has optimization,0.5086923241615295
translation,425,104,hyperparameters,learning rate,starts decaying at,step 15000,learning rate starts decaying at step 15000,0.7172027826309204
translation,425,104,hyperparameters,step 15000,with,decay rate,step 15000 with decay rate,0.6723268628120422
translation,425,104,hyperparameters,decay rate,of,0.95,decay rate of 0.95,0.590995192527771
translation,425,104,hyperparameters,0.95,for,every 5000 steps,0.95 for every 5000 steps,0.6232880353927612
translation,425,104,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,425,105,hyperparameters,mini-batch size,for,update,mini-batch size for update,0.609596848487854
translation,425,105,hyperparameters,update,set at,64,update set at 64,0.6714870929718018
translation,425,105,hyperparameters,hyperparameters,has,mini-batch size,hyperparameters has mini-batch size,0.4961773455142975
translation,425,106,hyperparameters,"dropout ( srivastava et al. , 2014 ) ratio",as,0.3,"dropout ( srivastava et al. , 2014 ) ratio as 0.3",0.5280733704566956
translation,425,106,hyperparameters,beam size,as,5,beam size as 5,0.594243049621582
translation,425,106,hyperparameters,hyperparameters,set,"dropout ( srivastava et al. , 2014 ) ratio","hyperparameters set dropout ( srivastava et al. , 2014 ) ratio",0.5773057341575623
translation,425,106,hyperparameters,hyperparameters,set,beam size,hyperparameters set beam size,0.6436296701431274
translation,425,107,hyperparameters,maximum number of iterations,for,dynamic reasoning,maximum number of iterations for dynamic reasoning,0.6017941832542419
translation,425,107,hyperparameters,maximum number of iterations,set to,3,maximum number of iterations set to 3,0.717141330242157
translation,425,107,hyperparameters,dynamic reasoning,set to,3,dynamic reasoning set to 3,0.6289841532707214
translation,425,107,hyperparameters,hyperparameters,has,maximum number of iterations,hyperparameters has maximum number of iterations,0.503545343875885
translation,425,6,model,approach,named,reinforced dynamic reasoning ( redr ) network,approach named reinforced dynamic reasoning ( redr ) network,0.6921099424362183
translation,425,6,model,reasoning procedure,in,dynamic manner,reasoning procedure in dynamic manner,0.5360336899757385
translation,425,6,model,asked and what to ask next,about,passage,asked and what to ask next about passage,0.6394742727279663
translation,425,6,model,model,propose,approach,model propose approach,0.6953083872795105
translation,425,7,model,meaningful questions,leverage,popular question answering ( qa ) model,meaningful questions leverage popular question answering ( qa ) model,0.7105713486671448
translation,425,7,model,popular question answering ( qa ) model,to provide,feedback,popular question answering ( qa ) model to provide feedback,0.6432332992553711
translation,425,7,model,popular question answering ( qa ) model,fine - tune,question generator,popular question answering ( qa ) model fine - tune question generator,0.6764619946479797
translation,425,7,model,question generator,using,reinforcement learning mechanism,question generator using reinforcement learning mechanism,0.6347089409828186
translation,425,31,model,novel framework,named,reinforced dynamic reasoning ( redr ) network,novel framework named reinforced dynamic reasoning ( redr ) network,0.6628485321998596
translation,425,31,model,model,present,novel framework,model present novel framework,0.7050476670265198
translation,425,32,model,encoding representation,based on,soft decision maker,encoding representation based on soft decision maker,0.64317786693573
translation,425,32,model,soft decision maker,to generate,coherent question,soft decision maker to generate coherent question,0.6566205620765686
translation,425,34,model,popular and effective reading comprehension ( or qa ) model,to predict,answer,popular and effective reading comprehension ( or qa ) model to predict answer,0.7356975078582764
translation,425,34,model,popular and effective reading comprehension ( or qa ) model,use,answer quality,popular and effective reading comprehension ( or qa ) model use answer quality,0.6159549951553345
translation,425,34,model,answer quality,as,rewards,answer quality as rewards,0.49640005826950073
translation,425,34,model,rewards,to fine -tune,model,rewards to fine -tune model,0.7167564034461975
translation,425,34,model,model,based on,reinforcement learning mechanism,model based on reinforcement learning mechanism,0.5917229056358337
translation,425,34,model,model,leverage,popular and effective reading comprehension ( or qa ) model,model leverage popular and effective reading comprehension ( or qa ) model,0.7185929417610168
translation,425,115,results,results,on,coqa dataset,results on coqa dataset,0.5177827477455139
translation,425,116,results,our model redr and its variants,perform,much better,our model redr and its variants perform much better,0.5611681938171387
translation,425,116,results,much better,than,baselines,much better than baselines,0.6178333163261414
translation,425,116,results,results,has,our model redr and its variants,results has our model redr and its variants,0.5413504242897034
translation,425,133,results,nqg,in,all aspects,nqg in all aspects,0.5334773063659668
translation,425,133,results,our method,has,almost outperforms,our method has almost outperforms,0.6179611682891846
translation,425,133,results,almost outperforms,has,nqg,almost outperforms has nqg,0.6153988242149353
translation,425,133,results,results,see that,our method,results see that our method,0.631170928478241
translation,425,134,results,three methods,obtain,similar scores,three methods obtain similar scores,0.5509971380233765
translation,425,134,results,naturalness,has,three methods,naturalness has three methods,0.5629622340202332
translation,425,134,results,results,For,naturalness,results For naturalness,0.5894036889076233
translation,425,135,results,"relevance , coherence and answerability aspects",there is,obvious gap,"relevance , coherence and answerability aspects there is obvious gap",0.6399625539779663
translation,425,135,results,obvious gap,between,generative models,obvious gap between generative models,0.7013787031173706
translation,425,135,results,obvious gap,between,human annotation,obvious gap between human annotation,0.6586474180221558
translation,425,135,results,results,observe,"relevance , coherence and answerability aspects","results observe relevance , coherence and answerability aspects",0.5528356432914734
translation,425,135,results,results,on,"relevance , coherence and answerability aspects","results on relevance , coherence and answerability aspects",0.5325517654418945
translation,425,141,results,redr and nqg,generate,fewer yes / no questions,redr and nqg generate fewer yes / no questions,0.6722615957260132
translation,425,141,results,fewer yes / no questions,than,humans,fewer yes / no questions than humans,0.6143384575843811
translation,425,141,results,results,has,redr and nqg,results has redr and nqg,0.5475704669952393
translation,426,159,baselines,svm,employs,support vector machine,svm employs support vector machine,0.5120379328727722
translation,426,159,baselines,support vector machine,along with,word embedding features,support vector machine along with word embedding features,0.6080228686332703
translation,426,159,baselines,baselines,has,svm,baselines has svm,0.5812874436378479
translation,426,161,baselines,lstm,concatenates,question and answer text,lstm concatenates question and answer text,0.7289787530899048
translation,426,161,baselines,baselines,has,lstm,baselines has lstm,0.5395978093147278
translation,426,162,baselines,bidirectional lstm model,concatenates,question and answer text,bidirectional lstm model concatenates question and answer text,0.699066698551178
translation,426,162,baselines,question and answer text,as,sequence,question and answer text as sequence,0.57377028465271
translation,426,162,baselines,bi-lstm,has,bidirectional lstm model,bi-lstm has bidirectional lstm model,0.577119767665863
translation,426,162,baselines,baselines,has,bi-lstm,baselines has bi-lstm,0.5433008670806885
translation,426,163,baselines,baselines,has,bidirectional - match,baselines has bidirectional - match,0.5554221868515015
translation,426,165,baselines,baselines,has,atoq - match,baselines has atoq - match,0.5970693826675415
translation,426,168,baselines,baselines,has,qtoa - match,baselines has qtoa - match,0.5619460344314575
translation,426,170,baselines,bidirectional - match qa,takes,sentence segmentation strategy,bidirectional - match qa takes sentence segmentation strategy,0.6360998749732971
translation,426,170,baselines,bidirectional - match qa,employs,qa bidirectional matching mechanism,bidirectional - match qa employs qa bidirectional matching mechanism,0.5966125726699829
translation,426,170,baselines,bidirectional - match qa,does not employ,self-matching attention mechanism,bidirectional - match qa does not employ self-matching attention mechanism,0.6913990378379822
translation,426,170,baselines,baselines,has,bidirectional - match qa,baselines has bidirectional - match qa,0.544808566570282
translation,426,171,baselines,hmn,is,hierarchical matching network model,hmn is hierarchical matching network model,0.5210103988647461
translation,426,171,baselines,hmn,employs,qa bidirectional matching mechanism,hmn employs qa bidirectional matching mechanism,0.5762859582901001
translation,426,171,baselines,hmn,employs,self-matching attention mechanism,hmn employs self-matching attention mechanism,0.5303736329078674
translation,426,171,baselines,hierarchical matching network model,takes,sentence segmentation strategy,hierarchical matching network model takes sentence segmentation strategy,0.5786488652229309
translation,426,171,baselines,baselines,has,hmn,baselines has hmn,0.6033270955085754
translation,426,174,baselines,baselines,has,cnn - tensor,baselines has cnn - tensor,0.5562032461166382
translation,426,175,baselines,stateof - the - art approach,to,sentence - level sentiment classification,stateof - the - art approach to sentence - level sentiment classification,0.5012208819389343
translation,426,175,baselines,stateof - the - art approach,models,n-gram interactions,stateof - the - art approach models n-gram interactions,0.709117591381073
translation,426,175,baselines,n-gram interactions,based on,tensor product,n-gram interactions based on tensor product,0.6546207070350647
translation,426,175,baselines,all non- consecutive n-gram vectors,as,feature mapping operator,all non- consecutive n-gram vectors as feature mapping operator,0.5330235958099365
translation,426,175,baselines,feature mapping operator,for,cnns,feature mapping operator for cnns,0.6088581681251526
translation,426,176,baselines,baselines,has,attention-lstm,baselines has attention-lstm,0.5375404953956604
translation,426,179,baselines,bimpm,is,state- ofthe - art approach,bimpm is state- ofthe - art approach,0.6048154830932617
translation,426,179,baselines,state- ofthe - art approach,to,qa matching,state- ofthe - art approach to qa matching,0.5567656755447388
translation,426,179,baselines,baselines,has,bimpm,baselines has bimpm,0.6019688844680786
translation,426,181,baselines,hmn,takes,sentence segmentation strategy,hmn takes sentence segmentation strategy,0.605144739151001
translation,426,181,baselines,proposed hierarchical matching network,employs,qa bidirectional matching mechanism,proposed hierarchical matching network employs qa bidirectional matching mechanism,0.5394055843353271
translation,426,181,baselines,proposed hierarchical matching network,employs,self-matching attention mechanism,proposed hierarchical matching network employs self-matching attention mechanism,0.5110170841217041
translation,426,181,baselines,hmn,has,proposed hierarchical matching network,hmn has proposed hierarchical matching network,0.5760859251022339
translation,426,181,baselines,baselines,has,hmn,baselines has hmn,0.6033270955085754
translation,426,145,experimental-setup,word segmentation and embeddings,segment,text,word segmentation and embeddings segment text,0.701310932636261
translation,426,145,experimental-setup,fu-dannlp,segment,text,fu-dannlp segment text,0.7918086051940918
translation,426,145,experimental-setup,text,into,chinese words,text into chinese words,0.5140244960784912
translation,426,145,experimental-setup,word2vec,pre-train,word embeddings,word2vec pre-train word embeddings,0.6771432757377625
translation,426,145,experimental-setup,word segmentation and embeddings,has,fu-dannlp,word segmentation and embeddings has fu-dannlp,0.6336408257484436
translation,426,145,experimental-setup,experimental setup,has,word segmentation and embeddings,experimental setup has word segmentation and embeddings,0.5281100273132324
translation,426,146,experimental-setup,vector dimensionality,set to,100,vector dimensionality set to 100,0.7174469828605652
translation,426,146,experimental-setup,experimental setup,has,vector dimensionality,experimental setup has vector dimensionality,0.5000405311584473
translation,426,147,experimental-setup,corenlp 5,segment,question and answer text,corenlp 5 segment question and answer text,0.6871522665023804
translation,426,147,experimental-setup,question and answer text,into,sentences,question and answer text into sentences,0.5681819319725037
translation,426,147,experimental-setup,sentence segmentation,has,corenlp 5,sentence segmentation has corenlp 5,0.6250482797622681
translation,426,147,experimental-setup,experimental setup,has,sentence segmentation,experimental setup has sentence segmentation,0.5273749232292175
translation,426,150,experimental-setup,weight matrices,given their,initial values,weight matrices given their initial values,0.686069905757904
translation,426,150,experimental-setup,initial values,by,sampling,initial values by sampling,0.593473494052887
translation,426,150,experimental-setup,sampling,from,"uniform distribution u ( ?0.01 , 0.01 )","sampling from uniform distribution u ( ?0.01 , 0.01 )",0.5771490931510925
translation,426,150,experimental-setup,experimental setup,has,weight matrices,experimental setup has weight matrices,0.4944308400154114
translation,426,151,experimental-setup,lstm hidden states,set to,128,lstm hidden states set to 128,0.6159071326255798
translation,426,151,experimental-setup,mini-batch,of,32 instances,mini-batch of 32 instances,0.5530344843864441
translation,426,151,experimental-setup,experimental setup,has,lstm hidden states,experimental setup has lstm hidden states,0.5147871971130371
translation,426,152,experimental-setup,dropout rate,set to,0.2,dropout rate set to 0.2,0.6595621705055237
translation,426,152,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,426,180,experimental-setup,matching representation,to perform,qa - style sentiment classification,matching representation to perform qa - style sentiment classification,0.6515601277351379
translation,426,180,experimental-setup,qa - style sentiment classification,with,softmax classifier,qa - style sentiment classification with softmax classifier,0.6030079126358032
translation,426,180,experimental-setup,experimental setup,use,matching representation,experimental setup use matching representation,0.5731217265129089
translation,426,7,model,three - stage hierarchical matching network,to explore,deep sentiment information,three - stage hierarchical matching network to explore deep sentiment information,0.5815326571464539
translation,426,7,model,deep sentiment information,in,qa text pair,deep sentiment information in qa text pair,0.5043906569480896
translation,426,7,model,model,propose,three - stage hierarchical matching network,model propose three - stage hierarchical matching network,0.6383664011955261
translation,426,8,model,question and answer text,into,sentences,question and answer text into sentences,0.5681819319725037
translation,426,8,model,"number of [ q-sentence , asentence ] units",in,each qa text pair,"number of [ q-sentence , asentence ] units in each qa text pair",0.49734482169151306
translation,426,8,model,model,segment,question and answer text,model segment question and answer text,0.7712838053703308
translation,426,46,model,model,propose,hierarchical matching network model,model propose hierarchical matching network model,0.5897684693336487
translation,426,47,model,question and answer text,into,sentences,question and answer text into sentences,0.5681819319725037
translation,426,47,model,"[ q- sentence , a-sentence ] units",for,each qa text pair,"[ q- sentence , a-sentence ] units for each qa text pair",0.595109760761261
translation,426,47,model,model,segment,question and answer text,model segment question and answer text,0.7712838053703308
translation,426,47,model,model,construct,"[ q- sentence , a-sentence ] units","model construct [ q- sentence , a-sentence ] units",0.681026041507721
translation,426,48,model,qa bidirectional matching layer,encode,"each [ q-sentence , a-sentence ] unit","qa bidirectional matching layer encode each [ q-sentence , a-sentence ] unit",0.7391537427902222
translation,426,48,model,"each [ q-sentence , a-sentence ] unit",for exploring,sentiment information,"each [ q-sentence , a-sentence ] unit for exploring sentiment information",0.7138105630874634
translation,426,48,model,model,by using,qa bidirectional matching layer,model by using qa bidirectional matching layer,0.6507964730262756
translation,426,160,model,question and answer text,in,qa text pair,question and answer text in qa text pair,0.5455551147460938
translation,426,160,model,chained,as,sequence,chained as sequence,0.5884300470352173
translation,426,160,model,model,has,question and answer text,model has question and answer text,0.5702863931655884
translation,426,183,results,matching strategy,i.e.,bimpm,matching strategy i.e. bimpm,0.6926105618476868
translation,426,183,results,matching strategy,i.e.,our approach ( hmn ),matching strategy i.e. our approach ( hmn ),0.6795113682746887
translation,426,183,results,matching strategy,i.e.,outperform,matching strategy i.e. outperform,0.7102601528167725
translation,426,183,results,outperform,has,other approaches,outperform has other approaches,0.6127994656562805
translation,426,183,results,results,approaches that take,matching strategy,results approaches that take matching strategy,0.5857309699058533
translation,426,184,results,significantly outperforms,in terms of,macro - f1 and accuracy,significantly outperforms in terms of macro - f1 and accuracy,0.7325332760810852
translation,426,184,results,all the other baseline approaches,in terms of,macro - f1 and accuracy,all the other baseline approaches in terms of macro - f1 and accuracy,0.6790369153022766
translation,426,184,results,proposed approach ( hmn ),has,significantly outperforms,proposed approach ( hmn ) has significantly outperforms,0.597334623336792
translation,426,184,results,significantly outperforms,has,all the other baseline approaches,significantly outperforms has all the other baseline approaches,0.5698859095573425
translation,426,184,results,results,has,proposed approach ( hmn ),results has proposed approach ( hmn ),0.5773079991340637
translation,426,194,results,approaches,based on,matching strategy ( bimpm and hmn ),approaches based on matching strategy ( bimpm and hmn ),0.7096848487854004
translation,426,194,results,matching strategy ( bimpm and hmn ),are,well - performed,matching strategy ( bimpm and hmn ) are well - performed,0.5239318609237671
translation,426,194,results,well - performed,when,question and answer,well - performed when question and answer,0.6698684692382812
translation,426,194,results,question and answer,carrying,different,question and answer carrying different,0.7408034801483154
translation,426,194,results,results,find that,approaches,results find that approaches,0.6378222703933716
translation,426,196,results,performs better,than,other approaches,performs better than other approaches,0.5879167914390564
translation,426,196,results,other approaches,when dealing with,conflict instances,other approaches when dealing with conflict instances,0.7248693704605103
translation,426,196,results,proposed approach ( hmn ),has,performs better,proposed approach ( hmn ) has performs better,0.6033318042755127
translation,426,196,results,results,has,proposed approach ( hmn ),results has proposed approach ( hmn ),0.5773079991340637
translation,427,12,model,suggestions,to help,users,suggestions to help users,0.6791714429855347
translation,427,12,model,users,to complete,questions,users to complete questions,0.667422354221344
translation,427,13,model,first order logic ( fol ) representation,using,grammar,first order logic ( fol ) representation using grammar,0.6569182276725769
translation,427,13,model,grammar,derived from,interlinked datasets,grammar derived from interlinked datasets,0.687595784664154
translation,427,13,model,executable queries,including,sql,executable queries including sql,0.7190933227539062
translation,427,91,model,natural language questions,to,intermediate logical representation,natural language questions to intermediate logical representation,0.4813150465488434
translation,427,91,model,intermediate logical representation,based on,grammar,intermediate logical representation based on grammar,0.6389549374580383
translation,427,91,model,grammar,derived from,multiple interlinked datasets,grammar derived from multiple interlinked datasets,0.7040227651596069
translation,427,79,results,entities,from,our kb,entities from our kb,0.5747727155685425
translation,427,79,results,entities,to,entity mentions,entities to entity mentions,0.5272967219352722
translation,427,79,results,entities,perform,additional analytics,entities perform additional analytics,0.5801581740379333
translation,427,79,results,entity mentions,in,large news corpus,entity mentions in large news corpus,0.4489916265010834
translation,427,79,results,additional analytics,based on,named entity recognition,additional analytics based on named entity recognition,0.6476972103118896
translation,427,79,results,additional analytics,based on,sentiment analysis techniques,additional analytics based on sentiment analysis techniques,0.6539735794067383
translation,427,79,results,results,linking,entities,results linking entities,0.6809852123260498
translation,428,35,ablation-analysis,smart instantiation selection,to,quality of generated questions,smart instantiation selection to quality of generated questions,0.5522571206092834
translation,428,35,ablation-analysis,ablation analysis,point at,importance,ablation analysis point at importance,0.6564551591873169
translation,428,188,baselines,first random baseline,chooses,relation,first random baseline chooses relation,0.772468090057373
translation,428,188,baselines,first random baseline,instantiates it with,random pair of entities,first random baseline instantiates it with random pair of entities,0.7346115708351135
translation,428,188,baselines,relation,out of,all possible relations,relation out of all possible relations,0.6020585298538208
translation,428,188,baselines,relation,instantiates it with,random pair of entities,relation instantiates it with random pair of entities,0.7454235553741455
translation,428,188,baselines,randomly,out of,all possible relations,randomly out of all possible relations,0.6129215955734253
translation,428,188,baselines,all possible relations,in,database,all possible relations in database,0.5280001759529114
translation,428,188,baselines,relation,has,randomly,relation has randomly,0.6581482291221619
translation,428,188,baselines,baselines,has,first random baseline,baselines has first random baseline,0.5962615013122559
translation,428,157,experimental-setup,support vector machines ( svm ) implementation,of,"libsvm ( chang and lin , 2011 )","support vector machines ( svm ) implementation of libsvm ( chang and lin , 2011 )",0.5279536843299866
translation,428,157,experimental-setup,support vector machines ( svm ) implementation,with,linear kernel,support vector machines ( svm ) implementation with linear kernel,0.6097984910011292
translation,428,157,experimental-setup,linear kernel,as,classifier,linear kernel as classifier,0.5902982354164124
translation,428,157,experimental-setup,experimental setup,utilize,support vector machines ( svm ) implementation,experimental setup utilize support vector machines ( svm ) implementation,0.5821254849433899
translation,428,61,model,per template,for,relevance matching,per template for relevance matching,0.6370695233345032
translation,428,61,model,single entity model per template slot,that identify,valid instantiations,single entity model per template slot that identify valid instantiations,0.6333351731300354
translation,428,61,model,entity pair model,detects,pairs,entity pair model detects pairs,0.7010002732276917
translation,428,61,model,compared together,under,template,compared together under template,0.6938825249671936
translation,428,61,model,context profile,has,per template,context profile has per template,0.5755593180656433
translation,428,61,model,pairs,has,of entities,pairs has of entities,0.6015185713768005
translation,428,257,model,algorithm,of,two parts,algorithm of two parts,0.6222735047340393
translation,428,258,model,offline part,identifies,comparable relations,offline part identifies comparable relations,0.6563006639480591
translation,428,258,model,comparable relations,in,large collection of questions,comparable relations in large collection of questions,0.5223869681358337
translation,428,258,model,model,has,offline part,model has offline part,0.585130512714386
translation,428,34,results,full algorithm,provided,45 % more correct instantiations,full algorithm provided 45 % more correct instantiations,0.6147750020027161
translation,428,34,results,full algorithm,provided,46 % more relevant suggestions,full algorithm provided 46 % more relevant suggestions,0.6546278595924377
translation,428,34,results,46 % more relevant suggestions,compared to,stronger baseline,46 % more relevant suggestions compared to stronger baseline,0.6518301963806152
translation,428,34,results,results,show,full algorithm,results show full algorithm,0.6971162557601929
translation,428,201,results,result,is that,our full algorithm,result is that our full algorithm,0.6421555876731873
translation,428,201,results,our full algorithm,has,substantially outperforms,our full algorithm has substantially outperforms,0.6106487512588501
translation,428,201,results,substantially outperforms,has,stronger relevance baseline,substantially outperforms has stronger relevance baseline,0.5990362167358398
translation,428,202,results,correctness score,by,45 %,correctness score by 45 %,0.5665870308876038
translation,428,203,results,performance,is,just under 80 %,performance is just under 80 %,0.5837794542312622
translation,428,203,results,just under 80 %,showing,high quality entity pair selection,just under 80 % showing high quality entity pair selection,0.7079427242279053
translation,428,203,results,high quality entity pair selection,for,relations,high quality entity pair selection for relations,0.5711424946784973
translation,428,212,results,relevance baseline,beats,random baseline,relevance baseline beats random baseline,0.7007258534431458
translation,428,212,results,random baseline,by,28 %,random baseline by 28 %,0.6058984398841858
translation,428,212,results,random baseline,in terms of,relevance,random baseline in terms of relevance,0.69973224401474
translation,428,212,results,28 %,in terms of,relevance,28 % in terms of relevance,0.6826149821281433
translation,428,212,results,baselines,has,relevance baseline,baselines has relevance baseline,0.6011893153190613
translation,428,212,results,results,Comparing between,baselines,results Comparing between baselines,0.7020766139030457
translation,428,214,results,correctness,by,23 %,correctness by 23 %,0.5828630924224854
translation,428,214,results,23 %,over,random baseline,23 % over random baseline,0.6691929697990417
translation,428,214,results,results,improved,correctness,results improved correctness,0.6472265720367432
translation,428,264,results,full algorithm,by,46 %,full algorithm by 46 %,0.5675259828567505
translation,428,264,results,outperformed,by,45 %,outperformed by 45 %,0.6247742176055908
translation,428,264,results,outperformed,by,46 %,outperformed by 46 %,0.6212179660797119
translation,428,264,results,baseline,by,45 %,baseline by 45 %,0.6001457571983337
translation,428,264,results,45 %,on,question correctness,45 % on question correctness,0.5021626353263855
translation,428,264,results,46 %,on,question relevance,46 % on question relevance,0.5085328221321106
translation,428,264,results,full algorithm,has,outperformed,full algorithm has outperformed,0.6292240023612976
translation,428,264,results,outperformed,has,baseline,outperformed has baseline,0.614183247089386
translation,428,264,results,results,has,full algorithm,results has full algorithm,0.5737771391868591
translation,429,196,ablation-analysis,ablation,on,all evaluated components,ablation on all evaluated components,0.5757443308830261
translation,429,196,ablation-analysis,all evaluated components,in,our approach,all evaluated components in our approach,0.5324336886405945
translation,429,196,ablation-analysis,all evaluated components,led to,performance drop,all evaluated components led to performance drop,0.7045344710350037
translation,429,196,ablation-analysis,ablation analysis,on,all evaluated components,ablation analysis on all evaluated components,0.520658552646637
translation,429,197,ablation-analysis,more than 10 %,on,four components,more than 10 % on four components,0.5792484283447266
translation,429,197,ablation-analysis,four components,including,rsn prior mem,four components including rsn prior mem,0.625911295413971
translation,429,197,ablation-analysis,inferred result,could not be served as,context,inferred result could not be served as context,0.6445310711860657
translation,429,197,ablation-analysis,ablation analysis,has,drop,ablation analysis has drop,0.5666978359222412
translation,429,172,hyperparameters,connected cells length,was,16,connected cells length was 16,0.6275131702423096
translation,429,172,hyperparameters,hyperparameters,maximum size of,connected cells length,hyperparameters maximum size of connected cells length,0.7364364862442017
translation,429,173,hyperparameters,network,optimized via,"adam ( kingma and ba , 2014 )","network optimized via adam ( kingma and ba , 2014 )",0.7614563703536987
translation,429,173,hyperparameters,"adam ( kingma and ba , 2014 )",with,learning rate,"adam ( kingma and ba , 2014 ) with learning rate",0.5989180207252502
translation,429,173,hyperparameters,"adam ( kingma and ba , 2014 )",with,batch size,"adam ( kingma and ba , 2014 ) with batch size",0.6181668639183044
translation,429,173,hyperparameters,learning rate,of,10 ?4,learning rate of 10 ?4,0.6496500372886658
translation,429,173,hyperparameters,batch size,of,64,batch size of 64,0.6741159558296204
translation,429,173,hyperparameters,hyperparameters,has,network,hyperparameters has network,0.5504763126373291
translation,429,174,hyperparameters,gradient clipping,with,clipnorm,gradient clipping with clipnorm,0.6380595564842224
translation,429,174,hyperparameters,gradient clipping,employed,early stopping,gradient clipping employed early stopping,0.6209678053855896
translation,429,174,hyperparameters,clipnorm,of,8,clipnorm of 8,0.6516690254211426
translation,429,174,hyperparameters,early stopping,based on,validation accuracy,early stopping based on validation accuracy,0.6149506568908691
translation,429,174,hyperparameters,hyperparameters,used,gradient clipping,hyperparameters used gradient clipping,0.5766851902008057
translation,429,174,hyperparameters,hyperparameters,employed,early stopping,hyperparameters employed early stopping,0.6491143703460693
translation,429,175,hyperparameters,word embedding,leveraged,300 - dimension pre-trained word vectors,word embedding leveraged 300 - dimension pre-trained word vectors,0.6367061734199524
translation,429,175,hyperparameters,300 - dimension pre-trained word vectors,from,glove,300 - dimension pre-trained word vectors from glove,0.5627835392951965
translation,429,175,hyperparameters,glove,where,word embeddings,glove where word embeddings,0.6348764300346375
translation,429,175,hyperparameters,initialized randomly,using,standard uniform distribution,initialized randomly using standard uniform distribution,0.620658814907074
translation,429,175,hyperparameters,hyperparameters,For,word embedding,hyperparameters For word embedding,0.5246535539627075
translation,429,176,hyperparameters,out - of- vocabulary words,initialized with,zero vectors,out - of- vocabulary words initialized with zero vectors,0.779390275478363
translation,429,176,hyperparameters,hyperparameters,has,out - of- vocabulary words,hyperparameters has out - of- vocabulary words,0.5295700430870056
translation,429,177,hyperparameters,hidden units,in,gru,hidden units in gru,0.5385899543762207
translation,429,177,hyperparameters,hidden units,set to,256,hidden units set to 256,0.6812804341316223
translation,429,177,hyperparameters,gru,set to,256,gru set to 256,0.7427805662155151
translation,429,177,hyperparameters,recurrent weights,initialized by,random orthogonal matrices,recurrent weights initialized by random orthogonal matrices,0.6729325652122498
translation,429,177,hyperparameters,hyperparameters,number of,hidden units,hyperparameters number of hidden units,0.7116135358810425
translation,429,177,hyperparameters,hyperparameters,has,recurrent weights,hyperparameters has recurrent weights,0.5275602340698242
translation,429,178,hyperparameters,other weights,in,gru,other weights in gru,0.5651434659957886
translation,429,178,hyperparameters,other weights,initialized from,uniform distribution,other weights initialized from uniform distribution,0.6776046752929688
translation,429,178,hyperparameters,uniform distribution,between,?0.01 and 0.01,uniform distribution between ?0.01 and 0.01,0.6518419981002808
translation,429,178,hyperparameters,hyperparameters,has,other weights,hyperparameters has other weights,0.5084188580513
translation,429,179,hyperparameters,exponential moving averages,on,model weights,exponential moving averages on model weights,0.5732349753379822
translation,429,179,hyperparameters,exponential moving averages,with,decay rate,exponential moving averages with decay rate,0.6676276922225952
translation,429,179,hyperparameters,exponential moving averages,used them at,test time,exponential moving averages used them at test time,0.6973428130149841
translation,429,179,hyperparameters,decay rate,of,0.999,decay rate of 0.999,0.583235502243042
translation,429,179,hyperparameters,test time,instead of,raw weights,test time instead of raw weights,0.6215865015983582
translation,429,179,hyperparameters,hyperparameters,maintained,exponential moving averages,hyperparameters maintained exponential moving averages,0.6682705879211426
translation,429,180,hyperparameters,variational dropout,of,0.15,variational dropout of 0.15,0.5624344944953918
translation,429,180,hyperparameters,0.15,used across,network,0.15 used across network,0.7157939672470093
translation,429,180,hyperparameters,maximum reasoning step,set to,5,maximum reasoning step set to 5,0.7495354413986206
translation,429,180,hyperparameters,hyperparameters,has,variational dropout,hyperparameters has variational dropout,0.4920363128185272
translation,429,5,model,"given document , question and options",in,context aware way,"given document , question and options in context aware way",0.5042656660079956
translation,429,5,model,model,encode,"given document , question and options","model encode given document , question and options",0.7520254850387573
translation,429,6,model,new network,to solve,inference problem,new network to solve inference problem,0.6736170649528503
translation,429,6,model,new network,by decomposing it into,series of attentionbased reasoning steps,new network by decomposing it into series of attentionbased reasoning steps,0.6344789862632751
translation,429,6,model,model,propose,new network,model propose new network,0.7354010939598083
translation,429,10,model,termination mechanism,to,dynamically determine,termination mechanism to dynamically determine,0.5510232448577881
translation,429,10,model,network,trained by,reinforcement learning,network trained by reinforcement learning,0.7633647918701172
translation,429,10,model,dynamically determine,has,uncertain reasoning depth,dynamically determine has uncertain reasoning depth,0.5280750393867493
translation,429,10,model,dynamically determine,has,network,dynamically determine has network,0.6100950241088867
translation,429,10,model,model,has,termination mechanism,model has termination mechanism,0.5445875525474548
translation,429,43,model,inference problem,proposing,novel network,inference problem proposing novel network,0.6829344630241394
translation,429,43,model,novel network,consists of,set of operational cells,novel network consists of set of operational cells,0.6874653697013855
translation,429,44,model,each cell,designed with,structural prior,each cell designed with structural prior,0.6350525617599487
translation,429,44,model,structural prior,to capture,inner working procedure,structural prior to capture inner working procedure,0.7349151372909546
translation,429,44,model,inner working procedure,of,elementary reasoning step,inner working procedure of elementary reasoning step,0.5336183309555054
translation,429,44,model,model,has,each cell,model has each cell,0.5692753195762634
translation,429,95,model,master unit,analyzes,question details,master unit analyzes question details,0.649593710899353
translation,429,95,model,question details,to focus on,certain aspect,question details to focus on certain aspect,0.7267480492591858
translation,429,95,model,certain aspect,via,self-attention,certain aspect via self-attention,0.6867157816886902
translation,429,95,model,reader unit,extracts,related content,reader unit extracts related content,0.6575322151184082
translation,429,95,model,related content,guided by,question aspect,related content guided by question aspect,0.6727241277694702
translation,429,95,model,related content,guided by,text context,related content guided by text context,0.6375620365142822
translation,429,95,model,writer unit,iteratively integrates,content,writer unit iteratively integrates content,0.7251323461532593
translation,429,95,model,content,with,preceding results,content with preceding results,0.6516941785812378
translation,429,95,model,preceding results,from,memory,preceding results from memory,0.5948252081871033
translation,429,95,model,preceding results,to produce,new intermediate result,preceding results to produce new intermediate result,0.6575158834457397
translation,429,95,model,memory,to produce,new intermediate result,memory to produce new intermediate result,0.7034103274345398
translation,429,95,model,model,has,master unit,model has master unit,0.5872249007225037
translation,429,203,model,long- range dependencies,in,reasoning process,long- range dependencies in reasoning process,0.5158878564834595
translation,429,203,model,long- range dependencies,by,skipping,long- range dependencies by skipping,0.5754061341285706
translation,429,203,model,skipping,improve,performance,skipping improve performance,0.7303107976913452
translation,429,203,model,model,has,udt gate,model has udt gate,0.5928077697753906
translation,429,187,results,approach,for,race data set,approach for race data set,0.6119549870491028
translation,429,187,results,approach,achieved,best performance,approach achieved best performance,0.7826266884803772
translation,429,187,results,individual baselines,on,all three data sets,individual baselines on all three data sets,0.5336440205574036
translation,429,187,results,our approach,achieved,best performance,our approach achieved best performance,0.7143116593360901
translation,429,187,results,second one ( i.e. oft ),in terms of,average accuracy,second one ( i.e. oft ) in terms of average accuracy,0.6789613366127014
translation,429,187,results,average accuracy,by,"4.12 % , 5.00 %","average accuracy by 4.12 % , 5.00 %",0.5527814030647278
translation,429,187,results,average accuracy,over,"4.12 % , 5.00 %","average accuracy over 4.12 % , 5.00 %",0.6423272490501404
translation,429,187,results,"4.12 % , 5.00 %",on,race -m and race -h,"4.12 % , 5.00 % on race -m and race -h",0.5504554510116577
translation,429,187,results,approach,has,outperformed,approach has outperformed,0.6537045240402222
translation,429,187,results,outperformed,has,individual baselines,outperformed has individual baselines,0.6238554120063782
translation,429,187,results,race data set,has,our approach,race data set has our approach,0.5657753944396973
translation,429,187,results,outperformed,has,second one ( i.e. oft ),outperformed has second one ( i.e. oft ),0.6075928807258606
translation,429,187,results,results,for,race data set,results for race data set,0.5940874814987183
translation,429,188,results,outperformance,was,"5.55 % , 7.14 %","outperformance was 5.55 % , 7.14 %",0.5972283482551575
translation,429,188,results,"5.55 % , 7.14 %",over,ph baseline,"5.55 % , 7.14 % over ph baseline",0.6616236567497253
translation,429,188,results,second best,on,mc160multi and mc500 - multi,second best on mc160multi and mc500 - multi,0.564724326133728
translation,429,188,results,mctest data set,has,outperformance,mctest data set has outperformance,0.566489040851593
translation,429,188,results,results,On,mctest data set,results On mctest data set,0.5702913403511047
translation,429,189,results,our approach,led to,performance boost,our approach led to performance boost,0.6049672961235046
translation,429,189,results,performance boost,against,second best one ( i.e. strategies ),performance boost against second best one ( i.e. strategies ),0.6220190525054932
translation,429,189,results,macro- average f1,by,4.06 %,macro- average f1 by 4.06 %,0.5585255026817322
translation,429,189,results,macro- average f1,over,4.06 %,macro- average f1 over 4.06 %,0.6141542792320251
translation,429,189,results,multirc data set,has,our approach,multirc data set has our approach,0.5830157399177551
translation,429,189,results,results,For,multirc data set,results For multirc data set,0.6476024985313416
translation,430,88,ablation-analysis,other individual features,across,semeval - 2016 and semeval - 2017 test datasets,other individual features across semeval - 2016 and semeval - 2017 test datasets,0.6555001735687256
translation,430,88,ablation-analysis,tf - kld and tk features,were,minimally effective,tf - kld and tk features were minimally effective,0.609207808971405
translation,430,90,ablation-analysis,wa features,contributed,most,wa features contributed most,0.6717872619628906
translation,430,90,ablation-analysis,gains,of,0.041 and 0.034,gains of 0.041 and 0.034,0.6254182457923889
translation,430,89,results,dramatic difference,between,two years,dramatic difference between two years,0.7155098915100098
translation,430,89,results,0.022 gain,in,map,0.022 gain in map,0.5226775407791138
translation,430,89,results,2017,produced,0.010 reduction,2017 produced 0.010 reduction,0.6724230051040649
translation,430,89,results,ir system features,has,dramatic difference,ir system features has dramatic difference,0.5913500785827637
translation,430,89,results,two years,has,in 2016,two years has in 2016,0.5809968709945679
translation,430,89,results,results,has,ir system features,results has ir system features,0.5584203600883484
translation,430,96,results,semantic word alignment features,provided,largest contributed and consistent boost,semantic word alignment features provided largest contributed and consistent boost,0.6216285824775696
translation,430,96,results,largest contributed and consistent boost,in,map,largest contributed and consistent boost in map,0.5295982360839844
translation,430,96,results,four feature sources,has,semantic word alignment features,four feature sources has semantic word alignment features,0.5206504464149475
translation,430,97,results,features,derived from,tf - kld and tree kernel methods,features derived from tf - kld and tree kernel methods,0.6383901238441467
translation,430,97,results,results,has,features,results has features,0.550495445728302
translation,431,106,ablation-analysis,five feature templates,resulted in,largest drop,five feature templates resulted in largest drop,0.6707490682601929
translation,431,106,ablation-analysis,largest drop,to,performance,largest drop to performance,0.5450664758682251
translation,431,106,ablation-analysis,largest drop,on,development set,largest drop on development set,0.5452103018760681
translation,431,106,ablation-analysis,performance,on,development set,performance on development set,0.6032232642173767
translation,431,106,ablation-analysis,ablation analysis,present,five feature templates,ablation analysis present five feature templates,0.6791449189186096
translation,431,91,experiments,webqa,obtained,32.6 f 1,webqa obtained 32.6 f 1,0.6235795021057129
translation,431,91,experiments,32.6 f 1,compared to,40.9 f 1,32.6 f 1 compared to 40.9 f 1,0.6349946856498718
translation,431,91,experiments,40.9 f 1,of,compq,40.9 f 1 of compq,0.5944221615791321
translation,431,92,experiments,candidate extraction step,finds,correct answer,candidate extraction step finds correct answer,0.6578971743583679
translation,431,92,experiments,correct answer,in,top-k candidates,correct answer in top-k candidates,0.49440550804138184
translation,431,92,experiments,top-k candidates,in,65.9 %,top-k candidates in 65.9 %,0.4795527160167694
translation,431,92,experiments,top-k candidates,in,62.7 %,top-k candidates in 62.7 %,0.47555017471313477
translation,431,92,experiments,65.9 %,of,development examples,65.9 % of development examples,0.5410708785057068
translation,431,92,experiments,62.7 %,of,test examples,62.7 % of test examples,0.5415210723876953
translation,431,93,experiments,test f 1,on,examples,test f 1 on examples,0.5425910353660583
translation,431,93,experiments,examples,for which,candidate extraction succeeded ( webqa - subset ),examples for which candidate extraction succeeded ( webqa - subset ),0.5796154737472534
translation,431,93,experiments,candidate extraction succeeded ( webqa - subset ),is,"51.9 ( 53.4 p@1 , 67.5 mrr )","candidate extraction succeeded ( webqa - subset ) is 51.9 ( 53.4 p@1 , 67.5 mrr )",0.5211769342422485
translation,431,95,experiments,compq,obtained,42.2 f 1,compq obtained 42.2 f 1,0.6199498176574707
translation,431,95,experiments,42.2 f 1,on,test set,42.2 f 1 on test set,0.5323829650878906
translation,431,95,experiments,42.2 f 1,compared to,40.9 f 1,42.2 f 1 compared to 40.9 f 1,0.6272968053817749
translation,431,95,experiments,40.9 f 1,when training on,com - plexquestions only,40.9 f 1 when training on com - plexquestions only,0.7396504282951355
translation,431,17,model,simple log-linear model,in the spirit of,traditional web- based qa systems,simple log-linear model in the spirit of traditional web- based qa systems,0.693672239780426
translation,431,17,model,simple log-linear model,extracting,answer,simple log-linear model extracting answer,0.6859458684921265
translation,431,17,model,questions,by,querying,questions by querying,0.5895357131958008
translation,431,17,model,answer,from,returned web snippets,answer from returned web snippets,0.5178129076957703
translation,431,17,model,answers,has,questions,answers has questions,0.6036847829818726
translation,431,17,model,querying,has,web,querying has web,0.5872565507888794
translation,431,17,model,model,develop,simple log-linear model,model develop simple log-linear model,0.6098017692565918
translation,431,20,results,simple qa model,performs,reasonably well,simple qa model performs reasonably well,0.5951616168022156
translation,431,20,results,35 f 1,compared to,state - of- the- art,35 f 1 compared to state - of- the- art,0.5719174146652222
translation,431,20,results,results,find that,simple qa model,results find that simple qa model,0.6788318753242493
translation,431,21,results,subset of questions,for which,right answer,subset of questions for which right answer,0.609014630317688
translation,431,21,results,right answer,can be found in,one of the web snippets,right answer can be found in one of the web snippets,0.5362952351570129
translation,431,21,results,one of the web snippets,outperform,semantic parser,one of the web snippets outperform semantic parser,0.6707137823104858
translation,431,21,results,results,for,subset of questions,results for subset of questions,0.6011155247688293
translation,431,22,results,superlatives and relation composition constructions,are,challenging,superlatives and relation composition constructions are challenging,0.5357060432434082
translation,431,22,results,challenging,for,webbased qa system,challenging for webbased qa system,0.6072582006454468
translation,431,22,results,conjunctions and events,are,easier,conjunctions and events are easier,0.6185510754585266
translation,431,22,results,results,find that,superlatives and relation composition constructions,results find that superlatives and relation composition constructions,0.6040624976158142
translation,431,96,results,predictions,to,subset,predictions to subset,0.6160492300987244
translation,431,96,results,subset,for which,candidate extraction,subset for which candidate extraction,0.5850332975387573
translation,431,96,results,f 1,of,compq - subset,f 1 of compq - subset,0.6629093289375305
translation,431,96,results,compq - subset,is,48.5,compq - subset is 48.5,0.5983464121818542
translation,431,96,results,48.5,is,3.4 f 1 points lower,48.5 is 3.4 f 1 points lower,0.5802302360534668
translation,431,96,results,3.4 f 1 points lower,than,webqa - subset,3.4 f 1 points lower than webqa - subset,0.5721619129180908
translation,431,96,results,predictions,has,f 1,predictions has f 1,0.6205563545227051
translation,431,96,results,subset,has,f 1,subset has f 1,0.6843515038490295
translation,431,96,results,candidate extraction,has,succeeded,candidate extraction has succeeded,0.6390401124954224
translation,431,96,results,results,Restricting,predictions,results Restricting predictions,0.6370304822921753
translation,432,79,experimental-setup,our word embeddings,with,skipgram embeddings,our word embeddings with skipgram embeddings,0.5896291136741638
translation,432,79,experimental-setup,skipgram embeddings,of,dimensionality 50,skipgram embeddings of dimensionality 50,0.574880838394165
translation,432,79,experimental-setup,skipgram embeddings,jointly trained on,english wikipedia dump,skipgram embeddings jointly trained on english wikipedia dump,0.721612811088562
translation,432,79,experimental-setup,skipgram embeddings,jointly trained on,jacana corpus,skipgram embeddings jointly trained on jacana corpus,0.7437435984611511
translation,432,79,experimental-setup,experimental setup,pre-initialize,our word embeddings,experimental setup pre-initialize our word embeddings,0.7171958088874817
translation,432,80,experimental-setup,input sentences,encoded with,fixed - sized vectors,input sentences encoded with fixed - sized vectors,0.7196295857429504
translation,432,80,experimental-setup,fixed - sized vectors,using,cnn,fixed - sized vectors using cnn,0.669861912727356
translation,432,80,experimental-setup,window,of size,5,window of size 5,0.7464333772659302
translation,432,80,experimental-setup,window,followed by,global max pooling,window followed by global max pooling,0.606056272983551
translation,432,80,experimental-setup,output,of,100 dimensions,output of 100 dimensions,0.6105902194976807
translation,432,80,experimental-setup,experimental setup,has,input sentences,experimental setup has input sentences,0.5105776190757751
translation,432,81,experimental-setup,single non-linear hidden layer,whose,size,single non-linear hidden layer whose size,0.6276246905326843
translation,432,81,experimental-setup,single non-linear hidden layer,size of,sentence embeddings,single non-linear hidden layer size of sentence embeddings,0.634242057800293
translation,432,81,experimental-setup,experimental setup,use,single non-linear hidden layer,experimental setup use single non-linear hidden layer,0.6046779155731201
translation,432,82,experimental-setup,word overlap embeddings,set to,5 dimensions,word overlap embeddings set to 5 dimensions,0.6161274313926697
translation,432,82,experimental-setup,experimental setup,has,word overlap embeddings,experimental setup has word overlap embeddings,0.4955584704875946
translation,432,83,experimental-setup,activation function,for,convolution and hidden layers,activation function for convolution and hidden layers,0.5840976238250732
translation,432,83,experimental-setup,activation function,both,convolution and hidden layers,activation function both convolution and hidden layers,0.6495709419250488
translation,432,83,experimental-setup,activation function,is,relu,activation function is relu,0.6688944697380066
translation,432,83,experimental-setup,convolution and hidden layers,is,relu,convolution and hidden layers is relu,0.5436795353889465
translation,432,83,experimental-setup,experimental setup,has,activation function,experimental setup has activation function,0.48881152272224426
translation,432,84,experimental-setup,experimental setup,During,training,experimental setup During training,0.6835477948188782
translation,432,85,experimental-setup,sgd,with,adam update rule,sgd with adam update rule,0.6103169918060303
translation,432,85,experimental-setup,sgd,setting,learning rate,sgd setting learning rate,0.4353514611721039
translation,432,85,experimental-setup,learning rate,for,pre-training and fine tuning phases,learning rate for pre-training and fine tuning phases,0.6031410694122314
translation,432,85,experimental-setup,? to 10 ?4 and 10 ?5,for,pre-training and fine tuning phases,? to 10 ?4 and 10 ?5 for pre-training and fine tuning phases,0.6635532379150391
translation,432,85,experimental-setup,experimental setup,used,sgd,experimental setup used sgd,0.6008865237236023
translation,432,5,model,structural representations,in,nns,structural representations in nns,0.5461089015007019
translation,432,5,model,svm model,using,tree kernels ( tks ),svm model using tree kernels ( tks ),0.6896371245384216
translation,432,5,model,tree kernels ( tks ),on,relatively few pairs of questions ( few thousands ),tree kernels ( tks ) on relatively few pairs of questions ( few thousands ),0.5206323266029358
translation,432,5,model,predicting labels,on,large corpus of question pairs,predicting labels on large corpus of question pairs,0.5028135180473328
translation,432,5,model,model,inject,structural representations,model inject structural representations,0.6797489523887634
translation,432,5,model,model,learning,svm model,model learning svm model,0.7231984734535217
translation,432,18,model,syntactic information,in,nns,syntactic information in nns,0.5008661150932312
translation,432,18,model,injecting,has,syntactic information,injecting has syntactic information,0.5377709269523621
translation,432,19,model,tk - based model,on,few thousands training examples,tk - based model on few thousands training examples,0.5181528329849243
translation,432,19,model,tk - based model,apply,classifier,tk - based model apply classifier,0.6292723417282104
translation,432,19,model,classifier,to,much larger set of unlabeled training examples,classifier to much larger set of unlabeled training examples,0.5095086693763733
translation,432,19,model,much larger set of unlabeled training examples,to generate,automatic annotation,much larger set of unlabeled training examples to generate automatic annotation,0.6312049031257629
translation,432,19,model,nns,on,automatic data,nns on automatic data,0.5823754668235779
translation,432,19,model,nns,on,smaller gs data,nns on smaller gs data,0.5843706727027893
translation,432,19,model,nns,on,smaller gs data,nns on smaller gs data,0.5843706727027893
translation,432,19,model,model,train,tk - based model,model train tk - based model,0.6764621734619141
translation,432,19,model,model,pretrain,nns,model pretrain nns,0.7498379945755005
translation,432,20,results,two different datasets,i.e.,quora and qatar living ( ql ),two different datasets i.e. quora and qatar living ( ql ),0.729055643081665
translation,432,20,results,two different datasets,from,semeval,two different datasets from semeval,0.5536109209060669
translation,432,20,results,two different datasets,show,nns,two different datasets show nns,0.6260145306587219
translation,432,20,results,two different datasets,show,accuracy,two different datasets show accuracy,0.6113583445549011
translation,432,20,results,two different datasets,show,nns,two different datasets show nns,0.6260145306587219
translation,432,20,results,two different datasets,when,nns,two different datasets when nns,0.6275269985198975
translation,432,20,results,two different datasets,when,nns,two different datasets when nns,0.6275269985198975
translation,432,20,results,quora and qatar living ( ql ),from,semeval,quora and qatar living ( ql ) from semeval,0.591627836227417
translation,432,20,results,nns,are,pre-trained,nns are pre-trained,0.5967141389846802
translation,432,20,results,nns,achieve,accuracy,nns achieve accuracy,0.6737696528434753
translation,432,20,results,pre-trained,on,predicted data,pre-trained on predicted data,0.5791706442832947
translation,432,20,results,accuracy,higher than,one of tk models,accuracy higher than one of tk models,0.7293627262115479
translation,432,20,results,boosted,by,fine-tuning,boosted by fine-tuning,0.5867148041725159
translation,432,20,results,fine-tuning,on,available gs data,fine-tuning on available gs data,0.5721285939216614
translation,432,22,results,improvement,of,our approach,improvement of our approach,0.5891880393028259
translation,432,22,results,improvement,obtained only when,"very different classifier , i.e. , tk - based","improvement obtained only when very different classifier , i.e. , tk - based",0.6836397051811218
translation,432,22,results,"very different classifier , i.e. , tk - based",used to label,large portion of the data,"very different classifier , i.e. , tk - based used to label large portion of the data",0.7366074919700623
translation,432,86,results,nns,obtain,higher accuracy,nns obtain higher accuracy,0.5927938222885132
translation,432,86,results,gs data,generated by,fv,gs data generated by fv,0.7282927632331848
translation,432,86,results,higher accuracy,than,fv and tk,higher accuracy than fv and tk,0.5908268690109253
translation,432,86,results,cnns,pre-trained with,data,cnns pre-trained with data,0.8074660897254944
translation,432,86,results,data,generated by,fv,data generated by fv,0.6722385883331299
translation,432,86,results,data,generated by,self-training setting,data generated by self-training setting,0.6968584060668945
translation,432,86,results,not improve,on,baseline model,not improve on baseline model,0.5928813219070435
translation,432,87,results,cnns and lstms,trained on,data,cnns and lstms trained on data,0.7681732773780823
translation,432,87,results,data,labelled by,tk model,data labelled by tk model,0.7508047819137573
translation,432,87,results,results,when,cnns and lstms,results when cnns and lstms,0.5895918011665344
translation,432,88,results,better results,than,original models,better results than original models,0.5923969149589539
translation,432,88,results,original models,trained on,same amount of data,original models trained on same amount of data,0.7444427609443665
translation,432,100,results,cnns,have,lower performance,cnns have lower performance,0.5713449120521545
translation,432,100,results,lower performance,than,tk models,lower performance than tk models,0.5979447960853577
translation,432,100,results,tk models,as,"2,669 pairs","tk models as 2,669 pairs",0.574755847454071
translation,432,100,results,results,has,cnns,results has cnns,0.5934457182884216
translation,433,133,ablation-analysis,stage 1 overall f1 score,for,corefqaext model,stage 1 overall f1 score for corefqaext model,0.6179141402244568
translation,433,133,ablation-analysis,dropped down significantly,to,63.6 %,dropped down significantly to 63.6 %,0.6048839092254639
translation,433,133,ablation-analysis,ablation analysis,has,stage 1 overall f1 score,ablation analysis has stage 1 overall f1 score,0.5138446688652039
translation,433,143,ablation-analysis,better log loss,of,0.195,better log loss of 0.195,0.5565542578697205
translation,433,147,baselines,cased and uncased versions,of,all the three individual models,cased and uncased versions of all the three individual models,0.579333484172821
translation,433,147,baselines,all the three individual models,on,stage 2 test data,all the three individual models on stage 2 test data,0.519970178604126
translation,433,147,baselines,corefmulti and corefseq,on,stage 2 test data,corefmulti and corefseq on stage 2 test data,0.5614871382713318
translation,433,147,baselines,ensembling,has,cased and uncased versions,ensembling has cased and uncased versions,0.5902217626571655
translation,433,53,experimental-setup,pytorch - pretrained - bert 2 library,to implement,all models,pytorch - pretrained - bert 2 library to implement all models,0.6519656181335449
translation,433,53,experimental-setup,experimental setup,chose,pytorch - pretrained - bert 2 library,experimental setup chose pytorch - pretrained - bert 2 library,0.6180506348609924
translation,433,78,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,433,78,experimental-setup,adam optimizer,with,l2 weight decay,adam optimizer with l2 weight decay,0.5716142058372498
translation,433,78,experimental-setup,adam optimizer,with,learning rate warmup,adam optimizer with learning rate warmup,0.6213589906692505
translation,433,78,experimental-setup,adam optimizer,with,linear decay,adam optimizer with linear decay,0.5852043628692627
translation,433,78,experimental-setup,learning rate,of,"1e - 5 , ? 1 =0.9 , ? 2 =0.999","learning rate of 1e - 5 , ? 1 =0.9 , ? 2 =0.999",0.5951126217842102
translation,433,78,experimental-setup,l2 weight decay,of,0.01,l2 weight decay of 0.01,0.5991005897521973
translation,433,78,experimental-setup,learning rate warmup,over,first 10 %,learning rate warmup over first 10 %,0.6937269568443298
translation,433,78,experimental-setup,first 10 %,of,total training steps,first 10 % of total training steps,0.565615713596344
translation,433,78,experimental-setup,linear decay,of,learning rate,linear decay of learning rate,0.5832511782646179
translation,433,78,experimental-setup,experimental setup,use,adam optimizer,experimental setup use adam optimizer,0.5987385511398315
translation,433,79,experimental-setup,maximum sequence length,set to,300,maximum sequence length set to 300,0.7457641959190369
translation,433,79,experimental-setup,batch size,of,12,batch size of 12,0.6833499670028687
translation,433,79,experimental-setup,12,used during,training,12 used during training,0.733593761920929
translation,433,79,experimental-setup,experimental setup,has,maximum sequence length,experimental setup has maximum sequence length,0.5124795436859131
translation,433,79,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,433,80,experimental-setup,bert large uncased pre-trained model,for initializing,weights,bert large uncased pre-trained model for initializing weights,0.7655107378959656
translation,433,80,experimental-setup,weights,of,bert layers,weights of bert layers,0.5859065055847168
translation,433,80,experimental-setup,experimental setup,use,bert large uncased pre-trained model,experimental setup use bert large uncased pre-trained model,0.5542641282081604
translation,433,83,experimental-setup,last 12 bert encoder layers ( layer 13 to layer 24 ),freeze,layers 1 to 12,last 12 bert encoder layers ( layer 13 to layer 24 ) freeze layers 1 to 12,0.6559351086616516
translation,433,83,experimental-setup,experimental setup,fine- tune,last 12 bert encoder layers ( layer 13 to layer 24 ),experimental setup fine- tune last 12 bert encoder layers ( layer 13 to layer 24 ),0.6746121048927307
translation,433,86,experimental-setup,hyperparameter c,for,logistic regression,hyperparameter c for logistic regression,0.5836014747619629
translation,433,86,experimental-setup,hyperparameter c,set to,0.1,hyperparameter c set to 0.1,0.6676037907600403
translation,433,86,experimental-setup,experimental setup,has,hyperparameter c,experimental setup has hyperparameter c,0.5037539005279541
translation,433,87,experimental-setup,2 epochs,on,nvidia k80 gpu,2 epochs on nvidia k80 gpu,0.532645046710968
translation,433,87,experimental-setup,experimental setup,trained for,2 epochs,experimental setup trained for 2 epochs,0.5991547703742981
translation,433,103,experimental-setup,batch size,of,4,batch size of 4,0.6922571659088135
translation,433,103,experimental-setup,batch size,initialize,bert layers,batch size initialize bert layers,0.7389329075813293
translation,433,103,experimental-setup,4,for,training,4 for training,0.6267807483673096
translation,433,103,experimental-setup,bert layers,with,weights,bert layers with weights,0.6698080897331238
translation,433,103,experimental-setup,weights,from,bert large uncased pre-trained model,weights from bert large uncased pre-trained model,0.5641421675682068
translation,433,103,experimental-setup,experimental setup,use,batch size,experimental setup use batch size,0.6168987154960632
translation,433,104,experimental-setup,layers 12 to 24,of,bert encoder,layers 12 to 24 of bert encoder,0.618259847164154
translation,433,104,experimental-setup,bert encoder,are,fine-tuned,bert encoder are fine-tuned,0.6387596130371094
translation,433,104,experimental-setup,experimental setup,has,layers 12 to 24,experimental setup has layers 12 to 24,0.5840237736701965
translation,433,105,experimental-setup,5 - fold cross validation,with,test prediction,5 - fold cross validation with test prediction,0.648637592792511
translation,433,105,experimental-setup,test prediction,averaging,each fold,test prediction averaging each fold,0.7102223038673401
translation,433,105,experimental-setup,experimental setup,use,5 - fold cross validation,experimental setup use 5 - fold cross validation,0.6015127301216125
translation,433,106,experimental-setup,100 minutes,to run on,stage 1 data,100 minutes to run on stage 1 data,0.7981553077697754
translation,433,106,experimental-setup,stage 1 data,on,nvidia k80 gpu,stage 1 data on nvidia k80 gpu,0.5286452174186707
translation,433,119,experimental-setup,dropout,of,0.1,dropout of 0.1,0.6162222623825073
translation,433,119,experimental-setup,0.1,applied before,inputs,0.1 applied before inputs,0.7223529815673828
translation,433,119,experimental-setup,inputs,are fed from,bert 's last encoder layer,inputs are fed from bert 's last encoder layer,0.7271542549133301
translation,433,119,experimental-setup,bert 's last encoder layer,to,feed forward neural network,bert 's last encoder layer to feed forward neural network,0.5944372415542603
translation,433,119,experimental-setup,experimental setup,has,dropout,experimental setup has dropout,0.5067690014839172
translation,433,120,experimental-setup,model,trained for,30 epochs,model trained for 30 epochs,0.8140823841094971
translation,433,120,experimental-setup,30 epochs,with,batch size,30 epochs with batch size,0.6014043092727661
translation,433,120,experimental-setup,batch size,of,10,batch size of 10,0.7046762704849243
translation,433,120,experimental-setup,experimental setup,trained for,30 epochs,experimental setup trained for 30 epochs,0.7293474674224854
translation,433,120,experimental-setup,experimental setup,has,model,experimental setup has model,0.5338840484619141
translation,433,121,experimental-setup,layers 12 to 24,of,bert encoder,layers 12 to 24 of bert encoder,0.618259847164154
translation,433,121,experimental-setup,bert encoder,are,fine-tuned,bert encoder are fine-tuned,0.6387596130371094
translation,433,121,experimental-setup,experimental setup,has,layers 12 to 24,experimental setup has layers 12 to 24,0.5840237736701965
translation,433,122,experimental-setup,learning rate,of,1e - 5,learning rate of 1e - 5,0.6323861479759216
translation,433,122,experimental-setup,1e - 5,used with,triangular learning rate scheduler,1e - 5 used with triangular learning rate scheduler,0.6936774849891663
translation,433,122,experimental-setup,triangular learning rate scheduler,whose,steps per cycle,triangular learning rate scheduler whose steps per cycle,0.7115164399147034
translation,433,122,experimental-setup,steps per cycle,set to,100 times,steps per cycle set to 100 times,0.6871834993362427
translation,433,122,experimental-setup,length,of,training data,length of training data,0.588007926940918
translation,433,122,experimental-setup,100 times,has,length,100 times has length,0.6265722513198853
translation,433,122,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,433,123,experimental-setup,5 fold cross validation,with,test prediction,5 fold cross validation with test prediction,0.6608917117118835
translation,433,123,experimental-setup,test prediction,averaging from,each fold,test prediction averaging from each fold,0.7270618677139282
translation,433,124,experimental-setup,105 minutes,to run on,stage 1 data,105 minutes to run on stage 1 data,0.787739098072052
translation,433,124,experimental-setup,stage 1 data,on,nvidia k80 gpu,stage 1 data on nvidia k80 gpu,0.5286452174186707
translation,433,135,experimental-setup,last four encoder layer outputs,of,bert,last four encoder layer outputs of bert,0.5894565582275391
translation,433,135,experimental-setup,experimental setup,concatenating,last four encoder layer outputs,experimental setup concatenating last four encoder layer outputs,0.6458565592765808
translation,433,22,model,extractive question answering formulation,leverages,"bert ( devlin et al. , 2018 ) pre-trained representations","extractive question answering formulation leverages bert ( devlin et al. , 2018 ) pre-trained representations",0.6655740737915039
translation,433,22,model,significantly improves,upon,"best baseline ( webster et al. , 2018 )","significantly improves upon best baseline ( webster et al. , 2018 )",0.5607479214668274
translation,433,22,model,significantly improves,has,22.2 % absolute improvement,significantly improves has 22.2 % absolute improvement,0.5967210531234741
translation,433,22,model,model,propose,extractive question answering formulation,model propose extractive question answering formulation,0.6543140411376953
translation,433,81,model,24 layers,producing,1024 dimensional hidden representation,24 layers producing 1024 dimensional hidden representation,0.6611787676811218
translation,433,82,model,whole system,trained in,end-to - end fashion,whole system trained in end-to - end fashion,0.7466541528701782
translation,433,82,model,model,has,whole system,model has whole system,0.6059533357620239
translation,433,114,model,single hidden layer feedforward neural network,with,relu activation,single hidden layer feedforward neural network with relu activation,0.6300941109657288
translation,433,10,results,ensemble of qa and bert - based multiple choice and sequence classification models,improves,f1,ensemble of qa and bert - based multiple choice and sequence classification models improves f1,0.6398588418960571
translation,433,10,results,23.3 % absolute improvement,upon,baseline,23.3 % absolute improvement upon baseline,0.5740492939949036
translation,433,10,results,f1,has,23.3 % absolute improvement,f1 has 23.3 % absolute improvement,0.574719250202179
translation,433,10,results,results,has,ensemble of qa and bert - based multiple choice and sequence classification models,results has ensemble of qa and bert - based multiple choice and sequence classification models,0.5211005806922913
translation,433,126,results,best log loss,in,stage 2,best log loss in stage 2,0.543198823928833
translation,433,126,results,results,has,ensemble model,results has ensemble model,0.5478004217147827
translation,433,146,results,ensembling,of,uncased version,ensembling of uncased version,0.6209785342216492
translation,433,146,results,version,delivered,further performance gains,version delivered further performance gains,0.7069670557975769
translation,433,146,results,3 % absolute f1 improvement,upon,uncased corefqaext,3 % absolute f1 improvement upon uncased corefqaext,0.6511484384536743
translation,433,146,results,further performance gains,has,3 % absolute f1 improvement,further performance gains has 3 % absolute f1 improvement,0.5554615259170532
translation,433,146,results,results,has,ensembling,results has ensembling,0.5693673491477966
translation,434,25,experiments,qa setting,where,annotators,qa setting where annotators,0.5592637062072754
translation,434,25,experiments,annotators,presented with,single ques - tion,annotators presented with single ques - tion,0.6359227299690247
translation,434,25,experiments,annotators,presented with,several automatically - retrieved text fragments,annotators presented with several automatically - retrieved text fragments,0.6299315690994263
translation,434,136,results,all methods,used,lemma baseline,all methods used lemma baseline,0.662734866142273
translation,434,136,results,lemma baseline,as,first step,lemma baseline as first step,0.5800097584724426
translation,434,136,results,first step,to identify,positive examples,first step to identify positive examples,0.692453145980835
translation,434,136,results,all methods,has,performance,all methods has performance,0.5792716145515442
translation,434,136,results,performance,has,drops,performance has drops,0.5993483662605286
translation,434,136,results,drops,has,dramatically,drops has dramatically,0.6438937783241272
translation,434,136,results,results,has,all methods,results has all methods,0.48065561056137085
translation,434,139,results,entailment graph,with,ppdb,entailment graph with ppdb,0.5975230932235718
translation,434,139,results,slightly improves,has,performance,slightly improves has performance,0.5808963179588318
translation,434,139,results,results,Unifying,entailment graph,results Unifying entailment graph,0.710233747959137
translation,435,5,experiments,qu - bigir system,Arabic subtask D of,se-meval 2017 task 3,qu - bigir system Arabic subtask D of se-meval 2017 task 3,0.821862518787384
translation,435,7,model,different similarity features,that encodes,lexical and semantic pairwise similarity,different similarity features that encodes lexical and semantic pairwise similarity,0.6187470555305481
translation,435,7,model,lexical and semantic pairwise similarity,of,text pairs,lexical and semantic pairwise similarity of text pairs,0.5902506113052368
translation,435,22,model,two types,of,similarity features,two types of similarity features,0.5672889947891235
translation,435,22,model,model,utilize,two types,model utilize two types,0.6316981315612793
translation,435,23,model,similarity features,based on,term representation,similarity features based on term representation,0.5645910501480103
translation,435,23,model,term representation,for,given pairs,term representation for given pairs,0.6190489530563354
translation,435,23,model,given pairs,has,of text,given pairs has of text,0.618944525718689
translation,435,23,model,model,employ,similarity features,model employ similarity features,0.5939217805862427
translation,435,96,results,best official submission,is,contrastive - 2,best official submission is contrastive - 2,0.6098750829696655
translation,435,96,results,contrastive - 2,using,term-based similarity features,contrastive - 2 using term-based similarity features,0.6759020090103149
translation,435,96,results,contrastive - 2,using,semantic word2vec similarity features,contrastive - 2 using semantic word2vec similarity features,0.6303658485412598
translation,436,53,hyperparameters,varying amounts of samples,ranging from,5 % to 100 % ),varying amounts of samples ranging from 5 % to 100 % ),0.663191020488739
translation,436,53,hyperparameters,at random,from,train set,at random from train set,0.6041626334190369
translation,436,53,hyperparameters,train set,to fine - tune,each model,train set to fine - tune each model,0.7340579032897949
translation,436,53,hyperparameters,varying amounts of samples,has,at random,varying amounts of samples has at random,0.6089972257614136
translation,436,53,hyperparameters,hyperparameters,draw,varying amounts of samples,hyperparameters draw varying amounts of samples,0.629244863986969
translation,436,54,hyperparameters,two epochs,with,learning rate,two epochs with learning rate,0.675565242767334
translation,436,54,hyperparameters,two epochs,with,batch size,two epochs with batch size,0.6545960903167725
translation,436,54,hyperparameters,learning rate,of,2e - 5,learning rate of 2e - 5,0.6410878896713257
translation,436,54,hyperparameters,batch size,of,96,batch size of 96,0.6534026861190796
translation,436,29,results,nucleus sampling,produces,better qa results,nucleus sampling produces better qa results,0.6456367373466492
translation,436,29,results,better qa results,than,beam search,better qa results than beam search,0.6224386096000671
translation,436,29,results,results,has,nucleus sampling,results has nucleus sampling,0.5278762578964233
translation,436,67,results,improves,with,model size,improves with model size,0.6254168748855591
translation,436,67,results,improves,with,# of training instances,improves with # of training instances,0.5878899097442627
translation,436,67,results,model performance,has,improves,model performance has improves,0.6272051930427551
translation,436,67,results,results,has,model performance,results has model performance,0.5472817420959473
translation,436,68,results,best intrinsic evaluation results,for,all eight models,best intrinsic evaluation results for all eight models,0.5709779858589172
translation,436,68,results,best intrinsic evaluation results,is,competitive,best intrinsic evaluation results is competitive,0.5182445049285889
translation,436,68,results,competitive,in,qa,competitive in qa,0.6111297011375427
translation,436,68,results,competitive,in,lowest- resource setup ( 5 % training data ),competitive in lowest- resource setup ( 5 % training data ),0.5221996307373047
translation,436,68,results,qa,in,lowest- resource setup ( 5 % training data ),qa in lowest- resource setup ( 5 % training data ),0.5322158932685852
translation,436,68,results,beam,has,best intrinsic evaluation results,beam has best intrinsic evaluation results,0.575600802898407
translation,436,71,results,five questions per prompt,from,large - 100 % model,five questions per prompt from large - 100 % model,0.5594517588615417
translation,436,71,results,large - 100 % model,with,ns@.95,large - 100 % model with ns@.95,0.6209995746612549
translation,436,71,results,ns@.95,provides,additional improvement,ns@.95 provides additional improvement,0.6758562326431274
translation,436,71,results,results,Sampling,five questions per prompt,results Sampling five questions per prompt,0.6811394691467285
translation,436,79,results,in- domain model performance,on,qa,in- domain model performance on qa,0.5631294846534729
translation,436,79,results,very similar,to,gt,very similar to gt,0.6290192604064941
translation,436,79,results,very similar,to,gt,very similar to gt,0.6290192604064941
translation,436,79,results,very similar,of,gt,very similar of gt,0.6494520902633667
translation,436,79,results,zero-shot score,on,newsqa,zero-shot score on newsqa,0.5304617285728455
translation,436,79,results,zero-shot score,within,roughly 4 points,zero-shot score within roughly 4 points,0.6257640719413757
translation,436,79,results,roughly 4 points,of,gt,roughly 4 points of gt,0.6364273428916931
translation,436,79,results,results,has,in- domain model performance,results has in- domain model performance,0.5593950748443604
translation,437,55,ablation-analysis,all content-free words,reduces,validation accuracy,all content-free words reduces validation accuracy,0.6313196420669556
translation,437,55,ablation-analysis,validation accuracy,of,network,validation accuracy of network,0.6263978481292725
translation,437,55,ablation-analysis,network,from,33.5 % 1,network from 33.5 % 1,0.5698066353797913
translation,437,55,ablation-analysis,33.5 % 1,to,28.5 %,33.5 % 1 to 28.5 %,0.6061980128288269
translation,437,55,ablation-analysis,ablation analysis,Dropping,all content-free words,ablation analysis Dropping all content-free words,0.7248843312263489
translation,437,56,ablation-analysis,content- free phrases,to,question,content- free phrases to question,0.5944333076477051
translation,437,56,ablation-analysis,network 's accuracy,from,33.5 % to 3.3 %,network 's accuracy from 33.5 % to 3.3 %,0.5461379885673523
translation,437,56,ablation-analysis,drops,has,network 's accuracy,drops has network 's accuracy,0.6260115504264832
translation,437,16,model,techniques,to analyze,sensitivity,techniques to analyze sensitivity,0.6850613951683044
translation,437,16,model,sensitivity,of,deep learning model,sensitivity of deep learning model,0.6006442308425903
translation,437,16,model,deep learning model,to,question words,deep learning model to question words,0.4970046877861023
translation,437,16,model,model,propose,techniques,model propose techniques,0.6862584352493286
translation,437,30,model,visualizations,of,attributions,visualizations of attributions,0.5814161896705627
translation,437,30,model,attributions,to make,analysis,attributions to make analysis,0.642392098903656
translation,437,30,model,analysis,has,easy,analysis has easy,0.642676055431366
translation,437,30,model,model,propose,visualizations,model propose visualizations,0.6458147168159485
translation,438,69,experimental-setup,backpropagation algorithm,to compute,gradients,backpropagation algorithm to compute gradients,0.7180043458938599
translation,438,69,experimental-setup,gradients,of,network,gradients of network,0.5913940072059631
translation,438,69,experimental-setup,experimental setup,has,backpropagation algorithm,experimental setup has backpropagation algorithm,0.5153045654296875
translation,438,70,experimental-setup,bow - cnn architecture,implemented using,"theano ( bergstra et al. , 2010 )","bow - cnn architecture implemented using theano ( bergstra et al. , 2010 )",0.5766987204551697
translation,438,104,experimental-setup,word embeddings,used in,our experiments,word embeddings used in our experiments,0.5775914192199707
translation,438,104,experimental-setup,initialized,by means of,unsupervised pretraining,initialized by means of unsupervised pretraining,0.6444006562232971
translation,438,104,experimental-setup,experimental setup,has,word embeddings,experimental setup has word embeddings,0.4932878315448761
translation,438,105,experimental-setup,pre-training,using,skipgram nn architecture,pre-training using skipgram nn architecture,0.6737973690032959
translation,438,105,experimental-setup,skipgram nn architecture,available in,word2vec tool,skipgram nn architecture available in word2vec tool,0.6149376630783081
translation,438,105,experimental-setup,experimental setup,perform,pre-training,experimental setup perform pre-training,0.5816611051559448
translation,438,117,experiments,askubuntu english algorithm @ 1 @5 @ 10 @1,represents,improvement,askubuntu english algorithm @ 1 @5 @ 10 @1 represents improvement,0.6670891046524048
translation,438,117,experiments,best ir baseline ( lmj ),in terms of,accuracy@1,best ir baseline ( lmj ) in terms of accuracy@1,0.6672089099884033
translation,438,117,experiments,best ir baseline ( lmj ),represents,improvement,best ir baseline ( lmj ) represents improvement,0.5924715995788574
translation,438,117,experiments,improvement,of,21.9 %,improvement of 21.9 %,0.5606208443641663
translation,438,5,model,model,propose,new neural network architecture,model propose new neural network architecture,0.680702269077301
translation,438,6,model,bag-ofwords ( bow ) representation,with,distributed vector representation,bag-ofwords ( bow ) representation with distributed vector representation,0.5857425332069397
translation,438,6,model,distributed vector representation,created by,convolutional neural network ( cnn ),distributed vector representation created by convolutional neural network ( cnn ),0.5924510955810547
translation,438,6,model,model,call,bow - cnn,model call bow - cnn,0.5859071612358093
translation,438,21,model,hybrid neural network architecture,call,bow - cnn,hybrid neural network architecture call bow - cnn,0.5935736894607544
translation,438,21,model,model,propose,hybrid neural network architecture,model propose hybrid neural network architecture,0.6688980460166931
translation,438,22,model,traditional bag-of-words ( bow ) representation,with,distributed vector representation,traditional bag-of-words ( bow ) representation with distributed vector representation,0.5876914858818054
translation,438,22,model,distributed vector representation,created by,cnn,distributed vector representation created by cnn,0.642774224281311
translation,438,22,model,distributed vector representation,to retrieve,semantically equivalent questions,distributed vector representation to retrieve semantically equivalent questions,0.7127422094345093
translation,438,22,model,model,combines,traditional bag-of-words ( bow ) representation,model combines traditional bag-of-words ( bow ) representation,0.6937640309333801
translation,438,23,model,ranking loss function,in,training,ranking loss function in training,0.470723956823349
translation,438,23,model,bow - cnn,learns to represent,questions,bow - cnn learns to represent questions,0.7065333127975464
translation,438,23,model,questions,while,learning,questions while learning,0.6679989099502563
translation,438,23,model,ranking loss function,has,bow - cnn,ranking loss function has bow - cnn,0.5340601205825806
translation,438,23,model,training,has,bow - cnn,training has bow - cnn,0.5584778189659119
translation,438,23,model,model,Using,ranking loss function,model Using ranking loss function,0.6178681254386902
translation,438,133,model,hybrid neural network architecture,combines,bag-ofwords,hybrid neural network architecture combines bag-ofwords,0.6704338192939758
translation,438,133,model,bag-ofwords,with,distributed vector representations,bag-ofwords with distributed vector representations,0.5901241898536682
translation,438,133,model,distributed vector representations,created by,cnn,distributed vector representations created by cnn,0.6353683471679688
translation,438,133,model,distributed vector representations,to retrieve,semantically equivalent questions,distributed vector representations to retrieve semantically equivalent questions,0.6881558299064636
translation,438,133,model,hybrid neural network architecture,has,bow - cnn,hybrid neural network architecture has bow - cnn,0.5753409266471863
translation,438,133,model,model,propose,hybrid neural network architecture,model propose hybrid neural network architecture,0.6688980460166931
translation,438,114,results,six ir algorithms,for,title and all settings,six ir algorithms for title and all settings,0.5830102562904358
translation,438,114,results,bow - cnn,has,outperforms,bow - cnn has outperforms,0.6247945427894592
translation,438,114,results,outperforms,has,six ir algorithms,outperforms has six ir algorithms,0.5583147406578064
translation,438,119,results,tfidf model,in,two datasets,tfidf model in two datasets,0.5084347128868103
translation,438,119,results,tfidf model,for,cases title and all,tfidf model for cases title and all,0.5943210124969482
translation,438,119,results,bow - cnn,has,consistently outperforms,bow - cnn has consistently outperforms,0.622658908367157
translation,438,119,results,consistently outperforms,has,tfidf model,consistently outperforms has tfidf model,0.5769672989845276
translation,438,119,results,results,see that,bow - cnn,results see that bow - cnn,0.6060609221458435
translation,438,121,results,cnn,for,long texts,cnn for long texts,0.6162290573120117
translation,438,121,results,bow - cnn,for,short texts,bow - cnn for short texts,0.5768492817878723
translation,438,121,results,bow - cnn,for,long texts,bow - cnn for long texts,0.5716279745101929
translation,438,121,results,bow - cnn,for,long texts,bow - cnn for long texts,0.5716279745101929
translation,438,121,results,cnn,for,long texts,cnn for long texts,0.6162290573120117
translation,438,121,results,cnn,has,outperforms,cnn has outperforms,0.5822307467460632
translation,438,121,results,outperforms,has,bow - cnn,outperforms has bow - cnn,0.597756028175354
translation,438,121,results,outperforms,has,cnn,outperforms has cnn,0.5795024633407593
translation,438,130,results,idf,to initialize,bow weight vector,idf to initialize bow weight vector,0.6830822229385376
translation,438,130,results,bow weight vector,is,better,bow weight vector is better,0.5256333947181702
translation,438,130,results,better,than using,same weight ( ones ),better than using same weight ( ones ),0.7248892784118652
translation,438,130,results,results,using,idf,results using idf,0.7082870602607727
translation,438,134,results,our approach,outperforms,traditional bow approaches,our approach outperforms traditional bow approaches,0.676683783531189
translation,438,134,results,our approach,for,long texts,our approach for long texts,0.6044859886169434
translation,438,134,results,short texts,for,long texts,short texts for long texts,0.6081053018569946
translation,438,134,results,pure cnn,obtains,best results,pure cnn obtains best results,0.5727813839912415
translation,438,134,results,pure cnn,for,long texts,pure cnn for long texts,0.5922648906707764
translation,438,134,results,pure cnn,initializing,bow weight vector,pure cnn initializing bow weight vector,0.6145874857902527
translation,438,134,results,bow - cnn,is,more effective,bow - cnn is more effective,0.560880720615387
translation,438,134,results,bow - cnn,initializing,bow weight vector,bow - cnn initializing bow weight vector,0.5769591927528381
translation,438,134,results,bow weight vector,with,idf values,bow weight vector with idf values,0.5881552696228027
translation,438,134,results,short texts,has,pure cnn,short texts has pure cnn,0.5742810368537903
translation,438,134,results,long texts,has,bow - cnn,long texts has bow - cnn,0.5752108097076416
translation,439,151,experimental-setup,"n ce = 5 ,","N L = 4 ,",n r = 5,"n ce = 5 , N L = 4 , n r = 5",0.81959468126297
translation,439,151,experimental-setup,experimental setup,use,"n ce = 5 ,","experimental setup use n ce = 5 ,",0.5829880833625793
translation,439,151,experimental-setup,experimental setup,use,n r = 5,experimental setup use n r = 5,0.6154397130012512
translation,439,158,experimental-setup,model,for,4 epochs,model for 4 epochs,0.6581888198852539
translation,439,158,experimental-setup,model,on,4 v100 gpus,model on 4 v100 gpus,0.528405487537384
translation,439,158,experimental-setup,4 epochs,on,4 v100 gpus,4 epochs on 4 v100 gpus,0.5376977920532227
translation,439,158,experimental-setup,4 v100 gpus,using,"adam optimizer ( kingma and ba , 2014 )","4 v100 gpus using adam optimizer ( kingma and ba , 2014 )",0.6103101372718811
translation,439,158,experimental-setup,"adam optimizer ( kingma and ba , 2014 )",with,initial learning rate,"adam optimizer ( kingma and ba , 2014 ) with initial learning rate",0.5920190811157227
translation,439,158,experimental-setup,"adam optimizer ( kingma and ba , 2014 )",with,batch size,"adam optimizer ( kingma and ba , 2014 ) with batch size",0.6135864853858948
translation,439,158,experimental-setup,initial learning rate,of,5 * 10 ?5,initial learning rate of 5 * 10 ?5,0.6353524923324585
translation,439,158,experimental-setup,batch size,of,512,batch size of 512,0.6329059600830078
translation,439,158,experimental-setup,experimental setup,pretrain,model,experimental setup pretrain model,0.7209702730178833
translation,439,160,experimental-setup,model,for,6 epochs,model for 6 epochs,0.6411380171775818
translation,439,160,experimental-setup,model,for,10 epochs,model for 10 epochs,0.6332400441169739
translation,439,160,experimental-setup,model,if,10 epochs,model if 10 epochs,0.6235711574554443
translation,439,160,experimental-setup,model,been,pre-trained,model been pre-trained,0.5834141373634338
translation,439,160,experimental-setup,model,is being trained from,scratch,model is being trained from scratch,0.8016814589500427
translation,439,160,experimental-setup,6 epochs,if,pre-trained,6 epochs if pre-trained,0.5385774374008179
translation,439,160,experimental-setup,6 epochs,been,pre-trained,6 epochs been pre-trained,0.538152277469635
translation,439,160,experimental-setup,10 epochs,if,model,10 epochs if model,0.6290462613105774
translation,439,160,experimental-setup,model,is being trained from,scratch,model is being trained from scratch,0.8016814589500427
translation,439,160,experimental-setup,experimental setup,fine- tune,model,experimental setup fine- tune model,0.6560331583023071
translation,439,161,experimental-setup,batch size,is,512,batch size is 512,0.5885169506072998
translation,439,161,experimental-setup,adam optimizer,with,initial learning rate,adam optimizer with initial learning rate,0.5838022828102112
translation,439,161,experimental-setup,initial learning rate,of,10 ?4,initial learning rate of 10 ?4,0.6392531991004944
translation,439,161,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,439,157,experiments,color and patterns combinations,for,attributes,color and patterns combinations for attributes,0.6513197422027588
translation,439,157,experiments,23 object categories,has,5301,23 object categories has 5301,0.6064382195472717
translation,439,157,experiments,23 object categories,has,color and patterns combinations,23 object categories has color and patterns combinations,0.5731652975082397
translation,439,157,experiments,5301,has,color and patterns combinations,5301 has color and patterns combinations,0.6133590936660767
translation,439,157,experiments,63 different,has,position combinations,63 different has position combinations,0.5905201435089111
translation,439,6,model,stl - cqa,improves,question / answering,stl - cqa improves question / answering,0.6886224746704102
translation,439,6,model,question / answering,through,sequential elements localization,question / answering through sequential elements localization,0.7094192504882812
translation,439,6,model,question / answering,through,question encoding,question / answering through question encoding,0.7033267617225647
translation,439,6,model,question / answering,through,structural transformer - based learning approach,question / answering through structural transformer - based learning approach,0.6655977368354797
translation,439,6,model,model,propose,stl - cqa,model propose stl - cqa,0.7032372951507568
translation,439,23,model,transformer - based model,to exploit,structural properties,transformer - based model to exploit structural properties,0.6942802667617798
translation,439,23,model,structural properties,of,data visualisations,structural properties of data visualisations,0.5500258207321167
translation,439,23,model,much deeper and better interpretations,to,generated answers,much deeper and better interpretations to generated answers,0.5579124093055725
translation,439,23,model,model,propose,transformer - based model,model propose transformer - based model,0.6238863468170166
translation,439,152,model,4 layers,in,language model,4 layers in language model,0.5243610143661499
translation,439,152,model,model,use,4 layers,model use 4 layers,0.6994897723197937
translation,439,138,results,proposed stl - cqa method,has,outperforms,proposed stl - cqa method has outperforms,0.5870031118392944
translation,439,138,results,outperforms,has,prior baselines,outperforms has prior baselines,0.6014935374259949
translation,439,138,results,results,demonstrate,proposed stl - cqa method,results demonstrate proposed stl - cqa method,0.6227163076400757
translation,439,169,results,prefil,has,outperform,prefil has outperform,0.6288857460021973
translation,439,169,results,outperform,has,human baselines,outperform has human baselines,0.5814085006713867
translation,439,170,results,stl - cqa,improves over,prefil,stl - cqa improves over prefil,0.7677624821662903
translation,439,170,results,prefil,specially in,complex reasoning questions,prefil specially in complex reasoning questions,0.6308497786521912
translation,439,170,results,results,has,stl - cqa,results has stl - cqa,0.5333121418952942
translation,440,55,ablation-analysis,wh-q task,helps,cumulative model,wh-q task helps cumulative model,0.6109052896499634
translation,440,55,ablation-analysis,improve,on,y /n-q,improve on y /n-q,0.6541604399681091
translation,440,55,ablation-analysis,reaching results,beyond,task-specific model,reaching results beyond task-specific model,0.6459383964538574
translation,440,55,ablation-analysis,task-specific model,from,.52 to .74,task-specific model from .52 to .74,0.4766012728214264
translation,440,55,ablation-analysis,cumulative model,has,improve,cumulative model has improve,0.6335499286651611
translation,440,60,ablation-analysis,order of tasks,plays,important role,order of tasks plays important role,0.6903449296951294
translation,440,60,ablation-analysis,wh ?y/ n,facilitates,cl,wh ?y/ n facilitates cl,0.6835429668426514
translation,440,60,ablation-analysis,cl,more than,opposite order,cl more than opposite order,0.6886684894561768
translation,440,60,ablation-analysis,at place,when,wh,at place when wh,0.7839916944503784
translation,440,60,ablation-analysis,opposite order,has,less forgetting,opposite order has less forgetting,0.6145356893539429
translation,440,60,ablation-analysis,ablation analysis,show,order of tasks,ablation analysis show order of tasks,0.6308957934379578
translation,440,45,results,results,of,per-task models,results of per-task models,0.537622332572937
translation,440,45,results,per-task models,show,large performance gap,per-task models show large performance gap,0.6353480219841003
translation,440,45,results,large performance gap,between,two tasks,large performance gap between two tasks,0.675841212272644
translation,440,45,results,results,of,per-task models,results of per-task models,0.537622332572937
translation,440,45,results,results,has,results,results has results,0.48582205176353455
translation,440,62,results,rehearsal,mitigates,forgetting,rehearsal mitigates forgetting,0.750100314617157
translation,440,62,results,works better,than,ewc,works better than ewc,0.6342650055885315
translation,440,62,results,forgetting,to,limiting degree,forgetting to limiting degree,0.6144090890884399
translation,440,62,results,rehearsal,has,works better,rehearsal has works better,0.5805109143257141
translation,440,62,results,results,has,rehearsal,results has rehearsal,0.5228058695793152
translation,440,65,results,model,trained on,wh-q,model trained on wh-q,0.7655112743377686
translation,440,65,results,wh-q,discriminates,wh-questions,wh-q discriminates wh-questions,0.7675997018814087
translation,440,65,results,wh-questions,about,different attributes,wh-questions about different attributes,0.6533417701721191
translation,440,65,results,different attributes,has,very well,different attributes has very well,0.5568902492523193
translation,440,65,results,results,has,model,results has model,0.5339115858078003
translation,441,127,hyperparameters,squad,using,standard bert fine -tuning recipe,squad using standard bert fine -tuning recipe,0.6890737414360046
translation,441,127,hyperparameters,standard bert fine -tuning recipe,for,squad,standard bert fine -tuning recipe for squad,0.6313575506210327
translation,441,128,hyperparameters,warmup linear   schedule,for,pt and ft,warmup linear   schedule for pt and ft,0.6325461268424988
translation,441,8,model,iterative generation of synthetic qa pairs,to realize,unsupervised self adaptation,iterative generation of synthetic qa pairs to realize unsupervised self adaptation,0.6771056652069092
translation,441,8,model,model,investigate,iterative generation of synthetic qa pairs,model investigate iterative generation of synthetic qa pairs,0.625182032585144
translation,441,9,model,iterative generalizations,maximize,approximation,iterative generalizations maximize approximation,0.8122829794883728
translation,441,9,model,approximation,of,lower bound,approximation of lower bound,0.5319982171058655
translation,441,9,model,lower bound,on,probability,lower bound on probability,0.5598678588867188
translation,441,9,model,probability,of,adaptation data,probability of adaptation data,0.6272416114807129
translation,441,9,model,model,present,iterative generalizations,model present iterative generalizations,0.7183871269226074
translation,441,19,model,rtc,for,adapting,rtc for adapting,0.7274156808853149
translation,441,19,model,novel iterative generalizations,of,rtc,novel iterative generalizations of rtc,0.6051624417304993
translation,441,19,model,rtc,for,adapting,rtc for adapting,0.7274156808853149
translation,441,19,model,adapting,in,"task -specific , target data specific manner","adapting in task -specific , target data specific manner",0.5471184849739075
translation,441,19,model,model,explore,novel iterative generalizations,model explore novel iterative generalizations,0.7295326590538025
translation,441,21,model,question - answering system,used as,surrogate likelihood function,question - answering system used as surrogate likelihood function,0.6170399188995361
translation,441,21,model,surrogate likelihood function,for,question and answer generators,surrogate likelihood function for question and answer generators,0.6094666123390198
translation,441,36,model,iteratively pretraining both the qa system and the qa generators,on,synthetic data,iteratively pretraining both the qa system and the qa generators on synthetic data,0.5349334478378296
translation,441,36,model,synthetic data,generated by,most recent fine - tuned qa generators,synthetic data generated by most recent fine - tuned qa generators,0.6548610329627991
translation,441,36,model,model,investigate,iteratively pretraining both the qa system and the qa generators,model investigate iteratively pretraining both the qa system and the qa generators,0.6260690689086914
translation,441,145,results,rtc - ft and qap - ft,do,additional filtering,rtc - ft and qap - ft do additional filtering,0.5247789621353149
translation,441,145,results,rtc - ft and qap - ft,do,slightly outperform,rtc - ft and qap - ft do slightly outperform,0.5151274800300598
translation,441,145,results,slightly outperform,using,prioritized beam search results,slightly outperform using prioritized beam search results,0.66220623254776
translation,441,145,results,prioritized beam search results,as,gt adap - tation data ( bsa - ft ),prioritized beam search results as gt adap - tation data ( bsa - ft ),0.5511471033096313
translation,441,145,results,rtc - ft and qap - ft,has,slightly outperform,rtc - ft and qap - ft has slightly outperform,0.5960634350776672
translation,441,145,results,results,see that,rtc - ft and qap - ft,results see that rtc - ft and qap - ft,0.6551516652107239
translation,441,146,results,significantly more efficient,to,train,significantly more efficient to train,0.5949968695640564
translation,441,146,results,train,than,bsa,train than bsa,0.6350101232528687
translation,441,148,results,filtering,improves,pt model performance,filtering improves pt model performance,0.7428125739097595
translation,441,148,results,pt model performance,has,substantially,pt model performance has substantially,0.593635082244873
translation,441,148,results,results,has,filtering,results has filtering,0.5329250693321228
translation,441,154,results,qap - based adaptation,in combination with,bert - based bidirectional lm - pretraining ( lmpt ),qap - based adaptation in combination with bert - based bidirectional lm - pretraining ( lmpt ),0.6605488061904907
translation,441,154,results,qap - based adaptation,find,qap,qap - based adaptation find qap,0.6178073287010193
translation,441,154,results,qap - based adaptation,find,qap adaptation,qap - based adaptation find qap adaptation,0.5757683515548706
translation,441,154,results,qap,outperforms,lmpt,qap outperforms lmpt,0.7651317119598389
translation,441,154,results,qap,outperforms,lmpt,qap outperforms lmpt,0.7651317119598389
translation,441,154,results,qap adaptation,leads to,best results,qap adaptation leads to best results,0.65860915184021
translation,441,154,results,results,compare,qap - based adaptation,results compare qap - based adaptation,0.6157592535018921
translation,441,155,results,iterative qap,has,significantly outper - forms,iterative qap has significantly outper - forms,0.6070102453231812
translation,441,155,results,significantly outper - forms,has,baseline,significantly outper - forms has baseline,0.627869188785553
translation,441,155,results,slightly outperforms,has,iterative rtc,slightly outperforms has iterative rtc,0.5859832167625427
translation,441,155,results,results,has,iterative qap,results has iterative qap,0.5625507831573486
translation,441,157,results,all 3 methods,able to,improve,all 3 methods able to improve,0.6761560440063477
translation,441,157,results,all 3 methods,able to,outperform and improve,all 3 methods able to outperform and improve,0.674515962600708
translation,441,157,results,outperform and improve,upon,lmpt,outperform and improve upon lmpt,0.6268656849861145
translation,441,157,results,improve,has,baseline system,improve has baseline system,0.623170793056488
translation,441,157,results,results,has,all 3 methods,results has all 3 methods,0.48286306858062744
translation,441,172,results,pt models,trained only on,generated data,pt models trained only on generated data,0.7767452001571655
translation,441,172,results,pt models,able to outperform,baseline system,pt models able to outperform baseline system,0.7781420350074768
translation,441,172,results,per target context paragraph,are,generated,per target context paragraph are generated,0.5736937522888184
translation,441,172,results,sufficient number of synthetic example,has,per target context paragraph,sufficient number of synthetic example has per target context paragraph,0.554352879524231
translation,441,172,results,results,has,pt models,results has pt models,0.5103173851966858
translation,441,174,results,"all 8,32 adapted models",has,outperform,"all 8,32 adapted models has outperform",0.6068175435066223
translation,441,174,results,outperform,has,18 k gt only model,outperform has 18 k gt only model,0.6012144088745117
translation,441,174,results,results,has,"all 8,32 adapted models","results has all 8,32 adapted models",0.5119215250015259
translation,442,4,model,novel segmentation guided attention based network,call,segattend - net,novel segmentation guided attention based network call segattend - net,0.6199344992637634
translation,442,5,model,image segmentation maps,generated by,fully convolutional deep neural network,image segmentation maps generated by fully convolutional deep neural network,0.6028842329978943
translation,442,5,model,image segmentation maps,use,refined attention maps,image segmentation maps use refined attention maps,0.6257827281951904
translation,442,5,model,fully convolutional deep neural network,to refine,our attention maps,fully convolutional deep neural network to refine our attention maps,0.6713544726371765
translation,442,5,model,refined attention maps,to make,model,refined attention maps to make model,0.6189842820167542
translation,442,5,model,model,focus on,relevant parts,model focus on relevant parts,0.7390744686126709
translation,442,5,model,relevant parts,of,image,relevant parts of image,0.5931122899055481
translation,442,5,model,relevant parts,to answer,question,relevant parts to answer question,0.7009158730506897
translation,442,5,model,model,use,image segmentation maps,model use image segmentation maps,0.6642268300056458
translation,442,5,model,model,focus on,relevant parts,model focus on relevant parts,0.7390744686126709
translation,442,19,model,approach,guide,attention maps,approach guide attention maps,0.6725001931190491
translation,442,19,model,attention maps,to learn to focus on,right regions,attention maps to learn to focus on right regions,0.6614446043968201
translation,442,19,model,right regions,by giving them,pixel level grounded annotations,right regions by giving them pixel level grounded annotations,0.6242043375968933
translation,442,19,model,pixel level grounded annotations,in the form of,segmentation maps,pixel level grounded annotations in the form of segmentation maps,0.6812122464179993
translation,442,50,model,better attention maps,use them to improve,performance,better attention maps use them to improve performance,0.6947876214981079
translation,442,50,model,performance,on,vqa task,performance on vqa task,0.5412395596504211
translation,442,8,results,previous benchmark,by,1.5 % margin,previous benchmark by 1.5 % margin,0.560396134853363
translation,442,8,results,1.5 % margin,improving,question answering accuracy,1.5 % margin improving question answering accuracy,0.6658974885940552
translation,442,8,results,question answering accuracy,from,54.1 %,question answering accuracy from 54.1 %,0.500663697719574
translation,442,8,results,54.1 %,to,55.6 %,54.1 % to 55.6 %,0.5953631401062012
translation,442,8,results,results,achieve,state of the art results,results achieve state of the art results,0.5441499352455139
translation,442,8,results,results,beat,previous benchmark,results beat previous benchmark,0.7602723240852356
translation,442,108,results,existing best reported result,close to,1.5 % margin,existing best reported result close to 1.5 % margin,0.695002019405365
translation,442,108,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,442,108,results,outperforms,has,existing best reported result,outperforms has existing best reported result,0.6011053323745728
translation,442,108,results,results,note,our model,results note our model,0.6268720030784607
translation,442,109,results,substantial improvements,in,all the question categories,substantial improvements in all the question categories,0.5326836109161377
translation,442,109,results,results,achieve,substantial improvements,results achieve substantial improvements,0.6258140206336975
translation,443,77,baselines,neural sequence - to-sequence ( seq2seq ) model,with,attention and a copy mechanism,neural sequence - to-sequence ( seq2seq ) model with attention and a copy mechanism,0.5961440801620483
translation,443,147,experimental-setup,embeddings,with,"glove ( pennington et al. , 2014 )","embeddings with glove ( pennington et al. , 2014 )",0.6399614214897156
translation,443,147,experimental-setup,batch-size,of,16,batch-size of 16,0.6798274517059326
translation,443,147,experimental-setup,16,for,200000 steps,16 for 200000 steps,0.6728086471557617
translation,443,147,experimental-setup,experimental setup,initialize,embeddings,experimental setup initialize embeddings,0.6790481209754944
translation,443,147,experimental-setup,experimental setup,train with,batch-size,experimental setup train with batch-size,0.7189651727676392
translation,443,148,experimental-setup,experimental setup,use,"opennmt ( klein et al. , 2018 ) implementation","experimental setup use opennmt ( klein et al. , 2018 ) implementation",0.5910894274711609
translation,443,9,experiments,canard,train,seq2seq models,canard train seq2seq models,0.6713346242904663
translation,443,9,experiments,dataset,of,"40,527 questions","dataset of 40,527 questions",0.5341638922691345
translation,443,9,experiments,"40,527 questions",based on,"quac ( choi et al. , 2018 )","40,527 questions based on quac ( choi et al. , 2018 )",0.6753045320510864
translation,443,9,experiments,seq2seq models,for incorporating,context,seq2seq models for incorporating context,0.7058294415473938
translation,443,9,experiments,context,into,standalone questions,context into standalone questions,0.5888246297836304
translation,443,9,experiments,canard,has,dataset,canard has dataset,0.6013619899749756
translation,443,78,model,input sequence,by concatenating,all utterances,input sequence by concatenating all utterances,0.6737118363380432
translation,443,78,model,input sequence,adding,special separator token,input sequence adding special separator token,0.6852695345878601
translation,443,78,model,all utterances,in,history h,all utterances in history h,0.574589192867279
translation,443,78,model,all utterances,prepending,q m,all utterances prepending q m,0.6906127333641052
translation,443,78,model,special separator token,between,utterances,special separator token between utterances,0.6451358199119568
translation,443,78,model,model,construct,input sequence,model construct input sequence,0.7542240023612976
translation,443,134,results,improves,up to,four points,improves up to four points,0.6624147295951843
translation,443,134,results,four points,over,naive baselines,four points over naive baselines,0.7574153542518616
translation,443,134,results,seq2seq,has,improves,seq2seq has improves,0.6272957921028137
translation,443,134,results,still well below,has,human accuracy,still well below has human accuracy,0.592539370059967
translation,443,134,results,results,has,seq2seq,results has seq2seq,0.5235289335250854
translation,444,26,experimental-setup,inner product space,during,training,inner product space during training,0.7228628396987915
translation,444,26,experimental-setup,inner product space,to avoid,explicit mapping,inner product space to avoid explicit mapping,0.6388881802558899
translation,444,26,experimental-setup,experimental setup,kernelize,inner product space,experimental setup kernelize inner product space,0.6819337010383606
translation,444,85,experimental-setup,all encoded phrase representations,of,all documents,all encoded phrase representations of all documents,0.5822486877441406
translation,444,85,experimental-setup,all documents,in,wikipedia,all documents in wikipedia,0.5435711741447449
translation,444,85,experimental-setup,experimental setup,pre-compute and store,all encoded phrase representations,experimental setup pre-compute and store all encoded phrase representations,0.7133204340934753
translation,444,101,experiments,curatedtrec,constructed from,real user queries,curatedtrec constructed from real user queries,0.6899828910827637
translation,444,101,experiments,our model,achieves,stateof - the - art performance,our model achieves stateof - the - art performance,0.6225630640983582
translation,444,101,experiments,curatedtrec,has,our model,curatedtrec has our model,0.6016372442245483
translation,444,101,experiments,real user queries,has,our model,real user queries has our model,0.5545678734779358
translation,444,5,model,each phrase embedding,by augmenting it with,contextualized sparse representation ( sparc ),each phrase embedding by augmenting it with contextualized sparse representation ( sparc ),0.7114005088806152
translation,444,6,model,rectified self-attention,to indirectly learn,sparse vectors,rectified self-attention to indirectly learn sparse vectors,0.7040044069290161
translation,444,6,model,sparse vectors,in,n-gram vocabulary space,sparse vectors in n-gram vocabulary space,0.47483953833580017
translation,444,6,model,model,leverage,rectified self-attention,model leverage rectified self-attention,0.7680859565734863
translation,444,23,model,method,to learn,contextualized sparse representation ( sparc ),method to learn contextualized sparse representation ( sparc ),0.5986900329589844
translation,444,23,model,contextualized sparse representation ( sparc ),for,each phrase,contextualized sparse representation ( sparc ) for each phrase,0.5858356356620789
translation,444,23,model,effectiveness,in,opendomain qa,effectiveness in opendomain qa,0.5303509831428528
translation,444,23,model,opendomain qa,under,phrase retrieval setup,opendomain qa under phrase retrieval setup,0.6004502177238464
translation,444,23,model,model,introduce,method,model introduce method,0.6449885368347168
translation,444,25,model,rectified self-attention weights,on,neighboring n-grams,rectified self-attention weights on neighboring n-grams,0.5356366634368896
translation,444,25,model,rectified self-attention weights,encode,rich lexical information,rectified self-attention weights encode rich lexical information,0.7167690396308899
translation,444,25,model,neighboring n-grams,to scale up,cardinality,neighboring n-grams to scale up cardinality,0.7076416611671448
translation,444,25,model,cardinality,to,n-gram vocabulary space ( billions ),cardinality to n-gram vocabulary space ( billions ),0.5336756110191345
translation,444,25,model,rich lexical information,in,each sparse vector,rich lexical information in each sparse vector,0.5241261124610901
translation,444,25,model,model,leverage,rectified self-attention weights,model leverage rectified self-attention weights,0.7617080211639404
translation,444,27,results,sparc,improves,previous phrase retrieval model,sparc improves previous phrase retrieval model,0.6206340789794922
translation,444,27,results,"denspi ( seo et al. , 2019 )",by augmenting,phrase embedding,"denspi ( seo et al. , 2019 ) by augmenting phrase embedding",0.6443086266517639
translation,444,27,results,more than 4 %,in,curatedtrec and squad - open,more than 4 % in curatedtrec and squad - open,0.5756121873855591
translation,444,27,results,previous phrase retrieval model,has,"denspi ( seo et al. , 2019 )","previous phrase retrieval model has denspi ( seo et al. , 2019 )",0.5268236994743347
translation,444,27,results,results,has,sparc,results has sparc,0.5762256383895874
translation,444,102,results,all other models,which are either,distant - or semi-supervised,all other models which are either distant - or semi-supervised,0.5802619457244873
translation,444,102,results,distant - or semi-supervised,with,at least 45x faster inference,distant - or semi-supervised with at least 45x faster inference,0.5986977815628052
translation,444,102,results,outperforms,has,all other models,outperforms has all other models,0.5782700181007385
translation,444,104,results,multi-passage bert,utilizes,dedicated document retriever,multi-passage bert utilizes dedicated document retriever,0.610979437828064
translation,444,104,results,all end-to - end models,with,large margin,all end-to - end models with large margin,0.6302661895751953
translation,444,104,results,large margin,in,squad - open,large margin in squad - open,0.5295767784118652
translation,444,104,results,multi-passage bert,has,outperforms,multi-passage bert has outperforms,0.6170728802680969
translation,444,104,results,dedicated document retriever,has,outperforms,dedicated document retriever has outperforms,0.6167227029800415
translation,444,104,results,outperforms,has,all end-to - end models,outperforms has all end-to - end models,0.5623809099197388
translation,444,104,results,results,has,multi-passage bert,results has multi-passage bert,0.5309326648712158
translation,444,107,results,results,effect of,contextualized sparse representations,results effect of contextualized sparse representations,0.6457605361938477
translation,444,109,results,trigram features,in,sparc,trigram features in sparc,0.4909656345844269
translation,444,109,results,trigram features,worse than using,uni- / bigram representations,trigram features worse than using uni- / bigram representations,0.6930081248283386
translation,444,109,results,stronger regularization,for,high- order n-gram features,stronger regularization for high- order n-gram features,0.5669928789138794
translation,444,109,results,results,adding,trigram features,results adding trigram features,0.589060366153717
translation,445,64,experimental-setup,sample data,from,arabic gigaword,sample data from arabic gigaword,0.5821430683135986
translation,445,64,experimental-setup,unannotated arabic data,provided in,task website,unannotated arabic data provided in task website,0.640652060508728
translation,445,5,model,supervised machine learning approach,to automatically re-rank,set of qa pairs,supervised machine learning approach to automatically re-rank set of qa pairs,0.739806592464447
translation,445,5,model,set of qa pairs,according to,relevance,set of qa pairs according to relevance,0.6430246233940125
translation,445,5,model,relevance,to,given question,relevance to given question,0.5439595580101013
translation,445,5,model,model,applied,supervised machine learning approach,model applied supervised machine learning approach,0.6835481524467468
translation,445,6,model,features,based on,latent semantic models,features based on latent semantic models,0.5659416317939758
translation,445,6,model,latent semantic models,namely,wtmf,latent semantic models namely wtmf,0.6676565408706665
translation,445,6,model,set of lexical features,based on,string length and surface level matching,set of lexical features based on string length and surface level matching,0.6186006665229797
translation,445,6,model,model,employ,features,model employ features,0.6423981785774231
translation,445,81,results,lexical features ( lex ),using,wtmf features ( wtmf ),lexical features ( lex ) using wtmf features ( wtmf ),0.6608062386512756
translation,445,81,results,results,using,lexical features ( lex ),results using lexical features ( lex ),0.6553441286087036
translation,445,81,results,results,using,combined features ( wtmf + lex ),results using combined features ( wtmf + lex ),0.6380037665367126
translation,445,85,results,combined wtmf + lex setting,has,outperformed,combined wtmf + lex setting has outperformed,0.6079773902893066
translation,445,85,results,outperformed,has,other settings,outperformed has other settings,0.6379686594009399
translation,445,87,results,wtmf + lex based system,improved,map,wtmf + lex based system improved map,0.7464083433151245
translation,445,87,results,by about 1 % increase,from,wtmf and the lex based system,by about 1 % increase from wtmf and the lex based system,0.6079045534133911
translation,445,87,results,map,has,by about 1 % increase,map has by about 1 % increase,0.5902518630027771
translation,445,87,results,results,has,wtmf + lex based system,results has wtmf + lex based system,0.5982226729393005
translation,445,88,results,significant improvement,over,baselines,significant improvement over baselines,0.6725573539733887
translation,445,88,results,significant improvement,over,relatively modest improvements,significant improvement over relatively modest improvements,0.6371223330497742
translation,445,88,results,baselines,for,dev set,baselines for dev set,0.583463191986084
translation,445,88,results,relatively modest improvements,in,test set,relatively modest improvements in test set,0.5170086026191711
translation,445,88,results,relatively modest improvements,with,map 45.73 and 61.16,relatively modest improvements with map 45.73 and 61.16,0.6478359699249268
translation,446,103,ablation-analysis,features,in,question group,features in question group,0.5089335441589355
translation,446,103,ablation-analysis,features,within,subsequent,features within subsequent,0.7249820232391357
translation,446,103,ablation-analysis,question group,prove,most useful single source,question group prove most useful single source,0.5719696283340454
translation,446,103,ablation-analysis,features,within,subsequent,features within subsequent,0.7249820232391357
translation,446,103,ablation-analysis,features,prove to be,more useful,features prove to be more useful,0.5989359021186829
translation,446,103,ablation-analysis,more useful,features in,precedent,more useful features in precedent,0.738757848739624
translation,446,103,ablation-analysis,ablation analysis,has,features,ablation analysis has features,0.5585079789161682
translation,446,8,model,simple n-gram based language model,to classify,rhetorical questions,simple n-gram based language model to classify rhetorical questions,0.7345792055130005
translation,446,8,model,rhetorical questions,in,switchboard dialogue act corpus,rhetorical questions in switchboard dialogue act corpus,0.47316423058509827
translation,446,8,model,model,present,simple n-gram based language model,model present simple n-gram based language model,0.6158680319786072
translation,446,101,results,performance,of,feature sets,performance of feature sets,0.6022565364837646
translation,446,101,results,performance,trained on,5960 questions ( with context ),performance trained on 5960 questions ( with context ),0.7069645524024963
translation,446,101,results,feature sets,trained on,5960 questions ( with context ),feature sets trained on 5960 questions ( with context ),0.7146187424659729
translation,446,101,results,5960 questions ( with context ),in,switchboard corpus,5960 questions ( with context ) in switchboard corpus,0.5134108066558838
translation,446,101,results,5960 questions ( with context ),tested on,2555 remaining questions,5960 questions ( with context ) tested on 2555 remaining questions,0.687354326248169
translation,446,101,results,feature sets,has,cross-valided,feature sets has cross-valided,0.6292158365249634
translation,446,104,results,f 1 - score,of,30.14 %,f 1 - score of 30.14 %,0.5576650500297546
translation,446,104,results,30.14 %,training on,contextual features alone,30.14 % training on contextual features alone,0.7227111458778381
translation,446,110,results,best results,when including,precedent and subsequent context,best results when including precedent and subsequent context,0.7806358337402344
translation,446,110,results,precedent and subsequent context,along with,question,precedent and subsequent context along with question,0.6636492013931274
translation,446,110,results,question,in,our feature space,question in our feature space,0.5233870148658752
translation,446,111,results,contextual cues,from both,directly before and after the question,contextual cues from both directly before and after the question,0.6564356684684753
translation,446,111,results,classifiers,trained on,naive question -only feature space,classifiers trained on naive question -only feature space,0.7235268354415894
translation,446,111,results,outperforms,has,classifiers,outperforms has classifiers,0.6142870187759399
translation,446,111,results,results,incorporating,contextual cues,results incorporating contextual cues,0.617684006690979
translation,446,121,results,our classifier,achieves,accuracy,our classifier achieves accuracy,0.6947753429412842
translation,446,121,results,accuracy,of,81 %,accuracy of 81 %,0.6141095757484436
translation,446,121,results,accuracy,of,84 %,accuracy of 84 %,0.6091142892837524
translation,446,121,results,81 %,when trained on,questions alone,81 % when trained on questions alone,0.6700887084007263
translation,446,121,results,84 %,when integrating,precedent and subsequent context,84 % when integrating precedent and subsequent context,0.7364034652709961
translation,446,121,results,results,has,our classifier,results has our classifier,0.5870696902275085
translation,446,126,results,n-gram features,from,context,n-gram features from context,0.5142881870269775
translation,446,126,results,n-gram features,improves,performance,n-gram features improves performance,0.6705644726753235
translation,446,127,results,53.71 % f 1 - score,by adding,features,53.71 % f 1 - score by adding features,0.739041268825531
translation,446,127,results,53.71 % f 1 - score,is,10 % improvement,53.71 % f 1 - score is 10 % improvement,0.55841463804245
translation,446,127,results,53.71 % f 1 - score,about,10 % improvement,53.71 % f 1 - score about 10 % improvement,0.575836718082428
translation,446,127,results,features,extracted from,preceding and the subsequent utterances,features extracted from preceding and the subsequent utterances,0.5572850704193115
translation,446,127,results,features,is,10 % improvement,features is 10 % improvement,0.6005097031593323
translation,446,127,results,features,about,10 % improvement,features about 10 % improvement,0.6496815085411072
translation,446,127,results,features,from,question,features from question,0.5682612061500549
translation,446,127,results,10 % improvement,from,baseline classifier,10 % improvement from baseline classifier,0.5549356937408447
translation,446,127,results,features,from,question,features from question,0.5682612061500549
translation,446,127,results,results,achieve,53.71 % f 1 - score,results achieve 53.71 % f 1 - score,0.6182492971420288
translation,447,46,baselines,widely - used object detector,in,image captioning and vqa,widely - used object detector in image captioning and vqa,0.48706769943237305
translation,447,46,baselines,"faster r-cnn ( ren et al. , 2015 b )",has,widely - used object detector,"faster r-cnn ( ren et al. , 2015 b ) has widely - used object detector",0.5052331686019897
translation,447,46,baselines,baselines,select,"faster r-cnn ( ren et al. , 2015 b )","baselines select faster r-cnn ( ren et al. , 2015 b )",0.5371604561805725
translation,447,51,baselines,k bounding box regions,comes with,d-dimensional feature vector,k bounding box regions comes with d-dimensional feature vector,0.5860447883605957
translation,447,63,baselines,frcnn,use,default visual features,frcnn use default visual features,0.5804470181465149
translation,447,63,baselines,frcnn,use,bounding boxes,frcnn use bounding boxes,0.6262079477310181
translation,447,63,baselines,default visual features,from,faster r-cnn detector,default visual features from faster r-cnn detector,0.500988781452179
translation,447,63,baselines,default visual features,from,faster r-cnn detector,default visual features from faster r-cnn detector,0.500988781452179
translation,447,63,baselines,default visual features,from,faster r-cnn detector,default visual features from faster r-cnn detector,0.500988781452179
translation,447,63,baselines,ultra,use,bounding boxes,ultra use bounding boxes,0.6210017800331116
translation,447,63,baselines,bounding boxes,from,faster r-cnn detector,bounding boxes from faster r-cnn detector,0.5183782577514648
translation,447,73,hyperparameters,multi-modal inputs,to,sequence of encoder feature vectors,multi-modal inputs to sequence of encoder feature vectors,0.5722675919532776
translation,447,73,hyperparameters,global features,by,graph - rise,global features by graph - rise,0.5721186399459839
translation,447,73,hyperparameters,global features,by,"graph-rise ( ultra , dense 64d )","global features by graph-rise ( ultra , dense 64d )",0.5465528964996338
translation,447,73,hyperparameters,dense 64d vector,extracted from,whole image,dense 64d vector extracted from whole image,0.5403796434402466
translation,447,73,hyperparameters,box-region features,by,"faster r-cnn ( fr - cnn , sparse 2048d )","box-region features by faster r-cnn ( fr - cnn , sparse 2048d )",0.5127533078193665
translation,447,73,hyperparameters,"graph-rise ( ultra , dense 64d )",extracted from,each cropped image region,"graph-rise ( ultra , dense 64d ) extracted from each cropped image region",0.5594241619110107
translation,447,73,hyperparameters,each cropped image region,resized to,224x224,each cropped image region resized to 224x224,0.7064516544342041
translation,447,73,hyperparameters,l,obtained by embedding,predicted object semantic labels,l obtained by embedding predicted object semantic labels,0.7419577836990356
translation,447,73,hyperparameters,label embeddings,obtained by embedding,predicted object semantic labels,label embeddings obtained by embedding predicted object semantic labels,0.6520640254020691
translation,447,73,hyperparameters,predicted object semantic labels,from,google cloud vision apis,predicted object semantic labels from google cloud vision apis,0.5241541862487793
translation,447,73,hyperparameters,hyperparameters,To convert,multi-modal inputs,hyperparameters To convert multi-modal inputs,0.622677743434906
translation,447,6,model,featurization,for,down-stream tasks,featurization for down-stream tasks,0.5946843028068542
translation,447,6,model,model,examine,effect,model examine effect,0.6584254503250122
translation,447,26,model,ultra-fine - grained semantic labels,for,featurization,ultra-fine - grained semantic labels for featurization,0.6043336391448975
translation,447,26,model,object detection modules,trained on,visual genome ( vg ),object detection modules trained on visual genome ( vg ),0.7387896776199341
translation,447,26,model,object detection modules,applied to,out - of- domain images,object detection modules applied to out - of- domain images,0.6491281390190125
translation,447,26,model,image captioning,on,conceptual captions dataset,image captioning on conceptual captions dataset,0.508375883102417
translation,447,26,model,image captioning,on,vizwiz dataset,image captioning on vizwiz dataset,0.50804203748703
translation,447,26,model,vqa,on,vizwiz dataset,vqa on vizwiz dataset,0.5514011979103088
translation,447,26,model,out - of- domain images,has,image captioning,out - of- domain images has image captioning,0.5531941056251526
translation,447,26,model,model,leverage,ultra-fine - grained semantic labels,model leverage ultra-fine - grained semantic labels,0.715218722820282
translation,447,45,model,one,responsible for,category - agnostic box proposal,one responsible for category - agnostic box proposal,0.585590660572052
translation,447,45,model,one,other for,featurizing,one other for featurizing,0.7368941903114319
translation,447,45,model,each cropped region,for,semantic label prediction,each cropped region for semantic label prediction,0.5904288291931152
translation,447,45,model,featurizing,has,each cropped region,featurizing has each cropped region,0.5828158259391785
translation,447,45,model,model,responsible for,category - agnostic box proposal,model responsible for category - agnostic box proposal,0.6002148985862732
translation,447,45,model,model,other for,featurizing,model other for featurizing,0.7047072649002075
translation,447,45,model,model,other for,each cropped region,model other for each cropped region,0.6211575865745544
translation,447,45,model,model,has,one,model has one,0.5633423924446106
translation,447,59,model,"graph- based , semi-supervised representation learning approach",called,graph -regularized image semantic embedding ( graph - rise ),"graph- based , semi-supervised representation learning approach called graph -regularized image semantic embedding ( graph - rise )",0.6187980771064758
translation,447,59,model,model,exploit,"graph- based , semi-supervised representation learning approach","model exploit graph- based , semi-supervised representation learning approach",0.7235107421875
translation,447,72,model,encoder-decoder model,whose,basic building block,encoder-decoder model whose basic building block,0.6507458090782166
translation,447,72,model,basic building block,is,transformer network,basic building block is transformer network,0.530297577381134
translation,447,72,model,model,adopt,encoder-decoder model,model adopt encoder-decoder model,0.6726495027542114
translation,447,50,results,map@50,of,10.96,map@50 of 10.96,0.5918374061584473
translation,447,50,results,map@50,of,1.5,map@50 of 1.5,0.6563030481338501
translation,447,50,results,10.96,for,object detection,10.96 for object detection,0.5948811769485474
translation,447,50,results,1.5,for,attribute detection,1.5 for attribute detection,0.5308529138565063
translation,447,50,results,results,achieve,map@50,results achieve map@50,0.6827101707458496
translation,447,53,results,output features,task of,vqa,output features task of vqa,0.7041285634040833
translation,447,53,results,accuracy,of,66.9 %,accuracy of 66.9 %,0.5609021782875061
translation,447,53,results,66.9 %,on,validation set,66.9 % on validation set,0.5439651608467102
translation,447,53,results,validation set,of,vqa2 dataset,validation set of vqa2 dataset,0.587209165096283
translation,447,53,results,results,Using,output features,results Using output features,0.6572242975234985
translation,447,87,results,our models,with,ultra features,our models with ultra features,0.6723179221153259
translation,447,87,results,clearly outperform,ones with,frcnn,clearly outperform ones with frcnn,0.7058762311935425
translation,447,87,results,ultra features,has,clearly outperform,ultra features has clearly outperform,0.6475329399108887
translation,447,87,results,results,has,our models,results has our models,0.5733726620674133
translation,447,93,results,all other single baselines,for,cider and spice,all other single baselines for cider and spice,0.6235830187797546
translation,447,93,results,all other single baselines,for,tie on rouge -l,all other single baselines for tie on rouge -l,0.6225272417068481
translation,447,93,results,g + b-ultra + l model,has,outperforms,g + b-ultra + l model has outperforms,0.6241379380226135
translation,447,93,results,outperforms,has,all other single baselines,outperforms has all other single baselines,0.5802904963493347
translation,447,93,results,results,"As of August 30 , 2019",g + b-ultra + l model,"results As of August 30 , 2019 g + b-ultra + l model",0.6470105051994324
translation,447,112,results,results,on,vizwiz benchmark,results on vizwiz benchmark,0.5505943298339844
translation,447,113,results,our model,with,frcnn,our model with frcnn,0.6569712162017822
translation,447,113,results,frcnn,provides,strong baseline,frcnn provides strong baseline,0.6358752846717834
translation,447,113,results,slightly outperforming,has,previous - best model,slightly outperforming has previous - best model,0.5878885388374329
translation,447,113,results,slightly outperforming,has,ban,slightly outperforming has ban,0.6341674327850342
translation,447,113,results,results,has,our model,results has our model,0.5871725678443909
translation,447,114,results,model,using,ultra features,model using ultra features,0.7162339687347412
translation,447,114,results,outperforms,by,significant margin,outperforms by significant margin,0.6588555574417114
translation,447,114,results,one using fr - cnn,by,significant margin,one using fr - cnn by significant margin,0.6092064380645752
translation,447,114,results,1.8 % accuracy,on,  all   question types,1.8 % accuracy on   all   question types,0.49787652492523193
translation,447,114,results,outperforms,has,one using fr - cnn,outperforms has one using fr - cnn,0.5984583497047424
translation,447,114,results,significant margin,has,1.8 % accuracy,significant margin has 1.8 % accuracy,0.5551204681396484
translation,447,114,results,results,has,model,results has model,0.5339115858078003
translation,449,83,ablation-analysis,bert - rc,without using,passage scores,bert - rc without using passage scores,0.6444814801216125
translation,449,83,ablation-analysis,number of passages,has,bert - rc,number of passages has bert - rc,0.5820732116699219
translation,449,83,ablation-analysis,passage scores,has,decreases,passage scores has decreases,0.6116349697113037
translation,449,83,ablation-analysis,decreases,has,performance significantly,decreases has performance significantly,0.59219890832901
translation,449,83,ablation-analysis,ablation analysis,increasing,number of passages,ablation analysis increasing number of passages,0.704295814037323
translation,449,42,baselines,multi-passage bert,normalizes,probability distributions p s and p e,multi-passage bert normalizes probability distributions p s and p e,0.7003129720687866
translation,449,42,baselines,probability distributions p s and p e,for,each passage independently,probability distributions p s and p e for each passage independently,0.634338915348053
translation,449,42,baselines,probability distributions p s and p e,may cause,incomparable answer scores,probability distributions p s and p e may cause incomparable answer scores,0.7120210528373718
translation,449,42,baselines,incomparable answer scores,across,passages,incomparable answer scores across passages,0.6765555143356323
translation,449,42,baselines,baselines,has,multi-passage bert,baselines has multi-passage bert,0.6138389110565186
translation,449,52,experiments,question - answer pairs,are,squad 1.1,question - answer pairs are squad 1.1,0.5677962899208069
translation,449,52,experiments,question - answer pairs,from,squad 1.1,question - answer pairs from squad 1.1,0.5539197325706482
translation,449,52,experiments,opensquad,has,question - answer pairs,opensquad has question - answer pairs,0.627021849155426
translation,449,64,hyperparameters,window size,as,100 words,window size as 100 words,0.5439631342887878
translation,449,64,hyperparameters,stride,as,50 words,stride as 50 words,0.5821024179458618
translation,449,64,hyperparameters,hyperparameters,set,window size,hyperparameters set window size,0.6380046606063843
translation,449,64,hyperparameters,hyperparameters,set,stride,hyperparameters set stride,0.6581343412399292
translation,449,6,model,multi-passage bert model,to globally normalize,answer scores,multi-passage bert model to globally normalize answer scores,0.6705745458602905
translation,449,6,model,answer scores,across,all passages,answer scores across all passages,0.610075056552887
translation,449,6,model,all passages,of,same question,all passages of same question,0.5943036675453186
translation,449,6,model,better answers,by utilizing,more passages,better answers by utilizing more passages,0.6134387254714966
translation,449,6,model,model,propose,multi-passage bert model,model propose multi-passage bert model,0.6216102838516235
translation,449,43,model,global normalization method,to normalize,answer scores,global normalization method to normalize answer scores,0.6309008598327637
translation,449,43,model,answer scores,among,multiple passages,answer scores among multiple passages,0.5280420184135437
translation,449,43,model,model,leverage,global normalization method,model leverage global normalization method,0.7341983914375305
translation,449,7,results,articles,into,passages,articles into passages,0.5828951597213745
translation,449,7,results,passages,with,length,passages with length,0.5704699158668518
translation,449,7,results,passages,by,sliding window,passages by sliding window,0.5977809429168701
translation,449,7,results,passages,improves,performance,passages improves performance,0.6558458805084229
translation,449,7,results,length,of,100 words,length of 100 words,0.5806260704994202
translation,449,7,results,sliding window,improves,performance,sliding window improves performance,0.7417404651641846
translation,449,7,results,performance,by,4 %,performance by 4 %,0.5974894165992737
translation,449,7,results,results,splitting,articles,results splitting articles,0.5762754678726196
translation,449,8,results,passage ranker,to select,high-quality passages,passage ranker to select high-quality passages,0.6774535179138184
translation,449,8,results,multipassage bert,gains,additional 2 %,multipassage bert gains additional 2 %,0.7564828395843506
translation,449,8,results,passage ranker,has,multipassage bert,passage ranker has multipassage bert,0.5854211449623108
translation,449,8,results,high-quality passages,has,multipassage bert,high-quality passages has multipassage bert,0.5866031646728516
translation,449,8,results,results,By leveraging,passage ranker,results By leveraging passage ranker,0.665235161781311
translation,449,9,results,four standard benchmarks,showed,multi-passage bert,four standard benchmarks showed multi-passage bert,0.6528900861740112
translation,449,9,results,multi-passage bert,has,outperforms,multi-passage bert has outperforms,0.6170728802680969
translation,449,9,results,outperforms,has,all state - of - the - art models,outperforms has all state - of - the - art models,0.5486364960670471
translation,449,29,results,global normalization,makes,qa model,global normalization makes qa model,0.6505256295204163
translation,449,29,results,global normalization,splitting,articles,global normalization splitting articles,0.8006317019462585
translation,449,29,results,global normalization,leveraging,bert - based passage ranker,global normalization leveraging bert - based passage ranker,0.7418888211250305
translation,449,29,results,more stable,while pinpointing,answers,more stable while pinpointing answers,0.7179228663444519
translation,449,29,results,answers,from,large number of passages,answers from large number of passages,0.5701643824577332
translation,449,29,results,articles,into,passages,articles into passages,0.5828951597213745
translation,449,29,results,articles,by,sliding window,articles by sliding window,0.6277381777763367
translation,449,29,results,passages,with,length,passages with length,0.5704699158668518
translation,449,29,results,passages,by,sliding window,passages by sliding window,0.5977809429168701
translation,449,29,results,length,of,100 words,length of 100 words,0.5806260704994202
translation,449,29,results,length,by,sliding window,length by sliding window,0.560384213924408
translation,449,29,results,sliding window,brings,4 % improvements,sliding window brings 4 % improvements,0.6200922727584839
translation,449,29,results,bert - based passage ranker,gives us,extra 2 % improvements,bert - based passage ranker gives us extra 2 % improvements,0.7059721946716309
translation,449,29,results,explicit inter-sentence matching,is,not helpful,explicit inter-sentence matching is not helpful,0.553466260433197
translation,449,29,results,not helpful,for,bert,not helpful for bert,0.6513002514839172
translation,449,29,results,qa model,has,more stable,qa model has more stable,0.5867390036582947
translation,449,29,results,results,show,global normalization,results show global normalization,0.6442037224769592
translation,449,29,results,results,leveraging,bert - based passage ranker,results leveraging bert - based passage ranker,0.6761950850486755
translation,449,30,results,state- ofthe - art models,on,four standard benchmarks,state- ofthe - art models on four standard benchmarks,0.47985488176345825
translation,449,30,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,449,30,results,outperforms,has,all state - of - the - art models,outperforms has all state - of - the - art models,0.5486364960670471
translation,449,60,results,single-sentence passages,leveraging,fixed - length passages,single-sentence passages leveraging fixed - length passages,0.6729310750961304
translation,449,60,results,fixed - length passages,works,better,fixed - length passages works better,0.6420665979385376
translation,449,60,results,passages,with,100 words,passages with 100 words,0.646230161190033
translation,449,60,results,passages,works,best,passages works best,0.6778492331504822
translation,449,60,results,100 words,works,best,100 words works best,0.6270636916160583
translation,449,60,results,results,Comparing to,single-sentence passages,results Comparing to single-sentence passages,0.6015576124191284
translation,449,66,results,4.1 % f 1,has,improvements,4.1 % f 1 has improvements,0.6024087071418762
translation,449,68,results,results,has,effect of passage ranker,results has effect of passage ranker,0.545661211013794
translation,449,76,results,reranked passages,gives us,0.9 % em,reranked passages gives us 0.9 % em,0.6910974979400635
translation,449,76,results,reranked passages,gives us,1.0 % f 1 improvements,reranked passages gives us 1.0 % f 1 improvements,0.6404551267623901
translation,449,76,results,reranked passages,leveraging,passage scores,reranked passages leveraging passage scores,0.6332955360412598
translation,449,76,results,passage scores,gives us,1.5 % em,passage scores gives us 1.5 % em,0.6521429419517517
translation,449,76,results,passage scores,gives us,1.7 % f 1 improvements,passage scores gives us 1.7 % f 1 improvements,0.6360408067703247
translation,449,76,results,results,only using,reranked passages,results only using reranked passages,0.7209981083869934
translation,449,76,results,results,leveraging,passage scores,results leveraging passage scores,0.5260106921195984
translation,449,85,results,performance,from,multi-passage bert,performance from multi-passage bert,0.5981385707855225
translation,449,85,results,multi-passage bert,without using,passage scores,multi-passage bert without using passage scores,0.6970542073249817
translation,449,85,results,increases,at,beginning,increases at beginning,0.6350800395011902
translation,449,85,results,flattens out,after,passage number,flattens out after passage number,0.7131119966506958
translation,449,85,results,passage number,is,over 10,passage number is over 10,0.6435365080833435
translation,449,85,results,passage scores,has,increases,passage scores has increases,0.616238534450531
translation,449,85,results,results,has,performance,results has performance,0.5972660779953003
translation,449,86,results,multi-passage bert,gets,better performance,multi-passage bert gets better performance,0.6540036201477051
translation,449,86,results,better performance,while using,more passages,better performance while using more passages,0.6788638830184937
translation,449,86,results,passage scores,has,multi-passage bert,passage scores has multi-passage bert,0.5981736183166504
translation,449,86,results,results,utilizing,passage scores,results utilizing passage scores,0.5249277353286743
translation,449,100,results,our multi-passage bert model,works,consistently better,our multi-passage bert model works consistently better,0.6240704655647278
translation,449,100,results,all state - of - the - art models,across,all benchmarks,all state - of - the - art models across all benchmarks,0.6408689618110657
translation,449,100,results,consistently better,than,our bert - rc model,consistently better than our bert - rc model,0.5951357483863831
translation,449,100,results,same settings,except,global normalization,same settings except global normalization,0.712178111076355
translation,449,100,results,our multi-passage bert model,has,outperforms,our multi-passage bert model has outperforms,0.5993847250938416
translation,449,100,results,outperforms,has,all state - of - the - art models,outperforms has all state - of - the - art models,0.5486364960670471
translation,449,100,results,results,see that,our multi-passage bert model,results see that our multi-passage bert model,0.6450415849685669
translation,449,101,results,our model,improves by,21.4 % em and 21.5 % f 1,our model improves by 21.4 % em and 21.5 % f 1,0.7104519009590149
translation,449,101,results,our model,improves by,5.8 % em and 6.5 % f 1,our model improves by 5.8 % em and 6.5 % f 1,0.7050473093986511
translation,449,101,results,21.4 % em and 21.5 % f 1,over,all non-bert models,21.4 % em and 21.5 % f 1 over all non-bert models,0.6478729248046875
translation,449,101,results,5.8 % em and 6.5 % f 1,over,bert - based models,5.8 % em and 6.5 % f 1 over bert - based models,0.6573542952537537
translation,449,101,results,opensquad dataset,has,our model,opensquad dataset has our model,0.5665900111198425
translation,449,101,results,results,on,opensquad dataset,results on opensquad dataset,0.5351563096046448
translation,449,102,results,bert - large model,makes,multi-passage bert,bert - large model makes multi-passage bert,0.6027565002441406
translation,449,102,results,better,on,triviaqa and opensquad datasets,better on triviaqa and opensquad datasets,0.5299447178840637
translation,449,102,results,multi-passage bert,has,better,multi-passage bert has better,0.6301312446594238
translation,450,193,ablation-analysis,gold labels,for,subtask b,gold labels for subtask b,0.6187942028045654
translation,450,193,ablation-analysis,gold labels,adds,+ 2.05 map points absolute,gold labels adds + 2.05 map points absolute,0.6350324749946594
translation,450,193,ablation-analysis,subtask b,adds,+ 2.05 map points absolute,subtask b adds + 2.05 map points absolute,0.6461370587348938
translation,450,193,ablation-analysis,ablation analysis,using,gold labels,ablation analysis using gold labels,0.6637077927589417
translation,450,200,baselines,variants,of,crf model,variants of crf model,0.5961576700210571
translation,450,200,baselines,crf model,for,joint learning,crf model for joint learning,0.5966745018959045
translation,450,177,hyperparameters,rmsprop,for,learning,rmsprop for learning,0.6564554572105408
translation,450,178,hyperparameters,up to 100 epochs,with,patience,up to 100 epochs with patience,0.6648778915405273
translation,450,178,hyperparameters,up to 100 epochs,with,rectified linear units ( relu ),up to 100 epochs with rectified linear units ( relu ),0.6020533442497253
translation,450,178,hyperparameters,up to 100 epochs,with,l 2 regularization,up to 100 epochs with l 2 regularization,0.6530710458755493
translation,450,178,hyperparameters,up to 100 epochs,with,"dropout et al. , 2014","up to 100 epochs with dropout et al. , 2014",0.5896016359329224
translation,450,178,hyperparameters,patience,of,25,patience of 25,0.6808786988258362
translation,450,178,hyperparameters,rectified linear units ( relu ),as,activation functions,rectified linear units ( relu ) as activation functions,0.545550525188446
translation,450,178,hyperparameters,l 2 regularization,on,weights,l 2 regularization on weights,0.5144827365875244
translation,450,178,hyperparameters,"dropout et al. , 2014",of,hidden units,"dropout et al. , 2014 of hidden units",0.5524279475212097
translation,450,178,hyperparameters,hyperparameters,use,up to 100 epochs,hyperparameters use up to 100 epochs,0.6037173271179199
translation,450,181,hyperparameters,crf model,initialize,node-level weights,crf model initialize node-level weights,0.7589280605316162
translation,450,181,hyperparameters,crf model,set,edge-level weights,crf model set edge-level weights,0.6675836443901062
translation,450,181,hyperparameters,node-level weights,from,output layer weights,node-level weights from output layer weights,0.5096110701560974
translation,450,181,hyperparameters,output layer weights,of,dnns,output layer weights of dnns,0.5664573311805725
translation,450,181,hyperparameters,edge-level weights,to,0,edge-level weights to 0,0.5477533340454102
translation,450,181,hyperparameters,hyperparameters,For,crf model,hyperparameters For crf model,0.5633574724197388
translation,450,182,hyperparameters,rmsprop,with,loopy bp,rmsprop with loopy bp,0.657151460647583
translation,450,182,hyperparameters,hyperparameters,train using,rmsprop,hyperparameters train using rmsprop,0.7047048807144165
translation,450,183,hyperparameters,node parameters,according to,best settings,node parameters according to best settings,0.6343085169792175
translation,450,183,hyperparameters,best settings,of,dnn,best settings of dnn,0.6084775328636169
translation,450,183,hyperparameters,"0.001 , 0.05 , and 0.0001",for,"a , b , and c","0.001 , 0.05 , and 0.0001 for a , b , and c",0.6069127917289734
translation,450,183,hyperparameters,hyperparameters,regularize,node parameters,hyperparameters regularize node parameters,0.6477599740028381
translation,450,5,model,good answers,with respect to,thread question,good answers with respect to thread question,0.7021124958992004
translation,450,5,model,thread question,in,question -comment thread,thread question in question -comment thread,0.5228500366210938
translation,450,6,model,deep neural networks ( dnns ),to learn,meaningful task -specific embeddings,deep neural networks ( dnns ) to learn meaningful task -specific embeddings,0.6247351169586182
translation,450,6,model,meaningful task -specific embeddings,incorporate into,conditional random field ( crf ) model,meaningful task -specific embeddings incorporate into conditional random field ( crf ) model,0.5888158679008484
translation,450,6,model,conditional random field ( crf ) model,for,multitask setting,conditional random field ( crf ) model for multitask setting,0.5732370615005493
translation,450,6,model,conditional random field ( crf ) model,performing,joint learning,conditional random field ( crf ) model performing joint learning,0.603388786315918
translation,450,6,model,joint learning,over,complex graph structure,joint learning over complex graph structure,0.6643422245979309
translation,450,6,model,model,use,deep neural networks ( dnns ),model use deep neural networks ( dnns ),0.6524738073348999
translation,450,37,model,effective and efficient approach,for,cqa subtasks,effective and efficient approach for cqa subtasks,0.6140140295028687
translation,450,37,model,model,combining,simple dnns,model combining simple dnns,0.7144342660903931
translation,450,39,model,structured conditional models,for,cqa subtasks,structured conditional models for cqa subtasks,0.5681760311126709
translation,450,39,model,feed-forward dnn,first used to build,vectors,feed-forward dnn first used to build vectors,0.7020222544670105
translation,450,39,model,vectors,for,each individual subtask,vectors for each individual subtask,0.663897693157196
translation,450,39,model,reconciled,in,multitask crf,reconciled in multitask crf,0.5732019543647766
translation,450,38,results,dnns,perform,very well,dnns perform very well,0.583773136138916
translation,450,38,results,very well,on,questionquestion similarity and answer selection subtasks,very well on questionquestion similarity and answer selection subtasks,0.4749932885169983
translation,450,38,results,question - question - relatedness,influence,answerselection,question - question - relatedness influence answerselection,0.7432403564453125
translation,450,38,results,crfs,exploit,dependencies,crfs exploit dependencies,0.7224555611610413
translation,450,38,results,dependencies,between,subtasks,dependencies between subtasks,0.6708244681358337
translation,450,38,results,results,show,dnns,results show dnns,0.606467068195343
translation,450,186,results,dnn models,for,"subtasks a , b and c","dnn models for subtasks a , b and c",0.608648955821991
translation,450,186,results,"subtasks a , b and c",on,test set,"subtasks a , b and c on test set",0.5519950985908508
translation,450,190,results,individual dnn models,for,subtasks b and c,individual dnn models for subtasks b and c,0.5595499873161316
translation,450,190,results,subtasks b and c,are,very competitive,subtasks b and c are very competitive,0.586297869682312
translation,450,190,results,very competitive,falling between,first and the second best,very competitive falling between first and the second best,0.7022354602813721
translation,450,190,results,results,see that,individual dnn models,results see that individual dnn models,0.5660331845283508
translation,450,192,results,subtask c,see that,sizeable gains,subtask c see that sizeable gains,0.6960057020187378
translation,450,192,results,sizeable gains,are,possible,sizeable gains are possible,0.5970543622970581
translation,450,192,results,sizeable gains,when using,gold labels,sizeable gains when using gold labels,0.6799758076667786
translation,450,192,results,possible,when using,gold labels,possible when using gold labels,0.7234981656074524
translation,450,192,results,gold labels,for,subtasks a and b,gold labels for subtasks a and b,0.5698940753936768
translation,450,192,results,results,Looking at,subtask c,results Looking at subtask c,0.5669723153114319
translation,450,192,results,results,for,subtask c,results for subtask c,0.5909541249275208
translation,450,209,results,locally normalized joint model,yields,much lower results,locally normalized joint model yields much lower results,0.6651981472969055
translation,450,209,results,much lower results,than,globally normalized crf,much lower results than globally normalized crf,0.5998103022575378
translation,450,209,results,"54.32 , 59.87 , and 61.76",in,"map , avgrec and mrr","54.32 , 59.87 , and 61.76 in map , avgrec and mrr",0.5255646109580994
translation,450,209,results,globally normalized crf,has,"54.32 , 59.87 , and 61.76","globally normalized crf has 54.32 , 59.87 , and 61.76",0.5311797261238098
translation,450,209,results,results,Note,locally normalized joint model,results Note locally normalized joint model,0.5912064909934998
translation,450,214,results,our scores,better than,best results,our scores better than best results,0.7141034603118896
translation,450,214,results,best results,achieved by,system,best results achieved by system,0.7217687964439392
translation,450,214,results,system,at,semeval - 2016 task 3 subtask c,system at semeval - 2016 task 3 subtask c,0.5293650031089783
translation,450,214,results,55.41,on,map,55.41 on map,0.5833055973052979
translation,450,214,results,61.48,on,mrr,61.48 on mrr,0.5632513761520386
translation,450,216,results,much improvements,for,subtask b.,much improvements for subtask b.,0.6329588294029236
translation,450,216,results,ir measures,are,unaltered,ir measures are unaltered,0.6132857203483582
translation,450,216,results,ir measures,basically,unaltered,ir measures basically unaltered,0.6984016299247742
translation,450,216,results,pipeline approach,has,ir measures,pipeline approach has ir measures,0.5755247473716736
translation,450,218,results,improve consistently,over,pipeline approach,improve consistently over pipeline approach,0.73918217420578
translation,450,218,results,ir measures,has,improve consistently,ir measures has improve consistently,0.6179625391960144
translation,450,219,results,effect,on,accuracy - p- r - f 1,effect on accuracy - p- r - f 1,0.5412576198577881
translation,450,219,results,accuracy - p- r - f 1,with,larger differences,accuracy - p- r - f 1 with larger differences,0.6410828828811646
translation,450,219,results,results,has,effect,results has effect,0.5185524821281433
translation,450,221,results,accuracy,improves,more than two points absolute,accuracy improves more than two points absolute,0.6985707879066467
translation,450,221,results,accuracy,improves,recall,accuracy improves recall,0.7136191725730896
translation,450,221,results,recall,boosts,f 1,recall boosts f 1,0.7269775867462158
translation,450,221,results,recall,has,increases,recall has increases,0.6245496273040771
translation,450,221,results,f 1,has,to almost 60,f 1 has to almost 60,0.6292149424552917
translation,450,221,results,results,has,accuracy,results has accuracy,0.5888755321502686
translation,450,224,results,a-c and b-c architecture,with,fully connected b nodes,a-c and b-c architecture with fully connected b nodes,0.6792199015617371
translation,450,224,results,fine- grained connection,of,a and c nodes,fine- grained connection of a and c nodes,0.6195956468582153
translation,450,225,results,increases,over,dnn b,increases over dnn b,0.7097119092941284
translation,450,225,results,dnn b,in,map ( + 0.61 ),dnn b in map ( + 0.61 ),0.5707602500915527
translation,450,225,results,results,has,best results,results has best results,0.542218804359436
translation,450,228,results,best results,for,subtask b,best results for subtask b,0.6212983131408691
translation,450,228,results,best results,slightly better than,best system,best results slightly better than best system,0.7345237731933594
translation,450,228,results,best system,at,semeval - 2016 task 3,best system at semeval - 2016 task 3,0.5056120753288269
translation,450,228,results,results,note,best results,results note best results,0.6211935877799988
translation,452,110,baselines,qg model,based on,model,qg model based on model,0.6833611130714417
translation,452,110,baselines,small modifications,using,py - torch,small modifications using py - torch,0.6896814703941345
translation,452,110,baselines,baselines,has,qg model,baselines has qg model,0.5681811571121216
translation,452,70,experiments,qg module,employ,one of the current state - of - the - art qg models,qg module employ one of the current state - of - the - art qg models,0.5432485938072205
translation,452,109,hyperparameters,optimization,used,adam optimizer,optimization used adam optimizer,0.559220016002655
translation,452,109,hyperparameters,adam optimizer,with,weight decay,adam optimizer with weight decay,0.6082910299301147
translation,452,109,hyperparameters,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,452,109,hyperparameters,learning rate,of,5e - 5,learning rate of 5e - 5,0.6587256789207458
translation,452,109,hyperparameters,hyperparameters,For,optimization,hyperparameters For optimization,0.5834490656852722
translation,452,7,model,model,propose,interrogative - word - aware question generation ( iwaqg ),model propose interrogative - word - aware question generation ( iwaqg ),0.6589601039886475
translation,452,8,model,first module,predicts,interrogative word,first module predicts interrogative word,0.7407281398773193
translation,452,8,model,interrogative word,provided to,second module,interrogative word provided to second module,0.6696901321411133
translation,452,8,model,second module,to create,question,second module to create question,0.7044755816459656
translation,452,8,model,model,has,first module,model has first module,0.5858337879180908
translation,452,34,model,pipelined system,composed of,two modules,pipelined system composed of two modules,0.7317330241203308
translation,452,34,model,interrogative - word classifier,predicts,interrogative word,interrogative - word classifier predicts interrogative word,0.6939695477485657
translation,452,34,model,qg model,generates,question,qg model generates question,0.6708672642707825
translation,452,34,model,question,conditioned on,predicted interrogative word,question conditioned on predicted interrogative word,0.7227780222892761
translation,452,34,model,interrogative -word - aware question generation ( iwaqg ),has,pipelined system,interrogative -word - aware question generation ( iwaqg ) has pipelined system,0.5727251768112183
translation,452,34,model,model,propose,interrogative -word - aware question generation ( iwaqg ),model propose interrogative -word - aware question generation ( iwaqg ),0.6589601039886475
translation,452,108,model,feed -forward network,consists of,single layer,feed -forward network consists of single layer,0.6702139973640442
translation,452,108,model,model,has,feed -forward network,model has feed -forward network,0.5714539289474487
translation,452,111,model,encoder,uses,bilstm,encoder uses bilstm,0.5503222346305847
translation,452,111,model,encoder,uses,lstm,encoder uses lstm,0.5542671084403992
translation,452,111,model,decoder,uses,lstm,decoder uses lstm,0.5617783665657043
translation,452,111,model,model,has,encoder,model has encoder,0.5940273404121399
translation,452,179,model,model,predict,interrogative word,model predict interrogative word,0.7294096350669861
translation,452,119,results,interrogative - word classifier,achieves,accuracy,interrogative - word classifier achieves accuracy,0.679061770439148
translation,452,119,results,accuracy,of,73.8 %,accuracy of 73.8 %,0.5554830431938171
translation,452,119,results,73.8 %,on,test set,73.8 % on test set,0.5307407379150391
translation,452,119,results,test set,of,squad,test set of squad,0.6356548070907593
translation,452,119,results,results,has,interrogative - word classifier,results has interrogative - word classifier,0.5119353532791138
translation,452,127,results,all other models,in,all the metrics,all other models in all the metrics,0.5000185966491699
translation,452,127,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,452,127,results,outperforms,has,all other models,outperforms has all other models,0.5782700181007385
translation,452,127,results,results,has,our model,results has our model,0.5871725678443909
translation,452,138,results,total recall,of using,only the qg module,total recall of using only the qg module,0.6534924507141113
translation,452,138,results,only the qg module,is,68.29 %,only the qg module is 68.29 %,0.574198305606842
translation,452,138,results,recall,of,our proposed system,recall of our proposed system,0.5940301418304443
translation,452,138,results,recall,is,74.10 %,recall is 74.10 %,0.5675816535949707
translation,452,138,results,74.10 %,has,improvement,74.10 % has improvement,0.5734086036682129
translation,452,139,results,recall,would be,99.72 %,recall would be 99.72 %,0.6274735331535339
translation,452,139,results,perfect interrogative - word classifier,has,recall,perfect interrogative - word classifier has recall,0.5781762003898621
translation,452,139,results,99.72 %,has,dramatic improvement,99.72 % has dramatic improvement,0.5689823627471924
translation,452,139,results,results,assume,perfect interrogative - word classifier,results assume perfect interrogative - word classifier,0.6207252144813538
translation,452,168,results,three features,yields to,best performance,three features yields to best performance,0.6230215430259705
translation,452,168,results,results,combination of,three features,results combination of three features,0.6146631240844727
translation,452,170,results,overall recall,is,high,overall recall is high,0.5749943256378174
translation,452,170,results,higher,than,qg module,higher than qg module,0.6177875399589539
translation,452,170,results,higher,using,qg module,higher using qg module,0.7368299961090088
translation,452,170,results,interrogative - word prediction task,as,independent classification problem,interrogative - word prediction task as independent classification problem,0.536717414855957
translation,452,170,results,independent classification problem,yields to,higher recall,independent classification problem yields to higher recall,0.5938562154769897
translation,452,170,results,results,modeling,interrogative - word prediction task,results modeling interrogative - word prediction task,0.5713441371917725
translation,453,136,ablation-analysis,upward trend,observe,direct result,upward trend observe direct result,0.5654956102371216
translation,453,136,ablation-analysis,upward trend,not,direct result,upward trend not direct result,0.6306068301200867
translation,453,136,ablation-analysis,direct result,of,users,direct result of users,0.6252151131629944
translation,453,136,ablation-analysis,users,looking for,different types of information,users looking for different types of information,0.5814025402069092
translation,453,136,ablation-analysis,c50 q,has,upward trend,c50 q has upward trend,0.6182279586791992
translation,454,163,ablation-analysis,ws - tb,with,dqg,ws - tb with dqg,0.6655121445655823
translation,454,163,ablation-analysis,additional variation,with,generative component,additional variation with generative component,0.62758868932724
translation,454,163,ablation-analysis,ablation analysis,complementing,ws - tb,ablation analysis complementing ws - tb,0.7131799459457397
translation,454,6,baselines,weak supervision,using,title and body of a question,weak supervision using title and body of a question,0.6651308536529541
translation,454,6,baselines,baselines,propose,two novel methods,baselines propose two novel methods,0.6900521516799927
translation,454,28,baselines,models,on,titlebody pairs,models on titlebody pairs,0.5468010306358337
translation,454,146,baselines,baselines,has,dqg,baselines has dqg,0.5631546378135681
translation,454,137,hyperparameters,early stopping,using,bleu scores,early stopping using bleu scores,0.6738651394844055
translation,454,137,hyperparameters,bleu scores,to avoid,overfitting,bleu scores to avoid overfitting,0.569499135017395
translation,454,137,hyperparameters,hyperparameters,perform,early stopping,hyperparameters perform early stopping,0.5698978304862976
translation,454,23,model,two novel methods,for,scenarios,two novel methods for scenarios,0.6758456826210022
translation,454,23,model,weak supervision,with,title-body pairs ( ws - tb ),weak supervision with title-body pairs ( ws - tb ),0.6248223781585693
translation,454,23,model,model,propose,two novel methods,model propose two novel methods,0.7149108648300171
translation,454,132,model,input,with,lstms,input with lstms,0.6323713064193726
translation,454,132,model,different attention mechanisms,including,multi-headed self-attention,different attention mechanisms including multi-headed self-attention,0.6372397541999817
translation,454,132,model,model,first encodes,input,model first encodes input,0.7486180067062378
translation,454,145,results,all weakly supervised techniques,as well as,dqg,all weakly supervised techniques as well as dqg,0.5978381633758545
translation,454,145,results,dqg,are,effective training methods,dqg are effective training methods,0.6051906943321228
translation,454,145,results,results,show,all weakly supervised techniques,results show all weakly supervised techniques,0.6040116548538208
translation,454,147,results,direct transfer,from,similar source domain,direct transfer from similar source domain,0.6325086355209351
translation,454,147,results,encoder-decoder approach,on,askubuntu - lei,encoder-decoder approach on askubuntu - lei,0.5963022708892822
translation,454,147,results,methods,has,outperform,methods has outperform,0.6413140296936035
translation,454,147,results,outperform,has,direct transfer,outperform has direct transfer,0.6452175378799438
translation,454,147,results,results,has,methods,results has methods,0.44259312748908997
translation,454,148,results,ws - tb,is,most effective method,ws - tb is most effective method,0.560498833656311
translation,454,148,results,consistently outperforms,has,adversarial domain transfer,consistently outperforms has adversarial domain transfer,0.5974267721176147
translation,454,148,results,adversarial domain transfer,has,0.9 pp on average,adversarial domain transfer has 0.9 pp on average,0.5688751339912415
translation,454,149,results,large differences,between,three methods,large differences between three methods,0.6873493194580078
translation,454,149,results,dqg,suitable for,training,dqg suitable for training,0.7842183709144592
translation,454,149,results,results,not observe,large differences,results not observe large differences,0.7214841842651367
translation,454,154,results,perform better,than,supervised training,perform better than supervised training,0.5855512022972107
translation,454,154,results,our methods,has,perform better,our methods has perform better,0.5798438787460327
translation,454,155,results,smaller improvements,for,ws - qa,smaller improvements for ws - qa,0.6543298363685608
translation,454,155,results,ws - qa,has,2.8 pp on avg,ws - qa has 2.8 pp on avg,0.6181308031082153
translation,454,155,results,results,observe,smaller improvements,results observe smaller improvements,0.6059702634811401
translation,454,156,results,performances,for,rcnn,performances for rcnn,0.580479085445404
translation,454,156,results,rcnn,on,askubuntu - lei,rcnn on askubuntu - lei,0.652151882648468
translation,454,156,results,askubuntu - lei,are,mostly unchanged,askubuntu - lei are mostly unchanged,0.6015356183052063
translation,454,156,results,mostly unchanged,with,minor improvements,mostly unchanged with minor improvements,0.5637326836585999
translation,454,156,results,minor improvements,on,dev,minor improvements on dev,0.618501603603363
translation,454,156,results,results,has,performances,results has performances,0.5711642503738403
translation,454,159,results,model performance,has,consistently improves,model performance has consistently improves,0.612298846244812
translation,454,159,results,increase,has,training data,increase has training data,0.5579116940498352
translation,454,159,results,results,see that,model performance,results see that model performance,0.6490950584411621
translation,454,162,results,smaller cqa platforms,with,fewer questions,smaller cqa platforms with fewer questions,0.6245464086532593
translation,454,165,results,larger numbers,of,title- body pairs,larger numbers of title- body pairs,0.5966443419456482
translation,454,165,results,ws - tb,achieve,better performances,ws - tb achieve better performances,0.6221807599067688
translation,454,165,results,larger numbers,has,dqg,larger numbers has dqg,0.6483237147331238
translation,454,165,results,larger numbers,has,ws - tb,larger numbers has ws - tb,0.6402497887611389
translation,454,165,results,title- body pairs,has,dqg,title- body pairs has dqg,0.5659990310668945
translation,454,165,results,title- body pairs,has,ws - tb,title- body pairs has ws - tb,0.575023353099823
translation,454,165,results,results,use,larger numbers,results use larger numbers,0.6971133947372437
translation,454,190,results,substantially improves,by,5 pp,substantially improves by 5 pp,0.6751635074615479
translation,454,190,results,5 pp,worse than,coala,5 pp worse than coala,0.7105562686920166
translation,454,190,results,all available title- body pairs,has,bilstm model,all available title- body pairs has bilstm model,0.5468706488609314
translation,454,190,results,bilstm model,has,substantially improves,bilstm model has substantially improves,0.5672763586044312
translation,454,190,results,results,use,all available title- body pairs,results use all available title- body pairs,0.6026774048805237
translation,454,200,results,number of training examples,has,model performances,number of training examples has model performances,0.528420090675354
translation,454,200,results,model performances,has,consistently improve,model performances has consistently improve,0.5730553865432739
translation,454,200,results,results,increasing,number of training examples,results increasing number of training examples,0.6254487037658691
translation,454,210,results,bleu scores,of,mqan model,bleu scores of mqan model,0.5371359586715698
translation,454,210,results,mqan model,for,qg,mqan model for qg,0.651252269744873
translation,454,210,results,mqan model,are,not very high,mqan model are not very high,0.5974912047386169
translation,454,210,results,not very high,between,13.3- 18.9 bleu,not very high between 13.3- 18.9 bleu,0.5922781229019165
translation,454,210,results,results,found that,bleu scores,results found that bleu scores,0.5910279750823975
translation,454,212,results,overlap scores,of,questionanswer pairs,overlap scores of questionanswer pairs,0.5732757449150085
translation,454,212,results,questionanswer pairs,are,lower,questionanswer pairs are lower,0.5940492749214172
translation,454,212,results,questionanswer pairs,when considering,title - answer pairs,questionanswer pairs when considering title - answer pairs,0.6682224869728088
translation,454,212,results,results,has,overlap scores,results has overlap scores,0.5249346494674683
translation,455,166,baselines,qc,uses,pretrained captioning model,qc uses pretrained captioning model,0.5788211822509766
translation,455,166,baselines,relevance,based on,learned similarity,relevance based on learned similarity,0.6163914203643799
translation,455,166,baselines,learned similarity,between,question and image caption,learned similarity between question and image caption,0.6262170672416687
translation,455,168,hyperparameters,caption and question,embedded as,fixed length vector,caption and question embedded as fixed length vector,0.7593598961830139
translation,455,168,hyperparameters,fixed length vector,through,encoding lstm,fixed length vector through encoding lstm,0.6429328322410583
translation,455,168,hyperparameters,hyperparameters,has,caption and question,hyperparameters has caption and question,0.562554657459259
translation,455,194,hyperparameters,false premise detection model ( fpd ) model,on,all premises,false premise detection model ( fpd ) model on all premises,0.5238903164863586
translation,455,194,hyperparameters,all premises,in,qrpe dataset,all premises in qrpe dataset,0.5271704792976379
translation,455,194,hyperparameters,hyperparameters,trained,false premise detection model ( fpd ) model,hyperparameters trained false premise detection model ( fpd ) model,0.7121325135231018
translation,455,24,model,question premises,enable,vqa models,question premises enable vqa models,0.7272776961326599
translation,455,24,model,vqa models,to respond,more intelligently,vqa models to respond more intelligently,0.6893676519393921
translation,455,24,model,more intelligently,to,irrelevant or previously unseen questions,more intelligently to irrelevant or previously unseen questions,0.584711492061615
translation,455,24,model,model,show,question premises,model show question premises,0.7008631229400635
translation,455,25,model,premise extraction pipeline,based on,"spice ( anderson et al. , 2016 )","premise extraction pipeline based on spice ( anderson et al. , 2016 )",0.6519548296928406
translation,455,25,model,modern vqa models,in the face of,irrelevant or previously unseen questions,modern vqa models in the face of irrelevant or previously unseen questions,0.6862589716911316
translation,455,25,model,model,develop,premise extraction pipeline,model develop premise extraction pipeline,0.6748253107070923
translation,455,161,model,attention based hierarchical co-attention vqa model,for,task,attention based hierarchical co-attention vqa model for task,0.5889419317245483
translation,455,161,model,task,of,question relevance,task of question relevance,0.5746453404426575
translation,455,161,model,model,extend,attention based hierarchical co-attention vqa model,model extend attention based hierarchical co-attention vqa model,0.6639194488525391
translation,455,15,results,augmenting,results in,improvements,augmenting results in improvements,0.6721179485321045
translation,455,15,results,standard vqa training,with,simple premise- based questions,standard vqa training with simple premise- based questions,0.5994022488594055
translation,455,15,results,standard vqa training,results in,improvements,standard vqa training results in improvements,0.6124690175056458
translation,455,15,results,improvements,tasks requiring,compositional reasoning,improvements tasks requiring compositional reasoning,0.6489328742027283
translation,455,15,results,augmenting,has,standard vqa training,augmenting has standard vqa training,0.4816179573535919
translation,455,15,results,results,find that,augmenting,results find that augmenting,0.6794024705886841
translation,455,15,results,results,find that,standard vqa training,results find that standard vqa training,0.5904417037963867
translation,455,174,results,simple vqa - bin model,achieves,66.5 % accuracy,simple vqa - bin model achieves 66.5 % accuracy,0.6648619771003723
translation,455,174,results,attention based model hiecoatt - bin,attains,70.74 % accuracy,attention based model hiecoatt - bin attains 70.74 % accuracy,0.6019631028175354
translation,455,174,results,results,find that,simple vqa - bin model,results find that simple vqa - bin model,0.6635626554489136
translation,455,174,results,results,find that,attention based model hiecoatt - bin,results find that attention based model hiecoatt - bin,0.6350770592689514
translation,455,175,results,captionsimilarity based qc - sim model,obtaining,accuracy,captionsimilarity based qc - sim model obtaining accuracy,0.6774147152900696
translation,455,175,results,accuracy,of,74.35 %,accuracy of 74.35 %,0.5474010705947876
translation,455,175,results,captionsimilarity based qc - sim model,has,significantly outperforms,captionsimilarity based qc - sim model has significantly outperforms,0.6047786474227905
translation,455,175,results,significantly outperforms,has,baseline,significantly outperforms has baseline,0.6019589304924011
translation,455,175,results,results,has,captionsimilarity based qc - sim model,results has captionsimilarity based qc - sim model,0.5726315975189209
translation,455,177,results,extracted premise representations,consistently improves,performance,extracted premise representations consistently improves performance,0.7943919897079468
translation,455,177,results,performance,of,base models,performance of base models,0.6071491241455078
translation,455,177,results,results,addition of,extracted premise representations,results addition of extracted premise representations,0.5906569361686707
translation,455,178,results,qpc - sim,being,best performing approach,qpc - sim being best performing approach,0.6724989414215088
translation,455,178,results,best performing approach,at,75.31 % accuracy,best performing approach at 75.31 % accuracy,0.47582635283470154
translation,455,178,results,qpc - sim,has,outperform,qpc - sim has outperform,0.654405951499939
translation,455,178,results,outperform,has,no-premise information counterparts,outperform has no-premise information counterparts,0.6014431715011597
translation,455,178,results,results,has,vqa - bin-prem,results has vqa - bin-prem,0.5776921510696411
translation,455,195,results,our fpd model,achieves,accuracy,our fpd model achieves accuracy,0.7002862691879272
translation,455,195,results,accuracy,of,61.12 %,accuracy of 61.12 %,0.5516791939735413
translation,455,195,results,61.12 %,on,qrpe dataset,61.12 % on qrpe dataset,0.5251105427742004
translation,455,195,results,results,has,our fpd model,results has our fpd model,0.5644266605377197
translation,455,221,results,minor improvement,of,0.34 %,minor improvement of 0.34 %,0.5011510252952576
translation,455,221,results,0.34 %,on,standard split under top - 1k - a premise question augmentation,0.34 % on standard split under top - 1k - a premise question augmentation,0.5497376918792725
translation,455,221,results,results,find,minor improvement,results find minor improvement,0.6250413060188293
translation,455,222,results,compositional split,observe,1.16 % gain,compositional split observe 1.16 % gain,0.6189520359039307
translation,455,222,results,1.16 % gain,with,top - 1k - a augmentation,1.16 % gain with top - 1k - a augmentation,0.6561706066131592
translation,455,222,results,top - 1k - a augmentation,over,no augmentation,top - 1k - a augmentation over no augmentation,0.6858831644058228
translation,455,222,results,results,On,compositional split,results On compositional split,0.5448398590087891
translation,456,86,results,smaller word count,yields,better results,smaller word count yields better results,0.729038655757904
translation,456,139,results,special symbols,improved,results,special symbols improved results,0.7342873215675354
translation,456,139,results,special symbols,has,as words,special symbols has as words,0.5700502991676331
translation,456,139,results,results,Including,special symbols,results Including special symbols,0.6821973919868469
translation,457,8,experiments,novel dataset xqa,for,cross-lingual openqa research,novel dataset xqa for cross-lingual openqa research,0.5783032178878784
translation,457,28,experiments,cross-lingual openqa dataset,called,xqa,cross-lingual openqa dataset called xqa,0.6279045343399048
translation,457,32,experiments,multilingual bert model,achieves,best performance,multilingual bert model achieves best performance,0.6522496342658997
translation,457,32,experiments,best performance,in,al-most all target languages,best performance in al-most all target languages,0.5122044086456299
translation,457,105,hyperparameters,multiple paragraphs,adopt,shared - normalization,multiple paragraphs adopt shared - normalization,0.6592081189155579
translation,457,105,hyperparameters,shared - normalization,as,training objective,shared - normalization as training objective,0.4906313121318817
translation,457,105,hyperparameters,training objective,on sampling,paragraphs,training objective on sampling paragraphs,0.6906030178070068
translation,457,105,hyperparameters,paragraphs,as,training object,paragraphs as training object,0.527269184589386
translation,457,105,hyperparameters,training object,for,all models,training object for all models,0.5838743448257446
translation,457,105,hyperparameters,hyperparameters,To handle,multiple paragraphs,hyperparameters To handle multiple paragraphs,0.6866373419761658
translation,457,105,hyperparameters,hyperparameters,adopt,shared - normalization,hyperparameters adopt shared - normalization,0.6226595640182495
translation,457,106,hyperparameters,restructured,merging,consecutive paragraphs,restructured merging consecutive paragraphs,0.7378219962120056
translation,457,106,hyperparameters,consecutive paragraphs,up to,400 tokens,consecutive paragraphs up to 400 tokens,0.6422213912010193
translation,457,106,hyperparameters,hyperparameters,has,documents,hyperparameters has documents,0.5504506230354309
translation,457,109,hyperparameters,glove 300 - dimensional word vector,in,translate - test setting,glove 300 - dimensional word vector in translate - test setting,0.5089811086654663
translation,457,109,hyperparameters,300 - dimensional skipgram word vector,trained on,chinese / german wikipedia dumps,300 - dimensional skipgram word vector trained on chinese / german wikipedia dumps,0.69911128282547
translation,457,109,hyperparameters,chinese / german wikipedia dumps,in,translate -train setting,chinese / german wikipedia dumps in translate -train setting,0.5036414265632629
translation,457,109,hyperparameters,hyperparameters,use,glove 300 - dimensional word vector,hyperparameters use glove 300 - dimensional word vector,0.5633218288421631
translation,457,109,hyperparameters,hyperparameters,use,300 - dimensional skipgram word vector,hyperparameters use 300 - dimensional skipgram word vector,0.5882307887077332
translation,457,117,results,retrieval results,for,questions,retrieval results for questions,0.6174335479736328
translation,457,117,results,retrieval results,for,answers to questions,retrieval results for answers to questions,0.6101168394088745
translation,457,117,results,questions,from,"english , french and chinese set","questions from english , french and chinese set",0.5791336894035339
translation,457,117,results,questions,from,"portuguese , polish and russian set","questions from portuguese , polish and russian set",0.5976765155792236
translation,457,117,results,questions,from,"portuguese , polish and russian set","questions from portuguese , polish and russian set",0.5976765155792236
translation,457,117,results,answers to questions,from,"portuguese , polish and russian set","answers to questions from portuguese , polish and russian set",0.5826494097709656
translation,457,117,results,answers to questions,are,much harder to retrieve,answers to questions are much harder to retrieve,0.5704548954963684
translation,457,117,results,results,has,retrieval results,results has retrieval results,0.5718685984611511
translation,457,118,results,retrieval performance,in,all languages,retrieval performance in all languages,0.4750056564807892
translation,457,118,results,question length,has,increases,question length has increases,0.5919619202613831
translation,457,118,results,increases,has,retrieval performance,increases has retrieval performance,0.5909184813499451
translation,457,118,results,results,as,question length,results as question length,0.5069629549980164
translation,457,123,results,performance,of,multilingual bert model,performance of multilingual bert model,0.5452017784118652
translation,457,123,results,multilingual bert model,worse than,monolingual bert model,multilingual bert model worse than monolingual bert model,0.6902195811271667
translation,457,123,results,english test set,has,performance,english test set has performance,0.5742465257644653
translation,457,123,results,results,In,english test set,results In english test set,0.5387011766433716
translation,457,124,results,multilingual model,achieves,best result,multilingual model achieves best result,0.6674700379371643
translation,457,124,results,multilingual model,ability in capturing,answers,multilingual model ability in capturing answers,0.7201271057128906
translation,457,124,results,best result,ability in capturing,answers,best result ability in capturing answers,0.7159423828125
translation,457,124,results,answers,for,questions,answers for questions,0.628709077835083
translation,457,124,results,answers,across,various languages,answers across various languages,0.7200829386711121
translation,457,124,results,questions,across,various languages,questions across various languages,0.7014105319976807
translation,457,124,results,almost all target languages,has,multilingual model,almost all target languages has multilingual model,0.5510919690132141
translation,457,124,results,results,In,almost all target languages,results In almost all target languages,0.5032938718795776
translation,457,125,results,documentqa,by,large margin,documentqa by large margin,0.5820757150650024
translation,457,125,results,consistently outperforms,by,large margin,consistently outperforms by large margin,0.6161152720451355
translation,457,125,results,documentqa,by,large margin,documentqa by large margin,0.5820757150650024
translation,457,125,results,large margin,in,all target languages,large margin in all target languages,0.5214616656303406
translation,457,125,results,documentqa,has,bert,documentqa has bert,0.6874470114707947
translation,457,125,results,bert,has,bert,bert has bert,0.6503072381019592
translation,457,125,results,bert,has,consistently outperforms,bert has consistently outperforms,0.6507187485694885
translation,457,125,results,consistently outperforms,has,documentqa,consistently outperforms has documentqa,0.6259064674377441
translation,457,125,results,results,compare,documentqa,results compare documentqa,0.7174404859542847
translation,457,128,results,outperform,in,all cases,outperform in all cases,0.5809333920478821
translation,457,128,results,translatetest methods,in,all cases,translatetest methods in all cases,0.5126791000366211
translation,457,128,results,all cases,except for,documen - tqa,all cases except for documen - tqa,0.7310793995857239
translation,457,128,results,translate-train methods,has,outperform,translate-train methods has outperform,0.6083366274833679
translation,457,128,results,outperform,has,translatetest methods,outperform has translatetest methods,0.5938987731933594
translation,457,128,results,results,has,translate-train methods,results has translate-train methods,0.5153840184211731
translation,457,136,results,consistently outperforms,in,translation - based methods,consistently outperforms in translation - based methods,0.5450061559677124
translation,457,136,results,documentqa,in,translation - based methods,documentqa in translation - based methods,0.5360298156738281
translation,457,136,results,bert,has,consistently outperforms,bert has consistently outperforms,0.6507187485694885
translation,457,136,results,consistently outperforms,has,documentqa,consistently outperforms has documentqa,0.6259064674377441
translation,457,136,results,results,has,bert,results has bert,0.43097156286239624
translation,457,168,results,multilingual bert model,achieves,relatively good results,multilingual bert model achieves relatively good results,0.6218467354774475
translation,457,168,results,relatively good results,on,xqa dataset,relatively good results on xqa dataset,0.5204885005950928
translation,458,150,ablation-analysis,significant effect,of,semantic-enriched document representations,significant effect of semantic-enriched document representations,0.5884808301925659
translation,458,150,ablation-analysis,semantic-enriched document representations,equipped with,auxiliary content selection,semantic-enriched document representations equipped with auxiliary content selection,0.6642294526100159
translation,458,150,ablation-analysis,auxiliary content selection,for generating,deep questions,auxiliary content selection for generating deep questions,0.6819164156913757
translation,458,150,ablation-analysis,ablation analysis,shows,significant effect,ablation analysis shows significant effect,0.673371434211731
translation,458,162,ablation-analysis,bleu - 4 score,of,our model,bleu - 4 score of our model,0.5413272976875305
translation,458,162,ablation-analysis,our model,dramatically drops to,13.85,our model dramatically drops to 13.85,0.6975631713867188
translation,458,162,ablation-analysis,semantic graph,has,", -w / o semantic graph","semantic graph has , -w / o semantic graph",0.587821364402771
translation,458,162,ablation-analysis,semantic graph,has,bleu - 4 score,semantic graph has bleu - 4 score,0.5411421060562134
translation,458,162,ablation-analysis,", -w / o semantic graph",has,bleu - 4 score,", -w / o semantic graph has bleu - 4 score",0.6049075126647949
translation,458,162,ablation-analysis,ablation analysis,not employ,semantic graph,ablation analysis not employ semantic graph,0.6036977171897888
translation,458,163,ablation-analysis,generating questions,purely from,semantic graph,generating questions purely from semantic graph,0.6475734114646912
translation,458,163,ablation-analysis,generating questions,is,unsatisfactory,generating questions is unsatisfactory,0.5828000903129578
translation,458,167,ablation-analysis,"normal ggnn ( a3 , -w / o multi-relation & attention",to encode,semantic graph,"normal ggnn ( a3 , -w / o multi-relation & attention to encode semantic graph",0.7091208100318909
translation,458,167,ablation-analysis,drops,to,14.15 ( ?3.61 % ),drops to 14.15 ( ?3.61 % ),0.5939685106277466
translation,458,167,ablation-analysis,drops,in,bleu - 4,drops in bleu - 4,0.6207287311553955
translation,458,167,ablation-analysis,14.15 ( ?3.61 % ),in,bleu - 4,14.15 ( ?3.61 % ) in bleu - 4,0.5825793743133545
translation,458,167,ablation-analysis,14.15 ( ?3.61 % ),compared to,model,14.15 ( ?3.61 % ) compared to model,0.704961895942688
translation,458,167,ablation-analysis,model,with,"att- ggnn ( a4 , -w / o multi-task )","model with att- ggnn ( a4 , -w / o multi-task )",0.6406265497207642
translation,458,167,ablation-analysis,"normal ggnn ( a3 , -w / o multi-relation & attention",has,performance,"normal ggnn ( a3 , -w / o multi-relation & attention has performance",0.5730394721031189
translation,458,167,ablation-analysis,performance,has,drops,performance has drops,0.5993483662605286
translation,458,167,ablation-analysis,ablation analysis,Using,"normal ggnn ( a3 , -w / o multi-relation & attention","ablation analysis Using normal ggnn ( a3 , -w / o multi-relation & attention",0.694123387336731
translation,458,171,ablation-analysis,bleu - 4 score,drops from,15.53 to 14.66,bleu - 4 score drops from 15.53 to 14.66,0.7049620151519775
translation,458,171,ablation-analysis,content selection task,has,", -w / o multi-task","content selection task has , -w / o multi-task",0.5680020451545715
translation,458,171,ablation-analysis,content selection task,has,bleu - 4 score,content selection task has bleu - 4 score,0.5221434235572815
translation,458,171,ablation-analysis,", -w / o multi-task",has,bleu - 4 score,", -w / o multi-task has bleu - 4 score",0.5862060189247131
translation,458,171,ablation-analysis,ablation analysis,turning off,content selection task,ablation analysis turning off content selection task,0.6789840459823608
translation,458,193,ablation-analysis,our model,greatly reduces,semantic errors,our model greatly reduces semantic errors,0.7192599177360535
translation,458,193,ablation-analysis,semantic errors,to,8.3 %,semantic errors to 8.3 %,0.5330333113670349
translation,458,193,ablation-analysis,ablation analysis,has,our model,ablation analysis has our model,0.5855822563171387
translation,458,133,baselines,document,as,input,document as input,0.5649763345718384
translation,458,133,baselines,input,to decode,question,input to decode question,0.7491689920425415
translation,458,133,baselines,seq2seq + attn,has,basic seq2seq model,seq2seq + attn has basic seq2seq model,0.5890645384788513
translation,458,133,baselines,baselines,has,seq2seq + attn,baselines has seq2seq + attn,0.5667077898979187
translation,458,134,baselines,seq2seq model,with,feature - rich encoder,seq2seq model with feature - rich encoder,0.6096142530441284
translation,458,134,baselines,feature - rich encoder,containing,"answer position , pos and ner information","feature - rich encoder containing answer position , pos and ner information",0.6308115124702454
translation,458,135,baselines,"ass2s ( kim et al. , 2019 )",learns to decode,questions,"ass2s ( kim et al. , 2019 ) learns to decode questions",0.7354407906532288
translation,458,135,baselines,questions,from,answer-separated passage encoder,questions from answer-separated passage encoder,0.5726528167724609
translation,458,135,baselines,answer-separated passage encoder,together with,keyword - net based answer encoder,answer-separated passage encoder together with keyword - net based answer encoder,0.5995568633079529
translation,458,135,baselines,baselines,has,"ass2s ( kim et al. , 2019 )","baselines has ass2s ( kim et al. , 2019 )",0.5241251587867737
translation,458,136,baselines,s2sa-at-mp-gsa,:,enhanced seq2seq model,s2sa-at-mp-gsa : enhanced seq2seq model,0.6330053210258484
translation,458,136,baselines,enhanced seq2seq model,incorporating,gated self-attention,enhanced seq2seq model incorporating gated self-attention,0.7012357115745544
translation,458,136,baselines,enhanced seq2seq model,incorporating,maxout-pointers,enhanced seq2seq model incorporating maxout-pointers,0.749656617641449
translation,458,136,baselines,maxout-pointers,to encode,richer passage - level contexts,maxout-pointers to encode richer passage - level contexts,0.7090613842010498
translation,458,136,baselines,s2sa-at-mp-gsa,has,enhanced seq2seq model,s2sa-at-mp-gsa has enhanced seq2seq model,0.5769038200378418
translation,458,136,baselines,baselines,has,s2sa-at-mp-gsa,baselines has s2sa-at-mp-gsa,0.5570844411849976
translation,458,138,baselines,word- level content selection,has,before generation,word- level content selection has before generation,0.553207516670227
translation,458,138,baselines,baselines,has,cgc -qg,baselines has cgc -qg,0.5440537929534912
translation,458,141,baselines,baselines,share,1 - layer gru document encoder,baselines share 1 - layer gru document encoder,0.6549038887023926
translation,458,141,baselines,baselines,share,question decoder,baselines share question decoder,0.7021045088768005
translation,458,141,baselines,question decoder,with,hidden units,question decoder with hidden units,0.6283292174339294
translation,458,141,baselines,hidden units,of,512 dimensions,hidden units of 512 dimensions,0.5619491934776306
translation,458,141,baselines,baselines,has,baselines,baselines has baselines,0.6036415696144104
translation,458,142,hyperparameters,word embeddings,initialized with,300 dimensional pre-trained glove,word embeddings initialized with 300 dimensional pre-trained glove,0.723637044429779
translation,458,142,hyperparameters,hyperparameters,has,word embeddings,hyperparameters has word embeddings,0.4784160554409027
translation,458,143,hyperparameters,graph encoder,plus,pos and answer tag embeddings,graph encoder plus pos and answer tag embeddings,0.6552963256835938
translation,458,143,hyperparameters,node embedding size,is,256,node embedding size is 256,0.586983323097229
translation,458,143,hyperparameters,pos and answer tag embeddings,with,32 - d,pos and answer tag embeddings with 32 - d,0.6546528339385986
translation,458,143,hyperparameters,graph encoder,has,node embedding size,graph encoder has node embedding size,0.498266339302063
translation,458,143,hyperparameters,hyperparameters,For,graph encoder,hyperparameters For graph encoder,0.5516923666000366
translation,458,144,hyperparameters,number of layers k,set to,3,number of layers k set to 3,0.7030052542686462
translation,458,144,hyperparameters,hidden state size,is,256,hidden state size is 256,0.5838548541069031
translation,458,144,hyperparameters,hyperparameters,has,number of layers k,hyperparameters has number of layers k,0.5234067440032959
translation,458,144,hyperparameters,hyperparameters,has,hidden state size,hyperparameters has hidden state size,0.5078210830688477
translation,458,5,model,global structure,of,document,global structure of document,0.6151059865951538
translation,458,5,model,reasoning,propose,novel framework,reasoning propose novel framework,0.7063536047935486
translation,458,5,model,novel framework,first constructs,semantic -level graph,novel framework first constructs semantic -level graph,0.6160934567451477
translation,458,5,model,novel framework,then encodes,semantic graph,novel framework then encodes semantic graph,0.5981694459915161
translation,458,5,model,semantic -level graph,for,input document,semantic -level graph for input document,0.5467513799667358
translation,458,5,model,semantic -level graph,by introducing,attention - based ggnn ( att - ggnn ),semantic -level graph by introducing attention - based ggnn ( att - ggnn ),0.6619612574577332
translation,458,5,model,semantic graph,by introducing,attention - based ggnn ( att - ggnn ),semantic graph by introducing attention - based ggnn ( att - ggnn ),0.6744220852851868
translation,458,5,model,model,to capture,global structure,model to capture global structure,0.7054470777511597
translation,458,5,model,model,facilitate,reasoning,model facilitate reasoning,0.6626635193824768
translation,458,5,model,model,propose,novel framework,model propose novel framework,0.720393180847168
translation,458,6,model,document-level and graphlevel representations,to perform,joint training,document-level and graphlevel representations to perform joint training,0.6105133891105652
translation,458,6,model,joint training,of,content selection and question decoding,joint training of content selection and question decoding,0.4877781569957733
translation,458,6,model,model,fuse,document-level and graphlevel representations,model fuse document-level and graphlevel representations,0.7015400528907776
translation,458,26,model,novel graph encoder,incorporates,attention mechanism,novel graph encoder incorporates attention mechanism,0.6511310338973999
translation,458,26,model,attention mechanism,into,gated graph neural network ( ggnn ),attention mechanism into gated graph neural network ( ggnn ),0.5718498826026917
translation,458,26,model,attention mechanism,to dynamically model,interactions,attention mechanism to dynamically model interactions,0.6849205493927002
translation,458,26,model,interactions,between,different semantic relations,interactions between different semantic relations,0.6616108417510986
translation,458,26,model,node-level semantic graph representations,to obtain,unified semantic - aware passage representations,node-level semantic graph representations to obtain unified semantic - aware passage representations,0.5851365923881531
translation,458,26,model,unified semantic - aware passage representations,for,question decoding,unified semantic - aware passage representations for question decoding,0.5913615822792053
translation,458,26,model,auxiliary content selection task,jointly trains with,question decoding,auxiliary content selection task jointly trains with question decoding,0.7369827628135681
translation,458,26,model,auxiliary content selection task,assists,model,auxiliary content selection task assists model,0.6220076084136963
translation,458,26,model,question decoding,assists,model,question decoding assists model,0.675974428653717
translation,458,26,model,model,in selecting,relevant contexts,model in selecting relevant contexts,0.7091680765151978
translation,458,26,model,relevant contexts,in,semantic graph,relevant contexts in semantic graph,0.5307660698890686
translation,458,26,model,relevant contexts,to form,proper reasoning chain,relevant contexts to form proper reasoning chain,0.6384508013725281
translation,458,26,model,model,proposing,novel graph encoder,model proposing novel graph encoder,0.7156063914299011
translation,458,54,model,dp -( dependency parsing ),to construct,semantic graph,dp -( dependency parsing ) to construct semantic graph,0.6613311767578125
translation,458,54,model,methods,to construct,semantic graph,methods to construct semantic graph,0.7170992493629456
translation,458,84,model,multiple relations,base,our model,multiple relations base our model,0.7441355586051941
translation,458,84,model,our model,on,multi-relation gated graph neural network ( ggnn ),our model on multi-relation gated graph neural network ( ggnn ),0.5503668785095215
translation,458,84,model,multi-relation gated graph neural network ( ggnn ),provides,separate transformation matrix,multi-relation gated graph neural network ( ggnn ) provides separate transformation matrix,0.6159397959709167
translation,458,84,model,separate transformation matrix,for,each edge type,separate transformation matrix for each edge type,0.5955804586410522
translation,458,84,model,model,To represent,multiple relations,model To represent multiple relations,0.7263437509536743
translation,458,118,model,weight sharing,on,input representations,weight sharing on input representations,0.5601629614830017
translation,458,169,model,att- ggnn model ( p2 ),incorporates,attention,att- ggnn model ( p2 ) incorporates attention,0.7432112693786621
translation,458,169,model,attention,into,normal ggnn,attention into normal ggnn,0.6328097581863403
translation,458,169,model,attention,effectively leverages,information,attention effectively leverages information,0.7049907445907593
translation,458,169,model,information,across,multiple node and edge types,information across multiple node and edge types,0.7052079439163208
translation,458,169,model,model,has,att- ggnn model ( p2 ),model has att- ggnn model ( p2 ),0.6002278923988342
translation,458,210,model,novel framework,incorporates,semantic graphs,novel framework incorporates semantic graphs,0.6589088439941406
translation,458,210,model,semantic graphs,to enhance,input document representations,semantic graphs to enhance input document representations,0.5291308760643005
translation,458,210,model,semantic graphs,generate,questions,semantic graphs generate questions,0.661294162273407
translation,458,210,model,model,propose,novel framework,model propose novel framework,0.720393180847168
translation,458,7,results,performance,over,questions,performance over questions,0.6644017696380615
translation,458,7,results,requiring reasoning,over,multiple facts,requiring reasoning over multiple facts,0.6624084711074829
translation,458,7,results,hotpotqa deep-question centric dataset,has,our model,hotpotqa deep-question centric dataset has our model,0.5381190180778503
translation,458,7,results,our model,has,greatly improves,our model has greatly improves,0.5898662209510803
translation,458,7,results,greatly improves,has,performance,greatly improves has performance,0.591407299041748
translation,458,7,results,results,On,hotpotqa deep-question centric dataset,results On hotpotqa deep-question centric dataset,0.49919596314430237
translation,458,148,results,all other baselines,in,bleu,all other baselines in bleu,0.4603009819984436
translation,458,148,results,two versions of our model - p1 and p2,has,consistently outperform,two versions of our model - p1 and p2 has consistently outperform,0.5708568096160889
translation,458,148,results,consistently outperform,has,all other baselines,consistently outperform has all other baselines,0.5614976286888123
translation,458,149,results,our model,with,dp - based semantic graph ( p2 ),our model with dp - based semantic graph ( p2 ),0.6266011595726013
translation,458,149,results,our model,achieves,absolute improvement,our model achieves absolute improvement,0.6665875911712646
translation,458,149,results,dp - based semantic graph ( p2 ),achieves,absolute improvement,dp - based semantic graph ( p2 ) achieves absolute improvement,0.6747480630874634
translation,458,149,results,absolute improvement,of,2.05,absolute improvement of 2.05,0.5327328443527222
translation,458,149,results,2.05,in,bleu - 4 ( + 15.2 % ),2.05 in bleu - 4 ( + 15.2 % ),0.5769484639167786
translation,458,149,results,document- level qg model,employs,gated self-attention,document- level qg model employs gated self-attention,0.5644113421440125
translation,458,149,results,results,has,our model,results has our model,0.5871725678443909
translation,458,151,results,cgc - qg ( b6 ),exhibits,unusual pattern,cgc - qg ( b6 ) exhibits unusual pattern,0.6733481884002686
translation,458,151,results,unusual pattern,compared with,other methods,unusual pattern compared with other methods,0.6442811489105225
translation,458,151,results,worst bleu - 1,among,all baselines,worst bleu - 1 among all baselines,0.5411302447319031
translation,458,151,results,results,results of,cgc - qg ( b6 ),results results of cgc - qg ( b6 ),0.8022767901420593
translation,458,152,results,cgc - qg,performs,word- level content selection,cgc - qg performs word- level content selection,0.621288001537323
translation,458,152,results,word- level content selection,tends to include,many irrelevant words,word- level content selection tends to include many irrelevant words,0.6780725717544556
translation,458,152,results,many irrelevant words,in,question,many irrelevant words in question,0.5362163186073303
translation,458,152,results,many irrelevant words,leading to,lengthy questions,many irrelevant words leading to lengthy questions,0.6814399361610413
translation,458,152,results,results,has,cgc - qg,results has cgc - qg,0.5522769093513489
translation,458,156,results,dp - based graph ( p2 ),performs,slightly better,dp - based graph ( p2 ) performs slightly better,0.5968261957168579
translation,458,156,results,slightly better,has,+ 3.3 % in bleu - 4,slightly better has + 3.3 % in bleu - 4,0.5758340954780579
translation,458,181,results,questions,of,better quality,questions of better quality,0.5534194111824036
translation,458,181,results,better quality,than,baselines,better quality than baselines,0.5896489024162292
translation,458,185,results,our model,by incorporating,semantic graph,our model by incorporating semantic graph,0.6325926780700684
translation,458,185,results,our model,produces,questions,our model produces questions,0.6573447585105896
translation,458,185,results,our model,utilizes,more context,our model utilizes more context,0.6300181746482849
translation,458,185,results,questions,with,fewer semantic errors,questions with fewer semantic errors,0.6109530925750732
translation,458,185,results,questions,utilizes,more context,questions utilizes more context,0.6703928112983704
translation,458,185,results,decrease,when,input document,decrease when input document,0.6898174285888672
translation,458,185,results,input document,becomes,longer,input document becomes longer,0.6360788941383362
translation,458,185,results,metrics,has,decrease,metrics has decrease,0.5898053646087646
translation,458,185,results,results,demonstrate,our model,results demonstrate our model,0.6473821401596069
translation,458,191,results,baselines,have,more unreasonable subject - predicate - object collocations ( semantic errors ),baselines have more unreasonable subject - predicate - object collocations ( semantic errors ),0.5450090169906616
translation,458,191,results,more unreasonable subject - predicate - object collocations ( semantic errors ),than,our model,more unreasonable subject - predicate - object collocations ( semantic errors ) than our model,0.5604320168495178
translation,458,191,results,results,find that,baselines,results find that baselines,0.6485356092453003
translation,458,192,results,cgc - qg ( b6 ),tends to copy,irrelevant contents,cgc - qg ( b6 ) tends to copy irrelevant contents,0.7570471167564392
translation,458,192,results,largest semantic error rate,of,26.4 %,largest semantic error rate of 26.4 %,0.5473513007164001
translation,458,192,results,26.4 %,among,three methods,26.4 % among three methods,0.5757821202278137
translation,458,192,results,irrelevant contents,from,input document,irrelevant contents from input document,0.522682785987854
translation,458,192,results,cgc - qg ( b6 ),has,largest semantic error rate,cgc - qg ( b6 ) has largest semantic error rate,0.5960134267807007
translation,458,192,results,results,has,cgc - qg ( b6 ),results has cgc - qg ( b6 ),0.5646390318870544
translation,458,195,results,cgc - qg,remarkably produces,more unanswerable questions,cgc - qg remarkably produces more unanswerable questions,0.6932523846626282
translation,458,195,results,more unanswerable questions,than,other two models,more unanswerable questions than other two models,0.5969212651252747
translation,458,195,results,our model,achieves,comparable results,our model achieves comparable results,0.6530072093009949
translation,458,195,results,comparable results,with,s2sa - at- mp-gsa ( b4 ),comparable results with s2sa - at- mp-gsa ( b4 ),0.6707375049591064
translation,458,195,results,results,has,cgc - qg,results has cgc - qg,0.5522769093513489
translation,459,8,experiments,simple classifierbased models,for predicting,which questions,simple classifierbased models for predicting which questions,0.7435466647148132
translation,459,199,hyperparameters,l2,-,regularized logistic regression,l2 - regularized logistic regression,0.5638231635093689
translation,459,199,hyperparameters,regularized logistic regression,by,"liblinear ( fan et al. , 2008 )","regularized logistic regression by liblinear ( fan et al. , 2008 )",0.4637814462184906
translation,459,199,hyperparameters,"liblinear ( fan et al. , 2008 )",for,binary classification,"liblinear ( fan et al. , 2008 ) for binary classification",0.5503770112991333
translation,459,199,hyperparameters,l2,has,regularized logistic regression,l2 has regularized logistic regression,0.5308627486228943
translation,459,199,hyperparameters,hyperparameters,use,l2,hyperparameters use l2,0.6128666996955872
translation,459,199,hyperparameters,hyperparameters,use,regularized logistic regression,hyperparameters use regularized logistic regression,0.5903708338737488
translation,460,202,ablation-analysis,rationales,extracted by,our proposed model ( w/ o r s+b +m ),rationales extracted by our proposed model ( w/ o r s+b +m ),0.7224920392036438
translation,460,202,ablation-analysis,significantly drops,with,great margin,significantly drops with great margin,0.7322784066200256
translation,460,202,ablation-analysis,rationales,has,performance,rationales has performance,0.58333820104599
translation,460,202,ablation-analysis,our proposed model ( w/ o r s+b +m ),has,performance,our proposed model ( w/ o r s+b +m ) has performance,0.5899631381034851
translation,460,202,ablation-analysis,performance,has,significantly drops,performance has significantly drops,0.6223364472389221
translation,460,202,ablation-analysis,ablation analysis,When removing,rationales,ablation analysis When removing rationales,0.7788805961608887
translation,460,104,hyperparameters,input contexts and questions,truncated to,max length 384,input contexts and questions truncated to max length 384,0.6221240758895874
translation,460,104,hyperparameters,hyperparameters,has,input contexts and questions,hyperparameters has input contexts and questions,0.5329378247261047
translation,460,105,hyperparameters,training,use,"adam ( kingma and ba , 2014 )","training use adam ( kingma and ba , 2014 )",0.6256451606750488
translation,460,105,hyperparameters,"adam ( kingma and ba , 2014 )",as,our optimizer,"adam ( kingma and ba , 2014 ) as our optimizer",0.49311700463294983
translation,460,105,hyperparameters,our optimizer,with,learning rate,our optimizer with learning rate,0.6119175553321838
translation,460,105,hyperparameters,learning rate,of,5e ? 6,learning rate of 5e ? 6,0.6425655484199524
translation,460,105,hyperparameters,"albert - large ( lan et al. , 2019 )",as,encoder structure,"albert - large ( lan et al. , 2019 ) as encoder structure",0.5259604454040527
translation,460,105,hyperparameters,encoder structure,in,our model,encoder structure in our model,0.5039775371551514
translation,460,105,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
translation,460,106,hyperparameters,model,with,batch size 12,model with batch size 12,0.6920885443687439
translation,460,106,hyperparameters,model,tune,hyperparameters,model tune hyperparameters,0.6666514873504639
translation,460,106,hyperparameters,3 epochs,with,batch size 12,3 epochs with batch size 12,0.6278210878372192
translation,460,106,hyperparameters,hyperparameters,train,model,hyperparameters train model,0.7064553499221802
translation,460,6,model,novel approach,leverages,benefits,novel approach leverages benefits,0.7926714420318604
translation,460,6,model,benefits,of,multi-task learning,benefits of multi-task learning,0.5455337166786194
translation,460,6,model,benefits,of,transfer learning,benefits of transfer learning,0.5629647374153137
translation,460,6,model,transfer learning,for generating rationales,question answering,transfer learning for generating rationales question answering,0.7465023398399353
translation,460,6,model,question answering,in,zeroshot fashion,question answering in zeroshot fashion,0.5555402040481567
translation,460,6,model,model,propose,novel approach,model propose novel approach,0.7168048620223999
translation,460,24,model,multiple tasks / domains,by feeding,corresponding data,multiple tasks / domains by feeding corresponding data,0.7135654091835022
translation,460,25,model,question answering,for learning,capability of rationalization,question answering for learning capability of rationalization,0.7031747102737427
translation,460,25,model,model,utilize,question answering,model utilize question answering,0.6209610104560852
translation,460,135,results,our model,successfully transfers,capability of rationalization,our model successfully transfers capability of rationalization,0.7167735695838928
translation,460,135,results,results,demonstrate,our model,results demonstrate our model,0.6473821401596069
translation,460,146,results,proposed model,capable of extracting,reasonable rationales,proposed model capable of extracting reasonable rationales,0.6953475475311279
translation,460,146,results,reasonable rationales,from,target domain,reasonable rationales from target domain,0.5327439904212952
translation,460,146,results,multi-task learning,has,proposed model,multi-task learning has proposed model,0.5524622797966003
translation,460,146,results,results,leveraging,multi-task learning,results leveraging multi-task learning,0.6087900400161743
translation,460,147,results,two rating tasks ( s+ b + m ),obtains,better performance,two rating tasks ( s+ b + m ) obtains better performance,0.5644312500953674
translation,460,147,results,better performance,than,one trained without movie rating ( s+ b ),better performance than one trained without movie rating ( s+ b ),0.5530052185058594
translation,460,156,results,two supervised baselines,with,large margins,two supervised baselines with large margins,0.582322895526886
translation,460,156,results,our proposed models,has,outperform,our proposed models has outperform,0.6129095554351807
translation,460,156,results,outperform,has,two supervised baselines,outperform has two supervised baselines,0.5572476983070374
translation,460,156,results,results,shown that,our proposed models,results shown that our proposed models,0.6469773650169373
translation,460,157,results,our method,can achieve,remarkable performance,our method can achieve remarkable performance,0.6651470065116882
translation,460,157,results,remarkable performance,in,zero-shot setting,remarkable performance in zero-shot setting,0.538507878780365
translation,460,157,results,any annotated rationales,has,our method,any annotated rationales has our method,0.6148248314857483
translation,460,157,results,results,without using,any annotated rationales,results without using any annotated rationales,0.7193461060523987
translation,460,158,results,qa model,with,squad,qa model with squad,0.7046844363212585
translation,460,158,results,squad,obtain,similar performance,squad obtain similar performance,0.6352426409721375
translation,460,158,results,similar performance,with,two supervised baselines,similar performance with two supervised baselines,0.6268665194511414
translation,460,158,results,training,has,qa model,training has qa model,0.6060786843299866
translation,460,162,results,additional movie rating or / and beer rating tasks,for,multi-task training,additional movie rating or / and beer rating tasks for multi-task training,0.5594791173934937
translation,460,162,results,performance,for,all cases,performance for all cases,0.6532698273658752
translation,460,162,results,additional movie rating or / and beer rating tasks,has,our model,additional movie rating or / and beer rating tasks has our model,0.5468462109565735
translation,460,162,results,multi-task training,has,our model,multi-task training has our model,0.5612938404083252
translation,460,162,results,our model,has,significantly improves,our model has significantly improves,0.5935119986534119
translation,460,162,results,significantly improves,has,performance,significantly improves has performance,0.5962982177734375
translation,460,162,results,results,With,additional movie rating or / and beer rating tasks,results With additional movie rating or / and beer rating tasks,0.5199853777885437
translation,460,163,results,results,has,best model,results has best model,0.5634682774543762
translation,460,201,results,simple qa model,trained on,squad only ( w/ o r s ),simple qa model trained on squad only ( w/ o r s ),0.7631960511207581
translation,460,201,results,randomly removing,has,extracted rationales ( w/ o r random ),randomly removing has extracted rationales ( w/ o r random ),0.5883985161781311
translation,460,201,results,extracted rationales ( w/ o r random ),has,performance,extracted rationales ( w/ o r random ) has performance,0.5817722082138062
translation,460,201,results,simple qa model,has,performance,simple qa model has performance,0.5886700749397278
translation,460,203,results,our model,extracts,rationales,our model extracts rationales,0.6533554792404175
translation,460,203,results,our model,preserves,good comprehensiveness,our model preserves good comprehensiveness,0.7087122797966003
translation,460,203,results,rationales,with,higher precision,rationales with higher precision,0.6166687607765198
translation,460,203,results,results,prove,our model,results prove our model,0.6580586433410645
translation,461,4,model,large scale qa pairs,automatically crawled and processed from,community - qa website,large scale qa pairs automatically crawled and processed from community - qa website,0.7330417037010193
translation,461,4,model,large scale qa pairs,used as,training data,large scale qa pairs used as training data,0.5990765690803528
translation,461,11,model,questions,from,given passages,questions from given passages,0.567539632320404
translation,461,11,model,given passages,using,neural networks,given passages using neural networks,0.6894344687461853
translation,461,11,model,training data,reflect,commonly,training data reflect commonly,0.7029114365577698
translation,461,11,model,questions,generated based on,natural language passages,questions generated based on natural language passages,0.6290537714958191
translation,461,11,model,model,generate,questions,model generate questions,0.6742465496063232
translation,461,12,model,large scale high-quality training data,from,community - qa ( cqa ) website,large scale high-quality training data from community - qa ( cqa ) website,0.5498233437538147
translation,461,26,model,question cluster ( or qc ) based approach,to mine,frequently - asked question patterns,question cluster ( or qc ) based approach to mine frequently - asked question patterns,0.6592087745666504
translation,461,26,model,frequently - asked question patterns,from,large scale cqa questions,frequently - asked question patterns from large scale cqa questions,0.5540621876716614
translation,461,119,results,qg results,based on,original labeled training sets,qg results based on original labeled training sets,0.6111626029014587
translation,461,119,results,g - qg,achieves,comparable or better results,g - qg achieves comparable or better results,0.6435310244560242
translation,461,119,results,results,Comparing to,qg results,results Comparing to qg results,0.6554225087165833
translation,461,121,results,generation - based qg,performs,better,generation - based qg performs better,0.6447177529335022
translation,461,121,results,better,than,retrieval - based qg,better than retrieval - based qg,0.5816722512245178
translation,461,121,results,results,has,generation - based qg,results has generation - based qg,0.5852459073066711
translation,461,122,results,outputs,find that,question pattern prediction,outputs find that question pattern prediction,0.6716765761375427
translation,461,122,results,outputs,find that,retrieval - based and generation - based methods,outputs find that retrieval - based and generation - based methods,0.69185870885849
translation,461,122,results,outputs,for,question pattern prediction,outputs for question pattern prediction,0.6340479254722595
translation,461,122,results,retrieval - based and generation - based methods,perform,similarly,retrieval - based and generation - based methods perform similarly,0.6297543048858643
translation,461,122,results,question pattern prediction,has,retrieval - based and generation - based methods,question pattern prediction has retrieval - based and generation - based methods,0.5764443874359131
translation,461,122,results,results,analyzing,outputs,results analyzing outputs,0.5903264284133911
translation,461,123,results,performs better,than,retrieval - based qg,performs better than retrieval - based qg,0.5944942831993103
translation,461,123,results,retrieval - based qg,on,question topic selection,retrieval - based qg on question topic selection,0.5176112055778503
translation,461,123,results,generation - based qg,has,performs better,generation - based qg has performs better,0.6039328575134277
translation,461,123,results,results,has,generation - based qg,results has generation - based qg,0.5852459073066711
translation,461,146,results,bleu 4 score,is,0.2031,bleu 4 score is 0.2031,0.5839060544967651
translation,461,146,results,bleu 4 score,is,0.1301,bleu 4 score is 0.1301,0.5783093571662903
translation,461,146,results,0.2031,on,wik -iqg +,0.2031 on wik -iqg +,0.594394862651825
translation,461,146,results,0.1301,on,wikiqg -,0.1301 on wikiqg -,0.6068170070648193
translation,461,146,results,results,has,bleu 4 score,results has bleu 4 score,0.5501192212104797
translation,462,76,ablation-analysis,benefit,from,scale of squad,benefit from scale of squad,0.5830788612365723
translation,462,76,ablation-analysis,scale of squad,for,transfer learning,scale of squad for transfer learning,0.6299247741699219
translation,462,76,ablation-analysis,transfer learning,to,smaller wikiqa,transfer learning to smaller wikiqa,0.599815309047699
translation,462,76,ablation-analysis,span-supervision,plays,significant role,span-supervision plays significant role,0.7231623530387878
translation,462,76,ablation-analysis,ablation analysis,see,clear sign,ablation analysis see clear sign,0.6292976140975952
translation,462,80,ablation-analysis,map,on,wikiqa,map on wikiqa,0.5872691869735718
translation,462,80,ablation-analysis,map,drops as,size of squad,map drops as size of squad,0.6878029108047485
translation,462,80,ablation-analysis,wikiqa,drops as,size of squad,wikiqa drops as size of squad,0.6904452443122864
translation,462,80,ablation-analysis,size of squad,has,decreases,size of squad has decreases,0.6277297139167786
translation,462,80,ablation-analysis,ablation analysis,has,map,ablation analysis has map,0.5346465706825256
translation,462,73,experiments,transfer learning models,achieve,better results,transfer learning models achieve better results,0.6251413226127625
translation,462,73,experiments,better results,with,pretraining,better results with pretraining,0.6351864337921143
translation,462,73,experiments,pretraining,on,span-level supervision ( squad ),pretraining on span-level supervision ( squad ),0.5420759916305542
translation,462,73,experiments,span-level supervision ( squad ),than,coarser sentence - level supervision ( squad -t ),span-level supervision ( squad ) than coarser sentence - level supervision ( squad -t ),0.5421775579452515
translation,462,74,experiments,ensemble,of,12 different training runs,ensemble of 12 different training runs,0.5635485649108887
translation,462,74,experiments,12 different training runs,on,same bidaf architecture,12 different training runs on same bidaf architecture,0.5400267243385315
translation,462,18,hyperparameters,source and target models,adopt,"bidaf ( seo et al. , 2017 )","source and target models adopt bidaf ( seo et al. , 2017 )",0.6169859766960144
translation,462,18,hyperparameters,hyperparameters,For,source and target models,hyperparameters For source and target models,0.5665065050125122
translation,462,15,model,"coarser , sentencelevel qa",through,standard transfer learning,"coarser , sentencelevel qa through standard transfer learning",0.6122192144393921
translation,462,15,model,standard transfer learning,of,model,standard transfer learning of model,0.5921807885169983
translation,462,15,model,technique,of,model,technique of model,0.6179788708686829
translation,462,15,model,model,trained on,"large , spansupervised qa dataset","model trained on large , spansupervised qa dataset",0.7485012412071228
translation,462,15,model,standard transfer learning,has,technique,standard transfer learning has technique,0.5852634310722351
translation,462,15,model,model,address,"coarser , sentencelevel qa","model address coarser , sentencelevel qa",0.533843457698822
translation,462,15,model,model,trained on,"large , spansupervised qa dataset","model trained on large , spansupervised qa dataset",0.7485012412071228
translation,462,6,results,previous best model,by,more than 8 %,previous best model by more than 8 %,0.5899068117141724
translation,462,6,results,wikiqa,has,our model,wikiqa has our model,0.6280222535133362
translation,462,6,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,462,6,results,outperforms,has,previous best model,outperforms has previous best model,0.6195195317268372
translation,462,6,results,results,For,wikiqa,results For wikiqa,0.5981549024581909
translation,462,16,results,target task,benefits from,scale of the source dataset,target task benefits from scale of the source dataset,0.6640182137489319
translation,462,16,results,target task,capability of,finegrained span supervision,target task capability of finegrained span supervision,0.6544555425643921
translation,462,16,results,finegrained span supervision,to better learn,syntactic and lexical information,finegrained span supervision to better learn syntactic and lexical information,0.6287054419517517
translation,462,16,results,results,demonstrate,target task,results demonstrate target task,0.5463055372238159
translation,462,20,results,8 % improvement,in,wikiqa,8 % improvement in wikiqa,0.5358720421791077
translation,462,20,results,1 % improevement,in,se-meval,1 % improevement in se-meval,0.6628512144088745
translation,462,20,results,results,show,8 % improvement,results show 8 % improvement,0.6785626411437988
translation,462,20,results,results,show,1 % improevement,results show 1 % improevement,0.7086116075515747
translation,462,72,results,second ranking system,in,semeval - 2016,second ranking system in semeval - 2016,0.5416143536567688
translation,462,72,results,outperforms,has,second ranking system,outperforms has second ranking system,0.6177781224250793
translation,462,72,results,1 % behind,has,first ranking system,1 % behind has first ranking system,0.5878503918647766
translation,462,72,results,results,has,outperforms,results has outperforms,0.6657275557518005
translation,462,75,results,highest - ranking system,in,wikiqa,highest - ranking system in wikiqa,0.5094749927520752
translation,462,75,results,highest - ranking system,by,more than 8 %,highest - ranking system by more than 8 %,0.5686537027359009
translation,462,75,results,best system,in,semeval - 2016,best system in semeval - 2016,0.5464296340942383
translation,462,75,results,best system,by,1 %,best system by 1 %,0.6047290563583374
translation,462,75,results,semeval - 2016,by,1 %,semeval - 2016 by 1 %,0.6316714882850647
translation,462,75,results,outperforms,has,highest - ranking system,outperforms has highest - ranking system,0.5934069752693176
translation,462,90,results,attention map,of,squad - pretrained model,attention map of squad - pretrained model,0.544025719165802
translation,462,90,results,squad - pretrained model,is,more sparse,squad - pretrained model is more sparse,0.5371875166893005
translation,462,90,results,results,see that,attention map,results see that attention map,0.6663457751274109
translation,463,126,baselines,qa - based model,retrieves,related posts,qa - based model retrieves related posts,0.7847356200218201
translation,463,126,baselines,related posts,in,discussion forum,related posts in discussion forum,0.48854342103004456
translation,463,126,baselines,discussion forum,for,each question,discussion forum for each question,0.6289160847663879
translation,463,126,baselines,languagemodeling baseline,examines,how well,languagemodeling baseline examines how well,0.5585549473762512
translation,463,126,baselines,languagemodeling baseline,examines,fine-tuned version,languagemodeling baseline examines fine-tuned version,0.5476364493370056
translation,463,126,baselines,pre-trained language models,do at directly producing,answers,pre-trained language models do at directly producing answers,0.7138526439666748
translation,463,126,baselines,fine-tuned version,of,language -model baseline,fine-tuned version of language -model baseline,0.5304036140441895
translation,463,126,baselines,how well,has,pre-trained language models,how well has pre-trained language models,0.5018242001533508
translation,463,126,baselines,baselines,explore,three baseline models,baselines explore three baseline models,0.6528985500335693
translation,463,153,results,best-performing automatic system,is,meaningfully behind,best-performing automatic system is meaningfully behind,0.576322615146637
translation,463,153,results,best-performing automatic system,still,meaningfully behind,best-performing automatic system still meaningfully behind,0.7003905177116394
translation,463,153,results,human performance,in,all metrics,human performance in all metrics,0.48302099108695984
translation,463,153,results,meaningfully behind,has,human performance,meaningfully behind has human performance,0.5830897688865662
translation,463,153,results,results,see that,best-performing automatic system,results see that best-performing automatic system,0.6820761561393738
translation,463,155,results,gpt - 2,has,without fine-tuning,gpt - 2 has without fine-tuning,0.589280903339386
translation,463,155,results,without fine-tuning,has,outperforms,without fine-tuning has outperforms,0.6230424046516418
translation,463,155,results,outperforms,has,baseline qa implementation,outperforms has baseline qa implementation,0.5841710567474365
translation,463,155,results,2,has,outperforms,2 has outperforms,0.6428407430648804
translation,463,155,results,results,see that,gpt - 2,results see that gpt - 2,0.632522463798523
translation,464,72,experiments,subtask a,achieve,good results,subtask a achieve good results,0.6733894944190979
translation,464,72,experiments,good results,ranked,4 th out of 12 teams,good results ranked 4 th out of 12 teams,0.6898290514945984
translation,464,73,experiments,subtask b,apply,simple approach,subtask b apply simple approach,0.6740809082984924
translation,464,73,experiments,result,is,reasonable,result is reasonable,0.6331623196601868
translation,464,73,experiments,subtask b,has,result,subtask b has result,0.6112905740737915
translation,464,73,experiments,simple approach,has,result,simple approach has result,0.5966346263885498
translation,464,84,experiments,our system,achieves,reasonable performance,our system achieves reasonable performance,0.6810755729675293
translation,464,84,experiments,reasonable performance,on,yes and unsure classes,reasonable performance on yes and unsure classes,0.5239312052726746
translation,464,84,experiments,no capability,to capture,no class,no capability to capture no class,0.6509876847267151
translation,464,19,model,several different linguistic features,from,semantic textual similarity ( sts ) system,several different linguistic features from semantic textual similarity ( sts ) system,0.5522786378860474
translation,464,90,model,supervised system,considers,multiple linguistic features,supervised system considers multiple linguistic features,0.639989972114563
translation,464,90,model,multiple linguistic features,such as,lexical,multiple linguistic features such as lexical,0.6764927506446838
translation,464,90,model,multiple linguistic features,such as,string,multiple linguistic features such as string,0.6660658717155457
translation,464,90,model,multiple linguistic features,such as,some task -specific features,multiple linguistic features such as some task -specific features,0.6508046388626099
translation,464,90,model,model,present,supervised system,model present supervised system,0.7227314710617065
translation,464,78,results,evaluation,on,"3 - classes ( good , bad , and potential )","evaluation on 3 - classes ( good , bad , and potential )",0.519168496131897
translation,464,78,results,our system,dramatically penalized by,low performance,our system dramatically penalized by low performance,0.7186917662620544
translation,464,78,results,low performance,on detecting,bad comments,low performance on detecting bad comments,0.6749811172485352
translation,464,78,results,evaluation,has,our system,evaluation has our system,0.5943111181259155
translation,464,78,results,"3 - classes ( good , bad , and potential )",has,our system,"3 - classes ( good , bad , and potential ) has our system",0.5950579047203064
translation,464,78,results,results,for,evaluation,results for evaluation,0.6358369588851929
translation,464,83,results,evaluation,on,"4 - classes ( good , bad , dialog and po-tential )","evaluation on 4 - classes ( good , bad , dialog and po-tential )",0.5341797471046448
translation,464,83,results,our system performance,rises,significantly,our system performance rises significantly,0.716159999370575
translation,464,83,results,our system,shows,good capability,our system shows good capability,0.6840243935585022
translation,464,83,results,good capability,to distinguish between,dialog and other comments,good capability to distinguish between dialog and other comments,0.6540408730506897
translation,464,83,results,evaluation,has,our system performance,evaluation has our system performance,0.5560169816017151
translation,464,83,results,"4 - classes ( good , bad , dialog and po-tential )",has,our system performance,"4 - classes ( good , bad , dialog and po-tential ) has our system performance",0.5850510597229004
translation,465,18,model,novel neural - network model,integrates,notion of expected value of perfect information,novel neural - network model integrates notion of expected value of perfect information,0.6077232360839844
translation,465,18,model,notion of expected value of perfect information,has,),notion of expected value of perfect information has ),0.5776487588882446
translation,465,18,model,model,integrates,notion of expected value of perfect information,model integrates notion of expected value of perfect information,0.6221692562103271
translation,465,19,model,novel dataset,derived from,stackexchange,novel dataset derived from stackexchange,0.6752313375473022
translation,465,19,model,novel dataset,to learn,model,novel dataset to learn model,0.6228769421577454
translation,465,19,model,model,to ask,clarifying questions,model to ask clarifying questions,0.7042540311813354
translation,465,19,model,clarifying questions,by looking at,types of questions people,clarifying questions by looking at types of questions people,0.6616300940513611
translation,465,19,model,types of questions people,has,ask ( ?4.1 ),types of questions people has ask ( ?4.1 ),0.5190755128860474
translation,465,117,model,model,for generating,clarification question,model for generating clarification question,0.7308523058891296
translation,465,117,model,one word at a time,given,words,one word at a time given words,0.7356945872306824
translation,465,117,model,clarification question,has,one word at a time,clarification question has one word at a time,0.5933462381362915
translation,465,117,model,model,propose,model,model propose model,0.6740307211875916
translation,465,117,model,model,for generating,clarification question,model for generating clarification question,0.7308523058891296
translation,465,119,model,question generation model,decide,each time step,question generation model decide each time step,0.7203584313392639
translation,465,119,model,question generation model,decide,topic specific word,question generation model decide topic specific word,0.6851287484169006
translation,465,119,model,question generation model,at,each time step,question generation model at each time step,0.5502421855926514
translation,465,119,model,topic specific word,retrieved from,current post,topic specific word retrieved from current post,0.5260160565376282
translation,465,119,model,topic specific word,incorporating,template - based method,topic specific word incorporating template - based method,0.7440255284309387
translation,465,119,model,template - based method,into,more general neural network framework,template - based method into more general neural network framework,0.6250209212303162
translation,465,119,model,model,build,question generation model,model build question generation model,0.72672438621521
translation,465,30,results,significant improvements,from using,evpi formalism,significant improvements from using evpi formalism,0.7169533371925354
translation,465,30,results,evpi formalism,over,standard feedforward network architectures,evpi formalism over standard feedforward network architectures,0.6345482468605042
translation,465,30,results,evpi formalism,over,bag-ofngrams baselines,evpi formalism over bag-ofngrams baselines,0.6395127773284912
translation,465,30,results,results,show,significant improvements,results show significant improvements,0.6506668925285339
translation,465,93,results,outperforms,by,few percentage points,outperforms by few percentage points,0.6279995441436768
translation,465,93,results,outperforms,at least,few percentage points,outperforms at least few percentage points,0.6077090501785278
translation,465,93,results,all the baselines,by,few percentage points,all the baselines by few percentage points,0.579271674156189
translation,465,93,results,all the baselines,at least,few percentage points,all the baselines at least few percentage points,0.5654760599136353
translation,465,93,results,all the evaluation metrics,has,evpi,all the evaluation metrics has evpi,0.5814609527587891
translation,465,93,results,evpi,has,outperforms,evpi has outperforms,0.6507065892219543
translation,465,93,results,outperforms,has,all the baselines,outperforms has all the baselines,0.596747100353241
translation,465,93,results,results,see that,all the evaluation metrics,results see that all the evaluation metrics,0.6070544123649597
translation,465,93,results,results,for,all the evaluation metrics,results for all the evaluation metrics,0.5729387402534485
translation,465,94,results,final performance,of,51 % recall,final performance of 51 % recall,0.5507227182388306
translation,465,94,results,51 % recall,at,3,51 % recall at 3,0.6125923991203308
translation,465,94,results,51 % recall,in,  hard   setting,51 % recall in   hard   setting,0.5163472890853882
translation,465,94,results,  hard   setting,is,encouraging,  hard   setting is encouraging,0.6046183705329895
translation,465,94,results,results,has,final performance,results has final performance,0.5871657133102417
translation,466,128,baselines,baselines,experiment,emr - bigru and emr - transformer,baselines experiment emr - bigru and emr - transformer,0.7080774307250977
translation,466,132,baselines,baselines,has,lifo ( last - in first - out ),baselines has lifo ( last - in first - out ),0.5713182687759399
translation,466,135,baselines,baselines,has,emr -independent,baselines has emr -independent,0.5707287192344666
translation,466,136,baselines,baseline emr,learns,importance,baseline emr learns importance,0.6406294703483582
translation,466,136,baselines,importance,of,each memory entry,importance of each memory entry,0.609126091003418
translation,466,137,baselines,baselines,has,emr - bigru,baselines has emr - bigru,0.5717015266418457
translation,466,138,baselines,emr,implemented using,bigru,emr implemented using bigru,0.6354990005493164
translation,466,138,baselines,bigru,considers,relative importance,bigru considers relative importance,0.7025029063224792
translation,466,138,baselines,relative importance,of,each memory entry,relative importance of each memory entry,0.6041451692581177
translation,466,138,baselines,each memory entry,to,neighbors,each memory entry to neighbors,0.5878379940986633
translation,466,138,baselines,neighbors,when learning,memory replacement policy,neighbors when learning memory replacement policy,0.682512104511261
translation,466,138,baselines,baselines,has,emr,baselines has emr,0.566661536693573
translation,466,139,baselines,baselines,has,emr - transformer,baselines has emr - transformer,0.6116985082626343
translation,466,150,baselines,"memn2n ( sukhbaatar et al. , 2015 )",consists of,embedded layer,"memn2n ( sukhbaatar et al. , 2015 ) consists of embedded layer",0.6153082847595215
translation,466,150,baselines,"memn2n ( sukhbaatar et al. , 2015 )",consists of,multihop mechanism,"memn2n ( sukhbaatar et al. , 2015 ) consists of multihop mechanism",0.6218137145042419
translation,466,150,baselines,multihop mechanism,extracts,high - level inference,multihop mechanism extracts high - level inference,0.6313446164131165
translation,466,151,experimental-setup,memn2n,with,position encoding representation,memn2n with position encoding representation,0.636339545249939
translation,466,151,experimental-setup,memn2n,with,3 hops,memn2n with 3 hops,0.7402304410934448
translation,466,151,experimental-setup,memn2n,with,adjacent weight tying,memn2n with adjacent weight tying,0.6988759636878967
translation,466,151,experimental-setup,experimental setup,use,memn2n,experimental setup use memn2n,0.627545177936554
translation,466,152,experimental-setup,dimension,of,memory representations,dimension of memory representations,0.55278480052948
translation,466,152,experimental-setup,memory representations,to,k = 20,memory representations to k = 20,0.5772157907485962
translation,466,152,experimental-setup,experimental setup,set,dimension,experimental setup set dimension,0.59503173828125
translation,466,155,experimental-setup,our model and the baselines,using,"adam optimizer ( kingma and ba , 2014 )","our model and the baselines using adam optimizer ( kingma and ba , 2014 )",0.6592258810997009
translation,466,155,experimental-setup,"adam optimizer ( kingma and ba , 2014 )",with,learning rate 0.0005,"adam optimizer ( kingma and ba , 2014 ) with learning rate 0.0005",0.5878836512565613
translation,466,155,experimental-setup,learning rate 0.0005,for,400 k steps,learning rate 0.0005 for 400 k steps,0.6025570034980774
translation,466,155,experimental-setup,experimental setup,train,our model and the baselines,experimental setup train our model and the baselines,0.6083682179450989
translation,466,170,experimental-setup,20 words,into,each memory cell,20 words into each memory cell,0.5621334910392761
translation,466,170,experimental-setup,each memory cell,using,gru,each memory cell using gru,0.6811832189559937
translation,466,170,experimental-setup,number of cells,to,20,number of cells to 20,0.6127655506134033
translation,466,170,experimental-setup,experimental setup,embed,20 words,experimental setup embed 20 words,0.7215728759765625
translation,466,170,experimental-setup,experimental setup,set,number of cells,experimental setup set number of cells,0.6191905736923218
translation,466,186,experimental-setup,efficient training,use,features,efficient training use features,0.6546400785446167
translation,466,186,experimental-setup,features,ResNet - 101 pretrained on,imagenet dataset,features ResNet - 101 pretrained on imagenet dataset,0.7423754930496216
translation,466,186,experimental-setup,experimental setup,For,efficient training,experimental setup For efficient training,0.5759395956993103
translation,466,187,experimental-setup,embedding subtitles and question - answering pairs,use,glove,embedding subtitles and question - answering pairs use glove,0.656281590461731
translation,466,187,experimental-setup,experimental setup,For,embedding subtitles and question - answering pairs,experimental setup For embedding subtitles and question - answering pairs,0.5662593841552734
translation,466,188,experimental-setup,training,restrict,number of memory entries,training restrict number of memory entries,0.6472923755645752
translation,466,188,experimental-setup,number of memory entries,for,our episodic reader,number of memory entries for our episodic reader,0.5661943554878235
translation,466,188,experimental-setup,our episodic reader,as,20,our episodic reader as 20,0.6244422793388367
translation,466,188,experimental-setup,each memory entry,contains,subtitle,each memory entry contains subtitle,0.705635130405426
translation,466,188,experimental-setup,each memory entry,encoding of,video frame,each memory entry encoding of video frame,0.6784901022911072
translation,466,188,experimental-setup,each memory entry,encoding of,subtitle,each memory entry encoding of subtitle,0.6935498714447021
translation,466,188,experimental-setup,experimental setup,For,training,experimental setup For training,0.5809131264686584
translation,466,189,experimental-setup,our model and the baseline models,using,adam optimizer,our model and the baseline models using adam optimizer,0.645601212978363
translation,466,189,experimental-setup,our model and the baseline models,with,initial learning rate,our model and the baseline models with initial learning rate,0.6006559133529663
translation,466,189,experimental-setup,adam optimizer,with,initial learning rate,adam optimizer with initial learning rate,0.5838022828102112
translation,466,189,experimental-setup,initial learning rate,of,0.0001,initial learning rate of 0.0001,0.5696305632591248
translation,466,189,experimental-setup,experimental setup,train,our model and the baseline models,experimental setup train our model and the baseline models,0.6191132068634033
translation,466,154,experiments,varying memory size,has,"5 , 10 and 15","varying memory size has 5 , 10 and 15",0.5792262554168701
translation,466,169,experiments,pre-trained model,from,deep bidirectional transformers ( bert ),pre-trained model from deep bidirectional transformers ( bert ),0.5747811198234558
translation,466,169,experiments,pre-trained model,is,current state - of - the - art model,pre-trained model is current state - of - the - art model,0.5356524586677551
translation,466,169,experiments,current state - of - the - art model,for,squad challenge,current state - of - the - art model for squad challenge,0.5829377174377441
translation,466,169,experiments,current state - of - the - art model,trains,several transformers,current state - of - the - art model trains several transformers,0.7449145317077637
translation,466,169,experiments,squad challenge,trains,several transformers,squad challenge trains several transformers,0.8014676570892334
translation,466,169,experiments,several transformers,for,pretraining,several transformers for pretraining,0.660771369934082
translation,466,169,experiments,several transformers,for predicting,indices,several transformers for predicting indices,0.7159358263015747
translation,466,169,experiments,indices,of,exact location,indices of exact location,0.5548644661903381
translation,466,169,experiments,exact location,of,answer,exact location of answer,0.5422882437705994
translation,466,185,experiments,qa module,use,multi-stream model,qa module use multi-stream model,0.6355786323547363
translation,466,185,experiments,multi-stream model,for,multi-modal video qa,multi-stream model for multi-modal video qa,0.6101032495498657
translation,466,190,experiments,reinforce,to train,policy,reinforce to train policy,0.7331315875053406
translation,466,6,model,novel end-to - end deep network model,for,reading comprehension,novel end-to - end deep network model for reading comprehension,0.5241609215736389
translation,466,6,model,novel end-to - end deep network model,refer to,episodic memory reader ( emr ),novel end-to - end deep network model refer to episodic memory reader ( emr ),0.6200332641601562
translation,466,6,model,episodic memory reader ( emr ),sequentially reads,input contexts,episodic memory reader ( emr ) sequentially reads input contexts,0.7500573396682739
translation,466,6,model,episodic memory reader ( emr ),replacing,memories,episodic memory reader ( emr ) replacing memories,0.6745969653129578
translation,466,6,model,input contexts,into,external memory,input contexts into external memory,0.5678921937942505
translation,466,6,model,model,propose,novel end-to - end deep network model,model propose novel end-to - end deep network model,0.6076908111572266
translation,466,6,model,model,replacing,memories,model replacing memories,0.7428645491600037
translation,466,7,model,rl agent,to replace,memory entry,rl agent to replace memory entry,0.6792110204696655
translation,466,7,model,rl agent,to replace,memory entries,rl agent to replace memory entries,0.6839275360107422
translation,466,7,model,memory entry,when,memory,memory entry when memory,0.6424233317375183
translation,466,7,model,memory entry,when,full,memory entry when full,0.667868435382843
translation,466,7,model,memory entry,is,full,memory entry is full,0.5678292512893677
translation,466,7,model,memory entry,to maximize,qa accuracy,memory entry to maximize qa accuracy,0.6353318691253662
translation,466,7,model,memory entry,encoding,external memory,memory entry encoding external memory,0.7588164806365967
translation,466,7,model,memory,is,full,memory is full,0.5869385600090027
translation,466,7,model,qa accuracy,at,future timepoint,qa accuracy at future timepoint,0.5503449440002441
translation,466,7,model,external memory,using,gru,external memory using gru,0.6538985371589661
translation,466,7,model,external memory,using,transformer architecture,external memory using transformer architecture,0.6842171549797058
translation,466,7,model,transformer architecture,to learn,representations,transformer architecture to learn representations,0.580179750919342
translation,466,7,model,representations,considers,relative importance,representations considers relative importance,0.7088488936424255
translation,466,7,model,relative importance,between,memory entries,relative importance between memory entries,0.6649814248085022
translation,466,7,model,model,train,rl agent,model train rl agent,0.6962015628814697
translation,466,7,model,model,encoding,external memory,model encoding external memory,0.7677904963493347
translation,466,22,model,episodic memory reader ( emr ),learns to retain,most important context vectors,episodic memory reader ( emr ) learns to retain most important context vectors,0.7617217302322388
translation,466,22,model,episodic memory reader ( emr ),replacing,memory entries,episodic memory reader ( emr ) replacing memory entries,0.6809216141700745
translation,466,22,model,most important context vectors,in,external memory,most important context vectors in external memory,0.4504566788673401
translation,466,22,model,memory entries,to maximize,accuracy,memory entries to maximize accuracy,0.6571178436279297
translation,466,22,model,accuracy,on,unseen question,accuracy on unseen question,0.5836510062217712
translation,466,22,model,unseen question,given at,future timestep,unseen question given at future timestep,0.6155056953430176
translation,466,24,model,memory management problem,as,learning problem,memory management problem as learning problem,0.478849321603775
translation,466,24,model,memory representation and the scheduling agent,using,reinforcement learning,memory representation and the scheduling agent using reinforcement learning,0.6281396746635437
translation,466,24,model,model,pose,memory management problem,model pose memory management problem,0.725534975528717
translation,466,24,model,model,train,memory representation and the scheduling agent,model train memory representation and the scheduling agent,0.6770229339599609
translation,466,33,model,novel end-to - end memoryaugmented neural architecture,for solving,qa,novel end-to - end memoryaugmented neural architecture for solving qa,0.6995815634727478
translation,466,33,model,novel end-to - end memoryaugmented neural architecture,train,scheduling agent,novel end-to - end memoryaugmented neural architecture train scheduling agent,0.6410454511642456
translation,466,33,model,scheduling agent,via,reinforcement learning,scheduling agent via reinforcement learning,0.5976604223251343
translation,466,33,model,reinforcement learning,to store,most important memory entries,reinforcement learning to store most important memory entries,0.729982852935791
translation,466,33,model,most important memory entries,for solving,future qa tasks,most important memory entries for solving future qa tasks,0.6344472765922546
translation,466,33,model,model,propose,novel end-to - end memoryaugmented neural architecture,model propose novel end-to - end memoryaugmented neural architecture,0.6650466322898865
translation,466,33,model,model,train,scheduling agent,model train scheduling agent,0.689103364944458
translation,466,129,model,rule- based memory scheduling policy,replaces,oldest memory entry,rule- based memory scheduling policy replaces oldest memory entry,0.7258803248405457
translation,466,140,model,emr,utilizes,transformer,emr utilizes transformer,0.6240537166595459
translation,466,140,model,transformer,to model,global relative importance,transformer to model global relative importance,0.7510784268379211
translation,466,140,model,global relative importance,between,memory entries,global relative importance between memory entries,0.6590368151664734
translation,466,140,model,model,has,emr,model has emr,0.5952630639076233
translation,466,157,results,our model ( emr - bigru and emr - transformer ),with,higher gain,our model ( emr - bigru and emr - transformer ) with higher gain,0.6444402933120728
translation,466,157,results,outperform,with,higher gain,outperform with higher gain,0.7580955624580383
translation,466,157,results,higher gain,in the case of,noisy dataset,higher gain in the case of noisy dataset,0.646659791469574
translation,466,157,results,our model ( emr - bigru and emr - transformer ),has,outperform,our model ( emr - bigru and emr - transformer ) has outperform,0.580884575843811
translation,466,157,results,outperform,has,baselines,outperform has baselines,0.6363358497619629
translation,466,157,results,results,has,our model ( emr - bigru and emr - transformer ),results has our model ( emr - bigru and emr - transformer ),0.5546958446502686
translation,466,158,results,results,has,emr - independent,results has emr - independent,0.5358244776725769
translation,466,174,results,emr models,consider,relative importance,emr models consider relative importance,0.672446072101593
translation,466,174,results,relative importance,between,memory entries ( emr - bigru and emr - transformer ),relative importance between memory entries ( emr - bigru and emr - transformer ),0.6328018307685852
translation,466,174,results,outperform,both,rulebased baselines,outperform both rulebased baselines,0.6719515919685364
translation,466,174,results,outperform,both,emr - independent,outperform both emr - independent,0.6674423217773438
translation,466,174,results,emr models,has,outperform,emr models has outperform,0.6047444939613342
translation,466,174,results,memory entries ( emr - bigru and emr - transformer ),has,outperform,memory entries ( emr - bigru and emr - transformer ) has outperform,0.5897724628448486
translation,466,174,results,results,see that,emr models,results see that emr models,0.6277713775634766
translation,466,175,results,lifo,performs,quite well,lifo performs quite well,0.6391808986663818
translation,466,175,results,quite well,unlike,other rule- based scheduling,quite well unlike other rule- based scheduling,0.6663357615470886
translation,466,175,results,other rule- based scheduling,where,most answers,other rule- based scheduling where most answers,0.6362267732620239
translation,466,175,results,results,has,interesting observation,results has interesting observation,0.5519416928291321
translation,466,197,results,significantly outperform,including,emr - independent,significantly outperform including emr - independent,0.725030243396759
translation,466,197,results,emr variants,has,significantly outperform,emr variants has significantly outperform,0.5901142954826355
translation,466,197,results,significantly outperform,has,all baselines,significantly outperform has all baselines,0.5758785009384155
translation,466,197,results,results,observe,emr variants,results observe emr variants,0.5918689370155334
translation,466,198,results,models,perform,well,models perform well,0.6802467703819275
translation,466,198,results,well,when,size of the memory,well when size of the memory,0.6805799603462219
translation,466,198,results,results,observe,models,results observe models,0.5709750056266785
translation,466,199,results,size of memory,is,small,size of memory is small,0.586712658405304
translation,466,199,results,gap,between,different models,gap between different models,0.7464235424995422
translation,466,199,results,different models,are,larger,different models are larger,0.6270250082015991
translation,466,199,results,larger,with,emr - transformer,larger with emr - transformer,0.7217205762863159
translation,466,199,results,emr - transformer,obtain - ing,best accuracy,emr - transformer obtain - ing best accuracy,0.7145320773124695
translation,466,199,results,size of memory,has,gap,size of memory has gap,0.590935230255127
translation,466,199,results,small,has,gap,small has gap,0.6324284076690674
translation,467,157,ablation-analysis,choice of section and alignment,are,important components,choice of section and alignment are important components,0.5780373215675354
translation,467,157,ablation-analysis,important components,of,our model,important components of our model,0.5702155828475952
translation,467,157,ablation-analysis,ablation analysis,shows,choice of section and alignment,ablation analysis shows choice of section and alignment,0.6521750092506409
translation,467,128,baselines,sw and sw +d baselines,match,bag of words,sw and sw +d baselines match bag of words,0.7042281627655029
translation,467,128,baselines,bag of words,constructed from,question and the answer answer candidate,bag of words constructed from question and the answer answer candidate,0.70620197057724
translation,467,128,baselines,question and the answer answer candidate,to,retrieved document,question and the answer answer candidate to retrieved document,0.5816172957420349
translation,467,128,baselines,baselines,has,sw and sw +d baselines,baselines has sw and sw +d baselines,0.5728996396064758
translation,467,129,baselines,rte baseline,uses,"textual entailment ( stern and dagan , 2012 )","rte baseline uses textual entailment ( stern and dagan , 2012 )",0.5343668460845947
translation,467,129,baselines,"textual entailment ( stern and dagan , 2012 )",to score,answer candidates,"textual entailment ( stern and dagan , 2012 ) to score answer candidates",0.6363060474395752
translation,467,129,baselines,baselines,has,rte baseline,baselines has rte baseline,0.6107080578804016
translation,467,64,hyperparameters,beam search,with,fixed beam size ( 5 ),beam search with fixed beam size ( 5 ),0.6780638694763184
translation,467,64,hyperparameters,fixed beam size ( 5 ),for,inference,fixed beam size ( 5 ) for inference,0.6202274560928345
translation,467,9,model,latent answer-entailing structures,that align,question - answers,latent answer-entailing structures that align question - answers,0.6169324517250061
translation,467,9,model,question - answers,with,appropriate snippets,question - answers with appropriate snippets,0.6358960270881653
translation,467,9,model,appropriate snippets,in,curriculum,appropriate snippets in curriculum,0.581257164478302
translation,467,24,model,joint model,trained in,max-margin fashion,joint model trained in max-margin fashion,0.7496684193611145
translation,467,24,model,max-margin fashion,using,latent structural svm ( lssvm ),max-margin fashion using latent structural svm ( lssvm ),0.6683779358863831
translation,467,24,model,latent structural svm ( lssvm ),where,answer-entailing structures,latent structural svm ( lssvm ) where answer-entailing structures,0.5812623500823975
translation,467,24,model,answer-entailing structures,are,latent,answer-entailing structures are latent,0.5769901275634766
translation,467,24,model,model,has,joint model,model has joint model,0.553891658782959
translation,467,148,results,all the lssvm models,have,better performance,all the lssvm models have better performance,0.5665178298950195
translation,467,148,results,better performance,than,all the baselines,better performance than all the baselines,0.5584839582443237
translation,467,149,results,improvement,handle,negation,improvement handle negation,0.7401005625724792
translation,467,149,results,negation,using,heuristic,negation using heuristic,0.7418169975280762
translation,467,149,results,mtlssvms,showed,boost,mtlssvms showed boost,0.6840956211090088
translation,467,149,results,boost,over,single task lssvm,boost over single task lssvm,0.6348816156387329
translation,467,149,results,results,found,improvement,results found improvement,0.7216019630432129
translation,467,150,results,qtype classification scheme,found to work better,qword classification,qtype classification scheme found to work better qword classification,0.6700909733772278
translation,467,150,results,qword classification,classifies,questions,qword classification classifies questions,0.7657298445701599
translation,467,150,results,questions,based on,question word,questions based on question word,0.6448321342468262
translation,467,150,results,results,has,qtype classification scheme,results has qtype classification scheme,0.5447743535041809
translation,468,113,experiments,higher correlation,with,human evaluation,higher correlation with human evaluation,0.6391621828079224
translation,468,113,experiments,lowest correlation,with,clqa accuracy,lowest correlation with clqa accuracy,0.6293882727622986
translation,468,113,experiments,ribes,has,lowest correlation,ribes has lowest correlation,0.5980870723724365
translation,468,89,results,commercial systems,with,gt,commercial systems with gt,0.7254809737205505
translation,468,89,results,commercial systems,with,gt,commercial systems with gt,0.7254809737205505
translation,468,89,results,gt,being,2nd best,gt being 2nd best,0.6394346356391907
translation,468,89,results,gt,on,ribes,gt on ribes,0.6992206573486328
translation,468,89,results,2nd best,on,bleu and nist,2nd best on bleu and nist,0.4934857487678528
translation,468,89,results,yt,higher than,gt,yt higher than gt,0.6644851565361023
translation,468,89,results,yt,higher than,manual evaluation,yt higher than manual evaluation,0.6685734391212463
translation,468,89,results,gt,on,ribes,gt on ribes,0.6992206573486328
translation,468,89,results,gt,on,manual evaluation,gt on manual evaluation,0.5487743616104126
translation,468,95,results,accuracy,of,or set,accuracy of or set,0.6300703287124634
translation,468,95,results,accuracy,about,53 %,accuracy about 53 %,0.622562050819397
translation,468,95,results,or set,is,53 %,or set is 53 %,0.6880356073379517
translation,468,95,results,results,see that,accuracy,results see that accuracy,0.6826137900352478
translation,468,96,results,accuracy,of,ht set,accuracy of ht set,0.566169798374176
translation,468,96,results,highest,of,translated data sets,highest of translated data sets,0.6035800576210022
translation,468,96,results,results,has,accuracy,results has accuracy,0.5888755321502686
translation,469,4,model,model,introduce,question - answer meaning representations ( qamrs ),model introduce question - answer meaning representations ( qamrs ),0.6585257053375244
translation,469,21,model,model,introduce,question - answer meaning representations ( qamrs ),model introduce question - answer meaning representations ( qamrs ),0.6585257053375244
translation,469,32,model,work,between,multiple annotators,work between multiple annotators,0.5719196200370789
translation,469,32,model,multiple annotators,in,novel crowdsourcing scheme,multiple annotators in novel crowdsourcing scheme,0.4974772036075592
translation,469,32,model,dataset,of,"over 100,000 qa pairs","dataset of over 100,000 qa pairs",0.5798580050468445
translation,469,32,model,"over 100,000 qa pairs",for,"5,000 sentences","over 100,000 qa pairs for 5,000 sentences",0.6018718481063843
translation,469,32,model,"5,000 sentences",in,newswire and wikipedia domains,"5,000 sentences in newswire and wikipedia domains",0.49875760078430176
translation,469,32,model,model,distribute,work,model distribute work,0.600874662399292
translation,469,74,results,single annotators,cover,over 60 %,single annotators cover over 60 %,0.6654351949691772
translation,469,74,results,over 60 %,of,relationships,over 60 % of relationships,0.6430807709693909
translation,469,74,results,coverage,quickly increases with,number of annotators,coverage quickly increases with number of annotators,0.6010604500770569
translation,469,74,results,reaching over 90 %,with,all five,reaching over 90 % with all five,0.727137565612793
translation,469,74,results,results,has,single annotators,results has single annotators,0.49997130036354065
translation,469,102,results,training and testing,with,squad metrics,training and testing with squad metrics,0.6463109254837036
translation,469,102,results,squad metrics,on,qamr,squad metrics on qamr,0.5501697659492493
translation,469,102,results,model,achieves,70.8 % exact match,model achieves 70.8 % exact match,0.6548457145690918
translation,469,102,results,model,achieves,79.7 % f1 score,model achieves 79.7 % f1 score,0.6343940496444702
translation,469,102,results,training and testing,has,model,training and testing has model,0.5797998309135437
translation,469,102,results,squad metrics,has,model,squad metrics has model,0.5750896334648132
translation,469,102,results,qamr,has,model,qamr has model,0.6051715016365051
translation,469,102,results,results,has,training and testing,results has training and testing,0.5415257215499878
translation,469,103,results,performance,to,75.7 % exact match,performance to 75.7 % exact match,0.5157819390296936
translation,469,103,results,performance,to,83.9 % f1,performance to 83.9 % f1,0.5424162149429321
translation,469,103,results,83.9 % f1,by pooling,our training set,83.9 % f1 by pooling our training set,0.6993914246559143
translation,469,103,results,our training set,with,squad training data,our training set with squad training data,0.6464683413505554
translation,469,103,results,results,improve,performance,results improve performance,0.6927347779273987
translation,470,136,ablation-analysis,increases,from,38.33,increases from 38.33,0.5705631971359253
translation,470,136,ablation-analysis,38.33,to,39.98,38.33 to 39.98,0.5882222652435303
translation,470,136,ablation-analysis,machine translation evaluation ( mte ) features,has,map,machine translation evaluation ( mte ) features has map,0.5541735291481018
translation,470,136,ablation-analysis,map,has,increases,map has increases,0.5971288084983826
translation,470,136,ablation-analysis,ablation analysis,add,machine translation evaluation ( mte ) features,ablation analysis add machine translation evaluation ( mte ) features,0.5990537405014038
translation,470,59,experimental-setup,madamira toolkit,for segmenting,arabic texts,madamira toolkit for segmenting arabic texts,0.7682064175605774
translation,470,59,experimental-setup,experimental setup,used,madamira toolkit,experimental setup used madamira toolkit,0.5885112881660461
translation,470,96,experimental-setup,c parameter,of,svm,c parameter of svm,0.6237133741378784
translation,470,96,experimental-setup,c parameter,set to,1,c parameter set to 1,0.7540761828422546
translation,470,96,experimental-setup,experimental setup,has,c parameter,experimental setup has c parameter,0.5195258259773254
translation,470,49,experiments,arabic instances,extracted from,three medical fora,arabic instances extracted from three medical fora,0.5591809153556824
translation,470,135,experiments,cont 1,using,embedding features,cont 1 using embedding features,0.6877183318138123
translation,470,135,experiments,embedding features,from,"belinkov et al. , 2015 )","embedding features from belinkov et al. , 2015 )",0.49850818514823914
translation,470,135,experiments,embedding features,from,average system,embedding features from average system,0.5522668957710266
translation,470,135,experiments,embedding features,is,average system,embedding features is average system,0.5625936388969421
translation,470,135,experiments,task d,has,cont 1,task d has cont 1,0.6007580757141113
translation,470,7,model,convkn,combines,convolutional tree kernels,convkn combines convolutional tree kernels,0.6866350173950195
translation,470,7,model,convkn,combines,additional manually designed features,convkn combines additional manually designed features,0.6939023733139038
translation,470,7,model,convolutional tree kernels,with,convolutional neural networks,convolutional tree kernels with convolutional neural networks,0.5778719782829285
translation,470,7,model,additional manually designed features,including,text similarity,additional manually designed features including text similarity,0.7145130634307861
translation,470,7,model,additional manually designed features,including,thread specific features,additional manually designed features including thread specific features,0.6974707841873169
translation,470,7,model,model,has,convkn,model has convkn,0.6358674168586731
translation,470,8,model,tree kernels,to,syntactic trees,tree kernels to syntactic trees,0.5109112858772278
translation,470,8,model,tree kernels,for,reranking task,tree kernels for reranking task,0.5743882060050964
translation,470,8,model,syntactic trees,of,arabic sentences,syntactic trees of arabic sentences,0.5209710597991943
translation,470,8,model,syntactic trees,for,reranking task,syntactic trees for reranking task,0.5557003021240234
translation,470,91,model,svm,operating on,two kernels,svm operating on two kernels,0.7571462988853455
translation,470,91,model,tree kernel,applied to,structures,tree kernel applied to structures,0.6741698384284973
translation,470,91,model,structures,without,question and focus classification,structures without question and focus classification,0.7536845207214355
translation,470,91,model,polynomial kernel,applied to,feature vector,polynomial kernel applied to feature vector,0.6873530149459839
translation,470,91,model,feature vector,is,concatenation,feature vector is concatenation,0.5679178237915039
translation,470,91,model,concatenation,of,feature vector,concatenation of feature vector,0.5742021799087524
translation,470,91,model,question and answer embeddings,learned on,training set,question and answer embeddings learned on training set,0.6323380470275879
translation,470,91,model,training set,by,convolutional neural network ( cnn ),training set by convolutional neural network ( cnn ),0.5423339009284973
translation,470,91,model,polynomial kernel,has,of degree 3,polynomial kernel has of degree 3,0.547015905380249
translation,470,104,model,svm,operating on,two kernels,svm operating on two kernels,0.7571462988853455
translation,470,104,model,syntactic tree kernel ( sst ),to,constituency trees,syntactic tree kernel ( sst ) to constituency trees,0.5491007566452026
translation,470,104,model,constituency trees,of,question texts,constituency trees of question texts,0.5672622323036194
translation,470,104,model,linear kernel,applied to,features,linear kernel applied to features,0.6775177717208862
translation,470,9,results,our approaches,obtained,second best results,our approaches obtained second best results,0.6311253905296326
translation,470,9,results,second best results,in,three out of four tasks,second best results in three out of four tasks,0.4962744116783142
translation,470,9,results,results,has,our approaches,results has our approaches,0.5885946750640869
translation,470,24,results,all our three systems,using,relational models,all our three systems using relational models,0.697165846824646
translation,470,24,results,all our three systems,achieved,second official position,all our three systems achieved second official position,0.6760204434394836
translation,470,24,results,relational models,based on,tree kernels,relational models based on tree kernels,0.5831186175346375
translation,470,24,results,relational models,achieved,second official position,relational models achieved second official position,0.6496258974075317
translation,470,24,results,results,has,all our three systems,results has all our three systems,0.5726273059844971
translation,470,128,results,second position,for,"tasks a , b , and d.","second position for tasks a , b , and d.",0.5666574835777283
translation,470,128,results,tree kernels,give,no major boost,tree kernels give no major boost,0.6453034281730652
translation,470,128,results,third position,on,test set,third position on test set,0.5657594799995422
translation,470,128,results,"tasks a , b , and d.",has,tree kernels,"tasks a , b , and d. has tree kernels",0.5546754598617554
translation,470,128,results,results,achieved,second position,results achieved second position,0.7042490839958191
translation,470,129,results,joint model cont 1,run on top of,our primary system,joint model cont 1 run on top of our primary system,0.7652509212493896
translation,470,129,results,joint model cont 1,able to improve,more than one point,joint model cont 1 able to improve more than one point,0.7912353277206421
translation,470,129,results,results,has,joint model cont 1,results has joint model cont 1,0.5812633633613586
translation,470,134,results,our feature vectors only,results in,average performance,our feature vectors only results in average performance,0.5724126696586609
translation,470,134,results,average performance,in,challenge,average performance in challenge,0.527926504611969
translation,470,134,results,results,use of,our feature vectors only,results use of our feature vectors only,0.6442641615867615
translation,470,139,results,primary system,combines,tree kernels,primary system combines tree kernels,0.7689935564994812
translation,470,139,results,tree kernels,with,embedding features,tree kernels with embedding features,0.6123566627502441
translation,470,139,results,embedding features,improving them by,7 absolute points,embedding features improving them by 7 absolute points,0.6280913352966309
translation,470,139,results,second position,with,map,second position with map,0.673435389995575
translation,470,139,results,map,of,45.50,map of 45.50,0.6053062677383423
translation,470,139,results,results,has,primary system,results has primary system,0.5567602515220642
translation,471,6,baselines,evinets,scores,candidate answer entities,evinets scores candidate answer entities,0.7467324137687683
translation,471,6,baselines,candidate answer entities,by combining,available supporting evidence,candidate answer entities by combining available supporting evidence,0.6727993488311768
translation,471,6,baselines,available supporting evidence,e.g.,structured knowledge bases,available supporting evidence e.g. structured knowledge bases,0.6807678937911987
translation,471,6,baselines,available supporting evidence,e.g.,unstructured text documents,available supporting evidence e.g. unstructured text documents,0.6361637115478516
translation,471,6,baselines,baselines,has,evinets,baselines has evinets,0.5943135023117065
translation,471,23,baselines,question,set of,relevant pieces of information,question set of relevant pieces of information,0.7095357179641724
translation,471,23,baselines,relevant pieces of information,e.g.,sentences,relevant pieces of information e.g. sentences,0.757422149181366
translation,471,23,baselines,relevant pieces of information,extracts,mentioned entities,relevant pieces of information extracts mentioned entities,0.6090532541275024
translation,471,23,baselines,sentences,from,corpora,sentences from corpora,0.5481934547424316
translation,471,23,baselines,sentences,from,knowledge base triples,sentences from knowledge base triples,0.5041571855545044
translation,471,23,baselines,mentioned entities,as,candidate answers,mentioned entities as candidate answers,0.5056549310684204
translation,471,23,baselines,question,has,evinets,question has evinets,0.6534221172332764
translation,471,55,baselines,askmsr,select,best answer,askmsr select best answer,0.7279726266860962
translation,471,55,baselines,askmsr +,select,best answer,askmsr + select best answer,0.7332468628883362
translation,471,55,baselines,best answer,based on,frequency of entity mentions,best answer based on frequency of entity mentions,0.6049864292144775
translation,471,56,baselines,kbqa,identify,possible topic entities,kbqa identify possible topic entities,0.5857123732566833
translation,471,56,baselines,kbqa,select,answer,kbqa select answer,0.7643427848815918
translation,471,56,baselines,"aqqu ( bast and haussmann , 2015 )",identify,possible topic entities,"aqqu ( bast and haussmann , 2015 ) identify possible topic entities",0.6028968095779419
translation,471,56,baselines,"aqqu ( bast and haussmann , 2015 )",select,answer,"aqqu ( bast and haussmann , 2015 ) select answer",0.7142211198806763
translation,471,56,baselines,possible topic entities,of,question,possible topic entities of question,0.5731746554374695
translation,471,56,baselines,answer,from,candidates,answer from candidates,0.5917008519172668
translation,471,56,baselines,baselines,has,kbqa,baselines has kbqa,0.5992349982261658
translation,471,48,hyperparameters,end-to - end,by optimizing,cross entropy loss function,end-to - end by optimizing cross entropy loss function,0.7110493183135986
translation,471,48,hyperparameters,cross entropy loss function,using,adam algorithm,cross entropy loss function using adam algorithm,0.6243690252304077
translation,471,48,hyperparameters,hyperparameters,trained,end-to - end,hyperparameters trained end-to - end,0.7198280096054077
translation,471,52,hyperparameters,embeddings,initialized with,300 - dimensional vectors,embeddings initialized with 300 - dimensional vectors,0.7790859937667847
translation,471,52,hyperparameters,300 - dimensional vectors,pre-trained with,glove,300 - dimensional vectors pre-trained with glove,0.7786360383033752
translation,471,52,hyperparameters,hyperparameters,has,embeddings,hyperparameters has embeddings,0.548468291759491
translation,471,53,hyperparameters,embeddings,for,multi-word entity names,embeddings for multi-word entity names,0.5472970008850098
translation,471,53,hyperparameters,embeddings,obtained by,averaging the word vectors,embeddings obtained by averaging the word vectors,0.6140958666801453
translation,471,53,hyperparameters,averaging the word vectors,of,constituent words,averaging the word vectors of constituent words,0.5644852519035339
translation,471,53,hyperparameters,hyperparameters,has,embeddings,hyperparameters has embeddings,0.548468291759491
translation,471,5,model,model,proposes,evinets,model proposes evinets,0.7726816534996033
translation,471,7,model,evinets,represents,each piece,evinets represents each piece,0.6670421361923218
translation,471,7,model,evinets,aggregates,support,evinets aggregates support,0.6789409518241882
translation,471,7,model,each piece,of,evidence,each piece of evidence,0.6142100691795349
translation,471,7,model,each piece,scores,relevance,each piece scores relevance,0.7400166392326355
translation,471,7,model,evidence,with,dense embeddings vector,evidence with dense embeddings vector,0.6211175322532654
translation,471,7,model,relevance,to,question,relevance to question,0.5386306643486023
translation,471,7,model,support,for,each candidate,support for each candidate,0.6582888960838318
translation,471,7,model,each candidate,to predict,final scores,each candidate to predict final scores,0.7217958569526672
translation,471,7,model,model,has,evinets,model has evinets,0.6324306130409241
translation,471,8,model,variety of models,for,semantic similarity scoring,variety of models for semantic similarity scoring,0.5382260680198669
translation,471,8,model,variety of models,for,information aggregation,variety of models for information aggregation,0.5446640253067017
translation,471,22,model,model,introduce,evinets,model introduce evinets,0.7172092795372009
translation,471,64,results,evinets,achieves,competitive results,evinets achieves competitive results,0.6912174820899963
translation,471,64,results,evinets,beating,kv memn2n,evinets beating kv memn2n,0.5844215750694275
translation,471,64,results,competitive results,on,dataset,competitive results on dataset,0.5129984021186829
translation,471,64,results,kv memn2n,by,13 %,kv memn2n by 13 %,0.6209316849708557
translation,471,64,results,13 %,in,f1 score,13 % in f1 score,0.55870521068573
translation,471,64,results,results,has,evinets,results has evinets,0.5390831232070923
translation,471,82,results,textual data,helps,significantly,textual data helps significantly,0.6483592391014099
translation,471,82,results,text2kb,improves,precision,text2kb improves precision,0.6977280974388123
translation,471,82,results,precision,to,0.17,precision to 0.17,0.533778190612793
translation,471,82,results,candidate entities,by,popularity,candidate entities by popularity,0.5306278467178345
translation,471,82,results,popularity,in,retrieved documents,popularity in retrieved documents,0.49742746353149414
translation,471,82,results,results,Adding,textual data,results Adding textual data,0.6382017135620117
translation,471,83,results,text,along with,kb evidence,text along with kb evidence,0.5440450310707092
translation,471,83,results,kb evidence,gave,higher performance metrics,kb evidence gave higher performance metrics,0.6694570183753967
translation,471,83,results,boosting f1,from,0.271 to 0.291,boosting f1 from 0.271 to 0.291,0.5435842871665955
translation,471,83,results,higher performance metrics,has,boosting f1,higher performance metrics has boosting f1,0.5569633841514587
translation,471,83,results,results,Using,text,results Using text,0.6280155181884766
translation,471,84,results,evinets,beating,askmsr,evinets beating askmsr,0.6091649532318115
translation,471,84,results,evinets,beating,kv memn2n,evinets beating kv memn2n,0.5844215750694275
translation,471,84,results,significantly improves,over,baseline approaches,significantly improves over baseline approaches,0.6655810475349426
translation,471,84,results,askmsr,by,28 %,askmsr by 28 %,0.6544826030731201
translation,471,84,results,kv memn2n,by,almost 80 %,kv memn2n by almost 80 %,0.5977262258529663
translation,471,84,results,almost 80 %,in,f1 score,almost 80 % in f1 score,0.5643244385719299
translation,471,84,results,evinets,has,significantly improves,evinets has significantly improves,0.6315935254096985
translation,471,84,results,results,has,evinets,results has evinets,0.5390831232070923
translation,472,8,ablation-analysis,performances,of,all systems,performances of all systems,0.5701912641525269
translation,472,8,ablation-analysis,performances,dropped by,around 30 points,performances dropped by around 30 points,0.7474913001060486
translation,472,8,ablation-analysis,all systems,dropped by,around 30 points,all systems dropped by around 30 points,0.7346672415733337
translation,472,8,ablation-analysis,cqa - b-2017 test data,has,performances,cqa - b-2017 test data has performances,0.5558791756629944
translation,472,8,ablation-analysis,ablation analysis,On,cqa - b-2017 test data,ablation analysis On cqa - b-2017 test data,0.5432936549186707
translation,472,7,results,best system,in,all measures,best system in all measures,0.5132501125335693
translation,472,7,results,best system,with,77.47 map and 80.57 accuracy,best system with 77.47 map and 80.57 accuracy,0.5908106565475464
translation,472,7,results,all measures,with,77.47 map and 80.57 accuracy,all measures with 77.47 map and 80.57 accuracy,0.6134356260299683
translation,472,7,results,cqa - b-2016 test data,has,our rqe system,cqa - b-2016 test data has our rqe system,0.6059743762016296
translation,472,7,results,our rqe system,has,outperformed,our rqe system has outperformed,0.605196475982666
translation,472,7,results,outperformed,has,best system,outperformed has best system,0.6538243293762207
translation,472,7,results,results,Tested on,cqa - b-2016 test data,results Tested on cqa - b-2016 test data,0.7162236571311951
translation,472,9,results,primary system,obtained,44.62 map,primary system obtained 44.62 map,0.5918143391609192
translation,472,9,results,primary system,obtained,67.27 accuracy,primary system obtained 67.27 accuracy,0.604789674282074
translation,472,9,results,primary system,obtained,47.25 f1 score,primary system obtained 47.25 f1 score,0.603844165802002
translation,472,9,results,results,has,primary system,results has primary system,0.5567602515220642
translation,472,10,results,best system,achieved,47.22 map,best system achieved 47.22 map,0.6850576400756836
translation,472,10,results,best system,achieved,42.37 f1 score,best system achieved 42.37 f1 score,0.6516415476799011
translation,472,10,results,cqa - b-2017,has,best system,cqa - b-2017 has best system,0.608199417591095
translation,472,10,results,results,has,cqa - b-2017,results has cqa - b-2017,0.5760726928710938
translation,472,11,results,our system,ranked,sixth,our system ranked sixth,0.699856698513031
translation,472,11,results,our system,ranked,third,our system ranked third,0.7445734739303589
translation,472,11,results,sixth,in terms of,map,sixth in terms of map,0.7501821517944336
translation,472,11,results,third,in terms of,f1,third in terms of f1,0.7387877106666565
translation,472,11,results,results,has,our system,results has our system,0.5954442024230957
translation,472,65,results,primary system,obtained,44.62 map,primary system obtained 44.62 map,0.5918143391609192
translation,472,65,results,primary system,ranked,sixth,primary system ranked sixth,0.7544353604316711
translation,472,65,results,sixth,over,13 participating teams,sixth over 13 participating teams,0.701331615447998
translation,472,65,results,results,has,primary system,results has primary system,0.5567602515220642
translation,472,68,results,best system,on,2016 test data,best system on 2016 test data,0.5227667689323425
translation,472,68,results,our results,has,outperformed,our results has outperformed,0.5834813117980957
translation,472,68,results,outperformed,has,best system,outperformed has best system,0.6538243293762207
translation,472,68,results,results,has,our results,results has our results,0.5639954209327698
translation,472,69,results,general drop,of,30 points,general drop of 30 points,0.5806331634521484
translation,472,69,results,30 points,on,performance,30 points on performance,0.5637109875679016
translation,472,69,results,30 points,observed,cqa - b-2017 test data,30 points observed cqa - b-2017 test data,0.6688500046730042
translation,472,69,results,performance,for,all systems,performance for all systems,0.625953197479248
translation,472,69,results,results,has,general drop,results has general drop,0.5422043204307556
translation,473,199,ablation-analysis,unsupervised pretraining,helps,shared encoder and decoder layers,unsupervised pretraining helps shared encoder and decoder layers,0.5374950170516968
translation,473,199,ablation-analysis,shared encoder and decoder layers,capture,higher - level languageindependent information,shared encoder and decoder layers capture higher - level languageindependent information,0.7246725559234619
translation,473,199,ablation-analysis,higher - level languageindependent information,giving,improvement,higher - level languageindependent information giving improvement,0.6618207097053528
translation,473,199,ablation-analysis,improvement,of,approximately 7,improvement of approximately 7,0.6471481919288635
translation,473,199,ablation-analysis,approximately 7,in,bleu - 4 scores,approximately 7 in bleu - 4 scores,0.5650609135627747
translation,473,199,ablation-analysis,ablation analysis,has,unsupervised pretraining,ablation analysis has unsupervised pretraining,0.5560685396194458
translation,473,89,baselines,baselines,has,subword embeddings,baselines has subword embeddings,0.5114806890487671
translation,473,161,baselines,clqg,is,main cross-lingual question generation model,clqg is main cross-lingual question generation model,0.527419924736023
translation,473,161,baselines,main cross-lingual question generation model,where,encoder and decoder layers,main cross-lingual question generation model where encoder and decoder layers,0.5749862194061279
translation,473,161,baselines,encoder and decoder layers,initialized in,unsupervised pretraining phase,encoder and decoder layers initialized in unsupervised pretraining phase,0.7042333483695984
translation,473,161,baselines,unsupervised pretraining phase,using,primary and secondary language monolingual corpora,unsupervised pretraining phase using primary and secondary language monolingual corpora,0.6345082521438599
translation,473,161,baselines,primary and secondary language monolingual corpora,followed by,joint supervised qg training,primary and secondary language monolingual corpora followed by joint supervised qg training,0.6388149857521057
translation,473,161,baselines,joint supervised qg training,using,qg datasets,joint supervised qg training using qg datasets,0.6429365873336792
translation,473,161,baselines,qg datasets,in,primary and secondary languages,qg datasets in primary and secondary languages,0.5015568137168884
translation,473,161,baselines,baselines,has,clqg,baselines has clqg,0.5806058645248413
translation,473,162,baselines,baselines,has,clqg + parallel,baselines has clqg + parallel,0.5946097373962402
translation,473,58,experimental-setup,monolingual corpora,available in,primary ( hindi / chinese ) and secondary ( english ) languages,monolingual corpora available in primary ( hindi / chinese ) and secondary ( english ) languages,0.6660604476928711
translation,473,58,experimental-setup,monolingual corpora,for,unsupervised pretraining,monolingual corpora for unsupervised pretraining,0.5714161992073059
translation,473,58,experimental-setup,experimental setup,use,monolingual corpora,experimental setup use monolingual corpora,0.5392030477523804
translation,473,59,experimental-setup,denoising autoencoders,along with,back - translation,denoising autoencoders along with back - translation,0.6038324236869812
translation,473,59,experimental-setup,denoising autoencoders,for,pretraining,denoising autoencoders for pretraining,0.5676261186599731
translation,473,59,experimental-setup,pretraining,has,language models,pretraining has language models,0.5497258305549622
translation,473,59,experimental-setup,language models,has,in both the primary and secondary languages,language models has in both the primary and secondary languages,0.5487050414085388
translation,473,59,experimental-setup,experimental setup,use,denoising autoencoders,experimental setup use denoising autoencoders,0.575493574142456
translation,473,59,experimental-setup,experimental setup,for,pretraining,experimental setup for pretraining,0.5743495225906372
translation,473,90,experimental-setup,data,using,bpe ( byte pair encoding ),data using bpe ( byte pair encoding ),0.6740267872810364
translation,473,90,experimental-setup,bpe ( byte pair encoding ),has,embeddings,bpe ( byte pair encoding ) has embeddings,0.5683388113975525
translation,473,90,experimental-setup,experimental setup,represent,data,experimental setup represent data,0.5775303840637207
translation,473,91,experimental-setup,bpe embeddings,for,unsupervised pretraining,bpe embeddings for unsupervised pretraining,0.599084198474884
translation,473,91,experimental-setup,bpe embeddings,for,supervised qg training phase,bpe embeddings for supervised qg training phase,0.5909493565559387
translation,473,91,experimental-setup,bpe embeddings,as,supervised qg training phase,bpe embeddings as supervised qg training phase,0.5205442905426025
translation,473,91,experimental-setup,experimental setup,use,bpe embeddings,experimental setup use bpe embeddings,0.5982599854469299
translation,473,130,experimental-setup,our model,in,tensorflow,our model in tensorflow,0.5319178700447083
translation,473,130,experimental-setup,experimental setup,implemented,our model,experimental setup implemented our model,0.7312729358673096
translation,473,132,experimental-setup,300 hidden units,for,each layer,300 hidden units for each layer,0.5528777241706848
translation,473,132,experimental-setup,each layer,of,transformer,each layer of transformer,0.6630942225456238
translation,473,132,experimental-setup,transformer,with,number of attention heads,transformer with number of attention heads,0.6515058279037476
translation,473,132,experimental-setup,number of attention heads,set to,6,number of attention heads set to 6,0.7147284150123596
translation,473,132,experimental-setup,experimental setup,used,300 hidden units,experimental setup used 300 hidden units,0.5864073634147644
translation,473,133,experimental-setup,size,of,bpe embeddings,size of bpe embeddings,0.5910364985466003
translation,473,133,experimental-setup,bpe embeddings,to,300,bpe embeddings to 300,0.6577852368354797
translation,473,133,experimental-setup,experimental setup,set,size,experimental setup set size,0.6679509878158569
translation,473,135,experimental-setup,residual dropout,set to,0.2,residual dropout set to 0.2,0.6702276468276978
translation,473,135,experimental-setup,0.2,to prevent,overfitting,0.2 to prevent overfitting,0.5513148307800293
translation,473,136,experimental-setup,unsupervised pretraining and supervised qg training stages,used,adam optimizer,unsupervised pretraining and supervised qg training stages used adam optimizer,0.573696494102478
translation,473,136,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,473,136,experimental-setup,adam optimizer,with,batch size,adam optimizer with batch size,0.606801450252533
translation,473,136,experimental-setup,learning rate,of,1e?5,learning rate of 1e?5,0.6333191394805908
translation,473,136,experimental-setup,batch size,of,64,batch size of 64,0.6741159558296204
translation,473,136,experimental-setup,experimental setup,During,unsupervised pretraining and supervised qg training stages,experimental setup During unsupervised pretraining and supervised qg training stages,0.6618251800537109
translation,473,143,experimental-setup,all the weights,including,bpe embeddings,all the weights including bpe embeddings,0.7135422229766846
translation,473,143,experimental-setup,all the weights,from,pretraining phase,all the weights from pretraining phase,0.560529887676239
translation,473,143,experimental-setup,experimental setup,initialize,all the weights,experimental setup initialize all the weights,0.8017153739929199
translation,473,153,experimental-setup,transformer model,using,qg dataset,transformer model using qg dataset,0.662784218788147
translation,473,153,experimental-setup,qg dataset,in,primary language,qg dataset in primary language,0.46926721930503845
translation,473,153,experimental-setup,experimental setup,train,transformer model,experimental setup train transformer model,0.6359142065048218
translation,473,142,experiments,6000 chinese sentence -question pairs,from,dureader dataset,6000 chinese sentence -question pairs from dureader dataset,0.5471901297569275
translation,473,142,experiments,6000 chinese sentence -question pairs,to train,supervised qg model,6000 chinese sentence -question pairs to train supervised qg model,0.6259585618972778
translation,473,142,experiments,supervised qg model,in,chinese,supervised qg model in chinese,0.5267661213874817
translation,473,9,model,unsupervised pretraining,of,language models,unsupervised pretraining of language models,0.49587351083755493
translation,473,9,model,language models,in both,primary and secondary languages,language models in both primary and secondary languages,0.6526362299919128
translation,473,9,model,joint supervised training,for,qg,joint supervised training for qg,0.652671217918396
translation,473,9,model,qg,in,both languages,qg in both languages,0.5674852132797241
translation,473,9,model,model,propose,cross-lingual qg model,model propose cross-lingual qg model,0.6116854548454285
translation,473,18,model,cross-lingual model,for leveraging,large question answering dataset,cross-lingual model for leveraging large question answering dataset,0.6673939824104309
translation,473,18,model,large question answering dataset,in,secondary language,large question answering dataset in secondary language,0.44176360964775085
translation,473,18,model,models,for,qg,models for qg,0.6539313197135925
translation,473,18,model,models,with,significantly smaller question answering dataset,models with significantly smaller question answering dataset,0.6148673892021179
translation,473,18,model,qg,with,significantly smaller question answering dataset,qg with significantly smaller question answering dataset,0.6200464367866516
translation,473,18,model,model,present,cross-lingual model,model present cross-lingual model,0.6688344478607178
translation,473,26,model,crosslingual model,to leverage,resources,crosslingual model to leverage resources,0.7300257682800293
translation,473,26,model,crosslingual model,to leverage,learning,crosslingual model to leverage learning,0.7218559384346008
translation,473,26,model,resources,available in,secondary language,resources available in secondary language,0.6345278024673462
translation,473,26,model,learning,to automatically generate,questions,learning to automatically generate questions,0.6975929141044617
translation,473,26,model,model,propose,crosslingual model,model propose crosslingual model,0.6679757833480835
translation,473,27,model,models,for,alignment,models for alignment,0.6418940424919128
translation,473,27,model,alignment,between,primary and secondary languages,alignment between primary and secondary languages,0.6670528650283813
translation,473,27,model,primary and secondary languages,in,unsupervised manner,primary and secondary languages in unsupervised manner,0.5116254687309265
translation,473,27,model,unsupervised manner,using,monolingual text,unsupervised manner using monolingual text,0.6687889695167542
translation,473,27,model,monolingual text,in,both languages,monolingual text in both languages,0.4958335757255554
translation,473,27,model,model,train,models,model train models,0.6628757119178772
translation,473,27,model,model,train,alignment,model train alignment,0.6876411437988281
translation,473,47,model,unsupervised pretraining phase,consisting of,denoising autoencoding and back -translation,unsupervised pretraining phase consisting of denoising autoencoding and back -translation,0.7054359912872314
translation,473,52,model,in both the encoder and the decoder,whose,weights,in both the encoder and the decoder whose weights,0.6512988805770874
translation,473,52,model,weights,updated using,data,weights updated using data,0.7460035681724548
translation,473,52,model,data,in,both languages,data in both languages,0.5173705220222473
translation,473,52,model,shared layers,has,in both the encoder and the decoder,shared layers has in both the encoder and the decoder,0.5829170942306519
translation,473,52,model,model,enforce,shared layers,model enforce shared layers,0.7687978744506836
translation,473,54,model,encoder and decoder layers,use,"newly released transformer ( vaswani et al. , 2017 ) model","encoder and decoder layers use newly released transformer ( vaswani et al. , 2017 ) model",0.5835714340209961
translation,473,54,model,model,For,encoder and decoder layers,model For encoder and decoder layers,0.5844725966453552
translation,473,55,model,encoders and decoders,consist of,stack of four identical layers,encoders and decoders consist of stack of four identical layers,0.6647411584854126
translation,473,55,model,stack of four identical layers,of which,two layers,stack of four identical layers of which two layers,0.5438077449798584
translation,473,55,model,model,has,encoders and decoders,model has encoders and decoders,0.5759453177452087
translation,473,56,model,transformer,consists of,multi-headed selfattention model,transformer consists of multi-headed selfattention model,0.6645167469978333
translation,473,56,model,multi-headed selfattention model,followed by,position - wise fully connected feed -forward network,multi-headed selfattention model followed by position - wise fully connected feed -forward network,0.6527807116508484
translation,473,56,model,model,Each layer of,transformer,model Each layer of transformer,0.6800908446311951
translation,473,87,model,first few layers,of,decoder,first few layers of decoder,0.6085933446884155
translation,473,87,model,last few layers,has,of the encoder,last few layers has of the encoder,0.5561294555664062
translation,473,87,model,model,has,weight sharing,model has weight sharing,0.5478658080101013
translation,473,134,model,best model,uses,two independent encoder and decoder layers,best model uses two independent encoder and decoder layers,0.5849130153656006
translation,473,134,model,best model,uses,two shared encoder and decoder layers,best model uses two shared encoder and decoder layers,0.5757771134376526
translation,473,134,model,two independent encoder and decoder layers,for,both languages,two independent encoder and decoder layers for both languages,0.5845775008201599
translation,473,134,model,model,has,best model,model has best model,0.5455926060676575
translation,473,166,results,all the other models,for,hindi,all the other models for hindi,0.6054009199142456
translation,473,166,results,clqg + parallel,has,outperforms,clqg + parallel has outperforms,0.6255537867546082
translation,473,166,results,outperforms,has,all the other models,outperforms has all the other models,0.5910621881484985
translation,473,166,results,results,observe,clqg + parallel,results observe clqg + parallel,0.599725604057312
translation,473,167,results,parallel fine-tuning,does not give,significant improvements,parallel fine-tuning does not give significant improvements,0.716400146484375
translation,473,167,results,significant improvements,over,clqg,significant improvements over clqg,0.7022917866706848
translation,473,167,results,chinese,has,parallel fine-tuning,chinese has parallel fine-tuning,0.5320394039154053
translation,473,167,results,results,For,chinese,results For chinese,0.6007780432701111
translation,473,183,results,our crosslingual model,performs,significantly better,our crosslingual model performs significantly better,0.6233927607536316
translation,473,183,results,significantly better,than,transformer model,significantly better than transformer model,0.5679478645324707
translation,473,183,results,significantly better,on,  relevance  ,significantly better on   relevance  ,0.47201281785964966
translation,473,183,results,  relevance  ,cost of,agreement,  relevance   cost of agreement,0.6598891019821167
translation,473,183,results,results,has,our crosslingual model,results has our crosslingual model,0.5345337986946106
translation,473,198,results,shared architecture,does not directly benefit,english qg dataset,shared architecture does not directly benefit english qg dataset,0.6756105422973633
translation,473,198,results,english qg dataset,with,simple weight sharing,english qg dataset with simple weight sharing,0.6019477248191833
translation,473,198,results,results,observe,shared architecture,results observe shared architecture,0.6353380084037781
translation,473,203,results,modest performance improvements,on,standard evaluation metrics,modest performance improvements on standard evaluation metrics,0.46723124384880066
translation,473,203,results,english squad data,in,main task,english squad data in main task,0.48072803020477295
translation,473,203,results,results,obtain,modest performance improvements,results obtain modest performance improvements,0.5688880681991577
translation,474,64,ablation-analysis,other pre-processing features,not necessary for,mrqa - tokenized data,other pre-processing features not necessary for mrqa - tokenized data,0.7312461137771606
translation,474,64,ablation-analysis,ablation analysis,remove,whitespace tokenization,ablation analysis remove whitespace tokenization,0.6438131332397461
translation,474,14,baselines,significant quantities,of,unlabelled text,significant quantities of unlabelled text,0.5847607851028442
translation,474,14,baselines,baselines,compare,bert and xlnet,baselines compare bert and xlnet,0.7170689105987549
translation,474,137,experimental-setup,model,on,8 nvidia tesla v100 gpus,model on 8 nvidia tesla v100 gpus,0.5238428711891174
translation,474,137,experimental-setup,experimental setup,trained,model,experimental setup trained model,0.6766582131385803
translation,474,138,experimental-setup,bbc and xbc,used,learning rate,bbc and xbc used learning rate,0.6599912643432617
translation,474,138,experimental-setup,bbc and xbc,used,single - gpu batch size,bbc and xbc used single - gpu batch size,0.6313343644142151
translation,474,138,experimental-setup,bbc and xbc,used,gradient accumulation,bbc and xbc used gradient accumulation,0.5957449078559875
translation,474,138,experimental-setup,learning rate,of,5e ? 5,learning rate of 5e ? 5,0.6546747088432312
translation,474,138,experimental-setup,single - gpu batch size,of,25,single - gpu batch size of 25,0.6210052371025085
translation,474,138,experimental-setup,gradient accumulation,of,1,gradient accumulation of 1,0.6653692722320557
translation,474,138,experimental-setup,gradient accumulation,yielding,effective batch size,gradient accumulation yielding effective batch size,0.6374065279960632
translation,474,138,experimental-setup,effective batch size,of,200,effective batch size of 200,0.6779114007949829
translation,474,138,experimental-setup,experimental setup,For,bbc and xbc,experimental setup For bbc and xbc,0.6361732482910156
translation,474,139,experimental-setup,xlc,used,learning rate,xlc used learning rate,0.6200705766677856
translation,474,139,experimental-setup,xlc,used,single - gpu batch size,xlc used single - gpu batch size,0.6094865202903748
translation,474,139,experimental-setup,xlc,used,gradient accumulation,xlc used gradient accumulation,0.5810989737510681
translation,474,139,experimental-setup,learning rate,of,2e ? 5,learning rate of 2e ? 5,0.6425188183784485
translation,474,139,experimental-setup,single - gpu batch size,of,6,single - gpu batch size of 6,0.6294607520103455
translation,474,139,experimental-setup,gradient accumulation,of,3,gradient accumulation of 3,0.6656100749969482
translation,474,139,experimental-setup,effective batch size,of,6 ? 8 ? 3 = 144,effective batch size of 6 ? 8 ? 3 = 144,0.6499810218811035
translation,474,139,experimental-setup,experimental setup,For,xlc,experimental setup For xlc,0.6165074110031128
translation,474,70,experiments,worst,on,out-domain ( o ) macro-average,worst on out-domain ( o ) macro-average,0.5504087209701538
translation,474,19,results,much simpler techniques,offer,significant improvements,much simpler techniques offer significant improvements,0.6313501000404358
translation,474,19,results,results,has,much simpler techniques,results has much simpler techniques,0.5502762198448181
translation,474,57,results,first fine-tuned,on,multidomain dataset,first fine-tuned on multidomain dataset,0.49348607659339905
translation,474,57,results,higher exact match ( em ),than,n't,higher exact match ( em ) than n't,0.6228358745574951
translation,474,57,results,higher exact match ( em ),has,almost universally,higher exact match ( em ) has almost universally,0.5835089683532715
translation,474,57,results,results,has,set of models,results has set of models,0.5955486297607422
translation,474,145,results,exaggerated,at,shorter max sequence length ( msl ),exaggerated at shorter max sequence length ( msl ),0.5639470815658569
translation,474,145,results,shorter max sequence length ( msl ),of,200,shorter max sequence length ( msl ) of 200,0.6069638133049011
translation,474,145,results,200,including,na segments,200 including na segments,0.7740870118141174
translation,474,145,results,na segments,increases,out-domain em,na segments increases out-domain em,0.7062121033668518
translation,474,145,results,out-domain em,from,43.78 to 50.04,out-domain em from 43.78 to 50.04,0.5101190209388733
translation,474,145,results,43.78 to 50.04,on,xbc model,43.78 to 50.04 on xbc model,0.5512828826904297
translation,474,145,results,results,has,improvement,results has improvement,0.6248279809951782
translation,474,153,results,searchqa and triviaqa,benefit the most,some form of data augmentation,searchqa and triviaqa benefit the most some form of data augmentation,0.5955490469932556
translation,474,153,results,some form of data augmentation,by,more than one f1 point,some form of data augmentation by more than one f1 point,0.5969081521034241
translation,474,153,results,results,notice,searchqa and triviaqa,results notice searchqa and triviaqa,0.6992109417915344
translation,475,166,ablation-analysis,length,of,cloze questions,length of cloze questions,0.5790859460830688
translation,475,166,ablation-analysis,cloze questions,helps,translation components,cloze questions helps translation components,0.6399283409118652
translation,475,166,ablation-analysis,translation components,produce,"simpler , more precise questions","translation components produce simpler , more precise questions",0.6508781909942627
translation,475,166,ablation-analysis,ablation analysis,Reducing,length,ablation analysis Reducing length,0.7589595317840576
translation,475,171,ablation-analysis,mean improvement,of,1.8 f1,mean improvement of 1.8 f1,0.5306878089904785
translation,475,171,ablation-analysis,1.8 f1,on,bert - base,1.8 f1 on bert - base,0.5576014518737793
translation,475,171,ablation-analysis,ablation analysis,has,unsupervised nmt question translation,ablation analysis has unsupervised nmt question translation,0.5215937495231628
translation,475,37,experiments,large corpus,of,natural questions,large corpus of natural questions,0.4864861071109772
translation,475,37,experiments,large corpus,of,cloze questions,large corpus of cloze questions,0.5291031002998352
translation,475,37,experiments,unaligned corpus,of,cloze questions,unaligned corpus of cloze questions,0.5859943628311157
translation,475,37,experiments,seq2seq model,to map between,natural and cloze question domains,seq2seq model to map between natural and cloze question domains,0.7490779757499695
translation,475,37,experiments,natural and cloze question domains,using,de-noising auto-encoding,natural and cloze question domains using de-noising auto-encoding,0.6940212845802307
translation,475,37,experiments,natural and cloze question domains,combination of,online back -translation,natural and cloze question domains combination of online back -translation,0.6015754342079163
translation,475,37,experiments,natural and cloze question domains,combination of,de-noising auto-encoding,natural and cloze question domains combination of de-noising auto-encoding,0.6578473448753357
translation,475,5,model,high quality training data,required for,extractive qa,high quality training data required for extractive qa,0.693930447101593
translation,475,5,model,model,explore,high quality training data,model explore high quality training data,0.6982152462005615
translation,475,6,model,learning,to generate,"context , question and answer triples","learning to generate context , question and answer triples",0.6812646389007568
translation,475,6,model,"context , question and answer triples",in,unsupervised manner,"context , question and answer triples in unsupervised manner",0.5144668817520142
translation,475,38,results,unsupervised qa,lead to,performances,unsupervised qa lead to performances,0.5907055139541626
translation,475,38,results,performances,surpassing,early supervised approaches,performances surpassing early supervised approaches,0.7521654963493347
translation,475,39,results,forms of cloze   translation,that produce,( unnatural ) questions,forms of cloze   translation that produce ( unnatural ) questions,0.6458206176757812
translation,475,39,results,( unnatural ) questions,via,word removal,( unnatural ) questions via word removal,0.6540728211402893
translation,475,39,results,flips,of,cloze question,flips of cloze question,0.6126313805580139
translation,475,39,results,cloze question,lead to,better performance,cloze question lead to better performance,0.7227670550346375
translation,475,39,results,better performance,than,informed rule- based translator,better performance than informed rule- based translator,0.5911022424697876
translation,475,39,results,results,show,forms of cloze   translation,results show forms of cloze   translation,0.5687583684921265
translation,475,40,results,unsupervised seq2seq model,has,outperforms,unsupervised seq2seq model has outperforms,0.5800925493240356
translation,475,40,results,outperforms,has,both,outperforms has both,0.6678271293640137
translation,475,40,results,both,has,noise and rule-based system,both has noise and rule-based system,0.639484167098999
translation,475,40,results,results,has,unsupervised seq2seq model,results has unsupervised seq2seq model,0.5213920474052429
translation,475,149,results,our approach,has,significantly outperforms,our approach has significantly outperforms,0.6118320226669312
translation,475,149,results,significantly outperforms,has,baseline systems,significantly outperforms has baseline systems,0.5916866660118103
translation,475,149,results,surpasses,has,early supervised methods,surpasses has early supervised methods,0.5646123290061951
translation,475,149,results,results,has,our approach,results has our approach,0.6050099730491638
translation,475,157,results,qa models,is,more effective,qa models is more effective,0.581660807132721
translation,475,157,results,more effective,than,maximising,more effective than maximising,0.5891276597976685
translation,475,157,results,maximising,has,question likelihood,maximising has question likelihood,0.5544053316116333
translation,475,165,results,sub-clauses,for,generation,sub-clauses for generation,0.6060206890106201
translation,475,165,results,sub-clauses,leads to,shorter common subsequences,sub-clauses leads to shorter common subsequences,0.6764090061187744
translation,475,165,results,generation,leads to,shorter questions,generation leads to shorter questions,0.6856886744499207
translation,475,165,results,shorter common subsequences,to,context,shorter common subsequences to context,0.5181524753570557
translation,475,165,results,results,using,sub-clauses,results using sub-clauses,0.6372899413108826
translation,475,167,results,sub-clauses,leads to,on average + 4.0 f1,sub-clauses leads to on average + 4.0 f1,0.6677311062812805
translation,475,167,results,on average + 4.0 f1,across,equivalent sentencelevel bert - base models,on average + 4.0 f1 across equivalent sentencelevel bert - base models,0.6315538883209229
translation,475,167,results,results,Using,sub-clauses,results Using sub-clauses,0.6372899413108826
translation,475,174,results,bert - large,gives,further boost,bert - large gives further boost,0.7070810198783875
translation,475,174,results,further boost,improving,best configuration,further boost improving best configuration,0.717270016670227
translation,475,174,results,best configuration,by,6.9 f1,best configuration by 6.9 f1,0.5444087386131287
translation,475,174,results,results,has,bert - large,results has bert - large,0.6052632927894592
translation,475,204,results,68 %,of,questions,68 % of questions,0.609772801399231
translation,475,204,results,questions,generated by,unmt model,questions generated by unmt model,0.6782592535018921
translation,475,204,results,well - formed,compared to,92.3 %,well - formed compared to 92.3 %,0.6576633453369141
translation,475,204,results,75.6 %,for,rule- based system,75.6 % for rule- based system,0.6168208718299866
translation,475,204,results,92.3 %,for,squad questions,92.3 % for squad questions,0.6190098524093628
translation,475,204,results,results,find that,68 %,results find that 68 %,0.6337223649024963
translation,475,205,results,language model pretraining,improves,quality of questions,language model pretraining improves quality of questions,0.6670727133750916
translation,475,205,results,quality of questions,generated by,unmt model,quality of questions generated by unmt model,0.6204781532287598
translation,475,205,results,78.5 %,classified as,well - formed,78.5 % classified as well - formed,0.6036002039909363
translation,475,205,results,78.5 %,surpassing,rule- based system,78.5 % surpassing rule- based system,0.7183846235275269
translation,475,205,results,results,note,language model pretraining,results note language model pretraining,0.5375067591667175
translation,475,205,results,results,using,language model pretraining,results using language model pretraining,0.5944600701332092
translation,475,239,results,  noisy cloze   system,consisting of,very simple rules and noise,  noisy cloze   system consisting of very simple rules and noise,0.7042536735534668
translation,475,239,results,  noisy cloze   system,performs,nearly as well,  noisy cloze   system performs nearly as well,0.6361033320426941
translation,475,239,results,nearly as well,as,our more complex best-performing system,nearly as well as our more complex best-performing system,0.585271418094635
translation,475,239,results,results,note,  noisy cloze   system,results note   noisy cloze   system,0.6011338829994202
translation,476,137,ablation-analysis,models,trained with,paragraph inputs,models trained with paragraph inputs,0.724364161491394
translation,476,137,ablation-analysis,models,trained with,sentence inputs,models trained with sentence inputs,0.7180704474449158
translation,476,137,ablation-analysis,models,trained with,sentence inputs,models trained with sentence inputs,0.7180704474449158
translation,476,137,ablation-analysis,paragraph inputs,where,duplication rates,paragraph inputs where duplication rates,0.6043350696563721
translation,476,137,ablation-analysis,maxout pointer,reduces,duplication rates,maxout pointer reduces duplication rates,0.6714676022529602
translation,476,137,ablation-analysis,duplication rates,to,same level,duplication rates to same level,0.615208089351654
translation,476,137,ablation-analysis,duplication rates,half of,values,duplication rates half of values,0.6812037229537964
translation,476,137,ablation-analysis,duplication rates,half of,same level,duplication rates half of same level,0.6927908658981323
translation,476,137,ablation-analysis,values,in,basic copy,values in basic copy,0.5374703407287598
translation,476,137,ablation-analysis,same level,as,model,same level as model,0.6104810237884521
translation,476,137,ablation-analysis,model,trained with,sentence inputs,model trained with sentence inputs,0.7408336997032166
translation,476,137,ablation-analysis,models,has,maxout pointer,models has maxout pointer,0.5624123811721802
translation,476,137,ablation-analysis,paragraph inputs,has,maxout pointer,paragraph inputs has maxout pointer,0.543816864490509
translation,476,137,ablation-analysis,ablation analysis,For,models,ablation analysis For models,0.61053466796875
translation,476,95,baselines,split2,split,dev* set,split2 split dev* set,0.7665878534317017
translation,476,95,baselines,dev* set,into,dev and test sets,dev* set into dev and test sets,0.6172312498092651
translation,476,95,baselines,dev and test sets,has,randomly,dev and test sets has randomly,0.6289374828338623
translation,476,102,hyperparameters,encoding,has,bidirectional lstm,encoding has bidirectional lstm,0.5483115911483765
translation,476,102,hyperparameters,hyperparameters,For,encoding,hyperparameters For encoding,0.5883936882019043
translation,476,103,hyperparameters,cell hidden size,was,600,cell hidden size was 600,0.6457692980766296
translation,476,103,hyperparameters,hyperparameters,has,cell hidden size,hyperparameters has cell hidden size,0.5294819474220276
translation,476,104,hyperparameters,dropout,with,probability 0.3,dropout with probability 0.3,0.6265801191329956
translation,476,104,hyperparameters,probability 0.3,applied between,vertical lstm stacks,probability 0.3 applied between vertical lstm stacks,0.6950351595878601
translation,476,104,hyperparameters,hyperparameters,has,dropout,hyperparameters has dropout,0.5324090719223022
translation,476,105,hyperparameters,word embedding,used,pre-trained glove word vectors,word embedding used pre-trained glove word vectors,0.5625362396240234
translation,476,105,hyperparameters,pre-trained glove word vectors,with,300 dimensions,pre-trained glove word vectors with 300 dimensions,0.6206281781196594
translation,476,105,hyperparameters,hyperparameters,For,word embedding,hyperparameters For word embedding,0.5246535539627075
translation,476,106,hyperparameters,dimension,of,answer tagging meta-word embedding,dimension of answer tagging meta-word embedding,0.5261114239692688
translation,476,106,hyperparameters,answer tagging meta-word embedding,was,3,answer tagging meta-word embedding was 3,0.5700265169143677
translation,476,106,hyperparameters,hyperparameters,has,dimension,hyperparameters has dimension,0.514893651008606
translation,476,108,hyperparameters,optimization,used,sgd,optimization used sgd,0.5824800133705139
translation,476,108,hyperparameters,sgd,with,"momentum ( qian , 1999","sgd with momentum ( qian , 1999",0.6333440542221069
translation,476,108,hyperparameters,hyperparameters,For,optimization,hyperparameters For optimization,0.5834490656852722
translation,476,109,hyperparameters,learning rate,initially set to,0.1,learning rate initially set to 0.1,0.6681355237960815
translation,476,109,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,476,110,hyperparameters,models,trained with,20 epochs,models trained with 20 epochs,0.7354478240013123
translation,476,110,hyperparameters,hyperparameters,has,models,hyperparameters has models,0.5447477102279663
translation,476,111,hyperparameters,mini-batch size,for,parameter update,mini-batch size for parameter update,0.5821319222450256
translation,476,111,hyperparameters,parameter update,was,64,parameter update was 64,0.5837366580963135
translation,476,111,hyperparameters,hyperparameters,has,mini-batch size,hyperparameters has mini-batch size,0.4961773455142975
translation,476,9,model,model,propose,maxout pointer mechanism,model propose maxout pointer mechanism,0.6840652823448181
translation,476,29,model,previous sequence to sequence attention model,with,maxout pointer mechanism,previous sequence to sequence attention model with maxout pointer mechanism,0.6163218021392822
translation,476,29,model,previous sequence to sequence attention model,with,gated self-attention encoder,previous sequence to sequence attention model with gated self-attention encoder,0.6159579157829285
translation,476,29,model,existing neural qg approaches,with,sentence or paragraph,existing neural qg approaches with sentence or paragraph,0.6038836240768433
translation,476,29,model,sentence or paragraph,as,inputs,sentence or paragraph as inputs,0.5204306244850159
translation,476,29,model,outperforms,has,existing neural qg approaches,outperforms has existing neural qg approaches,0.5982785820960999
translation,476,29,model,model,extend,previous sequence to sequence attention model,model extend previous sequence to sequence attention model,0.6903980374336243
translation,476,80,model,model,propose,new maxout pointer mechanism,model propose new maxout pointer mechanism,0.6876281499862671
translation,476,186,model,sequence to sequence network,contains,gated self-attention encoder,sequence to sequence network contains gated self-attention encoder,0.6040961146354675
translation,476,186,model,sequence to sequence network,contains,maxout pointer decoder,sequence to sequence network contains maxout pointer decoder,0.643913745880127
translation,476,186,model,sequence to sequence network,to address,answer - aware qg problem,sequence to sequence network to address answer - aware qg problem,0.645453691482544
translation,476,186,model,maxout pointer decoder,to address,answer - aware qg problem,maxout pointer decoder to address answer - aware qg problem,0.6308103203773499
translation,476,186,model,answer - aware qg problem,for,long text input,answer - aware qg problem for long text input,0.5751086473464966
translation,476,186,model,model,proposed,sequence to sequence network,model proposed sequence to sequence network,0.7292423248291016
translation,476,31,results,first model,demonstrates,large improvement,first model demonstrates large improvement,0.6380590796470642
translation,476,31,results,large improvement,with,paragraph,large improvement with paragraph,0.6919615864753723
translation,476,31,results,as input,over,sentence as input,as input over sentence as input,0.6854333281517029
translation,476,31,results,paragraph,has,as input,paragraph has as input,0.630772054195404
translation,476,31,results,results,is,first model,results is first model,0.5424137711524963
translation,476,122,results,answer tagging,dramatically boosts,performance,answer tagging dramatically boosts performance,0.6650534272193909
translation,476,122,results,s2s -a,has,answer tagging,s2s -a has answer tagging,0.5969657897949219
translation,476,124,results,copy mechanism,improves,performance,copy mechanism improves performance,0.7307570576667786
translation,476,124,results,performance,on,qg task,performance on qg task,0.5601118803024292
translation,476,127,results,performance,is,lower,performance is lower,0.6222158074378967
translation,476,127,results,lower,when,paragraph,lower when paragraph,0.7036082744598389
translation,476,127,results,paragraph,given as,input,paragraph given as input,0.6430129408836365
translation,476,127,results,input,than,sentence,input than sentence,0.5807011127471924
translation,476,127,results,sentence,as,input,sentence as input,0.5344812870025635
translation,476,127,results,results,has,performance,results has performance,0.5972660779953003
translation,476,132,results,basic copy mechanism,in,all metrics,basic copy mechanism in all metrics,0.4819067120552063
translation,476,132,results,maxout pointer mechanism,has,outperforms,maxout pointer mechanism has outperforms,0.6150012612342834
translation,476,132,results,outperforms,has,basic copy mechanism,outperforms has basic copy mechanism,0.5987658500671387
translation,476,132,results,results,has,maxout pointer mechanism,results has maxout pointer mechanism,0.5712995529174805
translation,476,154,results,beam search decoding,boosts,all metrics,beam search decoding boosts all metrics,0.756810188293457
translation,476,154,results,all metrics,for,sentence and paragraph inputs,all metrics for sentence and paragraph inputs,0.5524215698242188
translation,476,154,results,results,see,beam search decoding,results see beam search decoding,0.578096330165863
translation,476,160,results,our model,with,maxout pointer and gated selfattention,our model with maxout pointer and gated selfattention,0.6310949921607971
translation,476,160,results,maxout pointer and gated selfattention,achieves,state - of - the - art results,maxout pointer and gated selfattention achieves state - of - the - art results,0.6358852386474609
translation,476,160,results,state - of - the - art results,in,qg,state - of - the - art results in qg,0.5645936727523804
translation,476,160,results,results,has,our model,results has our model,0.5871725678443909
translation,476,188,results,state - of - the - art approaches,with,paragraph or sentence inputs,state - of - the - art approaches with paragraph or sentence inputs,0.5816827416419983
translation,476,188,results,exceeded,has,state - of - the - art approaches,exceeded has state - of - the - art approaches,0.5906723141670227
translation,477,32,ablation-analysis,both separate modules,obviously improve,performances,both separate modules obviously improve performances,0.740959644317627
translation,477,32,ablation-analysis,performances,of,qg task,performances of qg task,0.5695410966873169
translation,477,32,ablation-analysis,both separate modules,has,pg auxiliary task,both separate modules has pg auxiliary task,0.6142792105674744
translation,477,32,ablation-analysis,ablation analysis,show,both separate modules,ablation analysis show both separate modules,0.6481213569641113
translation,477,5,model,paraphrase knowledge,into,question generation ( qg ),paraphrase knowledge into question generation ( qg ),0.5814822316169739
translation,477,5,model,paraphrase knowledge,to generate,human-like questions,paraphrase knowledge to generate human-like questions,0.669416069984436
translation,477,5,model,question generation ( qg ),to generate,human-like questions,question generation ( qg ) to generate human-like questions,0.689264178276062
translation,477,5,model,model,incorporate,paraphrase knowledge,model incorporate paraphrase knowledge,0.6871530413627625
translation,477,6,model,two -hand hybrid model,leveraging,self- built paraphrase resource,two -hand hybrid model leveraging self- built paraphrase resource,0.691830575466156
translation,477,6,model,self- built paraphrase resource,automatically conducted by,simple back -translation method,self- built paraphrase resource automatically conducted by simple back -translation method,0.7753693461418152
translation,477,6,model,model,present,two -hand hybrid model,model present two -hand hybrid model,0.6382017731666565
translation,477,7,model,multi-task learning,with,sentence - level paraphrase generation ( pg ),multi-task learning with sentence - level paraphrase generation ( pg ),0.6271405816078186
translation,477,7,model,model,conduct,multi-task learning,model conduct multi-task learning,0.6259341835975647
translation,478,120,baselines,cnn,using,word embeddings and machine translation evaluation metrics,cnn using word embeddings and machine translation evaluation metrics,0.5891661643981934
translation,478,120,baselines,two classifiers,used,lexical similarity features,two classifiers used lexical similarity features,0.5725836753845215
translation,478,120,baselines,baselines,has,classifiers,baselines has classifiers,0.6089369058609009
translation,478,7,hyperparameters,cnn,uses,word embeddings and machine translation evaluation scores,cnn uses word embeddings and machine translation evaluation scores,0.5418097972869873
translation,478,7,hyperparameters,word embeddings and machine translation evaluation scores,as,features,word embeddings and machine translation evaluation scores as features,0.4688461124897003
translation,478,7,hyperparameters,hyperparameters,has,cnn,hyperparameters has cnn,0.5197938084602356
translation,478,95,hyperparameters,network training,used,"adadelta ( zeiler , 2012 )","network training used adadelta ( zeiler , 2012 )",0.5460696816444397
translation,478,95,hyperparameters,"adadelta ( zeiler , 2012 )",to update,weights,"adadelta ( zeiler , 2012 ) to update weights",0.7807872295379639
translation,478,95,hyperparameters,weights,of,model,weights of model,0.6060100197792053
translation,478,95,hyperparameters,initial learning rate,to,? = 10 ?4,initial learning rate to ? = 10 ?4,0.5768792629241943
translation,478,95,hyperparameters,"dropout ( srivastava et al. , 2014 )",added to,input layer,"dropout ( srivastava et al. , 2014 ) added to input layer",0.5865424275398254
translation,478,95,hyperparameters,input layer,of,mlp,input layer of mlp,0.5906241536140442
translation,478,95,hyperparameters,input layer,with,l 2 - regularization,input layer with l 2 - regularization,0.5650539398193359
translation,478,95,hyperparameters,hyperparameters,For,network training,hyperparameters For network training,0.5741385221481323
translation,478,95,hyperparameters,hyperparameters,set,initial learning rate,hyperparameters set initial learning rate,0.5817552208900452
translation,478,96,hyperparameters,dropout rate,set to,0.5,dropout rate set to 0.5,0.6910414695739746
translation,478,96,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,478,94,model,mlp,combined,three fully - connected hidden layers,mlp combined three fully - connected hidden layers,0.6777359843254089
translation,478,94,model,mlp,with,softmax layer,mlp with softmax layer,0.653793215751648
translation,478,94,model,three fully - connected hidden layers,contained,512 nodes each,three fully - connected hidden layers contained 512 nodes each,0.5259481072425842
translation,478,94,model,relu,as,activation function,relu as activation function,0.5622032880783081
translation,478,94,model,model,has,mlp,model has mlp,0.6622222661972046
translation,478,119,model,method,based on,combination of three different classifiers,method based on combination of three different classifiers,0.6802331209182739
translation,478,119,model,combination of three different classifiers,for,duplicate question ranking,combination of three different classifiers for duplicate question ranking,0.5738628506660461
translation,478,6,results,map score,of,70.2 %,map score of 70.2 %,0.540407121181488
translation,478,6,results,70.2 %,on,test set,70.2 % on test set,0.5300400257110596
translation,478,6,results,support vector machine ( svm ),trained over,lexical similarity features,support vector machine ( svm ) trained over lexical similarity features,0.7260369658470154
translation,478,6,results,three classifiers,has,naivebayes classifier,three classifiers has naivebayes classifier,0.5209036469459534
translation,478,6,results,results,obtained,map score,results obtained map score,0.7102351784706116
translation,478,22,results,combined system,achieved,map score,combined system achieved map score,0.7635874152183533
translation,478,22,results,map score,of,70.2 %,map score of 70.2 %,0.540407121181488
translation,478,22,results,70.2 %,on,test set,70.2 % on test set,0.5300400257110596
translation,478,22,results,results,has,combined system,results has combined system,0.5987387895584106
translation,478,107,results,cnn model,obtained,best results,cnn model obtained best results,0.6485905051231384
translation,478,107,results,test set,has,cnn model,test set has cnn model,0.5617198348045349
translation,478,107,results,results,On,test set,results On test set,0.582119882106781
translation,478,108,results,all three models,performed,better,all three models performed better,0.3059951663017273
translation,478,108,results,better,on,test set,better on test set,0.5528749227523804
translation,478,108,results,better,on,development set,better on development set,0.5785938501358032
translation,478,108,results,better,than on,development set,better than on development set,0.6543411612510681
translation,478,108,results,test set,than on,development set,test set than on development set,0.6372206211090088
translation,478,108,results,results,interesting to see that,all three models,results interesting to see that all three models,0.5960046052932739
translation,479,68,baselines,vqa dataset,named,multilingual and codemixed visual question answering,vqa dataset named multilingual and codemixed visual question answering,0.6495900750160217
translation,479,68,baselines,multilingual and codemixed visual question answering,has,( mcvqa ),multilingual and codemixed visual question answering has ( mcvqa ),0.5687175989151001
translation,479,187,baselines,bi-linear attention network,has,"kim et al. , 2018 )","bi-linear attention network has kim et al. , 2018 )",0.5385003089904785
translation,479,245,baselines,baselines,has,cross-lingual training of object - level features,baselines has cross-lingual training of object - level features,0.5405601263046265
translation,479,177,experimental-setup,english,use,fasttext word embedding,english use fasttext word embedding,0.5887847542762756
translation,479,177,experimental-setup,fasttext word embedding,dimension,300,fasttext word embedding dimension 300,0.660916805267334
translation,479,178,experimental-setup,hindi sentences,from,bojar et al . ( 2014 ),hindi sentences from bojar et al . ( 2014 ),0.4962472915649414
translation,479,178,experimental-setup,hindi sentences,train,word embedding,hindi sentences train word embedding,0.6086896657943726
translation,479,178,experimental-setup,word embedding,dimension,300,word embedding dimension 300,0.6979199051856995
translation,479,178,experimental-setup,word embedding,using,word embedding algorithm,word embedding using word embedding algorithm,0.6970418691635132
translation,479,178,experimental-setup,300,using,word embedding algorithm,300 using word embedding algorithm,0.6824737191200256
translation,479,178,experimental-setup,experimental setup,use,hindi sentences,experimental setup use hindi sentences,0.5383461713790894
translation,479,185,experimental-setup,words,in,question,words in question,0.5680095553398132
translation,479,185,experimental-setup,words,CNN filter size=,"{ 2 , 3 }","words CNN filter size= { 2 , 3 }",0.7144351005554199
translation,479,185,experimental-setup,shared cnn layers=1,# of,shared bi-lstm layers,shared cnn layers=1 # of shared bi-lstm layers,0.5935266017913818
translation,479,185,experimental-setup,hidden dimension,=,1000,hidden dimension = 1000,0.6944918632507324
translation,479,185,experimental-setup,image level and object level feature dimension,=,2048,image level and object level feature dimension = 2048,0.6565442681312561
translation,479,185,experimental-setup,spatial location,in,image level feature,spatial location in image level feature,0.5400302410125732
translation,479,185,experimental-setup,spatial location,# of objects in,object level feature,spatial location # of objects in object level feature,0.7080808281898499
translation,479,185,experimental-setup,image level feature,=,100,image level feature = 100,0.6831979751586914
translation,479,185,experimental-setup,rank,in,bi-linear pooling,rank in bi-linear pooling,0.5215322971343994
translation,479,185,experimental-setup,# of epochs,=,100,# of epochs = 100,0.7014289498329163
translation,479,185,experimental-setup,question,has,=15,question has =15,0.5748132467269897
translation,479,185,experimental-setup,shared bi-lstm layers,has,=2,shared bi-lstm layers has =2,0.6019983291625977
translation,479,185,experimental-setup,object level feature,has,=36,object level feature has =36,0.6168126463890076
translation,479,185,experimental-setup,bi-linear pooling,has,=3,bi-linear pooling has =3,0.6264345049858093
translation,479,185,experimental-setup,initial learning rate,has,=0.002,initial learning rate has =0.002,0.5377046465873718
translation,479,185,experimental-setup,experimental setup,CNN filter size=,"{ 2 , 3 }","experimental setup CNN filter size= { 2 , 3 }",0.6903875470161438
translation,479,185,experimental-setup,experimental setup,CNN filter size=,initial learning rate,experimental setup CNN filter size= initial learning rate,0.7042883634567261
translation,479,4,model,effective deep learning framework,for,multilingual and codemixed visual question answering,effective deep learning framework for multilingual and codemixed visual question answering,0.5398115515708923
translation,479,4,model,model,propose,effective deep learning framework,model propose effective deep learning framework,0.6254764795303345
translation,479,5,model,answers,from,questions,answers from questions,0.5725440979003906
translation,479,5,model,answers,in,codemixed ( hinglish : hindi-english ) languages,answers in codemixed ( hinglish : hindi-english ) languages,0.534008264541626
translation,479,5,model,questions,in,"hindi , english","questions in hindi , english",0.46961840987205505
translation,479,5,model,model,capable of,answers,model capable of answers,0.740781843662262
translation,479,10,model,multilingual and code-mixed questions,introduce,hierarchy,multilingual and code-mixed questions introduce hierarchy,0.6284939050674438
translation,479,10,model,hierarchy,of,shared layers,hierarchy of shared layers,0.5977272987365723
translation,479,10,model,model,To better encode,multilingual and code-mixed questions,model To better encode multilingual and code-mixed questions,0.7174512147903442
translation,479,11,model,behaviour,of,shared layers,behaviour of shared layers,0.5512766242027283
translation,479,11,model,shared layers,by,attention - based soft layer sharing mechanism,shared layers by attention - based soft layer sharing mechanism,0.5386069416999817
translation,479,11,model,model,control,behaviour,model control behaviour,0.7727836966514587
translation,479,12,model,bi-linear attention,with,residual connection,bi-linear attention with residual connection,0.627305805683136
translation,479,12,model,residual connection,to fuse,language and image features,residual connection to fuse language and image features,0.7398689389228821
translation,479,12,model,model,uses,bi-linear attention,model uses bi-linear attention,0.5631870627403259
translation,479,35,model,technique,for,multilingual and code-mixed vqa,technique for multilingual and code-mixed vqa,0.648444414138794
translation,479,35,model,model,propose,technique,model propose technique,0.6912915110588074
translation,479,36,model,proposed method,consists of,three components,proposed method consists of three components,0.741250216960907
translation,479,36,model,model,has,proposed method,model has proposed method,0.5846953392028809
translation,479,37,model,first component,is,multilingual question encoding,first component is multilingual question encoding,0.5566760897636414
translation,479,37,model,multilingual question encoding,transforms,given question,multilingual question encoding transforms given question,0.6569880247116089
translation,479,37,model,given question,to,feature representation,given question to feature representation,0.5364750027656555
translation,479,37,model,model,has,first component,model has first component,0.5507974624633789
translation,479,38,model,multilinguality and code-mixing,in,questions,multilinguality and code-mixing in questions,0.5313692688941956
translation,479,39,model,multilingual embedding,coupled with,hierarchy,multilingual embedding coupled with hierarchy,0.7037774920463562
translation,479,39,model,hierarchy,of,shared layers,hierarchy of shared layers,0.5977272987365723
translation,479,39,model,model,use,multilingual embedding,model use multilingual embedding,0.577277421951294
translation,479,40,model,attention mechanism,on,shared layers,attention mechanism on shared layers,0.5524164438247681
translation,479,40,model,shared layers,to learn,language specific question representation,shared layers to learn language specific question representation,0.5935800075531006
translation,479,40,model,model,employ,attention mechanism,model employ attention mechanism,0.5382089614868164
translation,479,41,model,self-attention,to obtain,improved question representation,self-attention to obtain improved question representation,0.5398926138877869
translation,479,41,model,improved question representation,by considering,other words,improved question representation by considering other words,0.6485302448272705
translation,479,41,model,other words,in,question,other words in question,0.5695122480392456
translation,479,41,model,model,utilize,self-attention,model utilize self-attention,0.5907983183860779
translation,479,42,model,second component,obtains,effective image representation,second component obtains effective image representation,0.6011646389961243
translation,479,42,model,image features ),obtains,effective image representation,image features ) obtains effective image representation,0.5917984843254089
translation,479,42,model,effective image representation,from,object level and pixel level features,effective image representation from object level and pixel level features,0.5506134033203125
translation,479,42,model,second component,has,image features ),second component has image features ),0.6031852960586548
translation,479,42,model,model,has,second component,model has second component,0.5531283020973206
translation,479,174,model,gradient,after computing,loss,gradient after computing loss,0.7387610077857971
translation,479,174,model,loss,of,each mini-batch,loss of each mini-batch,0.6228268146514893
translation,479,174,model,each mini-batch,from,"given language of sample ( question , image , answer )","each mini-batch from given language of sample ( question , image , answer )",0.5495560765266418
translation,479,174,model,model,update,gradient,model update gradient,0.8232543468475342
translation,479,192,results,proposed model,achieves,overall accuracy,proposed model achieves overall accuracy,0.6750085949897766
translation,479,192,results,state- of- the - art english,with,65.37 % overall accuracy,state- of- the - art english with 65.37 % overall accuracy,0.578065037727356
translation,479,192,results,overall accuracy,of,64.51 % and 64.69 %,overall accuracy of 64.51 % and 64.69 %,0.5620642900466919
translation,479,192,results,64.51 % and 64.69 %,on,hindi and code-mixed vqa,64.51 % and 64.69 % on hindi and code-mixed vqa,0.526885449886322
translation,479,192,results,proposed model,has,outperforms,proposed model has outperforms,0.642342746257782
translation,479,192,results,outperforms,has,state- of- the - art english,outperforms has state- of- the - art english,0.57732754945755
translation,479,192,results,results,has,proposed model,results has proposed model,0.5938616394996643
translation,479,201,results,results,has,comparison to the non-english vqa,results has comparison to the non-english vqa,0.561532199382782
translation,480,5,model,approach,for incorporating,both of these signals,approach for incorporating both of these signals,0.7280653715133667
translation,480,5,model,both of these signals,in,unified framework,both of these signals in unified framework,0.5471373796463013
translation,480,5,model,unified framework,based on,natural logic,unified framework based on natural logic,0.5871723294258118
translation,480,5,model,model,propose,approach,model propose approach,0.6953083872795105
translation,480,26,model,natural logic inference engine,handle,relational entailment and meronymy,natural logic inference engine handle relational entailment and meronymy,0.5814005732536316
translation,480,26,model,model,extend,natural logic inference engine,model extend natural logic inference engine,0.6847931146621704
translation,481,149,ablation-analysis,r-gcn,lose,8.0 points,r-gcn lose 8.0 points,0.7432808876037598
translation,481,149,ablation-analysis,ablation analysis,remove,r-gcn,ablation analysis remove r-gcn,0.7082343697547913
translation,481,169,ablation-analysis,coreference links and complement edges,seem to play,more marginal role,coreference links and complement edges seem to play more marginal role,0.6421870589256287
translation,481,169,ablation-analysis,ablation analysis,notice,coreference links and complement edges,ablation analysis notice coreference links and complement edges,0.6622203588485718
translation,481,94,baselines,pre-trained bi-directional lan-guage model,relies on,character - based input representation,pre-trained bi-directional lan-guage model relies on character - based input representation,0.6842581033706665
translation,481,94,baselines,elmo,has,pre-trained bi-directional lan-guage model,elmo has pre-trained bi-directional lan-guage model,0.5531465411186218
translation,481,5,model,information spread,within,documents,information spread within documents,0.6757183074951172
translation,481,5,model,information spread,across,multiple documents,information spread across multiple documents,0.7066521048545837
translation,481,5,model,model,introduce,neural model,model introduce neural model,0.6195321083068848
translation,481,6,model,inference problem,on,graph,inference problem on graph,0.549210786819458
translation,481,6,model,model,frame it as,inference problem,model frame it as inference problem,0.6424270868301392
translation,481,32,model,training,has,expensive document encoders,training has expensive document encoders,0.5498867034912109
translation,481,32,model,model,avoid,training,model avoid training,0.6431753635406494
translation,481,33,model,small query encoder,has,gcn layers,small query encoder has gcn layers,0.529274046421051
translation,481,34,model,contextualized embeddings ( elmo ),to obtain,initial ( local ) representations,contextualized embeddings ( elmo ) to obtain initial ( local ) representations,0.6043562293052673
translation,481,34,model,initial ( local ) representations,of,nodes,initial ( local ) representations of nodes,0.5726013779640198
translation,481,79,model,multi-step reasoning,by transforming,node representations,multi-step reasoning by transforming node representations,0.66408371925354
translation,481,79,model,node representations,with,differentiable message passing algorithm,node representations with differentiable message passing algorithm,0.6285731792449951
translation,481,79,model,differentiable message passing algorithm,propagates,information,differentiable message passing algorithm propagates information,0.7337572574615479
translation,481,79,model,information,through,entity graph,information through entity graph,0.5920036435127258
translation,481,79,model,model,approaches,multi-step reasoning,model approaches multi-step reasoning,0.7102799415588379
translation,481,80,model,graph convolutional network ( gcn ),employ,relational - gcns,graph convolutional network ( gcn ) employ relational - gcns,0.5440742373466492
translation,481,80,model,model,parameterized by,graph convolutional network ( gcn ),model parameterized by graph convolutional network ( gcn ),0.7329230308532715
translation,481,80,model,model,employ,relational - gcns,model employ relational - gcns,0.6069258451461792
translation,481,166,model,each type of relations independently,remove,connections,each type of relations independently remove connections,0.7188460826873779
translation,481,166,model,each type of relations independently,remove,connections,each type of relations independently remove connections,0.7188460826873779
translation,481,166,model,connections,of,mentions,connections of mentions,0.5928649306297302
translation,481,166,model,connections,of,edges,connections of edges,0.5570292472839355
translation,481,166,model,connections,between,mentions,connections between mentions,0.7021358609199524
translation,481,166,model,mentions,co-occur in,same document ( doc - based ),mentions co-occur in same document ( doc - based ),0.7093002796173096
translation,481,166,model,mentions,matching,exactly ( match ),mentions matching exactly ( match ),0.744283139705658
translation,481,166,model,mentions,matching,exactly ( match ),mentions matching exactly ( match ),0.744283139705658
translation,481,166,model,edges,predicted by,coreference system ( coref ),edges predicted by coreference system ( coref ),0.7322019934654236
translation,481,38,results,full entity - gcn model,achieves,over 2 % improvement,full entity - gcn model achieves over 2 % improvement,0.6690618991851807
translation,481,38,results,over 2 % improvement,over,best previously - published results,over 2 % improvement over best previously - published results,0.6689291596412659
translation,481,38,results,recurrent document encoders,has,full entity - gcn model,recurrent document encoders has full entity - gcn model,0.5362403392791748
translation,481,126,results,outperforms,by,over 2 % points,outperforms by over 2 % points,0.6006318926811218
translation,481,126,results,all previous work,by,over 2 % points,all previous work by over 2 % points,0.5717262029647827
translation,481,126,results,entity - gcn,has,best single model without coreference edges,entity - gcn has best single model without coreference edges,0.5434998869895935
translation,481,126,results,best single model without coreference edges,has,outperforms,best single model without coreference edges has outperforms,0.6012530326843262
translation,481,126,results,outperforms,has,all previous work,outperforms has all previous work,0.5792452692985535
translation,481,126,results,results,has,entity - gcn,results has entity - gcn,0.5618014335632324
translation,481,148,results,first important observation,replacing,elmo,first important observation replacing elmo,0.6881816387176514
translation,481,148,results,elmo,by,glove,elmo by glove,0.6554710268974304
translation,481,148,results,elmo,yields,competitive system,elmo yields competitive system,0.7172582745552063
translation,481,148,results,competitive system,ranks far above,baselines,competitive system ranks far above baselines,0.6849043369293213
translation,481,148,results,competitive system,even above,coref-gru,competitive system even above coref-gru,0.6801929473876953
translation,481,148,results,coref-gru,in terms of,accuracy,coref-gru in terms of accuracy,0.7014018893241882
translation,481,148,results,accuracy,on,) validation set,accuracy on ) validation set,0.548285961151123
translation,481,148,results,results,replacing,elmo,results replacing elmo,0.5923036336898804
translation,481,148,results,results,has,first important observation,results has first important observation,0.554558515548706
translation,481,161,results,elmo representations,capture,predictive context features,elmo representations capture predictive context features,0.7028771042823792
translation,481,161,results,predictive context features,without being explicitly trained for,task,predictive context features without being explicitly trained for task,0.749840497970581
translation,481,161,results,results,shows that,elmo representations,results shows that elmo representations,0.6612547636032104
translation,481,165,results,only marginal improvements,with respect to,elmo alone,only marginal improvements with respect to elmo alone,0.6569820046424866
translation,481,165,results,only marginal improvements,in both,unmasked and masked setting,only marginal improvements in both unmasked and masked setting,0.7027155160903931
translation,481,165,results,results,observe,only marginal improvements,results observe only marginal improvements,0.6072043776512146
translation,481,174,results,coreference,observe,performance degradation,coreference observe performance degradation,0.5762815475463867
translation,481,174,results,performance degradation,on,test set,performance degradation on test set,0.5789337754249573
translation,481,174,results,results,with,coreference,results with coreference,0.6957241892814636
translation,481,181,results,most results,are,stronger,most results are stronger,0.5722964406013489
translation,481,181,results,stronger,for,masked settings,stronger for masked settings,0.6395084261894226
translation,481,181,results,results,are,stronger,results are stronger,0.606817901134491
translation,481,181,results,results,has,most results,results has most results,0.520275354385376
translation,481,194,results,considered harder,for,entity - gcn,considered harder for entity - gcn,0.6490068435668945
translation,481,194,results,results,questions regarding,places ( birth and death ),results questions regarding places ( birth and death ),0.7368344664573669
translation,482,23,baselines,"qanet ( yu et al. , 2018 )",has,efficient convolution and selfattention - based neural network,"qanet ( yu et al. , 2018 ) has efficient convolution and selfattention - based neural network",0.5506529211997986
translation,482,23,baselines,"bert ( devlin et al. , 2018 )",has,transformer - based pre-trained model,"bert ( devlin et al. , 2018 ) has transformer - based pre-trained model",0.5713037252426147
translation,482,175,baselines,three baselines,using,wikipedia 's search api,three baselines using wikipedia 's search api,0.6477930545806885
translation,482,175,baselines,second,through,google custom search engine,second through google custom search engine,0.654666543006897
translation,482,175,baselines,google custom search engine,restricted to,arabic wikipedia site,google custom search engine restricted to arabic wikipedia site,0.7081636786460876
translation,482,175,baselines,baselines,implement,three baselines,baselines implement three baselines,0.6644371151924133
translation,482,191,baselines,tf - idf reader,based on,4,tf - idf reader based on 4,0.7109929919242859
translation,482,191,baselines,4,-,gram features,4 - gram features,0.5642248392105103
translation,482,191,baselines,embedding approach,where,candidate,embedding approach where candidate,0.6996910572052002
translation,482,191,baselines,candidate,with,highest cosine similarity,candidate with highest cosine similarity,0.6690949201583862
translation,482,191,baselines,highest cosine similarity,with respect to,fast - text embeddings,highest cosine similarity with respect to fast - text embeddings,0.6217838525772095
translation,482,192,baselines,"qanet ( yu et al. , 2018 )",has,competitive mrc network,"qanet ( yu et al. , 2018 ) has competitive mrc network",0.5999974608421326
translation,482,192,baselines,baselines,compare against,"qanet ( yu et al. , 2018 )","baselines compare against qanet ( yu et al. , 2018 )",0.6276295781135559
translation,482,190,experimental-setup,candidate answers,by considering,every text span,candidate answers by considering every text span,0.7002132534980774
translation,482,190,experimental-setup,every text span,of,length,every text span of length,0.6162907481193542
translation,482,190,experimental-setup,maximally 10 words,in,each sentence,maximally 10 words in each sentence,0.5311342477798462
translation,482,190,experimental-setup,each sentence,as,candidate,each sentence as candidate,0.5611733198165894
translation,482,190,experimental-setup,length,has,maximally 10 words,length has maximally 10 words,0.5991788506507874
translation,482,196,experimental-setup,12 - layers,with,"h = 768 , 12 - heads","12 - layers with h = 768 , 12 - heads",0.6852273941040039
translation,482,196,experimental-setup,12 - layers,with,inputs,12 - layers with inputs,0.6421970129013062
translation,482,196,experimental-setup,"h = 768 , 12 - heads",for,self attention,"h = 768 , 12 - heads for self attention",0.5994274020195007
translation,482,196,experimental-setup,inputs,padded to,384 tokens,inputs padded to 384 tokens,0.7041513323783875
translation,482,7,experiments,arabic qa datasets,present,arabic reading comprehension dataset ( arcd ),arabic qa datasets present arabic reading comprehension dataset ( arcd ),0.6325414180755615
translation,482,7,experiments,arabic qa datasets,present,machine translation,arabic qa datasets present machine translation,0.5366548299789429
translation,482,7,experiments,arabic reading comprehension dataset ( arcd ),composed of,"1,395 questions","arabic reading comprehension dataset ( arcd ) composed of 1,395 questions",0.642764151096344
translation,482,7,experiments,arabic reading comprehension dataset ( arcd ),composed of,machine translation,arabic reading comprehension dataset ( arcd ) composed of machine translation,0.6386134624481201
translation,482,7,experiments,"1,395 questions",posed by,crowdworkers,"1,395 questions posed by crowdworkers",0.5907236933708191
translation,482,7,experiments,crowdworkers,on,wikipedia articles,crowdworkers on wikipedia articles,0.5457991361618042
translation,482,7,experiments,machine translation,of,stanford question answering dataset ( arabic - squad ),machine translation of stanford question answering dataset ( arabic - squad ),0.5356162190437317
translation,482,24,experiments,document retriever and reader,build,open domain qa system,document retriever and reader build open domain qa system,0.7174487709999084
translation,482,24,experiments,open domain qa system,named,soqal,open domain qa system named soqal,0.7172114253044128
translation,482,24,experiments,open domain qa system,by combining,confidence scores,open domain qa system by combining confidence scores,0.6431614756584167
translation,482,26,experiments,hierarchical tf - idf retriever,is,competitive,hierarchical tf - idf retriever is competitive,0.5864986181259155
translation,482,26,experiments,competitive,with,google search,competitive with google search,0.7080061435699463
translation,482,26,experiments,our bert reader,is,current state - of - the - art,our bert reader is current state - of - the - art,0.580471396446228
translation,482,26,experiments,current state - of - the - art,for,reading comprehension,current state - of - the - art for reading comprehension,0.5325973629951477
translation,482,176,experiments,embedding based retriever,using,fasttext embeddings,embedding based retriever using fasttext embeddings,0.6602945923805237
translation,482,176,experiments,fasttext embeddings,has,300 dimensional wikipedia pre-trained word embeddings,fasttext embeddings has 300 dimensional wikipedia pre-trained word embeddings,0.514836311340332
translation,482,176,experiments,each paragraph,has,representation,each paragraph has representation,0.5896694660186768
translation,482,180,experiments,google search,with,k = 10,google search with k = 10,0.6550261378288269
translation,482,180,experiments,google search,with,k = 350,google search with k = 350,0.6569795608520508
translation,482,180,experiments,k = 10,is,golden standard,k = 10 is golden standard,0.6104094982147217
translation,482,180,experiments,golden standard,with,75.6 %,golden standard with 75.6 %,0.6182355284690857
translation,482,180,experiments,tf -idf,using,bigram features,tf -idf using bigram features,0.655472457408905
translation,482,180,experiments,k = 350,able to come close,73.5 %,k = 350 able to come close 73.5 %,0.6825147271156311
translation,482,197,experiments,training set,of,arabic - squad,training set of arabic - squad,0.611166775226593
translation,482,197,experiments,arabic - squad,for,2 epochs,arabic - squad for 2 epochs,0.6718648672103882
translation,482,197,experiments,learning rate,of,3 ? 10 ?5,learning rate of 3 ? 10 ?5,0.652306854724884
translation,482,197,experiments,qanet,use,fasttext embeddings,qanet use fasttext embeddings,0.6082158088684082
translation,482,197,experiments,qanet,train for,total of 4 epochs,qanet train for total of 4 epochs,0.7320683598518372
translation,482,202,experiments,bert and qanet,on,test set,bert and qanet on test set,0.5696080923080444
translation,482,202,experiments,test set,of,arabic - squad,test set of arabic - squad,0.5988510251045227
translation,482,202,experiments,arabic - squad,reach,44.4 and 48.6 f1 scores,arabic - squad reach 44.4 and 48.6 f1 scores,0.6996358036994934
translation,482,209,experiments,open domain approach soqal,on,arcd - test,open domain approach soqal on arcd - test,0.5916881561279297
translation,482,8,model,open domain question answering in arabic ( soqal ),based on,two components,open domain question answering in arabic ( soqal ) based on two components,0.6538282632827759
translation,482,8,model,document retriever,using,hierarchical tf - idf approach,document retriever using hierarchical tf - idf approach,0.670621395111084
translation,482,8,model,neural reading comprehension model,using,pre-trained bi-directional transformer bert,neural reading comprehension model using pre-trained bi-directional transformer bert,0.6700703501701355
translation,482,8,model,two components,has,document retriever,two components has document retriever,0.5817272663116455
translation,482,8,model,two components,has,neural reading comprehension model,two components has neural reading comprehension model,0.5112200975418091
translation,482,179,results,simple tf - idf unigram retriever,able to beat,wikipedia api baseline,simple tf - idf unigram retriever able to beat wikipedia api baseline,0.6790284514427185
translation,482,179,results,results,find that,simple tf - idf unigram retriever,results find that simple tf - idf unigram retriever,0.6352117657661438
translation,482,179,results,results,even,simple tf - idf unigram retriever,results even simple tf - idf unigram retriever,0.6536418795585632
translation,482,181,results,hierarchical approach,of adding,second 4 - gram tf - idf retriever,hierarchical approach of adding second 4 - gram tf - idf retriever,0.6101356744766235
translation,482,181,results,second 4 - gram tf - idf retriever,to,bigram k = 1000 retriever,second 4 - gram tf - idf retriever to bigram k = 1000 retriever,0.541813313961029
translation,482,181,results,bigram k = 1000 retriever,achieves,respectable 65.3 %,bigram k = 1000 retriever achieves respectable 65.3 %,0.6639924645423889
translation,482,181,results,respectable 65.3 %,improving on,single bigram,respectable 65.3 % improving on single bigram,0.7491030097007751
translation,482,181,results,single bigram,by,17.6 %,single bigram by 17.6 %,0.5399717092514038
translation,482,181,results,reduction,of,8.2 %,reduction of 8.2 %,0.5765385627746582
translation,482,181,results,8.2 %,from,full k = 350 retriever,8.2 % from full k = 350 retriever,0.5556961894035339
translation,482,181,results,results,Using,hierarchical approach,results Using hierarchical approach,0.6744081974029541
translation,482,201,results,embedding and tf - idf readers,reach,sentence match accuracy,embedding and tf - idf readers reach sentence match accuracy,0.6480013132095337
translation,482,201,results,sentence match accuracy,of,almost 75 %,sentence match accuracy of almost 75 %,0.5788695812225342
translation,482,201,results,results,has,embedding and tf - idf readers,results has embedding and tf - idf readers,0.5096757411956787
translation,482,203,results,both neural mrc models,able to perform,well,both neural mrc models able to perform well,0.701906144618988
translation,482,203,results,well,transferring,knowledge,well transferring knowledge,0.7719402313232422
translation,482,203,results,knowledge,from,arabic - squad,knowledge from arabic - squad,0.607906699180603
translation,482,203,results,arabic - squad,with,bert,arabic - squad with bert,0.740447998046875
translation,482,203,results,bert,reaching,remarkable 90.08 sm accuracy,bert reaching remarkable 90.08 sm accuracy,0.7382584810256958
translation,482,203,results,arcd,has,both neural mrc models,arcd has both neural mrc models,0.6087700724601746
translation,482,203,results,results,without having been trained on,arcd,results without having been trained on arcd,0.7014041543006897
translation,482,207,results,both datasets,obtain,improvement,both datasets obtain improvement,0.6129103302955627
translation,482,207,results,improvement,of,8.3 %,improvement of 8.3 %,0.568259596824646
translation,482,207,results,8.3 %,on,f1 score,8.3 % on f1 score,0.5260331034660339
translation,482,207,results,8.3 %,with,total score,8.3 % with total score,0.6014882922172546
translation,482,207,results,total score,of,61.3,total score of 61.3,0.5144357085227966
translation,482,207,results,results,combining,both datasets,results combining both datasets,0.6704176664352417
translation,483,130,ablation-analysis,accuracy,of,qacnn,accuracy of qacnn,0.6024371385574341
translation,483,130,ablation-analysis,qacnn,drops by,4.6 %,qacnn drops by 4.6 %,0.7440623641014099
translation,483,130,ablation-analysis,4.6 %,compared to,fine-tuning,4.6 % compared to fine-tuning,0.6386351585388184
translation,483,130,ablation-analysis,mc500,has,accuracy,mc500 has accuracy,0.6053109169006348
translation,483,130,ablation-analysis,fine-tuning,has,last two fully - connected layers,fine-tuning has last two fully - connected layers,0.5134458541870117
translation,483,130,ablation-analysis,ablation analysis,For,mc500,ablation analysis For mc500,0.6491294503211975
translation,483,150,ablation-analysis,small amount of source data,can,help,small amount of source data can help,0.5713247656822205
translation,483,150,ablation-analysis,ablation analysis,find that,small amount of source data,ablation analysis find that small amount of source data,0.6194868087768555
translation,483,150,ablation-analysis,ablation analysis,even,small amount of source data,ablation analysis even small amount of source data,0.6384176015853882
translation,483,25,experiments,"mctest ( richardson et al. , 2013 )",with,transfer learning,"mctest ( richardson et al. , 2013 ) with transfer learning",0.6147825121879578
translation,483,25,experiments,transfer learning,from,"movieqa ( tapaswi et al. , 2016 )","transfer learning from movieqa ( tapaswi et al. , 2016 )",0.5056977868080139
translation,483,178,experiments,qacnn and a memn2n,as,qa models,qacnn and a memn2n as qa models,0.5604444146156311
translation,483,178,experiments,qa models,with,movieqa,qa models with movieqa,0.6469218134880066
translation,483,178,experiments,movieqa,as,source task,movieqa as source task,0.47882768511772156
translation,483,178,experiments,mctest,as,target tasks,mctest as target tasks,0.5275444984436035
translation,483,86,hyperparameters,hyperparameters,adopt,end-to - end memory network ( memn2n ),hyperparameters adopt end-to - end memory network ( memn2n ),0.6311807632446289
translation,483,86,hyperparameters,hyperparameters,adopt,query - based attention cnn,hyperparameters adopt query - based attention cnn,0.5907554030418396
translation,483,26,model,model,pre-trained on,movieqa,model pre-trained on movieqa,0.7344359755516052
translation,483,7,results,state - of - the - art,on,all target datasets,state - of - the - art on all target datasets,0.4847985506057739
translation,483,7,results,previous best model,by,7 %,previous best model by 7 %,0.598101794719696
translation,483,7,results,outperforms,has,previous best model,outperforms has previous best model,0.6195195317268372
translation,483,121,results,previous best models,on,toefl - manual,previous best models on toefl - manual,0.5173574090003967
translation,483,121,results,previous best models,by,7 %,previous best models by 7 %,0.5924153923988342
translation,483,121,results,toefl - manual,by,7 %,toefl - manual by 7 %,0.6103811264038086
translation,483,121,results,"toefl - asr ( fang et al. , 2016 )",by,6.5 %,"toefl - asr ( fang et al. , 2016 ) by 6.5 %",0.54012531042099
translation,483,121,results,"mc160 ( wang et al. , 2015 )",by,1.1 %,"mc160 ( wang et al. , 2015 ) by 1.1 %",0.5426635146141052
translation,483,121,results,"mc160 ( wang et al. , 2015 )",by,1.3 %,"mc160 ( wang et al. , 2015 ) by 1.3 %",0.5396879315376282
translation,483,121,results,"mc160 ( wang et al. , 2015 )",by,1.3 %,"mc160 ( wang et al. , 2015 ) by 1.3 %",0.5396879315376282
translation,483,121,results,mc500,by,1.3 %,mc500 by 1.3 %,0.6167160868644714
translation,483,121,results,", 2016 )",by,1.3 %,", 2016 ) by 1.3 %",0.5665205121040344
translation,483,121,results,transfer learning,has,qacnn,transfer learning has qacnn,0.5785887837409973
translation,483,121,results,qacnn,has,outperforms,qacnn has outperforms,0.6526733040809631
translation,483,121,results,outperforms,has,previous best models,outperforms has previous best models,0.601570725440979
translation,483,121,results,results,with,transfer learning,results with transfer learning,0.5967845916748047
translation,483,134,results,memn2n,trained directly on,target dataset,memn2n trained directly on target dataset,0.751259446144104
translation,483,134,results,target dataset,without pre-training,movieqa,target dataset without pre-training movieqa,0.7555100321769714
translation,483,134,results,fine-tuning,on,target dataset,fine-tuning on target dataset,0.5398470759391785
translation,483,134,results,results,to see that,memn2n,results to see that memn2n,0.6529691219329834
translation,483,144,results,more training data,used for,fine-tuning,more training data used for fine-tuning,0.6496644616127014
translation,483,144,results,better,has,model 's performance,better has model 's performance,0.5581145286560059
translation,483,144,results,results,has,more training data,results has more training data,0.5420150756835938
translation,483,165,results,without ground truth,in,target dataset,without ground truth in target dataset,0.46823665499687195
translation,483,165,results,without ground truth,for,supervised fine-tuning,without ground truth for supervised fine-tuning,0.5975790023803711
translation,483,165,results,transfer learning,from,source dataset,transfer learning from source dataset,0.5110346078872681
translation,483,165,results,performance,through,simple iterative self - labeling mechanism,performance through simple iterative self - labeling mechanism,0.6977437138557434
translation,483,165,results,improve,has,performance,improve has performance,0.5578044652938843
translation,483,165,results,results,observe,without ground truth,results observe without ground truth,0.6204159259796143
translation,483,166,results,qacnn,achieves,highest testing accuracy,qacnn achieves highest testing accuracy,0.6901906728744507
translation,483,166,results,highest testing accuracy,at,epoch 7 and 8,highest testing accuracy at epoch 7 and 8,0.5483061671257019
translation,483,166,results,without fine-tuning,by,approximately 4 % and 5 %,without fine-tuning by approximately 4 % and 5 %,0.641687273979187
translation,483,166,results,toefl - manual and toefl - asr,has,qacnn,toefl - manual and toefl - asr has qacnn,0.6227648854255676
translation,483,166,results,outperforming,has,counterpart,outperforming has counterpart,0.6323723196983337
translation,483,166,results,outperforming,has,without fine-tuning,outperforming has without fine-tuning,0.5832394957542419
translation,483,166,results,counterpart,has,without fine-tuning,counterpart has without fine-tuning,0.6302703022956848
translation,483,166,results,results,For,toefl - manual and toefl - asr,results For toefl - manual and toefl - asr,0.587568461894989
translation,483,167,results,qacnn,achieves,peak,qacnn achieves peak,0.6856057643890381
translation,483,167,results,peak,at,epoch 3 and 6,peak at epoch 3 and 6,0.5572693943977356
translation,483,167,results,without fine- tuning,by,about 2 % and 6 %,without fine- tuning by about 2 % and 6 %,0.6433903574943542
translation,483,167,results,mc160 and mc500,has,qacnn,mc160 and mc500 has qacnn,0.6632196307182312
translation,483,167,results,outperforming,has,counterpart,outperforming has counterpart,0.6323723196983337
translation,483,167,results,outperforming,has,without fine- tuning,outperforming has without fine- tuning,0.5832394957542419
translation,483,167,results,counterpart,has,without fine- tuning,counterpart has without fine- tuning,0.6302703022956848
translation,483,167,results,results,For,mc160 and mc500,results For mc160 and mc500,0.6236329674720764
translation,483,168,results,performance,of,unsupervised transfer learning,performance of unsupervised transfer learning,0.5871511101722717
translation,483,168,results,unsupervised transfer learning,still worse than,supervised transfer learning,unsupervised transfer learning still worse than supervised transfer learning,0.6393584609031677
translation,483,168,results,results,show that,performance,results show that performance,0.510168194770813
translation,483,179,results,performance,of,both models,performance of both models,0.5794581770896912
translation,483,179,results,both models,on,target datasets,both models on target datasets,0.5589392781257629
translation,483,179,results,movieqa,has,performance,movieqa has performance,0.5559262633323669
translation,483,179,results,results,pre-training on,movieqa,results pre-training on movieqa,0.7141129374504089
translation,484,7,model,automated question answering,for,middle school science topics,automated question answering for middle school science topics,0.5470336675643921
translation,484,7,model,curio,has,automated question answering,curio has automated question answering,0.6005319356918335
translation,484,7,model,model,introduce,curio,model introduce curio,0.6347122192382812
translation,485,26,ablation-analysis,embedding,optimized for,maximizing,embedding optimized for maximizing,0.8218194246292114
translation,485,26,ablation-analysis,inner products,of,question and relevant passage vectors,inner products of question and relevant passage vectors,0.5729336142539978
translation,485,26,ablation-analysis,maximizing,has,inner products,maximizing has inner products,0.528256893157959
translation,485,108,experimental-setup,dpr model,trained using,in- batch negative setting,dpr model trained using in- batch negative setting,0.732820451259613
translation,485,108,experimental-setup,in- batch negative setting,with,batch size,in- batch negative setting with batch size,0.6819749474525452
translation,485,108,experimental-setup,batch size,of,128,batch size of 128,0.6836684346199036
translation,485,108,experimental-setup,experimental setup,has,dpr model,experimental setup has dpr model,0.5282593369483948
translation,485,109,experimental-setup,question and passage encoders,for,up to 40 epochs,question and passage encoders for up to 40 epochs,0.6038590669631958
translation,485,109,experimental-setup,question and passage encoders,with,learning rate,question and passage encoders with learning rate,0.6105915307998657
translation,485,109,experimental-setup,up to 40 epochs,for,"large datasets ( nq , triviaqa , squad )","up to 40 epochs for large datasets ( nq , triviaqa , squad )",0.5666937828063965
translation,485,109,experimental-setup,up to 40 epochs,for,"small datasets ( trec , wq )","up to 40 epochs for small datasets ( trec , wq )",0.576766848564148
translation,485,109,experimental-setup,100 epochs,for,"small datasets ( trec , wq )","100 epochs for small datasets ( trec , wq )",0.560847282409668
translation,485,109,experimental-setup,learning rate,of,10 ?5,learning rate of 10 ?5,0.6583234071731567
translation,485,109,experimental-setup,learning rate,using,adam,learning rate using adam,0.6733478903770447
translation,485,109,experimental-setup,10 ?5,using,adam,10 ?5 using adam,0.6955401301383972
translation,485,109,experimental-setup,linear scheduling,with,warm - up,linear scheduling with warm - up,0.669691801071167
translation,485,109,experimental-setup,linear scheduling,with,dropout rate,linear scheduling with dropout rate,0.6333652138710022
translation,485,109,experimental-setup,dropout rate,has,0.1,dropout rate has 0.1,0.5073758363723755
translation,485,109,experimental-setup,experimental setup,trained,question and passage encoders,experimental setup trained question and passage encoders,0.6929382681846619
translation,485,123,experimental-setup,bm25 parameters b = 0.4,has,document length normalization ),bm25 parameters b = 0.4 has document length normalization ),0.5163452625274658
translation,485,123,experimental-setup,experimental setup,has,bm25 parameters b = 0.4,experimental setup has bm25 parameters b = 0.4,0.539912760257721
translation,485,138,experimental-setup,experimental setup,has,in - batch negative training,experimental setup has in - batch negative training,0.5240231156349182
translation,485,168,experimental-setup,passage retrieval speed,on,server,passage retrieval speed on server,0.5035575032234192
translation,485,168,experimental-setup,server,with,intel xeon cpu e5-2698 v4 @ 2.20 ghz,server with intel xeon cpu e5-2698 v4 @ 2.20 ghz,0.5178163647651672
translation,485,168,experimental-setup,experimental setup,profiled,passage retrieval speed,experimental setup profiled passage retrieval speed,0.6911411881446838
translation,485,192,experimental-setup,batch size,of,16,batch size of 16,0.6842944622039795
translation,485,192,experimental-setup,batch size,of,4,batch size of 4,0.6922571659088135
translation,485,192,experimental-setup,16,for,"large ( nq , triviaqa , squad )","16 for large ( nq , triviaqa , squad )",0.6219247579574585
translation,485,192,experimental-setup,4,for,"small ( trec , wq ) datasets","4 for small ( trec , wq ) datasets",0.5580451488494873
translation,485,192,experimental-setup,experimental setup,use,batch size,experimental setup use batch size,0.6168987154960632
translation,485,192,experimental-setup,experimental setup,for,"small ( trec , wq ) datasets","experimental setup for small ( trec , wq ) datasets",0.5643171668052673
translation,485,194,experimental-setup,experiments,done on,eight 32gb gpus,experiments done on eight 32gb gpus,0.621471643447876
translation,485,194,experimental-setup,experimental setup,done on,eight 32gb gpus,experimental setup done on eight 32gb gpus,0.6452752947807312
translation,485,194,experimental-setup,experimental setup,has,experiments,experimental setup has experiments,0.5502888560295105
translation,485,118,experiments,dpr,performs,consistently better,dpr performs consistently better,0.6609157919883728
translation,485,118,experiments,consistently better,than,bm25,consistently better than bm25,0.6366584897041321
translation,485,118,experiments,bm25,on,all datasets,bm25 on all datasets,0.5122029185295105
translation,485,118,experiments,squad,has,dpr,squad has dpr,0.5976907014846802
translation,485,5,model,retrieval,practically implemented using,dense representations,retrieval practically implemented using dense representations,0.7316542863845825
translation,485,5,model,dense representations,where,embeddings,dense representations where embeddings,0.6297526955604553
translation,485,5,model,embeddings,learned from,small number of questions and passages,embeddings learned from small number of questions and passages,0.6707430481910706
translation,485,5,model,small number of questions and passages,by,simple dualencoder framework,small number of questions and passages by simple dualencoder framework,0.5728368759155273
translation,485,5,model,model,show that,retrieval,model show that retrieval,0.5478585362434387
translation,485,191,model,training objective,maximize,marginal log-likelihood,training objective maximize marginal log-likelihood,0.7390291690826416
translation,485,191,model,marginal log-likelihood,of,all the correct answer spans,marginal log-likelihood of all the correct answer spans,0.6026066541671753
translation,485,191,model,all the correct answer spans,in,positive passage,all the correct answer spans in positive passage,0.5239240527153015
translation,485,191,model,model,has,training objective,model has training objective,0.5122246146202087
translation,485,137,results,more training examples ( from 1 k to 59 k ),improves,retrieval accuracy,more training examples ( from 1 k to 59 k ) improves retrieval accuracy,0.6802619099617004
translation,485,137,results,results,Adding,more training examples ( from 1 k to 59 k ),results Adding more training examples ( from 1 k to 59 k ),0.7001376748085022
translation,485,150,results,single bm25 negative passage,improves,result substantially,single bm25 negative passage improves result substantially,0.6914933919906616
translation,485,150,results,two,has,does not help,two has does not help,0.6065742373466492
translation,485,150,results,results,adding,single bm25 negative passage,results adding single bm25 negative passage,0.6159126162528992
translation,485,150,results,results,adding,two,results adding two,0.6213003993034363
translation,485,161,results,dpr,generalizes,well,dpr generalizes well,0.7685452699661255
translation,485,161,results,well,with,3 - 5 points loss,well with 3 - 5 points loss,0.6730740070343018
translation,485,161,results,3 - 5 points loss,from,best performing fine - tuned model,3 - 5 points loss from best performing fine - tuned model,0.5890405774116516
translation,485,161,results,best performing fine - tuned model,in,top - 20 retrieval accuracy,best performing fine - tuned model in top - 20 retrieval accuracy,0.4827907383441925
translation,485,161,results,75.0/89.1,for,webquestions and trec,75.0/89.1 for webquestions and trec,0.6062830686569214
translation,485,161,results,top - 20 retrieval accuracy,has,69.9/86.3,top - 20 retrieval accuracy has 69.9/86.3,0.5404053926467896
translation,485,161,results,greatly outperforming,has,bm25 baseline,greatly outperforming has bm25 baseline,0.5750594139099121
translation,485,161,results,greatly outperforming,has,),greatly outperforming has ),0.6699875593185425
translation,485,161,results,bm25 baseline,has,),bm25 baseline has ),0.6119022965431213
translation,485,161,results,results,find that,dpr,results find that dpr,0.667201578617096
translation,485,198,results,accuracy,leads to,better final qa results,accuracy leads to better final qa results,0.656558096408844
translation,485,198,results,answers,extracted from,passages,answers extracted from passages,0.5882967114448547
translation,485,198,results,passages,retrieved by,dpr,passages retrieved by dpr,0.6180726885795593
translation,485,198,results,better final qa results,has,answers,better final qa results has answers,0.5680431127548218
translation,485,198,results,squad,has,answers,squad has answers,0.6514284014701843
translation,485,199,results,large datasets,like,nq and triviaqa,large datasets like nq and triviaqa,0.6203473806381226
translation,485,199,results,models,trained using,multiple datasets ( multi ),models trained using multiple datasets ( multi ),0.7336090803146362
translation,485,199,results,multiple datasets ( multi ),perform,comparably,multiple datasets ( multi ) perform comparably,0.5821524858474731
translation,485,199,results,comparably,trained using,individual training set ( single ),comparably trained using individual training set ( single ),0.7588279247283936
translation,485,199,results,large datasets,has,models,large datasets has models,0.5300772190093994
translation,485,199,results,nq and triviaqa,has,models,nq and triviaqa has models,0.6133891940116882
translation,485,199,results,results,For,large datasets,results For large datasets,0.5654430389404297
translation,485,200,results,smaller datasets,like,wq and trec,smaller datasets like wq and trec,0.6287485957145691
translation,485,200,results,smaller datasets,has,multidataset setting,smaller datasets has multidataset setting,0.5406977534294128
translation,485,200,results,wq and trec,has,multidataset setting,wq and trec has multidataset setting,0.5601373910903931
translation,485,200,results,multidataset setting,has,clear advantage,multidataset setting has clear advantage,0.5558995604515076
translation,485,200,results,results,on,smaller datasets,results on smaller datasets,0.5212916731834412
translation,485,201,results,previous stateof - the - art results,on,four out of the five datasets,previous stateof - the - art results on four out of the five datasets,0.4811980426311493
translation,485,201,results,1 % to 12 % absolute differences,in,exact match accuracy,1 % to 12 % absolute differences in exact match accuracy,0.5536264181137085
translation,485,201,results,dpr - based models,has,outperform,dpr - based models has outperform,0.6030398011207581
translation,485,201,results,outperform,has,previous stateof - the - art results,outperform has previous stateof - the - art results,0.562682569026947
translation,485,201,results,results,has,dpr - based models,results has dpr - based models,0.5069544315338135
translation,485,203,results,to outperform,on,nq and triviaqa,to outperform on nq and triviaqa,0.6188568472862244
translation,485,203,results,pairs,of,questions and answers,pairs of questions and answers,0.615723729133606
translation,485,205,results,results,of,dpr,results of dpr,0.5889840722084045
translation,485,205,results,dpr,on,wq and trec,dpr on wq and trec,0.5932283401489258
translation,485,205,results,dpr,are,less competitive,dpr are less competitive,0.6064149141311646
translation,485,205,results,dpr,adding,more question - answer pairs,dpr adding more question - answer pairs,0.691185712814331
translation,485,205,results,wq and trec,in,single - dataset setting,wq and trec in single - dataset setting,0.5420531630516052
translation,485,205,results,more question - answer pairs,helps,boost,more question - answer pairs helps boost,0.6727075576782227
translation,485,205,results,more question - answer pairs,achieving,new state of the art,more question - answer pairs achieving new state of the art,0.6160692572593689
translation,485,205,results,boost,achieving,new state of the art,boost achieving new state of the art,0.6521884202957153
translation,485,205,results,boost,has,performance,boost has performance,0.5791411995887756
translation,485,205,results,results,of,dpr,results of dpr,0.5889840722084045
translation,486,5,experiments,automatic news chatbot,draws content,diverse set of news articles,automatic news chatbot draws content diverse set of news articles,0.6402722001075745
translation,486,72,experiments,"gpt2 language model ( radford et al. , 2019 )",task of,question generation,"gpt2 language model ( radford et al. , 2019 ) task of question generation",0.685196578502655
translation,486,72,experiments,question generation,using,squad 2.0 dataset,question generation using squad 2.0 dataset,0.642906665802002
translation,486,16,model,news articles,into,chatrooms,news articles into chatrooms,0.5398693084716797
translation,486,16,model,chatrooms,revolving around,story,chatrooms revolving around story,0.6929953694343567
translation,486,18,model,each news story,set of,essential questions,each news story set of essential questions,0.6698940396308899
translation,486,18,model,each news story,link,each question,each news story link each question,0.6322311162948608
translation,486,18,model,each question,with,content,each question with content,0.6066066026687622
translation,486,18,model,model,For,each news story,model For each news story,0.5884167551994324
translation,486,126,results,systems with question recommendation enabled,obtained,higher average satisfaction,systems with question recommendation enabled obtained higher average satisfaction,0.6561613082885742
translation,486,126,results,higher average satisfaction,on,most measures,higher average satisfaction on most measures,0.4633403420448303
translation,486,126,results,most measures,than,noqr setting,most measures than noqr setting,0.5685443878173828
translation,486,126,results,quis satisfaction scores,has,systems with question recommendation enabled,quis satisfaction scores has systems with question recommendation enabled,0.614290177822113
translation,486,126,results,systems with question recommendation enabled,has,topqr and randqr ),systems with question recommendation enabled has topqr and randqr ),0.6359116435050964
translation,486,126,results,results,has,quis satisfaction scores,results has quis satisfaction scores,0.49273020029067993
translation,486,128,results,participants,rated,suggested questions,participants rated suggested questions,0.6683590412139893
translation,486,128,results,suggested questions,for,topqr,suggested questions for topqr,0.693526029586792
translation,486,128,results,almost 1 point higher,than,randqr,almost 1 point higher than randqr,0.614020824432373
translation,486,128,results,topqr,has,almost 1 point higher,topqr has almost 1 point higher,0.6120005249977112
translation,486,128,results,results,has,participants,results has participants,0.4287026822566986
translation,486,129,results,answers,to be,more informative,answers to be more informative,0.5764235258102417
translation,486,129,results,more informative,in,topqr setting,more informative in topqr setting,0.5447369813919067
translation,486,129,results,results,judged,answers,results judged answers,0.6687729358673096
translation,487,68,experimental-setup,indri toolkit,to implement,language model,indri toolkit to implement language model,0.6232163906097412
translation,487,68,experimental-setup,experimental setup,has,indri toolkit,experimental setup has indri toolkit,0.523474395275116
translation,487,6,model,5w1h question reformulation patterns,from,large scale search log data,5w1h question reformulation patterns from large scale search log data,0.6052599549293518
translation,487,6,model,model,automatically mined,5w1h question reformulation patterns,model automatically mined 5w1h question reformulation patterns,0.791343629360199
translation,487,7,model,question reformulations,incorporated into,retrieval model,question reformulations incorporated into retrieval model,0.6389544606208801
translation,487,7,model,model,has,question reformulations,model has question reformulations,0.5714610815048218
translation,487,15,model,method,of,automatically mining 5w1h question 1 reformulation patterns,method of automatically mining 5w1h question 1 reformulation patterns,0.5622762441635132
translation,487,15,model,automatically mining 5w1h question 1 reformulation patterns,to improve,search relevance of 5w1h questions,automatically mining 5w1h question 1 reformulation patterns to improve search relevance of 5w1h questions,0.6642544865608215
translation,487,15,model,model,propose,method,model propose method,0.6280754208564758
translation,487,21,model,reformulation patterns,as,key concept,reformulation patterns as key concept,0.5776104927062988
translation,487,21,model,reformulation patterns,propose,question reformulation framework,reformulation patterns propose question reformulation framework,0.6419091820716858
translation,487,21,model,model,Using,reformulation patterns,model Using reformulation patterns,0.689990222454071
translation,487,77,results,retrieval performance,of,natural language questions,retrieval performance of natural language questions,0.5247147679328918
translation,487,77,results,question reformulations,has,significantly improve,question reformulations has significantly improve,0.5675553679466248
translation,487,77,results,significantly improve,has,retrieval performance,significantly improve has retrieval performance,0.574329674243927
translation,487,77,results,results,shows,question reformulations,results shows question reformulations,0.6479905843734741
translation,487,77,results,results,using,question reformulations,results using question reformulations,0.65530925989151
translation,487,78,results,around 3 % improvement,with respect to,ndcg,around 3 % improvement with respect to ndcg,0.6367610692977905
translation,487,78,results,very interesting result,for,web search,very interesting result for web search,0.5998166799545288
translation,487,78,results,queries ),has,around 3 % improvement,queries ) has around 3 % improvement,0.6190950274467468
translation,487,82,results,performance,of,orig,performance of orig,0.6952046751976013
translation,487,82,results,performance,improved by,around 30 %,performance improved by around 30 %,0.7366166114807129
translation,487,82,results,around 30 %,from,0.2926,around 30 % from 0.2926,0.596882164478302
translation,487,82,results,0.2926,to,0.3826,0.2926 to 0.3826,0.6033925414085388
translation,487,82,results,0.3826,with respect to,ndcg@1,0.3826 with respect to ndcg@1,0.6547483801841736
translation,487,82,results,best question reformulation,has,performance,best question reformulation has performance,0.5801532864570618
translation,487,90,results,reformulations,make,morphological change,reformulations make morphological change,0.6558893918991089
translation,487,90,results,morphological change,do,not have much effect,morphological change do not have much effect,0.5015868544578552
translation,487,90,results,not have much effect,on,improving,not have much effect on improving,0.6078439354896545
translation,487,90,results,improving,has,original question,improving has original question,0.5940098762512207
translation,487,90,results,results,has,reformulations,results has reformulations,0.5352088809013367
translation,487,102,results,both nostop and dropone,perform,worse,both nostop and dropone perform worse,0.6748251914978027
translation,487,102,results,worse,than,original question,worse than original question,0.5687575936317444
translation,487,102,results,worse,using,original question,worse using original question,0.651925265789032
translation,487,102,results,results,shows,both nostop and dropone,results shows both nostop and dropone,0.6400031447410583
translation,487,103,results,proposed method,has,outperforms,proposed method has outperforms,0.6315754055976868
translation,487,103,results,outperforms,has,all the baselines,outperforms has all the baselines,0.596747100353241
translation,487,103,results,results,has,proposed method,results has proposed method,0.5845219492912292
translation,489,172,ablation-analysis,multirc,show that,additional supervision,multirc show that additional supervision,0.5565412044525146
translation,489,172,ablation-analysis,additional supervision,for,relevance module,additional supervision for relevance module,0.6377456188201904
translation,489,172,ablation-analysis,additional supervision,leads to,even further improvements,additional supervision leads to even further improvements,0.6628290414810181
translation,489,172,ablation-analysis,even further improvements,in,score,even further improvements in score,0.6171488761901855
translation,489,172,ablation-analysis,ablation analysis,In,multirc,ablation analysis In multirc,0.5597777962684631
translation,489,184,ablation-analysis,multirc,only,first,multirc only first,0.7655434608459473
translation,489,184,ablation-analysis,multirc,only,significant contributor,multirc only significant contributor,0.7173284888267517
translation,489,184,ablation-analysis,ablation analysis,For,multirc,ablation analysis For multirc,0.6322123408317566
translation,489,185,ablation-analysis,bigger factor,for,multirc,bigger factor for multirc,0.6845136880874634
translation,489,185,ablation-analysis,ablation analysis,shows,re-purposing,ablation analysis shows re-purposing,0.6591339707374573
translation,489,145,baselines,baselines,using,entailment,baselines using entailment,0.6928813457489014
translation,489,145,baselines,entailment,as,black - box,entailment as black - box,0.538512110710144
translation,489,145,baselines,baselines,using,entailment,baselines using entailment,0.6928813457489014
translation,489,150,baselines,open-bookqa,report,published baselines,open-bookqa report published baselines,0.6829718947410583
translation,489,150,baselines,question match,with,elmo ( qm + elmo ),question match with elmo ( qm + elmo ),0.6409952640533447
translation,489,150,baselines,question to answer esim,with,elmo ( esim + elmo ),question to answer esim with elmo ( esim + elmo ),0.6318052411079407
translation,489,150,baselines,best result,with,knowledge enhanced reader ( ker ),best result with knowledge enhanced reader ( ker ),0.6469555497169495
translation,489,150,baselines,baselines,For,open-bookqa,baselines For open-bookqa,0.5913035273551941
translation,489,151,baselines,baselines,has,large transformer based models,baselines has large transformer based models,0.5406141877174377
translation,489,152,baselines,openai -transformer ( oft ),pre-trained on,large-scale language modeling task,openai -transformer ( oft ) pre-trained on large-scale language modeling task,0.7396371364593506
translation,489,152,baselines,openai -transformer ( oft ),finetuned on,respective datasets,openai -transformer ( oft ) finetuned on respective datasets,0.7248213887214661
translation,489,140,experimental-setup,allennlp,to implement,our models,allennlp to implement our models,0.7287783622741699
translation,489,140,experimental-setup,allennlp,ran them on,beaker,allennlp ran them on beaker,0.6537220478057861
translation,489,140,experimental-setup,experimental setup,used,pytorch,experimental setup used pytorch,0.5899403691291809
translation,489,158,experiments,ensemble version of openai transformer,by,3.0 points in accuracy,ensemble version of openai transformer by 3.0 points in accuracy,0.5702157616615295
translation,489,158,experiments,openbookqa test set,has,multee with glove,openbookqa test set has multee with glove,0.6226959228515625
translation,489,158,experiments,multee with glove,has,outperforms,multee with glove has outperforms,0.6817124485969543
translation,489,158,experiments,outperforms,has,ensemble version of openai transformer,outperforms has ensemble version of openai transformer,0.5829172134399414
translation,489,6,model,general architecture,effectively use,entailment models,general architecture effectively use entailment models,0.6604599952697754
translation,489,6,model,entailment models,for,multi-hop qa tasks,entailment models for multi-hop qa tasks,0.5567654371261597
translation,489,6,model,multee,has,general architecture,multee has general architecture,0.6299335360527039
translation,489,6,model,model,introduce,multee,model introduce multee,0.6467968821525574
translation,489,7,model,multee,uses,local module,multee uses local module,0.6595667600631714
translation,489,7,model,multee,uses,global module,multee uses global module,0.6611505746841431
translation,489,7,model,local module,helps locate,important sentences,local module helps locate important sentences,0.6740406155586243
translation,489,7,model,global module,aggregates,information,global module aggregates information,0.7135311365127563
translation,489,7,model,information,by effectively incorporating,importance weights,information by effectively incorporating importance weights,0.6875494718551636
translation,489,7,model,model,has,multee,model has multee,0.66465824842453
translation,489,8,model,entailment functions,pre-trained on,large scale nli datasets,entailment functions pre-trained on large scale nli datasets,0.7026877403259277
translation,489,13,model,neural entailment models,for,qa,neural entailment models for qa,0.5865232348442078
translation,489,13,model,repurposing,has,neural entailment models,repurposing has neural entailment models,0.5122100114822388
translation,489,53,model,entailment based qa model,with,two components,entailment based qa model with two components,0.6148568391799927
translation,489,53,model,multee,with,two components,multee with two components,0.7282975316047668
translation,489,53,model,sentence relevance model,learns to,focus,sentence relevance model learns to focus,0.6508941054344177
translation,489,53,model,focus,on,relevant sentences,focus on relevant sentences,0.5282551050186157
translation,489,53,model,multi-layer aggregator,uses,entailment model,multi-layer aggregator uses entailment model,0.5209211707115173
translation,489,53,model,entailment model,to obtain,multiple layers,entailment model to obtain multiple layers,0.5979026556015015
translation,489,53,model,multiple layers,of,question - relevant representations,multiple layers of question - relevant representations,0.5689418315887451
translation,489,53,model,question - relevant representations,for,premises,question - relevant representations for premises,0.6301513910293579
translation,489,53,model,sentence - level scores,from,relevance model,sentence - level scores from relevance model,0.51893550157547
translation,489,53,model,model,propose,entailment based qa model,model propose entailment based qa model,0.6338488459587097
translation,489,121,model,aggregation,at,twolevels,aggregation at twolevels,0.5552121996879578
translation,489,121,model,aggregation,one at,cross-attention level ( ca ),aggregation one at cross-attention level ( ca ),0.5922262072563171
translation,489,121,model,aggregation,one at,final layer ( fl ),aggregation one at final layer ( fl ),0.6147295832633972
translation,489,121,model,model,uses,aggregation,model uses aggregation,0.679521381855011
translation,489,174,model,multee,performs,aggregation,multee performs aggregation,0.6571674346923828
translation,489,174,model,aggregation,at,two levels,aggregation at two levels,0.5970052480697632
translation,489,174,model,model,has,multee,model has multee,0.66465824842453
translation,489,190,model,binary cross entropy loss ( bce ),allows,model,binary cross entropy loss ( bce ) allows model,0.6378476619720459
translation,489,190,model,model,attend to,more relevant sentences,model attend to more relevant sentences,0.6511067152023315
translation,489,190,model,more relevant sentences,thereby increasing,recall,more relevant sentences thereby increasing recall,0.675567626953125
translation,489,190,model,recall,without too much drop,precision,recall without too much drop precision,0.7518998980522156
translation,489,190,model,model,has,binary cross entropy loss ( bce ),model has binary cross entropy loss ( bce ),0.5481383204460144
translation,489,212,model,multee,has,novel qa model,multee has novel qa model,0.6470792889595032
translation,489,212,model,model,propose,multee,model propose multee,0.6907428503036499
translation,489,10,results,entailment function,pre-trained on,nli datasets,entailment function pre-trained on nli datasets,0.7543784379959106
translation,489,10,results,qa models,trained only on,target qa datasets,qa models trained only on target qa datasets,0.7220902442932129
translation,489,10,results,qa models,trained only on,openai transformer models,qa models trained only on openai transformer models,0.724151611328125
translation,489,10,results,entailment function,has,multee,entailment function has multee,0.6343758702278137
translation,489,10,results,nli datasets,has,multee,nli datasets has multee,0.6331360936164856
translation,489,10,results,multee,has,outperforms,multee has outperforms,0.6731275320053101
translation,489,10,results,outperforms,has,qa models,outperforms has qa models,0.632635772228241
translation,489,10,results,results,When using,entailment function,results When using entailment function,0.6908543705940247
translation,489,33,results,elmo contextual embeddings,matches,state - of - the - art results,elmo contextual embeddings matches state - of - the - art results,0.6819344758987427
translation,489,33,results,state - of - the - art results,achieved with,large transfomer - based models,state - of - the - art results achieved with large transfomer - based models,0.655380129814148
translation,489,33,results,large transfomer - based models,trained on,sequence of large scale tasks,large transfomer - based models trained on sequence of large scale tasks,0.7555568218231201
translation,489,33,results,results,Multee using,elmo contextual embeddings,results Multee using elmo contextual embeddings,0.5844762325286865
translation,489,156,results,black - box entailment baselines ( concat and max ),pretrained on,same data,black - box entailment baselines ( concat and max ) pretrained on same data,0.7464790344238281
translation,489,156,results,multee,has,outperforms,multee has outperforms,0.6731275320053101
translation,489,156,results,outperforms,has,black - box entailment baselines ( concat and max ),outperforms has black - box entailment baselines ( concat and max ),0.5957518219947815
translation,489,156,results,results,has,multee,results has multee,0.576582670211792
translation,489,159,results,outperforms,comparable to,ensemble version,outperforms comparable to ensemble version,0.6770066618919373
translation,489,159,results,single model version,of,reading strategies system,single model version of reading strategies system,0.5949286818504333
translation,489,159,results,outperforms,has,single model version,outperforms has single model version,0.625635027885437
translation,489,159,results,results,has,outperforms,results has outperforms,0.6657275557518005
translation,489,160,results,multee with elmo,outperforms,ensemble version of openai transformer,multee with elmo outperforms ensemble version of openai transformer,0.7309132814407349
translation,489,160,results,ensemble version of openai transformer,by,1.9 points,ensemble version of openai transformer by 1.9 points,0.5760939717292786
translation,489,160,results,ensemble version of openai transformer,by,2.7,ensemble version of openai transformer by 2.7,0.5802647471427917
translation,489,160,results,ensemble version of openai transformer,by,6.3,ensemble version of openai transformer by 6.3,0.5743369460105896
translation,489,160,results,1.9 points,in,f1a,1.9 points in f1a,0.5528054237365723
translation,489,160,results,2.7,in,f1 m,2.7 in f1 m,0.6056013107299805
translation,489,160,results,6.3,in,em,6.3 in em,0.6219454407691956
translation,489,160,results,multirc dev set,has,multee with elmo,multirc dev set has multee with elmo,0.6494694352149963
translation,489,160,results,results,On,multirc dev set,results On multirc dev set,0.6168397665023804
translation,489,161,results,outperforms,comparable to,ensemble version,outperforms comparable to ensemble version,0.6770066618919373
translation,489,161,results,single model version,of,reading strategies system,single model version of reading strategies system,0.5949286818504333
translation,489,161,results,outperforms,has,single model version,outperforms has single model version,0.625635027885437
translation,489,161,results,results,has,outperforms,results has outperforms,0.6657275557518005
translation,489,163,results,elmo contextual embeddings,helped in,multirc,elmo contextual embeddings helped in multirc,0.595068633556366
translation,489,163,results,elmo contextual embeddings,not help,openbookqa,elmo contextual embeddings not help openbookqa,0.5953788757324219
translation,489,165,results,more prominent,in,openbookqa,more prominent in openbookqa,0.5625082850456238
translation,489,165,results,openbookqa,than in,multirc,openbookqa than in multirc,0.7250597476959229
translation,489,165,results,results,gains from,multee,results gains from multee,0.6944279074668884
translation,489,177,results,multi-layer aggregation,better than,ca or fl alone,multi-layer aggregation better than ca or fl alone,0.7686872482299805
translation,489,177,results,results,shows that,multi-layer aggregation,results shows that multi-layer aggregation,0.6727067828178406
translation,489,181,results,model,trained from,scratch,model trained from scratch,0.7898691892623901
translation,489,181,results,model,is,substantially worse,model is substantially worse,0.6002184152603149
translation,489,181,results,scratch,is,substantially worse,scratch is substantially worse,0.6053496599197388
translation,489,181,results,substantially worse,in the case of,openbookqa,substantially worse in the case of openbookqa,0.6189303994178772
translation,489,181,results,substantially worse,highlighting,benefits,substantially worse highlighting benefits,0.6692174077033997
translation,489,181,results,results,has,model,results has model,0.5339115858078003
translation,489,183,results,results,In the case of,openbookqa,results In the case of openbookqa,0.697746753692627
translation,489,192,results,our model,with,bce loss,our model with bce loss,0.657620370388031
translation,489,192,results,bce loss,distribute,attention,bce loss distribute attention,0.6373578906059265
translation,489,192,results,attention,especially to,sentences,attention especially to sentences,0.6596381664276123
translation,489,192,results,sentences,close to,relevant ones,sentences close to relevant ones,0.7076273560523987
translation,490,9,model,ranking answers,with respect to,relevance,ranking answers with respect to relevance,0.6571094393730164
translation,490,9,model,ranking answers,return,top n answers,ranking answers return top n answers,0.7059870958328247
translation,490,9,model,retrieval,has,of candidate answers in the collection,retrieval has of candidate answers in the collection,0.5742526054382324
translation,490,9,model,model,has,qa,model has qa,0.6210176348686218
translation,490,16,model,novel answer ranking approach,for,mlqa,novel answer ranking approach for mlqa,0.6234844326972961
translation,490,24,results,not more effective,than,l2t,not more effective than l2t,0.6312044262886047
translation,490,24,results,l2 ct,has,outperformed,l2 ct has outperformed,0.6219027638435364
translation,490,24,results,outperformed,has,baseline,outperformed has baseline,0.614183247089386
translation,490,24,results,results,has,l2 ct,results has l2 ct,0.516400933265686
translation,491,253,ablation-analysis,performance gap,between,b-ranker and b -qa,performance gap between b-ranker and b -qa,0.649937629699707
translation,491,253,ablation-analysis,b-ranker and b -qa,3.4- 5.2 % in,p@1,b-ranker and b -qa 3.4- 5.2 % in p@1,0.7337604761123657
translation,491,253,ablation-analysis,b-ranker and b -qa,4.9- 5.3 % in,map,b-ranker and b -qa 4.9- 5.3 % in map,0.703329861164093
translation,491,253,ablation-analysis,ablation analysis,has,performance gap,ablation analysis has performance gap,0.5463806986808777
translation,491,259,ablation-analysis,performance gap,between,msa +swc +sa and the others,performance gap between msa +swc +sa and the others,0.6234328746795654
translation,491,259,ablation-analysis,msa +swc +sa and the others,confirms,all the features,msa +swc +sa and the others confirms all the features,0.6134756803512573
translation,491,259,ablation-analysis,all the features,contributed to,higher performance,all the features contributed to higher performance,0.5129433274269104
translation,491,259,ablation-analysis,ablation analysis,has,performance gap,ablation analysis has performance gap,0.5463806986808777
translation,491,244,baselines,baselines,has,b-ranker,baselines has b-ranker,0.568719208240509
translation,491,245,baselines,system,ranks,all n correct answers,system ranks all n correct answers,0.7429258823394775
translation,491,245,baselines,all n correct answers,as,top n results,all n correct answers as top n results,0.4895947277545929
translation,491,245,baselines,top n results,of,20 answer candidates,top n results of 20 answer candidates,0.5602622032165527
translation,491,245,baselines,upperbound,has,system,upperbound has system,0.6509943604469299
translation,491,245,baselines,baselines,has,upperbound,baselines has upperbound,0.605918824672699
translation,491,224,experimental-setup,tinysvm,with,linear kernel,tinysvm with linear kernel,0.5977038145065308
translation,491,224,experimental-setup,linear kernel,for training,our re-ranker,linear kernel for training our re-ranker,0.7352581024169922
translation,491,224,experimental-setup,experimental setup,use,tinysvm,experimental setup use tinysvm,0.5835052132606506
translation,491,10,experiments,semantic word classes,for improving,why,semantic word classes for improving why,0.6676246523857117
translation,491,10,experiments,semantic word classes,for improving,-question answering ( why - qa ),semantic word classes for improving -question answering ( why - qa ),0.693999171257019
translation,491,10,experiments,why,on,largescale web corpus,why on largescale web corpus,0.5199808478355408
translation,491,10,experiments,-question answering ( why - qa ),on,largescale web corpus,-question answering ( why - qa ) on largescale web corpus,0.5172379016876221
translation,491,10,experiments,sentiment analysis,has,semantic word classes,sentiment analysis has semantic word classes,0.5525923371315002
translation,491,10,experiments,why,has,-question answering ( why - qa ),why has -question answering ( why - qa ),0.5962099432945251
translation,491,243,experiments,b- qa,has,answer retrieval system,b- qa has answer retrieval system,0.5731973648071289
translation,491,7,results,semantic word classes,for,ranking,semantic word classes for ranking,0.5774863958358765
translation,491,7,results,semantic word classes,on,set,semantic word classes on set,0.5238966345787048
translation,491,7,results,set,of,850 why -questions,set of 850 why -questions,0.6248482465744019
translation,491,7,results,15.2 % improvement,in,precision,15.2 % improvement in precision,0.5157412886619568
translation,491,7,results,precision,at,top - 1 answer,precision at top - 1 answer,0.5506751537322998
translation,491,7,results,ranking,has,answers,ranking has answers,0.6234630942344666
translation,491,7,results,answers,has,to why -questions,answers has to why -questions,0.5692589282989502
translation,491,7,results,850 why -questions,has,our method,850 why -questions has our method,0.6187759637832642
translation,491,248,results,proposed method,achieved,best performance,proposed method achieved best performance,0.7575127482414246
translation,491,248,results,best performance,both in,cv ( set1 ) and cv ( set1 + set2 ),best performance both in cv ( set1 ) and cv ( set1 + set2 ),0.6415814161300659
translation,491,248,results,results,has,proposed method,results has proposed method,0.5845219492912292
translation,491,249,results,our method,shows,significant improvement,our method shows significant improvement,0.641867458820343
translation,491,249,results,significant improvement,over,our answer retrieval method,significant improvement over our answer retrieval method,0.6717010736465454
translation,491,249,results,11.4- 15.2 %,in,p@1,11.4- 15.2 % in p@1,0.6016191840171814
translation,491,249,results,10.7- 12.1 %,in,map,10.7- 12.1 % in map,0.5789979696273804
translation,491,249,results,significant improvement,has,11.4- 15.2 %,significant improvement has 11.4- 15.2 %,0.5096876621246338
translation,491,249,results,results,has,our method,results has our method,0.5589964985847473
translation,491,250,results,improvement,over,b-ranker,improvement over b-ranker,0.7084883451461792
translation,491,250,results,improvement,over,b-ranker + cr,improvement over b-ranker + cr,0.68869549036026
translation,491,250,results,improvement,over,b-ranker + wn,improvement over b-ranker + wn,0.6902651786804199
translation,491,250,results,improvement,shows,effectiveness,improvement shows effectiveness,0.6520145535469055
translation,491,250,results,7.6 - 10 %,in,p@1,7.6 - 10 % in p@1,0.6088103652000427
translation,491,250,results,5.7- 6.6 %,in,map,5.7- 6.6 % in map,0.5822849273681641
translation,491,250,results,effectiveness,of,proposed feature set,effectiveness of proposed feature set,0.6248303055763245
translation,491,250,results,proposed feature set,over,features,proposed feature set over features,0.6327669024467468
translation,491,250,results,b-ranker + wn,has,7.6 - 10 %,b-ranker + wn has 7.6 - 10 %,0.5905323624610901
translation,491,250,results,results,has,improvement,results has improvement,0.6248279809951782
translation,491,251,results,b-ranker + cr and b-ranker +wn,not show,significant performance improvement,b-ranker + cr and b-ranker +wn not show significant performance improvement,0.7099817395210266
translation,491,251,results,significant performance improvement,over,b-ranker,significant performance improvement over b-ranker,0.6886724233627319
translation,491,251,results,results,has,b-ranker + cr and b-ranker +wn,results has b-ranker + cr and b-ranker +wn,0.5377709865570068
translation,491,252,results,causal relation,prove,effective,causal relation prove effective,0.7157949209213257
translation,491,252,results,wordnet features,prove,effective,wordnet features prove effective,0.6385502219200134
translation,491,254,results,all systems,consistently show,better performance,all systems consistently show better performance,0.7119579315185547
translation,491,254,results,better performance,in,cv ( set1 + set2 ),better performance in cv ( set1 + set2 ),0.5451711416244507
translation,491,254,results,better performance,than,cv ( set1 ),better performance than cv ( set1 ),0.599995493888855
translation,491,254,results,cv ( set1 + set2 ),than,cv ( set1 ),cv ( set1 + set2 ) than cv ( set1 ),0.6207234859466553
translation,491,254,results,results,has,all systems,results has all systems,0.5592814683914185
translation,491,260,results,significant performance improvement,by,sa,significant performance improvement by sa,0.6025725603103638
translation,491,260,results,significant performance improvement,by,sw c,significant performance improvement by sw c,0.5955396294593811
translation,491,260,results,sw c,features from,semantic word classes,sw c features from semantic word classes,0.7347838282585144
translation,491,260,results,results,has,significant performance improvement,results has significant performance improvement,0.5669288039207458
translation,491,261,results,sa@w,made,0.4- 0.7 % improvement,sa@w made 0.4- 0.7 % improvement,0.6136338114738464
translation,491,261,results,useful,in training,re-ranker,useful in training re-ranker,0.8355374932289124
translation,491,261,results,0.4- 0.7 % improvement,over,msa + swc,0.4- 0.7 % improvement over msa + swc,0.6365036964416504
translation,491,267,results,performance,of,our method,performance of our method,0.5712061524391174
translation,491,267,results,performance,was,66.6 %,performance was 66.6 %,0.5768721103668213
translation,491,267,results,our method,was,64.8 %,our method was 64.8 %,0.5593090653419495
translation,491,267,results,our method,was,66.6 %,our method was 66.6 %,0.5608258843421936
translation,491,267,results,64.8 %,in,p@1,64.8 % in p@1,0.5982635021209717
translation,491,267,results,66.6 %,in,map,66.6 % in map,0.5848437547683716
translation,491,267,results,results,has,performance,results has performance,0.5972660779953003
translation,492,174,baselines,opensource code,of,"kvmem ( saha et al. , 2018 )","opensource code of kvmem ( saha et al. , 2018 )",0.519109845161438
translation,492,174,baselines,opensource code,of,cipitr,opensource code of cipitr,0.5939455032348633
translation,492,174,baselines,cipitr,to train,model,cipitr to train model,0.6909886598587036
translation,492,179,baselines,three baseline methods,on,cqa dataset,three baseline methods on cqa dataset,0.5166422128677368
translation,492,168,experimental-setup,our model,in,pytorch,our model in pytorch,0.5748273730278015
translation,492,168,experimental-setup,reptile meta-learning algorithm,to optimize,meta-learned policy,reptile meta-learning algorithm to optimize meta-learned policy,0.703071117401123
translation,492,168,experimental-setup,experimental setup,implemented,our model,experimental setup implemented our model,0.7312729358673096
translation,492,168,experimental-setup,experimental setup,employed,reptile meta-learning algorithm,experimental setup employed reptile meta-learning algorithm,0.6300561428070068
translation,492,169,experimental-setup,weights,of,model and the word embeddings,weights of model and the word embeddings,0.5457960963249207
translation,492,169,experimental-setup,model and the word embeddings,were,randomly initialized and further updated,model and the word embeddings were randomly initialized and further updated,0.5423100590705872
translation,492,169,experimental-setup,randomly initialized and further updated,process of,training,randomly initialized and further updated process of training,0.6466829180717468
translation,492,169,experimental-setup,experimental setup,has,weights,experimental setup has weights,0.5104771852493286
translation,492,170,experimental-setup,meta-learning,set,? 1 = 1e?4,meta-learning set ? 1 = 1e?4,0.6767935752868652
translation,492,170,experimental-setup,meta-learning,set,? 2 = 0.1,meta-learning set ? 2 = 0.1,0.6229333877563477
translation,492,170,experimental-setup,experimental setup,In,meta-learning,experimental setup In meta-learning,0.5262598991394043
translation,492,173,experiments,adam optimizer,used in,rl,adam optimizer used in rl,0.6356309652328491
translation,492,173,experiments,adam optimizer,to maximizes,expected reward,adam optimizer to maximizes expected reward,0.677525520324707
translation,492,173,experiments,rl,to maximizes,expected reward,rl to maximizes expected reward,0.7071687579154968
translation,492,6,model,model,proposes,metareinforcement,model proposes metareinforcement,0.6934563517570496
translation,492,7,model,our method,quickly and effectively adapts,meta-learned programmer,our method quickly and effectively adapts meta-learned programmer,0.7932606339454651
translation,492,7,model,meta-learned programmer,to,new questions,meta-learned programmer to new questions,0.5849581956863403
translation,492,7,model,new questions,based on,most similar questions,new questions based on most similar questions,0.6232001185417175
translation,492,7,model,most similar questions,retrieved from,training data,most similar questions retrieved from training data,0.5574423670768738
translation,492,7,model,model,has,our method,model has our method,0.5725589394569397
translation,492,8,model,meta-learned policy,to learn,good programming policy,meta-learned policy to learn good programming policy,0.6064695119857788
translation,492,8,model,good programming policy,utilizing,trial trajectories,good programming policy utilizing trial trajectories,0.6558954119682312
translation,492,8,model,good programming policy,utilizing,rewards,good programming policy utilizing rewards,0.6498894691467285
translation,492,8,model,rewards,for,similar questions,rewards for similar questions,0.6454141736030579
translation,492,8,model,similar questions,in,support set,similar questions in support set,0.5429600477218628
translation,492,8,model,model,has,meta-learned policy,model has meta-learned policy,0.512839674949646
translation,492,43,model,meta- rl approach,for,cqa ( mrl - cqa ),meta- rl approach for cqa ( mrl - cqa ),0.6343812346458435
translation,492,43,model,meta- rl approach,where,model,meta- rl approach where model,0.6527417302131653
translation,492,43,model,model,adapts to,target question,model adapts to target question,0.7537960410118103
translation,492,43,model,target question,by,trials,target question by trials,0.5926456451416016
translation,492,43,model,corresponding reward signals,on,retrieved instances,corresponding reward signals on retrieved instances,0.5026353597640991
translation,492,43,model,model,propose,meta- rl approach,model propose meta- rl approach,0.6822569370269775
translation,492,44,model,meta-learning stage,learns,rl policy,meta-learning stage learns rl policy,0.7210269570350647
translation,492,44,model,rl policy,across,tasks,rl policy across tasks,0.6612558364868164
translation,492,44,model,rl policy,for,collecting trial trajectories,rl policy for collecting trial trajectories,0.5766476392745972
translation,492,44,model,rl policy,for,learning,rl policy for learning,0.6072955131530762
translation,492,44,model,collecting trial trajectories,for,effective learning,collecting trial trajectories for effective learning,0.5851755142211914
translation,492,44,model,model,In,meta-learning stage,model In meta-learning stage,0.522769033908844
translation,492,107,model,cqa,as,rl problem,cqa as rl problem,0.5601953864097595
translation,492,107,model,rl problem,under,few-shot learning conditions,rl problem under few-shot learning conditions,0.6057958006858826
translation,492,107,model,meta,-,rl techniques,meta - rl techniques,0.6398599743843079
translation,492,107,model,meta,to adapt,programmer,meta to adapt programmer,0.7147486805915833
translation,492,107,model,meta,has,rl techniques,meta has rl techniques,0.59819096326828
translation,492,107,model,model,view,cqa,model view cqa,0.7029197216033936
translation,492,107,model,model,make use of,meta,model make use of meta,0.6957504153251648
translation,492,107,model,model,make use of,rl techniques,model make use of rl techniques,0.7352369427680969
translation,492,127,model,relevance function,measures,similarity,relevance function measures similarity,0.5495504140853882
translation,492,127,model,similarity,between,two questions,similarity between two questions,0.6987560987472534
translation,492,127,model,"number of kb artifacts ( i.e. , entities , relations , and types )",in,questions,"number of kb artifacts ( i.e. , entities , relations , and types ) in questions",0.4518769383430481
translation,492,127,model,model,propose,relevance function,model propose relevance function,0.6560214757919312
translation,492,253,model,meta-learning method,to,npi,meta-learning method to npi,0.5565780997276306
translation,492,253,model,npi,in,cqa,npi in cqa,0.6383615136146545
translation,492,253,model,model,propose,meta-learning method,model propose meta-learning method,0.6678277850151062
translation,492,254,model,meta-reinforcement learning approach,to effectively adapt,meta-learned programmer,meta-reinforcement learning approach to effectively adapt meta-learned programmer,0.7171255946159363
translation,492,254,model,meta-learned programmer,to,new questions,meta-learned programmer to new questions,0.5849581956863403
translation,492,254,model,new questions,based on,most similar questions retrieved,new questions based on most similar questions retrieved,0.613764226436615
translation,492,254,model,model,take,meta-reinforcement learning approach,model take meta-reinforcement learning approach,0.6527016162872314
translation,492,47,results,our method,achieves,state - of- the - art performance,our method achieves state - of- the - art performance,0.599297285079956
translation,492,47,results,state - of- the - art performance,on,cqa dataset,state - of- the - art performance on cqa dataset,0.5206348299980164
translation,492,47,results,cqa dataset,with,overall macro and micro f1 scores,cqa dataset with overall macro and micro f1 scores,0.6171954274177551
translation,492,47,results,overall macro and micro f1 scores,of,66.25 % and 77.71 %,overall macro and micro f1 scores of 66.25 % and 77.71 %,0.5480882525444031
translation,492,47,results,results,has,our method,results has our method,0.5589964985847473
translation,492,189,results,mrl - cqa model,achieves,overall best macro and micro f1 values,mrl - cqa model achieves overall best macro and micro f1 values,0.643089771270752
translation,492,189,results,overall best macro and micro f1 values,achieving,state - of - the - art results,overall best macro and micro f1 values achieving state - of - the - art results,0.6377394199371338
translation,492,189,results,state - of - the - art results,of,66.25 % and 77.71 %,state - of - the - art results of 66.25 % and 77.71 %,0.5428569316864014
translation,492,190,results,mrl - cqa,achieves,best or second - best performance,mrl - cqa achieves best or second - best performance,0.6873680353164673
translation,492,190,results,best or second - best performance,in,six out of the seven categories,best or second - best performance in six out of the seven categories,0.4868980050086975
translation,492,190,results,results,has,mrl - cqa,results has mrl - cqa,0.5431365370750427
translation,492,191,results,mrl - cqa,delivers,best performance,mrl - cqa delivers best performance,0.6619113087654114
translation,492,191,results,best performance,in,all three types,best performance in all three types,0.4981837868690491
translation,492,191,results,three hardest categories,has,mrl - cqa,three hardest categories has mrl - cqa,0.6250351667404175
translation,492,191,results,results,Of,three hardest categories,results Of three hardest categories,0.5618371367454529
translation,492,199,results,much worse,in,all the categories,much worse in all the categories,0.5541442036628723
translation,492,199,results,all the categories,than,cip - sep,all the categories than cip - sep,0.6191501021385193
translation,492,204,results,hard categories,comparing,f1 scores,hard categories comparing f1 scores,0.6865674257278442
translation,492,204,results,hard categories,observed that,nsm,hard categories observed that nsm,0.6785724759101868
translation,492,204,results,f1 scores,of,pg and mrl - cqa,f1 scores of pg and mrl - cqa,0.6147206425666809
translation,492,204,results,f1 scores,observed that,nsm,f1 scores observed that nsm,0.647244393825531
translation,492,204,results,pg and mrl - cqa,observed that,nsm,pg and mrl - cqa observed that nsm,0.7024838328361511
translation,492,204,results,nsm,performed,competitively,nsm performed competitively,0.26884037256240845
translation,492,204,results,results,In,hard categories,results In hard categories,0.5376691222190857
translation,492,205,results,nsm,performed,second best,nsm performed second best,0.2514913082122803
translation,492,205,results,second best,in,  simple question   and   comparative reasoning   categories,second best in   simple question   and   comparative reasoning   categories,0.523197591304779
translation,492,205,results,results,has,nsm,results has nsm,0.53862464427948
Blogs,0,171,ablation-analysis,all levels,are,important,all levels are important,0.6070966124534607
Blogs,0,171,ablation-analysis,significantly outperforms,not using,any linguistic attention,significantly outperforms not using any linguistic attention,0.6725569367408752
Blogs,0,171,ablation-analysis,our final model,has,significantly outperforms,our final model has significantly outperforms,0.6005896329879761
Blogs,0,171,ablation-analysis,any linguistic attention,has,1.1 % difference,any linguistic attention has 1.1 % difference,0.53859943151474
Blogs,0,171,ablation-analysis,ablation analysis,Note,all levels,ablation analysis Note all levels,0.6495137810707092
Blogs,0,171,ablation-analysis,ablation analysis,Note,our final model,ablation analysis Note our final model,0.6192034482955933
Blogs,0,137,baselines,open-ended test scenario,compare,method,open-ended test scenario compare method,0.6705718636512756
Blogs,0,137,baselines,baselines,For,open-ended test scenario,baselines For open-ended test scenario,0.6225544810295105
Blogs,0,138,baselines,multiple choice,compare with,region sel.,multiple choice compare with region sel.,0.647971510887146
Blogs,0,138,baselines,region sel.,has,fda [ 11 ],region sel. has fda [ 11 ],0.5638477802276611
Blogs,0,139,baselines,2,-,vis + blstm,2 - vis + blstm,0.6662620902061462
Blogs,0,139,baselines,2,-,img - cnn,2 - img - cnn,0.5524632930755615
Blogs,0,139,baselines,2,-,san,2 - san,0.7553892135620117
Blogs,0,139,baselines,vis + blstm,on,coco -qa,vis + blstm on coco -qa,0.5766251683235168
Blogs,0,139,baselines,san,on,coco -qa,san on coco -qa,0.6807703971862793
Blogs,0,139,baselines,],on,coco -qa,] on coco -qa,0.6365223526954651
Blogs,0,139,baselines,2,has,vis + blstm,2 has vis + blstm,0.6404960751533508
Blogs,0,139,baselines,vis + blstm,has,img - cnn,vis + blstm has img - cnn,0.5233082175254822
Blogs,0,139,baselines,vis + blstm,has,san,vis + blstm has san,0.6756484508514404
Blogs,0,139,baselines,vis + blstm,has,],vis + blstm has ],0.6328766942024231
Blogs,0,139,baselines,san,has,],san has ],0.686016857624054
Blogs,0,127,experimental-setup,torch,to develop,our model,torch to develop our model,0.7801887392997742
Blogs,0,127,experimental-setup,experimental setup,use,torch,experimental setup use torch,0.599155068397522
Blogs,0,128,experimental-setup,rmsprop optimizer,with,base learning rate,rmsprop optimizer with base learning rate,0.5863265991210938
Blogs,0,128,experimental-setup,rmsprop optimizer,with,momentum,rmsprop optimizer with momentum,0.5936768054962158
Blogs,0,128,experimental-setup,rmsprop optimizer,with,weight- decay,rmsprop optimizer with weight- decay,0.5965908169746399
Blogs,0,128,experimental-setup,base learning rate,of,4e - 4,base learning rate of 4e - 4,0.6564604640007019
Blogs,0,128,experimental-setup,momentum,has,0.99,momentum has 0.99,0.5715008974075317
Blogs,0,128,experimental-setup,weight- decay,has,1e - 8,weight- decay has 1e - 8,0.5811696648597717
Blogs,0,128,experimental-setup,experimental setup,use,rmsprop optimizer,experimental setup use rmsprop optimizer,0.6190452575683594
Blogs,0,129,experimental-setup,batch size,to be,300,batch size to be 300,0.6135265827178955
Blogs,0,129,experimental-setup,up to 256 epochs,with,early stopping,up to 256 epochs with early stopping,0.6394361257553101
Blogs,0,129,experimental-setup,early stopping,if,validation accuracy,early stopping if validation accuracy,0.5783944129943848
Blogs,0,129,experimental-setup,validation accuracy,not improved in,last 5 epochs,validation accuracy not improved in last 5 epochs,0.6826601028442383
Blogs,0,129,experimental-setup,experimental setup,set,batch size,experimental setup set batch size,0.6767397522926331
Blogs,0,129,experimental-setup,experimental setup,train for,up to 256 epochs,experimental setup train for up to 256 epochs,0.6969927549362183
Blogs,0,130,experimental-setup,coco - qa,size of,hidden layer w s,coco - qa size of hidden layer w s,0.7068402767181396
Blogs,0,130,experimental-setup,hidden layer w s,set to,512 and 1024,hidden layer w s set to 512 and 1024,0.7127101421356201
Blogs,0,130,experimental-setup,512 and 1024,for,vqa,512 and 1024 for vqa,0.6825955510139465
Blogs,0,130,experimental-setup,experimental setup,For,coco - qa,experimental setup For coco - qa,0.6281641125679016
Blogs,0,131,experimental-setup,other word embedding and hidden layers,were,vectors,other word embedding and hidden layers were vectors,0.556693971157074
Blogs,0,131,experimental-setup,vectors,size,512,vectors size 512,0.7641131281852722
Blogs,0,131,experimental-setup,experimental setup,has,other word embedding and hidden layers,experimental setup has other word embedding and hidden layers,0.5230025053024292
Blogs,0,132,experimental-setup,dropout,with,probability 0.5,dropout with probability 0.5,0.6440638303756714
Blogs,0,132,experimental-setup,probability 0.5,on,each layer,probability 0.5 on each layer,0.5625101327896118
Blogs,0,132,experimental-setup,experimental setup,apply,dropout,experimental setup apply dropout,0.5288015604019165
Blogs,0,6,model,model,for,vqa,model for vqa,0.684484601020813
Blogs,0,7,model,hierarchical fashion,via,novel 1 - dimensional convolution neural networks ( cnn ),hierarchical fashion via novel 1 - dimensional convolution neural networks ( cnn ),0.6886977553367615
Blogs,0,7,model,model,reasons about,question,model reasons about question,0.7244479656219482
Blogs,0,19,model,novel multi-modal attention model,for,vqa,novel multi-modal attention model for vqa,0.5850293636322021
Blogs,0,19,model,model,present,novel multi-modal attention model,model present novel multi-modal attention model,0.6077468991279602
Blogs,0,20,model,novel mechanism,jointly reasons about,visual attention and question attention,novel mechanism jointly reasons about visual attention and question attention,0.7057859897613525
Blogs,0,20,model,visual attention and question attention,refer to as,co-attention,visual attention and question attention refer to as co-attention,0.5661987662315369
Blogs,0,20,model,model,propose,novel mechanism,model propose novel mechanism,0.7055184841156006
Blogs,0,21,model,natural symmetry,between,image and question,natural symmetry between image and question,0.6922915577888489
Blogs,0,21,model,model,has,natural symmetry,model has natural symmetry,0.554289698600769
Blogs,0,23,model,hierarchical architecture,co-attends to,image and question,hierarchical architecture co-attends to image and question,0.7650004625320435
Blogs,0,23,model,image and question,at,three levels,image and question at three levels,0.5965785384178162
Blogs,0,23,model,model,build,hierarchical architecture,model build hierarchical architecture,0.7466304302215576
Blogs,0,24,model,word level,embed,words,word level embed words,0.7526406049728394
Blogs,0,24,model,words,to,vector space,words to vector space,0.5185826420783997
Blogs,0,24,model,vector space,through,embedding matrix,vector space through embedding matrix,0.6402916312217712
Blogs,0,24,model,model,At,word level,model At word level,0.5542347431182861
Blogs,0,25,model,1 - dimensional convolution neural networks,to capture,information,1 - dimensional convolution neural networks to capture information,0.6776748895645142
Blogs,0,25,model,information,contained in,"unigrams , bigrams and trigrams","information contained in unigrams , bigrams and trigrams",0.712005615234375
Blogs,0,25,model,phrase level,has,1 - dimensional convolution neural networks,phrase level has 1 - dimensional convolution neural networks,0.563528299331665
Blogs,0,25,model,model,At,phrase level,model At phrase level,0.5453327894210815
Blogs,0,26,model,word representations,with,temporal filters,word representations with temporal filters,0.5980136394500732
Blogs,0,26,model,word representations,combine,various n-gram responses,word representations combine various n-gram responses,0.6674304604530334
Blogs,0,26,model,temporal filters,of,varying support,temporal filters of varying support,0.5835280418395996
Blogs,0,26,model,various n-gram responses,by pooling them into,single phrase level representation,various n-gram responses by pooling them into single phrase level representation,0.6823979616165161
Blogs,0,26,model,model,convolve,word representations,model convolve word representations,0.7685118317604065
Blogs,0,27,model,question level,use,recurrent neural networks,question level use recurrent neural networks,0.6127606630325317
Blogs,0,27,model,recurrent neural networks,to encode,entire question,recurrent neural networks to encode entire question,0.7512335777282715
Blogs,0,27,model,model,At,question level,model At question level,0.5807465314865112
Blogs,0,83,model,first mechanism,call,parallel co-attention,first mechanism call parallel co-attention,0.5890675783157349
Blogs,0,83,model,first mechanism,generates,image and question attention simultaneously,first mechanism generates image and question attention simultaneously,0.6366239190101624
Blogs,0,83,model,model,has,first mechanism,model has first mechanism,0.562108039855957
Blogs,0,142,results,our approach,improves,state of art,our approach improves state of art,0.5840200781822205
Blogs,0,142,results,state of art,from,60.4 % ( dmn + [ 23 ] ),state of art from 60.4 % ( dmn + [ 23 ] ),0.5094298720359802
Blogs,0,142,results,state of art,from,64.2 % ( fda [ 11 ] ),state of art from 64.2 % ( fda [ 11 ] ),0.5036188364028931
Blogs,0,142,results,state of art,from,64.2 % ( fda [ 11 ] ),state of art from 64.2 % ( fda [ 11 ] ),0.5036188364028931
Blogs,0,142,results,60.4 % ( dmn + [ 23 ] ),to,62.1 %,60.4 % ( dmn + [ 23 ] ) to 62.1 %,0.575753390789032
Blogs,0,142,results,64.2 % ( fda [ 11 ] ),to,66.1 %,64.2 % ( fda [ 11 ] ) to 66.1 %,0.5637027621269226
Blogs,0,142,results,66.1 %,on,multiple -choice,66.1 % on multiple -choice,0.47117483615875244
Blogs,0,142,results,66.1 %,has,ours a + resnet ),66.1 % has ours a + resnet ),0.5839720964431763
Blogs,0,142,results,results,see that,our approach,results see that our approach,0.6574503183364868
Blogs,0,143,results,question type other and num,achieve,3.4 % and 1.4 % improvement,question type other and num achieve 3.4 % and 1.4 % improvement,0.6017940044403076
Blogs,0,143,results,question type other and num,achieve,4.0 % and 1.1 %,question type other and num achieve 4.0 % and 1.1 %,0.5783803462982178
Blogs,0,143,results,3.4 % and 1.4 % improvement,on,open-ended questions,3.4 % and 1.4 % improvement on open-ended questions,0.49295201897621155
Blogs,0,143,results,4.0 % and 1.1 %,on,multiple -choice questions,4.0 % and 1.1 % on multiple -choice questions,0.49724218249320984
Blogs,0,143,results,results,for,question type other and num,results for question type other and num,0.6076404452323914
Blogs,0,144,results,vgg features,in,all cases,vgg features in all cases,0.5501684546470642
Blogs,0,144,results,resnet features,has,outperform or match,resnet features has outperform or match,0.6016504168510437
Blogs,0,144,results,outperform or match,has,vgg features,outperform or match has vgg features,0.5739163756370544
Blogs,0,147,results,results,on,coco - qa test set,results on coco - qa test set,0.5507461428642273
Blogs,0,148,results,our model,improves,state - of- the - art,our model improves state - of- the - art,0.6049377918243408
Blogs,0,148,results,state - of- the - art,from,61.6 %,state - of- the - art from 61.6 %,0.5210619568824768
Blogs,0,148,results,"san ( 2 , cnn )",to,65.4 %,"san ( 2 , cnn ) to 65.4 %",0.5636298656463623
Blogs,0,148,results,"san ( 2 , cnn )",to,ours a + resnet ),"san ( 2 , cnn ) to ours a + resnet )",0.5676884055137634
Blogs,0,148,results,vqa,has,our model,vqa has our model,0.5720446109771729
Blogs,0,148,results,61.6 %,has,"san ( 2 , cnn )","61.6 % has san ( 2 , cnn )",0.5667208433151245
Blogs,0,148,results,"san ( 2 , cnn )",has,65.4 %,"san ( 2 , cnn ) has 65.4 %",0.563681423664093
Blogs,0,148,results,65.4 %,has,ours a + resnet ),65.4 % has ours a + resnet ),0.5823851823806763
Blogs,0,149,results,parallel co-attention,performs,better,parallel co-attention performs better,0.6663646697998047
Blogs,0,149,results,better,than,alternating co-attention,better than alternating co-attention,0.5862603187561035
Blogs,0,149,results,results,observe,parallel co-attention,results observe parallel co-attention,0.5212159752845764
Blogs,0,172,results,question attention alone model,better than,lstm q+i,question attention alone model better than lstm q+i,0.7145652174949646
Blogs,0,172,results,question attention alone model,worse than,image attention alone,question attention alone model worse than image attention alone,0.7110068202018738
Blogs,0,172,results,improvement,of,0.5 %,improvement of 0.5 %,0.5903324484825134
Blogs,0,172,results,drop,of,1.1 %,drop of 1.1 %,0.6238871812820435
Blogs,0,172,results,results,has,question attention alone model,results has question attention alone model,0.5574511885643005
Blogs,0,173,results,alternating co-attention,for,one more round,alternating co-attention for one more round,0.634971559047699
Blogs,0,173,results,alternating co-attention,with,improvement,alternating co-attention with improvement,0.667358934879303
Blogs,0,173,results,improvement,of,0.3 %,improvement of 0.3 %,0.5725396275520325
Blogs,0,173,results,results,performed,alternating co-attention,results performed alternating co-attention,0.29582327604293823
Blogs,1,257,ablation-analysis,root - type,can achieve,high qa accuracy,root - type can achieve high qa accuracy,0.7105847001075745
Blogs,1,257,ablation-analysis,high qa accuracy,when using,notext input,high qa accuracy when using notext input,0.6998982429504395
Blogs,1,257,ablation-analysis,ablation analysis,Regarding,different types of qa pairs,ablation analysis Regarding different types of qa pairs,0.6186235547065735
Blogs,1,203,baselines,non-neural approaches,extract,sentences,non-neural approaches extract sentences,0.6668779850006104
Blogs,1,203,baselines,sentences,from,source article,sentences from source article,0.5841383934020996
Blogs,1,203,baselines,sentences,form,summary,sentences form summary,0.7175372838973999
Blogs,1,211,baselines,summarunner,presents,autoregressive sequence labeling method,summarunner presents autoregressive sequence labeling method,0.6640098690986633
Blogs,1,211,baselines,autoregressive sequence labeling method,based on,recurrent neural networks,autoregressive sequence labeling method based on recurrent neural networks,0.6413489580154419
Blogs,1,211,baselines,baselines,has,summarunner,baselines has summarunner,0.5826749801635742
Blogs,1,214,baselines,distraction -m3,trains,summarization system,distraction -m3 trains summarization system,0.7421886920928955
Blogs,1,214,baselines,summarization system,to distract,attention,summarization system to distract attention,0.6688007116317749
Blogs,1,214,baselines,attention,to traverse,different regions,attention to traverse different regions,0.7874721884727478
Blogs,1,214,baselines,different regions,of,source article,different regions of source article,0.5691860318183899
Blogs,1,214,baselines,baselines,has,distraction -m3,baselines has distraction -m3,0.5976544618606567
Blogs,1,267,baselines,baselines,has,cnn,baselines has cnn,0.5907226204872131
Blogs,1,187,hyperparameters,hidden state dimension,of,lstm,hidden state dimension of lstm,0.5725870728492737
Blogs,1,187,hyperparameters,hidden state dimension,to be,256,hidden state dimension to be 256,0.5605120658874512
Blogs,1,187,hyperparameters,lstm,to be,256,lstm to be 256,0.5123810172080994
Blogs,1,187,hyperparameters,hyperparameters,set,hidden state dimension,hyperparameters set hidden state dimension,0.6264743208885193
Blogs,1,193,hyperparameters,sinusoidal positional encodings,of,30 dimensions,sinusoidal positional encodings of 30 dimensions,0.586737871170044
Blogs,1,193,hyperparameters,"et al. , 2017 )",of,30 dimensions,"et al. , 2017 ) of 30 dimensions",0.5759252905845642
Blogs,1,193,hyperparameters,hyperparameters,use,100 - dimensional word embeddings,hyperparameters use 100 - dimensional word embeddings,0.5437304973602295
Blogs,1,194,hyperparameters,maximum article length,set to,400 words,maximum article length set to 400 words,0.5853229761123657
Blogs,1,194,hyperparameters,hyperparameters,has,maximum article length,hyperparameters has maximum article length,0.47672945261001587
Blogs,1,196,hyperparameters,at most 10 qa pairs ( k=10 ),to guide,extraction,at most 10 qa pairs ( k=10 ) to guide extraction,0.6398481726646423
Blogs,1,196,hyperparameters,extraction,of,summary segments,extraction of summary segments,0.6170009970664978
Blogs,1,197,hyperparameters,mini-batch training,with,"adam optimizer ( kingma and ba , 2014 )","mini-batch training with adam optimizer ( kingma and ba , 2014 )",0.6231120228767395
Blogs,1,197,hyperparameters,"adam optimizer ( kingma and ba , 2014 )",where,mini-batch,"adam optimizer ( kingma and ba , 2014 ) where mini-batch",0.5925194025039673
Blogs,1,197,hyperparameters,mini-batch,contains,128 articles,mini-batch contains 128 articles,0.6938420534133911
Blogs,1,197,hyperparameters,mini-batch,contains,qa pairs,mini-batch contains qa pairs,0.654100775718689
Blogs,1,197,hyperparameters,hyperparameters,apply,mini-batch training,hyperparameters apply mini-batch training,0.5783742666244507
Blogs,1,198,hyperparameters,summary ratio,set to,0.15,summary ratio set to 0.15,0.6933051943778992
Blogs,1,198,hyperparameters,0.15,yielding,extractive summaries,0.15 yielding extractive summaries,0.6322953701019287
Blogs,1,198,hyperparameters,extractive summaries,has,of about 60 words,extractive summaries has of about 60 words,0.5868194699287415
Blogs,1,198,hyperparameters,hyperparameters,has,summary ratio,hyperparameters has summary ratio,0.5305749773979187
Blogs,1,35,model,human abstracts,to guide,extraction of summary text units,human abstracts to guide extraction of summary text units,0.6433182954788208
Blogs,1,41,model,novel reinforcement learning framework,to explore,space of possible extractive summaries,novel reinforcement learning framework to explore space of possible extractive summaries,0.6464291214942932
Blogs,1,41,model,novel reinforcement learning framework,assess,each summary,novel reinforcement learning framework assess each summary,0.6407633423805237
Blogs,1,41,model,each summary,using,novel reward function,each summary using novel reward function,0.6255669593811035
Blogs,1,41,model,novel reward function,judging,summary 's,novel reward function judging summary 's,0.6644484400749207
Blogs,1,41,model,novel reward function,judging,"fluency , length","novel reward function judging fluency , length",0.58107990026474
Blogs,1,41,model,novel reward function,judging,competency,novel reward function judging competency,0.6696186065673828
Blogs,1,41,model,competency,to answer,important questions,competency to answer important questions,0.6990929841995239
Blogs,1,41,model,summary 's,has,adequacy,summary 's has adequacy,0.5762854814529419
Blogs,1,41,model,model,utilize,novel reinforcement learning framework,model utilize novel reinforcement learning framework,0.5321887731552124
Blogs,1,71,model,novel supervised framework,encouraging,selection of consecutive sequences of words,novel supervised framework encouraging selection of consecutive sequences of words,0.6365816593170166
Blogs,1,71,model,selection of consecutive sequences of words,to form,extractive summary,selection of consecutive sequences of words to form extractive summary,0.6400794982910156
Blogs,1,71,model,model,present,novel supervised framework,model present novel supervised framework,0.6468057036399841
Blogs,1,72,model,reinforcement learning,to explore,space,reinforcement learning to explore space,0.6723940372467041
Blogs,1,72,model,reinforcement learning,promote,"fluent , adequate , and competent","reinforcement learning promote fluent , adequate , and competent",0.597131609916687
Blogs,1,72,model,space,of,possible extractive summaries,space of possible extractive summaries,0.560110867023468
Blogs,1,72,model,"fluent , adequate , and competent",in,question answering,"fluent , adequate , and competent in question answering",0.45087096095085144
Blogs,1,72,model,model,leverage,reinforcement learning,model leverage reinforcement learning,0.7140412926673889
Blogs,1,215,model,graph attention,introduces,graph- based attention mechanism,graph attention introduces graph- based attention mechanism,0.609276533126831
Blogs,1,215,model,graph- based attention mechanism,to enhance,encoderdecoder framework,graph- based attention mechanism to enhance encoderdecoder framework,0.672297477722168
Blogs,1,215,model,model,has,graph attention,model has graph attention,0.5476001501083374
Blogs,1,42,results,extractive summaries,yielding,highest expected rewards,extractive summaries yielding highest expected rewards,0.6134320497512817
Blogs,1,228,results,our qasumm methods,focusing on,chunk extraction,our qasumm methods focusing on chunk extraction,0.7338860630989075
Blogs,1,228,results,chunk extraction,perform,on par,chunk extraction perform on par,0.5600650310516357
Blogs,1,228,results,on par,with,competitive systems,on par with competitive systems,0.7219261527061462
Blogs,1,228,results,results,has,our qasumm methods,results has our qasumm methods,0.5411455631256104
Blogs,1,237,results,variants,of,qasumm method,variants of qasumm method,0.6211110353469849
Blogs,1,237,results,variants,find that,qasumm + root,variants find that qasumm + root,0.6538283824920654
Blogs,1,237,results,qasumm method,find that,qasumm + root,qasumm method find that qasumm + root,0.6512143015861511
Blogs,1,237,results,qasumm + root,achieves,highest scores,qasumm + root achieves highest scores,0.6712303161621094
Blogs,1,237,results,highest scores,on,dm dataset,highest scores on dm dataset,0.5106605887413025
Blogs,1,237,results,results,Among,variants,results Among variants,0.5687075853347778
Blogs,1,238,results,qasumm +ner,performs,consistently well,qasumm +ner performs consistently well,0.6261714100837708
Blogs,1,238,results,consistently well,on,cnn and dm datasets,consistently well on cnn and dm datasets,0.5295956134796143
Blogs,1,238,results,results,has,qasumm +ner,results has qasumm +ner,0.5592830777168274
Blogs,1,252,results,question - answering with goldsumm,performs,best,question - answering with goldsumm performs best,0.6088256239891052
Blogs,1,252,results,best,for,all qa types,best for all qa types,0.6185498237609863
Blogs,1,252,results,results,observe,question - answering with goldsumm,results observe question - answering with goldsumm,0.560562789440155
Blogs,1,253,results,outperforms,using,full - text,outperforms using full - text,0.6408496499061584
Blogs,1,253,results,scenarios,using,full - text,scenarios using full - text,0.6195931434631348
Blogs,1,253,results,full - text,as,source input,full - text as source input,0.5124569535255432
Blogs,1,253,results,outperforms,has,scenarios,outperforms has scenarios,0.6612445116043091
Blogs,1,253,results,results,has,outperforms,results has outperforms,0.6657275557518005
Blogs,1,255,results,performance,of,qa - summ + noq,performance of qa - summ + noq,0.6138985753059387
Blogs,1,255,results,qa - summ + noq,in between,notext and goldsumm,qa - summ + noq in between notext and goldsumm,0.7383760213851929
Blogs,1,255,results,notext and goldsumm,for,all answer types,notext and goldsumm for all answer types,0.6152620315551758
Blogs,1,255,results,results,observe that,performance,results observe that performance,0.6149371862411499
Blogs,1,259,results,ner - type qa pairs,work,best,ner - type qa pairs work best,0.6408628821372986
Blogs,1,259,results,best,for,goldsumm and full - text,best for goldsumm and full - text,0.576815128326416
Blogs,1,259,results,results,has,ner - type qa pairs,results has ner - type qa pairs,0.543272852897644
Blogs,1,260,results,subj / obj - type qa pairs,have,smallest gap,subj / obj - type qa pairs have smallest gap,0.5550492405891418
Blogs,1,260,results,smallest gap,between,train / dev accuracies,smallest gap between train / dev accuracies,0.6113104224205017
Blogs,1,260,results,results,find,subj / obj - type qa pairs,results find subj / obj - type qa pairs,0.5708459615707397
Blogs,1,270,results,extracting chunks,performs,superior,extracting chunks performs superior,0.6177071928977966
Blogs,1,270,results,extracting chunks,combining,chunks,extracting chunks combining chunks,0.768095850944519
Blogs,1,270,results,chunks,with,lstm representations,chunks with lstm representations,0.6277991533279419
Blogs,1,270,results,lstm representations,yield,highest scores,lstm representations yield highest scores,0.7207468748092651
Blogs,1,270,results,results,find that,extracting chunks,results find that extracting chunks,0.6608311533927917
Blogs,1,270,results,results,combining,chunks,results combining chunks,0.6124908328056335
Blogs,1,283,results,all systems,resulted in,similar performance times,all systems resulted in similar performance times,0.6270067095756531
Blogs,1,283,results,human abstracts,has,all systems,human abstracts has all systems,0.5943697094917297
Blogs,1,284,results,large margin,in,qa accuracy,large margin in qa accuracy,0.563387930393219
Blogs,1,284,results,large margin,compared to,abstractive and our supervised approach,large margin compared to abstractive and our supervised approach,0.6266524195671082
Blogs,1,284,results,qa accuracy,in,our full system,qa accuracy in our full system,0.5048866868019104
Blogs,1,284,results,results,observe,large margin,results observe large margin,0.6539188027381897
Blogs,1,285,results,informativeness,of,summaries,informativeness of summaries,0.6002655625343323
Blogs,1,285,results,informativeness,yielded,higher performance,informativeness yielded higher performance,0.6097134947776794
Blogs,1,285,results,summaries,to be,same,summaries to be same,0.6581137180328369
Blogs,1,285,results,summaries,to be,our systems,summaries to be our systems,0.5651859641075134
Blogs,1,285,results,same,has,our systems,same has our systems,0.6465820074081421
Blogs,2,87,hyperparameters,one million steps,with,max sequence length,one million steps with max sequence length,0.6400564312934875
Blogs,2,87,hyperparameters,one million steps,with,batch size,one million steps with batch size,0.6748462915420532
Blogs,2,87,hyperparameters,max sequence length,of,128,max sequence length of 128,0.6188582181930542
Blogs,2,87,hyperparameters,batch size,of,512,batch size of 512,0.6329059600830078
Blogs,2,87,hyperparameters,hyperparameters,trained for,one million steps,hyperparameters trained for one million steps,0.6964808702468872
Blogs,2,4,model,data-driven research,at,national library of sweden ( kb ),data-driven research at national library of sweden ( kb ),0.5391793251037598
Blogs,2,4,model,model,introduces,swedish bert (   kb - bert   ),model introduces swedish bert (   kb - bert   ),0.6138019561767578
Blogs,2,5,model,kb 's collections,to create and train,new language -specific bert model,kb 's collections to create and train new language -specific bert model,0.6496701836585999
Blogs,2,5,model,new language -specific bert model,for,swedish,new language -specific bert model for swedish,0.6221331357955933
Blogs,2,5,model,model,used,kb 's collections,model used kb 's collections,0.6281377673149109
Blogs,2,96,results,our model,outperforms,existing berts,our model outperforms existing berts,0.7438795566558838
Blogs,2,96,results,existing berts,trained for,multilingual understanding,existing berts trained for multilingual understanding,0.7560698986053467
Blogs,2,96,results,existing berts,specifically for,swedish by arbetsf ? rmedlingen,existing berts specifically for swedish by arbetsf ? rmedlingen,0.631878137588501
Blogs,2,96,results,multilingual understanding,by,google,multilingual understanding by google,0.6099311113357544
Blogs,2,96,results,our model,has,kb - bert,our model has kb - bert,0.6527401208877563
Blogs,2,96,results,results,show,our model,results show our model,0.6888449192047119
Blogs,4,183,ablation-analysis,noise,is,low,noise is low,0.6107912659645081
Blogs,4,183,ablation-analysis,noise,has,component,noise has component,0.5843780636787415
Blogs,4,183,ablation-analysis,low,has,component,low has component,0.6262804269790649
Blogs,4,183,ablation-analysis,l,has,very successfully,l has very successfully,0.6675172448158264
Blogs,4,222,ablation-analysis,greatly reduced,compared to,respective vqa accuracy,greatly reduced compared to respective vqa accuracy,0.6442518830299377
Blogs,4,222,ablation-analysis,ablation analysis,notice,all balanced pair accuracies,ablation analysis notice all balanced pair accuracies,0.6986890435218811
Blogs,4,257,experimental-setup,learning rate,increased from,0.001 to 0.0015,learning rate increased from 0.001 to 0.0015,0.6842507719993591
Blogs,4,257,experimental-setup,doubled,to,256,doubled to 256,0.676264762878418
Blogs,4,257,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
Blogs,4,257,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
Blogs,4,258,experimental-setup,model,trained for,100 epochs,model trained for 100 epochs,0.7761797308921814
Blogs,4,258,experimental-setup,100 epochs,instead of,"100,000 iterations","100 epochs instead of 100,000 iterations",0.6324622631072998
Blogs,4,258,experimental-setup,1697 iterations per epoch,to train on,training set,1697 iterations per epoch to train on training set,0.7303453087806702
Blogs,4,258,experimental-setup,100 epochs,has,1697 iterations per epoch,100 epochs has 1697 iterations per epoch,0.5709776282310486
Blogs,4,258,experimental-setup,experimental setup,has,model,experimental setup has model,0.5338840484619141
Blogs,4,6,model,neural network component,allows,robust counting,neural network component allows robust counting,0.6957897543907166
Blogs,4,6,model,robust counting,from,object proposals,robust counting from object proposals,0.556135892868042
Blogs,4,6,model,model,propose,neural network component,model propose neural network component,0.7107517123222351
Blogs,4,207,model,output,of,component,output of component,0.6522974967956543
Blogs,4,207,model,component,linearly projected into,same space,component linearly projected into same space,0.7994080185890198
Blogs,4,207,model,same space,as,hidden layer,same space as hidden layer,0.5857877731323242
Blogs,4,207,model,hidden layer,of,classifier,hidden layer of classifier,0.6343075037002563
Blogs,4,207,model,classifier,followed by,relu activation,classifier followed by relu activation,0.6771615147590637
Blogs,4,207,model,addition,features in,hidden layer,addition features in hidden layer,0.7386733293533325
Blogs,4,207,model,model,has,output,model has output,0.5534584522247314
Blogs,4,182,results,counting component,performs,better,counting component performs better,0.6968150734901428
Blogs,4,182,results,better,than,baseline,better than baseline,0.6157954335212708
Blogs,4,182,results,baseline,in,most cases,baseline in most cases,0.6253023743629456
Blogs,4,184,results,overlaps,are,limited,overlaps are limited,0.6369546055793762
Blogs,4,184,results,moderate noise levels,has,decently,moderate noise levels has decently,0.6195569038391113
Blogs,4,209,results,results,on,official vqa v2 leaderboard,results on official vqa v2 leaderboard,0.5371932983398438
Blogs,4,210,results,baseline,with,our component,baseline with our component,0.690666675567627
Blogs,4,210,results,significantly higher accuracy,on,number questions,significantly higher accuracy on number questions,0.5288360714912415
Blogs,4,210,results,significantly higher accuracy,without compromising,accuracy,significantly higher accuracy without compromising accuracy,0.7144536375999451
Blogs,4,210,results,accuracy,on,other categories,accuracy on other categories,0.4901086986064911
Blogs,4,210,results,baseline,has,significantly higher accuracy,baseline has significantly higher accuracy,0.5744352340698242
Blogs,4,210,results,our component,has,significantly higher accuracy,our component has significantly higher accuracy,0.593864917755127
Blogs,4,210,results,results,has,baseline,results has baseline,0.5317873954772949
Blogs,4,211,results,8 - model ensemble,on,number category,8 - model ensemble on number category,0.5794953107833862
Blogs,4,211,results,outperform,has,8 - model ensemble,outperform has 8 - model ensemble,0.5774462223052979
Blogs,4,219,results,nms - based approach,using,iou threshold,nms - based approach using iou threshold,0.6972256898880005
Blogs,4,219,results,nms - based approach,using,score thresholding,nms - based approach using score thresholding,0.6996667981147766
Blogs,4,219,results,nms - based approach,no,score thresholding,nms - based approach no score thresholding,0.6212971210479736
Blogs,4,219,results,iou threshold,of,0.5,iou threshold of 0.5,0.6428158283233643
Blogs,4,219,results,score thresholding,based on,validation set performance,score thresholding based on validation set performance,0.6499776840209961
Blogs,4,219,results,does not improve,on,baseline,does not improve on baseline,0.5804669260978699
Blogs,4,219,results,nms - based approach,has,does not improve,nms - based approach has does not improve,0.6170358061790466
Blogs,4,219,results,validation set performance,has,does not improve,validation set performance has does not improve,0.6115297675132751
Blogs,4,219,results,results,has,nms - based approach,results has nms - based approach,0.5642296671867371
Blogs,5,7,baselines,popular vqa dataset,by collecting,complementary images,popular vqa dataset by collecting complementary images,0.6430234313011169
Blogs,5,7,baselines,complementary images,such that,every question,complementary images such that every question,0.6134874820709229
Blogs,5,7,baselines,every question,in,balanced dataset,every question in balanced dataset,0.4991014301776886
Blogs,5,119,baselines,baselines,has,hierarchical co-attention ( hiecoatt ),baselines has hierarchical co-attention ( hiecoatt ),0.5892627835273743
Blogs,5,29,experiments,balanced vqa dataset,with,significantly reduced language biases,balanced vqa dataset with significantly reduced language biases,0.6024419069290161
Blogs,5,125,model,multimodal compact bilinear pooling mechanism,to attend,image features,multimodal compact bilinear pooling mechanism to attend image features,0.6258169412612915
Blogs,5,125,model,multimodal compact bilinear pooling mechanism,combine,attended image features,multimodal compact bilinear pooling mechanism combine attended image features,0.6303627490997314
Blogs,5,125,model,attended image features,with,language features,attended image features with language features,0.5856501460075378
Blogs,5,125,model,model,uses,multimodal compact bilinear pooling mechanism,model uses multimodal compact bilinear pooling mechanism,0.5374894738197327
Blogs,5,125,model,model,combine,attended image features,model combine attended image features,0.6768679022789001
Blogs,5,49,results,state - of- art vqa models,on,balanced dataset,state - of- art vqa models on balanced dataset,0.4665563404560089
Blogs,5,49,results,state - of- art vqa models,show that,models,state - of- art vqa models show that models,0.4547266364097595
Blogs,5,49,results,models,trained on,existing ' unbalanced ' vqa dataset,models trained on existing ' unbalanced ' vqa dataset,0.735726535320282
Blogs,5,49,results,models,perform,poorly,models perform poorly,0.624846875667572
Blogs,5,49,results,poorly,on,balanced dataset,poorly on balanced dataset,0.5678462982177734
Blogs,5,49,results,results,evaluate,state - of- art vqa models,results evaluate state - of- art vqa models,0.5780975222587585
Blogs,5,56,results,language biases,resulting in,dataset,language biases resulting in dataset,0.6389754414558411
Blogs,5,56,results,results,reduce,language biases,results reduce language biases,0.5546687245368958
Blogs,5,138,results,current state - of- art vqa models,trained on,original ( unbalanced ) vqa dataset,current state - of- art vqa models trained on original ( unbalanced ) vqa dataset,0.7058088779449463
Blogs,5,138,results,current state - of- art vqa models,perform,significantly worse,current state - of- art vqa models perform significantly worse,0.5566633939743042
Blogs,5,138,results,significantly worse,evaluated on,our balanced dataset,significantly worse evaluated on our balanced dataset,0.6853324174880981
Blogs,5,138,results,significantly worse,evaluating on,original unbalanced vqa dataset,significantly worse evaluating on original unbalanced vqa dataset,0.6783196330070496
Blogs,5,138,results,results,see that,current state - of- art vqa models,results see that current state - of- art vqa models,0.5756241679191589
Blogs,5,140,results,performance,has,improves,performance has improves,0.6178221106529236
Blogs,5,141,results,models,trained on,complete balanced dataset,models trained on complete balanced dataset,0.7447292804718018
Blogs,5,141,results,accuracy,improves by,2 - 3 %,accuracy improves by 2 - 3 %,0.7051101922988892
Blogs,5,141,results,models,has,accuracy,models has accuracy,0.5575966835021973
Blogs,5,141,results,complete balanced dataset,has,accuracy,complete balanced dataset has accuracy,0.5710964202880859
Blogs,5,141,results,results,when,models,results when models,0.654464602470398
Blogs,5,144,results,question-only approach,performs,significantly worse,question-only approach performs significantly worse,0.5879588723182678
Blogs,5,144,results,significantly worse,on,balanced dataset,significantly worse on balanced dataset,0.5120643377304077
Blogs,5,144,results,balanced dataset,compared to,unbalanced dataset,balanced dataset compared to unbalanced dataset,0.6438148617744446
Blogs,5,164,results,largest source of improvement,from,ub to b half b,largest source of improvement from ub to b half b,0.5483450889587402
Blogs,5,164,results,ub to b half b,is,  yes / no   answer-type,ub to b half b is   yes / no   answer-type,0.6098369359970093
Blogs,5,164,results,%,for,mcb,% for mcb,0.7128374576568604
Blogs,5,164,results,%,for,mcb,% for mcb,0.7128374576568604
Blogs,5,164,results,%,for,hiecoatt,% for hiecoatt,0.7579248547554016
Blogs,5,164,results,3 %,for,mcb,3 % for mcb,0.7080756425857544
Blogs,5,164,results,? 2 %,for,hiecoatt,? 2 % for hiecoatt,0.7516049742698669
Blogs,5,164,results,state - of- art vqa models,has,largest source of improvement,state - of- art vqa models has largest source of improvement,0.5366078019142151
Blogs,5,164,results,  yes / no   answer-type,has,%,  yes / no   answer-type has %,0.5258476138114929
Blogs,5,164,results,number   answer-type,has,3 %,number   answer-type has 3 %,0.5761064887046814
Blogs,5,164,results,results,for,state - of- art vqa models,results for state - of- art vqa models,0.5440773963928223
Blogs,5,233,results,significantly outperforms,as well as,vqa [ 3 ] model,significantly outperforms as well as vqa [ 3 ] model,0.6275122165679932
Blogs,5,233,results,our explanation model,has,significantly outperforms,our explanation model has significantly outperforms,0.6101616024971008
Blogs,5,233,results,significantly outperforms,has,random baseline,significantly outperforms has random baseline,0.5805304646492004
Blogs,5,233,results,results,see that,our explanation model,results see that our explanation model,0.7083145380020142
Blogs,6,124,ablation-analysis,actual squad1.1 dataset,allows us to achieve,1.7 and 1.2 point boost,actual squad1.1 dataset allows us to achieve 1.7 and 1.2 point boost,0.6861283779144287
Blogs,6,124,ablation-analysis,1.7 and 1.2 point boost,to,our em and f1 scores,1.7 and 1.2 point boost to our em and f1 scores,0.5429717302322388
Blogs,6,159,ablation-analysis,model scale,further to,1.2 billion parameters,model scale further to 1.2 billion parameters,0.6024190783500671
Blogs,6,159,ablation-analysis,model scale,improves,answer generation quality f1,model scale improves answer generation quality f1,0.6874242424964905
Blogs,6,159,ablation-analysis,1.2 billion parameters,improves,answer generation quality f1,1.2 billion parameters improves answer generation quality f1,0.6526508927345276
Blogs,6,159,ablation-analysis,answer generation quality f1,by,0.4,answer generation quality f1 by 0.4,0.5516582727432251
Blogs,6,159,ablation-analysis,em,improves by,0.1,em improves by 0.1,0.5945749282836914
Blogs,6,159,ablation-analysis,ablation analysis,find,model scale,ablation analysis find model scale,0.5836995840072632
Blogs,6,159,ablation-analysis,ablation analysis,increasing,model scale,ablation analysis increasing model scale,0.7457748055458069
Blogs,6,177,ablation-analysis,our gpt - 2 model,achieving,reasonable question generation,our gpt - 2 model achieving reasonable question generation,0.697587251663208
Blogs,6,177,ablation-analysis,pretraining,has,our gpt - 2 model,pretraining has our gpt - 2 model,0.5653044581413269
Blogs,6,94,experimental-setup,"gpt - 2 models ( radford et al. , 2019 )",used for,question generation,"gpt - 2 models ( radford et al. , 2019 ) used for question generation",0.679867684841156
Blogs,6,94,experimental-setup,"gpt - 2 models ( radford et al. , 2019 )",pretrained on,174gb corpora,"gpt - 2 models ( radford et al. , 2019 ) pretrained on 174gb corpora",0.7313902974128723
Blogs,6,94,experimental-setup,question generation,pretrained on,174gb corpora,question generation pretrained on 174gb corpora,0.7537816166877747
Blogs,6,94,experimental-setup,174gb corpora,used in,megatron - lm,174gb corpora used in megatron - lm,0.6496340036392212
Blogs,6,94,experimental-setup,174gb corpora,used in,"realnews ( zellers et al. , 2019 )","174gb corpora used in realnews ( zellers et al. , 2019 )",0.6552038192749023
Blogs,6,94,experimental-setup,experimental setup,has,"gpt - 2 models ( radford et al. , 2019 )","experimental setup has gpt - 2 models ( radford et al. , 2019 )",0.5213035941123962
Blogs,6,95,experimental-setup,gpt - 2 models,trained at,batch size,gpt - 2 models trained at batch size,0.7010563611984253
Blogs,6,95,experimental-setup,gpt - 2 models,trained at,"adamw ( loshchilov & hutter , 2018 )","gpt - 2 models trained at adamw ( loshchilov & hutter , 2018 )",0.6960114240646362
Blogs,6,95,experimental-setup,gpt - 2 models,trained at,global gradient norm clipping,gpt - 2 models trained at global gradient norm clipping,0.6814786195755005
Blogs,6,95,experimental-setup,gpt - 2 models,trained at,normal initialization,gpt - 2 models trained at normal initialization,0.6738288402557373
Blogs,6,95,experimental-setup,batch size,of,512,batch size of 512,0.6329059600830078
Blogs,6,95,experimental-setup,batch size,for,300k iterations,batch size for 300k iterations,0.568132758140564
Blogs,6,95,experimental-setup,512,for,300k iterations,512 for 300k iterations,0.5779311060905457
Blogs,6,95,experimental-setup,300k iterations,with,3 k iterations,300k iterations with 3 k iterations,0.6769000291824341
Blogs,6,95,experimental-setup,3 k iterations,of,warmup,3 k iterations of warmup,0.5973308086395264
Blogs,6,95,experimental-setup,"adamw ( loshchilov & hutter , 2018 )",for,optimization,"adamw ( loshchilov & hutter , 2018 ) for optimization",0.6047849059104919
Blogs,6,95,experimental-setup,learning rate,of,1.5e - 4,learning rate of 1.5e - 4,0.5943935513496399
Blogs,6,95,experimental-setup,learning rate,decaying,linearly,learning rate decaying linearly,0.7937746644020081
Blogs,6,95,experimental-setup,1.5e - 4,decaying,linearly,1.5e - 4 decaying linearly,0.6516156792640686
Blogs,6,95,experimental-setup,linearly,to,1e - 5,linearly to 1e - 5,0.652428925037384
Blogs,6,95,experimental-setup,weight decay,of,0.01,weight decay of 0.01,0.603249192237854
Blogs,6,95,experimental-setup,global gradient norm clipping,of,1.0,global gradient norm clipping of 1.0,0.5541549921035767
Blogs,6,95,experimental-setup,normal initialization,of,"? ? n ( 0 , 0.02 )","normal initialization of ? ? n ( 0 , 0.02 )",0.5586292743682861
Blogs,6,95,experimental-setup,optimization,has,learning rate,optimization has learning rate,0.4759896993637085
Blogs,6,95,experimental-setup,experimental setup,has,gpt - 2 models,experimental setup has gpt - 2 models,0.5567178130149841
Blogs,6,96,experimental-setup,gpt - 2 models,used,same hyperparameters,gpt - 2 models used same hyperparameters,0.6441545486450195
Blogs,6,96,experimental-setup,same hyperparameters,except for,batch size,same hyperparameters except for batch size,0.6342489123344421
Blogs,6,96,experimental-setup,same hyperparameters,except for,learning rate,same hyperparameters except for learning rate,0.6304497718811035
Blogs,6,96,experimental-setup,batch size,of,32,batch size of 32,0.6741614937782288
Blogs,6,96,experimental-setup,learning rate,of,2e - 5,learning rate of 2e - 5,0.6410878896713257
Blogs,6,96,experimental-setup,2e - 5,decaying to,zero,2e - 5 decaying to zero,0.7740945816040039
Blogs,6,96,experimental-setup,zero,over,six epochs,zero over six epochs,0.7298402786254883
Blogs,6,96,experimental-setup,six epochs,of,finetuning data,six epochs of finetuning data,0.6031056046485901
Blogs,6,96,experimental-setup,experimental setup,Finetuning,gpt - 2 models,experimental setup Finetuning gpt - 2 models,0.7643218040466309
Blogs,6,103,experimental-setup,bert models,for,filtration,bert models for filtration,0.7136543989181519
Blogs,6,103,experimental-setup,bert models,for,filtration,bert models for filtration,0.7136543989181519
Blogs,6,103,experimental-setup,bert models,done with,learning rate,bert models done with learning rate,0.6644358038902283
Blogs,6,103,experimental-setup,bert models,done with,cosine decay schedule,bert models done with cosine decay schedule,0.6530970931053162
Blogs,6,103,experimental-setup,learning rate,of,1e - 5,learning rate of 1e - 5,0.6323861479759216
Blogs,6,103,experimental-setup,cosine decay schedule,over,2 epochs,cosine decay schedule over 2 epochs,0.7095196843147278
Blogs,6,103,experimental-setup,2 epochs,of,training data,2 epochs of training data,0.57123202085495
Blogs,6,103,experimental-setup,experimental setup,Finetuning,bert models,experimental setup Finetuning bert models,0.7332741618156433
Blogs,6,113,experimental-setup,mixed precision training,on,nvidia v100 gpus,mixed precision training on nvidia v100 gpus,0.48828524351119995
Blogs,6,113,experimental-setup,experimental setup,trained with,mixed precision training,experimental setup trained with mixed precision training,0.7407509088516235
Blogs,6,213,hyperparameters,overgeneration,sample,one question,overgeneration sample one question,0.7489253282546997
Blogs,6,213,hyperparameters,overgeneration,one with,top -p ( p = 0.9 ) nucleus sampling,overgeneration one with top -p ( p = 0.9 ) nucleus sampling,0.2774689197540283
Blogs,6,213,hyperparameters,one question,with,top -k ( k = 40 ) sampling,one question with top -k ( k = 40 ) sampling,0.6686967611312866
Blogs,6,213,hyperparameters,one question,with,top -p ( p = 0.9 ) nucleus sampling,one question with top -p ( p = 0.9 ) nucleus sampling,0.637792706489563
Blogs,6,213,hyperparameters,hyperparameters,To perform,overgeneration,hyperparameters To perform overgeneration,0.6473208069801331
Blogs,6,12,model,question generators,on,labeled data,question generators on labeled data,0.5434548258781433
Blogs,6,12,model,question generators,uses,pretrained gpt - 2 decoder models,question generators uses pretrained gpt - 2 decoder models,0.5310307741165161
Blogs,6,12,model,concatenation,of,"context , answer , and question tokens","concatenation of context , answer , and question tokens",0.5512832403182983
Blogs,6,7,results,squad1.1 question answering,achieve,higher accuracy,squad1.1 question answering achieve higher accuracy,0.6146108508110046
Blogs,6,7,results,higher accuracy,using,synthetic questions and answers,higher accuracy using synthetic questions and answers,0.6607744693756104
Blogs,6,10,results,our methodology,to,squad2.0,our methodology to squad2.0,0.556906521320343
Blogs,6,10,results,squad2.0,show,2.8 absolute gain,squad2.0 show 2.8 absolute gain,0.6377492547035217
Blogs,6,10,results,2.8 absolute gain,on,em score,2.8 absolute gain on em score,0.5445185899734497
Blogs,6,10,results,2.8 absolute gain,compared to,prior work,2.8 absolute gain compared to prior work,0.5605018138885498
Blogs,6,10,results,prior work,using,synthetic data,prior work using synthetic data,0.6283978819847107
Blogs,6,10,results,results,apply,our methodology,results apply our methodology,0.581484854221344
Blogs,6,13,results,large generative transformer models,up to,8.3b parameters,large generative transformer models up to 8.3b parameters,0.5648163557052612
Blogs,6,13,results,large generative transformer models,improves,quality of generated questions,large generative transformer models improves quality of generated questions,0.6446401476860046
Blogs,6,13,results,results,pretraining,large generative transformer models,results pretraining large generative transformer models,0.6535381078720093
Blogs,6,36,results,scores,of,88.4 and 94.1,scores of 88.4 and 94.1,0.5615087151527405
Blogs,6,36,results,88.4 and 94.1,versus,supervised training,88.4 and 94.1 versus supervised training,0.6430784463882446
Blogs,6,36,results,88.4 and 94.1,achieves,87.7 em and 94.0 f1,88.4 and 94.1 achieves 87.7 em and 94.0 f1,0.6353594064712524
Blogs,6,36,results,supervised training,achieves,87.7 em and 94.0 f1,supervised training achieves 87.7 em and 94.0 f1,0.643739640712738
Blogs,6,36,results,results,achieve,scores,results achieve scores,0.5672783851623535
Blogs,6,37,results,resulting model,on,real squad1.1 data,resulting model on real squad1.1 data,0.5375103950500488
Blogs,6,37,results,real squad1.1 data,reaches,89.4 em,real squad1.1 data reaches 89.4 em,0.7166900038719177
Blogs,6,37,results,real squad1.1 data,reaches,95.1 f1 score,real squad1.1 data reaches 95.1 f1 score,0.6773371696472168
Blogs,6,37,results,real squad1.1 data,higher than,any prior bertbased approach,real squad1.1 data higher than any prior bertbased approach,0.6517101526260376
Blogs,6,37,results,95.1 f1 score,higher than,any prior bertbased approach,95.1 f1 score higher than any prior bertbased approach,0.6649986505508423
Blogs,6,37,results,results,Finetuning,resulting model,results Finetuning resulting model,0.6363301873207092
Blogs,6,40,results,em,of,88.4,em of 88.4,0.6168514490127563
Blogs,6,40,results,f1,of,93.9,f1 of 93.9,0.5851284265518188
Blogs,6,62,results,answer candidates,with,dataset answers,answer candidates with dataset answers,0.6078630685806274
Blogs,6,62,results,answer candidates,through,end-to - end model - based approaches,answer candidates through end-to - end model - based approaches,0.6991564631462097
Blogs,6,62,results,our performance,comparable to,fullysupervised baseline,our performance comparable to fullysupervised baseline,0.6708530783653259
Blogs,6,62,results,end-to - end model - based approaches,has,our performance,end-to - end model - based approaches has our performance,0.550916314125061
Blogs,6,62,results,results,aligning,answer candidates,results aligning answer candidates,0.6478098034858704
Blogs,6,138,results,pretraining tasks,has,pretraining scale,pretraining tasks has pretraining scale,0.5568758249282837
Blogs,6,138,results,pretraining tasks,has,synthetic data,pretraining tasks has synthetic data,0.5352398157119751
Blogs,6,138,results,model scale,has,synthetic data,model scale has synthetic data,0.5505506992340088
Blogs,6,138,results,synthetic data,has,improves,synthetic data has improves,0.5949409008026123
Blogs,6,138,results,results,improve,pretraining tasks,results improve pretraining tasks,0.5724780559539795
Blogs,6,158,results,answer generation quality,by,1.4 em and 0.3 f1,answer generation quality by 1.4 em and 0.3 f1,0.5686770081520081
Blogs,6,158,results,1.4 em and 0.3 f1,between,bert - large and our 345 million parameter answer generation model,1.4 em and 0.3 f1 between bert - large and our 345 million parameter answer generation model,0.6081883907318115
Blogs,6,158,results,results,improvements in,pretraining data and tasks,results improvements in pretraining data and tasks,0.6746299266815186
Blogs,7,109,baselines,two image feature inputs,start and the end of,sentence,two image feature inputs start and the end of sentence,0.6836731433868408
Blogs,7,109,baselines,two image feature inputs,with,different learned linear transformations,two image feature inputs with different learned linear transformations,0.604773998260498
Blogs,7,109,baselines,sentence,with,different learned linear transformations,sentence with different learned linear transformations,0.6202320456504822
Blogs,7,109,baselines,lstms,going in,forward and backward directions,lstms going in forward and backward directions,0.6877199411392212
Blogs,7,109,baselines,baselines,has,vis + blstm,baselines has vis + blstm,0.5263214707374573
Blogs,7,114,baselines,  full   model,is,simple average of the three models,  full   model is simple average of the three models,0.5553226470947266
Blogs,7,114,baselines,full,has,  full   model,full has   full   model,0.5595760941505432
Blogs,7,114,baselines,baselines,has,full,baselines has full,0.5944684743881226
Blogs,7,118,baselines,mode,based on,question type,mode based on question type,0.639739453792572
Blogs,7,118,baselines,baselines,has,guess,baselines has guess,0.6508995294570923
Blogs,7,121,baselines,bow,designed,set of   blind   models,bow designed set of   blind   models,0.5932058691978455
Blogs,7,121,baselines,set of   blind   models,given,questions,set of   blind   models given questions,0.7504801750183105
Blogs,7,121,baselines,questions,without,images,questions without images,0.7687709927558899
Blogs,7,121,baselines,baselines,has,bow,baselines has bow,0.6360316872596741
Blogs,7,123,baselines,question words,into,lstm,question words into lstm,0.5592907667160034
Blogs,7,123,baselines,baselines,has,lstm,baselines has lstm,0.5395978093147278
Blogs,7,124,baselines,img,trained,counterpart   deaf   model,img trained counterpart   deaf   model,0.7349976301193237
Blogs,7,124,baselines,baselines,has,img,baselines has img,0.5373448133468628
Blogs,7,134,baselines,nearest neighbors baseline approach,performs,very well,nearest neighbors baseline approach performs very well,0.5640770196914673
Blogs,7,134,baselines,baselines,has,k-nn,baselines has k-nn,0.5469836592674255
Blogs,7,54,experimental-setup,cnn part,of,our model,cnn part of our model,0.5208131670951843
Blogs,7,54,experimental-setup,cnn part,kept,frozen,cnn part kept frozen,0.7169509530067444
Blogs,7,54,experimental-setup,our model,kept,frozen,our model kept frozen,0.675431489944458
Blogs,7,54,experimental-setup,frozen,during,training,frozen during training,0.7916955947875977
Blogs,7,54,experimental-setup,experimental setup,has,cnn part,experimental setup has cnn part,0.5576770305633545
Blogs,7,5,model,neural networks and visual semantic embeddings,without,intermediate stages,neural networks and visual semantic embeddings without intermediate stages,0.6689477562904358
Blogs,7,5,model,neural networks and visual semantic embeddings,to predict,answers,neural networks and visual semantic embeddings to predict answers,0.6250411868095398
Blogs,7,5,model,intermediate stages,such as,object detection,intermediate stages such as object detection,0.5827900171279907
Blogs,7,5,model,intermediate stages,such as,image segmentation,intermediate stages such as image segmentation,0.6237288117408752
Blogs,7,5,model,answers,simple questions about,images,answers simple questions about images,0.7970008254051208
Blogs,7,5,model,model,propose to use,neural networks and visual semantic embeddings,model propose to use neural networks and visual semantic embeddings,0.6878485679626465
Blogs,7,7,model,question generation algorithm,converts,image descriptions,question generation algorithm converts image descriptions,0.6522124409675598
Blogs,7,7,model,image descriptions,into,qa form,image descriptions into qa form,0.6511760354042053
Blogs,7,16,model,generic end-to- end qa model,using,visual semantic embeddings,generic end-to- end qa model using visual semantic embeddings,0.5747027397155762
Blogs,7,16,model,visual semantic embeddings,to connect,cnn,visual semantic embeddings to connect cnn,0.5978268980979919
Blogs,7,16,model,visual semantic embeddings,to connect,recurrent neural net ( rnn ),visual semantic embeddings to connect recurrent neural net ( rnn ),0.5816835761070251
Blogs,7,16,model,automatic question generation algorithm,converts,description sentences,automatic question generation algorithm converts description sentences,0.5686002969741821
Blogs,7,16,model,description sentences,into,questions,description sentences into questions,0.57557213306427
Blogs,7,144,results,results,on,daquar and coco - qa,results on daquar and coco - qa,0.553086519241333
Blogs,7,148,results,existing approach,in terms of,answer accuracy and wups,existing approach in terms of answer accuracy and wups,0.7083361148834229
Blogs,7,148,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
Blogs,7,148,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
Blogs,7,148,results,outperforms,has,existing approach,outperforms has existing approach,0.6274049282073975
Blogs,7,148,results,results,observe,our model,results observe our model,0.6353915333747864
Blogs,7,149,results,vis +lstm and malinkowski et al . 's recurrent neural network model,achieved,somewhat similar performance,vis +lstm and malinkowski et al . 's recurrent neural network model achieved somewhat similar performance,0.6855635046958923
Blogs,7,149,results,somewhat similar performance,on,daquar,somewhat similar performance on daquar,0.6022284030914307
Blogs,7,149,results,results,has,vis +lstm and malinkowski et al . 's recurrent neural network model,results has vis +lstm and malinkowski et al . 's recurrent neural network model,0.5446808934211731
Blogs,7,150,results,all three models,boosts,performance,all three models boosts performance,0.6775514483451843
Blogs,7,150,results,performance,by,1 - 2 %,performance by 1 - 2 %,0.5953799486160278
Blogs,7,150,results,outperforming,has,other models,outperforming has other models,0.5912055373191833
Blogs,7,151,results,img + bow model,is,very strong,img + bow model is very strong,0.5409553050994873
Blogs,7,151,results,very strong,on,both datasets,very strong on both datasets,0.48713546991348267
Blogs,7,151,results,results,see that,img + bow model,results see that img + bow model,0.629114031791687
Blogs,7,157,results,blind models,by,large margin,blind models by large margin,0.5514585375785828
Blogs,7,157,results,large margin,on,coco - qa,large margin on coco - qa,0.6128606796264648
Blogs,7,157,results,non-blind models,has,outperform,non-blind models has outperform,0.6117192506790161
Blogs,7,157,results,outperform,has,blind models,outperform has blind models,0.6046385169029236
Blogs,7,157,results,results,has,non-blind models,results has non-blind models,0.5383472442626953
Blogs,7,164,results,fine-tuning,normalizing,cnn hidden image features,fine-tuning normalizing cnn hidden image features,0.6820369362831116
Blogs,7,164,results,word embedding,results in,better performance,word embedding results in better performance,0.6020072102546692
Blogs,7,164,results,cnn hidden image features,into,zero-mean and unit-variance,cnn hidden image features into zero-mean and unit-variance,0.5509687066078186
Blogs,7,164,results,cnn hidden image features,helps achieve,faster training time,cnn hidden image features helps achieve faster training time,0.6438292264938354
Blogs,7,164,results,fine-tuning,has,word embedding,fine-tuning has word embedding,0.5228998064994812
Blogs,7,164,results,results,observed,fine-tuning,results observed fine-tuning,0.6238250732421875
Blogs,7,164,results,results,normalizing,cnn hidden image features,results normalizing cnn hidden image features,0.6722899675369263
Blogs,7,165,results,bidirectional lstm model,further boost,result,bidirectional lstm model further boost result,0.6853188276290894
Blogs,7,165,results,result,by,little,result by little,0.6294168829917908
Blogs,7,165,results,results,has,bidirectional lstm model,results has bidirectional lstm model,0.5401583909988403
Blogs,7,167,results,original cnn,trained for,imagenet challenge,original cnn trained for imagenet challenge,0.6706234216690063
Blogs,7,167,results,img + bow,benefited significantly from,single object recognition ability,img + bow benefited significantly from single object recognition ability,0.5926463007926941
Blogs,7,167,results,original cnn,has,img + bow,original cnn has img + bow,0.4958612024784088
Blogs,7,167,results,imagenet challenge,has,img + bow,imagenet challenge has img + bow,0.5141999125480652
Blogs,7,167,results,results,has,original cnn,results has original cnn,0.5568488240242004
Blogs,7,171,results,full model,improves,accuracy,full model improves accuracy,0.7261021137237549
Blogs,7,171,results,accuracy,by,50 %,accuracy by 50 %,0.6234867572784424
Blogs,7,171,results,50 %,compared to,img model,50 % compared to img model,0.7192924618721008
Blogs,7,171,results,results,has,full model,results has full model,0.523858904838562
Blogs,7,173,results,daquar,could not observe,any advantage,daquar could not observe any advantage,0.6325254440307617
Blogs,7,173,results,any advantage,in,counting ability,any advantage in counting ability,0.5318623185157776
Blogs,7,173,results,counting ability,of,img + bow and the vis + lstm model,counting ability of img + bow and the vis + lstm model,0.5877810120582581
Blogs,7,173,results,counting ability,compared to,blind baselines,counting ability compared to blind baselines,0.6117218732833862
Blogs,7,173,results,results,In,daquar,results In daquar,0.6118186712265015
Blogs,7,178,results,color,In,coco - qa,color In coco - qa,0.5940804481506348
Blogs,7,178,results,significant win,for,img + bow and the vis + lstm,significant win for img + bow and the vis + lstm,0.6328017711639404
Blogs,7,178,results,significant win,on,color-type questions,significant win on color-type questions,0.5297413468360901
Blogs,7,178,results,img + bow and the vis + lstm,against,blind ones,img + bow and the vis + lstm against blind ones,0.6727194786071777
Blogs,7,178,results,blind ones,on,color-type questions,blind ones on color-type questions,0.5138752460479736
Blogs,7,178,results,results,has,color,results has color,0.5057007074356079
Blogs,8,228,baselines,structured svm,incorporates,external labeled data,structured svm incorporates external labeled data,0.6698557734489441
Blogs,8,228,baselines,external labeled data,from,existing nlp tasks,external labeled data from existing nlp tasks,0.4949348270893097
Blogs,8,53,model,model,propose,framework,model propose framework,0.666053295135498
Blogs,8,260,results,n,-,gram and lstm baselines,n - gram and lstm baselines,0.5880518555641174
Blogs,8,260,results,standard memnns,has,outperform,standard memnns has outperform,0.619864821434021
Blogs,8,260,results,outperform,has,n,outperform has n,0.7325218319892883
Blogs,8,260,results,outperform,has,gram and lstm baselines,outperform has gram and lstm baselines,0.595887303352356
Blogs,8,260,results,n,has,gram and lstm baselines,n has gram and lstm baselines,0.5788949131965637
Blogs,8,266,results,adaptive approach,gives,straight - forward improvement,adaptive approach gives straight - forward improvement,0.607408344745636
Blogs,8,266,results,adaptive approach,gives,( small ) improvements,adaptive approach gives ( small ) improvements,0.5883227586746216
Blogs,8,266,results,adaptive approach,gives,( small ) improvements,adaptive approach gives ( small ) improvements,0.5883227586746216
Blogs,8,266,results,straight - forward improvement,in,tasks 3 and 16,straight - forward improvement in tasks 3 and 16,0.4893121123313904
Blogs,8,266,results,( small ) improvements,in,8 and 19,( small ) improvements in 8 and 19,0.5656313896179199
Blogs,8,266,results,results,has,adaptive approach,results has adaptive approach,0.5844767093658447
Blogs,8,272,results,structured svm,does not perform,better,structured svm does not perform better,0.7388725876808167
Blogs,8,272,results,better,still,failing,better still failing,0.7645778656005859
Blogs,8,272,results,failing,at,9 tasks,failing at 9 tasks,0.5221072435379028
Blogs,8,272,results,results,has,structured svm,results has structured svm,0.5908066630363464
Blogs,8,273,results,better,than,vanilla memnns ( without extensions ),better than vanilla memnns ( without extensions ),0.5814065337181091
Blogs,8,273,results,vanilla memnns ( without extensions ),on,"tasks 6 , 9 and 10","vanilla memnns ( without extensions ) on tasks 6 , 9 and 10",0.5395894646644592
Blogs,8,273,results,"tasks 6 , 9 and 10",where,hand- built feature conjunctions,"tasks 6 , 9 and 10 where hand- built feature conjunctions",0.5441622138023376
Blogs,8,273,results,hand- built feature conjunctions,capture,necessary nonlinearities,hand- built feature conjunctions capture necessary nonlinearities,0.7319881319999695
Blogs,8,274,results,memnn ( am + ng + nl ),seems to do,significantly worse,memnn ( am + ng + nl ) seems to do significantly worse,0.650115966796875
Blogs,8,274,results,significantly worse,on,tasks,significantly worse on tasks,0.516948938369751
Blogs,8,274,results,tasks,requiring,"three ( and sometimes , two ) supporting facts","tasks requiring three ( and sometimes , two ) supporting facts",0.6596214175224304
Blogs,8,274,results,results,compared to,memnn ( am + ng + nl ),results compared to memnn ( am + ng + nl ),0.641545295715332
Blogs,8,351,results,does not perform better,than,memnns,does not perform better than memnns,0.5948800444602966
Blogs,8,351,results,failing,at,9 tasks,failing at 9 tasks,0.5221072435379028
Blogs,8,351,results,structured svm,has,does not perform better,structured svm has does not perform better,0.6171784400939941
Blogs,8,351,results,results,has,structured svm,results has structured svm,0.5908066630363464
Blogs,9,251,baselines,baselines,has,prior (   yes   ),baselines has prior (   yes   ),0.5481270551681519
Blogs,9,254,baselines,baselines,has,per q-type prior,baselines has per q-type prior,0.5872213840484619
Blogs,9,257,baselines,nearest neighbor,Given,"test image , question pair","nearest neighbor Given test image , question pair",0.6997825503349304
Blogs,9,257,baselines,"test image , question pair",first find,k nearest neighbor questions,"test image , question pair first find k nearest neighbor questions",0.642826497554779
Blogs,9,257,baselines,"test image , question pair",first find,associated images,"test image , question pair first find associated images",0.6545392870903015
Blogs,9,257,baselines,associated images,from,training set,associated images from training set,0.5642402172088623
Blogs,9,268,baselines,norm i,are,2 normalized activations,norm i are 2 normalized activations,0.5306112766265869
Blogs,9,268,baselines,2 normalized activations,from,last hidden layer,2 normalized activations from last hidden layer,0.5519264936447144
Blogs,9,268,baselines,last hidden layer,of,vggnet,last hidden layer of vggnet,0.551103949546814
Blogs,9,270,baselines,three embeddings,has,bag-of-words question ( bow q ),three embeddings has bag-of-words question ( bow q ),0.5971183180809021
Blogs,9,274,baselines,lstm q,with,one hidden layer,lstm q with one hidden layer,0.626903772354126
Blogs,9,274,baselines,lstm,with,one hidden layer,lstm with one hidden layer,0.6389833092689514
Blogs,9,274,baselines,one hidden layer,to obtain,1024 - dim embedding,one hidden layer to obtain 1024 - dim embedding,0.5804229974746704
Blogs,9,274,baselines,1024 - dim embedding,for,question,1024 - dim embedding for question,0.6176701784133911
Blogs,9,274,baselines,lstm q,has,lstm,lstm q has lstm,0.5930533409118652
Blogs,9,274,baselines,baselines,has,lstm q,baselines has lstm q,0.5797204375267029
Blogs,9,277,baselines,lstm,with,two hidden layers,lstm with two hidden layers,0.5861539244651794
Blogs,9,277,baselines,two hidden layers,to obtain,2048 - dim embedding,two hidden layers to obtain 2048 - dim embedding,0.5801377892494202
Blogs,9,277,baselines,2048 - dim embedding,for,question,2048 - dim embedding for question,0.6250718832015991
Blogs,9,277,baselines,2048 - dim embedding,followed by,fully - connected layer + tanh non-linearity,2048 - dim embedding followed by fully - connected layer + tanh non-linearity,0.6650307774543762
Blogs,9,277,baselines,fully - connected layer + tanh non-linearity,to transform,2048 - dim embedding,fully - connected layer + tanh non-linearity to transform 2048 - dim embedding,0.7291309833526611
Blogs,9,277,baselines,2048 - dim embedding,to,1024 - dim,2048 - dim embedding to 1024 - dim,0.5867710709571838
Blogs,9,277,baselines,deeper lstm q,has,lstm,deeper lstm q has lstm,0.576751172542572
Blogs,9,277,baselines,baselines,has,deeper lstm q,baselines has deeper lstm q,0.5777552127838135
Blogs,9,281,baselines,image embedding,first transformed to,1024 - dim,image embedding first transformed to 1024 - dim,0.6584061980247498
Blogs,9,281,baselines,1024 - dim,by,fully - connected layer + tanh non-linearity,1024 - dim by fully - connected layer + tanh non-linearity,0.5616076588630676
Blogs,9,281,baselines,fully - connected layer + tanh non-linearity,to match,lstm embedding,fully - connected layer + tanh non-linearity to match lstm embedding,0.6600813269615173
Blogs,9,281,baselines,lstm embedding,of,question,lstm embedding of question,0.5581955313682556
Blogs,9,281,baselines,lstm q + i,has,image embedding,lstm q + i has image embedding,0.5381043553352356
Blogs,9,281,baselines,baselines,For,lstm q + i,baselines For lstm q + i,0.5836928486824036
Blogs,9,250,hyperparameters,answer,from,top 1 k answers,answer from top 1 k answers,0.5340299010276794
Blogs,9,250,hyperparameters,top 1 k answers,of,vqa train / val dataset,top 1 k answers of vqa train / val dataset,0.5503803491592407
Blogs,9,250,hyperparameters,hyperparameters,randomly choose,answer,hyperparameters randomly choose answer,0.7145264148712158
Blogs,9,275,hyperparameters,question word,encoded with,300 - dim embedding,question word encoded with 300 - dim embedding,0.7492119073867798
Blogs,9,275,hyperparameters,300 - dim embedding,by,fully - connected layer,300 - dim embedding by fully - connected layer,0.5291987657546997
Blogs,9,275,hyperparameters,fully - connected layer,+,tanh non-linearity,fully - connected layer + tanh non-linearity,0.6353135704994202
Blogs,9,275,hyperparameters,tanh non-linearity,fed to,lstm,tanh non-linearity fed to lstm,0.6385896801948547
Blogs,9,275,hyperparameters,hyperparameters,has,question word,hyperparameters has question word,0.5600332021713257
Blogs,9,262,model,2 - channel vision ( image ) + language ( question ) model,culminates with,softmax,2 - channel vision ( image ) + language ( question ) model culminates with softmax,0.6511692404747009
Blogs,9,262,model,softmax,over,k possible outputs,softmax over k possible outputs,0.6457448601722717
Blogs,9,262,model,model,develop,2 - channel vision ( image ) + language ( question ) model,model develop 2 - channel vision ( image ) + language ( question ) model,0.6190880537033081
Blogs,9,239,results,results,better than,humans,results better than humans,0.558944046497345
Blogs,9,239,results,humans,shown,questions alone,humans shown questions alone,0.7021270394325256
Blogs,9,239,results,results,has,results,results has results,0.48582205176353455
Blogs,9,292,results,accuracy,of,baselines and methods,accuracy of baselines and methods,0.5567294955253601
Blogs,9,292,results,baselines and methods,for,open-ended and multiple -choice tasks,baselines and methods for open-ended and multiple -choice tasks,0.48367294669151306
Blogs,9,292,results,baselines and methods,both,open-ended and multiple -choice tasks,baselines and methods both open-ended and multiple -choice tasks,0.5729702115058899
Blogs,9,292,results,open-ended and multiple -choice tasks,on,vqa test - dev,open-ended and multiple -choice tasks on vqa test - dev,0.5231440663337708
Blogs,9,292,results,vqa test - dev,for,real images,vqa test - dev for real images,0.5527042150497437
Blogs,9,293,results,vision- alone model,completely ignores,question,vision- alone model completely ignores question,0.7978675961494446
Blogs,9,293,results,vision- alone model,performs,rather poorly,vision- alone model performs rather poorly,0.6041169166564941
Blogs,9,293,results,rather poorly,has,open-ended,rather poorly has open-ended,0.57573401927948
Blogs,9,293,results,open-ended,has,28.13 %,open-ended has 28.13 %,0.5067142248153687
Blogs,9,293,results,multiple-choice,has,30.53 %,multiple-choice has 30.53 %,0.5403236150741577
Blogs,9,293,results,results,has,vision- alone model,results has vision- alone model,0.5232524871826172
Blogs,9,294,results,vision - alone model ( i ),performs,worse,vision - alone model ( i ) performs worse,0.644905686378479
Blogs,9,294,results,worse,than,prior (   yes   ) baseline,worse than prior (   yes   ) baseline,0.547385573387146
Blogs,9,294,results,open-ended task,has,vision - alone model ( i ),open-ended task has vision - alone model ( i ),0.5507751703262329
Blogs,9,294,results,results,on,open-ended task,results on open-ended task,0.4863594174385071
Blogs,9,302,results,answered,using,scene-level information,answered using scene-level information,0.6875074505805969
Blogs,9,302,results,scene-level information,such as,what sport,scene-level information such as what sport,0.664219081401825
Blogs,9,302,results,scene-level information,see,improvement,scene-level information see improvement,0.5570366382598877
Blogs,9,302,results,results,for,questions,results for questions,0.5702887773513794
Blogs,9,303,results,answer,may be contained in,generic caption,answer may be contained in generic caption,0.6546372771263123
Blogs,9,303,results,generic caption,see,improvement,generic caption see improvement,0.5684717297554016
Blogs,9,303,results,improvement,such as,what animal,improvement such as what animal,0.6765339970588684
Blogs,9,303,results,results,for,questions,results for questions,0.5702887773513794
Blogs,9,304,results,results,worse than,human accuracies,results worse than human accuracies,0.693446159362793
Blogs,9,304,results,all question types,has,results,all question types has results,0.537274181842804
Blogs,9,304,results,results,For,all question types,results For all question types,0.5368905663490295
Blogs,9,336,results,kolmogorov -smirnov test,to determine,underlying distributions,kolmogorov -smirnov test to determine underlying distributions,0.6204172968864441
Blogs,9,336,results,significant difference,for,all three parts of speech ( p < .001 ),significant difference for all three parts of speech ( p < .001 ),0.5855672955513
Blogs,9,336,results,all three parts of speech ( p < .001 ),for,real images,all three parts of speech ( p < .001 ) for real images,0.597743570804596
Blogs,9,336,results,all three parts of speech ( p < .001 ),for,abstract scenes,all three parts of speech ( p < .001 ) for abstract scenes,0.6093294620513916
Blogs,9,336,results,results,Using,kolmogorov -smirnov test,results Using kolmogorov -smirnov test,0.5883175134658813
Blogs,9,364,results,multiple - choice accuracies,are,more or less same,multiple - choice accuracies are more or less same,0.5704579949378967
Blogs,9,364,results,multiple - choice accuracies,are,significantly better,multiple - choice accuracies are significantly better,0.535564661026001
Blogs,9,364,results,more or less same,for,  yes / no   questions,more or less same for   yes / no   questions,0.6016783714294434
Blogs,9,364,results,% increase,for,real images,% increase for real images,0.6245089173316956
Blogs,9,364,results,? 11 % increase,for,abstract scenes,? 11 % increase for abstract scenes,0.6231384873390198
Blogs,9,364,results,openended answer,has,multiple - choice accuracies,openended answer has multiple - choice accuracies,0.5638842582702637
Blogs,9,364,results,significantly better,has,% increase,significantly better has % increase,0.5597070455551147
Blogs,9,364,results,significantly better,has,? 11 % increase,significantly better has ? 11 % increase,0.570884108543396
Blogs,9,364,results,results,In comparison to,openended answer,results In comparison to openended answer,0.6741586923599243
Blogs,9,365,results,accuracy,using,multiple choice,accuracy using multiple choice,0.6765971779823303
Blogs,9,365,results,results,increase in,accuracy,results increase in accuracy,0.6758071780204773
Blogs,9,468,results,language- alone methods,that ignore,image,language- alone methods that ignore image,0.7335648536682129
Blogs,9,468,results,language- alone methods,perform,surprisingly well,language- alone methods perform surprisingly well,0.5478841662406921
Blogs,9,468,results,bow q,achieving,48.09 %,bow q achieving 48.09 %,0.6638070940971375
Blogs,9,468,results,bow q,achieving,48.09 %,bow q achieving 48.09 %,0.6638070940971375
Blogs,9,468,results,bow q,achieving,48.76 %,bow q achieving 48.76 %,0.6575385332107544
Blogs,9,468,results,48.09 %,on,open-ended,48.09 % on open-ended,0.44385233521461487
Blogs,9,468,results,lstm q,achieving,48.76 %,lstm q achieving 48.76 %,0.619804859161377
Blogs,9,468,results,48.76 %,on,open - ended,48.76 % on open - ended,0.4474453032016754
Blogs,9,468,results,language- alone methods,has,per q-type prior,language- alone methods has per q-type prior,0.5767946243286133
Blogs,9,468,results,open-ended,has,53.68 % on multiple-choice ),open-ended has 53.68 % on multiple-choice ),0.5125443339347839
Blogs,9,468,results,open - ended,has,54.75 %,open - ended has 54.75 %,0.5105107426643372
Blogs,9,468,results,outperforming,has,nearest neighbor baseline,outperforming has nearest neighbor baseline,0.5905044674873352
Blogs,9,468,results,nearest neighbor baseline,has,open-ended,nearest neighbor baseline has open-ended,0.597992479801178
Blogs,9,468,results,open-ended,has,42.70 %,open-ended has 42.70 %,0.5136278867721558
Blogs,9,468,results,multiple - choice,has,48.49 %,multiple - choice has 48.49 %,0.5391812920570374
Blogs,9,476,results,our model,able to,significantly outperform,our model able to significantly outperform,0.7166519165039062
Blogs,9,476,results,significantly outperform,both,vision-alone and languagealone baselines,significantly outperform both vision-alone and languagealone baselines,0.6789020299911499
Blogs,9,476,results,results,see that,our model,results see that our model,0.6820751428604126
Blogs,9,477,results,results,on,multiple -choice,results on multiple -choice,0.4408847689628601
Blogs,9,477,results,multiple -choice,are,better,multiple -choice are better,0.6078062653541565
Blogs,9,477,results,better,than,open-ended,better than open-ended,0.5880767703056335
Blogs,9,477,results,results,on,multiple -choice,results on multiple -choice,0.4408847689628601
Blogs,9,478,results,significantly worse,than,human performance,significantly worse than human performance,0.6102992296218872
Blogs,9,478,results,results,has,all methods,results has all methods,0.48065561056137085
Blogs,10,1192,baselines,baselines,has,convai competition,baselines has convai competition,0.603452205657959
Blogs,10,1196,experiments,ntcir stc,focuses on,conversation,ntcir stc focuses on conversation,0.7608308792114258
Blogs,10,1196,experiments,conversation,via,short texts,conversation via short texts,0.6244734525680542
Blogs,10,1465,model,elmo,combines,intermediate layer representations,elmo combines intermediate layer representations,0.6932921409606934
Blogs,10,1465,model,elmo,pre-train,deep bidirection representations,elmo pre-train deep bidirection representations,0.6647058725357056
Blogs,10,1465,model,intermediate layer representations,in,bilstm,intermediate layer representations in bilstm,0.46826910972595215
Blogs,10,1465,model,intermediate layer representations,where,combination weights,intermediate layer representations where combination weights,0.5927476286888123
Blogs,10,1465,model,combination weights,optimized on,task -specific training data.bert,combination weights optimized on task -specific training data.bert,0.7413841485977173
Blogs,10,1465,model,deep bidirection representations,by jointly conditioning on,left and right context,deep bidirection representations by jointly conditioning on left and right context,0.6808637380599976
Blogs,10,1465,model,left and right context,in,all layers,left and right context in all layers,0.5184316039085388
Blogs,11,153,ablation-analysis,char-level and word- level embeddings,contribute towards,model 's performance,char-level and word- level embeddings contribute towards model 's performance,0.603459358215332
Blogs,11,153,ablation-analysis,ablation analysis,has,char-level and word- level embeddings,ablation analysis has char-level and word- level embeddings,0.5635137557983398
Blogs,11,157,ablation-analysis,c2q attention,proves to be,critical,c2q attention proves to be critical,0.677102267742157
Blogs,11,157,ablation-analysis,critical,with,drop,critical with drop,0.7213087677955627
Blogs,11,157,ablation-analysis,drop,of,more than 10 points,drop of more than 10 points,0.6230027079582214
Blogs,11,157,ablation-analysis,ablation analysis,has,c2q attention,ablation analysis has c2q attention,0.5731401443481445
Blogs,11,49,experimental-setup,pre-trained word vectors,to obtain,fixed word embedding,pre-trained word vectors to obtain fixed word embedding,0.5819923281669617
Blogs,11,49,experimental-setup,fixed word embedding,of,each word,fixed word embedding of each word,0.6024660468101501
Blogs,11,49,experimental-setup,pre-trained word vectors,has,glove,pre-trained word vectors has glove,0.5855448842048645
Blogs,11,49,experimental-setup,experimental setup,use,pre-trained word vectors,experimental setup use pre-trained word vectors,0.5400256514549255
Blogs,11,139,experimental-setup,100 1d filters,for,cnn char embedding,100 1d filters for cnn char embedding,0.5603349804878235
Blogs,11,139,experimental-setup,100 1d filters,with,width,100 1d filters with width,0.6448174118995667
Blogs,11,139,experimental-setup,width,of,5,width of 5,0.7088584303855896
Blogs,11,139,experimental-setup,experimental setup,use,100 1d filters,experimental setup use 100 1d filters,0.6027165651321411
Blogs,11,140,experimental-setup,hidden state size ( d ),of,model,hidden state size ( d ) of model,0.5768765807151794
Blogs,11,140,experimental-setup,model,is,100,model is 100,0.6629937887191772
Blogs,11,140,experimental-setup,experimental setup,has,hidden state size ( d ),experimental setup has hidden state size ( d ),0.5212352275848389
Blogs,11,141,experimental-setup,model,has,2.6 million parameters,model has 2.6 million parameters,0.5225839018821716
Blogs,11,141,experimental-setup,model,about,2.6 million parameters,model about 2.6 million parameters,0.6041601896286011
Blogs,11,141,experimental-setup,experimental setup,has,model,experimental setup has model,0.5338840484619141
Blogs,11,142,experimental-setup,"adadelta ( zeiler , 2012 ) optimizer",with,minibatch size,"adadelta ( zeiler , 2012 ) optimizer with minibatch size",0.5757989287376404
Blogs,11,142,experimental-setup,"adadelta ( zeiler , 2012 ) optimizer",with,initial learning rate,"adadelta ( zeiler , 2012 ) optimizer with initial learning rate",0.5986990332603455
Blogs,11,142,experimental-setup,"adadelta ( zeiler , 2012 ) optimizer",with,12 epochs,"adadelta ( zeiler , 2012 ) optimizer with 12 epochs",0.6043483018875122
Blogs,11,142,experimental-setup,minibatch size,of,60,minibatch size of 60,0.6419041156768799
Blogs,11,142,experimental-setup,initial learning rate,of,0.5,initial learning rate of 0.5,0.6004254221916199
Blogs,11,142,experimental-setup,initial learning rate,for,12 epochs,initial learning rate for 12 epochs,0.6030810475349426
Blogs,11,142,experimental-setup,experimental setup,use,"adadelta ( zeiler , 2012 ) optimizer","experimental setup use adadelta ( zeiler , 2012 ) optimizer",0.5940023064613342
Blogs,11,143,experimental-setup,"dropout ( srivastava et al. , 2014 ) rate",of,0.2,"dropout ( srivastava et al. , 2014 ) rate of 0.2",0.5728970170021057
Blogs,11,143,experimental-setup,0.2,used for,cnn,0.2 used for cnn,0.6476068496704102
Blogs,11,143,experimental-setup,0.2,used for,all lstm layers,0.2 used for all lstm layers,0.6052280068397522
Blogs,11,143,experimental-setup,0.2,used for,linear transformation,0.2 used for linear transformation,0.6118425130844116
Blogs,11,143,experimental-setup,linear transformation,before,softmax,linear transformation before softmax,0.6385871171951294
Blogs,11,143,experimental-setup,softmax,for,answers,softmax for answers,0.6663388013839722
Blogs,11,143,experimental-setup,experimental setup,has,"dropout ( srivastava et al. , 2014 ) rate","experimental setup has dropout ( srivastava et al. , 2014 ) rate",0.5079212188720703
Blogs,11,144,experimental-setup,moving averages,of,all weights,moving averages of all weights,0.5905746817588806
Blogs,11,144,experimental-setup,all weights,of,model,all weights of model,0.6298975348472595
Blogs,11,144,experimental-setup,all weights,maintained with,exponential decay rate,all weights maintained with exponential decay rate,0.6802895665168762
Blogs,11,144,experimental-setup,exponential decay rate,of,0.999,exponential decay rate of 0.999,0.5577678084373474
Blogs,11,144,experimental-setup,training,has,moving averages,training has moving averages,0.6035670042037964
Blogs,11,144,experimental-setup,experimental setup,During,training,experimental setup During training,0.6835477948188782
Blogs,11,145,experimental-setup,moving averages,instead of,raw weights,moving averages instead of raw weights,0.6310737729072571
Blogs,11,145,experimental-setup,test time,has,moving averages,test time has moving averages,0.5992967486381531
Blogs,11,145,experimental-setup,experimental setup,At,test time,experimental setup At test time,0.5272576808929443
Blogs,11,146,experimental-setup,training process,takes,roughly 20 hours,training process takes roughly 20 hours,0.6424494385719299
Blogs,11,146,experimental-setup,roughly 20 hours,on,single titan x gpu,roughly 20 hours on single titan x gpu,0.5080558657646179
Blogs,11,146,experimental-setup,experimental setup,has,training process,experimental setup has training process,0.5342507362365723
Blogs,11,223,experimental-setup,training process,takes,roughly 60 hours,training process takes roughly 60 hours,0.638382077217102
Blogs,11,223,experimental-setup,roughly 60 hours,on,eight titan x gpus,roughly 60 hours on eight titan x gpus,0.5004115700721741
Blogs,11,265,experiments,dailymail test,has,our single - run model,dailymail test has our single - run model,0.578057587146759
Blogs,11,265,experiments,our single - run model,has,outperforms,our single - run model has outperforms,0.6148750185966492
Blogs,11,265,experiments,outperforms,has,best ensemble method,outperforms has best ensemble method,0.6039777398109436
Blogs,11,293,experiments,allenai.github.io/,has,bi-att-flow,allenai.github.io/ has bi-att-flow,0.6135090589523315
Blogs,11,7,model,multi-stage hierarchical process,represents,context,multi-stage hierarchical process represents context,0.6362245082855225
Blogs,11,7,model,context,at,different levels of granularity,context at different levels of granularity,0.5547928810119629
Blogs,11,7,model,bidirectional attention flow mechanism,to obtain,query - aware context representation,bidirectional attention flow mechanism to obtain query - aware context representation,0.5399522185325623
Blogs,11,7,model,query - aware context representation,without,early summarization,query - aware context representation without early summarization,0.6680271625518799
Blogs,11,7,model,bi-directional attention flow ( bidaf ) network,has,multi-stage hierarchical process,bi-directional attention flow ( bidaf ) network has multi-stage hierarchical process,0.5351670384407043
Blogs,11,7,model,model,introduce,bi-directional attention flow ( bidaf ) network,model introduce bi-directional attention flow ( bidaf ) network,0.6423549056053162
Blogs,11,17,model,hierarchical multi-stage architecture,for modeling,representations,hierarchical multi-stage architecture for modeling representations,0.7167647480964661
Blogs,11,17,model,representations,of,context paragraph,representations of context paragraph,0.5634952783584595
Blogs,11,17,model,representations,at,different levels of granularity,representations at different levels of granularity,0.5780228972434998
Blogs,11,17,model,bi-directional attention flow ( bidaf ) network,has,hierarchical multi-stage architecture,bi-directional attention flow ( bidaf ) network has hierarchical multi-stage architecture,0.5521481037139893
Blogs,11,17,model,model,introduce,bi-directional attention flow ( bidaf ) network,model introduce bi-directional attention flow ( bidaf ) network,0.6423549056053162
Blogs,11,18,model,bidaf,includes,"character - level , word-level , and contextual embeddings","bidaf includes character - level , word-level , and contextual embeddings",0.6179031729698181
Blogs,11,18,model,bidaf,uses,bi-directional attention flow,bidaf uses bi-directional attention flow,0.5782430171966553
Blogs,11,18,model,bi-directional attention flow,to obtain,query - aware context representation,bi-directional attention flow to obtain query - aware context representation,0.5268716812133789
Blogs,11,18,model,model,has,bidaf,model has bidaf,0.6748911142349243
Blogs,11,19,model,attention mechanism,offers,following improvements,attention mechanism offers following improvements,0.7123911380767822
Blogs,11,19,model,following improvements,to,previously popular attention paradigms,following improvements to previously popular attention paradigms,0.5378788709640503
Blogs,11,19,model,model,has,attention mechanism,model has attention mechanism,0.5215663909912109
Blogs,11,23,model,model,use,memory -less attention mechanism,model use memory -less attention mechanism,0.6423578262329102
Blogs,11,27,model,attention,at,each time step,attention at each time step,0.6140859127044678
Blogs,11,27,model,each time step,to be,unaffected,each time step to be unaffected,0.6368986368179321
Blogs,11,27,model,unaffected,from,incorrect attendances,unaffected from incorrect attendances,0.6002210378646851
Blogs,11,27,model,incorrect attendances,at,previous time steps,incorrect attendances at previous time steps,0.5271971821784973
Blogs,11,27,model,model,allows,attention,model allows attention,0.7328705787658691
Blogs,11,29,model,attention mechanisms,in,both directions,attention mechanisms in both directions,0.4614515006542206
Blogs,11,29,model,both directions,which provide,complimentary information,both directions which provide complimentary information,0.7408615946769714
Blogs,11,29,model,complimentary information,to,each other,complimentary information to each other,0.6023620963096619
Blogs,11,38,model,attention flow layer,couples,query and context vectors,attention flow layer couples query and context vectors,0.7168657779693604
Blogs,11,38,model,attention flow layer,produces,set of queryaware feature vectors,attention flow layer produces set of queryaware feature vectors,0.6082838773727417
Blogs,11,38,model,set of queryaware feature vectors,for,each word,set of queryaware feature vectors for each word,0.6044611930847168
Blogs,11,38,model,each word,in,context,each word in context,0.49463269114494324
Blogs,11,38,model,model,has,attention flow layer,model has attention flow layer,0.5458298921585083
Blogs,11,59,model,attention flow layer,not used to summarize,query and context,attention flow layer not used to summarize query and context,0.6930738687515259
Blogs,11,59,model,query and context,into,single feature vectors,query and context into single feature vectors,0.5865672826766968
Blogs,11,60,model,attention vector,at,each time step,attention vector at each time step,0.5419794917106628
Blogs,11,60,model,flow through,to,subsequent modeling layer,flow through to subsequent modeling layer,0.6021007299423218
Blogs,11,60,model,model,has,attention vector,model has attention vector,0.5330941677093506
Blogs,11,138,model,each paragraph and question,tokenized by,regular-expression - based word tokenizer ( ptb tokenizer ),each paragraph and question tokenized by regular-expression - based word tokenizer ( ptb tokenizer ),0.8363651037216187
Blogs,11,138,model,each paragraph and question,fed into,model,each paragraph and question fed into model,0.7277486324310303
Blogs,11,138,model,model,has,each paragraph and question,model has each paragraph and question,0.582953929901123
Blogs,11,28,results,our experiments,show,memory -less attention,our experiments show memory -less attention,0.6118946075439453
Blogs,11,28,results,memory -less attention,gives,clear advantage,memory -less attention gives clear advantage,0.580925703048706
Blogs,11,28,results,clear advantage,over,dynamic attention,clear advantage over dynamic attention,0.6771841645240784
Blogs,11,30,results,outperforms,on,highly -competitive,outperforms on highly -competitive,0.5560272336006165
Blogs,11,30,results,all previous approaches,on,highly -competitive,all previous approaches on highly -competitive,0.5052604079246521
Blogs,11,30,results,bidaf model,has,outperforms,bidaf model has outperforms,0.656779408454895
Blogs,11,30,results,outperforms,has,all previous approaches,outperforms has all previous approaches,0.5911725759506226
Blogs,11,30,results,highly -competitive,has,stanford question answering dataset ( squad ) test set,highly -competitive has stanford question answering dataset ( squad ) test set,0.5327639579772949
Blogs,11,30,results,results,has,bidaf model,results has bidaf model,0.5539949536323547
Blogs,11,31,results,modification,to,only the output layer,modification to only the output layer,0.586010754108429
Blogs,11,31,results,bidaf,achieves,state - of- the - art results,bidaf achieves state - of- the - art results,0.6605641841888428
Blogs,11,31,results,state - of- the - art results,on,cnn / dailymail cloze test,state - of- the - art results on cnn / dailymail cloze test,0.48622047901153564
Blogs,11,31,results,modification,has,bidaf,modification has bidaf,0.65267014503479
Blogs,11,31,results,only the output layer,has,bidaf,only the output layer has bidaf,0.6465727090835571
Blogs,11,31,results,results,With,modification,results With modification,0.6065728068351746
Blogs,11,150,results,bidaf ( ensemble ),achieves,em score,bidaf ( ensemble ) achieves em score,0.666892409324646
Blogs,11,150,results,bidaf ( ensemble ),achieves,f1 score,bidaf ( ensemble ) achieves f1 score,0.6688649654388428
Blogs,11,150,results,bidaf ( ensemble ),achieves,outperforming,bidaf ( ensemble ) achieves outperforming,0.7018903493881226
Blogs,11,150,results,em score,of,73.3,em score of 73.3,0.524579644203186
Blogs,11,150,results,f1 score,of,81.1,f1 score of 81.1,0.5322656631469727
Blogs,11,150,results,outperforming,has,all previous approaches,outperforming has all previous approaches,0.6052617430686951
Blogs,11,161,results,dynamically computed attention,by,more than 3 points,dynamically computed attention by more than 3 points,0.566119372844696
Blogs,11,161,results,proposed static attention,has,outperforms,proposed static attention has outperforms,0.6316153407096863
Blogs,11,161,results,outperforms,has,dynamically computed attention,outperforms has dynamically computed attention,0.5845584273338318
Blogs,11,264,results,previous single - run models,on,both datasets,previous single - run models on both datasets,0.4957588016986847
Blogs,11,264,results,both datasets,for,val and test data,both datasets for val and test data,0.6166641116142273
Blogs,11,264,results,bidaf,has,outperforms,bidaf has outperforms,0.6620983481407166
Blogs,11,264,results,outperforms,has,previous single - run models,outperforms has previous single - run models,0.5747308731079102
Blogs,11,264,results,results,has,bidaf,results has bidaf,0.5638124346733093
Blogs,12,5,model,low-dimensional embeddings,of,words and knowledge base constituents,low-dimensional embeddings of words and knowledge base constituents,0.5466876029968262
Blogs,12,5,model,natural language questions,against,candidate answers,natural language questions against candidate answers,0.6105466485023499
Blogs,12,5,model,model,learns,low-dimensional embeddings,model learns low-dimensional embeddings,0.7073069214820862
Blogs,12,136,results,c 2,by,c 1,c 2 by c 1,0.6272550225257874
Blogs,12,136,results,c 2,induces,large drop,c 2 induces large drop,0.767754077911377
Blogs,12,136,results,large drop,in,performance,large drop in performance,0.5769874453544617
Blogs,12,136,results,results,Replacing,c 2,results Replacing c 2,0.61207115650177
Blogs,12,137,results,all 2 - hops connections,as,candidate set,all 2 - hops connections as candidate set,0.5309732556343079
Blogs,12,137,results,all 2 - hops connections,is,detrimental,all 2 - hops connections is detrimental,0.603380024433136
Blogs,12,137,results,results,using,all 2 - hops connections,results using all 2 - hops connections,0.630066454410553
Blogs,12,138,results,hypothesis,that,richer representation,hypothesis that richer representation,0.6800903677940369
Blogs,12,138,results,richer representation,for,answers,richer representation for answers,0.6512700319290161
Blogs,12,138,results,richer representation,store,more pertinent information,richer representation store more pertinent information,0.7510485053062439
Blogs,12,138,results,results,verify,hypothesis,results verify hypothesis,0.5772077441215515
Blogs,12,139,results,greatly improve,upon,model,greatly improve upon model,0.6218125820159912
Blogs,12,139,results,greatly improve,corresponds to,setting,greatly improve corresponds to setting,0.7233827114105225
Blogs,12,139,results,setting,with,path representation,setting with path representation,0.6564592123031616
Blogs,12,139,results,setting,with,c 1,setting with c 1,0.6573624014854431
Blogs,12,145,results,ensemble,has,improves,ensemble has improves,0.6209949254989624
Blogs,12,145,results,improves,has,state- of - the-art,improves has state- of - the-art,0.5516366362571716
Blogs,12,145,results,results,has,ensemble,results has ensemble,0.5487152338027954
Blogs,13,48,experimental-setup,code,implemented in,torch,code implemented in torch,0.7889990210533142
Blogs,13,48,experimental-setup,experimental setup,has,code,experimental setup has code,0.5089583992958069
Blogs,13,49,experimental-setup,training,takes,about 10 hours,training takes about 10 hours,0.6437757611274719
Blogs,13,49,experimental-setup,about 10 hours,on,single gpu nvidia titan black,about 10 hours on single gpu nvidia titan black,0.5018359422683716
Blogs,13,49,experimental-setup,experimental setup,has,training,experimental setup has training,0.5312813520431519
Blogs,13,104,experiments,visual question answering,on,coco dataset,visual question answering on coco dataset,0.4907759428024292
Blogs,13,104,experiments,visual question answering,our implementation of,simple baseline,visual question answering our implementation of simple baseline,0.6879193782806396
Blogs,13,104,experiments,simple baseline,achieves,comparable performance,simple baseline achieves comparable performance,0.7001023292541504
Blogs,13,104,experiments,comparable performance,to,several recently proposed recurrent neural network - based approaches,comparable performance to several recently proposed recurrent neural network - based approaches,0.5563924312591553
Blogs,13,5,model,word features,from,question,word features from question,0.5584232211112976
Blogs,13,5,model,cnn features,from,image,cnn features from image,0.5163580775260925
Blogs,13,5,model,image,to predict,answer,image to predict answer,0.790460467338562
Blogs,14,138,ablation-analysis,features,contribute to,performance,features contribute to performance,0.6973457336425781
Blogs,14,138,ablation-analysis,performance,of,our final system,performance of our final system,0.5824689269065857
Blogs,14,27,baselines,module,using,tf - idf matching,module using tf - idf matching,0.6901460289955139
Blogs,14,27,baselines,model,trained to detect,answer spans,model trained to detect answer spans,0.7534635066986084
Blogs,14,27,baselines,answer spans,in,few returned documents,answer spans in few returned documents,0.5060937404632568
Blogs,14,27,baselines,document retriever,has,module,document retriever has module,0.5936124324798584
Blogs,14,61,baselines,best performing system,uses,bigram counts,best performing system uses bigram counts,0.6622776389122009
Blogs,14,61,baselines,bigram counts,while preserving,speed and memory efficiency,bigram counts while preserving speed and memory efficiency,0.7013292908668518
Blogs,14,61,baselines,speed and memory efficiency,by using,"hashing of ( weinberger et al. , 2009 )","speed and memory efficiency by using hashing of ( weinberger et al. , 2009 )",0.666500985622406
Blogs,14,61,baselines,"hashing of ( weinberger et al. , 2009 )",to map,bigrams,"hashing of ( weinberger et al. , 2009 ) to map bigrams",0.6764851808547974
Blogs,14,61,baselines,bigrams,to,2 24 bins,bigrams to 2 24 bins,0.5743897557258606
Blogs,14,61,baselines,bigrams,with,unsigned murmur3 hash,bigrams with unsigned murmur3 hash,0.6597990989685059
Blogs,14,61,baselines,baselines,has,best performing system,baselines has best performing system,0.5688688158988953
Blogs,14,144,baselines,three versions of drqa,evaluate,impact,three versions of drqa evaluate impact,0.6574262380599976
Blogs,14,144,baselines,impact,of using,distant supervision,impact of using distant supervision,0.6786856055259705
Blogs,14,144,baselines,sources,provided to,document reader,sources provided to document reader,0.7028728127479553
Blogs,14,144,baselines,single document reader model,trained on,squad training set,single document reader model trained on squad training set,0.7167181372642517
Blogs,14,144,baselines,single document reader model,used on,all evaluation sets,single document reader model used on all evaluation sets,0.6685177683830261
Blogs,14,144,baselines,squad,has,single document reader model,squad has single document reader model,0.5744699239730835
Blogs,14,145,baselines,document reader model,pre-trained on,squad,document reader model pre-trained on squad,0.7244415879249573
Blogs,14,145,baselines,document reader model,fine-tuned for,each dataset independently,document reader model fine-tuned for each dataset independently,0.7221560478210449
Blogs,14,145,baselines,each dataset independently,using,distant supervision ( ds ) training set,each dataset independently using distant supervision ( ds ) training set,0.6472519040107727
Blogs,14,145,baselines,fine-tune ( ds ),has,document reader model,fine-tune ( ds ) has document reader model,0.5642406344413757
Blogs,14,145,baselines,baselines,has,fine-tune ( ds ),baselines has fine-tune ( ds ),0.5649312734603882
Blogs,14,146,baselines,single document reader model,jointly trained on,squad training set,single document reader model jointly trained on squad training set,0.7621451616287231
Blogs,14,146,baselines,single document reader model,jointly trained on,all the ds sources,single document reader model jointly trained on all the ds sources,0.724902331829071
Blogs,14,146,baselines,multitask ( ds ),has,single document reader model,multitask ( ds ) has single document reader model,0.567093551158905
Blogs,14,146,baselines,baselines,has,multitask ( ds ),baselines has multitask ( ds ),0.5655684471130371
Blogs,14,147,baselines,full wikipedia setting,use,streamlined model,full wikipedia setting use streamlined model,0.6141484975814819
Blogs,14,147,baselines,streamlined model,that does not use,corenlp parsed f token features,streamlined model that does not use corenlp parsed f token features,0.6813682913780212
Blogs,14,147,baselines,streamlined model,that does not use,lemmas,streamlined model that does not use lemmas,0.6853023171424866
Blogs,14,147,baselines,lemmas,for,f exact match,lemmas for f exact match,0.5951289534568787
Blogs,14,128,experimental-setup,3 - layer bidirectional lstms,with,h = 128 hidden units,3 - layer bidirectional lstms with h = 128 hidden units,0.5593356490135193
Blogs,14,128,experimental-setup,h = 128 hidden units,for,paragraph and question encoding,h = 128 hidden units for paragraph and question encoding,0.5685473084449768
Blogs,14,128,experimental-setup,h = 128 hidden units,both,paragraph and question encoding,h = 128 hidden units both paragraph and question encoding,0.6317239999771118
Blogs,14,128,experimental-setup,experimental setup,use,3 - layer bidirectional lstms,experimental setup use 3 - layer bidirectional lstms,0.5769892930984497
Blogs,14,129,experimental-setup,stanford corenlp toolkit,for,tokenization,stanford corenlp toolkit for tokenization,0.5797368288040161
Blogs,14,129,experimental-setup,stanford corenlp toolkit,generating,partof-speech,stanford corenlp toolkit generating partof-speech,0.623599112033844
Blogs,14,129,experimental-setup,stanford corenlp toolkit,generating,named entity tags,stanford corenlp toolkit generating named entity tags,0.5976753234863281
Blogs,14,129,experimental-setup,experimental setup,apply,stanford corenlp toolkit,experimental setup apply stanford corenlp toolkit,0.5846158266067505
Blogs,14,130,experimental-setup,training examples,sorted by,length,training examples sorted by length,0.6869457960128784
Blogs,14,130,experimental-setup,training examples,divided into,minibatches,training examples divided into minibatches,0.5956485867500305
Blogs,14,130,experimental-setup,length,of,paragraph,length of paragraph,0.5935345888137817
Blogs,14,130,experimental-setup,minibatches,of,32 examples each,minibatches of 32 examples each,0.5371328592300415
Blogs,14,130,experimental-setup,experimental setup,has,training examples,experimental setup has training examples,0.5167377591133118
Blogs,14,131,experimental-setup,adamax,for,optimization,adamax for optimization,0.6393342614173889
Blogs,14,132,experimental-setup,dropout,with,p = 0.3,dropout with p = 0.3,0.6808434128761292
Blogs,14,132,experimental-setup,p = 0.3,applied to,word embeddings,p = 0.3 applied to word embeddings,0.6542688012123108
Blogs,14,132,experimental-setup,p = 0.3,applied to,all the hidden units,p = 0.3 applied to all the hidden units,0.6821873188018799
Blogs,14,132,experimental-setup,all the hidden units,of,lstms,all the hidden units of lstms,0.5534578561782837
Blogs,14,132,experimental-setup,experimental setup,has,dropout,experimental setup has dropout,0.5067690014839172
Blogs,14,6,model,search component,based on,bigram hashing,search component based on bigram hashing,0.6402387022972107
Blogs,14,6,model,search component,based on,tf - idf matching,search component based on tf - idf matching,0.6382916569709778
Blogs,14,6,model,tf - idf matching,with,multi-layer recurrent neural network model,tf - idf matching with multi-layer recurrent neural network model,0.5834466814994812
Blogs,14,6,model,multi-layer recurrent neural network model,trained to detect,answers,multi-layer recurrent neural network model trained to detect answers,0.694253146648407
Blogs,14,6,model,answers,in,wikipedia paragraphs,answers in wikipedia paragraphs,0.512421190738678
Blogs,14,26,model,mrs,requiring,open-domain system,mrs requiring open-domain system,0.7132119536399841
Blogs,14,26,model,evaluate,has,mrs,evaluate has mrs,0.6013189554214478
Blogs,14,26,model,model,show,multiple existing qa datasets,model show multiple existing qa datasets,0.663821816444397
Blogs,14,60,results,our system,by taking,local word order,our system by taking local word order,0.5651758909225464
Blogs,14,60,results,local word order,into,account,local word order into account,0.5797669291496277
Blogs,14,60,results,local word order,with,n-gram features,local word order with n-gram features,0.6288476586341858
Blogs,14,60,results,account,with,n-gram features,account with n-gram features,0.6583914160728455
Blogs,14,60,results,results,improve,our system,results improve our system,0.6569348573684692
Blogs,14,124,results,all datasets,indicate,our simple approach,all datasets indicate our simple approach,0.5844476819038391
Blogs,14,124,results,outperforms,especially with,bigram hashing,outperforms especially with bigram hashing,0.6461012959480286
Blogs,14,124,results,wikipedia search,especially with,bigram hashing,wikipedia search especially with bigram hashing,0.5486482381820679
Blogs,14,124,results,our simple approach,has,outperforms,our simple approach has outperforms,0.6350935697555542
Blogs,14,124,results,outperforms,has,wikipedia search,outperforms has wikipedia search,0.6095978617668152
Blogs,14,135,results,our system ( single model ),achieve,70.0 % exact match,our system ( single model ) achieve 70.0 % exact match,0.6117669939994812
Blogs,14,135,results,our system ( single model ),achieve,79.0 % f1 scores,our system ( single model ) achieve 79.0 % f1 scores,0.5866430997848511
Blogs,14,135,results,our system ( single model ),on,test set,our system ( single model ) on test set,0.5357226133346558
Blogs,14,135,results,our system ( single model ),match,top performance,our system ( single model ) match top performance,0.7439539432525635
Blogs,14,135,results,79.0 % f1 scores,on,test set,79.0 % f1 scores on test set,0.5190798044204712
Blogs,14,135,results,results,has,our system ( single model ),results has our system ( single model ),0.5580957531929016
Blogs,14,139,results,our system,able to achieve,f1,our system able to achieve f1,0.6300980448722839
Blogs,14,139,results,f1,over,77 %,f1 over 77 %,0.6962659358978271
Blogs,14,139,results,aligned question embedding feature,has,our system,aligned question embedding feature has our system,0.5671501755714417
Blogs,14,139,results,few manual features,has,our system,few manual features has our system,0.5710113048553467
Blogs,14,139,results,results,Without,aligned question embedding feature,results Without aligned question embedding feature,0.6873493790626526
Blogs,14,154,results,drqa,provides,reasonable performance,drqa provides reasonable performance,0.6539055109024048
Blogs,14,156,results,single model,trained only on,squad,single model trained only on squad,0.7576228380203247
Blogs,14,156,results,outperformed,on,all four of the datasets,outperformed on all four of the datasets,0.538938581943512
Blogs,14,156,results,outperformed,by,multitask model,outperformed by multitask model,0.6071705222129822
Blogs,14,156,results,multitask model,that uses,distant supervision,multitask model that uses distant supervision,0.5522639155387878
Blogs,14,156,results,results,has,single model,results has single model,0.5429759621620178
Blogs,14,157,results,performance,training on,squad alone,performance training on squad alone,0.7552482485771179
Blogs,14,157,results,squad alone,is,not far behind,squad alone is not far behind,0.6059628129005432
Blogs,14,157,results,results,has,performance,results has performance,0.5972660779953003
Blogs,14,158,results,improvement,from,squad to multitask ( ds ),improvement from squad to multitask ( ds ),0.5667929649353027
Blogs,14,158,results,improvement,not from,task transfer,improvement not from task transfer,0.5956947207450867
Blogs,14,158,results,squad to multitask ( ds ),not from,task transfer,squad to multitask ( ds ) not from task transfer,0.6459498405456543
Blogs,14,158,results,each dataset,using,ds,each dataset using ds,0.6741527915000916
Blogs,14,158,results,ds,gives,improvements,ds gives improvements,0.6729122400283813
Blogs,14,158,results,results,majority of,improvement,results majority of improvement,0.7366255521774292
Blogs,14,160,results,unconstrained qa system,using,redundant resources,unconstrained qa system using redundant resources,0.6952860355377197
Blogs,14,160,results,results,compare to,unconstrained qa system,results compare to unconstrained qa system,0.6645339727401733
Blogs,14,161,results,our performance,is,not too far behind,our performance is not too far behind,0.5976038575172424
Blogs,14,161,results,not too far behind,on,curatedtrec,not too far behind on curatedtrec,0.6481958031654358
Blogs,15,216,hyperparameters,memnns,fixed,embedding dimension,memnns fixed embedding dimension,0.6932808756828308
Blogs,15,216,hyperparameters,memnns,fixed,learning rate,memnns fixed learning rate,0.7027561664581299
Blogs,15,216,hyperparameters,memnns,fixed,margin ?,memnns fixed margin ?,0.7543589472770691
Blogs,15,216,hyperparameters,embedding dimension,to,100,embedding dimension to 100,0.6331753134727478
Blogs,15,216,hyperparameters,learning rate,to,0.01,learning rate to 0.01,0.5674041509628296
Blogs,15,216,hyperparameters,margin ?,to,0.1 and 10 epochs,margin ? to 0.1 and 10 epochs,0.5870730876922607
Blogs,15,216,hyperparameters,0.1 and 10 epochs,of,training,0.1 and 10 epochs of training,0.6008569598197937
Blogs,15,216,hyperparameters,hyperparameters,For,memnns,hyperparameters For memnns,0.5390564203262329
Blogs,15,3,model,learning models,called,memory networks,learning models called memory networks,0.5693320631980896
Blogs,15,17,model,rectify,has,problem,rectify has problem,0.6065871715545654
Blogs,15,17,model,model,called,memory networks,model called memory networks,0.6016121506690979
Blogs,15,352,model,memnn rnn and lstms,effectively fed,output,memnn rnn and lstms effectively fed output,0.712501049041748
Blogs,15,352,model,output,of,o module,output of o module,0.6421160697937012
Blogs,15,352,model,model,has,memnn rnn and lstms,model has memnn rnn and lstms,0.5625975728034973
Blogs,15,249,results,lstms,better than,rnns,lstms better than rnns,0.679455041885376
Blogs,15,249,results,results,has,lstms,results has lstms,0.5372941493988037
Blogs,15,253,results,memnns,in,multi-word answer setting,memnns in multi-word answer setting,0.5010080337524414
Blogs,15,253,results,results,tested,memnns,results tested memnns,0.6497493982315063
Blogs,15,360,results,memnns,using,rnns,memnns using rnns,0.683849573135376
Blogs,15,360,results,outperforming,has,memnns,outperforming has memnns,0.5441964268684387
