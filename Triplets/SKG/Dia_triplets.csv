topic,paper_ID,sentence_ID,info-unit,sub,pred,obj,triplets,pred_weights
translation,0,108,ablation-analysis,model,with,only the second stage generation ( ours - first ),model with only the second stage generation ( ours - first ),0.6393296718597412
translation,0,108,ablation-analysis,model,with,model,model with model,0.6442975997924805
translation,0,108,ablation-analysis,model,with,without the discriminator ( ours - disc ),model with without the discriminator ( ours - disc ),0.6593663692474365
translation,0,108,ablation-analysis,model,with,multireference bow loss ( ours - mbow ),model with multireference bow loss ( ours - mbow ),0.6036054491996765
translation,0,108,ablation-analysis,model,with,gmm prior networks ( ours + gmp ),model with gmm prior networks ( ours + gmp ),0.6266167759895325
translation,0,108,ablation-analysis,model,with,gmm prior networks ( ours + gmp ),model with gmm prior networks ( ours + gmp ),0.6266167759895325
translation,0,108,ablation-analysis,model,with,gmm prior networks ( ours + gmp ),model with gmm prior networks ( ours + gmp ),0.6266167759895325
translation,0,108,ablation-analysis,model,has,without the discriminator ( ours - disc ),model has without the discriminator ( ours - disc ),0.5916255116462708
translation,0,108,ablation-analysis,ablation analysis,with,model,ablation analysis with model,0.5867980718612671
translation,0,108,ablation-analysis,ablation analysis,with,model,ablation analysis with model,0.5867980718612671
translation,0,108,ablation-analysis,ablation analysis,with,gmm prior networks ( ours + gmp ),ablation analysis with gmm prior networks ( ours + gmp ),0.6155651211738586
translation,0,167,ablation-analysis,discriminator,facilitates extracting,common feature,discriminator facilitates extracting common feature,0.7499061822891235
translation,0,167,ablation-analysis,discriminator,yields,more relevant responses,discriminator yields more relevant responses,0.6810777187347412
translation,0,167,ablation-analysis,more relevant responses,to,input query,more relevant responses to input query,0.571918785572052
translation,0,167,ablation-analysis,ablation analysis,has,discriminator,ablation analysis has discriminator,0.5136631727218628
translation,0,103,baselines,vanilla sequence - to-sequence model,with,attention mechanism,vanilla sequence - to-sequence model with attention mechanism,0.6271110773086548
translation,0,103,baselines,attention mechanism,where,standard beam search,attention mechanism where standard beam search,0.5979989767074585
translation,0,103,baselines,standard beam search,applied in,testing,standard beam search applied in testing,0.6831748485565186
translation,0,103,baselines,testing,to generate,multiple different responses,testing to generate multiple different responses,0.7062130570411682
translation,0,103,baselines,s2s,has,vanilla sequence - to-sequence model,s2s has vanilla sequence - to-sequence model,0.5623412728309631
translation,0,104,baselines,vanilla sequence - to-sequence model,with,modified diversity - promoting beam search method,vanilla sequence - to-sequence model with modified diversity - promoting beam search method,0.6042215824127197
translation,0,105,baselines,modified multiple responding mechanisms enhanced dialogue model,introduces,responding mechanism embeddings,modified multiple responding mechanisms enhanced dialogue model introduces responding mechanism embeddings,0.6217516660690308
translation,0,105,baselines,responding mechanism embeddings,for,diverse response generation,responding mechanism embeddings for diverse response generation,0.6164748072624207
translation,0,105,baselines,mms,has,modified multiple responding mechanisms enhanced dialogue model,mms has modified multiple responding mechanisms enhanced dialogue model,0.581436038017273
translation,0,105,baselines,baselines,has,mms,baselines has mms,0.6099420189857483
translation,0,106,baselines,vanilla cvae model,with and without,bow ( bag - of- word ) loss,vanilla cvae model with and without bow ( bag - of- word ) loss,0.6472124457359314
translation,0,106,baselines,cvae,has,vanilla cvae model,cvae has vanilla cvae model,0.5754662752151489
translation,0,106,baselines,bow ( bag - of- word ) loss,has,cvae + bow and cvae,bow ( bag - of- word ) loss has cvae + bow and cvae,0.571053683757782
translation,0,106,baselines,baselines,has,cvae,baselines has cvae,0.5393452048301697
translation,0,107,baselines,conditional wasserstein autoencoder model,for,dialogue generation,conditional wasserstein autoencoder model for dialogue generation,0.5690383911132812
translation,0,107,baselines,conditional wasserstein autoencoder model,models,distribution of data,conditional wasserstein autoencoder model models distribution of data,0.7453198432922363
translation,0,107,baselines,distribution of data,by training,gan,distribution of data by training gan,0.7873658537864685
translation,0,107,baselines,gan,within,latent variable space,gan within latent variable space,0.6552595496177673
translation,0,107,baselines,wae,has,conditional wasserstein autoencoder model,wae has conditional wasserstein autoencoder model,0.5318920016288757
translation,0,107,baselines,baselines,has,wae,baselines has wae,0.6365041136741638
translation,0,144,experimental-setup,encoder and decoder,set to,one layer,encoder and decoder set to one layer,0.6811297535896301
translation,0,144,experimental-setup,one layer,with,gru cells,one layer with gru cells,0.6726019978523254
translation,0,144,experimental-setup,gru cells,where,hidden state size,gru cells where hidden state size,0.6490774154663086
translation,0,144,experimental-setup,hidden state size,of,gru,hidden state size of gru,0.6042205095291138
translation,0,144,experimental-setup,gru,is,256,gru is 256,0.6436044573783875
translation,0,144,experimental-setup,gru,is,256,gru is 256,0.6436044573783875
translation,0,144,experimental-setup,gru,is,256,gru is 256,0.6436044573783875
translation,0,144,experimental-setup,gru,is,256,gru is 256,0.6436044573783875
translation,0,144,experimental-setup,utterance length,limited to,50,utterance length limited to 50,0.6714082360267639
translation,0,144,experimental-setup,vocabulary size,is,"50,000","vocabulary size is 50,000",0.5994434952735901
translation,0,144,experimental-setup,word embedding dimension,is,256,word embedding dimension is 256,0.5879477262496948
translation,0,144,experimental-setup,word embeddings,shared by,encoder and decoder,word embeddings shared by encoder and decoder,0.6685371994972229
translation,0,144,experimental-setup,parameters,initialized from,"uniform distribution [ - 0.08 , 0.08 ]","parameters initialized from uniform distribution [ - 0.08 , 0.08 ]",0.6936792731285095
translation,0,144,experimental-setup,"adam ( kingma and ba , 2014 )",for,optimization,"adam ( kingma and ba , 2014 ) for optimization",0.6054565906524658
translation,0,144,experimental-setup,"adam ( kingma and ba , 2014 )",with,mini-batch size,"adam ( kingma and ba , 2014 ) with mini-batch size",0.605026364326477
translation,0,144,experimental-setup,"adam ( kingma and ba , 2014 )",with,initialized learning rate,"adam ( kingma and ba , 2014 ) with initialized learning rate",0.606167197227478
translation,0,144,experimental-setup,optimization,with,mini-batch size,optimization with mini-batch size,0.6342182159423828
translation,0,144,experimental-setup,mini-batch size,has,128,mini-batch size has 128,0.6187843084335327
translation,0,144,experimental-setup,initialized learning rate,has,0.001,initialized learning rate has 0.001,0.5229707360267639
translation,0,145,experimental-setup,latent variable,adopt,component number,latent variable adopt component number,0.5951412320137024
translation,0,145,experimental-setup,component number,of,mixture gaussian,component number of mixture gaussian,0.5715444684028625
translation,0,145,experimental-setup,mixture gaussian,for,prior networks,mixture gaussian for prior networks,0.5790647268295288
translation,0,145,experimental-setup,prior networks,in,wae,prior networks in wae,0.639793336391449
translation,0,145,experimental-setup,prior networks,set to,5,prior networks set to 5,0.6864249110221863
translation,0,145,experimental-setup,wae,set to,5,wae set to 5,0.7740615010261536
translation,0,145,experimental-setup,dimensional size,has,256,dimensional size has 256,0.6180058717727661
translation,0,145,experimental-setup,experimental setup,For,latent variable,experimental setup For latent variable,0.5856719017028809
translation,0,146,experimental-setup,initialized learning rate,as,0.0002,initialized learning rate as 0.0002,0.5231330990791321
translation,0,146,experimental-setup,128 different kernels,for,each kernel size,128 different kernels for each kernel size,0.571118175983429
translation,0,146,experimental-setup,each kernel size,in,"{ 2 , 3 , 4 }","each kernel size in { 2 , 3 , 4 }",0.5627734065055847
translation,0,146,experimental-setup,experimental setup,use,128 different kernels,experimental setup use 128 different kernels,0.6070625185966492
translation,0,147,experimental-setup,response bag,limited to,10,response bag limited to 10,0.653977632522583
translation,0,147,experimental-setup,10,where,instances inside,10 where instances inside,0.6811436414718628
translation,0,147,experimental-setup,randomly sampled,for,each mini-batch,randomly sampled for each mini-batch,0.605891764163971
translation,0,147,experimental-setup,experimental setup,size of,response bag,experimental setup size of response bag,0.7239688038825989
translation,0,148,experimental-setup,models,implemented with,pytorch 0.4.1 4,models implemented with pytorch 0.4.1 4,0.6950232982635498
translation,0,148,experimental-setup,experimental setup,implemented with,pytorch 0.4.1 4,experimental setup implemented with pytorch 0.4.1 4,0.6347583532333374
translation,0,148,experimental-setup,experimental setup,has,models,experimental setup has models,0.5060054659843445
translation,0,91,hyperparameters,model,pre-train,word embedding,model pre-train word embedding,0.692238450050354
translation,0,91,hyperparameters,word embedding,using,glove,word embedding using glove,0.6908194422721863
translation,0,91,hyperparameters,hyperparameters,pre-train,word embedding,hyperparameters pre-train word embedding,0.6999962329864502
translation,0,93,hyperparameters,vanishing latent variable problem,of,cvae,vanishing latent variable problem of cvae,0.5462490916252136
translation,0,93,hyperparameters,vanishing latent variable problem,adopt,kl annealing strategy,vanishing latent variable problem adopt kl annealing strategy,0.627338171005249
translation,0,93,hyperparameters,kl annealing strategy,where,weight,kl annealing strategy where weight,0.6417582035064697
translation,0,93,hyperparameters,weight,of,kl term,weight of kl term,0.5768377184867859
translation,0,93,hyperparameters,gradually increased,during,training,gradually increased during training,0.781878650188446
translation,0,93,hyperparameters,hyperparameters,To overcome,vanishing latent variable problem,hyperparameters To overcome vanishing latent variable problem,0.6856166124343872
translation,0,93,hyperparameters,hyperparameters,adopt,kl annealing strategy,hyperparameters adopt kl annealing strategy,0.6683677434921265
translation,0,6,model,multiple references,by considering,correlation,multiple references by considering correlation,0.7181434631347656
translation,0,6,model,multiple references,modeling,1 - to -n mapping,multiple references modeling 1 - to -n mapping,0.780142068862915
translation,0,6,model,correlation,of,different valid responses,correlation of different valid responses,0.5797386765480042
translation,0,6,model,1 - to -n mapping,with,novel two -step generation architecture,1 - to -n mapping with novel two -step generation architecture,0.6365559101104736
translation,0,6,model,model,utilize,multiple references,model utilize multiple references,0.6611922383308411
translation,0,6,model,model,modeling,1 - to -n mapping,model modeling 1 - to -n mapping,0.8526252508163452
translation,0,7,model,first generation phase,extracts,common features,first generation phase extracts common features,0.6797648668289185
translation,0,7,model,common features,of,different responses,common features of different responses,0.5905592441558838
translation,0,7,model,common features,can generate,multiple diverse and appropriate responses,common features can generate multiple diverse and appropriate responses,0.716459333896637
translation,0,7,model,distinctive features,obtained in,second phase,distinctive features obtained in second phase,0.6273309588432312
translation,0,7,model,distinctive features,can generate,multiple diverse and appropriate responses,distinctive features can generate multiple diverse and appropriate responses,0.7107559442520142
translation,0,7,model,model,has,first generation phase,model has first generation phase,0.5691747665405273
translation,0,17,model,novel response generation model,for,open-domain conversation,novel response generation model for open-domain conversation,0.6096291542053223
translation,0,17,model,novel response generation model,learns to generate,multiple diverse responses,novel response generation model learns to generate multiple diverse responses,0.7540490031242371
translation,0,17,model,multiple diverse responses,with,multiple references,multiple diverse responses with multiple references,0.6078986525535583
translation,0,17,model,multiple diverse responses,by considering,correlation,multiple diverse responses by considering correlation,0.7702144384384155
translation,0,17,model,correlation,has,of different responses,correlation has of different responses,0.5744645595550537
translation,0,17,model,model,propose,novel response generation model,model propose novel response generation model,0.6652249693870544
translation,0,50,model,each specific response separately,by,two -step generation architecture,each specific response separately by two -step generation architecture,0.5959755182266235
translation,0,50,model,model,consider,multiple responses jointly,model consider multiple responses jointly,0.7238872647285461
translation,0,50,model,model,consider,each specific response separately,model consider each specific response separately,0.7611833214759827
translation,0,50,model,model,model,each specific response separately,model model each specific response separately,0.7673094868659973
translation,0,52,model,novel response generation model,for,short - text conversation,novel response generation model for short - text conversation,0.5650686025619507
translation,0,52,model,novel response generation model,models,multiple valid responses,novel response generation model models multiple valid responses,0.7616588473320007
translation,0,52,model,multiple valid responses,for,given query jointly,multiple valid responses for given query jointly,0.6221067905426025
translation,0,52,model,model,propose,novel response generation model,model propose novel response generation model,0.6652249693870544
translation,0,58,model,correlation,of,multiple responses,correlation of multiple responses,0.6123730540275574
translation,0,58,model,multiple responses,with,novel twostep generation architecture,multiple responses with novel twostep generation architecture,0.6134039759635925
translation,0,58,model,novel twostep generation architecture,where,response bag {y },novel twostep generation architecture where response bag {y },0.6017566323280334
translation,0,58,model,each response y,modeled by,two separate features,each response y modeled by two separate features,0.7000769376754761
translation,0,58,model,two separate features,obtained in,each generation phase,two separate features obtained in each generation phase,0.6598522067070007
translation,0,58,model,model,consider,correlation,model consider correlation,0.6837907433509827
translation,0,183,model,novel two -step generation architecture,correlation of,multiple valid responses,novel two -step generation architecture correlation of multiple valid responses,0.6981781721115112
translation,0,183,model,one- to - many queryresponse mapping problem,has,in open-domain conversation,one- to - many queryresponse mapping problem has in open-domain conversation,0.5141159296035767
translation,0,183,model,model,tackle,one- to - many queryresponse mapping problem,model tackle one- to - many queryresponse mapping problem,0.6179583072662354
translation,0,183,model,model,propose,novel two -step generation architecture,model propose novel two -step generation architecture,0.6797235608100891
translation,0,184,model,multiple responses,as,response bag,multiple responses as response bag,0.5805952548980713
translation,0,184,model,common and distinct features,of,different responses,common and distinct features of different responses,0.566008985042572
translation,0,184,model,different responses,in,two generation phases,different responses in two generation phases,0.558929979801178
translation,0,184,model,model,viewing,multiple responses,model viewing multiple responses,0.772538423538208
translation,0,184,model,model,extracts,common and distinct features,model extracts common and distinct features,0.6714522242546082
translation,0,152,results,competitive baselines,on,various evaluation metrics,competitive baselines on various evaluation metrics,0.46506980061531067
translation,0,152,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,0,152,results,outperforms,has,competitive baselines,outperforms has competitive baselines,0.5906397700309753
translation,0,152,results,results,show,our model,results show our model,0.6888449192047119
translation,0,153,results,"seq2seq based models ( s2s , s2s -db and mms )",tend to generate,fluent utterances,"seq2seq based models ( s2s , s2s -db and mms ) tend to generate fluent utterances",0.7478138208389282
translation,0,153,results,"seq2seq based models ( s2s , s2s -db and mms )",share,some overlapped words,"seq2seq based models ( s2s , s2s -db and mms ) share some overlapped words",0.7074823975563049
translation,0,153,results,some overlapped words,with,references,some overlapped words with references,0.5975558161735535
translation,0,153,results,results,has,"seq2seq based models ( s2s , s2s -db and mms )","results has seq2seq based models ( s2s , s2s -db and mms )",0.514677882194519
translation,0,156,results,latent variable,to control,discourse- level diversity,latent variable to control discourse- level diversity,0.6884289979934692
translation,0,156,results,discourse- level diversity,has,diverse responses,discourse- level diversity has diverse responses,0.5526018738746643
translation,0,157,results,our model,achieve,best or second best performances,our model achieve best or second best performances,0.5996348857879639
translation,0,157,results,best or second best performances,on,different automatic evaluation metrics,best or second best performances on different automatic evaluation metrics,0.48694467544555664
translation,0,157,results,improvements,are,most consistent,improvements are most consistent,0.5606865286827087
translation,0,157,results,most consistent,on,bleu - 1 and embedding - based metrics,most consistent on bleu - 1 and embedding - based metrics,0.5050405263900757
translation,0,159,results,our model,gains,promising promotions,our model gains promising promotions,0.783294677734375
translation,0,159,results,promising promotions,over,previous methods,promising promotions over previous methods,0.6630781292915344
translation,0,159,results,promising promotions,on generating,appropriate responses,promising promotions on generating appropriate responses,0.6544770002365112
translation,0,159,results,appropriate responses,with,diverse expressions,appropriate responses with diverse expressions,0.6160569190979004
translation,0,160,results,readability,for,s2s,readability for s2s,0.6293906569480896
translation,0,160,results,readability,for,s2s,readability for s2s,0.6293906569480896
translation,0,160,results,readability,for,s2s,readability for s2s,0.6293906569480896
translation,0,160,results,diversity,for,cvae,diversity for cvae,0.6052937507629395
translation,0,160,results,diversity,for,s2s,diversity for s2s,0.6707196235656738
translation,0,160,results,diversity,for,cvae,diversity for cvae,0.6052937507629395
translation,0,160,results,diversity,for,s2s,diversity for s2s,0.6707196235656738
translation,0,160,results,relevance,for,cvae,relevance for cvae,0.6309415698051453
translation,0,160,results,baselines,show,limited overall performances,baselines show limited overall performances,0.6066688895225525
translation,0,160,results,obvious superiority,has,readability,obvious superiority has readability,0.5167493224143982
translation,0,160,results,obvious superiority,has,baselines,obvious superiority has baselines,0.5766717791557312
translation,0,160,results,inferiority,has,diversity,inferiority has diversity,0.579840898513794
translation,0,160,results,inferiority,has,baselines,inferiority has baselines,0.5866861939430237
translation,0,160,results,results,With,obvious superiority,results With obvious superiority,0.6296428442001343
translation,0,164,results,metrics,show,system,metrics show system,0.6412511467933655
translation,0,164,results,system,benefit from,common feature,system benefit from common feature,0.6902579069137573
translation,0,164,results,common feature,for,better relevance,common feature for better relevance,0.6078552007675171
translation,0,164,results,better relevance,to,query,better relevance to query,0.53703773021698
translation,0,164,results,metrics,has,system,metrics has system,0.6037818789482117
translation,0,166,results,effects,of,discriminator,effects of discriminator,0.6222245693206787
translation,0,166,results,effects,of,modified multireference bag-of- word loss ( mbow ),effects of modified multireference bag-of- word loss ( mbow ),0.5709111094474792
translation,0,166,results,results,validate,effects,results validate effects,0.5262877941131592
translation,0,172,results,responses,generated by,our model,responses generated by our model,0.7005192637443542
translation,0,172,results,responses,show,better quality,responses show better quality,0.6392475962638855
translation,0,172,results,our model,show,better quality,our model show better quality,0.652752161026001
translation,0,172,results,better quality,achieving,high relevance and diversity,better quality achieving high relevance and diversity,0.6489660143852234
translation,0,172,results,results,has,responses,results has responses,0.5085731744766235
translation,2,4,baselines,goal-oriented dialogues,grounded in,associated documents,goal-oriented dialogues grounded in associated documents,0.7165132164955139
translation,2,174,baselines,multi-class sequence classifier,popular for,glue tasks,multi-class sequence classifier popular for glue tasks,0.6892240047454834
translation,2,174,baselines,bertforsequenceclassification model,has,multi-class sequence classifier,bertforsequenceclassification model has multi-class sequence classifier,0.551884651184082
translation,2,216,baselines,multi-class sequence classifier,based on,bertformultiplechoice,multi-class sequence classifier based on bertformultiplechoice,0.7071567177772522
translation,2,216,baselines,multi-class sequence classifier,using,pretrained bert- base -uncased model,multi-class sequence classifier using pretrained bert- base -uncased model,0.6542927026748657
translation,2,216,baselines,pretrained bert- base -uncased model,as,encoder,pretrained bert- base -uncased model as encoder,0.5166407227516174
translation,2,216,baselines,full document,into,account,full document into account,0.5424569845199585
translation,2,216,baselines,full document,to create,index,full document to create index,0.529281497001648
translation,2,2,experiments,doc2dial,has,goal-oriented document - grounded dialogue,doc2dial has goal-oriented document - grounded dialogue,0.5722585916519165
translation,2,175,hyperparameters,pretrained bert- base -uncased model,as,encoder,pretrained bert- base -uncased model as encoder,0.5166407227516174
translation,2,175,hyperparameters,hyperparameters,use,pretrained bert- base -uncased model,hyperparameters use pretrained bert- base -uncased model,0.5746452808380127
translation,2,5,model,dialogue flows,based on,content elements,dialogue flows based on content elements,0.6784754395484924
translation,2,5,model,content elements,corresponds to,higher - level relations,content elements corresponds to higher - level relations,0.6651086807250977
translation,2,5,model,higher - level relations,across,text sections,higher - level relations across text sections,0.7034550905227661
translation,2,5,model,lower - level relations,between,discourse units,lower - level relations between discourse units,0.6225148439407349
translation,2,5,model,discourse units,within,section,discourse units within section,0.7002543807029724
translation,2,5,model,model,construct,dialogue flows,model construct dialogue flows,0.7391067147254944
translation,2,6,model,dialogue flows,to,crowd contributors,dialogue flows to crowd contributors,0.5541622638702393
translation,2,6,model,model,present,dialogue flows,model present dialogue flows,0.6775920391082764
translation,2,26,model,highlevel hierarchical relations,between,document components,highlevel hierarchical relations between document components,0.6216044425964355
translation,2,26,model,low-level semantic relations,between,discourse units,low-level semantic relations between discourse units,0.5950215458869934
translation,2,26,model,low-level semantic relations,to dynamically create,outlines of dialogues,low-level semantic relations to dynamically create outlines of dialogues,0.6871666312217712
translation,2,26,model,model,utilize,highlevel hierarchical relations,model utilize highlevel hierarchical relations,0.6460711359977722
translation,2,26,model,model,call,dialogue flows,model call dialogue flows,0.6726332306861877
translation,2,217,model,bert model,takes,dialogue context d and a document y together,bert model takes dialogue context d and a document y together,0.6471959352493286
translation,2,217,model,dialogue context d and a document y together,as,sequence,dialogue context d and a document y together as sequence,0.5189074277877808
translation,2,217,model,model,has,bert model,model has bert model,0.6192761659622192
translation,2,71,results,conversations,by,second setting,conversations by second setting,0.6162153482437134
translation,2,71,results,conversations,tend to be,more coherent,conversations tend to be more coherent,0.6733076572418213
translation,2,71,results,conversations,tend to be,less time consuming,conversations tend to be less time consuming,0.6740453839302063
translation,2,71,results,second setting,tend to be,less time consuming,second setting tend to be less time consuming,0.6671241521835327
translation,2,71,results,results,find that,conversations,results find that conversations,0.5693145394325256
translation,2,193,results,significant improvement,after including,dialogue act information,significant improvement after including dialogue act information,0.6722624897956848
translation,2,193,results,results,see,significant improvement,results see significant improvement,0.6090703010559082
translation,2,207,results,better results,with,preprocessing method,better results with preprocessing method,0.5902481079101562
translation,2,207,results,original document structure,at,larger scale,original document structure at larger scale,0.5187919735908508
translation,2,207,results,results,has,bleu scores,results has bleu scores,0.5230661034584045
translation,2,208,results,our bleu scores,are,significantly lower,our bleu scores are significantly lower,0.6038022637367249
translation,2,208,results,sharc dev dataset,has,our bleu scores,sharc dev dataset has our bleu scores,0.547267735004425
translation,2,222,results,dl - based approach,shows,better performance,dl - based approach shows better performance,0.6586784720420837
translation,3,208,ablation-analysis,user similarity,influences,performance,user similarity influences performance,0.6940219402313232
translation,3,208,ablation-analysis,performance,of,our model,performance of our model,0.5847885608673096
translation,3,138,baselines,competing methods,has,pretrain-only,competing methods has pretrain-only,0.5434224605560303
translation,3,138,baselines,baselines,has,competing methods,baselines has competing methods,0.504300057888031
translation,3,146,baselines,cmaml,try,two variants,cmaml try two variants,0.6538733243942261
translation,3,146,baselines,baselines,has,cmaml,baselines has cmaml,0.6063379645347595
translation,3,155,experiments,c score,in,persona- chat,c score in persona- chat,0.5593903660774231
translation,3,155,experiments,c score,in,mojitalk,c score in mojitalk,0.5317776203155518
translation,3,155,experiments,c score,uses,pre-trained natural language inference model,c score uses pre-trained natural language inference model,0.5737968683242798
translation,3,155,experiments,c score,uses,emotion classifier,c score uses emotion classifier,0.6128683090209961
translation,3,155,experiments,c score,in,mojitalk,c score in mojitalk,0.5317776203155518
translation,3,155,experiments,c score,uses,emotion classifier,c score uses emotion classifier,0.6128683090209961
translation,3,155,experiments,persona- chat,in,mojitalk,persona- chat in mojitalk,0.5441716909408569
translation,3,155,experiments,pre-trained natural language inference model,to measure,response consistency,pre-trained natural language inference model to measure response consistency,0.6465511322021484
translation,3,155,experiments,pre-trained natural language inference model,to measure,e-acc,pre-trained natural language inference model to measure e-acc,0.6268569231033325
translation,3,155,experiments,pre-trained natural language inference model,in,mojitalk,pre-trained natural language inference model in mojitalk,0.4961875081062317
translation,3,155,experiments,response consistency,with,persona description,response consistency with persona description,0.5183780789375305
translation,3,155,experiments,response consistency,in,mojitalk,response consistency in mojitalk,0.5133309364318848
translation,3,155,experiments,e-acc,in,mojitalk,e-acc in mojitalk,0.5667595267295837
translation,3,155,experiments,e-acc,uses,emotion classifier,e-acc uses emotion classifier,0.5898352265357971
translation,3,155,experiments,mojitalk,uses,emotion classifier,mojitalk uses emotion classifier,0.5950456261634827
translation,3,155,experiments,emotion classifier,to predict,correlation,emotion classifier to predict correlation,0.7152769565582275
translation,3,155,experiments,correlation,between,response,correlation between response,0.7007623910903931
translation,3,155,experiments,correlation,between,designated emotion,correlation between designated emotion,0.6239235401153564
translation,3,155,experiments,c score,has,e-acc,c score has e-acc,0.5963984727859497
translation,3,131,hyperparameters,data ratio,for,meta-training and meta-testing,data ratio for meta-training and meta-testing,0.6484271287918091
translation,3,131,hyperparameters,data ratio,is,10:1,data ratio is 10:1,0.6322274804115295
translation,3,131,hyperparameters,meta-training and meta-testing,is,10:1,meta-training and meta-testing is 10:1,0.5679777264595032
translation,3,131,hyperparameters,both datasets,has,data ratio,both datasets has data ratio,0.5986130833625793
translation,3,131,hyperparameters,hyperparameters,On,both datasets,hyperparameters On both datasets,0.45779603719711304
translation,3,133,hyperparameters,shared module,based on,seq2seq model,shared module based on seq2seq model,0.6827171444892883
translation,3,133,hyperparameters,seq2seq model,with,pre-trained glove embedding,seq2seq model with pre-trained glove embedding,0.5975366830825806
translation,3,133,hyperparameters,4 - layer mlp,for,private module,4 - layer mlp for private module,0.5947727560997009
translation,3,133,hyperparameters,pre-trained glove embedding,has,lstm unit,pre-trained glove embedding has lstm unit,0.548239529132843
translation,3,133,hyperparameters,hyperparameters,implement,shared module,hyperparameters implement shared module,0.6609822511672974
translation,3,134,hyperparameters,dimension,of,hidden state,dimension of hidden state,0.5578262209892273
translation,3,134,hyperparameters,dimension,of,mlp 's output,dimension of mlp 's output,0.5863587260246277
translation,3,134,hyperparameters,dimension,set to,300,dimension set to 300,0.7713988423347473
translation,3,134,hyperparameters,mlp 's output,set to,300,mlp 's output set to 300,0.7386071085929871
translation,3,134,hyperparameters,hyperparameters,has,dimension,hyperparameters has dimension,0.514893651008606
translation,3,135,hyperparameters,cmaml,pretrain,model,cmaml pretrain model,0.6854429244995117
translation,3,135,hyperparameters,cmaml,pretrain,model,cmaml pretrain model,0.6854429244995117
translation,3,135,hyperparameters,cmaml,re-train,model,cmaml re-train model,0.7183182835578918
translation,3,135,hyperparameters,model,for,10 epochs,model for 10 epochs,0.6332400441169739
translation,3,135,hyperparameters,model,for,5 steps,model for 5 steps,0.6576237082481384
translation,3,135,hyperparameters,5 steps,to prune,private network,5 steps to prune private network,0.7500263452529907
translation,3,135,hyperparameters,hyperparameters,In,cmaml,hyperparameters In cmaml,0.5114843249320984
translation,3,136,hyperparameters,l-1 weight,in,re-training stage,l-1 weight in re-training stage,0.5047744512557983
translation,3,136,hyperparameters,l-1 weight,is,0.001,l-1 weight is 0.001,0.5510115623474121
translation,3,136,hyperparameters,hyperparameters,has,l-1 weight,hyperparameters has l-1 weight,0.5248969793319702
translation,3,136,hyperparameters,hyperparameters,has,threshold,hyperparameters has threshold,0.557809591293335
translation,3,7,model,algorithm,customize,unique dialogue model,algorithm customize unique dialogue model,0.6586484313011169
translation,3,7,model,unique dialogue model,for,each task,unique dialogue model for each task,0.6162905097007751
translation,3,7,model,each task,in,few-shot setting,each task in few-shot setting,0.5076244473457336
translation,3,7,model,model,propose,algorithm,model propose algorithm,0.729954719543457
translation,3,8,model,dialogue model,consists of,shared module,dialogue model consists of shared module,0.6927430629730225
translation,3,8,model,dialogue model,consists of,gating module,dialogue model consists of gating module,0.6959010362625122
translation,3,8,model,dialogue model,consists of,private module,dialogue model consists of private module,0.671943724155426
translation,3,12,model,generative model,on,large-scale dialogues,generative model on large-scale dialogues,0.5288959741592407
translation,3,12,model,fine-tune,on,task -specific data corpus,fine-tune on task -specific data corpus,0.5275445580482483
translation,3,12,model,model,pre-train,generative model,model pre-train generative model,0.7190941572189331
translation,3,28,model,customized model agnostic meta- learning algorithm ( cmaml ),able to customize,dialogue models,customized model agnostic meta- learning algorithm ( cmaml ) able to customize dialogue models,0.7730042338371277
translation,3,28,model,dialogue models,in,parameter and model structure perspective,dialogue models in parameter and model structure perspective,0.5337377190589905
translation,3,28,model,parameter and model structure perspective,under,maml framework,parameter and model structure perspective under maml framework,0.6589268445968628
translation,3,28,model,model,propose,customized model agnostic meta- learning algorithm ( cmaml ),model propose customized model agnostic meta- learning algorithm ( cmaml ),0.6659892797470093
translation,3,29,model,dialogue model,of,each task,dialogue model of each task,0.5756519436836243
translation,3,29,model,each task,consists of,three parts,each task consists of three parts,0.6315017342567444
translation,3,29,model,shared module,to learn,general language generation ability,shared module to learn general language generation ability,0.5837065577507019
translation,3,29,model,shared module,to learn,common characteristics,shared module to learn common characteristics,0.6182742714881897
translation,3,29,model,common characteristics,among,tasks,common characteristics among tasks,0.6243689060211182
translation,3,29,model,private module,to model,unique characteristic of this task,private module to model unique characteristic of this task,0.6700592637062073
translation,3,29,model,gate,to absorb,information,gate to absorb information,0.7123310565948486
translation,3,29,model,information,from,shared and private modules,information from shared and private modules,0.5670329928398132
translation,3,29,model,three parts,has,shared module,three parts has shared module,0.6190703511238098
translation,3,29,model,model,has,dialogue model,model has dialogue model,0.5918687582015991
translation,3,33,model,pruning algorithm,adjust,network structure,pruning algorithm adjust network structure,0.674289345741272
translation,3,33,model,network structure,for,better fitting,network structure for better fitting,0.6387727856636047
translation,3,33,model,better fitting,has,training data,better fitting has training data,0.5470346808433533
translation,3,33,model,model,propose,pruning algorithm,model propose pruning algorithm,0.6671295762062073
translation,3,52,model,new meta-learning algorithm,based on,maml,new meta-learning algorithm based on maml,0.6753805875778198
translation,3,52,model,task -specific characteristics,for,generation models,task -specific characteristics for generation models,0.5870511531829834
translation,3,52,model,model,propose,new meta-learning algorithm,model propose new meta-learning algorithm,0.6950917840003967
translation,3,64,model,multilayer perception ( mlp ),in,decoder,multilayer perception ( mlp ) in decoder,0.529952883720398
translation,3,64,model,model,design,multilayer perception ( mlp ),model design multilayer perception ( mlp ),0.5807583332061768
translation,3,139,model,unified dialogue generation model,with,data,unified dialogue generation model with data,0.6214785575866699
translation,3,139,model,data,from,all training tasks,data from all training tasks,0.5437719225883484
translation,3,139,model,model,pre-train,unified dialogue generation model,model pre-train unified dialogue generation model,0.6636157035827637
translation,3,170,results,pretrain - only methods,provide,borderlines,pretrain - only methods provide borderlines,0.6228696703910828
translation,3,170,results,borderlines,of,all methods,borderlines of all methods,0.5885014533996582
translation,3,170,results,persona- chat dataset,has,pretrain - only methods,persona- chat dataset has pretrain - only methods,0.5400742292404175
translation,3,170,results,results,In,persona- chat dataset,results In persona- chat dataset,0.5009280443191528
translation,3,171,results,seq2spg,achieves,best performance,seq2spg achieves best performance,0.7058355212211609
translation,3,171,results,best performance,in terms of,automatic and human measurements,best performance in terms of automatic and human measurements,0.6707294583320618
translation,3,171,results,pretrain- only,has,seq2spg,pretrain- only has seq2spg,0.5981169939041138
translation,3,171,results,results,In,pretrain- only,results In pretrain- only,0.511827290058136
translation,3,172,results,performance,of,competing methods,performance of competing methods,0.5995926260948181
translation,3,172,results,competing methods,in,mojitalk dataset,competing methods in mojitalk dataset,0.5099379420280457
translation,3,172,results,highest dist,among,all the methods,highest dist among all the methods,0.5842804908752441
translation,3,172,results,results,Most of the,performance,results Most of the performance,0.6837766766548157
translation,3,173,results,all non-meta- learning methods ( pretrain- only and finetune ),consistently produce,random word sequences,all non-meta- learning methods ( pretrain- only and finetune ) consistently produce random word sequences,0.747857928276062
translation,3,174,results,meta-learning - based methods,has,survive,meta-learning - based methods has survive,0.5334139466285706
translation,3,174,results,results,has,meta-learning - based methods,results has meta-learning - based methods,0.500697135925293
translation,3,176,results,finetune methods,make,no significant differences,finetune methods make no significant differences,0.6306297779083252
translation,3,176,results,no significant differences,on,"c score , e-acc","no significant differences on c score , e-acc",0.5513346791267395
translation,3,176,results,no significant differences,on,task consistency,no significant differences on task consistency,0.5147879123687744
translation,3,176,results,task consistency,compared with,pretrain - only methods,task consistency compared with pretrain - only methods,0.6453330516815186
translation,3,176,results,both datasets,has,finetune methods,both datasets has finetune methods,0.5575627088546753
translation,3,176,results,results,On,both datasets,results On both datasets,0.49870386719703674
translation,3,177,results,all meta-learning methods,including,maml and cmaml,all meta-learning methods including maml and cmaml,0.661071240901947
translation,3,177,results,maml and cmaml,has,outperforms,maml and cmaml has outperforms,0.6404469609260559
translation,3,177,results,outperforms,has,finetune,outperforms has finetune,0.6555609703063965
translation,3,177,results,results,has,all meta-learning methods,results has all meta-learning methods,0.4636458158493042
translation,3,178,results,cmaml-seq2spg,obtain,22.2%/12.5 % and 11.8 % / 5.6 %,cmaml-seq2spg obtain 22.2%/12.5 % and 11.8 % / 5.6 %,0.5579818487167358
translation,3,178,results,improvement,on,c score and eacc,improvement on c score and eacc,0.5570014715194702
translation,3,178,results,maml - seq2seq and maml -seq2spg,has,cmaml-seq2spg,maml - seq2seq and maml -seq2spg has cmaml-seq2spg,0.6374518871307373
translation,3,178,results,22.2%/12.5 % and 11.8 % / 5.6 %,has,improvement,22.2%/12.5 % and 11.8 % / 5.6 % has improvement,0.5287761092185974
translation,3,178,results,results,Compared with,maml - seq2seq and maml -seq2spg,results Compared with maml - seq2seq and maml -seq2spg,0.6629083752632141
translation,3,178,results,results,Compared with,cmaml-seq2spg,results Compared with cmaml-seq2spg,0.6427332162857056
translation,3,180,results,cmaml -seq2spg,achieves,good improvement,cmaml -seq2spg achieves good improvement,0.6616688966751099
translation,3,180,results,cmaml -seq2spg,achieves,lim-ited improvement,cmaml -seq2spg achieves lim-ited improvement,0.713783860206604
translation,3,180,results,good improvement,compared with,other baselines,good improvement compared with other baselines,0.6384301781654358
translation,3,180,results,lim-ited improvement,on,e-acc,lim-ited improvement on e-acc,0.6101368069648743
translation,3,180,results,lim-ited improvement,on,task consistency score,lim-ited improvement on task consistency score,0.5012519955635071
translation,3,180,results,lim-ited improvement,compared with,persona- chat,lim-ited improvement compared with persona- chat,0.7059499025344849
translation,3,180,results,task consistency score,compared with,persona- chat,task consistency score compared with persona- chat,0.6159722208976746
translation,3,180,results,mojitalk,has,cmaml -seq2spg,mojitalk has cmaml -seq2spg,0.6215758919715881
translation,3,180,results,results,observe,mojitalk,results observe mojitalk,0.5697444677352905
translation,3,180,results,results,in,mojitalk,results in mojitalk,0.5052408576011658
translation,3,187,results,maml,helps to,increase,maml helps to increase,0.6896575093269348
translation,3,187,results,maml,helps to,not as good,maml helps to not as good,0.716194748878479
translation,3,187,results,not as good,as,proposed cmaml methods,not as good as proposed cmaml methods,0.5769672393798828
translation,3,187,results,increase,has,model differences,increase has model differences,0.5596326589584351
translation,3,187,results,results,has,maml,results has maml,0.5715363621711731
translation,3,188,results,cmaml -seq2spg,achieves,highest model difference scores,cmaml -seq2spg achieves highest model difference scores,0.6701890826225281
translation,3,188,results,highest model difference scores,on,two datasets,highest model difference scores on two datasets,0.5163556337356567
translation,3,188,results,results,has,cmaml -seq2spg,results has cmaml -seq2spg,0.5347530841827393
translation,3,191,results,highest scores,on,both datasets,highest scores on both datasets,0.47846078872680664
translation,3,191,results,cmaml -seq2spg,has,highest scores,cmaml -seq2spg has highest scores,0.5621394515037537
translation,3,191,results,results,has,cmaml -seq2spg,results has cmaml -seq2spg,0.5347530841827393
translation,3,192,results,cmaml -seq2sp g,has,relatively low ? scores,cmaml -seq2sp g has relatively low ? scores,0.6042616367340088
translation,3,192,results,results,observe that,cmaml -seq2sp g,results observe that cmaml -seq2sp g,0.5903505682945251
translation,3,198,results,non-meta-learning methods,including,pretrain-only and finetune,non-meta-learning methods including pretrain-only and finetune,0.6843395233154297
translation,3,198,results,quality scores,improve as,quantity of training data,quality scores improve as quantity of training data,0.6651957035064697
translation,3,198,results,non-meta-learning methods,has,quality scores,non-meta-learning methods has quality scores,0.5258287191390991
translation,3,198,results,pretrain-only and finetune,has,quality scores,pretrain-only and finetune has quality scores,0.5571526288986206
translation,3,198,results,quantity of training data,has,increases,quantity of training data has increases,0.5847785472869873
translation,3,198,results,results,For,non-meta-learning methods,results For non-meta-learning methods,0.5616952180862427
translation,3,199,results,not changed too much,on,bleu scores,not changed too much on bleu scores,0.5215728878974915
translation,3,199,results,bleu scores,along with,data growth,bleu scores along with data growth,0.6021035313606262
translation,3,199,results,c scores,keep,increasing,c scores keep increasing,0.6776760220527649
translation,3,199,results,results,has,maml methods,results has maml methods,0.4764048457145691
translation,3,200,results,bleu score and c score,of,cmaml -seq2spg,bleu score and c score of cmaml -seq2spg,0.6010771989822388
translation,3,200,results,cmaml -seq2spg,keep,increasing,cmaml -seq2spg keep increasing,0.6680338382720947
translation,3,200,results,cmaml -seq2spg,achieves,best performance,cmaml -seq2spg achieves best performance,0.6882976293563843
translation,3,200,results,increasing,with,data growth,increasing with data growth,0.658865213394165
translation,3,200,results,best performance,among,all the tasks,best performance among all the tasks,0.5601338148117065
translation,3,200,results,results,has,bleu score and c score,results has bleu score and c score,0.5598441958427429
translation,3,205,results,performance,of,all the methods,performance of all the methods,0.5491371750831604
translation,3,205,results,all the methods,close to,each other,all the methods close to each other,0.7177332639694214
translation,3,205,results,each other,in,similar-user setting,each other in similar-user setting,0.5335443019866943
translation,3,205,results,results,has,performance,results has performance,0.5972660779953003
translation,3,207,results,cmaml -seq2spg,performs,best,cmaml -seq2spg performs best,0.6082931160926819
translation,3,207,results,best,on,c score and bleu,best on c score and bleu,0.5296239256858826
translation,3,207,results,dissimilar-users setting,has,cmaml -seq2spg,dissimilar-users setting has cmaml -seq2spg,0.589549720287323
translation,3,207,results,results,In,dissimilar-users setting,results In dissimilar-users setting,0.5607403516769409
translation,3,209,results,bleu,in,similar-users setting,bleu in similar-users setting,0.4974094033241272
translation,3,209,results,bleu,is,high,bleu is high,0.5860702991485596
translation,3,209,results,c score,is,low,c score is low,0.6236871480941772
translation,3,209,results,dissimilar-users setting,has,bleu,dissimilar-users setting has bleu,0.5242646932601929
translation,4,71,ablation-analysis,contextual model,dependent on,h,contextual model dependent on h,0.7388202548027039
translation,4,71,ablation-analysis,improvement,of,dli task,improvement of dli task,0.5685850381851196
translation,4,71,ablation-analysis,ablation analysis,more,contextual model,ablation analysis more contextual model,0.6449755430221558
translation,4,101,baselines,single- turn slu model,without,memory mechanism,single- turn slu model without memory mechanism,0.7086711525917053
translation,4,101,baselines,nomem,has,single- turn slu model,nomem has single- turn slu model,0.5709554553031921
translation,4,102,baselines,memnet,with,attention based memory retrieval,memnet with attention based memory retrieval,0.6101207137107849
translation,4,102,baselines,baselines,has,memnet,baselines has memnet,0.5386731028556824
translation,4,103,baselines,sden,with,sequential encoder based memory retrieval,sden with sequential encoder based memory retrieval,0.6014755368232727
translation,4,103,baselines,baselines,has,sden,baselines has sden,0.5875926613807678
translation,4,22,experiments,dli,trained with,contextual slu,dli trained with contextual slu,0.7758542895317078
translation,4,22,experiments,contextual slu,if,utterances,contextual slu if utterances,0.6133418083190918
translation,4,22,experiments,right utterance,from,remaining candidates,right utterance from remaining candidates,0.5836034417152405
translation,4,22,experiments,remaining candidates,based on,previously sorted context,remaining candidates based on previously sorted context,0.6436299085617065
translation,4,22,experiments,contextual slu,has,jointly,contextual slu has jointly,0.6232355833053589
translation,4,60,hyperparameters,training batch size,is,64,training batch size is 64,0.5879431366920471
translation,4,60,hyperparameters,all models,with,adam optimizer,all models with adam optimizer,0.574597179889679
translation,4,60,hyperparameters,adam optimizer,with,"default parameters ( kingma and ba , 2014 )","adam optimizer with default parameters ( kingma and ba , 2014 )",0.5877625346183777
translation,4,60,hyperparameters,hyperparameters,train,all models,hyperparameters train all models,0.6430407166481018
translation,4,61,hyperparameters,training,with,five epochs ' early stop,training with five epochs ' early stop,0.6803463101387024
translation,4,61,hyperparameters,up to 30 epochs,with,five epochs ' early stop,up to 30 epochs with five epochs ' early stop,0.6287522912025452
translation,4,61,hyperparameters,five epochs ' early stop,on,validation loss,five epochs ' early stop on validation loss,0.539511501789093
translation,4,61,hyperparameters,training,has,up to 30 epochs,training has up to 30 epochs,0.578665018081665
translation,4,62,hyperparameters,word embedding size,is,100,word embedding size is 100,0.6116927862167358
translation,4,62,hyperparameters,hidden size,of,all rnn layer,hidden size of all rnn layer,0.5730100870132446
translation,4,62,hyperparameters,all rnn layer,is,64,all rnn layer is 64,0.5818501710891724
translation,4,62,hyperparameters,hyperparameters,has,word embedding size,hyperparameters has word embedding size,0.494825541973114
translation,4,62,hyperparameters,hyperparameters,has,hidden size,hyperparameters has hidden size,0.535134494304657
translation,4,64,hyperparameters,dropout rate,set to be,0.3,dropout rate set to be 0.3,0.6402994394302368
translation,4,64,hyperparameters,0.3,to avoid,over-fitting,0.3 to avoid over-fitting,0.6262426376342773
translation,4,64,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,4,6,model,new dialogue logistic inference ( dli ),to consolidate,context memory,new dialogue logistic inference ( dli ) to consolidate context memory,0.6622171998023987
translation,4,6,model,context memory,jointly with,slu,context memory jointly with slu,0.6051079034805298
translation,4,6,model,slu,in,multi-task framework,slu in multi-task framework,0.5553469061851501
translation,4,6,model,model,propose,new dialogue logistic inference ( dli ),model propose new dialogue logistic inference ( dli ),0.6746799349784851
translation,4,7,model,dli,defined as,sorting,dli defined as sorting,0.6608912348747253
translation,4,7,model,dli,shares,same memory encoder and retrieval mechanism,dli shares same memory encoder and retrieval mechanism,0.70595782995224
translation,4,7,model,sorting,shares,same memory encoder and retrieval mechanism,sorting shares same memory encoder and retrieval mechanism,0.7454973459243774
translation,4,7,model,shuffled dialogue session,into,original logical order,shuffled dialogue session into original logical order,0.5813322067260742
translation,4,7,model,same memory encoder and retrieval mechanism,as,slu model,same memory encoder and retrieval mechanism as slu model,0.5500732064247131
translation,4,7,model,sorting,has,shuffled dialogue session,sorting has shuffled dialogue session,0.6374112963676453
translation,4,7,model,model,has,dli,model has dli,0.64745032787323
translation,4,21,model,multi-task learning approach,for,multi-turn slu,multi-task learning approach for multi-turn slu,0.6204521059989929
translation,4,21,model,multi-task learning approach,consolidating,context memory,multi-task learning approach consolidating context memory,0.6772762537002563
translation,4,21,model,model,propose,multi-task learning approach,model propose multi-task learning approach,0.6610496044158936
translation,4,81,model,novel dialogue logistic inference task,for,contextual slu,novel dialogue logistic inference task for contextual slu,0.5777865052223206
translation,4,81,model,novel dialogue logistic inference task,with which,memory encoding and retrieval components,novel dialogue logistic inference task with which memory encoding and retrieval components,0.616565465927124
translation,4,81,model,model,propose,novel dialogue logistic inference task,model propose novel dialogue logistic inference task,0.6310344934463501
translation,4,67,results,all contextual slu models,with,memory mechanism,all contextual slu models with memory mechanism,0.6218807101249695
translation,4,67,results,memory mechanism,benefit from,our dialogue logistic dependent multi-task framework,memory mechanism benefit from our dialogue logistic dependent multi-task framework,0.6234347224235535
translation,4,67,results,our dialogue logistic dependent multi-task framework,especially on,slot filling task,our dialogue logistic dependent multi-task framework especially on slot filling task,0.5965285897254944
translation,4,72,results,performance,of,memnet,performance of memnet,0.5809146165847778
translation,4,72,results,performance,of,memnet,performance of memnet,0.5809146165847778
translation,4,72,results,memnet,with,sden,memnet with sden,0.710534393787384
translation,4,72,results,memnet,find that,our sden ?,memnet find that our sden ?,0.739022970199585
translation,4,72,results,our sden ?,stronger than,memnet,our sden ? stronger than memnet,0.7846222519874573
translation,4,72,results,dialogue length,has,increased,dialogue length has increased,0.6174494624137878
translation,4,72,results,results,Comparing,performance,results Comparing performance,0.7179481983184814
translation,4,73,results,improvements,on,kvret *,improvements on kvret *,0.5503895282745361
translation,4,73,results,improvements,on,kvret,improvements on kvret,0.5747097730636597
translation,4,73,results,improvements,higher than,kvret,improvements higher than kvret,0.7408412098884583
translation,4,73,results,kvret *,higher than,kvret,kvret * higher than kvret,0.7202072143554688
translation,4,73,results,results,see that,improvements,results see that improvements,0.7084582448005676
translation,5,181,baselines,baselines,has,data augmentation ( da ) navigation,baselines has data augmentation ( da ) navigation,0.5805402994155884
translation,5,17,experiments,agents,using,human-human collaborative vision - and - dialogue navigation ( cvdn ) dataset,agents using human-human collaborative vision - and - dialogue navigation ( cvdn ) dataset,0.6595839262008667
translation,5,173,experiments,questioner and guide models,use,rmsprop optimizer,questioner and guide models use rmsprop optimizer,0.6613640785217285
translation,5,173,experiments,rmsprop optimizer,with,learning rate,rmsprop optimizer with learning rate,0.5848580598831177
translation,5,173,experiments,learning rate,has,0.0001,learning rate has 0.0001,0.5380781888961792
translation,5,167,hyperparameters,"n avigator , questioner , and guide agents",as,encoder-decoder lstm models,"n avigator , questioner , and guide agents as encoder-decoder lstm models",0.5340146422386169
translation,5,167,hyperparameters,encoder-decoder lstm models,with,512 hidden dimensions,encoder-decoder lstm models with 512 hidden dimensions,0.6141983270645142
translation,5,167,hyperparameters,hyperparameters,initialize,"n avigator , questioner , and guide agents","hyperparameters initialize n avigator , questioner , and guide agents",0.7635636329650879
translation,5,169,hyperparameters,512 dimensional penultimate resnet layer,for,image observations i t,512 dimensional penultimate resnet layer for image observations i t,0.6161038279533386
translation,5,169,hyperparameters,embed words w,in,256 dimensions,embed words w in 256 dimensions,0.5130641460418701
translation,5,169,hyperparameters,embed actions,in,32 dimensions,embed actions in 32 dimensions,0.5420673489570618
translation,5,169,hyperparameters,hyperparameters,use,512 dimensional penultimate resnet layer,hyperparameters use 512 dimensional penultimate resnet layer,0.5362460613250732
translation,5,170,hyperparameters,word history,up to,160 tokens,word history up to 160 tokens,0.6321596503257751
translation,5,172,hyperparameters,n avigator models,with,adam optimizer,n avigator models with adam optimizer,0.6134946942329407
translation,5,172,hyperparameters,n avigator models,with,learning rate,n avigator models with learning rate,0.6194915771484375
translation,5,172,hyperparameters,n avigator models,with,learning rate,n avigator models with learning rate,0.6194915771484375
translation,5,172,hyperparameters,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,5,172,hyperparameters,adam optimizer,with,weight decay,adam optimizer with weight decay,0.6082910299301147
translation,5,172,hyperparameters,learning rate,of,0.0001,learning rate of 0.0001,0.5900294780731201
translation,5,172,hyperparameters,0.0001,with,weight decay,0.0001 with weight decay,0.5742313861846924
translation,5,172,hyperparameters,weight decay,has,0.0005,weight decay has 0.0005,0.5380997657775879
translation,5,172,hyperparameters,hyperparameters,optimize,n avigator models,hyperparameters optimize n avigator models,0.676889181137085
translation,5,174,hyperparameters,cvdn data,with,batches,cvdn data with batches,0.665189802646637
translation,5,174,hyperparameters,batches,of size,100,batches of size 100,0.7226727604866028
translation,5,174,hyperparameters,batches,",",000 iterations,"batches , 000 iterations",0.5806530117988586
translation,5,174,hyperparameters,100,for,20,100 for 20,0.5611101984977722
translation,5,174,hyperparameters,20,",",000 iterations,"20 , 000 iterations",0.5775651335716248
translation,5,174,hyperparameters,hyperparameters,pretrained on,cvdn data,hyperparameters pretrained on cvdn data,0.7756654620170593
translation,5,175,hyperparameters,models,trained with,batches,models trained with batches,0.774842381477356
translation,5,175,hyperparameters,batches,of,10,batches of 10,0.7440423965454102
translation,5,175,hyperparameters,batches,for,rmm,batches for rmm,0.6909292936325073
translation,5,175,hyperparameters,rmm,with,n = 3,rmm with n = 3,0.6392255425453186
translation,5,175,hyperparameters,5,",",000 iterations,"5 , 000 iterations",0.5996094942092896
translation,5,175,hyperparameters,self - play,has,models,self - play has models,0.6094565987586975
translation,5,175,hyperparameters,hyperparameters,During,self - play,hyperparameters During self - play,0.6793015003204346
translation,5,176,hyperparameters,dropout rate,of,0.5,dropout rate of 0.5,0.6072384119033813
translation,5,176,hyperparameters,dropout rate,used during,all training,dropout rate used during all training,0.6767750978469849
translation,5,176,hyperparameters,0.5,used during,all training,0.5 used during all training,0.6811681985855103
translation,5,176,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,5,177,hyperparameters,n avigator models,trained using,student sampling,n avigator models trained using student sampling,0.7154243588447571
translation,5,177,hyperparameters,hyperparameters,has,n avigator models,hyperparameters has n avigator models,0.5159339904785156
translation,5,7,model,model,propose,recursive mental model ( rmm ),model propose recursive mental model ( rmm ),0.6420926451683044
translation,5,24,model,and guide agents,to perform,end-to-end,and guide agents to perform end-to-end,0.6779590845108032
translation,5,24,model,and guide agents,to perform,collaborative dialogues,and guide agents to perform collaborative dialogues,0.6315344572067261
translation,5,24,model,collaborative dialogues,with,question generation,collaborative dialogues with question generation,0.5982577204704285
translation,5,24,model,collaborative dialogues,with,question answering ( c3 ),collaborative dialogues with question answering ( c3 ),0.6434844732284546
translation,5,24,model,collaborative dialogues,with,navigation,collaborative dialogues with navigation,0.6230998039245605
translation,5,24,model,collaborative dialogues,conditioned on,dialogue history,collaborative dialogues conditioned on dialogue history,0.6954114437103271
translation,5,24,model,question generation,has,c2 ),question generation has c2 ),0.6330183148384094
translation,5,24,model,navigation,has,c4 ),navigation has c4 ),0.6803116798400879
translation,5,53,model,model,has,reinforcement learning - based dialogue modeling,model has reinforcement learning - based dialogue modeling,0.5415735244750977
translation,5,82,model,"three sequence- to-sequence ( bahdanau et al. , 2015 ) models",to perform,as n avigator,"three sequence- to-sequence ( bahdanau et al. , 2015 ) models to perform as n avigator",0.6963587403297424
translation,5,82,model,"three sequence- to-sequence ( bahdanau et al. , 2015 ) models",to perform,questioner,"three sequence- to-sequence ( bahdanau et al. , 2015 ) models to perform questioner",0.6852629780769348
translation,5,82,model,"three sequence- to-sequence ( bahdanau et al. , 2015 ) models",to perform,guide,"three sequence- to-sequence ( bahdanau et al. , 2015 ) models to perform guide",0.7260837554931641
translation,5,82,model,model,present,"three sequence- to-sequence ( bahdanau et al. , 2015 ) models","model present three sequence- to-sequence ( bahdanau et al. , 2015 ) models",0.6366176605224609
translation,5,83,model,"lstm ( hochreiter and schmidhuber , 1997 ) encoder",for,dialogue history,"lstm ( hochreiter and schmidhuber , 1997 ) encoder for dialogue history",0.550194501876831
translation,5,83,model,model,rely on,"lstm ( hochreiter and schmidhuber , 1997 ) encoder","model rely on lstm ( hochreiter and schmidhuber , 1997 ) encoder",0.6739753484725952
translation,5,84,model,visual observations,take,"penultimate resnet ( he et al. , 2015 ) layer","visual observations take penultimate resnet ( he et al. , 2015 ) layer",0.5505949854850769
translation,5,84,model,"penultimate resnet ( he et al. , 2015 ) layer",as,image observation,"penultimate resnet ( he et al. , 2015 ) layer as image observation",0.4742274582386017
translation,5,84,model,model,To encode,visual observations,model To encode visual observations,0.7800258994102478
translation,5,168,model,n avigator encoder,is,forward lstm,n avigator encoder is forward lstm,0.5380948185920715
translation,5,168,model,questioner and guide speaker models,use,bi-lstm encoders,questioner and guide speaker models use bi-lstm encoders,0.5836304426193237
translation,5,168,model,model,has,n avigator encoder,model has n avigator encoder,0.5704885721206665
translation,5,171,model,value / critic module,is,linear layer,value / critic module is linear layer,0.5034125447273254
translation,5,171,model,linear layer,with,relu,linear layer with relu,0.6295327544212341
translation,5,171,model,linear layer,with,dropout,linear layer with dropout,0.6630899906158447
translation,5,171,model,dropout,on top of,hidden state,dropout on top of hidden state,0.6557259559631348
translation,5,171,model,model,has,value / critic module,model has value / critic module,0.5676564574241638
translation,5,199,results,rmm - based agents,perform,slightly less well,rmm - based agents perform slightly less well,0.5595963597297668
translation,5,199,results,slightly less well,than,baseline sequence - to-sequence models,slightly less well than baseline sequence - to-sequence models,0.5602084994316101
translation,5,199,results,baseline sequence - to-sequence models,on,goal progress,baseline sequence - to-sequence models on goal progress,0.4895884394645691
translation,5,199,results,seen environments,has,rmm - based agents,seen environments has rmm - based agents,0.5708934664726257
translation,5,199,results,results,In,seen environments,results In seen environments,0.5941095948219299
translation,5,204,results,model,with,sampling,model with sampling,0.6427550315856934
translation,5,204,results,sampling,generalizes,best,sampling generalizes best,0.7772862315177917
translation,5,204,results,best,to,unseen environments,best to unseen environments,0.6001665592193604
translation,5,204,results,results,given,rmm,results given rmm,0.6468669772148132
translation,5,231,results,other two agents,in terms of,goal progress,other two agents in terms of goal progress,0.6442310214042664
translation,5,231,results,goal progress,for,each dialogue act,goal progress for each dialogue act,0.6217565536499023
translation,5,231,results,rmm,has,consistently outperforms,rmm has consistently outperforms,0.6273834109306335
translation,5,231,results,consistently outperforms,has,other two agents,consistently outperforms has other two agents,0.5683138370513916
translation,5,231,results,results,has,rmm,results has rmm,0.5415871739387512
translation,6,12,model,annotation scheme,based on,corpora of natural conversations,annotation scheme based on corpora of natural conversations,0.5953702926635742
translation,6,12,model,annotation scheme,provides,several layers of annotations,annotation scheme provides several layers of annotations,0.6181668639183044
translation,6,12,model,corpora of natural conversations,in,"several languages ( english , spanish , and dutch )","corpora of natural conversations in several languages ( english , spanish , and dutch )",0.5026472210884094
translation,6,12,model,several layers of annotations,for,qaps,several layers of annotations for qaps,0.6565872430801392
translation,6,12,model,model,propose,annotation scheme,model propose annotation scheme,0.6600000858306885
translation,7,5,model,end-to - end trainable text - to - sql guided framework,to learn,neural agent,end-to - end trainable text - to - sql guided framework to learn neural agent,0.6309131979942322
translation,7,5,model,neural agent,interacts with,kbs,neural agent interacts with kbs,0.7172596454620361
translation,7,5,model,kbs,using,generated sql queries,kbs using generated sql queries,0.6835921406745911
translation,7,5,model,airconcierge,has,end-to - end trainable text - to - sql guided framework,airconcierge has end-to - end trainable text - to - sql guided framework,0.6154559850692749
translation,7,5,model,model,propose,airconcierge,model propose airconcierge,0.6626040935516357
translation,7,6,model,neural agent,first learns to,ask and confirm,neural agent first learns to ask and confirm,0.7311009168624878
translation,7,6,model,customer 's intent,during,multi-turn interactions,customer 's intent during multi-turn interactions,0.7001469135284424
translation,7,6,model,ground,into,executable sql queries,ground into executable sql queries,0.6250728964805603
translation,7,6,model,user constraints,into,executable sql queries,user constraints into executable sql queries,0.5845874547958374
translation,7,6,model,relevant information,from,kbs,relevant information from kbs,0.5595192909240723
translation,7,6,model,ask and confirm,has,customer 's intent,ask and confirm has customer 's intent,0.5935613512992859
translation,7,6,model,ground,has,user constraints,ground has user constraints,0.5382296442985535
translation,7,6,model,model,has,neural agent,model has neural agent,0.5286149978637695
translation,7,22,model,sql - guided task - oriented dialogue system,efficiently work with,"real-world , large-scale kbs","sql - guided task - oriented dialogue system efficiently work with real-world , large-scale kbs",0.7064448595046997
translation,7,22,model,"real-world , large-scale kbs",by formulating,sql queries,"real-world , large-scale kbs by formulating sql queries",0.6125942468643188
translation,7,22,model,sql queries,based on,context,sql queries based on context,0.65492182970047
translation,7,22,model,sql queries,to retrieve,relevant information,sql queries to retrieve relevant information,0.7718473672866821
translation,7,22,model,context,of,dialogue,context of dialogue,0.5731639862060547
translation,7,22,model,relevant information,from,kbs,relevant information from kbs,0.5595192909240723
translation,7,22,model,airconcierge,has,sql - guided task - oriented dialogue system,airconcierge has sql - guided task - oriented dialogue system,0.5764133334159851
translation,7,22,model,model,propose,airconcierge,model propose airconcierge,0.6626040935516357
translation,8,175,ablation-analysis,synergy,between,abundant response selection reddit data,synergy between abundant response selection reddit data,0.6668961048126221
translation,8,175,ablation-analysis,synergy,between,scarce in- domain data,synergy between scarce in- domain data,0.6572177410125732
translation,8,175,ablation-analysis,scarce in- domain data,effectively achieved through,proposed training regime,scarce in- domain data effectively achieved through proposed training regime,0.7041682600975037
translation,8,175,ablation-analysis,both components,crucial for,final improved performance,both components crucial for final improved performance,0.7715372443199158
translation,8,175,ablation-analysis,final improved performance,in,each target domain,final improved performance in each target domain,0.5192261338233948
translation,8,175,ablation-analysis,ablation analysis,indicate that,synergy,ablation analysis indicate that synergy,0.6531826257705688
translation,8,123,baselines,"unique ( question , answer ) pairs",added to,final dataset,"unique ( question , answer ) pairs added to final dataset",0.6754339337348938
translation,8,123,baselines,"unique ( question , answer ) pairs",divided into,"training ( 70 % ) , validation ( 20 % ) and test portions ( 10 % )","unique ( question , answer ) pairs divided into training ( 70 % ) , validation ( 20 % ) and test portions ( 10 % )",0.6042338609695435
translation,8,123,baselines,baselines,has,"unique ( question , answer ) pairs","baselines has unique ( question , answer ) pairs",0.5850980281829834
translation,8,124,baselines,two standard ir baselines,based on,keyword matching,two standard ir baselines based on keyword matching,0.5734901428222656
translation,8,124,baselines,keyword matching,has,simple tf - idf query- response scoring,keyword matching has simple tf - idf query- response scoring,0.5613538026809692
translation,8,124,baselines,baselines,evaluate,two standard ir baselines,baselines evaluate two standard ir baselines,0.680471658706665
translation,8,131,baselines,two variants,bidirectional transformer model of,devlin et al . ( 2019 ),two variants bidirectional transformer model of devlin et al . ( 2019 ),0.7069308757781982
translation,8,131,baselines,baselines,compare to,two variants,baselines compare to two variants,0.6784846186637878
translation,8,217,baselines,pretrained general- domain ( reddit ) response selection model,directly applied on,each target task,pretrained general- domain ( reddit ) response selection model directly applied on each target task,0.6337507963180542
translation,8,217,baselines,ft - direct,after pretraining,large response selection model,ft - direct after pretraining large response selection model,0.6939160823822021
translation,8,217,baselines,large response selection model,on,reddit,large response selection model on reddit,0.5628774762153625
translation,8,217,baselines,each target task,by directly continuing,training,each target task by directly continuing training,0.6536038517951965
translation,8,217,baselines,training,on,( much smaller ) target domain data,training on ( much smaller ) target domain data,0.5544174313545227
translation,8,217,baselines,- batch mixing,of,reddit input-response pairs,- batch mixing of reddit input-response pairs,0.6194496750831604
translation,8,217,baselines,reddit input-response pairs,with,target domain pairs,reddit input-response pairs with target domain pairs,0.6425270438194275
translation,8,217,baselines,reddit - direct,has,pretrained general- domain ( reddit ) response selection model,reddit - direct has pretrained general- domain ( reddit ) response selection model,0.5411055088043213
translation,8,217,baselines,baselines,has,reddit - direct,baselines has reddit - direct,0.5946290493011475
translation,8,218,baselines,response selection model,on,each target task,response selection model on each target task,0.5011314749717712
translation,8,218,baselines,response selection model,without leveraging,general - domain reddit data,response selection model without leveraging general - domain reddit data,0.7329210042953491
translation,8,102,hyperparameters,sgd,setting,initial learning rate,sgd setting initial learning rate,0.43020448088645935
translation,8,102,hyperparameters,sgd,decaying,learning rate,sgd decaying learning rate,0.7268555164337158
translation,8,102,hyperparameters,initial learning rate,to,0.03,initial learning rate to 0.03,0.5441789031028748
translation,8,102,hyperparameters,learning rate,by,0.3x every 1 m training steps,learning rate by 0.3x every 1 m training steps,0.565446138381958
translation,8,102,hyperparameters,0.3x every 1 m training steps,after,first 2.5 m steps,0.3x every 1 m training steps after first 2.5 m steps,0.7009112238883972
translation,8,103,hyperparameters,unigram and bigram embedding gradients,by,batch size,unigram and bigram embedding gradients by batch size,0.5328798890113831
translation,8,103,hyperparameters,hyperparameters,scale,unigram and bigram embedding gradients,hyperparameters scale unigram and bigram embedding gradients,0.6807848215103149
translation,8,104,hyperparameters,batch size,is,500,batch size is 500,0.6552711129188538
translation,8,104,hyperparameters,attention projection dimensionality,is,64,attention projection dimensionality is 64,0.5480965375900269
translation,8,104,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,8,104,hyperparameters,hyperparameters,has,attention projection dimensionality,hyperparameters has attention projection dimensionality,0.4867248237133026
translation,8,105,hyperparameters,label smoothing technique,to reduce,overfitting,label smoothing technique to reduce overfitting,0.6263542175292969
translation,8,105,hyperparameters,hyperparameters,apply,label smoothing technique,hyperparameters apply label smoothing technique,0.5792936682701111
translation,8,130,hyperparameters,fixed mean-pooling,of,elmo contextualised embeddings,fixed mean-pooling of elmo contextualised embeddings,0.5298258662223816
translation,8,130,hyperparameters,fixed mean-pooling,pretrained on,bidirectional lm task,fixed mean-pooling pretrained on bidirectional lm task,0.7979910373687744
translation,8,130,hyperparameters,bidirectional lm task,using,lm 1b words benchmark,bidirectional lm task using lm 1b words benchmark,0.6154153943061829
translation,8,130,hyperparameters,hyperparameters,run,fixed mean-pooling,hyperparameters run fixed mean-pooling,0.7002687454223633
translation,8,210,model,model,focus on,response selection pretraining,model focus on response selection pretraining,0.7228877544403076
translation,8,33,results,proposed two -step response selection training regime,is,more effective,proposed two -step response selection training regime is more effective,0.5679436922073364
translation,8,33,results,more effective,than,directly applying offthe-shelf state - of - the - art sentence encoders,more effective than directly applying offthe-shelf state - of - the - art sentence encoders,0.5534160137176514
translation,8,33,results,results,show,proposed two -step response selection training regime,results show proposed two -step response selection training regime,0.6528711915016174
translation,8,163,results,saturates,provided with,sufficient number of parameters,saturates provided with sufficient number of parameters,0.6296573877334595
translation,8,163,results,model,has,saturates,model has saturates,0.614077627658844
translation,8,163,results,results,suggest,model,results suggest model,0.5904608964920044
translation,8,164,results,scores,show,benefits,scores show benefits,0.7013520002365112
translation,8,164,results,benefits,of,selfattention,benefits of selfattention,0.5388541221618652
translation,8,164,results,benefits,of,positional embeddings,benefits of positional embeddings,0.5744743943214417
translation,8,164,results,positional embeddings,instead of,deep feed -forward averaging,positional embeddings instead of deep feed -forward averaging,0.5681403279304504
translation,8,164,results,deep feed -forward averaging,of,input unigram and bigram embeddings,deep feed -forward averaging of input unigram and bigram embeddings,0.5279197096824646
translation,8,164,results,results,has,scores,results has scores,0.5219217538833618
translation,8,166,results,large gap,with,unigram-only model variant,large gap with unigram-only model variant,0.6736295223236084
translation,8,169,results,benefits,of,reddit pretraining and fine-tuning,benefits of reddit pretraining and fine-tuning,0.5727701187133789
translation,8,169,results,reddit pretraining and fine-tuning,observed in,all tasks,reddit pretraining and fine-tuning observed in all tasks,0.6315327882766724
translation,8,169,results,all tasks,regardless of,in-domain data size,all tasks regardless of in-domain data size,0.6777259111404419
translation,8,170,results,large gains,over,target - only model,large gains over target - only model,0.6979266405105591
translation,8,170,results,target - only model,trains,domain-specific response selection encoder,target - only model trains domain-specific response selection encoder,0.7283782362937927
translation,8,170,results,results,report,large gains,results report large gains,0.7031916975975037
translation,8,173,results,reddit - direct,suggests,fine-tuning,reddit - direct suggests fine-tuning,0.6644654870033264
translation,8,173,results,fine-tuning,even with,small amount of in-domain data,fine-tuning even with small amount of in-domain data,0.6768063902854919
translation,8,173,results,fine-tuning,lead to,large improvements,fine-tuning lead to large improvements,0.7102314829826355
translation,8,173,results,gains,are,+ 32.5 %,gains are + 32.5 %,0.5935760736465454
translation,8,173,results,gains,are,+ 22.8 %,gains are + 22.8 %,0.5921281576156616
translation,8,173,results,gains,are,+ 11.5 %,gains are + 11.5 %,0.5964553356170654
translation,8,173,results,+ 67.5 %,on,banking,+ 67.5 % on banking,0.602885365486145
translation,8,173,results,+ 32.5 %,on,ubuntu,+ 32.5 % on ubuntu,0.49278756976127625
translation,8,173,results,+ 22.8 %,on,amazonqa,+ 22.8 % on amazonqa,0.5558266043663025
translation,8,173,results,+ 11.5 %,on,open - sub,+ 11.5 % on open - sub,0.5669686198234558
translation,8,173,results,results,comparison to,reddit - direct,results comparison to reddit - direct,0.6806698441505432
translation,8,178,results,results,of,tf -idf and bm25,results of tf -idf and bm25,0.5546985864639282
translation,8,178,results,tf -idf and bm25,reveal,lexical evidence,tf -idf and bm25 reveal lexical evidence,0.6487752199172974
translation,8,178,results,tf -idf and bm25,achieves,reasonable performance,tf -idf and bm25 achieves reasonable performance,0.6756121516227722
translation,8,178,results,lexical evidence,from,preceding input,lexical evidence from preceding input,0.5454917550086975
translation,8,178,results,lexical evidence,can,partially help,lexical evidence can partially help,0.5573937892913818
translation,8,178,results,partially help,in,response selection task,partially help in response selection task,0.47354087233543396
translation,8,178,results,reasonable performance,across,target tasks,reasonable performance across target tasks,0.6662318706512451
translation,8,178,results,results,of,tf -idf and bm25,results of tf -idf and bm25,0.5546985864639282
translation,8,178,results,results,has,results,results has results,0.48582205176353455
translation,8,181,results,substantially outperformed,by,proposed fine-tuning approach,substantially outperformed by proposed fine-tuning approach,0.5981501340866089
translation,8,181,results,proposed fine-tuning approach,across,board,proposed fine-tuning approach across board,0.6476016044616699
translation,8,183,results,off - the-shelf sentence encoders,such as,bert,off - the-shelf sentence encoders such as bert,0.639673113822937
translation,8,183,results,off - the-shelf sentence encoders,USE,bert,off - the-shelf sentence encoders USE bert,0.5962621569633484
translation,8,183,results,off - the-shelf sentence encoders,leads to,degraded performance,off - the-shelf sentence encoders leads to degraded performance,0.62556391954422
translation,8,183,results,bert,directly on,in-domain sentences,bert directly on in-domain sentences,0.6767191290855408
translation,8,183,results,degraded performance,compared even to,tf - idf,degraded performance compared even to tf - idf,0.6918731927871704
translation,8,183,results,reddit - direct baseline,without,in - domain fine-tuning,reddit - direct baseline without in - domain fine-tuning,0.7068809866905212
translation,8,183,results,results,Using,off - the-shelf sentence encoders,results Using off - the-shelf sentence encoders,0.6003250479698181
translation,8,184,results,map method,leads to,substantial gains,map method leads to substantial gains,0.6906952261924744
translation,8,184,results,map,leads to,substantial gains,map leads to substantial gains,0.7237743735313416
translation,8,184,results,substantial gains,over,sim,substantial gains over sim,0.7200753092765808
translation,8,184,results,sim,for,all vector-based baseline models,sim for all vector-based baseline models,0.5957302451133728
translation,8,189,results,worse,on,reddit test set,worse on reddit test set,0.520709216594696
translation,8,189,results,more in - domain training data,has,ft - direct,more in - domain training data has ft - direct,0.5869594812393188
translation,8,189,results,results,With,more in - domain training data,results With more in - domain training data,0.6159584522247314
translation,8,190,results,ft - mixed,manages to maintain,high performance,ft - mixed manages to maintain high performance,0.7758698463439941
translation,8,190,results,high performance,on,reddit,high performance on reddit,0.5788739323616028
translation,8,190,results,reddit,due to,batch mixing,reddit due to batch mixing,0.7171606421470642
translation,8,190,results,batch mixing,used in,fine-tuning process,batch mixing used in fine-tuning process,0.6832376718521118
translation,8,190,results,results,has,ft - mixed,results has ft - mixed,0.5302112102508545
translation,9,83,baselines,k-means,for,clustering,k-means for clustering,0.6376880407333374
translation,9,83,baselines,gold intent number,as,cluster number,gold intent number as cluster number,0.5445753335952759
translation,9,83,baselines,cluster number,of,models,cluster number of models,0.649566113948822
translation,9,77,experiments,results vector,for,clustering,results vector for clustering,0.610526978969574
translation,9,77,experiments,clustering,as,baseline systems,clustering as baseline systems,0.5554620623588562
translation,9,81,experiments,clickthrough data,for,pre-training,clickthrough data for pre-training,0.5864447951316833
translation,9,81,experiments,clickthrough data,get,encoded sentence vector,clickthrough data get encoded sentence vector,0.5451135039329529
translation,9,81,experiments,encoded sentence vector,for,clustering,encoded sentence vector for clustering,0.6516303420066833
translation,9,76,hyperparameters,radial basis function ( rbf ),used with,sklearn default settings,radial basis function ( rbf ) used with sklearn default settings,0.6706677675247192
translation,9,76,hyperparameters,hyperparameters,has,radial basis function ( rbf ),hyperparameters has radial basis function ( rbf ),0.535115122795105
translation,9,79,hyperparameters,topic distribution,for,clustering,topic distribution for clustering,0.6222518086433411
translation,9,79,hyperparameters,hyperparameters,use,topic distribution,hyperparameters use topic distribution,0.6019948720932007
translation,9,85,hyperparameters,glove,as,pre-trained word embedding,glove as pre-trained word embedding,0.5002760887145996
translation,9,85,hyperparameters,pre-trained word embedding,in,proposed systems,pre-trained word embedding in proposed systems,0.5264865756034851
translation,9,85,hyperparameters,hyperparameters,leverage,glove,hyperparameters leverage glove,0.7760518193244934
translation,9,86,hyperparameters,pos tag,labeled through,nltk toolkit,pos tag labeled through nltk toolkit,0.6515510082244873
translation,9,86,hyperparameters,hyperparameters,has,pos tag,hyperparameters has pos tag,0.5310216546058655
translation,9,87,hyperparameters,topic number,of,btm,topic number of btm,0.6106116771697998
translation,9,87,hyperparameters,btm,set to,20,btm set to 20,0.7452986836433411
translation,9,87,hyperparameters,hyperparameters,has,topic number,hyperparameters has topic number,0.517425537109375
translation,9,88,hyperparameters,encoded feature dimension,set to,30,encoded feature dimension set to 30,0.7401119470596313
translation,9,88,hyperparameters,encoded feature dimension,concatenated to,120 dimension assembled vector,encoded feature dimension concatenated to 120 dimension assembled vector,0.7499380707740784
translation,9,88,hyperparameters,hyperparameters,has,encoded feature dimension,hyperparameters has encoded feature dimension,0.5195767879486084
translation,9,6,model,our framework auto-dialabel,to automatically cluster,dialogue intents and slots,our framework auto-dialabel to automatically cluster dialogue intents and slots,0.7122766971588135
translation,9,6,model,model,propose,our framework auto-dialabel,model propose our framework auto-dialabel,0.7628611922264099
translation,9,7,model,set of context features,leverage,autoencoder,set of context features leverage autoencoder,0.6868265867233276
translation,9,7,model,set of context features,adapt,dynamic hierarchical clustering method,set of context features adapt dynamic hierarchical clustering method,0.7600694298744202
translation,9,7,model,autoencoder,for,feature assembly,autoencoder for feature assembly,0.5764209032058716
translation,9,7,model,dynamic hierarchical clustering method,for,intent and slot labeling,dynamic hierarchical clustering method for intent and slot labeling,0.6186650991439819
translation,9,7,model,model,collect,set of context features,model collect set of context features,0.6591947078704834
translation,9,7,model,model,adapt,dynamic hierarchical clustering method,model adapt dynamic hierarchical clustering method,0.7675719857215881
translation,9,97,results,our proposed auto-dialabel,achieves,high intent labeling accuracy,our proposed auto-dialabel achieves high intent labeling accuracy,0.6773224472999573
translation,9,97,results,outperforms,by,large margin,outperforms by large margin,0.6290680766105652
translation,9,97,results,other baseline systems,by,large margin,other baseline systems by large margin,0.5723528861999512
translation,9,97,results,high intent labeling accuracy,has,84.1 % ),high intent labeling accuracy has 84.1 % ),0.5408416390419006
translation,9,97,results,outperforms,has,other baseline systems,outperforms has other baseline systems,0.5852217674255371
translation,9,97,results,results,find that,our proposed auto-dialabel,results find that our proposed auto-dialabel,0.6627510786056519
translation,10,115,ablation-analysis,both structured views ( topic view + stage view ),increased,rouge scores,both structured views ( topic view + stage view ) increased rouge scores,0.6541585326194763
translation,10,115,ablation-analysis,ablation analysis,utilizing,both structured views ( topic view + stage view ),ablation analysis utilizing both structured views ( topic view + stage view ),0.6784874796867371
translation,10,159,ablation-analysis,percentage,of,24 %,percentage of 24 %,0.6158798336982727
translation,10,159,ablation-analysis,24 %,with,worst rouge - 2,24 % with worst rouge - 2,0.67017662525177
translation,10,159,ablation-analysis,improper gendered pronouns,seemed to,severely decrease,improper gendered pronouns seemed to severely decrease,0.6615813970565796
translation,10,159,ablation-analysis,incorrect reasoning,has,percentage,incorrect reasoning has percentage,0.5687409043312073
translation,10,159,ablation-analysis,ablation analysis,has,incorrect reasoning,ablation analysis has incorrect reasoning,0.5654608607292175
translation,10,168,ablation-analysis,high correlations,between,role & language change,high correlations between role & language change,0.6136361360549927
translation,10,168,ablation-analysis,high correlations,between,referral & coreference,high correlations between referral & coreference,0.6363381743431091
translation,10,168,ablation-analysis,high correlations,between,incorrect reasoning,high correlations between incorrect reasoning,0.592705488204956
translation,10,168,ablation-analysis,incorrect reasoning,indicated,interactions,incorrect reasoning indicated interactions,0.7212850451469421
translation,10,168,ablation-analysis,between multiple participants,with,frequent co-references,between multiple participants with frequent co-references,0.6177775263786316
translation,10,168,ablation-analysis,role & language change,has,referral & coreference,role & language change has referral & coreference,0.5636692047119141
translation,10,168,ablation-analysis,interactions,has,between multiple participants,interactions has between multiple participants,0.5680303573608398
translation,10,168,ablation-analysis,ablation analysis,has,high correlations,ablation analysis has high correlations,0.5337845087051392
translation,10,94,baselines,pointer generator,added,separators,pointer generator added separators,0.6532207727432251
translation,10,94,baselines,separators,between,each utterance,separators between each utterance,0.649141788482666
translation,10,94,baselines,separators,used it as,input,separators used it as input,0.5866089463233948
translation,10,94,baselines,input,for,pointer generator model,input for pointer generator model,0.5846856832504272
translation,10,95,baselines,dynamicconv + gpt -2,/,news,dynamicconv + gpt -2 / news,0.6530939340591431
translation,10,95,baselines,gpt - 2,to initialize,token embeddings,gpt - 2 to initialize token embeddings,0.6983269453048706
translation,10,95,baselines,dynamicconv + gpt -2,has,news,dynamicconv + gpt -2 has news,0.6560547351837158
translation,10,97,baselines,fast abs rl,selects,salient sentences,fast abs rl selects salient sentences,0.7289263010025024
translation,10,97,baselines,fast abs rl,rewrites,abstractively,fast abs rl rewrites abstractively,0.7257165312767029
translation,10,97,baselines,abstractively,via,sentence - level policy gradient methods,abstractively via sentence - level policy gradient methods,0.5974616408348083
translation,10,97,baselines,baselines,has,fast abs rl,baselines has fast abs rl,0.6020229458808899
translation,10,104,baselines,multi-view bart,experimented with,different view combinations,multi-view bart experimented with different view combinations,0.7643900513648987
translation,10,104,baselines,best generic view - global view,combined with,two structured views ( stage and topic view ),best generic view - global view combined with two structured views ( stage and topic view ),0.6830887198448181
translation,10,104,baselines,different view combinations,has,best generic view - global view,different view combinations has best generic view - global view,0.5694082379341125
translation,10,104,baselines,baselines,has,multi-view bart,baselines has multi-view bart,0.5637631416320801
translation,10,102,hyperparameters,topic view,via,c99,topic view via c99,0.7300269603729248
translation,10,102,hyperparameters,topic view,set,window size,topic view set window size,0.6361809968948364
translation,10,102,hyperparameters,topic view,set,std coefficient,topic view set std coefficient,0.6200399398803711
translation,10,102,hyperparameters,window size,has,4,window size has 4,0.6379761099815369
translation,10,102,hyperparameters,std coefficient,has,1,std coefficient has 1,0.5541843175888062
translation,10,102,hyperparameters,hyperparameters,For extracting,topic view,hyperparameters For extracting topic view,0.6925475597381592
translation,10,103,hyperparameters,stage view,set,number of hidden states,stage view set number of hidden states,0.6652286052703857
translation,10,103,hyperparameters,number of hidden states,in,hmm,number of hidden states in hmm,0.5271549820899963
translation,10,103,hyperparameters,4,in,hmm,4 in hmm,0.5868125557899475
translation,10,103,hyperparameters,number of hidden states,has,4,number of hidden states has 4,0.5674239993095398
translation,10,103,hyperparameters,hyperparameters,For extracting,stage view,hyperparameters For extracting stage view,0.7202798128128052
translation,10,106,hyperparameters,one- layer lstm,for,encoding sections,one- layer lstm for encoding sections,0.617424488067627
translation,10,106,hyperparameters,hyperparameters,used,one- layer lstm,hyperparameters used one- layer lstm,0.5821813941001892
translation,10,107,hyperparameters,learning rate,for,section encoder and multi-view attention,learning rate for section encoder and multi-view attention,0.5857412219047546
translation,10,107,hyperparameters,section encoder and multi-view attention,set,3e - 3,section encoder and multi-view attention set 3e - 3,0.6711081862449646
translation,10,107,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,10,108,hyperparameters,temperature t,was,0.2,temperature t was 0.2,0.5578066110610962
translation,10,108,hyperparameters,hyperparameters,has,temperature t,hyperparameters has temperature t,0.5064308643341064
translation,10,109,hyperparameters,beam search size,during,inference,beam search size during inference,0.6951203942298889
translation,10,109,hyperparameters,inference,for,all the models,inference for all the models,0.6185235381126404
translation,10,109,hyperparameters,all the models,was,4,all the models was 4,0.6480462551116943
translation,10,109,hyperparameters,hyperparameters,has,beam search size,hyperparameters has beam search size,0.5214741826057434
translation,10,6,model,different views,to represent,conversations,different views to represent conversations,0.7012431025505066
translation,10,6,model,multi-view decoder,to incorporate,different views,multi-view decoder to incorporate different views,0.6907024383544922
translation,10,6,model,different views,to generate,dialogue summaries,different views to generate dialogue summaries,0.6642111539840698
translation,10,6,model,structures,has,of unstructured daily chats,structures has of unstructured daily chats,0.5557487607002258
translation,10,6,model,model,proposes,multi-view sequence - tosequence model,model proposes multi-view sequence - tosequence model,0.7027788758277893
translation,10,44,model,diverse conversational structures,including,topic segments,diverse conversational structures including topic segments,0.6868729591369629
translation,10,44,model,diverse conversational structures,including,conversational stages,diverse conversational structures including conversational stages,0.6825288534164429
translation,10,44,model,diverse conversational structures,including,dialogue overview,diverse conversational structures including dialogue overview,0.6776241064071655
translation,10,44,model,diverse conversational structures,including,utterances,diverse conversational structures including utterances,0.6568133234977722
translation,10,44,model,diverse conversational structures,to design,multiview model,diverse conversational structures to design multiview model,0.6235455870628357
translation,10,44,model,utterances,to design,multiview model,utterances to design multiview model,0.6203390955924988
translation,10,44,model,multiview model,for,dialogue summarization,multiview model for dialogue summarization,0.5467069149017334
translation,10,44,model,model,leverage,diverse conversational structures,model leverage diverse conversational structures,0.6914240717887878
translation,10,73,model,our base encoders and decoders,with,transformer based pre-trained model,our base encoders and decoders with transformer based pre-trained model,0.6410094499588013
translation,10,113,results,conversations,into,blocks,conversations into blocks,0.657062292098999
translation,10,113,results,blocks,from,structured views ( stage view and topic view ),blocks from structured views ( stage view and topic view ),0.5934021472930908
translation,10,113,results,blocks,boosted,performance,blocks boosted performance,0.7242988348007202
translation,10,114,results,any performance boost,when combining,generic global view,any performance boost when combining generic global view,0.6947707533836365
translation,10,114,results,generic global view,with,topic or conversational stage views,generic global view with topic or conversational stage views,0.6184651851654053
translation,10,114,results,results,not see,any performance boost,results not see any performance boost,0.658367395401001
translation,10,118,results,gains,from,multi-view bart ( topic + stage ),gains from multi-view bart ( topic + stage ),0.605634331703186
translation,10,118,results,multi-view bart ( topic + stage ),mainly from,precision scores,multi-view bart ( topic + stage ) mainly from precision scores,0.595475971698761
translation,10,118,results,multi-view bart ( topic + stage ),mainly from,recall scores,multi-view bart ( topic + stage ) mainly from recall scores,0.6119983196258545
translation,10,118,results,precision scores,while,recall scores,precision scores while recall scores,0.5591347217559814
translation,10,118,results,recall scores,kept,comparable,recall scores kept comparable,0.6899023652076721
translation,10,127,results,multi-view model,achieved,highest human annotation scores,multi-view model achieved highest human annotation scores,0.662381649017334
translation,10,127,results,highest human annotation scores,than,generic ( discrete or global ) view,highest human annotation scores than generic ( discrete or global ) view,0.540346622467041
translation,11,136,baselines,seq2seq + att,is,standard seq2seq model,seq2seq + att is standard seq2seq model,0.5914813876152039
translation,11,136,baselines,standard seq2seq model,with,attention mechanism,standard seq2seq model with attention mechanism,0.5979354381561279
translation,11,136,baselines,hred,uses,hierarchical encoder-decoder framework,hred uses hierarchical encoder-decoder framework,0.572882890701294
translation,11,136,baselines,hierarchical encoder-decoder framework,to model,all the context utterances,hierarchical encoder-decoder framework to model all the context utterances,0.6787477135658264
translation,11,136,baselines,hred +hd,augments,hred,hred +hd augments hred,0.6573761701583862
translation,11,136,baselines,hred,with,historical dialogues,hred with historical dialogues,0.6456711292266846
translation,11,138,baselines,recosa,uses,self-attention mechanism,recosa uses self-attention mechanism,0.6212196946144104
translation,11,138,baselines,self-attention mechanism,to measure,relevance,self-attention mechanism to measure relevance,0.7387139797210693
translation,11,138,baselines,relevance,between,response and each context,relevance between response and each context,0.661154568195343
translation,11,138,baselines,baselines,has,recosa,baselines has recosa,0.6309071183204651
translation,11,143,baselines,recosa,is,  state- ofthe - art   method,recosa is   state- ofthe - art   method,0.587506115436554
translation,11,143,baselines,recosa,performs,better,recosa performs better,0.6991414427757263
translation,11,143,baselines,better,than,traditional   seq2seq + att   and   hred,better than traditional   seq2seq + att   and   hred,0.6143314242362976
translation,11,143,baselines,baselines,has,recosa,baselines has recosa,0.6309071183204651
translation,11,134,experimental-setup,methods,run on,server,methods run on server,0.5236797332763672
translation,11,134,experimental-setup,server,configured with,tesla v100 gpu,server configured with tesla v100 gpu,0.6697618961334229
translation,11,134,experimental-setup,server,configured with,2 cpu,server configured with 2 cpu,0.7235422730445862
translation,11,134,experimental-setup,server,configured with,32g memory,server configured with 32g memory,0.7086019515991211
translation,11,134,experimental-setup,experimental setup,run on,server,experimental setup run on server,0.7603636980056763
translation,11,134,experimental-setup,experimental setup,has,methods,experimental setup has methods,0.44329318404197693
translation,11,5,model,novel and extensible dialogue generation method,by leveraging,sellers ' historical dialogue information,novel and extensible dialogue generation method by leveraging sellers ' historical dialogue information,0.6922982931137085
translation,11,5,model,model,propose,novel and extensible dialogue generation method,model propose novel and extensible dialogue generation method,0.661051332950592
translation,11,6,model,proposed model,capable of detecting,most related responses,proposed model capable of detecting most related responses,0.718133270740509
translation,11,6,model,most related responses,from,sellers ' historical dialogues,most related responses from sellers ' historical dialogues,0.5742365717887878
translation,11,6,model,most related responses,further enhance,current dialogue generation quality,most related responses further enhance current dialogue generation quality,0.5755515098571777
translation,11,6,model,historical dialogue representation learning,has,proposed model,historical dialogue representation learning has proposed model,0.5438882112503052
translation,11,6,model,historical dialogue selection mechanism,has,proposed model,historical dialogue selection mechanism has proposed model,0.538077175617218
translation,11,6,model,model,utilizing,historical dialogue representation learning,model utilizing historical dialogue representation learning,0.632050633430481
translation,11,6,model,model,utilizing,historical dialogue selection mechanism,model utilizing historical dialogue selection mechanism,0.6592322587966919
translation,11,56,model,novel and extensible model,integrates,sellers ' historical dialogues,novel and extensible model integrates sellers ' historical dialogues,0.6224510669708252
translation,11,56,model,novel and extensible model,avoids,interference,novel and extensible model avoids interference,0.6141267418861389
translation,11,56,model,sellers ' historical dialogues,into,multi-turn dialogue generating process,sellers ' historical dialogues into multi-turn dialogue generating process,0.5328821539878845
translation,11,56,model,interference,from,background noise,interference from background noise,0.5786483883857727
translation,11,56,model,model,propose,novel and extensible model,model propose novel and extensible model,0.6453076004981995
translation,11,189,model,generation module and the copy module,contribute differently to,generation,generation module and the copy module contribute differently to generation,0.6357112526893616
translation,11,189,model,gate,has,generation module and the copy module,gate has generation module and the copy module,0.5865523815155029
translation,11,189,model,model,has,gate,model has gate,0.6086838841438293
translation,11,145,results,  recosa + hd   and   hred + hd  ,achieve,further improvements,  recosa + hd   and   hred + hd   achieve further improvements,0.6504296064376831
translation,11,145,results,further improvements,on,all the metrics,further improvements on all the metrics,0.5096616744995117
translation,11,145,results,results,observed that,  recosa + hd   and   hred + hd  ,results observed that   recosa + hd   and   hred + hd  ,0.675876259803772
translation,11,147,results,our model,performs,better,our model performs better,0.6546649932861328
translation,11,147,results,better,than,  recosa + hd   and   hred + hd  ,better than   recosa + hd   and   hred + hd  ,0.6215041875839233
translation,11,147,results,results,has,our model,results has our model,0.5871725678443909
translation,11,151,results,our model,with,50 % historical dialogues,our model with 50 % historical dialogues,0.6606380343437195
translation,11,151,results,our model,performs,better,our model performs better,0.6546649932861328
translation,11,151,results,our model,slightly worse than,our model,our model slightly worse than our model,0.673923134803772
translation,11,151,results,better,than,recosa,better than recosa,0.6640336513519287
translation,11,151,results,recosa,on,nearly all the metrics,recosa on nearly all the metrics,0.5652307868003845
translation,11,151,results,our model,trained on,full historical dialogues,our model trained on full historical dialogues,0.7255504131317139
translation,11,151,results,results,show that,our model,results show that our model,0.5436914563179016
translation,11,178,results,models,using,historical dialogues,models using historical dialogues,0.668159008026123
translation,11,178,results,historical dialogues,usually generate,more high-quality responses,historical dialogues usually generate more high-quality responses,0.6837171316146851
translation,11,178,results,more high-quality responses,than,other competitors,more high-quality responses than other competitors,0.6123205423355103
translation,11,178,results,results,observe that,models,results observe that models,0.5942480564117432
translation,11,179,results,our model,obtains,highest weighted score,our model obtains highest weighted score,0.6263725161552429
translation,11,179,results,highest weighted score,among,all the methods,highest weighted score among all the methods,0.5710344910621643
translation,11,179,results,results,has,our model,results has our model,0.5871725678443909
translation,11,180,results,historical dialogues,helps to generate,high- quality responses,historical dialogues helps to generate high- quality responses,0.7195920348167419
translation,11,180,results,high- quality responses,are,more consistent,high- quality responses are more consistent,0.550689160823822
translation,11,180,results,more consistent,with,sellers ' real responses,more consistent with sellers ' real responses,0.5959554314613342
translation,11,180,results,sellers ' real responses,in,customer service scenario,sellers ' real responses in customer service scenario,0.5420454740524292
translation,11,180,results,results,proves,historical dialogues,results proves historical dialogues,0.6419365406036377
translation,11,180,results,results,using,historical dialogues,results using historical dialogues,0.6065602898597717
translation,11,187,results,copy,find that,copy mechanism,copy find that copy mechanism,0.672648012638092
translation,11,187,results,results,find that,copy mechanism,results find that copy mechanism,0.699383556842804
translation,11,187,results,results,has,copy,results has copy,0.49794527888298035
translation,11,197,results,re,-,cosa + hd,re - cosa + hd,0.6646849513053894
translation,11,197,results,our approach,generate,much better responses,our approach generate much better responses,0.6336652636528015
translation,11,197,results,much better responses,by using,external information,much better responses by using external information,0.6160085797309875
translation,11,197,results,external information,from,historical dialogue,external information from historical dialogue,0.5416554808616638
translation,11,197,results,external information,contains,similar responses,external information contains similar responses,0.5927587151527405
translation,11,197,results,similar responses,to,ground truth,similar responses to ground truth,0.5651299953460693
translation,11,197,results,re,has,cosa + hd,re has cosa + hd,0.6818820834159851
translation,11,197,results,results,has,re,results has re,0.6227100491523743
translation,12,191,ablation-analysis,vhred ( - k ),gets,worst ppl / bleu / rouge,vhred ( - k ) gets worst ppl / bleu / rouge,0.6181223392486572
translation,12,191,ablation-analysis,rnn - based models,has,vhred ( - k ),rnn - based models has vhred ( - k ),0.6159458160400391
translation,12,191,ablation-analysis,ablation analysis,In,rnn - based models,ablation analysis In rnn - based models,0.5475422143936157
translation,12,205,ablation-analysis,ablation analysis,has,ablation study,ablation analysis has ablation study,0.5261484980583191
translation,12,207,ablation-analysis,all the automatic evaluation indexes,has,significantly reduce,all the automatic evaluation indexes has significantly reduce,0.5334296822547913
translation,12,207,ablation-analysis,ablation analysis,see that,all the automatic evaluation indexes,ablation analysis see that all the automatic evaluation indexes,0.6216440796852112
translation,12,209,ablation-analysis,performance,is,declining,performance is declining,0.629026472568512
translation,12,209,ablation-analysis,declining,indicates that,guiding process,declining indicates that guiding process,0.6755086779594421
translation,12,209,ablation-analysis,guiding process,is,useful,guiding process is useful,0.5572305917739868
translation,12,209,ablation-analysis,ablation analysis,has,performance,ablation analysis has performance,0.5053174495697021
translation,12,210,ablation-analysis,significant tests,on,ppl / bleu / ku-2/qku - 2,significant tests on ppl / bleu / ku-2/qku - 2,0.5524969100952148
translation,12,210,ablation-analysis,ppl / bleu / ku-2/qku - 2,reveal,effectiveness,ppl / bleu / ku-2/qku - 2 reveal effectiveness,0.6978489756584167
translation,12,210,ablation-analysis,effectiveness,of,each component,effectiveness of each component,0.5642370581626892
translation,12,210,ablation-analysis,ablation analysis,has,significant tests,ablation analysis has significant tests,0.5460548996925354
translation,12,176,experimental-setup,opennmt - py,as,code framework,opennmt - py as code framework,0.5497466921806335
translation,12,177,experimental-setup,"pre-trained 300 dimension word embedding ( mikolov et al. , 2013 )",shared by,"dialogue , document , and generated responses","pre-trained 300 dimension word embedding ( mikolov et al. , 2013 ) shared by dialogue , document , and generated responses",0.6430168747901917
translation,12,177,experimental-setup,dimension,of,hidden size,dimension of hidden size,0.5720718502998352
translation,12,177,experimental-setup,hidden size,is,300,hidden size is 300,0.6228809952735901
translation,12,179,experimental-setup,layers,of,both encoder and decoder,layers of both encoder and decoder,0.6137396693229675
translation,12,179,experimental-setup,both encoder and decoder,set to,3,both encoder and decoder set to 3,0.7234280705451965
translation,12,179,experimental-setup,number of heads,in,multi-head attention,number of heads in multi-head attention,0.5287878513336182
translation,12,179,experimental-setup,multi-head attention,is,8,multi-head attention is 8,0.5655282139778137
translation,12,179,experimental-setup,filter size,is,2048,filter size is 2048,0.6036906242370605
translation,12,179,experimental-setup,transformer - based models,has,layers,transformer - based models has layers,0.570838451385498
translation,12,179,experimental-setup,transformer - based models,has,number of heads,transformer - based models has number of heads,0.5637831091880798
translation,12,179,experimental-setup,transformer - based models,has,filter size,transformer - based models has filter size,0.5442445278167725
translation,12,179,experimental-setup,experimental setup,For,transformer - based models,experimental setup For transformer - based models,0.6025317907333374
translation,12,180,experimental-setup,"adam ( ? = 0.001 , ? 1 = 0.9 , ? 2 = 0.999 , and = 10 ?8 )",for,optimization,"adam ( ? = 0.001 , ? 1 = 0.9 , ? 2 = 0.999 , and = 10 ?8 ) for optimization",0.6186057925224304
translation,12,181,experimental-setup,beam size,set to,5,beam size set to 5,0.7865644693374634
translation,12,181,experimental-setup,5,in,decoder,5 in decoder,0.5816633105278015
translation,12,181,experimental-setup,experimental setup,has,beam size,experimental setup has beam size,0.46948209404945374
translation,12,182,experimental-setup,words,of,document,words of document,0.6431632041931152
translation,12,182,experimental-setup,words,to,800,words to 800,0.6497842073440552
translation,12,182,experimental-setup,document,to,800,document to 800,0.620713472366333
translation,12,182,experimental-setup,dialogue utterance,to,40,dialogue utterance to 40,0.6224117875099182
translation,12,182,experimental-setup,experimental setup,truncate,words,experimental setup truncate words,0.6533138155937195
translation,12,7,model,compare aggregate transformer ( cat ),to jointly denoise,dialogue context,compare aggregate transformer ( cat ) to jointly denoise dialogue context,0.723714292049408
translation,12,7,model,compare aggregate transformer ( cat ),aggregate,document information,compare aggregate transformer ( cat ) aggregate document information,0.7848592400550842
translation,12,7,model,document information,for,response generation,document information for response generation,0.6083463430404663
translation,12,7,model,model,propose,compare aggregate transformer ( cat ),model propose compare aggregate transformer ( cat ),0.6613950729370117
translation,12,47,model,"novel transformerbased ( vaswani et al. , 2017 ) model",for understanding,dialogues,"novel transformerbased ( vaswani et al. , 2017 ) model for understanding dialogues",0.7181084752082825
translation,12,47,model,"novel transformerbased ( vaswani et al. , 2017 ) model",generate,informative responses,"novel transformerbased ( vaswani et al. , 2017 ) model generate informative responses",0.642839789390564
translation,12,47,model,informative responses,in,dgd,informative responses in dgd,0.5378322005271912
translation,12,47,model,informative responses,named,compare aggregate transformer ( cat ),informative responses named compare aggregate transformer ( cat ),0.7323449850082397
translation,12,47,model,model,propose,"novel transformerbased ( vaswani et al. , 2017 ) model","model propose novel transformerbased ( vaswani et al. , 2017 ) model",0.6487916111946106
translation,12,178,model,3 - layer gru,applied for,encoder and decoder,3 - layer gru applied for encoder and decoder,0.6538421511650085
translation,12,178,model,rnn - based models,has,3 - layer bidirectional gru,rnn - based models has 3 - layer bidirectional gru,0.562399685382843
translation,12,178,model,model,For,rnn - based models,model For rnn - based models,0.6457261443138123
translation,12,193,results,vhred ( a ),gets,better ppl / bleu / rouge / ku / qku,vhred ( a ) gets better ppl / bleu / rouge / ku / qku,0.6572847962379456
translation,12,193,results,better ppl / bleu / rouge / ku / qku,than,vhred ( c ) model,better ppl / bleu / rouge / ku / qku than vhred ( c ) model,0.5889112949371338
translation,12,193,results,results,has,vhred ( a ),results has vhred ( a ),0.5797569751739502
translation,12,194,results,itdd model,gets,better ppl / bleu / rouge -l/ku / qku,itdd model gets better ppl / bleu / rouge -l/ku / qku,0.6208000779151917
translation,12,194,results,better ppl / bleu / rouge -l/ku / qku,than,t-dd model,better ppl / bleu / rouge -l/ku / qku than t-dd model,0.5793870091438293
translation,12,194,results,transformer- based models,has,itdd model,transformer- based models has itdd model,0.5741965174674988
translation,12,194,results,results,Among,transformer- based models,results Among transformer- based models,0.5771790146827698
translation,12,195,results,cat -edd and the cat - dd models,achieve,better performance,cat -edd and the cat - dd models achieve better performance,0.6427439451217651
translation,12,195,results,better performance,than,t-dd and the t-edd models,better performance than t-dd and the t-edd models,0.5761876702308655
translation,12,195,results,results,has,cat -edd and the cat - dd models,results has cat -edd and the cat - dd models,0.5092594027519226
translation,12,197,results,itdd model,on,all metrics,itdd model on all metrics,0.5235287547111511
translation,12,197,results,cat -edd model,has,outperforms,cat -edd model has outperforms,0.6253765225410461
translation,12,197,results,outperforms,has,itdd model,outperforms has itdd model,0.5960370302200317
translation,12,197,results,results,has,cat -edd model,results has cat -edd model,0.5368790626525879
translation,12,198,results,cat -edd,does not perform,as good,cat -edd does not perform as good,0.760928750038147
translation,12,198,results,as good,as,cat - dd,as good as cat - dd,0.7482258677482605
translation,12,198,results,results,has,cat -edd,results has cat -edd,0.5621722340583801
translation,12,199,results,transformer - based models,get,better performance,transformer - based models get better performance,0.5989816784858704
translation,12,199,results,better performance,on,ppl / bleu / rouge,better performance on ppl / bleu / rouge,0.5272037386894226
translation,12,199,results,rnn - based models,has,transformer - based models,rnn - based models has transformer - based models,0.5836132168769836
translation,12,199,results,results,Comparing with,rnn - based models,results Comparing with rnn - based models,0.6231421232223511
translation,12,201,results,vhred ( c ) and the vhred ( a ),get,better ku and worse qku,vhred ( c ) and the vhred ( a ) get better ku and worse qku,0.5797656774520874
translation,12,201,results,better ku and worse qku,than,transformer - based models,better ku and worse qku than transformer - based models,0.5756974816322327
translation,12,201,results,results,has,vhred ( c ) and the vhred ( a ),results has vhred ( c ) and the vhred ( a ),0.5766572952270508
translation,12,204,results,our model,shows,more advantages,our model shows more advantages,0.679620623588562
translation,12,204,results,more advantages,in,datasets,more advantages in datasets,0.5325740575790405
translation,12,204,results,more advantages,with,more topic transfer,more advantages with more topic transfer,0.644737958908081
translation,12,204,results,results,has,our model,results has our model,0.5871725678443909
translation,12,225,results,relevance score,higher than,sampled data,relevance score higher than sampled data,0.6774753928184509
translation,12,225,results,reduced test set and the validation set,has,relevance score,reduced test set and the validation set has relevance score,0.5822667479515076
translation,12,225,results,results,on,reduced test set and the validation set,results on reduced test set and the validation set,0.5772563219070435
translation,13,199,ablation-analysis,to 3 m,with,p neg soft switch,to 3 m with p neg soft switch,0.6943537592887878
translation,13,199,ablation-analysis,p neg soft switch,improves,negation f1,p neg soft switch improves negation f1,0.7310711741447449
translation,13,199,ablation-analysis,2m - base,has,to 3 m,2m - base has to 3 m,0.6362283229827881
translation,13,199,ablation-analysis,negation f1,has,76.9 vs 70.1 ),negation f1 has 76.9 vs 70.1 ),0.5529288053512573
translation,13,199,ablation-analysis,ablation analysis,extending,2m - base,ablation analysis extending 2m - base,0.7462911009788513
translation,13,207,ablation-analysis,local snippets,observe that,increased copying ability,local snippets observe that increased copying ability,0.5682940483093262
translation,13,207,ablation-analysis,increased copying ability,adding,concept attention,increased copying ability adding concept attention,0.6859301924705505
translation,13,207,ablation-analysis,concept attention,reduce,performance,concept attention reduce performance,0.6818697452545166
translation,13,207,ablation-analysis,ablation analysis,On,local snippets,ablation analysis On local snippets,0.5705441236495972
translation,13,158,experimental-setup,vocabulary size,of,50 k,vocabulary size of 50 k,0.6240697503089905
translation,13,158,experimental-setup,50 k,with,128 dimensional embeddings,50 k with 128 dimensional embeddings,0.6139382719993591
translation,13,159,experimental-setup,training parameters,followed,adagrad,training parameters followed adagrad,0.6189442276954651
translation,13,159,experimental-setup,training parameters,with,learning rate,training parameters with learning rate,0.6025553345680237
translation,13,159,experimental-setup,training parameters,with,adagrad,training parameters with adagrad,0.5727906227111816
translation,13,159,experimental-setup,learning rate,of,0.15,learning rate of 0.15,0.6145498156547546
translation,13,159,experimental-setup,adagrad,as,optimizer,adagrad as optimizer,0.5433330535888672
translation,13,159,experimental-setup,experimental setup,has,training parameters,experimental setup has training parameters,0.49305152893066406
translation,13,161,experimental-setup,models,pretrained on,cnn - daily mail corpus,models pretrained on cnn - daily mail corpus,0.7469791769981384
translation,13,161,experimental-setup,models,finetuned on,conversational data,models finetuned on conversational data,0.6608467102050781
translation,13,161,experimental-setup,conversational data,from,our in- house chat - based telehealth platform,conversational data from our in- house chat - based telehealth platform,0.5733591914176941
translation,13,161,experimental-setup,experimental setup,pretrained on,cnn - daily mail corpus,experimental setup pretrained on cnn - daily mail corpus,0.7690479159355164
translation,13,161,experimental-setup,experimental setup,finetuned on,conversational data,experimental setup finetuned on conversational data,0.6838071346282959
translation,13,161,experimental-setup,experimental setup,has,models,experimental setup has models,0.5060054659843445
translation,13,162,experimental-setup,pretraining,took,approximately 2 days,pretraining took approximately 2 days,0.6308430433273315
translation,13,162,experimental-setup,approximately 2 days,on,single nvidia titan xp gpu,approximately 2 days on single nvidia titan xp gpu,0.5199660062789917
translation,13,162,experimental-setup,finetuning,took,under 2 hours,finetuning took under 2 hours,0.692064106464386
translation,13,162,experimental-setup,experimental setup,has,pretraining,experimental setup has pretraining,0.5295986533164978
translation,13,36,experiments,our own dataset,using,conversations,our own dataset using conversations,0.7466475367546082
translation,13,36,experiments,conversations,from,telemedicine platform,conversations from telemedicine platform,0.6270477175712585
translation,13,36,experiments,conversations,obtain,reference summaries,conversations obtain reference summaries,0.5848163962364197
translation,13,36,experiments,reference summaries,from,healthcare professionals,reference summaries from healthcare professionals,0.554607629776001
translation,13,7,model,variation,of,pointer generator network,variation of pointer generator network,0.5983049869537354
translation,13,7,model,variation,introduce,penalty,variation introduce penalty,0.6428331136703491
translation,13,7,model,variation,explicitly model,negations,variation explicitly model negations,0.7918868660926819
translation,13,7,model,pointer generator network,introduce,penalty,pointer generator network introduce penalty,0.6426807641983032
translation,13,7,model,pointer generator network,explicitly model,negations,pointer generator network explicitly model negations,0.7809565663337708
translation,13,7,model,penalty,on,generator distribution,penalty on generator distribution,0.5372071862220764
translation,13,41,model,baseline pointer generator network,by introducing,penalty,baseline pointer generator network by introducing penalty,0.6483853459358215
translation,13,41,model,penalty,to,generator distribution,penalty to generator distribution,0.5427328944206238
translation,13,41,model,generator distribution,to guarantee,network,generator distribution to guarantee network,0.6030209064483643
translation,13,41,model,network,to,extractive summarization,network to extractive summarization,0.5453566908836365
translation,13,41,model,network,has,defaults,network has defaults,0.6269239187240601
translation,13,41,model,model,extend,baseline pointer generator network,model extend baseline pointer generator network,0.6599330902099609
translation,13,8,results,important properties,of,medical conversations,important properties of medical conversations,0.5478575229644775
translation,13,8,results,medical conversations,such as,medical knowledge,medical conversations such as medical knowledge,0.6187164783477783
translation,13,8,results,medical knowledge,coming from,standardized medical ontologies,medical knowledge coming from standardized medical ontologies,0.5529365539550781
translation,13,8,results,standardized medical ontologies,has,better,standardized medical ontologies has better,0.5699430108070374
translation,13,8,results,results,captures,important properties,results captures important properties,0.7307708859443665
translation,13,170,results,2m - pgen,generates,better summaries,2m - pgen generates better summaries,0.7052656412124634
translation,13,200,results,3m - neg,by incorporating,negation attention,3m - neg by incorporating negation attention,0.6883375644683838
translation,13,200,results,negation attention,see,even larger improvement,negation attention see even larger improvement,0.5716935396194458
translation,13,200,results,even larger improvement,in,negation f1 ( 81.5 ),even larger improvement in negation f1 ( 81.5 ),0.5316166877746582
translation,13,200,results,results,extending,3m - neg,results extending 3m - neg,0.5950211882591248
translation,14,7,experiments,human judges,annotate for,each entity,human judges annotate for each entity,0.7533263564109802
translation,14,7,experiments,each entity,in,conversation,each entity in conversation,0.5427302718162537
translation,14,6,model,cost-efficient and robust evaluation framework,replaces,human-bot conversations,cost-efficient and robust evaluation framework replaces human-bot conversations,0.6769559383392334
translation,14,6,model,human-bot conversations,conversations between,bots,human-bot conversations conversations between bots,0.7725889682769775
translation,14,6,model,spot the bot,has,cost-efficient and robust evaluation framework,spot the bot has cost-efficient and robust evaluation framework,0.5855414867401123
translation,14,6,model,model,introduce,spot the bot,model introduce spot the bot,0.6279086470603943
translation,14,25,model,cost-efficient evaluation methodology,to,rank,cost-efficient evaluation methodology to rank,0.5537351369857788
translation,14,25,model,several bots,with regard to,ability to disguise as humans,several bots with regard to ability to disguise as humans,0.6627441644668579
translation,14,25,model,spot the bot framework,has,cost-efficient evaluation methodology,spot the bot framework has cost-efficient evaluation methodology,0.595716118812561
translation,14,25,model,rank,has,several bots,rank has several bots,0.5838148593902588
translation,14,25,model,model,present,spot the bot framework,model present spot the bot framework,0.6844210028648376
translation,15,117,hyperparameters,policies,trained using,gp - sarsa algorithm,policies trained using gp - sarsa algorithm,0.7458562254905701
translation,15,117,hyperparameters,policies,trained using,summary action space,policies trained using summary action space,0.6496965289115906
translation,15,117,hyperparameters,summary action space,of,rl policy,summary action space of rl policy,0.5132004618644714
translation,15,117,hyperparameters,rl policy,contains,20 actions,rl policy contains 20 actions,0.63883376121521
translation,15,117,hyperparameters,hyperparameters,has,policies,hyperparameters has policies,0.54888516664505
translation,15,8,model,on-line learning framework,whereby,dialogue policy,on-line learning framework whereby dialogue policy,0.5669564604759216
translation,15,8,model,dialogue policy,jointly trained alongside,reward model,dialogue policy jointly trained alongside reward model,0.7173914909362793
translation,15,8,model,reward model,via,active learning,reward model via active learning,0.6548708081245422
translation,15,8,model,active learning,with,gaussian process model,active learning with gaussian process model,0.6185464262962341
translation,15,8,model,model,propose,on-line learning framework,model propose on-line learning framework,0.6558499932289124
translation,15,9,model,gaussian process,operates on,continuous space dialogue representation,gaussian process operates on continuous space dialogue representation,0.7313885688781738
translation,15,9,model,continuous space dialogue representation,generated in,unsupervised fashion,continuous space dialogue representation generated in unsupervised fashion,0.6500550508499146
translation,15,9,model,unsupervised fashion,using,recurrent neural network encoder-decoder,unsupervised fashion using recurrent neural network encoder-decoder,0.6989945769309998
translation,15,9,model,model,has,gaussian process,model has gaussian process,0.5578736066818237
translation,15,32,model,limit,has,requests for feedback,limit has requests for feedback,0.608135461807251
translation,15,32,model,feedback,has,would be useful,feedback has would be useful,0.5652287006378174
translation,15,32,model,model,has,active learning,model has active learning,0.5605125427246094
translation,15,34,model,gpc,operates on,fixed - length observation space,gpc operates on fixed - length observation space,0.7222267985343933
translation,15,34,model,gpc,operates on,dialogues,gpc operates on dialogues,0.750784695148468
translation,15,34,model,recurrent neural network ( rnn ) - based embedding function,to provide,fixed - length dialogue representations,recurrent neural network ( rnn ) - based embedding function to provide fixed - length dialogue representations,0.6214832663536072
translation,15,34,model,model,has,gpc,model has gpc,0.5847983956336975
translation,15,35,model,on - line,from,scratch,on - line from scratch,0.643681526184082
translation,15,35,model,reward estimator,has,on - line,reward estimator has on - line,0.519911527633667
translation,15,35,model,model,learns,dialogue policy,model learns dialogue policy,0.6959184408187866
translation,15,116,model,"buds belief state tracker ( thomson and young , 2010 )",factorises,dialogue state,"buds belief state tracker ( thomson and young , 2010 ) factorises dialogue state",0.6855106353759766
translation,15,116,model,dialogue state,using,dynamic bayesian network,dialogue state using dynamic bayesian network,0.6377812027931213
translation,15,116,model,template based natural language generator,to map,system semantic actions,template based natural language generator to map system semantic actions,0.677785336971283
translation,15,116,model,system semantic actions,into,natural language responses,system semantic actions into natural language responses,0.5354275703430176
translation,15,116,model,natural language responses,to,user,natural language responses to user,0.5458652973175049
translation,15,155,results,80 % subjective success rate,after,500 training dialogues,80 % subjective success rate after 500 training dialogues,0.6551660299301147
translation,15,155,results,80 % subjective success rate,approximately,500 training dialogues,80 % subjective success rate approximately 500 training dialogues,0.6563558578491211
translation,15,155,results,results,has,all four systems,results has all four systems,0.5440701842308044
translation,15,156,results,obj=subj system,is,relatively poor,obj=subj system is relatively poor,0.6081986427307129
translation,15,156,results,relatively poor,compared to,others,relatively poor compared to others,0.6949901580810547
translation,15,156,results,results,has,obj=subj system,results has obj=subj system,0.5781461596488953
translation,15,165,results,consistently performed better,than,subj system,consistently performed better than subj system,0.6096215844154358
translation,15,165,results,online gp system,has,consistently performed better,online gp system has consistently performed better,0.5904328227043152
translation,15,165,results,results,seen that,online gp system,results seen that online gp system,0.7394555807113647
translation,15,177,results,on- line gp system,was,significantly better,on- line gp system was significantly better,0.6183916330337524
translation,15,179,results,effectiveness,of,proposed reward model,effectiveness of proposed reward model,0.5651820302009583
translation,15,179,results,proposed reward model,for,policy learning,proposed reward model for policy learning,0.5944094657897949
translation,15,179,results,results,verify,effectiveness,results verify effectiveness,0.5863670110702515
translation,15,188,results,successful dialogues,accurately predicted by,proposed model,successful dialogues accurately predicted by proposed model,0.7290419936180115
translation,15,188,results,results,has,successful dialogues,results has successful dialogues,0.5453625917434692
translation,16,129,baselines,eapc,existence of,human teachers,eapc existence of human teachers,0.6646988987922668
translation,16,129,baselines,human teachers,during,learning process,human teachers during learning process,0.6186900734901428
translation,16,129,baselines,baselines,has,eapc,baselines has eapc,0.5795043706893921
translation,16,142,baselines,"rl - based agents ( dqn , dqfd , s 2 agent )",are,mlps,"rl - based agents ( dqn , dqfd , s 2 agent ) are mlps",0.5694670081138611
translation,16,142,baselines,mlps,with,tanh activations,mlps with tanh activations,0.6147250533103943
translation,16,142,baselines,baselines,has,"rl - based agents ( dqn , dqfd , s 2 agent )","baselines has rl - based agents ( dqn , dqfd , s 2 agent )",0.5500189661979675
translation,16,65,experiments,dialogue policy learning,as,markov decision process ( mdp ) problem,dialogue policy learning as markov decision process ( mdp ) problem,0.5135478377342224
translation,16,140,hyperparameters,imitation model agents,for,all domains,imitation model agents for all domains,0.5551106333732605
translation,16,140,hyperparameters,imitation model agents,are,single layer mlps,imitation model agents are single layer mlps,0.5648218989372253
translation,16,140,hyperparameters,all domains,are,single layer mlps,all domains are single layer mlps,0.5786609053611755
translation,16,140,hyperparameters,single layer mlps,with,50 hidden dimensions,single layer mlps with 50 hidden dimensions,0.6073245406150818
translation,16,140,hyperparameters,tanh,as,activation function,tanh as activation function,0.5730298757553101
translation,16,143,hyperparameters,one hidden layer,with,60 hidden nodes,one hidden layer with 60 hidden nodes,0.6518175601959229
translation,16,143,hyperparameters,policy network q,has,one hidden layer,policy network q has one hidden layer,0.5692607164382935
translation,16,143,hyperparameters,hyperparameters,has,policy network q,hyperparameters has policy network q,0.5460055470466614
translation,16,146,hyperparameters,discount factor,as,? = 0.9,discount factor as ? = 0.9,0.5590311288833618
translation,16,146,hyperparameters,hyperparameters,set,discount factor,hyperparameters set discount factor,0.5682007074356079
translation,16,150,hyperparameters,confidence factor c,used in,policy shaping,confidence factor c used in policy shaping,0.6719224452972412
translation,16,150,hyperparameters,policy shaping,set,0.7,policy shaping set 0.7,0.5921496748924255
translation,16,150,hyperparameters,hyperparameters,has,confidence factor c,hyperparameters has confidence factor c,0.5240919589996338
translation,16,206,hyperparameters,200 epochs,for,movie and movie - ext,200 epochs for movie and movie - ext,0.653461217880249
translation,16,7,model,from demonstrations,through,policy shaping,from demonstrations through policy shaping,0.7071458101272583
translation,16,7,model,from demonstrations,through,reward shaping,from demonstrations through reward shaping,0.6754533052444458
translation,16,7,model,dialogue policy,has,from demonstrations,dialogue policy has from demonstrations,0.6172690987586975
translation,16,7,model,model,efficiently learns,dialogue policy,model efficiently learns dialogue policy,0.7221710681915283
translation,16,8,model,imitation model,to distill,knowledge,imitation model to distill knowledge,0.627230703830719
translation,16,8,model,knowledge,from,demonstrations,knowledge from demonstrations,0.6052128076553345
translation,16,8,model,model,use,imitation model,model use imitation model,0.6238720417022705
translation,16,31,model,new strategy,of leveraging,human demonstrations,new strategy of leveraging human demonstrations,0.732487142086029
translation,16,31,model,human demonstrations,to learn,dialogue policy efficiently,human demonstrations to learn dialogue policy efficiently,0.6708924174308777
translation,16,31,model,model,propose,new strategy,model propose new strategy,0.7295149564743042
translation,16,32,model,dialogue agent,termed,s 2 agent,dialogue agent termed s 2 agent,0.6605712175369263
translation,16,32,model,dialogue agent,learns,dialogue policy,dialogue agent learns dialogue policy,0.6676167249679565
translation,16,32,model,dialogue policy,from,demonstrations,dialogue policy from demonstrations,0.5900998115539551
translation,16,32,model,demonstrations,trough,policy shaping,demonstrations trough policy shaping,0.7513100504875183
translation,16,32,model,demonstrations,trough,reward shaping,demonstrations trough reward shaping,0.7010429501533508
translation,16,32,model,model,has,dialogue agent,model has dialogue agent,0.577430009841919
translation,16,39,model,deep neural networks,which represent,state-action space,deep neural networks which represent state-action space,0.6471520662307739
translation,16,39,model,deep neural networks,distill,knowledge,deep neural networks distill knowledge,0.6568986177444458
translation,16,39,model,state-action space,with,function approximation,state-action space with function approximation,0.6300026774406433
translation,16,39,model,knowledge,from,human demonstrations,knowledge from human demonstrations,0.5750436186790466
translation,16,39,model,knowledge,to estimate,feedback,knowledge to estimate feedback,0.7331125736236572
translation,16,39,model,model,propose to use,deep neural networks,model propose to use deep neural networks,0.6921654343605042
translation,16,63,model,final action,based on,actions,final action based on actions,0.6678750514984131
translation,16,63,model,final action,based on,imitation model,final action based on imitation model,0.6492281556129456
translation,16,63,model,final action,Followed by,reward shaping module,final action Followed by reward shaping module,0.6608077883720398
translation,16,63,model,actions,from,policy model,actions from policy model,0.5526959300041199
translation,16,63,model,imitation model,attempting to generate,more reliable exploration trajectories,imitation model attempting to generate more reliable exploration trajectories,0.65021151304245
translation,16,63,model,imitation model,Followed by,reward shaping module,imitation model Followed by reward shaping module,0.6425604224205017
translation,16,63,model,reward shaping module,encourages,demonstration similar state-actions,reward shaping module encourages demonstration similar state-actions,0.6146875619888306
translation,16,63,model,demonstration similar state-actions,by providing,extra intrinsic reward signals,demonstration similar state-actions by providing extra intrinsic reward signals,0.6490322351455688
translation,16,63,model,model,reconciles,final action,model reconciles final action,0.6854929327964783
translation,16,128,model,teaching via example action with predicted critique ( eapc ),leverages,real-time human demonstrations,teaching via example action with predicted critique ( eapc ) leverages real-time human demonstrations,0.7402216792106628
translation,16,128,model,real-time human demonstrations,to improve,policy learning,real-time human demonstrations to improve policy learning,0.650803804397583
translation,16,128,model,model,has,teaching via example action with predicted critique ( eapc ),model has teaching via example action with predicted critique ( eapc ),0.6070259213447571
translation,16,141,model,im agent,used in,policy shaping,im agent used in policy shaping,0.7245452404022217
translation,16,141,model,policy shaping,to reconcile,policy,policy shaping to reconcile policy,0.6831626296043396
translation,16,141,model,model,has,im agent,model has im agent,0.5899158120155334
translation,16,145,model,- greedy,utilized for,policy exploration,- greedy utilized for policy exploration,0.6663894653320312
translation,16,145,model,model,has,- greedy,model has - greedy,0.6341179609298706
translation,16,112,results,reward,calculated from,reward shaping,reward calculated from reward shaping,0.5863514542579651
translation,16,112,results,reward shaping,is,more informative and demonstration guided,reward shaping is more informative and demonstration guided,0.5380694270133972
translation,16,112,results,more informative and demonstration guided,than,human-defined reward,more informative and demonstration guided than human-defined reward,0.5904814004898071
translation,16,112,results,human-defined reward,mitigates,reward sparsity issue,human-defined reward mitigates reward sparsity issue,0.7302554845809937
translation,16,112,results,results,has,reward,results has reward,0.4562392234802246
translation,16,163,results,dqn agent,performs,better,dqn agent performs better,0.6208275556564331
translation,16,163,results,better,than,im agent,better than im agent,0.6296066045761108
translation,16,163,results,results,has,dqn agent,results has dqn agent,0.5667219161987305
translation,16,172,results,s 2 agent w/o rs,learn,dialogue policy,s 2 agent w/o rs learn dialogue policy,0.6862821578979492
translation,16,172,results,much faster,than,all the baselines,much faster than all the baselines,0.5884608030319214
translation,16,172,results,dialogue policy,has,much faster,dialogue policy has much faster,0.5975189805030823
translation,16,172,results,results,has,s 2 agent w/o rs,results has s 2 agent w/o rs,0.49435657262802124
translation,16,173,results,movie domain,achieves,nearly a 60 % success rate,movie domain achieves nearly a 60 % success rate,0.6717632412910461
translation,16,173,results,nearly a 60 % success rate,using,only 20 epochs,nearly a 60 % success rate using only 20 epochs,0.6991612315177917
translation,16,173,results,results,In,movie domain,results In movie domain,0.5274842381477356
translation,16,174,results,second- best agent dqfd,achieves,20 % successful rate,second- best agent dqfd achieves 20 % successful rate,0.6872908473014832
translation,16,174,results,20 % successful rate,at,epoch 20,20 % successful rate at epoch 20,0.6170342564582825
translation,16,174,results,results,has,second- best agent dqfd,results has second- best agent dqfd,0.5819605588912964
translation,16,188,results,our proposed shaping mechanisms,improve,policy learning speed and quality,our proposed shaping mechanisms improve policy learning speed and quality,0.6683500409126282
translation,16,188,results,our proposed shaping mechanisms,robust to,number of demonstrations,our proposed shaping mechanisms robust to number of demonstrations,0.6610270738601685
translation,16,188,results,results,has,our proposed shaping mechanisms,results has our proposed shaping mechanisms,0.527495265007019
translation,16,189,results,s 2 agent,achieves,5 % higher success rate,s 2 agent achieves 5 % higher success rate,0.6836549043655396
translation,16,189,results,5 % higher success rate,than,dqfd and eapc,5 % higher success rate than dqfd and eapc,0.6086606979370117
translation,16,189,results,5 % higher success rate,than,10 %,5 % higher success rate than 10 %,0.5854107737541199
translation,16,189,results,dqfd and eapc,in,movie domain,dqfd and eapc in movie domain,0.529810905456543
translation,16,189,results,10 %,in,taxi domain,10 % in taxi domain,0.554362416267395
translation,16,189,results,demonstrations,has,s 2 agent,demonstrations has s 2 agent,0.5578609704971313
translation,16,190,results,gap,between,dqfd and s 2 agent,gap between dqfd and s 2 agent,0.6946544647216797
translation,16,190,results,dqfd and s 2 agent,becomes,larger,dqfd and s 2 agent becomes larger,0.7330694794654846
translation,16,190,results,dqfd and s 2 agent,showing that,policy mechanisms,dqfd and s 2 agent showing that policy mechanisms,0.6561980843544006
translation,16,190,results,policy mechanisms,benefit from,more human demonstrations available,policy mechanisms benefit from more human demonstrations available,0.6406754851341248
translation,16,190,results,number of demonstrations,has,increases,number of demonstrations has increases,0.6069941520690918
translation,16,190,results,increases,has,gap,increases has gap,0.6205227971076965
translation,16,199,results,quickly adapt,to,new environment,quickly adapt to new environment,0.5729422569274902
translation,16,199,results,both s 2 agent and s 2 agent w/o rs,has,quickly adapt,both s 2 agent and s 2 agent w/o rs has quickly adapt,0.5878950357437134
translation,16,199,results,outperform,has,im agent,outperform has im agent,0.6532416939735413
translation,16,199,results,results,see,both s 2 agent and s 2 agent w/o rs,results see both s 2 agent and s 2 agent w/o rs,0.5833587050437927
translation,16,199,results,results,has,both s 2 agent and s 2 agent w/o rs,results has both s 2 agent and s 2 agent w/o rs,0.5387133359909058
translation,16,200,results,dqfd,leverages,human demonstrations,dqfd leverages human demonstrations,0.7750063538551331
translation,16,200,results,lags behind,showing that,shaping,lags behind showing that shaping,0.6558078527450562
translation,16,200,results,w/o rs,showing that,shaping,w/o rs showing that shaping,0.3454350233078003
translation,16,200,results,shaping,in,policy space,shaping in policy space,0.5122605562210083
translation,16,200,results,shaping,is,more effective,shaping is more effective,0.6069316267967224
translation,16,200,results,more effective,solely adding,supervised learning loss,more effective solely adding supervised learning loss,0.7147344350814819
translation,16,200,results,supervised learning loss,for,q-learning,supervised learning loss for q-learning,0.5527949333190918
translation,16,200,results,lags behind,has,w/o rs,lags behind has w/o rs,0.6378868818283081
translation,16,200,results,results,has,dqfd,results has dqfd,0.574614942073822
translation,16,201,results,dqn,to explore,better policy,dqn to explore better policy,0.6827808022499084
translation,16,201,results,reward shaping,has,benefits,reward shaping has benefits,0.5099335312843323
translation,16,201,results,benefits,has,dqn,benefits has dqn,0.6089943647384644
translation,16,201,results,results,has,reward shaping,results has reward shaping,0.49155616760253906
translation,16,214,results,s 2 agent and s 2 agent w/o rs,perform,consistently better,s 2 agent and s 2 agent w/o rs perform consistently better,0.6033991575241089
translation,16,214,results,consistently better,than,dqn and dqfd,consistently better than dqn and dqfd,0.595787525177002
translation,16,215,results,s 2 agent,achieves,best performance,s 2 agent achieves best performance,0.6870959997177124
translation,16,215,results,best performance,in terms of,success rate,best performance in terms of success rate,0.7086392641067505
translation,16,215,results,best performance,in terms of,user rating,best performance in terms of user rating,0.6526288986206055
translation,16,215,results,results,has,s 2 agent,results has s 2 agent,0.5411126017570496
translation,17,183,ablation-analysis,copt,both,distinct - 1 and distinct - 2,copt both distinct - 1 and distinct - 2,0.722541868686676
translation,17,183,ablation-analysis,distinct - 1 and distinct - 2,in,regs and stepgan,distinct - 1 and distinct - 2 in regs and stepgan,0.6179458498954773
translation,17,183,ablation-analysis,improvement,is,significant,improvement is significant,0.5879829525947571
translation,17,183,ablation-analysis,copt,has,improvement,copt has improvement,0.6083115339279175
translation,17,183,ablation-analysis,ablation analysis,After introducing,copt,ablation analysis After introducing copt,0.720149040222168
translation,17,148,baselines,diversitypromoting gan,introduces,language model based discriminator,diversitypromoting gan introduces language model based discriminator,0.603602945804596
translation,17,148,baselines,dpgan,has,diversitypromoting gan,dpgan has diversitypromoting gan,0.5519545674324036
translation,17,148,baselines,baselines,has,dpgan,baselines has dpgan,0.5962370038032532
translation,17,151,baselines,variants,with,copt,variants with copt,0.6873053908348083
translation,17,151,baselines,copt,using,opennmt,copt using opennmt,0.7176228761672974
translation,17,155,hyperparameters,vectors,to initialize,word embeddings,vectors to initialize word embeddings,0.675733745098114
translation,17,155,hyperparameters,300 dimensional glove,has,vectors,300 dimensional glove has vectors,0.5867341160774231
translation,17,155,hyperparameters,hyperparameters,use,300 dimensional glove,hyperparameters use 300 dimensional glove,0.6114683151245117
translation,17,157,hyperparameters,number of hidden units,is,500,number of hidden units is 500,0.5824295282363892
translation,17,157,hyperparameters,hyperparameters,has,number of hidden units,hyperparameters has number of hidden units,0.4962380826473236
translation,17,158,hyperparameters,adversarial learning process,use,adam algorithm,adversarial learning process use adam algorithm,0.600034773349762
translation,17,158,hyperparameters,adam algorithm,to alternately optimize,g and d,adam algorithm to alternately optimize g and d,0.7223575115203857
translation,17,158,hyperparameters,g and d,for,one batch and five batches,g and d for one batch and five batches,0.6583213210105896
translation,17,158,hyperparameters,hyperparameters,During,adversarial learning process,hyperparameters During adversarial learning process,0.6322173476219177
translation,17,159,hyperparameters,batch size,is,64,batch size is 64,0.6388692259788513
translation,17,159,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,17,160,hyperparameters,learning rate,from,1e - 6 to 1e - 3,learning rate from 1e - 6 to 1e - 3,0.5962201952934265
translation,17,160,hyperparameters,hyperparameters,tested,learning rate,hyperparameters tested learning rate,0.634881854057312
translation,17,5,model,potential responses,by,counterfactual reasoning,potential responses by counterfactual reasoning,0.5654163360595703
translation,17,5,model,model,explore,potential responses,model explore potential responses,0.6775797009468079
translation,17,6,model,counterfactual reasoning model,automatically infers,outcome,counterfactual reasoning model automatically infers outcome,0.7635446190834045
translation,17,6,model,outcome,of,alternative policy,outcome of alternative policy,0.6036166548728943
translation,17,6,model,observed response,has,counterfactual reasoning model,observed response has counterfactual reasoning model,0.5732473731040955
translation,17,6,model,model,Given,observed response,model Given observed response,0.7691179513931274
translation,17,20,model,dialogue generator,as,structural causal model ( scm ),dialogue generator as structural causal model ( scm ),0.5415821671485901
translation,17,20,model,structural causal model ( scm ),describes,generation process,structural causal model ( scm ) describes generation process,0.6143345236778259
translation,17,20,model,generation process,with,two ingredients,generation process with two ingredients,0.6640627384185791
translation,17,48,model,concept of counterfactual reasoning,with,dialogue generation,concept of counterfactual reasoning with dialogue generation,0.6340135931968689
translation,17,48,model,concept of counterfactual reasoning,cast,generation model,concept of counterfactual reasoning cast generation model,0.6121690273284912
translation,17,48,model,generation model,as,scm,generation model as scm,0.544515073299408
translation,17,48,model,scm,under,adversarial learning framework,scm under adversarial learning framework,0.6475116014480591
translation,17,48,model,model,connect,concept of counterfactual reasoning,model connect concept of counterfactual reasoning,0.6625707149505615
translation,17,48,model,model,cast,generation model,model cast generation model,0.6829445958137512
translation,17,156,model,encoder and the decoder,are,two -layer lstm in g,encoder and the decoder are two -layer lstm in g,0.5740213990211487
translation,17,156,model,encoder and the decoder,are,single layer lstm in d,encoder and the decoder are single layer lstm in d,0.5818690061569214
translation,17,156,model,model,has,encoder and the decoder,model has encoder and the decoder,0.5959324836730957
translation,17,154,results,more words,observes,no improvement,more words observes no improvement,0.7139267325401306
translation,17,154,results,more words,takes,more time,more words takes more time,0.6189355850219727
translation,17,154,results,more time,for,training,more time for training,0.6057295203208923
translation,17,154,results,results,Including,more words,results Including more words,0.7145554423332214
translation,17,161,results,stepgan + copt,achieve,best performance,stepgan + copt achieve best performance,0.6351593136787415
translation,17,161,results,best performance,on,1e - 5,best performance on 1e - 5,0.5752328038215637
translation,17,161,results,results,has,regs + copt,results has regs + copt,0.5590682625770569
translation,17,179,results,outperform,HRED in,distinct - 1 and distinct - 2,outperform HRED in distinct - 1 and distinct - 2,0.7387993931770325
translation,17,179,results,regs and stepgan,has,outperform,regs and stepgan has outperform,0.6408731937408447
translation,17,179,results,results,has,regs and stepgan,results has regs and stepgan,0.5712082386016846
translation,17,180,results,dpgan,compared with,hred,dpgan compared with hred,0.7276946306228638
translation,17,185,results,regs and stepgan,achieve,higher bleu scores,regs and stepgan achieve higher bleu scores,0.6097244024276733
translation,17,185,results,higher bleu scores,with,copt,higher bleu scores with copt,0.6868337988853455
translation,17,185,results,improvements,of,bleu - 1 and bleu - 2,improvements of bleu - 1 and bleu - 2,0.5845290422439575
translation,17,185,results,bleu - 1 and bleu - 2,are,significant ( p < 0.05 ),bleu - 1 and bleu - 2 are significant ( p < 0.05 ),0.5627679824829102
translation,17,185,results,results,terms of,bleu,results terms of bleu,0.6273961663246155
translation,17,187,results,less significant result,of,bleu - 3 and bleu - 4,less significant result of bleu - 3 and bleu - 4,0.5952861309051514
translation,17,187,results,sparsity,of,tri-grams and four-grams,sparsity of tri-grams and four-grams,0.5894842743873596
translation,17,187,results,results,has,less significant result,results has less significant result,0.5384920835494995
translation,17,190,results,copt,helps improve,quality of responses,copt helps improve quality of responses,0.656218945980072
translation,17,190,results,results,indicate,copt,results indicate copt,0.5628736615180969
translation,17,211,results,counterfactual responses,generated with,copt,counterfactual responses generated with copt,0.667751669883728
translation,17,211,results,copt,achieve,higher average,copt achieve higher average,0.6776555776596069
translation,17,211,results,higher average,than,standard responses,higher average than standard responses,0.6047210097312927
translation,17,211,results,results,has,counterfactual responses,results has counterfactual responses,0.48559707403182983
translation,18,165,ablation-analysis,training of receiver,reduced,linearly,training of receiver reduced linearly,0.7094394564628601
translation,18,165,ablation-analysis,linearly,from,10 to 0.5,linearly from 10 to 0.5,0.5738803148269653
translation,18,165,ablation-analysis,ablation analysis,In,training of receiver,ablation analysis In training of receiver,0.529129147529602
translation,18,196,ablation-analysis,hits@1,validate,important role,hits@1 validate important role,0.6532295346260071
translation,18,196,ablation-analysis,important role,of,auxiliary task,important role of auxiliary task,0.5684061646461487
translation,18,196,ablation-analysis,ablation analysis,on,hits@1,ablation analysis on hits@1,0.587410032749176
translation,18,198,ablation-analysis,rs.3,obtained,25 % relative improvement,rs.3 obtained 25 % relative improvement,0.6337162852287292
translation,18,198,ablation-analysis,25 % relative improvement,on,bleu,25 % relative improvement on bleu,0.5675665736198425
translation,18,171,baselines,baselines,fall into,three categories,baselines fall into three categories,0.7626052498817444
translation,18,172,baselines,kv profile memory,was,official baseline,kv profile memory was official baseline,0.5882892608642578
translation,18,172,baselines,official baseline,employed,memory network,official baseline employed memory network,0.6549990773200989
translation,18,172,baselines,memory network,along with,profile information,memory network along with profile information,0.6507573127746582
translation,18,172,baselines,dually interactive matching network,proposed,dual matching architecture,dually interactive matching network proposed dual matching architecture,0.6573747992515564
translation,18,172,baselines,dual matching architecture,to,match,dual matching architecture to match,0.5867236256599426
translation,18,172,baselines,match,between,responses,match between responses,0.7304680943489075
translation,18,172,baselines,match,between,corresponding contexts,match between corresponding contexts,0.6807832717895508
translation,18,173,baselines,language model,implemented as,generative baselines,language model implemented as generative baselines,0.5425158739089966
translation,18,173,baselines,seq2seq with attention mechanism,implemented as,generative baselines,seq2seq with attention mechanism implemented as generative baselines,0.585064172744751
translation,18,173,baselines,generative baselines,for,dialogue generation,generative baselines for dialogue generation,0.5711414813995361
translation,18,173,baselines,language model,has,generative profile memory,language model has generative profile memory,0.486267626285553
translation,18,173,baselines,baselines,has,language model,baselines has language model,0.5359828472137451
translation,18,61,experiments,transmitter,via,maximum likelihood estimation ( mle ),transmitter via maximum likelihood estimation ( mle ),0.6690552830696106
translation,18,61,experiments,maximum likelihood estimation ( mle ),on,supervised dialogue generation task,maximum likelihood estimation ( mle ) on supervised dialogue generation task,0.5196556448936462
translation,18,164,hyperparameters,"adam ( kingma and ba , 2015 ) optimizer",with,learning rate,"adam ( kingma and ba , 2015 ) optimizer with learning rate",0.6114457249641418
translation,18,164,hyperparameters,learning rate,of,6.25e - 5,learning rate of 6.25e - 5,0.5975595712661743
translation,18,164,hyperparameters,6.25e - 5,for,receiver and transmitter,6.25e - 5 for receiver and transmitter,0.6077011823654175
translation,18,164,hyperparameters,receiver and transmitter,in,supervised learning,receiver and transmitter in supervised learning,0.5255921483039856
translation,18,164,hyperparameters,hyperparameters,used,"adam ( kingma and ba , 2015 ) optimizer","hyperparameters used adam ( kingma and ba , 2015 ) optimizer",0.5859174728393555
translation,18,166,hyperparameters,self - play phase,of,transmitter,self - play phase of transmitter,0.627124547958374
translation,18,166,hyperparameters,learning rate,set as,1e - 6,learning rate set as 1e - 6,0.6169568300247192
translation,18,166,hyperparameters,self - play phase,has,learning rate,self - play phase has learning rate,0.5600674748420715
translation,18,166,hyperparameters,transmitter,has,learning rate,transmitter has learning rate,0.5101584196090698
translation,18,166,hyperparameters,hyperparameters,In,self - play phase,hyperparameters In self - play phase,0.49112582206726074
translation,18,167,hyperparameters,"m , ? , ? , ? , ? 1 , ? 2 and ? 3",set as,"0.4 , 0.1 , 1e -4 , 0.5 , 0.4 , 0.1 and 0.5","m , ? , ? , ? , ? 1 , ? 2 and ? 3 set as 0.4 , 0.1 , 1e -4 , 0.5 , 0.4 , 0.1 and 0.5",0.601701557636261
translation,18,167,hyperparameters,hyperparameters,set as,"0.4 , 0.1 , 1e -4 , 0.5 , 0.4 , 0.1 and 0.5","hyperparameters set as 0.4 , 0.1 , 1e -4 , 0.5 , 0.4 , 0.1 and 0.5",0.5984609127044678
translation,18,168,hyperparameters,supervised training,of,transmitter,supervised training of transmitter,0.6154678463935852
translation,18,168,hyperparameters,transmitter,lasted for,2 epochs,transmitter lasted for 2 epochs,0.7162957191467285
translation,18,168,hyperparameters,self - play fine-tuning,comprised,2000 dialogues,self - play fine-tuning comprised 2000 dialogues,0.6577509641647339
translation,18,168,hyperparameters,2000 dialogues,where,number of turns,2000 dialogues where number of turns,0.6078833341598511
translation,18,168,hyperparameters,number of turns,was,3,number of turns was 3,0.6445971727371216
translation,18,168,hyperparameters,hyperparameters,has,supervised training,hyperparameters has supervised training,0.5246361494064331
translation,18,168,hyperparameters,hyperparameters,has,self - play fine-tuning,hyperparameters has self - play fine-tuning,0.5251948237419128
translation,18,169,hyperparameters,beam search size,set as,2,beam search size set as 2,0.7006782293319702
translation,18,169,hyperparameters,hyperparameters,has,beam search size,hyperparameters has beam search size,0.5214741826057434
translation,18,6,model,transmitter - receiver based framework,with the aim of,explicitly modeling,transmitter - receiver based framework with the aim of explicitly modeling,0.5925642848014832
translation,18,6,model,p 2 bot,has,transmitter - receiver based framework,p 2 bot has transmitter - receiver based framework,0.5840173363685608
translation,18,6,model,explicitly modeling,has,understanding,explicitly modeling has understanding,0.624281108379364
translation,18,6,model,model,propose,p 2 bot,model propose p 2 bot,0.6996713876724243
translation,18,7,model,mutual persona perception,to enhance,quality of personalized dialogue generation,mutual persona perception to enhance quality of personalized dialogue generation,0.6703321933746338
translation,18,34,model,persona perception bot ( p 2 bot ),explicitly modeling,understanding,persona perception bot ( p 2 bot ) explicitly modeling understanding,0.7722093462944031
translation,18,34,model,understanding,between,interlocutors,understanding between interlocutors,0.5986241698265076
translation,18,34,model,interlocutors,with,transmitter - receiver framework,interlocutors with transmitter - receiver framework,0.6642382144927979
translation,18,34,model,model,propose,persona perception bot ( p 2 bot ),model propose persona perception bot ( p 2 bot ),0.6355172395706177
translation,18,35,model,mutual persona perception,better suited to describe,information exchange process,mutual persona perception better suited to describe information exchange process,0.6655041575431824
translation,18,35,model,information exchange process,empowers,interlocutors,information exchange process empowers interlocutors,0.7204383611679077
translation,18,35,model,novel concept,has,mutual persona perception,novel concept has mutual persona perception,0.5771414637565613
translation,18,35,model,interlocutors,has,to get to know each other,interlocutors has to get to know each other,0.5826757550239563
translation,18,36,model,p 2 bot,for,personalized dialogue generation,p 2 bot for personalized dialogue generation,0.5650346875190735
translation,18,36,model,p 2 bot,employ,supervised training,p 2 bot employ supervised training,0.5876542925834656
translation,18,36,model,p 2 bot,employ,self - play fine-tuning,p 2 bot employ self - play fine-tuning,0.6086881756782532
translation,18,36,model,self - play fine-tuning,piloted by,reward signals,self - play fine-tuning piloted by reward signals,0.6832036375999451
translation,18,36,model,reward signals,characterizing,mutual persona perception,reward signals characterizing mutual persona perception,0.6469974517822266
translation,18,36,model,model,to train,p 2 bot,model to train p 2 bot,0.7455687522888184
translation,18,36,model,model,employ,supervised training,model employ supervised training,0.5947847366333008
translation,18,62,model,model,has,self-play model finetuning,model has self-play model finetuning,0.5753505825996399
translation,18,243,model,transmitter - receiver framework,explicitly models,understanding,transmitter - receiver framework explicitly models understanding,0.771314263343811
translation,18,243,model,understanding,between,interlocutors,understanding between interlocutors,0.5986241698265076
translation,18,243,model,p 2 bot,has,transmitter - receiver framework,p 2 bot has transmitter - receiver framework,0.5795605778694153
translation,18,243,model,model,propose,p 2 bot,model propose p 2 bot,0.6996713876724243
translation,18,182,results,our approach,achieves,new state - of - the - art performance,our approach achieves new state - of - the - art performance,0.6485505700111389
translation,18,182,results,new state - of - the - art performance,on,ppl and f1,new state - of - the - art performance on ppl and f1,0.5603370070457458
translation,18,182,results,highly competitive performance,on,hits@1,highly competitive performance on hits@1,0.5444197058677673
translation,18,182,results,our approach,has,outperforms,our approach has outperforms,0.6385829448699951
translation,18,182,results,outperforms,has,almost all baselines,outperforms has almost all baselines,0.5941314697265625
translation,18,182,results,results,has,our approach,results has our approach,0.6050099730491638
translation,18,183,results,our approach,achieves,best performance,our approach achieves best performance,0.6786253452301025
translation,18,183,results,our approach,obtaining,relative improvement,our approach obtaining relative improvement,0.5867795944213867
translation,18,183,results,best performance,obtaining,relative improvement,best performance obtaining relative improvement,0.63383948802948
translation,18,183,results,relative improvement,of,13.4 %,relative improvement of 13.4 %,0.5499129891395569
translation,18,183,results,13.4 %,on,f1,13.4 % on f1,0.5883410573005676
translation,18,183,results,13.4 %,against,strongest baseline,13.4 % against strongest baseline,0.6015334725379944
translation,18,183,results,revised mode,has,our approach,revised mode has our approach,0.6272832751274109
translation,18,183,results,results,In,revised mode,results In revised mode,0.532763659954071
translation,18,190,results,results,consistent with,automatic evaluation results,results consistent with automatic evaluation results,0.5573641657829285
translation,18,190,results,automatic evaluation results,demonstrating,superiority,automatic evaluation results demonstrating superiority,0.7528179287910461
translation,18,190,results,superiority,of,p 2 bot,superiority of p 2 bot,0.7012128233909607
translation,18,190,results,p 2 bot,against,baselines,p 2 bot against baselines,0.6971837282180786
translation,18,190,results,results,consistent with,automatic evaluation results,results consistent with automatic evaluation results,0.5573641657829285
translation,18,190,results,results,has,results,results has results,0.48582205176353455
translation,18,197,results,gains,in,bleu and f1,gains in bleu and f1,0.5346697568893433
translation,18,197,results,bleu and f1,are,very small,bleu and f1 are very small,0.569884181022644
translation,18,197,results,all the variants,has,gains,all the variants has gains,0.6308172345161438
translation,18,197,results,results,Across,all the variants,results Across all the variants,0.6512861251831055
translation,18,220,results,our approach,achieved,excellent results,our approach achieved excellent results,0.6546636819839478
translation,18,220,results,excellent results,on,original and revised modes,excellent results on original and revised modes,0.5004070401191711
translation,18,220,results,results,has,our approach,results has our approach,0.6050099730491638
translation,19,154,ablation-analysis,occasionally chitchat,in,real-world set,occasionally chitchat in real-world set,0.5886054039001465
translation,19,154,ablation-analysis,nurses and patients,has,occasionally chitchat,nurses and patients has occasionally chitchat,0.5869212746620178
translation,19,154,ablation-analysis,ablation analysis,has,nurses and patients,ablation analysis has nurses and patients,0.4880226254463196
translation,19,146,hyperparameters,l2 regularization and dropout ( rate = 0.2 ),to alleviate,overfitting,l2 regularization and dropout ( rate = 0.2 ) to alleviate overfitting,0.6016147136688232
translation,19,146,hyperparameters,hyperparameters,to alleviate,overfitting,hyperparameters to alleviate overfitting,0.6152921319007874
translation,19,146,hyperparameters,hyperparameters,has,l2 regularization and dropout ( rate = 0.2 ),hyperparameters has l2 regularization and dropout ( rate = 0.2 ),0.47926342487335205
translation,20,60,baselines,baseline,is,sequence - to-sequence model,baseline is sequence - to-sequence model,0.5551229119300842
translation,20,60,baselines,sequence - to-sequence model,with,copy attention,sequence - to-sequence model with copy attention,0.6244381666183472
translation,20,60,baselines,sequence - to-sequence model,trained on,e2e dataset,sequence - to-sequence model trained on e2e dataset,0.7488680481910706
translation,20,60,baselines,e2e dataset,using,neural machine translation framework open-nmt,e2e dataset using neural machine translation framework open-nmt,0.623276948928833
translation,20,60,baselines,baselines,has,baseline,baselines has baseline,0.6124745607376099
translation,20,65,baselines,neural system,with,semantic reranker,neural system with semantic reranker,0.5840436220169067
translation,20,65,baselines,semantic reranker,as,final step,semantic reranker as final step,0.5168258547782898
translation,20,65,baselines,final step,to improve,accuracy,final step to improve accuracy,0.7554965615272522
translation,20,65,baselines,augmented dataset,in which,attribute - value pairs,augmented dataset in which attribute - value pairs,0.6186095476150513
translation,20,65,baselines,individual sentences,in,utterance,individual sentences in utterance,0.554323136806488
translation,20,65,baselines,individual sentences,in,template based -system,individual sentences in template based -system,0.5661747455596924
translation,20,65,baselines,"slug2slug ( juraska et al. , 2018 )",has,neural system,"slug2slug ( juraska et al. , 2018 ) has neural system",0.5490154027938843
translation,20,65,baselines,template based -system,has,"tuda ( puzikov and gurevych , 2018 )","template based -system has tuda ( puzikov and gurevych , 2018 )",0.574669361114502
translation,20,147,experimental-setup,20 epochs,of,data,20 epochs of data,0.6392782330513
translation,20,147,experimental-setup,20 epochs,takes,15 minutes,20 epochs takes 15 minutes,0.6950384974479675
translation,20,147,experimental-setup,15 minutes,using,two nvidia 1080 ti gpu cards,15 minutes using two nvidia 1080 ti gpu cards,0.6118256449699402
translation,20,147,experimental-setup,experimental setup,train for,20 epochs,experimental setup train for 20 epochs,0.6967481970787048
translation,20,5,model,data augmentation approach,restrict,output,data augmentation approach restrict output,0.6939147114753723
translation,20,5,model,data augmentation approach,guarantee,reliability,data augmentation approach guarantee reliability,0.8190855979919434
translation,20,5,model,output,of,network,output of network,0.6230159997940063
translation,20,5,model,model,propose,data augmentation approach,model propose data augmentation approach,0.6756566762924194
translation,20,23,results,small amount of unconstrained diversity,able to achieve,100 % semantic accuracy,small amount of unconstrained diversity able to achieve 100 % semantic accuracy,0.6595823168754578
translation,20,23,results,100 % semantic accuracy,on,e2e dataset,100 % semantic accuracy on e2e dataset,0.4969494044780731
translation,20,74,results,semantic accuracy,of,our proposed method,semantic accuracy of our proposed method,0.5579045414924622
translation,20,74,results,semantic accuracy,on par with,template system,semantic accuracy on par with template system,0.7078814506530762
translation,20,74,results,semantic accuracy,achieve,100 % accuracy,semantic accuracy achieve 100 % accuracy,0.6255162358283997
translation,20,74,results,our proposed method,on par with,template system,our proposed method on par with template system,0.6823764443397522
translation,20,74,results,struggle,with,best system,struggle with best system,0.6797134280204773
translation,20,74,results,best system,achieving,92 %,best system achieving 92 %,0.6364727020263672
translation,20,74,results,neural systems,has,struggle,neural systems has struggle,0.5613028407096863
translation,20,74,results,results,demonstrates,semantic accuracy,results demonstrates semantic accuracy,0.6662484407424927
translation,21,62,hyperparameters,how quickly,learns from,user feedback,how quickly learns from user feedback,0.6463015079498291
translation,21,62,hyperparameters,model,learns from,user feedback,model learns from user feedback,0.6993204951286316
translation,21,62,hyperparameters,parameter ' initial learning rate,of,adam,parameter ' initial learning rate of adam,0.5672675371170044
translation,21,62,hyperparameters,how quickly,has,model,how quickly has model,0.5303063988685608
translation,21,62,hyperparameters,hyperparameters,control,how quickly,hyperparameters control how quickly,0.6520634889602661
translation,21,62,hyperparameters,hyperparameters,control,model,hyperparameters control model,0.7504308819770813
translation,21,4,model,"online , end-to-end , neural generative conversational model",for,opendomain dialogue,"online , end-to-end , neural generative conversational model for opendomain dialogue",0.5445005893707275
translation,21,4,model,model,propose,"online , end-to-end , neural generative conversational model","model propose online , end-to-end , neural generative conversational model",0.6190603375434875
translation,21,6,model,novel interactive learning mechanism,based on,hamming - diverse beam search,novel interactive learning mechanism based on hamming - diverse beam search,0.6692304015159607
translation,21,6,model,novel interactive learning mechanism,based on,one- character userfeedback,novel interactive learning mechanism based on one- character userfeedback,0.6101263761520386
translation,21,6,model,hamming - diverse beam search,for,response generation,hamming - diverse beam search for response generation,0.6143861413002014
translation,21,6,model,model,devise,novel interactive learning mechanism,model devise novel interactive learning mechanism,0.7118789553642273
translation,21,13,model,online deep active learning,integrated with,standard neural network based dialogue systems,online deep active learning integrated with standard neural network based dialogue systems,0.7115984559059143
translation,21,13,model,standard neural network based dialogue systems,to enhance,open-domain conversational skills,standard neural network based dialogue systems to enhance open-domain conversational skills,0.6522847414016724
translation,21,13,model,model,demonstrate,online deep active learning,model demonstrate online deep active learning,0.6352195143699646
translation,21,14,model,architectural backbone,of,model,architectural backbone of model,0.5578539371490479
translation,21,14,model,architectural backbone,is,seq2seq framework,architectural backbone is seq2seq framework,0.5807247161865234
translation,21,14,model,model,is,seq2seq framework,model is seq2seq framework,0.5611405968666077
translation,21,14,model,seq2seq framework,initially undergoes,offline supervised learning,seq2seq framework initially undergoes offline supervised learning,0.7072311043739319
translation,21,14,model,offline supervised learning,on,two different types of conversational datasets,offline supervised learning on two different types of conversational datasets,0.4820620119571686
translation,21,14,model,model,has,architectural backbone,model has architectural backbone,0.5382700562477112
translation,21,24,model,online deep active learning,as a form of,reinforcement,online deep active learning as a form of reinforcement,0.6385037302970886
translation,21,24,model,reinforcement,in,novel way,reinforcement in novel way,0.5868450403213501
translation,21,24,model,model,use,online deep active learning,model use online deep active learning,0.5935959219932556
translation,21,25,model,model,use,diversity - promoting decoding heuristic,model use diversity - promoting decoding heuristic,0.6650927066802979
translation,21,29,model,seq2seq framework,consisting of,one encoderdecoder layer,seq2seq framework consisting of one encoderdecoder layer,0.67804354429245
translation,21,29,model,one encoderdecoder layer,containing,300 lstm units,one encoderdecoder layer containing 300 lstm units,0.6081570982933044
translation,21,29,model,model,has,architectural backbone,model has architectural backbone,0.5382700562477112
translation,21,53,model,dbs,incorporates,diversity,dbs incorporates diversity,0.7582494616508484
translation,21,53,model,diversity,between,beams,diversity between beams,0.6691314578056335
translation,21,53,model,diversity,between,beams,diversity between beams,0.6691314578056335
translation,21,53,model,diversity,by maximizing,objective,diversity by maximizing objective,0.6790047883987427
translation,21,53,model,objective,consists of,standard sequence likelihood term,objective consists of standard sequence likelihood term,0.6348251700401306
translation,21,53,model,objective,consists of,dissimilarity metric,objective consists of dissimilarity metric,0.6175194978713989
translation,21,53,model,dissimilarity metric,between,beams,dissimilarity metric between beams,0.6743524074554443
translation,21,53,model,model,has,dbs,model has dbs,0.6546236872673035
translation,21,106,model,model,developed,end-to - end,model developed end-to - end,0.7012025117874146
translation,21,107,model,seq2seq framework,with,online deep active learning,seq2seq framework with online deep active learning,0.5830165147781372
translation,21,107,model,online deep active learning,to overcome,known short-comings,online deep active learning to overcome known short-comings,0.591895580291748
translation,21,107,model,known short-comings,with respect to,dialogue generation,known short-comings with respect to dialogue generation,0.6883962750434875
translation,21,107,model,model,augments,seq2seq framework,model augments seq2seq framework,0.6626985669136047
translation,21,52,results,results,has,dbs,results has dbs,0.45050349831581116
translation,21,83,results,drops significantly,for,higher values,drops significantly for higher values,0.6796277761459351
translation,21,83,results,higher values,of,learning rate,higher values of learning rate,0.5986641645431519
translation,21,83,results,response quality,has,drops significantly,response quality has drops significantly,0.6169599890708923
translation,21,94,results,sl2,generates,more relevant and appropriate responses,sl2 generates more relevant and appropriate responses,0.6227623820304871
translation,21,94,results,more relevant and appropriate responses,than,sl1,more relevant and appropriate responses than sl1,0.5916828513145447
translation,21,94,results,sl1,in,many cases,sl1 in many cases,0.582714319229126
translation,21,94,results,results,see that,sl2,results see that sl2,0.6565071940422058
translation,21,96,results,sl2 + oal,generates,"more interesting , relevant and engaging responses","sl2 + oal generates more interesting , relevant and engaging responses",0.6237572431564331
translation,21,96,results,"more interesting , relevant and engaging responses",than,sl2,"more interesting , relevant and engaging responses than sl2",0.600176990032196
translation,21,96,results,results,see that,sl2 + oal,results see that sl2 + oal,0.6518141031265259
translation,22,37,ablation-analysis,dynamics model,as,reward function,dynamics model as reward function,0.48088106513023376
translation,22,37,ablation-analysis,dynamics model,promotes,more stable policy learning,dynamics model promotes more stable policy learning,0.6748970150947571
translation,22,37,ablation-analysis,ablation analysis,Using,dynamics model,ablation analysis Using dynamics model,0.655302107334137
translation,22,164,ablation-analysis,both action learning and dynamics model,essential to,superiority,both action learning and dynamics model essential to superiority,0.7372204065322876
translation,22,164,ablation-analysis,superiority,of,act- vrnn,superiority of act- vrnn,0.6136972904205322
translation,22,164,ablation-analysis,ablation analysis,find that,both action learning and dynamics model,ablation analysis find that both action learning and dynamics model,0.6142451167106628
translation,22,193,ablation-analysis,effects of dynamics model based reward function,in,act- vrnn,effects of dynamics model based reward function in act- vrnn,0.5512481927871704
translation,22,193,ablation-analysis,ablation analysis,study,effects of dynamics model based reward function,ablation analysis study effects of dynamics model based reward function,0.579566478729248
translation,22,195,ablation-analysis,both stochastic and deterministic states,in,vrnn,both stochastic and deterministic states in vrnn,0.57199627161026
translation,22,195,ablation-analysis,both stochastic and deterministic states,are,important,both stochastic and deterministic states are important,0.6124876737594604
translation,22,195,ablation-analysis,vrnn,are,important,vrnn are important,0.6542595624923706
translation,22,195,ablation-analysis,ablation analysis,see that,both stochastic and deterministic states,ablation analysis see that both stochastic and deterministic states,0.6793392896652222
translation,22,157,baselines,ppo,using,hand -crafted rewards setting,ppo using hand -crafted rewards setting,0.6160332560539246
translation,22,157,baselines,baselines,compare,act - vrnn,baselines compare act - vrnn,0.654955267906189
translation,22,159,baselines,ss - vrnn,uses,vrnn,ss - vrnn uses vrnn,0.5971534848213196
translation,22,159,baselines,vrnn,that consumes,predicted action labels,vrnn that consumes predicted action labels,0.5572395324707031
translation,22,159,baselines,predicted action labels,instead of,action embeddings,predicted action labels instead of action embeddings,0.538653552532196
translation,22,159,baselines,predicted action labels,instead of,action embeddings,predicted action labels instead of action embeddings,0.538653552532196
translation,22,159,baselines,act - gdpl,feeds,expert demonstrations,act - gdpl feeds expert demonstrations,0.7952646017074585
translation,22,159,baselines,expert demonstrations,enriched by,action embeddings,expert demonstrations enriched by action embeddings,0.691565215587616
translation,22,159,baselines,action embeddings,to,same reward function,action embeddings to same reward function,0.5466180443763733
translation,22,159,baselines,same reward function,as,gdpl,same reward function as gdpl,0.5778186321258545
translation,22,194,baselines,four different models,as,reward function,four different models as reward function,0.5237930417060852
translation,22,194,baselines,dynamics model,having,only deterministic states,dynamics model having only deterministic states,0.6543063521385193
translation,22,108,experiments,dialogue progress,use,variational recurrent neural network ( vrnn ),dialogue progress use variational recurrent neural network ( vrnn ),0.6357434988021851
translation,22,183,experiments,effects of action learning module,in,act- vrnn,effects of action learning module in act- vrnn,0.5594586133956909
translation,22,150,hyperparameters,grid search,to find,best hyperparameters,grid search to find best hyperparameters,0.5902838706970215
translation,22,150,hyperparameters,best hyperparameters,for,models,best hyperparameters for models,0.5897504687309265
translation,22,150,hyperparameters,hyperparameters,use,grid search,hyperparameters use grid search,0.67574143409729
translation,22,151,hyperparameters,action embedding dimensionality,among,"{ 50 , 75 , 100 , 150 , 200 }","action embedding dimensionality among { 50 , 75 , 100 , 150 , 200 }",0.5730067491531372
translation,22,151,hyperparameters,stochastic latent state size,in,vrnn,stochastic latent state size in vrnn,0.5460349321365356
translation,22,151,hyperparameters,vrnn,among,"{ 16 , 32 , 64 , 128 , 256 }","vrnn among { 16 , 32 , 64 , 128 , 256 }",0.6160452365875244
translation,22,151,hyperparameters,deterministic latent state size,among,"{ 25 , 50 , 75 , 100 , 150 }","deterministic latent state size among { 25 , 50 , 75 , 100 , 150 }",0.5811116695404053
translation,22,151,hyperparameters,hyperparameters,choose,action embedding dimensionality,hyperparameters choose action embedding dimensionality,0.6127617359161377
translation,22,151,hyperparameters,hyperparameters,choose,stochastic latent state size,hyperparameters choose stochastic latent state size,0.6616247892379761
translation,22,8,model,novel reward learning approach,for,semisupervised policy learning,novel reward learning approach for semisupervised policy learning,0.5599555969238281
translation,22,8,model,model,propose,novel reward learning approach,model propose novel reward learning approach,0.6731384992599487
translation,22,9,model,dynamics model,as,reward function,dynamics model as reward function,0.48088106513023376
translation,22,9,model,reward function,models,"dialogue progress ( i.e. , state-action sequences )","reward function models dialogue progress ( i.e. , state-action sequences )",0.6804932951927185
translation,22,9,model,"dialogue progress ( i.e. , state-action sequences )",based on,expert demonstrations,"dialogue progress ( i.e. , state-action sequences ) based on expert demonstrations",0.6385818123817444
translation,22,10,model,dynamics model,computes,rewards,dynamics model computes rewards,0.6986048817634583
translation,22,10,model,rewards,by predicting whether,dialogue progress,rewards by predicting whether dialogue progress,0.6372037529945374
translation,22,10,model,dialogue progress,consistent with,expert demonstrations,dialogue progress consistent with expert demonstrations,0.6304835677146912
translation,22,10,model,model,has,dynamics model,model has dynamics model,0.5574620366096497
translation,22,11,model,action embeddings,for,better generalization,action embeddings for better generalization,0.5279030203819275
translation,22,11,model,better generalization,of,reward function,better generalization of reward function,0.5231471657752991
translation,22,11,model,model,propose to learn,action embeddings,model propose to learn action embeddings,0.7323639392852783
translation,22,30,model,dialogue policies,in,semi-supervised setting,dialogue policies in semi-supervised setting,0.5155709981918335
translation,22,30,model,semi-supervised setting,where,system action,semi-supervised setting where system action,0.5854445695877075
translation,22,30,model,system action,of,expert demonstrations only need to be partially annotated,system action of expert demonstrations only need to be partially annotated,0.5388132929801941
translation,22,30,model,model,propose to learn,dialogue policies,model propose to learn dialogue policies,0.7333698868751526
translation,22,31,model,implicitly trained stochastic dynamics model,as,reward function,implicitly trained stochastic dynamics model as reward function,0.5022813081741333
translation,22,31,model,reward function,to replace,conventional reward function,reward function to replace conventional reward function,0.671710193157196
translation,22,31,model,conventional reward function,restricted to,stateaction pairs,conventional reward function restricted to stateaction pairs,0.7247616648674011
translation,22,31,model,model,use,implicitly trained stochastic dynamics model,model use implicitly trained stochastic dynamics model,0.6084807515144348
translation,22,39,model,dynamics model,in,action embedding space,dynamics model in action embedding space,0.491309255361557
translation,22,39,model,effect of system utterances,on,dialogue progress,effect of system utterances on dialogue progress,0.6117725372314453
translation,22,39,model,model,learning,dynamics model,model learning dynamics model,0.7061523199081421
translation,22,40,model,action embedding learning,by incorporating,embedding function,action embedding learning by incorporating embedding function,0.7022203803062439
translation,22,40,model,embedding function,into,generative models framework,embedding function into generative models framework,0.570906937122345
translation,22,40,model,generative models framework,for,semi-supervised learning,generative models framework for semi-supervised learning,0.5480366945266724
translation,22,40,model,model,achieve,action embedding learning,model achieve action embedding learning,0.6026722192764282
translation,22,104,model,more stable reward estimation,than,adversarial reward learning,more stable reward estimation than adversarial reward learning,0.5421167612075806
translation,22,104,model,more stable reward estimation,propose,reward estimator,more stable reward estimation propose reward estimator,0.6086832880973816
translation,22,104,model,reward estimator,based on,dialogues progress,reward estimator based on dialogues progress,0.6457438468933105
translation,22,104,model,model,To achieve,more stable reward estimation,model To achieve more stable reward estimation,0.6538487076759338
translation,22,107,model,model,to explicitly model,dialogue progress,model to explicitly model dialogue progress,0.7332399487495422
translation,22,107,model,dialogue progress,without,negative sampling,dialogue progress without negative sampling,0.7340380549430847
translation,22,107,model,negative sampling,required by,adversarial learning,negative sampling required by adversarial learning,0.6020088195800781
translation,22,107,model,rewards,estimated as,local - probabilities,rewards estimated as local - probabilities,0.6483650803565979
translation,22,107,model,local - probabilities,assigned to,taken actions,local - probabilities assigned to taken actions,0.723933756351471
translation,22,107,model,model,learn,model,model learn model,0.6338090896606445
translation,22,173,model,generalization ability,modeling,dialogue progress,generalization ability modeling dialogue progress,0.7191159129142761
translation,22,173,model,dialogue progress,in,action embedding space,dialogue progress in action embedding space,0.5188450217247009
translation,22,173,model,action embedding space,for,reward estimation,action embedding space for reward estimation,0.5441164374351501
translation,22,173,model,model,has,act - vrnn,model has act - vrnn,0.5924238562583923
translation,22,211,model,generative model,to jointly infer,action labels,generative model to jointly infer action labels,0.6845928430557251
translation,22,211,model,generative model,to jointly infer,learn action embeddings,generative model to jointly infer learn action embeddings,0.6520087718963623
translation,22,211,model,model,formulate,generative model,model formulate generative model,0.6650065779685974
translation,22,161,results,consistently outperforms,uses,fully and partially annotated dialogues,consistently outperforms uses fully and partially annotated dialogues,0.5952112078666687
translation,22,161,results,other models,in,setting,other models in setting,0.5896519422531128
translation,22,161,results,other models,uses,fully and partially annotated dialogues,other models uses fully and partially annotated dialogues,0.5563057065010071
translation,22,161,results,our proposed model,has,consistently outperforms,our proposed model has consistently outperforms,0.6073158979415894
translation,22,161,results,consistently outperforms,has,other models,consistently outperforms has other models,0.5706311464309692
translation,22,161,results,fully and partially annotated dialogues,has,),fully and partially annotated dialogues has ),0.5981413722038269
translation,22,161,results,results,shows,our proposed model,results shows our proposed model,0.7126691341400146
translation,22,162,results,act - vrnn,improves,task completion,act - vrnn improves task completion,0.6543458700180054
translation,22,162,results,task completion,measured by,entity - f1 and success,task completion measured by entity - f1 and success,0.7288998365402222
translation,22,162,results,less cost,by,turns,less cost by turns,0.6096320152282715
translation,22,162,results,results,has,act - vrnn,results has act - vrnn,0.5154500603675842
translation,22,165,results,act - vrnn,achieves,19.8 % and 11.2 % improvements,act - vrnn achieves 19.8 % and 11.2 % improvements,0.6813795566558838
translation,22,165,results,19.8 % and 11.2 % improvements,over,ss - vrnn and act - gdpl,19.8 % and 11.2 % improvements over ss - vrnn and act - gdpl,0.6540168523788452
translation,22,165,results,19.8 % and 11.2 % improvements,under,success,19.8 % and 11.2 % improvements under success,0.5903298258781433
translation,22,165,results,success,when having,20 % fully annotated dialogues,success when having 20 % fully annotated dialogues,0.6098218560218811
translation,22,165,results,results,has,act - vrnn,results has act - vrnn,0.5154500603675842
translation,22,167,results,improvements,brought by,semi-vae enhancement,improvements brought by semi-vae enhancement,0.7427142858505249
translation,22,167,results,limited,for,baselines,limited for baselines,0.6600205302238464
translation,22,167,results,ratio of fully annotated dialogues,is,low,ratio of fully annotated dialogues is low,0.5631449222564697
translation,22,167,results,results,find that,improvements,results find that improvements,0.6680544018745422
translation,22,168,results,ss - ppo and ss - gdpl,achieve,6 % and 7 % improvements,ss - ppo and ss - gdpl achieve 6 % and 7 % improvements,0.6495730876922607
translation,22,168,results,6 % and 7 % improvements,over,counterparts,6 % and 7 % improvements over counterparts,0.6402395963668823
translation,22,168,results,6 % and 7 % improvements,under,success,6 % and 7 % improvements under success,0.6265822649002075
translation,22,168,results,success,when having,5 % fully annotated dialogues,success when having 5 % fully annotated dialogues,0.6180910468101501
translation,22,168,results,results,has,ss - ppo and ss - gdpl,results has ss - ppo and ss - gdpl,0.5299081802368164
translation,22,176,results,act- vrnn,has,significantly outperforms,act- vrnn has significantly outperforms,0.6143973469734192
translation,22,176,results,significantly outperforms,has,baselines,significantly outperforms has baselines,0.6014122366905212
translation,22,176,results,results,see that,act- vrnn,results see that act- vrnn,0.6242861747741699
translation,22,177,results,act - vrnn,outperforms,ss - gdpl,act - vrnn outperforms ss - gdpl,0.7777543067932129
translation,22,177,results,ss - gdpl,by,43 % and 44 %,ss - gdpl by 43 % and 44 %,0.6272813677787781
translation,22,177,results,43 % and 44 %,under,entity - f1 and success,43 % and 44 % under entity - f1 and success,0.603065550327301
translation,22,180,results,results,of,act- vrnn and baselines,results of act- vrnn and baselines,0.5234563946723938
translation,22,180,results,act- vrnn and baselines,see that,act - vrnn,act- vrnn and baselines see that act - vrnn,0.6311726570129395
translation,22,180,results,act - vrnn,better exploit,additional partially labeled dialogues,act - vrnn better exploit additional partially labeled dialogues,0.7239609360694885
translation,22,180,results,results,comparing,results,results comparing results,0.5670907497406006
translation,22,180,results,results,of,act- vrnn and baselines,results of act- vrnn and baselines,0.5234563946723938
translation,22,181,results,ss - gdpl,achieves,2.3 %,ss - gdpl achieves 2.3 %,0.6619863510131836
translation,22,181,results,2.3 %,under,success,2.3 % under success,0.6022710204124451
translation,22,181,results,act - vrnn,achieves,more than 5 %,act - vrnn achieves more than 5 %,0.7153164744377136
translation,22,181,results,results,has,ss - gdpl,results has ss - gdpl,0.5624094605445862
translation,22,189,results,other variants,in,each configuration,other variants in each configuration,0.5436980724334717
translation,22,189,results,act - vrnn,has,outperforms,act - vrnn has outperforms,0.6326201558113098
translation,22,189,results,outperforms,has,other variants,outperforms has other variants,0.5728906989097595
translation,22,189,results,results,see that,act - vrnn,results see that act - vrnn,0.6242861747741699
translation,22,191,results,state transition based objective,formulated,fits well,state transition based objective formulated fits well,0.6786550283432007
translation,22,191,results,fits well,with,vrnn based reward estimator,fits well with vrnn based reward estimator,0.6307150721549988
translation,22,191,results,results,find that,state transition based objective,results find that state transition based objective,0.6625242829322815
translation,22,192,results,act-vrnn and ss - vrnn,optimized considering,state transitions,act-vrnn and ss - vrnn optimized considering state transitions,0.7509125471115112
translation,22,192,results,state transitions,achieve,performance gains,state transitions achieve performance gains,0.6742036938667297
translation,22,192,results,results,has,act-vrnn and ss - vrnn,results has act-vrnn and ss - vrnn,0.5105038285255432
translation,23,202,experiments,adversarial dialogues,played by,adversarial agent and target agent,adversarial dialogues played by adversarial agent and target agent,0.73809415102005
translation,23,198,hyperparameters,reinforcement learning attack,use,learning rate,reinforcement learning attack use learning rate,0.6099907159805298
translation,23,198,hyperparameters,reinforcement learning attack,set,discount factor,reinforcement learning attack set discount factor,0.5850232243537903
translation,23,198,hyperparameters,learning rate,of,0.1,learning rate of 0.1,0.6136453747749329
translation,23,198,hyperparameters,gradients,above,1.0,gradients above 1.0,0.6941365003585815
translation,23,198,hyperparameters,hyperparameters,In,reinforcement learning attack,hyperparameters In reinforcement learning attack,0.4953145682811737
translation,23,199,hyperparameters,adversarial agent,for,4 epochs,adversarial agent for 4 epochs,0.5923899412155151
translation,23,199,hyperparameters,hyperparameters,train,adversarial agent,hyperparameters train adversarial agent,0.6506062150001526
translation,23,6,model,algorithms,to evaluate,robustness,algorithms to evaluate robustness,0.6814566254615784
translation,23,6,model,robustness,of,dialogue agent,robustness of dialogue agent,0.5754720568656921
translation,23,6,model,model,develop,algorithms,model develop algorithms,0.6839556694030762
translation,23,182,model,model,on,5808 dialogues,model on 5808 dialogues,0.6024092435836792
translation,23,182,model,5808 dialogues,based on,2236 unique scenarios,5808 dialogues based on 2236 unique scenarios,0.6398710608482361
translation,23,182,model,model,train,model,model train model,0.6881781220436096
translation,23,200,results,our adversarial agent,could gain,2.32 score advantage,our adversarial agent could gain 2.32 score advantage,0.6872445940971375
translation,23,200,results,our adversarial agent,could gain,4.25 advantage,our adversarial agent could gain 4.25 advantage,0.6980017423629761
translation,23,200,results,2.32 score advantage,against,rl agent,2.32 score advantage against rl agent,0.6226218342781067
translation,23,200,results,4.25 advantage,against,sv agent,4.25 advantage against sv agent,0.6593549847602844
translation,23,200,results,100 % agreement rate,has,our adversarial agent,100 % agreement rate has our adversarial agent,0.5483613014221191
translation,23,200,results,results,with,100 % agreement rate,results with 100 % agreement rate,0.581383228302002
translation,23,201,results,our agent,achieves,relatively high positive advantage rate,our agent achieves relatively high positive advantage rate,0.6370168328285217
translation,23,201,results,relatively high positive advantage rate,at,84.45 % and 69.35 %,relatively high positive advantage rate at 84.45 % and 69.35 %,0.5157830119132996
translation,23,201,results,results,has,our agent,results has our agent,0.5930655002593994
translation,23,208,results,transfer attack,is,not successful-only,transfer attack is not successful-only,0.6013323068618774
translation,23,208,results,not successful-only,has,- 0.13 and - 1.189 score advantage,not successful-only has - 0.13 and - 1.189 score advantage,0.5733141303062439
translation,23,208,results,results,observe,transfer attack,results observe transfer attack,0.6314852237701416
translation,23,215,results,reactive attack,could achieve,better results,reactive attack could achieve better results,0.6734520196914673
translation,23,215,results,better results,than,black - box method,better results than black - box method,0.6070464253425598
translation,23,215,results,black - box method,with,4.98 score advantage,black - box method with 4.98 score advantage,0.5911756157875061
translation,23,215,results,5.40 score advantage,against,sv agent,5.40 score advantage against sv agent,0.6367537975311279
translation,23,215,results,5.40 score advantage,against,rl agent,5.40 score advantage against rl agent,0.6273483037948608
translation,23,215,results,4.98 score advantage,against,rl agent,4.98 score advantage against rl agent,0.60593181848526
translation,23,215,results,results,observe,reactive attack,results observe reactive attack,0.6174263954162598
translation,24,6,baselines,opensource end-to - end statistical spoken dialogue system toolkit,implementations of,statistical approaches,opensource end-to - end statistical spoken dialogue system toolkit implementations of statistical approaches,0.6452878713607788
translation,24,6,baselines,statistical approaches,for,all dialogue system modules,statistical approaches for all dialogue system modules,0.576259970664978
translation,24,6,baselines,pydial,has,opensource end-to - end statistical spoken dialogue system toolkit,pydial has opensource end-to - end statistical spoken dialogue system toolkit,0.5335096120834351
translation,24,6,baselines,baselines,present,pydial,baselines present pydial,0.7319117188453674
translation,24,130,baselines,py - dial,contains,rule-based semantic decoders,py - dial contains rule-based semantic decoders,0.6052430868148804
translation,24,130,baselines,py - dial,contains,statistical decoder,py - dial contains statistical decoder,0.672235369682312
translation,24,130,baselines,rule-based semantic decoders,for,all domains,rule-based semantic decoders for all domains,0.6118730902671814
translation,24,130,baselines,statistical decoder,for,camrestaurants,statistical decoder for camrestaurants,0.6577324271202087
translation,24,130,baselines,baselines,has,py - dial,baselines has py - dial,0.5946318507194519
translation,24,131,baselines,pydial,comprises,template - based language generation,pydial comprises template - based language generation,0.6325574517250061
translation,24,131,baselines,pydial,comprises,statistical lstm - based generator,pydial comprises statistical lstm - based generator,0.6731859445571899
translation,24,131,baselines,template - based language generation,for,most domains,template - based language generation for most domains,0.5800280570983887
translation,24,131,baselines,statistical lstm - based generator,for,camrestaurants,statistical lstm - based generator for camrestaurants,0.6371926665306091
translation,24,131,baselines,baselines,has,pydial,baselines has pydial,0.6191821694374084
translation,24,18,experimental-setup,pydial,implemented in,python,pydial implemented in python,0.7109431028366089
translation,24,18,experimental-setup,experimental setup,has,pydial,experimental setup has pydial,0.6045315861701965
translation,24,2,experiments,pydial,has,multi-domain statistical dialogue,pydial has multi-domain statistical dialogue,0.566421389579773
translation,24,19,experiments,pydial,supports,multi-domain applications,pydial supports multi-domain applications,0.6748015284538269
translation,24,19,experiments,multi-domain applications,in which,conversation,multi-domain applications in which conversation,0.6758780479431152
translation,24,19,experiments,conversation,range over,number of different topics,conversation range over number of different topics,0.7369205951690674
translation,24,17,model,pydial,has,multi-domain statistical spoken dialogue system toolkit,pydial has multi-domain statistical spoken dialogue system toolkit,0.5369410514831543
translation,24,17,model,model,present,pydial,model present pydial,0.7654305100440979
translation,24,136,results,policies,achieve,task success rate,policies achieve task success rate,0.6303759217262268
translation,24,136,results,task success rate,of,95.4 %,task success rate of 95.4 %,0.5306004285812378
translation,24,136,results,task success rate,of,92.0 %,task success rate of 92.0 %,0.5250093936920166
translation,24,136,results,95.4 %,for,camrestaurants,95.4 % for camrestaurants,0.6370956897735596
translation,24,136,results,92.0 %,for,sfrestaurants,92.0 % for sfrestaurants,0.6579103469848633
translation,24,136,results,92.0 %,evaluated with,"1,000 additional dialogues","92.0 % evaluated with 1,000 additional dialogues",0.6844044923782349
translation,24,136,results,"1,000 training dialogues",has,policies,"1,000 training dialogues has policies",0.5824400782585144
translation,24,136,results,results,After,"1,000 training dialogues","results After 1,000 training dialogues",0.6560267210006714
translation,25,123,ablation-analysis,previous conversation - level rnn hidden vector ( wohid2attn ),leads to,statistically significant drops,previous conversation - level rnn hidden vector ( wohid2attn ) leads to statistically significant drops,0.6811349391937256
translation,25,123,ablation-analysis,previous conversation - level rnn hidden vector ( wohid2attn ),increases in,negative log-likelihood,previous conversation - level rnn hidden vector ( wohid2attn ) increases in negative log-likelihood,0.5974273085594177
translation,25,123,ablation-analysis,statistically significant drops,in,accuracy,statistically significant drops in accuracy,0.532297670841217
translation,25,123,ablation-analysis,negative log-likelihood,on,test partitions,negative log-likelihood on test partitions,0.5998610854148865
translation,25,123,ablation-analysis,test partitions,of,both datasets,test partitions of both datasets,0.5596939325332642
translation,25,123,ablation-analysis,ablation analysis,Removing,da connection ( woda2attn ),ablation analysis Removing da connection ( woda2attn ),0.7669100761413574
translation,25,123,ablation-analysis,ablation analysis,Removing,previous conversation - level rnn hidden vector ( wohid2attn ),ablation analysis Removing previous conversation - level rnn hidden vector ( wohid2attn ),0.7105669379234314
translation,25,97,baselines,recurrent convolutional neural network model,from,"kalchbrenner and blunsom , 2013","recurrent convolutional neural network model from kalchbrenner and blunsom , 2013",0.5405795574188232
translation,25,97,baselines,rcnn,has,recurrent convolutional neural network model,rcnn has recurrent convolutional neural network model,0.5399737358093262
translation,25,105,baselines,random forest + prev da,uses,previous da tag,random forest + prev da uses previous da tag,0.5664116740226746
translation,25,105,baselines,random forest classifier,uses,previous da tag,random forest classifier uses previous da tag,0.5758975744247437
translation,25,105,baselines,random forest,has,instance - based random forest classifier,random forest has instance - based random forest classifier,0.5738757252693176
translation,25,105,baselines,random forest + prev da,has,random forest classifier,random forest + prev da has random forest classifier,0.5831745862960815
translation,25,89,experimental-setup,word-embedding size,set to,250,word-embedding size set to 250,0.739393949508667
translation,25,89,experimental-setup,da - embedding size,to,180,da - embedding size to 180,0.5971453189849854
translation,25,89,experimental-setup,experimental setup,has,word-embedding size,experimental setup has word-embedding size,0.5191894769668579
translation,25,89,experimental-setup,experimental setup,has,da - embedding size,experimental setup has da - embedding size,0.5348323583602905
translation,25,90,experimental-setup,hidden dimension,of,utterance - level rnn,hidden dimension of utterance - level rnn,0.570122480392456
translation,25,90,experimental-setup,hidden dimension,of,conversationlevel rnn,hidden dimension of conversationlevel rnn,0.5951778292655945
translation,25,90,experimental-setup,hidden dimension,of,conversationlevel rnn,hidden dimension of conversationlevel rnn,0.5951778292655945
translation,25,90,experimental-setup,utterance - level rnn,set to,160,utterance - level rnn set to 160,0.6536930203437805
translation,25,90,experimental-setup,hidden dimension,of,conversationlevel rnn,hidden dimension of conversationlevel rnn,0.5951778292655945
translation,25,90,experimental-setup,conversationlevel rnn,set to,250,conversationlevel rnn set to 250,0.7335606217384338
translation,25,90,experimental-setup,experimental setup,has,hidden dimension,experimental setup has hidden dimension,0.5457288026809692
translation,25,90,experimental-setup,experimental setup,has,hidden dimension,experimental setup has hidden dimension,0.5457288026809692
translation,25,91,experimental-setup,experimental setup,implemented with,cnn package,experimental setup implemented with cnn package,0.6940603852272034
translation,25,5,model,"sequence of utterances ( i.e. , conversational contributions )",comprising,sequence of tokens,"sequence of utterances ( i.e. , conversational contributions ) comprising sequence of tokens",0.6129259467124939
translation,25,5,model,model,has,input,model has input,0.5536856055259705
translation,25,6,model,hierarchical nature,of,dialogue data,hierarchical nature of dialogue data,0.5786668658256531
translation,25,6,model,two nested rnns,that capture,long- range dependencies,two nested rnns that capture long- range dependencies,0.6337125897407532
translation,25,6,model,long- range dependencies,at,dialogue level and the utterance level,long- range dependencies at dialogue level and the utterance level,0.5643386244773865
translation,25,6,model,model,leverages,hierarchical nature,model leverages hierarchical nature,0.7792569994926453
translation,25,7,model,attention mechanism,focuses on,salient tokens,attention mechanism focuses on salient tokens,0.6905726194381714
translation,25,7,model,salient tokens,in,utterances,salient tokens in utterances,0.5463992357254028
translation,25,7,model,model,combined with,attention mechanism,model combined with attention mechanism,0.6568159461021423
translation,25,12,model,hierarchical rnn,for labeling,"sequence of utterances ( i.e. , contributions )","hierarchical rnn for labeling sequence of utterances ( i.e. , contributions )",0.7201958298683167
translation,25,12,model,"sequence of utterances ( i.e. , contributions )",in,dialogue,"sequence of utterances ( i.e. , contributions ) in dialogue",0.5105131268501282
translation,25,12,model,dialogue,with,dialogue acts ( das ),dialogue with dialogue acts ( das ),0.6743608713150024
translation,25,12,model,model,propose,hierarchical rnn,model propose hierarchical rnn,0.680599570274353
translation,25,22,model,attention mechanism,yielded,performance improvements,attention mechanism yielded performance improvements,0.6257524490356445
translation,25,22,model,performance improvements,in,da classification,performance improvements in da classification,0.5370557904243469
translation,25,22,model,performance improvements,compared to,traditional rnns,performance improvements compared to traditional rnns,0.6088612675666809
translation,25,22,model,da classification,compared to,traditional rnns,da classification compared to traditional rnns,0.6061885952949524
translation,25,22,model,model,has,attention mechanism,model has attention mechanism,0.5215663909912109
translation,25,22,model,model,incorporate into,model,model incorporate into model,0.6597319841384888
translation,25,22,model,model,incorporate into,attention mechanism,model incorporate into attention mechanism,0.6325598359107971
translation,25,156,model,novel hierarchical rnn,for learning,sequences of das,novel hierarchical rnn for learning sequences of das,0.6810470819473267
translation,25,156,model,model,proposed,novel hierarchical rnn,model proposed novel hierarchical rnn,0.7088868021965027
translation,25,157,model,hierarchical nature,of,dialogue data,hierarchical nature of dialogue data,0.5786668658256531
translation,25,157,model,two nested rnns,that capture,long-range dependencies,two nested rnns that capture long-range dependencies,0.6337125897407532
translation,25,157,model,long-range dependencies,at,conversation level and the utterance level,long-range dependencies at conversation level and the utterance level,0.5636979341506958
translation,25,157,model,model,leverages,hierarchical nature,model leverages hierarchical nature,0.7792569994926453
translation,25,158,model,attention mechanism,to focus on,salient tokens,attention mechanism to focus on salient tokens,0.6808393001556396
translation,25,158,model,salient tokens,in,utterances,salient tokens in utterances,0.5463992357254028
translation,25,158,model,model,combine,attention mechanism,model combine attention mechanism,0.6705759763717651
translation,25,158,model,model,with,attention mechanism,model with attention mechanism,0.594793438911438
translation,25,23,results,our empirical results,show,hierarchical rnn model,our empirical results show hierarchical rnn model,0.6342896819114685
translation,25,23,results,hierarchical rnn model,with,attentional mechanism,hierarchical rnn model with attentional mechanism,0.59487384557724
translation,25,23,results,strong baselines,on,two popular datasets,strong baselines on two popular datasets,0.4525085389614105
translation,25,23,results,maptask,has,"anderson et al. , 1991 )","maptask has anderson et al. , 1991 )",0.5707179307937622
translation,25,23,results,results,has,our empirical results,results has our empirical results,0.5055272579193115
translation,25,88,results,different embedding sizes and hidden layer dimensions,for,our model ha - rnn,different embedding sizes and hidden layer dimensions for our model ha - rnn,0.573775589466095
translation,25,88,results,results,experimented with,different embedding sizes and hidden layer dimensions,results experimented with different embedding sizes and hidden layer dimensions,0.694485068321228
translation,25,98,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,25,98,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
translation,25,98,results,results,show,our model,results show our model,0.6888449192047119
translation,25,103,results,better performance,of,our model,better performance of our model,0.562050998210907
translation,25,103,results,our model,compared to,rcnn,our model compared to rcnn,0.6302774548530579
translation,25,103,results,rcnn,shows that,summarizing utterances,rcnn shows that summarizing utterances,0.6692691445350647
translation,25,103,results,summarizing utterances,with,rnn,summarizing utterances with rnn,0.6514610052108765
translation,25,103,results,rnn,augmented with,attention architecture,rnn augmented with attention architecture,0.7079641819000244
translation,25,103,results,more effective,than using,convolution architecture,more effective than using convolution architecture,0.6772373914718628
translation,25,103,results,convolution architecture,for,da sequence labeling,convolution architecture for da sequence labeling,0.5672878623008728
translation,25,103,results,results,has,better performance,results has better performance,0.6032086610794067
translation,25,106,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,25,106,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
translation,25,106,results,baselines,has,statistically significant,baselines has statistically significant,0.5406145453453064
translation,25,106,results,results,show,our model,results show our model,0.6888449192047119
translation,25,107,results,insights,taking into account,conversational dependencies,insights taking into account conversational dependencies,0.6364662051200867
translation,25,107,results,conversational dependencies,between,das,conversational dependencies between das,0.6617792844772339
translation,25,107,results,conversational dependencies,has,substantially improves,conversational dependencies has substantially improves,0.5663539171218872
translation,25,107,results,das,has,substantially improves,das has substantially improves,0.6347908973693848
translation,25,107,results,substantially improves,has,da - labeling performance,substantially improves has da - labeling performance,0.54067063331604
translation,25,107,results,results,reinforce,insights,results reinforce insights,0.6790160536766052
translation,25,131,results,difference,between,performance,difference between performance,0.6736118197441101
translation,25,131,results,performance,of,ha - rnn and woconvrnn,performance of ha - rnn and woconvrnn,0.5915918350219727
translation,25,131,results,performance,is,statistically significant,performance is statistically significant,0.5685962438583374
translation,25,131,results,ha - rnn and woconvrnn,is,statistically significant,ha - rnn and woconvrnn is statistically significant,0.5918160080909729
translation,25,131,results,statistically significant,for,test set,statistically significant for test set,0.6348453760147095
translation,25,131,results,results,has,difference,results has difference,0.5636705756187439
translation,25,142,results,woda2attn,no change in,performance,woda2attn no change in performance,0.7679547071456909
translation,25,142,results,performance,between,oracle and greedy conditions,performance between oracle and greedy conditions,0.6190237998962402
translation,25,142,results,results,has,woda2attn,results has woda2attn,0.5598973631858826
translation,25,143,results,models,employ,da connection,models employ da connection,0.5789431929588318
translation,25,143,results,da connection,to compute,attention signal,da connection to compute attention signal,0.656565248966217
translation,25,143,results,da connection,show,slight improvement,da connection show slight improvement,0.6556470990180969
translation,25,143,results,slight improvement,in,accuracy,slight improvement in accuracy,0.5197557806968689
translation,25,143,results,slight improvement,when using,correct da,slight improvement when using correct da,0.6911855340003967
translation,25,143,results,accuracy,when using,correct da,accuracy when using correct da,0.6256877183914185
translation,25,143,results,correct da,as,input,correct da as input,0.6009160876274109
translation,25,143,results,correct da,instead of,predicted da,correct da instead of predicted da,0.5791937112808228
translation,25,143,results,results,has,models,results has models,0.5335168838500977
translation,25,144,results,wda2da,shows,large improvements,wda2da shows large improvements,0.7065448760986328
translation,25,144,results,large improvements,when using,correct da,large improvements when using correct da,0.7522609233856201
translation,25,144,results,3.5 %,on,switchboard,3.5 % on switchboard,0.5284436345100403
translation,25,144,results,6.8 %,on,maptask,6.8 % on maptask,0.5376526117324829
translation,25,144,results,correct da,has,3.5 %,correct da has 3.5 %,0.5658602714538574
translation,25,144,results,results,has,wda2da,results has wda2da,0.5372153520584106
translation,26,187,experimental-setup,our model,using,tensorflow,our model using tensorflow,0.676898181438446
translation,26,187,experimental-setup,tensorflow,using,sgd,tensorflow using sgd,0.7018362879753113
translation,26,187,experimental-setup,sgd,as,optimizer,sgd as optimizer,0.5566533803939819
translation,26,187,experimental-setup,optimizer,with,learning rate,optimizer with learning rate,0.6228271722793579
translation,26,187,experimental-setup,optimizer,with,batch size,optimizer with batch size,0.6249857544898987
translation,26,187,experimental-setup,learning rate,of,0.1,learning rate of 0.1,0.6136453747749329
translation,26,187,experimental-setup,batch size,of,64,batch size of 64,0.6741159558296204
translation,26,187,experimental-setup,experimental setup,implemented,our model,experimental setup implemented our model,0.7312729358673096
translation,26,188,experimental-setup,seq2seq model,implemented using,4 layers of gru,seq2seq model implemented using 4 layers of gru,0.6267367005348206
translation,26,188,experimental-setup,4 layers of gru,with,hidden unit 384,4 layers of gru with hidden unit 384,0.6480504870414734
translation,26,188,experimental-setup,experimental setup,has,seq2seq model,experimental setup has seq2seq model,0.5299274921417236
translation,26,224,results,bleu score,comparing,generated response,bleu score comparing generated response,0.6551390290260315
translation,26,224,results,bleu score,comparing,ground truth,bleu score comparing ground truth,0.5676714181900024
translation,26,224,results,ground truth,is,around 23,ground truth is around 23,0.627153754234314
translation,26,224,results,ground truth,around,68.7,ground truth around 68.7,0.6493256688117981
translation,26,224,results,68.7,for,synthesized data,68.7 for synthesized data,0.579513430595398
translation,26,224,results,around 23,for,airdialogue,around 23 for airdialogue,0.6872755289077759
translation,26,224,results,results,has,bleu score,results has bleu score,0.5436024069786072
translation,26,232,results,significantly improvements,across,all measures,significantly improvements across all measures,0.6327418684959412
translation,26,232,results,all measures,for,self - play models,all measures for self - play models,0.5430395603179932
translation,26,232,results,self - play models,compare to,supervised learning models,self - play models compare to supervised learning models,0.662011444568634
translation,26,232,results,results,see,significantly improvements,results see significantly improvements,0.5976051688194275
translation,27,162,ablation-analysis,number of contradicting utterances,predicted by,model,number of contradicting utterances predicted by model,0.7318599820137024
translation,27,162,ablation-analysis,number of utterances,entail,profile sentence,number of utterances entail profile sentence,0.6979330778121948
translation,27,162,ablation-analysis,nli reranking,has,substantially reduces,nli reranking has substantially reduces,0.572862446308136
translation,27,162,ablation-analysis,substantially reduces,has,number of contradicting utterances,substantially reduces has number of contradicting utterances,0.5733271241188049
translation,27,162,ablation-analysis,increases,has,number of utterances,increases has number of utterances,0.6167212128639221
translation,27,162,ablation-analysis,ablation analysis,has,nli reranking,ablation analysis has nli reranking,0.5254427790641785
translation,27,114,baselines,sentence encoding method,use,in-fersent,sentence encoding method use in-fersent,0.6070560812950134
translation,27,114,baselines,in-fersent,encodes,sentence,in-fersent encodes sentence,0.770710289478302
translation,27,114,baselines,sentence,using,bidirectional lstm,sentence using bidirectional lstm,0.6728272438049316
translation,27,114,baselines,bidirectional lstm,followed by,max-pooling,bidirectional lstm followed by max-pooling,0.6328104138374329
translation,27,114,baselines,max-pooling,over,output states,max-pooling over output states,0.6672172546386719
translation,27,19,experiments,dialogue nli,contains,sentence pairs,dialogue nli contains sentence pairs,0.6105157136917114
translation,27,19,experiments,sentence pairs,labeled as,entailment,sentence pairs labeled as entailment,0.6242654323577881
translation,27,19,experiments,sentence pairs,labeled as,neutral,sentence pairs labeled as neutral,0.7112109661102295
translation,27,19,experiments,sentence pairs,labeled as,contradiction,sentence pairs labeled as contradiction,0.6894110441207886
translation,27,19,experiments,dataset,has,dialogue nli,dataset has dialogue nli,0.6026914119720459
translation,27,136,experiments,model,using,"parlai ( miller et al. , 2017 )","model using parlai ( miller et al. , 2017 )",0.6465233564376831
translation,27,136,experiments,model,using,hyper-parameters,model using hyper-parameters,0.645788311958313
translation,27,136,experiments,"parlai ( miller et al. , 2017 )",on,personachat : self original task,"parlai ( miller et al. , 2017 ) on personachat : self original task",0.5041944980621338
translation,27,136,experiments,personachat : self original task,using,hyper-parameters,personachat : self original task using hyper-parameters,0.6337425112724304
translation,27,136,experiments,hyper-parameters,given for,kvmemnnagent,hyper-parameters given for kvmemnnagent,0.7058922648429871
translation,27,20,model,nli,to improve,consistency,nli to improve consistency,0.7413736581802368
translation,27,20,model,consistency,of,dialogue models,consistency of dialogue models,0.5954213738441467
translation,27,20,model,dialogue models,using,simple method,dialogue models using simple method,0.71088707447052
translation,27,20,model,simple method,where,utterances,simple method where utterances,0.634517252445221
translation,27,20,model,utterances,are,re-ranked,utterances are re-ranked,0.6218205690383911
translation,27,20,model,re-ranked,using,nli model,re-ranked using nli model,0.6844402551651001
translation,27,20,model,nli model,trained on,dialogue nli,nli model trained on dialogue nli,0.7460179328918457
translation,27,20,model,model,demonstrate,nli,model demonstrate nli,0.689741849899292
translation,27,120,results,test performance,of,esim (,test performance of esim (,0.6070587635040283
translation,27,120,results,esim (,has,88.2 % ) and infersent ( 85.68 % ),esim ( has 88.2 % ) and infersent ( 85.68 % ),0.555364727973938
translation,27,120,results,results,on,dialogue nli gold test set,results on dialogue nli gold test set,0.5300174355506897
translation,27,120,results,results,has,test performance,results has test performance,0.5774044394493103
translation,27,121,results,infersent model,trained on,snli,infersent model trained on snli,0.7228800654411316
translation,27,121,results,infersent model,performs,poorly,infersent model performs poorly,0.7180392742156982
translation,27,121,results,poorly,evaluated on,proposed dialogue nli,poorly evaluated on proposed dialogue nli,0.7734512090682983
translation,27,121,results,proposed dialogue nli,has,47.03 % ),proposed dialogue nli has 47.03 % ),0.5706567764282227
translation,27,123,results,lower,than,hypothesis-only baseline,lower than hypothesis-only baseline,0.6033267974853516
translation,27,123,results,hypothesis-only baseline,for,snli,hypothesis-only baseline for snli,0.6267415881156921
translation,27,123,results,hypothesisonly performance,has,51.52 %,hypothesisonly performance has 51.52 %,0.5612112283706665
translation,27,123,results,snli,has,69.00 %,snli has 69.00 %,0.5927619934082031
translation,27,123,results,results,has,hypothesisonly performance,results has hypothesisonly performance,0.5145087242126465
translation,27,159,results,re-ranking,on,three evaluation sets,re-ranking on three evaluation sets,0.5106030106544495
translation,27,160,results,nli re-ranking,improves,all three metrics,nli re-ranking improves all three metrics,0.67841637134552
translation,27,160,results,all three metrics,on,all the evaluation sets,all three metrics on all the evaluation sets,0.5140689015388489
translation,27,160,results,results,has,nli re-ranking,results has nli re-ranking,0.5575169920921326
translation,27,161,results,improves,measured by,hits@1,improves measured by hits@1,0.7441035509109497
translation,27,161,results,dialogue performance,has,improves,dialogue performance has improves,0.62739497423172
translation,27,161,results,results,has,dialogue performance,results has dialogue performance,0.549129068851471
translation,27,181,results,natural language inference re-ranking,improves,all the metrics,natural language inference re-ranking improves all the metrics,0.6242504715919495
translation,27,181,results,natural language inference re-ranking,notably,contradiction score,natural language inference re-ranking notably contradiction score,0.6448544859886169
translation,27,181,results,all the metrics,notably,contradiction score,all the metrics notably contradiction score,0.6657606959342957
translation,27,181,results,fine- grained consistency score,has,0.27 vs. 0.35 ),fine- grained consistency score has 0.27 vs. 0.35 ),0.550446093082428
translation,27,181,results,contradiction score,has,0.25 vs. 0.16 ),contradiction score has 0.25 vs. 0.16 ),0.5802552103996277
translation,27,181,results,results,has,natural language inference re-ranking,results has natural language inference re-ranking,0.5027092695236206
translation,27,182,results,results,with,conclusions,results with conclusions,0.6156105995178223
translation,27,182,results,results,has,results,results has results,0.48582205176353455
translation,28,9,model,computational study,of,tolerance,computational study of tolerance,0.6387404203414917
translation,28,9,model,tolerance,in the context of,online discussions,tolerance in the context of online discussions,0.7233971357345581
translation,28,9,model,model,perform,computational study,model perform computational study,0.6236699223518372
translation,29,204,ablation-analysis,our self-conscious agent s 1,increases,entail@1,our self-conscious agent s 1 increases entail@1,0.6569880843162537
translation,29,204,ablation-analysis,entail@1,along with,hits@1 accuracy,entail@1 along with hits@1 accuracy,0.5988909602165222
translation,29,204,ablation-analysis,hits@1 accuracy,of,literal agents s 0,hits@1 accuracy of literal agents s 0,0.5909858345985413
translation,29,204,ablation-analysis,our self-conscious agent s 1,has,significantly reduces,our self-conscious agent s 1 has significantly reduces,0.5660988092422485
translation,29,204,ablation-analysis,significantly reduces,has,contradict@1 scores,significantly reduces has contradict@1 scores,0.6184026598930359
translation,29,204,ablation-analysis,ablation analysis,has,our self-conscious agent s 1,ablation analysis has our self-conscious agent s 1,0.5780127644538879
translation,29,187,baselines,tranfertransfo,based on,gpt,tranfertransfo based on gpt,0.7096854448318481
translation,29,187,baselines,baselines,has,tranfertransfo,baselines has tranfertransfo,0.58018559217453
translation,29,8,model,existing dialogue agents,with,public self -consciousness,existing dialogue agents with public self -consciousness,0.6379289031028748
translation,29,8,model,on the fly,through,imaginary listener,on the fly through imaginary listener,0.6987037658691406
translation,29,8,model,public self -consciousness,has,on the fly,public self -consciousness has on the fly,0.5666679739952087
translation,29,8,model,model,endow,existing dialogue agents,model endow existing dialogue agents,0.6318719387054443
translation,29,10,model,framework,by learning,distractor selection,framework by learning distractor selection,0.6683878898620605
translation,29,10,model,model,extend,framework,model extend framework,0.6595011353492737
translation,29,55,model,learning method,for,distractor selection,learning method for distractor selection,0.5969011783599854
translation,29,55,model,model,extend,rational speech acts framework,model extend rational speech acts framework,0.6728973388671875
translation,29,70,model,rsa framework,be adopted in,dialogue agents,rsa framework be adopted in dialogue agents,0.6846738457679749
translation,29,70,model,dialogue agents,to alleviate,inconsistency problem,dialogue agents to alleviate inconsistency problem,0.6809588670730591
translation,29,70,model,model,explore,rsa framework,model explore rsa framework,0.6908714175224304
translation,29,56,results,our approach,improves,consistency,our approach improves consistency,0.7089545130729675
translation,29,56,results,consistency,of,three recent generative agents,consistency of three recent generative agents,0.5805359482765198
translation,29,56,results,consistency,of,personachat,consistency of personachat,0.6682759523391724
translation,29,56,results,consistency,over,dialogue nli,consistency over dialogue nli,0.7116795182228088
translation,29,56,results,consistency,over,personachat,consistency over personachat,0.6987576484680176
translation,29,56,results,results,has,our approach,results has our approach,0.6050099730491638
translation,29,211,results,learned distractors,are,more effective,learned distractors are more effective,0.5395485162734985
translation,29,211,results,more effective,than,random,more effective than random,0.6380597352981567
translation,29,211,results,random,for,pragmatic agents,random for pragmatic agents,0.6474258303642273
translation,29,211,results,results,concludes,learned distractors,results concludes learned distractors,0.6020389199256897
translation,29,214,results,all other generative dialogue agents,in terms of,consistency related metrics,all other generative dialogue agents in terms of consistency related metrics,0.6097281575202942
translation,29,214,results,our model s 1,has,outperforms,our model s 1 has outperforms,0.6409822702407837
translation,29,214,results,outperforms,has,all other generative dialogue agents,outperforms has all other generative dialogue agents,0.5882323384284973
translation,29,214,results,results,has,our model s 1,results has our model s 1,0.5703251361846924
translation,29,216,results,our approach,improves,f1 score,our approach improves f1 score,0.6402660608291626
translation,29,216,results,f1 score,for,transfertransfo and blender,f1 score for transfertransfo and blender,0.6472267508506775
translation,29,216,results,results,has,our approach,results has our approach,0.6050099730491638
translation,29,221,results,base agents,with,no self -consciousness,base agents with no self -consciousness,0.634986937046051
translation,29,221,results,base agents,improve,consistency,base agents improve consistency,0.6781046986579895
translation,29,221,results,our agents,improve,consistency,our agents improve consistency,0.6794845461845398
translation,29,221,results,consistency,in,all three metrics,consistency in all three metrics,0.5016520023345947
translation,29,221,results,consistency,when using,additional nli models,consistency when using additional nli models,0.7588320970535278
translation,29,221,results,base agents,has,our agents,base agents has our agents,0.5932108759880066
translation,29,221,results,no self -consciousness,has,our agents,no self -consciousness has our agents,0.5698760151863098
translation,29,221,results,results,Compared to,base agents,results Compared to base agents,0.689288854598999
translation,29,222,results,our agents without nli,for,controlseq2seq and transfertransfo,our agents without nli for controlseq2seq and transfertransfo,0.6885471343994141
translation,29,222,results,base agents with nli,on,hits@1,base agents with nli on hits@1,0.5643938779830933
translation,29,222,results,controlseq2seq and transfertransfo,has,outperform,controlseq2seq and transfertransfo has outperform,0.5756271481513977
translation,29,222,results,outperform,has,base agents with nli,outperform has base agents with nli,0.6364248394966125
translation,29,222,results,base agents with nli,has,nli ),base agents with nli has nli ),0.655883252620697
translation,29,252,results,outperforms,in terms of,consistency,outperforms in terms of consistency,0.6786410212516785
translation,29,252,results,other literal agents,on,all three datasets,other literal agents on all three datasets,0.5025186538696289
translation,29,252,results,other literal agents,in terms of,consistency,other literal agents in terms of consistency,0.6277827024459839
translation,29,252,results,s 1 agent,has,outperforms,s 1 agent has outperforms,0.6345032453536987
translation,29,252,results,outperforms,has,other literal agents,outperforms has other literal agents,0.6173899173736572
translation,29,252,results,results,has,s 1 agent,results has s 1 agent,0.5155089497566223
translation,30,5,baselines,baselines,has,sequenceto-sequence,baselines has sequenceto-sequence,0.6093053221702576
translation,30,41,baselines,set of heuristics,to derive,training data,set of heuristics to derive training data,0.6534413695335388
translation,30,41,baselines,set of heuristics,train,retriever,set of heuristics train retriever,0.704217791557312
translation,30,41,baselines,set of heuristics,train,retriever,set of heuristics train retriever,0.704217791557312
translation,30,41,baselines,retriever,in,distant supervised fashion,retriever in distant supervised fashion,0.5422683358192444
translation,30,41,baselines,retriever,along with,seq2seq dialogue generation model,retriever along with seq2seq dialogue generation model,0.62153559923172
translation,30,41,baselines,"gumbel -softmax ( jang et al. , 2017 )",as,approximation,"gumbel -softmax ( jang et al. , 2017 ) as approximation",0.5222958922386169
translation,30,41,baselines,approximation,of,non-differentiable selecting process,approximation of non-differentiable selecting process,0.5360622406005859
translation,30,41,baselines,retriever,along with,seq2seq dialogue generation model,retriever along with seq2seq dialogue generation model,0.62153559923172
translation,30,148,baselines,simple attention,over,input context,simple attention over input context,0.6186004877090454
translation,30,148,baselines,input context,at,each time step,input context at each time step,0.5322632193565369
translation,30,148,baselines,baselines,including,attn seq2seq,baselines including attn seq2seq,0.6682724952697754
translation,30,149,baselines,sequenceto-sequence architecture,with,attention - based copy mechanism,sequenceto-sequence architecture with attention - based copy mechanism,0.6317005753517151
translation,30,149,baselines,attention - based copy mechanism,over,encoder context,attention - based copy mechanism over encoder context,0.6346468329429626
translation,30,149,baselines,ptr-unk,has,ptr-unk,ptr-unk has ptr-unk,0.650722324848175
translation,30,149,baselines,baselines,has,ptr-unk,baselines has ptr-unk,0.5735282897949219
translation,30,149,baselines,baselines,has,ptr-unk,baselines has ptr-unk,0.5735282897949219
translation,30,150,baselines,baselines,has,"kv net ( eric et al. , 2017 )","baselines has kv net ( eric et al. , 2017 )",0.5359456539154053
translation,30,152,baselines,dialogue history and kb entities,as,input,dialogue history and kb entities as input,0.5399994850158691
translation,30,152,baselines,pointer gate,to control,generating,pointer gate to control generating,0.7047242522239685
translation,30,152,baselines,pointer gate,selecting,input,pointer gate selecting input,0.7357531785964966
translation,30,152,baselines,input,as,output,input as output,0.5734860301017761
translation,30,152,baselines,mem2seq,has,mem2seq,mem2seq has mem2seq,0.6353585124015808
translation,30,152,baselines,generating,has,vocabulary word,generating has vocabulary word,0.5921920537948608
translation,30,152,baselines,baselines,has,mem2seq,baselines has mem2seq,0.5307025909423828
translation,30,152,baselines,baselines,has,mem2seq,baselines has mem2seq,0.5307025909423828
translation,30,153,baselines,dsr,leveraged,dialogue state representation,dsr leveraged dialogue state representation,0.5485824346542358
translation,30,153,baselines,dsr,applied,copying mechanism,dsr applied copying mechanism,0.6685357689857483
translation,30,153,baselines,dialogue state representation,to retrieve,kb,dialogue state representation to retrieve kb,0.7298538684844971
translation,30,153,baselines,dialogue state representation,applied,copying mechanism,dialogue state representation applied copying mechanism,0.6854256987571716
translation,30,153,baselines,copying mechanism,to retrieve,entities,copying mechanism to retrieve entities,0.6910123825073242
translation,30,153,baselines,entities,from,knowledge base,entities from knowledge base,0.5297764539718628
translation,30,153,baselines,baselines,has,dsr,baselines has dsr,0.5560033321380615
translation,30,140,experiments,three - hop memory network,to model,our kb - retriever,three - hop memory network to model our kb - retriever,0.6941235065460205
translation,30,154,experiments,incar dataset,for,"attn seq2seq , ptr-unk and mem2seq","incar dataset for attn seq2seq , ptr-unk and mem2seq",0.6080325841903687
translation,30,141,hyperparameters,dimensionalities,of,embedding,dimensionalities of embedding,0.6057977080345154
translation,30,141,hyperparameters,dimensionalities,of,lstm hidden units,dimensionalities of lstm hidden units,0.5462164282798767
translation,30,141,hyperparameters,embedding,selected from,"{ 100 , 200 }","embedding selected from { 100 , 200 }",0.654466450214386
translation,30,141,hyperparameters,lstm hidden units,selected from,"{ 50 , 100 , 150 , 200 , 350 }","lstm hidden units selected from { 50 , 100 , 150 , 200 , 350 }",0.5679861307144165
translation,30,141,hyperparameters,hyperparameters,has,dimensionalities,hyperparameters has dimensionalities,0.5054460167884827
translation,30,142,hyperparameters,dropout,use in,our framework,dropout use in our framework,0.6269780993461609
translation,30,142,hyperparameters,dropout,selected from,"{ 0.25 , 0.5 , 0.75 }","dropout selected from { 0.25 , 0.5 , 0.75 }",0.5981801748275757
translation,30,142,hyperparameters,our framework,selected from,"{ 0.25 , 0.5 , 0.75 }","our framework selected from { 0.25 , 0.5 , 0.75 }",0.5780277252197266
translation,30,142,hyperparameters,batch size,selected from,"{ 1 , 2 }","batch size selected from { 1 , 2 }",0.6006180047988892
translation,30,142,hyperparameters,hyperparameters,has,dropout,hyperparameters has dropout,0.5324090719223022
translation,30,143,hyperparameters,l2 regularization,used on,our model,l2 regularization used on our model,0.6287559270858765
translation,30,143,hyperparameters,our model,with,tension,our model with tension,0.6905981302261353
translation,30,143,hyperparameters,tension,of,5 ? 10 ?6,tension of 5 ? 10 ?6,0.638383150100708
translation,30,143,hyperparameters,tension,for reducing,overfitting,tension for reducing overfitting,0.6913546323776245
translation,30,143,hyperparameters,hyperparameters,has,l2 regularization,hyperparameters has l2 regularization,0.455279141664505
translation,30,144,hyperparameters,retriever,with,distant supervision,retriever with distant supervision,0.6438716053962708
translation,30,144,hyperparameters,retriever,adopt,weight typing trick,retriever adopt weight typing trick,0.6677762866020203
translation,30,144,hyperparameters,hyperparameters,For training,retriever,hyperparameters For training retriever,0.6984830498695374
translation,30,145,hyperparameters,"adam ( kingma and ba , 2014 )",to optimize,parameters,"adam ( kingma and ba , 2014 ) to optimize parameters",0.6932469606399536
translation,30,145,hyperparameters,"adam ( kingma and ba , 2014 )",adopt,suggested hyper-parameters,"adam ( kingma and ba , 2014 ) adopt suggested hyper-parameters",0.6315122842788696
translation,30,145,hyperparameters,parameters,in,our model,parameters in our model,0.5232008695602417
translation,30,145,hyperparameters,suggested hyper-parameters,for,optimization,suggested hyper-parameters for optimization,0.6171027421951294
translation,30,145,hyperparameters,hyperparameters,adopt,suggested hyper-parameters,hyperparameters adopt suggested hyper-parameters,0.6021699905395508
translation,30,6,model,queries,in,two steps,queries in two steps,0.6137292981147766
translation,30,6,model,kb,in,two steps,kb in two steps,0.6136826872825623
translation,30,6,model,consistency,of,generated entities,consistency of generated entities,0.5578870177268982
translation,30,6,model,queries,has,kb,queries has kb,0.597358226776123
translation,30,6,model,model,propose,novel framework,model propose novel framework,0.720393180847168
translation,30,7,model,kb retrieval component,explicitly returns,most relevant kb row,kb retrieval component explicitly returns most relevant kb row,0.8033205270767212
translation,30,7,model,most relevant kb row,given,dialogue history,most relevant kb row given dialogue history,0.7130727171897888
translation,30,7,model,model,introduce,kb retrieval component,model introduce kb retrieval component,0.6287848949432373
translation,30,8,model,retrieval result,filter,irrelevant entities,retrieval result filter irrelevant entities,0.7180167436599731
translation,30,8,model,irrelevant entities,in,seq2seq response generation model,irrelevant entities in seq2seq response generation model,0.48557496070861816
translation,30,8,model,irrelevant entities,to improve,consistency,irrelevant entities to improve consistency,0.6405729651451111
translation,30,8,model,consistency,among,output entities,consistency among output entities,0.5993979573249817
translation,30,8,model,model,has,retrieval result,model has retrieval result,0.5659644603729248
translation,30,9,model,attention mechanism,to address,most correlated kb column,attention mechanism to address most correlated kb column,0.5841437578201294
translation,30,9,model,model,perform,attention mechanism,model perform attention mechanism,0.5804224014282227
translation,30,35,model,retrieval module - kb - retriever,to explicitly query,kb,retrieval module - kb - retriever to explicitly query kb,0.728213369846344
translation,30,36,model,memory network,to select,most relevant row,memory network to select most relevant row,0.7339083552360535
translation,30,110,model,first method,take advantage of,similarity,first method take advantage of similarity,0.6587122678756714
translation,30,110,model,first method,design,set of heuristics,first method design set of heuristics,0.627185583114624
translation,30,110,model,similarity,between,surface string of kb entries,similarity between surface string of kb entries,0.6025433540344238
translation,30,110,model,similarity,between,reference response,similarity between reference response,0.6841987371444702
translation,30,110,model,set of heuristics,to extract,training data,set of heuristics to extract training data,0.7042748332023621
translation,30,110,model,model,In,first method,model In first method,0.5475743412971497
translation,30,43,results,retrievers,trained with,distant -supervision and gumbel - softmax technique,retrievers trained with distant -supervision and gumbel - softmax technique,0.7617873549461365
translation,30,43,results,compared systems,in,automatic and human evaluations,compared systems in automatic and human evaluations,0.5022733807563782
translation,30,43,results,distant -supervision and gumbel - softmax technique,has,outperform,distant -supervision and gumbel - softmax technique has outperform,0.5958804488182068
translation,30,43,results,outperform,has,compared systems,outperform has compared systems,0.6319670081138611
translation,30,43,results,results,Both,retrievers,results Both retrievers,0.6883437633514404
translation,30,43,results,results,has,retrievers,results has retrievers,0.5741752982139587
translation,30,162,results,our framework,with,two methods,our framework with two methods,0.5949563980102539
translation,30,162,results,two methods,still outperform,kv net,two methods still outperform kv net,0.6716899871826172
translation,30,162,results,kv net,in,in - car dataset,kv net in in - car dataset,0.5435558557510376
translation,30,164,results,our framework,trained with,distant supervision,our framework trained with distant supervision,0.6824221014976501
translation,30,164,results,our framework,trained with,the gumbel - softmax,our framework trained with the gumbel - softmax,0.703748345375061
translation,30,164,results,the gumbel - softmax,beats,all existing models,the gumbel - softmax beats all existing models,0.6816356778144836
translation,30,164,results,all existing models,on,two datasets,all existing models on two datasets,0.5231648087501526
translation,30,165,results,outperforms,on,bleu and f1 metrics,outperforms on bleu and f1 metrics,0.5334573984146118
translation,30,165,results,each baseline,on,bleu and f1 metrics,each baseline on bleu and f1 metrics,0.5132551193237305
translation,30,165,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,30,165,results,outperforms,has,each baseline,outperforms has each baseline,0.6010501980781555
translation,30,165,results,results,has,our model,results has our model,0.5871725678443909
translation,30,166,results,our model,with,gumbel - softmax,our model with gumbel - softmax,0.6484269499778748
translation,30,166,results,highest bleu,compared with,baselines,highest bleu compared with baselines,0.6206364631652832
translation,30,166,results,our framework,generate,more fluent response,our framework generate more fluent response,0.6891774535179138
translation,30,166,results,incar dataset,has,our model,incar dataset has our model,0.5776952505111694
translation,30,166,results,gumbel - softmax,has,highest bleu,gumbel - softmax has highest bleu,0.5645369291305542
translation,30,166,results,results,In,incar dataset,results In incar dataset,0.5389012098312378
translation,30,167,results,our framework,achieved,2.5 % improvement,our framework achieved 2.5 % improvement,0.6443712115287781
translation,30,167,results,our framework,achieved,1.8 % improvement,our framework achieved 1.8 % improvement,0.6469379663467407
translation,30,167,results,our framework,achieved,3.5 % improvement,our framework achieved 3.5 % improvement,0.646773099899292
translation,30,167,results,2.5 % improvement,on,navigate domain,2.5 % improvement on navigate domain,0.5309156775474548
translation,30,167,results,1.8 % improvement,on,weather domain,1.8 % improvement on weather domain,0.5020844340324402
translation,30,167,results,3.5 % improvement,on,calendar domain,3.5 % improvement on calendar domain,0.5193875432014465
translation,30,167,results,3.5 % improvement,on,f1 metric,3.5 % improvement on f1 metric,0.528126060962677
translation,30,167,results,results,has,our framework,results has our framework,0.6097875237464905
translation,30,169,results,effectiveness,of,our framework,effectiveness of our framework,0.586419403553009
translation,30,169,results,camrest dataset,has,same trend of improvement,camrest dataset has same trend of improvement,0.5975175499916077
translation,30,169,results,results,In,camrest dataset,results In camrest dataset,0.5294339656829834
translation,30,170,results,model,trained with,gumbel - softmax,model trained with gumbel - softmax,0.7374933958053589
translation,30,170,results,gumbel - softmax,outperforms with,distant supervision method,gumbel - softmax outperforms with distant supervision method,0.7590110898017883
translation,30,204,results,other baseline models,on,all metrics,other baseline models on all metrics,0.4505794942378998
translation,30,204,results,our framework,has,outperforms,our framework has outperforms,0.6379533410072327
translation,30,204,results,outperforms,has,other baseline models,outperforms has other baseline models,0.570330023765564
translation,30,204,results,results,has,our framework,results has our framework,0.6097875237464905
translation,30,205,results,most significant improvement,from,correctness,most significant improvement from correctness,0.5839322805404663
translation,30,205,results,correctness,indicating,our model,correctness indicating our model,0.6417049765586853
translation,30,205,results,our model,retrieve,accurate entity,our model retrieve accurate entity,0.6557450890541077
translation,30,205,results,our model,generate,more informative information,our model generate more informative information,0.6460587978363037
translation,30,205,results,accurate entity,from,kb,accurate entity from kb,0.5366235971450806
translation,30,205,results,results,has,most significant improvement,results has most significant improvement,0.5218278765678406
translation,31,4,baselines,negotiation strategies,within,online version of the game,negotiation strategies within online version of the game,0.6413488984107971
translation,31,64,results,win rate,well below,25 % win rate,win rate well below 25 % win rate,0.7555754780769348
translation,31,64,results,deep learning bot,beats,human players,deep learning bot beats human players,0.7113407254219055
translation,31,67,results,bots,using,negotiation strategy,bots using negotiation strategy,0.6688376069068909
translation,31,67,results,negotiation strategy,with,persuasion,negotiation strategy with persuasion,0.6122219562530518
translation,31,67,results,human players,achieved,lower win rates,human players achieved lower win rates,0.6521864533424377
translation,31,67,results,lower win rates,than against,bot,lower win rates than against bot,0.708721399307251
translation,31,67,results,lower win rates,than against,bot,lower win rates than against bot,0.708721399307251
translation,31,67,results,bot,with,"original , rule- based negotiation strategy","bot with original , rule- based negotiation strategy",0.6365084648132324
translation,31,67,results,bot,with,supervised learning strategy,bot with supervised learning strategy,0.6634758114814758
translation,31,67,results,bot,with,supervised learning strategy,bot with supervised learning strategy,0.6634758114814758
translation,31,67,results,much lower win rates,than,bot,much lower win rates than bot,0.5867322683334351
translation,31,67,results,bot,with,supervised learning strategy,bot with supervised learning strategy,0.6634758114814758
translation,31,67,results,bots,has,human players,bots has human players,0.585528552532196
translation,31,67,results,negotiation strategy,has,human players,negotiation strategy has human players,0.5415830612182617
translation,31,67,results,persuasion,has,human players,persuasion has human players,0.5587958693504333
translation,31,67,results,supervised learning strategy,has,26.7 % vs. 44.4 %,supervised learning strategy has 26.7 % vs. 44.4 %,0.5422544479370117
translation,31,67,results,results,Against,bots,results Against bots,0.6177919507026672
translation,31,68,results,persuasion and deep learning bots,has,outperform,persuasion and deep learning bots has outperform,0.5891299843788147
translation,31,68,results,outperform,has,rule-based and supervised learning baselines,outperform has rule-based and supervised learning baselines,0.5890687108039856
translation,31,68,results,results,In terms of,average victory points,results In terms of average victory points,0.6976550221443176
translation,32,219,ablation-analysis,result,indicates,two systems,result indicates two systems,0.7077785730361938
translation,32,219,ablation-analysis,two systems,complementary to,each other,two systems complementary to each other,0.6947790384292603
translation,32,219,ablation-analysis,simple combination,is,already effective,simple combination is already effective,0.5268653631210327
translation,32,219,ablation-analysis,already effective,in providing,significant performance boost,already effective in providing significant performance boost,0.657691478729248
translation,32,219,ablation-analysis,ablation analysis,has,result,ablation analysis has result,0.5194553136825562
translation,32,173,hyperparameters,number of kernels,of,encoder,number of kernels of encoder,0.608319878578186
translation,32,173,hyperparameters,of ma - cnn,to be,300,of ma - cnn to be 300,0.5536624789237976
translation,32,173,hyperparameters,hyperparameters,set,number of kernels,hyperparameters set number of kernels,0.6144114136695862
translation,32,174,hyperparameters,kernels,widths,3 to 5,kernels widths 3 to 5,0.7666906714439392
translation,32,174,hyperparameters,3 to 5,for,cnn encoder,3 to 5 for cnn encoder,0.5922132134437561
translation,32,174,hyperparameters,hyperparameters,use,kernels,hyperparameters use kernels,0.6408552527427673
translation,32,175,hyperparameters,non-linearities,in,models,non-linearities in models,0.5616983771324158
translation,32,175,hyperparameters,non-linearities,are,rectified linear units nair and hinton ( 2010 ),non-linearities are rectified linear units nair and hinton ( 2010 ),0.5551540851593018
translation,32,175,hyperparameters,hyperparameters,has,non-linearities,hyperparameters has non-linearities,0.5494210124015808
translation,32,176,hyperparameters,"adadelta ( zeiler , 2012 )",as,optimizer,"adadelta ( zeiler , 2012 ) as optimizer",0.5067919492721558
translation,32,176,hyperparameters,"adadelta ( zeiler , 2012 )",use,recommended values,"adadelta ( zeiler , 2012 ) use recommended values",0.6665704250335693
translation,32,176,hyperparameters,optimizer,for,whole ma - cnn,optimizer for whole ma - cnn,0.5762012004852295
translation,32,176,hyperparameters,recommended values,for,hyperparameters,recommended values for hyperparameters,0.5706246495246887
translation,32,176,hyperparameters,hyperparameters,has,"? = 0.9 , = 1 ? 10 ?6 , learning rate = 1.0 )","hyperparameters has ? = 0.9 , = 1 ? 10 ?6 , learning rate = 1.0 )",0.5464325547218323
translation,32,176,hyperparameters,hyperparameters,use,recommended values,hyperparameters use recommended values,0.6471703052520752
translation,32,177,hyperparameters,embeddings,with,word2 vec,embeddings with word2 vec,0.6573508977890015
translation,32,177,hyperparameters,hyperparameters,initialize,embeddings,hyperparameters initialize embeddings,0.7610669732093811
translation,32,178,hyperparameters,episodic training,set,number of shots,episodic training set number of shots,0.6539841294288635
translation,32,178,hyperparameters,number of shots,to be,10,number of shots to be 10,0.6187170147895813
translation,32,178,hyperparameters,hyperparameters,For,episodic training,hyperparameters For episodic training,0.5671020746231079
translation,32,17,model,paraphrasing,be used to create,novel synthetic training items,paraphrasing be used to create novel synthetic training items,0.6356658339500427
translation,32,17,model,model,investigating,low-frequency questions,model investigating low-frequency questions,0.6988456845283508
translation,32,52,model,kaiser et al . 's ( 2017 ) memory module,together with,cnn encoder,kaiser et al . 's ( 2017 ) memory module together with cnn encoder,0.5809727907180786
translation,32,52,model,model,use,kaiser et al . 's ( 2017 ) memory module,model use kaiser et al . 's ( 2017 ) memory module,0.622263491153717
translation,32,52,model,model,as,main model,model as main model,0.5671764612197876
translation,32,52,model,model,as,memoryaugmented cnn classifier ( ma - cnn ),model as memoryaugmented cnn classifier ( ma - cnn ),0.5105863213539124
translation,32,53,model,ma - cnn 's one - shot learning capability,better use of,data augmentation,ma - cnn 's one - shot learning capability better use of data augmentation,0.6181889176368713
translation,32,53,model,data augmentation,to achieve,better performance,data augmentation to achieve better performance,0.6466145515441895
translation,32,53,model,model,take advantage of,ma - cnn 's one - shot learning capability,model take advantage of ma - cnn 's one - shot learning capability,0.6414632201194763
translation,32,172,model,word - based features,in,encoder,word - based features in encoder,0.5213349461555481
translation,32,172,model,model,use,word - based features,model use word - based features,0.6513010859489441
translation,32,18,results,two methods,work,best,two methods work best,0.6732860207557678
translation,32,18,results,two methods,work,in combination,two methods work in combination,0.6521272659301758
translation,32,18,results,better advantage,of,augmented data,better advantage of augmented data,0.6126881241798401
translation,32,18,results,augmented data,created by,paraphrasing,augmented data created by paraphrasing,0.6209425330162048
translation,32,18,results,best,has,in combination,best has in combination,0.5999771356582642
translation,32,18,results,results,find,two methods,results find two methods,0.5447496175765991
translation,32,125,results,multiple word senses,picking,first sense,multiple word senses picking first sense,0.7236650586128235
translation,32,125,results,first sense,produced,higher map score,first sense produced higher map score,0.6612967848777771
translation,32,125,results,higher map score,than,variety of other selection algorithms,higher map score than variety of other selection algorithms,0.5591058731079102
translation,32,125,results,results,in the case of,multiple word senses,results in the case of multiple word senses,0.6212630271911621
translation,32,186,results,ma - cnn,performs,very well,ma - cnn performs very well,0.5766741633415222
translation,32,186,results,very well,on,rare labels,very well on rare labels,0.5444958806037903
translation,32,186,results,results,has,ma - cnn,results has ma - cnn,0.5723671317100525
translation,32,187,results,performance difference,between,stacked cnn model and ma - cnn,performance difference between stacked cnn model and ma - cnn,0.5700235962867737
translation,32,187,results,stacked cnn model and ma - cnn,is,highly significant,stacked cnn model and ma - cnn is highly significant,0.5671559572219849
translation,32,187,results,stacked cnn model and ma - cnn,shows that,pairwise -classification approach,stacked cnn model and ma - cnn shows that pairwise -classification approach,0.6301576495170593
translation,32,187,results,highly significant,shows that,pairwise -classification approach,highly significant shows that pairwise -classification approach,0.7249000072479248
translation,32,187,results,pairwise -classification approach,paired with,episodic training,pairwise -classification approach paired with episodic training,0.7303387522697449
translation,32,187,results,pairwise -classification approach,is,really powerful,pairwise -classification approach is really powerful,0.5750548243522644
translation,32,187,results,episodic training,is,really powerful,episodic training is really powerful,0.578517496585846
translation,32,187,results,really powerful,on,items,really powerful on items,0.5515313744544983
translation,32,187,results,items,which belong to,labels,items which belong to labels,0.7557021975517273
translation,32,187,results,labels,with,few training instances,labels with few training instances,0.607225239276886
translation,32,187,results,results,has,performance difference,results has performance difference,0.5745248198509216
translation,32,188,results,ma - cnn,does not perform,as well,ma - cnn does not perform as well,0.7142590880393982
translation,32,188,results,as well,as,cnn ensemble,as well as cnn ensemble,0.6057742238044739
translation,32,188,results,cnn ensemble,on,all labels,cnn ensemble on all labels,0.5157824754714966
translation,32,188,results,results,see that,ma - cnn,results see that ma - cnn,0.6102305054664612
translation,32,194,results,benefit,in terms of,rare label accuracy,benefit in terms of rare label accuracy,0.6980912685394287
translation,32,194,results,rare label accuracy,by using,augmented dataset,rare label accuracy by using augmented dataset,0.6299754977226257
translation,32,194,results,both models,has,benefit,both models has benefit,0.5876051187515259
translation,32,194,results,results,see that,both models,results see that both models,0.6143448948860168
translation,32,195,results,difference,between,ma - cnn,difference between ma - cnn,0.6447417140007019
translation,32,195,results,ma - cnn,trained with,only the gold dataset and the augmented dataset,ma - cnn trained with only the gold dataset and the augmented dataset,0.7165224552154541
translation,32,195,results,only the gold dataset and the augmented dataset,is,highly significant,only the gold dataset and the augmented dataset is highly significant,0.5509726405143738
translation,32,195,results,even better performance,on,rare labels,even better performance on rare labels,0.5298534631729126
translation,32,195,results,results,has,difference,results has difference,0.5636705756187439
translation,32,196,results,performance,of,both models,performance of both models,0.5794581770896912
translation,32,196,results,both models,does,not significantly change,both models does not significantly change,0.32673248648643494
translation,32,196,results,not significantly change,showing,paraphrases,not significantly change showing paraphrases,0.7475077509880066
translation,32,196,results,paraphrases,are,high enough quality,paraphrases are high enough quality,0.5712409615516663
translation,32,196,results,paraphrases,of,high enough quality,paraphrases of high enough quality,0.5873965620994568
translation,32,196,results,full accuracy,has,performance,full accuracy has performance,0.5797281861305237
translation,32,196,results,results,for,full accuracy,results for full accuracy,0.5919588208198547
translation,32,197,results,effect,of using,pseudo-oracle,effect of using pseudo-oracle,0.6984942555427551
translation,32,197,results,effect,of using,manually filtered data,effect of using manually filtered data,0.7470363974571228
translation,32,197,results,manually filtered data,on,rare labels,manually filtered data on rare labels,0.554827094078064
translation,32,197,results,results,has,effects of data augmentation,results has effects of data augmentation,0.557717502117157
translation,32,198,results,ma - cnn,able to use,data augmentation,ma - cnn able to use data augmentation,0.6347457766532898
translation,32,198,results,data augmentation,directly benefits,rare labels,data augmentation directly benefits rare labels,0.7309991717338562
translation,32,198,results,results,find that,ma - cnn,results find that ma - cnn,0.6213558316230774
translation,32,199,results,ma - cnn,benefits from,human filtered data,ma - cnn benefits from human filtered data,0.6293400526046753
translation,32,199,results,results,has,ma - cnn,results has ma - cnn,0.5723671317100525
translation,32,206,results,simple lexical substitution,already good at providing,information,simple lexical substitution already good at providing information,0.7250518202781677
translation,32,206,results,information,helpful to,ma - cnn,information helpful to ma - cnn,0.6600052714347839
translation,32,206,results,neural machine back translation,is,even better method,neural machine back translation is even better method,0.5589384436607361
translation,32,206,results,even better method,at providing,paraphrases,even better method at providing paraphrases,0.6096914410591125
translation,32,206,results,positive impact,on,rare label accuracy,positive impact on rare label accuracy,0.5673295259475708
translation,32,206,results,results,has,simple lexical substitution,results has simple lexical substitution,0.5573506951332092
translation,32,207,results,paraphrases,generated by,both methods,paraphrases generated by both methods,0.6869454979896545
translation,32,207,results,paraphrases,find that,paraphrases,paraphrases find that paraphrases,0.6511792540550232
translation,32,207,results,paraphrases,from,back translation,paraphrases from back translation,0.5695748329162598
translation,32,207,results,paraphrases,are,more diverse,paraphrases are more diverse,0.6171579360961914
translation,32,207,results,paraphrases,contain,more novel words,paraphrases contain more novel words,0.6018323302268982
translation,32,207,results,more diverse,in,phrasal structure,more diverse in phrasal structure,0.47730162739753723
translation,32,207,results,more novel words,than,lexical substitution,more novel words than lexical substitution,0.6182458996772766
translation,32,207,results,results,inspect,paraphrases,results inspect paraphrases,0.6474801898002625
translation,32,218,results,combiner,get,50.98 % accuracy,combiner get 50.98 % accuracy,0.5751197934150696
translation,32,218,results,combiner,get,79.86 % accuracy,combiner get 79.86 % accuracy,0.5708073973655701
translation,32,218,results,50.98 % accuracy,on,rare labels,50.98 % accuracy on rare labels,0.519324541091919
translation,32,218,results,79.86 % accuracy,on,all labels,79.86 % accuracy on all labels,0.5243445634841919
translation,32,218,results,results,With,combiner,results With combiner,0.6670477390289307
translation,32,220,results,accuracy,on,rare labels,accuracy on rare labels,0.5306203365325928
translation,32,220,results,not as high,as,ma - cnn,not as high as ma - cnn,0.6155799627304077
translation,32,220,results,ma - cnn,higher than,stacked cnn model,ma - cnn higher than stacked cnn model,0.5987172722816467
translation,32,220,results,stacked cnn model,by,5 points,stacked cnn model by 5 points,0.5689067244529724
translation,32,220,results,accuracy increase,on,all labels,accuracy increase on all labels,0.5060574412345886
translation,33,300,ablation-analysis,vocabulary selection,combined with,specificity ( * / v/s ),vocabulary selection combined with specificity ( * / v/s ),0.6854920387268066
translation,33,300,ablation-analysis,vocabulary selection,combined with,specificity,vocabulary selection combined with specificity,0.6984837055206299
translation,33,300,ablation-analysis,specificity ( * / v/s ),leads to,small decrease,specificity ( * / v/s ) leads to small decrease,0.6233395338058472
translation,33,300,ablation-analysis,small decrease,in,efficiency,small decrease in efficiency,0.5710679292678833
translation,33,300,ablation-analysis,efficiency,for,c on heiskell,efficiency for c on heiskell,0.7009133100509644
translation,33,300,ablation-analysis,efficiency,over,specificity,efficiency over specificity,0.7006754279136658
translation,33,300,ablation-analysis,ablation analysis,has,vocabulary selection,ablation analysis has vocabulary selection,0.5451151132583618
translation,33,25,baselines,oddmer,instantiates,focus agent,oddmer instantiates focus agent,0.7719586491584778
translation,33,25,baselines,focus agent,prompts,users,focus agent prompts users,0.7547122240066528
translation,33,25,baselines,users,for,values of intelligible attributes,users for values of intelligible attributes,0.6349618434906006
translation,33,25,baselines,values of intelligible attributes,ordered by,efficiency,values of intelligible attributes ordered by efficiency,0.6456506848335266
translation,33,25,baselines,each candidate focus,has,oddmer,each candidate focus has oddmer,0.6246118545532227
translation,33,22,experiments,oddmer 's vocabulary selection module,uses,supervised learning,oddmer 's vocabulary selection module uses supervised learning,0.5953019261360168
translation,33,22,experiments,supervised learning,to determine,each table 's intelligible attributes,supervised learning to determine each table 's intelligible attributes,0.6689267754554749
translation,33,239,experiments,freely available database,for,virtual game eve online,freely available database for virtual game eve online,0.5619581937789917
translation,33,239,experiments,massively multiplayer online roleplaying game,with,"over 400,000 subscribers","massively multiplayer online roleplaying game with over 400,000 subscribers",0.6566810011863708
translation,33,239,experiments,eve,has,freely available database,eve has freely available database,0.550986647605896
translation,33,239,experiments,eve,has,massively multiplayer online roleplaying game,eve has massively multiplayer online roleplaying game,0.5658071041107178
translation,33,239,experiments,freely available database,has,massively multiplayer online roleplaying game,freely available database has massively multiplayer online roleplaying game,0.5274062156677246
translation,33,239,experiments,virtual game eve online,has,massively multiplayer online roleplaying game,virtual game eve online has massively multiplayer online roleplaying game,0.5681951642036438
translation,33,14,model,open dialogue manager ( odm ),generates,"dialogue states , actions , and strategies","open dialogue manager ( odm ) generates dialogue states , actions , and strategies",0.6129505038261414
translation,33,14,model,"dialogue states , actions , and strategies",from,knowledge,"dialogue states , actions , and strategies from knowledge",0.5347608327865601
translation,33,14,model,model,has,open dialogue manager ( odm ),model has open dialogue manager ( odm ),0.6060941219329834
translation,33,292,results,ordering prompts,by,specificity,ordering prompts by specificity,0.6024256348609924
translation,33,292,results,ordering prompts,yields,sharp increase,ordering prompts yields sharp increase,0.7286173105239868
translation,33,292,results,sharp increase,in,efficiency,sharp increase in efficiency,0.5792036056518555
translation,33,292,results,efficiency,for,both users,efficiency for both users,0.5809038877487183
translation,33,292,results,specificity,has,without vocabulary selection,specificity has without vocabulary selection,0.6113756895065308
translation,33,292,results,results,has,ordering prompts,results has ordering prompts,0.5724979043006897
translation,33,297,results,vocabulary selection,is,more effective,vocabulary selection is more effective,0.5607859492301941
translation,33,297,results,more effective,than,specificity,more effective than specificity,0.5807574391365051
translation,33,297,results,specificity,for,l,specificity for l,0.626535952091217
translation,33,297,results,specificity,for,heiskell,specificity for heiskell,0.6973375678062439
translation,33,297,results,l,on,heiskell,l on heiskell,0.6400265097618103
translation,33,297,results,results,has,vocabulary selection,results has vocabulary selection,0.5163557529449463
translation,33,305,results,domain knowledge model,predicts,close to 100 % knowledge,domain knowledge model predicts close to 100 % knowledge,0.701340913772583
translation,33,305,results,results,has,domain knowledge model,results has domain knowledge model,0.5698732137680054
translation,34,114,baselines,nbt,applies,cnn,nbt applies cnn,0.5913680791854858
translation,34,114,baselines,cnn,over,word embeddings,cnn over word embeddings,0.6428557634353638
translation,34,114,baselines,cnn,learned over,"paraphrase database ( wieting et al. , 2015 )","cnn learned over paraphrase database ( wieting et al. , 2015 )",0.6942936778068542
translation,34,114,baselines,cnn,instead of,delexicalised n-gram features,cnn instead of delexicalised n-gram features,0.6142737865447998
translation,34,114,baselines,word embeddings,learned over,"paraphrase database ( wieting et al. , 2015 )","word embeddings learned over paraphrase database ( wieting et al. , 2015 )",0.6963576078414917
translation,34,114,baselines,baselines,has,nbt,baselines has nbt,0.5819033980369568
translation,34,117,baselines,- self-attn,use,meanpooling,- self-attn use meanpooling,0.7035245299339294
translation,34,117,baselines,meanpooling,instead of,self-attention,meanpooling instead of self-attention,0.6417111754417419
translation,34,155,baselines,nbt,uses,pretrained embeddings,nbt uses pretrained embeddings,0.6022275686264038
translation,34,155,baselines,pretrained embeddings,tailored to retain,semantic relationships,pretrained embeddings tailored to retain semantic relationships,0.6570022702217102
translation,34,155,baselines,semantic relationships,by injecting,semantic similarity constraints,semantic relationships by injecting semantic similarity constraints,0.6158913969993591
translation,34,155,baselines,semantic similarity constraints,from,paraphrase database,semantic similarity constraints from paraphrase database,0.5183244943618774
translation,34,100,hyperparameters,"fixed , pretrained glove embeddings",as well as,character n-gram embeddings,"fixed , pretrained glove embeddings as well as character n-gram embeddings",0.5812993049621582
translation,34,100,hyperparameters,character n-gram embeddings,has,"hashimoto et al. , 2017 )","character n-gram embeddings has hashimoto et al. , 2017 )",0.5268987417221069
translation,34,100,hyperparameters,hyperparameters,use,"fixed , pretrained glove embeddings","hyperparameters use fixed , pretrained glove embeddings",0.5891370177268982
translation,34,102,hyperparameters,regularization,apply,dropout,regularization apply dropout,0.657685399055481
translation,34,102,hyperparameters,dropout,with,0.2 drop rate,dropout with 0.2 drop rate,0.5933901071548462
translation,34,102,hyperparameters,0.2 drop rate,to,output,0.2 drop rate to output,0.561059832572937
translation,34,102,hyperparameters,output,of,each local module and each global module,output of each local module and each global module,0.6174464225769043
translation,34,102,hyperparameters,hyperparameters,For,regularization,hyperparameters For regularization,0.5561942458152771
translation,34,103,hyperparameters,development split,for,hyperparameter tuning,development split for hyperparameter tuning,0.6084262132644653
translation,34,103,hyperparameters,development split,apply,early stopping,development split apply early stopping,0.6708934307098389
translation,34,103,hyperparameters,early stopping,using,joint goal accuracy,early stopping using joint goal accuracy,0.6359369158744812
translation,34,103,hyperparameters,hyperparameters,use,development split,hyperparameters use development split,0.6444909572601318
translation,34,103,hyperparameters,hyperparameters,apply,early stopping,hyperparameters apply early stopping,0.6104239225387573
translation,34,5,model,global-locally self-attentive dialogue state tracker ( glad ),learns,representations,global-locally self-attentive dialogue state tracker ( glad ) learns representations,0.7072105407714844
translation,34,5,model,representations,of,user utterance and previous system actions,representations of user utterance and previous system actions,0.5589002966880798
translation,34,5,model,representations,with,global - local modules,representations with global - local modules,0.6178760528564453
translation,34,5,model,user utterance and previous system actions,with,global - local modules,user utterance and previous system actions with global - local modules,0.6358625292778015
translation,34,5,model,model,propose,global-locally self-attentive dialogue state tracker ( glad ),model propose global-locally self-attentive dialogue state tracker ( glad ),0.6774064898490906
translation,34,6,model,global modules,to share,parameters,global modules to share parameters,0.6170388460159302
translation,34,6,model,parameters,between,estimators,parameters between estimators,0.6762593984603882
translation,34,6,model,estimators,for,different types ( called slots ) of dialogue states,estimators for different types ( called slots ) of dialogue states,0.6170678734779358
translation,34,6,model,local modules,to learn,slot-specific features,local modules to learn slot-specific features,0.5860632658004761
translation,34,6,model,model,uses,global modules,model uses global modules,0.657309353351593
translation,34,6,model,model,uses,local modules,model uses local modules,0.6607721447944641
translation,34,127,model,self-attention,allows,"slot-specific , robust feature learning","self-attention allows slot-specific , robust feature learning",0.6205340027809143
translation,34,127,model,model,has,self-attention,model has self-attention,0.5249809622764587
translation,34,8,results,glad,obtains,88.1 % joint goal accuracy,glad obtains 88.1 % joint goal accuracy,0.5506166219711304
translation,34,8,results,glad,obtains,97.1 % request accuracy,glad obtains 97.1 % request accuracy,0.6004658937454224
translation,34,8,results,glad,obtains,outperforming,glad obtains outperforming,0.6484103798866272
translation,34,8,results,97.1 % request accuracy,on,woz,97.1 % request accuracy on woz,0.5918566584587097
translation,34,8,results,outperforming,by,3.7 % and 5.5 %,outperforming by 3.7 % and 5.5 %,0.6210479736328125
translation,34,8,results,prior work,by,3.7 % and 5.5 %,prior work by 3.7 % and 5.5 %,0.5544973015785217
translation,34,8,results,outperforming,has,prior work,outperforming has prior work,0.5523391366004944
translation,34,8,results,results,has,glad,results has glad,0.6074339151382446
translation,34,9,results,dstc2,obtains,97.5 % request accuracy,dstc2 obtains 97.5 % request accuracy,0.5934230089187622
translation,34,9,results,our model,obtains,74.5 % joint goal accuracy,our model obtains 74.5 % joint goal accuracy,0.5358513593673706
translation,34,9,results,our model,obtains,97.5 % request accuracy,our model obtains 97.5 % request accuracy,0.5590399503707886
translation,34,9,results,outperforming,by,1.1 % and 1.0%,outperforming by 1.1 % and 1.0%,0.6182661056518555
translation,34,9,results,prior work,by,1.1 % and 1.0%,prior work by 1.1 % and 1.0%,0.5601242184638977
translation,34,9,results,dstc2,has,our model,dstc2 has our model,0.6271383762359619
translation,34,9,results,outperforming,has,prior work,outperforming has prior work,0.5523391366004944
translation,34,9,results,results,On,dstc2,results On dstc2,0.5325322151184082
translation,34,109,results,word dropout,to be,helpful,word dropout to be helpful,0.5799525380134583
translation,34,109,results,helpful,for,woz task,helpful for woz task,0.6368408799171448
translation,34,109,results,results,not find,word dropout,results not find word dropout,0.5736968517303467
translation,34,115,results,woz dataset,find that,glad,woz dataset find that glad,0.680488646030426
translation,34,115,results,glad,significantly improves upon,previous state - of- theart performance,glad significantly improves upon previous state - of- theart performance,0.7975418567657471
translation,34,115,results,previous state - of- theart performance,by,3.7 %,previous state - of- theart performance by 3.7 %,0.5611250996589661
translation,34,115,results,previous state - of- theart performance,by,5.5 %,previous state - of- theart performance by 5.5 %,0.5659002065658569
translation,34,115,results,3.7 %,on,joint goal tracking accuracy,3.7 % on joint goal tracking accuracy,0.4723840355873108
translation,34,115,results,5.5 %,on,turn - level request tracking accuracy,5.5 % on turn - level request tracking accuracy,0.49277618527412415
translation,34,115,results,results,On,woz dataset,results On woz dataset,0.5527948141098022
translation,34,116,results,glad,improves upon,previous state of the art performance,glad improves upon previous state of the art performance,0.7402753829956055
translation,34,116,results,previous state of the art performance,by,1.1 %,previous state of the art performance by 1.1 %,0.5437424182891846
translation,34,116,results,previous state of the art performance,by,1.0 %,previous state of the art performance by 1.0 %,0.5464491844177246
translation,34,116,results,1.1 %,on,joint goal tracking accuracy,1.1 % on joint goal tracking accuracy,0.4790421724319458
translation,34,116,results,1.0 %,on,turn - level request tracking accuracy,1.0 % on turn - level request tracking accuracy,0.5001308917999268
translation,34,116,results,dstc dataset,has,glad,dstc dataset has glad,0.5903698205947876
translation,34,116,results,results,On,dstc dataset,results On dstc dataset,0.5498953461647034
translation,34,128,results,consistent drop,in,performance,consistent drop in performance,0.5860300064086914
translation,34,128,results,performance,models that use,mean-pooling,performance models that use mean-pooling,0.6828769445419312
translation,34,128,results,results,observe,consistent drop,results observe consistent drop,0.6368207931518555
translation,34,132,results,global- local sharing,improves,goal tracking,global- local sharing improves goal tracking,0.6878966093063354
translation,34,132,results,results,has,global- local sharing,results has global- local sharing,0.5463324785232544
translation,34,139,results,both variants,for,rare slot-value pairs,both variants for rare slot-value pairs,0.620686948299408
translation,34,139,results,glad,has,consistently outperforms,glad has consistently outperforms,0.6210971474647522
translation,34,139,results,consistently outperforms,has,both variants,consistently outperforms has both variants,0.5926523208618164
translation,34,139,results,results,has,glad,results has glad,0.6074339151382446
translation,35,68,baselines,c-seq2seq conditional sequence-to-sequence ( c- seq2seq ),is,best generative model,c-seq2seq conditional sequence-to-sequence ( c- seq2seq ) is best generative model,0.5601481199264526
translation,35,68,baselines,best generative model,on,stc - sefun dataset,best generative model on stc - sefun dataset,0.496939092874527
translation,35,68,baselines,baselines,has,c-seq2seq conditional sequence-to-sequence ( c- seq2seq ),baselines has c-seq2seq conditional sequence-to-sequence ( c- seq2seq ),0.564081072807312
translation,35,70,baselines,c-seq2seq,follows,widely used encoder-decoder framework,c-seq2seq follows widely used encoder-decoder framework,0.6579830646514893
translation,35,70,baselines,baselines,has,c-seq2seq,baselines has c-seq2seq,0.5464699864387512
translation,35,202,experiments,our meta-learning based models,adapt,faster,our meta-learning based models adapt faster,0.7637662291526794
translation,35,202,experiments,our meta-learning based models,adapt,better,our meta-learning based models adapt better,0.7616757154464722
translation,35,202,experiments,transfer learning based model mtl + ft,has,our meta-learning based models,transfer learning based model mtl + ft has our meta-learning based models,0.581851065158844
translation,35,202,experiments,faster,has,lower fine - tune step,faster has lower fine - tune step,0.5651856064796448
translation,35,202,experiments,better,has,lower perplexity,better has lower perplexity,0.5947751402854919
translation,35,156,hyperparameters,most frequent 30 k words,as,our vocabulary,most frequent 30 k words as our vocabulary,0.5260754227638245
translation,35,156,hyperparameters,"pretrained em- beddings ( song et al. , 2018 )",for,initialization,"pretrained em- beddings ( song et al. , 2018 ) for initialization",0.5625404715538025
translation,35,156,hyperparameters,hyperparameters,take,most frequent 30 k words,hyperparameters take most frequent 30 k words,0.5862489938735962
translation,35,156,hyperparameters,hyperparameters,use,"pretrained em- beddings ( song et al. , 2018 )","hyperparameters use pretrained em- beddings ( song et al. , 2018 )",0.5520563125610352
translation,35,157,hyperparameters,sentence function embedding,dimension,20,sentence function embedding dimension 20,0.611371636390686
translation,35,157,hyperparameters,sentence function embedding,learned through,training,sentence function embedding learned through training,0.6671956777572632
translation,35,157,hyperparameters,20,is,randomly initialized,20 is randomly initialized,0.5990176796913147
translation,35,157,hyperparameters,hyperparameters,has,sentence function embedding,hyperparameters has sentence function embedding,0.5226580500602722
translation,35,158,hyperparameters,two - layer lstms,in,both encoder and decoder,two - layer lstms in both encoder and decoder,0.506891131401062
translation,35,158,hyperparameters,lstms hidden unit size,set to,400,lstms hidden unit size set to 400,0.6887009143829346
translation,35,158,hyperparameters,hyperparameters,use,two - layer lstms,hyperparameters use two - layer lstms,0.5957613587379456
translation,35,159,hyperparameters,"dropout ( srivastava et al. , 2014 )",with,probability p = 0.3,"dropout ( srivastava et al. , 2014 ) with probability p = 0.3",0.6094828844070435
translation,35,160,hyperparameters,trainable parameters,except,word embeddings,trainable parameters except word embeddings,0.5791491270065308
translation,35,160,hyperparameters,randomly initialized,with,uniform distribution,randomly initialized with uniform distribution,0.6236567497253418
translation,35,160,hyperparameters,uniform distribution,in,"( ?0.1 , 0.1 )","uniform distribution in ( ?0.1 , 0.1 )",0.5258050560951233
translation,35,160,hyperparameters,hyperparameters,has,trainable parameters,hyperparameters has trainable parameters,0.475637286901474
translation,35,161,hyperparameters,teacher - forcing,for,training,teacher - forcing for training,0.6302192211151123
translation,35,161,hyperparameters,hyperparameters,adopt,teacher - forcing,hyperparameters adopt teacher - forcing,0.6582732796669006
translation,35,162,hyperparameters,model,with,lowest perplexity,model with lowest perplexity,0.642731249332428
translation,35,165,hyperparameters,sgd,as,optimizer,sgd as optimizer,0.5566533803939819
translation,35,165,hyperparameters,sgd,as,initial learning rate,sgd as initial learning rate,0.5187010169029236
translation,35,165,hyperparameters,sgd,with,initial learning rate,sgd with initial learning rate,0.5956356525421143
translation,35,165,hyperparameters,optimizer,with,minibatch size,optimizer with minibatch size,0.5723190307617188
translation,35,165,hyperparameters,optimizer,with,initial learning rate,optimizer with initial learning rate,0.5991587042808533
translation,35,165,hyperparameters,minibatch size,of,64,minibatch size of 64,0.6199161410331726
translation,35,165,hyperparameters,initial learning rate,of,1.0,initial learning rate of 1.0,0.5808937549591064
translation,35,165,hyperparameters,hyperparameters,use,sgd,hyperparameters use sgd,0.6199065446853638
translation,35,167,hyperparameters,model,for,8 epochs,model for 8 epochs,0.6378179788589478
translation,35,167,hyperparameters,model,start having,learning rate,model start having learning rate,0.6291484236717224
translation,35,167,hyperparameters,learning rate,after,3 epoch,learning rate after 3 epoch,0.7232792377471924
translation,35,167,hyperparameters,hyperparameters,meta-train,model,hyperparameters meta-train model,0.7712512016296387
translation,35,167,hyperparameters,hyperparameters,start having,learning rate,hyperparameters start having learning rate,0.5954974293708801
translation,35,168,hyperparameters,fine-tuned,with,sgd optimizer,fine-tuned with sgd optimizer,0.6284012198448181
translation,35,168,hyperparameters,sgd optimizer,with,minibatch size,sgd optimizer with minibatch size,0.5714394450187683
translation,35,168,hyperparameters,sgd optimizer,with,learning rate,sgd optimizer with learning rate,0.5834531188011169
translation,35,168,hyperparameters,minibatch size,of,64,minibatch size of 64,0.6199161410331726
translation,35,168,hyperparameters,learning rate,of,0.1,learning rate of 0.1,0.6136453747749329
translation,35,168,hyperparameters,hyperparameters,has,models,hyperparameters has models,0.5447477102279663
translation,35,169,hyperparameters,gradient norm upper bound,to,3 and 1,gradient norm upper bound to 3 and 1,0.5353018641471863
translation,35,169,hyperparameters,gradient norm upper bound,during,training and finetuning,gradient norm upper bound during training and finetuning,0.6851459741592407
translation,35,169,hyperparameters,hyperparameters,set,gradient norm upper bound,hyperparameters set gradient norm upper bound,0.6053789258003235
translation,35,10,model,dialogue generation,conditioned on,different sentence functions,dialogue generation conditioned on different sentence functions,0.6921522617340088
translation,35,10,model,modelagnostic meta-learning,to,high- resource sentence functions data,modelagnostic meta-learning to high- resource sentence functions data,0.5256547331809998
translation,35,10,model,model,treat,dialogue generation,model treat dialogue generation,0.5773577094078064
translation,35,10,model,model,apply,modelagnostic meta-learning,model apply modelagnostic meta-learning,0.6437738537788391
translation,35,32,model,our own approach,to utilize,underlying structure,our own approach to utilize underlying structure,0.7456536293029785
translation,35,32,model,underlying structure,of,sentence functions,underlying structure of sentence functions,0.5414244532585144
translation,35,32,model,model,develop,our own approach,model develop our own approach,0.6426700949668884
translation,35,33,model,proposed sml,explicitly tailors,transferable knowledge,proposed sml explicitly tailors transferable knowledge,0.7603455781936646
translation,35,33,model,transferable knowledge,among,different sentence functions,transferable knowledge among different sentence functions,0.5646084547042847
translation,35,33,model,model,has,proposed sml,model has proposed sml,0.6105532646179199
translation,35,34,model,learned representations,of,fine- grained sentence functions,learned representations of fine- grained sentence functions,0.5185691714286804
translation,35,34,model,fine- grained sentence functions,as,parameter gates,fine- grained sentence functions as parameter gates,0.5372293591499329
translation,35,34,model,parameter gates,to influence,globally shared parameter initialization,parameter gates to influence globally shared parameter initialization,0.629868745803833
translation,35,34,model,model,utilizes,learned representations,model utilizes learned representations,0.5681028962135315
translation,35,154,model,meta-trained model,on,each target task,meta-trained model on each target task,0.5201163291931152
translation,35,154,model,model,fine- tune,meta-trained model,model fine- tune meta-trained model,0.7028385400772095
translation,35,11,results,sml,enhances,meta-learning effectiveness,sml enhances meta-learning effectiveness,0.621406614780426
translation,35,11,results,meta-learning effectiveness,by promoting,knowledge customization,meta-learning effectiveness by promoting knowledge customization,0.700294017791748
translation,35,11,results,knowledge customization,among,different sentence functions,knowledge customization among different sentence functions,0.5513085126876831
translation,35,11,results,knowledge customization,simultaneously preserving,knowledge generalization,knowledge customization simultaneously preserving knowledge generalization,0.7333530187606812
translation,35,11,results,knowledge generalization,for,similar sentence functions,knowledge generalization for similar sentence functions,0.5979133248329163
translation,35,11,results,results,has,sml,results has sml,0.5592203140258789
translation,35,191,results,mtl +ft,achieves,better performance,mtl +ft achieves better performance,0.7004954814910889
translation,35,191,results,better performance,than,mtl,better performance than mtl,0.5918394923210144
translation,35,191,results,results,has,mtl +ft,results has mtl +ft,0.5601397752761841
translation,35,194,results,best / second - best human evaluation results,across,most of the metrics,best / second - best human evaluation results across most of the metrics,0.6310607194900513
translation,35,196,results,huge improvement,on,accuracy,huge improvement on accuracy,0.5188324451446533
translation,35,196,results,accuracy,of,given sentence functions ( accu ),accuracy of given sentence functions ( accu ),0.572046160697937
translation,35,197,results,maml,in,most of the cases,maml in most of the cases,0.5843170285224915
translation,35,197,results,sml,has,outperforms,sml has outperforms,0.6503958106040955
translation,35,197,results,outperforms,has,maml,outperforms has maml,0.619353711605072
translation,35,197,results,results,has,sml,results has sml,0.5592203140258789
translation,35,203,results,not reliable enough,to evaluate,response generation,not reliable enough to evaluate response generation,0.693834125995636
translation,35,203,results,maml and sml,achieve,slightly better results,maml and sml achieve slightly better results,0.6283193230628967
translation,35,203,results,slightly better results,than,baselines,slightly better results than baselines,0.6023306846618652
translation,35,203,results,response generation,has,maml and sml,response generation has maml and sml,0.6214413046836853
translation,35,203,results,results,has,bleu,results has bleu,0.5385738611221313
translation,35,205,results,"mtl +ft , maml and sml",achieve,comparable performance,"mtl +ft , maml and sml achieve comparable performance",0.6360657811164856
translation,35,205,results,comparable performance,with regard to,unigram / bigram diversity ( dist1 / 2 ),comparable performance with regard to unigram / bigram diversity ( dist1 / 2 ),0.630034327507019
translation,35,205,results,unigram / bigram diversity ( dist1 / 2 ),of,generated responses,unigram / bigram diversity ( dist1 / 2 ) of generated responses,0.5724014639854431
translation,35,205,results,results,has,"mtl +ft , maml and sml","results has mtl +ft , maml and sml",0.5305100083351135
translation,35,213,results,fine-tuning,on,responses,fine-tuning on responses,0.5208091139793396
translation,35,213,results,responses,of,target sentence function,responses of target sentence function,0.5895350575447083
translation,35,213,results,mtl + ft,can capture,correct response pattern,mtl + ft can capture correct response pattern,0.6428656578063965
translation,35,213,results,correct response pattern,in,some cases,correct response pattern in some cases,0.5173410773277283
translation,35,213,results,fine-tuning,has,mtl + ft,fine-tuning has mtl + ft,0.5924859046936035
translation,35,213,results,responses,has,mtl + ft,responses has mtl + ft,0.6194325685501099
translation,35,213,results,target sentence function,has,mtl + ft,target sentence function has mtl + ft,0.585965633392334
translation,35,213,results,results,With,fine-tuning,results With fine-tuning,0.6183133125305176
translation,36,168,ablation-analysis,performance,of,kaware,performance of kaware,0.6229102611541748
translation,36,168,ablation-analysis,performance,of,taware,performance of taware,0.6090666651725769
translation,36,168,ablation-analysis,performance,compared to,one- hop,performance compared to one- hop,0.6294155716896057
translation,36,168,ablation-analysis,performance,enhance,taware,performance enhance taware,0.6651666760444641
translation,36,168,ablation-analysis,performance,of,taware,performance of taware,0.6090666651725769
translation,36,168,ablation-analysis,multi-hop memnet,has,deteriorate,multi-hop memnet has deteriorate,0.569131076335907
translation,36,168,ablation-analysis,deteriorate,has,performance,deteriorate has performance,0.6049640774726868
translation,36,168,ablation-analysis,ablation analysis,empirically find that,multi-hop memnet,ablation analysis empirically find that multi-hop memnet,0.6381863951683044
translation,36,172,ablation-analysis,significantly change,when,knowledge graphs,significantly change when knowledge graphs,0.6552743911743164
translation,36,172,ablation-analysis,knowledge graphs,are,largely updated ( all ),knowledge graphs are largely updated ( all ),0.5685246586799622
translation,36,172,ablation-analysis,knowledge graphs,achieve,good accurate change rate,knowledge graphs achieve good accurate change rate,0.6371345520019531
translation,36,172,ablation-analysis,"memnet , taware and kaware",has,significantly change,"memnet , taware and kaware has significantly change",0.6098558902740479
translation,36,174,ablation-analysis,knowledge graphs,are,slightly updated ( last1 and last2 ),knowledge graphs are slightly updated ( last1 and last2 ),0.5718854665756226
translation,36,174,ablation-analysis,accurate changes,is,significantly low,accurate changes is significantly low,0.5748589634895325
translation,36,174,ablation-analysis,accurate changes,has,over total changes,accurate changes has over total changes,0.6224825978279114
translation,36,174,ablation-analysis,ablation analysis,when,knowledge graphs,ablation analysis when knowledge graphs,0.6013292670249939
translation,36,162,baselines,prior knowledgegrounded conversation models,has,memory network,prior knowledgegrounded conversation models has memory network,0.5487300753593445
translation,36,155,experimental-setup,both encoder and decoder,for,hgzhz,both encoder and decoder for hgzhz,0.619868814945221
translation,36,155,experimental-setup,hgzhz,are,256 dimension,hgzhz are 256 dimension,0.6402255892753601
translation,36,155,experimental-setup,256 dimension,with,1 layer,256 dimension with 1 layer,0.6711741089820862
translation,36,155,experimental-setup,256 dimension,with,1 layer,256 dimension with 1 layer,0.6711741089820862
translation,36,155,experimental-setup,256 dimension,with,1 layer,256 dimension with 1 layer,0.6711741089820862
translation,36,155,experimental-setup,friends,are,128 dimension,friends are 128 dimension,0.6392498016357422
translation,36,155,experimental-setup,128 dimension,with,1 layer,128 dimension with 1 layer,0.6712151765823364
translation,36,155,experimental-setup,experimental setup,has,both encoder and decoder,experimental setup has both encoder and decoder,0.5538230538368225
translation,36,160,experimental-setup,experimental setup,has,early stopping,experimental setup has early stopping,0.5249444842338562
translation,36,166,experimental-setup,multi-hop version,of,"memnet ( weston et al. , 2015 b )","multi-hop version of memnet ( weston et al. , 2015 b )",0.527228593826294
translation,36,166,experimental-setup,multi-hop version,implemented,multi ),multi-hop version implemented multi ),0.6704263687133789
translation,36,166,experimental-setup,experimental setup,has,multi-hop version,experimental setup has multi-hop version,0.5502728819847107
translation,36,208,experiments,taware and qadpt,predict,more correct knowledge entities,taware and qadpt predict more correct knowledge entities,0.6556926965713501
translation,36,208,experiments,taware and qadpt,tend to diminish,generic words,taware and qadpt tend to diminish generic words,0.7411507368087769
translation,36,35,model,multi-hop reasoning,on,graph structure,multi-hop reasoning on graph structure,0.5517313480377197
translation,36,35,model,graph structure,into,neural conversation generation model,graph structure into neural conversation generation model,0.5422026515007019
translation,36,35,model,model,incorporates,multi-hop reasoning,model incorporates multi-hop reasoning,0.7231460809707642
translation,36,36,model,quick adaptive dynamic knowledge - grounded neural conversation model ( qadpt ),based on,seq2seq model,quick adaptive dynamic knowledge - grounded neural conversation model ( qadpt ) based on seq2seq model,0.6480871438980103
translation,36,36,model,seq2seq model,with,widely - used copy mechanism,seq2seq model with widely - used copy mechanism,0.596845805644989
translation,36,36,model,model,called,quick adaptive dynamic knowledge - grounded neural conversation model ( qadpt ),model called quick adaptive dynamic knowledge - grounded neural conversation model ( qadpt ),0.6630051732063293
translation,36,37,model,transition matrix,for,markov chain,transition matrix for markov chain,0.6171593070030212
translation,36,37,model,model,To enable,multi-hop reasoning,model To enable multi-hop reasoning,0.7128471732139587
translation,36,163,model,topic-aware model ( taware ),by attending on,knowledge graphs,topic-aware model ( taware ) by attending on knowledge graphs,0.6425614953041077
translation,36,163,model,topic-aware model ( taware ),using,two separate output projection layers,topic-aware model ( taware ) using two separate output projection layers,0.6492161750793457
translation,36,163,model,two separate output projection layers,has,generic words and all knowledge graph entities,two separate output projection layers has generic words and all knowledge graph entities,0.5780532360076904
translation,36,163,model,model,leverage,topic-aware model ( taware ),model leverage topic-aware model ( taware ),0.7769867181777954
translation,36,175,results,better performance,on,last1,better performance on last1,0.565910279750824
translation,36,175,results,baselines,has,kaware,baselines has kaware,0.6270788311958313
translation,36,175,results,kaware,has,better performance,kaware has better performance,0.598472535610199
translation,36,175,results,results,Among,baselines,results Among baselines,0.6001414060592651
translation,36,176,results,all baselines,when,knowledge graphs,all baselines when knowledge graphs,0.5854182839393616
translation,36,176,results,slightly change ( last1 and last2 ),in terms of,accurate change rate,slightly change ( last1 and last2 ) in terms of accurate change rate,0.683413028717041
translation,36,176,results,qadpt,has,outperforms,qadpt has outperforms,0.6658132672309875
translation,36,176,results,outperforms,has,all baselines,outperforms has all baselines,0.5888515114784241
translation,36,176,results,knowledge graphs,has,slightly change ( last1 and last2 ),knowledge graphs has slightly change ( last1 and last2 ),0.6149911284446716
translation,36,176,results,results,has,qadpt,results has qadpt,0.5698676705360413
translation,36,177,results,proportion of accurate changes,over,total changes,proportion of accurate changes over total changes,0.6258013844490051
translation,36,177,results,proportion of accurate changes,show,significantly better performance,proportion of accurate changes show significantly better performance,0.6399390697479248
translation,36,177,results,significantly better performance,than,prior models,significantly better performance than prior models,0.5819336175918579
translation,36,177,results,results,has,proportion of accurate changes,results has proportion of accurate changes,0.5674745440483093
translation,36,181,results,memnet,for,kw - acc and kw / generic,memnet for kw - acc and kw / generic,0.634871780872345
translation,36,181,results,all other models,by,kw / generic precision ( 100 % ),all other models by kw / generic precision ( 100 % ),0.5358832478523254
translation,36,181,results,hgzhz and friends,has,taware +multi and qadpt,hgzhz and friends has taware +multi and qadpt,0.6713057160377502
translation,36,181,results,taware +multi and qadpt,has,significantly outperform,taware +multi and qadpt has significantly outperform,0.6060073971748352
translation,36,181,results,significantly outperform,has,memnet,significantly outperform has memnet,0.5955583453178406
translation,36,181,results,memnet,has,outperforms,memnet has outperforms,0.6498783826828003
translation,36,181,results,outperforms,has,all other models,outperforms has all other models,0.5782700181007385
translation,36,181,results,results,On,hgzhz and friends,results On hgzhz and friends,0.5106447339057922
translation,36,184,results,all models,similar levels of,bleu - 2 and ppl,all models similar levels of bleu - 2 and ppl,0.5664343237876892
translation,36,184,results,qadpt+ multi,has,slightly better distinct scores,qadpt+ multi has slightly better distinct scores,0.5938859581947327
translation,36,184,results,results,show,all models,results show all models,0.5996013283729553
translation,36,184,results,results,show,qadpt+ multi,results show qadpt+ multi,0.6631434559822083
translation,36,201,results,qadpt,wins,baselines,qadpt wins baselines,0.7557156682014465
translation,36,201,results,baselines,by,fluency,baselines by fluency,0.6335839033126831
translation,36,201,results,more often,by,fluency,more often by fluency,0.6681476831436157
translation,36,201,results,infomation fields,than,fluency fields,infomation fields than fluency fields,0.565218448638916
translation,36,201,results,baselines,has,more often,baselines has more often,0.5909813642501831
translation,36,201,results,results,has,qadpt,results has qadpt,0.5698676705360413
translation,36,206,results,excel,at,different aspects,excel at different aspects,0.5956296324729919
translation,36,206,results,results,demonstrate,"memnet , taware and qadpt","results demonstrate memnet , taware and qadpt",0.5712445974349976
translation,36,207,results,memnet,successfully incorporate,knowledge graphs,memnet successfully incorporate knowledge graphs,0.6753220558166504
translation,36,207,results,memnet,generate,sentences,memnet generate sentences,0.6960667967796326
translation,36,207,results,sentences,with,appropriate knowledge entities,sentences with appropriate knowledge entities,0.5907166004180908
translation,36,207,results,sentences,with,generic words,sentences with generic words,0.5987379550933838
translation,36,207,results,results,has,memnet,results has memnet,0.55446857213974
translation,36,209,results,mem-net and taware,show,ability,mem-net and taware show ability,0.6931739449501038
translation,36,209,results,ability,to update,responses,ability to update responses,0.6879379153251648
translation,36,209,results,responses,when,knowledge graphs,responses when knowledge graphs,0.6560878157615662
translation,36,209,results,knowledge graphs,are,largely changed,knowledge graphs are largely changed,0.6084630489349365
translation,36,209,results,zero-shot adaptation,has,mem-net and taware,zero-shot adaptation has mem-net and taware,0.6110095381736755
translation,39,161,ablation-analysis,decreases,has,performance,decreases has performance,0.5981842875480652
translation,39,161,ablation-analysis,ablation analysis,without weighting,instance filter,ablation analysis without weighting instance filter,0.787641704082489
translation,39,94,baselines,"opensubtitles ( lison and tiedemann , 2016 )",group of,human-human conversations,"opensubtitles ( lison and tiedemann , 2016 ) group of human-human conversations",0.6952511072158813
translation,39,94,baselines,human-human conversations,converted from,movie transcripts,human-human conversations converted from movie transcripts,0.6512508392333984
translation,39,94,baselines,baselines,has,"opensubtitles ( lison and tiedemann , 2016 )","baselines has opensubtitles ( lison and tiedemann , 2016 )",0.5373904705047607
translation,39,97,baselines,rnn - based sequence - to-sequence model,with,attention mechanisms,rnn - based sequence - to-sequence model with attention mechanisms,0.60976243019104
translation,39,97,baselines,rnn - based sequence - to-sequence model,with,attention mechanisms,rnn - based sequence - to-sequence model with attention mechanisms,0.60976243019104
translation,39,97,baselines,rnn - based sequence - to-sequence model,trained with,attention mechanisms,rnn - based sequence - to-sequence model trained with attention mechanisms,0.7126955986022949
translation,39,97,baselines,latent variable model,using,conditional variational auto-encoder,latent variable model using conditional variational auto-encoder,0.5968271493911743
translation,39,97,baselines,encoder-decoder architecture,relying solely on,attention mechanisms,encoder-decoder architecture relying solely on attention mechanisms,0.6777940392494202
translation,39,97,baselines,seq2seq,has,rnn - based sequence - to-sequence model,seq2seq has rnn - based sequence - to-sequence model,0.551906943321228
translation,39,97,baselines,cvae,has,latent variable model,cvae has latent variable model,0.548965334892273
translation,39,97,baselines,transformer,has,encoder-decoder architecture,transformer has encoder-decoder architecture,0.5801298022270203
translation,39,98,baselines,cvae and gan,for augmenting,training data,cvae and gan for augmenting training data,0.7166596055030823
translation,39,98,baselines,training data,to generate,more diversified expressions,training data to generate more diversified expressions,0.6783207654953003
translation,39,100,baselines,clustering,clusters,highentropy samples,clustering clusters highentropy samples,0.7462949752807617
translation,39,100,baselines,highentropy samples,as,noises,highentropy samples as noises,0.5609633922576904
translation,39,100,baselines,baselines,has,clustering,baselines has clustering,0.6206364631652832
translation,39,102,baselines,several widely used metrics,to measure,performance,several widely used metrics to measure performance,0.682457447052002
translation,39,102,baselines,performance,of,dialogue generation models,performance of dialogue generation models,0.562310516834259
translation,39,102,baselines,performance,including,bleu,performance including bleu,0.6024749279022217
translation,39,102,baselines,performance,including,embedding - based metrics,performance including embedding - based metrics,0.6952239871025085
translation,39,102,baselines,performance,including,entropy - based metrics,performance including entropy - based metrics,0.710121750831604
translation,39,102,baselines,performance,including,distinct metrics,performance including distinct metrics,0.6822561621665955
translation,39,102,baselines,dialogue generation models,including,bleu,dialogue generation models including bleu,0.5915418267250061
translation,39,102,baselines,dialogue generation models,including,embedding - based metrics,dialogue generation models including embedding - based metrics,0.6632219552993774
translation,39,102,baselines,dialogue generation models,including,entropy - based metrics,dialogue generation models including entropy - based metrics,0.6651026010513306
translation,39,106,baselines,greedily matching words,in,two utterances,greedily matching words in two utterances,0.5583781003952026
translation,39,106,baselines,greedily matching words,based on,cosine similarities,greedily matching words based on cosine similarities,0.6046727895736694
translation,39,106,baselines,cosine similarities,between,embeddings,cosine similarities between embeddings,0.6643497347831726
translation,39,106,baselines,cosine similarity,between,largest extreme values,cosine similarity between largest extreme values,0.6490753889083862
translation,39,106,baselines,largest extreme values,among,word embeddings,largest extreme values among word embeddings,0.5824562311172485
translation,39,106,baselines,word embeddings,in,two utterances,word embeddings in two utterances,0.5201213359832764
translation,39,106,baselines,embedding extrema ( ext ),has,cosine similarity,embedding extrema ( ext ) has cosine similarity,0.5588990449905396
translation,39,106,baselines,baselines,has,embedding greedy ( gre ),baselines has embedding greedy ( gre ),0.5666797161102295
translation,39,116,experiments,back - translation,in,sentence - level dialogue augmentation,back - translation in sentence - level dialogue augmentation,0.501049280166626
translation,39,116,experiments,sentence - level dialogue augmentation,use,transformer model,sentence - level dialogue augmentation use transformer model,0.5531731843948364
translation,39,116,experiments,transformer model,trained on,"en-de and en-ru wmt '19 news translation tasks ( ng et al. , 2019 )","transformer model trained on en-de and en-ru wmt '19 news translation tasks ( ng et al. , 2019 )",0.7070493698120117
translation,39,116,experiments,tokenized,with,moses tokenizer,tokenized with moses tokenizer,0.656125545501709
translation,39,107,hyperparameters,glove vectors,as,word embeddings,glove vectors as word embeddings,0.5048059821128845
translation,39,107,hyperparameters,hyperparameters,use,glove vectors,hyperparameters use glove vectors,0.6254991292953491
translation,39,115,hyperparameters,replacement probability,set to,15 %,replacement probability set to 15 %,0.7493947148323059
translation,39,115,hyperparameters,hyperparameters,has,replacement probability,hyperparameters has replacement probability,0.5345882773399353
translation,39,117,hyperparameters,translation tasks,i.e.,word representations,translation tasks i.e. word representations,0.5709507465362549
translation,39,117,hyperparameters,word representations,of size,1024,word representations of size 1024,0.7228980660438538
translation,39,117,hyperparameters,dropout,with,0.8 keep probability,dropout with 0.8 keep probability,0.6537240743637085
translation,39,117,hyperparameters,feedforward layers,with dimension,4096,feedforward layers with dimension 4096,0.7253720164299011
translation,39,117,hyperparameters,6 blocks,in,encoder and decoder,6 blocks in encoder and decoder,0.5374146699905396
translation,39,117,hyperparameters,encoder and decoder,with,16 attention heads,encoder and decoder with 16 attention heads,0.6195651888847351
translation,39,118,hyperparameters,"adam ( kingma and ba , 2015 ) optimizer",using,initial learning rate 7e - 4,"adam ( kingma and ba , 2015 ) optimizer using initial learning rate 7e - 4",0.6385802626609802
translation,39,118,hyperparameters,hyperparameters,optimized with,"adam ( kingma and ba , 2015 ) optimizer","hyperparameters optimized with adam ( kingma and ba , 2015 ) optimizer",0.7119957208633423
translation,39,119,hyperparameters,dialogue models implementation,adopt,2 - layer bidirectional lstm,dialogue models implementation adopt 2 - layer bidirectional lstm,0.5899006724357605
translation,39,119,hyperparameters,dialogue models implementation,adopt,unidirectional one,dialogue models implementation adopt unidirectional one,0.6897093653678894
translation,39,119,hyperparameters,2 - layer bidirectional lstm,as,encoder,2 - layer bidirectional lstm as encoder,0.5305489301681519
translation,39,119,hyperparameters,unidirectional one,as,decoder,unidirectional one as decoder,0.6102378368377686
translation,39,119,hyperparameters,decoder,for,seq2seq and cvae,decoder for seq2seq and cvae,0.6616566777229309
translation,39,119,hyperparameters,decoder,both,seq2seq and cvae,decoder both seq2seq and cvae,0.7046611905097961
translation,39,119,hyperparameters,hyperparameters,Regarding,dialogue models implementation,hyperparameters Regarding dialogue models implementation,0.5752974152565002
translation,39,120,hyperparameters,hidden size,set to,256,hidden size set to 256,0.7408077716827393
translation,39,120,hyperparameters,latent size,used in,cvae,latent size used in cvae,0.688204288482666
translation,39,120,hyperparameters,latent size,set to,64,latent size set to 64,0.7008756399154663
translation,39,120,hyperparameters,hyperparameters,has,hidden size,hyperparameters has hidden size,0.535134494304657
translation,39,120,hyperparameters,hyperparameters,has,latent size,hyperparameters has latent size,0.5146878361701965
translation,39,121,hyperparameters,transformer model,for,dialogue generation,transformer model for dialogue generation,0.6124239563941956
translation,39,121,hyperparameters,dialogue generation,configured with,512 hidden size,dialogue generation configured with 512 hidden size,0.6938265562057495
translation,39,121,hyperparameters,6 blocks,in both,encoder and decoder,6 blocks in both encoder and decoder,0.6242591142654419
translation,39,121,hyperparameters,hyperparameters,has,transformer model,hyperparameters has transformer model,0.5474389791488647
translation,39,9,model,data manipulation framework,to proactively reshape,data distribution,data manipulation framework to proactively reshape data distribution,0.6938072443008423
translation,39,9,model,data distribution,towards,reliable samples,data distribution towards reliable samples,0.650507390499115
translation,39,9,model,data distribution,by augmenting and highlighting,effective learning samples,data distribution by augmenting and highlighting effective learning samples,0.7192978858947754
translation,39,9,model,reducing,has,effect of inefficient samples simultaneously,reducing has effect of inefficient samples simultaneously,0.5628296136856079
translation,39,9,model,model,propose,data manipulation framework,model propose data manipulation framework,0.678605854511261
translation,39,10,model,data manipulation model,selectively augments,training samples,data manipulation model selectively augments training samples,0.7258942723274231
translation,39,10,model,data manipulation model,assigns,importance weight,data manipulation model assigns importance weight,0.6043872237205505
translation,39,10,model,importance weight,to,each instance,importance weight to each instance,0.5199049115180969
translation,39,10,model,each instance,to reform,training data,each instance to reform training data,0.6307928562164307
translation,39,10,model,model,has,data manipulation model,model has data manipulation model,0.526485025882721
translation,39,28,model,novel learnable data manipulation framework,to proactively reshape,data distribution,novel learnable data manipulation framework to proactively reshape data distribution,0.7314210534095764
translation,39,28,model,data distribution,towards,reliable samples,data distribution towards reliable samples,0.650507390499115
translation,39,28,model,data distribution,by augmenting and highlighting,effective learning samples,data distribution by augmenting and highlighting effective learning samples,0.7192978858947754
translation,39,28,model,weights,of,inefficient samples simultaneously,weights of inefficient samples simultaneously,0.598360538482666
translation,39,28,model,model,propose,novel learnable data manipulation framework,model propose novel learnable data manipulation framework,0.6732766628265381
translation,39,29,model,data manipulation model,selectively augments,training samples,data manipulation model selectively augments training samples,0.7258942723274231
translation,39,29,model,training samples,in terms of both,word level and sentence level,training samples in terms of both word level and sentence level,0.7060889005661011
translation,39,29,model,training samples,using,masked language models,training samples using masked language models,0.6659319996833801
translation,39,29,model,masked language models,such as,"bert ( devlin et al. , 2019 ) and back - translation ( sennrich et al. , 2016 ) technique","masked language models such as bert ( devlin et al. , 2019 ) and back - translation ( sennrich et al. , 2016 ) technique",0.5800113677978516
translation,39,29,model,more effective data samples,has,data manipulation model,more effective data samples has data manipulation model,0.5474064350128174
translation,39,29,model,model,to generate,more effective data samples,model to generate more effective data samples,0.7203389406204224
translation,39,30,model,weights,of,inefficient samples,weights of inefficient samples,0.5774124264717102
translation,39,30,model,inefficient samples,from,original training samples and the augmented samples,inefficient samples from original training samples and the augmented samples,0.5438805222511292
translation,39,30,model,data manipulation model,assigns,importance weight,data manipulation model assigns importance weight,0.6043872237205505
translation,39,30,model,importance weight,to,each sample,importance weight to each sample,0.5675100684165955
translation,39,30,model,importance weight,to adapt,sample effect,importance weight to adapt sample effect,0.726270854473114
translation,39,30,model,each sample,to adapt,sample effect,each sample to adapt sample effect,0.7685607671737671
translation,39,30,model,sample effect,on,dialogue model training,sample effect on dialogue model training,0.5585962533950806
translation,39,30,model,weights,has,data manipulation model,weights has data manipulation model,0.5838642120361328
translation,39,30,model,model,To reduce,weights,model To reduce weights,0.7140101790428162
translation,39,99,model,calibration network,measures,quality of data samples,calibration network measures quality of data samples,0.6106637120246887
translation,39,99,model,calibration network,enables,weighted training,calibration network enables weighted training,0.6556498408317566
translation,39,99,model,weighted training,for,dialogue generation,weighted training for dialogue generation,0.5924121141433716
translation,39,99,model,calibration,has,calibration network,calibration has calibration network,0.5562902092933655
translation,39,99,model,model,has,calibration,model has calibration,0.52693772315979
translation,39,126,results,automatic evaluation results,of,proposed learning framework,automatic evaluation results of proposed learning framework,0.547614336013794
translation,39,126,results,proposed data manipulation framework,brings,solid improvements,proposed data manipulation framework brings solid improvements,0.6395598649978638
translation,39,126,results,solid improvements,for,all the three architectures,solid improvements for all the three architectures,0.6077733039855957
translation,39,126,results,vanilla training procedure,has,proposed data manipulation framework,vanilla training procedure has proposed data manipulation framework,0.5721996426582336
translation,39,126,results,results,Compared with,vanilla training procedure,results Compared with vanilla training procedure,0.6084948182106018
translation,39,126,results,results,has,automatic evaluation results,results has automatic evaluation results,0.5548956990242004
translation,39,129,results,outperforms,on,most of metrics,outperforms on most of metrics,0.5038354992866516
translation,39,129,results,baseline methods,on,most of metrics,baseline methods on most of metrics,0.44106295704841614
translation,39,129,results,data manipulation framework,has,outperforms,data manipulation framework has outperforms,0.597369372844696
translation,39,129,results,outperforms,has,baseline methods,outperforms has baseline methods,0.5753374099731445
translation,39,130,results,improvement,on,distinct metrics,improvement on distinct metrics,0.5250023007392883
translation,39,130,results,distinct metrics,of,our model,distinct metrics of our model,0.563998818397522
translation,39,130,results,distinct metrics,is,much greater,distinct metrics is much greater,0.5948840379714966
translation,39,130,results,our model,is,much greater,our model is much greater,0.6273221373558044
translation,39,130,results,neural dialogue model,generating,more diverse responses,neural dialogue model generating more diverse responses,0.6924127340316772
translation,39,130,results,results,has,improvement,results has improvement,0.6248279809951782
translation,39,156,results,performance,over,vanilla seq2seq baseline,performance over vanilla seq2seq baseline,0.6395294666290283
translation,39,156,results,sentence - level augmentation,performs,slightly better,sentence - level augmentation performs slightly better,0.5649809241294861
translation,39,156,results,slightly better,than,word-level augmentation,slightly better than word-level augmentation,0.5638848543167114
translation,39,156,results,both augmentation mechanisms,has,improve,both augmentation mechanisms has improve,0.5940583348274231
translation,39,156,results,improve,has,performance,improve has performance,0.5578044652938843
translation,39,156,results,results,has,both augmentation mechanisms,results has both augmentation mechanisms,0.5063244700431824
translation,39,160,results,performance,degrades,rapidly,performance degrades rapidly,0.8573783040046692
translation,39,160,results,different variants,has,without data augmentation,different variants has without data augmentation,0.5894006490707397
translation,39,160,results,different variants,has,performance,different variants has performance,0.5646084547042847
translation,39,160,results,without data augmentation,has,performance,without data augmentation has performance,0.58493572473526
translation,39,160,results,results,Among,different variants,results Among different variants,0.6159852743148804
translation,40,5,model,analyzers,to learn,dialogue -specific properties,analyzers to learn dialogue -specific properties,0.5946249961853027
translation,40,5,model,dialogue -specific properties,along with,domain knowledge,dialogue -specific properties along with domain knowledge,0.5712588429450989
translation,40,5,model,domain knowledge,of,conversations,domain knowledge of conversations,0.592308521270752
translation,40,5,model,conversations,from,wikipedia,conversations from wikipedia,0.6036836504936218
translation,40,78,results,baseline,by,12.60 and 13.50,baseline by 12.60 and 13.50,0.5671166181564331
translation,40,78,results,12.60 and 13.50,in,fmeasure,12.60 and 13.50 in fmeasure,0.6002441644668579
translation,40,78,results,both approaches,has,intersection and union,both approaches has intersection and union,0.6333542466163635
translation,40,78,results,both approaches,has,outperformed,both approaches has outperformed,0.5881195068359375
translation,40,78,results,intersection and union,has,outperformed,intersection and union has outperformed,0.6303141117095947
translation,40,78,results,outperformed,has,baseline,outperformed has baseline,0.614183247089386
translation,40,78,results,results,has,both approaches,results has both approaches,0.5230107307434082
translation,40,79,results,intersection strategy,contributed to produce,more precise outputs,intersection strategy contributed to produce more precise outputs,0.5971338748931885
translation,40,79,results,more precise outputs,than,others,more precise outputs than others,0.6348381042480469
translation,40,79,results,other proposed approach,with,union,other proposed approach with union,0.6580949425697327
translation,40,79,results,union,achieved,more gain,union achieved more gain,0.7439476847648621
translation,40,79,results,more gain,in,recall,more gain in recall,0.5398178100585938
translation,40,79,results,more gain,with,slightly better f-measure,more gain with slightly better f-measure,0.6650497913360596
translation,40,79,results,slightly better f-measure,than,former one,slightly better f-measure than former one,0.5648096799850464
translation,41,158,ablation-analysis,greater influence,than,generation data,greater influence than generation data,0.6128085851669312
translation,41,158,ablation-analysis,retrieval data,has,greater influence,retrieval data has greater influence,0.5726698040962219
translation,41,158,ablation-analysis,ablation analysis,help from,retrieval data,ablation analysis help from retrieval data,0.7360575795173645
translation,41,126,baselines,matching scores,between,long utterance and response candidates,matching scores between long utterance and response candidates,0.6359939575195312
translation,41,126,baselines,long utterance and response candidates,including,"rnn ( lowe et al. , 2015 )","long utterance and response candidates including rnn ( lowe et al. , 2015 )",0.6362318992614746
translation,41,126,baselines,long utterance and response candidates,including,"cnn ( lowe et al. , 2015 )","long utterance and response candidates including cnn ( lowe et al. , 2015 )",0.6501600742340088
translation,41,126,baselines,long utterance and response candidates,including,"lstm ( lowe et al. , 2015 )","long utterance and response candidates including lstm ( lowe et al. , 2015 )",0.643112301826477
translation,41,126,baselines,long utterance and response candidates,including,"bi-lstm ( kadlec et al. , 2015 )","long utterance and response candidates including bi-lstm ( kadlec et al. , 2015 )",0.6414470076560974
translation,41,155,baselines,smn,well as,dam,smn well as dam,0.7076439261436462
translation,41,8,model,grayscale data,be,automatically constructed,grayscale data be automatically constructed,0.5377458930015564
translation,41,8,model,automatically constructed,without,human effort,automatically constructed without human effort,0.7357744574546814
translation,41,8,model,model,show that,grayscale data,model show that grayscale data,0.5681229829788208
translation,41,9,model,response generation models,as,automatic grayscale data generators,response generation models as automatic grayscale data generators,0.5191758871078491
translation,41,28,model,grayscale data,from,standard dialogue datasets,grayscale data from standard dialogue datasets,0.5683631896972656
translation,41,28,model,standard dialogue datasets,where,golden dialogue context and response pairs,standard dialogue datasets where golden dialogue context and response pairs,0.5521798133850098
translation,41,28,model,model,automatically construct,grayscale data,model automatically construct grayscale data,0.7156968712806702
translation,41,29,model,model,resort to,off - the-shelf retrieval algorithms and generation models,model resort to off - the-shelf retrieval algorithms and generation models,0.7051953673362732
translation,41,33,model,multi-level ranking objective,jointly combines,multiple binary contrastive estimations,multi-level ranking objective jointly combines multiple binary contrastive estimations,0.7916266918182373
translation,41,33,model,model,has,multi-level ranking objective,model has multi-level ranking objective,0.4948168396949768
translation,41,77,model,seq2seq model,with,attention mechanism,seq2seq model with attention mechanism,0.6091710329055786
translation,41,77,model,attention mechanism,for,response generation,attention mechanism for response generation,0.5484714508056641
translation,41,77,model,model,train,seq2seq model,model train seq2seq model,0.7313029766082764
translation,41,34,results,grayscale data,partly simulates,real-world response distractors,grayscale data partly simulates real-world response distractors,0.7522225379943848
translation,41,34,results,grayscale data,reduces,gap,grayscale data reduces gap,0.7523924112319946
translation,41,34,results,gap,between,training and testing,gap between training and testing,0.6692854762077332
translation,41,34,results,gap,leading to,better distinguishing ability,gap leading to better distinguishing ability,0.6912505626678467
translation,41,34,results,better distinguishing ability,for,strong response distractors,better distinguishing ability for strong response distractors,0.6081011295318604
translation,41,34,results,results,has,grayscale data,results has grayscale data,0.5453270673751831
translation,41,149,results,performance,of,all four matching models,performance of all four matching models,0.5499149560928345
translation,41,149,results,all four matching models,in terms of,various metrics,all four matching models in terms of various metrics,0.6436983346939087
translation,41,149,results,our training approach,has,significantly improves,our training approach has significantly improves,0.6022414565086365
translation,41,149,results,significantly improves,has,performance,significantly improves has performance,0.5962982177734375
translation,41,149,results,results,see that,our training approach,results see that our training approach,0.6186257004737854
translation,41,151,results,interesting observation,is that,less-accurate matching architecture,interesting observation is that less-accurate matching architecture,0.6204293370246887
translation,41,151,results,less-accurate matching architecture,with,proposed training approach,less-accurate matching architecture with proposed training approach,0.6444052457809448
translation,41,151,results,stronger matching architecture,with,traditional training paradigm,stronger matching architecture with traditional training paradigm,0.619223952293396
translation,41,151,results,outperform,has,stronger matching architecture,outperform has stronger matching architecture,0.6063404083251953
translation,41,157,results,retrieval data and generation data,make,irreplaceable contributions,retrieval data and generation data make irreplaceable contributions,0.6431568264961243
translation,41,157,results,irreplaceable contributions,to,overall performance,irreplaceable contributions to overall performance,0.5090900659561157
translation,41,157,results,combination of both worlds,makes,best results,combination of both worlds makes best results,0.564956545829773
translation,41,157,results,results,find that,retrieval data and generation data,results find that retrieval data and generation data,0.6650267243385315
translation,41,168,results,improvements,of,grayscale data,improvements of grayscale data,0.6045288443565369
translation,41,168,results,grayscale data,without,mro,grayscale data without mro,0.7581376433372498
translation,41,168,results,mro,are,quite limited,mro are quite limited,0.5879032611846924
translation,41,168,results,quite limited,compared to,original counterparts,quite limited compared to original counterparts,0.6754124760627747
translation,41,168,results,original counterparts,without,grayscale data,original counterparts without grayscale data,0.7563255429267883
translation,41,168,results,results,has,improvements,results has improvements,0.615561842918396
translation,41,186,results,models,trained with,our approach,models trained with our approach,0.6735982894897461
translation,41,186,results,outperform,trained with,co-teaching framework,outperform trained with co-teaching framework,0.700589120388031
translation,41,186,results,our approach,has,outperform,our approach has outperform,0.6391545534133911
translation,41,186,results,results,see that,models,results see that models,0.6295062303543091
translation,41,187,results,smn + cot and dam + cot,obtain,further improvements,smn + cot and dam + cot obtain further improvements,0.5684908628463745
translation,41,187,results,further improvements,after adding,our multi-level ranking objectives,further improvements after adding our multi-level ranking objectives,0.6752406358718872
translation,41,187,results,results,has,smn + cot and dam + cot,results has smn + cot and dam + cot,0.5310063362121582
translation,42,52,baselines,6 generative models,with,3 decoding methods,6 generative models with 3 decoding methods,0.49079418182373047
translation,42,52,baselines,6 generative models,has,"s2s ( sutskever et al. , 2014 )","6 generative models has s2s ( sutskever et al. , 2014 )",0.5245955586433411
translation,42,52,baselines,6 generative models,has,attentional s2s,6 generative models has attentional s2s,0.5230333805084229
translation,42,52,baselines,3 decoding methods,has,greedy decoding,3 decoding methods has greedy decoding,0.5663191080093384
translation,42,52,baselines,nucleus sampling,has,"holtzman et al. , 2019","nucleus sampling has holtzman et al. , 2019",0.5605989694595337
translation,42,6,model,power,of,semi-supervised training,power of semi-supervised training,0.5972551703453064
translation,42,6,model,power,of,pretrained ( masked ) language models,power of pretrained ( masked ) language models,0.5735724568367004
translation,42,6,model,model,build,reference -free evaluator,model build reference -free evaluator,0.6995217204093933
translation,42,23,model,strong pretrained models,such as,"roberta ( liu et al. , 2019 )","strong pretrained models such as roberta ( liu et al. , 2019 )",0.5910801887512207
translation,42,23,model,strong pretrained models,to obtain,better text representations,strong pretrained models to obtain better text representations,0.5162380337715149
translation,42,23,model,model,make use of,strong pretrained models,model make use of strong pretrained models,0.6749523878097534
translation,42,20,results,referencedependent evaluators ' performance,remove,ground - truth responses,referencedependent evaluators ' performance remove ground - truth responses,0.6131359934806824
translation,42,20,results,degrades significantly,remove,ground - truth responses,degrades significantly remove ground - truth responses,0.7072291374206543
translation,42,20,results,ground - truth responses,from,test data,ground - truth responses from test data,0.5194073915481567
translation,42,20,results,referencedependent evaluators ' performance,has,degrades significantly,referencedependent evaluators ' performance has degrades significantly,0.5841270685195923
translation,42,20,results,results,find that,referencedependent evaluators ' performance,results find that referencedependent evaluators ' performance,0.5917199850082397
translation,42,22,results,ruber,can be further improved,supervised training,ruber can be further improved supervised training,0.7437963485717773
translation,42,22,results,supervised training,on,small amount of annotated data,supervised training on small amount of annotated data,0.4998406767845154
translation,42,22,results,results,show that,ruber,results show that ruber,0.5443540811538696
translation,42,24,results,three methods,has,"reference-free , semi-supervised , robertabased evaluator","three methods has reference-free , semi-supervised , robertabased evaluator",0.5487715005874634
translation,42,24,results,"reference-free , semi-supervised , robertabased evaluator",has,better correlation and robustness,"reference-free , semi-supervised , robertabased evaluator has better correlation and robustness",0.528235673904419
translation,42,24,results,results,combining,three methods,results combining three methods,0.6604547500610352
translation,42,84,results,evaluator,trained on,dd corpus,evaluator trained on dd corpus,0.6882206201553345
translation,42,84,results,evaluator,achieves,even higher correlation scores,evaluator achieves even higher correlation scores,0.6252173781394958
translation,42,84,results,even higher correlation scores,applied to,pc corpus,even higher correlation scores applied to pc corpus,0.735980749130249
translation,42,84,results,results,has,evaluator,results has evaluator,0.5632435083389282
translation,42,85,results,performance degradation,observed,evaluator,performance degradation observed evaluator,0.6627643704414368
translation,42,85,results,performance degradation,when applying,evaluator,performance degradation when applying evaluator,0.6727269887924194
translation,42,85,results,evaluator,trained on,pc corpus,evaluator trained on pc corpus,0.6988686919212341
translation,42,85,results,pc corpus,to,dd corpus,pc corpus to dd corpus,0.559110164642334
translation,42,85,results,results,has,performance degradation,results has performance degradation,0.5868020057678223
translation,42,95,results,ground - truth,from,test data,ground - truth from test data,0.53685462474823
translation,42,95,results,roberta evaluator,achieves,0.62 pearson 's correlation,roberta evaluator achieves 0.62 pearson 's correlation,0.5737720727920532
translation,42,95,results,roberta evaluator,achieves,0.64 spearman 's correlation,roberta evaluator achieves 0.64 spearman 's correlation,0.5938701033592224
translation,42,95,results,ground - truth,has,roberta evaluator,ground - truth has roberta evaluator,0.5242297649383545
translation,42,95,results,test data,has,roberta evaluator,test data has roberta evaluator,0.5458537936210632
translation,42,95,results,results,After removing,ground - truth,results After removing ground - truth,0.6677055954933167
translation,42,96,results,evaluator,achieves,good performances,evaluator achieves good performances,0.618757426738739
translation,42,96,results,good performances,on,diverse responses,good performances on diverse responses,0.507843017578125
translation,42,96,results,good performances,on,different corpora,good performances on different corpora,0.44348323345184326
translation,42,96,results,results,has,evaluator,results has evaluator,0.5632435083389282
translation,43,141,baselines,four popular variants,proposed to improve,diversity of generated utterances,four popular variants proposed to improve diversity of generated utterances,0.6999958753585815
translation,43,154,baselines,conditional variational autoencoder,injects,diversity,conditional variational autoencoder injects diversity,0.6599076390266418
translation,43,154,baselines,diversity,by imposing,stochastical latent variables,diversity by imposing stochastical latent variables,0.6545363068580627
translation,43,154,baselines,baselines,has,conditional variational autoencoder,baselines has conditional variational autoencoder,0.5359464883804321
translation,43,156,baselines,approaches,incorporate,collected nonconversational text,approaches incorporate collected nonconversational text,0.6752592921257019
translation,43,156,baselines,baselines,incorporate,collected nonconversational text,baselines incorporate collected nonconversational text,0.6761478781700134
translation,43,201,baselines,nucleus sampling,performs,best,nucleus sampling performs best,0.6641583442687988
translation,43,201,baselines,best,among,all seq2seq variants,best among all seq2seq variants,0.5768516063690186
translation,43,201,baselines,four approaches,leveraging,non-conversational text,four approaches leveraging non-conversational text,0.7049658894538879
translation,43,201,baselines,standard seq2seq model,has,nucleus sampling,standard seq2seq model has nucleus sampling,0.5402029156684875
translation,43,37,experimental-setup,forward and backward transduction model,on,parallel conversational corpus,forward and backward transduction model on parallel conversational corpus,0.5458714365959167
translation,43,37,experimental-setup,experimental setup,pre-train,forward and backward transduction model,experimental setup pre-train forward and backward transduction model,0.6942295432090759
translation,43,136,experiments,vocabulary,with,character - based segmentation,vocabulary with character - based segmentation,0.650536060333252
translation,43,136,experiments,character - based segmentation,for,chinese,character - based segmentation for chinese,0.6283982992172241
translation,43,133,hyperparameters,encoder / decoder structure,with,hidden size 500,encoder / decoder structure with hidden size 500,0.676720142364502
translation,43,133,hyperparameters,encoder / decoder structure,with,word embedding size 300,encoder / decoder structure with word embedding size 300,0.6756594181060791
translation,43,133,hyperparameters,hyperparameters,use,two -layer lstm,hyperparameters use two -layer lstm,0.5993269681930542
translation,43,134,hyperparameters,"adam optimizer ( kingma and ba , 2015 )",with,initial learning rate,"adam optimizer ( kingma and ba , 2015 ) with initial learning rate",0.5927683115005493
translation,43,134,hyperparameters,initial learning rate,of,0.15,initial learning rate of 0.15,0.5833383202552795
translation,43,134,hyperparameters,hyperparameters,trained with,"adam optimizer ( kingma and ba , 2015 )","hyperparameters trained with adam optimizer ( kingma and ba , 2015 )",0.7312348484992981
translation,43,135,hyperparameters,batch size,as,256,batch size as 256,0.5569828748703003
translation,43,135,hyperparameters,gradients clipping,of,5,gradients clipping of 5,0.5945581793785095
translation,43,135,hyperparameters,hyperparameters,set,batch size,hyperparameters set batch size,0.6402808427810669
translation,43,135,hyperparameters,hyperparameters,use,gradients clipping,hyperparameters use gradients clipping,0.6133019924163818
translation,43,138,hyperparameters,utterances,cut down to,at most 50 tokens,utterances cut down to at most 50 tokens,0.7511650323867798
translation,43,138,hyperparameters,utterances,fed to,every batch,utterances fed to every batch,0.7158483266830444
translation,43,138,hyperparameters,hyperparameters,has,utterances,hyperparameters has utterances,0.5495290756225586
translation,43,139,hyperparameters,our models,based on,opennmt toolkit,our models based on opennmt toolkit,0.6710776686668396
translation,43,139,hyperparameters,hyperparameters,implement,our models,hyperparameters implement our models,0.6288490891456604
translation,43,149,hyperparameters,number of groups,as,"5 , ? = 0.3","number of groups as 5 , ? = 0.3",0.5406906008720398
translation,43,149,hyperparameters,hamming diversity,as,penalty function,hamming diversity as penalty function,0.5346561670303345
translation,43,149,hyperparameters,hyperparameters,set,number of groups,hyperparameters set number of groups,0.6683459281921387
translation,43,149,hyperparameters,hyperparameters,use,hamming diversity,hyperparameters use hamming diversity,0.6690683364868164
translation,43,155,hyperparameters,latent variable,dimension,100,latent variable dimension 100,0.7818710207939148
translation,43,155,hyperparameters,latent variable,utilize,kl - annealing strategy,latent variable utilize kl - annealing strategy,0.6341728568077087
translation,43,155,hyperparameters,kl - annealing strategy,with,step 350 k,kl - annealing strategy with step 350 k,0.6696736216545105
translation,43,155,hyperparameters,word drop-out rate,of,0.3,word drop-out rate of 0.3,0.6230712532997131
translation,43,155,hyperparameters,word drop-out rate,to alleviate,posterior collapse problem,word drop-out rate to alleviate posterior collapse problem,0.6904183626174927
translation,43,155,hyperparameters,0.3,to alleviate,posterior collapse problem,0.3 to alleviate posterior collapse problem,0.6490136384963989
translation,43,155,hyperparameters,hyperparameters,use,latent variable,hyperparameters use latent variable,0.6403775215148926
translation,43,155,hyperparameters,hyperparameters,utilize,kl - annealing strategy,hyperparameters utilize kl - annealing strategy,0.6060459017753601
translation,43,6,model,perspective,to,diversify,perspective to diversify,0.5968112945556641
translation,43,6,model,diversify,has,dialogue generation,diversify has dialogue generation,0.568264901638031
translation,43,6,model,model,propose,perspective,model propose perspective,0.6940332651138306
translation,43,30,model,cheap way,to diversify,dialogue generation,cheap way to diversify dialogue generation,0.6941567063331604
translation,43,30,model,model,explore,cheap way,model explore cheap way,0.7383079528808594
translation,43,178,results,models,trained only on,conversational corpus,models trained only on conversational corpus,0.711357831954956
translation,43,178,results,standard seq2seq,performed,worst,standard seq2seq performed worst,0.30886074900627136
translation,43,178,results,models,has,standard seq2seq,models has standard seq2seq,0.534737229347229
translation,43,178,results,conversational corpus,has,standard seq2seq,conversational corpus has standard seq2seq,0.5424082279205322
translation,43,178,results,results,Among,models,results Among models,0.6251620650291443
translation,43,180,results,nucleus sampling and cvae,generated,most diverse responses,nucleus sampling and cvae generated most diverse responses,0.6313363909721375
translation,43,180,results,results,has,nucleus sampling and cvae,results has nucleus sampling and cvae,0.482635498046875
translation,43,181,results,non-conversational corpus,has,diversity of generated responses,non-conversational corpus has diversity of generated responses,0.5793053507804871
translation,43,181,results,diversity of generated responses,has,improves dramatically,diversity of generated responses has improves dramatically,0.5966295003890991
translation,43,181,results,results,incorporating,non-conversational corpus,results incorporating non-conversational corpus,0.625754177570343
translation,43,182,results,retrieval - based system and our model,perform,best,retrieval - based system and our model perform best,0.5909002423286438
translation,43,182,results,even better,than,human references,even better than human references,0.6125790476799011
translation,43,182,results,cases,has,even better,cases has even better,0.5899866223335266
translation,43,182,results,results,has,retrieval - based system and our model,results has retrieval - based system and our model,0.5937215089797974
translation,43,186,results,our model,improves over,standard seq2seq,our model improves over standard seq2seq,0.758231520652771
translation,43,186,results,bit,after,one iteration,bit after one iteration,0.7289281487464905
translation,43,186,results,standard seq2seq,has,bit,standard seq2seq has bit,0.5551722049713135
translation,43,186,results,results,observe,our model,results observe our model,0.6353915333747864
translation,43,202,results,all models,perform,decently well,all models perform decently well,0.605476975440979
translation,43,202,results,decently well,as for,fluency,decently well as for fluency,0.7216165065765381
translation,43,202,results,fluency,except,weighted average one,fluency except weighted average one,0.6902651190757751
translation,43,202,results,results,has,all models,results has all models,0.5029959678649902
translation,44,113,ablation-analysis,effective,for,next da prediction,effective for next da prediction,0.6699918508529663
translation,44,113,ablation-analysis,superior performance,of,daseq-only,superior performance of daseq-only,0.6074213981628418
translation,44,113,ablation-analysis,daseq-only,to,utteranceseq,daseq-only to utteranceseq,0.5959940552711487
translation,44,113,ablation-analysis,ablation analysis,has,sequence of das,ablation analysis has sequence of das,0.5706068873405457
translation,44,117,ablation-analysis,sequence of utterances,contributes to,accuracy,sequence of utterances contributes to accuracy,0.6846674680709839
translation,44,118,ablation-analysis,effective,to predict,infrequent das,effective to predict infrequent das,0.8092890381813049
translation,44,118,ablation-analysis,effective,to predict,common das,effective to predict common das,0.7907567024230957
translation,44,118,ablation-analysis,ablation analysis,imply,sequence of das,ablation analysis imply sequence of das,0.5550636649131775
translation,44,118,ablation-analysis,ablation analysis,imply,sequence of utterances,ablation analysis imply sequence of utterances,0.5679112076759338
translation,44,77,hyperparameters,dimensions,of,word embedding,dimensions of word embedding,0.5467033982276917
translation,44,77,hyperparameters,dimensions,of,da embedding,dimensions of da embedding,0.5803983211517334
translation,44,77,hyperparameters,word embedding,to,300,word embedding to 300,0.591537356376648
translation,44,77,hyperparameters,da embedding,to,100,da embedding to 100,0.6906729936599731
translation,44,77,hyperparameters,hyperparameters,set,dimensions,hyperparameters set dimensions,0.6602594256401062
translation,44,9,model,method,to predict,da,method to predict da,0.7102534770965576
translation,44,9,model,da,of,next response,da of next response,0.6138408184051514
translation,44,9,model,next response,based on,history of previous utterances and their das,next response based on history of previous utterances and their das,0.6634598970413208
translation,44,28,model,sequences,text surfaces and DAs of,utterances,sequences text surfaces and DAs of utterances,0.767805814743042
translation,44,28,model,utterances,using,recurrent neural network ( rnn ),utterances using recurrent neural network ( rnn ),0.7457630038261414
translation,44,28,model,model,independently encodes,sequences,model independently encodes sequences,0.7899155616760254
translation,44,76,model,gated recurrent unit ( gru ),to,each rnn,gated recurrent unit ( gru ) to each rnn,0.5640454292297363
translation,44,76,model,each rnn,in,our model,each rnn in our model,0.5194960236549377
translation,44,76,model,model,apply,gated recurrent unit ( gru ),model apply gated recurrent unit ( gru ),0.6365325450897217
translation,44,94,model,model,has,utterance-only,model has utterance-only,0.5952185988426208
translation,44,34,results,our model,considers,history of utterances and their das,our model considers history of utterances and their das,0.6453561782836914
translation,44,34,results,outperforms,considers,input utterance,outperforms considers input utterance,0.7070110440254211
translation,44,34,results,baseline,considers,input utterance,baseline considers input utterance,0.6357026100158691
translation,44,34,results,input utterance,by,10.8 % f1,input utterance by 10.8 % f1,0.5530657768249512
translation,44,34,results,input utterance,by,3.0 % accuracy,input utterance by 3.0 % accuracy,0.5518529415130615
translation,44,34,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,44,34,results,outperforms,has,baseline,outperforms has baseline,0.6131853461265564
translation,44,34,results,results,show,our model,results show our model,0.6888449192047119
translation,44,102,results,our model,exhibits,best performances,our model exhibits best performances,0.6384576559066772
translation,44,102,results,"recall , f1 , and accuracy",has,"32.5 % , 32.4 % , and 69.7 %","recall , f1 , and accuracy has 32.5 % , 32.4 % , and 69.7 %",0.5372247695922852
translation,44,108,results,max - probability,performs,quite poorly,max - probability performs quite poorly,0.6187237501144409
translation,44,108,results,quite poorly,rather than,other neural network based model,quite poorly rather than other neural network based model,0.6414619088172913
translation,44,108,results,results,has,max - probability,results has max - probability,0.5855147838592529
translation,44,110,results,utterance-seq,achieves,1.8 % higher accuracy,utterance-seq achieves 1.8 % higher accuracy,0.6670808792114258
translation,44,110,results,1.8 % higher accuracy,than,utterance-only,1.8 % higher accuracy than utterance-only,0.55610591173172
translation,44,110,results,results,has,utterance-seq,results has utterance-seq,0.5259868502616882
translation,44,111,results,utteranceseq,on,f1,utteranceseq on f1,0.6105554103851318
translation,44,111,results,utteranceseq,by,6.5 %,utteranceseq by 6.5 %,0.5994569063186646
translation,44,111,results,da + utterance -seq,has,outperforms,da + utterance -seq has outperforms,0.6271289587020874
translation,44,111,results,outperforms,has,utteranceseq,outperforms has utteranceseq,0.6277632713317871
translation,44,111,results,results,has,da + utterance -seq,results has da + utterance -seq,0.5412882566452026
translation,44,114,results,daseq-only,performs,4.1 % higher macro- f1,daseq-only performs 4.1 % higher macro- f1,0.6189041137695312
translation,44,114,results,daseq-only,performs,1.4 % lower accuracy,daseq-only performs 1.4 % lower accuracy,0.600111722946167
translation,44,114,results,4.1 % higher macro- f1,than,utterance-seq,4.1 % higher macro- f1 than utterance-seq,0.5553919076919556
translation,44,114,results,4.1 % higher macro- f1,than,utterance-seq,4.1 % higher macro- f1 than utterance-seq,0.5553919076919556
translation,44,114,results,1.4 % lower accuracy,than,utterance-seq,1.4 % lower accuracy than utterance-seq,0.5580542087554932
translation,44,114,results,results,has,daseq-only,results has daseq-only,0.5469257831573486
translation,44,115,results,daseq + utterance,achieves,5.5 % higher f1,daseq + utterance achieves 5.5 % higher f1,0.6678452491760254
translation,44,115,results,5.5 % higher f1,than,utterance-seq,5.5 % higher f1 than utterance-seq,0.5580688118934631
translation,44,115,results,results,has,daseq + utterance,results has daseq + utterance,0.563926637172699
translation,44,116,results,das,of,single - turn or a sequence,das of single - turn or a sequence,0.6380405426025391
translation,44,116,results,das,either,single - turn or a sequence,das either single - turn or a sequence,0.8061723709106445
translation,44,116,results,das,largely boost,"precision , recall , and f1","das largely boost precision , recall , and f1",0.7056208252906799
translation,44,116,results,single - turn or a sequence,largely boost,"precision , recall , and f1","single - turn or a sequence largely boost precision , recall , and f1",0.7200579643249512
translation,44,116,results,results,has,das,results has das,0.5354359149932861
translation,44,122,results,utterance-seq,on,all the das,utterance-seq on all the das,0.5819051861763
translation,44,122,results,proposed model,has,outperforms,proposed model has outperforms,0.642342746257782
translation,44,122,results,outperforms,has,utterance-seq,outperforms has utterance-seq,0.5227330327033997
translation,44,122,results,results,has,proposed model,results has proposed model,0.5938616394996643
translation,44,123,results,infrequent tags,of,agreement,infrequent tags of agreement,0.6046451926231384
translation,44,123,results,infrequent tags,of,greeting,infrequent tags of greeting,0.5682383179664612
translation,44,123,results,infrequent tags,of,question,infrequent tags of question,0.6041345596313477
translation,44,123,results,infrequent tags,of,apology,infrequent tags of apology,0.576994001865387
translation,44,123,results,infrequent tags,show,significant improvements,infrequent tags show significant improvements,0.6190945506095886
translation,44,123,results,significant improvements,between,6.1 % and 34.6 %,significant improvements between 6.1 % and 34.6 %,0.610108494758606
translation,44,123,results,results,has,infrequent tags,results has infrequent tags,0.5944963693618774
translation,45,147,ablation-analysis,adaptive objective,boost,performance,adaptive objective boost performance,0.7099741697311401
translation,45,147,ablation-analysis,ablation analysis,show,state transition prediction task,ablation analysis show state transition prediction task,0.6134979724884033
translation,45,148,ablation-analysis,state transition prediction task,reduces,joint accuracy,state transition prediction task reduces joint accuracy,0.6160799860954285
translation,45,148,ablation-analysis,state transition prediction task,reduces,joint accuracy,state transition prediction task reduces joint accuracy,0.6160799860954285
translation,45,148,ablation-analysis,joint accuracy,by,0.69 %,joint accuracy by 0.69 %,0.5411688089370728
translation,45,148,ablation-analysis,joint accuracy,decreases by,1.10 %,joint accuracy decreases by 1.10 %,0.6896671652793884
translation,45,148,ablation-analysis,1.10 %,without,adaptive objective fine-tuning,1.10 % without adaptive objective fine-tuning,0.7160390615463257
translation,45,148,ablation-analysis,ablation analysis,Removing,state transition prediction task,ablation analysis Removing state transition prediction task,0.6960939764976501
translation,45,149,ablation-analysis,our model,with,adaptive objective,our model with adaptive objective,0.6628298163414001
translation,45,149,ablation-analysis,joint accuracy,decreases by,1.55 %,joint accuracy decreases by 1.55 %,0.6843630075454712
translation,45,149,ablation-analysis,state transition prediction task,has,joint accuracy,state transition prediction task has joint accuracy,0.5110662579536438
translation,45,149,ablation-analysis,ablation analysis,remove,state transition prediction task,ablation analysis remove state transition prediction task,0.6733575463294983
translation,45,150,ablation-analysis,adaptive objective,with,"original focal loss ( ? = 1 , ? = 2 )","adaptive objective with original focal loss ( ? = 1 , ? = 2 )",0.6335278153419495
translation,45,150,ablation-analysis,adaptive objective,leads to,0.45 % drop,adaptive objective leads to 0.45 % drop,0.650859534740448
translation,45,150,ablation-analysis,ablation analysis,replace,adaptive objective,ablation analysis replace adaptive objective,0.6225442886352539
translation,45,157,ablation-analysis,slight joint accuracy drop,of,0.24 %,slight joint accuracy drop of 0.24 %,0.5524497628211975
translation,45,157,ablation-analysis,0.24 %,after removing,global- local fusion gate,0.24 % after removing global- local fusion gate,0.7114524245262146
translation,45,158,ablation-analysis,slot-turn attention and context encoder,leads to,decrease,slot-turn attention and context encoder leads to decrease,0.7097563743591309
translation,45,158,ablation-analysis,decrease,by,0.15 % and 1.72 %,decrease by 0.15 % and 1.72 %,0.6295519471168518
translation,45,158,ablation-analysis,ablation analysis,removing,slot-turn attention and context encoder,ablation analysis removing slot-turn attention and context encoder,0.7482523918151855
translation,45,159,ablation-analysis,wordlevel relevant information,of,"{ 1 , ? ? ? , t}","wordlevel relevant information of { 1 , ? ? ? , t}",0.5536022186279297
translation,45,159,ablation-analysis,joint accuracy,reduces by,6.72 %,joint accuracy reduces by 6.72 %,0.6222701072692871
translation,45,172,ablation-analysis,our adaptive objective,not only improve,performance,our adaptive objective not only improve performance,0.7011593580245972
translation,45,172,ablation-analysis,performance,of,relatively hard slots,performance of relatively hard slots,0.601050078868866
translation,45,172,ablation-analysis,performance,of,relatively easy slots,performance of relatively easy slots,0.5824255347251892
translation,45,172,ablation-analysis,performance,of,relatively easy slots,performance of relatively easy slots,0.5824255347251892
translation,45,172,ablation-analysis,performance,of,relatively easy slots,performance of relatively easy slots,0.5824255347251892
translation,45,172,ablation-analysis,boost,has,performance,boost has performance,0.5791411995887756
translation,45,172,ablation-analysis,ablation analysis,proves that,our adaptive objective,ablation analysis proves that our adaptive objective,0.5909482836723328
translation,45,128,baselines,dst - qa,uses,dynamically - evolving knowledge graph,dst - qa uses dynamically - evolving knowledge graph,0.5554026961326599
translation,45,128,baselines,dynamically - evolving knowledge graph,to learn,relationships,dynamically - evolving knowledge graph to learn relationships,0.5883039832115173
translation,45,128,baselines,relationships,between,slot pairs,relationships between slot pairs,0.7150853276252747
translation,45,128,baselines,som - dst,considers,dialogue state,som - dst considers dialogue state,0.6622097492218018
translation,45,128,baselines,som - dst,proposes,selectively overwriting mechanism,som - dst proposes selectively overwriting mechanism,0.64719158411026
translation,45,128,baselines,dialogue state,as,explicit fixed - size memory,dialogue state as explicit fixed - size memory,0.5157062411308289
translation,45,128,baselines,baselines,has,dst - qa,baselines has dst - qa,0.5882466435432434
translation,45,30,experiments,relevant context,employ,state transition prediction task,relevant context employ state transition prediction task,0.5411392450332642
translation,45,30,experiments,state transition prediction task,to assist,dst learning,state transition prediction task to assist dst learning,0.6167362332344055
translation,45,135,hyperparameters,pre-trained bert model,has,12 layers,pre-trained bert model has 12 layers,0.5520052313804626
translation,45,135,hyperparameters,pre-trained bert model,has,12 self-attention heads,pre-trained bert model has 12 self-attention heads,0.5694742202758789
translation,45,135,hyperparameters,12 layers,of,784 hidden units,12 layers of 784 hidden units,0.5571439862251282
translation,45,135,hyperparameters,12 self-attention heads,For,multi-head attention,12 self-attention heads For multi-head attention,0.6058811545372009
translation,45,135,hyperparameters,multi-head attention,set,heads count and hidden size,multi-head attention set heads count and hidden size,0.6436852216720581
translation,45,135,hyperparameters,heads count and hidden size,to,4 and 784,heads count and hidden size to 4 and 784,0.6015060544013977
translation,45,135,hyperparameters,hyperparameters,employ,pre-trained bert model,hyperparameters employ pre-trained bert model,0.49936312437057495
translation,45,135,hyperparameters,hyperparameters,For,multi-head attention,hyperparameters For multi-head attention,0.5533403158187866
translation,45,136,hyperparameters,context encoder,set,transformer layers,context encoder set transformer layers,0.6279107332229614
translation,45,136,hyperparameters,transformer layers,to,6,transformer layers to 6,0.6045917272567749
translation,45,136,hyperparameters,hyperparameters,For,context encoder,hyperparameters For context encoder,0.5108715295791626
translation,45,137,hyperparameters,max sequence length,of,all inputs,max sequence length of all inputs,0.5888087153434753
translation,45,137,hyperparameters,all inputs,to,64,all inputs to 64,0.6084914207458496
translation,45,137,hyperparameters,hyperparameters,set,max sequence length,hyperparameters set max sequence length,0.6196408867835999
translation,45,137,hyperparameters,hyperparameters,set,batch size,hyperparameters set batch size,0.6402808427810669
translation,45,138,hyperparameters,adam optimizer,set,warmup proportion,adam optimizer set warmup proportion,0.6395756602287292
translation,45,138,hyperparameters,warmup proportion,to,0.1,warmup proportion to 0.1,0.5614251494407654
translation,45,138,hyperparameters,hyperparameters,set,warmup proportion,hyperparameters set warmup proportion,0.634702742099762
translation,45,139,hyperparameters,joint training phase,set,peak learning rate,joint training phase set peak learning rate,0.6474187970161438
translation,45,139,hyperparameters,peak learning rate,to,1e - 4,peak learning rate to 1e - 4,0.5759562253952026
translation,45,139,hyperparameters,hyperparameters,in,joint training phase,hyperparameters in joint training phase,0.44529107213020325
translation,45,140,hyperparameters,fine-tuning phase,set,2,fine-tuning phase set 2,0.67318195104599
translation,45,140,hyperparameters,fine-tuning phase,set,peak learning rate,fine-tuning phase set peak learning rate,0.634624183177948
translation,45,140,hyperparameters,peak learning rate,to,1e - 5,peak learning rate to 1e - 5,0.5836686491966248
translation,45,140,hyperparameters,hyperparameters,At,fine-tuning phase,hyperparameters At fine-tuning phase,0.49132904410362244
translation,45,141,hyperparameters,training,stopped,early,training stopped early,0.7952266931533813
translation,45,141,hyperparameters,early,when,validation loss,early when validation loss,0.6646761298179626
translation,45,141,hyperparameters,not improved,for,15 consecutive epochs,not improved for 15 consecutive epochs,0.6223935484886169
translation,45,141,hyperparameters,hyperparameters,has,training,hyperparameters has training,0.519983172416687
translation,45,7,model,dst,through employing,contextual hierarchical attention network,dst through employing contextual hierarchical attention network,0.6593813300132751
translation,45,7,model,contextual hierarchical attention network,discern,relevant information,contextual hierarchical attention network discern relevant information,0.6486388444900513
translation,45,7,model,contextual hierarchical attention network,learn,contextual representations,contextual hierarchical attention network learn contextual representations,0.6564025282859802
translation,45,7,model,relevant information,at,word level and turn level,relevant information at word level and turn level,0.542512834072113
translation,45,7,model,model,enhance,dst,model enhance dst,0.683566153049469
translation,45,8,model,adaptive objective,to alleviate,slot imbalance problem,adaptive objective to alleviate slot imbalance problem,0.6485767364501953
translation,45,8,model,slot imbalance problem,by dynamically adjust,weights,slot imbalance problem by dynamically adjust weights,0.7821301221847534
translation,45,8,model,weights,of,different slots,weights of different slots,0.6276424527168274
translation,45,8,model,different slots,during,training,different slots during training,0.7369286417961121
translation,45,8,model,model,propose,adaptive objective,model propose adaptive objective,0.7048900127410889
translation,45,26,model,effective model,equipped with,contextual hierarchical attention network ( chan ),effective model equipped with contextual hierarchical attention network ( chan ),0.7077558040618896
translation,45,26,model,contextual hierarchical attention network ( chan ),to fully exploit,relevant context,contextual hierarchical attention network ( chan ) to fully exploit relevant context,0.6688531041145325
translation,45,26,model,relevant context,from,dialogue history,relevant context from dialogue history,0.566406786441803
translation,45,26,model,adaptive objective,to alleviate,slot imbalance problem,adaptive objective to alleviate slot imbalance problem,0.6485767364501953
translation,45,26,model,model,propose,effective model,model propose effective model,0.6346461772918701
translation,45,27,model,slot,firstly retrieves,word-level relevant information,slot firstly retrieves word-level relevant information,0.7634766697883606
translation,45,27,model,word-level relevant information,from,utterances,word-level relevant information from utterances,0.5901244878768921
translation,45,27,model,utterances,at,each turn,utterances at each turn,0.5724389553070068
translation,45,27,model,chan,has,slot,chan has slot,0.6305991411209106
translation,45,27,model,model,In,chan,model In chan,0.638816773891449
translation,45,49,model,context encoder,between,word-level and turn-level attention,context encoder between word-level and turn-level attention,0.61118084192276
translation,45,49,model,word-level and turn-level attention,to capture,contextual representations,word-level and turn-level attention to capture contextual representations,0.6638779640197754
translation,45,49,model,contextual representations,of,relevant information,contextual representations of relevant information,0.5540792942047119
translation,45,49,model,relevant information,from,dialogue history,relevant information from dialogue history,0.5490162968635559
translation,45,49,model,model,exploit,context encoder,model exploit context encoder,0.7781280279159546
translation,45,77,model,context encoder,contains,stack of n identical layers,context encoder contains stack of n identical layers,0.5667501091957092
translation,45,77,model,model,has,context encoder,model has context encoder,0.5556648969650269
translation,45,78,model,layer,has,two sub-layers,layer has two sub-layers,0.6229441165924072
translation,45,78,model,model,has,layer,model has layer,0.587888777256012
translation,45,126,model,hyst,employs,hierarchical encoder,hyst employs hierarchical encoder,0.6082888841629028
translation,45,126,model,hyst,takes,hybrid way,hyst takes hybrid way,0.719519317150116
translation,45,126,model,hybrid way,combining,predefined -ontology and open-vocabulary settings,hybrid way combining predefined -ontology and open-vocabulary settings,0.7808125615119934
translation,45,126,model,model,has,hyst,model has hyst,0.6242734789848328
translation,45,127,model,trade,encodes,whole dialogue context,trade encodes whole dialogue context,0.7632924914360046
translation,45,127,model,trade,decodes,value,trade decodes value,0.713292121887207
translation,45,127,model,value,for,every slot,value for every slot,0.6559250950813293
translation,45,127,model,every slot,using,copyaugmented decoder,every slot using copyaugmented decoder,0.7075622081756592
translation,45,127,model,model,has,trade,model has trade,0.5830050706863403
translation,45,129,model,sumbt,exploits,bert,sumbt exploits bert,0.687208890914917
translation,45,129,model,bert,as,encoder,bert as encoder,0.6353098154067993
translation,45,129,model,encoder,of,"utterances , slots and values","encoder of utterances , slots and values",0.58417809009552
translation,45,129,model,model,has,sumbt,model has sumbt,0.6577731370925903
translation,45,176,results,64.49 % improvements,come from,historical information inference,64.49 % improvements come from historical information inference,0.6020804047584534
translation,45,176,results,historical information inference,demonstrates,our model,historical information inference demonstrates our model,0.6431891322135925
translation,46,149,ablation-analysis,coupling,of,entropy series,coupling of entropy series,0.6025900840759277
translation,46,149,ablation-analysis,entropy series,in,frequency space,entropy series in frequency space,0.52849280834198
translation,46,149,ablation-analysis,entropy series,negatively correlated with,task success,entropy series negatively correlated with task success,0.7332352995872498
translation,46,149,ablation-analysis,ablation analysis,has,coupling,ablation analysis has coupling,0.5513368248939514
translation,46,180,ablation-analysis,mean of rp,significant predictor of,task success,mean of rp significant predictor of task success,0.7414683103561401
translation,46,180,ablation-analysis,djd,has,mean of rp,djd has mean of rp,0.5811477899551392
translation,46,180,ablation-analysis,ablation analysis,For,djd,ablation analysis For djd,0.6233036518096924
translation,46,205,ablation-analysis,significant improvement,in,model 's explanatory power,significant improvement in model 's explanatory power,0.4307602047920227
translation,46,205,ablation-analysis,model 's explanatory power,i.e.,r 2,model 's explanatory power i.e. r 2,0.616918683052063
translation,46,205,ablation-analysis,model 's explanatory power,gained after,pso and rp features,model 's explanatory power gained after pso and rp features,0.7043584585189819
translation,46,205,ablation-analysis,ablation analysis,has,significant improvement,ablation analysis has significant improvement,0.5377434492111206
translation,46,208,ablation-analysis,frequency domain features ( pso and rp ),of,sentence information density,frequency domain features ( pso and rp ) of sentence information density,0.5552334785461426
translation,46,208,ablation-analysis,sentence information density,capture,some hidden factors,sentence information density capture some hidden factors,0.7053530216217041
translation,46,208,ablation-analysis,some hidden factors,of,task success,some hidden factors of task success,0.5435057878494263
translation,46,208,ablation-analysis,some hidden factors,unexplained by,alignment approach,some hidden factors unexplained by alignment approach,0.7721595764160156
translation,46,208,ablation-analysis,ablation analysis,indicate,frequency domain features ( pso and rp ),ablation analysis indicate frequency domain features ( pso and rp ),0.581318736076355
translation,46,209,ablation-analysis,information contribution,matters a lot to,success of the collaboration,information contribution matters a lot to success of the collaboration,0.6351305842399597
translation,46,209,ablation-analysis,how people coordinate,has,information contribution,how people coordinate has information contribution,0.48537012934684753
translation,46,200,model,model,add,new pso and rp,model add new pso and rp,0.6940464377403259
translation,46,140,results,reliable predictor,in,both models,reliable predictor in both models,0.4848603308200836
translation,46,140,results,results,has,pso,results has pso,0.5363168716430664
translation,46,145,results,new model,uses,pso,new model uses pso,0.6248877048492432
translation,46,145,results,new model,yields,significant coefficients,new model yields significant coefficients,0.6332151889801025
translation,46,145,results,pso,to predict,box -cox transformed pathdev,pso to predict box -cox transformed pathdev,0.719629168510437
translation,46,145,results,significant coefficients,has,? = 3.85,significant coefficients has ? = 3.85,0.5236009359359741
translation,46,145,results,results,has,new model,results has new model,0.581872820854187
translation,46,173,results,non-significant coefficients,for,both models,non-significant coefficients for both models,0.5223883390426636
translation,46,173,results,non-significant coefficients,for,djd,non-significant coefficients for djd,0.6108343601226807
translation,46,173,results,non-significant coefficients,For,map task,non-significant coefficients For map task,0.5962837338447571
translation,46,173,results,non-significant coefficients,For,djd,non-significant coefficients For djd,0.6108343601226807
translation,46,173,results,results,get,non-significant coefficients,results get non-significant coefficients,0.5052970051765442
translation,46,206,results,best model,by adding,pso and rp,best model by adding pso and rp,0.6990584135055542
translation,46,206,results,best model,gives,about 60 % increase,best model gives about 60 % increase,0.6219413876533508
translation,46,206,results,pso and rp,as,predictors,pso and rp as predictors,0.6162950992584229
translation,46,206,results,predictors,without,interaction term,predictors without interaction term,0.6428589820861816
translation,46,206,results,about 60 % increase,of,r 2,about 60 % increase of r 2,0.6316137909889221
translation,46,206,results,r 2,compared to,r&m 's full model,r 2 compared to r&m 's full model,0.6552752256393433
translation,46,206,results,results,has,best model,results has best model,0.5634682774543762
translation,46,207,results,performance,exceeds,r&,performance exceeds r&,0.5947405099868774
translation,46,207,results,alignment features,has,performance,alignment features has performance,0.5708734393119812
translation,46,207,results,only ( length ) and the frequency features,has,performance,only ( length ) and the frequency features has performance,0.6093946099281311
translation,46,207,results,r&,has,m 's full model,r& has m 's full model,0.5512011647224426
translation,46,207,results,results,exclude,alignment features,results exclude alignment features,0.6409823894500732
translation,47,174,ablation-analysis,ablation analysis,observe,increased user utterance length,ablation analysis observe increased user utterance length,0.5874309539794922
translation,47,6,model,hindi-english human-machine dialogue system,elicits,code-switching conversations,hindi-english human-machine dialogue system elicits code-switching conversations,0.7014531493186951
translation,47,6,model,code-switching conversations,in,controlled setting,code-switching conversations in controlled setting,0.5539035201072693
translation,47,6,model,model,propose,hindi-english human-machine dialogue system,model propose hindi-english human-machine dialogue system,0.6209625005722046
translation,47,170,results,task success,is,significantly better,task success is significantly better,0.5598598718643188
translation,47,170,results,significantly better,when,agent,significantly better when agent,0.6720207333564758
translation,47,170,results,agent,uses,cs strategy ( 62 % ),agent uses cs strategy ( 62 % ),0.6031987071037292
translation,47,170,results,agent,compared to,agent 's monolingual strategies,agent compared to agent 's monolingual strategies,0.6107780933380127
translation,47,170,results,cs strategy ( 62 % ),compared to,agent 's monolingual strategies,cs strategy ( 62 % ) compared to agent 's monolingual strategies,0.6511380672454834
translation,47,170,results,agent 's monolingual strategies,has,48 % ),agent 's monolingual strategies has 48 % ),0.5684112310409546
translation,47,170,results,results,observe,task success,results observe task success,0.5675550699234009
translation,47,173,results,informality,improves,dialogue quality,informality improves dialogue quality,0.6628822088241577
translation,47,173,results,results,has,informality,results has informality,0.5582287311553955
translation,48,132,ablation-analysis,all features,contribute to,overall performance,all features contribute to overall performance,0.6407144069671631
translation,48,132,ablation-analysis,color and size,seem to have,largest impact,color and size seem to have largest impact,0.6030899286270142
translation,48,132,ablation-analysis,ablation analysis,see that,all features,ablation analysis see that all features,0.6411044001579285
translation,48,31,experimental-setup,https,:,//github.com,https : //github.com,0.6116153597831726
translation,48,31,experimental-setup,https,:,alab-nii,https : alab-nii,0.765264093875885
translation,48,31,experimental-setup,https,/,alab-nii,https / alab-nii,0.7322239875793457
translation,48,31,experimental-setup,//github.com,/,alab-nii,//github.com / alab-nii,0.6322490572929382
translation,48,31,experimental-setup,alab-nii,/,onecommon,alab-nii / onecommon,0.6783913969993591
translation,48,212,experimental-setup,https,:,github.com,https : github.com,0.6102899312973022
translation,48,212,experimental-setup,https,:,alab-nii,https : alab-nii,0.765264093875885
translation,48,212,experimental-setup,https,//,github.com,https // github.com,0.6462398171424866
translation,48,212,experimental-setup,https,/,alab-nii,https / alab-nii,0.7322239875793457
translation,48,212,experimental-setup,github.com,/,alab-nii,github.com / alab-nii,0.6189552545547485
translation,48,212,experimental-setup,alab-nii,has,/onecommon,alab-nii has /onecommon,0.6836469173431396
translation,48,7,model,linguistic structures,based on,spatial expressions,linguistic structures based on spatial expressions,0.5491610765457153
translation,48,7,model,linguistic structures,provide,comprehensive and reliable annotation,linguistic structures provide comprehensive and reliable annotation,0.5149776339530945
translation,48,7,model,comprehensive and reliable annotation,for,600 dialogues,comprehensive and reliable annotation for 600 dialogues,0.5862443447113037
translation,48,7,model,model,analyze,linguistic structures,model analyze linguistic structures,0.6285535097122192
translation,48,150,results,human annotation,passes,most of our tests,human annotation passes most of our tests,0.6728435158729553
translation,48,151,results,ref models,often make,invalid predictions,ref models often make invalid predictions,0.6681013703346252
translation,48,151,results,invalid predictions,with,overall poor performance,invalid predictions with overall poor performance,0.6073507070541382
translation,48,151,results,results,confirmed,ref models,results confirmed ref models,0.6925064921379089
translation,48,152,results,"direction , proximity and region categories",found that,numref model,"direction , proximity and region categories found that numref model",0.639036238193512
translation,48,152,results,numref model,performs,on par,numref model performs on par,0.6578822135925293
translation,48,152,results,numref model,performs,only marginally better,numref model performs only marginally better,0.6145463585853577
translation,48,152,results,only marginally better,than,ablated version,only marginally better than ablated version,0.5965985059738159
translation,48,152,results,underperforms,for,simple relations,underperforms for simple relations,0.6264508962631226
translation,48,152,results,results,In,"direction , proximity and region categories","results In direction , proximity and region categories",0.48296862840652466
translation,48,154,results,numref,performs,reasonably well,numref performs reasonably well,0.6645814180374146
translation,48,154,results,color / size comparison,has,numref,color / size comparison has numref,0.5594326853752136
translation,48,154,results,color / size comparison,has,outperforming,color / size comparison has outperforming,0.606072187423706
translation,48,154,results,outperforming,has,all other models,outperforming has all other models,0.5945013761520386
translation,48,154,results,results,In,color / size comparison,results In color / size comparison,0.5391451716423035
translation,48,162,results,subject / object properties,has,human performance,subject / object properties has human performance,0.502652645111084
translation,48,162,results,results,In terms of,subject / object properties,results In terms of subject / object properties,0.6910059452056885
translation,49,69,baselines,unreferenced ruber,as,lstm - based baselines,unreferenced ruber as lstm - based baselines,0.5158169269561768
translation,49,69,baselines,baselines,use,infersent,baselines use infersent,0.6395319104194641
translation,49,70,baselines,lstm encoder,replaced with,pre-trained bert encoder,lstm encoder replaced with pre-trained bert encoder,0.6817153692245483
translation,49,70,baselines,baselines,compare against,bert - nli,baselines compare against bert - nli,0.7282326221466064
translation,49,82,experiments,ruber and infersent baselines,are,weak,ruber and infersent baselines are weak,0.5727763175964355
translation,49,6,model,unreferenced automated evaluation metric,uses,large pre-trained language models,unreferenced automated evaluation metric uses large pre-trained language models,0.5624330639839172
translation,49,6,model,large pre-trained language models,to extract,latent representations,large pre-trained language models to extract latent representations,0.6428972482681274
translation,49,6,model,large pre-trained language models,leverages,temporal transitions,large pre-trained language models leverages temporal transitions,0.6920766830444336
translation,49,6,model,latent representations,of,utterances,latent representations of utterances,0.612036406993866
translation,49,6,model,model,propose,unreferenced automated evaluation metric,model propose unreferenced automated evaluation metric,0.6407714486122131
translation,49,19,model,completely unsupervised unreferenced metric maude,leverages,state - of - the - art pretrained language models,completely unsupervised unreferenced metric maude leverages state - of - the - art pretrained language models,0.6634801626205444
translation,49,19,model,completely unsupervised unreferenced metric maude,combined with,novel discoursestructure aware text encoder and contrastive training approach,completely unsupervised unreferenced metric maude combined with novel discoursestructure aware text encoder and contrastive training approach,0.682984471321106
translation,49,19,model,completely unsupervised unreferenced metric maude,good correlation with,human judgements,completely unsupervised unreferenced metric maude good correlation with human judgements,0.5926392078399658
translation,49,19,model,state - of - the - art pretrained language models,combined with,novel discoursestructure aware text encoder and contrastive training approach,state - of - the - art pretrained language models combined with novel discoursestructure aware text encoder and contrastive training approach,0.6882442235946655
translation,49,81,results,maude scores,perform,robustly,maude scores perform robustly,0.62685227394104
translation,49,81,results,robustly,across,all the setups,robustly across all the setups,0.717483401298523
translation,49,81,results,results,observe,maude scores,results observe maude scores,0.5428197383880615
translation,49,83,results,distilbert -nli baseline,performs,significantly better,distilbert -nli baseline performs significantly better,0.6039695143699646
translation,49,83,results,significantly better,than,infersent and ruber,significantly better than infersent and ruber,0.6024662852287292
translation,49,83,results,maude,scores,even better and more consistently,maude scores even better and more consistently,0.6777946352958679
translation,49,83,results,results,has,distilbert -nli baseline,results has distilbert -nli baseline,0.5532005429267883
translation,49,98,results,distilbert,proves to be,better,distilbert proves to be better,0.7520551085472107
translation,49,98,results,better,half of,quality measures,better half of quality measures,0.605011522769928
translation,49,98,results,calibrated human judgements,has,distilbert,calibrated human judgements has distilbert,0.6034169793128967
translation,49,98,results,results,in case of,calibrated human judgements,results in case of calibrated human judgements,0.6755947470664978
translation,49,99,results,maude,achieves,marginally better overall correlation,maude achieves marginally better overall correlation,0.6747291684150696
translation,49,99,results,marginally better overall correlation,for,calibrated human judgements,marginally better overall correlation for calibrated human judgements,0.6524360775947571
translation,49,99,results,results,has,maude,results has maude,0.5548445582389832
translation,49,102,results,large pre-trained language models,provides,significant boost,large pre-trained language models provides significant boost,0.5790867805480957
translation,49,102,results,significant boost,in,human correlation scores,significant boost in human correlation scores,0.5220813155174255
translation,49,102,results,results,using,large pre-trained language models,results using large pre-trained language models,0.6526750326156616
translation,50,121,ablation-analysis,architectures,of,entlib and entnet,architectures of entlib and entnet,0.606698215007782
translation,50,121,ablation-analysis,architectures,help with,lower frequency characters,architectures help with lower frequency characters,0.6435213685035706
translation,50,121,ablation-analysis,entlib and entnet,help with,lower frequency characters,entlib and entnet help with lower frequency characters,0.6799478530883789
translation,50,121,ablation-analysis,performance,on,main characters,performance on main characters,0.5412384867668152
translation,50,121,ablation-analysis,not hurting,has,performance,not hurting has performance,0.5992889404296875
translation,50,121,ablation-analysis,ablation analysis,has,architectures,ablation analysis has architectures,0.5268837213516235
translation,50,50,baselines,character identification,as,classification task,character identification as classification task,0.5193421244621277
translation,50,50,baselines,baselines,approach,character identification,baselines approach character identification,0.6536499261856079
translation,50,85,baselines,entnet ( dynamic memory ),adaptation of,recurrent entity networks,entnet ( dynamic memory ) adaptation of recurrent entity networks,0.6417128443717957
translation,50,85,baselines,recurrent entity networks,to,task,recurrent entity networks to task,0.5105313658714294
translation,50,85,baselines,baselines,has,entnet ( dynamic memory ),baselines has entnet ( dynamic memory ),0.5533792972564697
translation,50,218,experimental-setup,final models,trained in,batch mode,final models trained in batch mode,0.7846541404724121
translation,50,218,experimental-setup,batch mode,using,adam optimizer,batch mode using adam optimizer,0.7074432373046875
translation,50,218,experimental-setup,25 scenes,given to,model,25 scenes given to model,0.6840039491653442
translation,50,218,experimental-setup,model,in,chunks,model in chunks,0.5737672448158264
translation,50,218,experimental-setup,chunks,of,750 tokens,chunks of 750 tokens,0.603997528553009
translation,50,218,experimental-setup,experimental setup,has,final models,experimental setup has final models,0.547492504119873
translation,50,219,experimental-setup,token embeddings ( w t ),initialized with,300 - dimensional word2vec vectors,token embeddings ( w t ) initialized with 300 - dimensional word2vec vectors,0.7514325380325317
translation,50,219,experimental-setup,token embeddings ( w t ),initialized with,entity ( or speaker ) embeddings ( w e ),token embeddings ( w t ) initialized with entity ( or speaker ) embeddings ( w e ),0.7360576391220093
translation,50,219,experimental-setup,h i,set to,500 units,h i set to 500 units,0.6985509395599365
translation,50,219,experimental-setup,entity ( or speaker ) embeddings ( w e ),to,k = 150 units,entity ( or speaker ) embeddings ( w e ) to k = 150 units,0.5146574378013611
translation,50,219,experimental-setup,experimental setup,has,token embeddings ( w t ),experimental setup has token embeddings ( w t ),0.5462788343429565
translation,50,224,experimental-setup,another random search,over,remaining hyperparameters,another random search over remaining hyperparameters,0.6760846376419067
translation,50,224,experimental-setup,> 200 models ),over,remaining hyperparameters,> 200 models ) over remaining hyperparameters,0.6394910216331482
translation,50,224,experimental-setup,learning rate,sampled from,logarithmic interval,learning rate sampled from logarithmic interval,0.6977856159210205
translation,50,224,experimental-setup,before and after lstm,sampled from,0.0-0.3 and 0.0-0.1,before and after lstm sampled from 0.0-0.3 and 0.0-0.1,0.6504772901535034
translation,50,224,experimental-setup,weight decay,sampled from,10 ?6 - 10 ?2,weight decay sampled from 10 ?6 - 10 ?2,0.6784683465957642
translation,50,224,experimental-setup,another random search,has,> 200 models ),another random search has > 200 models ),0.6018032431602478
translation,50,224,experimental-setup,logarithmic interval,has,0.001 - 0.05,logarithmic interval has 0.001 - 0.05,0.5172570943832397
translation,50,224,experimental-setup,dropout,has,before and after lstm,dropout has before and after lstm,0.5755505561828613
translation,50,224,experimental-setup,experimental setup,performed,another random search,experimental setup performed another random search,0.23733209073543549
translation,50,225,experiments,best model,training on,all the training data,best model training on all the training data,0.7268159985542297
translation,50,225,experiments,each type,i.e.,bilstm,each type i.e. bilstm,0.6888712048530579
translation,50,225,experiments,each type,i.e.,"entlib , and entnet","each type i.e. entlib , and entnet",0.7477003931999207
translation,50,225,experiments,all the training data,without,cross-validation,all the training data without cross-validation,0.6912160515785217
translation,50,225,experiments,cross-validation,for,"20 , 80 and 80 epochs","cross-validation for 20 , 80 and 80 epochs",0.5952911972999573
translation,50,109,hyperparameters,entity ( speaker / referent ) embeddings,were,randomly initialized,entity ( speaker / referent ) embeddings were randomly initialized,0.5697622299194336
translation,50,109,hyperparameters,hyperparameters,has,entity ( speaker / referent ) embeddings,hyperparameters has entity ( speaker / referent ) embeddings,0.5328628420829773
translation,50,110,hyperparameters,models,with,backpropagation,models with backpropagation,0.6354422569274902
translation,50,110,hyperparameters,models,using,standard negative log-likelihood loss function,models using standard negative log-likelihood loss function,0.6104647517204285
translation,50,110,hyperparameters,backpropagation,using,standard negative log-likelihood loss function,backpropagation using standard negative log-likelihood loss function,0.5727970004081726
translation,50,110,hyperparameters,hyperparameters,train,models,hyperparameters train models,0.6666569709777832
translation,50,120,results,all models,perform,on a par,all models perform on a par,0.651273250579834
translation,50,120,results,on a par,on,main entities,on a par on main entities,0.6331577897071838
translation,50,120,results,outperform,by,substantial margin,outperform by substantial margin,0.6581037044525146
translation,50,120,results,bilstm,by,substantial margin,bilstm by substantial margin,0.6351897716522217
translation,50,120,results,bilstm,when,all characters,bilstm when all characters,0.6419302821159363
translation,50,120,results,substantial margin,when,all characters,substantial margin when all characters,0.6412440538406372
translation,50,120,results,all characters,are,to be predicted,all characters are to be predicted,0.6076105833053589
translation,50,120,results,entity -centric models,has,outperform,entity -centric models has outperform,0.5843340754508972
translation,50,120,results,outperform,has,bilstm,outperform has bilstm,0.5757801532745361
translation,50,120,results,results,has,all models,results has all models,0.5029959678649902
translation,50,197,results,models,get,accuracies,models get accuracies,0.5744661688804626
translation,50,197,results,accuracies,near,0,accuracies near 0,0.6786950826644897
translation,51,14,baselines,baselines,has,supervised learning ( sl ),baselines has supervised learning ( sl ),0.5616668462753296
translation,51,97,baselines,cas,adopts,single gru,cas adopts single gru,0.6877244114875793
translation,51,97,baselines,cas,uses,three different fully connected layers,cas uses three different fully connected layers,0.6362926363945007
translation,51,97,baselines,single gru,for,decoding,single gru for decoding,0.659207284450531
translation,51,97,baselines,three different fully connected layers,for mapping,output,three different fully connected layers for mapping output,0.7390220761299133
translation,51,97,baselines,output,of,gru,output of gru,0.6363301277160645
translation,51,97,baselines,output,to,act and slots,output to act and slots,0.6358931660652161
translation,51,97,baselines,single gru,has,),single gru has ),0.630660355091095
translation,51,97,baselines,baselines,has,cas,baselines has cas,0.6157743334770203
translation,51,102,experiments,seq2seq and copy seq2seq,use,beam search,seq2seq and copy seq2seq use beam search,0.6458131074905396
translation,51,102,experiments,beam search,with,beam size 10,beam search with beam size 10,0.7035357356071472
translation,51,102,experiments,beam size 10,during,inference,beam size 10 during inference,0.7071229815483093
translation,51,93,hyperparameters,classification,replicates,msr challenge,classification replicates msr challenge,0.6581611037254333
translation,51,93,hyperparameters,msr challenge,has,policy network architecture,msr challenge has policy network architecture,0.5609853863716125
translation,51,93,hyperparameters,policy network architecture,has,two fully connected layers,policy network architecture has two fully connected layers,0.5631738901138306
translation,51,93,hyperparameters,hyperparameters,has,classification,hyperparameters has classification,0.5643922686576843
translation,51,101,hyperparameters,two fully connected layers,of size,128,two fully connected layers of size 128,0.6469007134437561
translation,51,101,hyperparameters,hidden size,of,64,hidden size of 64,0.667656421661377
translation,51,101,hyperparameters,teacherforcing rate,of,0.5,teacherforcing rate of 0.5,0.6179816722869873
translation,51,101,hyperparameters,classification architecture,has,two fully connected layers,classification architecture has two fully connected layers,0.5059636831283569
translation,51,101,hyperparameters,hyperparameters,has,classification architecture,hyperparameters has classification architecture,0.5207616090774536
translation,51,101,hyperparameters,hyperparameters,has,remaining models,hyperparameters has remaining models,0.4957391917705536
translation,51,104,hyperparameters,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,51,104,hyperparameters,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,51,9,model,novel policy model,based on,recurrent cell,novel policy model based on recurrent cell,0.6611025929450989
translation,51,9,model,recurrent cell,called,gated continue - act- slots ( gcas ),recurrent cell called gated continue - act- slots ( gcas ),0.7229979038238525
translation,51,9,model,model,has,novel policy model,model has novel policy model,0.5567163825035095
translation,51,28,model,novel policy model,to output,multiple actions per turn,novel policy model to output multiple actions per turn,0.7315928339958191
translation,51,28,model,model,introduce,novel policy model,model introduce novel policy model,0.6083908677101135
translation,51,94,model,last activation,from,softmax,last activation from softmax,0.5120395421981812
translation,51,94,model,last activation,to predict,probabilities,last activation to predict probabilities,0.7468958497047424
translation,51,94,model,softmax,to,sigmoid,softmax to sigmoid,0.55387282371521
translation,51,94,model,sigmoid,to predict,probabilities,sigmoid to predict probabilities,0.7745511531829834
translation,51,94,model,probabilities,for,each act-slot pair,probabilities for each act-slot pair,0.6564537286758423
translation,51,94,model,model,replace,last activation,model replace last activation,0.6503570079803467
translation,51,96,model,seq2seq,encodes,dialogue state,seq2seq encodes dialogue state,0.7641582489013672
translation,51,96,model,dialogue state,as,sequence,dialogue state as sequence,0.5746245384216309
translation,51,96,model,sequence,with,attention,sequence with attention,0.6666933298110962
translation,51,96,model,sequence,with,attention,sequence with attention,0.6666933298110962
translation,51,96,model,copy seq2seq,adds,copy mechanism,copy seq2seq adds copy mechanism,0.6744163632392883
translation,51,96,model,copy mechanism,to,seq2seq,copy mechanism to seq2seq,0.5972139239311218
translation,51,96,model,seq2seq,allows,copying,seq2seq allows copying,0.7103541493415833
translation,51,96,model,words,from,encoder input,words from encoder input,0.5820156335830688
translation,51,96,model,copying,has,words,copying has words,0.6221392154693604
translation,51,96,model,model,decodes,copy seq2seq,model decodes copy seq2seq,0.7678120136260986
translation,51,96,model,model,has,seq2seq,model has seq2seq,0.5933537483215332
translation,51,100,model,gcas,uses,proposed recurrent cell,gcas uses proposed recurrent cell,0.6359224915504456
translation,51,100,model,proposed recurrent cell,contains,"separate continue , act and slots unit","proposed recurrent cell contains separate continue , act and slots unit",0.6478694081306458
translation,51,100,model,"separate continue , act and slots unit",are,sequentially connected,"separate continue , act and slots unit are sequentially connected",0.6456325650215149
translation,51,100,model,model,has,gcas,model has gcas,0.5745207667350769
translation,51,142,model,model,introduced,multi-act dialogue policy model,model introduced multi-act dialogue policy model,0.6209837794303894
translation,51,106,results,all other methods,on,entity f 1,all other methods on entity f 1,0.5535758137702942
translation,51,106,results,entity f 1,in,all three domains,entity f 1 in all three domains,0.495485782623291
translation,51,106,results,gcas,has,outperforms,gcas has outperforms,0.6508126854896545
translation,51,106,results,outperforms,has,all other methods,outperforms has all other methods,0.5550965666770935
translation,51,106,results,results,has,gcas,results has gcas,0.5117381811141968
translation,51,107,results,performance advantage,of,gcas,performance advantage of gcas,0.607755720615387
translation,51,107,results,gcas,in,taxi and restaurant domains,gcas in taxi and restaurant domains,0.5304362773895264
translation,51,107,results,gcas,is,small,gcas is small,0.6254124641418457
translation,51,107,results,seq2seq,has,performance advantage,seq2seq has performance advantage,0.5579268336296082
translation,51,107,results,results,Compared to,seq2seq,results Compared to seq2seq,0.6286125183105469
translation,51,109,results,gcas,outperformed by,classification model,gcas outperformed by classification model,0.7560621500015259
translation,51,109,results,all other models,in terms of,success f 1,all other models in terms of success f 1,0.714998185634613
translation,51,109,results,success f 1,in,movie and restaurant domain,success f 1 in movie and restaurant domain,0.4951862692832947
translation,51,109,results,classification model,in,taxi domain,classification model in taxi domain,0.5152360200881958
translation,51,109,results,gcas,has,outperforms,gcas has outperforms,0.6508126854896545
translation,51,109,results,outperforms,has,all other models,outperforms has all other models,0.5782700181007385
translation,51,109,results,results,has,gcas,results has gcas,0.5117381811141968
translation,51,114,results,all other models,in,acts prediction,all other models in acts prediction,0.5070105791091919
translation,51,114,results,acts prediction,in terms of,f 1 score,acts prediction in terms of f 1 score,0.6451792120933533
translation,51,114,results,cas and gcas,has,outperform,cas and gcas has outperform,0.6430161595344543
translation,51,114,results,outperform,has,all other models,outperform has all other models,0.5848680138587952
translation,51,114,results,results,has,cas and gcas,results has cas and gcas,0.5066879987716675
translation,51,117,results,frame level,has,gcas,frame level has gcas,0.6060760617256165
translation,51,117,results,outperforms,has,all other methods,outperforms has all other methods,0.5550965666770935
translation,51,117,results,results,At,frame level,results At frame level,0.5701984763145447
translation,51,131,results,gcas,performs,better,gcas performs better,0.6771761178970337
translation,51,131,results,better,than,other methods,better than other methods,0.5735819935798645
translation,51,131,results,other methods,on,all slots,other methods on all slots,0.5371030569076538
translation,51,131,results,all slots,in,movie and restaurant domains,all slots in movie and restaurant domains,0.5179201364517212
translation,51,131,results,results,has,gcas,results has gcas,0.5117381811141968
translation,51,135,results,gcas 's performance,close to,other methods,gcas 's performance close to other methods,0.6574705243110657
translation,51,135,results,other methods,on,critical-slots,other methods on critical-slots,0.5581639409065247
translation,51,135,results,results,has,gcas 's performance,results has gcas 's performance,0.5393293499946594
translation,51,139,results,gcas,better predict,non-critical slots,gcas better predict non-critical slots,0.6511433720588684
translation,51,139,results,non-critical slots,than,other methods,non-critical slots than other methods,0.5807628035545349
translation,51,139,results,results,has,gcas,results has gcas,0.5117381811141968
translation,52,107,baselines,baselines,has,baseline 1 : support vector machines,baselines has baseline 1 : support vector machines,0.5644685626029968
translation,52,116,baselines,baselines,has,baseline 2 : conditional random fields,baselines has baseline 2 : conditional random fields,0.5899893641471863
translation,52,117,baselines,temporal aspects,has,conditional random fields ( crfs ),temporal aspects has conditional random fields ( crfs ),0.5829208493232727
translation,52,121,baselines,cnn architecture,compared,two different models,cnn architecture compared two different models,0.6594835519790649
translation,52,121,baselines,first one,learned,word embeddings,first one learned word embeddings,0.6760420799255371
translation,52,121,baselines,word embeddings,from scratch,random parameters,word embeddings from scratch random parameters,0.7135683298110962
translation,52,121,baselines,word embeddings,with,random parameters,word embeddings with random parameters,0.5826239585876465
translation,52,121,baselines,other,initialized with,word2vec,other initialized with word2vec,0.6897207498550415
translation,52,115,hyperparameters,"svm light ( joachims , 1999 )",building,each binary classfier,"svm light ( joachims , 1999 ) building each binary classfier",0.6492781639099121
translation,52,115,hyperparameters,each binary classfier,with,linear kernel,each binary classfier with linear kernel,0.6153061985969543
translation,52,115,hyperparameters,hyperparameters,has,"svm light ( joachims , 1999 )","hyperparameters has svm light ( joachims , 1999 )",0.5063239932060242
translation,52,122,hyperparameters,dense vector,with,dimension,dense vector with dimension,0.683100700378418
translation,52,122,hyperparameters,dimension,of,k = 300,dimension of k = 300,0.650107204914093
translation,52,122,hyperparameters,k = 300,for,each word,k = 300 for each word,0.6019154191017151
translation,52,122,hyperparameters,each word,in,utterances,each word in utterances,0.5632169246673584
translation,52,128,hyperparameters,training,done with,stochastic gradient descent ( sgd ),training done with stochastic gradient descent ( sgd ),0.7218902707099915
translation,52,128,hyperparameters,stochastic gradient descent ( sgd ),by minimizing,categorical cross entropy loss,stochastic gradient descent ( sgd ) by minimizing categorical cross entropy loss,0.6653907895088196
translation,52,128,hyperparameters,categorical cross entropy loss,on,training set,categorical cross entropy loss on training set,0.5203418731689453
translation,52,128,hyperparameters,hyperparameters,has,training,hyperparameters has training,0.519983172416687
translation,52,132,hyperparameters,rcnn - based models,initialized with,pretrained word2vec model,rcnn - based models initialized with pretrained word2vec model,0.6929233074188232
translation,52,132,hyperparameters,pretrained word2vec model,in,training phase,pretrained word2vec model in training phase,0.45782771706581116
translation,52,132,hyperparameters,hyperparameters,has,rcnn - based models,hyperparameters has rcnn - based models,0.5160786509513855
translation,52,133,hyperparameters,dimension,of,hidden layers,dimension of hidden layers,0.5987681150436401
translation,52,133,hyperparameters,hidden layers,of,recurrent cells,hidden layers of recurrent cells,0.5665611624717712
translation,52,133,hyperparameters,recurrent cells,chosen to be,| s| = 500,recurrent cells chosen to be | s| = 500,0.6487723588943481
translation,52,133,hyperparameters,| s| = 500,based on,development set,| s| = 500 based on development set,0.6376920938491821
translation,52,133,hyperparameters,hyperparameters,has,dimension,hyperparameters has dimension,0.514893651008606
translation,52,5,model,conversational contexts,along,multiple turns,conversational contexts along multiple turns,0.6153596639633179
translation,52,28,model,transitions,between,segments,transitions between segments,0.7716162204742432
translation,52,28,model,segments,belonging to,same topic,segments belonging to same topic,0.7161390781402588
translation,52,113,model,third model,takes,concatenation,third model takes concatenation,0.7274633049964905
translation,52,113,model,concatenation,has,of both bag of n-grams and doc2vec features,concatenation has of both bag of n-grams and doc2vec features,0.5288421511650085
translation,52,113,model,model,has,third model,model has third model,0.5888231992721558
translation,52,140,results,sequential extensions,with,crf models,sequential extensions with crf models,0.6127500534057617
translation,52,140,results,significant improvements ( p < 0.05 ),from,svm models,significant improvements ( p < 0.05 ) from svm models,0.5454700589179993
translation,52,140,results,significant improvements ( p < 0.05 ),in,all the schedules,significant improvements ( p < 0.05 ) in all the schedules,0.5566612482070923
translation,52,140,results,svm models,in,all the schedules,svm models in all the schedules,0.5499092936515808
translation,52,140,results,two baseline families,has,sequential extensions,two baseline families has sequential extensions,0.5799899101257324
translation,52,140,results,results,Comparing between,two baseline families,results Comparing between two baseline families,0.6570748686790466
translation,52,141,results,doc2vec features,failed to achieve,comparable performances,doc2vec features failed to achieve comparable performances,0.6965028047561646
translation,52,141,results,comparable performances,to,simplest bag-of-ngrams features,comparable performances to simplest bag-of-ngrams features,0.5067944526672363
translation,52,141,results,svm and crf models,has,doc2vec features,svm and crf models has doc2vec features,0.540915310382843
translation,52,141,results,results,in,svm and crf models,results in svm and crf models,0.5009912252426147
translation,52,144,results,cnn,initialized with,pre-trained word2vec model,cnn initialized with pre-trained word2vec model,0.6881365180015564
translation,52,144,results,cnn,achieved,higher performances,cnn achieved higher performances,0.6877198219299316
translation,52,144,results,pre-trained word2vec model,achieved,higher performances,pre-trained word2vec model achieved higher performances,0.628555178642273
translation,52,144,results,higher performances,by,"8.38 % , 6.41 % , and 7.21 %","higher performances by 8.38 % , 6.41 % , and 7.21 %",0.5570199489593506
translation,52,144,results,higher performances,than,best baseline results,higher performances than best baseline results,0.5517188906669617
translation,52,144,results,"8.38 % , 6.41 % , and 7.21 %",in,f-measure,"8.38 % , 6.41 % , and 7.21 % in f-measure",0.5279948711395264
translation,52,144,results,"8.38 % , 6.41 % , and 7.21 %",under,each schedule,"8.38 % , 6.41 % , and 7.21 % under each schedule",0.5448643565177917
translation,52,144,results,f-measure,under,each schedule,f-measure under each schedule,0.6678287982940674
translation,52,144,results,results,has,cnn,results has cnn,0.5897790193557739
translation,52,149,results,some rnn models,showed,little performance gains,some rnn models showed little performance gains,0.6914690732955933
translation,52,149,results,little performance gains,over,svm baselines,little performance gains over svm baselines,0.6929043531417847
translation,52,149,results,svm baselines,with,doc2vec features,svm baselines with doc2vec features,0.5648890733718872
translation,52,149,results,even worse,than,crf model,even worse than crf model,0.5579619407653809
translation,52,149,results,crf model,with,same features,crf model with same features,0.6444795727729797
translation,52,149,results,results,has,some rnn models,results has some rnn models,0.5566405057907104
translation,52,150,results,cnns,to,rnns,cnns to rnns,0.5472735166549683
translation,52,150,results,rnns,contributed to,performance improvements,rnns contributed to performance improvements,0.48379018902778625
translation,52,150,results,performance improvements,from,baselines,performance improvements from baselines,0.5299918055534363
translation,52,150,results,performance improvements,from,cnn models,performance improvements from cnn models,0.48068997263908386
translation,52,150,results,results,has,rcnn models,results has rcnn models,0.5061226487159729
translation,52,151,results,uni-directional rnn,preferred in,rnn models,uni-directional rnn preferred in rnn models,0.6278985142707825
translation,52,151,results,rnn models,only with,doc2vec,rnn models only with doc2vec,0.6095136404037476
translation,52,151,results,bi-directional lstm,showed,better results,bi-directional lstm showed better results,0.6880220770835876
translation,52,151,results,better results,in,rcnn architectures,better results in rcnn architectures,0.5128729343414307
translation,52,151,results,results,has,uni-directional rnn,results has uni-directional rnn,0.5190011262893677
translation,52,152,results,bidirectional lrcn model,achieved,best performances,bidirectional lrcn model achieved best performances,0.6784200668334961
translation,52,152,results,best performances,against,all the others,best performances against all the others,0.6167134642601013
translation,52,152,results,best results,with,bi-directional rcnn,best results with bi-directional rcnn,0.6253969073295593
translation,52,152,results,results,has,bidirectional lrcn model,results has bidirectional lrcn model,0.5324589610099792
translation,52,153,results,segmentation performances,considering,beginning of each segment,segmentation performances considering beginning of each segment,0.7055859565734863
translation,52,153,results,beginning of each segment,predicted by,best model,beginning of each segment predicted by best model,0.7506653666496277
translation,52,153,results,best model,of,each architecture family,best model of each architecture family,0.5806206464767456
translation,52,154,results,proposed cnn and lrcn models,demonstrated,better capabilities,proposed cnn and lrcn models demonstrated better capabilities,0.6846950650215149
translation,52,154,results,better capabilities,of detecting,topic transitions,better capabilities of detecting topic transitions,0.7026776671409607
translation,52,154,results,topic transitions,in,intra-categorical and inter-categorical conditions,topic transitions in intra-categorical and inter-categorical conditions,0.5788050889968872
translation,52,154,results,topic transitions,than,baselines,topic transitions than baselines,0.5671534538269043
translation,52,154,results,results,has,proposed cnn and lrcn models,results has proposed cnn and lrcn models,0.5421440005302429
translation,52,164,results,enhanced capabilities,of,models,enhanced capabilities of models,0.628876268863678
translation,52,164,results,models,in distinguishing between,' o ' and other labels,models in distinguishing between ' o ' and other labels,0.770516574382782
translation,52,164,results,results,has,enhanced capabilities,results has enhanced capabilities,0.5844201445579529
translation,52,165,results,sequential architectures,in,crf and lrcn models,sequential architectures in crf and lrcn models,0.5099086761474609
translation,52,165,results,crf and lrcn models,showed,effectiveness,crf and lrcn models showed effectiveness,0.6492498517036438
translation,52,165,results,effectiveness,especially in,boundary detection,effectiveness especially in boundary detection,0.6134598255157471
translation,52,165,results,results,has,sequential architectures,results has sequential architectures,0.5699204206466675
translation,53,50,ablation-analysis,finetuned,on,meddialog -cn,finetuned on meddialog -cn,0.6395502090454102
translation,53,50,ablation-analysis,ablation analysis,has,bert - gpt and gpt,ablation analysis has bert - gpt and gpt,0.5766934752464294
translation,53,109,baselines,inefficiency,due to,sequential nature,inefficiency due to sequential nature,0.6964292526245117
translation,53,109,baselines,self-attention,to capture,long- range dependency,self-attention to capture long- range dependency,0.6793332695960999
translation,53,109,baselines,long- range dependency,among,tokens,long- range dependency among tokens,0.5974542498588562
translation,53,109,baselines,long- range dependency,by calculating,similarity,long- range dependency by calculating similarity,0.6649521589279175
translation,53,109,baselines,similarity,between,each pair of tokens,similarity between each pair of tokens,0.6501022577285767
translation,53,5,experiments,chinese dataset,with,3.4 million conversations,chinese dataset with 3.4 million conversations,0.5902332663536072
translation,53,5,experiments,chinese dataset,with,11.3 million utterances,chinese dataset with 11.3 million utterances,0.5926560163497925
translation,53,5,experiments,chinese dataset,with,660.2 million tokens,chinese dataset with 660.2 million tokens,0.5852618217468262
translation,53,5,experiments,chinese dataset,covering,172 specialties of diseases,chinese dataset covering 172 specialties of diseases,0.6998860239982605
translation,53,5,experiments,3.4 million conversations,between,patients and doctors,3.4 million conversations between patients and doctors,0.554537296295166
translation,53,5,experiments,english dataset,with,0.26 million conversations,english dataset with 0.26 million conversations,0.600440502166748
translation,53,5,experiments,english dataset,with,0.51 million utterances,english dataset with 0.51 million utterances,0.5993258953094482
translation,53,5,experiments,english dataset,with,44.53 million tokens,english dataset with 44.53 million tokens,0.5871008038520813
translation,53,5,experiments,0.26 million conversations,covering,96 specialties of diseases,0.26 million conversations covering 96 specialties of diseases,0.6867765784263611
translation,53,5,experiments,large-scale medical dialogue datasets,has,meddialog,large-scale medical dialogue datasets has meddialog,0.575282633304596
translation,53,53,experiments,models,trained on,large-scale meddialog - cn dataset,models trained on large-scale meddialog - cn dataset,0.7396078109741211
translation,53,53,experiments,large-scale meddialog - cn dataset,to improve,performance,large-scale meddialog - cn dataset to improve performance,0.6802424788475037
translation,53,53,experiments,performance,in,low-resource dialogue generation tasks,performance in low-resource dialogue generation tasks,0.521693766117096
translation,53,53,experiments,dataset size,is,small,dataset size is small,0.5494599938392639
translation,53,156,experiments,bert - gpt,pretrained on,chinese corpus,bert - gpt pretrained on chinese corpus,0.7225516438484192
translation,53,156,experiments,chinese corpus,collected from,large scale chinese corpus,chinese corpus collected from large scale chinese corpus,0.5844094753265381
translation,53,156,experiments,large scale chinese corpus,for,nlp,large scale chinese corpus for nlp,0.5752018094062805
translation,53,138,hyperparameters,hidden state size,is,768,hidden state size is 768,0.5968788266181946
translation,53,138,hyperparameters,hyperparameters,has,hidden state size,hyperparameters has hidden state size,0.5078210830688477
translation,53,139,hyperparameters,optimization,of,weight parameters,optimization of weight parameters,0.5507138967514038
translation,53,139,hyperparameters,optimization,performed using,stochastic gradient descent,optimization performed using stochastic gradient descent,0.5007056593894958
translation,53,139,hyperparameters,weight parameters,performed using,stochastic gradient descent,weight parameters performed using stochastic gradient descent,0.569304883480072
translation,53,139,hyperparameters,stochastic gradient descent,with,learning rate,stochastic gradient descent with learning rate,0.6182537078857422
translation,53,139,hyperparameters,learning rate,of,1e - 4,learning rate of 1e - 4,0.6311931014060974
translation,53,139,hyperparameters,hyperparameters,has,optimization,hyperparameters has optimization,0.5086923241615295
translation,53,140,hyperparameters,maximum length,of,input sequences,maximum length of input sequences,0.5892125368118286
translation,53,140,hyperparameters,maximum length,of,output sequences,maximum length of output sequences,0.6018902063369751
translation,53,140,hyperparameters,input sequences,truncated to,400,input sequences truncated to 400,0.6483420729637146
translation,53,140,hyperparameters,output sequences,truncated to,100,output sequences truncated to 100,0.6533721685409546
translation,53,140,hyperparameters,hyperparameters,has,maximum length,hyperparameters has maximum length,0.5061417818069458
translation,53,141,hyperparameters,"dialogpt -small ( zhang et al. , 2019 ) architecture",with,10 layers,"dialogpt -small ( zhang et al. , 2019 ) architecture with 10 layers",0.6150013208389282
translation,53,141,hyperparameters,gpt,has,"dialogpt -small ( zhang et al. , 2019 ) architecture","gpt has dialogpt -small ( zhang et al. , 2019 ) architecture",0.5940564870834351
translation,53,141,hyperparameters,hyperparameters,For,gpt,hyperparameters For gpt,0.5929980278015137
translation,53,142,hyperparameters,embedding size,to,768,embedding size to 768,0.5867283344268799
translation,53,142,hyperparameters,context size,to,300,context size to 300,0.6043838858604431
translation,53,142,hyperparameters,hyperparameters,set,embedding size,hyperparameters set embedding size,0.6328905820846558
translation,53,142,hyperparameters,hyperparameters,set,context size,hyperparameters set context size,0.611404299736023
translation,53,143,hyperparameters,epsilon hyperparameter,set to,1e - 5,epsilon hyperparameter set to 1e - 5,0.7273823022842407
translation,53,143,hyperparameters,layer normalization,has,epsilon hyperparameter,layer normalization has epsilon hyperparameter,0.4895496964454651
translation,53,143,hyperparameters,hyperparameters,In,layer normalization,hyperparameters In layer normalization,0.44364166259765625
translation,53,144,hyperparameters,multi-head self-attention,set,number of heads,multi-head self-attention set number of heads,0.6424075365066528
translation,53,144,hyperparameters,number of heads,to,12,number of heads to 12,0.5892803072929382
translation,53,144,hyperparameters,hyperparameters,In,multi-head self-attention,hyperparameters In multi-head self-attention,0.4879608750343323
translation,53,145,hyperparameters,weight parameters,learned with,"adam ( kingma and ba , 2014 )","weight parameters learned with adam ( kingma and ba , 2014 )",0.6871588826179504
translation,53,145,hyperparameters,hyperparameters,has,weight parameters,hyperparameters has weight parameters,0.4623759686946869
translation,53,146,hyperparameters,initial learning rate,set to,1.5e - 4,initial learning rate set to 1.5e - 4,0.6940512657165527
translation,53,146,hyperparameters,batch size,set to,32,batch size set to 32,0.733751654624939
translation,53,146,hyperparameters,hyperparameters,has,initial learning rate,hyperparameters has initial learning rate,0.4469831883907318
translation,53,146,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,53,147,hyperparameters,learning rate scheduler,set to,noam,learning rate scheduler set to noam,0.7101799249649048
translation,53,147,hyperparameters,noam,with,2000 warm - up steps,noam with 2000 warm - up steps,0.6841863989830017
translation,53,147,hyperparameters,hyperparameters,has,learning rate scheduler,hyperparameters has learning rate scheduler,0.49544891715049744
translation,53,148,hyperparameters,top -k random sampling,with,k = 50,top -k random sampling with k = 50,0.6316879987716675
translation,53,148,hyperparameters,k = 50,used for,decoding,k = 50 used for decoding,0.6962559223175049
translation,53,148,hyperparameters,hyperparameters,has,top -k random sampling,hyperparameters has top -k random sampling,0.5233637690544128
translation,53,137,model,bert encoder and gpt decoder,Transformers with,12 layers,bert encoder and gpt decoder Transformers with 12 layers,0.7378143668174744
translation,53,137,model,bert - gpt,has,bert encoder and gpt decoder,bert - gpt has bert encoder and gpt decoder,0.6128627061843872
translation,53,137,model,model,In,bert - gpt,model In bert - gpt,0.5966227054595947
translation,53,163,results,bert - gpt,achieves,lower perplexity,bert - gpt achieves lower perplexity,0.6671064496040344
translation,53,163,results,lower perplexity,than,transformer,lower perplexity than transformer,0.6348474025726318
translation,53,163,results,results,has,bert - gpt,results has bert - gpt,0.5599484443664551
translation,53,165,results,pretraining,enables,model,pretraining enables model,0.7202558517456055
translation,53,165,results,model,to better capture,linguistic structure,model to better capture linguistic structure,0.7119407653808594
translation,53,165,results,linguistic structure,among,words,linguistic structure among words,0.5906559824943542
translation,53,165,results,linguistic structure,yields,lower perplexity,linguistic structure yields lower perplexity,0.6944714188575745
translation,53,165,results,results,has,pretraining,results has pretraining,0.5475599765777588
translation,53,166,results,machine translation metrics,including,nist -4,machine translation metrics including nist -4,0.6560301780700684
translation,53,166,results,machine translation metrics,including,bleu -2,machine translation metrics including bleu -2,0.5518957376480103
translation,53,166,results,machine translation metrics,including,bleu -4,machine translation metrics including bleu -4,0.5691556334495544
translation,53,166,results,machine translation metrics,including,meteor,machine translation metrics including meteor,0.5972927212715149
translation,53,166,results,bert - gpt,performs,worse,bert - gpt performs worse,0.7103366851806641
translation,53,166,results,worse,than,transformer,worse than transformer,0.6682938933372498
translation,53,166,results,machine translation metrics,has,bert - gpt,machine translation metrics has bert - gpt,0.5889374613761902
translation,53,166,results,results,on,machine translation metrics,results on machine translation metrics,0.4589499235153198
translation,53,171,results,bert - gpt and transformer,are,on par,bert - gpt and transformer are on par,0.6408230066299438
translation,53,171,results,on par,indicates,similar capability,on par indicates similar capability,0.7153833508491516
translation,53,171,results,similar capability,in generating,diverse responses,similar capability in generating diverse responses,0.6548528671264648
translation,53,171,results,diversity metrics,has,bert - gpt and transformer,diversity metrics has bert - gpt and transformer,0.6149382591247559
translation,53,171,results,results,on,diversity metrics,results on diversity metrics,0.49030670523643494
translation,53,172,results,bert - gpt,has,gpt,bert - gpt has gpt,0.667747974395752
translation,53,172,results,gpt,has,worse perplexity,gpt has worse perplexity,0.5800670981407166
translation,53,172,results,better,has,machine translation scores,better has machine translation scores,0.5406872034072876
translation,53,172,results,results,compared with,bert - gpt,results compared with bert - gpt,0.6746881008148193
translation,53,207,results,"transformer , gpt , and bert - gpt",using,pretraining ( pt ),"transformer , gpt , and bert - gpt using pretraining ( pt )",0.6724634766578674
translation,53,207,results,pretraining ( pt ),on,meddialog - cn,pretraining ( pt ) on meddialog - cn,0.5797920823097229
translation,53,207,results,pretraining ( pt ),achieves,significantly better performance,pretraining ( pt ) achieves significantly better performance,0.6629353761672974
translation,53,207,results,significantly better performance,than,not using pretraining ( no - pt ),significantly better performance than not using pretraining ( no - pt ),0.5767582654953003
translation,53,207,results,results,For,"transformer , gpt , and bert - gpt","results For transformer , gpt , and bert - gpt",0.5988190770149231
translation,54,114,ablation-analysis,role-level attention,has,all types of time - decay functions,role-level attention has all types of time - decay functions,0.5398043394088745
translation,54,114,ablation-analysis,all types of time - decay functions,has,significantly improve,all types of time - decay functions has significantly improve,0.5584235787391663
translation,54,114,ablation-analysis,significantly improve,has,results,significantly improve has results,0.5303159356117249
translation,54,114,ablation-analysis,ablation analysis,For,role-level attention,ablation analysis For role-level attention,0.5750130414962769
translation,54,92,hyperparameters,mini-batch adam,as,optimizer,mini-batch adam as optimizer,0.5341653227806091
translation,54,92,hyperparameters,optimizer,with,batch size,optimizer with batch size,0.6249857544898987
translation,54,92,hyperparameters,batch size,of,256 examples,batch size of 256 examples,0.5637235641479492
translation,54,92,hyperparameters,hyperparameters,choose,mini-batch adam,hyperparameters choose mini-batch adam,0.6439456939697266
translation,54,93,hyperparameters,size,of,each hidden recurrent layer,size of each hidden recurrent layer,0.6184373497962952
translation,54,93,hyperparameters,each hidden recurrent layer,is,128,each hidden recurrent layer is 128,0.5646968483924866
translation,54,93,hyperparameters,hyperparameters,has,size,hyperparameters has size,0.5313372611999512
translation,54,94,hyperparameters,hyperparameters,use,pre-trained 200 - dimensional word embeddings glov e,hyperparameters use pre-trained 200 - dimensional word embeddings glov e,0.55142742395401
translation,54,95,hyperparameters,30 training epochs,without,early stop approach,30 training epochs without early stop approach,0.7279860973358154
translation,54,95,hyperparameters,hyperparameters,apply,30 training epochs,hyperparameters apply 30 training epochs,0.5551602840423584
translation,54,9,model,various types of time - decay attention,on,sentence - level and speaker - level,various types of time - decay attention on sentence - level and speaker - level,0.5427993535995483
translation,54,9,model,various types of time - decay attention,proposes,flexible universal time - decay attention mechanism,various types of time - decay attention proposes flexible universal time - decay attention mechanism,0.6107096672058105
translation,54,9,model,model,designs and investigates,various types of time - decay attention,model designs and investigates various types of time - decay attention,0.6015440225601196
translation,54,9,model,model,proposes,flexible universal time - decay attention mechanism,model proposes flexible universal time - decay attention mechanism,0.6111415028572083
translation,54,23,model,- aware attention mechanism,in,neural models,- aware attention mechanism in neural models,0.5353645086288452
translation,54,23,model,- aware attention mechanism,in,speaker role modeling,- aware attention mechanism in speaker role modeling,0.4952382445335388
translation,54,45,model,time - aware attention,based on,investigation of different time - decay functions,time - aware attention based on investigation of different time - decay functions,0.5974181890487671
translation,54,45,model,expanding,has,time - aware attention,expanding has time - aware attention,0.528126060962677
translation,54,45,model,model,focuses on,expanding,model focuses on expanding,0.7827257513999939
translation,54,45,model,model,focuses on,time - aware attention,model focuses on time - aware attention,0.6928032040596008
translation,54,26,results,state- ofthe - art understanding performance,in,dialogue benchmark dstc dataset,state- ofthe - art understanding performance in dialogue benchmark dstc dataset,0.4765506982803345
translation,54,105,results,almost all time - aware results,are,better,almost all time - aware results are better,0.5492621064186096
translation,54,105,results,better,than,three baselines,better than three baselines,0.6200436353683472
translation,54,108,results,three types,of,sentence - level timedecay attention,three types of sentence - level timedecay attention,0.5555613040924072
translation,54,108,results,three types,only,convex time - decay attention,three types only convex time - decay attention,0.6477577090263367
translation,54,108,results,sentence - level timedecay attention,has,convex time - decay attention,sentence - level timedecay attention has convex time - decay attention,0.5440110564231873
translation,54,108,results,convex time - decay attention,has,significantly outperforms,convex time - decay attention has significantly outperforms,0.601530909538269
translation,54,108,results,significantly outperforms,has,baselines,significantly outperforms has baselines,0.6014122366905212
translation,54,108,results,results,among,three types,results among three types,0.5726131200790405
translation,54,109,results,convex functions,perform,best,convex functions perform best,0.6297725439071655
translation,54,109,results,best,among,three types of time - decay functions,best among three types of time - decay functions,0.6031139492988586
translation,54,110,results,end-toend trainable setting,results in,better performance,end-toend trainable setting results in better performance,0.6144845485687256
translation,54,110,results,better performance,for,most cases,better performance for most cases,0.6030444502830505
translation,54,110,results,results,has,end-toend trainable setting,results has end-toend trainable setting,0.5377108454704285
translation,54,113,results,end-to - end trainable universal time - decay attention,achieves,best performance,end-to - end trainable universal time - decay attention achieves best performance,0.6578763723373413
translation,54,113,results,best performance,where,flexible time - aware attention,best performance where flexible time - aware attention,0.5920680165290833
translation,54,113,results,flexible time - aware attention,obtains,2.9 % relative improvement,flexible time - aware attention obtains 2.9 % relative improvement,0.558224618434906
translation,54,113,results,2.9 % relative improvement,compared to,model,2.9 % relative improvement compared to model,0.6875739097595215
translation,54,113,results,2.9 % relative improvement,compared to,model,2.9 % relative improvement compared to model,0.6875739097595215
translation,54,113,results,model,without,attention mechanism,model without attention mechanism,0.7010951638221741
translation,54,113,results,model,using,content - aware attention only,model using content - aware attention only,0.7147234678268433
translation,54,113,results,model,using,content - aware attention only,model using content - aware attention only,0.7147234678268433
translation,54,113,results,sentence - level attention,has,end-to - end trainable universal time - decay attention,sentence - level attention has end-to - end trainable universal time - decay attention,0.5370190143585205
translation,54,113,results,results,For,sentence - level attention,results For sentence - level attention,0.5029299259185791
translation,54,117,results,best results,from,end-to - end trainable universal time - decay function,best results from end-to - end trainable universal time - decay function,0.5330930948257446
translation,54,117,results,results,has,best results,results has best results,0.542218804359436
translation,54,123,results,content- aware attention,focus on,important contexts,content- aware attention focus on important contexts,0.7058551907539368
translation,54,123,results,content- aware attention,even performs,slightly worse,content- aware attention even performs slightly worse,0.7191765308380127
translation,54,123,results,important contexts,for improving,understanding performance,important contexts for improving understanding performance,0.7246960401535034
translation,54,123,results,understanding performance,in,complex dialogues,understanding performance in complex dialogues,0.5316158533096313
translation,54,123,results,slightly worse,than,contextual model without attention,slightly worse than contextual model without attention,0.5748056173324585
translation,54,123,results,results,has,content- aware attention,results has content- aware attention,0.5065293908119202
translation,54,127,results,our universal time - decay attention,keeps,improvement,our universal time - decay attention keeps improvement,0.6850810050964355
translation,54,127,results,without too much performance drop,by involving,learned temporal attention,without too much performance drop by involving learned temporal attention,0.6418609619140625
translation,54,127,results,improvement,has,without too much performance drop,improvement has without too much performance drop,0.6031339764595032
translation,54,127,results,results,between,time - aware only,results between time - aware only,0.6491760611534119
translation,54,127,results,results,find that,our universal time - decay attention,results find that our universal time - decay attention,0.6509838700294495
translation,54,130,results,role-level attention,shows that,all results,role-level attention shows that all results,0.5923402905464172
translation,54,130,results,all results,with,various time - decay attention mecha- nisms,all results with various time - decay attention mecha- nisms,0.636538028717041
translation,54,130,results,various time - decay attention mecha- nisms,are,better,various time - decay attention mecha- nisms are better,0.6051496863365173
translation,54,130,results,better,than,one with only contentaware attention,better than one with only contentaware attention,0.5660495162010193
translation,54,130,results,results,For,role-level attention,results For role-level attention,0.5129014253616333
translation,54,137,results,proposed universal time - decay attention,achieve,similar performance,proposed universal time - decay attention achieve similar performance,0.614533543586731
translation,54,137,results,similar performance,for,sentence - level and role-level attention,similar performance for sentence - level and role-level attention,0.5593770146369934
translation,54,137,results,sentence - level and role-level attention,has,76.67 % and 76.75 %,sentence - level and role-level attention has 76.67 % and 76.75 %,0.5458576679229736
translation,54,142,results,the content - aware models,become,slightly worse,the content - aware models become slightly worse,0.5800129771232605
translation,54,142,results,slightly worse,with,increasing context lengths,slightly worse with increasing context lengths,0.6405398845672607
translation,54,142,results,models,has,without attention,models has without attention,0.6145606637001038
translation,54,142,results,results,has,models,results has models,0.5335168838500977
translation,54,153,results,performance,on,sentence and role levels,performance on sentence and role levels,0.5368073582649231
translation,54,153,results,proposed timedecay attention mechanisms,has,significantly improve,proposed timedecay attention mechanisms has significantly improve,0.5648601055145264
translation,54,153,results,significantly improve,has,performance,significantly improve has performance,0.5721634030342102
translation,55,196,ablation-analysis,eye-gaze information,from,agents ' firstperson perspective,eye-gaze information from agents ' firstperson perspective,0.5321648120880127
translation,55,196,ablation-analysis,eye-gaze information,is,effective,eye-gaze information is effective,0.5512553453445435
translation,55,196,ablation-analysis,effective,in understanding,human intentions,effective in understanding human intentions,0.7274947762489319
translation,55,196,ablation-analysis,ablation analysis,indicate,eye-gaze information,ablation analysis indicate eye-gaze information,0.5791458487510681
translation,55,158,baselines,baselines,has,baseline models,baselines has baseline models,0.5690722465515137
translation,55,175,baselines,two neural network models,for,image encoding,two neural network models for image encoding,0.5955086350440979
translation,55,175,baselines,baselines,employ,two neural network models,baselines employ two neural network models,0.5557221174240112
translation,55,176,experimental-setup,16 - layer vggnet,replace,last linear layer,16 - layer vggnet replace last linear layer,0.5836442112922668
translation,55,176,experimental-setup,last linear layer,named,fc6,last linear layer named fc6,0.5912413597106934
translation,55,176,experimental-setup,last linear layer,with,learnable linear layer,last linear layer with learnable linear layer,0.6225642561912537
translation,55,176,experimental-setup,learnable linear layer,whose,output dimensionality,learnable linear layer whose output dimensionality,0.5932339429855347
translation,55,176,experimental-setup,output dimensionality,is,4096,output dimensionality is 4096,0.6118907332420349
translation,55,176,experimental-setup,experimental setup,use,16 - layer vggnet,experimental setup use 16 - layer vggnet,0.5927720665931702
translation,55,176,experimental-setup,experimental setup,replace,last linear layer,experimental setup replace last linear layer,0.5577258467674255
translation,55,177,experimental-setup,4096dimensional vector,as,image features,4096dimensional vector as image features,0.5636124014854431
translation,55,177,experimental-setup,experimental setup,use,4096dimensional vector,experimental setup use 4096dimensional vector,0.6241271495819092
translation,55,178,experimental-setup,experimental setup,use,50 - layer resnet,experimental setup use 50 - layer resnet,0.5604386925697327
translation,55,181,experimental-setup,"adam ( kingma and ba , 2015 ) optimizer",for,training,"adam ( kingma and ba , 2015 ) optimizer for training",0.5868462920188904
translation,55,181,experimental-setup,experimental setup,used,"adam ( kingma and ba , 2015 ) optimizer","experimental setup used adam ( kingma and ba , 2015 ) optimizer",0.6132118105888367
translation,55,182,experimental-setup,learning rate,fixed at,0.0001,learning rate fixed at 0.0001,0.679526686668396
translation,55,182,experimental-setup,mini-batch size,fixed at,64,mini-batch size fixed at 64,0.7268431782722473
translation,55,182,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,55,182,experimental-setup,experimental setup,has,mini-batch size,experimental setup has mini-batch size,0.5507700443267822
translation,55,185,experimental-setup,lstm - based text encoding,used,"mecab ( kudo et al. , 2004 )","lstm - based text encoding used mecab ( kudo et al. , 2004 )",0.525746762752533
translation,55,185,experimental-setup,lstm - based text encoding,used,"fasttext ( bojanowski et al. , 2017 )","lstm - based text encoding used fasttext ( bojanowski et al. , 2017 )",0.5124290585517883
translation,55,185,experimental-setup,lstm - based text encoding,used,"fasttext ( bojanowski et al. , 2017 )","lstm - based text encoding used fasttext ( bojanowski et al. , 2017 )",0.5124290585517883
translation,55,185,experimental-setup,"mecab ( kudo et al. , 2004 )",for,tokenization,"mecab ( kudo et al. , 2004 ) for tokenization",0.5987749099731445
translation,55,185,experimental-setup,"fasttext ( bojanowski et al. , 2017 )",for,word embeddings,"fasttext ( bojanowski et al. , 2017 ) for word embeddings",0.5363682508468628
translation,55,185,experimental-setup,"fasttext ( bojanowski et al. , 2017 )",pretrained on,japanese common - crawl and wikipedia articles,"fasttext ( bojanowski et al. , 2017 ) pretrained on japanese common - crawl and wikipedia articles",0.7206687331199646
translation,55,185,experimental-setup,word embeddings,pretrained on,japanese common - crawl and wikipedia articles,word embeddings pretrained on japanese common - crawl and wikipedia articles,0.7486981749534607
translation,55,185,experimental-setup,experimental setup,For,lstm - based text encoding,experimental setup For lstm - based text encoding,0.57712322473526
translation,55,186,experimental-setup,word-embedding and lstm dimensions,set to,300 and 100,word-embedding and lstm dimensions set to 300 and 100,0.6980720162391663
translation,55,186,experimental-setup,experimental setup,has,word-embedding and lstm dimensions,experimental setup has word-embedding and lstm dimensions,0.4948182702064514
translation,55,187,experimental-setup,bert - based encoding,used,bert model,bert - based encoding used bert model,0.558203399181366
translation,55,187,experimental-setup,bert model,named,bert- base-japanese- whole-word- masking,bert model named bert- base-japanese- whole-word- masking,0.6979689598083496
translation,55,187,experimental-setup,bert model,pre-trained on,japanese wikipedia,bert model pre-trained on japanese wikipedia,0.7393444776535034
translation,55,187,experimental-setup,bert- base-japanese- whole-word- masking,from,"hugging face 's ( wolf et al. , 2019 ) library","bert- base-japanese- whole-word- masking from hugging face 's ( wolf et al. , 2019 ) library",0.5546301007270813
translation,55,187,experimental-setup,"hugging face 's ( wolf et al. , 2019 ) library",pre-trained on,japanese wikipedia,"hugging face 's ( wolf et al. , 2019 ) library pre-trained on japanese wikipedia",0.7678175568580627
translation,55,187,experimental-setup,japanese wikipedia,using,whole -word - masking,japanese wikipedia using whole -word - masking,0.6426349878311157
translation,55,187,experimental-setup,experimental setup,For,bert - based encoding,experimental setup For bert - based encoding,0.5952466130256653
translation,55,188,experimental-setup,data augmentation,applied,random cropping,data augmentation applied random cropping,0.726834774017334
translation,55,188,experimental-setup,data augmentation,applied,random horizontal flipping,data augmentation applied random horizontal flipping,0.7294951677322388
translation,55,188,experimental-setup,data augmentation,applied,normalization transformations,data augmentation applied normalization transformations,0.7122849822044373
translation,55,188,experimental-setup,normalization transformations,to,original images,normalization transformations to original images,0.529198408126831
translation,55,188,experimental-setup,original images,during,training,original images during training,0.7014447450637817
translation,55,188,experimental-setup,experimental setup,For,data augmentation,experimental setup For data augmentation,0.5511353015899658
translation,55,184,experiments,training,converged,approximately 3 days,training converged approximately 3 days,0.6466204524040222
translation,55,184,experiments,training,converged,1 day,training converged 1 day,0.7362242341041565
translation,55,184,experiments,approximately 3 days,for,verbal response selection task,approximately 3 days for verbal response selection task,0.5297775268554688
translation,55,184,experiments,1 day,for,non-verbal response selection task,1 day for non-verbal response selection task,0.5168823003768921
translation,55,184,experiments,1 day,on,nvidia geforce gtx 1080 gpu,1 day on nvidia geforce gtx 1080 gpu,0.5377048850059509
translation,55,179,model,last fully connected layer,as,image features,last fully connected layer as image features,0.4794057309627533
translation,55,179,model,model,use,last fully connected layer,model use last fully connected layer,0.6187424659729004
translation,55,194,results,first-person images,improve,verbal and non-verbal response-selection performance,first-person images improve verbal and non-verbal response-selection performance,0.5654716491699219
translation,55,194,results,verbal and non-verbal response-selection performance,by,up to 5.6 points,verbal and non-verbal response-selection performance by up to 5.6 points,0.5654345750808716
translation,55,194,results,almost all encoder combinations,has,first-person images,almost all encoder combinations has first-person images,0.5767143368721008
translation,55,194,results,results,For,almost all encoder combinations,results For almost all encoder combinations,0.6227593421936035
translation,55,195,results,eye-gaze locations,improve,performance further,eye-gaze locations improve performance further,0.6682819128036499
translation,55,195,results,performance further,by,up to 1.4 points,performance further by up to 1.4 points,0.6388099789619446
translation,55,195,results,bert,has,eye-gaze locations,bert has eye-gaze locations,0.615208089351654
translation,55,195,results,results,when using,bert,results when using bert,0.5464324951171875
translation,55,199,results,vggnet,achieves,higher scores,vggnet achieves higher scores,0.6852557063102722
translation,55,199,results,vggnet,using,all the input modalities,vggnet using all the input modalities,0.6596136093139648
translation,55,199,results,higher scores,than,resnet,higher scores than resnet,0.6030859351158142
translation,55,199,results,all the input modalities,achieves,highest r 10 @1 score,all the input modalities achieves highest r 10 @1 score,0.6520693302154541
translation,55,199,results,highest r 10 @1 score,of,53.6 %,highest r 10 @1 score of 53.6 %,0.5319291353225708
translation,55,200,results,best r 10 @1 score,for,nonverbal response selection,best r 10 @1 score for nonverbal response selection,0.5900932550430298
translation,55,200,results,best r 10 @1 score,is,about 7 points worse,best r 10 @1 score is about 7 points worse,0.5641438364982605
translation,55,200,results,nonverbal response selection,is,about 7 points worse,nonverbal response selection is about 7 points worse,0.5602667331695557
translation,55,200,results,about 7 points worse,than,score,about 7 points worse than score,0.5555524826049805
translation,55,200,results,score,for,verbal - response selection,score for verbal - response selection,0.5982795357704163
translation,55,200,results,results,has,best r 10 @1 score,results has best r 10 @1 score,0.5816450715065002
translation,56,199,ablation-analysis,all three sentence - level features,improved,model 's f1,all three sentence - level features improved model 's f1,0.6590980291366577
translation,56,199,ablation-analysis,ablation analysis,has,all three sentence - level features,ablation analysis has all three sentence - level features,0.5419300198554993
translation,56,200,ablation-analysis,sentiment feature,increased,model 's f1 score,sentiment feature increased model 's f1 score,0.6570156812667847
translation,56,219,ablation-analysis,donation information,showed,significant positive effect,donation information showed significant positive effect,0.6678908467292786
translation,56,219,ablation-analysis,significant positive effect,on,donation outcome ( p < 0.05 ),significant positive effect on donation outcome ( p < 0.05 ),0.5127527117729187
translation,56,219,ablation-analysis,ablation analysis,has,persuasion strategies,ablation analysis has persuasion strategies,0.5465862154960632
translation,56,228,ablation-analysis,big-five personality analysis,shows that,more agreeable participants,big-five personality analysis shows that more agreeable participants,0.6346352696418762
translation,56,228,ablation-analysis,more likely,to,donate,more likely to donate,0.6045681834220886
translation,56,228,ablation-analysis,moral foundation analysis,shows,participants who care for others,moral foundation analysis shows participants who care for others,0.6145489811897278
translation,56,228,ablation-analysis,participants who care for others,have,higher probability for donation,participants who care for others have higher probability for donation,0.4954462945461273
translation,56,228,ablation-analysis,portrait value analysis,shows,participants who endorse benevolence more are,portrait value analysis shows participants who endorse benevolence more are,0.5864520072937012
translation,56,228,ablation-analysis,participants who care for others,has,more,participants who care for others has more,0.538142204284668
translation,56,228,ablation-analysis,ablation analysis,has,big-five personality analysis,ablation analysis has big-five personality analysis,0.5086799263954163
translation,56,246,ablation-analysis,donation probability,of,people,donation probability of people,0.5957624316215515
translation,56,246,ablation-analysis,donation probability,of,people who endorse fairness and authority,donation probability of people who endorse fairness and authority,0.5495059490203857
translation,56,246,ablation-analysis,donation probability,of,people who endorse fairness and authority,donation probability of people who endorse fairness and authority,0.5495059490203857
translation,56,246,ablation-analysis,negatively,associated with,donation probability,negatively associated with donation probability,0.7129636406898499
translation,56,246,ablation-analysis,donation probability,of,people who endorse fairness and authority,donation probability of people who endorse fairness and authority,0.5495059490203857
translation,56,246,ablation-analysis,personal - related inquiry,has,significantly increases,personal - related inquiry has significantly increases,0.5959793925285339
translation,56,246,ablation-analysis,significantly increases,has,donation probability,significantly increases has donation probability,0.5956351161003113
translation,56,246,ablation-analysis,people,has,who endorse freedom and care,people has who endorse freedom and care,0.5288207530975342
translation,56,246,ablation-analysis,ablation analysis,has,personal - related inquiry,ablation analysis has personal - related inquiry,0.5492078065872192
translation,56,184,baselines,self-attention blstm ( blstm ),considers,single- layer bidirectional lstm,self-attention blstm ( blstm ) considers single- layer bidirectional lstm,0.5763224363327026
translation,56,184,baselines,single- layer bidirectional lstm,with,selfattention mechanism,single- layer bidirectional lstm with selfattention mechanism,0.599337637424469
translation,56,184,baselines,baselines,has,self-attention blstm ( blstm ),baselines has self-attention blstm ( blstm ),0.5624541640281677
translation,56,194,baselines,baselines,experimented with,four different context embedding methods,baselines experimented with four different context embedding methods,0.6341496109962463
translation,56,178,hyperparameters,initial learning rate,to be,0.001,initial learning rate to be 0.001,0.5409823060035706
translation,56,178,hyperparameters,initial learning rate,applied,exponential decay,initial learning rate applied exponential decay,0.706426739692688
translation,56,178,hyperparameters,exponential decay,every,100 steps,exponential decay every 100 steps,0.7012279629707336
translation,56,178,hyperparameters,hyperparameters,set,initial learning rate,hyperparameters set initial learning rate,0.5817552208900452
translation,56,178,hyperparameters,hyperparameters,applied,exponential decay,hyperparameters applied exponential decay,0.7063907384872437
translation,56,179,hyperparameters,training batch size,was,32,training batch size was 32,0.5964075326919556
translation,56,179,hyperparameters,hyperparameters,has,training batch size,hyperparameters has training batch size,0.4909193217754364
translation,56,180,hyperparameters,"dropout ( srivastava et al. , 2014 )",to reduce,over-fitting,"dropout ( srivastava et al. , 2014 ) to reduce over-fitting",0.6414870619773865
translation,56,180,hyperparameters,hyperparameters,has,"dropout ( srivastava et al. , 2014 )","hyperparameters has dropout ( srivastava et al. , 2014 )",0.47014522552490234
translation,56,181,hyperparameters,300 - dimension pre-trained fasttext,),word embedding,300 - dimension pre-trained fasttext ) word embedding,0.5485159158706665
translation,56,181,hyperparameters,300 - dimension pre-trained fasttext,as,word embedding,300 - dimension pre-trained fasttext as word embedding,0.5330001711845398
translation,56,181,hyperparameters,hyperparameters,adopted,300 - dimension pre-trained fasttext,hyperparameters adopted 300 - dimension pre-trained fasttext,0.5987900495529175
translation,56,182,hyperparameters,rcnn model,used,single- layer bidirectional lstm,rcnn model used single- layer bidirectional lstm,0.5477004051208496
translation,56,182,hyperparameters,single- layer bidirectional lstm,with,hidden size,single- layer bidirectional lstm with hidden size,0.5883346796035767
translation,56,182,hyperparameters,hidden size,of,200,hidden size of 200,0.6639924049377441
translation,56,182,hyperparameters,hyperparameters,has,rcnn model,hyperparameters has rcnn model,0.5100829005241394
translation,56,185,hyperparameters,finetuning,set,attention dimension,finetuning set attention dimension,0.6347588300704956
translation,56,185,hyperparameters,attention dimension,to be,150,attention dimension to be 150,0.5946667790412903
translation,56,185,hyperparameters,hyperparameters,After,finetuning,hyperparameters After finetuning,0.6722642183303833
translation,56,186,model,convolutional neural network ( cnn ),uses,multiple convolution kernels,convolutional neural network ( cnn ) uses multiple convolution kernels,0.5578935742378235
translation,56,186,model,multiple convolution kernels,to extract,textual features,multiple convolution kernels to extract textual features,0.6852025985717773
translation,56,186,model,model,has,convolutional neural network ( cnn ),model has convolutional neural network ( cnn ),0.5491653084754944
translation,56,191,results,hybrid rcnn,with,all the features ( sentence embedding,hybrid rcnn with all the features ( sentence embedding,0.6128369569778442
translation,56,191,results,hybrid rcnn,reached,highest accuracy ( 74.8 % ),hybrid rcnn reached highest accuracy ( 74.8 % ),0.6982418894767761
translation,56,195,results,rnn,achieved,best result,rnn achieved best result,0.7653853297233582
translation,56,195,results,rnn,achieved,f1 ( 59.3 % ),rnn achieved f1 ( 59.3 % ),0.6614584922790527
translation,56,195,results,best result,has,74.4 % ),best result has 74.4 % ),0.5386309623718262
translation,56,195,results,results,found,rnn,results found rnn,0.653596043586731
translation,56,196,results,context,improved,model performance,context improved model performance,0.7832847237586975
translation,56,196,results,model performance,has,slightly but not significantly,model performance has slightly but not significantly,0.5601077675819397
translation,56,196,results,results,suggest,context,results suggest context,0.5724361538887024
translation,56,196,results,results,incorporating,context,results incorporating context,0.639300525188446
translation,56,230,results,decision style side,participants who are,rational decision makers,decision style side participants who are rational decision makers,0.7042489647865295
translation,56,230,results,more likely,to,donate,more likely to donate,0.6045681834220886
translation,56,230,results,more likely,to,donate,more likely to donate,0.6045681834220886
translation,56,230,results,donate,has,p < 0.05 ),donate has p < 0.05 ),0.6266204714775085
translation,56,230,results,results,On,decision style side,results On decision style side,0.5298284292221069
translation,56,294,results,hybrid rcnn model,with,all the features,hybrid rcnn model with all the features,0.6475733518600464
translation,56,294,results,hybrid rcnn model,performed,best,hybrid rcnn model performed best,0.2537747323513031
translation,56,294,results,best,on,annset,best on annset,0.5788920521736145
translation,56,294,results,results,has,hybrid rcnn model,results has hybrid rcnn model,0.5520923733711243
translation,57,113,ablation-analysis,teaching strategy eapc,achieve,requirement safety and efficiency,teaching strategy eapc achieve requirement safety and efficiency,0.6345510482788086
translation,57,113,ablation-analysis,requirement safety and efficiency,of,on- line dialogue policy learning,requirement safety and efficiency of on- line dialogue policy learning,0.5665715336799622
translation,57,116,baselines,critic-advice ( ca ),where,combination of both ( eapc ),critic-advice ( ca ) where combination of both ( eapc ),0.6121405363082886
translation,57,116,baselines,critic-advice ( ca ),teacher gives,reward,critic-advice ( ca ) teacher gives reward,0.7776193618774414
translation,57,116,baselines,critic-advice ( ca ),teacher gives,combination of both ( eapc ),critic-advice ( ca ) teacher gives combination of both ( eapc ),0.7361549735069275
translation,57,116,baselines,example action ( ea ),teacher gives,action,example action ( ea ) teacher gives action,0.7088083028793335
translation,57,94,experiments,rule- based tracker,for,dialogue state tracking,rule- based tracker for dialogue state tracking,0.6161759495735168
translation,57,31,model,new three - party turnlevel human-machine hybrid learning framework,to address,both the safety and the efficiency issues,new three - party turnlevel human-machine hybrid learning framework to address both the safety and the efficiency issues,0.6329168677330017
translation,57,31,model,both the safety and the efficiency issues,of,on- line policy learning,both the safety and the efficiency issues of on- line policy learning,0.5825662612915039
translation,57,31,model,model,propose,new three - party turnlevel human-machine hybrid learning framework,model propose new three - party turnlevel human-machine hybrid learning framework,0.6404016613960266
translation,57,33,model,model,In,companion teaching framework,model In companion teaching framework,0.5676999688148499
translation,57,115,model,human teacher,in,dialogue policy training loop,human teacher in dialogue policy training loop,0.5367968082427979
translation,57,115,model,novel framework,has,companion teaching,novel framework has companion teaching,0.6099757552146912
translation,57,115,model,learning process,has,safe and efficient,learning process has safe and efficient,0.5357075929641724
translation,57,115,model,model,propose,novel framework,model propose novel framework,0.720393180847168
translation,57,111,results,500 dialogues interaction,obtain,nearly 70 % success rate,500 dialogues interaction obtain nearly 70 % success rate,0.554752767086029
translation,57,111,results,22.4 %,has,higher,22.4 % has higher,0.5582422018051147
translation,57,111,results,results,After,500 dialogues interaction,results After 500 dialogues interaction,0.6810010075569153
translation,58,191,ablation-analysis,accuracy,introduction of,new nlg templates,accuracy introduction of new nlg templates,0.7327024936676025
translation,58,191,ablation-analysis,drops,to,50.51 %,drops to 50.51 %,0.6267003417015076
translation,58,191,ablation-analysis,50.51 %,in,interactive evaluation,50.51 % in interactive evaluation,0.5058032870292664
translation,58,191,ablation-analysis,50.51 %,introduction of,new nlg templates,50.51 % introduction of new nlg templates,0.6724621653556824
translation,58,191,ablation-analysis,accuracy,has,drops,accuracy has drops,0.6229715347290039
translation,58,191,ablation-analysis,ablation analysis,has,accuracy,ablation analysis has accuracy,0.4860230088233948
translation,58,151,experiments,imitation learning,perform,mini-batch model update,imitation learning perform mini-batch model update,0.5500988960266113
translation,58,151,experiments,mini-batch model update,after collecting,every 25 dialogues,mini-batch model update after collecting every 25 dialogues,0.6513420343399048
translation,58,165,experiments,our model,achieves,promising performance,our model achieves promising performance,0.6450070142745972
translation,58,165,experiments,promising performance,on,individual slot tracking,promising performance on individual slot tracking,0.5662054419517517
translation,58,165,experiments,promising performance,on,joint slot tracking accuracy,promising performance on joint slot tracking accuracy,0.5071538686752319
translation,58,165,experiments,movie booking domain,has,our model,movie booking domain has our model,0.5281063914299011
translation,58,145,hyperparameters,size,of,dialogue-level and utterance - level lstm state,size of dialogue-level and utterance - level lstm state,0.5937541127204895
translation,58,145,hyperparameters,dialogue-level and utterance - level lstm state,set as,200 and 150,dialogue-level and utterance - level lstm state set as 200 and 150,0.6249823570251465
translation,58,145,hyperparameters,hyperparameters,has,size,hyperparameters has size,0.5313372611999512
translation,58,146,hyperparameters,word embedding size,is,300,word embedding size is 300,0.6083614230155945
translation,58,146,hyperparameters,hyperparameters,has,word embedding size,hyperparameters has word embedding size,0.494825541973114
translation,58,147,hyperparameters,embedding size,for,system action and slot values,embedding size for system action and slot values,0.6294581890106201
translation,58,147,hyperparameters,system action and slot values,set as,32,system action and slot values set as 32,0.6525061130523682
translation,58,147,hyperparameters,hyperparameters,has,embedding size,hyperparameters has embedding size,0.4976881444454193
translation,58,148,hyperparameters,hidden layer size,of,policy network,hidden layer size of policy network,0.5892255902290344
translation,58,148,hyperparameters,policy network,set as,100,policy network set as 100,0.6798125505447388
translation,58,148,hyperparameters,hyperparameters,has,hidden layer size,hyperparameters has hidden layer size,0.4991928040981293
translation,58,149,hyperparameters,adam optimization method,with,initial learning rate,adam optimization method with initial learning rate,0.5682985782623291
translation,58,149,hyperparameters,initial learning rate,of,1e - 3,initial learning rate of 1e - 3,0.6228042244911194
translation,58,149,hyperparameters,hyperparameters,use,adam optimization method,hyperparameters use adam optimization method,0.6346614360809326
translation,58,150,hyperparameters,dropout rate,of,0.5,dropout rate of 0.5,0.6072384119033813
translation,58,150,hyperparameters,dropout rate,applied during,supervised training,dropout rate applied during supervised training,0.6665200591087341
translation,58,150,hyperparameters,supervised training,to prevent,model from over-fitting,supervised training to prevent model from over-fitting,0.6246704459190369
translation,58,150,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,58,152,hyperparameters,system actions,sampled from,learned policy,system actions sampled from learned policy,0.6251171827316284
translation,58,152,hyperparameters,learned policy,to encourage,exploration,learned policy to encourage exploration,0.729547917842865
translation,58,152,hyperparameters,hyperparameters,has,system actions,hyperparameters has system actions,0.5288714170455933
translation,58,7,model,hybrid imitation and reinforcement learning method,with which,dialogue agent,hybrid imitation and reinforcement learning method with which dialogue agent,0.5836265087127686
translation,58,7,model,dialogue agent,effectively learn from,interaction,dialogue agent effectively learn from interaction,0.6492836475372314
translation,58,7,model,interaction,with,users,interaction with users,0.6236870884895325
translation,58,7,model,interaction,by learning from,human teaching and feedback,interaction by learning from human teaching and feedback,0.6938443779945374
translation,58,7,model,model,propose,hybrid imitation and reinforcement learning method,model propose hybrid imitation and reinforcement learning method,0.661943256855011
translation,58,30,model,dialogue state distribution mismatch,between,offline training and rl interactive learning,dialogue state distribution mismatch between offline training and rl interactive learning,0.6228071451187134
translation,58,30,model,dialogue state distribution mismatch,propose,hybrid imitation and reinforcement learning method,dialogue state distribution mismatch propose hybrid imitation and reinforcement learning method,0.6154857277870178
translation,58,30,model,model,propose,hybrid imitation and reinforcement learning method,model propose hybrid imitation and reinforcement learning method,0.661943256855011
translation,58,31,model,agent,to interact with,users,agent to interact with users,0.721519410610199
translation,58,31,model,users,using,own policy,users using own policy,0.6716886758804321
translation,58,31,model,own policy,learned from,supervised pre-training,own policy learned from supervised pre-training,0.7178151607513428
translation,58,108,model,system,in,supervised manner,system in supervised manner,0.5600385069847107
translation,58,108,model,supervised manner,by fitting,task - oriented dialogue samples,supervised manner by fitting task - oriented dialogue samples,0.7770782709121704
translation,58,108,model,model,train,system,model train system,0.7461904883384705
translation,58,116,model,dialogue imitation learning,allows,dialogue agent,dialogue imitation learning allows dialogue agent,0.6486445069313049
translation,58,116,model,dialogue agent,to learn from,human teaching,dialogue agent to learn from human teaching,0.6473477482795715
translation,58,116,model,model,propose,dialogue imitation learning,model propose dialogue imitation learning,0.6578818559646606
translation,58,202,model,hybrid learning approach,using,end-to - end trainable neural network model,hybrid learning approach using end-to - end trainable neural network model,0.6869388818740845
translation,58,202,model,model,propose,hybrid learning approach,model propose hybrid learning approach,0.7022833824157715
translation,58,164,results,proposed model,achieves,near state- ofthe - art dialogue state tracking results,proposed model achieves near state- ofthe - art dialogue state tracking results,0.6694183945655823
translation,58,164,results,near state- ofthe - art dialogue state tracking results,on,dstc2 corpus,near state- ofthe - art dialogue state tracking results on dstc2 corpus,0.5318816304206848
translation,58,164,results,near state- ofthe - art dialogue state tracking results,on,individual slot tracking,near state- ofthe - art dialogue state tracking results on individual slot tracking,0.5273125767707825
translation,58,164,results,near state- ofthe - art dialogue state tracking results,on,individual slot tracking,near state- ofthe - art dialogue state tracking results on individual slot tracking,0.5273125767707825
translation,58,164,results,near state- ofthe - art dialogue state tracking results,on,joint slot tracking,near state- ofthe - art dialogue state tracking results on joint slot tracking,0.5295732617378235
translation,58,164,results,results,has,proposed model,results has proposed model,0.5938616394996643
translation,58,179,results,interactive learning,with,rl,interactive learning with rl,0.6695411801338196
translation,58,179,results,rl,using,weak form of supervision,rl using weak form of supervision,0.6471948623657227
translation,58,179,results,weak form of supervision,from,user feedback,weak form of supervision from user feedback,0.5311925411224365
translation,58,179,results,weak form of supervision,improves,task success rate,weak form of supervision improves task success rate,0.6744798421859741
translation,58,179,results,task success rate,with,growing number of user interactions,task success rate with growing number of user interactions,0.6530974507331848
translation,58,179,results,results,see that,interactive learning,results see that interactive learning,0.626489520072937
translation,58,186,results,generated dialogues,find that,sl + rl model,generated dialogues find that sl + rl model,0.5981627106666565
translation,58,186,results,sl + rl model,can handle,easy tasks,sl + rl model can handle easy tasks,0.7170101404190063
translation,58,186,results,fails,to complete,more challenging tasks,fails to complete more challenging tasks,0.6950212717056274
translation,58,186,results,easy tasks,has,well,easy tasks has well,0.5603154301643372
translation,58,186,results,results,looking into,generated dialogues,results looking into generated dialogues,0.6116775870323181
translation,58,188,results,imitation plus rl models,attempt to learn,better strategies,imitation plus rl models attempt to learn better strategies,0.7186728715896606
translation,58,188,results,better strategies,to handle,more challenging tasks,better strategies to handle more challenging tasks,0.7079318165779114
translation,58,188,results,better strategies,resulting in,higher task success rates,better strategies resulting in higher task success rates,0.6295462250709534
translation,58,188,results,slightly increased dialogue length,comparing to,sl + rl model,slightly increased dialogue length comparing to sl + rl model,0.7104966640472412
translation,58,188,results,results,has,imitation plus rl models,results has imitation plus rl models,0.5066831111907959
translation,58,189,results,dialogue state tracking accuracy,in,just a few hundred interactive learning sessions,dialogue state tracking accuracy in just a few hundred interactive learning sessions,0.5202276110649109
translation,58,189,results,quickly improves,has,dialogue state tracking accuracy,quickly improves has dialogue state tracking accuracy,0.5528333783149719
translation,58,189,results,results,has,dialogue state tracking accuracy,results has dialogue state tracking accuracy,0.5426565408706665
translation,58,190,results,joint slots tracking accuracy,in,evaluation of sl model,joint slots tracking accuracy in evaluation of sl model,0.48209622502326965
translation,58,190,results,evaluation of sl model,using,fixed corpus,evaluation of sl model using fixed corpus,0.6417269706726074
translation,58,190,results,fixed corpus,is,84.57 %,fixed corpus is 84.57 %,0.5273041725158691
translation,58,190,results,results,has,joint slots tracking accuracy,results has joint slots tracking accuracy,0.5129498839378357
translation,58,192,results,imitation learning with human teaching,effectively adapts,neural dialogue model,imitation learning with human teaching effectively adapts neural dialogue model,0.6844052672386169
translation,58,192,results,neural dialogue model,to,new user input and dialogue state distributions,neural dialogue model to new user input and dialogue state distributions,0.5328501462936401
translation,58,192,results,neural dialogue model,improving,dst accuracy,neural dialogue model improving dst accuracy,0.6395480036735535
translation,58,192,results,dst accuracy,to,67.47 %,dst accuracy to 67.47 %,0.549777626991272
translation,58,192,results,67.47 %,has,after only 500 imitation dialogue learning sessions,67.47 % has after only 500 imitation dialogue learning sessions,0.566166877746582
translation,58,192,results,results,has,imitation learning with human teaching,results has imitation learning with human teaching,0.5530887246131897
translation,58,193,results,rl,on top of,sl model and il model,rl on top of sl model and il model,0.7126030921936035
translation,58,193,results,task success rate,by optimizing,dialogue policy,task success rate by optimizing dialogue policy,0.6940144896507263
translation,58,193,results,results,has,encouraging observation,results has encouraging observation,0.5502797365188599
translation,58,212,results,imitation learning,on,supervised training model,imitation learning on supervised training model,0.5289329290390015
translation,58,212,results,results,applying,imitation learning,results applying imitation learning,0.600271999835968
translation,59,16,experiments,pomdp dialogue manager ( dm ),uses,compressed belief space,pomdp dialogue manager ( dm ) uses compressed belief space,0.5381548404693604
translation,59,16,experiments,compressed belief space,modified version of the,value directed compression ( vdc ) algorithm,compressed belief space modified version of the value directed compression ( vdc ) algorithm,0.6980282068252563
translation,59,62,experiments,demonstration system,provides,restaurant finder system,demonstration system provides restaurant finder system,0.6374509930610657
translation,59,62,experiments,restaurant finder system,for,"city of edinburgh ( scotland , uk )","restaurant finder system for city of edinburgh ( scotland , uk )",0.6201328039169312
translation,59,5,model,power set,of,possible user goals,power set of possible user goals,0.5625944137573242
translation,59,5,model,power set,allows,complex sets of user goals,power set allows complex sets of user goals,0.6826200485229492
translation,59,5,model,complex sets of user goals,to be,represented,complex sets of user goals to be represented,0.5781639218330383
translation,59,5,model,complex sets of user goals,leads to,more natural dialogues,complex sets of user goals leads to more natural dialogues,0.6427896022796631
translation,59,5,model,model,Reasoning over,power set,model Reasoning over power set,0.7135663628578186
translation,60,153,ablation-analysis,representation quality,of,learned state embeddings,representation quality of learned state embeddings,0.4642504155635834
translation,60,153,ablation-analysis,representation quality,leads to,higher gan - vae performance,representation quality leads to higher gan - vae performance,0.628162145614624
translation,60,153,ablation-analysis,higher gan - vae performance,over,gan - ae,higher gan - vae performance over gan - ae,0.6951687932014465
translation,60,153,ablation-analysis,ablation analysis,has,representation quality,ablation analysis has representation quality,0.4947172999382019
translation,60,125,baselines,pre-trained reward,using,two different types of rl methods,pre-trained reward using two different types of rl methods,0.6231914758682251
translation,60,125,baselines,deep q-network ( dqn ),is,off-policy rl algorithm,deep q-network ( dqn ) is off-policy rl algorithm,0.5516210198402405
translation,60,125,baselines,ppo,is,policy - gradient - based rl method,ppo is policy - gradient - based rl method,0.5614861845970154
translation,60,125,baselines,baselines,validate,pre-trained reward,baselines validate pre-trained reward,0.5933536291122437
translation,60,135,baselines,wdqn,provide,three different dialogue agents,wdqn provide three different dialogue agents,0.5742446780204773
translation,60,135,baselines,three different dialogue agents,trained with,reward functions,three different dialogue agents trained with reward functions,0.6985808610916138
translation,60,135,baselines,reward functions,from,human,reward functions from human,0.568252682685852
translation,60,135,baselines,reward functions,from,"gan - ae ,","reward functions from gan - ae ,",0.548999011516571
translation,60,135,baselines,reward functions,from,gan - vae,reward functions from gan - vae,0.5596010088920593
translation,60,61,hyperparameters,one- hot embeddings,to represent,actions,one- hot embeddings to represent actions,0.641557514667511
translation,60,61,hyperparameters,hyperparameters,utilize,one- hot embeddings,hyperparameters utilize one- hot embeddings,0.5486965775489807
translation,60,114,hyperparameters,output,of,gumbel -softmax function,output of gumbel -softmax function,0.5917107462882996
translation,60,114,hyperparameters,gumbel -softmax function,is,one-hot representation ( 300 dimensions ),gumbel -softmax function is one-hot representation ( 300 dimensions ),0.5435782670974731
translation,60,114,hyperparameters,hyperparameters,has,output,hyperparameters has output,0.5167950391769409
translation,60,115,hyperparameters,temperature,for,function,temperature for function,0.6366455554962158
translation,60,115,hyperparameters,temperature,set to,0.8,temperature set to 0.8,0.6703528761863708
translation,60,115,hyperparameters,function,set to,0.8,function set to 0.8,0.6475327014923096
translation,60,115,hyperparameters,hyperparameters,implemented,' straight - through ' gumbel -softmax estimator,hyperparameters implemented ' straight - through ' gumbel -softmax estimator,0.6804758906364441
translation,60,115,hyperparameters,hyperparameters,implemented,temperature,hyperparameters implemented temperature,0.6901099681854248
translation,60,121,hyperparameters,history buffer,with size,10 k,history buffer with size 10 k,0.8410863876342773
translation,60,121,hyperparameters,10 k,to record,simulated state-action pairs,10 k to record simulated state-action pairs,0.6798617243766785
translation,60,121,hyperparameters,simulated state-action pairs,from,generator,simulated state-action pairs from generator,0.5743434429168701
translation,60,121,hyperparameters,simulated state-action pairs,from,generator,simulated state-action pairs from generator,0.5743434429168701
translation,60,121,hyperparameters,generator,where,state-action pairs,generator where state-action pairs,0.6170670390129089
translation,60,121,hyperparameters,state-action pairs,replaced randomly by,newly generated pairs,state-action pairs replaced randomly by newly generated pairs,0.7231835722923279
translation,60,121,hyperparameters,newly generated pairs,from,generator,newly generated pairs from generator,0.589613676071167
translation,60,167,hyperparameters,training,warm - up,all the dialogue agents,training warm - up all the dialogue agents,0.7114219665527344
translation,60,167,hyperparameters,all the dialogue agents,with,human dialogues,all the dialogue agents with human dialogues,0.6193941831588745
translation,60,167,hyperparameters,human dialogues,via,imitation learning,human dialogues via imitation learning,0.6091246008872986
translation,60,167,hyperparameters,hyperparameters,Before initiating,training,hyperparameters Before initiating training,0.6792204976081848
translation,60,11,model,discriminator,with,auxiliary dialogue generator,discriminator with auxiliary dialogue generator,0.5974444150924683
translation,60,11,model,discriminator,incorporate,derived reward model,discriminator incorporate derived reward model,0.6676140427589417
translation,60,11,model,derived reward model,into,common rl method,derived reward model into common rl method,0.5589848160743713
translation,60,11,model,common rl method,to guide,dialogue policy learning,common rl method to guide dialogue policy learning,0.6741904616355896
translation,60,11,model,model,train,discriminator,model train discriminator,0.7298986911773682
translation,60,29,model,new approach,for training,dialogue policy,new approach for training dialogue policy,0.7244500517845154
translation,60,29,model,dialogue policy,by decomposing,adversarial learning method,dialogue policy by decomposing adversarial learning method,0.6766141653060913
translation,60,29,model,adversarial learning method,into,two sequential steps,adversarial learning method into two sequential steps,0.5972661375999451
translation,60,29,model,model,propose,new approach,model propose new approach,0.7215121984481812
translation,60,30,model,reward function,using,auxiliary dialogue state generator,reward function using auxiliary dialogue state generator,0.6100858449935913
translation,60,30,model,auxiliary dialogue state generator,where,loss,auxiliary dialogue state generator where loss,0.6086558103561401
translation,60,30,model,loss,from,discriminator,loss from discriminator,0.5738129615783691
translation,60,30,model,discriminator,can be backpropagated to,generator directly,discriminator can be backpropagated to generator directly,0.7296290993690491
translation,60,30,model,model,learn,reward function,model learn reward function,0.6737400889396667
translation,60,43,model,dialogue policy,has,consecutively,dialogue policy has consecutively,0.6561473608016968
translation,60,43,model,model,train,reward model,model train reward model,0.6961838603019714
translation,60,44,model,reward model,introduce,auxiliary generator,reward model introduce auxiliary generator,0.651266872882843
translation,60,44,model,auxiliary generator,to explore,potential dialogue situations,auxiliary generator to explore potential dialogue situations,0.6924006342887878
translation,60,44,model,model,To train,reward model,model To train reward model,0.6719833612442017
translation,60,117,model,discriminator,is,three - layer mlp,discriminator is three - layer mlp,0.5430856347084045
translation,60,117,model,three - layer mlp,takes as input,concatenation,three - layer mlp takes as input concatenation,0.6590136885643005
translation,60,117,model,concatenation,of,latent state representation ( 64 dimensions ),concatenation of latent state representation ( 64 dimensions ),0.561125636100769
translation,60,117,model,model,has,discriminator,model has discriminator,0.5425825715065002
translation,60,126,model,vanilla dqn,to,wdqn,vanilla dqn to wdqn,0.5883367657661438
translation,60,126,model,wdqn,where,real dialogue state-action pairs,wdqn where real dialogue state-action pairs,0.6156213283538818
translation,60,126,model,real dialogue state-action pairs,from,training set,real dialogue state-action pairs from training set,0.5344734191894531
translation,60,126,model,real dialogue state-action pairs,used to,warm up,real dialogue state-action pairs used to warm up,0.6214928030967712
translation,60,126,model,real dialogue state-action pairs,gradually removed from,training buffer,real dialogue state-action pairs gradually removed from training buffer,0.70468670129776
translation,60,126,model,dialogue policy,at,very beginning,dialogue policy at very beginning,0.5740073919296265
translation,60,126,model,dialogue policy,gradually removed from,training buffer,dialogue policy gradually removed from training buffer,0.7408716678619385
translation,60,126,model,warm up,has,dialogue policy,warm up has dialogue policy,0.6321800947189331
translation,60,145,results,dialogue policy,trained with,gan - vae,dialogue policy trained with gan - vae,0.7266791462898254
translation,60,145,results,gan - vae,shows,best performance,gan - vae shows best performance,0.6944897174835205
translation,60,145,results,best performance,in terms of,convergence speed,best performance in terms of convergence speed,0.6748308539390564
translation,60,145,results,best performance,in terms of,success rate,best performance in terms of success rate,0.7086392641067505
translation,60,145,results,results,has,dialogue policy,results has dialogue policy,0.5676446557044983
translation,60,146,results,updating signal,from,handcrafted reward function r human,updating signal from handcrafted reward function r human,0.548632800579071
translation,60,146,results,handcrafted reward function r human,guide,dialogue policy,handcrafted reward function r human guide dialogue policy,0.6452627182006836
translation,60,146,results,dialogue policy,to,reasonable performance,dialogue policy to reasonable performance,0.5450490713119507
translation,60,146,results,gan - vae and gan - ae,has,updating signal,gan - vae and gan - ae has updating signal,0.5772131681442261
translation,60,146,results,results,In comparison with,gan - vae and gan - ae,results In comparison with gan - vae and gan - ae,0.6778815984725952
translation,60,148,results,policy,with,r human,policy with r human,0.6904983520507812
translation,60,148,results,r human,converges to,lower success rate,r human converges to lower success rate,0.7600887417793274
translation,60,148,results,lower success rate,compare to,gan - vae,lower success rate compare to gan - vae,0.7072854042053223
translation,60,148,results,lower success rate,compare to,gan -ae,lower success rate compare to gan -ae,0.6864461302757263
translation,60,148,results,results,has,policy,results has policy,0.4798124134540558
translation,60,182,results,dialogue agent,benefits from,pre-trained reward function r gan - vae,dialogue agent benefits from pre-trained reward function r gan - vae,0.6388141512870789
translation,60,182,results,results,Comparing,ppo ( gan - vae ),results Comparing ppo ( gan - vae ),0.6285275220870972
translation,60,183,results,agents,trained using,handcrafted reward function,agents trained using handcrafted reward function,0.7287431359291077
translation,60,183,results,handcrafted reward function,such as,dqn ( human ),handcrafted reward function such as dqn ( human ),0.6268911361694336
translation,60,183,results,handcrafted reward function,such as,ppo ( human ),handcrafted reward function such as ppo ( human ),0.6394787430763245
translation,60,183,results,handcrafted reward function,share,similar final performance,handcrafted reward function share similar final performance,0.6739574074745178
translation,60,183,results,similar final performance,has,87 %,similar final performance has 87 %,0.5551832914352417
translation,60,184,results,ppo - based ones,from incorporating,reward signals,ppo - based ones from incorporating reward signals,0.6780146956443787
translation,60,184,results,reward signals,from,same reward function,reward signals from same reward function,0.5435512661933899
translation,60,199,results,dqn new ( gan - vae + nohotel ),benefits from,reward function,dqn new ( gan - vae + nohotel ) benefits from reward function,0.6741377115249634
translation,60,199,results,dqn new ( gan - vae + nohotel ),benefits from,outperforms,dqn new ( gan - vae + nohotel ) benefits from outperforms,0.6910869479179382
translation,60,199,results,reward function,trained in,different domains,reward function trained in different domains,0.6860561966896057
translation,60,199,results,outperforms,has,dqn ( human ),outperforms has dqn ( human ),0.592196524143219
translation,60,199,results,results,conclude,dqn new ( gan - vae + nohotel ),results conclude dqn new ( gan - vae + nohotel ),0.6124694347381592
translation,60,200,results,agents dqn new ( gan - vae + fulldomain ),trained using,reward,agents dqn new ( gan - vae + fulldomain ) trained using reward,0.7013114094734192
translation,60,200,results,dqn ori ( gan - vae + fulldomain ),trained using,reward,dqn ori ( gan - vae + fulldomain ) trained using reward,0.7535603642463684
translation,60,200,results,reward,from,all domains,reward from all domains,0.6111117601394653
translation,60,200,results,reward,have,better performance,reward have better performance,0.5053205490112305
translation,60,200,results,better performance,compared to,dqn new ( gan - vae + nohotel ),better performance compared to dqn new ( gan - vae + nohotel ),0.6859523057937622
translation,60,200,results,results,has,agents dqn new ( gan - vae + fulldomain ),results has agents dqn new ( gan - vae + fulldomain ),0.5911899209022522
translation,61,131,ablation-analysis,meret - rl,shows,increment,meret - rl shows increment,0.7329502105712891
translation,61,131,ablation-analysis,increment,due to,embedding changes,increment due to embedding changes,0.7397382855415344
translation,61,131,ablation-analysis,embedding changes,uses,bert,embedding changes uses bert,0.7039659023284912
translation,61,131,ablation-analysis,embedding changes,uses,bert,embedding changes uses bert,0.7039659023284912
translation,61,131,ablation-analysis,bert,instead of,glove,bert instead of glove,0.6709027290344238
translation,61,131,ablation-analysis,pre-trained language representation,of,bert,pre-trained language representation of bert,0.5176630020141602
translation,61,131,ablation-analysis,ablation analysis,has,meret - rl,ablation analysis has meret - rl,0.5786255598068237
translation,61,156,ablation-analysis,namerelated slots,in,"restaurant , attraction , and hotel domains","namerelated slots in restaurant , attraction , and hotel domains",0.48123598098754883
translation,61,156,ablation-analysis,"restaurant , attraction , and hotel domains",have,highest error rates,"restaurant , attraction , and hotel domains have highest error rates",0.5389506816864014
translation,61,156,ablation-analysis,ablation analysis,has,namerelated slots,ablation analysis has namerelated slots,0.5522973537445068
translation,61,64,baselines,trade,uses,bi-gru,trade uses bi-gru,0.6823408603668213
translation,61,64,baselines,baselines,has,trade,baselines has trade,0.6254712343215942
translation,61,155,experiments,book stay,in,hotel domain,book stay in hotel domain,0.5041351914405823
translation,61,155,experiments,book day,in,restaurant domain,book day in restaurant domain,0.5301227569580078
translation,61,155,experiments,numberrelated slots,has,book stay,numberrelated slots has book stay,0.5402390956878662
translation,61,114,hyperparameters,bi-gru networks,with,hidden size,bi-gru networks with hidden size,0.6249163150787354
translation,61,114,hyperparameters,hidden size,of,768,hidden size of 768,0.6723939776420593
translation,61,114,hyperparameters,768,to be,encoder and the decoder,768 to be encoder and the decoder,0.6013147830963135
translation,61,114,hyperparameters,hyperparameters,choose,bi-gru networks,hyperparameters choose bi-gru networks,0.6539598107337952
translation,61,115,hyperparameters,"adam ( kingma and ba , 2015 )",with,learning rate,"adam ( kingma and ba , 2015 ) with learning rate",0.5996310710906982
translation,61,115,hyperparameters,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,61,115,hyperparameters,hyperparameters,optimized using,"adam ( kingma and ba , 2015 )","hyperparameters optimized using adam ( kingma and ba , 2015 )",0.6826328635215759
translation,61,116,hyperparameters,learning rate,to,half,learning rate to half,0.5718860030174255
translation,61,116,hyperparameters,half,if,validation loss,half if validation loss,0.6466213464736938
translation,61,116,hyperparameters,validation loss,has,increases,validation loss has increases,0.5911926627159119
translation,61,116,hyperparameters,hyperparameters,reduce,learning rate,hyperparameters reduce learning rate,0.6217336058616638
translation,61,117,hyperparameters,"batch ( ioffe and szegedy , 2015 ) size",to,32,"batch ( ioffe and szegedy , 2015 ) size to 32",0.5808970332145691
translation,61,117,hyperparameters,"dropout ( zaremba et al. , 2014 ) rate",to,0.2,"dropout ( zaremba et al. , 2014 ) rate to 0.2",0.50834721326828
translation,61,117,hyperparameters,hyperparameters,set,"batch ( ioffe and szegedy , 2015 ) size","hyperparameters set batch ( ioffe and szegedy , 2015 ) size",0.6030026078224182
translation,61,117,hyperparameters,hyperparameters,set,"dropout ( zaremba et al. , 2014 ) rate","hyperparameters set dropout ( zaremba et al. , 2014 ) rate",0.5581631064414978
translation,61,142,hyperparameters,number of training samples k,from,target domains,number of training samples k from target domains,0.5293075442314148
translation,61,142,hyperparameters,number of training samples k,varies,1 to 10,number of training samples k varies 1 to 10,0.704014778137207
translation,61,142,hyperparameters,hyperparameters,has,number of training samples k,hyperparameters has number of training samples k,0.509993851184845
translation,61,4,model,dialogue state tracker ( dst ),is,core component,dialogue state tracker ( dst ) is core component,0.5551221370697021
translation,61,4,model,core component,of,modular task - oriented dialogue system,core component of modular task - oriented dialogue system,0.5673052668571472
translation,61,4,model,model,has,dialogue state tracker ( dst ),model has dialogue state tracker ( dst ),0.5671396255493164
translation,61,9,model,model,propose,meta-reinforced multi-domain state generator ( meret ),model propose meta-reinforced multi-domain state generator ( meret ),0.6626343727111816
translation,61,11,model,neural model based dst generator,with,reward manager,neural model based dst generator with reward manager,0.6320966482162476
translation,61,11,model,reward manager,built on,policy gradient reinforcement learning ( r - l ),reward manager built on policy gradient reinforcement learning ( r - l ),0.6837781071662903
translation,61,11,model,policy gradient reinforcement learning ( r - l ),to fine - tune,generator,policy gradient reinforcement learning ( r - l ) to fine - tune generator,0.7053273320198059
translation,61,11,model,model,enhance,neural model based dst generator,model enhance neural model based dst generator,0.6407341361045837
translation,61,13,model,dst meta-learning model,with,few domains,dst meta-learning model with few domains,0.6039319038391113
translation,61,13,model,dst meta-learning model,with,new domain,dst meta-learning model with new domain,0.6188086271286011
translation,61,13,model,few domains,as,source domains,few domains as source domains,0.4802766740322113
translation,61,13,model,new domain,as,target domain,new domain as target domain,0.5567277669906616
translation,61,13,model,model,train,dst meta-learning model,model train dst meta-learning model,0.679413378238678
translation,61,14,model,model- agnostic metalearning ( maml ) algorithm,to,dst,model- agnostic metalearning ( maml ) algorithm to dst,0.5505495667457581
translation,61,14,model,obtained meta-learning model,used for,new domain adaptation,obtained meta-learning model used for new domain adaptation,0.6322136521339417
translation,61,14,model,model,apply,model- agnostic metalearning ( maml ) algorithm,model apply model- agnostic metalearning ( maml ) algorithm,0.635404109954834
translation,61,17,model,dst module,takes,user utterance and the dialogue history,dst module takes user utterance and the dialogue history,0.5976771116256714
translation,61,17,model,dst module,outputs,belief estimate,dst module outputs belief estimate,0.7626835107803345
translation,61,17,model,user utterance and the dialogue history,as,input,user utterance and the dialogue history as input,0.5225602984428406
translation,61,17,model,belief estimate,of,dialogue state,belief estimate of dialogue state,0.5685233473777771
translation,61,17,model,each dialogue turn,has,dst module,each dialogue turn has dst module,0.596867024898529
translation,61,17,model,model,For,each dialogue turn,model For each dialogue turn,0.6362709403038025
translation,61,51,model,dst accuracy,propose,new framework,dst accuracy propose new framework,0.7015290260314941
translation,61,51,model,new framework,contains,state generator and reward manager,new framework contains state generator and reward manager,0.6284984946250916
translation,61,51,model,model,To improve,dst accuracy,model To improve dst accuracy,0.6748528480529785
translation,61,97,model,maml,to perform,dialogue state tracking,maml to perform dialogue state tracking,0.6307190656661987
translation,61,97,model,dialogue state tracking,for,new domains,dialogue state tracking for new domains,0.5922081470489502
translation,61,97,model,model,apply,maml,model apply maml,0.6505276560783386
translation,61,199,model,model- agnostic meta-learning ( maml ),for training,dst meta-learning model,model- agnostic meta-learning ( maml ) for training dst meta-learning model,0.7263600826263428
translation,61,199,model,dst meta-learning model,with,few domains,dst meta-learning model with few domains,0.6039319038391113
translation,61,199,model,dst meta-learning model,with,new domain,dst meta-learning model with new domain,0.6188086271286011
translation,61,199,model,few domains,as,training domains,few domains as training domains,0.46674683690071106
translation,61,199,model,new domain,as,testing domain,new domain as testing domain,0.5176557898521423
translation,61,199,model,new domain,to achieve,multi-domain adaptation,new domain to achieve multi-domain adaptation,0.6695979237556458
translation,61,199,model,testing domain,to achieve,multi-domain adaptation,testing domain to achieve multi-domain adaptation,0.6645908355712891
translation,61,199,model,model,apply,model- agnostic meta-learning ( maml ),model apply model- agnostic meta-learning ( maml ),0.637890100479126
translation,61,48,results,trade,able to,help boost,trade able to help boost,0.6405682563781738
translation,61,48,results,dst accuracy,up to,48.62 %,dst accuracy up to 48.62 %,0.6421827673912048
translation,61,48,results,48.62 %,with,multiwoz corpus,48.62 % with multiwoz corpus,0.604371190071106
translation,61,48,results,help boost,has,dst accuracy,help boost has dst accuracy,0.5767306089401245
translation,61,48,results,results,has,trade,results has trade,0.505282998085022
translation,61,124,results,results,with,meret,results with meret,0.6655089259147644
translation,61,125,results,meret,achieves,joint goal accuracy,meret achieves joint goal accuracy,0.6630612015724182
translation,61,125,results,joint goal accuracy,of,50.91 %,joint goal accuracy of 50.91 %,0.5197706818580627
translation,61,125,results,50.91 %,is,2.12 %,50.91 % is 2.12 %,0.5979332327842712
translation,61,125,results,50.91 %,is,2.29 % higher,50.91 % is 2.29 % higher,0.5975539684295654
translation,61,125,results,2.12 %,above,latest state - of- the - art dst model comer,2.12 % above latest state - of- the - art dst model comer,0.6834587454795837
translation,61,125,results,2.29 % higher,than,trade,2.29 % higher than trade,0.5803911685943604
translation,61,125,results,results,has,meret,results has meret,0.602344274520874
translation,61,130,results,meret -bert,remove,"bert , acc 50.35 % , + 1.73 %","meret -bert remove bert , acc 50.35 % , + 1.73 %",0.6585951447486877
translation,61,130,results,meret -bert,has,same embedding glove,meret -bert has same embedding glove,0.6254119873046875
translation,61,130,results,same embedding glove,with,trade,same embedding glove with trade,0.7274816632270813
translation,61,130,results,meret -bert,has,same embedding glove,meret -bert has same embedding glove,0.6254119873046875
translation,61,130,results,"bert , acc 50.35 % , + 1.73 %",has,same embedding glove,"bert , acc 50.35 % , + 1.73 % has same embedding glove",0.5950361490249634
translation,61,130,results,results,has,meret -bert,results has meret -bert,0.5500379204750061
translation,61,132,results,meret 's advantage,mainly comes from,rl,meret 's advantage mainly comes from rl,0.727231502532959
translation,61,132,results,results,see that,meret 's advantage,results see that meret 's advantage,0.6711411476135254
translation,61,138,results,meret,achieves,substantial higher accuracy,meret achieves substantial higher accuracy,0.7110565304756165
translation,61,138,results,meret,achieves,64.7 % joint goal accuracy,meret achieves 64.7 % joint goal accuracy,0.6498116850852966
translation,61,138,results,substantial higher accuracy,comparing to,other two setups,substantial higher accuracy comparing to other two setups,0.7378060817718506
translation,61,138,results,64.7 % joint goal accuracy,for,taxi domain,64.7 % joint goal accuracy for taxi domain,0.5692506432533264
translation,61,138,results,43.10 %,for,attraction domain,43.10 % for attraction domain,0.5970244407653809
translation,61,139,results,similar,obtained for,slot accuracies,similar obtained for slot accuracies,0.7367010116577148
translation,61,139,results,advantages,obtained for,slot accuracies,advantages obtained for slot accuracies,0.6662120819091797
translation,61,139,results,slot accuracies,for,both target domains,slot accuracies for both target domains,0.5862269401550293
translation,61,139,results,similar,has,advantages,similar has advantages,0.5538843274116516
translation,61,139,results,results,has,similar,results has similar,0.5674096941947937
translation,61,139,results,results,has,advantages,results has advantages,0.5523803234100342
translation,61,146,results,accuracy,with,k = 5,accuracy with k = 5,0.6631805300712585
translation,61,146,results,accuracy,with,training meret from scratch,accuracy with training meret from scratch,0.6059754490852356
translation,61,146,results,accuracy,with,training meret from scratch,accuracy with training meret from scratch,0.6059754490852356
translation,61,146,results,k = 5,of,attraction domain,k = 5 of attraction domain,0.6223744750022888
translation,61,146,results,k = 5,surpasses,accuracy,k = 5 surpasses accuracy,0.6320779919624329
translation,61,146,results,accuracy,with,training meret from scratch,accuracy with training meret from scratch,0.6059754490852356
translation,61,146,results,training meret from scratch,using,1 % ( 30 dialogues ),training meret from scratch using 1 % ( 30 dialogues ),0.6438684463500977
translation,61,146,results,1 % ( 30 dialogues ),of,attraction domain data,1 % ( 30 dialogues ) of attraction domain data,0.5654721260070801
translation,62,222,ablation-analysis,did not help,in,forecasting setting,did not help in forecasting setting,0.5538449883460999
translation,62,222,ablation-analysis,model,with,self 42 utterance attention,model with self 42 utterance attention,0.6619310975074768
translation,62,222,ablation-analysis,self 42 utterance attention,is,sufficient,self 42 utterance attention is sufficient,0.5816219449043274
translation,62,222,ablation-analysis,ablation analysis,has,word attention ( gmgru h ),ablation analysis has word attention ( gmgru h ),0.5886077284812927
translation,62,249,ablation-analysis,window size,to,16,window size to 16,0.5941939353942871
translation,62,249,ablation-analysis,window size,has,overall reflection results,window size has overall reflection results,0.5611222982406616
translation,62,249,ablation-analysis,16,has,overall reflection results,16 has overall reflection results,0.5459048748016357
translation,62,249,ablation-analysis,overall reflection results,has,improved,overall reflection results has improved,0.5790017247200012
translation,62,249,ablation-analysis,ablation analysis,increasing,window size,ablation analysis increasing window size,0.7053272128105164
translation,62,260,ablation-analysis,ablation analysis,Increasing,history window size,ablation analysis Increasing history window size,0.7167121171951294
translation,62,265,ablation-analysis,overall performance,drops by,3.4 points,overall performance drops by 3.4 points,0.7083262205123901
translation,62,265,ablation-analysis,drop,by,3.3 and 5 points,drop by 3.3 and 5 points,0.6354798078536987
translation,62,265,ablation-analysis,word- level attention,has,overall performance,word- level attention has overall performance,0.533187985420227
translation,62,265,ablation-analysis,ablation analysis,remove,word- level attention,ablation analysis remove word- level attention,0.6301643252372742
translation,62,266,ablation-analysis,attention,to,bidaf,attention to bidaf,0.6626914143562317
translation,62,266,ablation-analysis,attention,decreases,performance,attention decreases performance,0.6714437007904053
translation,62,266,ablation-analysis,performance,by,2 points,performance by 2 points,0.6287950873374939
translation,62,266,ablation-analysis,performance,about,2 points,performance about 2 points,0.6717811226844788
translation,62,266,ablation-analysis,ablation analysis,Changing,attention,ablation analysis Changing attention,0.7670143246650696
translation,62,201,baselines,wellstudied linguistic features,tagging,current utterance,wellstudied linguistic features tagging current utterance,0.7164682745933533
translation,62,201,baselines,current utterance,with,past and future utterance,current utterance with past and future utterance,0.653942346572876
translation,62,201,baselines,current utterance,with,crf and memm,current utterance with crf and memm,0.6746944189071655
translation,62,201,baselines,past and future utterance,with,crf and memm,past and future utterance with crf and memm,0.6695957779884338
translation,62,205,baselines,bigru elmo,is,simple but robust baseline model,bigru elmo is simple but robust baseline model,0.5619112849235535
translation,62,205,baselines,client codes,has,bigru elmo,client codes has bigru elmo,0.607516884803772
translation,62,182,experimental-setup,tokenized and lower - cased utterances,using,spacy,tokenized and lower - cased utterances using spacy,0.6362717151641846
translation,62,183,experimental-setup,words,concatenated,300 - dimensional glove embeddings,words concatenated 300 - dimensional glove embeddings,0.6672794222831726
translation,62,183,experimental-setup,300 - dimensional glove embeddings,with,elmo vectors,300 - dimensional glove embeddings with elmo vectors,0.6191598176956177
translation,62,183,experimental-setup,experimental setup,To embed,words,experimental setup To embed words,0.6723175644874573
translation,62,59,experiments,previous session - informed approaches,that use,information,previous session - informed approaches that use information,0.7702714800834656
translation,62,59,experiments,information,from,future utterances,information from future utterances,0.5852973461151123
translation,62,59,experiments,categorization task,has,our models,categorization task has our models,0.5260180830955505
translation,62,59,experiments,our models,has,even outperform,our models has even outperform,0.5847745537757874
translation,62,59,experiments,even outperform,has,previous session - informed approaches,even outperform has previous session - informed approaches,0.5601076483726501
translation,62,5,model,behavioral codes,used to asses,psychotherapy treatment style,behavioral codes used to asses psychotherapy treatment style,0.6623470783233643
translation,62,5,model,psychotherapy treatment style,called,motivational interviewing ( mi ),psychotherapy treatment style called motivational interviewing ( mi ),0.6229588985443115
translation,62,5,model,psychotherapy treatment style,effective for addressing,substance abuse and related problems,psychotherapy treatment style effective for addressing substance abuse and related problems,0.6697882413864136
translation,62,7,model,model,define,neural network models,model define neural network models,0.6108676195144653
translation,62,109,model,hierarchical recurrent encoder,to encode,dialogues,hierarchical recurrent encoder to encode dialogues,0.7252929210662842
translation,62,109,model,dialogues,specifically,hierarchical gated recurrent unit ( hgru ),dialogues specifically hierarchical gated recurrent unit ( hgru ),0.681516706943512
translation,62,109,model,hierarchical gated recurrent unit ( hgru ),with,utterance,hierarchical gated recurrent unit ( hgru ) with utterance,0.6755110025405884
translation,62,109,model,hierarchical gated recurrent unit ( hgru ),with,dialogue encoder,hierarchical gated recurrent unit ( hgru ) with dialogue encoder,0.6381054520606995
translation,62,109,model,model,use,hierarchical recurrent encoder,model use hierarchical recurrent encoder,0.6063772439956665
translation,62,206,results,previous best no-context model,by,more than 2 points,previous best no-context model by more than 2 points,0.5754399299621582
translation,62,206,results,more than 2 points,on,macro f 1,more than 2 points on macro f 1,0.6392000913619995
translation,62,206,results,outperforms,has,previous best no-context model,outperforms has previous best no-context model,0.5742767453193665
translation,62,206,results,results,has,outperforms,results has outperforms,0.6657275557518005
translation,62,207,results,more sophisticated model c c,gets,1 point improvement,more sophisticated model c c gets 1 point improvement,0.5884445905685425
translation,62,207,results,dialogue history,has,more sophisticated model c c,dialogue history has more sophisticated model c c,0.5978369116783142
translation,62,207,results,results,Using,dialogue history,results Using dialogue history,0.5990660190582275
translation,62,210,results,therapist codes,only incorporating,gmgru - based word - level attention,therapist codes only incorporating gmgru - based word - level attention,0.6874650120735168
translation,62,210,results,our proposed model f t,uses,gmgru - based word - level attention,our proposed model f t uses gmgru - based word - level attention,0.5440526008605957
translation,62,210,results,our proposed model f t,uses,anchorbased multi-head multihop sentence - level attention,our proposed model f t uses anchorbased multi-head multihop sentence - level attention,0.5365197658538818
translation,62,210,results,our proposed model f t,achieve,best overall performance,our proposed model f t achieve best overall performance,0.6536938548088074
translation,62,210,results,therapist codes,has,gmgru h,therapist codes has gmgru h,0.5600517392158508
translation,62,210,results,gmgru - based word - level attention,has,gmgru h,gmgru - based word - level attention has gmgru h,0.6349265575408936
translation,62,210,results,gmgru h,has,already outperformed,gmgru h has already outperformed,0.6214349269866943
translation,62,210,results,already outperformed,has,many baselines,already outperformed has many baselines,0.6199805736541748
translation,62,210,results,results,For,therapist codes,results For therapist codes,0.5241007208824158
translation,62,211,results,outperform,take advantage of,future utterances,outperform take advantage of future utterances,0.6850049495697021
translation,62,211,results,approaches,take advantage of,future utterances,approaches take advantage of future utterances,0.6825692653656006
translation,62,211,results,our models,has,outperform,our models has outperform,0.6103132367134094
translation,62,211,results,outperform,has,approaches,outperform has approaches,0.6554949879646301
translation,62,211,results,results,note,our models,results note our models,0.62925124168396
translation,62,212,results,both client and therapist codes,concatenating,dialogue history,both client and therapist codes concatenating dialogue history,0.620489776134491
translation,62,212,results,dialogue history,with,concat c,dialogue history with concat c,0.7276140451431274
translation,62,212,results,dialogue history,performs,worse,dialogue history performs worse,0.6814418435096741
translation,62,212,results,worse,than,hierarchical method,worse than hierarchical method,0.606651246547699
translation,62,212,results,results,For,both client and therapist codes,results For both client and therapist codes,0.5023056268692017
translation,62,226,results,therapist labels,always predicted,"three most frequent labels ( fa , gi , and res )","therapist labels always predicted three most frequent labels ( fa , gi , and res )",0.8008794188499451
translation,62,226,results,recall@3,is,67.7,recall@3 is 67.7,0.638300895690918
translation,62,226,results,recall@3,only,67.7,recall@3 only 67.7,0.6989752054214478
translation,62,226,results,therapist labels,has,recall@3,therapist labels has recall@3,0.6105126738548279
translation,62,226,results,"three most frequent labels ( fa , gi , and res )",has,recall@3,"three most frequent labels ( fa , gi , and res ) has recall@3",0.6117597222328186
translation,62,226,results,results,For,therapist labels,results For therapist labels,0.549946129322052
translation,62,261,results,biggest improvements,for,categorizing therapist codes,biggest improvements for categorizing therapist codes,0.5914756655693054
translation,62,261,results,categorizing therapist codes,especially for,res and rec,categorizing therapist codes especially for res and rec,0.6753511428833008
translation,62,261,results,results,has,biggest improvements,results has biggest improvements,0.5709327459335327
translation,63,231,experiments,beam search,with,100 beam size,beam search with 100 beam size,0.6531941294670105
translation,63,231,experiments,beam search,for,text generation,beam search for text generation,0.6010617613792419
translation,63,163,hyperparameters,training,augment,samples,training augment samples,0.7144457101821899
translation,63,163,hyperparameters,samples,from,opensubtitles,samples from opensubtitles,0.6568422913551331
translation,63,163,hyperparameters,opensubtitles,with,random personas,opensubtitles with random personas,0.6690286993980408
translation,63,163,hyperparameters,profilememory +,to actively prioritize,defaultfact,profilememory + to actively prioritize defaultfact,0.6201649904251099
translation,63,163,hyperparameters,defaultfact,over,fake facts,defaultfact over fake facts,0.6913517117500305
translation,63,163,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
translation,63,164,hyperparameters,single layer lstm,for,both the encoder and the decoder,single layer lstm for both the encoder and the decoder,0.5775354504585266
translation,63,164,hyperparameters,both the encoder and the decoder,with,hidden size,both the encoder and the decoder with hidden size,0.6579287648200989
translation,63,164,hyperparameters,hidden size,of,1024,hidden size of 1024,0.6436027884483337
translation,63,164,hyperparameters,hyperparameters,use,single layer lstm,hyperparameters use single layer lstm,0.5536746978759766
translation,63,165,hyperparameters,word embeddings,of size,300,word embeddings of size 300,0.692033052444458
translation,63,165,hyperparameters,word embeddings,initialized with,glove word vectors,word embeddings initialized with glove word vectors,0.7148382067680359
translation,63,165,hyperparameters,hyperparameters,has,word embeddings,hyperparameters has word embeddings,0.4784160554409027
translation,63,166,hyperparameters,20 epochs,to maximize,likelihood,20 epochs to maximize likelihood,0.7164297103881836
translation,63,166,hyperparameters,likelihood,of,data,likelihood of data,0.6083109378814697
translation,63,166,hyperparameters,likelihood,by using,sgd,likelihood by using sgd,0.6738260984420776
translation,63,166,hyperparameters,sgd,with,momentum,sgd with momentum,0.7183332443237305
translation,63,166,hyperparameters,momentum,with,batch size,momentum with batch size,0.6645553112030029
translation,63,166,hyperparameters,batch size,has,128,batch size has 128,0.6398258209228516
translation,63,166,hyperparameters,hyperparameters,trained for,20 epochs,hyperparameters trained for 20 epochs,0.6829384565353394
translation,63,167,hyperparameters,increased,compared to,previous epoch,increased compared to previous epoch,0.6978296637535095
translation,63,167,hyperparameters,validation perplexity,has,increased,validation perplexity has increased,0.579684853553772
translation,63,167,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,63,171,hyperparameters,weights,for,encoding memories,weights for encoding memories,0.6127141714096069
translation,63,171,hyperparameters,learned,during,training,learned during training,0.7516074180603027
translation,63,171,hyperparameters,0.01,for,top 100 frequent words,0.01 for top 100 frequent words,0.5736916065216064
translation,63,171,hyperparameters,1,for,others,1 for others,0.6616073846817017
translation,63,171,hyperparameters,hyperparameters,has,weights,hyperparameters has weights,0.5201958417892456
translation,63,254,model,new metric discoveryscore,to assess,engagingness,new metric discoveryscore to assess engagingness,0.679938793182373
translation,63,254,model,engagingness,of,dialogue,engagingness of dialogue,0.5337551236152649
translation,63,254,model,dialogue,based on,intuition,dialogue based on intuition,0.692878246307373
translation,63,254,model,intuition,that,more interested,intuition that more interested,0.6535355448722839
translation,63,254,model,model,introduce,new metric discoveryscore,model introduce new metric discoveryscore,0.6475987434387207
translation,63,170,results,encoded memories,gives,better performance,encoded memories gives better performance,0.647530198097229
translation,63,170,results,better performance,than,pre-attention,better performance than pre-attention,0.5910715460777283
translation,63,170,results,results,has,encoded memories,results has encoded memories,0.5191358923912048
translation,63,177,results,our re-implementation,of,seq2seq and pro-filememory,our re-implementation of seq2seq and pro-filememory,0.6014123558998108
translation,63,177,results,seq2seq and pro-filememory,show,better performance,seq2seq and pro-filememory show better performance,0.6656346917152405
translation,63,177,results,results,has,our re-implementation,results has our re-implementation,0.5494418144226074
translation,63,178,results,additional data,such as,opensubtitles,additional data such as opensubtitles,0.5588344931602478
translation,63,178,results,additional data,improves,performance,additional data improves performance,0.700272798538208
translation,63,178,results,results,Including,additional data,results Including additional data,0.6075361967086792
translation,63,179,results,our pro-filememory +,performs,better,our pro-filememory + performs better,0.6806693077087402
translation,63,179,results,better,than,profile -memory,better than profile -memory,0.6401116847991943
translation,63,179,results,results,has,our pro-filememory +,results has our pro-filememory +,0.5692497491836548
translation,63,214,results,dis - coveryscore,to measure,engaging,dis - coveryscore to measure engaging,0.7918094396591187
translation,63,214,results,engaging,has,conversation,engaging has conversation,0.5661244988441467
translation,63,243,results,discoveryscore - oriented re-ranking,makes,conversations,discoveryscore - oriented re-ranking makes conversations,0.6496426463127136
translation,63,243,results,more engaging,for,all type of the chatbot models,more engaging for all type of the chatbot models,0.628594160079956
translation,63,243,results,conversations,has,more engaging,conversations has more engaging,0.5563941597938538
translation,63,243,results,results,demonstrate,discoveryscore - oriented re-ranking,results demonstrate discoveryscore - oriented re-ranking,0.5846022367477417
translation,64,180,ablation-analysis,combined contribution,of capturing,relevant dialogue content and interaction,combined contribution of capturing relevant dialogue content and interaction,0.7271974682807922
translation,64,180,ablation-analysis,relevant dialogue content and interaction,using,latent variables,relevant dialogue content and interaction using latent variables,0.6250671744346619
translation,64,180,ablation-analysis,relevant dialogue content and interaction,combined with,pragmatic information,relevant dialogue content and interaction combined with pragmatic information,0.6592286825180054
translation,64,180,ablation-analysis,ablation analysis,has,most striking observation,ablation analysis has most striking observation,0.49451473355293274
translation,64,181,ablation-analysis,allobj,used in,conjunction,allobj used in conjunction,0.6852462291717529
translation,64,181,ablation-analysis,joint contribution,pushed,performance,joint contribution pushed performance,0.6753243207931519
translation,64,181,ablation-analysis,performance,to,69.1 accuracy,performance to 69.1 accuracy,0.5107772946357727
translation,64,181,ablation-analysis,considerable improvement,over using,each one,considerable improvement over using each one,0.7697139978408813
translation,64,181,ablation-analysis,considerable improvement,over using,65.1,considerable improvement over using 65.1,0.6769574284553528
translation,64,181,ablation-analysis,considerable improvement,over using,65.4,considerable improvement over using 65.4,0.6856940388679504
translation,64,181,ablation-analysis,65.1,for,deterministic system,65.1 for deterministic system,0.575485110282898
translation,64,181,ablation-analysis,deterministic system,using,pragmatic information,deterministic system using pragmatic information,0.6484362483024597
translation,64,181,ablation-analysis,65.4,of,latent - variable formulation,65.4 of latent - variable formulation,0.556952178478241
translation,64,181,ablation-analysis,allobj,has,joint contribution,allobj has joint contribution,0.51219242811203
translation,64,181,ablation-analysis,conjunction,has,joint contribution,conjunction has joint contribution,0.553826630115509
translation,64,181,ablation-analysis,69.1 accuracy,has,considerable improvement,69.1 accuracy has considerable improvement,0.5591515302658081
translation,64,181,ablation-analysis,each one,has,in isolation,each one has in isolation,0.6245556473731995
translation,64,181,ablation-analysis,in isolation,has,65.1,in isolation has 65.1,0.5418746471405029
translation,64,181,ablation-analysis,in isolation,has,65.4,in isolation has 65.4,0.5566000938415527
translation,64,181,ablation-analysis,ablation analysis,in,allobj,ablation analysis in allobj,0.5829972624778748
translation,64,12,model,discriminative latent variable model,able to capture,overall structure,discriminative latent variable model able to capture overall structure,0.7029434442520142
translation,64,12,model,overall structure,of,dialogue,overall structure of dialogue,0.616539478302002
translation,64,12,model,overall structure,of,dialogue,overall structure of dialogue,0.616539478302002
translation,64,12,model,specific acts,result of,dialogue,specific acts result of dialogue,0.7330443263053894
translation,64,12,model,model,devise,discriminative latent variable model,model devise discriminative latent variable model,0.6811807751655579
translation,64,13,model,preceding dialogue,to,particular action,preceding dialogue to particular action,0.5856072306632996
translation,64,13,model,binary structured relationship,among,utterances,binary structured relationship among utterances,0.617083728313446
translation,64,13,model,binary structured relationship,taking into account,pragmatic effect,binary structured relationship taking into account pragmatic effect,0.6380189657211304
translation,64,13,model,pragmatic effect,introduced by,different speakers ' perspectives,pragmatic effect introduced by different speakers ' perspectives,0.6458863615989685
translation,64,13,model,model,relevance of,preceding dialogue,model relevance of preceding dialogue,0.7120141386985779
translation,64,21,model,key properties,of,dialogue interactions,key properties of dialogue interactions,0.5535026788711548
translation,64,21,model,learning,to identify and classify,courtroom objections,learning to identify and classify courtroom objections,0.6617462635040283
translation,64,21,model,identifying,has,key properties,identifying has key properties,0.5950255393981934
translation,64,21,model,model,focus on,identifying,model focus on identifying,0.7532828450202942
translation,64,183,results,sustained objections,are,easier to predict,sustained objections are easier to predict,0.5992634892463684
translation,64,183,results,easier to predict,than,overruled objections,easier to predict than overruled objections,0.6168709397315979
translation,64,183,results,results,observe,sustained objections,results observe sustained objections,0.5749812722206116
translation,64,199,results,objection types,using,latent variables modeling,objection types using latent variables modeling,0.6538660526275635
translation,64,199,results,latent variables modeling,results in,considerable improvement,latent variables modeling results in considerable improvement,0.6510412096977234
translation,64,199,results,considerable improvement,in,performance,considerable improvement in performance,0.5340750813484192
translation,64,199,results,results,see that,objection types,results see that objection types,0.6458288431167603
translation,64,199,results,results,across,objection types,results across objection types,0.6450150012969971
translation,64,253,results,consider - able improvement,when using,our latent learning framework,consider - able improvement when using our latent learning framework,0.735042929649353
translation,64,253,results,our latent learning framework,with,pragmatic information,our latent learning framework with pragmatic information,0.6280370950698853
translation,64,253,results,results,show,consider - able improvement,results show consider - able improvement,0.688296914100647
translation,65,72,baselines,singleinput openai gpt,using,token type embedding,singleinput openai gpt using token type embedding,0.6672139763832092
translation,65,72,baselines,token type embedding,to distinguish,different parts,token type embedding to distinguish different parts,0.6503596901893616
translation,65,72,baselines,different parts,of,single concatenated input,different parts of single concatenated input,0.5932231545448303
translation,65,73,baselines,original gpt,with,gpt2,original gpt with gpt2,0.6145977973937988
translation,65,73,baselines,gpt2,denoted as,transfergpt2,gpt2 denoted as transfergpt2,0.6843814253807068
translation,65,74,baselines,"mi -gpt ( golovanov et al. , 2019 )",uses,openai gpt,"mi -gpt ( golovanov et al. , 2019 ) uses openai gpt",0.5536760091781616
translation,65,74,baselines,openai gpt,in both encoder and decoder,average pooling,openai gpt in both encoder and decoder average pooling,0.7441674470901489
translation,65,74,baselines,openai gpt,with,average pooling,openai gpt with average pooling,0.6664204001426697
translation,65,74,baselines,average pooling,as,attention fusion method,average pooling as attention fusion method,0.5238977074623108
translation,65,74,baselines,baselines,has,"mi -gpt ( golovanov et al. , 2019 )","baselines has mi -gpt ( golovanov et al. , 2019 )",0.5106789469718933
translation,65,76,baselines,architecture,using,gpt2,architecture using gpt2,0.6881227493286133
translation,65,76,baselines,architecture,average as,fusion method ( gpt2 - avg ),architecture average as fusion method ( gpt2 - avg ),0.7266749739646912
translation,65,76,baselines,gpt2,as,base model,gpt2 as base model,0.5971548557281494
translation,65,76,baselines,baselines,has,architecture,baselines has architecture,0.6045894026756287
translation,65,89,baselines,encoder-decoder frameworks,has,outperform,encoder-decoder frameworks has outperform,0.5829102396965027
translation,65,89,baselines,outperform,has,single - input ( si ) models,outperform has single - input ( si ) models,0.5568839907646179
translation,65,89,baselines,baselines,has,multiinput ( mi ) models,baselines has multiinput ( mi ) models,0.5584539175033569
translation,65,111,baselines,baselines,has,gpt2 - weight,baselines has gpt2 - weight,0.5499975681304932
translation,65,7,experiments,dialogue models,with,multiple input sources,dialogue models with multiple input sources,0.5963782072067261
translation,65,7,experiments,dialogue models,adapted from,pretrained language model gpt2,dialogue models adapted from pretrained language model gpt2,0.5141003727912903
translation,65,8,model,various methods,to fuse,multiple separate attention information,various methods to fuse multiple separate attention information,0.6922433376312256
translation,65,8,model,multiple separate attention information,corresponding to,different sources,multiple separate attention information corresponding to different sources,0.5936224460601807
translation,65,8,model,model,explore,various methods,model explore various methods,0.7316109538078308
translation,65,88,results,gpt2,is,more powerful,gpt2 is more powerful,0.570842981338501
translation,65,88,results,more powerful,than,openai gpt,more powerful than openai gpt,0.5989322662353516
translation,65,88,results,openai gpt,under,same architecture,openai gpt under same architecture,0.6365209221839905
translation,65,91,results,all attention fusion methods,of,our model,all attention fusion methods of our model,0.5249013900756836
translation,65,91,results,all attention fusion methods,make,improvements,all attention fusion methods make improvements,0.6489042639732361
translation,65,91,results,our model,make,improvements,our model make improvements,0.6995553970336914
translation,65,91,results,improvements,compared to,baseline gpt2 - avg,improvements compared to baseline gpt2 - avg,0.6150767803192139
translation,65,91,results,results,has,all attention fusion methods,results has all attention fusion methods,0.5195003747940063
translation,65,92,results,weighting methods,have,higher scores,weighting methods have higher scores,0.5429680347442627
translation,65,92,results,higher scores,than,other two kinds of fusion methods,higher scores than other two kinds of fusion methods,0.5503473877906799
translation,65,92,results,other two kinds of fusion methods,on,most metrics,other two kinds of fusion methods on most metrics,0.4936162233352661
translation,65,95,results,gtp2 - dw,shows,no improvement,gtp2 - dw shows no improvement,0.7046389579772949
translation,65,95,results,no improvement,compared to,gpt2 -sw,no improvement compared to gpt2 -sw,0.6889218091964722
translation,65,95,results,results,find that,gtp2 - dw,results find that gtp2 - dw,0.6163681745529175
translation,65,100,results,mi models,using,"average fusion ( mi - gpt , gpt2 - avg )","mi models using average fusion ( mi - gpt , gpt2 - avg )",0.6435983180999756
translation,65,100,results,mi models,show,lower h-rel scores,mi models show lower h-rel scores,0.6357440948486328
translation,65,100,results,si models,has,mi models,si models has mi models,0.5641725659370422
translation,65,100,results,results,compared with,si models,results compared with si models,0.6720528602600098
translation,65,109,results,all methods,perform,comparably,all methods perform comparably,0.5680339336395264
translation,65,109,results,comparably,when,dialogue history,comparably when dialogue history,0.7263006567955017
translation,65,109,results,dialogue history,is,short,dialogue history is short,0.6189347505569458
translation,65,109,results,results,has,all methods,results has all methods,0.48065561056137085
translation,65,110,results,longer dialog history,models with,weighting fusion methods,longer dialog history models with weighting fusion methods,0.7086168527603149
translation,65,110,results,results,With,longer dialog history,results With longer dialog history,0.6213634610176086
translation,65,112,results,results,much better than,gpt2 - avg,results much better than gpt2 - avg,0.6773326992988586
translation,65,114,results,gpt2 with weighting fusion methods,to,transfergpt2,gpt2 with weighting fusion methods to transfergpt2,0.5921643376350403
translation,65,114,results,si models,when,dialogue history,si models when dialogue history,0.6165198087692261
translation,65,114,results,dialogue history,is,long,dialogue history is long,0.6326569318771362
translation,65,114,results,outperform,has,si models,outperform has si models,0.612523078918457
translation,65,114,results,results,compare,gpt2 with weighting fusion methods,results compare gpt2 with weighting fusion methods,0.6505540609359741
translation,65,115,results,si models,beat,mi baselines,si models beat mi baselines,0.7021749019622803
translation,65,115,results,mi baselines,with,average fusion,mi baselines with average fusion,0.6550065875053406
translation,65,115,results,average fusion,under,all conditions,average fusion under all conditions,0.6730055809020996
translation,65,115,results,results,see that,si models,results see that si models,0.642154335975647
translation,66,125,experimental-setup,fair comparison,among,methods,fair comparison among methods,0.6509528756141663
translation,66,125,experimental-setup,numbers of hidden nodes,set to,512,numbers of hidden nodes set to 512,0.685546338558197
translation,66,125,experimental-setup,batch sizes,set to,32,batch sizes set to 32,0.6632431745529175
translation,66,125,experimental-setup,fair comparison,has,numbers of hidden nodes,fair comparison has numbers of hidden nodes,0.5505936741828918
translation,66,125,experimental-setup,methods,has,numbers of hidden nodes,methods has numbers of hidden nodes,0.5427953004837036
translation,66,125,experimental-setup,methods,has,numbers of hidden nodes,methods has numbers of hidden nodes,0.5427953004837036
translation,66,126,experimental-setup,max length,of,dialogue turns,max length of dialogue turns,0.6089134812355042
translation,66,126,experimental-setup,dialogue turns,is,15,dialogue turns is 15,0.5601911544799805
translation,66,126,experimental-setup,max sentence length,is,50,max sentence length is 50,0.6043004393577576
translation,66,126,experimental-setup,experimental setup,has,max length,experimental setup has max length,0.52577805519104
translation,66,126,experimental-setup,experimental setup,has,max sentence length,experimental setup has max sentence length,0.5037837624549866
translation,66,127,experimental-setup,head number,of,recosa model,head number of recosa model,0.6300897002220154
translation,66,127,experimental-setup,recosa model,set as,6,recosa model set as 6,0.6867781281471252
translation,66,127,experimental-setup,experimental setup,has,head number,experimental setup has head number,0.4883655607700348
translation,66,128,experimental-setup,adam,utilized for,optimization,adam utilized for optimization,0.6855039000511169
translation,66,128,experimental-setup,learning rate,set to be,0.0001,learning rate set to be 0.0001,0.6785368919372559
translation,66,128,experimental-setup,experimental setup,has,adam,experimental setup has adam,0.4964992105960846
translation,66,128,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,66,129,experimental-setup,models,on,tesla k80 gpu card,models on tesla k80 gpu card,0.5225602984428406
translation,66,129,experimental-setup,tesla k80 gpu card,with,tensorflow,tesla k80 gpu card with tensorflow,0.6036877036094666
translation,66,129,experimental-setup,experimental setup,run,models,experimental setup run models,0.6445184350013733
translation,66,9,model,word level lstm encoder,conducted,initial representation,word level lstm encoder conducted initial representation,0.5637081861495972
translation,66,9,model,word level lstm encoder,to obtain,initial representation,word level lstm encoder to obtain initial representation,0.5437416434288025
translation,66,9,model,initial representation,of,each context,initial representation of each context,0.5687827467918396
translation,66,9,model,model,has,word level lstm encoder,model has word level lstm encoder,0.48179805278778076
translation,66,32,model,self-attention mechanism,to measure,relevance,self-attention mechanism to measure relevance,0.7387139797210693
translation,66,32,model,relevance,between,response and each context,relevance between response and each context,0.661154568195343
translation,66,34,model,word-level lstm encoder,to obtain,fixed - dimensional representation,word-level lstm encoder to obtain fixed - dimensional representation,0.5627056360244751
translation,66,34,model,fixed - dimensional representation,of,each context,fixed - dimensional representation of each context,0.5704213976860046
translation,66,34,model,model,use,word-level lstm encoder,model use word-level lstm encoder,0.5460731983184814
translation,66,63,model,word- level encoder,encodes,each context,word- level encoder encodes each context,0.7281908988952637
translation,66,63,model,each context,as,low-dimension representation,each context as low-dimension representation,0.47995632886886597
translation,66,63,model,model,has,word- level encoder,model has word- level encoder,0.5196163058280945
translation,66,143,results,attention - based models,such as,wseq,attention - based models such as wseq,0.6565314531326294
translation,66,143,results,attention - based models,such as,hran,attention - based models such as hran,0.654607355594635
translation,66,143,results,attention - based models,such as,hvmn,attention - based models such as hvmn,0.6383994221687317
translation,66,143,results,attention - based models,such as,outperform,attention - based models such as outperform,0.6730024218559265
translation,66,143,results,outperform,in terms of,bleu,outperform in terms of bleu,0.6575438380241394
translation,66,143,results,outperform,in terms of,distinct - 2 measures,outperform in terms of distinct - 2 measures,0.7322049736976624
translation,66,143,results,traditional hred baselines,in terms of,bleu,traditional hred baselines in terms of bleu,0.6790181994438171
translation,66,143,results,traditional hred baselines,in terms of,distinct - 2 measures,traditional hred baselines in terms of distinct - 2 measures,0.7077763080596924
translation,66,143,results,attention - based models,has,outperform,attention - based models has outperform,0.5952038764953613
translation,66,143,results,outperform,has,traditional hred baselines,outperform has traditional hred baselines,0.6103762984275818
translation,66,143,results,results,see that,attention - based models,results see that attention - based models,0.6170386672019958
translation,66,151,results,improvements,of,our model,improvements of our model,0.5960047841072083
translation,66,151,results,improvements,are,significant,improvements are significant,0.6254523992538452
translation,66,151,results,significant,on,chinese and english datasets,significant on chinese and english datasets,0.5054994821548462
translation,66,152,results,recosa model,ability to produce,high quality and diverse responses,recosa model ability to produce high quality and diverse responses,0.7401492595672607
translation,66,152,results,high quality and diverse responses,compared with,baseline methods,high quality and diverse responses compared with baseline methods,0.6413354277610779
translation,66,152,results,results,has,recosa model,results has recosa model,0.5413064360618591
translation,66,158,results,recosa,achieves,preference gains ( win subtracts loss ),recosa achieves preference gains ( win subtracts loss ),0.6678825616836548
translation,66,158,results,"hran , wseq and hvmn",has,recosa,"hran , wseq and hvmn has recosa",0.6348509192466736
translation,66,158,results,preference gains ( win subtracts loss ),has,"10.35 % , 25.86 % and 13.8 %","preference gains ( win subtracts loss ) has 10.35 % , 25.86 % and 13.8 %",0.530493974685669
translation,66,158,results,results,Compared with,"hran , wseq and hvmn","results Compared with hran , wseq and hvmn",0.6526284217834473
translation,66,160,results,improvements,of,our model,improvements of our model,0.5960047841072083
translation,66,160,results,improvements,are,significant,improvements are significant,0.6254523992538452
translation,66,160,results,significant,on,datasets,significant on datasets,0.5514390468597412
translation,66,179,results,wseq,obtains,best score,wseq obtains best score,0.6442567706108093
translation,66,179,results,best score,for,"p@1 , r@1 and f1@1","best score for p@1 , r@1 and f1@1",0.6084496378898621
translation,66,179,results,results,see that,wseq,results see that wseq,0.6674765348434448
translation,66,183,results,recosa,performs,better,recosa performs better,0.6991414427757263
translation,66,183,results,better,in,most cases,better in most cases,0.5555280447006226
translation,66,183,results,hran and wseq,has,recosa,hran and wseq has recosa,0.6453902125358582
translation,66,183,results,results,Compared with,hran and wseq,results Compared with hran and wseq,0.6961597204208374
translation,67,10,ablation-analysis,annotation errors,results in,relative error reduction,annotation errors results in relative error reduction,0.6082607507705688
translation,67,10,ablation-analysis,relative error reduction,of,19.4 and 52 %,relative error reduction of 19.4 and 52 %,0.5627486705780029
translation,67,10,ablation-analysis,relative error reduction,between,19.4 and 52 %,relative error reduction between 19.4 and 52 %,0.6038209199905396
translation,67,10,ablation-analysis,19.4 and 52 %,across,all architectures,19.4 and 52 % across all architectures,0.6790419220924377
translation,67,10,ablation-analysis,ablation analysis,Fixing,annotation errors,ablation analysis Fixing annotation errors,0.7449806928634644
translation,68,140,hyperparameters,foreground topics,chosen among,multiples of five,foreground topics chosen among multiples of five,0.7397041320800781
translation,68,140,hyperparameters,multiples of five,between,number of states,multiples of five between number of states,0.6613366007804871
translation,68,140,hyperparameters,multiples of five,four times,number of states,multiples of five four times number of states,0.6841272115707397
translation,68,140,hyperparameters,weights,for,state transition,weights for state transition,0.6177499890327454
translation,68,140,hyperparameters,state transition,has,) and foreground topics,state transition has ) and foreground topics,0.587571382522583
translation,68,140,hyperparameters,hyperparameters,has,weights,hyperparameters has weights,0.5201958417892456
translation,68,141,hyperparameters,dirichlet hyperparameters,use,"? f = 0.1 , ? a = 0.1 , ? = 0.001","dirichlet hyperparameters use ? f = 0.1 , ? a = 0.1 , ? = 0.001",0.5930573344230652
translation,68,141,hyperparameters,dirichlet hyperparameters,use,"1 , ? b = 1","dirichlet hyperparameters use 1 , ? b = 1",0.5902500152587891
translation,68,141,hyperparameters,"? f = 0.1 , ? a = 0.1 , ? = 0.001",to induce,sparsity,"? f = 0.1 , ? a = 0.1 , ? = 0.001 to induce sparsity",0.637753427028656
translation,68,141,hyperparameters,"1 , ? b = 1",for,uniform distribution,"1 , ? b = 1 for uniform distribution",0.6062997579574585
translation,68,141,hyperparameters,uniform distribution,over,all configurations,uniform distribution over all configurations,0.6981385946273804
translation,68,141,hyperparameters,hyperparameters,For,dirichlet hyperparameters,hyperparameters For dirichlet hyperparameters,0.5347744226455688
translation,68,142,hyperparameters,each corpus,into,five groups,each corpus into five groups,0.5986414551734924
translation,68,142,hyperparameters,three groups,for,training,three groups for training,0.6444557309150696
translation,68,142,hyperparameters,hyperparameters,randomly split,each corpus,hyperparameters randomly split each corpus,0.7446110248565674
translation,68,144,hyperparameters,number of sampling iterations,chosen such that,log-likelihood of the data,number of sampling iterations chosen such that log-likelihood of the data,0.21747322380542755
translation,68,144,hyperparameters,log-likelihood of the data,has,converged,log-likelihood of the data has converged,0.5595375895500183
translation,68,144,hyperparameters,hyperparameters,has,number of sampling iterations,hyperparameters has number of sampling iterations,0.5152260661125183
translation,68,145,hyperparameters,each fold,take,10 samples,each fold take 10 samples,0.6772714257240295
translation,68,145,hyperparameters,each fold,compute,mean and standard deviation,each fold compute mean and standard deviation,0.665661633014679
translation,68,145,hyperparameters,10 samples,during,inference,10 samples during inference,0.6439158320426941
translation,68,145,hyperparameters,10 samples,with,interval,10 samples with interval,0.7066112160682678
translation,68,145,hyperparameters,inference,on,test data,inference on test data,0.5678231120109558
translation,68,145,hyperparameters,interval,of,10 iterations,interval of 10 iterations,0.6425527930259705
translation,68,145,hyperparameters,mean and standard deviation,of,50 samples,mean and standard deviation of 50 samples,0.6158283352851868
translation,68,145,hyperparameters,50 samples,from,all folds,50 samples from all folds,0.6165049076080322
translation,68,145,hyperparameters,hyperparameters,For,each fold,hyperparameters For each fold,0.5679323673248291
translation,68,145,hyperparameters,hyperparameters,compute,mean and standard deviation,hyperparameters compute mean and standard deviation,0.6677944660186768
translation,68,4,model,model,unsupervised model of,dialogue act sequences in conversation,model unsupervised model of dialogue act sequences in conversation,0.6870050430297852
translation,68,5,model,topical themes,as transitioning more slowly,dialogue acts,topical themes as transitioning more slowly dialogue acts,0.7140751481056213
translation,68,5,model,contentrelated words,to focus on,conversational function words,contentrelated words to focus on conversational function words,0.7256143093109131
translation,68,5,model,conversational function words,that signal,dialogue acts,conversational function words that signal dialogue acts,0.6747797131538391
translation,68,5,model,dialogue acts,has,in conversation,dialogue acts has in conversation,0.5794802308082581
translation,68,5,model,model,By modeling,topical themes,model By modeling topical themes,0.7954484820365906
translation,68,5,model,model,de-emphasizes,contentrelated words,model de-emphasizes contentrelated words,0.7458528280258179
translation,68,14,model,model,unsupervised model of,da sequences in conversation,model unsupervised model of da sequences in conversation,0.7451958060264587
translation,68,21,model,two types of language models,learned,foreground language models,two types of language models learned foreground language models,0.6665640473365784
translation,68,21,model,two types of language models,learned,background language models,two types of language models learned background language models,0.6629709601402283
translation,68,21,model,foreground language models,that capture,da - related words,foreground language models that capture da - related words,0.6451930403709412
translation,68,21,model,foreground language models,that capture,background language models,foreground language models that capture background language models,0.6623213887214661
translation,68,21,model,background language models,for,content words,background language models for content words,0.4908505380153656
translation,68,81,model,each latent state,represents,utterance - level da,each latent state represents utterance - level da,0.6528335809707642
translation,68,81,model,utterance - level da,mixture of,foreground topics,utterance - level da mixture of foreground topics,0.6304306387901306
translation,68,81,model,foreground topics,represents,sentence - level da,foreground topics represents sentence - level da,0.6103010177612305
translation,68,8,results,contentrelated words,yields,improvement,contentrelated words yields improvement,0.7317836880683899
translation,68,8,results,improvement,on,cnet corpus,improvement on cnet corpus,0.5231958627700806
translation,68,8,results,speaker tendencies,on,nps corpus,speaker tendencies on nps corpus,0.545306384563446
translation,68,8,results,advantageous,on,nps corpus,advantageous on nps corpus,0.5790916085243225
translation,68,8,results,results,De-emphasizing,contentrelated words,results De-emphasizing contentrelated words,0.7029018998146057
translation,68,8,results,results,utilizing,speaker tendencies,results utilizing speaker tendencies,0.571790874004364
translation,68,164,results,our model,performs,significantly better,our model performs significantly better,0.6192936301231384
translation,68,164,results,our model,performs,marginally better,our model performs marginally better,0.6135770678520203
translation,68,164,results,significantly better,than,baselines,significantly better than baselines,0.6141255497932434
translation,68,164,results,baselines,for,cnet,baselines for cnet,0.6247645020484924
translation,68,164,results,marginally better,for,nps,marginally better for nps,0.6898963451385498
translation,68,164,results,results,has,our model,results has our model,0.5871725678443909
translation,68,180,results,v-measure,by,0.17,v-measure by 0.17,0.5850368142127991
translation,68,180,results,results,has,learned background topics,results has learned background topics,0.565640926361084
translation,68,181,results,background topics,learned without,domain labels,background topics learned without domain labels,0.6375821232795715
translation,68,181,results,background topics,learned with,domain labels ( csm vs. csm + domain ),background topics learned with domain labels ( csm vs. csm + domain ),0.6241707801818848
translation,68,181,results,results,promising that,background topics,results promising that background topics,0.6597673892974854
translation,68,205,results,cnet,benefits little from,speaker preferences ( ? = 1 ),cnet benefits little from speaker preferences ( ? = 1 ),0.7276177406311035
translation,68,205,results,results,has,cnet,results has cnet,0.5886574983596802
translation,68,209,results,our model 's ability,to account for,multi-level structure,our model 's ability to account for multi-level structure,0.6933013200759888
translation,68,209,results,our model 's ability,improves,accuracy,our model 's ability improves accuracy,0.6948699951171875
translation,68,209,results,multi-level structure,improves,accuracy,multi-level structure improves accuracy,0.6883957982063293
translation,68,209,results,accuracy,of,da recognition,accuracy of da recognition,0.6199437975883484
translation,68,209,results,da recognition,for,both corpora ( csm vs. csm - multi-level ),da recognition for both corpora ( csm vs. csm - multi-level ),0.5599570870399475
translation,68,209,results,results,has,our model 's ability,results has our model 's ability,0.552945613861084
translation,68,210,results,nps,where,multilevel structure,nps where multilevel structure,0.6761248707771301
translation,68,210,results,nps,where,multilevel structure,nps where multilevel structure,0.6761248707771301
translation,68,210,results,multilevel structure,is,not explicit,multilevel structure is not explicit,0.61369788646698
translation,68,210,results,improvement,comes from,simple heuristics,improvement comes from simple heuristics,0.65330570936203
translation,68,210,results,simple heuristics,for inferring,multilevel structure,simple heuristics for inferring multilevel structure,0.7434350848197937
translation,68,210,results,multilevel structure,based on,user mentions,multilevel structure based on user mentions,0.6677776575088501
translation,68,210,results,nps,has,improvement,nps has improvement,0.5831604599952698
translation,68,210,results,results,For,nps,results For nps,0.5497944355010986
translation,69,81,ablation-analysis,tagging alone ( - lm ),improve,joint accuracy,tagging alone ( - lm ) improve joint accuracy,0.6739845275878906
translation,69,81,ablation-analysis,joint accuracy,on,mul-tiwoz 2.0,joint accuracy on mul-tiwoz 2.0,0.5681273937225342
translation,69,81,ablation-analysis,joint accuracy,on,auxiliary language modeling ( - tagging ),joint accuracy on auxiliary language modeling ( - tagging ),0.5350773930549622
translation,69,81,ablation-analysis,mul-tiwoz 2.0,by,1.53 %,mul-tiwoz 2.0 by 1.53 %,0.6016405820846558
translation,69,81,ablation-analysis,auxiliary language modeling ( - tagging ),by,2.74 %,auxiliary language modeling ( - tagging ) by 2.74 %,0.547264039516449
translation,69,81,ablation-analysis,ablation analysis,has,tagging alone ( - lm ),ablation analysis has tagging alone ( - lm ),0.5312867760658264
translation,69,80,experiments,several previous state - of - the - art models,including,trade,several previous state - of - the - art models including trade,0.5743351578712463
translation,69,80,experiments,52.04 %,of,joint accuracy,52.04 % of joint accuracy,0.5386849641799927
translation,69,80,experiments,97.26 %,of,slot accuracy,97.26 % of slot accuracy,0.5514937043190002
translation,69,80,experiments,slot accuracy,on,multiwoz 2.0,slot accuracy on multiwoz 2.0,0.5626086592674255
translation,69,80,experiments,+ language modeling,has,significantly outperforms,+ language modeling has significantly outperforms,0.6258342266082764
translation,69,80,experiments,significantly outperforms,has,several previous state - of - the - art models,significantly outperforms has several previous state - of - the - art models,0.5410366654396057
translation,69,77,hyperparameters,both the sizes of hidden states and word embeddings,set to,400,both the sizes of hidden states and word embeddings set to 400,0.6762216091156006
translation,69,77,hyperparameters,multi-task learning model,has,both the sizes of hidden states and word embeddings,multi-task learning model has both the sizes of hidden states and word embeddings,0.535280168056488
translation,69,77,hyperparameters,hyperparameters,In,multi-task learning model,hyperparameters In multi-task learning model,0.4870370626449585
translation,69,78,hyperparameters,batch size,to,8,batch size to 8,0.6451488733291626
translation,69,78,hyperparameters,delay update mechanism,with,different step sizes,delay update mechanism with different step sizes,0.6300515532493591
translation,69,78,hyperparameters,different step sizes,to train,model,different step sizes to train model,0.7286311984062195
translation,69,78,hyperparameters,hyperparameters,set,batch size,hyperparameters set batch size,0.6402808427810669
translation,69,78,hyperparameters,hyperparameters,applied,delay update mechanism,hyperparameters applied delay update mechanism,0.6930258870124817
translation,69,4,model,multi-task learning model,with,simple yet effective utterance tagging technique,multi-task learning model with simple yet effective utterance tagging technique,0.6058496832847595
translation,69,4,model,multi-task learning model,with,bidirectional language model,multi-task learning model with bidirectional language model,0.5836425423622131
translation,69,4,model,auxiliary task,for,task - oriented dialogue state generation,auxiliary task for task - oriented dialogue state generation,0.5588096380233765
translation,69,4,model,model,propose,multi-task learning model,model propose multi-task learning model,0.6281071305274963
translation,69,23,model,bi-directional language modeling module,upstream of,model,bi-directional language modeling module upstream of model,0.6319267153739929
translation,69,23,model,model,as,auxiliary task,model as auxiliary task,0.531260073184967
translation,69,23,model,model,integrate,bi-directional language modeling module,model integrate bi-directional language modeling module,0.6971355676651001
translation,69,61,model,concatenated dialogue context,into,embedding layer,concatenated dialogue context into embedding layer,0.5426434278488159
translation,69,61,model,model,feed,concatenated dialogue context,model feed concatenated dialogue context,0.6682063937187195
translation,69,28,results,both methods,achieve,significant improvements,both methods achieve significant improvements,0.5669340491294861
translation,69,28,results,significant improvements,over,baselines,significant improvements over baselines,0.6777081489562988
translation,69,28,results,significant improvements,in,all evaluation metrics,significant improvements in all evaluation metrics,0.47160646319389343
translation,69,28,results,baselines,in,all evaluation metrics,baselines in all evaluation metrics,0.443045049905777
translation,69,28,results,results,has,both methods,results has both methods,0.46599507331848145
translation,69,82,results,number of delay update steps,on,dst,number of delay update steps on dst,0.5913841724395752
translation,69,82,results,results,shows,impact,results shows impact,0.6381149888038635
translation,69,86,results,performance,of,baseline model,performance of baseline model,0.5962602496147156
translation,69,86,results,drops sharply,increase of,dialogue context length,drops sharply increase of dialogue context length,0.6947523355484009
translation,69,86,results,baseline model,has,drops sharply,baseline model has drops sharply,0.5930835008621216
translation,69,87,results,our model,performs,better,our model performs better,0.6546649932861328
translation,69,87,results,better,than,baseline,better than baseline,0.6157954335212708
translation,69,87,results,baseline,in,all cases,baseline in all cases,0.5950533151626587
translation,69,87,results,results,find that,our model,results find that our model,0.6804299354553223
translation,70,194,ablation-analysis,biggest drop,in,accuracy ( 3.2 percentage point ),biggest drop in accuracy ( 3.2 percentage point ),0.5157943367958069
translation,70,194,ablation-analysis,accuracy ( 3.2 percentage point ),on,uo problem - domain,accuracy ( 3.2 percentage point ) on uo problem - domain,0.5071723461151123
translation,70,194,ablation-analysis,model,trained on,training set,model trained on training set,0.7798627018928528
translation,70,194,ablation-analysis,training set,of,ur problem-domain,training set of ur problem-domain,0.5548528432846069
translation,70,194,ablation-analysis,ablation analysis,has,biggest drop,ablation analysis has biggest drop,0.5887184739112854
translation,70,196,ablation-analysis,accuracy,of,eagrid,accuracy of eagrid,0.6253500580787659
translation,70,196,ablation-analysis,eagrid,on,test set,eagrid on test set,0.591239333152771
translation,70,196,ablation-analysis,test set,of,uo,test set of uo,0.6458000540733337
translation,70,196,ablation-analysis,test set,of,uo,test set of uo,0.6458000540733337
translation,70,196,ablation-analysis,uo,drops from,71.72 %,uo drops from 71.72 %,0.7288690805435181
translation,70,196,ablation-analysis,71.72 %,trained for,uo,71.72 % trained for uo,0.7405295968055725
translation,70,196,ablation-analysis,71.72 %,to,58.7 %,71.72 % to 58.7 %,0.601886510848999
translation,70,196,ablation-analysis,58.7 %,trained for,ur,58.7 % trained for ur,0.7037610411643982
translation,70,196,ablation-analysis,ablation analysis,has,accuracy,ablation analysis has accuracy,0.4860230088233948
translation,70,121,baselines,baseline model,randomly ranks,dialogues,baseline model randomly ranks dialogues,0.7717190384864807
translation,70,121,baselines,dialogues,in,input dialogue -pair,dialogues in input dialogue -pair,0.5589216947555542
translation,70,121,baselines,baselines,has,baseline model,baselines has baseline model,0.5873855352401733
translation,70,122,baselines,cosim,represents,utterances,cosim represents utterances,0.7015498876571655
translation,70,122,baselines,baselines,has,cosim,baselines has cosim,0.5904297828674316
translation,70,125,baselines,baselines,has,"aseq ( gandhe and traum , 2016 )","baselines has aseq ( gandhe and traum , 2016 )",0.5482612252235413
translation,70,129,baselines,baselines,has,"eagrid ( cervone et al. , 2018 )","baselines has eagrid ( cervone et al. , 2018 )",0.5066894888877869
translation,70,130,baselines,best performing model,benefits from,entity and da transitions,best performing model benefits from entity and da transitions,0.7078008651733398
translation,70,130,baselines,entity and da transitions,between,utterances,entity and da transitions between utterances,0.6695425510406494
translation,70,137,baselines,baselines,has,s- dicoh,baselines has s- dicoh,0.6637303829193115
translation,70,138,baselines,dicoh,trained by,only the supervision signal,dicoh trained by only the supervision signal,0.7869442105293274
translation,70,138,baselines,only the supervision signal,for,coherence ranking,only the supervision signal for coherence ranking,0.6254962682723999
translation,70,138,baselines,coherence ranking,with,total loss l,coherence ranking with total loss l,0.5963735580444336
translation,70,138,baselines,coherence model,has,dicoh,coherence model has dicoh,0.6282269954681396
translation,70,140,baselines,baselines,has,m- dicoh,baselines has m- dicoh,0.6567566990852356
translation,70,141,baselines,our full model,trained by,proposed mtl,our full model trained by proposed mtl,0.7404163479804993
translation,70,141,baselines,proposed mtl,using,supervision signals,proposed mtl using supervision signals,0.6441187262535095
translation,70,141,baselines,supervision signals,for,coherence ranking,supervision signals for coherence ranking,0.6104339957237244
translation,70,141,baselines,supervision signals,for,dap,supervision signals for dap,0.6550543904304504
translation,70,165,baselines,s-dicoh,with,aseq and eagrid,s-dicoh with aseq and eagrid,0.6854344606399536
translation,70,165,baselines,s-dicoh,as,baseline models,s-dicoh as baseline models,0.5768592357635498
translation,70,165,baselines,baseline models,that use,da information,baseline models that use da information,0.7274776697158813
translation,70,165,baselines,baselines,compare,s-dicoh,baselines compare s-dicoh,0.7215564250946045
translation,70,282,experimental-setup,optimization,performed by,adam,optimization performed by adam,0.6528224945068359
translation,70,282,experimental-setup,adam,with,default parameter values,adam with default parameter values,0.5808930397033691
translation,70,282,experimental-setup,default parameter values,except for,learning rate,default parameter values except for learning rate,0.6362590193748474
translation,70,282,experimental-setup,experimental setup,has,optimization,experimental setup has optimization,0.5174741744995117
translation,70,283,experimental-setup,model,on,shuffled batches,model on shuffled batches,0.6202744841575623
translation,70,283,experimental-setup,shuffled batches,of,training data,shuffled batches of training data,0.6204999089241028
translation,70,283,experimental-setup,experimental setup,train,model,experimental setup train model,0.6514950394630432
translation,70,286,experimental-setup,tesla p100 gpu,running with,cuda v.10.1,tesla p100 gpu running with cuda v.10.1,0.6651895642280579
translation,70,286,experimental-setup,model,implemented in,pytorch7 framework version 1.1.0,model implemented in pytorch7 framework version 1.1.0,0.6706308126449585
translation,70,286,experimental-setup,experimental setup,has,training procedure,experimental setup has training procedure,0.4894740879535675
translation,70,286,experimental-setup,experimental setup,has,model,experimental setup has model,0.5338840484619141
translation,70,7,experiments,dialogue act prediction,as,auxiliary task,dialogue act prediction as auxiliary task,0.4975075423717499
translation,70,7,experiments,informative utterance representations,for,coherence assessment,informative utterance representations for coherence assessment,0.5836958885192871
translation,70,171,experiments,our proposed mtl approach,effectively leverages,dap task,our proposed mtl approach effectively leverages dap task,0.6809537410736084
translation,70,171,experiments,dap task,to learn,informative utterance vectors,dap task to learn informative utterance vectors,0.5535538196563721
translation,70,171,experiments,informative utterance vectors,for,dialogue coherence assessment,informative utterance vectors for dialogue coherence assessment,0.5912651419639587
translation,70,233,experiments,dailydialog,set,maximum number of epochs,dailydialog set maximum number of epochs,0.6148564219474792
translation,70,233,experiments,dailydialog,set,batch size,dailydialog set batch size,0.6874449253082275
translation,70,233,experiments,dailydialog,for,switchboard,dailydialog for switchboard,0.669802725315094
translation,70,233,experiments,maximum number of epochs,to,20,maximum number of epochs to 20,0.5938987731933594
translation,70,233,experiments,maximum number of epochs,set to,10,maximum number of epochs set to 10,0.7328699827194214
translation,70,233,experiments,batch size,to,128,batch size to 128,0.6251707673072815
translation,70,233,experiments,maximum number of epochs,set to,10,maximum number of epochs set to 10,0.7328699827194214
translation,70,233,experiments,switchboard,has,maximum number of epochs,switchboard has maximum number of epochs,0.5746182799339294
translation,70,149,hyperparameters,utterances,are,zero-padded and masked,utterances are zero-padded and masked,0.6521363258361816
translation,70,149,hyperparameters,hyperparameters,has,utterances,hyperparameters has utterances,0.5495290756225586
translation,70,150,hyperparameters,pretrained glove embeddings,of size,300,pretrained glove embeddings of size 300,0.6533082723617554
translation,70,150,hyperparameters,hyperparameters,use,pretrained glove embeddings,hyperparameters use pretrained glove embeddings,0.5342865586280823
translation,70,151,hyperparameters,cosim model,use,smart english stop word list,cosim model use smart english stop word list,0.6284874081611633
translation,70,151,hyperparameters,smart english stop word list,to eliminate,all stop words,smart english stop word list to eliminate all stop words,0.666343092918396
translation,70,151,hyperparameters,hyperparameters,For,cosim model,hyperparameters For cosim model,0.5589109063148499
translation,70,152,hyperparameters,aseq model,use,bi-grams,aseq model use bi-grams,0.6511815786361694
translation,70,152,hyperparameters,bi-grams,of,da labels,bi-grams of da labels,0.5908034443855286
translation,70,152,hyperparameters,hyperparameters,For,aseq model,hyperparameters For aseq model,0.5652401447296143
translation,70,154,hyperparameters,size,of,hidden states,size of hidden states,0.6102981567382812
translation,70,154,hyperparameters,hidden states,in,lstms,hidden states in lstms,0.5185956954956055
translation,70,154,hyperparameters,lstms,of,utterance module,lstms of utterance module,0.5771582126617432
translation,70,154,hyperparameters,utterance module,is,128,utterance module is 128,0.6162742972373962
translation,70,154,hyperparameters,dialogue module,is,256,dialogue module is 256,0.5944313406944275
translation,70,154,hyperparameters,dicoh,has,size,dicoh has size,0.6484103202819824
translation,70,154,hyperparameters,hyperparameters,In,dicoh,hyperparameters In dicoh,0.5624399185180664
translation,70,155,hyperparameters,parameters,optimized using,adam optimizer,parameters optimized using adam optimizer,0.6625661253929138
translation,70,155,hyperparameters,parameters,have,default values,parameters have default values,0.5581963062286377
translation,70,155,hyperparameters,adam optimizer,where,parameters,adam optimizer where parameters,0.6326077580451965
translation,70,155,hyperparameters,parameters,have,default values,parameters have default values,0.5581963062286377
translation,70,155,hyperparameters,default values,except,learning rate,default values except learning rate,0.6669703722000122
translation,70,155,hyperparameters,learning rate,initiated with,0.0005,learning rate initiated with 0.0005,0.6893079876899719
translation,70,155,hyperparameters,hyperparameters,has,parameters,hyperparameters has parameters,0.4783959984779358
translation,70,156,hyperparameters,dropout layer,with,p = 0.1,dropout layer with p = 0.1,0.6147269010543823
translation,70,156,hyperparameters,p = 0.1,applied to,utterance vectors,p = 0.1 applied to utterance vectors,0.7222930192947388
translation,70,156,hyperparameters,hyperparameters,has,dropout layer,hyperparameters has dropout layer,0.4751329720020294
translation,70,157,hyperparameters,model,for,20 epochs,model for 20 epochs,0.6305356025695801
translation,70,157,hyperparameters,model,for,10 epochs,model for 10 epochs,0.6332400441169739
translation,70,157,hyperparameters,20 epochs,on,dailydialog,20 epochs on dailydialog,0.6011185646057129
translation,70,157,hyperparameters,10 epochs,on,switchboard,10 epochs on switchboard,0.5882128477096558
translation,70,157,hyperparameters,hyperparameters,train,model,hyperparameters train model,0.7064553499221802
translation,70,24,model,utterance encoder,shared between,dap and the di-coh model,utterance encoder shared between dap and the di-coh model,0.6273589134216309
translation,70,27,model,perturbation methods,like,utterance ordering,perturbation methods like utterance ordering,0.6425969004631042
translation,70,27,model,perturbation methods,like,utterance insertion,perturbation methods like utterance insertion,0.6470527648925781
translation,70,27,model,perturbation methods,introduce,two dialoguerelevant perturbations,perturbation methods introduce two dialoguerelevant perturbations,0.595344066619873
translation,70,27,model,two dialoguerelevant perturbations,named,utterance replacement,two dialoguerelevant perturbations named utterance replacement,0.6862945556640625
translation,70,27,model,two dialoguerelevant perturbations,named,even utterance ordering,two dialoguerelevant perturbations named even utterance ordering,0.7005413174629211
translation,70,27,model,model,utilize,perturbation methods,model utilize perturbation methods,0.5852657556533813
translation,70,27,model,model,introduce,two dialoguerelevant perturbations,model introduce two dialoguerelevant perturbations,0.6584255695343018
translation,70,210,model,multi-task learning scenario,where,dialogue act prediction,multi-task learning scenario where dialogue act prediction,0.5345287919044495
translation,70,210,model,model,in,multi-task learning scenario,model in multi-task learning scenario,0.5145745873451233
translation,70,164,results,all the examined problem-domains,on both,dailydialog and switchboard,all the examined problem-domains on both dailydialog and switchboard,0.6769105195999146
translation,70,164,results,s-dicoh,has,strongly outperforms,s-dicoh has strongly outperforms,0.6374998092651367
translation,70,164,results,results,observe,s-dicoh,results observe s-dicoh,0.5741962194442749
translation,70,166,results,s-dicoh,surpasses,models,s-dicoh surpasses models,0.6656043529510498
translation,70,166,results,models,for,all problem-domains,models for all problem-domains,0.5913398265838623
translation,70,166,results,results,has,s-dicoh,results has s-dicoh,0.5601078271865845
translation,70,167,results,s- dicoh,achieves,lower accuracy,s- dicoh achieves lower accuracy,0.642080545425415
translation,70,167,results,lower accuracy,than,models,lower accuracy than models,0.6007663607597351
translation,70,167,results,models,for,all problem - domains,models for all problem - domains,0.5913398265838623
translation,70,167,results,all problem - domains,except,ui,all problem - domains except ui,0.699329674243927
translation,70,167,results,switchboard,has,s- dicoh,switchboard has s- dicoh,0.6681811213493347
translation,70,167,results,results,on,switchboard,results on switchboard,0.5612112879753113
translation,70,170,results,dap,as,auxiliary task,dap as auxiliary task,0.5219576954841614
translation,70,170,results,auxiliary task,to train,dicoh model,auxiliary task to train dicoh model,0.6975151896476746
translation,70,170,results,dicoh model,in,our mtl setup,dicoh model in our mtl setup,0.5573914051055908
translation,70,170,results,dicoh model,observe,m-dicoh,dicoh model observe m-dicoh,0.6419522166252136
translation,70,170,results,"random , cosim , and s-dicoh models",for,all problem-domains,"random , cosim , and s-dicoh models for all problem-domains",0.6026937365531921
translation,70,170,results,all problem-domains,on both,dailydialog and switchboard,all problem-domains on both dailydialog and switchboard,0.6353347301483154
translation,70,170,results,m-dicoh,has,substantially outperforms,m-dicoh has substantially outperforms,0.6305704712867737
translation,70,170,results,substantially outperforms,has,"random , cosim , and s-dicoh models","substantially outperforms has random , cosim , and s-dicoh models",0.5643568634986877
translation,70,170,results,results,employ,dap,results employ dap,0.48716235160827637
translation,70,172,results,aseq and eagrid models,explicitly use,gold da labels,aseq and eagrid models explicitly use gold da labels,0.7261039018630981
translation,70,172,results,gold da labels,during,evaluations,gold da labels during evaluations,0.6391927599906921
translation,70,172,results,m-dicoh,achieves,highest accuracy,m-dicoh achieves highest accuracy,0.6637418270111084
translation,70,172,results,highest accuracy,for,all problem-domains,highest accuracy for all problem-domains,0.5687277913093567
translation,70,172,results,aseq and eagrid models,has,m-dicoh,aseq and eagrid models has m-dicoh,0.6309089660644531
translation,70,172,results,results,Compared with,aseq and eagrid models,results Compared with aseq and eagrid models,0.6467858552932739
translation,70,173,results,s-dicoh,up to,eagrid,s-dicoh up to eagrid,0.6729220747947693
translation,70,173,results,s-dicoh,those of,eagrid,s-dicoh those of eagrid,0.6366819143295288
translation,70,173,results,eagrid,for,uo and euo,eagrid for uo and euo,0.6930309534072876
translation,70,173,results,switchboard,has,m- dicoh,switchboard has m- dicoh,0.6710809469223022
translation,70,173,results,results,on,switchboard,results on switchboard,0.5612112879753113
translation,70,174,results,lower accuracy,than,eagrid,lower accuracy than eagrid,0.6043091416358948
translation,70,174,results,lower accuracy,achieves for,ur,lower accuracy achieves for ur,0.69553142786026
translation,70,174,results,eagrid,achieves for,ur,eagrid achieves for ur,0.6056289672851562
translation,70,177,results,m-dicoh,uses,das,m-dicoh uses das,0.5620654225349426
translation,70,177,results,m-dicoh,performs,almost evenly,m-dicoh performs almost evenly,0.6297197341918945
translation,70,177,results,das,during,training,das during training,0.6066094040870667
translation,70,177,results,better utterance vectors,performs,almost evenly,better utterance vectors performs almost evenly,0.6131927371025085
translation,70,177,results,almost evenly,on,both corpora,almost evenly on both corpora,0.5487459897994995
translation,70,177,results,results,has,m-dicoh,results has m-dicoh,0.558883547782898
translation,70,190,results,all perturbations,to construct,training sets,all perturbations to construct training sets,0.7030030488967896
translation,70,190,results,all perturbations,observe that,m-dicoh,all perturbations observe that m-dicoh,0.5993751287460327
translation,70,190,results,m-dicoh,outperforms,eagrid,m-dicoh outperforms eagrid,0.7104678153991699
translation,70,190,results,eagrid,for,all test perturbations,eagrid for all test perturbations,0.5844123363494873
translation,70,190,results,results,For,all perturbations,results For all perturbations,0.6031035780906677
translation,70,191,results,m-dicoh and eagrid,achieve,highest accuracy,m-dicoh and eagrid achieve highest accuracy,0.6460282802581787
translation,70,191,results,highest accuracy,on,uo,highest accuracy on uo,0.5886211395263672
translation,70,191,results,results,among,all examined perturbations,results among all examined perturbations,0.597524881362915
translation,70,195,results,high- discrepancy,in,accuracy,high- discrepancy in accuracy,0.5321163535118103
translation,70,195,results,accuracy,of,eagrid model,accuracy of eagrid model,0.6112393736839294
translation,70,195,results,eagrid model,for,uo problemdomain,eagrid model for uo problemdomain,0.5912930965423584
translation,70,195,results,model,trained on,training sets,model trained on training sets,0.7834577560424805
translation,70,195,results,training sets,of,different problem-domains,training sets of different problem-domains,0.5572748184204102
translation,70,195,results,results,observe,high- discrepancy,results observe high- discrepancy,0.6139044761657715
translation,70,198,results,m-dicoh model,is,more robust,m-dicoh model is more robust,0.583573043346405
translation,70,198,results,more robust,than,eagrid model,more robust than eagrid model,0.578496515750885
translation,70,198,results,more robust,against,different types of perturbation,more robust against different types of perturbation,0.683650016784668
translation,70,198,results,results,confirm,m-dicoh model,results confirm m-dicoh model,0.5526883602142334
translation,70,207,results,svm - bow model,for,all problem-domains,svm - bow model for all problem-domains,0.5422317981719971
translation,70,207,results,s-dap and m-dap models,has,outperform,s-dap and m-dap models has outperform,0.6026538610458374
translation,70,207,results,outperform,has,svm - bow model,outperform has svm - bow model,0.5878820419311523
translation,70,207,results,results,has,s-dap and m-dap models,results has s-dap and m-dap models,0.49537670612335205
translation,70,211,results,coherence method,performs,on par,coherence method performs on par,0.5948164463043213
translation,70,211,results,for discriminating dialogues,from,various perturbations,for discriminating dialogues from various perturbations,0.6148779988288879
translation,70,211,results,various perturbations,on,dailydialog,various perturbations on dailydialog,0.6294799447059631
translation,70,211,results,on par,on,switchboard,on par on switchboard,0.6794735193252563
translation,70,211,results,coherence method,has,outperforms,coherence method has outperforms,0.6297979354858398
translation,70,211,results,outperforms,has,counterparts,outperforms has counterparts,0.6345208287239075
translation,70,211,results,counterparts,has,for discriminating dialogues,counterparts has for discriminating dialogues,0.5899642109870911
translation,70,211,results,results,has,coherence method,results has coherence method,0.530463695526123
translation,70,212,results,our model,benefits from,dialogue act prediction task,our model benefits from dialogue act prediction task,0.6034414768218994
translation,70,212,results,dialogue act prediction task,during,training,dialogue act prediction task during training,0.6435613036155701
translation,70,212,results,gold dialogue act labels,during,evaluations,gold dialogue act labels during evaluations,0.607643723487854
translation,70,212,results,results,has,our model,results has our model,0.5871725678443909
translation,70,271,results,m-dap model,works on par,s-dap model,m-dap model works on par s-dap model,0.7220335006713867
translation,70,271,results,results,observe,m-dap model,results observe m-dap model,0.5867272615432739
translation,71,19,experiments,very large-scale persona- based dialogue dataset,using,conversations,very large-scale persona- based dialogue dataset using conversations,0.647171139717102
translation,71,19,experiments,conversations,previously extracted from,reddit,conversations previously extracted from reddit,0.6077219247817993
translation,71,98,experiments,reddit lstms,use,hidden size,reddit lstms use hidden size,0.5743681788444519
translation,71,98,experiments,reddit lstms,concatenate,last hidden states,reddit lstms concatenate last hidden states,0.7034321427345276
translation,71,98,experiments,hidden size,of,150,hidden size of 150,0.6804213523864746
translation,71,98,experiments,last hidden states,for,both directions and layers,last hidden states for both directions and layers,0.5760771036148071
translation,71,98,experiments,final representation,of size,600,final representation of size 600,0.7426729202270508
translation,71,99,experiments,transformer architectures,on,reddit,transformer architectures on reddit,0.5988753437995911
translation,71,99,experiments,reddit,use,4 layers,reddit use 4 layers,0.6648741364479065
translation,71,99,experiments,4 layers,with,hidden size,4 layers with hidden size,0.6304432153701782
translation,71,99,experiments,hidden size,of,300 and 6 attention heads,hidden size of 300 and 6 attention heads,0.6106060147285461
translation,71,99,experiments,final representation,of size,300,final representation of size 300,0.7367262840270996
translation,71,100,experiments,spacy,for,part- of-speech tagging,spacy for part- of-speech tagging,0.6119807362556458
translation,71,100,experiments,part- of-speech tagging,to verify,persona extraction rules,part- of-speech tagging to verify persona extraction rules,0.6406753063201904
translation,71,96,hyperparameters,network parameters,using,adamax,network parameters using adamax,0.6469053626060486
translation,71,96,hyperparameters,adamax,with,learning rate,adamax with learning rate,0.6678062677383423
translation,71,96,hyperparameters,adamax,with,mini-batches,adamax with mini-batches,0.695901095867157
translation,71,96,hyperparameters,adamax,on,mini-batches,adamax on mini-batches,0.6023136377334595
translation,71,96,hyperparameters,learning rate,of,8e? 4,learning rate of 8e? 4,0.6376015543937683
translation,71,96,hyperparameters,learning rate,on,mini-batches,learning rate on mini-batches,0.5600284934043884
translation,71,96,hyperparameters,mini-batches,size,512,mini-batches size 512,0.7759993672370911
translation,71,96,hyperparameters,hyperparameters,optimize,network parameters,hyperparameters optimize network parameters,0.6422207355499268
translation,71,97,hyperparameters,embeddings,with,fasttext word vectors,embeddings with fasttext word vectors,0.6119444370269775
translation,71,97,hyperparameters,embeddings,during,learning,embeddings during learning,0.6399372816085815
translation,71,97,hyperparameters,hyperparameters,initialize,embeddings,hyperparameters initialize embeddings,0.7610669732093811
translation,71,20,results,simple heuristics,create,corpus,simple heuristics create corpus,0.6462777853012085
translation,71,20,results,corpus,of,over 5 million personas,corpus of over 5 million personas,0.5863823294639587
translation,71,20,results,over 5 million personas,spanning,more than 700 million conversations,over 5 million personas spanning more than 700 million conversations,0.69053053855896
translation,71,20,results,results,With,simple heuristics,results With simple heuristics,0.6635344624519348
translation,71,22,results,outperform,has,counterparts that do not have access to personas,outperform has counterparts that do not have access to personas,0.656460702419281
translation,71,23,results,coverage,of,our dataset,coverage of our dataset,0.60971999168396
translation,71,23,results,coverage,seems,very good,coverage seems very good,0.7172911763191223
translation,71,23,results,our dataset,seems,very good,our dataset seems very good,0.678658664226532
translation,71,23,results,results,has,coverage,results has coverage,0.5053455829620361
translation,72,125,ablation-analysis,performance significantly,with,6.6 % drop,performance significantly with 6.6 % drop,0.6599271893501282
translation,72,125,ablation-analysis,6.6 % drop,in,average,6.6 % drop in average,0.5632533431053162
translation,72,125,ablation-analysis,random sampling strategy,has,hurts,random sampling strategy has hurts,0.566607654094696
translation,72,125,ablation-analysis,hurts,has,performance significantly,hurts has performance significantly,0.5973554253578186
translation,72,125,ablation-analysis,ablation analysis,adopting,random sampling strategy,ablation analysis adopting random sampling strategy,0.6217950582504272
translation,72,126,ablation-analysis,entire graph branch,of,grade,entire graph branch of grade,0.5973244309425354
translation,72,126,ablation-analysis,k-hop neighboring representations,used for,initializing,k-hop neighboring representations used for initializing,0.6217570304870605
translation,72,126,ablation-analysis,node representations,in,dialogue graph,node representations in dialogue graph,0.5429985523223877
translation,72,126,ablation-analysis,node representations,in,dialogue graph,node representations in dialogue graph,0.5429985523223877
translation,72,126,ablation-analysis,node representations,in,dialogue graph,node representations in dialogue graph,0.5429985523223877
translation,72,126,ablation-analysis,hop-attention weights,used for computing,weight,hop-attention weights used for computing weight,0.6665995121002197
translation,72,126,ablation-analysis,weight,for,each edge,weight for each edge,0.6143221259117126
translation,72,126,ablation-analysis,each edge,in,dialogue graph,each edge in dialogue graph,0.5510033369064331
translation,72,126,ablation-analysis,initializing,has,node representations,initializing has node representations,0.5299545526504517
translation,72,126,ablation-analysis,ablation analysis,remove,entire graph branch,ablation analysis remove entire graph branch,0.7116669416427612
translation,72,126,ablation-analysis,ablation analysis,remove,k-hop neighboring representations,ablation analysis remove k-hop neighboring representations,0.6491822004318237
translation,72,126,ablation-analysis,ablation analysis,has,does the graph work ?,ablation analysis has does the graph work ?,0.5925494432449341
translation,72,130,ablation-analysis,graph information,result in,relatively poor performance,graph information result in relatively poor performance,0.6609184145927429
translation,72,130,ablation-analysis,too much,has,graph information,too much has graph information,0.5616388320922852
translation,72,130,ablation-analysis,ablation analysis,considering,too much,ablation analysis considering too much,0.6698681116104126
translation,72,100,experimental-setup,gat layer,set as,3,gat layer set as 3,0.6339503526687622
translation,72,100,experimental-setup,number of heads,is,4,number of heads is 4,0.6014258861541748
translation,72,100,experimental-setup,graph reasoning module,has,gat layer,graph reasoning module has gat layer,0.565026044845581
translation,72,100,experimental-setup,graph reasoning module,has,number of heads,graph reasoning module has number of heads,0.5551514625549316
translation,72,100,experimental-setup,experimental setup,For,graph reasoning module,experimental setup For graph reasoning module,0.5421003103256226
translation,72,101,experimental-setup,grade,use,"adam ( kingma and ba , 2014 )","grade use adam ( kingma and ba , 2014 )",0.6372679471969604
translation,72,101,experimental-setup,"adam ( kingma and ba , 2014 )",with,"? 1 = 0.9 , ? 2 = 0.999","adam ( kingma and ba , 2014 ) with ? 1 = 0.9 , ? 2 = 0.999",0.6422287821769714
translation,72,101,experimental-setup,"adam ( kingma and ba , 2014 )",with,learning rate,"adam ( kingma and ba , 2014 ) with learning rate",0.5989180207252502
translation,72,101,experimental-setup,"adam ( kingma and ba , 2014 )",set,batch size,"adam ( kingma and ba , 2014 ) set batch size",0.6491147875785828
translation,72,101,experimental-setup,"adam ( kingma and ba , 2014 )",set,learning rate,"adam ( kingma and ba , 2014 ) set learning rate",0.6035323143005371
translation,72,101,experimental-setup,batch size,as,16,batch size as 16,0.5711418390274048
translation,72,101,experimental-setup,learning rate,as,2e - 5,learning rate as 2e - 5,0.5745194554328918
translation,72,101,experimental-setup,experimental setup,To train,grade,experimental setup To train grade,0.6690216064453125
translation,72,102,experimental-setup,grade,implemented with,natural language processing toolkit,grade implemented with natural language processing toolkit,0.6932333111763
translation,72,102,experimental-setup,grade,implemented with,texar-pytorch,grade implemented with texar-pytorch,0.7101474404335022
translation,72,102,experimental-setup,experimental setup,has,grade,experimental setup has grade,0.5163329839706421
translation,72,7,model,topic-level dialogue graph,propose,new evaluation metric grade,topic-level dialogue graph propose new evaluation metric grade,0.6520025730133057
translation,72,7,model,new evaluation metric grade,stands for,graph-enhanced representations,new evaluation metric grade stands for graph-enhanced representations,0.6837092638015747
translation,72,7,model,graph-enhanced representations,for,automatic dialogue evaluation,graph-enhanced representations for automatic dialogue evaluation,0.5618057250976562
translation,72,7,model,model,Capitalized on,topic-level dialogue graph,model Capitalized on topic-level dialogue graph,0.6451637148857117
translation,72,8,model,grade,incorporates,coarsegrained utterance -level contextualized representations,grade incorporates coarsegrained utterance -level contextualized representations,0.7217127084732056
translation,72,8,model,grade,incorporates,fine- grained topic-level graph representations,grade incorporates fine- grained topic-level graph representations,0.6912280917167664
translation,72,8,model,fine- grained topic-level graph representations,to evaluate,dialogue coherence,fine- grained topic-level graph representations to evaluate dialogue coherence,0.6360669732093811
translation,72,8,model,model,has,grade,model has grade,0.5913273096084595
translation,72,24,model,metric,for,open-domain dialogue systems,metric for open-domain dialogue systems,0.5917301177978516
translation,72,24,model,metric,named as,graph-enhanced representation,metric named as graph-enhanced representation,0.6674551963806152
translation,72,24,model,graph-enhanced representation,for,automatic dialogue evaluation ( grade ),graph-enhanced representation for automatic dialogue evaluation ( grade ),0.6160804033279419
translation,72,24,model,graph-enhanced representation,explicitly models,topic transition dynamics,graph-enhanced representation explicitly models topic transition dynamics,0.7748238444328308
translation,72,24,model,topic transition dynamics,by reasoning over,dialogue graphs,topic transition dynamics by reasoning over dialogue graphs,0.7172926068305969
translation,72,24,model,topic transition dynamics,incorporates them into,utterance - level contextualized representations,topic transition dynamics incorporates them into utterance - level contextualized representations,0.6863762736320496
translation,72,26,model,grade,consists of,two semantic extraction branches,grade consists of two semantic extraction branches,0.70022052526474
translation,72,26,model,model,has,grade,model has grade,0.5913273096084595
translation,72,27,model,"bert ( devlin et al. , 2019 )",to learn,coarsegrained utterance - level contextualized representations,"bert ( devlin et al. , 2019 ) to learn coarsegrained utterance - level contextualized representations",0.6017215251922607
translation,72,27,model,fine- grained topiclevel graph representations,by constructing,topiclevel dialogue graphs,fine- grained topiclevel graph representations by constructing topiclevel dialogue graphs,0.6966617703437805
translation,72,27,model,fine- grained topiclevel graph representations,applying,graph neural network,fine- grained topiclevel graph representations applying graph neural network,0.6038587689399719
translation,72,27,model,graph neural network,on,graphs,graph neural network on graphs,0.5462315082550049
translation,72,27,model,graph neural network,to model,topic transition dynamics,graph neural network to model topic transition dynamics,0.664006769657135
translation,72,27,model,graphs,to model,topic transition dynamics,graphs to model topic transition dynamics,0.6631532311439514
translation,72,28,model,nodes and edges,by utilizing,evidence,nodes and edges by utilizing evidence,0.7230144739151001
translation,72,28,model,evidence,from,commonsense knowledge graph,evidence from commonsense knowledge graph,0.5507665276527405
translation,72,28,model,evidence,including,hop-attention weights,evidence including hop-attention weights,0.716816246509552
translation,72,28,model,model,determine,nodes and edges,model determine nodes and edges,0.6541056632995605
translation,72,29,model,grade,trained in,unsupervised manner,grade trained in unsupervised manner,0.7703966498374939
translation,72,29,model,unsupervised manner,with,data,unsupervised manner with data,0.6961236596107483
translation,72,29,model,data,automatically generated by,negative sampling strategy,data automatically generated by negative sampling strategy,0.7107495069503784
translation,72,29,model,negative sampling strategy,considering,lexical and semantic aspects,negative sampling strategy considering lexical and semantic aspects,0.6736475825309753
translation,72,29,model,lexical and semantic aspects,rather than,random sampling,lexical and semantic aspects rather than random sampling,0.6385851502418518
translation,72,29,model,model,has,grade,model has grade,0.5913273096084595
translation,72,98,model,transformer -ranker and transformer - generator,from,parlai platform,transformer -ranker and transformer - generator from parlai platform,0.6055976152420044
translation,72,98,model,transformer -ranker and transformer - generator,latter is,generation - based,transformer -ranker and transformer - generator latter is generation - based,0.7037801146507263
translation,72,98,model,model,deploy,transformer -ranker and transformer - generator,model deploy transformer -ranker and transformer - generator,0.7259773015975952
translation,72,113,results,test set,of,dailydialog dataset,test set of dailydialog dataset,0.5417847633361816
translation,72,113,results,our grade,obtains,highest correlations,our grade obtains highest correlations,0.620907723903656
translation,72,113,results,highest correlations,with,human judgements,highest correlations with human judgements,0.6211099624633789
translation,72,113,results,results,has,test set,results has test set,0.5966588854789734
translation,72,114,results,spearman value,of,grade,spearman value of grade,0.555522084236145
translation,72,114,results,spearman value,of,grade,spearman value of grade,0.555522084236145
translation,72,114,results,spearman value,of,grade,spearman value of grade,0.555522084236145
translation,72,114,results,grade,on,transformer - ranker,grade on transformer - ranker,0.5944242477416992
translation,72,114,results,bleurt,trained on,very large-scale dataset,bleurt trained on very large-scale dataset,0.6447563767433167
translation,72,114,results,averaged correlation result,of,grade,averaged correlation result of grade,0.5839095115661621
translation,72,119,results,grade,better at evaluating,coherence,grade better at evaluating coherence,0.7497569918632507
translation,72,119,results,significantly outperforms,on,human correlations,significantly outperforms on human correlations,0.5322086811065674
translation,72,119,results,baseline metrics,on,human correlations,baseline metrics on human correlations,0.5099917650222778
translation,72,119,results,grade,better at evaluating,coherence,grade better at evaluating coherence,0.7497569918632507
translation,72,119,results,coherence,of,high-quality responses,coherence of high-quality responses,0.6010556221008301
translation,72,119,results,grade,has,significantly outperforms,grade has significantly outperforms,0.659275472164154
translation,72,119,results,significantly outperforms,has,baseline metrics,significantly outperforms has baseline metrics,0.5943341255187988
translation,72,119,results,results,has,grade,results has grade,0.5094258189201355
translation,72,120,results,scatter plots,against,human judgements,scatter plots against human judgements,0.6833619475364685
translation,72,120,results,human judgements,for,dialogpt,human judgements for dialogpt,0.6547929644584656
translation,72,120,results,dialogpt,on,convai2 dataset,dialogpt on convai2 dataset,0.5598471760749817
translation,72,120,results,results,illustrates,scatter plots,results illustrates scatter plots,0.6335084438323975
translation,72,121,results,scores,predicted by,grade,scores predicted by grade,0.7174510955810547
translation,72,121,results,scores,closer to,human scores,scores closer to human scores,0.6293145418167114
translation,72,121,results,human scores,than,baseline metrics,human scores than baseline metrics,0.48986464738845825
translation,72,129,results,both the 1 st hop and the 2 nd hop neighboring nodes,brings,best performance,both the 1 st hop and the 2 nd hop neighboring nodes brings best performance,0.5824208855628967
translation,72,129,results,results,incorporating,both the 1 st hop and the 2 nd hop neighboring nodes,results incorporating both the 1 st hop and the 2 nd hop neighboring nodes,0.6820033192634583
translation,73,157,ablation-analysis,remarkable improvements,achieved,dialogue generation,remarkable improvements achieved dialogue generation,0.701935350894928
translation,73,157,ablation-analysis,remarkable improvements,for,dialogue generation,remarkable improvements for dialogue generation,0.6039613485336304
translation,73,157,ablation-analysis,discrete latent variables,has,remarkable improvements,discrete latent variables has remarkable improvements,0.5428819060325623
translation,73,157,ablation-analysis,ablation analysis,incorporation of,discrete latent variables,ablation analysis incorporation of discrete latent variables,0.6759666800498962
translation,73,186,ablation-analysis,flexible attention mechanism,to fully leverage,bi-directional context information,flexible attention mechanism to fully leverage bi-directional context information,0.6748987436294556
translation,73,188,ablation-analysis,discrete latent variable,able to,boost,discrete latent variable able to boost,0.6344303488731384
translation,73,188,ablation-analysis,boost,has,quality of response generation,boost has quality of response generation,0.5889389514923096
translation,73,124,baselines,dstc7 - avsd,provided,baseline system,dstc7 - avsd provided baseline system,0.678036630153656
translation,73,124,baselines,baseline system,built upon,hierarchical recurrent encoders,baseline system built upon hierarchical recurrent encoders,0.6399352550506592
translation,73,124,baselines,hierarchical recurrent encoders,with,multi-modal features,hierarchical recurrent encoders with multi-modal features,0.5989797711372375
translation,73,124,baselines,baselines,has,dstc7 - avsd,baselines has dstc7 - avsd,0.5646464824676514
translation,73,95,experimental-setup,maximum sequence length,of,context and response,maximum sequence length of context and response,0.6204602122306824
translation,73,95,experimental-setup,context and response,set to,256 and 50,context and response set to 256 and 50,0.7389428615570068
translation,73,95,experimental-setup,experimental setup,has,maximum sequence length,experimental setup has maximum sequence length,0.5124795436859131
translation,73,96,experimental-setup,number of transformer blocks,in,our model l,number of transformer blocks in our model l,0.5235745310783386
translation,73,96,experimental-setup,our model l,is,12,our model l is 12,0.6566454768180847
translation,73,96,experimental-setup,hidden embedding dimension d,is,768,hidden embedding dimension d is 768,0.5922977328300476
translation,73,96,experimental-setup,experimental setup,has,number of transformer blocks,experimental setup has number of transformer blocks,0.5158156156539917
translation,73,96,experimental-setup,experimental setup,has,hidden embedding dimension d,experimental setup has hidden embedding dimension d,0.526721715927124
translation,73,97,experimental-setup,batch size,set to,64,batch size set to 64,0.7451491355895996
translation,73,97,experimental-setup,k,set to,20,k set to 20,0.7227612733840942
translation,73,97,experimental-setup,20,for,discrete latent variable,20 for discrete latent variable,0.623609185218811
translation,73,97,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,73,98,experimental-setup,"adam optimizer ( kingma and ba , 2015 )",employed for,optimization,"adam optimizer ( kingma and ba , 2015 ) employed for optimization",0.6585124731063843
translation,73,98,experimental-setup,"adam optimizer ( kingma and ba , 2015 )",with,learning rate,"adam optimizer ( kingma and ba , 2015 ) with learning rate",0.5968794822692871
translation,73,98,experimental-setup,optimization,with,learning rate,optimization with learning rate,0.5936277508735657
translation,73,98,experimental-setup,learning rate,of,5e - 5,learning rate of 5e - 5,0.6587256789207458
translation,73,98,experimental-setup,experimental setup,has,"adam optimizer ( kingma and ba , 2015 )","experimental setup has adam optimizer ( kingma and ba , 2015 )",0.5367816686630249
translation,73,99,experimental-setup,pretraining,of,dialogue generation,pretraining of dialogue generation,0.5295075178146362
translation,73,99,experimental-setup,dialogue generation,carried out on,8 nvidia telsa v100 32g gpu cards,dialogue generation carried out on 8 nvidia telsa v100 32g gpu cards,0.6707392930984497
translation,73,99,experimental-setup,8 nvidia telsa v100 32g gpu cards,for,3.5 m steps,8 nvidia telsa v100 32g gpu cards for 3.5 m steps,0.5800663828849792
translation,73,99,experimental-setup,experimental setup,has,pretraining,experimental setup has pretraining,0.5295986533164978
translation,73,123,experiments,sequence to sequence with attention ( seq2seq ),employed as,baseline,sequence to sequence with attention ( seq2seq ) employed as baseline,0.6531190276145935
translation,73,123,experiments,baseline,for,experiments,baseline for experiments,0.6254751682281494
translation,73,123,experiments,experiments,on,persona - chat,experiments on persona - chat,0.536399781703949
translation,73,123,experiments,experiments,on,daily dialog,experiments on daily dialog,0.5300652980804443
translation,73,5,model,novel dialogue generation pre-training framework,to support,various kinds of conversations,novel dialogue generation pre-training framework to support various kinds of conversations,0.6120673418045044
translation,73,5,model,various kinds of conversations,including,chit-chat,various kinds of conversations including chit-chat,0.6626766920089722
translation,73,5,model,various kinds of conversations,including,knowledge grounded dialogues,various kinds of conversations including knowledge grounded dialogues,0.6681354641914368
translation,73,5,model,various kinds of conversations,including,conversational question answering,various kinds of conversations including conversational question answering,0.643692135810852
translation,73,5,model,model,propose,novel dialogue generation pre-training framework,model propose novel dialogue generation pre-training framework,0.649255096912384
translation,73,6,model,model,adopt,flexible attention mechanisms,model adopt flexible attention mechanisms,0.6403948068618774
translation,73,7,model,discrete latent variables,to tackle,inherent one - to - many mapping problem,discrete latent variables to tackle inherent one - to - many mapping problem,0.6908023357391357
translation,73,7,model,inherent one - to - many mapping problem,in,response generation,inherent one - to - many mapping problem in response generation,0.5337826013565063
translation,73,7,model,model,introduce,discrete latent variables,model introduce discrete latent variables,0.6772387623786926
translation,73,18,model,new method,to obtain,highquality pre-training model,new method to obtain highquality pre-training model,0.5733964443206787
translation,73,18,model,highquality pre-training model,for,dialogue generation,highquality pre-training model for dialogue generation,0.5772420763969421
translation,73,18,model,model,propose,new method,model propose new method,0.675626814365387
translation,73,19,model,twitter conversations,utilized to further pre-train,generation model,twitter conversations utilized to further pre-train generation model,0.7565917372703552
translation,73,19,model,distributions,has,large-scale reddit,distributions has large-scale reddit,0.5611909627914429
translation,73,20,model,flexible paradigm,integrating,uni-and bi-directional processing,flexible paradigm integrating uni-and bi-directional processing,0.7270088791847229
translation,73,21,model,discrete latent variable,to model,one-to -many relationship,discrete latent variable to model one-to -many relationship,0.7116622924804688
translation,73,21,model,one-to -many relationship,among,utterances,one-to -many relationship among utterances,0.6173287034034729
translation,73,21,model,utterances,in,conversations,utterances in conversations,0.5681173205375671
translation,73,34,model,discrete latent variables,into,transformer blocks,discrete latent variables into transformer blocks,0.6264256834983826
translation,73,34,model,transformer blocks,for,one - to -many relationship modeling,transformer blocks for one - to -many relationship modeling,0.6171244382858276
translation,73,34,model,one - to -many relationship modeling,where,two reciprocal tasks,one - to -many relationship modeling where two reciprocal tasks,0.5870997905731201
translation,73,34,model,two reciprocal tasks,of,response generation,two reciprocal tasks of response generation,0.5468607544898987
translation,73,34,model,two reciprocal tasks,of,latent act recognition,two reciprocal tasks of latent act recognition,0.5505170822143555
translation,73,34,model,model,encode,discrete latent variables,model encode discrete latent variables,0.7158136963844299
translation,73,129,results,"team of cmu ( sanabria et al. , 2019 )",obtains,best performance,"team of cmu ( sanabria et al. , 2019 ) obtains best performance",0.5335770845413208
translation,73,129,results,best performance,across,all the evaluation metrics,best performance across all the evaluation metrics,0.6587475538253784
translation,73,129,results,dstc7 - avsd,has,"team of cmu ( sanabria et al. , 2019 )","dstc7 - avsd has team of cmu ( sanabria et al. , 2019 )",0.5747843980789185
translation,73,129,results,results,In,dstc7 - avsd,results In dstc7 - avsd,0.5212840437889099
translation,73,153,results,our method,achieves,better performance,our method achieves better performance,0.6458921432495117
translation,73,153,results,better performance,across,all the metrics,better performance across all the metrics,0.6493945121765137
translation,73,153,results,all the metrics,on,persona - chat and daily dialog,all the metrics on persona - chat and daily dialog,0.528596043586731
translation,73,153,results,human evaluations,has,our method,human evaluations has our method,0.5126962661743164
translation,73,153,results,results,During,human evaluations,results During human evaluations,0.5898391604423523
translation,73,155,results,informativeness assessments,indicate,information,informativeness assessments indicate information,0.613714873790741
translation,73,155,results,information,in,our generated responses,information in our generated responses,0.5077726244926453
translation,73,155,results,information,is,significantly richer,information is significantly richer,0.5749650001525879
translation,73,155,results,significantly richer,compared with,baseline methods,significantly richer compared with baseline methods,0.7010888457298279
translation,73,155,results,results,has,informativeness assessments,results has informativeness assessments,0.5246465802192688
translation,73,156,results,our responses,coherent with,context,our responses coherent with context,0.7552385926246643
translation,73,156,results,our responses,favored most by,crowd-sourcing workers,our responses favored most by crowd-sourcing workers,0.5895055532455444
translation,73,156,results,results,has,our responses,results has our responses,0.5679654479026794
translation,73,158,results,generation quality,of,transformed - based approaches,generation quality of transformed - based approaches,0.5927698612213135
translation,73,158,results,generation quality,is,significantly better,generation quality is significantly better,0.5459669828414917
translation,73,158,results,significantly better,than,rnnbased methods ( seq2seq and ivae mi ),significantly better than rnnbased methods ( seq2seq and ivae mi ),0.5565415024757385
translation,73,162,results,our method,brought,new breakthrough,our method brought new breakthrough,0.6496108770370483
translation,73,162,results,new breakthrough,for,dstc7 - avsd,new breakthrough for dstc7 - avsd,0.6657931804656982
translation,73,162,results,results,demonstrate,our method,results demonstrate our method,0.6011868715286255
translation,74,20,baselines,opendial,is,"java-based , domain-independent framework","opendial is java-based , domain-independent framework",0.5504463911056519
translation,74,20,baselines,"java-based , domain-independent framework",for developing,probabilistic rule- based dialogue systems,"java-based , domain-independent framework for developing probabilistic rule- based dialogue systems",0.6254894733428955
translation,74,20,baselines,baselines,has,opendial,baselines has opendial,0.5730884671211243
translation,74,4,model,novel abstraction framework,called,fastdial,novel abstraction framework called fastdial,0.667807936668396
translation,74,4,model,fastdial,for designing,task oriented dialogue agents,fastdial for designing task oriented dialogue agents,0.6883478164672852
translation,74,6,model,generic and simple frame-slots data-structure,with,pre-defined dialogue policies,generic and simple frame-slots data-structure with pre-defined dialogue policies,0.6184326410293579
translation,74,6,model,fast design and implementation,price of,flexibility reduction,fast design and implementation price of flexibility reduction,0.7154867053031921
translation,74,6,model,model,use,generic and simple frame-slots data-structure,model use generic and simple frame-slots data-structure,0.6394862532615662
translation,74,21,model,dialogue policies,in,generic dialogue model,dialogue policies in generic dialogue model,0.49055683612823486
translation,74,46,model,architecture,of,dialogue agent,architecture of dialogue agent,0.5335074663162231
translation,74,46,model,dialogue agent,consists of,5 components,dialogue agent consists of 5 components,0.6793487668037415
translation,74,46,model,model,has,architecture,model has architecture,0.5575731992721558
translation,75,127,ablation-analysis,attnflow,shows,consistent performance drop,attnflow shows consistent performance drop,0.7120656371116638
translation,75,127,ablation-analysis,consistent performance drop,of,about 30 %,consistent performance drop of about 30 %,0.5752596855163574
translation,75,127,ablation-analysis,about 30 %,then,attnio - as,about 30 % then attnio - as,0.6011338233947754
translation,75,127,ablation-analysis,ablation analysis,note,attnflow,ablation analysis note attnflow,0.6031837463378906
translation,75,135,ablation-analysis,third ablation model,where,dialog - kg alignment,third ablation model where dialog - kg alignment,0.6003404259681702
translation,75,135,ablation-analysis,albert initialization,of,node embedding,albert initialization of node embedding,0.533383309841156
translation,75,135,ablation-analysis,albert initialization,leading to,performance gain,albert initialization leading to performance gain,0.633955717086792
translation,75,135,ablation-analysis,performance gain,of,about 2 %,performance gain of about 2 %,0.5683648586273193
translation,75,135,ablation-analysis,about 2 %,in,path@1,about 2 % in path@1,0.6199593544006348
translation,75,135,ablation-analysis,node embedding,has,helps,node embedding has helps,0.5936812162399292
translation,75,135,ablation-analysis,ablation analysis,find in,third ablation model,ablation analysis find in third ablation model,0.6276084184646606
translation,75,109,baselines,4 models,as,baselines,4 models as baselines,0.5357531905174255
translation,75,109,baselines,2019 ),as,baselines,2019 ) as baselines,0.5955974459648132
translation,75,109,baselines,baselines,take,4 models,baselines take 4 models,0.6559545397758484
translation,75,110,baselines,state - of - the - art model,designed to traverse,dialogue conditioned knowledge path,state - of - the - art model designed to traverse dialogue conditioned knowledge path,0.7415326833724976
translation,75,110,baselines,di-alkg walker,has,state - of - the - art model,di-alkg walker has state - of - the - art model,0.5634574890136719
translation,75,111,baselines,seq2seq,),"extended encoder-decoder ( parthasarathi and pineau , 2018 )","seq2seq ) extended encoder-decoder ( parthasarathi and pineau , 2018 )",0.6156461238861084
translation,75,111,baselines,tri-lstm,modified to fit,entity path retrieval task,tri-lstm modified to fit entity path retrieval task,0.6930031180381775
translation,75,130,baselines,"gru ( cho et al. , 2014 )",as,dialog encoder,"gru ( cho et al. , 2014 ) as dialog encoder",0.48052892088890076
translation,75,130,baselines,dialog encoder,in replace of,albert,dialog encoder in replace of albert,0.6858262419700623
translation,75,66,hyperparameters,maximum,of,3 last utterances,maximum of 3 last utterances,0.5819447636604309
translation,75,66,hyperparameters,3 last utterances,in,dialog,3 last utterances in dialog,0.5516283512115479
translation,75,66,hyperparameters,input,to,pretrained albert,input to pretrained albert,0.5481482744216919
translation,75,66,hyperparameters,hyperparameters,concatenate,maximum,hyperparameters concatenate maximum,0.7250223755836487
translation,75,6,model,rich structural information,in,kg,rich structural information in kg,0.5890764594078064
translation,75,6,model,rich structural information,based on,two directions of attention flows,rich structural information based on two directions of attention flows,0.6161797642707825
translation,75,6,model,attnio,has,new dialog-conditioned path traversal model,attnio has new dialog-conditioned path traversal model,0.5480247735977173
translation,75,6,model,model,present,attnio,model present attnio,0.5956850647926331
translation,75,7,model,at - tnio,capable of exploring,broad range,at - tnio capable of exploring broad range,0.7608181834220886
translation,75,7,model,at - tnio,learns to,flexibly adjust,at - tnio learns to flexibly adjust,0.7119303941726685
translation,75,7,model,varying range of plausible nodes and edges,to,attend,varying range of plausible nodes and edges to attend,0.5452778935432434
translation,75,7,model,attend,depending on,dialog context,attend depending on dialog context,0.746566891670227
translation,75,7,model,attention flows,has,at - tnio,attention flows has at - tnio,0.6619207262992859
translation,75,7,model,broad range,has,of multi-hop knowledge paths,broad range has of multi-hop knowledge paths,0.5579653382301331
translation,75,7,model,flexibly adjust,has,varying range of plausible nodes and edges,flexibly adjust has varying range of plausible nodes and edges,0.5616944432258606
translation,75,7,model,model,Through,attention flows,model Through attention flows,0.6531507968902588
translation,75,28,model,attnio ( attention inflow and out-flow ),has,novel kg path traversal model,attnio ( attention inflow and out-flow ) has novel kg path traversal model,0.5636522173881531
translation,75,28,model,model,propose,attnio ( attention inflow and out-flow ),model propose attnio ( attention inflow and out-flow ),0.6435050964355469
translation,75,29,model,attnio,models,kg traversal mechanism,attnio models kg traversal mechanism,0.7224641442298889
translation,75,29,model,kg traversal mechanism,into,two subprocesses,kg traversal mechanism into two subprocesses,0.5802228450775146
translation,75,120,results,outperforms,when supervised with,all entities,outperforms when supervised with all entities,0.68318772315979
translation,75,120,results,all baselines,in,path@k and tgt@k,all baselines in path@k and tgt@k,0.5589346289634705
translation,75,120,results,all baselines,when supervised with,all entities,all baselines when supervised with all entities,0.6123828887939453
translation,75,120,results,all entities,in,each path,all entities in each path,0.5263525247573853
translation,75,120,results,all entities,as,label ( attnio - as ),all entities as label ( attnio - as ),0.5310776233673096
translation,75,120,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,75,120,results,outperforms,has,all baselines,outperforms has all baselines,0.5888515114784241
translation,75,121,results,attnio - as,shows,significantly better performance,attnio - as shows significantly better performance,0.7186639904975891
translation,75,121,results,significantly better performance,in,metrics,significantly better performance in metrics,0.5038874745368958
translation,75,121,results,metrics,with,small k,metrics with small k,0.6392752528190613
translation,75,121,results,results,has,attnio - as,results has attnio - as,0.5882160663604736
translation,75,131,results,performance,of,attnio,performance of attnio,0.6418291330337524
translation,75,131,results,attnio,with,gru,attnio with gru,0.6852071285247803
translation,75,131,results,slightly degrades,from,albert,slightly degrades from albert,0.634624183177948
translation,75,131,results,gru,has,slightly degrades,gru has slightly degrades,0.6247293949127197
translation,75,131,results,outperforms,has,all existing models,outperforms has all existing models,0.5855481028556824
translation,75,143,results,relation path accuracy,of,attnio,relation path accuracy of attnio,0.6458542346954346
translation,75,143,results,attnio,in,both supervision setting,attnio in both supervision setting,0.5553662180900574
translation,75,143,results,results,has,relation path accuracy,results has relation path accuracy,0.5706284642219543
translation,75,144,results,relation path accuracy,clearly higher than,entity path accuracy,relation path accuracy clearly higher than entity path accuracy,0.6233649849891663
translation,75,144,results,entity path accuracy,implying,generalization capability,entity path accuracy implying generalization capability,0.6147907972335815
translation,75,144,results,results,has,relation path accuracy,results has relation path accuracy,0.5706284642219543
translation,76,69,baselines,vanilla models,has,seq2seq model with attention,vanilla models has seq2seq model with attention,0.5609644651412964
translation,76,69,baselines,baselines,has,vanilla models,baselines has vanilla models,0.5697152018547058
translation,76,105,baselines,seq2seq model,with,attention,seq2seq model with attention,0.6613655686378479
translation,76,105,baselines,seq2seq,has,seq2seq model,seq2seq has seq2seq model,0.5813721418380737
translation,76,105,baselines,pointer,has,pointer generative network,pointer has pointer generative network,0.5620248913764954
translation,76,105,baselines,pac,has,pick- and - combine model,pac has pick- and - combine model,0.5849663019180298
translation,76,105,baselines,baselines,has,seq2seq,baselines has seq2seq,0.5571820139884949
translation,76,160,baselines,mmi,has,state- of- the - art single - turn response generation model,mmi has state- of- the - art single - turn response generation model,0.4964138865470886
translation,76,164,baselines,smn,is,state - of - the - art multi-turn reranking modeling,smn is state - of - the - art multi-turn reranking modeling,0.5032315850257874
translation,76,164,baselines,sequential matching network ( smn ),is,state - of - the - art multi-turn reranking modeling,sequential matching network ( smn ) is state - of - the - art multi-turn reranking modeling,0.5207788944244385
translation,76,164,baselines,smn,has,sequential matching network ( smn ),smn has sequential matching network ( smn ),0.579326868057251
translation,76,164,baselines,baselines,has,smn,baselines has smn,0.5925542712211609
translation,76,107,hyperparameters,vocabulary size,set to,10k,vocabulary size set to 10k,0.7259076237678528
translation,76,107,hyperparameters,training,has,vocabulary size,training has vocabulary size,0.551749050617218
translation,76,107,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
translation,76,108,hyperparameters,size,of,each mini-batch,size of each mini-batch,0.6241683959960938
translation,76,108,hyperparameters,each mini-batch,is,64,each mini-batch is 64,0.6054544448852539
translation,76,108,hyperparameters,hyperparameters,has,size,hyperparameters has size,0.5313372611999512
translation,76,109,hyperparameters,parameters,updated by,"adam algorithm ( p and ba , 2014 )","parameters updated by adam algorithm ( p and ba , 2014 )",0.6510010361671448
translation,76,109,hyperparameters,"adam algorithm ( p and ba , 2014 )",with,betas,"adam algorithm ( p and ba , 2014 ) with betas",0.6483756899833679
translation,76,109,hyperparameters,"adam algorithm ( p and ba , 2014 )",with,eps,"adam algorithm ( p and ba , 2014 ) with eps",0.630944013595581
translation,76,109,hyperparameters,betas,set to,0.9 and 0.999,betas set to 0.9 and 0.999,0.701303243637085
translation,76,109,hyperparameters,eps,set to,1e - 8,eps set to 1e - 8,0.7258744239807129
translation,76,109,hyperparameters,hyperparameters,has,parameters,hyperparameters has parameters,0.4783959984779358
translation,76,110,hyperparameters,learning rate,is,0.25,learning rate is 0.25,0.5759779214859009
translation,76,110,hyperparameters,clipping threshold,of,gradients,clipping threshold of gradients,0.5807937979698181
translation,76,110,hyperparameters,gradients,is,0.1,gradients is 0.1,0.5540143251419067
translation,76,110,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,76,110,hyperparameters,hyperparameters,has,clipping threshold,hyperparameters has clipping threshold,0.5107904076576233
translation,76,111,hyperparameters,dropout rate,set to,0.5,dropout rate set to 0.5,0.6910414695739746
translation,76,111,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,76,112,hyperparameters,decoder hidden size,set to,512,decoder hidden size set to 512,0.7136318683624268
translation,76,112,hyperparameters,hyperparameters,has,"word embedding size , encoder hidden size","hyperparameters has word embedding size , encoder hidden size",0.5284706354141235
translation,76,113,hyperparameters,checkpoint,with,smallest validation loss,checkpoint with smallest validation loss,0.6391408443450928
translation,76,113,hyperparameters,beam-search size,set to,5,beam-search size set to 5,0.7838178873062134
translation,76,113,hyperparameters,inference stage,has,checkpoint,inference stage has checkpoint,0.5701022148132324
translation,76,113,hyperparameters,inference stage,has,beam-search size,inference stage has beam-search size,0.552829384803772
translation,76,113,hyperparameters,hyperparameters,During,inference stage,hyperparameters During inference stage,0.6481112241744995
translation,76,114,hyperparameters,pick stage,use,bert model,pick stage use bert model,0.6325177550315857
translation,76,114,hyperparameters,bert model,trained on,200g high-quality news data,bert model trained on 200g high-quality news data,0.7173593044281006
translation,76,114,hyperparameters,200g high-quality news data,from,tencent ai lab,200g high-quality news data from tencent ai lab,0.5466556549072266
translation,76,114,hyperparameters,hyperparameters,For,pick stage,hyperparameters For pick stage,0.6201674342155457
translation,76,34,model,multi-turn dialogue systems,to model,relation,multi-turn dialogue systems to model relation,0.7101735472679138
translation,76,34,model,relation,between,query ( original utterance ) and context ( previous utterances ) explicitly,relation between query ( original utterance ) and context ( previous utterances ) explicitly,0.6154669523239136
translation,76,34,model,relation,in,supervised manner,relation in supervised manner,0.556317150592804
translation,76,34,model,query ( original utterance ) and context ( previous utterances ) explicitly,in,supervised manner,query ( original utterance ) and context ( previous utterances ) explicitly in supervised manner,0.502955436706543
translation,76,34,model,model,facilitates,multi-turn dialogue systems,model facilitates multi-turn dialogue systems,0.607249915599823
translation,76,82,model,first stage,is,pick process,first stage is pick process,0.6249937415122986
translation,76,82,model,pick process,identifies,omitted words,pick process identifies omitted words,0.7274421453475952
translation,76,82,model,omitted words,in,previous utterances,omitted words in previous utterances,0.5134717226028442
translation,76,82,model,model,has,first stage,model has first stage,0.5593366026878357
translation,76,84,model,pre-trained deep bidirectional transformers,for,language understanding model ( bert ),pre-trained deep bidirectional transformers for language understanding model ( bert ),0.593036413192749
translation,76,84,model,pre-trained deep bidirectional transformers,as,classifier,pre-trained deep bidirectional transformers as classifier,0.5514389872550964
translation,76,84,model,classifier,to select,omitted words,classifier to select omitted words,0.6833683848381042
translation,76,84,model,omitted words,from,previous utterances,omitted words from previous utterances,0.5585927367210388
translation,76,84,model,model,fine - tune,pre-trained deep bidirectional transformers,model fine - tune pre-trained deep bidirectional transformers,0.7115522623062134
translation,76,106,model,one layer unidirectional lstm,as,encoder and decoder,one layer unidirectional lstm as encoder and decoder,0.5667533874511719
translation,76,106,model,encoder and decoder,of,each model,encoder and decoder of each model,0.6050872802734375
translation,76,106,model,model,adopt,one layer unidirectional lstm,model adopt one layer unidirectional lstm,0.6299209594726562
translation,76,131,results,pac model,higher than,other models,pac model higher than other models,0.7333174347877502
translation,76,131,results,other models,in,restoration precision,other models in restoration precision,0.47955548763275146
translation,76,131,results,results,has,pac model,results has pac model,0.5682077407836914
translation,76,132,results,words,selected by,pac model,words selected by pac model,0.6563034057617188
translation,76,132,results,more accurate,than,other models,more accurate than other models,0.5875072479248047
translation,76,132,results,results,demonstrates that,words,results demonstrates that words,0.5827991962432861
translation,76,134,results,pac model,achieves,highest scores,pac model achieves highest scores,0.6859298944473267
translation,76,134,results,results,has,pac model,results has pac model,0.5682077407836914
translation,76,138,results,highest quality score,among,all methods,highest quality score among all methods,0.5238277912139893
translation,76,138,results,proposed method,has,highest quality score,proposed method has highest quality score,0.5403251647949219
translation,76,138,results,results,has,proposed method,results has proposed method,0.5845219492912292
translation,76,139,results,model,with,higher score,model with higher score,0.6472979187965393
translation,76,139,results,higher score,in,human evaluation,higher score in human evaluation,0.4679203927516937
translation,76,139,results,human evaluation,has,higher restoration score,human evaluation has higher restoration score,0.5418748259544373
translation,76,139,results,results,find,model,results find model,0.5691511034965515
translation,76,185,results,results,of,single-turn and multi-turn dialogue systems,results of single-turn and multi-turn dialogue systems,0.5236530303955078
translation,76,186,results,both systems,get,better responses,both systems get better responses,0.5935021042823792
translation,76,186,results,better responses,after,restoration process,better responses after restoration process,0.6745719313621521
translation,76,186,results,pac model,is,most effective one,pac model is most effective one,0.601689875125885
translation,76,186,results,results,has,both systems,results has both systems,0.5481469035148621
translation,76,187,results,pac model,achieves,lowest nr,pac model achieves lowest nr,0.7064732909202576
translation,76,187,results,results,has,pac model,results has pac model,0.5682077407836914
translation,76,188,results,more than 50 % queries,get,better responses,more than 50 % queries get better responses,0.6101611256599426
translation,76,188,results,better responses,when,restored,better responses when restored,0.7077869176864624
translation,76,188,results,only 10 %,get,worse responses,only 10 % get worse responses,0.6017950177192688
translation,76,188,results,single- turn dialogue systems,has,more than 50 % queries,single- turn dialogue systems has more than 50 % queries,0.5965166091918945
translation,76,188,results,results,for,single- turn dialogue systems,results for single- turn dialogue systems,0.577346682548523
translation,76,189,results,response quality,of,nearly half of quires,response quality of nearly half of quires,0.5812363028526306
translation,76,189,results,% responses,get,worse,% responses get worse,0.6275335550308228
translation,76,189,results,multi-turn dialogue systems,has,restoration,multi-turn dialogue systems has restoration,0.5098897218704224
translation,76,189,results,restoration,has,improves,restoration has improves,0.6356985569000244
translation,76,189,results,improves,has,response quality,improves has response quality,0.5338258147239685
translation,76,189,results,results,For,multi-turn dialogue systems,results For multi-turn dialogue systems,0.5722296237945557
translation,77,105,experimental-setup,experimental setup,set,discount factor,experimental setup set discount factor,0.596992015838623
translation,77,106,experimental-setup,buffer size,of,b u and b h,buffer size of b u and b h,0.6380947828292847
translation,77,106,experimental-setup,b u and b h,set to,2000 and 2000 ?k planning steps,b u and b h set to 2000 and 2000 ?k planning steps,0.7404997944831848
translation,77,106,experimental-setup,experimental setup,has,buffer size,experimental setup has buffer size,0.5316854119300842
translation,77,107,experimental-setup,batch size,is,16,batch size is 16,0.643535315990448
translation,77,107,experimental-setup,learning rate,is,0.001,learning rate is 0.001,0.5655806064605713
translation,77,107,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,77,107,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,77,108,experimental-setup,gradient explosion,applied,gradient clipping,gradient explosion applied gradient clipping,0.6613385677337646
translation,77,108,experimental-setup,gradient clipping,on,all the model parameters,gradient clipping on all the model parameters,0.5228217244148254
translation,77,108,experimental-setup,gradient clipping,to,maximum norm = 1,gradient clipping to maximum norm = 1,0.51426100730896
translation,77,108,experimental-setup,all the model parameters,to,maximum norm = 1,all the model parameters to maximum norm = 1,0.5291882157325745
translation,77,108,experimental-setup,experimental setup,To prevent,gradient explosion,experimental setup To prevent gradient explosion,0.6406909227371216
translation,77,112,experimental-setup,optimizer,for,all the neural networks,optimizer for all the neural networks,0.6323617696762085
translation,77,112,experimental-setup,optimizer,is,"rmsprop ( hinton et al. , 2012 )","optimizer is rmsprop ( hinton et al. , 2012 )",0.4526212215423584
translation,77,112,experimental-setup,all the neural networks,is,"rmsprop ( hinton et al. , 2012 )","all the neural networks is rmsprop ( hinton et al. , 2012 )",0.4971281588077545
translation,77,112,experimental-setup,experimental setup,has,optimizer,experimental setup has optimizer,0.5528271794319153
translation,77,113,experimental-setup,maximum length,of,simulated dialogue,maximum length of simulated dialogue,0.6102182865142822
translation,77,113,experimental-setup,simulated dialogue,is,40,simulated dialogue is 40,0.570190966129303
translation,77,113,experimental-setup,experimental setup,has,maximum length,experimental setup has maximum length,0.4922432005405426
translation,77,117,experimental-setup,batch size,for,collecting experiences,batch size for collecting experiences,0.6627069115638733
translation,77,117,experimental-setup,collecting experiences,is,10,collecting experiences is 10,0.5826270580291748
translation,77,117,experimental-setup,simulated experience tuples,stored into,buffers,simulated experience tuples stored into buffers,0.6893922090530396
translation,77,117,experimental-setup,buffers,at,every episode,buffers at every episode,0.5627411007881165
translation,77,117,experimental-setup,10 ? ( k ? 1 ),has,simulated experience tuples,10 ? ( k ? 1 ) has simulated experience tuples,0.5972117781639099
translation,77,117,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,77,120,experimental-setup,mlps,with,one shared hidden layer,mlps with one shared hidden layer,0.63348788022995
translation,77,120,experimental-setup,mlps,with,hyperbolic -tangent activation,mlps with hyperbolic -tangent activation,0.6189426779747009
translation,77,120,experimental-setup,mlps,with,one encoding layer,mlps with one encoding layer,0.6284217238426208
translation,77,120,experimental-setup,one shared hidden layer,of size,160,one shared hidden layer of size 160,0.7556899189949036
translation,77,120,experimental-setup,one encoding layer,of,hidden size,one encoding layer of hidden size,0.6062642335891724
translation,77,120,experimental-setup,hidden size,for,each state and action input,hidden size for each state and action input,0.5795261859893799
translation,77,120,experimental-setup,80,for,each state and action input,80 for each state and action input,0.6357800960540771
translation,77,120,experimental-setup,hidden size,has,80,hidden size has 80,0.6196455359458923
translation,77,122,experimental-setup,lstm cell,utilized,hidden size,lstm cell utilized hidden size,0.586651086807251
translation,77,122,experimental-setup,hidden size,is,128,hidden size is 128,0.6033065915107727
translation,77,122,experimental-setup,proposed d3q framework,has,lstm cell,proposed d3q framework has lstm cell,0.5718956589698792
translation,77,122,experimental-setup,proposed d3q framework,has,hidden size,proposed d3q framework has hidden size,0.5683655738830566
translation,77,122,experimental-setup,experimental setup,In,proposed d3q framework,experimental setup In proposed d3q framework,0.5219524502754211
translation,77,4,model,model,presents,discriminative deep dyna -q ( d3q ) approach,model presents discriminative deep dyna -q ( d3q ) approach,0.6363053321838379
translation,77,5,model,ddq 's,incorporate,rnn - based discriminator,ddq 's incorporate rnn - based discriminator,0.6811789870262146
translation,77,5,model,rnn - based discriminator,in,d3q,rnn - based discriminator in d3q,0.555325448513031
translation,77,5,model,rnn - based discriminator,to differentiate,simulated experience,rnn - based discriminator to differentiate simulated experience,0.6873814463615417
translation,77,5,model,simulated experience,from,real user experience,simulated experience from real user experience,0.5727709531784058
translation,77,5,model,simulated experience,to control,quality of training data,simulated experience to control quality of training data,0.7384768724441528
translation,77,5,model,model,To obviate,ddq 's,model To obviate ddq 's,0.7464123964309692
translation,77,5,model,model,incorporate,rnn - based discriminator,model incorporate rnn - based discriminator,0.6758962273597717
translation,77,28,model,effectiveness of planning,proposes,discriminative deep dyna - q ( d3q ),effectiveness of planning proposes discriminative deep dyna - q ( d3q ),0.6403332948684692
translation,77,28,model,generative adversarial network ( gan ),incorporates,discriminator,generative adversarial network ( gan ) incorporates discriminator,0.658324658870697
translation,77,28,model,discriminator,into,planning process,discriminator into planning process,0.5845789313316345
translation,77,28,model,model,proposes,discriminative deep dyna - q ( d3q ),model proposes discriminative deep dyna - q ( d3q ),0.6516388058662415
translation,77,29,model,discriminator,trained to differentiate,simulated experiences,discriminator trained to differentiate simulated experiences,0.7662659883499146
translation,77,29,model,simulated experiences,from,real user experiences,simulated experiences from real user experiences,0.5633155107498169
translation,77,29,model,model,has,discriminator,model has discriminator,0.5425825715065002
translation,77,166,model,discriminative deep dyna - q ( d3q ),for,task - completion dialogue policy learning,discriminative deep dyna - q ( d3q ) for task - completion dialogue policy learning,0.5782755613327026
translation,77,167,model,discriminator,as,judge,discriminator as judge,0.5869168043136597
translation,77,167,model,proposed approach,capable of controlling,quality of simulated experience,proposed approach capable of controlling quality of simulated experience,0.7289589643478394
translation,77,167,model,quality of simulated experience,generated in,planning phase,quality of simulated experience generated in planning phase,0.6847594380378723
translation,77,167,model,quality of simulated experience,enables,efficient and robust dialogue policy learning,quality of simulated experience enables efficient and robust dialogue policy learning,0.6747472882270813
translation,77,167,model,discriminator,has,proposed approach,discriminator has proposed approach,0.5928699970245361
translation,77,167,model,judge,has,proposed approach,judge has proposed approach,0.6026321649551392
translation,77,167,model,model,With,discriminator,model With discriminator,0.6345618367195129
translation,77,138,results,proposed d3q framework,demonstrates,robustness,proposed d3q framework demonstrates robustness,0.6441070437431335
translation,77,138,results,robustness,to,number of planning steps,robustness to number of planning steps,0.5764142274856567
translation,77,138,results,results,has,proposed d3q framework,results has proposed d3q framework,0.586646556854248
translation,77,139,results,d3q,has,outperforms,d3q has outperforms,0.6714920401573181
translation,77,139,results,outperforms,has,ddq original setting,outperforms has ddq original setting,0.5929850339889526
translation,77,139,results,outperforms,has,d3q,outperforms has d3q,0.6701436638832092
translation,77,139,results,outperforms,has,without training discriminator,outperforms has without training discriminator,0.633044421672821
translation,77,139,results,d3q,has,without training discriminator,d3q has without training discriminator,0.6132944822311401
translation,77,139,results,results,shows,d3q,results shows d3q,0.6448447704315186
translation,77,149,results,d3q,demonstrating,robustness,d3q demonstrating robustness,0.8156566619873047
translation,77,149,results,d3q,has,significantly outperforms,d3q has significantly outperforms,0.6215999722480774
translation,77,149,results,significantly outperforms,has,baseline methods,significantly outperforms has baseline methods,0.5802999138832092
translation,77,149,results,results,show that,d3q,results show that d3q,0.4774656891822815
translation,77,150,results,d3q,shows,remarkable learning efficiency,d3q shows remarkable learning efficiency,0.6310932040214539
translation,77,150,results,remarkable learning efficiency,while extending,domain,remarkable learning efficiency while extending domain,0.6482968330383301
translation,77,150,results,even outperforms,has,dqn,even outperforms has dqn,0.6161103844642639
translation,77,150,results,even outperforms,has,5 ),even outperforms has 5 ),0.6901842951774597
translation,77,150,results,dqn,has,5 ),dqn has 5 ),0.6336674690246582
translation,77,150,results,results,has,d3q,results has d3q,0.5489767789840698
translation,77,164,results,other agents,in,both stages,other agents in both stages,0.5553533434867859
translation,77,164,results,proposed d3q,has,significantly outperforms,proposed d3q has significantly outperforms,0.6017339825630188
translation,77,164,results,significantly outperforms,has,other agents,significantly outperforms has other agents,0.5748934149742126
translation,78,4,model,joint modeling approach,to identify,salient discussion points,joint modeling approach to identify salient discussion points,0.7081511616706848
translation,78,4,model,joint modeling approach,to label,discourse relations,joint modeling approach to label discourse relations,0.6432018876075745
translation,78,4,model,salient discussion points,in,spoken meetings,salient discussion points in spoken meetings,0.49954336881637573
translation,78,4,model,discourse relations,between,speaker turns,discourse relations between speaker turns,0.5965204834938049
translation,78,4,model,model,present,joint modeling approach,model present joint modeling approach,0.6513820290565491
translation,78,32,model,joint modeling approach,to select,salient phrases,joint modeling approach to select salient phrases,0.6758297681808472
translation,78,32,model,salient phrases,reflecting,key discussion points,salient phrases reflecting key discussion points,0.7058230638504028
translation,78,32,model,discourse relations,between,speaker turns,discourse relations between speaker turns,0.5965204834938049
translation,78,32,model,speaker turns,in,spoken meetings,speaker turns in spoken meetings,0.5363196730613708
translation,78,32,model,model,propose,joint modeling approach,model propose joint modeling approach,0.6632049679756165
translation,78,253,model,svm classifier,trained with,ngram features ( unigrams and bigrams ),svm classifier trained with ngram features ( unigrams and bigrams ),0.7209190726280212
translation,78,253,model,model,consider,svm classifier,model consider svm classifier,0.6775648593902588
translation,78,8,results,classifiers,trained with,features,classifiers trained with features,0.7308230400085449
translation,78,8,results,features,constructed from,our model,features constructed from our model,0.6836021542549133
translation,78,8,results,features,achieve,significant better predictive performance,features achieve significant better predictive performance,0.6249409914016724
translation,78,8,results,significant better predictive performance,than,state - of- the- art,significant better predictive performance than state - of- the- art,0.5394004583358765
translation,78,8,results,results,has,classifiers,results has classifiers,0.5618173480033875
translation,78,41,results,our model,yields,accuracy,our model yields accuracy,0.6982574462890625
translation,78,41,results,accuracy,of,63.2,accuracy of 63.2,0.5735957026481628
translation,78,41,results,63.2,on,phrase selection,63.2 on phrase selection,0.49375781416893005
translation,78,41,results,significantly better,than,classifier,significantly better than classifier,0.591775119304657
translation,78,41,results,classifier,based on,support vector machines ( svm ),classifier based on support vector machines ( svm ),0.674032986164093
translation,78,41,results,results,show,our model,results show our model,0.6888449192047119
translation,78,42,results,discourse prediction component,obtains,better accuracy,discourse prediction component obtains better accuracy,0.6160958409309387
translation,78,42,results,better accuracy,than,state - of - the - art neural networkbased approach,better accuracy than state - of - the - art neural networkbased approach,0.5657969117164612
translation,78,42,results,state - of - the - art neural networkbased approach,has,59.2 vs. 54.2 ),state - of - the - art neural networkbased approach has 59.2 vs. 54.2 ),0.5528775453567505
translation,78,42,results,results,has,discourse prediction component,results has discourse prediction component,0.5177952647209167
translation,78,43,results,our model,trained with,latent discourse,our model trained with latent discourse,0.7039742469787598
translation,78,43,results,svms,on,ami and icsi corpora,svms on ami and icsi corpora,0.5165128707885742
translation,78,43,results,ami and icsi corpora,for,phrase selection,ami and icsi corpora for phrase selection,0.5739017724990845
translation,78,43,results,latent discourse,has,outperforms,latent discourse has outperforms,0.628045916557312
translation,78,43,results,outperforms,has,svms,outperforms has svms,0.6074677109718323
translation,78,43,results,results,has,our model,results has our model,0.5871725678443909
translation,78,256,results,all svms,trained with,our features,all svms trained with our features,0.7378230094909668
translation,78,256,results,our features,surpass,ngrams - based baseline,our features surpass ngrams - based baseline,0.698771059513092
translation,78,256,results,results,has,all svms,results has all svms,0.5329278111457825
translation,78,257,results,discourse features,has,word entrainment feature,discourse features has word entrainment feature,0.521684467792511
translation,78,257,results,discourse features,has,significantly outperform,discourse features has significantly outperform,0.5996804237365723
translation,78,257,results,significantly outperform,has,state - of - theart system,significantly outperform has state - of - theart system,0.5882492661476135
translation,79,4,model,text - based speaker identification,on,multiparty dialogues,text - based speaker identification on multiparty dialogues,0.5246337056159973
translation,79,23,results,best model,using,multi-document convolutional neural network,best model using multi-document convolutional neural network,0.6111129522323608
translation,79,23,results,best model,shows,accuracy,best model shows accuracy,0.6915348768234253
translation,79,23,results,multi-document convolutional neural network,shows,accuracy,multi-document convolutional neural network shows accuracy,0.6487860679626465
translation,79,23,results,multi-document convolutional neural network,shows,macro average f1 score,multi-document convolutional neural network shows macro average f1 score,0.6305496692657471
translation,79,23,results,accuracy,of,31.06 %,accuracy of 31.06 %,0.5647176504135132
translation,79,23,results,macro average f1 score,of,29.72,macro average f1 score of 29.72,0.5370242595672607
translation,79,23,results,results,has,best model,results has best model,0.5634682774543762
translation,79,91,results,cosine similarities,between,any given utterance,cosine similarities between any given utterance,0.643135666847229
translation,79,91,results,15 nearest neighbors,are,consistently above 98 %,15 nearest neighbors are consistently above 98 %,0.5769961476325989
translation,79,93,results,basic cnn model,able to outperform,baseline,basic cnn model able to outperform baseline,0.6991050839424133
translation,79,93,results,baseline,by,almost 9 %,baseline by almost 9 %,0.5986401438713074
translation,79,93,results,results,has,basic cnn model,results has basic cnn model,0.5676403045654297
translation,79,95,results,experiment,on,utterance concatenation dataset,experiment on utterance concatenation dataset,0.49325692653656006
translation,79,95,results,utterance concatenation dataset,yields,relatively high identification accuracy,utterance concatenation dataset yields relatively high identification accuracy,0.7101955413818359
translation,79,96,results,prediction labels,speakers present in,scene,prediction labels speakers present in scene,0.6587239503860474
translation,79,96,results,accuracy boosts,of,10 % and 12 %,accuracy boosts of 10 % and 12 %,0.6225116848945618
translation,79,96,results,10 % and 12 %,achieved on,two datasets,10 % and 12 % achieved on two datasets,0.6425671577453613
translation,79,96,results,results,When,prediction labels,results When prediction labels,0.6174597144126892
translation,80,124,results,m2 m,compares favorably to,dstc2,m2 m compares favorably to dstc2,0.6994820237159729
translation,80,124,results,dstc2,ratio of,unique unigrams and bigrams,dstc2 ratio of unique unigrams and bigrams,0.6626186370849609
translation,80,124,results,unique unigrams and bigrams,to,total number of tokens,unique unigrams and bigrams to total number of tokens,0.5321279764175415
translation,80,124,results,total number of tokens,in,dataset,total number of tokens in dataset,0.510399580001831
translation,80,124,results,results,has,m2 m,results has m2 m,0.5681145787239075
translation,80,142,results,end-toend optimization,with,rl,end-toend optimization with rl,0.6865880489349365
translation,80,142,results,agent,according to,human judges,agent according to human judges,0.6736600995063782
translation,80,142,results,agent,trained with,only supervised learning,agent trained with only supervised learning,0.7904626727104187
translation,80,142,results,only supervised learning,on,dataset,only supervised learning on dataset,0.5075705647468567
translation,80,142,results,results,has,end-toend optimization,results has end-toend optimization,0.5066066980361938
translation,80,148,results,results,has,end -to - end optimization with rl,results has end -to - end optimization with rl,0.559966504573822
translation,82,125,ablation-analysis,recurrence,improves,bleu score,recurrence improves bleu score,0.6901862025260925
translation,82,125,ablation-analysis,ablation analysis,incorporation of,recurrence,ablation analysis incorporation of recurrence,0.6730543971061707
translation,82,138,ablation-analysis,r t term,contributes,most,r t term contributes most,0.7073568105697632
translation,82,138,ablation-analysis,most,to,improvement,most to improvement,0.5886449813842773
translation,82,138,ablation-analysis,ablation analysis,has,r t term,ablation analysis has r t term,0.5299316048622131
translation,82,124,baselines,second one,introduces,additional recurrence,second one introduces additional recurrence,0.6302427053451538
translation,82,124,baselines,additional recurrence,to model,dependency,additional recurrence to model dependency,0.7195350527763367
translation,82,124,baselines,dependency,on,dialogue history,dependency on dialogue history,0.6061421036720276
translation,82,124,baselines,two baseline models,has,first,two baseline models has first,0.5622084140777588
translation,82,102,hyperparameters,early stopping,implemented based on,validation set,early stopping implemented based on validation set,0.7157714366912842
translation,82,102,hyperparameters,validation set,for,regularisation,validation set for regularisation,0.5988136529922485
translation,82,102,hyperparameters,gradient clipping,set to,1,gradient clipping set to 1,0.626135528087616
translation,82,102,hyperparameters,hyperparameters,has,early stopping,hyperparameters has early stopping,0.5431009531021118
translation,82,103,hyperparameters,hidden layer sizes,set to,50,hidden layer sizes set to 50,0.721225917339325
translation,82,103,hyperparameters,randomly initialised,between,- 0.3 and 0.3,randomly initialised between - 0.3 and 0.3,0.6773866415023804
translation,82,103,hyperparameters,- 0.3 and 0.3,including,word embeddings,- 0.3 and 0.3 including word embeddings,0.7198673486709595
translation,82,103,hyperparameters,hyperparameters,has,hidden layer sizes,hyperparameters has hidden layer sizes,0.48641785979270935
translation,82,103,hyperparameters,hyperparameters,has,weights,hyperparameters has weights,0.5201958417892456
translation,82,104,hyperparameters,vocabulary size,is,500,vocabulary size is 500,0.61527019739151
translation,82,104,hyperparameters,vocabulary size,around,500,vocabulary size around 500,0.7043612003326416
translation,82,104,hyperparameters,500,for,both input and output,500 for both input and output,0.6681917905807495
translation,82,104,hyperparameters,both input and output,in which,rare words,both input and output in which rare words,0.6224147081375122
translation,82,104,hyperparameters,words,can be,delexicalised,words can be delexicalised,0.704552412033081
translation,82,104,hyperparameters,hyperparameters,has,vocabulary size,hyperparameters has vocabulary size,0.5002733469009399
translation,82,105,hyperparameters,three convolutional layers,for,cnns,three convolutional layers for cnns,0.549941897392273
translation,82,105,hyperparameters,three convolutional layers,for,filter sizes,three convolutional layers for filter sizes,0.5706402063369751
translation,82,105,hyperparameters,filter sizes,set to,3,filter sizes set to 3,0.7449553608894348
translation,82,6,model,model,introduce,"neural network - based text - in , textout end-to - end trainable goal-oriented dialogue system","model introduce neural network - based text - in , textout end-to - end trainable goal-oriented dialogue system",0.5937632918357849
translation,82,18,model,neural network - based model,for,task - oriented dialogue systems,neural network - based model for task - oriented dialogue systems,0.5927708148956299
translation,82,18,model,model,propose,neural network - based model,model propose neural network - based model,0.6544060111045837
translation,82,18,model,model,is,end-to - end trainable,model is end-to - end trainable,0.5751516819000244
translation,82,37,model,belief tracking,enables,sequence of freeform natural language sentences,belief tracking enables sequence of freeform natural language sentences,0.6494381427764893
translation,82,37,model,sequence of freeform natural language sentences,mapped into,fixed,sequence of freeform natural language sentences mapped into fixed,0.7565534114837646
translation,82,37,model,query,has,db,query has db,0.6034191250801086
translation,82,37,model,model,enables,sequence of freeform natural language sentences,model enables sequence of freeform natural language sentences,0.6460992097854614
translation,82,162,model,novel crowdsourced data collection framework,inspired by,wizard - of - oz paradigm,novel crowdsourced data collection framework inspired by wizard - of - oz paradigm,0.6698127388954163
translation,82,162,model,model,presented,novel crowdsourced data collection framework,model presented novel crowdsourced data collection framework,0.6369825601577759
translation,82,19,results,proposed model,performs,given task,proposed model performs given task,0.6193270683288574
translation,82,19,results,very competitively,across,several metrics,very competitively across several metrics,0.704303503036499
translation,82,19,results,given task,has,very competitively,given task has very competitively,0.5713407397270203
translation,82,19,results,results,show,proposed model,results show proposed model,0.6848889589309692
translation,82,137,results,weighted decoding strategy,does not provide,significant improvement,weighted decoding strategy does not provide significant improvement,0.7230639457702637
translation,82,137,results,weighted decoding strategy,does not provide,greatly improve,weighted decoding strategy does not provide greatly improve,0.685017466545105
translation,82,137,results,significant improvement,in,bleu score,significant improvement in bleu score,0.5479136109352112
translation,82,137,results,greatly improve,has,task success rate ( ? 3 % ),greatly improve has task success rate ( ? 3 % ),0.5511016845703125
translation,82,150,results,average subjective success rate,was,98 %,average subjective success rate was 98 %,0.5970038771629333
translation,82,155,results,hdc system,achieved,? 95 % task success rate,hdc system achieved ? 95 % task success rate,0.7184538245201111
translation,82,155,results,results,has,hdc system,results has hdc system,0.5963447093963623
translation,82,156,results,considered better,than,handcrafted system ( hdc ),considered better than handcrafted system ( hdc ),0.6088109612464905
translation,82,156,results,164 dialogues tested,has,nn system ( nn ),164 dialogues tested has nn system ( nn ),0.5859408378601074
translation,82,156,results,results,Over,164 dialogues tested,results Over 164 dialogues tested,0.609830379486084
translation,82,157,results,nn system ( nn ),was,more efficient,nn system ( nn ) was more efficient,0.599819540977478
translation,82,157,results,nn system ( nn ),provided,more engaging conversation,nn system ( nn ) provided more engaging conversation,0.6494214534759521
translation,82,158,results,comprehension ability and naturalness,of,nn system,comprehension ability and naturalness of nn system,0.5755243897438049
translation,82,158,results,comprehension ability and naturalness,rated,higher,comprehension ability and naturalness rated higher,0.7226389646530151
translation,82,158,results,higher,suggests,learned system,higher suggests learned system,0.709884524345398
translation,82,158,results,results,has,comprehension ability and naturalness,results has comprehension ability and naturalness,0.48599347472190857
translation,82,161,results,end-to - end trainable,using,two supervision signals,end-to - end trainable using two supervision signals,0.6563344597816467
translation,82,161,results,end-to - end trainable,using,modest corpus,end-to - end trainable using modest corpus,0.6457392573356628
translation,82,161,results,modest corpus,of,training data,modest corpus of training data,0.531104564666748
translation,82,161,results,results,has,model,results has model,0.5339115858078003
translation,84,89,hyperparameters,number of candidates per sequence,chosen based on,grid search,number of candidates per sequence chosen based on grid search,0.7177733778953552
translation,84,89,hyperparameters,batch size,has,24 ),batch size has 24 ),0.6336570978164673
translation,84,89,hyperparameters,number of candidates per sequence,has,2 ),number of candidates per sequence has 2 ),0.5751392841339111
translation,84,89,hyperparameters,hyperparameters,chosen based on,grid search,hyperparameters chosen based on grid search,0.7149744033813477
translation,84,6,model,taskoriented dialogue model,operates solely on,text input,taskoriented dialogue model operates solely on text input,0.6346471905708313
translation,84,6,model,taskoriented dialogue model,bypasses,explicit policy and language generation modules,taskoriented dialogue model bypasses explicit policy and language generation modules,0.6820358037948608
translation,84,6,model,model,propose,taskoriented dialogue model,model propose taskoriented dialogue model,0.6512216329574585
translation,84,25,model,large generative models,pretrained on,large general - domain corpora,large generative models pretrained on large general - domain corpora,0.7136424779891968
translation,84,25,model,large generative models,support,task - oriented dialogue applications,large generative models support task - oriented dialogue applications,0.5342862606048584
translation,84,25,model,large general - domain corpora,support,task - oriented dialogue applications,large general - domain corpora support task - oriented dialogue applications,0.5901146531105042
translation,84,25,model,model,demonstrate,large generative models,model demonstrate large generative models,0.6271651983261108
translation,84,26,model,of diverse components,such as,word tokenization,of diverse components such as word tokenization,0.6397839188575745
translation,84,26,model,of diverse components,such as,multi-task learning,of diverse components such as multi-task learning,0.6084994077682495
translation,84,26,model,of diverse components,such as,probabilistic sampling,of diverse components such as probabilistic sampling,0.5849524736404419
translation,84,26,model,probabilistic sampling,to support,task - oriented applications,probabilistic sampling to support task - oriented applications,0.6178211569786072
translation,84,27,model,task - oriented dialogue framework,to operate entirely on,text input,task - oriented dialogue framework to operate entirely on text input,0.6706859469413757
translation,84,27,model,task - oriented dialogue framework,bypassing,explicit dialogue management module,task - oriented dialogue framework bypassing explicit dialogue management module,0.7210025191307068
translation,84,27,model,task - oriented dialogue framework,bypassing,domain-specific natural language generation module,task - oriented dialogue framework bypassing domain-specific natural language generation module,0.6843101382255554
translation,84,27,model,model,adapt,task - oriented dialogue framework,model adapt task - oriented dialogue framework,0.7213309407234192
translation,84,28,model,simple text,as,input,simple text as input,0.5325362086296082
translation,84,29,model,entire dialogue context,includes,belief state,entire dialogue context includes belief state,0.6097326874732971
translation,84,29,model,entire dialogue context,includes,database state,entire dialogue context includes database state,0.6397765278816223
translation,84,29,model,entire dialogue context,includes,previous turns,entire dialogue context includes previous turns,0.6021831631660461
translation,84,29,model,entire dialogue context,provided to,decoder,entire dialogue context provided to decoder,0.6573072075843811
translation,84,29,model,previous turns,provided to,decoder,previous turns provided to decoder,0.7022801041603088
translation,84,29,model,decoder,as,raw text,decoder as raw text,0.5321369171142578
translation,84,29,model,belief state,has,database state,belief state has database state,0.5865379571914673
translation,84,29,model,model,has,entire dialogue context,model has entire dialogue context,0.5908783078193665
translation,84,109,results,gpt2 models,improve,score,gpt2 models improve score,0.7260988354682922
translation,84,109,results,score,on,inform and success metrics,score on inform and success metrics,0.5556740164756775
translation,84,109,results,results,has,gpt2 models,results has gpt2 models,0.49155914783477783
translation,84,122,results,ranked higher,than,all neural models,ranked higher than all neural models,0.585949182510376
translation,84,122,results,largest difference,observed between,oracle and the baseline model,largest difference observed between oracle and the baseline model,0.6516878604888916
translation,84,122,results,results,has,original responses,results has original responses,0.5077987313270569
translation,84,123,results,generated output,from,gpt,generated output from gpt,0.606521487236023
translation,84,123,results,generated output,from,gpt2 model,generated output from gpt2 model,0.5615072250366211
translation,84,123,results,generated output,is,strongly preferred,generated output is strongly preferred,0.5919358730316162
translation,84,123,results,gpt,is,strongly preferred,gpt is strongly preferred,0.5751306414604187
translation,84,123,results,strongly preferred,against,neural baseline,strongly preferred against neural baseline,0.7103551626205444
translation,86,27,baselines,hssm,is,two -layer special boltzmann machine,hssm is two -layer special boltzmann machine,0.5247191190719604
translation,86,27,baselines,baselines,has,hssm,baselines has hssm,0.5669790506362915
translation,86,128,hyperparameters,number of estimation iterations,for,all the models,number of estimation iterations for all the models,0.6015735268592834
translation,86,128,hyperparameters,all the models,on,training sets,all the models on training sets,0.5188119411468506
translation,86,128,hyperparameters,all the models,on,held - out test sets,all the models on held - out test sets,0.5203729271888733
translation,86,128,hyperparameters,numver of iterations,for,inference,numver of iterations for inference,0.6062073111534119
translation,86,128,hyperparameters,numver of iterations,set to,1000,numver of iterations set to 1000,0.683354914188385
translation,86,128,hyperparameters,inference,set to,1000,inference set to 1000,0.7017470598220825
translation,86,128,hyperparameters,held - out test sets,has,numver of iterations,held - out test sets has numver of iterations,0.5740075707435608
translation,86,128,hyperparameters,hyperparameters,on,held - out test sets,hyperparameters on held - out test sets,0.49463701248168945
translation,86,128,hyperparameters,hyperparameters,has,number of estimation iterations,hyperparameters has number of estimation iterations,0.5072832107543945
translation,86,129,hyperparameters,learning,of,hssm,learning of hssm,0.6320636868476868
translation,86,129,hyperparameters,datasets,divided into,minibatches,datasets divided into minibatches,0.6110689640045166
translation,86,129,hyperparameters,minibatches,each has,15 dialogues,minibatches each has 15 dialogues,0.6428484320640564
translation,86,129,hyperparameters,learning,has,datasets,learning has datasets,0.5538579225540161
translation,86,129,hyperparameters,hssm,has,datasets,hssm has datasets,0.5278974771499634
translation,86,129,hyperparameters,hyperparameters,to speed - up,learning,hyperparameters to speed - up learning,0.7187244892120361
translation,86,4,model,unsupervised learning model,based on,boltzmann machine,unsupervised learning model based on boltzmann machine,0.6064008474349976
translation,86,4,model,boltzmann machine,for,dialogue structure analysis,boltzmann machine for dialogue structure analysis,0.5950835943222046
translation,86,4,model,model,propose,unsupervised learning model,model propose unsupervised learning model,0.6586899757385254
translation,86,5,model,three types of units,in,hidden layer,three types of units in hidden layer,0.506445586681366
translation,86,5,model,three types of units,to discovery,dialogue latent structures,three types of units to discovery dialogue latent structures,0.6791026592254639
translation,86,5,model,softmax units,represent,latent states,softmax units represent latent states,0.6027668118476868
translation,86,5,model,latent states,of,utterances,latent states of utterances,0.6461976170539856
translation,86,5,model,binary units,represent,latent topics,binary units represent latent topics,0.6203353404998779
translation,86,5,model,latent topics,specified by,dialogues,latent topics specified by dialogues,0.6575881242752075
translation,86,5,model,binary unit,represents,global general topic,binary unit represents global general topic,0.6326034665107727
translation,86,5,model,global general topic,shared across,whole dialogue corpus,global general topic shared across whole dialogue corpus,0.7221019268035889
translation,86,5,model,model,employs,three types of units,model employs three types of units,0.6151180863380432
translation,86,5,model,model,employs,binary units,model employs binary units,0.6261844635009766
translation,86,6,model,extra connections,between,adjacent hidden softmax units,extra connections between adjacent hidden softmax units,0.6316341757774353
translation,86,6,model,extra connections,to formulate,dependency,extra connections to formulate dependency,0.619676411151886
translation,86,6,model,dependency,between,latent states,dependency between latent states,0.6651286482810974
translation,86,6,model,model,contains,extra connections,model contains extra connections,0.6600168347358704
translation,86,23,model,boltzmann machine based undirected generative model,for,dialogue structure analysis,boltzmann machine based undirected generative model for dialogue structure analysis,0.5602225065231323
translation,86,23,model,model,develop,boltzmann machine based undirected generative model,model develop boltzmann machine based undirected generative model,0.5510837435722351
translation,86,26,model,hidden softmax sequence model ( hssm ),for,dialogue modeling and structure analysis,hidden softmax sequence model ( hssm ) for dialogue modeling and structure analysis,0.6113156676292419
translation,86,26,model,model,propose,hidden softmax sequence model ( hssm ),model propose hidden softmax sequence model ( hssm ),0.6434347629547119
translation,86,158,results,hssm,achieves,better performance,hssm achieves better performance,0.6865895986557007
translation,86,158,results,better performance,on,likelihood,better performance on likelihood,0.5397399663925171
translation,86,158,results,likelihood,than,all the other models,likelihood than all the other models,0.5527307391166687
translation,86,158,results,all the other models,under,different number of latent states,all the other models under different number of latent states,0.612861156463623
translation,86,158,results,results,observe,hssm,results observe hssm,0.5947332382202148
translation,86,159,results,much better,than,all traditional models,much better than all traditional models,0.5946765542030334
translation,86,159,results,all traditional models,on,airticketbooking dataset,all traditional models on airticketbooking dataset,0.4913967549800873
translation,86,159,results,twitter - post dataset,has,our model,twitter - post dataset has our model,0.5631576776504517
translation,86,159,results,our model,has,slightly surpasses,our model has slightly surpasses,0.5921579003334045
translation,86,159,results,slightly surpasses,has,lmhmms,slightly surpasses has lmhmms,0.6132777333259583
translation,86,159,results,results,On,twitter - post dataset,results On twitter - post dataset,0.5524951815605164
translation,86,165,results,hssm,exhibits,better performance,hssm exhibits better performance,0.6778104901313782
translation,86,165,results,better performance,than,all the other models,better performance than all the other models,0.5586982369422913
translation,86,165,results,results,see that,hssm,results see that hssm,0.6624829769134521
translation,86,166,results,conventional models,interesting that,"lmhmms , tmhmms and tmhmmss","conventional models interesting that lmhmms , tmhmms and tmhmmss",0.661531388759613
translation,86,166,results,conventional models,interesting that,lmhmm and tmhmm,conventional models interesting that lmhmm and tmhmm,0.6743057370185852
translation,86,166,results,"lmhmms , tmhmms and tmhmmss",achieve,worse performances,"lmhmms , tmhmms and tmhmmss achieve worse performances",0.5983003973960876
translation,86,166,results,worse performances,than,lmhmm and tmhmm,worse performances than lmhmm and tmhmm,0.5490991473197937
translation,86,166,results,results,For,conventional models,results For conventional models,0.5674310326576233
translation,86,172,results,boltzmann machine based undirected models,able to,generalize,boltzmann machine based undirected models able to generalize,0.5921919941902161
translation,86,172,results,much better,than,traditional directed generative model,much better than traditional directed generative model,0.5842102766036987
translation,86,172,results,model learning,is,more stable,model learning is more stable,0.5899351835250854
translation,86,172,results,generalize,has,much better,generalize has much better,0.6125361323356628
translation,86,172,results,results,has,boltzmann machine based undirected models,results has boltzmann machine based undirected models,0.5380738973617554
translation,86,187,results,more powerful ability,of discovering structures,latent states,more powerful ability of discovering structures latent states,0.6847752332687378
translation,86,187,results,more powerful ability,modeling,different word sources,more powerful ability modeling different word sources,0.7086823582649231
translation,86,187,results,different word sources,including,latent states,different word sources including latent states,0.6843618154525757
translation,86,187,results,different word sources,including,dialogue specific topics,different word sources including dialogue specific topics,0.7109516263008118
translation,86,187,results,different word sources,including,global general topic,different word sources including global general topic,0.6580116748809814
translation,87,19,ablation-analysis,- based generators,with,da gating -vector,- based generators with da gating -vector,0.6364610195159912
translation,87,19,ablation-analysis,da gating -vector,prevent,undesirable semantic repetitions,da gating -vector prevent undesirable semantic repetitions,0.6556626558303833
translation,87,19,ablation-analysis,ared - based generators,show signs,better adapting,ared - based generators show signs better adapting,0.7652899026870728
translation,87,19,ablation-analysis,better adapting,to,new domain,better adapting to new domain,0.6054777503013611
translation,87,125,ablation-analysis,different model components,shows,proposed models,different model components shows proposed models,0.6459274291992188
translation,87,125,ablation-analysis,proposed models,have,better performance,proposed models have better performance,0.5909204483032227
translation,87,125,ablation-analysis,proposed models,significantly reduce,slot error rate err score,proposed models significantly reduce slot error rate err score,0.6728144884109497
translation,87,125,ablation-analysis,better performance,with,higher,better performance with higher,0.6906065940856934
translation,87,125,ablation-analysis,slot error rate err score,by,large margin,slot error rate err score by large margin,0.5708889961242676
translation,87,125,ablation-analysis,large margin,about,2 % to 4 %,large margin about 2 % to 4 %,0.5763387680053711
translation,87,125,ablation-analysis,2 % to 4 %,in,every datasets,2 % to 4 % in every datasets,0.5857088565826416
translation,87,125,ablation-analysis,higher,has,bleu score,higher has bleu score,0.5449684858322144
translation,87,125,ablation-analysis,ablation analysis,contribution of,different model components,ablation analysis contribution of different model components,0.7176827192306519
translation,87,119,baselines,enc- dec,applied,attention - based encoderdecoder architecture,enc- dec applied attention - based encoderdecoder architecture,0.6508572697639465
translation,87,119,baselines,baselines,has,enc- dec,baselines has enc- dec,0.6259745359420776
translation,87,106,experimental-setup,generators,implemented using,ten-sorflow library,generators implemented using ten-sorflow library,0.7144231200218201
translation,87,106,experimental-setup,generators,trained with,"training , validation and testing ratio","generators trained with training , validation and testing ratio",0.7739405035972595
translation,87,106,experimental-setup,"training , validation and testing ratio",as,3:1:1,"training , validation and testing ratio as 3:1:1",0.5531201362609863
translation,87,106,experimental-setup,experimental setup,trained with,"training , validation and testing ratio","experimental setup trained with training , validation and testing ratio",0.7265312075614929
translation,87,106,experimental-setup,experimental setup,has,generators,experimental setup has generators,0.4874105453491211
translation,87,107,experimental-setup,"hidden layer size , beam size",set to be,80 and 10,"hidden layer size , beam size set to be 80 and 10",0.7174222469329834
translation,87,107,experimental-setup,generators,trained with,70 %,generators trained with 70 %,0.768047034740448
translation,87,107,experimental-setup,70 %,has,of dropout rate,70 % has of dropout rate,0.5607296824455261
translation,87,107,experimental-setup,experimental setup,has,"hidden layer size , beam size","experimental setup has hidden layer size , beam size",0.5341595411300659
translation,87,107,experimental-setup,experimental setup,has,generators,experimental setup has generators,0.4874105453491211
translation,87,108,experimental-setup,5 runs,with,different random initialization,5 runs with different random initialization,0.5895098447799683
translation,87,108,experimental-setup,different random initialization,of,network,different random initialization of network,0.5929770469665527
translation,87,108,experimental-setup,training,terminated by,early stopping,training terminated by early stopping,0.7444175481796265
translation,87,108,experimental-setup,experimental setup,performed,5 runs,experimental setup performed 5 runs,0.3120799660682678
translation,87,113,experimental-setup,each da,over- generated,20 candidate sentences,each da over- generated 20 candidate sentences,0.6728916764259338
translation,87,113,experimental-setup,each da,selected,top 5 realizations,each da selected top 5 realizations,0.6848936676979065
translation,87,113,experimental-setup,top 5 realizations,after,reranking,top 5 realizations after reranking,0.7033911347389221
translation,87,113,experimental-setup,experimental setup,For,each da,experimental setup For each da,0.662233293056488
translation,87,91,hyperparameters,back propagation,has,through time,back propagation has through time,0.6155556440353394
translation,87,91,hyperparameters,hyperparameters,initialized with,pretrained glove word embedding vectors,hyperparameters initialized with pretrained glove word embedding vectors,0.6884333491325378
translation,87,92,hyperparameters,early stopping mechanism,to prevent,over-fitting,early stopping mechanism to prevent over-fitting,0.6557961702346802
translation,87,92,hyperparameters,over-fitting,using,validation set,over-fitting using validation set,0.6902710795402527
translation,87,92,hyperparameters,hyperparameters,has,early stopping mechanism,hyperparameters has early stopping mechanism,0.5077470541000366
translation,87,5,model,recurrent neural network based encoder - decoder architecture,in which,lstm - based decoder,recurrent neural network based encoder - decoder architecture in which lstm - based decoder,0.6022244691848755
translation,87,5,model,lstm - based decoder,to,select,lstm - based decoder to select,0.5935444235801697
translation,87,5,model,lstm - based decoder,to produce,required utterances,lstm - based decoder to produce required utterances,0.6953056454658508
translation,87,5,model,aggregate semantic elements,produced by,attention mechanism,aggregate semantic elements produced by attention mechanism,0.6230810284614563
translation,87,5,model,aggregate semantic elements,to produce,required utterances,aggregate semantic elements to produce required utterances,0.6883126497268677
translation,87,5,model,attention mechanism,over,input elements,attention mechanism over input elements,0.6324234008789062
translation,87,5,model,model,presents,recurrent neural network based encoder - decoder architecture,model presents recurrent neural network based encoder - decoder architecture,0.6633912920951843
translation,87,6,model,surface realization,to produce,natural language sentences,surface realization to produce natural language sentences,0.6095702648162842
translation,87,22,model,refinement adjustment lstmbased component ( ralstm ),introduced to,decoder side,refinement adjustment lstmbased component ( ralstm ) introduced to decoder side,0.650387704372406
translation,87,22,model,semantic information,has,refinement adjustment lstmbased component ( ralstm ),semantic information has refinement adjustment lstmbased component ( ralstm ),0.5768606066703796
translation,87,23,model,unaligned data,by jointly training,sentence planning,unaligned data by jointly training sentence planning,0.7717773914337158
translation,87,23,model,unaligned data,by jointly training,surface realization,unaligned data by jointly training surface realization,0.7778880596160889
translation,87,23,model,surface realization,to produce,natural language sentences,surface realization to produce natural language sentences,0.6095702648162842
translation,87,23,model,model,learn from,unaligned data,model learn from unaligned data,0.6611785888671875
translation,87,141,model,extension of ared model,in which,ralstm component,extension of ared model in which ralstm component,0.6473481059074402
translation,87,141,model,ralstm component,to select and aggregate,semantic elements,ralstm component to select and aggregate semantic elements,0.7110934853553772
translation,87,141,model,ralstm component,to generate,required sentence,ralstm component to generate required sentence,0.6739579439163208
translation,87,141,model,semantic elements,produced by,encoder,semantic elements produced by encoder,0.6515650749206543
translation,87,141,model,model,present,extension of ared model,model present extension of ared model,0.6211561560630798
translation,87,123,results,proposed models,consistently achieve,better performance,proposed models consistently achieve better performance,0.6654547452926636
translation,87,123,results,better performance,regarding,both evaluation metrics,better performance regarding both evaluation metrics,0.5798400640487671
translation,87,123,results,both evaluation metrics,across,all domains,both evaluation metrics across all domains,0.6647343635559082
translation,87,123,results,all domains,in,all test cases,all domains in all test cases,0.5383937358856201
translation,87,123,results,results,has,proposed models,results has proposed models,0.5798037648200989
translation,87,126,results,"proposed models ( w/ o r , ralstm )",have,significant improved performance,"proposed models ( w/ o r , ralstm ) have significant improved performance",0.5491349697113037
translation,87,126,results,significant improved performance,on,both the evaluation metrics,significant improved performance on both the evaluation metrics,0.5105913281440735
translation,87,126,results,both the evaluation metrics,across,four domains,both the evaluation metrics across four domains,0.6836696267127991
translation,87,135,results,proposed models,tend to generate,more complete and concise sentences,proposed models tend to generate more complete and concise sentences,0.7174798846244812
translation,87,135,results,more complete and concise sentences,than,other models,more complete and concise sentences than other models,0.5866872072219849
translation,87,135,results,results,found that,proposed models,results found that proposed models,0.6962680220603943
translation,88,41,baselines,convlab - 2,provides,three models,convlab - 2 provides three models,0.5733045339584351
translation,88,41,baselines,milu,has,"lee et al. , 2019 b )","milu has lee et al. , 2019 b )",0.6066210865974426
translation,88,41,baselines,baselines,has,convlab - 2,baselines has convlab - 2,0.5481888651847839
translation,88,44,baselines,bertnlu,adds,two mlps,bertnlu adds two mlps,0.6559074521064758
translation,88,44,baselines,bertnlu,fine-tunes,parameters,bertnlu fine-tunes parameters,0.7630305886268616
translation,88,44,baselines,two mlps,on top of,bert,two mlps on top of bert,0.7763780951499939
translation,88,44,baselines,two mlps,on top of,slot tagging,two mlps on top of slot tagging,0.7250547409057617
translation,88,44,baselines,bert,for,intent classification,bert for intent classification,0.6448351144790649
translation,88,44,baselines,bert,for,slot tagging,bert for slot tagging,0.6771485805511475
translation,88,44,baselines,baselines,has,bertnlu,baselines has bertnlu,0.6263778209686279
translation,88,50,baselines,convlab - 2,integrates,four models,convlab - 2 integrates four models,0.6175234913825989
translation,88,50,baselines,trade,has,"wu et al. , 2019 )","trade has wu et al. , 2019 )",0.5818605422973633
translation,88,50,baselines,baselines,has,convlab - 2,baselines has convlab - 2,0.5481888651847839
translation,88,56,baselines,convlab - 2,provides,template - based method,convlab - 2 provides template - based method,0.6125858426094055
translation,88,56,baselines,convlab - 2,provides,sc - lstm,convlab - 2 provides sc - lstm,0.575879693031311
translation,88,56,baselines,baselines,has,convlab - 2,baselines has convlab - 2,0.5481888651847839
translation,88,58,baselines,convlab - 2,integrates,three models,convlab - 2 integrates three models,0.6196146011352539
translation,88,58,baselines,mdrg,is,baseline model,mdrg is baseline model,0.5590671300888062
translation,88,58,baselines,mdrg,on,multiwoz,mdrg on multiwoz,0.6208229064941406
translation,88,58,baselines,mdrg,is,baseline model,mdrg is baseline model,0.5590671300888062
translation,88,58,baselines,mdrg,on,multiwoz,mdrg on multiwoz,0.6208229064941406
translation,88,58,baselines,baseline model,on,multiwoz,baseline model on multiwoz,0.5858991742134094
translation,88,58,baselines,larl,has,"zhao et al. , 2019 )","larl has zhao et al. , 2019 )",0.6015753149986267
translation,88,58,baselines,baselines,has,convlab - 2,baselines has convlab - 2,0.5481888651847839
translation,88,62,baselines,convlab - 2,provides,"agenda-based ( schatzmann et al. , 2007 ) model","convlab - 2 provides agenda-based ( schatzmann et al. , 2007 ) model",0.5527376532554626
translation,88,62,baselines,convlab - 2,provides,neural network - based models,convlab - 2 provides neural network - based models,0.6109970808029175
translation,88,62,baselines,neural network - based models,including,hus,neural network - based models including hus,0.6747456192970276
translation,88,62,baselines,neural network - based models,including,variational variants,neural network - based models including variational variants,0.6543174386024475
translation,88,62,baselines,baselines,has,convlab - 2,baselines has convlab - 2,0.5481888651847839
translation,88,66,baselines,convlab - 2,extends,"sequicity ( lei et al. , 2018 )","convlab - 2 extends sequicity ( lei et al. , 2018 )",0.7380926012992859
translation,88,66,baselines,"sequicity ( lei et al. , 2018 )",to,multi-domain scenarios,"sequicity ( lei et al. , 2018 ) to multi-domain scenarios",0.5729866027832031
translation,88,66,baselines,switched,resets,belief span,switched resets belief span,0.790341317653656
translation,88,66,baselines,belief span,records,information,belief span records information,0.7442362904548645
translation,88,66,baselines,information,of,current domain,information of current domain,0.5659252405166626
translation,88,66,baselines,current domain,has,switched,current domain has switched,0.5962514281272888
translation,88,66,baselines,baselines,has,convlab - 2,baselines has convlab - 2,0.5481888651847839
translation,88,67,experiments,convlab - 2,integrates,"damd ( zhang et al. , 2019 )","convlab - 2 integrates damd ( zhang et al. , 2019 )",0.6711107492446899
translation,88,74,experiments,convlab - 2,offers,agenda-based user simulator,convlab - 2 offers agenda-based user simulator,0.7035185098648071
translation,88,74,experiments,convlab - 2,offers,complete set of models,convlab - 2 offers complete set of models,0.6687843799591064
translation,88,74,experiments,complete set of models,for building,traditional pipeline dialogue system,complete set of models for building traditional pipeline dialogue system,0.7279660105705261
translation,88,74,experiments,traditional pipeline dialogue system,on,camrest676 dataset,traditional pipeline dialogue system on camrest676 dataset,0.5241081118583679
translation,88,82,experiments,convlab - 2,implements,"rollouts rl ( lewis et al. , 2017 )","convlab - 2 implements rollouts rl ( lewis et al. , 2017 )",0.6165819764137268
translation,88,82,experiments,convlab - 2,implements,"larl ( zhao et al. , 2019 ) models","convlab - 2 implements larl ( zhao et al. , 2019 ) models",0.641627311706543
translation,88,86,experiments,convlab - 2,offers,rule- based user simulator,convlab - 2 offers rule- based user simulator,0.7121092677116394
translation,88,86,experiments,convlab - 2,offers,complete set of models,convlab - 2 offers complete set of models,0.6687843799591064
translation,88,86,experiments,complete set of models,for building,pipeline system,complete set of models for building pipeline system,0.7638930082321167
translation,88,86,experiments,pipeline system,on,crosswoz dataset,pipeline system on crosswoz dataset,0.5244144797325134
translation,88,4,model,task - oriented dialogue systems,with,state - of - the - art models,task - oriented dialogue systems with state - of - the - art models,0.5818262696266174
translation,88,4,model,task - oriented dialogue systems,perform,end-to - end evaluation,task - oriented dialogue systems perform end-to - end evaluation,0.5360844731330872
translation,88,4,model,convlab - 2,has,open-source toolkit,convlab - 2 has open-source toolkit,0.5422638058662415
translation,88,4,model,model,present,convlab - 2,model present convlab - 2,0.5934492349624634
translation,88,49,model,model,has,word-level dialogue state tracking word-level dst,model has word-level dialogue state tracking word-level dst,0.5286202430725098
translation,88,52,model,dialogue policy,receives,belief state,dialogue policy receives belief state,0.6644364595413208
translation,88,52,model,dialogue policy,outputs,system dialogue acts,dialogue policy outputs system dialogue acts,0.704348623752594
translation,88,52,model,dialogue policy,receives,belief state,dialogue policy receives belief state,0.6644364595413208
translation,88,52,model,dialogue policy,outputs,system dialogue acts,dialogue policy outputs system dialogue acts,0.704348623752594
translation,88,52,model,dialogue policy,has,dialogue policy,dialogue policy has dialogue policy,0.6124675869941711
translation,88,52,model,model,has,dialogue policy,model has dialogue policy,0.6005539298057556
translation,88,55,model,model,has,natural language generation ( nlg ),model has natural language generation ( nlg ),0.5951933264732361
translation,88,57,model,word-level policy,directly generates,natural language response,word-level policy directly generates natural language response,0.7744714021682739
translation,88,57,model,natural language response,than,dialogue acts,natural language response than dialogue acts,0.5839364528656006
translation,88,57,model,natural language response,according to,dialogue history and the belief state,natural language response according to dialogue history and the belief state,0.6451995372772217
translation,88,57,model,model,has,word-level policy,model has word-level policy,0.5538329482078552
translation,88,45,results,bertnlu,achieves,best performance,bertnlu achieves best performance,0.7055011987686157
translation,88,45,results,best performance,on,multiwoz,best performance on multiwoz,0.5786730051040649
translation,88,45,results,multiwoz,in comparison with,other models,multiwoz in comparison with other models,0.7136838436126709
translation,88,45,results,results,has,bertnlu,results has bertnlu,0.5847097635269165
translation,88,87,results,success rate,has,60.8 %,success rate has 60.8 %,0.5311844348907471
translation,88,87,results,inform f1,has,44.5 %,inform f1 has 44.5 %,0.5912817120552063
translation,88,131,results,statistics,from,1000 simulated dialogues,statistics from 1000 simulated dialogues,0.5593131184577942
translation,88,131,results,demo system,performs,poorest,demo system performs poorest,0.6117756962776184
translation,88,131,results,poorest,in,hotel domain,poorest in hotel domain,0.46535608172416687
translation,88,131,results,goal,in,hospital domain,goal in hospital domain,0.5083342790603638
translation,88,131,results,results,collected,statistics,results collected statistics,0.648662805557251
translation,89,111,baselines,encoder-decoder ralstm model,integrating with,variational inference ( r- vnlg and c-vnlg ),encoder-decoder ralstm model integrating with variational inference ( r- vnlg and c-vnlg ),0.6748405694961548
translation,89,111,baselines,modification,integrating with,variational inference ( r- vnlg and c-vnlg ),modification integrating with variational inference ( r- vnlg and c-vnlg ),0.7288927435874939
translation,89,100,hyperparameters,beam width,set to be,100 and 10,beam width set to be 100 and 10,0.7479205131530762
translation,89,100,hyperparameters,models,trained with,70 %,models trained with 70 %,0.7501965761184692
translation,89,100,hyperparameters,70 %,has,of keep dropout rate,70 % has of keep dropout rate,0.574787437915802
translation,89,100,hyperparameters,hyperparameters,trained with,70 %,hyperparameters trained with 70 %,0.7593157291412354
translation,89,100,hyperparameters,hyperparameters,has,hidden layer size,hyperparameters has hidden layer size,0.4991928040981293
translation,89,100,hyperparameters,hyperparameters,has,beam width,hyperparameters has beam width,0.5189401507377625
translation,89,100,hyperparameters,hyperparameters,has,models,hyperparameters has models,0.5447477102279663
translation,89,101,hyperparameters,5 runs,with,different random initialization,5 runs with different random initialization,0.5895098447799683
translation,89,101,hyperparameters,different random initialization,of,network,different random initialization of network,0.5929770469665527
translation,89,101,hyperparameters,training process,terminated by,early stopping,training process terminated by early stopping,0.7565604448318481
translation,89,101,hyperparameters,hyperparameters,performed,5 runs,hyperparameters performed 5 runs,0.25509002804756165
translation,89,102,hyperparameters,variational inference,set,latent variable size,variational inference set latent variable size,0.5768209099769592
translation,89,102,hyperparameters,latent variable size,to be,300,latent variable size to be 300,0.583522617816925
translation,89,102,hyperparameters,hyperparameters,For,variational inference,hyperparameters For variational inference,0.5242729187011719
translation,89,103,hyperparameters,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,89,103,hyperparameters,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,89,103,hyperparameters,learning rate,initially set to be,0.001,learning rate initially set to be 0.001,0.6421350240707397
translation,89,103,hyperparameters,learning rate,after,5 epochs,learning rate after 5 epochs,0.6959445476531982
translation,89,103,hyperparameters,decayed,using,exponential rate,decayed using exponential rate,0.7262519001960754
translation,89,103,hyperparameters,every epoch,using,exponential rate,every epoch using exponential rate,0.7063004374504089
translation,89,103,hyperparameters,exponential rate,of,0.95,exponential rate of 0.95,0.5632036328315735
translation,89,103,hyperparameters,5 epochs,has,learning rate,5 epochs has learning rate,0.5695738792419434
translation,89,103,hyperparameters,decayed,has,every epoch,decayed has every epoch,0.6296926736831665
translation,89,103,hyperparameters,hyperparameters,used,adam optimizer,hyperparameters used adam optimizer,0.5959904789924622
translation,89,7,model,variational neural - based generation,tackle,nlg problem of having limited labeled dataset,variational neural - based generation tackle nlg problem of having limited labeled dataset,0.5976400971412659
translation,89,7,model,variational inference,into,encoder-decoder generator,variational inference into encoder-decoder generator,0.5312743186950684
translation,89,7,model,novel auxiliary autoencoding,with,effective training procedure,novel auxiliary autoencoding with effective training procedure,0.6177045702934265
translation,89,7,model,model,presents,variational neural - based generation,model presents variational neural - based generation,0.6741074323654175
translation,89,7,model,model,introduce,novel auxiliary autoencoding,model introduce novel auxiliary autoencoding,0.6787785887718201
translation,89,39,model,encoder-decoder based variational model,to,natural language generation ( vnlg ),encoder-decoder based variational model to natural language generation ( vnlg ),0.5481096506118774
translation,89,39,model,encoder-decoder based variational model,by integrating,"variational autoencoder ( kingma and welling , 2013 )","encoder-decoder based variational model by integrating variational autoencoder ( kingma and welling , 2013 )",0.6171672344207764
translation,89,39,model,"variational autoencoder ( kingma and welling , 2013 )",into,encoder-decoder generator,"variational autoencoder ( kingma and welling , 2013 ) into encoder-decoder generator",0.5269895792007446
translation,89,39,model,model,propose,encoder-decoder based variational model,model propose encoder-decoder based variational model,0.6531407237052917
translation,89,167,model,low-resource nlg,integrating,variational inference,low-resource nlg integrating variational inference,0.6396228671073914
translation,89,167,model,low-resource nlg,introducing,novel auxiliary auto-encoding,low-resource nlg introducing novel auxiliary auto-encoding,0.6966018676757812
translation,89,109,results,proposed models,obtained,state - of - the - art performances,proposed models obtained state - of - the - art performances,0.6017609238624573
translation,89,109,results,state - of - the - art performances,regarding,evaluation metrics,state - of - the - art performances regarding evaluation metrics,0.5913395881652832
translation,89,109,results,evaluation metrics,across,all domains,evaluation metrics across all domains,0.6739217638969421
translation,89,109,results,all domains,in,all training scenarios,all domains in all training scenarios,0.5039340257644653
translation,89,109,results,results,has,proposed models,results has proposed models,0.5798037648200989
translation,89,112,results,variational generators,not only provide,compelling evidence,variational generators not only provide compelling evidence,0.6208415627479553
translation,89,112,results,variational generators,preserve,power,variational generators preserve power,0.746597409248352
translation,89,112,results,compelling evidence,on,adapting,compelling evidence on adapting,0.5465841293334961
translation,89,112,results,adapting,to,"new , unseen domain","adapting to new , unseen domain",0.608108639717102
translation,89,112,results,"new , unseen domain",when,target domain data,"new , unseen domain when target domain data",0.6256510019302368
translation,89,112,results,target domain data,is,scarce,target domain data is scarce,0.592586100101471
translation,89,112,results,power,of,original ral - stm,power of original ral - stm,0.6211254596710205
translation,89,112,results,results,shows,variational generators,results shows variational generators,0.6018655896186829
translation,89,115,results,slightly better results,than,c-vnlg,slightly better results than c-vnlg,0.6119816899299622
translation,89,115,results,c-vnlg,when providing,sufficient training data,c-vnlg when providing sufficient training data,0.6932436227798462
translation,89,115,results,sufficient training data,in,scr100,sufficient training data in scr100,0.5672132968902588
translation,89,115,results,r-vnlg model,has,slightly better results,r-vnlg model has slightly better results,0.5844934582710266
translation,89,115,results,results,has,r-vnlg model,results has r-vnlg model,0.5195391774177551
translation,89,116,results,modest training data,in,scr10,modest training data in scr10,0.5660174489021301
translation,89,116,results,latter model,demonstrates,significant improvement,latter model demonstrates significant improvement,0.6230367422103882
translation,89,116,results,significant improvement,compared to,former,significant improvement compared to former,0.7494475841522217
translation,89,116,results,former,in terms of both,bleu and err scores,former in terms of both bleu and err scores,0.6472876667976379
translation,89,116,results,bleu and err scores,by,large margin,bleu and err scores by large margin,0.564784586429596
translation,89,116,results,large margin,across,all four dataset,large margin across all four dataset,0.7321639060974121
translation,89,116,results,modest training data,has,latter model,modest training data has latter model,0.5670091509819031
translation,89,116,results,scr10,has,latter model,scr10 has latter model,0.6296160817146301
translation,89,116,results,results,with,modest training data,results with modest training data,0.6183823347091675
translation,89,125,results,all models,work,well,all models work well,0.6390311121940613
translation,89,125,results,well,when,sufficient training datasets,well when sufficient training datasets,0.658839225769043
translation,89,125,results,performances,of,proposed models,performances of proposed models,0.6239349246025085
translation,89,125,results,increase,as increasing,model components,increase as increasing model components,0.6701996326446533
translation,89,125,results,results,has,all models,results has all models,0.5029959678649902
translation,89,129,results,c-vnlg model,in,hotel domain,c-vnlg model in hotel domain,0.513676106929779
translation,89,129,results,c-vnlg model,reduce,slot error rate err,c-vnlg model reduce slot error rate err,0.6744846701622009
translation,89,129,results,bleu score,from,68.55 to 79.98,bleu score from 68.55 to 79.98,0.49855685234069824
translation,89,129,results,slot error rate err,by,large margin,slot error rate err by large margin,0.5611253976821899
translation,89,129,results,slot error rate err,compared to,ralstm baseline,slot error rate err compared to ralstm baseline,0.5758943557739258
translation,89,129,results,large margin,from,22.53 to 8.67,large margin from 22.53 to 8.67,0.4891200363636017
translation,89,129,results,variational inference,has,c-vnlg model,variational inference has c-vnlg model,0.5263552069664001
translation,89,129,results,significantly improve,has,bleu score,significantly improve has bleu score,0.552523136138916
translation,89,129,results,results,by integrating,variational inference,results by integrating variational inference,0.5870548486709595
translation,89,130,results,proposed models,have,much better performance,proposed models have much better performance,0.5797918438911438
translation,89,130,results,much better performance,over,previous ones,much better performance over previous ones,0.6828362345695496
translation,89,130,results,previous ones,in,scr10 scenario,previous ones in scr10 scenario,0.5713433027267456
translation,89,130,results,results,has,proposed models,results has proposed models,0.5798037648200989
translation,89,131,results,crossvae model,trained on,scr10 scenario,crossvae model trained on scr10 scenario,0.7502748370170593
translation,89,131,results,crossvae model,achieved,results,crossvae model achieved results,0.6423144340515137
translation,89,131,results,results,close to,"hlstm , sclstm , and encdec models","results close to hlstm , sclstm , and encdec models",0.5959137678146362
translation,89,131,results,"hlstm , sclstm , and encdec models",trained on,all training data ( scr100 ) scenario,"hlstm , sclstm , and encdec models trained on all training data ( scr100 ) scenario",0.723336935043335
translation,89,131,results,results,has,crossvae model,results has crossvae model,0.5278376936912537
translation,89,135,results,scr30 section,effectiveness of,proposed methods,scr30 section effectiveness of proposed methods,0.6737340688705444
translation,89,135,results,proposed methods,in which,cross -vae and dualvae,proposed methods in which cross -vae and dualvae,0.6779999732971191
translation,89,135,results,cross -vae and dualvae,mostly rank,best and second - best models,cross -vae and dualvae mostly rank best and second - best models,0.6780980229377747
translation,89,135,results,best and second - best models,compared with,baselines,best and second - best models compared with baselines,0.6604833602905273
translation,89,135,results,results,has,scr30 section,results has scr30 section,0.5591550469398499
translation,89,136,results,proposed models,show,superior ability,proposed models show superior ability,0.64006507396698
translation,89,136,results,superior ability,in leveraging,existing small training data,superior ability in leveraging existing small training data,0.7221364974975586
translation,89,136,results,previous methods,trained on,100 %,previous methods trained on 100 %,0.7222115397453308
translation,89,136,results,100 %,of,in- domain data,100 % of in- domain data,0.6056584715843201
translation,89,136,results,results,has,proposed models,results has proposed models,0.5798037648200989
translation,89,142,results,decreases,as,models,decreases as models,0.640831708908081
translation,89,142,results,models,trained on,more data,models trained on more data,0.7531095147132874
translation,89,142,results,bleu score,has,increases,bleu score has increases,0.6288427114486694
translation,89,142,results,slot error err,has,decreases,slot error err has decreases,0.6133831143379211
translation,89,143,results,crossvae model,clearly better than,previous models,crossvae model clearly better than previous models,0.7160343527793884
translation,89,143,results,previous models,in,all cases,previous models in all cases,0.5234520435333252
translation,89,143,results,ralstm ),in,all cases,ralstm ) in all cases,0.5937114953994751
translation,89,143,results,previous models,has,ralstm ),previous models has ralstm ),0.6093515157699585
translation,89,143,results,results,has,crossvae model,results has crossvae model,0.5278376936912537
translation,89,144,results,performance,of,"cross - vae , ralstm model","performance of cross - vae , ralstm model",0.567123532295227
translation,89,144,results,"cross - vae , ralstm model",starts to,saturate,"cross - vae , ralstm model starts to saturate",0.7408419847488403
translation,89,144,results,saturate,around,30 % and 50 %,saturate around 30 % and 50 %,0.718417763710022
translation,89,144,results,better,as,more training data,better as more training data,0.517501175403595
translation,89,144,results,better,providing,more training data,better providing more training data,0.6849355697631836
translation,89,145,results,crossvae,trained on,30 % of,crossvae trained on 30 % of,0.8455051779747009
translation,89,145,results,crossvae,achieve,better performance,crossvae achieve better performance,0.6764029860496521
translation,89,145,results,better performance,compared to,previous models,better performance compared to previous models,0.6743692755699158
translation,89,145,results,previous models,trained on,100 %,previous models trained on 100 %,0.7424777746200562
translation,89,145,results,100 %,of,in- domain data,100 % of in- domain data,0.6056584715843201
translation,89,152,results,cross - vae model,obtain,good results,cross - vae model obtain good results,0.59494549036026
translation,89,152,results,good results,irrespective of,low slot error rate err,good results irrespective of low slot error rate err,0.6458749771118164
translation,89,152,results,good results,irrespective of,high bleu score,good results irrespective of high bleu score,0.5885363221168518
translation,89,152,results,high bleu score,around,50.00 points,high bleu score around 50.00 points,0.6251396536827087
translation,90,125,baselines,lstmbased encoder and decoder,with,attention,lstmbased encoder and decoder with attention,0.635009229183197
translation,90,265,experimental-setup,same seq2seq model,from,s2s- flat baseline,same seq2seq model from s2s- flat baseline,0.5383597612380981
translation,90,265,experimental-setup,same seq2seq model,for,our constrained decoding exper-iments,same seq2seq model for our constrained decoding exper-iments,0.594091534614563
translation,90,265,experimental-setup,same seq2seq model,used,300 - dimensional glove word embeddings,same seq2seq model used 300 - dimensional glove word embeddings,0.5861805081367493
translation,90,265,experimental-setup,same seq2seq model,used,hidden dimension,same seq2seq model used hidden dimension,0.6471256017684937
translation,90,265,experimental-setup,dropout rate,of,0.2,dropout rate of 0.2,0.5832480192184448
translation,90,265,experimental-setup,hidden dimension,of,128,hidden dimension of 128,0.6499708890914917
translation,90,265,experimental-setup,128,in,encoder and the decoder,128 in encoder and the decoder,0.5805147886276245
translation,90,265,experimental-setup,experimental setup,used,same seq2seq model,experimental setup used same seq2seq model,0.6268376708030701
translation,90,266,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,90,266,experimental-setup,learning rate,of,0.002,learning rate of 0.002,0.5967087745666504
translation,90,266,experimental-setup,0.002,to train,seq2seq model,0.002 to train seq2seq model,0.6476941108703613
translation,90,266,experimental-setup,experimental setup,used,adam optimizer,experimental setup used adam optimizer,0.5961911082267761
translation,90,267,experimental-setup,learning rate,reduced by,factor,learning rate reduced by factor,0.7101114392280579
translation,90,267,experimental-setup,factor,of,5,factor of 5,0.7007017135620117
translation,90,267,experimental-setup,factor,if,validation loss,factor if validation loss,0.6580637097358704
translation,90,267,experimental-setup,validation loss,stops,decreasing,validation loss stops decreasing,0.7800475358963013
translation,90,267,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,90,7,model,tree-structured semantic representations,introduce,constrained decoding approach,tree-structured semantic representations introduce constrained decoding approach,0.5573041439056396
translation,90,7,model,constrained decoding approach,for,seq2seq models,constrained decoding approach for seq2seq models,0.5973991751670837
translation,90,7,model,model,propose using,tree-structured semantic representations,model propose using tree-structured semantic representations,0.6779360771179199
translation,90,7,model,model,introduce,constrained decoding approach,model introduce constrained decoding approach,0.6097224950790405
translation,90,11,model,lessons,from,pre-neural nlg systems,lessons from pre-neural nlg systems,0.6195989847183228
translation,90,11,model,lessons,into,neural framework,lessons into neural framework,0.650902271270752
translation,90,11,model,model,incorporating,lessons,model incorporating lessons,0.7805993556976318
translation,90,95,model,encoder and decoder,are,long short - term memory ( lstm ) - based,encoder and decoder are long short - term memory ( lstm ) - based,0.5410950183868408
translation,90,95,model,encoder and decoder,uses,beam search,encoder and decoder uses beam search,0.5899730324745178
translation,90,95,model,decoder,uses,beam search,decoder uses beam search,0.6128473877906799
translation,90,95,model,beam search,for,generation,beam search for generation,0.6258087158203125
translation,90,95,model,model,has,encoder and decoder,model has encoder and decoder,0.5709518790245056
translation,90,169,results,s2s-constr,improves,tree accuracy significantly,s2s-constr improves tree accuracy significantly,0.7183908224105835
translation,90,169,results,tree accuracy significantly,over,s2s- tree,tree accuracy significantly over s2s- tree,0.6702757477760315
translation,90,169,results,both the e2e and weather datasets,has,s2s-constr,both the e2e and weather datasets has s2s-constr,0.6228350400924683
translation,90,169,results,results,On,both the e2e and weather datasets,results On both the e2e and weather datasets,0.46333691477775574
translation,90,170,results,human evaluation metrics,models that are aware of,tree-structured mr ( s2s - tree and s2s - constr ),human evaluation metrics models that are aware of tree-structured mr ( s2s - tree and s2s - constr ),0.6783271431922913
translation,90,170,results,significantly better,on,correctness measures,significantly better on correctness measures,0.5360698103904724
translation,90,170,results,correctness measures,than,s2s- token,correctness measures than s2s- token,0.5708869099617004
translation,90,170,results,significantly better,than,s2s - flat,significantly better than s2s - flat,0.6076570749282837
translation,90,170,results,results,has,human evaluation metrics,results has human evaluation metrics,0.49231401085853577
translation,90,173,results,s2s-constr,show,significant improvements,s2s-constr show significant improvements,0.7047730684280396
translation,90,173,results,"s2s- tree , and s2s-token",show,significant improvements,"s2s- tree , and s2s-token show significant improvements",0.6257521510124207
translation,90,173,results,significant improvements,in,bleu,significant improvements in bleu,0.5370094180107117
translation,90,173,results,bleu,over,flat baseline,bleu over flat baseline,0.6441960334777832
translation,90,173,results,s2s-constr,has,"s2s- tree , and s2s-token","s2s-constr has s2s- tree , and s2s-token",0.5728052854537964
translation,90,173,results,results,has,s2s-constr,results has s2s-constr,0.48593977093696594
translation,90,175,results,bleu score increases,observed with,tree-based models,bleu score increases observed with tree-based models,0.6794915795326233
translation,90,175,results,tree-based models,are,not statistically significant,tree-based models are not statistically significant,0.5641003847122192
translation,90,175,results,not statistically significant,compared to,s2s- token,not statistically significant compared to s2s- token,0.7101436257362366
translation,90,175,results,e2e dataset,has,bleu score increases,e2e dataset has bleu score increases,0.5660346150398254
translation,90,175,results,results,note,e2e dataset,results note e2e dataset,0.574999213218689
translation,90,175,results,results,for,e2e dataset,results for e2e dataset,0.5929365754127502
translation,90,177,results,statistically significant,for,all models,statistically significant for all models,0.5592688918113708
translation,90,177,results,all models,on,our weather dataset,all models on our weather dataset,0.5118874907493591
translation,90,177,results,results,has,bleu score increases,results has bleu score increases,0.5827440619468689
translation,90,178,results,s2s-constr,fails to generate,any valid candidates,s2s-constr fails to generate any valid candidates,0.7882676124572754
translation,90,178,results,any valid candidates,for,~ 1.5 %,any valid candidates for ~ 1.5 %,0.631242573261261
translation,90,178,results,~ 1.5 %,of,weather test examples,~ 1.5 % of weather test examples,0.5987223982810974
translation,90,178,results,results,has,s2s-constr,results has s2s-constr,0.48593977093696594
translation,90,181,results,grammaticality,seems to,drop slightly,grammaticality seems to drop slightly,0.6903596520423889
translation,90,181,results,drop slightly,for,tree-based models,drop slightly for tree-based models,0.6645384430885315
translation,90,181,results,tree-based models,on,weather dataset,tree-based models on weather dataset,0.5322688817977905
translation,90,181,results,tree-based models,on,e2e dataset,tree-based models on e2e dataset,0.5464879274368286
translation,90,181,results,tree-based models,not on,e2e dataset,tree-based models not on e2e dataset,0.6392120122909546
translation,90,181,results,results,has,grammaticality,results has grammaticality,0.504309892654419
translation,90,192,results,models,with,enriched semantic representations,models with enriched semantic representations,0.5761234164237976
translation,90,192,results,enriched semantic representations,show,higher diversity,enriched semantic representations show higher diversity,0.6370975375175476
translation,90,192,results,higher diversity,than,neural baselines,higher diversity than neural baselines,0.5976173281669617
translation,90,192,results,results,All of,models,results All of models,0.5659689903259277
translation,90,193,results,improved bleu scores,adding,discourse relation information,improved bleu scores adding discourse relation information,0.679789662361145
translation,90,193,results,discourse relation information,to,input mrs,discourse relation information to input mrs,0.5656558871269226
translation,90,193,results,input mrs,increase,diversity,input mrs increase diversity,0.7557680010795593
translation,90,193,results,diversity,without incurring,losses,diversity without incurring losses,0.7377665638923645
translation,90,193,results,losses,on,automatic metrics,losses on automatic metrics,0.5466955900192261
translation,90,196,results,s2s-constr,achieves,more than 90 % tree accuracy,s2s-constr achieves more than 90 % tree accuracy,0.6984638571739197
translation,90,196,results,s2s-constr,achieves,more than 95 %,s2s-constr achieves more than 95 %,0.7239674925804138
translation,90,196,results,more than 90 % tree accuracy,with,just 2 k samples,more than 90 % tree accuracy with just 2 k samples,0.66204833984375
translation,90,196,results,more than 95 %,with,5 k samples,more than 95 % with 5 k samples,0.7131403088569641
translation,90,196,results,5 k samples,on,both datasets,5 k samples on both datasets,0.5081131458282471
translation,90,196,results,results,has,s2s-constr,results has s2s-constr,0.48593977093696594
translation,91,7,model,reference resolution model,works,"incrementally ( i.e. , word by word )","reference resolution model works incrementally ( i.e. , word by word )",0.5244633555412292
translation,91,7,model,reference resolution model,incorporate,extra-linguistic information,reference resolution model incorporate extra-linguistic information,0.6669631600379944
translation,91,7,model,"incrementally ( i.e. , word by word )",grounds,words,"incrementally ( i.e. , word by word ) grounds words",0.6744916439056396
translation,91,7,model,words,with,visually present properties,words with visually present properties,0.6597314476966858
translation,91,7,model,visually present properties,has,of objects,visually present properties has of objects,0.5649054050445557
translation,91,17,model,model,of,reference resolution,model of reference resolution,0.5638517141342163
translation,91,17,model,more natural data,coming from,corpus of human / human interactions,more natural data coming from corpus of human / human interactions,0.6436989903450012
translation,91,17,model,model,apply,model,model apply model,0.6039915680885315
translation,91,17,model,model,apply,reference resolution,model apply reference resolution,0.6218136548995972
translation,91,20,model,saliency,of,context,saliency of context,0.5637577176094055
translation,91,20,model,contextual information,into,account,contextual information into account,0.6119990944862366
translation,91,20,model,model,model,saliency,model model saliency,0.8329143524169922
translation,92,143,baselines,neural model,consisting of,seq2seq model,neural model consisting of seq2seq model,0.6730823516845703
translation,92,143,baselines,seq2seq model,with,attention,seq2seq model with attention,0.6613655686378479
translation,92,143,baselines,seq2seq model,with,attention and copying,seq2seq model with attention and copying,0.6251972317695618
translation,92,143,baselines,neural seq2seq model,with,attention and copying,neural seq2seq model with attention and copying,0.6248055696487427
translation,92,104,experimental-setup,"glove ( pennington et al. , 2014 ) embedding",for,input embeddings,"glove ( pennington et al. , 2014 ) embedding for input embeddings",0.5427910685539246
translation,92,130,experimental-setup,experimental setup,implemented and trained using,pytorch,experimental setup implemented and trained using pytorch,0.6796218752861023
translation,92,131,experimental-setup,"adam ( kingma and ba , 2014 ) optimizer",used for,all gradient based optimization,"adam ( kingma and ba , 2014 ) optimizer used for all gradient based optimization",0.6172772645950317
translation,92,131,experimental-setup,experimental setup,has,"adam ( kingma and ba , 2014 ) optimizer","experimental setup has adam ( kingma and ba , 2014 ) optimizer",0.5325181484222412
translation,92,132,experimental-setup,randomized hyperparameter grid search,to determine,learning rate,randomized hyperparameter grid search to determine learning rate,0.6304004788398743
translation,92,132,experimental-setup,randomized hyperparameter grid search,to determine,number of layers,randomized hyperparameter grid search to determine number of layers,0.6327569484710693
translation,92,132,experimental-setup,randomized hyperparameter grid search,to determine,dropout,randomized hyperparameter grid search to determine dropout,0.6232778429985046
translation,92,132,experimental-setup,randomized hyperparameter grid search,to determine,dimensions,randomized hyperparameter grid search to determine dimensions,0.6511408090591431
translation,92,132,experimental-setup,dimensions,of,hidden layers,dimensions of hidden layers,0.5607795715332031
translation,92,132,experimental-setup,experimental setup,used,randomized hyperparameter grid search,experimental setup used randomized hyperparameter grid search,0.5936334729194641
translation,92,133,experimental-setup,learning rate,of,0.000718,learning rate of 0.000718,0.5891067981719971
translation,92,133,experimental-setup,0.000718,for,all optimization,0.000718 for all optimization,0.5892714858055115
translation,92,134,experimental-setup,dropout value,of,0.1,dropout value of 0.1,0.6022539138793945
translation,92,134,experimental-setup,0.1,used for,all models,0.1 used for all models,0.6687523722648621
translation,92,134,experimental-setup,experimental setup,has,dropout value,experimental setup has dropout value,0.5243558287620544
translation,92,136,experimental-setup,both sequence to sequence models,for,idontknow task,both sequence to sequence models for idontknow task,0.6249792575836182
translation,92,136,experimental-setup,both sequence to sequence models,use,hidden size,both sequence to sequence models use hidden size,0.6394385099411011
translation,92,136,experimental-setup,both sequence to sequence models,use,hidden size,both sequence to sequence models use hidden size,0.6394385099411011
translation,92,136,experimental-setup,both sequence to sequence models,use,hidden size,both sequence to sequence models use hidden size,0.6394385099411011
translation,92,136,experimental-setup,hidden size,of,524,hidden size of 524,0.6810113787651062
translation,92,136,experimental-setup,hidden size,of,638,hidden size of 638,0.6731093525886536
translation,92,136,experimental-setup,hidden size,of,638,hidden size of 638,0.6731093525886536
translation,92,136,experimental-setup,hidden size,of,638,hidden size of 638,0.6731093525886536
translation,92,136,experimental-setup,524,within,lstm,524 within lstm,0.6754026412963867
translation,92,136,experimental-setup,hidden size,of,100,hidden size of 100,0.6616772413253784
translation,92,136,experimental-setup,hidden size,of,638,hidden size of 638,0.6731093525886536
translation,92,136,experimental-setup,hidden size,of,638,hidden size of 638,0.6731093525886536
translation,92,136,experimental-setup,100,for,attention layer,100 for attention layer,0.6073622107505798
translation,92,136,experimental-setup,hidden size,of,638,hidden size of 638,0.6731093525886536
translation,92,136,experimental-setup,638,for,copy layer,638 for copy layer,0.6722319722175598
translation,92,136,experimental-setup,dropout value,of,0.1,dropout value of 0.1,0.6022539138793945
translation,92,136,experimental-setup,experimental setup,has,both sequence to sequence models,experimental setup has both sequence to sequence models,0.5682075023651123
translation,92,135,model,lstms,are,bidirectional,lstms are bidirectional,0.5891790390014648
translation,92,135,model,bidirectional,with,single layer,bidirectional with single layer,0.6810281276702881
translation,92,135,model,model,has,lstms,model has lstms,0.5445783734321594
translation,92,146,results,neural model with copying ( s2sa + c ),are,rule- based baseline,neural model with copying ( s2sa + c ) are rule- based baseline,0.5691325068473816
translation,92,146,results,rule- based baseline,have,comparable performance,rule- based baseline have comparable performance,0.5228806734085083
translation,92,146,results,significantly outperforming,has,baseline neural model ( s2sa ),significantly outperforming has baseline neural model ( s2sa ),0.5830828547477722
translation,92,154,results,outperforms,both,rule- based baseline,outperforms both rule- based baseline,0.6487173438072205
translation,92,154,results,the s2sa model,for,idon - tknow,the s2sa model for idon - tknow,0.6555554866790771
translation,92,154,results,copy mechanism,has,outperforms,copy mechanism has outperforms,0.6395075917243958
translation,92,154,results,results,showing,copy mechanism,results showing copy mechanism,0.7443604469299316
translation,92,171,results,rule- based model,does,well,rule- based model does well,0.2792341411113739
translation,92,171,results,well,on,emotive dataset,well on emotive dataset,0.479128897190094
translation,92,171,results,results,has,rule- based model,results has rule- based model,0.5454078316688538
translation,92,172,results,overall worst,on,most metrics,overall worst on most metrics,0.5020520687103271
translation,92,172,results,overall worst,except,conciseness,overall worst except conciseness,0.6306456923484802
translation,92,172,results,results,has,s2sa model,results has s2sa model,0.5086715817451477
translation,92,173,results,s2sa + c model,performs,best,s2sa + c model performs best,0.5910869240760803
translation,92,173,results,s2sa + c model,performs,compares favorably,s2sa + c model performs compares favorably,0.6483791470527649
translation,92,173,results,best,on,appropriateness and fluency,best on appropriateness and fluency,0.5593333840370178
translation,92,173,results,appropriateness and fluency,for,idontknow dataset,appropriateness and fluency for idontknow dataset,0.6209361553192139
translation,92,173,results,compares favorably,with,rule- based model,compares favorably with rule- based model,0.6762534379959106
translation,92,173,results,rule- based model,for,other metrics,rule- based model for other metrics,0.6028883457183838
translation,92,173,results,results,has,s2sa + c model,results has s2sa + c model,0.5209519267082214
translation,92,244,results,s2sa + c,performs,best,s2sa + c performs best,0.6532493829727173
translation,92,244,results,s2sa + c,performs,best,s2sa + c performs best,0.6532493829727173
translation,92,244,results,s2sa + c,performs,best,s2sa + c performs best,0.6532493829727173
translation,92,244,results,best,on,idontknow,best on idontknow,0.5949463248252869
translation,92,244,results,best,on,emotive,best on emotive,0.5796291828155518
translation,92,244,results,rule-based,performs,best,rule-based performs best,0.64372318983078
translation,92,244,results,best,on,emotive,best on emotive,0.5796291828155518
translation,92,244,results,results,has,s2sa + c,results has s2sa + c,0.5323763489723206
translation,93,151,results,overall performance,has,knu - ci,overall performance has knu - ci,0.5545946359634399
translation,93,151,results,knu - ci,has,outperforms,knu - ci has outperforms,0.6504316926002502
translation,93,151,results,results,For,overall performance,results For overall performance,0.6399255394935608
translation,93,153,results,all systems,produce,better results,all systems produce better results,0.6505560278892517
translation,93,153,results,better results,for,main +,better results for main +,0.6528317928314209
translation,93,153,results,results,has,all systems,results has all systems,0.5592814683914185
translation,93,164,results,strength,of,amore - upf system,strength of amore - upf system,0.6251843571662903
translation,93,164,results,amore - upf system,for,secondary characters,amore - upf system for secondary characters,0.6152490377426147
translation,93,164,results,amore - upf system,using,bidirectional lstm model,amore - upf system using bidirectional lstm model,0.6491027474403381
translation,93,164,results,secondary characters,using,bidirectional lstm model,secondary characters using bidirectional lstm model,0.638439953327179
translation,93,164,results,results,describes,strength,results describes strength,0.5789266228675842
translation,94,222,baselines,gecor model,which,end-to - end generative ellipsis and coreference resolution model,gecor model which end-to - end generative ellipsis and coreference resolution model,0.5501444935798645
translation,94,222,baselines,gecor model,is,end-to - end generative ellipsis and coreference resolution model,gecor model is end-to - end generative ellipsis and coreference resolution model,0.5293023586273193
translation,94,222,baselines,end-to - end generative ellipsis and coreference resolution model,with,two encoders,end-to - end generative ellipsis and coreference resolution model with two encoders,0.5991666316986084
translation,94,222,baselines,end-to - end generative ellipsis and coreference resolution model,with,one decoder,end-to - end generative ellipsis and coreference resolution model with one decoder,0.6045630574226379
translation,94,222,baselines,one decoder,produce,pragmatically complete user utterance,one decoder produce pragmatically complete user utterance,0.6428641080856323
translation,94,222,baselines,pragmatically complete user utterance,via,generation and copying,pragmatically complete user utterance via generation and copying,0.5932028889656067
translation,94,5,experiments,risawoz,contains,11.2 k,risawoz contains 11.2 k,0.6130453944206238
translation,94,5,experiments,human-to-human ( h2h ) multiturn semantically annotated dialogues,with,more than 150k utterances,human-to-human ( h2h ) multiturn semantically annotated dialogues with more than 150k utterances,0.6243492364883423
translation,94,5,experiments,11.2 k,has,human-to-human ( h2h ) multiturn semantically annotated dialogues,11.2 k has human-to-human ( h2h ) multiturn semantically annotated dialogues,0.5588160157203674
translation,94,5,experiments,more than 150k utterances,has,spanning over 12 domains,more than 150k utterances has spanning over 12 domains,0.5992150902748108
translation,94,193,hyperparameters,vocabulary size,to,"8,000","vocabulary size to 8,000",0.585127055644989
translation,94,193,hyperparameters,hyperparameters,set,vocabulary size,hyperparameters set vocabulary size,0.613650918006897
translation,94,193,hyperparameters,hyperparameters,randomly initialize,50 - dimensional word embeddings,hyperparameters randomly initialize 50 - dimensional word embeddings,0.6973587870597839
translation,94,194,hyperparameters,size,of,hidden states,size of hidden states,0.6102981567382812
translation,94,194,hyperparameters,hidden states,set to,100,hidden states set to 100,0.7250307202339172
translation,94,194,hyperparameters,hyperparameters,has,size,hyperparameters has size,0.5313372611999512
translation,94,195,hyperparameters,model,with,learning rate,model with learning rate,0.6086345314979553
translation,94,195,hyperparameters,model,with,decay rate,model with decay rate,0.6353127956390381
translation,94,195,hyperparameters,learning rate,of,0.005,learning rate of 0.005,0.6149182915687561
translation,94,195,hyperparameters,decay rate,of,0.5,decay rate of 0.5,0.6363998651504517
translation,94,195,hyperparameters,hyperparameters,train,model,hyperparameters train model,0.7064553499221802
translation,94,208,hyperparameters,300 dimensional word vectors,from,"fasttext ( grave et al. , 2018 )","300 dimensional word vectors from fasttext ( grave et al. , 2018 )",0.522118330001831
translation,94,208,hyperparameters,300 dimensional word vectors,used for,e2e-coref model,300 dimensional word vectors used for e2e-coref model,0.60094153881073
translation,94,208,hyperparameters,hyperparameters,has,300 dimensional word vectors,hyperparameters has 300 dimensional word vectors,0.4717963635921478
translation,94,209,hyperparameters,size of hidden states,to,200,size of hidden states to 200,0.5857272148132324
translation,94,209,hyperparameters,hyperparameters,set,size of hidden states,hyperparameters set size of hidden states,0.5979295969009399
translation,94,209,hyperparameters,hyperparameters,set,number of layers,hyperparameters set number of layers,0.6222911477088928
translation,94,210,hyperparameters,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,94,210,hyperparameters,decay rate,of,0.999,decay rate of 0.999,0.583235502243042
translation,94,210,hyperparameters,hyperparameters,trained with,learning rate,hyperparameters trained with learning rate,0.6679986715316772
translation,94,211,hyperparameters,dropout rate,set to,0.2,dropout rate set to 0.2,0.6595621705055237
translation,94,211,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,94,223,hyperparameters,both the size of hidden states and word embeddings,to,300,both the size of hidden states and word embeddings to 300,0.5614104270935059
translation,94,223,hyperparameters,hyperparameters,set,both the size of hidden states and word embeddings,hyperparameters set both the size of hidden states and word embeddings,0.6082713007926941
translation,94,224,hyperparameters,) word vectors,to initialize,word embeddings,) word vectors to initialize word embeddings,0.721523106098175
translation,94,224,hyperparameters,word embeddings,in,embedding layer,word embeddings in embedding layer,0.4723983108997345
translation,94,224,hyperparameters,300 dimensional fasttext,has,) word vectors,300 dimensional fasttext has ) word vectors,0.5806096792221069
translation,94,224,hyperparameters,hyperparameters,use,300 dimensional fasttext,hyperparameters use 300 dimensional fasttext,0.589035153388977
translation,94,225,hyperparameters,model,with,learning rate,model with learning rate,0.6086345314979553
translation,94,225,hyperparameters,model,with,decay rate,model with decay rate,0.6353127956390381
translation,94,225,hyperparameters,learning rate,of,0.003,learning rate of 0.003,0.6038756370544434
translation,94,225,hyperparameters,decay rate,of,0.5,decay rate of 0.5,0.6363998651504517
translation,94,225,hyperparameters,hyperparameters,train,model,hyperparameters train model,0.7064553499221802
translation,94,226,hyperparameters,early stopping,used,dropout rate,early stopping used dropout rate,0.6115309000015259
translation,94,226,hyperparameters,dropout rate,is,0.5,dropout rate is 0.5,0.5554119348526001
translation,94,226,hyperparameters,hyperparameters,has,early stopping,hyperparameters has early stopping,0.5431009531021118
translation,94,226,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,94,4,model,a large-scale multidomain chinese wizard - of - oz dataset,with,rich semantic annotations,a large-scale multidomain chinese wizard - of - oz dataset with rich semantic annotations,0.5413568615913391
translation,94,4,model,risawoz,has,a large-scale multidomain chinese wizard - of - oz dataset,risawoz has a large-scale multidomain chinese wizard - of - oz dataset,0.5154309272766113
translation,94,4,model,model,propose,risawoz,model propose risawoz,0.6265251040458679
translation,94,23,model,large-scale chinese multi-domain wizard - of - oz task - oriented dialogue dataset,with,rich semantic annotations,large-scale chinese multi-domain wizard - of - oz task - oriented dialogue dataset with rich semantic annotations,0.5360451340675354
translation,94,23,model,risawoz,has,large-scale chinese multi-domain wizard - of - oz task - oriented dialogue dataset,risawoz has large-scale chinese multi-domain wizard - of - oz task - oriented dialogue dataset,0.48519614338874817
translation,94,23,model,model,propose,risawoz,model propose risawoz,0.6265251040458679
translation,94,206,model,all spans,in,text,all spans in text,0.5820647478103638
translation,94,206,model,text,as,potential mentions,text as potential mentions,0.5379198789596558
translation,94,206,model,distributions,over,all possible antecedents,distributions over all possible antecedents,0.6965231895446777
translation,94,206,model,all possible antecedents,for,each mention,all possible antecedents for each mention,0.5958167910575867
translation,94,206,model,model,considers,all spans,model considers all spans,0.7329897284507751
translation,94,206,model,model,learn,distributions,model learn distributions,0.6424534320831299
translation,94,184,results,randomly initialized word embeddings,of,100 dimensions,randomly initialized word embeddings of 100 dimensions,0.5008209943771362
translation,94,184,results,trade,achieves,"65.35 % , 50.49 % and 58.19 % joint accuracy","trade achieves 65.35 % , 50.49 % and 58.19 % joint accuracy",0.6735343337059021
translation,94,184,results,"65.35 % , 50.49 % and 58.19 % joint accuracy",on,"single-domain , multi-domain and all data","65.35 % , 50.49 % and 58.19 % joint accuracy on single-domain , multi-domain and all data",0.4892426133155823
translation,94,184,results,randomly initialized word embeddings,has,trade,randomly initialized word embeddings has trade,0.5657342672348022
translation,94,184,results,100 dimensions,has,trade,100 dimensions has trade,0.6582985520362854
translation,94,184,results,results,use,randomly initialized word embeddings,results use randomly initialized word embeddings,0.5620489120483398
translation,94,185,results,300 dimensional pretrained word vectors,from,fasttext,300 dimensional pretrained word vectors from fasttext,0.5444905757904053
translation,94,185,results,300 dimensional pretrained word vectors,has,trade,300 dimensional pretrained word vectors has trade,0.5330738425254822
translation,94,185,results,fasttext,has,trade,fasttext has trade,0.6390606164932251
translation,94,185,results,trade,has,performs a little better,trade has performs a little better,0.6563115119934082
translation,94,185,results,results,using,300 dimensional pretrained word vectors,results using 300 dimensional pretrained word vectors,0.5495309233665466
translation,94,186,results,mlcsg,achieves,"higher 73.04 % , 58.77 % and 66.16 % joint accuracy","mlcsg achieves higher 73.04 % , 58.77 % and 66.16 % joint accuracy",0.6374121308326721
translation,94,187,results,performances,of,two dst models,performances of two dst models,0.5493006706237793
translation,94,187,results,significantly drop,on,multidomain dialogues,significantly drop on multidomain dialogues,0.5866378545761108
translation,94,187,results,two dst models,has,significantly drop,two dst models has significantly drop,0.5728514790534973
translation,94,187,results,results,has,performances,results has performances,0.5711642503738403
translation,94,214,results,e2e-coref model,achieves,"84.49 % , 80.60 % , 82.41 % average f 1 score","e2e-coref model achieves 84.49 % , 80.60 % , 82.41 % average f 1 score",0.6312875151634216
translation,94,214,results,"84.49 % , 80.60 % , 82.41 % average f 1 score",on,"single-domain , multi-domain and all data","84.49 % , 80.60 % , 82.41 % average f 1 score on single-domain , multi-domain and all data",0.479447603225708
translation,94,215,results,worst,on,multi-domain dialogues,worst on multi-domain dialogues,0.5875476002693176
translation,94,215,results,multi-domain dialogues,where,coreference links may cross different domains,multi-domain dialogues where coreference links may cross different domains,0.5551450848579407
translation,94,231,results,gecor model,achieves,58.26 % em score,gecor model achieves 58.26 % em score,0.6425497531890869
translation,94,231,results,gecor model,achieves,87.50 % bleu score,gecor model achieves 87.50 % bleu score,0.6333622932434082
translation,94,231,results,gecor model,achieves,78.14 % resolution f 1 score,gecor model achieves 78.14 % resolution f 1 score,0.6416747570037842
translation,94,231,results,78.14 % resolution f 1 score,on,all data,78.14 % resolution f 1 score on all data,0.5124832987785339
translation,95,4,ablation-analysis,ablation analysis,describes,critical role dungeons and dragons dataset ( crd3 ),ablation analysis describes critical role dungeons and dragons dataset ( crd3 ),0.637974739074707
translation,95,10,experiments,data augmentation method,produces,"34,243 summarydialogue chunk pairs","data augmentation method produces 34,243 summarydialogue chunk pairs",0.61103355884552
translation,95,10,experiments,"34,243 summarydialogue chunk pairs",to support,current neural ml approaches,"34,243 summarydialogue chunk pairs to support current neural ml approaches",0.617703378200531
translation,95,230,results,combined model,in,recall and in f - 1,combined model in recall and in f - 1,0.5561433434486389
translation,95,230,results,purely extractive model,has,significantly outperforms,purely extractive model has significantly outperforms,0.6120501756668091
translation,95,230,results,significantly outperforms,has,combined model,significantly outperforms has combined model,0.6268795132637024
translation,95,230,results,results,has,purely extractive model,results has purely extractive model,0.5853121280670166
translation,95,241,results,dialogue summarization metrics,are,substantially lower,dialogue summarization metrics are substantially lower,0.5688802003860474
translation,95,241,results,news summarization metrics,has,dialogue summarization metrics,news summarization metrics has dialogue summarization metrics,0.5251851081848145
translation,95,241,results,results,Compared to,news summarization metrics,results Compared to news summarization metrics,0.5888406038284302
translation,96,143,baselines,siamese model,uses,dual encoder,siamese model uses dual encoder,0.5819434523582458
translation,96,143,baselines,dual encoder,for,conversation modeling,dual encoder for conversation modeling,0.6077240109443665
translation,96,143,baselines,conversation modeling,without,dialogue acts information,conversation modeling without dialogue acts information,0.660900890827179
translation,96,143,baselines,"siamese ( lowe et al. , 2017 )",has,siamese model,"siamese ( lowe et al. , 2017 ) has siamese model",0.5267713069915771
translation,96,144,baselines,dialogue acts,in,single - task setting,dialogue acts in single - task setting,0.5319297313690186
translation,96,144,baselines,dialogue acts,in,crossway fashion,dialogue acts in crossway fashion,0.5609673857688904
translation,96,144,baselines,dialogue acts,in,crossway fashion,dialogue acts in crossway fashion,0.5609673857688904
translation,96,144,baselines,baselines,has,siamese-pda-st + crossway,baselines has siamese-pda-st + crossway,0.5242530703544617
translation,96,146,baselines,hypothetical model,uses,actual dialogue acts,hypothetical model uses actual dialogue acts,0.6176852583885193
translation,96,146,baselines,actual dialogue acts,in,crossway fashion,actual dialogue acts in crossway fashion,0.5425790548324585
translation,96,146,baselines,siamese-ada + crossway,has,hypothetical model,siamese-ada + crossway has hypothetical model,0.5794581174850464
translation,96,146,baselines,baselines,has,siamese-ada + crossway,baselines has siamese-ada + crossway,0.5128589868545532
translation,96,149,baselines,predicted dialogue acts,of,context,predicted dialogue acts of context,0.5668506026268005
translation,96,149,baselines,context,in,multi-task setting,context in multi-task setting,0.5106563568115234
translation,96,149,baselines,baselines,has,siamese-pda-mt + context -da,baselines has siamese-pda-mt + context -da,0.5223516821861267
translation,96,131,hyperparameters,maximum batch size,is,32,maximum batch size is 32,0.6009846925735474
translation,96,131,hyperparameters,utterances,padded to,maximum length,utterances padded to maximum length,0.6725397109985352
translation,96,131,hyperparameters,maximum length,in,batch,maximum length in batch,0.5551215410232544
translation,96,131,hyperparameters,each batch,has,utterances,each batch has utterances,0.6339654922485352
translation,96,131,hyperparameters,hyperparameters,has,maximum batch size,hyperparameters has maximum batch size,0.49297305941581726
translation,96,132,hyperparameters,300 - dimensional glove embeddings,to initialize,word vectors,300 - dimensional glove embeddings to initialize word vectors,0.6801791191101074
translation,96,132,hyperparameters,word vectors,updated during,training,word vectors updated during training,0.6777145862579346
translation,96,132,hyperparameters,hyperparameters,use,300 - dimensional glove embeddings,hyperparameters use 300 - dimensional glove embeddings,0.5603818893432617
translation,96,133,hyperparameters,both context - response encoder and da - encoder,are,grus,both context - response encoder and da - encoder are grus,0.6152133345603943
translation,96,133,hyperparameters,grus,with,rnn_size,grus with rnn_size,0.6702330708503723
translation,96,133,hyperparameters,rnn_size,of,300,rnn_size of 300,0.6405702829360962
translation,96,133,hyperparameters,optimizing,between,100 to 500,optimizing between 100 to 500,0.7142585515975952
translation,96,133,hyperparameters,hyperparameters,has,both context - response encoder and da - encoder,hyperparameters has both context - response encoder and da - encoder,0.548322856426239
translation,96,134,hyperparameters,dropout,of,0.1,dropout of 0.1,0.6162222623825073
translation,96,134,hyperparameters,0.1,optimized over,0.0 to 0.7,0.1 optimized over 0.0 to 0.7,0.587099015712738
translation,96,134,hyperparameters,0.1,applied to,embeddings,0.1 applied to embeddings,0.6844038963317871
translation,96,134,hyperparameters,0.0 to 0.7,in steps of,0.1,0.0 to 0.7 in steps of 0.1,0.6063699126243591
translation,96,134,hyperparameters,embeddings,obtained from,output,embeddings obtained from output,0.6216539740562439
translation,96,134,hyperparameters,output,of,encoder,output of encoder,0.6085062026977539
translation,96,134,hyperparameters,hyperparameters,has,dropout,hyperparameters has dropout,0.5324090719223022
translation,96,135,hyperparameters,cross entropy,using,adam optimizer,cross entropy using adam optimizer,0.619016170501709
translation,96,135,hyperparameters,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,96,135,hyperparameters,learning rate,of,0.0003,learning rate of 0.0003,0.5981547236442566
translation,96,135,hyperparameters,hyperparameters,trained to minimize,cross entropy,hyperparameters trained to minimize cross entropy,0.7387704849243164
translation,96,136,hyperparameters,hyperparameters,trained for,200 epochs,hyperparameters trained for 200 epochs,0.6665350794792175
translation,96,16,model,novel model,bridges,gap,novel model bridges gap,0.7421571612358093
translation,96,16,model,gap,has,between theory and practice,gap has between theory and practice,0.561217188835144
translation,96,16,model,model,propose,novel model,model propose novel model,0.6891457438468933
translation,96,17,model,model,leverages,dialogue acts,model leverages dialogue acts,0.7201375961303711
translation,96,17,model,dialogue acts,for,response selection,dialogue acts for response selection,0.6308155059814453
translation,96,17,model,model,leverages,dialogue acts,model leverages dialogue acts,0.7201375961303711
translation,96,24,model,simple yet novel way,combining,dialogue act representations,simple yet novel way combining dialogue act representations,0.7540866136550903
translation,96,24,model,dialogue act representations,of,context and response,dialogue act representations of context and response,0.5691779851913452
translation,96,24,model,dialogue act representations,promotes,cross similarities,dialogue act representations promotes cross similarities,0.616936445236206
translation,96,24,model,utterance representations,of,context and response,utterance representations of context and response,0.5996906161308289
translation,96,24,model,utterance representations,promotes,cross similarities,utterance representations promotes cross similarities,0.6199356913566589
translation,96,24,model,model,has,simple yet novel way,model has simple yet novel way,0.6164878606796265
translation,96,69,model,crossway response selection module,uses,both dialogue act representations,crossway response selection module uses both dialogue act representations,0.5846604108810425
translation,96,69,model,both dialogue act representations,to select,right response,both dialogue act representations to select right response,0.6782224178314209
translation,96,69,model,right response,from,set of candidate responses,right response from set of candidate responses,0.5639891028404236
translation,96,101,model,multi-task model,uses,same shared context- response encoder,multi-task model uses same shared context- response encoder,0.595012366771698
translation,96,101,model,same shared context- response encoder,for,both tasks,same shared context- response encoder for both tasks,0.5918705463409424
translation,96,101,model,multi-task model,has,siamese-pda -mt + crossway,multi-task model has siamese-pda -mt + crossway,0.5490131974220276
translation,96,101,model,model,propose,multi-task model,model propose multi-task model,0.6666305661201477
translation,96,7,results,novel way,of combining,predicted dialogue acts,novel way of combining predicted dialogue acts,0.6852309107780457
translation,96,7,results,predicted dialogue acts,of,context and response,predicted dialogue acts of context and response,0.5799387693405151
translation,96,7,results,predicted dialogue acts,in,crossway fashion,predicted dialogue acts in crossway fashion,0.5453847050666809
translation,96,7,results,context and response,with,context ( previous utterances ) and response ( follow - up utterance ),context and response with context ( previous utterances ) and response ( follow - up utterance ),0.6136073470115662
translation,96,7,results,context ( previous utterances ) and response ( follow - up utterance ),in,crossway fashion,context ( previous utterances ) and response ( follow - up utterance ) in crossway fashion,0.5438787937164307
translation,96,7,results,results,proposes,novel way,results proposes novel way,0.7603344321250916
translation,96,25,results,ensemble model,robust to,errors,ensemble model robust to errors,0.7337436676025391
translation,96,25,results,errors,made by,any underlying components of the ensemble,errors made by any underlying components of the ensemble,0.6984010934829712
translation,96,25,results,ensemble model,has,outperforms,ensemble model has outperforms,0.6253306865692139
translation,96,25,results,outperforms,has,all other non-ensemble models,outperforms has all other non-ensemble models,0.5650522112846375
translation,96,25,results,results,has,ensemble model,results has ensemble model,0.5478004217147827
translation,96,26,results,dialogue act prediction,improves,performance,dialogue act prediction improves performance,0.6549574136734009
translation,96,26,results,performance,of,response selection,performance of response selection,0.6206562519073486
translation,96,27,results,important observation,is,significant performance boost,important observation is significant performance boost,0.5340350866317749
translation,96,27,results,significant performance boost,obtained from,proposed crossway model ( ensemble-model ),significant performance boost obtained from proposed crossway model ( ensemble-model ),0.5910998582839966
translation,96,27,results,significant performance boost,improves,mrr,significant performance boost improves mrr,0.7654825448989868
translation,96,27,results,significant performance boost,improves,accuracy,significant performance boost improves accuracy,0.7148188948631287
translation,96,27,results,mrr,for,response selection task,mrr for response selection task,0.5603368282318115
translation,96,27,results,accuracy,of,dialogue act prediction task,accuracy of dialogue act prediction task,0.5133004784584045
translation,96,27,results,results,has,important observation,results has important observation,0.557221531867981
translation,96,150,results,better,than,modelling them,better than modelling them,0.6075497269630432
translation,96,150,results,modelling them,has,independently ( single - task setting ),modelling them has independently ( single - task setting ),0.5687357187271118
translation,96,150,results,results,joint modeling of,dialogue act prediction task and response selection task ( multi-task setting ),results joint modeling of dialogue act prediction task and response selection task ( multi-task setting ),0.6407232284545898
translation,96,151,results,dialogue acts,of,response and context ( crossway ),dialogue acts of response and context ( crossway ),0.5776395797729492
translation,96,151,results,dialogue acts,performs,better,dialogue acts performs better,0.6491241455078125
translation,96,151,results,better,than using,either one,better than using either one,0.7447773814201355
translation,96,151,results,results,Combining,dialogue acts,results Combining dialogue acts,0.6247221827507019
translation,97,66,baselines,three baselines,use,same transformer - based encoder,three baselines use same transformer - based encoder,0.6222448945045471
translation,97,66,baselines,baselines,implemented,three baselines,baselines implemented three baselines,0.7040272355079651
translation,97,67,baselines,trans - gen,uses,pure generation decoder,trans - gen uses pure generation decoder,0.6174389719963074
translation,97,67,baselines,trans - gen,uses,pure pointer - based decoder,trans - gen uses pure pointer - based decoder,0.6372666358947754
translation,97,67,baselines,trans - gen,uses,hybrid pointer + generation decoder,trans - gen uses hybrid pointer + generation decoder,0.6123766303062439
translation,97,67,baselines,trans - gen,applies,pure pointer - based decoder,trans - gen applies pure pointer - based decoder,0.6367399096488953
translation,97,67,baselines,trans - gen,uses,hybrid pointer + generation decoder,trans - gen uses hybrid pointer + generation decoder,0.6123766303062439
translation,97,67,baselines,pure generation decoder,generates,words,pure generation decoder generates words,0.7318950295448303
translation,97,67,baselines,pure generation decoder,generates,word,pure generation decoder generates word,0.7158374190330505
translation,97,67,baselines,words,from,fixed vocabulary,words from fixed vocabulary,0.6041096448898315
translation,97,67,baselines,words,from,input,words from input,0.6342279314994812
translation,97,67,baselines,words,from,vocabulary,words from vocabulary,0.5560730695724487
translation,97,67,baselines,words,from,input,words from input,0.6342279314994812
translation,97,67,baselines,words,from,input,words from input,0.6342279314994812
translation,97,67,baselines,trans - pointer,applies,pure pointer - based decoder,trans - pointer applies pure pointer - based decoder,0.620404839515686
translation,97,67,baselines,pure pointer - based decoder,copy,word,pure pointer - based decoder copy word,0.7052344083786011
translation,97,67,baselines,word,from,input,word from input,0.6140804290771484
translation,97,67,baselines,word,from,input,word from input,0.6140804290771484
translation,97,67,baselines,trans - hybrid,uses,hybrid pointer + generation decoder,trans - hybrid uses hybrid pointer + generation decoder,0.6160294413566589
translation,97,67,baselines,baselines,has,trans - gen,baselines has trans - gen,0.6403139233589172
translation,97,17,model,information,of,semantic role labeling ( srl ),information of semantic role labeling ( srl ),0.5793889164924622
translation,97,17,model,information,to improve,sentence rewriting,information to improve sentence rewriting,0.6681692004203796
translation,97,17,model,model,incorporate,information,model incorporate information,0.7218281626701355
translation,97,26,model,traditional srl,to,conversational scenario,traditional srl to conversational scenario,0.5412901043891907
translation,97,26,model,traditional srl,by additionally annotating,dialogue dataset,traditional srl by additionally annotating dialogue dataset,0.733990490436554
translation,97,26,model,dialogue dataset,with,standard srl labels,dialogue dataset with standard srl labels,0.570859968662262
translation,97,26,model,model,extend,traditional srl,model extend traditional srl,0.7301105856895447
translation,97,27,model,rewriting model,based on,pre-trained roberta model,rewriting model based on pre-trained roberta model,0.6332557201385498
translation,97,27,model,pre-trained roberta model,takes,outputs,pre-trained roberta model takes outputs,0.6819244027137756
translation,97,27,model,outputs,of,srl parsing and dialogue history,outputs of srl parsing and dialogue history,0.5493415594100952
translation,97,27,model,srl parsing and dialogue history,as,inputs,srl parsing and dialogue history as inputs,0.5075360536575317
translation,97,27,model,model,has,rewriting model,model has rewriting model,0.5529588460922241
translation,97,96,model,novel srl - guided framework,for enhancing,dialogue rewriting,novel srl - guided framework for enhancing dialogue rewriting,0.6828780770301819
translation,97,96,model,model,introduce,novel srl - guided framework,model introduce novel srl - guided framework,0.6322607398033142
translation,97,69,results,baselines,on,two datasets,baselines on two datasets,0.5093981623649597
translation,97,69,results,srl information,has,our model,srl information has our model,0.5978981852531433
translation,97,69,results,our model,has,significantly outperforms,our model has significantly outperforms,0.6134821176528931
translation,97,69,results,significantly outperforms,has,baselines,significantly outperforms has baselines,0.6014122366905212
translation,97,69,results,results,even without,srl information,results even without srl information,0.6867437362670898
translation,97,70,results,model,with,pointer - based decoder,model with pointer - based decoder,0.6424973011016846
translation,97,70,results,model,achieves,better performance,model achieves better performance,0.6787512898445129
translation,97,70,results,better performance,than,generation - based and the hybrid one,better performance than generation - based and the hybrid one,0.5822251439094543
translation,97,70,results,results,see that,model,results see that model,0.640961766242981
translation,97,72,results,srl information,improve,performance,srl information improve performance,0.6781997084617615
translation,97,72,results,performance,by at,1.45 bleu - 1 and 1.6 bleu - 2 points,performance by at 1.45 bleu - 1 and 1.6 bleu - 2 points,0.46799013018608093
translation,97,72,results,performance,achieving,state - of - the - art performances,performance achieving state - of - the - art performances,0.6150394082069397
translation,97,72,results,results,incorporating,srl information,results incorporating srl information,0.6558117270469666
translation,97,77,results,latter one,is,significantly better,latter one is significantly better,0.5936788320541382
translation,97,77,results,significantly better,than,first one,significantly better than first one,0.6332119107246399
translation,97,84,results,srl parser,achieves,75.66 precision,srl parser achieves 75.66 precision,0.640514612197876
translation,97,84,results,srl parser,achieves,74.47 recall,srl parser achieves 74.47 recall,0.6329025030136108
translation,97,84,results,srl parser,achieves,75.06 f 1,srl parser achieves 75.06 f 1,0.6555062532424927
translation,97,84,results,results,find,srl parser,results find srl parser,0.5701140761375427
translation,97,86,results,all evaluation scores,are,significantly improved,all evaluation scores are significantly improved,0.5452329516410828
translation,98,178,ablation-analysis,significant entity f1 drop,on,multiwoz 2.1,significant entity f1 drop on multiwoz 2.1,0.5562854409217834
translation,98,178,ablation-analysis,3.8 %,in absolute value,33.9 % in ratio,3.8 % in absolute value 33.9 % in ratio,0.753192126750946
translation,98,178,ablation-analysis,multiwoz 2.1,verifies,superiority,multiwoz 2.1 verifies superiority,0.6139419078826904
translation,98,178,ablation-analysis,module,with,multi-hop reasoning ability,module with multi-hop reasoning ability,0.5985203385353088
translation,98,178,ablation-analysis,multi-hop reasoning ability,in retrieving,correct entities,multi-hop reasoning ability in retrieving correct entities,0.7128070592880249
translation,98,178,ablation-analysis,significant entity f1 drop,has,3.8 %,significant entity f1 drop has 3.8 %,0.5747170448303223
translation,98,154,baselines,standard sequence - to-sequence ( seq2seq ) models,with and without attention,pointer to unknown ( ptr - unk,standard sequence - to-sequence ( seq2seq ) models with and without attention pointer to unknown ( ptr - unk,0.713467001914978
translation,98,154,baselines,standard sequence - to-sequence ( seq2seq ) models,with and without attention,graphlstm,standard sequence - to-sequence ( seq2seq ) models with and without attention graphlstm,0.7174641489982605
translation,98,144,experimental-setup,our model,in,tensorflow,our model in tensorflow,0.5319178700447083
translation,98,144,experimental-setup,experimental setup,implement,our model,experimental setup implement our model,0.6453344821929932
translation,98,145,experimental-setup,grid search,to find,best hyper-parameters,grid search to find best hyper-parameters,0.6165519952774048
translation,98,145,experimental-setup,best hyper-parameters,for,our model,best hyper-parameters for our model,0.5444721579551697
translation,98,145,experimental-setup,best hyper-parameters,over,validation set,best hyper-parameters over validation set,0.6622052788734436
translation,98,145,experimental-setup,our model,over,validation set,our model over validation set,0.6687250733375549
translation,98,146,experimental-setup,all the embeddings,in,our implementation,all the embeddings in our implementation,0.5336340665817261
translation,98,146,experimental-setup,experimental setup,randomly initialize,all the embeddings,experimental setup randomly initialize all the embeddings,0.7394419312477112
translation,98,147,experimental-setup,embedding size,selected between,"[ 16 , 512 ]","embedding size selected between [ 16 , 512 ]",0.6805245280265808
translation,98,147,experimental-setup,embedding size,selected between,rnn hidden state,embedding size selected between rnn hidden state,0.607768177986145
translation,98,147,experimental-setup,embedding size,equivalent to,rnn hidden state,embedding size equivalent to rnn hidden state,0.6039049029350281
translation,98,147,experimental-setup,rnn hidden state,including,decoder,rnn hidden state including decoder,0.6570059061050415
translation,98,147,experimental-setup,experimental setup,has,embedding size,experimental setup has embedding size,0.5223031640052795
translation,98,148,experimental-setup,dropout,for,regularization,dropout for regularization,0.5746612548828125
translation,98,148,experimental-setup,regularization,on,both the encoder and the decoder,regularization on both the encoder and the decoder,0.6046249866485596
translation,98,148,experimental-setup,both the encoder and the decoder,to avoid,over-fitting,both the encoder and the decoder to avoid over-fitting,0.6579481959342957
translation,98,148,experimental-setup,dropout rate,set between,"[ 0.1,0.5 ]","dropout rate set between [ 0.1,0.5 ]",0.6187625527381897
translation,98,149,experimental-setup,adam optimizer,to accelerate,convergence,adam optimizer to accelerate convergence,0.6788145303726196
translation,98,149,experimental-setup,convergence,with,learning rate,convergence with learning rate,0.623816728591919
translation,98,149,experimental-setup,learning rate,chosen between,"[ 1e ?3 , 1e ?4 ]","learning rate chosen between [ 1e ?3 , 1e ?4 ]",0.6780778765678406
translation,98,149,experimental-setup,experimental setup,use,adam optimizer,experimental setup use adam optimizer,0.5987385511398315
translation,98,160,experimental-setup,bert- base - uncased model,from,huggingface library,bert- base - uncased model from huggingface library,0.5821516513824463
translation,98,160,experimental-setup,huggingface library,as,encoder,huggingface library as encoder,0.48558229207992554
translation,98,160,experimental-setup,encoder,to encode,dialogue history,encoder to encode dialogue history,0.7481331825256348
translation,98,160,experimental-setup,experimental setup,use,bert- base - uncased model,experimental setup use bert- base - uncased model,0.6091492772102356
translation,98,6,model,graph structural information,in,knowledge base,graph structural information in knowledge base,0.4774865210056305
translation,98,6,model,graph structural information,in,dependency parsing tree,graph structural information in dependency parsing tree,0.47768884897232056
translation,98,6,model,graph structural information,in,dependency parsing tree,graph structural information in dependency parsing tree,0.47768884897232056
translation,98,6,model,dependency parsing tree,of,dialogue,dependency parsing tree of dialogue,0.5283721685409546
translation,98,6,model,model,exploiting,graph structural information,model exploiting graph structural information,0.6978267431259155
translation,98,7,model,structural information,in,dialogue history,structural information in dialogue history,0.4936284124851227
translation,98,7,model,structural information,propose,new recurrent cell architecture,structural information propose new recurrent cell architecture,0.6019518375396729
translation,98,7,model,new recurrent cell architecture,allows,representation learning,new recurrent cell architecture allows representation learning,0.5808488726615906
translation,98,7,model,representation learning,on,graphs,representation learning on graphs,0.5299784541130066
translation,98,7,model,model,effectively leverage,structural information,model effectively leverage structural information,0.6931599974632263
translation,98,8,model,multi-hop reasoning ability,based on,graph structure,multi-hop reasoning ability based on graph structure,0.6525479555130005
translation,98,8,model,model,combines,multi-hop reasoning ability,model combines multi-hop reasoning ability,0.7112427949905396
translation,98,30,model,novel graph - based end-to - end task - oriented dialogue model ( graphdialog ),aimed to exploit,graph knowledge,novel graph - based end-to - end task - oriented dialogue model ( graphdialog ) aimed to exploit graph knowledge,0.6450327038764954
translation,98,30,model,graph knowledge,both in,dialogue history and kbs,graph knowledge both in dialogue history and kbs,0.6257644295692444
translation,98,30,model,model,propose,novel graph - based end-to - end task - oriented dialogue model ( graphdialog ),model propose novel graph - based end-to - end task - oriented dialogue model ( graphdialog ),0.6691309213638306
translation,98,31,model,gru,design,novel recurrent unit,gru design novel recurrent unit,0.5601356029510498
translation,98,31,model,novel recurrent unit,allows,multiple hidden states,novel recurrent unit allows multiple hidden states,0.599773645401001
translation,98,31,model,multiple hidden states,as,inputs,multiple hidden states as inputs,0.5529905557632446
translation,98,31,model,inputs,at,each timestep,inputs at each timestep,0.5575934648513794
translation,98,31,model,each timestep,such that,dialogue history,each timestep such that dialogue history,0.6406225562095642
translation,98,31,model,dialogue history,encoded with,graph structural information,dialogue history encoded with graph structural information,0.7096455097198486
translation,98,31,model,model,design,novel recurrent unit,model design novel recurrent unit,0.5821912884712219
translation,98,32,model,recurrent unit,employs,masked attention mechanism,recurrent unit employs masked attention mechanism,0.5351645350456238
translation,98,32,model,masked attention mechanism,to enable,variable input hidden states,masked attention mechanism to enable variable input hidden states,0.7042713761329651
translation,98,32,model,variable input hidden states,at,each timestep,variable input hidden states at each timestep,0.5387329459190369
translation,98,32,model,model,has,recurrent unit,model has recurrent unit,0.565582275390625
translation,98,36,model,multi-hop reasoning ability,with,graph,multi-hop reasoning ability with graph,0.6104041337966919
translation,98,36,model,graph,to exploit,relationships,graph to exploit relationships,0.7016755938529968
translation,98,36,model,between entities,in,kb,between entities in kb,0.5314860939979553
translation,98,36,model,relationships,has,between entities,relationships has between entities,0.548119306564331
translation,98,36,model,model,combine,multi-hop reasoning ability,model combine multi-hop reasoning ability,0.6960716843605042
translation,98,150,model,greedy strategy,to search for,target word,greedy strategy to search for target word,0.7403863072395325
translation,98,150,model,target word,in,decoder,target word in decoder,0.5434755682945251
translation,98,150,model,target word,without,advanced techniques,target word without advanced techniques,0.7223266959190369
translation,98,150,model,advanced techniques,like,beam-search,advanced techniques like beam-search,0.6512576937675476
translation,98,150,model,model,use,greedy strategy,model use greedy strategy,0.7256254553794861
translation,98,162,results,fine- tuned bert,by,large margin,fine- tuned bert by large margin,0.5686922669410706
translation,98,162,results,our mode,has,significantly outperforms,our mode has significantly outperforms,0.6125825643539429
translation,98,162,results,significantly outperforms,has,fine- tuned bert,significantly outperforms has fine- tuned bert,0.6210425496101379
translation,98,162,results,results,find that,our mode,results find that our mode,0.6892274618148804
translation,98,168,results,all the other baselines,by,large margin,all the other baselines by large margin,0.560373842716217
translation,98,168,results,large margin,both in,entity f1 and bleu score,large margin both in entity f1 and bleu score,0.600007176399231
translation,98,168,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,98,168,results,outperforms,has,all the other baselines,outperforms has all the other baselines,0.5866766571998596
translation,98,168,results,results,has,our model,results has our model,0.5871725678443909
translation,98,169,results,huge gap,between,multi-woz 2.1 and smd,huge gap between multi-woz 2.1 and smd,0.6090049743652344
translation,98,169,results,entity f1 and bleu score,has,huge gap,entity f1 and bleu score has huge gap,0.5964614152908325
translation,98,169,results,results,find that,entity f1 and bleu score,results find that entity f1 and bleu score,0.6232091784477234
translation,98,175,results,our model,without,graph encoder,our model without graph encoder,0.7158434987068176
translation,98,175,results,1.6 % absolute value loss,in,bleu score,1.6 % absolute value loss in bleu score,0.5260822176933289
translation,98,175,results,1.6 % absolute value loss,in,entity f1,1.6 % absolute value loss in entity f1,0.5355302095413208
translation,98,175,results,1.1 % absolute value loss,in,entity f1,1.1 % absolute value loss in entity f1,0.5353416800498962
translation,98,175,results,entity f1,on,multiwoz 2.1,entity f1 on multiwoz 2.1,0.564249575138092
translation,98,175,results,graph encoder,has,1.6 % absolute value loss,graph encoder has 1.6 % absolute value loss,0.5752298831939697
translation,98,175,results,1.6 % absolute value loss,has,over 25 % in ratio,1.6 % absolute value loss has over 25 % in ratio,0.5874269604682922
translation,98,175,results,1.1 % absolute value loss,has,9.8 % in ratio,1.1 % absolute value loss has 9.8 % in ratio,0.5621652603149414
translation,98,175,results,results,observe that,our model,results observe that our model,0.6364359259605408
translation,99,187,baselines,"mdbt ( ramadan et al. , 2018 )",applies,multiple bi-directional - lstm,"mdbt ( ramadan et al. , 2018 ) applies multiple bi-directional - lstm",0.5134655833244324
translation,99,187,baselines,multiple bi-directional - lstm,to jointly track,domain and states,multiple bi-directional - lstm to jointly track domain and states,0.7595321536064148
translation,99,187,baselines,baselines,has,"mdbt ( ramadan et al. , 2018 )","baselines has mdbt ( ramadan et al. , 2018 )",0.49296942353248596
translation,99,126,experimental-setup,word embeddings,initialized by,concatenation,word embeddings initialized by concatenation,0.6892766356468201
translation,99,126,experimental-setup,concatenation,of,pre-trained glove embeddings,concatenation of pre-trained glove embeddings,0.5550612211227417
translation,99,126,experimental-setup,experimental setup,has,word embeddings,experimental setup has word embeddings,0.4932878315448761
translation,99,127,experimental-setup,batch size,set as,32,batch size set as 32,0.6738126873970032
translation,99,127,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,99,128,experimental-setup,dimensions,of,hidden states,dimensions of hidden states,0.5456419587135315
translation,99,128,experimental-setup,hidden states,in,all grus,hidden states in all grus,0.563558042049408
translation,99,128,experimental-setup,experimental setup,has,dimensions,experimental setup has dimensions,0.4716714024543762
translation,99,129,experimental-setup,embedding dropout,used in,interactive encoder,embedding dropout used in interactive encoder,0.6229289174079895
translation,99,129,experimental-setup,interactive encoder,with,dropout rate 0.3,interactive encoder with dropout rate 0.3,0.6208488345146179
translation,99,129,experimental-setup,experimental setup,has,embedding dropout,experimental setup has embedding dropout,0.5178393125534058
translation,99,130,experimental-setup,word dropout,in,interactive encoder,word dropout in interactive encoder,0.5179798603057861
translation,99,130,experimental-setup,word dropout,to improve,model generalization,word dropout to improve model generalization,0.5832751393318176
translation,99,130,experimental-setup,dropout rate,set as,0.3,dropout rate set as 0.3,0.5706857442855835
translation,99,130,experimental-setup,experimental setup,adopt,word dropout,experimental setup adopt word dropout,0.5728946328163147
translation,99,131,experimental-setup,value generator,uses,"teacher -forcing ( williams and zipser , 1989 )","value generator uses teacher -forcing ( williams and zipser , 1989 )",0.5923700332641602
translation,99,131,experimental-setup,"teacher -forcing ( williams and zipser , 1989 )",with,probability 0.5,"teacher -forcing ( williams and zipser , 1989 ) with probability 0.5",0.606146514415741
translation,99,131,experimental-setup,training time,has,value generator,training time has value generator,0.550658106803894
translation,99,131,experimental-setup,experimental setup,At,training time,experimental setup At training time,0.49713820219039917
translation,99,133,experimental-setup,adam optimizer,to optimize,model,adam optimizer to optimize model,0.7261250615119934
translation,99,133,experimental-setup,model,with,initialized learning rate 0.001,model with initialized learning rate 0.001,0.6201395988464355
translation,99,133,experimental-setup,experimental setup,use,adam optimizer,experimental setup use adam optimizer,0.5987385511398315
translation,99,5,model,model,propose,parallel interactive networks ( pin ),model propose parallel interactive networks ( pin ),0.6785650253295898
translation,99,6,model,interactive encoder,to jointly model,in-turn dependencies and cross-turn dependencies,interactive encoder to jointly model in-turn dependencies and cross-turn dependencies,0.74680495262146
translation,99,6,model,model,integrate,interactive encoder,model integrate interactive encoder,0.7054956555366516
translation,99,7,model,slot-level context,to extract,more expressive features,slot-level context to extract more expressive features,0.6762728095054626
translation,99,7,model,more expressive features,for,different slots,more expressive features for different slots,0.6204053163528442
translation,99,7,model,model,has,slot-level context,model has slot-level context,0.5558120608329773
translation,99,89,model,value generator,takes,slot-level context,value generator takes slot-level context,0.6079440116882324
translation,99,89,model,value generator,uses,gru decoder,value generator uses gru decoder,0.6076105833053589
translation,99,89,model,slot-level context,as,input,slot-level context as input,0.5201157927513123
translation,99,89,model,gru decoder,to generate,value sequence,gru decoder to generate value sequence,0.7151501178741455
translation,99,89,model,value sequence,for,"each ( domain , slot ) pair","value sequence for each ( domain , slot ) pair",0.615251898765564
translation,99,89,model,model,uses,gru decoder,model uses gru decoder,0.6059569120407104
translation,99,89,model,model,has,value generator,model has value generator,0.5774537920951843
translation,99,124,model,model,implemented using,pytorch framework,model implemented using pytorch framework,0.6786549687385559
translation,99,132,model,"greedy search ( vinyals and le , 2015 )",used in,decoding process,"greedy search ( vinyals and le , 2015 ) used in decoding process",0.659683883190155
translation,99,132,model,model,has,"greedy search ( vinyals and le , 2015 )","model has greedy search ( vinyals and le , 2015 )",0.5586686134338379
translation,99,148,results,most of the models,building,classifiers,most of the models building classifiers,0.6735196709632874
translation,99,148,results,most of the models,building,models,most of the models building models,0.6880735754966736
translation,99,148,results,most of the models,building,classifiers and the copy system,most of the models building classifiers and the copy system,0.7012994289398193
translation,99,148,results,models,using,copy system,models using copy system,0.7028648853302002
translation,99,148,results,models,utilizing both,classifiers and the copy system,models utilizing both classifiers and the copy system,0.7249047756195068
translation,99,148,results,states,are,inferior,states are inferior,0.6934813857078552
translation,99,148,results,inferior,to,models,inferior to models,0.6065176725387573
translation,99,148,results,models,utilizing both,classifiers and the copy system,models utilizing both classifiers and the copy system,0.7249047756195068
translation,99,148,results,results,observe,most of the models,results observe most of the models,0.5725881457328796
translation,99,151,results,pin,achieves,significant 3.82 % and 1.27 % performance gain,pin achieves significant 3.82 % and 1.27 % performance gain,0.7026196122169495
translation,99,151,results,baseline model trade,has,pin,baseline model trade has pin,0.5565228462219238
translation,99,151,results,previous state - of- the - art model sst,has,pin,previous state - of- the - art model sst has pin,0.6182189583778381
translation,99,151,results,results,Compared with,baseline model trade,results Compared with baseline model trade,0.6946198344230652
translation,99,157,results,pin model,achieves,much higher goal accuracy,pin model achieves much higher goal accuracy,0.6888628602027893
translation,99,157,results,much higher goal accuracy,than,trade,much higher goal accuracy than trade,0.6166402101516724
translation,99,157,results,much higher goal accuracy,compared with,non-overlapping slots,much higher goal accuracy compared with non-overlapping slots,0.6565271615982056
translation,99,157,results,trade,on,all overlapping slots,trade on all overlapping slots,0.6109188795089722
translation,99,157,results,results,has,pin model,results has pin model,0.5485549569129944
translation,99,164,results,pin model,creates,much fewer prediction errors,pin model creates much fewer prediction errors,0.6218732595443726
translation,99,164,results,much fewer prediction errors,than,trade model,much fewer prediction errors than trade model,0.6107742786407471
translation,99,164,results,much fewer prediction errors,especially on,hotel domain and restaurant domain,much fewer prediction errors especially on hotel domain and restaurant domain,0.6057286262512207
translation,100,185,ablation-analysis,n diag,to,16 or 32,n diag to 16 or 32,0.5703501105308533
translation,100,185,ablation-analysis,n diag,find,jump,n diag find jump,0.6934607028961182
translation,100,185,ablation-analysis,jump,in,performance,jump in performance,0.4416681230068207
translation,100,185,ablation-analysis,ablation analysis,increase,n diag,ablation analysis increase n diag,0.7192721366882324
translation,100,7,baselines,bestperforming models,use,multi-level attention mechanism,bestperforming models use multi-level attention mechanism,0.6360237002372742
translation,100,7,baselines,multi-level attention mechanism,over,set of utterances,multi-level attention mechanism over set of utterances,0.6640112400054932
translation,100,7,baselines,baselines,has,bestperforming models,baselines has bestperforming models,0.5263966917991638
translation,100,5,model,persona embeddings,in,supervised character trope classification task,persona embeddings in supervised character trope classification task,0.48118653893470764
translation,100,168,results,attention model,shows,large improvement,attention model shows large improvement,0.6431238651275635
translation,100,168,results,large improvement,over,baseline models,large improvement over baseline models,0.7054293751716614
translation,100,168,results,results,has,attention model,results has attention model,0.5444419980049133
translation,100,184,results,triplet loss,with,memory modules,triplet loss with memory modules,0.6683416366577148
translation,100,184,results,triplet loss,with,memory,triplet loss with memory,0.6897683143615723
translation,100,184,results,greater performance,compared to,attn 3 model,greater performance compared to attn 3 model,0.7041285634040833
translation,100,184,results,results,Using,triplet loss,results Using triplet loss,0.6673486232757568
translation,101,198,ablation-analysis,ser,is,3.6 %,ser is 3.6 %,0.6118159890174866
translation,101,198,ablation-analysis,ser,just,3.6 %,ser just 3.6 %,0.6822748184204102
translation,101,198,ablation-analysis,extreme 5 - shot setting,has,ser,extreme 5 - shot setting has ser,0.5909242630004883
translation,101,198,ablation-analysis,ablation analysis,Even in,extreme 5 - shot setting,ablation analysis Even in extreme 5 - shot setting,0.6406970620155334
translation,101,27,baselines,schema- guided nlg,represents,slots,schema- guided nlg represents slots,0.6017728447914124
translation,101,27,baselines,slots,using,natural language descriptions,slots using natural language descriptions,0.6657553911209106
translation,101,28,baselines,template guided text generation ( t2g2 ),employs,simple template - based representation,template guided text generation ( t2g2 ) employs simple template - based representation,0.5550875663757324
translation,101,28,baselines,template guided text generation ( t2g2 ),formulates,nlg,template guided text generation ( t2g2 ) formulates nlg,0.685009241104126
translation,101,28,baselines,simple template - based representation,of,system actions,simple template - based representation of system actions,0.5894203782081604
translation,101,28,baselines,simple template - based representation,formulates,nlg,simple template - based representation formulates nlg,0.6677731871604919
translation,101,28,baselines,nlg,as,utterance rewriting task,nlg as utterance rewriting task,0.4941588044166565
translation,101,94,hyperparameters,model,fine-tuned on,corresponding dataset,model fine-tuned on corresponding dataset,0.7216298580169678
translation,101,94,hyperparameters,corresponding dataset,using,constant learning rate,corresponding dataset using constant learning rate,0.6895211935043335
translation,101,94,hyperparameters,corresponding dataset,using,batch size,corresponding dataset using batch size,0.6560582518577576
translation,101,94,hyperparameters,constant learning rate,of,0.001,constant learning rate of 0.001,0.5867102742195129
translation,101,94,hyperparameters,batch size,of,256,batch size of 256,0.6323750615119934
translation,101,94,hyperparameters,256,for,5000 steps,256 for 5000 steps,0.6718148589134216
translation,101,94,hyperparameters,hyperparameters,has,model,hyperparameters has model,0.5282720923423767
translation,101,96,hyperparameters,inference,use,beam search,inference use beam search,0.6354517936706543
translation,101,96,hyperparameters,beam search,with,width,beam search with width,0.6351640820503235
translation,101,96,hyperparameters,beam search,with,length penalty,beam search with length penalty,0.6435361504554749
translation,101,96,hyperparameters,width,of,4,width of 4,0.6958284378051758
translation,101,96,hyperparameters,length penalty,? =,0.6,length penalty ? = 0.6,0.6168525218963623
translation,101,96,hyperparameters,hyperparameters,During,inference,hyperparameters During inference,0.6701886057853699
translation,101,6,model,schemaguided approach,conditions,generation,schemaguided approach conditions generation,0.5923278331756592
translation,101,6,model,generation,on,schema,generation on schema,0.6055868268013
translation,101,6,model,schema,describing,api,schema describing api,0.7056921720504761
translation,101,6,model,model,propose,schemaguided approach,model propose schemaguided approach,0.6867167949676514
translation,101,7,model,small number of templates,growing linearly in,number of slots,small number of templates growing linearly in number of slots,0.6849080324172974
translation,101,7,model,number of slots,to convey,semantics,number of slots to convey semantics,0.6697486639022827
translation,101,7,model,semantics,of,api,semantics of api,0.5243924856185913
translation,101,8,model,utterances,for,arbitrary slot combination,utterances for arbitrary slot combination,0.6742666363716125
translation,101,8,model,first concatenated,to give,"semantically correct , but possibly incoherent and ungrammatical utterance","first concatenated to give semantically correct , but possibly incoherent and ungrammatical utterance",0.6603087782859802
translation,101,8,model,utterances,has,few simple templates,utterances has few simple templates,0.6210896968841553
translation,101,8,model,arbitrary slot combination,has,few simple templates,arbitrary slot combination has few simple templates,0.5969945788383484
translation,101,8,model,model,To generate,utterances,model To generate utterances,0.7606673836708069
translation,101,93,model,6 layers,in,encoder and decoder,6 layers in encoder and decoder,0.534135103225708
translation,101,93,model,6 layers,with,total of around 60 million parameters,6 layers with total of around 60 million parameters,0.6497783064842224
translation,101,93,model,encoder and decoder,with,total of around 60 million parameters,encoder and decoder with total of around 60 million parameters,0.6337733268737793
translation,101,34,results,first nlg results,on,schema- guided dialogue dataset,first nlg results on schema- guided dialogue dataset,0.5305300951004028
translation,101,35,results,proposed methods,robust to,out - of- domain inputs,proposed methods robust to out - of- domain inputs,0.734328031539917
translation,101,35,results,proposed methods,display,improved sample efficiency,proposed methods display improved sample efficiency,0.7389050126075745
translation,101,114,results,all models,exhibit,low ser scores,all models exhibit low ser scores,0.6463589072227478
translation,101,114,results,low ser scores,in,seen and unseen domains,low ser scores in seen and unseen domains,0.5459558367729187
translation,101,114,results,low ser scores,with,template approach,low ser scores with template approach,0.6848884224891663
translation,101,114,results,template approach,being,lowest,template approach being lowest,0.6682894825935364
translation,101,114,results,results,has,all models,results has all models,0.5029959678649902
translation,101,117,results,schema- guided representation,at par with,naive representation,schema- guided representation at par with naive representation,0.6413893699645996
translation,101,117,results,naive representation,on,seen domains,naive representation on seen domains,0.5647704601287842
translation,101,117,results,results,has,schema- guided representation,results has schema- guided representation,0.5506314635276794
translation,101,118,results,performance,on,unseen domains,performance on unseen domains,0.5602566003799438
translation,101,118,results,improve,has,performance,improve has performance,0.5578044652938843
translation,101,118,results,unseen domains,has,+ 0.9 bleu ),unseen domains has + 0.9 bleu ),0.5901570916175842
translation,101,118,results,results,has,slot descriptions,results has slot descriptions,0.5023033618927002
translation,101,120,results,t2g2,outperforms,naive,t2g2 outperforms naive,0.7926827669143677
translation,101,120,results,naive,by,1.7 bleu,naive by 1.7 bleu,0.5384224057197571
translation,101,120,results,seen domains,has,t2g2,seen domains has t2g2,0.6653212904930115
translation,101,120,results,results,For,seen domains,results For seen domains,0.657991886138916
translation,101,170,results,informativeness,notice,all models,informativeness notice all models,0.6566976308822632
translation,101,170,results,all models,perform,well,all models perform well,0.6411154866218567
translation,101,170,results,well,on,seen domains,well on seen domains,0.5644168257713318
translation,101,170,results,results,For,informativeness,results For informativeness,0.6021713614463806
translation,101,172,results,outperforms,by,large margin,outperforms by large margin,0.6290680766105652
translation,101,172,results,naive,by,large margin,naive by large margin,0.6141336560249329
translation,101,172,results,large margin,on,unseen domains,large margin on unseen domains,0.5355376601219177
translation,101,172,results,schema,has,outperforms,schema has outperforms,0.6481790542602539
translation,101,172,results,outperforms,has,naive,outperforms has naive,0.6435060501098633
translation,101,172,results,results,has,schema,results has schema,0.5288384556770325
translation,101,173,results,t2g2,improves upon,schema,t2g2 improves upon schema,0.777103841304779
translation,101,173,results,results,has,t2g2,results has t2g2,0.522894024848938
translation,101,175,results,naive and schema,see,large drops,naive and schema see large drops,0.6165499091148376
translation,101,175,results,large drops,on,unseen domains,large drops on unseen domains,0.5761162638664246
translation,101,175,results,t2g2,performs,equally well,t2g2 performs equally well,0.6451644897460938
translation,101,175,results,equally well,on,seen and unseen domains,equally well on seen and unseen domains,0.5579308271408081
translation,101,175,results,results,has,naive and schema,results has naive and schema,0.570452868938446
translation,101,196,results,improving,as,more training data,improving as more training data,0.5409677028656006
translation,101,196,results,more training data,becomes,available,more training data becomes available,0.603512704372406
translation,101,196,results,performance,has,improving,performance has improving,0.6198736429214478
translation,101,196,results,results,clearly see,performance,results clearly see performance,0.7082498073577881
translation,101,197,results,t2g2,gives,consistent improvements,t2g2 gives consistent improvements,0.645615816116333
translation,101,197,results,t2g2,reducing,ser,t2g2 reducing ser,0.7294667363166809
translation,101,197,results,consistent improvements,of,4 - 5 bleu,consistent improvements of 4 - 5 bleu,0.5766314268112183
translation,101,197,results,ser,by,large margin,ser by large margin,0.6198770403862
translation,101,197,results,all the k-shot settings,has,t2g2,all the k-shot settings has t2g2,0.628605306148529
translation,101,197,results,results,In,all the k-shot settings,results In all the k-shot settings,0.5036605000495911
translation,101,199,results,t2g2,in,80 - shot setting,t2g2 in 80 - shot setting,0.5573049187660217
translation,101,199,results,naive model,trained on,entire dataset,naive model trained on entire dataset,0.6931931376457214
translation,101,199,results,80 - shot setting,has,outperforms,80 - shot setting has outperforms,0.590604305267334
translation,101,199,results,outperforms,has,naive model,outperforms has naive model,0.5956633687019348
translation,101,199,results,results,has,t2g2,results has t2g2,0.522894024848938
translation,101,200,results,t2g2,on par with,80 - shot naive,t2g2 on par with 80 - shot naive,0.7144184112548828
translation,101,200,results,5 - shot setting,has,t2g2,5 - shot setting has t2g2,0.6157611012458801
translation,101,200,results,results,In,5 - shot setting,results In 5 - shot setting,0.5423924922943115
translation,101,212,results,joint modeling,leads to,winwin situation,joint modeling leads to winwin situation,0.6845262050628662
translation,101,212,results,winwin situation,improving,bleu,winwin situation improving bleu,0.6772642731666565
translation,101,212,results,winwin situation,reducing,ser,winwin situation reducing ser,0.7265738844871521
translation,101,212,results,bleu,by,3.4 points,bleu by 3.4 points,0.5879424810409546
translation,101,212,results,ser,from,4.7 %,ser from 4.7 %,0.5707806348800659
translation,101,212,results,4.7 %,has,to just 1 %,4.7 % has to just 1 %,0.5618435740470886
translation,101,219,results,model,benefits from,additional context,model benefits from additional context,0.742204487323761
translation,101,219,results,additional context,showing,improvement,additional context showing improvement,0.7402008771896362
translation,101,219,results,improvement,of,upto 6 bleu,improvement of upto 6 bleu,0.5678547620773315
translation,101,219,results,results,has,model,results has model,0.5339115858078003
translation,102,31,ablation-analysis,response contrastive learning,is,beneficial,response contrastive learning is beneficial,0.5667345523834229
translation,102,31,ablation-analysis,ablation analysis,find that,response contrastive learning,ablation analysis find that response contrastive learning,0.6089794039726257
translation,102,168,baselines,two versions,of,tod - bert,two versions of tod - bert,0.6351191997528076
translation,102,168,baselines,tod - bert - mlm,that only uses,mlm loss,tod - bert - mlm that only uses mlm loss,0.7246375679969788
translation,102,168,baselines,mlm loss,during,pre-training,mlm loss during pre-training,0.6635698676109314
translation,102,168,baselines,tod - bert - jnt,jointly trained with,mlm and rcl objectives,tod - bert - jnt jointly trained with mlm and rcl objectives,0.6348442435264587
translation,102,168,baselines,baselines,investigate,two versions,baselines investigate two versions,0.5933263301849365
translation,102,169,baselines,baselines,compare,tod - bert,baselines compare tod - bert,0.7488105297088623
translation,102,170,baselines,gpt - based model,use,mean pooling,gpt - based model use mean pooling,0.6404608488082886
translation,102,170,baselines,mean pooling,of,hidden states,mean pooling of hidden states,0.6000880002975464
translation,102,170,baselines,mean pooling,as,output representation,mean pooling as output representation,0.5538286566734314
translation,102,170,baselines,hidden states,as,output representation,hidden states as output representation,0.545448899269104
translation,102,170,baselines,better,than using,only the last token,better than using only the last token,0.7272189259529114
translation,102,94,experimental-setup,bert - base uncased model,is,transformer self-attention encoder,bert - base uncased model is transformer self-attention encoder,0.5590464472770691
translation,102,94,experimental-setup,bert - base uncased model,is,12 attention heads,bert - base uncased model is 12 attention heads,0.5719201564788818
translation,102,94,experimental-setup,transformer self-attention encoder,with,12 layers,transformer self-attention encoder with 12 layers,0.6155340075492859
translation,102,94,experimental-setup,12 attention heads,with,hidden size d b = 768,12 attention heads with hidden size d b = 768,0.6526665687561035
translation,102,94,experimental-setup,experimental setup,use,bert - base uncased model,experimental setup use bert - base uncased model,0.5963467359542847
translation,102,95,experimental-setup,speaker information,add,two special tokens,speaker information add two special tokens,0.6274850964546204
translation,102,95,experimental-setup,dialogue,add,two special tokens,dialogue add two special tokens,0.647741973400116
translation,102,95,experimental-setup,two special tokens,to,"bytepair embeddings ( mrk ?i? et al. , 2016 )","two special tokens to bytepair embeddings ( mrk ?i? et al. , 2016 )",0.5405492186546326
translation,102,119,experimental-setup,tod - bert,with,"adamw ( loshchilov and hutter , 2017 ) optimizer","tod - bert with adamw ( loshchilov and hutter , 2017 ) optimizer",0.6209468245506287
translation,102,119,experimental-setup,"adamw ( loshchilov and hutter , 2017 ) optimizer",with,dropout ratio,"adamw ( loshchilov and hutter , 2017 ) optimizer with dropout ratio",0.6001092791557312
translation,102,119,experimental-setup,dropout ratio,of,0.1,dropout ratio of 0.1,0.594265878200531
translation,102,119,experimental-setup,0.1,on,all layers and attention weights,0.1 on all layers and attention weights,0.5097213983535767
translation,102,119,experimental-setup,experimental setup,train,tod - bert,experimental setup train tod - bert,0.6886488199234009
translation,102,120,experimental-setup,experimental setup,has,gelu activation functions,experimental setup has gelu activation functions,0.5171722769737244
translation,102,121,experimental-setup,early -stopped,using,perplexity scores,early -stopped using perplexity scores,0.6067506670951843
translation,102,121,experimental-setup,perplexity scores,of,held - out development set,perplexity scores of held - out development set,0.5544356107711792
translation,102,121,experimental-setup,perplexity scores,with,mini-batches,perplexity scores with mini-batches,0.6210548281669617
translation,102,121,experimental-setup,mini-batches,containing,32 sequences,mini-batches containing 32 sequences,0.7028061747550964
translation,102,121,experimental-setup,32 sequences,of,maximum length,32 sequences of maximum length,0.5860846042633057
translation,102,121,experimental-setup,maximum length,has,512 tokens,maximum length has 512 tokens,0.6037092804908752
translation,102,121,experimental-setup,experimental setup,has,models,experimental setup has models,0.5060054659843445
translation,102,122,experimental-setup,experiments,conducted on,two nvidia tesla v100 gpus,experiments conducted on two nvidia tesla v100 gpus,0.6357418298721313
translation,102,122,experimental-setup,experimental setup,has,experiments,experimental setup has experiments,0.5502888560295105
translation,102,5,experiments,nine human-human and multi-turn task - oriented dialogue datasets,for,language modeling,nine human-human and multi-turn task - oriented dialogue datasets for language modeling,0.5799834132194519
translation,102,23,experiments,nine human-human and multi-turn task - oriented dialogue corpora,to train,task - oriented dialogue bert ( tod - bert ),nine human-human and multi-turn task - oriented dialogue corpora to train task - oriented dialogue bert ( tod - bert ),0.6543358564376831
translation,102,226,hyperparameters,k,for,k-means,k for k-means,0.6694371700286865
translation,102,226,hyperparameters,k-means,equal to,10 and 20,k-means equal to 10 and 20,0.6963246464729309
translation,102,226,hyperparameters,hyperparameters,set,k,hyperparameters set k,0.6529330611228943
translation,102,6,model,user and system tokens,into,masked language modeling,user and system tokens into masked language modeling,0.6236494183540344
translation,102,8,results,strong baselines,like,bert,strong baselines like bert,0.6427571773529053
translation,102,8,results,bert,on,four downstream taskoriented dialogue applications,bert on four downstream taskoriented dialogue applications,0.5318528413772583
translation,102,8,results,four downstream taskoriented dialogue applications,including,intention recognition,four downstream taskoriented dialogue applications including intention recognition,0.6717990636825562
translation,102,8,results,four downstream taskoriented dialogue applications,including,dialogue state tracking,four downstream taskoriented dialogue applications including dialogue state tracking,0.6941533088684082
translation,102,8,results,four downstream taskoriented dialogue applications,including,dialogue act prediction,four downstream taskoriented dialogue applications including dialogue act prediction,0.6797156929969788
translation,102,8,results,four downstream taskoriented dialogue applications,including,response selection,four downstream taskoriented dialogue applications including response selection,0.7032831907272339
translation,102,8,results,pre-trained task- oriented dialogue bert ( tod - bert ),has,outperforms,pre-trained task- oriented dialogue bert ( tod - bert ) has outperforms,0.5957903265953064
translation,102,8,results,outperforms,has,strong baselines,outperforms has strong baselines,0.6115288734436035
translation,102,8,results,results,has,pre-trained task- oriented dialogue bert ( tod - bert ),results has pre-trained task- oriented dialogue bert ( tod - bert ),0.507814347743988
translation,102,30,results,other strong baselines,such as,"gpt - 2 ( radford et al. , 2019 )","other strong baselines such as gpt - 2 ( radford et al. , 2019 )",0.6386186480522156
translation,102,30,results,other strong baselines,such as,"dialogpt ( zhang et al. , 2019 )","other strong baselines such as dialogpt ( zhang et al. , 2019 )",0.5772423148155212
translation,102,30,results,"dialogpt ( zhang et al. , 2019 )",on,all the selected downstream tasks,"dialogpt ( zhang et al. , 2019 ) on all the selected downstream tasks",0.5216302871704102
translation,102,30,results,tod - bert,has,outperforms,tod - bert has outperforms,0.7060930132865906
translation,102,30,results,outperforms,has,bert,outperforms has bert,0.7186265587806702
translation,102,30,results,outperforms,has,other strong baselines,outperforms has other strong baselines,0.5931946635246277
translation,102,30,results,results,observe,tod - bert,results observe tod - bert,0.5969559550285339
translation,102,32,results,stronger few-shot ability,than,bert,stronger few-shot ability than bert,0.5873768925666809
translation,102,32,results,bert,on,each task,bert on each task,0.624862015247345
translation,102,32,results,tod - bert,has,stronger few-shot ability,tod - bert has stronger few-shot ability,0.5835204720497131
translation,102,32,results,results,has,tod - bert,results has tod - bert,0.5926557779312134
translation,102,175,results,domain classification,on,mwoz,domain classification on mwoz,0.5576648712158203
translation,102,175,results,domain classification,on,mwoz,domain classification on mwoz,0.5576648712158203
translation,102,175,results,domain classification,on,mwoz,domain classification on mwoz,0.5576648712158203
translation,102,175,results,intent identification,on,oos,intent identification on oos,0.5804480910301208
translation,102,175,results,dialogue act prediction,on,mwoz,dialogue act prediction on mwoz,0.5688838362693787
translation,102,175,results,results,of,domain classification,results of domain classification,0.5748258233070374
translation,102,181,results,tod - bertjnt,achieves,highest in-scope and out - of-scope accuracy,tod - bertjnt achieves highest in-scope and out - of-scope accuracy,0.684224009513855
translation,102,181,results,results,has,tod - bertjnt,results has tod - bertjnt,0.5690892338752747
translation,102,183,results,in - domain accuracy improvement,compared to,bert,in - domain accuracy improvement compared to bert,0.6399034261703491
translation,102,183,results,bert,in,1 - shot setting,bert in 1 - shot setting,0.5767532587051392
translation,102,183,results,tod - bert - jnt,has,13.2 %,tod - bert - jnt has 13.2 %,0.5976768136024475
translation,102,183,results,13.2 %,has,all- intent accuracy improvement,13.2 % has all- intent accuracy improvement,0.5448060631752014
translation,102,183,results,16.3 %,has,in - domain accuracy improvement,16.3 % has in - domain accuracy improvement,0.5523523688316345
translation,102,183,results,results,has,tod - bert - jnt,results has tod - bert - jnt,0.5863181352615356
translation,102,190,results,bert,find,latter,bert find latter,0.6231339573860168
translation,102,190,results,tod - bertjnt,on,mwoz 2.1 dataset,tod - bertjnt on mwoz 2.1 dataset,0.5094891786575317
translation,102,190,results,latter,has,2.4 % joint goal accuracy improvement,latter has 2.4 % joint goal accuracy improvement,0.5502179265022278
translation,102,190,results,results,compare,bert,results compare bert,0.48626965284347534
translation,102,195,results,outperforms,in,setting,outperforms in setting,0.58339524269104
translation,102,195,results,bert,in,setting,bert in setting,0.532498300075531
translation,102,195,results,tod - bert,has,outperforms,tod - bert has outperforms,0.7060930132865906
translation,102,195,results,outperforms,has,bert,outperforms has bert,0.7186265587806702
translation,102,195,results,results,has,tod - bert,results has tod - bert,0.5926557779312134
translation,102,204,results,consistently works better,than,bert,consistently works better than bert,0.6347274780273438
translation,102,204,results,consistently works better,than,other baselines,consistently works better than other baselines,0.5322906374931335
translation,102,204,results,full data scenario,has,tod - bert,full data scenario has tod - bert,0.6619971990585327
translation,102,204,results,tod - bert,has,consistently works better,tod - bert has consistently works better,0.6260426640510559
translation,102,204,results,results,observe,full data scenario,results observe full data scenario,0.6443494558334351
translation,102,204,results,results,in,full data scenario,results in full data scenario,0.5578352212905884
translation,102,205,results,tod - bert - mlm,outperforms,bert,tod - bert - mlm outperforms bert,0.722986102104187
translation,102,205,results,bert,by,3.5 % micro- f1,bert by 3.5 % micro- f1,0.616074800491333
translation,102,205,results,bert,by,6.6 % macro- f1,bert by 6.6 % macro- f1,0.6056103706359863
translation,102,205,results,bert,on,mwoz corpus,bert on mwoz corpus,0.5802993178367615
translation,102,205,results,bert,in,1 % data scenario,bert in 1 % data scenario,0.616582989692688
translation,102,205,results,6.6 % macro- f1,on,mwoz corpus,6.6 % macro- f1 on mwoz corpus,0.5102898478507996
translation,102,205,results,mwoz corpus,in,1 % data scenario,mwoz corpus in 1 % data scenario,0.5436813831329346
translation,102,205,results,fewshot experiments,has,tod - bert - mlm,fewshot experiments has tod - bert - mlm,0.617994487285614
translation,102,205,results,results,In,fewshot experiments,results In fewshot experiments,0.5433736443519592
translation,102,216,results,tod - bert - jnt,achieves,65.8 % 1 - to - 100 accuracy,tod - bert - jnt achieves 65.8 % 1 - to - 100 accuracy,0.6712042689323425
translation,102,216,results,tod - bert - jnt,achieves,87.0 % 3 - to - 100 accuracy,tod - bert - jnt achieves 87.0 % 3 - to - 100 accuracy,0.6723467707633972
translation,102,216,results,87.0 % 3 - to - 100 accuracy,on,mwoz,87.0 % 3 - to - 100 accuracy on mwoz,0.5672672390937805
translation,102,216,results,87.0 % 3 - to - 100 accuracy,surpasses,bert,87.0 % 3 - to - 100 accuracy surpasses bert,0.6067534685134888
translation,102,216,results,bert,by,18.3 % and 11.5 %,bert by 18.3 % and 11.5 %,0.6294187903404236
translation,102,216,results,results,has,tod - bert - jnt,results has tod - bert - jnt,0.5863181352615356
translation,103,147,ablation-analysis,conkadi,expands,total generated entity number ( e use ),conkadi expands total generated entity number ( e use ),0.6599817276000977
translation,103,147,ablation-analysis,conkadi,expands,total generated entity number ( e use ),conkadi expands total generated entity number ( e use ),0.6599817276000977
translation,103,147,ablation-analysis,conkadi,has,conkadi,conkadi has conkadi,0.703585684299469
translation,103,147,ablation-analysis,ablation analysis,has,conkadi,ablation analysis has conkadi,0.6243479251861572
translation,103,147,ablation-analysis,ablation analysis,has,conkadi,ablation analysis has conkadi,0.6243479251861572
translation,103,148,ablation-analysis,e match,drops by,7.5 %,e match drops by 7.5 %,0.7570571303367615
translation,103,148,ablation-analysis,overall e use,increases by,10 %,overall e use increases by 10 %,0.739734411239624
translation,103,148,ablation-analysis,point- then - copy mechanism,has,overall e use,point- then - copy mechanism has overall e use,0.5672854781150818
translation,103,148,ablation-analysis,ablation analysis,After adding,point- then - copy mechanism,ablation analysis After adding point- then - copy mechanism,0.6703183054924011
translation,103,184,ablation-analysis,drops significantly,without using,context-knowledge fused result,drops significantly without using context-knowledge fused result,0.7456889748573303
translation,103,184,ablation-analysis,context-knowledge fused result,to initialize,decoder,context-knowledge fused result to initialize decoder,0.6854483485221863
translation,103,184,ablation-analysis,performance,has,drops significantly,performance has drops significantly,0.611855685710907
translation,103,185,ablation-analysis,glfact,affect,performance,glfact affect performance,0.6908820867538452
translation,103,192,ablation-analysis,kld,strongly related to,overall performance,kld strongly related to overall performance,0.6954111456871033
translation,103,193,ablation-analysis,importance,using,fused knowledge,importance using fused knowledge,0.6889035105705261
translation,103,193,ablation-analysis,fused knowledge,to initialize,decoder ( ckf ),fused knowledge to initialize decoder ( ckf ),0.6990532875061035
translation,103,212,ablation-analysis,ats2s m m i,is behind,our conkadi,ats2s m m i is behind our conkadi,0.6613489985466003
translation,103,212,ablation-analysis,ats2s m m i,find,mmi,ats2s m m i find mmi,0.5744463801383972
translation,103,212,ablation-analysis,mmi,effectively enhance,ats2s,mmi effectively enhance ats2s,0.6438882946968079
translation,103,60,baselines,conkadi,uses,human 's responses,conkadi uses human 's responses,0.6707310080528259
translation,103,60,baselines,human 's responses,as,posterior knowledge,human 's responses as posterior knowledge,0.5126515030860901
translation,103,60,baselines,posterior knowledge,in,training,posterior knowledge in training,0.5255464315414429
translation,103,60,baselines,baselines,has,conkadi,baselines has conkadi,0.6305895447731018
translation,103,126,hyperparameters,word embedding dimension,is,300,word embedding dimension is 300,0.5878181457519531
translation,103,126,hyperparameters,encoder,is,2 - layer bidirectional gru,encoder is 2 - layer bidirectional gru,0.5515694618225098
translation,103,126,hyperparameters,encoder,is,2 - layer gru,encoder is 2 - layer gru,0.5738245844841003
translation,103,126,hyperparameters,2 - layer bidirectional gru,with,512 units,2 - layer bidirectional gru with 512 units,0.6540791392326355
translation,103,126,hyperparameters,2 - layer bidirectional gru,with,512 units,2 - layer bidirectional gru with 512 units,0.6540791392326355
translation,103,126,hyperparameters,2 - layer bidirectional gru,with,512 units,2 - layer bidirectional gru with 512 units,0.6540791392326355
translation,103,126,hyperparameters,2 - layer gru,with,512 units,2 - layer gru with 512 units,0.6524524688720703
translation,103,126,hyperparameters,hyperparameters,has,word embedding dimension,hyperparameters has word embedding dimension,0.4810734987258911
translation,103,127,hyperparameters,adam,used to,optimizing model,adam used to optimizing model,0.6242205500602722
translation,103,127,hyperparameters,adam,if,perplexity,adam if perplexity,0.6444722414016724
translation,103,127,hyperparameters,optimizing model,with,initial learning rate lr = 0.0001,optimizing model with initial learning rate lr = 0.0001,0.5994210243225098
translation,103,127,hyperparameters,perplexity,begins to,increase,perplexity begins to increase,0.6716007590293884
translation,103,127,hyperparameters,perplexity,increases in,two continuous epochs,perplexity increases in two continuous epochs,0.6234794855117798
translation,103,127,hyperparameters,lr,will be,halved,lr will be halved,0.7345110177993774
translation,103,127,hyperparameters,perplexity,increases in,two continuous epochs,perplexity increases in two continuous epochs,0.6234794855117798
translation,103,127,hyperparameters,hyperparameters,has,adam,hyperparameters has adam,0.5514352917671204
translation,103,128,hyperparameters,maximum epoch number,is,20,maximum epoch number is 20,0.6049567461013794
translation,103,128,hyperparameters,ccm,has,maximum epoch number,ccm has maximum epoch number,0.520574152469635
translation,103,128,hyperparameters,hyperparameters,Following,ccm,hyperparameters Following ccm,0.674561619758606
translation,103,8,model,novel commonsense knowledge - aware dialogue generation,has,conkadi,novel commonsense knowledge - aware dialogue generation has conkadi,0.6044122576713562
translation,103,8,model,model,proposes,novel commonsense knowledge - aware dialogue generation,model proposes novel commonsense knowledge - aware dialogue generation,0.6744109988212585
translation,103,9,model,felicitous fact mechanism,to help,model,felicitous fact mechanism to help model,0.5984976291656494
translation,103,9,model,model,focus on,knowledge facts,model focus on knowledge facts,0.6786850690841675
translation,103,9,model,knowledge facts,highly relevant to,context,knowledge facts highly relevant to context,0.7160488963127136
translation,103,9,model,flexible mode fusion,facilitate,integration,flexible mode fusion facilitate integration,0.6305733323097229
translation,103,9,model,model,design,felicitous fact mechanism,model design felicitous fact mechanism,0.6452585458755493
translation,103,34,model,context knowledge - aware diverse and informative conversation generation,has,conkadi,context knowledge - aware diverse and informative conversation generation has conkadi,0.5953233242034912
translation,103,34,model,model,proposes,context knowledge - aware diverse and informative conversation generation,model proposes context knowledge - aware diverse and informative conversation generation,0.6508089303970337
translation,103,35,model,felicitous fact mechanism,to help,model,felicitous fact mechanism to help model,0.5984976291656494
translation,103,35,model,model,highlight,knowledge facts,model highlight knowledge facts,0.635099470615387
translation,103,35,model,knowledge facts,highly relevant to,context,knowledge facts highly relevant to context,0.7160488963127136
translation,103,35,model,model,design,felicitous fact mechanism,model design felicitous fact mechanism,0.6452585458755493
translation,103,121,model,gends,is,knowledge - aware model,gends is knowledge - aware model,0.5524138808250427
translation,103,121,model,knowledge - aware model,can generate,responses,knowledge - aware model can generate responses,0.7879163026809692
translation,103,121,model,responses,with,utilizing of entity words,responses with utilizing of entity words,0.6662167310714722
translation,103,121,model,model,has,gends,model has gends,0.6533657312393188
translation,103,207,model,model,proposes,novel knowledge - aware model conkadi,model proposes novel knowledge - aware model conkadi,0.6907110810279846
translation,103,46,results,latest knowledge - aware dialogue generation model,has,ccm,latest knowledge - aware dialogue generation model has ccm,0.5314047336578369
translation,103,59,results,conkadi,aware of,context,conkadi aware of context,0.7653050422668457
translation,103,59,results,context,when using,knowledge,context when using knowledge,0.7131220698356628
translation,103,59,results,ccm,has,conkadi,ccm has conkadi,0.6810246109962463
translation,103,59,results,results,In comparison with,ccm,results In comparison with ccm,0.5526353120803833
translation,103,140,results,relative score,seen that,overall performance,relative score seen that overall performance,0.6481294631958008
translation,103,140,results,overall performance,of,conkadi,overall performance of conkadi,0.5756508708000183
translation,103,140,results,overall performance,outperforms,baseline models,overall performance outperforms baseline models,0.7368635535240173
translation,103,140,results,conkadi,outperforms,baseline models,conkadi outperforms baseline models,0.7527890205383301
translation,103,140,results,results,reviewing,relative score,results reviewing relative score,0.6757856607437134
translation,103,141,results,baseline models,in terms of,all metrics,baseline models in terms of all metrics,0.5852673053741455
translation,103,141,results,baseline models,in terms of,almost all metrics,baseline models in terms of almost all metrics,0.5851054787635803
translation,103,141,results,baseline models,in terms of,almost all metrics,baseline models in terms of almost all metrics,0.5851054787635803
translation,103,141,results,all metrics,except,bleu - 3,all metrics except bleu - 3,0.6094212532043457
translation,103,141,results,bleu - 3,on,chinese weibo,bleu - 3 on chinese weibo,0.5174380540847778
translation,103,141,results,baseline models,in terms of,almost all metrics,baseline models in terms of almost all metrics,0.5851054787635803
translation,103,141,results,baseline models,on,english reddit,baseline models on english reddit,0.5208941698074341
translation,103,141,results,outperforms,has,baseline models,outperforms has baseline models,0.5940393209457397
translation,103,141,results,conkadi,has,outperforms,conkadi has outperforms,0.6663835644721985
translation,103,141,results,outperforms,has,baseline models,outperforms has baseline models,0.5940393209457397
translation,103,142,results,our conkadi,increases,overall performance,our conkadi increases overall performance,0.7362622022628784
translation,103,142,results,our conkadi,increases,overall performance,our conkadi increases overall performance,0.7362622022628784
translation,103,142,results,our conkadi,increases,overall performance,our conkadi increases overall performance,0.7362622022628784
translation,103,142,results,overall performance,by,153 % / 95 % ( arithmetic / geometric mean ),overall performance by 153 % / 95 % ( arithmetic / geometric mean ),0.5502955317497253
translation,103,142,results,overall performance,by,48 % / 25 %,overall performance by 48 % / 25 %,0.552044153213501
translation,103,142,results,overall performance,by,48 % / 25 %,overall performance by 48 % / 25 %,0.552044153213501
translation,103,142,results,153 % / 95 % ( arithmetic / geometric mean ),on,chinese dataset,153 % / 95 % ( arithmetic / geometric mean ) on chinese dataset,0.5404535531997681
translation,103,142,results,overall performance,by,48 % / 25 %,overall performance by 48 % / 25 %,0.552044153213501
translation,103,142,results,48 % / 25 %,on,english dataset,48 % / 25 % on english dataset,0.5268360376358032
translation,103,142,results,state- of- the- art method ccm,has,our conkadi,state- of- the- art method ccm has our conkadi,0.6115890741348267
translation,103,142,results,results,In comparison with,state- of- the- art method ccm,results In comparison with state- of- the- art method ccm,0.6630382537841797
translation,103,144,results,three knowledge - aware models,i.e.,gends,three knowledge - aware models i.e. gends,0.6939559578895569
translation,103,144,results,three knowledge - aware models,i.e.,ccm,three knowledge - aware models i.e. ccm,0.68250972032547
translation,103,144,results,three knowledge - aware models,i.e.,conkadi,three knowledge - aware models i.e. conkadi,0.681875467300415
translation,103,144,results,knowledge,has,three knowledge - aware models,knowledge has three knowledge - aware models,0.6020311713218689
translation,103,144,results,significantly outperform,has,other models,significantly outperform has other models,0.5593274831771851
translation,103,144,results,results,accessing,knowledge,results accessing knowledge,0.6412465572357178
translation,103,145,results,advantages,of,conkadi,advantages of conkadi,0.6049726605415344
translation,103,145,results,advantages,of,conkadi,advantages of conkadi,0.6049726605415344
translation,103,145,results,conkadi,higher utilization of,knowledge,conkadi higher utilization of knowledge,0.7562894225120544
translation,103,145,results,conkadi,higher utilization of,knowledge,conkadi higher utilization of knowledge,0.7562894225120544
translation,103,145,results,gends and ccm,has,advantages,gends and ccm has advantages,0.5665350556373596
translation,103,145,results,results,In comparison with,gends and ccm,results In comparison with gends and ccm,0.6300495862960815
translation,103,150,results,conkadi,is,more potential,conkadi is more potential,0.6341638565063477
translation,103,150,results,more potential,to find out,accurate knowledge,more potential to find out accurate knowledge,0.7093139886856079
translation,103,150,results,our e recall,much higher than,e recall,our e recall much higher than e recall,0.63670814037323
translation,103,150,results,e recall,of,gends and ccm,e recall of gends and ccm,0.6024039387702942
translation,103,150,results,results,has,conkadi,results has conkadi,0.564557671546936
translation,103,158,results,not ideal,on,english dataset,not ideal on english dataset,0.511755645275116
translation,103,158,results,chinese dataset,has,conkadi,chinese dataset has conkadi,0.5547190308570862
translation,103,158,results,conkadi,has,best overall performance,conkadi has best overall performance,0.5917299389839172
translation,103,158,results,results,On,chinese dataset,results On chinese dataset,0.5387145280838013
translation,103,174,results,2/3 agreement,is,97 %,2/3 agreement is 97 %,0.5219153165817261
translation,103,174,results,2/3 agreement,is,79.1 %,2/3 agreement is 79.1 %,0.5055227875709534
translation,103,174,results,3/3 agreement,is,79.1 %,3/3 agreement is 79.1 %,0.5196373462677002
translation,103,174,results,informativeness,has,2/3 agreement,informativeness has 2/3 agreement,0.5211915969848633
translation,103,174,results,informativeness,has,3/3 agreement,informativeness has 3/3 agreement,0.5325306057929993
translation,103,174,results,results,For,informativeness,results For informativeness,0.6021713614463806
translation,103,175,results,ats2s m m i,is,strongest baseline,ats2s m m i is strongest baseline,0.581281304359436
translation,103,175,results,strongest baseline,owing to,beam search,strongest baseline owing to beam search,0.712027907371521
translation,103,179,results,more notable advantages,in terms of,informativeness,more notable advantages in terms of informativeness,0.6290380954742432
translation,103,179,results,two metrics,has,conkadi,two metrics has conkadi,0.6121805310249329
translation,103,179,results,conkadi,has,more notable advantages,conkadi has more notable advantages,0.5681605339050293
translation,103,179,results,results,Comparing,two metrics,results Comparing two metrics,0.6790816187858582
translation,104,179,experiments,ten policies,for,each condition,ten policies for each condition,0.6441875100135803
translation,104,179,experiments,ten policies,for,turn-taking approach,ten policies for turn-taking approach,0.661791980266571
translation,104,180,hyperparameters,policies,trained using,qlearning,policies trained using qlearning,0.7043601274490356
translation,104,180,hyperparameters,? greedy search,for,10000 epochs,? greedy search for 10000 epochs,0.5890471935272217
translation,104,180,hyperparameters,10000 epochs,with,0.2,10000 epochs with 0.2,0.569200336933136
translation,104,180,hyperparameters,hyperparameters,has,policies,hyperparameters has policies,0.54888516664505
translation,104,6,model,importance - driven turn-bidding,treats,turn-taking,importance - driven turn-bidding treats turn-taking,0.6684850454330444
translation,104,6,model,turn-taking,as a,negotiative process,turn-taking as a negotiative process,0.5262848734855652
translation,104,6,model,model,treats,turn-taking,model treats turn-taking,0.6780365705490112
translation,104,7,model,bids,for,turn,bids for turn,0.6937168836593628
translation,104,7,model,bids,importance of,intended utterance,bids importance of intended utterance,0.6465091109275818
translation,104,7,model,turn,importance of,intended utterance,turn importance of intended utterance,0.6413979530334473
translation,104,7,model,reinforcement learning,to,indirectly learn,reinforcement learning to indirectly learn,0.5260140299797058
translation,104,7,model,each conversant,has,bids,each conversant has bids,0.6069537401199341
translation,104,7,model,indirectly learn,has,parameter,indirectly learn has parameter,0.5567352175712585
translation,104,7,model,model,has,each conversant,model has each conversant,0.5944914221763611
translation,104,25,model,importance - driven turn-bidding ( idtb ) model,conversants bid for,turn,importance - driven turn-bidding ( idtb ) model conversants bid for turn,0.818527340888977
translation,104,25,model,turn,based on,importance,turn based on importance,0.7404599785804749
translation,104,25,model,importance,of,utterance,importance of utterance,0.6676694750785828
translation,104,25,model,model,propose,importance - driven turn-bidding ( idtb ) model,model propose importance - driven turn-bidding ( idtb ) model,0.6637912392616272
translation,104,26,model,reinforcement learning,to map,given situation,reinforcement learning to map given situation,0.6948546171188354
translation,104,26,model,given situation,to,optimal utterance and bidding behavior,given situation to optimal utterance and bidding behavior,0.5476151704788208
translation,104,26,model,model,use,reinforcement learning,model use reinforcement learning,0.5791643261909485
translation,104,8,results,importance - driven turn-bidding,performs,better,importance - driven turn-bidding performs better,0.6335995197296143
translation,104,8,results,better,than,two current turntaking approaches,better than two current turntaking approaches,0.5879236459732056
translation,104,8,results,two current turntaking approaches,in,artificial collaborative slot-filling domain,two current turntaking approaches in artificial collaborative slot-filling domain,0.5272186398506165
translation,104,8,results,results,find that,importance - driven turn-bidding,results find that importance - driven turn-bidding,0.6310048699378967
translation,104,191,results,kr design,in,all single user conditions,kr design in all single user conditions,0.5573755502700806
translation,104,191,results,idtb turn-taking approach,has,outperforms,idtb turn-taking approach has outperforms,0.6250482797622681
translation,104,191,results,outperforms,has,kr design,outperforms has kr design,0.629906415939331
translation,104,191,results,novice,has,6.09 vs. 6.00 ),novice has 6.09 vs. 6.00 ),0.5060369372367859
translation,104,191,results,results,has,idtb turn-taking approach,results has idtb turn-taking approach,0.5428000688552856
translation,104,203,results,kr,has,outperforms,kr has outperforms,0.7065733075141907
translation,104,203,results,outperforms,has,su,outperforms has su,0.6669356822967529
translation,104,203,results,results,observe,kr,results observe kr,0.48743584752082825
translation,105,143,baselines,mdbt,leverages,semantic interactions,mdbt leverages semantic interactions,0.7758159041404724
translation,105,143,baselines,semantic interactions,between,dialogue utterances and ontology terms,semantic interactions between dialogue utterances and ontology terms,0.599717915058136
translation,105,143,baselines,shared representations,between,slots,shared representations between slots,0.7100309729576111
translation,105,143,baselines,slots,across,domains,slots across domains,0.749893069267273
translation,105,143,baselines,baselines,has,mdbt,baselines has mdbt,0.5751046538352966
translation,105,144,baselines,glad,By utilizing,system actions and user utterances,glad By utilizing system actions and user utterances,0.6575984358787537
translation,105,144,baselines,global modules,to share,parameters,global modules to share parameters,0.6170388460159302
translation,105,144,baselines,parameters,among,slot-value pairs,parameters among slot-value pairs,0.6074981093406677
translation,105,144,baselines,local modules,to learn,slot-specific features,local modules to learn slot-specific features,0.5860632658004761
translation,105,144,baselines,gce,replaces,slot-dependent rnn,gce replaces slot-dependent rnn,0.7135420441627502
translation,105,144,baselines,slot-dependent rnn,with,global conditioning encoder,slot-dependent rnn with global conditioning encoder,0.6271007657051086
translation,105,144,baselines,baselines,has,glad,baselines has glad,0.5778265595436096
translation,105,145,baselines,state- of- the - art model,of,single- domain dst,state- of- the - art model of single- domain dst,0.5331565737724304
translation,105,146,baselines,pointer networks,to generate,start and end positions,pointer networks to generate start and end positions,0.6760249137878418
translation,105,146,baselines,start and end positions,to perform,index - based copying,start and end positions to perform index - based copying,0.6510471105575562
translation,105,146,baselines,copy mechanism,shares,parameters,copy mechanism shares parameters,0.7221181392669678
translation,105,146,baselines,copy mechanism,to generate,dialogue states,copy mechanism to generate dialogue states,0.7001913189888
translation,105,146,baselines,parameters,across,domains,parameters across domains,0.7167588472366333
translation,105,146,baselines,dialogue states,from,user utterances,dialogue states from user utterances,0.5435653924942017
translation,105,132,hyperparameters,word embeddings,initialized with,400 - dimensional pre-trained em-beddings,word embeddings initialized with 400 - dimensional pre-trained em-beddings,0.7367207407951355
translation,105,132,hyperparameters,400 - dimensional pre-trained em-beddings,concatenated,glove embeddings,400 - dimensional pre-trained em-beddings concatenated glove embeddings,0.7263885140419006
translation,105,132,hyperparameters,hyperparameters,has,word embeddings,hyperparameters has word embeddings,0.4784160554409027
translation,105,133,hyperparameters,dimension,of,hidden units,dimension of hidden units,0.5992026925086975
translation,105,133,hyperparameters,dimension,of,node embeddings,dimension of node embeddings,0.5527431964874268
translation,105,133,hyperparameters,dimension,of,node embeddings,dimension of node embeddings,0.5527431964874268
translation,105,133,hyperparameters,hidden units,for,first layer,hidden units for first layer,0.5853694081306458
translation,105,133,hyperparameters,first layer,set to,512,first layer set to 512,0.714115560054779
translation,105,133,hyperparameters,dimension,of,node embeddings,dimension of node embeddings,0.5527431964874268
translation,105,133,hyperparameters,node embeddings,set to,400,node embeddings set to 400,0.6625738143920898
translation,105,133,hyperparameters,two -layer graph convolutional networks,has,dimension,two -layer graph convolutional networks has dimension,0.515238881111145
translation,105,133,hyperparameters,two -layer graph convolutional networks,has,dimension,two -layer graph convolutional networks has dimension,0.515238881111145
translation,105,133,hyperparameters,hyperparameters,For,two -layer graph convolutional networks,hyperparameters For two -layer graph convolutional networks,0.5533549785614014
translation,105,134,hyperparameters,input adjacency matrix a,by,row-normalization,input adjacency matrix a by row-normalization,0.5343142747879028
translation,105,134,hyperparameters,hyperparameters,initialize,input adjacency matrix a,hyperparameters initialize input adjacency matrix a,0.7662515640258789
translation,105,135,hyperparameters,node embeddings,to convert,dialogue states,node embeddings to convert dialogue states,0.6037474870681763
translation,105,135,hyperparameters,dialogue states,into,400 - dimensional vector representations,dialogue states into 400 - dimensional vector representations,0.5652908086776733
translation,105,135,hyperparameters,hyperparameters,use,node embeddings,hyperparameters use node embeddings,0.5716835856437683
translation,105,136,hyperparameters,training,set,dropout,training set dropout,0.6984118819236755
translation,105,136,hyperparameters,dropout,with,0.2 ratio,dropout with 0.2 ratio,0.6766482591629028
translation,105,136,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
translation,105,138,hyperparameters,"adam optimizer ( kingma and ba , 2015 )",with,batch size,"adam optimizer ( kingma and ba , 2015 ) with batch size",0.6134414076805115
translation,105,138,hyperparameters,batch size,of,32,batch size of 32,0.6741614937782288
translation,105,138,hyperparameters,hyperparameters,trained by using,"adam optimizer ( kingma and ba , 2015 )","hyperparameters trained by using adam optimizer ( kingma and ba , 2015 )",0.7062163352966309
translation,105,139,hyperparameters,early stopping,based on,joint goal accuracy,early stopping based on joint goal accuracy,0.609652578830719
translation,105,139,hyperparameters,hyperparameters,apply,early stopping,hyperparameters apply early stopping,0.6104239225387573
translation,105,7,model,multi-domain dst task,construct,dialogue state graph,multi-domain dst task construct dialogue state graph,0.6579230427742004
translation,105,7,model,dialogue state graph,to transfer,structured features,dialogue state graph to transfer structured features,0.6386035084724426
translation,105,7,model,structured features,among,related domain-slot pairs,structured features among related domain-slot pairs,0.5908862948417664
translation,105,7,model,related domain-slot pairs,across,domains,related domain-slot pairs across domains,0.7105565071105957
translation,105,8,model,graph information,of,dialogue states,graph information of dialogue states,0.5309920907020569
translation,105,8,model,dialogue states,by,graph convolutional networks,dialogue states by graph convolutional networks,0.4840545654296875
translation,105,8,model,hard copy mechanism,to directly copy,historical states,hard copy mechanism to directly copy historical states,0.618675708770752
translation,105,8,model,historical states,from,previous conversation,historical states from previous conversation,0.5344270467758179
translation,105,8,model,model,encode,graph information,model encode graph information,0.8002357482910156
translation,105,43,model,model,come up with,more scalable multi-domain dst model,model come up with more scalable multi-domain dst model,0.6280807256698608
translation,105,44,model,state graph,for,each conversation,state graph for each conversation,0.6141420602798462
translation,105,44,model,model,construct,state graph,model construct state graph,0.7278366684913635
translation,105,45,model,graph convolutional networks ( gcn ),to better encode,structured information,graph convolutional networks ( gcn ) to better encode structured information,0.7158492803573608
translation,105,45,model,structured information,representations of,history state nodes,structured information representations of history state nodes,0.6747446656227112
translation,105,45,model,model,introduce,graph convolutional networks ( gcn ),model introduce graph convolutional networks ( gcn ),0.6118636131286621
translation,105,46,model,gcn,recursively aggregates,neighbour information,gcn recursively aggregates neighbour information,0.7506037354469299
translation,105,46,model,gcn,then extracts,state-centric representations,gcn then extracts state-centric representations,0.6543514728546143
translation,105,46,model,neighbour information,over,dialogue state graph,neighbour information over dialogue state graph,0.6496330499649048
translation,105,46,model,dialogue state graph,via,efficient graph convolution operations,dialogue state graph via efficient graph convolution operations,0.6474408507347107
translation,105,46,model,state-centric representations,to benefit,feature,state-centric representations to benefit feature,0.6635068655014038
translation,105,46,model,feature,across,domains,feature across domains,0.7363254427909851
translation,105,46,model,each node,has,gcn,each node has gcn,0.6355732679367065
translation,105,46,model,model,For,each node,model For each node,0.6216675043106079
translation,105,47,model,previous states,from,dialogue history,previous states from dialogue history,0.5547457337379456
translation,105,47,model,previous states,propose,hard copy mechanism,previous states propose hard copy mechanism,0.684901237487793
translation,105,47,model,hard copy mechanism,for,decoder,hard copy mechanism for decoder,0.6591989994049072
translation,105,47,model,hard copy mechanism,to pick up,history states,hard copy mechanism to pick up history states,0.7129174470901489
translation,105,47,model,decoder,to pick up,history states,decoder to pick up history states,0.6461747884750366
translation,105,47,model,history states,has,directly,history states has directly,0.6583738327026367
translation,105,47,model,model,utilize,previous states,model utilize previous states,0.6556317806243896
translation,105,47,model,model,propose,hard copy mechanism,model propose hard copy mechanism,0.7009924650192261
translation,105,59,model,hard copy mechanism,in,dialogue decoder,hard copy mechanism in dialogue decoder,0.5415346622467041
translation,105,59,model,hard copy mechanism,to get,history states,hard copy mechanism to get history states,0.6458300948143005
translation,105,59,model,history states,from,last prediction,history states from last prediction,0.5565187931060791
translation,105,59,model,model,propose,hard copy mechanism,model propose hard copy mechanism,0.7009924650192261
translation,105,60,model,framework,consists of,four main components,framework consists of four main components,0.6970287561416626
translation,105,60,model,model,has,framework,model has framework,0.5441871285438538
translation,105,61,model,? state graph representation,extracts,graphstructured information,? state graph representation extracts graphstructured information,0.6295433640480042
translation,105,61,model,? state graph representation,provides,node representations,? state graph representation provides node representations,0.6597154140472412
translation,105,61,model,graphstructured information,of,dialogue states,graphstructured information of dialogue states,0.5567803978919983
translation,105,61,model,dialogue states,in,conversation,dialogue states in conversation,0.5590251684188843
translation,105,61,model,node representations,using,graph embeddings,node representations using graph embeddings,0.6223251223564148
translation,105,61,model,model,has,? state graph representation,model has ? state graph representation,0.5672675371170044
translation,105,62,model,dialogue encoder,models,history utterances and states,dialogue encoder models history utterances and states,0.723357617855072
translation,105,62,model,history utterances and states,of,previous turns,history utterances and states of previous turns,0.5413289666175842
translation,105,62,model,previous turns,into,sequence of fixed - length vectors,previous turns into sequence of fixed - length vectors,0.5477008819580078
translation,105,62,model,model,has,dialogue encoder,model has dialogue encoder,0.574840247631073
translation,105,9,results,our model,improves,performances,our model improves performances,0.711121141910553
translation,105,9,results,performances,of,multi-domain dst baseline ( trade ),performances of multi-domain dst baseline ( trade ),0.5389817357063293
translation,105,9,results,multi-domain dst baseline ( trade ),with,absolute joint accuracy,multi-domain dst baseline ( trade ) with absolute joint accuracy,0.6250062584877014
translation,105,9,results,absolute joint accuracy,of,2.0 % and 1.0 %,absolute joint accuracy of 2.0 % and 1.0 %,0.5645056366920471
translation,105,9,results,2.0 % and 1.0 %,on,multiwoz 2.0 and 2.1 dialogue datasets,2.0 % and 1.0 % on multiwoz 2.0 and 2.1 dialogue datasets,0.4969649016857147
translation,105,9,results,results,show,our model,results show our model,0.6888449192047119
translation,105,50,results,proposed multi-domain dst approach,improves,2.0 %,proposed multi-domain dst approach improves 2.0 %,0.6627119183540344
translation,105,50,results,2.0 %,/,1.0 %,2.0 % / 1.0 %,0.6210070252418518
translation,105,50,results,2.0 %,of,joint accuracy,2.0 % of joint accuracy,0.570708692073822
translation,105,50,results,1.0 %,of,joint accuracy,1.0 % of joint accuracy,0.5685540437698364
translation,105,50,results,joint accuracy,over,baseline,joint accuracy over baseline,0.6884251236915588
translation,105,50,results,results,show,proposed multi-domain dst approach,results show proposed multi-domain dst approach,0.6509191989898682
translation,105,148,results,gcdst,in,outperforming,gcdst in outperforming,0.5659064650535583
translation,105,148,results,outperforming,with,absolute improvements,outperforming with absolute improvements,0.7110196352005005
translation,105,148,results,absolute improvements,about,2 %,absolute improvements about 2 %,0.6394921541213989
translation,105,148,results,absolute improvements,about,1 %,absolute improvements about 1 %,0.6314274072647095
translation,105,148,results,2 %,on,multiwoz 2.0,2 % on multiwoz 2.0,0.5851672887802124
translation,105,148,results,2 %,on,multiwoz 2.1,2 % on multiwoz 2.1,0.5736781358718872
translation,105,148,results,2 %,on,multiwoz 2.1,2 % on multiwoz 2.1,0.5736781358718872
translation,105,148,results,1 %,on,multiwoz 2.1,1 % on multiwoz 2.1,0.5777959227561951
translation,105,148,results,outperforming,has,baseline ( trade ),outperforming has baseline ( trade ),0.6240322589874268
translation,105,148,results,outperforming,has,1 %,outperforming has 1 %,0.6237695813179016
translation,105,148,results,results,compare,gcdst,results compare gcdst,0.6463515162467957
translation,105,154,results,best performance,on,multiwoz 2.0 and 2.1,best performance on multiwoz 2.0 and 2.1,0.5642574429512024
translation,105,154,results,results,GCDST with,slot-connection,results GCDST with slot-connection,0.7056174278259277
translation,105,155,results,other two connection types,by,value ( valueconnection and slot / value-connection,other two connection types by value ( valueconnection and slot / value-connection,0.5461694598197937
translation,105,155,results,other two connection types,achieve,comparative performances,other two connection types achieve comparative performances,0.5815016627311707
translation,105,155,results,results,has,other two connection types,results has other two connection types,0.5633524656295776
translation,105,159,results,domain-connection,obtains,worse performances,domain-connection obtains worse performances,0.6165236830711365
translation,105,159,results,results,has,domain-connection,results has domain-connection,0.5109654664993286
translation,105,164,results,both of the state encoders,improve,gcdst,both of the state encoders improve gcdst,0.6601759791374207
translation,105,164,results,gcdst model,with,hard copy mechanism,gcdst model with hard copy mechanism,0.602225661277771
translation,105,164,results,gcdst model,is,slightly better,gcdst model is slightly better,0.5536285042762756
translation,105,164,results,hard copy mechanism,is,slightly better,hard copy mechanism is slightly better,0.5581061244010925
translation,105,164,results,slightly better,than,attention - based method,slightly better than attention - based method,0.5961161255836487
translation,105,164,results,slightly better,with,attention - based method,slightly better with attention - based method,0.6513373851776123
translation,105,164,results,results,observe,both of the state encoders,results observe both of the state encoders,0.5962846279144287
translation,105,172,results,consistent trend,of,baseline and gcdst,consistent trend of baseline and gcdst,0.6132495403289795
translation,105,172,results,consistent trend,both,baseline and gcdst,consistent trend both baseline and gcdst,0.7231221199035645
translation,105,172,results,baseline and gcdst,As,conversation,baseline and gcdst As conversation,0.5910402536392212
translation,105,172,results,conversation,progresses through,more turns,conversation progresses through more turns,0.7648113965988159
translation,105,172,results,performances,of,gcdst and the baseline,performances of gcdst and the baseline,0.581605851650238
translation,105,172,results,performances,of,decrease,performances of decrease,0.5979405641555786
translation,105,172,results,consistent trend,has,performances,consistent trend has performances,0.6256693601608276
translation,105,172,results,baseline and gcdst,has,performances,baseline and gcdst has performances,0.5573766231536865
translation,105,172,results,gcdst and the baseline,has,decrease,gcdst and the baseline has decrease,0.5566666126251221
translation,105,172,results,results,see,consistent trend,results see consistent trend,0.6222550868988037
translation,105,173,results,gcdst and the baseline,achieve,comparable performance,gcdst and the baseline achieve comparable performance,0.6321415901184082
translation,105,173,results,comparable performance,with,short dialogue history ( turn ? 6 ),comparable performance with short dialogue history ( turn ? 6 ),0.6717481017112732
translation,105,173,results,results,has,gcdst and the baseline,results has gcdst and the baseline,0.5440357327461243
translation,105,174,results,conversation,goes on,gcdst,conversation goes on gcdst,0.7927255630493164
translation,105,174,results,gcdst,performs,better,gcdst performs better,0.653413712978363
translation,105,174,results,better,with,longer dialogue contexts,better with longer dialogue contexts,0.6583676934242249
translation,105,174,results,conversation,has,gcdst,conversation has gcdst,0.6441425085067749
translation,105,177,results,average length,is,4.48 turns,average length is 4.48 turns,0.5672228336334229
translation,105,177,results,statistics,has,average length,statistics has average length,0.520233690738678
translation,105,177,results,gcdst,has,correctly predicts,gcdst has correctly predicts,0.646405041217804
translation,105,177,results,gcdst,has,average length,gcdst has average length,0.542253851890564
translation,105,177,results,baseline,has,fails,baseline has fails,0.6687555909156799
translation,106,74,hyperparameters,"sets of ( current speaker , previous speakers , next speakers , context words )",as,training examples,"sets of ( current speaker , previous speakers , next speakers , context words ) as training examples",0.45657771825790405
translation,106,74,hyperparameters,hyperparameters,start by,"sets of ( current speaker , previous speakers , next speakers , context words )","hyperparameters start by sets of ( current speaker , previous speakers , next speakers , context words )",0.6294590830802917
translation,106,74,hyperparameters,hyperparameters,collecting,"sets of ( current speaker , previous speakers , next speakers , context words )","hyperparameters collecting sets of ( current speaker , previous speakers , next speakers , context words )",0.5907459259033203
translation,106,152,hyperparameters,pre-trained contextualized word representations,from,neural language models ( elmo ),pre-trained contextualized word representations from neural language models ( elmo ),0.5465009212493896
translation,106,152,hyperparameters,pre-trained contextualized word representations,to generate,character names representations,pre-trained contextualized word representations to generate character names representations,0.6524142622947693
translation,106,152,hyperparameters,character names representations,sentences that include,names,character names representations sentences that include names,0.6626766920089722
translation,106,152,hyperparameters,hyperparameters,use,pre-trained contextualized word representations,hyperparameters use pre-trained contextualized word representations,0.5453343987464905
translation,106,162,hyperparameters,"gensim ( ?eh?ek and sojka , 2010 )",set,learning rate,"gensim ( ?eh?ek and sojka , 2010 ) set learning rate",0.6163760423660278
translation,106,162,hyperparameters,"gensim ( ?eh?ek and sojka , 2010 )",set,window size,"gensim ( ?eh?ek and sojka , 2010 ) set window size",0.6464194059371948
translation,106,162,hyperparameters,"gensim ( ?eh?ek and sojka , 2010 )",set,samples,"gensim ( ?eh?ek and sojka , 2010 ) set samples",0.6494777798652649
translation,106,162,hyperparameters,learning rate,to,0.1,learning rate to 0.1,0.5543926954269409
translation,106,162,hyperparameters,window size,to,4,window size to 4,0.614766001701355
translation,106,162,hyperparameters,samples,to,50,samples to 50,0.6216602921485901
translation,106,162,hyperparameters,50,for,negative sampling,50 for negative sampling,0.6555037498474121
translation,106,162,hyperparameters,hyperparameters,For,"gensim ( ?eh?ek and sojka , 2010 )","hyperparameters For gensim ( ?eh?ek and sojka , 2010 )",0.5638502836227417
translation,106,163,hyperparameters,30 epochs,to train,baselines,30 epochs to train baselines,0.6891592144966125
translation,106,163,hyperparameters,hyperparameters,run,30 epochs,hyperparameters run 30 epochs,0.6892256140708923
translation,106,164,hyperparameters,post-training,by,our models,post-training by our models,0.6078577637672424
translation,106,164,hyperparameters,post-training,use,gradient decent,post-training use gradient decent,0.6541140079498291
translation,106,164,hyperparameters,our models,use,gradient decent,our models use gradient decent,0.6860097050666809
translation,106,164,hyperparameters,gradient decent,to update,our parameters,gradient decent to update our parameters,0.7794865965843201
translation,106,164,hyperparameters,hyperparameters,For,post-training,hyperparameters For post-training,0.5465461611747742
translation,106,165,hyperparameters,learning rate,to,0.1,learning rate to 0.1,0.5543926954269409
translation,106,165,hyperparameters,decays,factor of,0.9,decays factor of 0.9,0.6431951522827148
translation,106,165,hyperparameters,learning rate,has,decays,learning rate has decays,0.5639533996582031
translation,106,165,hyperparameters,0.9,has,per 10 epochs,0.9 has per 10 epochs,0.5682880282402039
translation,106,166,hyperparameters,maximum 40 epochs,for,post-training,maximum 40 epochs for post-training,0.5524287819862366
translation,106,166,hyperparameters,hyperparameters,run,maximum 40 epochs,hyperparameters run maximum 40 epochs,0.6959162950515747
translation,106,167,hyperparameters,character embedding ( cbow ),use,context window,character embedding ( cbow ) use context window,0.6009795069694519
translation,106,167,hyperparameters,context window,of,size,context window of size,0.559281587600708
translation,106,167,hyperparameters,size,has,two,size has two,0.6613075733184814
translation,106,167,hyperparameters,hyperparameters,For,character embedding ( cbow ),hyperparameters For character embedding ( cbow ),0.5752325057983398
translation,106,4,model,new embedding model,to represent,movie characters and their interactions,new embedding model to represent movie characters and their interactions,0.6688880920410156
translation,106,4,model,movie characters and their interactions,in,dialogue,movie characters and their interactions in dialogue,0.544857382774353
translation,106,4,model,movie characters and their interactions,in,dialogue,movie characters and their interactions in dialogue,0.544857382774353
translation,106,4,model,movie characters and their interactions,other participants in,dialogue,movie characters and their interactions other participants in dialogue,0.7432606816291809
translation,106,4,model,dialogue,by encoding,same representation,dialogue by encoding same representation,0.8293584585189819
translation,106,4,model,same representation,language used by,characters,same representation language used by characters,0.7337266802787781
translation,106,4,model,model,introduce,new embedding model,model introduce new embedding model,0.6111253499984741
translation,106,39,model,dialogue turns,annotated with,corresponding speaker names,dialogue turns annotated with corresponding speaker names,0.7084439396858215
translation,106,39,model,vector representation,for,each of these characters,vector representation for each of these characters,0.5981156826019287
translation,106,39,model,each of these characters,captures,relation with other characters,each of these characters captures relation with other characters,0.7410703301429749
translation,106,40,model,approach,to embed,characters,approach to embed characters,0.7430970072746277
translation,106,40,model,characters,in,dialogues,characters in dialogues,0.5562729239463806
translation,106,60,model,character names,in,dialogue settings,character names in dialogue settings,0.5184969305992126
translation,106,60,model,different embeddings,for,characters,different embeddings for characters,0.6243089437484741
translation,106,60,model,different embeddings,reflects,relatedness,different embeddings reflects relatedness,0.6764232516288757
translation,106,60,model,characters,from,different story dialogues,characters from different story dialogues,0.5767658352851868
translation,106,60,model,relatedness,of,story characters,relatedness of story characters,0.5878474712371826
translation,106,60,model,speaker prediction,as,auxiliary supervision,speaker prediction as auxiliary supervision,0.4975365996360779
translation,106,60,model,auxiliary supervision,to improve,character representation,auxiliary supervision to improve character representation,0.6492637991905212
translation,106,60,model,model,representing,character names,model representing character names,0.6830832958221436
translation,106,60,model,model,learning,different embeddings,model learning different embeddings,0.7222582697868347
translation,106,42,results,model,yields,strong empirical performance,model yields strong empirical performance,0.7029988169670105
translation,106,43,results,large margin,has,several strong baselines,large margin has several strong baselines,0.5820046663284302
translation,106,177,results,character embedding model,has,consistently outperforms,character embedding model has consistently outperforms,0.6218776702880859
translation,106,177,results,consistently outperforms,has,traditional word2 vec baseline models,consistently outperforms has traditional word2 vec baseline models,0.5848119854927063
translation,106,177,results,results,show that,character embedding model,results show that character embedding model,0.5137045979499817
translation,106,184,results,classifiers,using,our character embedding models,classifiers using our character embedding models,0.6724061965942383
translation,106,184,results,classifiers,using,other models,classifiers using other models,0.7044926285743713
translation,106,184,results,classifiers,trained using,other models,classifiers trained using other models,0.7267578840255737
translation,106,184,results,classifiers,trained using,other models,classifiers trained using other models,0.7267578840255737
translation,106,184,results,our character embedding models,has,consistently outperforms,our character embedding models has consistently outperforms,0.6064639091491699
translation,106,184,results,consistently outperforms,has,classifiers,consistently outperforms has classifiers,0.6273003220558167
translation,106,184,results,results,Training,classifiers,results Training classifiers,0.7027063369750977
translation,106,199,results,our character embeddings,bring,improvements,our character embeddings bring improvements,0.6232941746711731
translation,106,199,results,improvements,over,pre-trained glove embeddings,improvements over pre-trained glove embeddings,0.6535443067550659
translation,106,199,results,results,use of,our character embeddings,results use of our character embeddings,0.5777671933174133
translation,107,104,experimental-setup,classifier probing,fine - tune,top layer,classifier probing fine - tune top layer,0.7155734896659851
translation,107,104,experimental-setup,top layer,with,consistent hyperparameter setting,top layer with consistent hyperparameter setting,0.6058638095855713
translation,107,104,experimental-setup,experimental setup,For,classifier probing,experimental setup For classifier probing,0.5987574458122253
translation,107,105,experimental-setup,"adamw ( loshchilov and hutter , 2017 ) optimizer",with,learning rate,"adamw ( loshchilov and hutter , 2017 ) optimizer with learning rate",0.598147988319397
translation,107,105,experimental-setup,"adamw ( loshchilov and hutter , 2017 ) optimizer",with,gradient clipping,"adamw ( loshchilov and hutter , 2017 ) optimizer with gradient clipping",0.6036669611930847
translation,107,105,experimental-setup,learning rate,has,5e ?5,learning rate has 5e ?5,0.6281245350837708
translation,107,105,experimental-setup,gradient clipping,has,1.0,gradient clipping has 1.0,0.5276963710784912
translation,107,105,experimental-setup,experimental setup,apply,"adamw ( loshchilov and hutter , 2017 ) optimizer","experimental setup apply adamw ( loshchilov and hutter , 2017 ) optimizer",0.5947006344795227
translation,107,107,experimental-setup,gmm clustering,from,scikit-learn library,gmm clustering from scikit-learn library,0.5563800930976868
translation,107,107,experimental-setup,gmm clustering,adopt,k-means implementation,gmm clustering adopt k-means implementation,0.6640288233757019
translation,107,107,experimental-setup,k-means implementation,from,faiss library,k-means implementation from faiss library,0.5373579859733582
translation,107,107,experimental-setup,experimental setup,use,gmm clustering,experimental setup use gmm clustering,0.6019420623779297
translation,107,107,experimental-setup,experimental setup,adopt,k-means implementation,experimental setup adopt k-means implementation,0.6231687664985657
translation,107,108,experimental-setup,experiments,conducted on,single nvidia tesla v100 gpu,experiments conducted on single nvidia tesla v100 gpu,0.6428636312484741
translation,107,108,experimental-setup,experimental setup,has,experiments,experimental setup has experiments,0.5502888560295105
translation,107,106,experiments,"k = 4 , 8 , 16 , 32 , 64 , 128 , 256",with,50 iterations,"k = 4 , 8 , 16 , 32 , 64 , 128 , 256 with 50 iterations",0.6185890436172485
translation,107,106,experiments,moving trend,for,mi probing,moving trend for mi probing,0.6943047046661377
translation,107,115,experiments,"convert , tod - bert - jnt , and tod - gpt2",achieve,best performance,"convert , tod - bert - jnt , and tod - gpt2 achieve best performance",0.6303939819335938
translation,107,123,experiments,tod - bert - jnt and convert,with,highest mutual information,tod - bert - jnt and convert with highest mutual information,0.6929107904434204
translation,107,123,experiments,tod - bert - jnt and convert,followed by,tod - gpt2 and distilbert,tod - bert - jnt and convert followed by tod - gpt2 and distilbert,0.6828089356422424
translation,107,6,model,feed-forward layer,as,classifier probe,feed-forward layer as classifier probe,0.5927512049674988
translation,107,6,model,classifier probe,on top of,fixed pretrained language model,classifier probe on top of fixed pretrained language model,0.6262830495834351
translation,107,6,model,fixed pretrained language model,with,annotated labels,fixed pretrained language model with annotated labels,0.5877552032470703
translation,107,6,model,annotated labels,in,supervised way,annotated labels in supervised way,0.5160281658172607
translation,107,6,model,model,fine- tune,feed-forward layer,model fine- tune feed-forward layer,0.7192814350128174
translation,107,7,model,unsupervised mutual information probe,to evaluate,mutual dependence,unsupervised mutual information probe to evaluate mutual dependence,0.6803301572799683
translation,107,7,model,mutual dependence,between,real clustering,mutual dependence between real clustering,0.7016500234603882
translation,107,7,model,mutual dependence,between,representation clustering,mutual dependence between representation clustering,0.6545166969299316
translation,107,7,model,model,propose,unsupervised mutual information probe,model propose unsupervised mutual information probe,0.7273596525192261
translation,107,20,model,probe,uses,supervision,probe uses supervision,0.6527242660522461
translation,107,20,model,supervision,to find,best transformation,supervision to find best transformation,0.6391302943229675
translation,107,20,model,best transformation,for,each sub-task,best transformation for each sub-task,0.5906726717948914
translation,107,20,model,model,has,probe,model has probe,0.6362772583961487
translation,107,21,model,mutual information probe,to investigate,language models,mutual information probe to investigate language models,0.6242363452911377
translation,107,21,model,language models,by directly clustering,output representations,language models by directly clustering output representations,0.6756842732429504
translation,107,21,model,model,present,mutual information probe,model present mutual information probe,0.7086366415023804
translation,107,57,model,task - oriented gpt2 model ( tod - gpt2 ),built on,gpt2 model,task - oriented gpt2 model ( tod - gpt2 ) built on gpt2 model,0.6783611178398132
translation,107,57,model,task - oriented gpt2 model ( tod - gpt2 ),pre-trained with,task - oriented datasets,task - oriented gpt2 model ( tod - gpt2 ) pre-trained with task - oriented datasets,0.7784029841423035
translation,107,57,model,model,train,task - oriented gpt2 model ( tod - gpt2 ),model train task - oriented gpt2 model ( tod - gpt2 ),0.7076306343078613
translation,107,30,results,and gpt2,not perform,well,and gpt2 not perform well,0.6593896746635437
translation,107,30,results,well,on,mutual information evaluation,well on mutual information evaluation,0.5445218682289124
translation,107,30,results,bert,has,surprisingly outperforms,bert has surprisingly outperforms,0.6671963930130005
translation,107,30,results,surprisingly outperforms,has,bert,surprisingly outperforms has bert,0.6737461090087891
translation,107,116,results,performance,of,convert and tod - bert - jnt,performance of convert and tod - bert - jnt,0.6484060287475586
translation,107,116,results,convert and tod - bert - jnt,suggests,helpful,convert and tod - bert - jnt suggests helpful,0.6046178340911865
translation,107,116,results,helpful,to,pre-train,helpful to pre-train,0.6099008917808533
translation,107,116,results,pre-train,with,response selection contrastive objective,pre-train with response selection contrastive objective,0.630666196346283
translation,107,116,results,results,has,performance,results has performance,0.5972660779953003
translation,107,134,results,pre-trained language models,shown,relatively high anmi score ( average 0.226 ),pre-trained language models shown relatively high anmi score ( average 0.226 ),0.6065635681152344
translation,107,134,results,pre-trained language models,closed,gap,pre-trained language models closed gap,0.6451343894004822
translation,107,134,results,gap,between,performance,gap between performance,0.6517583131790161
translation,107,134,results,gap,between,top model,gap between top model,0.7145615816116333
translation,107,134,results,results,find that,pre-trained language models,results find that pre-trained language models,0.561853289604187
translation,107,134,results,results,most of,pre-trained language models,results most of pre-trained language models,0.5569318532943726
translation,108,128,hyperparameters,rl agent,size of,hidden layer,rl agent size of hidden layer,0.6909579634666443
translation,108,128,hyperparameters,hidden layer,to,80,hidden layer to 80,0.6270195245742798
translation,108,128,hyperparameters,hyperparameters,For,rl agent,hyperparameters For rl agent,0.5498552322387695
translation,108,129,hyperparameters,hidden layer size,of,80,hidden layer size of 80,0.6321588158607483
translation,108,129,hyperparameters,hrl agent,has,both top-level and lowlevel dialogue policies,hrl agent has both top-level and lowlevel dialogue policies,0.5825844407081604
translation,108,129,hyperparameters,both top-level and lowlevel dialogue policies,has,hidden layer size,both top-level and lowlevel dialogue policies has hidden layer size,0.536537230014801
translation,108,129,hyperparameters,hyperparameters,For,hrl agent,hyperparameters For hrl agent,0.5831660628318787
translation,108,130,hyperparameters,rmsprop,to,optimize,rmsprop to optimize,0.5737003087997437
translation,108,130,hyperparameters,optimize,has,parameters,optimize has parameters,0.5561850070953369
translation,108,130,hyperparameters,hyperparameters,has,rmsprop,hyperparameters has rmsprop,0.510215699672699
translation,108,131,hyperparameters,hyperparameters,set,batch size,hyperparameters set batch size,0.6402808427810669
translation,108,132,hyperparameters,training,used,?- greedy strategy,training used ?- greedy strategy,0.6272115707397461
translation,108,132,hyperparameters,?- greedy strategy,for,exploration,?- greedy strategy for exploration,0.6539242267608643
translation,108,132,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
translation,108,6,model,hierarchical deep reinforcement learning approach,to learning,dialogue manager,hierarchical deep reinforcement learning approach to learning dialogue manager,0.5919294953346252
translation,108,6,model,dialogue manager,operates at,different temporal scales,dialogue manager operates at different temporal scales,0.6779652833938599
translation,108,6,model,model,proposing,hierarchical deep reinforcement learning approach,model proposing hierarchical deep reinforcement learning approach,0.7126025557518005
translation,108,7,model,dialogue manager,consists of,top-level dialogue policy,dialogue manager consists of top-level dialogue policy,0.6346109509468079
translation,108,7,model,dialogue manager,consists of,low-level dialogue policy,dialogue manager consists of low-level dialogue policy,0.639856219291687
translation,108,7,model,dialogue manager,consists of,global state tracker,dialogue manager consists of global state tracker,0.6341056227684021
translation,108,7,model,top-level dialogue policy,selects among,subtasks or options,top-level dialogue policy selects among subtasks or options,0.719342827796936
translation,108,7,model,low-level dialogue policy,selects,primitive actions,low-level dialogue policy selects primitive actions,0.6813970804214478
translation,108,7,model,primitive actions,to complete,subtask,primitive actions to complete subtask,0.666732132434845
translation,108,7,model,subtask,given by,top-level policy,subtask given by top-level policy,0.6604347229003906
translation,108,7,model,global state tracker,helps ensure,all cross-subtask constraints,global state tracker helps ensure all cross-subtask constraints,0.6794831156730652
translation,108,7,model,all cross-subtask constraints,has,be satisfied,all cross-subtask constraints has be satisfied,0.5681784152984619
translation,108,7,model,model,has,dialogue manager,model has dialogue manager,0.597151517868042
translation,108,26,model,task,using,mathematical framework,task using mathematical framework,0.6594611406326294
translation,108,26,model,hierarchical task decomposition,to train,composite taskcompletion dialogue agent,hierarchical task decomposition to train composite taskcompletion dialogue agent,0.6433247923851013
translation,108,26,model,mathematical framework,has,of options over mdps,mathematical framework has of options over mdps,0.5381989479064941
translation,108,26,model,model,formulating,task,model formulating task,0.7311835289001465
translation,108,27,model,dialogue manager,consists of,top-level dialogue policy,dialogue manager consists of top-level dialogue policy,0.6346109509468079
translation,108,27,model,dialogue manager,consists of,low-level dialogue policy,dialogue manager consists of low-level dialogue policy,0.639856219291687
translation,108,27,model,dialogue manager,consists of,global state tracker,dialogue manager consists of global state tracker,0.6341056227684021
translation,108,27,model,top-level dialogue policy,selects,subtasks ( options ),top-level dialogue policy selects subtasks ( options ),0.7041513323783875
translation,108,27,model,low-level dialogue policy,selects,primitive actions,low-level dialogue policy selects primitive actions,0.6813970804214478
translation,108,27,model,primitive actions,to complete,given subtask,primitive actions to complete given subtask,0.6707189679145813
translation,108,27,model,global state tracker,helps ensure,all cross-subtask constraints,global state tracker helps ensure all cross-subtask constraints,0.6794831156730652
translation,108,27,model,all cross-subtask constraints,has,be satisfied,all cross-subtask constraints has be satisfied,0.5681784152984619
translation,108,28,model,structural information,of,composite tasks,structural information of composite tasks,0.5993391275405884
translation,108,28,model,structural information,for,efficient exploration,structural information for efficient exploration,0.6130094528198242
translation,108,29,model,reward sparsity issue,equip,our agent,reward sparsity issue equip our agent,0.6170088052749634
translation,108,29,model,our agent,with,evaluation module ( internal critic ),our agent with evaluation module ( internal critic ),0.6302865147590637
translation,108,29,model,evaluation module ( internal critic ),gives,intrinsic reward signals,evaluation module ( internal critic ) gives intrinsic reward signals,0.5486755967140198
translation,108,29,model,intrinsic reward signals,indicating,likely,intrinsic reward signals indicating likely,0.6597736477851868
translation,108,29,model,model,to mitigate,reward sparsity issue,model to mitigate reward sparsity issue,0.661474347114563
translation,108,51,model,composite task -completion dialogue agent,consists of,four components,composite task -completion dialogue agent consists of four components,0.6531259417533875
translation,108,51,model,lstmbased language understanding module,for identifying,user intents,lstmbased language understanding module for identifying user intents,0.6766290664672852
translation,108,51,model,lstmbased language understanding module,extracting,associated slots,lstmbased language understanding module extracting associated slots,0.7444494962692261
translation,108,51,model,state tracker,for tracking,dialogue state,state tracker for tracking dialogue state,0.6972235441207886
translation,108,51,model,dialogue policy,selects,next action,dialogue policy selects next action,0.7249482870101929
translation,108,51,model,next action,based on,current state,next action based on current state,0.6122869253158569
translation,108,51,model,model - based natural language generator,for converting,agent actions,model - based natural language generator for converting agent actions,0.64311283826828
translation,108,51,model,agent actions,to,natural language responses,agent actions to natural language responses,0.5017271637916565
translation,108,51,model,four components,has,lstmbased language understanding module,four components has lstmbased language understanding module,0.5343242287635803
translation,108,51,model,model,has,composite task -completion dialogue agent,model has composite task -completion dialogue agent,0.5486758947372437
translation,108,192,model,policy learning problem,using,options framework,policy learning problem using options framework,0.6593921780586243
translation,108,192,model,policy learning problem,take,hierarchical deep rl approach,policy learning problem take hierarchical deep rl approach,0.6113524436950684
translation,108,192,model,hierarchical deep rl approach,to optimizing,policy,hierarchical deep rl approach to optimizing policy,0.6827856302261353
translation,108,192,model,model,formulate,policy learning problem,model formulate policy learning problem,0.6496443748474121
translation,108,192,model,model,take,hierarchical deep rl approach,model take hierarchical deep rl approach,0.6204898953437805
translation,108,149,results,hrl - based agent,yielded,more robust dialogue policies,hrl - based agent yielded more robust dialogue policies,0.6307414174079895
translation,108,149,results,flat rl - based agent,measured on,success rate,flat rl - based agent measured on success rate,0.6626476645469666
translation,108,149,results,all types of users,has,hrl - based agent,all types of users has hrl - based agent,0.6010940670967102
translation,108,149,results,more robust dialogue policies,has,outperforming,more robust dialogue policies has outperforming,0.5792936682701111
translation,108,149,results,outperforming,has,hand-crafted rule- based agents,outperforming has hand-crafted rule- based agents,0.5246738791465759
translation,108,149,results,results,For,all types of users,results For all types of users,0.5736709237098694
translation,108,152,results,hrl agent,has,significantly outperformed,hrl agent has significantly outperformed,0.6441125869750977
translation,108,152,results,significantly outperformed,has,rl agent,significantly outperformed has rl agent,0.6125772595405579
translation,108,152,results,results,has,hrl agent,results has hrl agent,0.5595330595970154
translation,108,170,results,rule + and flat rl agents,has,comparable success rates,rule + and flat rl agents has comparable success rates,0.5811119675636292
translation,108,170,results,results,found that,rule + and flat rl agents,results found that rule + and flat rl agents,0.6600163578987122
translation,108,187,results,consistently better,than,rl agent,consistently better than rl agent,0.5564343333244324
translation,108,187,results,rl agent,in terms of,success rate,rl agent in terms of success rate,0.6921353936195374
translation,108,187,results,rl agent,in terms of,user rating,rl agent in terms of user rating,0.6618528366088867
translation,108,187,results,all the cases,has,hrl agent,all the cases has hrl agent,0.6388341784477234
translation,108,187,results,results,For,all the cases,results For all the cases,0.613803505897522
translation,109,4,baselines,natural language generator,based on,sequence - to-sequence approach,natural language generator based on sequence - to-sequence approach,0.6640257239341736
translation,109,4,baselines,natural language strings,as well as,deep syntax dependency trees,natural language strings as well as deep syntax dependency trees,0.5333788990974426
translation,109,4,baselines,deep syntax dependency trees,from,input dialogue acts,deep syntax dependency trees from input dialogue acts,0.4871149957180023
translation,109,64,hyperparameters,seq2seq generator,use,adam optimizer,seq2seq generator use adam optimizer,0.5819425582885742
translation,109,64,hyperparameters,adam optimizer,to minimize,unweighted sequence cross-entropy,adam optimizer to minimize unweighted sequence cross-entropy,0.6922350525856018
translation,109,64,hyperparameters,hyperparameters,To train,seq2seq generator,hyperparameters To train seq2seq generator,0.659948468208313
translation,109,68,hyperparameters,adam optimizer,minimizing,crossentropy,adam optimizer minimizing crossentropy,0.6926241517066956
translation,109,68,hyperparameters,crossentropy,to train,reranker,crossentropy to train reranker,0.6835416555404663
translation,109,68,hyperparameters,hyperparameters,use,adam optimizer,hyperparameters use adam optimizer,0.6090166568756104
translation,109,74,hyperparameters,learning rate,set to,0.001,learning rate set to 0.001,0.6954665780067444
translation,109,74,hyperparameters,embedding size,has,50,embedding size has 50,0.6383599638938904
translation,109,74,hyperparameters,lstm cell size,has,128,lstm cell size has 128,0.5944570302963257
translation,109,74,hyperparameters,batch size,has,20,batch size has 20,0.6332210898399353
translation,109,75,hyperparameters,reranking penalty,for,decoding,reranking penalty for decoding,0.6121492981910706
translation,109,75,hyperparameters,decoding,is,100,decoding is 100,0.6640955209732056
translation,109,75,hyperparameters,hyperparameters,has,reranking penalty,hyperparameters has reranking penalty,0.48466235399246216
translation,109,11,model,nlg system,for,sds,nlg system for sds,0.7257900238037109
translation,109,11,model,deep syntax dependency trees,subsequently processed by,external surface realizer,deep syntax dependency trees subsequently processed by external surface realizer,0.5874133706092834
translation,109,13,model,generator,based on,sequence - tosequence ( seq2seq ) generation technique,generator based on sequence - tosequence ( seq2seq ) generation technique,0.704014241695404
translation,109,13,model,generator,based on,n-best list reranker,generator based on n-best list reranker,0.667921781539917
translation,109,13,model,sequence - tosequence ( seq2seq ) generation technique,combined with,beam search,sequence - tosequence ( seq2seq ) generation technique combined with beam search,0.7133063077926636
translation,109,13,model,sequence - tosequence ( seq2seq ) generation technique,combined with,n-best list reranker,sequence - tosequence ( seq2seq ) generation technique combined with n-best list reranker,0.6980812549591064
translation,109,13,model,n-best list reranker,to suppress,irrelevant information,n-best list reranker to suppress irrelevant information,0.660043478012085
translation,109,13,model,irrelevant information,in,outputs,irrelevant information in outputs,0.5371178984642029
translation,109,13,model,model,has,generator,model has generator,0.5837957262992859
translation,109,6,results,joint setup,offers,better performance,joint setup offers better performance,0.702544629573822
translation,109,6,results,joint setup,surpassing,state - of - the - art,joint setup surpassing state - of - the - art,0.7475469708442688
translation,109,6,results,better performance,surpassing,state - of - the - art,better performance surpassing state - of - the - art,0.7643982172012329
translation,109,6,results,state - of - the - art,with regards to,ngram-based scores,state - of - the - art with regards to ngram-based scores,0.6165834069252014
translation,109,6,results,state - of - the - art,providing,more relevant outputs,state - of - the - art providing more relevant outputs,0.6385439038276672
translation,109,6,results,results,has,joint setup,results has joint setup,0.5118283629417419
translation,109,14,results,sds,trainable from,unaligned pairs,sds trainable from unaligned pairs,0.7077375054359436
translation,109,14,results,unaligned pairs,of,mr and sentences,unaligned pairs of mr and sentences,0.6026231050491333
translation,109,89,results,beam search,brings,bleu improvement,beam search brings bleu improvement,0.6290719509124756
translation,109,89,results,beam search,keeps,most semantic errors,beam search keeps most semantic errors,0.6953818202018738
translation,109,89,results,bleu improvement,keeps,most semantic errors,bleu improvement keeps most semantic errors,0.6146041750907898
translation,109,89,results,results,has,beam search,results has beam search,0.5500618815422058
translation,109,90,results,reranker,able to,reduce,reranker able to reduce,0.6163744330406189
translation,109,90,results,reduce,number of,semantic errors,reduce number of semantic errors,0.6348678469657898
translation,109,90,results,increasing,has,automatic scores,increasing has automatic scores,0.6122753024101257
translation,109,90,results,automatic scores,has,considerably,automatic scores has considerably,0.6153501868247986
translation,109,90,results,results,has,reranker,results has reranker,0.5974451899528503
translation,109,91,results,larger beam,effect of,reranker,larger beam effect of reranker,0.6876954436302185
translation,109,91,results,reranker,resulting in,slightly improved outputs,reranker resulting in slightly improved outputs,0.6452092528343201
translation,109,91,results,results,Using,larger beam,results Using larger beam,0.7047793865203857
translation,109,97,results,semantic error rates,of,greedy and beam-search decoding,semantic error rates of greedy and beam-search decoding,0.5709379315376282
translation,109,97,results,semantic error rates,lower than for,string - based models,semantic error rates lower than for string - based models,0.67791748046875
translation,109,97,results,greedy and beam-search decoding,lower than for,string - based models,greedy and beam-search decoding lower than for string - based models,0.7040553689002991
translation,109,97,results,results,has,semantic error rates,results has semantic error rates,0.5239688158035278
translation,109,98,results,beam search,increase in,bleu,beam search increase in bleu,0.6889479160308838
translation,109,98,results,results,has,beam search,results has beam search,0.5500618815422058
translation,109,102,results,models generating trees,produce,less semantic errors,models generating trees produce less semantic errors,0.661842942237854
translation,109,102,results,models generating trees,gain,higher bleu / nist scores,models generating trees gain higher bleu / nist scores,0.7134361863136292
translation,109,102,results,reranker,has,models generating trees,reranker has models generating trees,0.5621895790100098
translation,109,102,results,results,Without,reranker,results Without reranker,0.7352014183998108
translation,110,22,experiments,telugu,is,agglutinative south indian language,telugu is agglutinative south indian language,0.5539048910140991
translation,110,22,experiments,agglutinative south indian language,belongs to,family of dravidian languages,agglutinative south indian language belongs to family of dravidian languages,0.7124840617179871
translation,110,96,hyperparameters,filter,of size,4,filter of size 4,0.7172988057136536
translation,110,96,hyperparameters,filter,used for,convolutions,filter used for convolutions,0.712130606174469
translation,110,96,hyperparameters,hyperparameters,has,filter,hyperparameters has filter,0.5657525062561035
translation,110,101,hyperparameters,hidden dimension,of,bi-lstm,hidden dimension of bi-lstm,0.5931493043899536
translation,110,101,hyperparameters,bi-lstm,is,64,bi-lstm is 64,0.5694122314453125
translation,110,101,hyperparameters,hyperparameters,has,hidden dimension,hyperparameters has hidden dimension,0.528815507888794
translation,110,102,hyperparameters,dropout rate,set,0.4,dropout rate set 0.4,0.6308208107948303
translation,110,102,hyperparameters,0.4,for avoiding,overfitting,0.4 for avoiding overfitting,0.5969836711883545
translation,110,102,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,110,106,hyperparameters,hidden dimension,of,lstm,hidden dimension of lstm,0.600773811340332
translation,110,106,hyperparameters,hidden dimension,is,32,hidden dimension is 32,0.5924611687660217
translation,110,106,hyperparameters,lstm,is,32,lstm is 32,0.5601624846458435
translation,110,106,hyperparameters,hyperparameters,has,hidden dimension,hyperparameters has hidden dimension,0.528815507888794
translation,110,107,hyperparameters,10 epochs,performed,drop out rate,10 epochs performed drop out rate,0.23094677925109863
translation,110,107,hyperparameters,drop out rate,set to,0.2,drop out rate set to 0.2,0.6658059358596802
translation,110,107,hyperparameters,hyperparameters,total of,10 epochs,hyperparameters total of 10 epochs,0.6078428030014038
translation,110,107,hyperparameters,hyperparameters,has,drop out rate,hyperparameters has drop out rate,0.5236823558807373
translation,110,168,results,lstm,has,outperforms,lstm has outperforms,0.6320200562477112
translation,110,168,results,outperforms,has,all the other algorithms,outperforms has all the other algorithms,0.5748441219329834
translation,110,168,results,results,notice,lstm,results notice lstm,0.6827359199523926
translation,111,7,model,novel discourse relation identification,tuned for,opendomain dialogue systems,novel discourse relation identification tuned for opendomain dialogue systems,0.6844045519828796
translation,111,7,model,model,designed,novel discourse relation identification,model designed novel discourse relation identification,0.6059169769287109
translation,111,8,model,method,to automatically extract,implicit discourse relation argument pairs and labels,method to automatically extract implicit discourse relation argument pairs and labels,0.7056542038917542
translation,111,8,model,implicit discourse relation argument pairs and labels,from,dataset of dialogic turns,implicit discourse relation argument pairs and labels from dataset of dialogic turns,0.5489422678947449
translation,111,31,model,feature - based model,with,different dialogue feature combinations,feature - based model with different dialogue feature combinations,0.6448142528533936
translation,111,31,model,feature - based model,enhanced,deep learning model,feature - based model enhanced deep learning model,0.646953821182251
translation,111,31,model,deep learning model,by incorporating,dialogue features,deep learning model by incorporating dialogue features,0.6452276706695557
translation,111,31,model,dialogue features,that utilize,aspects,dialogue features that utilize aspects,0.7393949627876282
translation,111,31,model,aspects,unique to,dialogue,aspects unique to dialogue,0.7904021739959717
translation,111,31,model,model,investigated,feature - based model,model investigated feature - based model,0.6726595163345337
translation,112,191,ablation-analysis,c,increases from,0.5 to 4,c increases from 0.5 to 4,0.6864842176437378
translation,112,191,ablation-analysis,final success rate,gets,improved,final success rate gets improved,0.6008514165878296
translation,112,191,ablation-analysis,c,has,final success rate,c has final success rate,0.5804239511489868
translation,112,191,ablation-analysis,0.5 to 4,has,final success rate,0.5 to 4 has final success rate,0.501697838306427
translation,112,5,baselines,mcts - ddu,performs,decision - time planning,mcts - ddu performs decision - time planning,0.5925161242485046
translation,112,5,baselines,decision - time planning,based on,dialogue state search trees,decision - time planning based on dialogue state search trees,0.6150504946708679
translation,112,5,baselines,dialogue state search trees,built by,monte carlo simulations,dialogue state search trees built by monte carlo simulations,0.6950947046279907
translation,112,150,baselines,dqn,using,real experiences,dqn using real experiences,0.6995943188667297
translation,112,150,baselines,agent,trained by,dqn,agent trained by dqn,0.7512404918670654
translation,112,150,baselines,dqn,using,real experiences,dqn using real experiences,0.6995943188667297
translation,112,150,baselines,dqn,has,agent,dqn has agent,0.6253786087036133
translation,112,150,baselines,baselines,has,dqn,baselines has dqn,0.5911124348640442
translation,112,228,baselines,mcts - ddu,uses,c = 4,mcts - ddu uses c = 4,0.6712260246276855
translation,112,228,baselines,mcts - ddu,runs,50 simulations,mcts - ddu runs 50 simulations,0.7085065841674805
translation,112,228,baselines,c = 4,in,uct d,c = 4 in uct d,0.6063212156295776
translation,112,228,baselines,50 simulations,per,dialogue turn,50 simulations per dialogue turn,0.6521242260932922
translation,112,228,baselines,baselines,has,mcts - ddu,baselines has mcts - ddu,0.5569069981575012
translation,112,157,experimental-setup,main components,of,qnetworks and models,main components of qnetworks and models,0.5772163271903992
translation,112,157,experimental-setup,qnetworks and models,implemented as,two -layer neural networks,qnetworks and models implemented as two -layer neural networks,0.6247000098228455
translation,112,157,experimental-setup,two -layer neural networks,with,hidden size,two -layer neural networks with hidden size,0.6181150078773499
translation,112,157,experimental-setup,two -layer neural networks,with,relu activation,two -layer neural networks with relu activation,0.6249148845672607
translation,112,157,experimental-setup,hidden size,being,80,hidden size being 80,0.6393378973007202
translation,112,157,experimental-setup,agents,has,main components,agents has main components,0.5586130619049072
translation,112,220,experimental-setup,one nvidia v100 gpu,as,computing infrastructure,one nvidia v100 gpu as computing infrastructure,0.5127403736114502
translation,112,220,experimental-setup,average runtime,is,about 2 hours per trial,average runtime is about 2 hours per trial,0.5383046269416809
translation,112,220,experimental-setup,experimental setup,use,one nvidia v100 gpu,experimental setup use one nvidia v100 gpu,0.5973011255264282
translation,112,229,experimental-setup,results,sampled at,"episode 100 , 200 , and 300","results sampled at episode 100 , 200 , and 300",0.6771020889282227
translation,112,229,experimental-setup,results,averaged over,three random seeds,results averaged over three random seeds,0.688684344291687
translation,112,229,experimental-setup,experimental setup,sampled at,"episode 100 , 200 , and 300","experimental setup sampled at episode 100 , 200 , and 300",0.6930516958236694
translation,112,229,experimental-setup,experimental setup,has,results,experimental setup has results,0.5482938289642334
translation,112,230,experimental-setup,initialized randomly,without,extra human conversational data pre-training,initialized randomly without extra human conversational data pre-training,0.6846956610679626
translation,112,230,experimental-setup,experimental setup,has,model parameters,experimental setup has model parameters,0.4974170923233032
translation,112,209,experiments,mcts - ddu agent,exceeds,recent background planning approaches,mcts - ddu agent exceeds recent background planning approaches,0.596078097820282
translation,112,209,experiments,recent background planning approaches,by,wide margin,recent background planning approaches by wide margin,0.5504389405250549
translation,112,209,experiments,wide margin,with,extraordinary data efficiency,wide margin with extraordinary data efficiency,0.6720951199531555
translation,112,209,experiments,movie-ticket booking task,has,mcts - ddu agent,movie-ticket booking task has mcts - ddu agent,0.5845997929573059
translation,112,4,model,double-q dueling network ( mcts - ddu ),for,task - completion dialogue policy learning,double-q dueling network ( mcts - ddu ) for task - completion dialogue policy learning,0.5853543877601624
translation,112,24,model,common baseline model,of,task -completion dialogue policy learning problem,common baseline model of task -completion dialogue policy learning problem,0.5381918549537659
translation,112,24,model,deep q-network ( dqn ),adopting,two variants,deep q-network ( dqn ) adopting two variants,0.681799054145813
translation,112,24,model,model,upgrade,common baseline model,model upgrade common baseline model,0.7666584253311157
translation,112,25,model,purpose,fully exploit,advanced valuebased methods,purpose fully exploit advanced valuebased methods,0.6548041105270386
translation,112,25,model,advanced valuebased methods,orthogonal to,planning,advanced valuebased methods orthogonal to planning,0.6856439709663391
translation,112,25,model,model,fully exploit,advanced valuebased methods,model fully exploit advanced valuebased methods,0.7553455829620361
translation,112,25,model,model,has,purpose,model has purpose,0.5476922392845154
translation,112,27,model,monte carlo tree search ( mcts ),as,decision - time planning,monte carlo tree search ( mcts ) as decision - time planning,0.5327938795089722
translation,112,156,model,mcts,with,qnetwork,mcts with qnetwork,0.6689351797103882
translation,112,156,model,decision time,has,actions,decision time has actions,0.5910207033157349
translation,112,156,model,model,in,decision time,model in decision time,0.5219781398773193
translation,112,207,model,novel way,to apply,deep model - based rl,novel way to apply deep model - based rl,0.6807510852813721
translation,112,207,model,deep model - based rl,to,task - completion dialogue policy learning,deep model - based rl to task - completion dialogue policy learning,0.5195772051811218
translation,112,207,model,model,introduces,novel way,model introduces novel way,0.7231190204620361
translation,112,208,model,advanced valuebased methods,with,mcts,advanced valuebased methods with mcts,0.6409061551094055
translation,112,208,model,mcts,as,decision - time planning,mcts as decision - time planning,0.553956151008606
translation,112,208,model,model,combine,advanced valuebased methods,model combine advanced valuebased methods,0.7020984888076782
translation,112,26,results,new baseline,achieve,comparable performance,new baseline achieve comparable performance,0.6415096521377563
translation,112,26,results,comparable performance,with,ddq,comparable performance with ddq,0.68911212682724
translation,112,26,results,results,show,new baseline,results show new baseline,0.6830980777740479
translation,112,165,results,highest performance,at,all times,highest performance at all times,0.5377716422080994
translation,112,165,results,mcts - ddu,has,highest performance,mcts - ddu has highest performance,0.579290509223938
translation,112,165,results,results,has,mcts - ddu,results has mcts - ddu,0.5333306789398193
translation,112,175,results,mcts - ddu,exceeds,ddu,mcts - ddu exceeds ddu,0.7270106077194214
translation,112,175,results,ddu,by,absolute 8.9 %,ddu by absolute 8.9 %,0.5798912644386292
translation,112,175,results,ddu,by,relative 9.8 %,ddu by relative 9.8 %,0.5827948451042175
translation,112,175,results,relative 9.8 %,with,73.3 % faster efficiency,relative 9.8 % with 73.3 % faster efficiency,0.617145836353302
translation,112,176,results,ddu,could achieve,slightly higher performance,ddu could achieve slightly higher performance,0.7116329669952393
translation,112,176,results,slightly higher performance,than,ddq,slightly higher performance than ddq,0.6074473261833191
translation,112,176,results,ddq,has,20 ),ddq has 20 ),0.6996118426322937
translation,112,176,results,results,has,interesting observation,results has interesting observation,0.5519416928291321
translation,112,178,results,advanced valuebased learning methods,bring,considerable improvement,advanced valuebased learning methods bring considerable improvement,0.6152986288070679
translation,112,178,results,results,Solely using,advanced valuebased learning methods,results Solely using advanced valuebased learning methods,0.6564452648162842
translation,112,182,results,prior knowledge,for,exploration,prior knowledge for exploration,0.6145908236503601
translation,112,182,results,performance,of,plain mcts,performance of plain mcts,0.5698737502098083
translation,112,182,results,much worse,than,mcts - ddu,much worse than mcts - ddu,0.5964189767837524
translation,112,182,results,prior knowledge,has,performance,prior knowledge has performance,0.5786580443382263
translation,112,182,results,exploration,has,performance,exploration has performance,0.5744457840919495
translation,112,187,results,mcts - du,runs into,performance bottleneck,mcts - du runs into performance bottleneck,0.7500354647636414
translation,112,187,results,performance bottleneck,after,200 episodes,performance bottleneck after 200 episodes,0.6908558011054993
translation,112,238,results,planning,outperform than,dqn,planning outperform than dqn,0.7433845400810242
translation,112,238,results,results,Methods incorporated with,planning,results Methods incorporated with planning,0.7139093279838562
translation,113,193,ablation-analysis,speaker - level encoder,is,slightly more important,speaker - level encoder is slightly more important,0.5223667621612549
translation,113,193,ablation-analysis,slightly more important,in,overall performance,slightly more important in overall performance,0.5150855779647827
translation,113,193,ablation-analysis,ablation analysis,remove,one at a time,ablation analysis remove one at a time,0.7152028679847717
translation,113,195,ablation-analysis,very poor f1 score,of,36.7 %,very poor f1 score of 36.7 %,0.5229885578155518
translation,113,196,ablation-analysis,ablation analysis,effect of,edge relation modelling,ablation analysis effect of edge relation modelling,0.6613306999206543
translation,113,199,ablation-analysis,very important,for modelling,emotional dynamics,very important for modelling emotional dynamics,0.7332830429077148
translation,113,199,ablation-analysis,ablation analysis,having,different relational edges,ablation analysis having different relational edges,0.6349214315414429
translation,113,139,baselines,baseline convolutional neural network based model,identical to,our utterance level feature extractor network,baseline convolutional neural network based model identical to our utterance level feature extractor network,0.5542024970054626
translation,113,149,baselines,baselines,has,c-lstm + att,baselines has c-lstm + att,0.5606198310852051
translation,113,152,baselines,cmn,models,utterance context,cmn models utterance context,0.6874228119850159
translation,113,152,baselines,utterance context,from,dialogue history,utterance context from dialogue history,0.5768158435821533
translation,113,152,baselines,dialogue history,using,two distinct grus,dialogue history using two distinct grus,0.6774619817733765
translation,113,152,baselines,two distinct grus,for,two speakers,two distinct grus for two speakers,0.7085379362106323
translation,113,152,baselines,baselines,has,cmn,baselines has cmn,0.5792206525802612
translation,113,155,baselines,icon,connects,outputs,icon connects outputs,0.7326902747154236
translation,113,155,baselines,outputs,of,individual speaker grus,outputs of individual speaker grus,0.629061222076416
translation,113,155,baselines,individual speaker grus,in,cmn,individual speaker grus in cmn,0.5694366693496704
translation,113,155,baselines,individual speaker grus,using,another gru,individual speaker grus using another gru,0.6714045405387878
translation,113,155,baselines,another gru,for,explicit inter-speaker modeling,another gru for explicit inter-speaker modeling,0.6388649344444275
translation,113,155,baselines,baselines,has,icon,baselines has icon,0.633277952671051
translation,113,174,baselines,dialoguernn,employs,gated recurrent unit ( gru ) network,dialoguernn employs gated recurrent unit ( gru ) network,0.5882506370544434
translation,113,174,baselines,gated recurrent unit ( gru ) network,to model,individual speaker states,gated recurrent unit ( gru ) network to model individual speaker states,0.6849271059036255
translation,113,174,baselines,baselines,has,dialoguernn,baselines has dialoguernn,0.6556341648101807
translation,113,64,hyperparameters,filters,of size,"3 , 4 and 5","filters of size 3 , 4 and 5",0.736494779586792
translation,113,64,hyperparameters,filters,with,50 feature maps,filters with 50 feature maps,0.6545858979225159
translation,113,64,hyperparameters,"3 , 4 and 5",with,50 feature maps,"3 , 4 and 5 with 50 feature maps",0.6566028594970703
translation,113,64,hyperparameters,hyperparameters,use,filters,hyperparameters use filters,0.6742263436317444
translation,113,65,hyperparameters,max-pooled,with,window size,max-pooled with window size,0.6773772239685059
translation,113,65,hyperparameters,window size,of,2,window size of 2,0.6635134220123291
translation,113,65,hyperparameters,window size,followed by,relu activation,window size followed by relu activation,0.649009108543396
translation,113,65,hyperparameters,2,followed by,relu activation,2 followed by relu activation,0.6649821996688843
translation,113,65,hyperparameters,hyperparameters,has,convoluted features,hyperparameters has convoluted features,0.5268322825431824
translation,113,120,hyperparameters,hyperparameters,optimized using,grid search,hyperparameters optimized using grid search,0.7157480120658875
translation,113,137,hyperparameters,utterance,in,every dialog,utterance in every dialog,0.5734360218048096
translation,113,137,hyperparameters,hyperparameters,has,utterance,hyperparameters has utterance,0.5468944907188416
translation,113,5,model,graph neural network based approach,to,erc,graph neural network based approach to erc,0.6096651554107666
translation,113,5,model,dialogue graph convolutional network ( dialoguegcn ),has,graph neural network based approach,dialogue graph convolutional network ( dialoguegcn ) has graph neural network based approach,0.6063200235366821
translation,113,5,model,model,present,dialogue graph convolutional network ( dialoguegcn ),model present dialogue graph convolutional network ( dialoguegcn ),0.6458701491355896
translation,113,6,model,self and inter-speaker dependency,of,interlocutors,self and inter-speaker dependency of interlocutors,0.5810932517051697
translation,113,6,model,interlocutors,to model,conversational context,interlocutors to model conversational context,0.6842107772827148
translation,113,6,model,conversational context,for,emotion recognition,conversational context for emotion recognition,0.5368176698684692
translation,113,6,model,model,leverage,self and inter-speaker dependency,model leverage self and inter-speaker dependency,0.7443613409996033
translation,113,7,model,dialoguegcn,addresses,context propagation issues,dialoguegcn addresses context propagation issues,0.6477392315864563
translation,113,7,model,context propagation issues,present in,rnn - based methods,context propagation issues present in rnn - based methods,0.6764828562736511
translation,113,7,model,graph network,has,dialoguegcn,graph network has dialoguegcn,0.5942202806472778
translation,113,7,model,model,Through,graph network,model Through graph network,0.6635074615478516
translation,113,62,model,single convolutional layer,followed by,max-pooling,single convolutional layer followed by max-pooling,0.5933405756950378
translation,113,62,model,fully connected layer,to obtain,feature representations,fully connected layer to obtain feature representations,0.5739699602127075
translation,113,62,model,feature representations,for,utterances,feature representations for utterances,0.6732997894287109
translation,113,62,model,model,use,single convolutional layer,model use single convolutional layer,0.5830445289611816
translation,113,80,model,local neighbourhood based convolutional feature transformation process,to create,enriched speaker - level contextually encoded features,local neighbourhood based convolutional feature transformation process to create enriched speaker - level contextually encoded features,0.5951018929481506
translation,113,80,model,model,propose,local neighbourhood based convolutional feature transformation process,model propose local neighbourhood based convolutional feature transformation process,0.6643453240394592
translation,113,143,model,utterance,fed to,network,utterance fed to network,0.7112435698509216
translation,113,143,model,memories,correspond to,previous utterances,memories correspond to previous utterances,0.6435431241989136
translation,113,143,model,continuously updated,in,multi-hop fashion,continuously updated in multi-hop fashion,0.5769676566123962
translation,113,143,model,model,has,utterance,model has utterance,0.6048228144645691
translation,113,167,results,dialoguegcn model,has,outperforms,dialoguegcn model has outperforms,0.6428497433662415
translation,113,167,results,outperforms,has,sota,outperforms has sota,0.622374951839447
translation,113,167,results,outperforms,has,all the baseline models,outperforms has all the baseline models,0.5834654569625854
translation,113,167,results,results,has,dialoguegcn model,results has dialoguegcn model,0.5615323781967163
translation,113,168,results,iemocap and avec,On,iemocap dataset,iemocap and avec On iemocap dataset,0.5490229725837708
translation,113,168,results,dialoguegcn,achieves,new state- of- theart average f1 - score,dialoguegcn achieves new state- of- theart average f1 - score,0.6676085591316223
translation,113,168,results,new state- of- theart average f1 - score,of,64.18 %,new state- of- theart average f1 - score of 64.18 %,0.5191355347633362
translation,113,168,results,accuracy,of,65.25 %,accuracy of 65.25 %,0.5595231652259827
translation,113,168,results,iemocap and avec,has,dialoguegcn,iemocap and avec has dialoguegcn,0.6432216763496399
translation,113,168,results,iemocap dataset,has,dialoguegcn,iemocap dataset has dialoguegcn,0.5893052816390991
translation,113,168,results,results,has,iemocap and avec,results has iemocap and avec,0.5468078255653381
translation,113,169,results,state - of - the - art,on,all the four emotion dimensions,state - of - the - art on all the four emotion dimensions,0.49252864718437195
translation,113,169,results,avec dataset,has,di-aloguegcn,avec dataset has di-aloguegcn,0.6159335374832153
translation,113,169,results,di-aloguegcn,has,outperforms,di-aloguegcn has outperforms,0.6387931704521179
translation,113,169,results,outperforms,has,state - of - the - art,outperforms has state - of - the - art,0.5525293946266174
translation,113,169,results,results,on,avec dataset,results on avec dataset,0.5698418617248535
translation,113,184,results,dialoguegcn model,achieves,new,dialoguegcn model achieves new,0.699729323387146
translation,113,184,results,dialoguegcn model,achieves,state - of- the - art f1 score,dialoguegcn model achieves state - of- the - art f1 score,0.6328145861625671
translation,113,184,results,state - of- the - art f1 score,of,58.10 %,state - of- the - art f1 score of 58.10 %,0.5150600075721741
translation,113,184,results,dialoguernn,by,more than 1 %,dialoguernn by more than 1 %,0.6512100100517273
translation,113,184,results,new,has,state - of- the - art f1 score,new has state - of- the - art f1 score,0.5461563467979431
translation,113,184,results,58.10 %,has,outperforming,58.10 % has outperforming,0.6014549136161804
translation,113,184,results,outperforming,has,dialoguernn,outperforming has dialoguernn,0.5882710218429565
translation,113,184,results,results,has,dialoguegcn model,results has dialoguegcn model,0.5615323781967163
translation,115,129,ablation-analysis,group - wise negative sampling,leads to,performance drop,group - wise negative sampling leads to performance drop,0.6650709509849548
translation,115,129,ablation-analysis,performance drop,with respect to,evaluation metrics,performance drop with respect to evaluation metrics,0.6068108677864075
translation,115,129,ablation-analysis,ablation analysis,ablating,group-wise positive sampling,ablation analysis ablating group-wise positive sampling,0.7374253869056702
translation,115,130,ablation-analysis,groupwise learning strategy,plays,key role,groupwise learning strategy plays key role,0.7003631591796875
translation,115,130,ablation-analysis,key role,achieving,strong performance,key role achieving strong performance,0.6737011671066284
translation,115,130,ablation-analysis,ablation analysis,demonstrates,groupwise learning strategy,ablation analysis demonstrates groupwise learning strategy,0.625635027885437
translation,115,134,ablation-analysis,response-side and context-side samplings,incorporated into,learning framework,response-side and context-side samplings incorporated into learning framework,0.674686074256897
translation,115,134,ablation-analysis,model,achieves,best performance,model achieves best performance,0.6728841066360474
translation,115,134,ablation-analysis,response-side and context-side samplings,has,model,response-side and context-side samplings has model,0.5596603751182556
translation,115,134,ablation-analysis,learning framework,has,model,learning framework has model,0.5756579041481018
translation,115,134,ablation-analysis,ablation analysis,when,response-side and context-side samplings,ablation analysis when response-side and context-side samplings,0.6383318305015564
translation,115,134,ablation-analysis,ablation analysis,both,response-side and context-side samplings,ablation analysis both response-side and context-side samplings,0.697929859161377
translation,115,138,ablation-analysis,training,without considering,matching degrees,training without considering matching degrees,0.7559322118759155
translation,115,138,ablation-analysis,matching degrees,leads to,consistent performance drop,matching degrees leads to consistent performance drop,0.676025927066803
translation,115,138,ablation-analysis,matching degrees,has,of samples,matching degrees has of samples,0.5738292932510376
translation,115,86,baselines,state - of - the - art models,including,seq2seq,state - of - the - art models including seq2seq,0.6440407633781433
translation,115,86,baselines,state - of - the - art models,including,hred,state - of - the - art models including hred,0.6932891607284546
translation,115,86,baselines,encoder-decoder architecture,relying solely on,attention mechanisms,encoder-decoder architecture relying solely on attention mechanisms,0.6777940392494202
translation,115,86,baselines,hierarchical recurrent attention network,for,multi-turn response generation,hierarchical recurrent attention network for multi-turn response generation,0.5486516952514648
translation,115,86,baselines,seq2seq,has,lstm - based sequence - to-sequence model,seq2seq has lstm - based sequence - to-sequence model,0.5511689186096191
translation,115,86,baselines,hred,has,hierarchical recurrent neural dialogue generation model,hred has hierarchical recurrent neural dialogue generation model,0.560671865940094
translation,115,86,baselines,transformer,has,encoder-decoder architecture,transformer has encoder-decoder architecture,0.5801298022270203
translation,115,86,baselines,hran,has,hierarchical recurrent attention network,hran has hierarchical recurrent attention network,0.5312743782997131
translation,115,89,baselines,adversarial training approach,for,response generation,adversarial training approach for response generation,0.590063214302063
translation,115,89,baselines,training objective,maximums,mutual information,training objective maximums mutual information,0.7623274326324463
translation,115,89,baselines,mutual information,between,dialogue context and its response,mutual information between dialogue context and its response,0.6338427662849426
translation,115,89,baselines,reinforcement learning framework,for,neural response generation,reinforcement learning framework for neural response generation,0.5487829446792603
translation,115,89,baselines,neural response generation,with,heuristic reward functions,neural response generation with heuristic reward functions,0.5715612173080444
translation,115,89,baselines,heuristic reward functions,to boost,response qualities,heuristic reward functions to boost response qualities,0.6688381433486938
translation,115,89,baselines,adversarial,has,adversarial training approach,adversarial has adversarial training approach,0.5908516049385071
translation,115,89,baselines,mmi,has,training objective,mmi has training objective,0.5304670333862305
translation,115,89,baselines,deeprl,has,reinforcement learning framework,deeprl has reinforcement learning framework,0.5169097185134888
translation,115,89,baselines,cvae,has,conditional variational auto-encoder learning framework,cvae has conditional variational auto-encoder learning framework,0.4726901054382324
translation,115,91,baselines,several standard metrics,),entropy - based metrics,several standard metrics ) entropy - based metrics,0.5130908489227295
translation,115,91,baselines,distinct metrics,has,"dist - { 1,2,3 } )","distinct metrics has dist - { 1,2,3 } )",0.6200177073478699
translation,115,93,experimental-setup,our model,in,"parlai ( miller et al. , 2017 )","our model in parlai ( miller et al. , 2017 )",0.5222427845001221
translation,115,93,experimental-setup,experimental setup,implement,our model,experimental setup implement our model,0.6453344821929932
translation,115,94,experimental-setup,pretrained word embeddings,produced by,fasttext,pretrained word embeddings produced by fasttext,0.57096928358078
translation,115,94,experimental-setup,dimensionality,of,word vectors,dimensionality of word vectors,0.5848363637924194
translation,115,94,experimental-setup,word vectors,is,300,word vectors is 300,0.6009408831596375
translation,115,95,experimental-setup,2 - layer lstm - based encoder and decoder,with,hidden size 256,2 - layer lstm - based encoder and decoder with hidden size 256,0.6121038794517517
translation,115,95,experimental-setup,2 - layer lstm - based encoder and decoder,used in,seq2seq,2 - layer lstm - based encoder and decoder used in seq2seq,0.6355984807014465
translation,115,95,experimental-setup,hidden size 256,used in,seq2seq,hidden size 256 used in seq2seq,0.6729196310043335
translation,115,96,experimental-setup,decoder,for,hred and hran,decoder for hred and hran,0.6485434174537659
translation,115,96,experimental-setup,decoder,both,hred and hran,decoder both hred and hran,0.684093177318573
translation,115,96,experimental-setup,experimental setup,use,base transformer configuration,experimental setup use base transformer configuration,0.5680399537086487
translation,115,97,experimental-setup,gru hidden size,set to,256,gru hidden size set to 256,0.7347975373268127
translation,115,97,experimental-setup,experimental setup,has,gru hidden size,experimental setup has gru hidden size,0.5445862412452698
translation,115,98,experimental-setup,result checkpoint,adopted as,reference model,result checkpoint adopted as reference model,0.6507955193519592
translation,115,98,experimental-setup,reference model,used in,our framework,reference model used in our framework,0.6353881359100342
translation,115,99,experimental-setup,"bm25 ( robertson and zaragoza , 2009 )",to construct,index,"bm25 ( robertson and zaragoza , 2009 ) to construct index",0.7318435311317444
translation,115,99,experimental-setup,index,used during,contrastive dual sampling procedure,index used during contrastive dual sampling procedure,0.7214665412902832
translation,115,99,experimental-setup,experimental setup,employ,"bm25 ( robertson and zaragoza , 2009 )","experimental setup employ bm25 ( robertson and zaragoza , 2009 )",0.5489037036895752
translation,115,100,experimental-setup,group size k,set to,3,group size k set to 3,0.7684106826782227
translation,115,100,experimental-setup,experimental setup,has,group size k,experimental setup has group size k,0.5472162365913391
translation,115,102,experimental-setup,models,by,"adam ( kingma and ba , 2015 )","models by adam ( kingma and ba , 2015 )",0.5878769159317017
translation,115,102,experimental-setup,"adam ( kingma and ba , 2015 )",with,initial learning rate,"adam ( kingma and ba , 2015 ) with initial learning rate",0.5912186503410339
translation,115,102,experimental-setup,"adam ( kingma and ba , 2015 )",with,batch size,"adam ( kingma and ba , 2015 ) with batch size",0.6160398721694946
translation,115,102,experimental-setup,initial learning rate,of,0.001,initial learning rate of 0.001,0.5703312754631042
translation,115,102,experimental-setup,batch size,of,128,batch size of 128,0.6836684346199036
translation,115,102,experimental-setup,experimental setup,optimize,models,experimental setup optimize models,0.652416467666626
translation,115,103,experimental-setup,systems,trained until,validation loss,systems trained until validation loss,0.7862229943275452
translation,115,103,experimental-setup,validation loss,fails to,decrease,validation loss fails to decrease,0.7182157635688782
translation,115,103,experimental-setup,decrease,for,5 checkpoints,decrease for 5 checkpoints,0.6494840383529663
translation,115,103,experimental-setup,experimental setup,has,systems,experimental setup has systems,0.525396466255188
translation,115,7,model,contrastive learning,into,dialogue generation,contrastive learning into dialogue generation,0.5316648483276367
translation,115,7,model,model,introduce,contrastive learning,model introduce contrastive learning,0.655332088470459
translation,115,10,model,multimapping relations,augment,contrastive dialogue learning,multimapping relations augment contrastive dialogue learning,0.6111304759979248
translation,115,10,model,contrastive dialogue learning,with,group -wise dual sampling,contrastive dialogue learning with group -wise dual sampling,0.5998789072036743
translation,115,10,model,model,To manage,multimapping relations,model To manage multimapping relations,0.7173195481300354
translation,115,136,model,sampled context-response pairs,with,varied matching degrees,sampled context-response pairs with varied matching degrees,0.608484148979187
translation,115,136,model,sampled context-response pairs,utilize,matching score,sampled context-response pairs utilize matching score,0.5917179584503174
translation,115,136,model,matching score,attached with,each sample,matching score attached with each sample,0.7238801121711731
translation,115,136,model,matching score,to adapt,instance effect,matching score to adapt instance effect,0.7048658132553101
translation,115,136,model,instance effect,on,model training,instance effect on model training,0.5546049475669861
translation,115,136,model,model,To discriminatively exploit,sampled context-response pairs,model To discriminatively exploit sampled context-response pairs,0.802238941192627
translation,115,109,results,dialogue models,using,proposed learning framework,dialogue models using proposed learning framework,0.6714949607849121
translation,115,109,results,dialogue models,witness,solid performance boosts,dialogue models witness solid performance boosts,0.8021692633628845
translation,115,109,results,solid performance boosts,on,three conversation datasets,solid performance boosts on three conversation datasets,0.49910783767700195
translation,115,109,results,solid performance boosts,compared to,vanilla training,solid performance boosts compared to vanilla training,0.6533259749412537
translation,115,109,results,results,training,dialogue models,results training dialogue models,0.6609664559364319
translation,115,114,results,outperforms,regarding,majority of evaluation metrics,outperforms regarding majority of evaluation metrics,0.6194459199905396
translation,115,114,results,previous approaches,regarding,majority of evaluation metrics,previous approaches regarding majority of evaluation metrics,0.5258504748344421
translation,115,114,results,our learning framework,has,outperforms,our learning framework has outperforms,0.6262679696083069
translation,115,114,results,outperforms,has,previous approaches,outperforms has previous approaches,0.6066344380378723
translation,115,114,results,results,observe,our learning framework,results observe our learning framework,0.613006591796875
translation,115,115,results,proposed framework,brings,relatively large improvement,proposed framework brings relatively large improvement,0.5750631093978882
translation,115,115,results,relatively large improvement,regarding,response diversity,relatively large improvement regarding response diversity,0.6334739923477173
translation,115,115,results,relatively large improvement,regarding,conversation coherence,relatively large improvement regarding conversation coherence,0.6285353302955627
translation,115,122,results,our learning framework,brings,more preferable replies,our learning framework brings more preferable replies,0.6356322169303894
translation,115,122,results,more preferable replies,compared with,competitors,more preferable replies compared with competitors,0.7435246109962463
translation,115,122,results,results,observe,our learning framework,results observe our learning framework,0.613006591796875
translation,115,128,results,performance significantly,on,all evaluation metrics,performance significantly on all evaluation metrics,0.46450114250183105
translation,115,128,results,disabling,has,groupwise learning,disabling has groupwise learning,0.6061413884162903
translation,115,128,results,groupwise learning,has,hurts,groupwise learning has hurts,0.5898486971855164
translation,115,128,results,hurts,has,performance significantly,hurts has performance significantly,0.5973554253578186
translation,115,141,results,group size k,leads to,continuous improvement,group size k leads to continuous improvement,0.6483888030052185
translation,115,141,results,continuous improvement,on,distinct metric,continuous improvement on distinct metric,0.5568066239356995
translation,115,141,results,other reference - based metrics,achieve,best results,other reference - based metrics achieve best results,0.5714216828346252
translation,115,141,results,best results,at,moderate group size,best results at moderate group size,0.5093167424201965
translation,115,141,results,results,increasing,group size k,results increasing group size k,0.6406523585319519
translation,116,182,ablation-analysis,segmenting dialogues,based on,distance,segmenting dialogues based on distance,0.6217300891876221
translation,116,182,ablation-analysis,distance,may,hurt,distance may hurt,0.688335657119751
translation,116,182,ablation-analysis,storyline,for,each sub dialogue,storyline for each sub dialogue,0.6791967153549194
translation,116,182,ablation-analysis,each sub dialogue,as,id - 7,each sub dialogue as id - 7,0.5995007753372192
translation,116,182,ablation-analysis,id - 7,worse than,"id - { 1 , 5 }","id - 7 worse than id - { 1 , 5 }",0.7236142158508301
translation,116,182,ablation-analysis,hurt,has,storyline,hurt has storyline,0.59796541929245
translation,116,182,ablation-analysis,ablation analysis,has,segmenting dialogues,ablation analysis has segmenting dialogues,0.586540162563324
translation,116,116,baselines,dam,is,hierarchical model,dam is hierarchical model,0.6352992057800293
translation,116,116,baselines,"et al. , 2018 )",is,hierarchical model,"et al. , 2018 ) is hierarchical model",0.5369224548339844
translation,116,116,baselines,hierarchical model,based entirely on,self and cross attention mechanisms,hierarchical model based entirely on self and cross attention mechanisms,0.6613094210624695
translation,116,116,baselines,dam,has,"et al. , 2018 )","dam has et al. , 2018 )",0.5686683058738708
translation,116,116,baselines,baselines,has,dam,baselines has dam,0.6155841946601868
translation,116,117,baselines,esim -18,are,two sequential models,esim -18 are two sequential models,0.5956100821495056
translation,116,117,baselines,esim - 19,are,two sequential models,esim - 19 are two sequential models,0.5952258706092834
translation,116,117,baselines,esim -18,has,"and huang , 2018 )","esim -18 has and huang , 2018 )",0.5749500393867493
translation,116,117,baselines,baselines,has,esim -18,baselines has esim -18,0.6136860847473145
translation,116,119,baselines,"imn ( gu et al. , 2019 )",is,hybrid model,"imn ( gu et al. , 2019 ) is hybrid model",0.5737258791923523
translation,116,119,baselines,hybrid model,with,sequential characteristics,hybrid model with sequential characteristics,0.6728951334953308
translation,116,119,baselines,hybrid model,with,hierarchical characteristics,hybrid model with hierarchical characteristics,0.6741870641708374
translation,116,119,baselines,sequential characteristics,at,matching layer,sequential characteristics at matching layer,0.520079493522644
translation,116,119,baselines,sequential characteristics,at,aggregation layer,sequential characteristics at aggregation layer,0.544369101524353
translation,116,119,baselines,hierarchical characteristics,at,aggregation layer,hierarchical characteristics at aggregation layer,0.5293117165565491
translation,116,119,baselines,baselines,has,"imn ( gu et al. , 2019 )","baselines has imn ( gu et al. , 2019 )",0.5292190909385681
translation,116,120,baselines,bi-encoder ( bi-enc ),are,stateof - the - art models,bi-encoder ( bi-enc ) are stateof - the - art models,0.5700979232788086
translation,116,120,baselines,poly-encoder ( poly - enc ),are,stateof - the - art models,poly-encoder ( poly - enc ) are stateof - the - art models,0.5672416687011719
translation,116,120,baselines,cross-encoder ( cross- enc ),are,stateof - the - art models,cross-encoder ( cross- enc ) are stateof - the - art models,0.5612476468086243
translation,116,120,baselines,stateof - the - art models,based on,pre-trained model,stateof - the - art models based on pre-trained model,0.594254195690155
translation,116,120,baselines,baselines,has,bi-encoder ( bi-enc ),baselines has bi-encoder ( bi-enc ),0.5484274625778198
translation,116,124,experimental-setup,learning rate decay,are,5e?5 and 0.4,learning rate decay are 5e?5 and 0.4,0.5847268104553223
translation,116,124,experimental-setup,experimental setup,has,initial learning rate,experimental setup has initial learning rate,0.49018073081970215
translation,116,124,experimental-setup,experimental setup,has,learning rate decay,experimental setup has learning rate decay,0.48170268535614014
translation,116,127,experimental-setup,experimental setup,has,bpe tokenizer,experimental setup has bpe tokenizer,0.5544474124908447
translation,116,128,experimental-setup,batch size,as,32,batch size as 32,0.5678326487541199
translation,116,128,experimental-setup,experimental setup,set,batch size,experimental setup set batch size,0.6767397522926331
translation,116,122,experiments,response selection task,implemented,our experiments,response selection task implemented our experiments,0.6993367671966553
translation,116,122,experiments,our experiments,based on,parlai,our experiments based on parlai,0.7337349057197571
translation,116,7,model,dialogue extraction algorithm,to transform,dialogue history,dialogue extraction algorithm to transform dialogue history,0.7034578323364258
translation,116,7,model,dialogue history,into,threads,dialogue history into threads,0.6299951076507568
translation,116,7,model,threads,based on,dependency relations,threads based on dependency relations,0.6344277858734131
translation,116,7,model,model,propose,dialogue extraction algorithm,model propose dialogue extraction algorithm,0.6904854774475098
translation,116,8,model,each thread,regarded as,self-contained sub-dialogue,each thread regarded as self-contained sub-dialogue,0.6265786290168762
translation,116,8,model,model,has,each thread,model has each thread,0.5454472303390503
translation,116,9,model,thread - encoder model,to encode,threads and candidates,thread - encoder model to encode threads and candidates,0.7407980561256409
translation,116,9,model,thread - encoder model,get,matching score,thread - encoder model get matching score,0.546402096748352
translation,116,9,model,threads and candidates,into,compact representations,threads and candidates into compact representations,0.6122804284095764
translation,116,9,model,compact representations,by,pre-trained transformers,compact representations by pre-trained transformers,0.5659132599830627
translation,116,9,model,matching score,through,attention layer,matching score through attention layer,0.6567919254302979
translation,116,9,model,model,propose,thread - encoder model,model propose thread - encoder model,0.6807593107223511
translation,116,35,model,model,propose,thread - encoder,model propose thread - encoder,0.702292799949646
translation,116,35,model,model,named,thread - encoder,model named thread - encoder,0.7171787619590759
translation,116,154,results,performance,of,our model,performance of our model,0.5847885608673096
translation,116,154,results,our model,similar to,bi-enc,our model similar to bi-enc,0.691696286201477
translation,116,154,results,our model,similar to,poly - enc,our model similar to poly - enc,0.6662813425064087
translation,116,154,results,poly - enc,on,ubuntuv2,poly - enc on ubuntuv2,0.5239699482917786
translation,116,154,results,results,has,performance,results has performance,0.5972660779953003
translation,116,161,results,our models,achieve,new state - of - the - art results,our models achieve new state - of - the - art results,0.6139376163482666
translation,116,161,results,new state - of - the - art results,on,dstc7 and dstc8 * dataset,new state - of - the - art results on dstc7 and dstc8 * dataset,0.5310719609260559
translation,116,161,results,results,has,our models,results has our models,0.5733726620674133
translation,116,162,results,multiple vectors,works,much better,multiple vectors works much better,0.6215112209320068
translation,116,162,results,much better,than using,only one representation,much better than using only one representation,0.7062233090400696
translation,116,162,results,results,using,multiple vectors,results using multiple vectors,0.6444344520568848
translation,116,177,results,our model,degrades to,bi-enc and poly- enc.,our model degrades to bi-enc and poly- enc.,0.7103357911109924
translation,116,177,results,dist-seg,segments,turns,dist-seg segments turns,0.795454204082489
translation,116,177,results,turns,based on,distance,turns based on distance,0.6797264218330383
translation,116,177,results,distance,to,next response,distance to next response,0.5958423614501953
translation,116,177,results,bi-enc and poly- enc.,has,dist-seg,bi-enc and poly- enc. has dist-seg,0.6122010946273804
translation,116,177,results,results,has,our model,results has our model,0.5871725678443909
translation,116,181,results,dependency relations,capture,salient information,dependency relations capture salient information,0.7237193584442139
translation,116,181,results,dependency relations,yields,better performance,dependency relations yields better performance,0.7192618250846863
translation,116,181,results,salient information,in,dialogue,salient information in dialogue,0.5451146364212036
translation,116,181,results,dialogue,has,more accurately,dialogue has more accurately,0.6092430949211121
translation,116,181,results,results,has,dependency relations,results has dependency relations,0.5231720209121704
translation,117,129,baselines,( l/t ) - ptr-net,has,pure pointer - based model,( l/t ) - ptr-net has pure pointer - based model,0.579163670539856
translation,117,129,baselines,baselines,has,( l/t ) - ptr-net,baselines has ( l/t ) - ptr-net,0.5875976085662842
translation,117,131,baselines,( l/t ),has,- ptr-gen,( l/t ) has - ptr-gen,0.6406497359275818
translation,117,131,baselines,- ptr-gen,has,hybrid pointer + generation model,- ptr-gen has hybrid pointer + generation model,0.5955747961997986
translation,117,131,baselines,baselines,has,( l/t ),baselines has ( l/t ),0.5304283499717712
translation,117,138,experimental-setup,hidden size,as,512,hidden size as 512,0.5782201290130615
translation,117,138,experimental-setup,hidden size,including,end-of-turn delimiter,hidden size including end-of-turn delimiter,0.68375563621521
translation,117,138,experimental-setup,hidden size,including,special unk token,hidden size including special unk token,0.6839374303817749
translation,117,138,experimental-setup,characters and 816 other tokens,including,end-of-turn delimiter,characters and 816 other tokens including end-of-turn delimiter,0.671326756477356
translation,117,138,experimental-setup,characters and 816 other tokens,including,special unk token,characters and 816 other tokens including special unk token,0.6754733920097351
translation,117,138,experimental-setup,special unk token,for,all unknown words,special unk token for all unknown words,0.6386278867721558
translation,117,138,experimental-setup,experimental setup,set,hidden size,experimental setup set hidden size,0.6804612278938293
translation,117,138,experimental-setup,experimental setup,including,special unk token,experimental setup including special unk token,0.6656721830368042
translation,117,199,experiments,social chatbot,contains,two separate engines,social chatbot contains two separate engines,0.576357901096344
translation,117,199,experiments,two separate engines,for,multi-turn and single-turn dialogues,two separate engines for multi-turn and single-turn dialogues,0.6421477794647217
translation,117,7,model,human utterance,as,pre-process,human utterance as pre-process,0.5571261048316956
translation,117,7,model,pre-process,to help,multi-turn dialgoue modelling,pre-process to help multi-turn dialgoue modelling,0.6241704225540161
translation,117,7,model,model,rewriting,human utterance,model rewriting human utterance,0.7558256387710571
translation,117,8,model,utterance,first,rewritten,utterance first rewritten,0.7635672092437744
translation,117,8,model,rewritten,to recover,all coreferred and omitted information,rewritten to recover all coreferred and omitted information,0.6689841151237488
translation,117,8,model,model,has,utterance,model has utterance,0.6048228144645691
translation,117,35,model,transformer architecture,to include,pointer network mechanism,transformer architecture to include pointer network mechanism,0.6121355891227722
translation,117,35,model,model,modify,transformer architecture,model modify transformer architecture,0.7012757658958435
translation,117,130,model,words,copied from,input,words copied from input,0.6353152990341187
translation,117,130,model,model,has,words,model has words,0.5726096034049988
translation,117,132,model,words,copied from,input,words copied from input,0.6353152990341187
translation,117,132,model,words,generated from,fixed vocabulary,words generated from fixed vocabulary,0.6550288200378418
translation,117,132,model,model,has,words,model has words,0.5726096034049988
translation,117,150,results,pointer - based models,has,outperform,pointer - based models has outperform,0.6011249423027039
translation,117,150,results,outperform,has,more complex generationbased and hybrid ones,outperform has more complex generationbased and hybrid ones,0.6049128174781799
translation,117,150,results,results,has,pointer - based models,results has pointer - based models,0.5369846820831299
translation,117,151,results,h and u n,combine,attention,h and u n combine attention,0.6769272685050964
translation,117,151,results,h and u n,performs,better,h and u n performs better,0.7080720663070679
translation,117,151,results,attention,with,learned,attention with learned,0.665502667427063
translation,117,151,results,attention,performs,better,attention performs better,0.6167702078819275
translation,117,151,results,better,than treating,whole dialogue tokens,better than treating whole dialogue tokens,0.6487345695495605
translation,117,151,results,whole dialogue tokens,as,single input,whole dialogue tokens as single input,0.5275400280952454
translation,117,151,results,results,Separately processing,h and u n,results Separately processing h and u n,0.7070104479789734
translation,117,152,results,proposed model,achieves,remarkably good performance,proposed model achieves remarkably good performance,0.6819630265235901
translation,117,152,results,remarkably good performance,with,55.84 %,remarkably good performance with 55.84 %,0.6327787637710571
translation,117,152,results,55.84 %,of,generations,55.84 % of generations,0.6130896210670471
translation,117,152,results,55.84 %,exactly matches,human reference,55.84 % exactly matches human reference,0.7824695706367493
translation,117,152,results,generations,exactly matches,human reference,generations exactly matches human reference,0.7687429189682007
translation,117,152,results,human reference,on,positive samples,human reference on positive samples,0.5725265145301819
translation,117,152,results,results,has,proposed model,results has proposed model,0.5938616394996643
translation,117,153,results,our model,properly copied,original utterances,our model properly copied original utterances,0.6206716895103455
translation,117,153,results,original utterances,in,98.14 %,original utterances in 98.14 %,0.49997851252555847
translation,117,153,results,98.14 %,of,cases,98.14 % of cases,0.5780495405197144
translation,117,153,results,negative samples,has,our model,negative samples has our model,0.5654181838035583
translation,117,153,results,results,For,negative samples,results For negative samples,0.6485066413879395
translation,117,163,results,our final model,reaching,recall score,our final model reaching recall score,0.7283276915550232
translation,117,163,results,outperforms,by,large margin,outperforms by large margin,0.6290680766105652
translation,117,163,results,outperforms,reaching,precision score,outperforms reaching precision score,0.6835284233093262
translation,117,163,results,outperforms,reaching,recall score,outperforms reaching recall score,0.6994724273681641
translation,117,163,results,others,by,large margin,others by large margin,0.5819982886314392
translation,117,163,results,precision score,of,93 %,precision score of 93 %,0.5804499983787537
translation,117,163,results,precision score,of,90 %,precision score of 90 %,0.585229754447937
translation,117,163,results,recall score,of,90 %,recall score of 90 %,0.580451250076294
translation,117,163,results,our final model,has,outperforms,our final model has outperforms,0.6125113368034363
translation,117,163,results,outperforms,has,others,outperforms has others,0.6126620769500732
translation,117,163,results,results,has,our final model,results has our final model,0.5639100670814514
translation,117,177,results,fluency,of,our model 's generations,fluency of our model 's generations,0.6611928343772888
translation,117,177,results,our model 's generations,is,very good,our model 's generations is very good,0.5818154811859131
translation,117,177,results,very good,slightly worse than,human reference,very good slightly worse than human reference,0.8035488128662109
translation,117,177,results,human reference,has,4.90 vs 4.97 ),human reference has 4.90 vs 4.97 ),0.5554143786430359
translation,117,177,results,results,has,fluency,results has fluency,0.45734649896621704
translation,117,191,results,combination,of,our rewriter,combination of our rewriter,0.6203636527061462
translation,117,191,results,intention classier,able to achieve,precision,intention classier able to achieve precision,0.6322325468063354
translation,117,191,results,precision,of,89.91 %,precision of 89.91 %,0.538833498954773
translation,117,191,results,outperforming,by,over 9 %,outperforming by over 9 %,0.6281846761703491
translation,117,191,results,original system,by,over 9 %,original system by over 9 %,0.5912865996360779
translation,117,191,results,combination,has,intention classier,combination has intention classier,0.5850829482078552
translation,117,191,results,our rewriter,has,intention classier,our rewriter has intention classier,0.585462212562561
translation,117,191,results,outperforming,has,original system,outperforming has original system,0.6458573341369629
translation,117,191,results,results,With,combination,results With combination,0.607431948184967
translation,118,13,model,chicken - and - egg problem,with,data-driven nlu approach,chicken - and - egg problem with data-driven nlu approach,0.650559663772583
translation,118,13,model,data-driven nlu approach,segments and identifies,multiple dialogue acts in single utterances,data-driven nlu approach segments and identifies multiple dialogue acts in single utterances,0.7552592158317566
translation,118,82,results,considerably improved performance,for,utterances,considerably improved performance for utterances,0.6870594620704651
translation,118,82,results,considerably improved performance,utterances that contain,multiple dialogue acts,considerably improved performance utterances that contain multiple dialogue acts,0.7350976467132568
translation,118,82,results,considerably improved performance,for,utterances,considerably improved performance for utterances,0.6870594620704651
translation,118,82,results,number of errors,for,utterances,number of errors for utterances,0.6283063292503357
translation,118,82,results,utterances,containing,only a single dialogue act,utterances containing only a single dialogue act,0.6927101612091064
translation,118,82,results,proposed approach,has,considerably improved performance,proposed approach has considerably improved performance,0.6122499108314514
translation,118,83,results,our approach,for segmenting and identifying,multiple dialogue acts,our approach for segmenting and identifying multiple dialogue acts,0.719459593296051
translation,118,83,results,overall fscore,by,about 4 %,overall fscore by about 4 %,0.5751590728759766
translation,118,83,results,overall fscore,by,about 2 %,overall fscore by about 2 %,0.5730304718017578
translation,118,83,results,overall fscore,by,about 2 %,overall fscore by about 2 %,0.5730304718017578
translation,118,83,results,about 4 %,compared to,first baseline,about 4 % compared to first baseline,0.7044692635536194
translation,118,83,results,about 2 %,compared to,second ( strong ) baseline,about 2 % compared to second ( strong ) baseline,0.7116177082061768
translation,118,83,results,increases,has,overall fscore,increases has overall fscore,0.5974513292312622
translation,119,131,ablation-analysis,standard deviation,of,final success rates,standard deviation of final success rates,0.5404222011566162
translation,119,131,ablation-analysis,i2a,has,adc,i2a has adc,0.6010394096374512
translation,119,149,ablation-analysis,adc,maintains,performance,adc maintains performance,0.6206390857696533
translation,119,149,ablation-analysis,performance,with,poor model,performance with poor model,0.6579132080078125
translation,119,149,ablation-analysis,amc 's performance,has,drops a lot,amc 's performance has drops a lot,0.6267737150192261
translation,119,125,baselines,baselines,has,adc,baselines has adc,0.5803765654563904
translation,119,175,baselines,acer,as,actorcritic model- free baseline,acer as actorcritic model- free baseline,0.5299432873725891
translation,119,175,baselines,baselines,used,acer,baselines used acer,0.623883068561554
translation,119,4,experiments,imagination augmented agent ( i2a ),in,spoken dialogue systems ( sds ),imagination augmented agent ( i2a ) in spoken dialogue systems ( sds ),0.5533309578895569
translation,119,107,hyperparameters,15 % semantic error rate ( ser ),included in,user simulator,15 % semantic error rate ( ser ) included in user simulator,0.661657989025116
translation,119,107,hyperparameters,asr error,has,15 % semantic error rate ( ser ),asr error has 15 % semantic error rate ( ser ),0.5805940628051758
translation,119,107,hyperparameters,hyperparameters,accommodate for,asr error,hyperparameters accommodate for asr error,0.6559315919876099
translation,119,111,hyperparameters,full dialogue belief state b,of size,268,full dialogue belief state b of size 268,0.6849156618118286
translation,119,111,hyperparameters,output action space,consists of,16 possible actions,output action space consists of 16 possible actions,0.5894346237182617
translation,119,111,hyperparameters,hyperparameters,input for,output action space,hyperparameters input for output action space,0.7269167900085449
translation,119,112,hyperparameters,size,of,mini-batch,size of mini-batch,0.6188174486160278
translation,119,112,hyperparameters,mini-batch,is,64,mini-batch is 64,0.6185778975486755
translation,119,112,hyperparameters,nn - based algorithms,has,size,nn - based algorithms has size,0.5834296941757202
translation,119,112,hyperparameters,hyperparameters,For,nn - based algorithms,hyperparameters For nn - based algorithms,0.5753473043441772
translation,119,113,hyperparameters,linearly reducing,from,0.3,linearly reducing from 0.3,0.5861856341362
translation,119,113,hyperparameters,0.3,down to,0,0.3 down to 0,0.6388059258460999
translation,119,113,hyperparameters,0,over,training process,0 over training process,0.6803452968597412
translation,119,113,hyperparameters,hyperparameters,has,greedy exploration,hyperparameters has greedy exploration,0.517536997795105
translation,119,114,hyperparameters,two hidden layers,of size,300 and 100,two hidden layers of size 300 and 100,0.6986499428749084
translation,119,114,hyperparameters,300 and 100,for,actor and critic,300 and 100 for actor and critic,0.6772096753120422
translation,119,114,hyperparameters,hyperparameters,has,two hidden layers,hyperparameters has two hidden layers,0.5036237239837646
translation,119,115,hyperparameters,adam optimiser,used with,initial learning rate,adam optimiser used with initial learning rate,0.6355430483818054
translation,119,115,hyperparameters,initial learning rate,of,"0.001 ( kingma and ba , 2014 )","initial learning rate of 0.001 ( kingma and ba , 2014 )",0.5379334688186646
translation,119,115,hyperparameters,hyperparameters,has,adam optimiser,hyperparameters has adam optimiser,0.5281989574432373
translation,119,124,hyperparameters,environment model,pre-trained with,400 dialogues,environment model pre-trained with 400 dialogues,0.7255626320838928
translation,119,124,hyperparameters,400 dialogues,generated by,interactions,400 dialogues generated by interactions,0.7106091976165771
translation,119,124,hyperparameters,interactions,between,simulated user and an agent,interactions between simulated user and an agent,0.6639001369476318
translation,119,124,hyperparameters,hyperparameters,has,environment model,hyperparameters has environment model,0.5111352205276489
translation,119,126,hyperparameters,ensemble weight w,is,0.5,ensemble weight w is 0.5,0.5813525319099426
translation,119,126,hyperparameters,0.5,for,each critic,0.5 for each critic,0.6717377305030823
translation,119,126,hyperparameters,hyperparameters,has,ensemble weight w,hyperparameters has ensemble weight w,0.481993168592453
translation,119,6,model,actor-double-critic ( adc ),to improve,stability and overall performance,actor-double-critic ( adc ) to improve stability and overall performance,0.6933293342590332
translation,119,6,model,stability and overall performance,of,i2a,stability and overall performance of i2a,0.5825309157371521
translation,119,6,model,model,propose,actor-double-critic ( adc ),model propose actor-double-critic ( adc ),0.6690384149551392
translation,119,7,model,adc,simplifies,architecture of i2a,adc simplifies architecture of i2a,0.5580829977989197
translation,119,7,model,architecture of i2a,to reduce,excessive parameters and hyperparameters,architecture of i2a to reduce excessive parameters and hyperparameters,0.6220276355743408
translation,119,7,model,model,has,adc,model has adc,0.6096106171607971
translation,119,27,model,new architecture,to augment,model - based information,new architecture to augment model - based information,0.6997710466384888
translation,119,27,model,model - based information,into,policy network,model - based information into policy network,0.5571737885475159
translation,119,27,model,actor-double -critic ( adc ),has,new architecture,actor-double -critic ( adc ) has new architecture,0.5886942148208618
translation,119,27,model,model,propose,actor-double -critic ( adc ),model propose actor-double -critic ( adc ),0.6690384149551392
translation,119,28,model,two critics,from,model-free and model - based data,two critics from model-free and model - based data,0.5828285813331604
translation,119,28,model,two critics,combining them in,ensemble,two critics combining them in ensemble,0.7363723516464233
translation,119,28,model,back - propagation,has,more efficient,back - propagation has more efficient,0.5748551487922668
translation,119,28,model,model,training,two critics,model training two critics,0.7006719708442688
translation,119,30,results,adc,takes,only half,adc takes only half,0.7338265180587769
translation,119,30,results,only half,of,baseline training data,only half of baseline training data,0.5727637410163879
translation,119,30,results,only half,to achieve,80 % success rate,only half to achieve 80 % success rate,0.7093970775604248
translation,119,30,results,sample efficiency,has,adc,sample efficiency has adc,0.5453783273696899
translation,119,30,results,results,Regarding,sample efficiency,results Regarding sample efficiency,0.5946710109710693
translation,119,31,results,adc,is,most stable approach,adc is most stable approach,0.5761855840682983
translation,119,31,results,most stable approach,among,all considered baselines,most stable approach among all considered baselines,0.5815265774726868
translation,119,31,results,results,has,adc,results has adc,0.502375066280365
translation,119,129,results,outperforms,in terms of,sample - efficiency,outperforms in terms of sample - efficiency,0.6404092907905579
translation,119,129,results,outperforms,in terms of,success rate,outperforms in terms of success rate,0.6945087909698486
translation,119,129,results,other methods,in terms of,sample - efficiency,other methods in terms of sample - efficiency,0.653204083442688
translation,119,129,results,other methods,in terms of,stability,other methods in terms of stability,0.6827301979064941
translation,119,129,results,other methods,in terms of,success rate,other methods in terms of success rate,0.6008331775665283
translation,119,129,results,adc,has,outperforms,adc has outperforms,0.6494978666305542
translation,119,129,results,outperforms,has,other methods,outperforms has other methods,0.5689877271652222
translation,119,129,results,other methods,has,considerably,other methods has considerably,0.5549749135971069
translation,119,151,results,aid,from,modelfree critic,aid from modelfree critic,0.6054026484489441
translation,119,151,results,aid,is,substantial,aid is substantial,0.6919735074043274
translation,119,151,results,modelfree critic,is,substantial,modelfree critic is substantial,0.6062806248664856
translation,119,151,results,results,has,aid,results has aid,0.44425296783447266
translation,119,162,results,environment 6,has,adc,environment 6 has adc,0.605190098285675
translation,119,162,results,adc,has,outperforms,adc has outperforms,0.6494978666305542
translation,119,162,results,outperforms,has,hand -crafted policy ( 89.6 % ),outperforms has hand -crafted policy ( 89.6 % ),0.5742087960243225
translation,119,162,results,results,in,environment 6,results in environment 6,0.5442173480987549
translation,119,163,results,adc,demonstrates,robustness,adc demonstrates robustness,0.6894205808639526
translation,119,163,results,robustness,in,all environments,robustness in all environments,0.4976997673511505
translation,119,163,results,all environments,especially for,environments without action masks,all environments especially for environments without action masks,0.6682711243629456
translation,119,163,results,results,has,adc,results has adc,0.502375066280365
translation,119,166,results,agent of acer and adc,have,57 % and 88 % success rate,agent of acer and adc have 57 % and 88 % success rate,0.5430765151977539
translation,119,166,results,results,has,agent of acer and adc,results has agent of acer and adc,0.5434978604316711
translation,119,172,results,policy optimisation algorithm,provides,model - based augmentation,policy optimisation algorithm provides model - based augmentation,0.6277607679367065
translation,119,172,results,results,has,policy optimisation algorithm,results has policy optimisation algorithm,0.5774100422859192
translation,120,103,baselines,baselines,has,bi-directional lstm,baselines has bi-directional lstm,0.531011700630188
translation,120,105,baselines,sequence to sequence model,with,vanilla rnn encoder-decoder,sequence to sequence model with vanilla rnn encoder-decoder,0.6544902324676514
translation,120,105,baselines,seq2seq,has,sequence to sequence model,seq2seq has sequence to sequence model,0.5910978317260742
translation,120,105,baselines,hred,has,hierarchical recurrent encoderdecoder model,hred has hierarchical recurrent encoderdecoder model,0.5505533814430237
translation,120,106,baselines,neural generative dialogue system,capable of,generating,neural generative dialogue system capable of generating,0.6591908931732178
translation,120,106,baselines,responses,based on,input message and related knowledge base ( kb ),responses based on input message and related knowledge base ( kb ),0.6588230133056641
translation,120,106,baselines,gends,has,neural generative dialogue system,gends has neural generative dialogue system,0.5644143223762512
translation,120,106,baselines,generating,has,responses,generating has responses,0.60882967710495
translation,120,106,baselines,baselines,has,gends,baselines has gends,0.5790000557899475
translation,120,107,baselines,three variants,of,neural diffusion dialogue generation model,three variants of neural diffusion dialogue generation model,0.5383627414703369
translation,120,107,baselines,baselines,has,three variants,baselines has three variants,0.5716367363929749
translation,120,108,baselines,- ori,is,original model,- ori is original model,0.6177992820739746
translation,120,108,baselines,original model,with,vanilla decoder,original model with vanilla decoder,0.6236242651939392
translation,120,108,baselines,original model,with,mask coefficient tracker,original model with mask coefficient tracker,0.6538166403770447
translation,120,108,baselines,baselines,has,- ori,baselines has - ori,0.6274445056915283
translation,120,91,experiments,multi-turn conversation corpus,grounded on,knowledge base,multi-turn conversation corpus grounded on knowledge base,0.6908661127090454
translation,120,5,model,neural knowledge diffusion ( nkd ) model,to introduce,knowledge,neural knowledge diffusion ( nkd ) model to introduce knowledge,0.6745694279670715
translation,120,5,model,knowledge,into,dialogue generation,knowledge into dialogue generation,0.5577341318130493
translation,120,5,model,model,propose,neural knowledge diffusion ( nkd ) model,model propose neural knowledge diffusion ( nkd ) model,0.6418346762657166
translation,120,7,model,facts matching,has,neural dialogue generation,facts matching has neural dialogue generation,0.5464096665382385
translation,120,7,model,model,With the help of,facts matching,model With the help of facts matching,0.6609151363372803
translation,120,27,model,neural knowledge diffusion ( nkd ) dialogue,to benefit,neural dialogue generation,neural knowledge diffusion ( nkd ) dialogue to benefit neural dialogue generation,0.6383771300315857
translation,120,27,model,model,propose,neural knowledge diffusion ( nkd ) dialogue,model propose neural knowledge diffusion ( nkd ) dialogue,0.6892872452735901
translation,120,28,model,nkd,learns to,match,nkd learns to match,0.630501389503479
translation,120,28,model,nkd,learns to,matched facts,nkd learns to matched facts,0.6829108595848083
translation,120,28,model,utterances,to,relevant facts,utterances to relevant facts,0.5617656111717224
translation,120,28,model,diffused,to,similar entities,diffused to similar entities,0.5744954943656921
translation,120,28,model,responses,with respect to,all the retrieved knowledge items,responses with respect to all the retrieved knowledge items,0.6758673191070557
translation,120,28,model,match,has,utterances,match has utterances,0.6474454998970032
translation,120,28,model,model,generates,responses,model generates responses,0.624602198600769
translation,120,28,model,model,has,nkd,model has nkd,0.6596657037734985
translation,120,116,results,performance,of,nkd,performance of nkd,0.6389749646186829
translation,120,116,results,nkd,is,slightly better,nkd is slightly better,0.6033867001533508
translation,120,116,results,slightly better,than,specific qa solution gends,slightly better than specific qa solution gends,0.6029554605484009
translation,120,116,results,lstm and hred,designed for,chi-chat,lstm and hred designed for chi-chat,0.6835721731185913
translation,120,116,results,chi-chat,has,almost fail,chi-chat has almost fail,0.6280323266983032
translation,120,116,results,results,has,performance,results has performance,0.5972660779953003
translation,120,117,results,all the variants of nkd models,generating,entities,all the variants of nkd models generating entities,0.7138237357139587
translation,120,117,results,entities,with,accuracy,entities with accuracy,0.61475670337677
translation,120,117,results,entities,with,accuracy,entities with accuracy,0.61475670337677
translation,120,117,results,accuracy,of,60 % to 70 %,accuracy of 60 % to 70 %,0.5942434668540955
translation,120,117,results,accuracy,of,77.6 %,accuracy of 77.6 %,0.5536056160926819
translation,120,117,results,nkd - gated,achieves,best performance,nkd - gated achieves best performance,0.7135313153266907
translation,120,117,results,best performance,with,accuracy,best performance with accuracy,0.6443129181861877
translation,120,117,results,best performance,with,recall,best performance with recall,0.6618418097496033
translation,120,117,results,accuracy,of,77.6 %,accuracy of 77.6 %,0.5536056160926819
translation,120,117,results,recall,of,77.3 %,recall of 77.3 %,0.5627081394195557
translation,120,117,results,results,has,all the variants of nkd models,results has all the variants of nkd models,0.4948934018611908
translation,120,119,results,gends,on,entire dataset,gends on entire dataset,0.5600830316543579
translation,120,119,results,relative improvement,over,gends,relative improvement over gends,0.704016923904419
translation,120,119,results,gends,is,even higher,gends is even higher,0.6378792524337769
translation,120,119,results,both nkd - ori and nkd - gated,has,outperform,both nkd - ori and nkd - gated has outperform,0.596490740776062
translation,120,119,results,outperform,has,gends,outperform has gends,0.6478527784347534
translation,120,119,results,results,has,both nkd - ori and nkd - gated,results has both nkd - ori and nkd - gated,0.4768659472465515
translation,120,122,results,nkdgated,achieves,highest accuracy and recall,nkdgated achieves highest accuracy and recall,0.6840550303459167
translation,120,122,results,nkdgated,generates,fewer entities,nkdgated generates fewer entities,0.716354250907898
translation,120,122,results,fewer entities,compared with,nkdori and nkd - gated,fewer entities compared with nkdori and nkd - gated,0.705394983291626
translation,120,122,results,nkd - atte,generates,more entities,nkd - atte generates more entities,0.7328541278839111
translation,120,122,results,nkd - atte,with,relatively low accuracies and recalls,nkd - atte with relatively low accuracies and recalls,0.6797478795051575
translation,120,122,results,results,noticed,nkdgated,results noticed nkdgated,0.6701069474220276
translation,120,122,results,results,noticed,nkd - atte,results noticed nkd - atte,0.633567750453949
translation,122,7,model,interaction,as,partially observable markov decision process ( pomdp ),interaction as partially observable markov decision process ( pomdp ),0.5464569926261902
translation,122,7,model,partially observable markov decision process ( pomdp ),over,rich state space,partially observable markov decision process ( pomdp ) over rich state space,0.6266642212867737
translation,122,7,model,rich state space,incorporating,"dialogue , user , and environment models","rich state space incorporating dialogue , user , and environment models",0.7027273774147034
translation,122,7,model,model,represent,interaction,model represent interaction,0.655130922794342
translation,122,31,model,planning problem,has,globally more tractable,planning problem has globally more tractable,0.5716654062271118
translation,122,8,results,preserved,using,mechanism,preserved using mechanism,0.7680444717407227
translation,122,8,results,mechanism,for dynamically constraining,action space,mechanism for dynamically constraining action space,0.728187620639801
translation,122,8,results,action space,based on,prior knowledge,action space based on prior knowledge,0.6292338371276855
translation,122,8,results,prior knowledge,over,locally relevant dialogue structures,prior knowledge over locally relevant dialogue structures,0.6326137781143188
translation,122,8,results,results,has,tractability,results has tractability,0.555144190788269
translation,123,150,ablation-analysis,intriguing observation,when incorporating,goal embedding module,intriguing observation when incorporating goal embedding module,0.6590972542762756
translation,123,150,ablation-analysis,intriguing observation,has,response diversity and goal focus,intriguing observation has response diversity and goal focus,0.49726206064224243
translation,123,150,ablation-analysis,goal embedding module,has,response diversity and goal focus,goal embedding module has response diversity and goal focus,0.5433218479156494
translation,123,150,ablation-analysis,ablation analysis,when incorporating,goal embedding module,ablation analysis when incorporating goal embedding module,0.6707825064659119
translation,123,150,ablation-analysis,ablation analysis,has,intriguing observation,ablation analysis has intriguing observation,0.5461517572402954
translation,123,161,ablation-analysis,g- duha,shows,substantial advantages,g- duha shows substantial advantages,0.6314816474914551
translation,123,161,ablation-analysis,substantial advantages,on,goal focus,substantial advantages on goal focus,0.4850636422634125
translation,123,161,ablation-analysis,substantial advantages,with,82.33 % wins,substantial advantages with 82.33 % wins,0.6182246208190918
translation,123,161,ablation-analysis,82.33 % wins,over,hred,82.33 % wins over hred,0.659832775592804
translation,123,161,ablation-analysis,ablation analysis,has,g- duha,ablation analysis has g- duha,0.556256890296936
translation,123,173,ablation-analysis,unplugging,observe,significant and consistent drops,unplugging observe significant and consistent drops,0.6510925889015198
translation,123,173,ablation-analysis,goal embedding module,observe,significant and consistent drops,goal embedding module observe significant and consistent drops,0.6249322295188904
translation,123,173,ablation-analysis,significant and consistent drops,on,"quality , diversity , and goal focus measures","significant and consistent drops on quality , diversity , and goal focus measures",0.42069897055625916
translation,123,173,ablation-analysis,"quality , diversity , and goal focus measures",for,dialogue and response generation tasks,"quality , diversity , and goal focus measures for dialogue and response generation tasks",0.5552959442138672
translation,123,173,ablation-analysis,unplugging,has,goal embedding module,unplugging has goal embedding module,0.590694010257721
translation,123,173,ablation-analysis,ablation analysis,When,unplugging,ablation analysis When unplugging,0.6900256276130676
translation,123,194,ablation-analysis,goal-embedded lm,has,augmentation,goal-embedded lm has augmentation,0.5500056743621826
translation,123,194,ablation-analysis,hurts,has,overall performance,hurts has overall performance,0.6045477390289307
translation,123,194,ablation-analysis,ablation analysis,For,goal-embedded lm,ablation analysis For goal-embedded lm,0.6113666892051697
translation,123,88,baselines,lm +g,adopt,rnn language model ( lm ),lm +g adopt rnn language model ( lm ),0.6485604047775269
translation,123,88,baselines,rnn language model ( lm ),with,3 - layer 200 - hidden-unit gru,rnn language model ( lm ) with 3 - layer 200 - hidden-unit gru,0.6331455707550049
translation,123,88,baselines,3 - layer 200 - hidden-unit gru,incorporating,goal embedding module,3 - layer 200 - hidden-unit gru incorporating goal embedding module,0.6783804893493652
translation,123,88,baselines,baselines,adopt,rnn language model ( lm ),baselines adopt rnn language model ( lm ),0.595187783241272
translation,123,89,baselines,larger lm,has,3 - layer 450hidden - unit gru,larger lm has 3 - layer 450hidden - unit gru,0.6072458028793335
translation,123,89,baselines,lm +g-xl,has,larger lm,lm +g-xl has larger lm,0.6607699990272522
translation,123,89,baselines,baselines,has,lm +g-xl,baselines has lm +g-xl,0.5616421103477478
translation,123,90,baselines,baselines,has,hierarchical recurrent encoder-decoder ( hred ),baselines has hierarchical recurrent encoder-decoder ( hred ),0.5557132959365845
translation,123,91,baselines,dialogues,use,hred,dialogues use hred,0.6553786993026733
translation,123,91,baselines,hred,as,baseline,hred as baseline,0.5630249381065369
translation,123,91,baselines,hred,that has,goal information,hred that has goal information,0.622955322265625
translation,123,91,baselines,baseline,that has,dialogue -specific architecture,baseline that has dialogue -specific architecture,0.5641890168190002
translation,123,93,baselines,hred - xl,use,larger hred,hred - xl use larger hred,0.6484010815620422
translation,123,93,baselines,larger hred,with,350 hidden units,larger hred with 350 hidden units,0.6476930379867554
translation,123,93,baselines,350 hidden units,for,all grus,350 hidden units for all grus,0.6143101453781128
translation,123,93,baselines,baselines,has,hred - xl,baselines has hred - xl,0.5898607969284058
translation,123,97,baselines,g- duha,uses,"2 - layer , 200- hidden-unit grus","g- duha uses 2 - layer , 200- hidden-unit grus",0.6722354888916016
translation,123,97,baselines,"2 - layer , 200- hidden-unit grus",as,"all encoders , decoders , and context rnns","2 - layer , 200- hidden-unit grus as all encoders , decoders , and context rnns",0.5211054086685181
translation,123,97,baselines,baselines,has,g- duha,baselines has g- duha,0.5642833709716797
translation,123,99,experiments,ffns,encoder attention and end of,dialogue prediction,ffns encoder attention and end of dialogue prediction,0.7506413459777832
translation,123,99,experiments,dialogue prediction,have,50 hidden units,dialogue prediction have 50 hidden units,0.5572595000267029
translation,123,145,experiments,lmgoal,performs,good,lmgoal performs good,0.6491217613220215
translation,123,145,experiments,lmgoal,performs,short,lmgoal performs short,0.6986283659934998
translation,123,145,experiments,good,on,precision,good on precision,0.5016406774520874
translation,123,145,experiments,short,on,recall,short on recall,0.573321521282196
translation,123,145,experiments,goal focus,has,lmgoal,goal focus has lmgoal,0.578834593296051
translation,123,189,experiments,state- of- the- art global -locally self-attentive dialogue state tracker ( glad ),as,benchmark dataset,state- of- the- art global -locally self-attentive dialogue state tracker ( glad ) as benchmark dataset,0.4801747500896454
translation,123,189,experiments,state- of- the- art global -locally self-attentive dialogue state tracker ( glad ),as,benchmark dataset,state- of- the- art global -locally self-attentive dialogue state tracker ( glad ) as benchmark dataset,0.4801747500896454
translation,123,189,experiments,woz restaurant reservation dataset,as,benchmark dataset,woz restaurant reservation dataset as benchmark dataset,0.4442232549190521
translation,123,92,hyperparameters,"encoder , decoder , and context rnn",are,2 - layer 200 - hidden-unit grus,"encoder , decoder , and context rnn are 2 - layer 200 - hidden-unit grus",0.5522462725639343
translation,123,92,hyperparameters,hyperparameters,has,"encoder , decoder , and context rnn","hyperparameters has encoder , decoder , and context rnn",0.5224677324295044
translation,123,96,hyperparameters,sequence length,capped to,22 and 36,sequence length capped to 22 and 36,0.6536135673522949
translation,123,96,hyperparameters,hyperparameters,has,max number of turns,hyperparameters has max number of turns,0.5184248685836792
translation,123,96,hyperparameters,hyperparameters,has,sequence length,hyperparameters has sequence length,0.521059513092041
translation,123,100,hyperparameters,ffns,of,context attention and goal embedding,ffns of context attention and goal embedding,0.5237575769424438
translation,123,100,hyperparameters,ffns,gets,100 and 200 hidden units,ffns gets 100 and 200 hidden units,0.5744127035140991
translation,123,100,hyperparameters,context attention and goal embedding,gets,100 and 200 hidden units,context attention and goal embedding gets 100 and 200 hidden units,0.5299939513206482
translation,123,100,hyperparameters,hyperparameters,has,ffns,hyperparameters has ffns,0.555223286151886
translation,123,101,hyperparameters,greedy decoding,for,utterance generation,greedy decoding for utterance generation,0.6088297367095947
translation,123,101,hyperparameters,hyperparameters,use,greedy decoding,hyperparameters use greedy decoding,0.6568648219108582
translation,123,102,hyperparameters,embeddings,with,pretrained fast - text vectors,embeddings with pretrained fast - text vectors,0.583041787147522
translation,123,102,hyperparameters,pretrained fast - text vectors,on,wiki-news ( 2018 ),pretrained fast - text vectors on wiki-news ( 2018 ),0.538336992263794
translation,123,102,hyperparameters,adam optimizer ( 2015 ),with,early - stopping,adam optimizer ( 2015 ) with early - stopping,0.6224209070205688
translation,123,102,hyperparameters,early - stopping,to prevent,overfitting,early - stopping to prevent overfitting,0.6473425626754761
translation,123,103,hyperparameters,predicted or ground - truth utterance,as,current input,predicted or ground - truth utterance as current input,0.500552773475647
translation,123,103,hyperparameters,uniformly at random,when,training,uniformly at random when training,0.7023504376411438
translation,123,103,hyperparameters,current input,has,uniformly at random,current input has uniformly at random,0.5212280750274658
translation,123,103,hyperparameters,hyperparameters,pick,predicted or ground - truth utterance,hyperparameters pick predicted or ground - truth utterance,0.680368959903717
translation,123,6,model,goal-embedded dual hierarchical attentional encoder-decoder ( g- duha ),able to center around,goals,goal-embedded dual hierarchical attentional encoder-decoder ( g- duha ) able to center around goals,0.727172315120697
translation,123,6,model,goal-embedded dual hierarchical attentional encoder-decoder ( g- duha ),capture,interlocutorlevel disparity,goal-embedded dual hierarchical attentional encoder-decoder ( g- duha ) capture interlocutorlevel disparity,0.7519093751907349
translation,123,6,model,interlocutorlevel disparity,while modeling,goal-oriented dialogues,interlocutorlevel disparity while modeling goal-oriented dialogues,0.759460985660553
translation,123,6,model,model,propose,goal-embedded dual hierarchical attentional encoder-decoder ( g- duha ),model propose goal-embedded dual hierarchical attentional encoder-decoder ( g- duha ),0.6612401604652405
translation,123,20,model,problems,via,three key features,problems via three key features,0.6490119695663452
translation,123,20,model,model,propose,goal-embedded dual hierarchical attentional encoder -decoder ( g- duha ),model propose goal-embedded dual hierarchical attentional encoder -decoder ( g- duha ),0.6612401604652405
translation,123,21,model,goal embedding module,summarizes,one or more goals,goal embedding module summarizes one or more goals,0.6736719012260437
translation,123,21,model,one or more goals,of,current dialogue,one or more goals of current dialogue,0.4980379343032837
translation,123,21,model,goal contexts,for,model,goal contexts for model,0.6151816248893738
translation,123,21,model,model,has,goal embedding module,model has goal embedding module,0.5667325854301453
translation,123,23,model,attentions,introduced on,word and dialogue levels,attentions introduced on word and dialogue levels,0.7304337620735168
translation,123,23,model,word and dialogue levels,to learn,temporal dependencies,word and dialogue levels to learn temporal dependencies,0.5819644331932068
translation,123,23,model,temporal dependencies,has,more easily,temporal dependencies has more easily,0.5590674877166748
translation,123,23,model,model,has,attentions,model has attentions,0.6162422299385071
translation,123,24,model,interlocutor -level disparity,while modeling,goal-oriented dialogues,interlocutor -level disparity while modeling goal-oriented dialogues,0.7412691116333008
translation,123,24,model,model,called,goal-embedded dual hierarchical attentional encoder-decoder ( g - duha ),model called goal-embedded dual hierarchical attentional encoder-decoder ( g - duha ),0.6489720940589905
translation,123,68,model,first hidden layer 's states,of,context rnn,first hidden layer 's states of context rnn,0.552786648273468
translation,123,68,model,context rnn,as,query,context rnn as query,0.5583112835884094
translation,123,68,model,query,for,attention mechanisms,query for attention mechanisms,0.6385688781738281
translation,123,68,model,model,At,utterance level,model At utterance level,0.5517463684082031
translation,123,68,model,model,uses,first hidden layer 's states,model uses first hidden layer 's states,0.615280270576477
translation,123,98,model,feed -forward networks,have,2 layers,feed -forward networks have 2 layers,0.5720682144165039
translation,123,98,model,2 layers,with,nonlinearity,2 layers with nonlinearity,0.6733143329620361
translation,123,98,model,model,has,feed -forward networks,model has feed -forward networks,0.5777013301849365
translation,123,119,results,quality measures,has,g- duha,quality measures has g- duha,0.5542890429496765
translation,123,119,results,g- duha,has,significantly outperforms,g- duha has significantly outperforms,0.6160984635353088
translation,123,119,results,significantly outperforms,has,other baselines,significantly outperforms has other baselines,0.5847466588020325
translation,123,119,results,results,For,quality measures,results For quality measures,0.37417978048324585
translation,123,120,results,goal-embedded lms,perform better than,hreds,goal-embedded lms perform better than hreds,0.6972482204437256
translation,123,120,results,benefits,of,our goal embedding module,benefits of our goal embedding module,0.5541760921478271
translation,123,120,results,results,has,goal-embedded lms,results has goal-embedded lms,0.5588977932929993
translation,123,121,results,no significant performance difference,with respect to,model size variants,no significant performance difference with respect to model size variants,0.696626603603363
translation,123,121,results,results,has,no significant performance difference,results has no significant performance difference,0.5077201128005981
translation,123,122,results,g- duha,on par with,goal-embedded lms,g- duha on par with goal-embedded lms,0.7136794328689575
translation,123,122,results,diversity evaluations,has,g- duha,diversity evaluations has g- duha,0.5858632922172546
translation,123,122,results,outperform,has,hred,outperform has hred,0.7460828423500061
translation,123,122,results,hred,has,significantly,hred has significantly,0.6661698818206787
translation,123,122,results,results,For,diversity evaluations,results For diversity evaluations,0.5989061594009399
translation,123,123,results,hred,delivers,highly repetitive outputs,hred delivers highly repetitive outputs,0.7099139094352722
translation,123,123,results,highly repetitive outputs,with,4 to 6 % distinct utterances,highly repetitive outputs with 4 to 6 % distinct utterances,0.6403730511665344
translation,123,123,results,highly repetitive outputs,only,4 to 6 % distinct utterances,highly repetitive outputs only 4 to 6 % distinct utterances,0.7029000520706177
translation,123,123,results,1000,has,generated dialogues,1000 has generated dialogues,0.5885210633277893
translation,123,123,results,1000,has,hred,1000 has hred,0.6506039500236511
translation,123,123,results,generated dialogues,has,hred,generated dialogues has hred,0.7013564109802246
translation,123,123,results,results,Of,1000,results Of 1000,0.6149458289146423
translation,123,144,results,outperforms,on,quality and goal focus measures,outperforms on quality and goal focus measures,0.5576882362365723
translation,123,144,results,others,on,quality and goal focus measures,others on quality and goal focus measures,0.4723193943500519
translation,123,144,results,rivals,on,diversity,rivals on diversity,0.5563416481018066
translation,123,144,results,diversity,on,agent and user responses,diversity on agent and user responses,0.5792410969734192
translation,123,144,results,g- duha,has,outperforms,g- duha has outperforms,0.6459656357765198
translation,123,144,results,outperforms,has,others,outperforms has others,0.6126620769500732
translation,123,144,results,rivals,has,lm - goal,rivals has lm - goal,0.6213423013687134
translation,123,144,results,results,has,g- duha,results has g- duha,0.5367313623428345
translation,123,149,results,fail,to predict,switch,fail to predict switch,0.7813916802406311
translation,123,149,results,switch,of,domain contexts,switch of domain contexts,0.6174890995025635
translation,123,149,results,switch,explains,performance gaps,switch explains performance gaps,0.7067411541938782
translation,123,151,results,performance,between,agent and user response generation,performance between agent and user response generation,0.6144386529922485
translation,123,151,results,agent and user response generation,observe,models,agent and user response generation observe models,0.6245186924934387
translation,123,151,results,models,achieve,higher quality and diversity,models achieve higher quality and diversity,0.6040275692939758
translation,123,151,results,models,achieve,lower goal focus,models achieve lower goal focus,0.6355211734771729
translation,123,151,results,lower goal focus,when modeling,agent 's responses,lower goal focus when modeling agent 's responses,0.7153787612915039
translation,123,151,results,results,Comparing,performance,results Comparing performance,0.7179481983184814
translation,123,162,results,outperforms,on,non-redundancy,outperforms on non-redundancy,0.5635803937911987
translation,123,162,results,hred,on,natural flow,hred on natural flow,0.5985385179519653
translation,123,162,results,hred,on,non-redundancy,hred on non-redundancy,0.633478045463562
translation,123,162,results,g- duha,has,outperforms,g- duha has outperforms,0.6459656357765198
translation,123,162,results,outperforms,has,hred,outperforms has hred,0.675796627998352
translation,123,162,results,hred,has,significantly,hred has significantly,0.6661698818206787
translation,123,162,results,results,has,g- duha,results has g- duha,0.5367313623428345
translation,123,167,results,generated samples,across,all rnn - based models,generated samples across all rnn - based models,0.697918713092804
translation,123,167,results,generated samples,almost free from,grammar error,generated samples almost free from grammar error,0.7305689454078674
translation,123,167,results,all rnn - based models,almost free from,grammar error,all rnn - based models almost free from grammar error,0.694364070892334
translation,123,167,results,results,has,generated samples,results has generated samples,0.5853853821754456
translation,123,179,results,dialogue and response generation tasks,show,consistent trend,dialogue and response generation tasks show consistent trend,0.6109791398048401
translation,123,180,results,dual architecture,for,interlocutorlevel modeling,dual architecture for interlocutorlevel modeling,0.6137096285820007
translation,123,180,results,dual architecture,leads to,solid increase,dual architecture leads to solid increase,0.701949954032898
translation,123,180,results,solid increase,in,utterance diversity,solid increase in utterance diversity,0.5799683332443237
translation,123,180,results,solid increase,in,moderate improvements,solid increase in moderate improvements,0.5552307963371277
translation,123,180,results,solid increase,as,moderate improvements,solid increase as moderate improvements,0.5331616401672363
translation,123,180,results,moderate improvements,on,quality and goal focus,moderate improvements on quality and goal focus,0.5063124299049377
translation,123,180,results,results,observe,dual architecture,results observe dual architecture,0.6511642336845398
translation,123,180,results,results,applying,dual architecture,results applying dual architecture,0.7072205543518066
translation,123,182,results,no significant effect,on,diversity and goal focus,no significant effect on diversity and goal focus,0.4944731295108795
translation,123,182,results,diversity and goal focus,on,both tasks,diversity and goal focus on both tasks,0.5037717223167419
translation,123,182,results,overall utterance quality,as,bleu scores,overall utterance quality as bleu scores,0.5009440779685974
translation,123,182,results,go up,by,a bit,go up by a bit,0.5618534088134766
translation,123,182,results,marginally improves,has,overall utterance quality,marginally improves has overall utterance quality,0.5858985185623169
translation,123,182,results,bleu scores,has,go up,bleu scores has go up,0.6251679062843323
translation,123,182,results,results,For,dialogue-level attention module,results For dialogue-level attention module,0.5472651124000549
translation,123,193,results,g-duha,achieved,improvement,g-duha achieved improvement,0.7210589647293091
translation,123,193,results,g-duha,achieved,outperform,g-duha achieved outperform,0.7155666947364807
translation,123,193,results,improvement,over,vanilla dataset,improvement over vanilla dataset,0.6671155691146851
translation,123,193,results,hred,on,turn requests,hred on turn requests,0.6388226747512817
translation,123,193,results,comparable,on,joint goal,comparable on joint goal,0.5506466627120972
translation,123,193,results,outperform,has,hred,outperform has hred,0.7460828423500061
translation,123,193,results,results,Augmentation with,g-duha,results Augmentation with g-duha,0.6954412460327148
translation,124,210,ablation-analysis,tscp,increasing,maximum dialogue state span l,tscp increasing maximum dialogue state span l,0.6348222494125366
translation,124,210,ablation-analysis,maximum dialogue state span l,from,8 to 20 tokens,maximum dialogue state span l from 8 to 20 tokens,0.5483934283256531
translation,124,210,ablation-analysis,maximum dialogue state span l,helps to improve,dst performance,maximum dialogue state span l helps to improve dst performance,0.6700312495231628
translation,124,210,ablation-analysis,tscp,has,increases,tscp has increases,0.6068679690361023
translation,124,210,ablation-analysis,increases,has,training time,increases has training time,0.5688081383705139
translation,124,210,ablation-analysis,training time,has,significantly,training time has significantly,0.5726503133773804
translation,124,210,ablation-analysis,ablation analysis,In,tscp,ablation analysis In tscp,0.5263098478317261
translation,124,223,ablation-analysis,loss function,to learn,dialogue act latent variable,loss function to learn dialogue act latent variable,0.5961675643920898
translation,124,223,ablation-analysis,hurt,has,generation performance,hurt has generation performance,0.5825713872909546
translation,124,223,ablation-analysis,ablation analysis,removing,loss function,ablation analysis removing loss function,0.731475830078125
translation,124,227,ablation-analysis,state tracker and response generator modules,learning,feature representations,state tracker and response generator modules learning feature representations,0.7431723475456238
translation,124,227,ablation-analysis,feature representations,through,deeper attention networks,feature representations through deeper attention networks,0.6083812713623047
translation,124,227,ablation-analysis,deeper attention networks,improve,quality,deeper attention networks improve quality,0.6935783624649048
translation,124,227,ablation-analysis,quality,of,predicted states and system responses,quality of predicted states and system responses,0.6337649822235107
translation,124,227,ablation-analysis,ablation analysis,In,state tracker and response generator modules,ablation analysis In state tracker and response generator modules,0.5421545505523682
translation,124,227,ablation-analysis,ablation analysis,both,state tracker and response generator modules,ablation analysis both state tracker and response generator modules,0.6584491729736328
translation,124,227,ablation-analysis,ablation analysis,learning,feature representations,ablation analysis learning feature representations,0.7639861702919006
translation,124,234,ablation-analysis,drops,in,taxi and hotel domain,drops in taxi and hotel domain,0.5627235174179077
translation,124,234,ablation-analysis,drops,in,taxi domain,drops in taxi domain,0.5873150825500488
translation,124,234,ablation-analysis,performance,has,drops,performance has drops,0.5993483662605286
translation,124,234,ablation-analysis,ablation analysis,has,performance,ablation analysis has performance,0.5053174495697021
translation,124,236,ablation-analysis,significant performance gap,of,about 10 points absolute score,significant performance gap of about 10 points absolute score,0.5289081931114197
translation,124,236,ablation-analysis,about 10 points absolute score,between,dst performances,about 10 points absolute score between dst performances,0.6494411826133728
translation,124,236,ablation-analysis,dst performances,in,singledomain and multi-domain dialogues,dst performances in singledomain and multi-domain dialogues,0.5299950838088989
translation,124,105,baselines,bi-level dialogue state tracker ( bdst ),has,slot-level dst,bi-level dialogue state tracker ( bdst ) has slot-level dst,0.5660199522972107
translation,124,105,baselines,baselines,has,bi-level dialogue state tracker ( bdst ),baselines has bi-level dialogue state tracker ( bdst ),0.5467538833618164
translation,124,176,experimental-setup,experimental setup,select,"d = 256 , h att = 8","experimental setup select d = 256 , h att = 8",0.643064022064209
translation,124,176,experimental-setup,experimental setup,select,n dst s = n dst d = n gen = 3,experimental setup select n dst s = n dst d = n gen = 3,0.660346269607544
translation,124,181,experimental-setup,all networks,with,"adam optimizer ( kingma and ba , 2015 )","all networks with adam optimizer ( kingma and ba , 2015 )",0.5928172469139099
translation,124,181,experimental-setup,all networks,with,decaying learning rate schedule,all networks with decaying learning rate schedule,0.6622596383094788
translation,124,181,experimental-setup,experimental setup,train,all networks,experimental setup train all networks,0.6283628344535828
translation,124,182,experimental-setup,best models,selected,validation loss,best models selected validation loss,0.558824896812439
translation,124,183,experimental-setup,greedy approach,to decode,all slots,greedy approach to decode all slots,0.7154223918914795
translation,124,183,experimental-setup,beam search,with,beam size 5,beam search with beam size 5,0.7065784335136414
translation,124,183,experimental-setup,experimental setup,used,greedy approach,experimental setup used greedy approach,0.6003087162971497
translation,124,183,experimental-setup,experimental setup,used,beam search,experimental setup used beam search,0.5615512132644653
translation,124,186,experimental-setup,all models,using,pytorch,all models using pytorch,0.7093179821968079
translation,124,186,experimental-setup,experimental setup,implemented,all models,experimental setup implemented all models,0.7549275755882263
translation,124,33,experiments,bdst,tracks,dialogue states,bdst tracks dialogue states,0.7781594395637512
translation,124,33,experiments,dialogue states,in,dialogue context,dialogue states in dialogue context,0.5152040123939514
translation,124,33,experiments,dialogue context,in,slot and domain levels,dialogue context in slot and domain levels,0.5462483167648315
translation,124,235,experiments,taxi domain,entangled with,other domains,taxi domain entangled with other domains,0.6498057842254639
translation,124,7,model,novel unified neural architecture,for,end-to - end conversational systems,novel unified neural architecture for end-to - end conversational systems,0.6122050285339355
translation,124,7,model,novel unified neural architecture,jointly train,bi-level state tracker,novel unified neural architecture jointly train bi-level state tracker,0.6780093908309937
translation,124,7,model,end-to - end conversational systems,in,multi-domain task - oriented dialogues,end-to - end conversational systems in multi-domain task - oriented dialogues,0.5066406726837158
translation,124,7,model,bi-level state tracker,which tracks,dialogue states,bi-level state tracker which tracks dialogue states,0.7327948212623596
translation,124,7,model,dialogue states,by learning,signals,dialogue states by learning signals,0.746360182762146
translation,124,7,model,joint dialogue act and response generator,incorporates,information,joint dialogue act and response generator incorporates information,0.7297278046607971
translation,124,7,model,joint dialogue act and response generator,models,dialogue acts and target responses simultaneously,joint dialogue act and response generator models dialogue acts and target responses simultaneously,0.69671231508255
translation,124,7,model,information,from,various input components,information from various input components,0.5583583116531372
translation,124,7,model,uniconv,has,novel unified neural architecture,uniconv has novel unified neural architecture,0.5799806714057922
translation,124,29,model,unified neural network architecture,for,end-to - end dialogue systems,unified neural network architecture for end-to - end dialogue systems,0.6132091283798218
translation,124,29,model,uniconv,has,unified neural network architecture,uniconv has unified neural network architecture,0.5796772837638855
translation,124,29,model,model,propose,uniconv,model propose uniconv,0.6987080574035645
translation,124,30,model,uniconv,consists of,bi-level state tracking ( bdst ) module,uniconv consists of bi-level state tracking ( bdst ) module,0.6759682893753052
translation,124,30,model,bi-level state tracking ( bdst ) module,embeds,natural language understanding,bi-level state tracking ( bdst ) module embeds natural language understanding,0.640475332736969
translation,124,30,model,natural language understanding,directly parse,dialogue context,natural language understanding directly parse dialogue context,0.6433777809143066
translation,124,30,model,dialogue context,into,structured dialogue state,dialogue context into structured dialogue state,0.5615795254707336
translation,124,30,model,semantic frame,output from,nlu module,semantic frame output from nlu module,0.7644767761230469
translation,124,30,model,model,has,uniconv,model has uniconv,0.641883909702301
translation,124,31,model,bdst,implicitly models and integrates,slot representations,bdst implicitly models and integrates slot representations,0.7151203751564026
translation,124,31,model,slot representations,from,dialogue contextual cues,slot representations from dialogue contextual cues,0.575400173664093
translation,124,31,model,slot values,in,each turn,slot values in each turn,0.5423569679260254
translation,124,31,model,explicit slot tagging features,from,nlu,explicit slot tagging features from nlu,0.5378761887550354
translation,124,31,model,model,has,bdst,model has bdst,0.6128435730934143
translation,124,34,model,output representations,from,two levels,output representations from two levels,0.5922331809997559
translation,124,34,model,output representations,combined in,late fusion approach,output representations combined in late fusion approach,0.7337117195129395
translation,124,34,model,late fusion approach,to learn,multi-domain dialogue states,late fusion approach to learn multi-domain dialogue states,0.5910822749137878
translation,124,34,model,model,has,output representations,model has output representations,0.5695722103118896
translation,124,35,model,dialogue state tracker,disentangles,slot and domain representation learning,dialogue state tracker disentangles slot and domain representation learning,0.7487998008728027
translation,124,35,model,slot and domain representation learning,while enabling,deep learning,slot and domain representation learning while enabling deep learning,0.6620465517044067
translation,124,35,model,deep learning,of,shared representations,deep learning of shared representations,0.5353181958198547
translation,124,35,model,shared representations,of,slots,shared representations of slots,0.6486846804618835
translation,124,35,model,slots,common among,domains,slots common among domains,0.7291495203971863
translation,124,35,model,model,has,dialogue state tracker,model has dialogue state tracker,0.5503655076026917
translation,124,36,model,uniconv,integrates,bdst,uniconv integrates bdst,0.7144383788108826
translation,124,36,model,uniconv,generates,system responses,uniconv generates system responses,0.6423120498657227
translation,124,36,model,bdst,with,joint dialogue act and response generator ( darg ),bdst with joint dialogue act and response generator ( darg ),0.646506130695343
translation,124,36,model,joint dialogue act and response generator ( darg ),simultaneously models,dialogue acts,joint dialogue act and response generator ( darg ) simultaneously models dialogue acts,0.7419789433479309
translation,124,36,model,joint dialogue act and response generator ( darg ),simultaneously models,dialogue acts,joint dialogue act and response generator ( darg ) simultaneously models dialogue acts,0.7419789433479309
translation,124,36,model,system responses,by learning,latent variable,system responses by learning latent variable,0.614266037940979
translation,124,36,model,latent variable,representing,dialogue acts,latent variable representing dialogue acts,0.6731005907058716
translation,124,36,model,semantically conditioning output response tokens,on,latent variable,semantically conditioning output response tokens on latent variable,0.5166406631469727
translation,124,36,model,model,has,uniconv,model has uniconv,0.641883909702301
translation,124,37,model,our models,to model,dialogue acts,our models to model dialogue acts,0.723745584487915
translation,124,37,model,our models,to model,dialogue acts,our models to model dialogue acts,0.723745584487915
translation,124,37,model,our models,utilize,distributed representations,our models utilize distributed representations,0.5655302405357361
translation,124,37,model,distributed representations,of,dialogue acts,distributed representations of dialogue acts,0.5617703795433044
translation,124,37,model,distributed representations,rather than,hard discrete output values,distributed representations rather than hard discrete output values,0.6018773317337036
translation,124,37,model,hard discrete output values,from,dialogue policy module,hard discrete output values from dialogue policy module,0.5490996837615967
translation,124,37,model,model,utilize,distributed representations,model utilize distributed representations,0.5890589356422424
translation,124,39,model,output representations,refined after,each step,output representations refined after each step,0.7541924118995667
translation,124,39,model,each step,to obtain,high-resolution signals,each step to obtain high-resolution signals,0.6211349368095398
translation,124,39,model,high-resolution signals,needed to generate,appropriate dialogue acts and responses,high-resolution signals needed to generate appropriate dialogue acts and responses,0.7386194467544556
translation,124,39,model,model,has,output representations,model has output representations,0.5695722103118896
translation,124,40,model,bdst and darg,for,end-to - end neural dialogue systems,bdst and darg for end-to - end neural dialogue systems,0.6273216009140015
translation,124,40,model,bdst and darg,from,input dialogues,bdst and darg from input dialogues,0.6233054399490356
translation,124,40,model,input dialogues,has,to output system responses,input dialogues has to output system responses,0.6013085842132568
translation,124,40,model,model,combine,bdst and darg,model combine bdst and darg,0.7554260492324829
translation,124,86,model,bi-level state tracker ( bdst ),to detect,contextual dependencies,bi-level state tracker ( bdst ) to detect contextual dependencies,0.6248951554298401
translation,124,86,model,contextual dependencies,to generate,dialogue states,contextual dependencies to generate dialogue states,0.6423658132553101
translation,124,86,model,model,has,bi-level state tracker ( bdst ),model has bi-level state tracker ( bdst ),0.5460693836212158
translation,124,87,model,dst,includes,2 modules,dst includes 2 modules,0.7303666472434998
translation,124,87,model,2 modules,for,slot-level and domain-level representation learning,2 modules for slot-level and domain-level representation learning,0.5247581005096436
translation,124,87,model,model,has,dst,model has dst,0.597681999206543
translation,124,88,model,each module,comprises,attention layers,each module comprises attention layers,0.6639934182167053
translation,124,88,model,each module,incorporate,important information,each module incorporate important information,0.6948906779289246
translation,124,88,model,attention layers,to project,domain or slot representations,attention layers to project domain or slot representations,0.6657094359397888
translation,124,88,model,important information,from,dialogue context,important information from dialogue context,0.5020267367362976
translation,124,88,model,important information,from,current user utterance,important information from current user utterance,0.5037161707878113
translation,124,88,model,important information,dialogue state of,previous turn,important information dialogue state of previous turn,0.7522101402282715
translation,124,88,model,important information,dialogue state of,current user utterance,important information dialogue state of current user utterance,0.7259522080421448
translation,124,88,model,model,has,each module,model has each module,0.5586554408073425
translation,124,90,model,joint dialogue act and response generator ( darg ),projects,target system response representations,joint dialogue act and response generator ( darg ) projects target system response representations,0.7559207677841187
translation,124,90,model,joint dialogue act and response generator ( darg ),enhances them with,information,joint dialogue act and response generator ( darg ) enhances them with information,0.7512279152870178
translation,124,90,model,target system response representations,enhances them with,information,target system response representations enhances them with information,0.6862123608589172
translation,124,90,model,information,from,various dialogue components,information from various dialogue components,0.5524153709411621
translation,124,90,model,model,has,joint dialogue act and response generator ( darg ),model has joint dialogue act and response generator ( darg ),0.583938717842102
translation,124,180,model,inference,in,dst and end-to - end tasks,inference in dst and end-to - end tasks,0.5607831478118896
translation,124,180,model,inference,decode,system responses,inference decode system responses,0.7274174094200134
translation,124,180,model,system responses,using,previously decoded state,system responses using previously decoded state,0.6751480102539062
translation,124,180,model,system responses,using,new predicted state,system responses using new predicted state,0.6998777985572815
translation,124,180,model,previously decoded state,as,input,previously decoded state as input,0.5209003686904907
translation,124,180,model,input,in,current turn,input in current turn,0.5631291270256042
translation,124,180,model,new predicted state,to query,dbs,new predicted state to query dbs,0.8308608531951904
translation,124,180,model,system responses,has,sequentially turn by turn,system responses has sequentially turn by turn,0.602109968662262
translation,124,180,model,model,During,inference,model During inference,0.6954593062400818
translation,124,226,model,semantic condition,on,each token,semantic condition on each token,0.5100993514060974
translation,124,226,model,each token,of,target response,each token of target response,0.5634005665779114
translation,124,226,model,dialogue flow,towards,successful task completion,dialogue flow towards successful task completion,0.6521329879760742
translation,124,226,model,model,By enforcing,semantic condition,model By enforcing semantic condition,0.7220866680145264
translation,124,189,results,outperform,showing,advantage,outperform showing advantage,0.7469925284385681
translation,124,189,results,fixed - vocabulary approaches,such as,hjst and fjst,fixed - vocabulary approaches such as hjst and fjst,0.6217524409294128
translation,124,189,results,fixed - vocabulary approaches,showing,advantage,fixed - vocabulary approaches showing advantage,0.7074077129364014
translation,124,189,results,advantage,of generating,unique slot values,advantage of generating unique slot values,0.7004693746566772
translation,124,189,results,our model,has,outperform,our model has outperform,0.6410396099090576
translation,124,189,results,outperform,has,fixed - vocabulary approaches,outperform has fixed - vocabulary approaches,0.6002851128578186
translation,124,189,results,results,has,our model,results has our model,0.5871725678443909
translation,124,199,results,our model,achieves,"very competitive inform , success , and bleu scores","our model achieves very competitive inform , success , and bleu scores",0.6473041772842407
translation,124,199,results,results,has,our model,results has our model,0.5871725678443909
translation,124,201,results,hdsa,in,inform score,hdsa in inform score,0.5459674000740051
translation,124,201,results,graph structure,to represent,acts,graph structure to represent acts,0.730744481086731
translation,124,201,results,our approach,is,simpler,our approach is simpler,0.6106987595558167
translation,124,201,results,simpler,able to outperform,hdsa,simpler able to outperform hdsa,0.807872474193573
translation,124,201,results,hdsa,in,inform score,hdsa in inform score,0.5459674000740051
translation,124,201,results,hdsa,has,our approach,hdsa has our approach,0.6223912835121155
translation,124,201,results,results,Compared to,hdsa,results Compared to hdsa,0.6763321161270142
translation,124,205,results,our models,on,multiwoz2.1,our models on multiwoz2.1,0.5846351385116577
translation,124,209,results,outperforms,in,all metrics,outperforms in all metrics,0.5249384641647339
translation,124,209,results,existing baselines,in,all metrics,existing baselines in all metrics,0.4500300884246826
translation,124,209,results,all metrics,except,inform score,all metrics except inform score,0.7082772850990295
translation,124,209,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,124,209,results,outperforms,has,existing baselines,outperforms has existing baselines,0.5949252247810364
translation,124,209,results,results,has,our model,results has our model,0.5871725678443909
translation,124,212,results,dst,has,improves,dst has improves,0.6219154596328735
translation,124,212,results,results,performance of,dst,results performance of dst,0.713472843170166
translation,124,218,results,dual architecture,improves,latency,dual architecture improves latency,0.7062444686889648
translation,124,218,results,latency,in,each attention layers,latency in each attention layers,0.5345053672790527
translation,124,218,results,each attention layers,as typically,d + s ds,each attention layers as typically d + s ds,0.6602347493171692
translation,124,218,results,results,using,dual architecture,results using dual architecture,0.6988375782966614
translation,124,220,results,last user utterance,performs,as well,last user utterance performs as well,0.6149803996086121
translation,124,220,results,as well,using,b,as well using b,0.7478068470954895
translation,124,220,results,results,Using,b t?1,results Using b t?1,0.6165797114372253
translation,124,229,results,our model,achieves,better performance,our model achieves better performance,0.6816908121109009
translation,124,229,results,better performance,as,number of attention heads,better performance as number of attention heads,0.5359593629837036
translation,124,229,results,end-to - end task,has,our model,end-to - end task has our model,0.5551432967185974
translation,124,229,results,number of attention heads,has,increases,number of attention heads has increases,0.614625096321106
translation,124,229,results,results,in,end-to - end task,results in end-to - end task,0.564139187335968
translation,124,233,results,dst,performs,consistently well,dst performs consistently well,0.6581549048423767
translation,124,233,results,consistently well,in,"3 domains attraction , restaurant , and train domains","consistently well in 3 domains attraction , restaurant , and train domains",0.4703920781612396
translation,125,19,model,automatic approach,to,build it break it fix it,automatic approach to build it break it fix it,0.5752138495445251
translation,125,19,model,automatic approach,to,strategy,automatic approach to strategy,0.602440595626831
translation,125,19,model,automatic approach,to,build it break it,automatic approach to build it break it,0.5846135020256042
translation,125,19,model,automatic approach,to,approach,automatic approach to approach,0.6047154664993286
translation,125,19,model,build it break it fix it,has,strategy,build it break it fix it has strategy,0.6186840534210205
translation,125,19,model,build it break it,has,approach,build it break it has approach,0.627530038356781
translation,125,19,model,model,develop,automatic approach,model develop automatic approach,0.6765100955963135
translation,125,22,model,fix sequence,over,number of iterations,fix sequence over number of iterations,0.6822623014450073
translation,125,22,model,model,repeat,"whole build , break ,","model repeat whole build , break ,",0.7007671594619751
translation,125,183,model,bert - based model variant,splits,last utterance ( to be classified ) and the rest of the history,bert - based model variant splits last utterance ( to be classified ) and the rest of the history,0.6735588312149048
translation,125,183,model,last utterance ( to be classified ) and the rest of the history,into,two dialogue segments,last utterance ( to be classified ) and the rest of the history into two dialogue segments,0.549734890460968
translation,125,183,model,model,build,bert - based model variant,model build bert - based model variant,0.7018588781356812
translation,125,76,results,bert - based model,use,bertbased architecture,bert - based model use bertbased architecture,0.6283655762672424
translation,125,76,results,outperforms,use,bertbased architecture,outperforms use bertbased architecture,0.6465358734130859
translation,125,76,results,bert - based model,has,outperforms,bert - based model has outperforms,0.6104244589805603
translation,125,76,results,outperforms,has,method,outperforms has method,0.6658044457435608
translation,125,76,results,results,use,bertbased architecture,results use bertbased architecture,0.6309707164764404
translation,125,76,results,results,has,bert - based model,results has bert - based model,0.5469893217086792
translation,125,189,results,best single - turn classifier,in,multi-turn set- up,best single - turn classifier in multi-turn set- up,0.5394781827926636
translation,125,189,results,task,becomes,easier,task becomes easier,0.6359591484069824
translation,125,189,results,average score per round,is,2.89,average score per round is 2.89,0.5611246228218079
translation,125,189,results,best single - turn classifier,has,task,best single - turn classifier has task,0.5393357872962952
translation,125,189,results,multi-turn set- up,has,task,multi-turn set- up has task,0.5599468946456909
translation,125,189,results,results,using,best single - turn classifier,results using best single - turn classifier,0.664068877696991
translation,125,197,results,fasttext model,performs,worse,fasttext model performs worse,0.6805932521820068
translation,125,197,results,worse,given,dialogue context,worse given dialogue context,0.7385756373405457
translation,125,197,results,results,has,fasttext model,results has fasttext model,0.5676124691963196
translation,125,199,results,simple bert - based architecture,observe,average of a 3.7 point increase,simple bert - based architecture observe average of a 3.7 point increase,0.6251073479652405
translation,125,199,results,average of a 3.7 point increase,in,offensive - class f1,average of a 3.7 point increase in offensive - class f1,0.5211523175239563
translation,125,199,results,offensive - class f1,with,addition of context,offensive - class f1 with addition of context,0.6542450189590454
translation,125,199,results,results,With,simple bert - based architecture,results With simple bert - based architecture,0.6670181751251221
translation,125,200,results,segments,to separate,context,segments to separate context,0.7009566426277161
translation,125,200,results,context,from,utterance,context from utterance,0.5900341868400574
translation,125,200,results,utterance,trying to,classify,utterance trying to classify,0.6553760170936584
translation,125,200,results,7.4 point increase,in,offensive - class f1,7.4 point increase in offensive - class f1,0.5438063740730286
translation,125,200,results,results,use,segments,results use segments,0.6173221468925476
translation,126,85,ablation-analysis,decreases,due to,decrease,decreases due to decrease,0.6669050455093384
translation,126,85,ablation-analysis,decrease,of,knowledge triples and utterances,decrease of knowledge triples and utterances,0.5984627604484558
translation,126,85,ablation-analysis,knowledge triples and utterances,in,three domains,knowledge triples and utterances in three domains,0.5212675929069519
translation,126,85,ablation-analysis,distinct - 4 score,has,decreases,distinct - 4 score has decreases,0.6073660850524902
translation,126,85,ablation-analysis,ablation analysis,has,distinct - 4 score,ablation analysis has distinct - 4 score,0.5534535646438599
translation,126,138,experimental-setup,jieba chinese word segmenter,employed for,tokenization,jieba chinese word segmenter employed for tokenization,0.6036746501922607
translation,126,138,experimental-setup,experimental setup,has,jieba chinese word segmenter,experimental setup has jieba chinese word segmenter,0.5217050313949585
translation,126,139,experimental-setup,200 - dimensional word embeddings,initialized by,song et al . ( 2018 ),200 - dimensional word embeddings initialized by song et al . ( 2018 ),0.6187236309051514
translation,126,139,experimental-setup,unmatched ones,randomly sampled from,"standard normal distribution n ( 0 , 1 )","unmatched ones randomly sampled from standard normal distribution n ( 0 , 1 )",0.7417595982551575
translation,126,139,experimental-setup,experimental setup,has,200 - dimensional word embeddings,experimental setup has 200 - dimensional word embeddings,0.5019996762275696
translation,126,140,experimental-setup,type of rnn network units,was,all gru,type of rnn network units was all gru,0.6386947631835938
translation,126,140,experimental-setup,type of rnn network units,was,gru cells,type of rnn network units was gru cells,0.5990697741508484
translation,126,140,experimental-setup,number of hidden units,of,gru cells,number of hidden units of gru cells,0.6022762060165405
translation,126,140,experimental-setup,gru cells,set to,200,gru cells set to 200,0.6824456453323364
translation,126,140,experimental-setup,experimental setup,has,type of rnn network units,experimental setup has type of rnn network units,0.545664370059967
translation,126,140,experimental-setup,experimental setup,has,number of hidden units,experimental setup has number of hidden units,0.5046243071556091
translation,126,141,experimental-setup,"adam ( kingma and ba , 2014 )",to optimize,all the models,"adam ( kingma and ba , 2014 ) to optimize all the models",0.6574631333351135
translation,126,141,experimental-setup,all the models,with,initial learning rate,all the models with initial learning rate,0.5988242626190186
translation,126,141,experimental-setup,initial learning rate,of,5 ? 10 ?5,initial learning rate of 5 ? 10 ?5,0.6562320590019226
translation,126,141,experimental-setup,initial learning rate,of,10 ?3,initial learning rate of 10 ?3,0.6364452838897705
translation,126,141,experimental-setup,5 ? 10 ?5,for,bert,5 ? 10 ?5 for bert,0.7020357251167297
translation,126,141,experimental-setup,experimental setup,has,"adam ( kingma and ba , 2014 )","experimental setup has adam ( kingma and ba , 2014 )",0.5447093844413757
translation,126,142,experimental-setup,mini-batch sizes,set to,2 dialogues,mini-batch sizes set to 2 dialogues,0.666634202003479
translation,126,142,experimental-setup,2 dialogues,for,lm,2 dialogues for lm,0.621260404586792
translation,126,142,experimental-setup,2 dialogues,for,32 pairs,2 dialogues for 32 pairs,0.574924886226654
translation,126,142,experimental-setup,32 pairs,of,post and response,32 pairs of post and response,0.627119779586792
translation,126,142,experimental-setup,post and response,for,seq2seq and hred,post and response for seq2seq and hred,0.63892662525177
translation,126,142,experimental-setup,experimental setup,has,mini-batch sizes,experimental setup has mini-batch sizes,0.5382490158081055
translation,126,5,experiments,topics,in,multi-turn conversations,topics in multi-turn conversations,0.5080801248550415
translation,126,5,experiments,multi-turn conversations,to,knowledge graphs,multi-turn conversations to knowledge graphs,0.5513179302215576
translation,126,5,experiments,chinese multi-domain knowledge-driven conversation dataset,has,kdconv,chinese multi-domain knowledge-driven conversation dataset has kdconv,0.5385425090789795
translation,126,6,experiments,4.5 k conversations,from,"three domains ( film , music , and travel )","4.5 k conversations from three domains ( film , music , and travel )",0.5604941248893738
translation,126,6,experiments,86 k utterances,with,average turn number,86 k utterances with average turn number,0.6707172393798828
translation,126,6,experiments,average turn number,of,19.0,average turn number of 19.0,0.5912256240844727
translation,126,187,experiments,chinese multi-domain corpus,for,knowledge-driven conversation generation,chinese multi-domain corpus for knowledge-driven conversation generation,0.4976501166820526
translation,126,21,model,chinese multi-domain dataset,towards,multi-turn knowledge-driven conversation,chinese multi-domain dataset towards multi-turn knowledge-driven conversation,0.5908628106117249
translation,126,21,model,multi-turn knowledge-driven conversation,suitable for,modeling,multi-turn knowledge-driven conversation suitable for modeling,0.6530978083610535
translation,126,21,model,knowledge interactions,in,multi-turn human-like dialogues,knowledge interactions in multi-turn human-like dialogues,0.5356667637825012
translation,126,21,model,knowledge interactions,including,knowledge planning,knowledge interactions including knowledge planning,0.7092210650444031
translation,126,21,model,knowledge interactions,including,knowledge grounding,knowledge interactions including knowledge grounding,0.6868180632591248
translation,126,21,model,knowledge interactions,including,knowledge adaptations,knowledge interactions including knowledge adaptations,0.7119770050048828
translation,126,21,model,kdconv,has,chinese multi-domain dataset,kdconv has chinese multi-domain dataset,0.5467262864112854
translation,126,21,model,modeling,has,knowledge interactions,modeling has knowledge interactions,0.558361291885376
translation,126,21,model,model,propose,kdconv,model propose kdconv,0.6963517069816589
translation,126,84,results,distinct - 4 score,is,0.54/0.51/0.42,distinct - 4 score is 0.54/0.51/0.42,0.5383041501045227
translation,126,84,results,0.54/0.51/0.42,for,film / music / travel domain,0.54/0.51/0.42 for film / music / travel domain,0.5986103415489197
translation,126,84,results,results,has,distinct - 4 score,results has distinct - 4 score,0.5482139587402344
translation,126,152,results,influence of knowledge,introducing,knowledge,influence of knowledge introducing knowledge,0.7144584059715271
translation,126,152,results,improved,in terms of,all the metrics,improved in terms of all the metrics,0.6631143093109131
translation,126,152,results,all the metrics,except,ppl,all the metrics except ppl,0.6856759190559387
translation,126,152,results,ppl,in,all the domains,ppl in all the domains,0.5614908337593079
translation,126,152,results,influence of knowledge,has,all the models,influence of knowledge has all the models,0.5761833190917969
translation,126,152,results,knowledge,has,all the models,knowledge has all the models,0.613167941570282
translation,126,152,results,results,has,influence of knowledge,results has influence of knowledge,0.5590724349021912
translation,126,153,results,all the models,obtain,higher hits@1 scores,all the models obtain higher hits@1 scores,0.5696014761924744
translation,126,153,results,higher hits@1 scores,obtains,improvement,higher hits@1 scores obtains improvement,0.6389554738998413
translation,126,153,results,bert,obtains,improvement,bert obtains improvement,0.636242687702179
translation,126,153,results,improvement,of,0.4,improvement of 0.4,0.6124352216720581
translation,126,153,results,0.4,on,hits@1,0.4 on hits@1,0.6001856923103333
translation,126,153,results,results,has,all the models,results has all the models,0.5321599841117859
translation,126,154,results,knowledge,into,bert,knowledge into bert,0.6620190739631653
translation,126,154,results,performance,of,hits@1,performance of hits@1,0.597352921962738
translation,126,154,results,knowledge,has,performance,knowledge has performance,0.5780047178268433
translation,126,154,results,bert,has,performance,bert has performance,0.6267343759536743
translation,126,154,results,hits@1,has,improves slightly,hits@1 has improves slightly,0.6362455487251282
translation,126,154,results,results,After incorporating,knowledge,results After incorporating knowledge,0.668165385723114
translation,126,155,results,seq2seq and hred,have,better bleu -k scores,seq2seq and hred have better bleu -k scores,0.558373212814331
translation,126,155,results,better bleu -k scores,obtains,improvement,better bleu -k scores obtains improvement,0.5845679640769958
translation,126,155,results,better bleu -k scores,means,better quality of generated responses,better bleu -k scores means better quality of generated responses,0.6276853084564209
translation,126,155,results,improvement,of,7.2,improvement of 7.2,0.60810786485672
translation,126,155,results,7.2,on,bleu - 4,7.2 on bleu - 4,0.5240480899810791
translation,126,155,results,results,has,seq2seq and hred,results has seq2seq and hred,0.5179386138916016
translation,126,156,results,two generation - based models,gain,larger distinct -k values,two generation - based models gain larger distinct -k values,0.73186194896698
translation,126,156,results,larger distinct -k values,in,music domain,larger distinct -k values in music domain,0.5467032790184021
translation,126,156,results,improvement,of,12.4,improvement of 12.4,0.6007910966873169
translation,126,156,results,12.4,on,distinct - 4,12.4 on distinct - 4,0.6285195350646973
translation,126,156,results,better diversity,of,generated results,better diversity of generated results,0.5569624900817871
translation,126,156,results,results,has,two generation - based models,results has two generation - based models,0.5005457401275635
translation,126,158,results,knowledge - aware bert model,achieves,best performance,knowledge - aware bert model achieves best performance,0.6888914108276367
translation,126,158,results,best performance,in,most of the metrics,best performance in most of the metrics,0.48439669609069824
translation,126,158,results,all the three domains,has,knowledge - aware bert model,all the three domains has knowledge - aware bert model,0.5718156099319458
translation,126,158,results,results,In,all the three domains,results In all the three domains,0.47929632663726807
translation,126,159,results,hred,performs,best,hred performs best,0.6882449984550476
translation,126,159,results,best,in,bleu -k and distinct -k,best in bleu -k and distinct -k,0.5351523756980896
translation,126,159,results,bleu -k and distinct -k,among,all the generationbased baselines,bleu -k and distinct -k among all the generationbased baselines,0.5557844042778015
translation,126,159,results,results,has,hred,results has hred,0.6006986498832703
translation,126,160,results,better results,of,bleu -k,better results of bleu -k,0.543664276599884
translation,126,160,results,bleu -k,in,film and music domains,bleu -k in film and music domains,0.5282054543495178
translation,126,160,results,better results,of,distinct -k,better results of distinct -k,0.6140929460525513
translation,126,160,results,distinct -k,in,film domain,distinct -k in film domain,0.6033872961997986
translation,126,160,results,knowledge - enhanced seq2seq,achieves,best hits@1/ 3 scores,knowledge - enhanced seq2seq achieves best hits@1/ 3 scores,0.6797482967376709
translation,126,160,results,best hits@1/ 3 scores,among,all the generation - based models,best hits@1/ 3 scores among all the generation - based models,0.5763769149780273
translation,126,160,results,knowledge - aware hred,has,better results,knowledge - aware hred has better results,0.5988497138023376
translation,126,160,results,results,has,knowledge - aware hred,results has knowledge - aware hred,0.5405522584915161
translation,126,162,results,performance,best in,film domain,performance best in film domain,0.6649507880210876
translation,126,162,results,performance,worst in,travel domain,performance worst in travel domain,0.7138688564300537
translation,126,162,results,retrievalbased models,has,performance,retrievalbased models has performance,0.5532416701316833
translation,126,162,results,results,For,retrievalbased models,results For retrievalbased models,0.5979732275009155
translation,126,163,results,improves,from,film domain,improves from film domain,0.5986790657043457
translation,126,163,results,improves,from,film domain,improves from film domain,0.5986790657043457
translation,126,163,results,improves,as,average number of utterances per dialogue,improves as average number of utterances per dialogue,0.5716095566749573
translation,126,163,results,film domain,to,travel domain,film domain to travel domain,0.5907374620437622
translation,126,163,results,film domain,to,travel domain,film domain to travel domain,0.5907374620437622
translation,126,163,results,decreases,from,24.4,decreases from 24.4,0.5966714024543762
translation,126,163,results,24.4,in,film domain,24.4 in film domain,0.5536021590232849
translation,126,163,results,16.1,in,travel domain,16.1 in travel domain,0.49599677324295044
translation,126,163,results,generationbased models,has,performance,generationbased models has performance,0.57652348279953
translation,126,163,results,performance,has,improves,performance has improves,0.6178221106529236
translation,126,163,results,average number of utterances per dialogue,has,decreases,average number of utterances per dialogue has decreases,0.6276821494102478
translation,126,163,results,results,For,generationbased models,results For generationbased models,0.620291531085968
translation,126,178,results,overall 3/3 10 agreement,for,fluency and coherence,overall 3/3 10 agreement for fluency and coherence,0.6228318214416504
translation,126,178,results,fluency and coherence,from,68.14 % to 81.33 %,fluency and coherence from 68.14 % to 81.33 %,0.48277589678764343
translation,126,178,results,68.14 % to 81.33 %,in,three domains,68.14 % to 81.33 % in three domains,0.5150267481803894
translation,126,178,results,results,has,overall 3/3 10 agreement,results has overall 3/3 10 agreement,0.5707151293754578
translation,126,180,results,results,has,results,results has results,0.48582205176353455
translation,126,181,results,other models,in,both metrics,other models in both metrics,0.47727033495903015
translation,126,181,results,both metrics,in,all the three domains,both metrics in all the three domains,0.5083525776863098
translation,126,181,results,knowledge - aware bert,has,outperforms,knowledge - aware bert has outperforms,0.6277208924293518
translation,126,181,results,outperforms,has,other models,outperforms has other models,0.5875559449195862
translation,126,181,results,results,has,knowledge - aware bert,results has knowledge - aware bert,0.5593082904815674
translation,126,183,results,fluency scores,of,both generation - based models,fluency scores of both generation - based models,0.5001741051673889
translation,126,183,results,both generation - based models,close to,2.00,both generation - based models close to 2.00,0.7264562249183655
translation,126,183,results,results,has,fluency scores,results has fluency scores,0.4856947362422943
translation,126,184,results,coherence scores,of,hred and knowledgeaware hred,coherence scores of hred and knowledgeaware hred,0.5685567855834961
translation,126,184,results,coherence scores,both,hred and knowledgeaware hred,coherence scores both hred and knowledgeaware hred,0.671083390712738
translation,126,184,results,hred and knowledgeaware hred,higher than,1.00,hred and knowledgeaware hred higher than 1.00,0.7154721021652222
translation,126,184,results,hred and knowledgeaware hred,still,huge gap,hred and knowledgeaware hred still huge gap,0.739712119102478
translation,126,184,results,hred and knowledgeaware hred,have,huge gap,hred and knowledgeaware hred have huge gap,0.5828227996826172
translation,126,184,results,huge gap,to,2.00,huge gap to 2.00,0.5846306681632996
translation,126,184,results,results,has,coherence scores,results has coherence scores,0.49353936314582825
translation,126,185,results,knowledge information,into,hred,knowledge information into hred,0.6240261793136597
translation,126,185,results,improved significantly,in,all the three domains,improved significantly in all the three domains,0.4960578382015228
translation,126,185,results,knowledge information,has,coherence score,knowledge information has coherence score,0.555830180644989
translation,126,185,results,hred,has,coherence score,hred has coherence score,0.5737041234970093
translation,126,185,results,results,After incorporating,knowledge information,results After incorporating knowledge information,0.6738855838775635
translation,127,193,ablation-analysis,incorporation of domain knowledge,are,effective approaches,incorporation of domain knowledge are effective approaches,0.5587253570556641
translation,127,193,ablation-analysis,effective approaches,for improving,dual encoder architecture,effective approaches for improving dual encoder architecture,0.7050179243087769
translation,127,193,ablation-analysis,ablation analysis,suggest,attention mechanism,ablation analysis suggest attention mechanism,0.5595778822898865
translation,127,193,ablation-analysis,ablation analysis,suggest,incorporation of domain knowledge,ablation analysis suggest incorporation of domain knowledge,0.6032374501228333
translation,127,193,ablation-analysis,ablation analysis,both,attention mechanism,ablation analysis both attention mechanism,0.6053327918052673
translation,127,69,baselines,match-lstm,improves,matching performance,match-lstm improves matching performance,0.6849666833877563
translation,127,69,baselines,", 2016 )",improves,matching performance,", 2016 ) improves matching performance",0.6578763127326965
translation,127,69,baselines,matching performance,by using,"lstm - based , attention - weighted sentence representations","matching performance by using lstm - based , attention - weighted sentence representations",0.6233048439025879
translation,127,69,baselines,baselines,has,match-lstm,baselines has match-lstm,0.5465887784957886
translation,127,70,baselines,qa -lstm,uses,simple attention mechanism,qa -lstm uses simple attention mechanism,0.5752297639846802
translation,127,70,baselines,qa -lstm,combines,lstm encoder,qa -lstm combines lstm encoder,0.7006343603134155
translation,127,70,baselines,lstm encoder,with,cnn,lstm encoder with cnn,0.5954582095146179
translation,127,70,baselines,baselines,has,qa -lstm,baselines has qa -lstm,0.5646029710769653
translation,127,174,baselines,attention and external knowledge augmented de,with,bi-directional gru ( ak - de - bigru ),attention and external knowledge augmented de with bi-directional gru ( ak - de - bigru ),0.6621617078781128
translation,127,184,baselines,de,using,gru or a bidirectional gru as encoder,de using gru or a bidirectional gru as encoder,0.7442682385444641
translation,127,184,baselines,attention augmented encoding,for,embedding,attention augmented encoding for embedding,0.6191221475601196
translation,127,184,baselines,different model variants,has,de,different model variants has de,0.639286994934082
translation,127,158,experimental-setup,"fasttext ( bojanowski et al. , 2016 )",to pre-train,word embeddings,"fasttext ( bojanowski et al. , 2016 ) to pre-train word embeddings",0.678451657295227
translation,127,158,experimental-setup,word embeddings,using,training set,word embeddings using training set,0.5762940645217896
translation,127,158,experimental-setup,training set,instead of using,offthe-shelf word embeddings,training set instead of using offthe-shelf word embeddings,0.6222946643829346
translation,127,159,experimental-setup,hidden dimension,of,our gru,hidden dimension of our gru,0.6058240532875061
translation,127,159,experimental-setup,our gru,to be,300,our gru to be 300,0.6388073563575745
translation,127,159,experimental-setup,experimental setup,set,hidden dimension,experimental setup set hidden dimension,0.6768732070922852
translation,127,160,experimental-setup,sequence length,of,context,sequence length of context,0.5557712316513062
translation,127,160,experimental-setup,sequence length,of,response,sequence length of response,0.6309739947319031
translation,127,160,experimental-setup,response,by,160,response by 160,0.6473082900047302
translation,127,160,experimental-setup,experimental setup,restricted,sequence length,experimental setup restricted sequence length,0.6467637419700623
translation,127,161,experimental-setup,smaller batch size,of,32,smaller batch size of 32,0.6471760272979736
translation,127,161,experimental-setup,experimental setup,use,smaller batch size,experimental setup use smaller batch size,0.6100409030914307
translation,127,162,experimental-setup,binary cross entropy loss,of,our model,binary cross entropy loss of our model,0.5126222372055054
translation,127,162,experimental-setup,our model,with respect to,training data,our model with respect to training data,0.6332647204399109
translation,127,162,experimental-setup,"adam ( kingma and ba , 2015 )",with,initial learning rate,"adam ( kingma and ba , 2015 ) with initial learning rate",0.5912186503410339
translation,127,162,experimental-setup,initial learning rate,of,0.0001,initial learning rate of 0.0001,0.5696305632591248
translation,127,162,experimental-setup,experimental setup,optimize,binary cross entropy loss,experimental setup optimize binary cross entropy loss,0.7025249600410461
translation,127,163,experimental-setup,our model,for,maximum of 20 epochs,our model for maximum of 20 epochs,0.5721575021743774
translation,127,163,experimental-setup,experimental setup,train,our model,experimental setup train our model,0.6429243087768555
translation,127,169,experimental-setup,end-toend,with,single 12gb gpu,end-toend with single 12gb gpu,0.636258065700531
translation,127,169,experimental-setup,model,has,end-toend,model has end-toend,0.579994261264801
translation,127,169,experimental-setup,experimental setup,train,model,experimental setup train model,0.6514950394630432
translation,127,169,experimental-setup,experimental setup,train,end-toend,experimental setup train end-toend,0.6659889221191406
translation,127,5,model,response selection,in,end-to - end multi-turn conversational dialogue setting,response selection in end-to - end multi-turn conversational dialogue setting,0.507715106010437
translation,127,6,model,architecture,applies,context level attention,architecture applies context level attention,0.6043117642402649
translation,127,6,model,architecture,incorporates,additional external knowledge,architecture incorporates additional external knowledge,0.6896694898605347
translation,127,6,model,additional external knowledge,provided by,descriptions,additional external knowledge provided by descriptions,0.6839935779571533
translation,127,6,model,descriptions,of,domain-specific words,descriptions of domain-specific words,0.5370907187461853
translation,127,6,model,model,has,architecture,model has architecture,0.5575731992721558
translation,127,7,model,bi-directional gated recurrent unit ( gru ),for encoding,context and responses,bi-directional gated recurrent unit ( gru ) for encoding context and responses,0.7490115761756897
translation,127,7,model,bi-directional gated recurrent unit ( gru ),attend over,context words,bi-directional gated recurrent unit ( gru ) attend over context words,0.6926915645599365
translation,127,7,model,context words,given,latent response representation,context words given latent response representation,0.6502033472061157
translation,127,7,model,model,uses,bi-directional gated recurrent unit ( gru ),model uses bi-directional gated recurrent unit ( gru ),0.6165159940719604
translation,127,8,model,external domain specific information,using,another gru,external domain specific information using another gru,0.6667690873146057
translation,127,8,model,another gru,for encoding,domain keyword descriptions,another gru for encoding domain keyword descriptions,0.7853389978408813
translation,127,8,model,model,incorporates,external domain specific information,model incorporates external domain specific information,0.7049470543861389
translation,127,28,model,novel neural network architecture,for,multi-turn response-selection,novel neural network architecture for multi-turn response-selection,0.6105759143829346
translation,127,28,model,model,propose,novel neural network architecture,model propose novel neural network architecture,0.6913375854492188
translation,127,29,model,neural network paradigm,attend over,important words,neural network paradigm attend over important words,0.6532713174819946
translation,127,29,model,important words,in,context utterance,important words in context utterance,0.503604531288147
translation,127,29,model,important words,given,response encoding,important words given response encoding,0.7045674324035645
translation,127,29,model,approach,to incorporate,additional domain knowledge,approach to incorporate additional domain knowledge,0.7012028694152832
translation,127,29,model,additional domain knowledge,into,neural network,additional domain knowledge into neural network,0.5882754325866699
translation,127,29,model,additional domain knowledge,by encoding,description,additional domain knowledge by encoding description,0.7650935649871826
translation,127,29,model,additional domain knowledge,using,bilinear operation,additional domain knowledge using bilinear operation,0.6829552054405212
translation,127,29,model,neural network,by encoding,description,neural network by encoding description,0.822124719619751
translation,127,29,model,description,of,domain specific words,description of domain specific words,0.5737866163253784
translation,127,29,model,domain specific words,with,gru,domain specific words with gru,0.640673041343689
translation,127,29,model,bilinear operation,to merge,resulting domain specific representations,bilinear operation to merge resulting domain specific representations,0.7542797923088074
translation,127,29,model,resulting domain specific representations,with,vanilla word embeddings,resulting domain specific representations with vanilla word embeddings,0.5728772878646851
translation,127,29,model,all other state - of - the - art methods,for,response selection,all other state - of - the - art methods for response selection,0.5664641261100769
translation,127,29,model,response selection,in,multi-turn setting,response selection in multi-turn setting,0.5492514371871948
translation,127,29,model,outperforms,has,all other state - of - the - art methods,outperforms has all other state - of - the - art methods,0.5317885875701904
translation,127,61,model,latter,is,dot product,latter is dot product,0.6365302801132202
translation,127,61,model,dot product,embeddings of,context and response,dot product embeddings of context and response,0.7293779850006104
translation,127,61,model,model,has,latter,model has latter,0.617664635181427
translation,127,176,results,all other models,used as,baselines,all other models used as baselines,0.6247678995132446
translation,127,176,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,127,176,results,outperforms,has,all other models,outperforms has all other models,0.5782700181007385
translation,127,176,results,results,has,our model,results has our model,0.5871725678443909
translation,127,177,results,largest improvement,of,our model,largest improvement of our model,0.5487021803855896
translation,127,177,results,our model,compared to,best of the baselines,our model compared to best of the baselines,0.6376670002937317
translation,127,177,results,best of the baselines,with respect to,r 10 @1,best of the baselines with respect to r 10 @1,0.653769850730896
translation,127,177,results,results,has,largest improvement,results has largest improvement,0.5710678100585938
translation,127,178,results,r 2 @1 and r 10 @5,observed,more modest improvements,r 2 @1 and r 10 @5 observed more modest improvements,0.6782214045524597
translation,127,178,results,more modest improvements,of,0.007 ( 0.8 % ) and 0.005 ( 0.5 % ),more modest improvements of 0.007 ( 0.8 % ) and 0.005 ( 0.5 % ),0.5445353984832764
translation,127,178,results,results,For,r 2 @1 and r 10 @5,results For r 2 @1 and r 10 @5,0.6278310418128967
translation,127,188,results,basic models,observed,around 4 % and 9 % improvement,basic models observed around 4 % and 9 % improvement,0.7169980406761169
translation,127,188,results,i.e. de - gru and de - bigru,observed,around 4 % and 9 % improvement,i.e. de - gru and de - bigru observed around 4 % and 9 % improvement,0.6015641093254089
translation,127,188,results,around 4 % and 9 % improvement,on,r 10 @1,around 4 % and 9 % improvement on r 10 @1,0.657260000705719
translation,127,188,results,around 4 % and 9 % improvement,when incorporating,attention mechanism,around 4 % and 9 % improvement when incorporating attention mechanism,0.7329173684120178
translation,127,188,results,basic models,has,i.e. de - gru and de - bigru,basic models has i.e. de - gru and de - bigru,0.5765723586082458
translation,127,188,results,attention mechanism,has,a-de - gru and a-de - bigru,attention mechanism has a-de - gru and a-de - bigru,0.6236306428909302
translation,127,188,results,results,With,basic models,results With basic models,0.5936782360076904
translation,127,189,results,domain knowledge,incorporated by,simple addition,domain knowledge incorporated by simple addition,0.7076776623725891
translation,127,189,results,simple addition,noticed,0.5 % further improvement,simple addition noticed 0.5 % further improvement,0.6433226466178894
translation,127,190,results,results,are,not as good,results are not as good,0.5653930306434631
translation,127,190,results,not as good,as,proposed weighted addition,not as good as proposed weighted addition,0.5768417119979858
translation,127,190,results,not as good,when using,proposed weighted addition,not as good when using proposed weighted addition,0.7253733277320862
translation,127,190,results,results,are,not as good,results are not as good,0.5653930306434631
translation,127,191,results,our method,of incorporating,domain knowledge,our method of incorporating domain knowledge,0.5843520164489746
translation,127,191,results,domain knowledge,in combination with,embeddings,domain knowledge in combination with embeddings,0.5639657974243164
translation,127,191,results,trained from scratch,with,fasttext,trained from scratch with fasttext,0.6453219652175903
translation,127,191,results,performance,gets,0.3 % better,performance gets 0.3 % better,0.6119336485862732
translation,127,191,results,0.3 % better,when using,pre-,0.3 % better when using pre-,0.7258318662643433
translation,127,191,results,our method,has,performance,our method has performance,0.5292724370956421
translation,127,191,results,domain knowledge,has,performance,domain knowledge has performance,0.5651469230651855
translation,127,191,results,embeddings,has,trained from scratch,embeddings has trained from scratch,0.5455114245414734
translation,127,191,results,),has,performance,) has performance,0.5912755727767944
translation,127,191,results,pre-,has,trained word2vec embeddings,pre- has trained word2vec embeddings,0.5139057040214539
translation,127,191,results,results,using,our method,results using our method,0.6528214812278748
translation,127,192,results,our model ( ak - de - bigru ),achieves,10 % of improvement,our model ( ak - de - bigru ) achieves 10 % of improvement,0.6568140983581543
translation,127,192,results,10 % of improvement,in terms of,r 10 @1 metric,10 % of improvement in terms of r 10 @1 metric,0.731151282787323
translation,127,192,results,de - bigru baseline,has,our model ( ak - de - bigru ),de - bigru baseline has our model ( ak - de - bigru ),0.5950247645378113
translation,127,192,results,results,compared to,de - bigru baseline,results compared to de - bigru baseline,0.6709157228469849
translation,127,194,results,baseline models,using,gru,baseline models using gru,0.6369968056678772
translation,127,194,results,gru,as,encoder,gru as encoder,0.5856178998947144
translation,127,194,results,encoder,is,better,encoder is better,0.6358198523521423
translation,127,194,results,better,than,bigru,better than bigru,0.6989766955375671
translation,127,194,results,better,using,bigru,better using bigru,0.7184519171714783
translation,127,194,results,results,for,baseline models,results for baseline models,0.5982160568237305
translation,128,105,baselines,two baseline models,has,ir baseline,two baseline models has ir baseline,0.5723443627357483
translation,128,105,baselines,baselines,consider,two baseline models,baselines consider two baseline models,0.6943766474723816
translation,128,56,experiments,persona chat,pair,two turkers,persona chat pair two turkers,0.7209819555282593
translation,128,56,experiments,persona chat,assign them,random ( original ) persona,persona chat assign them random ( original ) persona,0.6033652424812317
translation,128,56,experiments,persona chat,ask them,chat,persona chat ask them chat,0.5921276807785034
translation,128,56,experiments,random ( original ) persona,from,pool,random ( original ) persona from pool,0.5681467652320862
translation,128,56,experiments,random ( original ) persona,ask them,chat,random ( original ) persona ask them chat,0.6229454278945923
translation,128,6,model,partner,with,personal topics,partner with personal topics,0.6100081205368042
translation,128,6,model,profile information,about,interlocutors,profile information about interlocutors,0.6413712501525879
translation,128,6,model,model,trained to engage,partner,model trained to engage partner,0.746802806854248
translation,128,5,results,data,train,models,data train models,0.6840492486953735
translation,128,5,results,models,to,condition,models to condition,0.6318651437759399
translation,128,5,results,models,to,information,models to information,0.5981224179267883
translation,128,5,results,condition,on,given profile information,condition on given profile information,0.5860257148742676
translation,128,5,results,information,about,person they are talking to,information about person they are talking to,0.659697413444519
translation,128,5,results,person they are talking to,resulting in,improved dialogues,person they are talking to resulting in improved dialogues,0.6599024534225464
translation,128,5,results,results,collect,data,results collect data,0.5753158926963806
translation,128,26,results,generative or ranking case,conditioning,agent with persona information,generative or ranking case conditioning agent with persona information,0.7196270227432251
translation,128,26,results,agent with persona information,gives,improved prediction,agent with persona information gives improved prediction,0.6639538407325745
translation,128,26,results,improved prediction,of,next dialogue utterance,improved prediction of next dialogue utterance,0.579033613204956
translation,128,26,results,results,show experimentally,generative or ranking case,results show experimentally generative or ranking case,0.6674770712852478
translation,128,26,results,results,in,generative or ranking case,results in generative or ranking case,0.5424221754074097
translation,128,153,results,ir baseline,outperformed by,starspace,ir baseline outperformed by starspace,0.7219766974449158
translation,128,153,results,starspace,due to,learnt similarity metric,starspace due to learnt similarity metric,0.654286801815033
translation,128,153,results,learnt similarity metric,outperformed by,profile memory networks,learnt similarity metric outperformed by profile memory networks,0.7393930554389954
translation,128,153,results,profile memory networks,due to,attention mechanism,profile memory networks due to attention mechanism,0.6421568989753723
translation,128,153,results,attention mechanism,over,profiles,attention mechanism over profiles,0.6921920776367188
translation,128,153,results,ranking models,has,ir baseline,ranking models has ir baseline,0.49846741557121277
translation,128,153,results,results,For,ranking models,results For ranking models,0.6169298887252808
translation,128,154,results,profile memory networks,in,no persona case,profile memory networks in no persona case,0.5294268727302551
translation,128,154,results,outperform,in,no persona case,outperform in no persona case,0.5681949853897095
translation,128,154,results,profile memory networks,in,no persona case,profile memory networks in no persona case,0.5294268727302551
translation,128,154,results,profile memory networks,has,outperform,profile memory networks has outperform,0.6017698645591736
translation,128,154,results,outperform,has,profile memory networks,outperform has profile memory networks,0.573874294757843
translation,128,154,results,results,has,profile memory networks,results has profile memory networks,0.5679023265838623
translation,128,158,results,some gain,for,profile memory networks,some gain for profile memory networks,0.655880331993103
translation,128,158,results,profile memory networks,compared to,none,profile memory networks compared to none,0.6746112704277039
translation,128,158,results,results,see,some gain,results see some gain,0.6541747450828552
translation,129,23,baselines,prior,for,multinomial distributions,prior for multinomial distributions,0.5904971361160278
translation,129,23,baselines,baselines,has,dirichlet distribution,baselines has dirichlet distribution,0.557598888874054
translation,129,59,experimental-setup,word drop rate,set to,0.25,word drop rate set to 0.25,0.7052366733551025
translation,129,59,experimental-setup,dimensionality,of,latent variable z,dimensionality of latent variable z,0.5896315574645996
translation,129,59,experimental-setup,latent variable z,is,3,latent variable z is 3,0.5953395962715149
translation,129,59,experimental-setup,word embedding size,is,200 ?,word embedding size is 200 ?,0.6019461750984192
translation,129,59,experimental-setup,experimental setup,has,word drop rate,experimental setup has word drop rate,0.5261417627334595
translation,129,59,experimental-setup,experimental setup,has,dimensionality,experimental setup has dimensionality,0.4790812134742737
translation,129,59,experimental-setup,experimental setup,has,word embedding size,experimental setup has word embedding size,0.5138497352600098
translation,129,60,experimental-setup,weight parameter ?,of,kldivergence,weight parameter ? of kldivergence,0.582342267036438
translation,129,60,experimental-setup,kldivergence,initialized as,0,kldivergence initialized as 0,0.6565625667572021
translation,129,60,experimental-setup,kldivergence,gradually increased to,1,kldivergence gradually increased to 1,0.6265532970428467
translation,129,60,experimental-setup,1,at,20,1 at 20,0.6885146498680115
translation,129,60,experimental-setup,20,",","000-th/100 , 000 - th training steps","20 , 000-th/100 , 000 - th training steps",0.6521551012992859
translation,129,60,experimental-setup,"000-th/100 , 000 - th training steps",for,movie / ubuntu datasets,"000-th/100 , 000 - th training steps for movie / ubuntu datasets",0.5578486323356628
translation,129,60,experimental-setup,vhred,has,weight parameter ?,vhred has weight parameter ?,0.5920981168746948
translation,129,60,experimental-setup,experimental setup,For,vhred,experimental setup For vhred,0.6200757622718811
translation,129,62,experimental-setup,dataset,truncate,utterances,dataset truncate utterances,0.7703105211257935
translation,129,62,experimental-setup,dataset,split,train / validation / test sets,dataset split train / validation / test sets,0.7476823329925537
translation,129,62,experimental-setup,utterances,longer than,30 words,utterances longer than 30 words,0.7641161680221558
translation,129,62,experimental-setup,train / validation / test sets,by,0.8/0.1/0.1,train / validation / test sets by 0.8/0.1/0.1,0.5857454538345337
translation,129,63,experimental-setup,"adam optimizer ( kingma and ba , 2014 )",with,batch size,"adam optimizer ( kingma and ba , 2014 ) with batch size",0.6135864853858948
translation,129,63,experimental-setup,"adam optimizer ( kingma and ba , 2014 )",with,learning rate,"adam optimizer ( kingma and ba , 2014 ) with learning rate",0.5956289768218994
translation,129,63,experimental-setup,batch size,has,40,batch size has 40,0.6265621185302734
translation,129,63,experimental-setup,learning rate,has,1 ? 10 ?4,learning rate has 1 ? 10 ?4,0.5980836749076843
translation,129,63,experimental-setup,experimental setup,trained by,"adam optimizer ( kingma and ba , 2014 )","experimental setup trained by adam optimizer ( kingma and ba , 2014 )",0.7312822937965393
translation,129,64,experimental-setup,training time,stop,training,training time stop training,0.7262102365493774
translation,129,64,experimental-setup,training,when,loss,training when loss,0.6105409860610962
translation,129,64,experimental-setup,loss,on,validation set,loss on validation set,0.5221169590950012
translation,129,64,experimental-setup,does not decrease,within,5 epochs,does not decrease within 5 epochs,0.678557813167572
translation,129,64,experimental-setup,loss,has,does not decrease,loss has does not decrease,0.6276254653930664
translation,129,64,experimental-setup,validation set,has,does not decrease,validation set has does not decrease,0.6367306709289551
translation,129,64,experimental-setup,experimental setup,At,training time,experimental setup At training time,0.49713820219039917
translation,129,65,experimental-setup,"beam search ( wiseman and rush , 2016 )",with,beam size 5,"beam search ( wiseman and rush , 2016 ) with beam size 5",0.6422403454780579
translation,129,65,experimental-setup,beam size 5,generating,output responses,beam size 5 generating output responses,0.6796395778656006
translation,129,6,model,dirichlet distribution,with,flexible structures,dirichlet distribution with flexible structures,0.6652249097824097
translation,129,6,model,latent variables,in place of,traditional gaussian distribution,latent variables in place of traditional gaussian distribution,0.6646657586097717
translation,129,6,model,model,use,dirichlet distribution,model use dirichlet distribution,0.6643911004066467
translation,129,13,model,encoder rnn,encodes,each utterance,encoder rnn encodes each utterance,0.7160271406173706
translation,129,13,model,each utterance,into,fixed - size real- valued vector,each utterance into fixed - size real- valued vector,0.5905379056930542
translation,129,13,model,fixed - size real- valued vector,through,word embedding,fixed - size real- valued vector through word embedding,0.6474440097808838
translation,129,13,model,model,has,encoder rnn,model has encoder rnn,0.5552027821540833
translation,129,22,model,dirichlet distribution,to characterize,latent variable,dirichlet distribution to characterize latent variable,0.680061399936676
translation,129,22,model,latent variable,in,vhred,latent variable in vhred,0.5675964951515198
translation,129,22,model,latent variable,named,dir-vhred,latent variable named dir-vhred,0.7432910799980164
translation,129,22,model,model,propose to use,dirichlet distribution,model propose to use dirichlet distribution,0.7601955533027649
translation,129,75,results,lower reconstruction loss,indicates that,dirichlet prior,lower reconstruction loss indicates that dirichlet prior,0.6435137391090393
translation,129,75,results,dirichlet prior,better than,gaussian prior,dirichlet prior better than gaussian prior,0.6871759295463562
translation,129,75,results,gaussian prior,for reconstructing,responses,gaussian prior for reconstructing responses,0.6923127770423889
translation,129,75,results,dir-vhred,has,lower reconstruction loss,dir-vhred has lower reconstruction loss,0.5765954256057739
translation,129,75,results,results,has,dir-vhred,results has dir-vhred,0.5830801129341125
translation,129,83,results,dir-vhred,achieves,best performance,dir-vhred achieves best performance,0.7137312889099121
translation,129,83,results,best performance,among,three word-embedding metrics,best performance among three word-embedding metrics,0.5851882696151733
translation,129,83,results,three word-embedding metrics,on,1 - turn and 3 - turn responses,three word-embedding metrics on 1 - turn and 3 - turn responses,0.5544047951698303
translation,129,83,results,1 - turn and 3 - turn responses,of,two datasets,1 - turn and 3 - turn responses of two datasets,0.5882368087768555
translation,129,83,results,results,has,dir-vhred,results has dir-vhred,0.5830801129341125
translation,131,4,baselines,pyopendial,has,"python- based domain- independent , open-source toolkit","pyopendial has python- based domain- independent , open-source toolkit",0.5130120515823364
translation,131,18,model,pyopendial,in,python,pyopendial in python,0.5340598821640015
translation,131,18,model,open-source sds framework,re-implements,opendial,open-source sds framework re-implements opendial,0.6981587409973145
translation,131,18,model,opendial,in,python,opendial in python,0.5561280250549316
translation,131,18,model,monte carlo tree search,for,forward planning,monte carlo tree search for forward planning,0.6081054210662842
translation,131,18,model,pyopendial,has,open-source sds framework,pyopendial has open-source sds framework,0.5676670074462891
translation,131,18,model,model,developed,pyopendial,model developed pyopendial,0.6768801808357239
translation,131,19,model,pyopen-dial,inherits,original architectural design,pyopen-dial inherits original architectural design,0.6551187038421631
translation,131,19,model,pyopen-dial,extends,xml domain specification,pyopen-dial extends xml domain specification,0.6897783875465393
translation,131,19,model,original architectural design,of,opendial,original architectural design of opendial,0.5579253435134888
translation,131,19,model,various deep learning models,directly used for,sds components,various deep learning models directly used for sds components,0.6607414484024048
translation,131,19,model,model,has,pyopen-dial,model has pyopen-dial,0.6058441996574402
translation,131,62,model,number of new modules,including,monte,number of new modules including monte,0.6541797518730164
translation,131,62,model,number of new modules,including,basic speech - to - text and text- to-speech modules,number of new modules including basic speech - to - text and text- to-speech modules,0.664687991142273
translation,131,62,model,basic speech - to - text and text- to-speech modules,using,google speech apis,basic speech - to - text and text- to-speech modules using google speech apis,0.6725162267684937
translation,131,62,model,monte,has,- carlo tree search ( mcts ),monte has - carlo tree search ( mcts ),0.5773370862007141
translation,132,25,ablation-analysis,regression analyses,detecting,user disengagement,regression analyses detecting user disengagement,0.7205372452735901
translation,132,25,ablation-analysis,only user uncertainty,when modeling,performance,only user uncertainty when modeling performance,0.7057977914810181
translation,132,25,ablation-analysis,ablation analysis,show through,regression analyses,ablation analysis show through regression analyses,0.5981082916259766
translation,133,147,ablation-analysis,relatively small effect,on,precision,relatively small effect on precision,0.5067514777183533
translation,133,147,ablation-analysis,more substantial effect,on,recall,more substantial effect on recall,0.5812551379203796
translation,133,147,ablation-analysis,any feature source,has,relatively small effect,any feature source has relatively small effect,0.5191193222999573
translation,133,148,ablation-analysis,confusion detection features,seem to be,least essential,confusion detection features seem to be least essential,0.6305412650108337
translation,133,148,ablation-analysis,comparatively small drop,in,precision and recall values,comparatively small drop in precision and recall values,0.5470921397209167
translation,133,148,ablation-analysis,precision and recall values,when,removed,precision and recall values when removed,0.6640605926513672
translation,133,148,ablation-analysis,ablation analysis,has,confusion detection features,ablation analysis has confusion detection features,0.5533869862556458
translation,133,149,ablation-analysis,dialogue features,results in,greatest drop,dialogue features results in greatest drop,0.6618365049362183
translation,133,149,ablation-analysis,greatest drop,in,recall,greatest drop in recall,0.5563435554504395
translation,133,149,ablation-analysis,ablation analysis,Removing,dialogue features,ablation analysis Removing dialogue features,0.7287330627441406
translation,133,199,ablation-analysis,ablation analysis,has,feature ablation,ablation analysis has feature ablation,0.5267404913902283
translation,133,200,ablation-analysis,most critical feature source,without,similarity feature,most critical feature source without similarity feature,0.6347211599349976
translation,133,200,ablation-analysis,performance,is,consistently worse,performance is consistently worse,0.5934910178184509
translation,133,200,ablation-analysis,consistently worse,than,all other runs,consistently worse than all other runs,0.5411278605461121
translation,133,200,ablation-analysis,similarity feature,has,performance,similarity feature has performance,0.5773037672042847
translation,133,200,ablation-analysis,ablation analysis,has,most critical feature source,ablation analysis has most critical feature source,0.49472182989120483
translation,133,203,ablation-analysis,word form or pattern features,not,cause,word form or pattern features not cause,0.715317964553833
translation,133,203,ablation-analysis,significant change,in,performance,significant change in performance,0.5464552640914917
translation,133,203,ablation-analysis,cause,has,significant change,cause has significant change,0.5580570697784424
translation,133,203,ablation-analysis,ablation analysis,removing,word form or pattern features,ablation analysis removing word form or pattern features,0.7067577838897705
translation,133,141,results,classifier,achieves,above baseline precision,classifier achieves above baseline precision,0.6821780204772949
translation,133,141,results,classifier,producing,recall,classifier producing recall,0.7285653352737427
translation,133,141,results,above baseline precision,of,0.861,above baseline precision of 0.861,0.5478270649909973
translation,133,141,results,recall,of,0.751,recall of 0.751,0.5850986242294312
translation,133,141,results,entire feature set,has,classifier,entire feature set has classifier,0.5785791873931885
translation,133,192,results,entire feature set,results in,consistently above baseline performance,entire feature set results in consistently above baseline performance,0.6005581617355347
translation,133,202,results,contextual and punctuation features,shown to have,significant effect,contextual and punctuation features shown to have significant effect,0.5987924337387085
translation,133,202,results,significant effect,on,overall performance,significant effect on overall performance,0.5247509479522705
translation,133,247,results,rich set of features,from,discourse,rich set of features from discourse,0.5910263657569885
translation,133,247,results,proposed approach,differentiate between,problematic and unproblematic instances,proposed approach differentiate between problematic and unproblematic instances,0.703251302242279
translation,133,247,results,proposed approach,identify,term,proposed approach identify term,0.6632030606269836
translation,133,247,results,term,achieving,significantly above baseline performance,term achieving significantly above baseline performance,0.7093273401260376
translation,133,247,results,user intended to type,with,high precision,user intended to type with high precision,0.6703833937644958
translation,133,247,results,rich set of features,has,proposed approach,rich set of features has proposed approach,0.5855569243431091
translation,133,247,results,discourse,has,proposed approach,discourse has proposed approach,0.5984311103820801
translation,133,247,results,term,has,user intended to type,term has user intended to type,0.5861449241638184
translation,133,247,results,results,exploring,rich set of features,results exploring rich set of features,0.6775700449943542
translation,134,5,model,navigation and touristic questionanswering,in,integrated fashion,navigation and touristic questionanswering in integrated fashion,0.5671071410179138
translation,134,5,model,navigation and touristic questionanswering,using,shared dialogue context,navigation and touristic questionanswering using shared dialogue context,0.6707789897918701
translation,134,5,model,integrated fashion,using,shared dialogue context,integrated fashion using shared dialogue context,0.6863881349563599
translation,134,9,model,mobile dialogue system,called,spacebook,mobile dialogue system called spacebook,0.6728694438934326
translation,134,141,results,no statistically significant difference,between,systems,no statistically significant difference between systems,0.6230578422546387
translation,134,141,results,systems,in terms of,perceived task success,systems in terms of perceived task success,0.6687576174736023
translation,134,141,results,better task completion rate,in,"tasks 1 - 3 , 5 and 6","better task completion rate in tasks 1 - 3 , 5 and 6",0.5177542567253113
translation,134,141,results,baseline system,has,better task completion rate,baseline system has better task completion rate,0.5449118614196777
translation,134,141,results,results,show,no statistically significant difference,results show no statistically significant difference,0.5637724995613098
translation,134,147,results,our system,not performing,significantly better,our system not performing significantly better,0.6162711977958679
translation,134,147,results,significantly better,than,baseline system,significantly better than baseline system,0.5597087144851685
translation,134,147,results,more interesting,to,interact with,more interesting to interact with,0.6351600289344788
translation,134,147,results,interact with,than,baseline,interact with than baseline,0.6494271755218506
translation,134,147,results,baseline system,has,- sq10 except sq7 ),baseline system has - sq10 except sq7 ),0.629040539264679
translation,134,147,results,results,show,our system,results show our system,0.6763114333152771
translation,135,11,ablation-analysis,new findings,development of,bayesian game-theoretic model,new findings development of bayesian game-theoretic model,0.6846954822540283
translation,135,11,ablation-analysis,bayesian game-theoretic model,for,social talk,bayesian game-theoretic model for social talk,0.5700772404670715
translation,135,11,ablation-analysis,ablation analysis,development of,bayesian game-theoretic model,ablation analysis development of bayesian game-theoretic model,0.7020843625068665
translation,135,11,ablation-analysis,ablation analysis,has,new findings,ablation analysis has new findings,0.5361438989639282
translation,135,9,model,classification,for,ntus,classification for ntus,0.6630067229270935
translation,135,9,model,ntus,actually convey,certain non-topical coherence,ntus actually convey certain non-topical coherence,0.8077948093414307
translation,135,9,model,certain non-topical coherence,in,social talk,certain non-topical coherence in social talk,0.5102280378341675
translation,135,9,model,model,construct,classification,model construct classification,0.7844485640525818
translation,136,33,baselines,hmms,to model,struc- tural dependencies,hmms to model struc- tural dependencies,0.6992793083190918
translation,136,33,baselines,struc- tural dependencies,between,utterances,struc- tural dependencies between utterances,0.6093447804450989
translation,136,84,baselines,supervised approaches,use,bagof-words ( bow ) representation,supervised approaches use bagof-words ( bow ) representation,0.5649697780609131
translation,136,84,baselines,bagof-words ( bow ) representation,of,utterance,bagof-words ( bow ) representation of utterance,0.5506458878517151
translation,136,84,baselines,separate binary feature,representing,occurrence,separate binary feature representing occurrence,0.7381618022918701
translation,136,84,baselines,baselines,For,supervised approaches,baselines For supervised approaches,0.5434820055961609
translation,136,8,experimental-setup,gibbs sampling,for,posterior inference,gibbs sampling for posterior inference,0.5680449604988098
translation,136,8,experimental-setup,experimental setup,use,gibbs sampling,experimental setup use gibbs sampling,0.5957589149475098
translation,136,82,experimental-setup,global vectors ( glove ),for,word vector representation,global vectors ( glove ) for word vector representation,0.5850459337234497
translation,136,82,experimental-setup,experimental setup,use,global vectors ( glove ),experimental setup use global vectors ( glove ),0.6051342487335205
translation,136,83,experimental-setup,pre-trained word vectors,on,6b tokens,pre-trained word vectors on 6b tokens,0.49643775820732117
translation,136,83,experimental-setup,pre-trained word vectors,leads to,very good representation,pre-trained word vectors leads to very good representation,0.6117773056030273
translation,136,83,experimental-setup,6b tokens,from,wikipedia 2014 and gigaword,6b tokens from wikipedia 2014 and gigaword,0.5911834239959717
translation,136,83,experimental-setup,very good representation,of,short sentences,very good representation of short sentences,0.5268418192863464
translation,136,83,experimental-setup,experimental setup,use,pre-trained word vectors,experimental setup use pre-trained word vectors,0.5400256514549255
translation,136,90,experimental-setup,1000 iterations,of,gibbs sampling,1000 iterations of gibbs sampling,0.5593313574790955
translation,136,90,experimental-setup,experimental setup,perform,1000 iterations,experimental setup perform 1000 iterations,0.5609213709831238
translation,136,91,experimental-setup,number of clusters ( mixture size ),is,k = 42,number of clusters ( mixture size ) is k = 42,0.5646360516548157
translation,136,91,experimental-setup,experimental setup,has,number of clusters ( mixture size ),experimental setup has number of clusters ( mixture size ),0.5339794158935547
translation,136,92,experimental-setup,dimension,of,glove vectors,dimension of glove vectors,0.6241528987884521
translation,136,92,experimental-setup,glove vectors,ranges between,m = 50 and m = 300,glove vectors ranges between m = 50 and m = 300,0.7435552477836609
translation,136,92,experimental-setup,experimental setup,has,dimension,experimental setup has dimension,0.4825572967529297
translation,136,7,model,dialogue,as,hidden markov model,dialogue as hidden markov model,0.5131651759147644
translation,136,7,model,hidden markov model,with,emission probabilities,hidden markov model with emission probabilities,0.6577717065811157
translation,136,7,model,emission probabilities,estimated by,gaussian mixtures,emission probabilities estimated by gaussian mixtures,0.6425753235816956
translation,136,7,model,model,model,dialogue,model model dialogue,0.8490763902664185
translation,136,32,model,approach,to,unsupervised da induction,approach to unsupervised da induction,0.5552564859390259
translation,136,32,model,model,introduce,approach,model introduce approach,0.6844506859779358
translation,136,34,model,main novelty,use of,multivariate gaussian distribution,main novelty use of multivariate gaussian distribution,0.5619304180145264
translation,136,34,model,multivariate gaussian distribution,for,emissions ( utterances ),multivariate gaussian distribution for emissions ( utterances ),0.6675128936767578
translation,136,34,model,multivariate gaussian distribution,in,hmm,multivariate gaussian distribution in hmm,0.5773091912269592
translation,136,34,model,emissions ( utterances ),in,hmm,emissions ( utterances ) in hmm,0.5505192279815674
translation,136,34,model,model,has,main novelty,model has main novelty,0.5228181481361389
translation,136,81,model,linear combination,of,word vectors,linear combination of word vectors,0.5974432826042175
translation,136,81,model,weights,represented by,inversedocument -frequency ( idf ) values of words,weights represented by inversedocument -frequency ( idf ) values of words,0.7252365350723267
translation,136,81,model,model,use,linear combination,model use linear combination,0.6735134124755859
translation,136,9,results,results,on,standard switchboard - damsl corpus,results on standard switchboard - damsl corpus,0.47229334712028503
translation,136,112,results,best supervised baseline,is,approximately 1 % worse,best supervised baseline is approximately 1 % worse,0.516538143157959
translation,136,112,results,results,has,best supervised baseline,results has best supervised baseline,0.5689745545387268
translation,136,113,results,context information,proved to be,very useful,context information proved to be very useful,0.586239218711853
translation,136,113,results,all experiments,has,context information,all experiments has context information,0.5624363422393799
translation,136,113,results,results,In,all experiments,results In all experiments,0.5318937301635742
translation,136,114,results,best result,among,unsupervised models,best result among unsupervised models,0.6200463771820068
translation,136,114,results,unsupervised models,achieved with,300 - dimensional glove,unsupervised models achieved with 300 - dimensional glove,0.707786500453949
translation,136,114,results,300 - dimensional glove,has,f1 score 65.7 %,300 - dimensional glove has f1 score 65.7 %,0.5643157362937927
translation,136,114,results,results,has,best result,results has best result,0.5702329277992249
translation,136,115,results,outperform,achieving,v-measure 34.7 %,outperform achieving v-measure 34.7 %,0.6483350992202759
translation,136,115,results,outperform,achieving,mixedmembership hmm ( m4 ),outperform achieving mixedmembership hmm ( m4 ),0.691993236541748
translation,136,115,results,outperform,achieving,v-measure 18.0 %,outperform achieving v-measure 18.0 %,0.660086989402771
translation,136,115,results,outperform,achieving,v-measure 18.0 %,outperform achieving v-measure 18.0 %,0.660086989402771
translation,136,115,results,block hmm ( bhmm ),achieving,f1 score 41.1 %,block hmm ( bhmm ) achieving f1 score 41.1 %,0.5965234637260437
translation,136,115,results,block hmm ( bhmm ),achieving,mixedmembership hmm ( m4 ),block hmm ( bhmm ) achieving mixedmembership hmm ( m4 ),0.6454140543937683
translation,136,115,results,block hmm ( bhmm ),achieving,f1 score 45.1 %,block hmm ( bhmm ) achieving f1 score 45.1 %,0.5944545865058899
translation,136,115,results,mixedmembership hmm ( m4 ),achieving,f1 score 45.1 %,mixedmembership hmm ( m4 ) achieving f1 score 45.1 %,0.5954074263572693
translation,136,115,results,mixedmembership hmm ( m4 ),achieving,v-measure 18.0 %,mixedmembership hmm ( m4 ) achieving v-measure 18.0 %,0.6075594425201416
translation,136,115,results,mixedmembership hmm ( m4 ),has,"paul , 2012 )","mixedmembership hmm ( m4 ) has paul , 2012 )",0.5786245465278625
translation,136,115,results,results,has,outperform,results has outperform,0.642206609249115
translation,136,116,results,gmms,is,very promising direction,gmms is very promising direction,0.5636459589004517
translation,136,116,results,very promising direction,for,unsupervised da induction task,very promising direction for unsupervised da induction task,0.5783977508544922
translation,137,49,baselines,two families of systems,modeling,coarse dialogue acts and words,two families of systems modeling coarse dialogue acts and words,0.6938709616661072
translation,137,49,baselines,two families of systems,optimized by,supervised learning,two families of systems optimized by supervised learning,0.6717850565910339
translation,137,49,baselines,two families of systems,optimized by,reinforcement learning,two families of systems optimized by reinforcement learning,0.6782193183898926
translation,137,49,baselines,two families of systems,optimized by,domain knowledge,two families of systems optimized by domain knowledge,0.6885943412780762
translation,137,49,baselines,coarse dialogue acts and words,optimized by,reinforcement learning,coarse dialogue acts and words optimized by reinforcement learning,0.7070585489273071
translation,137,156,baselines,end-to - end neural models,that directly map,input dialogue context,end-to - end neural models that directly map input dialogue context,0.6786622405052185
translation,137,156,baselines,our modular models,that use,coarse dialogue,our modular models that use coarse dialogue,0.7567639350891113
translation,137,156,baselines,coarse dialogue,acts as,intermediate representation,coarse dialogue acts as intermediate representation,0.6775340437889099
translation,137,158,baselines,sequence - to-sequence model,with attention over,previous utterances and the scenario,sequence - to-sequence model with attention over previous utterances and the scenario,0.7231325507164001
translation,137,158,baselines,sl ( act ),with,retrieval - based generator,sl ( act ) with retrieval - based generator,0.6519880890846252
translation,137,158,baselines,model,with,rule- based parser,model with rule- based parser,0.6239106059074402
translation,137,158,baselines,model,with,learned neural dialogue manager,model with learned neural dialogue manager,0.649951696395874
translation,137,158,baselines,model,with,retrieval - based generator,model with retrieval - based generator,0.6396848559379578
translation,137,158,baselines,sl ( word ),has,sequence - to-sequence model,sl ( word ) has sequence - to-sequence model,0.5687479376792908
translation,137,158,baselines,sl ( act ),has,model,sl ( act ) has model,0.6094250679016113
translation,137,158,baselines,rule- based parser,has,learned neural dialogue manager,rule- based parser has learned neural dialogue manager,0.5710684061050415
translation,137,158,baselines,baselines,has,sl ( word ),baselines has sl ( word ),0.5846836566925049
translation,137,52,experiments,reinforcement learning,on,coarse dialogue acts,reinforcement learning on coarse dialogue acts,0.5617712140083313
translation,137,52,experiments,reinforcement learning,avoids,degenerate solutions,reinforcement learning avoids degenerate solutions,0.5965859889984131
translation,137,52,experiments,coarse dialogue acts,avoids,degenerate solutions,coarse dialogue acts avoids degenerate solutions,0.6470115184783936
translation,137,173,experiments,sl ( word ),use,sequence - to-sequence model,sl ( word ) use sequence - to-sequence model,0.6268367767333984
translation,137,173,experiments,sequence - to-sequence model,with attention over,3,sequence - to-sequence model with attention over 3,0.7805699706077576
translation,137,173,experiments,sequence - to-sequence model,with attention over,previous utterances,sequence - to-sequence model with attention over previous utterances,0.7155151963233948
translation,137,173,experiments,sequence - to-sequence model,with attention over,negotiation scenario,sequence - to-sequence model with attention over negotiation scenario,0.7336435914039612
translation,137,173,experiments,negotiation scenario,embedded as,continuous bag-of- words,negotiation scenario embedded as continuous bag-of- words,0.70073401927948
translation,137,173,experiments,3,has,previous utterances,3 has previous utterances,0.5658743977546692
translation,137,157,hyperparameters,word - based model and the act-based model,with,supervised learning ( sl ),word - based model and the act-based model with supervised learning ( sl ),0.6412358283996582
translation,137,157,hyperparameters,hyperparameters,training,word - based model and the act-based model,hyperparameters training word - based model and the act-based model,0.6849526166915894
translation,137,174,hyperparameters,sl ( word ) and sl ( act ),use,300 dimensional word vectors,sl ( word ) and sl ( act ) use 300 dimensional word vectors,0.6142331957817078
translation,137,174,hyperparameters,sl ( word ) and sl ( act ),use,two -layer lstm,sl ( word ) and sl ( act ) use two -layer lstm,0.5868160128593445
translation,137,174,hyperparameters,300 dimensional word vectors,initialized by,pretrained glove word vectors,300 dimensional word vectors initialized by pretrained glove word vectors,0.6769376397132874
translation,137,174,hyperparameters,two -layer lstm,with,300 hidden units,two -layer lstm with 300 hidden units,0.598874032497406
translation,137,174,hyperparameters,300 hidden units,for,encoder and the decoder,300 hidden units for encoder and the decoder,0.6282962560653687
translation,137,174,hyperparameters,300 hidden units,both the,encoder and the decoder,300 hidden units both the encoder and the decoder,0.6279265880584717
translation,137,174,hyperparameters,hyperparameters,For,sl ( word ) and sl ( act ),hyperparameters For sl ( word ) and sl ( act ),0.6396564245223999
translation,137,175,hyperparameters,parameters,initialized by,sampling,parameters initialized by sampling,0.6455408334732056
translation,137,175,hyperparameters,sampling,from,uniform distribution,sampling from uniform distribution,0.6390931606292725
translation,137,175,hyperparameters,uniform distribution,between,- 0.1 and 0.1,uniform distribution between - 0.1 and 0.1,0.6536284685134888
translation,137,175,hyperparameters,hyperparameters,has,parameters,hyperparameters has parameters,0.4783959984779358
translation,137,176,hyperparameters,optimization,use,"adagrad ( duchi et al. , 2010 )","optimization use adagrad ( duchi et al. , 2010 )",0.5421025156974792
translation,137,176,hyperparameters,"adagrad ( duchi et al. , 2010 )",with,learning rate,"adagrad ( duchi et al. , 2010 ) with learning rate",0.5384834408760071
translation,137,176,hyperparameters,"adagrad ( duchi et al. , 2010 )",with,mini-batch size,"adagrad ( duchi et al. , 2010 ) with mini-batch size",0.5463727116584778
translation,137,176,hyperparameters,learning rate,of,0.01,learning rate of 0.01,0.6152973175048828
translation,137,176,hyperparameters,mini-batch size,of,128,mini-batch size of 128,0.6346163749694824
translation,137,176,hyperparameters,hyperparameters,For,optimization,hyperparameters For optimization,0.5834490656852722
translation,137,177,hyperparameters,model,for,20 epochs,model for 20 epochs,0.6305356025695801
translation,137,177,hyperparameters,hyperparameters,train,model,hyperparameters train model,0.7064553499221802
translation,137,185,hyperparameters,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,137,185,hyperparameters,hyperparameters,train for,5000 episodes ( dialogues ),hyperparameters train for 5000 episodes ( dialogues ),0.6789538264274597
translation,137,8,model,modular approach,based on,coarse dialogue acts,modular approach based on coarse dialogue acts,0.6802675127983093
translation,137,8,model,model,propose,modular approach,model propose modular approach,0.7133122086524963
translation,137,9,results,strategy,using,supervised learning,strategy using supervised learning,0.7134717106819153
translation,137,9,results,strategy,using,reinforcement learning,strategy using reinforcement learning,0.6518973112106323
translation,137,9,results,strategy,using,domain-specific knowledge,strategy using domain-specific knowledge,0.6214224100112915
translation,137,9,results,strategy,without,degeneracy,strategy without degeneracy,0.7526870965957642
translation,137,9,results,domain-specific knowledge,without,degeneracy,domain-specific knowledge without degeneracy,0.7402876615524292
translation,137,9,results,our retrieval - based generation,maintain,context-awareness,our retrieval - based generation maintain context-awareness,0.6628047823905945
translation,137,9,results,our retrieval - based generation,produce,diverse utterances,our retrieval - based generation produce diverse utterances,0.693454384803772
translation,137,53,results,our modular model,maintains,reasonable human-like behavior,our modular model maintains reasonable human-like behavior,0.6845438480377197
translation,137,53,results,our modular model,optimizes,objective,our modular model optimizes objective,0.7811213135719299
translation,137,53,results,reasonable human-like behavior,optimizes,objective,reasonable human-like behavior optimizes objective,0.6336235404014587
translation,137,53,results,results,has,our modular model,results has our modular model,0.5715824365615845
translation,137,54,results,models,trained over,coarse dialogue acts,models trained over coarse dialogue acts,0.7637034058570862
translation,137,54,results,models,trained over,words,models trained over words,0.7587707042694092
translation,137,54,results,coarse dialogue acts,are,stronger negotiators,coarse dialogue acts are stronger negotiators,0.5693977475166321
translation,137,54,results,coarse dialogue acts,produce,more diverse utterances,coarse dialogue acts produce more diverse utterances,0.6944743394851685
translation,137,54,results,more diverse utterances,than,models,more diverse utterances than models,0.5754308104515076
translation,137,54,results,models,trained over,words,models trained over words,0.7587707042694092
translation,137,54,results,results,find that,models,results find that models,0.6197972297668457
translation,137,182,results,all models,are,human-like,all models are human-like,0.5814350247383118
translation,137,182,results,act - based models,better matches,human statistics,act - based models better matches human statistics,0.7817910313606262
translation,137,182,results,act - based models,with,rl,act - based models with rl,0.6868250370025635
translation,137,182,results,act - based models,optimize,reward,act - based models optimize reward,0.7647750973701477
translation,137,182,results,human statistics,across,all metrics,human statistics across all metrics,0.6692389249801636
translation,137,182,results,word - based models,becomes,degenerate,word - based models becomes degenerate,0.6002556681632996
translation,137,182,results,act-based models,optimize,reward,act-based models optimize reward,0.7647750973701477
translation,137,182,results,reward,while maintaining,human-likeness,reward while maintaining human-likeness,0.6885469555854797
translation,137,182,results,sl,has,all models,sl has all models,0.6299746632575989
translation,137,182,results,rl,has,word - based models,rl has word - based models,0.5750852227210999
translation,137,202,results,sl ( word ) and sl ( act ),achieved,similar scores,sl ( word ) and sl ( act ) achieved similar scores,0.7221404314041138
translation,137,202,results,similar scores,on,humanlikeness,similar scores on humanlikeness,0.496087908744812
translation,137,202,results,results,has,sl ( word ) and sl ( act ),results has sl ( word ) and sl ( act ),0.537564218044281
translation,138,8,model,unsupervised dialogue representation pre-training,on,large source of goal-oriented dialogues,unsupervised dialogue representation pre-training on large source of goal-oriented dialogues,0.4997035562992096
translation,138,8,model,large source of goal-oriented dialogues,in,multiple domains,large source of goal-oriented dialogues in multiple domains,0.4983340799808502
translation,138,8,model,model,perform,unsupervised dialogue representation pre-training,model perform unsupervised dialogue representation pre-training,0.5622825622558594
translation,138,20,model,generative goal-oriented dialogue model,designed for,fewshot learning,generative goal-oriented dialogue model designed for fewshot learning,0.6657270789146423
translation,138,20,model,dialogue knowledge transfer network ( or diktnet ),has,generative goal-oriented dialogue model,dialogue knowledge transfer network ( or diktnet ) has generative goal-oriented dialogue model,0.5572108626365662
translation,138,20,model,model,present,dialogue knowledge transfer network ( or diktnet ),model present dialogue knowledge transfer network ( or diktnet ),0.6468236446380615
translation,139,53,experimental-setup,f ?,using,standard transformer architecture,f ? using standard transformer architecture,0.6959934234619141
translation,139,53,experimental-setup,f ?,with,pre-trained glove embedding,f ? with pre-trained glove embedding,0.6414530277252197
translation,139,53,experimental-setup,standard transformer architecture,with,pre-trained glove embedding,standard transformer architecture with pre-trained glove embedding,0.6329669952392578
translation,139,53,experimental-setup,"adam ( kingma and ba , 2014 ) optimizer",with,warm - up learning rate strategy,"adam ( kingma and ba , 2014 ) optimizer with warm - up learning rate strategy",0.6135067939758301
translation,139,53,experimental-setup,"adam ( kingma and ba , 2014 ) optimizer",with,batch size,"adam ( kingma and ba , 2014 ) optimizer with batch size",0.6164420247077942
translation,139,53,experimental-setup,batch size,of,32,batch size of 32,0.6741614937782288
translation,139,53,experimental-setup,experimental setup,implemented,f ?,experimental setup implemented f ?,0.6454996466636658
translation,139,54,experimental-setup,meta-training,used,sgd,meta-training used sgd,0.6177248954772949
translation,139,54,experimental-setup,meta-training,used,adam,meta-training used adam,0.5806838274002075
translation,139,54,experimental-setup,meta-training,used,batch size,meta-training used batch size,0.5984797477722168
translation,139,54,experimental-setup,sgd,for,inner loop,sgd for inner loop,0.6325758099555969
translation,139,54,experimental-setup,adam,for,outer loop,adam for outer loop,0.6570020914077759
translation,139,54,experimental-setup,adam,with,learning rate ? = 0.01 and ? = 0.0003,adam with learning rate ? = 0.01 and ? = 0.0003,0.6480095982551575
translation,139,54,experimental-setup,outer loop,with,learning rate ? = 0.01 and ? = 0.0003,outer loop with learning rate ? = 0.01 and ? = 0.0003,0.6330766677856445
translation,139,54,experimental-setup,batch size,of,16,batch size of 16,0.6842944622039795
translation,139,54,experimental-setup,experimental setup,in,meta-training,experimental setup in meta-training,0.5180489420890808
translation,139,55,experimental-setup,beam search,with,beam size 5,beam search with beam size 5,0.7065784335136414
translation,139,55,experimental-setup,experimental setup,used,beam search,experimental setup used beam search,0.5615512132644653
translation,139,6,model,personalized dialogue learning,without using,any persona descriptions,personalized dialogue learning without using any persona descriptions,0.6982150077819824
translation,139,6,model,model,extend,model-agnostic meta-learning ( maml ),model extend model-agnostic meta-learning ( maml ),0.7177853584289551
translation,139,7,model,our model,learns to,quickly adapt,our model learns to quickly adapt,0.7687102556228638
translation,139,7,model,quickly adapt,to,new personas,quickly adapt to new personas,0.6045845746994019
translation,139,7,model,quickly adapt,by leveraging,few dialogue samples,quickly adapt by leveraging few dialogue samples,0.7262158989906311
translation,139,7,model,new personas,by leveraging,few dialogue samples,new personas by leveraging few dialogue samples,0.7220700979232788
translation,139,7,model,few dialogue samples,collected from,same user,few dialogue samples collected from same user,0.6900294423103333
translation,139,7,model,model,learns to,quickly adapt,model learns to quickly adapt,0.7331937551498413
translation,139,7,model,model,has,our model,model has our model,0.5983551740646362
translation,139,40,results,paml vs dialogue + persona,shows,our approach,paml vs dialogue + persona shows our approach,0.6331932544708252
translation,139,40,results,our approach,achieve,good consistency,our approach achieve good consistency,0.5869640111923218
translation,139,40,results,good consistency,by using,few dialogues,good consistency by using few dialogues,0.5721616744995117
translation,139,40,results,good consistency,shows,effectiveness,good consistency shows effectiveness,0.6116559505462646
translation,139,40,results,few dialogues,conditioning on,persona description,few dialogues conditioning on persona description,0.7069476246833801
translation,139,40,results,effectiveness,of,meta-learning,effectiveness of meta-learning,0.5801461338996887
translation,139,40,results,meta-learning,in,personalizing,meta-learning in personalizing,0.5614514350891113
translation,139,40,results,automatic and human evaluation,has,paml vs dialogue + persona,automatic and human evaluation has paml vs dialogue + persona,0.5900623798370361
translation,139,71,results,paml,achieve,consistently better results,paml achieve consistently better results,0.650352418422699
translation,139,71,results,consistently better results,in term of,dialogue consistency,consistently better results in term of dialogue consistency,0.6395393013954163
translation,139,71,results,dialogue consistency,in,automatic and human evaluation,dialogue consistency in automatic and human evaluation,0.501457929611206
translation,139,71,results,results,has,paml,results has paml,0.5535341501235962
translation,139,81,results,paml,achieve,high consistency score,paml achieve high consistency score,0.6419268846511841
translation,139,81,results,high consistency score,by using,3 dialogues,high consistency score by using 3 dialogues,0.618211567401886
translation,139,81,results,3 dialogues,better than,persona + dialogue,3 dialogues better than persona + dialogue,0.6086264252662659
translation,139,81,results,results,has,paml,results has paml,0.5535341501235962
translation,140,6,model,dm,based on,sentence clusters,dm based on sentence clusters,0.6554486751556396
translation,140,6,model,sentence clusters,which have,more powerful semantic representation ability,sentence clusters which have more powerful semantic representation ability,0.5905513763427734
translation,140,6,model,more powerful semantic representation ability,than,das,more powerful semantic representation ability than das,0.5570801496505737
translation,140,6,model,model,model,dm,model model dm,0.8581169247627258
translation,140,7,model,clustered,based on,internal information,clustered based on internal information,0.7325127124786377
translation,140,7,model,clustered,based on,external information,clustered based on external information,0.7085107564926147
translation,140,7,model,clustered,based on,external information,clustered based on external information,0.7085107564926147
translation,140,7,model,internal information,such as,words and sentence structures,internal information such as words and sentence structures,0.6581680774688721
translation,140,7,model,internal information,such as,context,internal information such as context,0.6369447708129883
translation,140,7,model,internal information,via,recurrent neural networks,internal information via recurrent neural networks,0.6545891165733337
translation,140,7,model,external information,such as,context,external information such as context,0.6242011189460754
translation,140,7,model,external information,via,recurrent neural networks,external information via recurrent neural networks,0.6457987427711487
translation,140,7,model,context,in,dialogue,context in dialogue,0.5441809892654419
translation,140,7,model,model,has,sentences,model has sentences,0.609123945236206
translation,141,83,ablation-analysis,learning,of,word-level dependency,learning of word-level dependency,0.49941757321357727
translation,141,83,ablation-analysis,utterance - level dependency,has,benefits,utterance - level dependency has benefits,0.5492205023765564
translation,141,83,ablation-analysis,benefits,has,learning,benefits has learning,0.5983625650405884
translation,141,83,ablation-analysis,ablation analysis,find that,utterance - level dependency,ablation analysis find that utterance - level dependency,0.5536984205245972
translation,141,97,ablation-analysis,coherence,decreased slightly in,seq2seq model,coherence decreased slightly in seq2seq model,0.686124324798584
translation,141,97,ablation-analysis,seq2seq model,increased significantly in,aem model,seq2seq model increased significantly in aem model,0.67926424741745
translation,141,97,ablation-analysis,attention mechanism,has,coherence,attention mechanism has coherence,0.5298116207122803
translation,141,97,ablation-analysis,ablation analysis,with,attention mechanism,ablation analysis with attention mechanism,0.5652862191200256
translation,141,71,hyperparameters,dialogue generation,set,maximum length,dialogue generation set maximum length,0.689606249332428
translation,141,71,hyperparameters,maximum length,to,15 words,maximum length to 15 words,0.546040952205658
translation,141,71,hyperparameters,15 words,for,each generated sentence,15 words for each generated sentence,0.6068513989448547
translation,141,71,hyperparameters,hyperparameters,For,dialogue generation,hyperparameters For dialogue generation,0.5506888031959534
translation,141,71,hyperparameters,hyperparameters,set,maximum length,hyperparameters set maximum length,0.6376456022262573
translation,141,72,hyperparameters,embedding size,has,64,embedding size has 64,0.6215901970863342
translation,141,72,hyperparameters,vocabulary size,has,40 k,vocabulary size has 40 k,0.5905035734176636
translation,141,73,hyperparameters,parameters,updated by,"adam algorithm ( kingma and ba , 2014 )","parameters updated by adam algorithm ( kingma and ba , 2014 )",0.6550723910331726
translation,141,73,hyperparameters,parameters,initialized by,sampling,parameters initialized by sampling,0.6455408334732056
translation,141,73,hyperparameters,sampling,from,"uniform distribution ( [ ?0.1 , 0.1 ] )","sampling from uniform distribution ( [ ?0.1 , 0.1 ] )",0.5873565077781677
translation,141,73,hyperparameters,hyperparameters,has,parameters,hyperparameters has parameters,0.4783959984779358
translation,141,74,hyperparameters,initial learning rate,is,0.002,initial learning rate is 0.002,0.5470441579818726
translation,141,74,hyperparameters,model,trained in,minibatches,model trained in minibatches,0.7747012972831726
translation,141,74,hyperparameters,minibatches,with,batch size,minibatches with batch size,0.632408857345581
translation,141,74,hyperparameters,batch size,of,256,batch size of 256,0.6323750615119934
translation,141,74,hyperparameters,hyperparameters,has,initial learning rate,hyperparameters has initial learning rate,0.4469831883907318
translation,141,74,hyperparameters,hyperparameters,has,model,hyperparameters has model,0.5282720923423767
translation,141,6,model,model,propose,auto-encoder matching ( aem ) model,model propose auto-encoder matching ( aem ) model,0.6454810500144958
translation,141,7,model,model,contains,two auto-encoders,model contains two auto-encoders,0.6449912190437317
translation,141,7,model,model,contains,one mapping module,model contains one mapping module,0.6635677814483643
translation,141,8,model,auto-encoders,learn,semantic representations,auto-encoders learn semantic representations,0.6685163974761963
translation,141,8,model,semantic representations,of,inputs and responses,semantic representations of inputs and responses,0.5600700974464417
translation,141,8,model,model,has,auto-encoders,model has auto-encoders,0.6028405427932739
translation,141,23,model,novel auto-encoder matching model,to learn,utterance - level dependency,novel auto-encoder matching model to learn utterance - level dependency,0.5404266715049744
translation,141,23,model,model,propose,novel auto-encoder matching model,model propose novel auto-encoder matching model,0.6038435101509094
translation,141,24,model,two auto-encoders,to learn,semantic representations,two auto-encoders to learn semantic representations,0.6144312024116516
translation,141,24,model,semantic representations,of,inputs and responses,semantic representations of inputs and responses,0.5600700974464417
translation,141,24,model,inputs and responses,in,unsupervised style,inputs and responses in unsupervised style,0.5100354552268982
translation,141,24,model,model,use,two auto-encoders,model use two auto-encoders,0.6348639130592346
translation,141,25,model,mapping module,taught to learn,utterance - level dependency,mapping module taught to learn utterance - level dependency,0.5994815230369568
translation,141,25,model,utterance - level representations,has,mapping module,utterance - level representations has mapping module,0.5490532517433167
translation,141,25,model,model,given,utterance - level representations,model given utterance - level representations,0.6342323422431946
translation,141,81,results,proposed aem model,has,significantly outperforms,proposed aem model has significantly outperforms,0.5953595638275146
translation,141,81,results,significantly outperforms,has,seq2seq model,significantly outperforms has seq2seq model,0.5877685546875
translation,141,81,results,results,has,proposed aem model,results has proposed aem model,0.5797286629676819
translation,141,84,results,improvement,from,aem model,improvement from aem model,0.5675355792045593
translation,141,84,results,aem model,to,aem + attention model,aem model to aem + attention model,0.5491606593132019
translation,141,84,results,aem + attention model,is,0.68 bleu - 4 point,aem + attention model is 0.68 bleu - 4 point,0.5229191780090332
translation,141,84,results,results,has,improvement,results has improvement,0.6248279809951782
translation,141,85,results,much more obvious,than,improvement,much more obvious than improvement,0.5905041694641113
translation,141,85,results,improvement,from,seq2seq model,improvement from seq2seq model,0.5740936994552612
translation,141,85,results,improvement,is,0.29 bleu - 4 point,improvement is 0.29 bleu - 4 point,0.5425072312355042
translation,141,85,results,seq2seq model,to,seq2seq +attention,seq2seq model to seq2seq +attention,0.5544543862342834
translation,141,88,results,aem model,achieves,significant improvement,aem model achieves significant improvement,0.655894935131073
translation,141,88,results,significant improvement,on,diversity of generated text,significant improvement on diversity of generated text,0.5516529679298401
translation,141,88,results,results,find that,aem model,results find that aem model,0.6580810546875
translation,141,94,results,inter-annotator agreement,is,satisfactory,inter-annotator agreement is satisfactory,0.5173493027687073
translation,141,94,results,results,has,inter-annotator agreement,results has inter-annotator agreement,0.42097318172454834
translation,141,95,results,pearson 's correlation coefficient,is,0.69,pearson 's correlation coefficient is 0.69,0.5199875831604004
translation,141,95,results,pearson 's correlation coefficient,is,0.57,pearson 's correlation coefficient is 0.57,0.5181222558021545
translation,141,95,results,0.69,on,coherence,0.69 on coherence,0.5865691304206848
translation,141,95,results,0.57,on,fluency,0.57 on fluency,0.5966835618019104
translation,141,95,results,results,has,pearson 's correlation coefficient,results has pearson 's correlation coefficient,0.4816996455192566
translation,141,96,results,aem model,outperforms,seq2seq model,aem model outperforms seq2seq model,0.7305375933647156
translation,141,96,results,seq2seq model,with,large margin,seq2seq model with large margin,0.6423600316047668
translation,141,99,results,aem + attention model,achieves,best g-score,aem + attention model achieves best g-score,0.6555042266845703
translation,142,219,ablation-analysis,triggers,observe,22.9 % absolute im - provement,triggers observe 22.9 % absolute im - provement,0.6683700084686279
translation,142,219,ablation-analysis,22.9 % absolute im - provement,in,f1,22.9 % absolute im - provement in f1,0.5669168829917908
translation,142,219,ablation-analysis,22.9 % absolute im - provement,on,relation types,22.9 % absolute im - provement on relation types,0.5281258225440979
translation,142,219,ablation-analysis,relation types,whose,inverse re- lation types,relation types whose inverse re- lation types,0.598260223865509
translation,142,219,ablation-analysis,ablation analysis,introduc - tion of,triggers,ablation analysis introduc - tion of triggers,0.7911641597747803
translation,142,114,baselines,cnn,adapt,three baselines,cnn adapt three baselines,0.8236232399940491
translation,142,114,baselines,bilstm,adapt,three baselines,bilstm adapt three baselines,0.7122594118118286
translation,142,114,baselines,three baselines,that use,different document encoders,three baselines that use different document encoders,0.6319477558135986
translation,142,114,baselines,baselines,has,cnn,baselines has cnn,0.5907226204872131
translation,142,5,experiments,dialogre,for studying,cross-sentence re,dialogre for studying cross-sentence re,0.6753299832344055
translation,142,9,experiments,dialogre,://,dataset.org /dialogre/,dialogre :// dataset.org /dialogre/,0.5584728121757507
translation,142,9,experiments,https,://,dataset.org /dialogre/,https :// dataset.org /dialogre/,0.6228700280189514
translation,142,16,experiments,all occurrences,of,36 possible relation types,all occurrences of 36 possible relation types,0.52297043800354
translation,142,16,experiments,36 possible relation types,exist between,pairs,36 possible relation types exist between pairs,0.6178122162818909
translation,142,16,experiments,pairs,of,arguments,pairs of arguments,0.6341894268989563
translation,142,16,experiments,arguments,in,"1,788 dialogues","arguments in 1,788 dialogues",0.5369839668273926
translation,142,16,experiments,"1,788 dialogues",originating from,complete transcripts of friends,"1,788 dialogues originating from complete transcripts of friends",0.678523063659668
translation,142,30,model,new performance evaluation metric,for,conversational setting,new performance evaluation metric for conversational setting,0.5869032144546509
translation,142,30,model,supplement,to,standard f1 measure,supplement to standard f1 measure,0.5946324467658997
translation,142,30,model,model,design,new performance evaluation metric,model design new performance evaluation metric,0.5783347487449646
translation,142,144,results,best performance,achieved by,method 2,best performance achieved by method 2,0.7194210886955261
translation,142,144,results,not superior,to,bert s,not superior to bert s,0.6885254979133606
translation,142,218,results,ground truth triggers,to,input sequence,ground truth triggers to input sequence,0.546879768371582
translation,142,218,results,input sequence,on,baseline,input sequence on baseline,0.5767858624458313
translation,142,218,results,16.4 % absolute improvement,compared to,bert baseline,16.4 % absolute improvement compared to bert baseline,0.6367722749710083
translation,142,218,results,results,append,ground truth triggers,results append ground truth triggers,0.5843185782432556
translation,143,60,baselines,targeted data collection method,refer to,positive - bias data collection,targeted data collection method refer to positive - bias data collection,0.5719761848449707
translation,143,60,baselines,targeted data collection method,refer to,bias controlled text generation,targeted data collection method refer to bias controlled text generation,0.5771288871765137
translation,143,168,experimental-setup,reddit conversations,extracted and obtained by,third party,reddit conversations extracted and obtained by third party,0.6642301678657532
translation,143,168,experimental-setup,experimental setup,pre-trained on,reddit conversations,experimental setup pre-trained on reddit conversations,0.7526795268058777
translation,143,172,experimental-setup,8 - layer decoders,with,512 dimensional embeddings,8 - layer decoders with 512 dimensional embeddings,0.5958885550498962
translation,143,172,experimental-setup,8 - layer decoders,with,16 attention heads,8 - layer decoders with 16 attention heads,0.6092337369918823
translation,143,172,experimental-setup,16 attention heads,based on,parlai transformer implementation,16 attention heads based on parlai transformer implementation,0.6802968382835388
translation,143,173,experimental-setup,beam search size,of,5,beam search size of 5,0.708274781703949
translation,143,173,experimental-setup,experimental setup,decode,beam search size,experimental setup decode beam search size,0.6891525983810425
translation,143,6,experiments,gender bias,in,six existing dialogue datasets,gender bias in six existing dialogue datasets,0.4723444879055023
translation,143,7,model,three techniques,to mitigate,gender bias,three techniques to mitigate gender bias,0.6995143890380859
translation,143,7,model,model,consider,three techniques,model consider three techniques,0.7582573294639587
translation,143,5,results,gender bias,in,dialogue data,gender bias in dialogue data,0.5194262266159058
translation,143,5,results,amplified,in,subsequent generative chit- chat dialogue models,amplified in subsequent generative chit- chat dialogue models,0.5528985261917114
translation,143,5,results,results,analyze,gender bias,results analyze gender bias,0.5579274296760559
translation,143,184,results,each method,improves on,metrics,each method improves on metrics,0.6882973313331604
translation,143,184,results,metrics,over,baseline transformer,metrics over baseline transformer,0.6863964200019836
translation,143,184,results,f1,over,baseline transformer,f1 over baseline transformer,0.7099824547767639
translation,143,184,results,all methods ( all ),is,most advantageous,all methods ( all ) is most advantageous,0.5094096064567566
translation,143,184,results,metrics,has,% gendered words,metrics has % gendered words,0.5457005500793457
translation,143,184,results,metrics,has,% male bias,metrics has % male bias,0.5007779002189636
translation,143,190,results,small differences,in,overall f1,small differences in overall f1,0.5332959294319153
translation,143,190,results,small differences,shows that,bias ctrl method,small differences shows that bias ctrl method,0.7119439244270325
translation,143,190,results,bias ctrl method,is,efficacious,bias ctrl method is efficacious,0.6282837390899658
translation,143,190,results,changing,has,bin,changing has bin,0.6421133875846863
translation,143,190,results,results,observe,changing,results observe changing,0.5351243615150452
translation,143,214,results,gender accurately,of,all model generations,gender accurately of all model generations,0.5907952189445496
translation,143,214,results,gender accurately,is,more challenging,gender accurately is more challenging,0.5599679946899414
translation,143,214,results,results,predicting,gender accurately,results predicting gender accurately,0.6761808395385742
translation,144,147,experimental-setup,proposed models,implemented using,pytorch framework,proposed models implemented using pytorch framework,0.6837105751037598
translation,144,147,experimental-setup,experimental setup,implemented using,pytorch framework,experimental setup implemented using pytorch framework,0.6574339270591736
translation,144,147,experimental-setup,experimental setup,has,proposed models,experimental setup has proposed models,0.5434987545013428
translation,144,149,experimental-setup,word embedding,concatenation of,pre-trained glove embeddings,word embedding concatenation of pre-trained glove embeddings,0.6497470736503601
translation,144,149,experimental-setup,word embedding,concatenation of,character n-gram embeddings,word embedding concatenation of character n-gram embeddings,0.6835032105445862
translation,144,149,experimental-setup,experimental setup,has,word embedding,experimental setup has word embedding,0.5124366283416748
translation,144,150,experimental-setup,hyper-parameters,by,grid search,hyper-parameters by grid search,0.6004341244697571
translation,144,150,experimental-setup,grid search,on,validation set,grid search on validation set,0.5906150937080383
translation,144,150,experimental-setup,experimental setup,tune,hyper-parameters,experimental setup tune hyper-parameters,0.6891013979911804
translation,144,151,experimental-setup,gae block,implemented with,bi-directional grus,gae block implemented with bi-directional grus,0.7024185061454773
translation,144,151,experimental-setup,hidden state dimension,of,gae,hidden state dimension of gae,0.5858006477355957
translation,144,151,experimental-setup,gae,is,50,gae is 50,0.6880940794944763
translation,144,151,experimental-setup,experimental setup,has,gae block,experimental setup has gae block,0.5617185235023499
translation,144,151,experimental-setup,experimental setup,has,hidden state dimension,experimental setup has hidden state dimension,0.508860170841217
translation,144,152,experimental-setup,hidden state dimension,of,gru,hidden state dimension of gru,0.5996890068054199
translation,144,152,experimental-setup,gru,used in,hierarchical encoder module,gru used in hierarchical encoder module,0.661915123462677
translation,144,152,experimental-setup,hierarchical encoder module,is,50,hierarchical encoder module is 50,0.5971291661262512
translation,144,152,experimental-setup,experimental setup,has,hidden state dimension,experimental setup has hidden state dimension,0.508860170841217
translation,144,153,experimental-setup,fixed learning rate,is,0.001,fixed learning rate is 0.001,0.5545720458030701
translation,144,153,experimental-setup,experimental setup,has,fixed learning rate,experimental setup has fixed learning rate,0.5376529097557068
translation,144,154,experimental-setup,adam optimizer,with,default setting,adam optimizer with default setting,0.6094359755516052
translation,144,154,experimental-setup,default setting,to optimize,models,default setting to optimize models,0.7299197912216187
translation,144,154,experimental-setup,experimental setup,has,adam optimizer,experimental setup has adam optimizer,0.5293667316436768
translation,144,161,experiments,proposed ten model,outperforms,previous models,proposed ten model outperforms previous models,0.7151564955711365
translation,144,161,experiments,previous models,on,dstc2 and woz datasets,previous models on dstc2 and woz datasets,0.5417856574058533
translation,144,6,model,temporally expressive networks ( ten ),to jointly model,two types of temporal dependencies,temporally expressive networks ( ten ) to jointly model two types of temporal dependencies,0.7077584862709045
translation,144,6,model,two types of temporal dependencies,in,dst,two types of temporal dependencies in dst,0.5099332332611084
translation,144,6,model,model,propose,temporally expressive networks ( ten ),model propose temporally expressive networks ( ten ),0.6802032589912415
translation,144,7,model,ten model,utilizes,power,ten model utilizes power,0.6740385890007019
translation,144,7,model,ten model,utilizes,probabilistic graphical models,ten model utilizes probabilistic graphical models,0.6314853429794312
translation,144,7,model,power,of,recurrent networks,power of recurrent networks,0.59974205493927
translation,144,7,model,power,of,probabilistic graphical models,power of probabilistic graphical models,0.5636516809463501
translation,144,7,model,model,has,ten model,model has ten model,0.6157158613204956
translation,144,33,model,novel temporally expressive networks ( ten ),to jointly model,temporal feature dependencies,novel temporally expressive networks ( ten ) to jointly model temporal feature dependencies,0.7087042331695557
translation,144,33,model,novel temporally expressive networks ( ten ),to jointly model,temporal state dependencies,novel temporally expressive networks ( ten ) to jointly model temporal state dependencies,0.6968799829483032
translation,144,33,model,model,propose,novel temporally expressive networks ( ten ),model propose novel temporally expressive networks ( ten ),0.6797494292259216
translation,144,34,model,turn- level state prediction,exploits,hierarchical recurrent networks,turn- level state prediction exploits hierarchical recurrent networks,0.6523658037185669
translation,144,34,model,hierarchical recurrent networks,to capture,temporal feature dependencies,hierarchical recurrent networks to capture temporal feature dependencies,0.5870808959007263
translation,144,34,model,temporal feature dependencies,across,dialogue turns,temporal feature dependencies across dialogue turns,0.6666156053543091
translation,144,34,model,model,to improve,turn- level state prediction,model to improve turn- level state prediction,0.6783379316329956
translation,144,35,model,state aggregation errors,introduce,factor graphs,state aggregation errors introduce factor graphs,0.6026556491851807
translation,144,35,model,factor graphs,to formulate,state dependencies,factor graphs to formulate state dependencies,0.6586510539054871
translation,144,35,model,factor graphs,employ,belief propagation,factor graphs employ belief propagation,0.5268736481666565
translation,144,35,model,belief propagation,to handle,uncertainties,belief propagation to handle uncertainties,0.7205276489257812
translation,144,35,model,uncertainties,in,state aggregation,uncertainties in state aggregation,0.539116382598877
translation,144,35,model,model,to reduce,state aggregation errors,model to reduce state aggregation errors,0.6761544346809387
translation,144,35,model,model,employ,belief propagation,model employ belief propagation,0.5795973539352417
translation,144,36,results,ten,shown to improve,accuracy,ten shown to improve accuracy,0.7412612438201904
translation,144,36,results,accuracy,of,turn - level state prediction,accuracy of turn - level state prediction,0.5936659574508667
translation,144,36,results,accuracy,of,state aggregation,accuracy of state aggregation,0.5839642286300659
translation,144,36,results,"dstc2 , woz and multiwoz datasets",has,ten,"dstc2 , woz and multiwoz datasets has ten",0.5717198848724365
translation,144,36,results,results,Evaluating on,"dstc2 , woz and multiwoz datasets","results Evaluating on dstc2 , woz and multiwoz datasets",0.6502964496612549
translation,144,162,results,ten,built upon,attention - based gru encoders,ten built upon attention - based gru encoders,0.6468913555145264
translation,144,162,results,ten,achieves,comparable performance,ten achieves comparable performance,0.7262274026870728
translation,144,162,results,comparable performance,with,sumbt,comparable performance with sumbt,0.7288902997970581
translation,144,162,results,comparable performance,without incorporating,pre-trained language models,comparable performance without incorporating pre-trained language models,0.6962194442749023
translation,144,162,results,results,worth noting,ten,results worth noting ten,0.5592532753944397
translation,144,164,results,ten -x model,obtains,"impressive 2.7 % , 0.5 % and 4.3 % performance gains","ten -x model obtains impressive 2.7 % , 0.5 % and 4.3 % performance gains",0.5955165028572083
translation,144,164,results,"impressive 2.7 % , 0.5 % and 4.3 % performance gains",on,"dstc2 , woz and multi-woz dataset","impressive 2.7 % , 0.5 % and 4.3 % performance gains on dstc2 , woz and multi-woz dataset",0.5321608185768127
translation,144,164,results,ten - xh,has,ten -x model,ten - xh has ten -x model,0.6147000789642334
translation,144,164,results,results,Comparing to,ten - xh,results Comparing to ten - xh,0.5841892957687378
translation,144,166,results,ten model,improves,ten -x model,ten model improves ten -x model,0.7233570218086243
translation,144,166,results,ten -x model,by,1.1 %,ten -x model by 1.1 %,0.5788617134094238
translation,144,166,results,ten -x model,by,1.5 %,ten -x model by 1.5 %,0.5884560346603394
translation,144,166,results,ten -x model,by,0.3 %,ten -x model by 0.3 %,0.5829218029975891
translation,144,166,results,1.1 %,on,dstc2 dataset,1.1 % on dstc2 dataset,0.5668421387672424
translation,144,166,results,1.5 %,on,woz dataset,1.5 % on woz dataset,0.5583362579345703
translation,144,166,results,0.3 %,on,multiwoz dataset,0.3 % on multiwoz dataset,0.5615487694740295
translation,144,166,results,results,has,ten model,results has ten model,0.5719439387321472
translation,144,167,results,uncertainties,with,belief propagation,uncertainties with belief propagation,0.6161274313926697
translation,144,167,results,belief propagation,in,state aggregation,belief propagation in state aggregation,0.535588800907135
translation,144,167,results,results,has,ten model,results has ten model,0.5719439387321472
translation,144,168,results,ten -y and ten,modeled,temporal feature dependencies,ten -y and ten modeled temporal feature dependencies,0.6639536619186401
translation,144,168,results,ten - y,performs,much worse,ten - y performs much worse,0.6338918805122375
translation,144,168,results,much worse,than,ten,much worse than ten,0.7076975703239441
translation,144,173,results,joint goal accuracy,of,proposed models,joint goal accuracy of proposed models,0.5872787237167358
translation,144,173,results,proposed models,generally decrease at,earlier turns,proposed models generally decrease at earlier turns,0.694635272026062
translation,144,173,results,turns,has,increase,turns has increase,0.6116501688957214
translation,144,179,results,ten -x,with,hierarchical encoder module,ten -x with hierarchical encoder module,0.6577361226081848
translation,144,179,results,ten -x,achieves,higher turn-level state accuracy,ten -x achieves higher turn-level state accuracy,0.7321215867996216
translation,144,179,results,results,observe,ten -x,results observe ten -x,0.5862289667129517
translation,145,164,ablation-analysis,pseudo ground - truth,plays,crucial role,pseudo ground - truth plays crucial role,0.7215545177459717
translation,145,164,ablation-analysis,pseudo ground - truth,removing,step,pseudo ground - truth removing step,0.7124483585357666
translation,145,164,ablation-analysis,crucial role,in,wizard,crucial role in wizard,0.571098268032074
translation,145,164,ablation-analysis,step,causes,dramatic performance drop,step causes dramatic performance drop,0.6778355836868286
translation,145,164,ablation-analysis,ablation analysis,conclude,pseudo ground - truth,ablation analysis conclude pseudo ground - truth,0.6313878893852234
translation,145,194,ablation-analysis,gpt - 2 ( 345 m ),further reduce,ppl,gpt - 2 ( 345 m ) further reduce ppl,0.6675451397895813
translation,145,194,ablation-analysis,ppl,does not bring,significant improvement,ppl does not bring significant improvement,0.6602165102958679
translation,145,194,ablation-analysis,significant improvement,to,f1,significant improvement to f1,0.595073401927948
translation,145,194,ablation-analysis,f1,over,gpt - 2 ( 117 m ),f1 over gpt - 2 ( 117 m ),0.646894633769989
translation,145,194,ablation-analysis,ablation analysis,has,gpt - 2 ( 345 m ),ablation analysis has gpt - 2 ( 345 m ),0.5675671100616455
translation,145,131,baselines,baselines,has,disentangled response decoder ( drd ),baselines has disentangled response decoder ( drd ),0.5669282078742981
translation,145,147,experimental-setup,"adam ( kingma and ba , 2015 ) optimizer",with,? 1 = 0.9 and ? 2 = 0.999,"adam ( kingma and ba , 2015 ) optimizer with ? 1 = 0.9 and ? 2 = 0.999",0.6056467294692993
translation,145,148,experimental-setup,"sim ( ? , ? )",as,unigram f1,"sim ( ? , ? ) as unigram f1",0.5256124138832092
translation,145,148,experimental-setup,gpt - 2 model,with,pseudo ground - truth,gpt - 2 model with pseudo ground - truth,0.5967706441879272
translation,145,148,experimental-setup,pseudo ground - truth,for,1000 steps,pseudo ground - truth for 1000 steps,0.6090558171272278
translation,145,148,experimental-setup,pseudo ground - truth,with,batch size,pseudo ground - truth with batch size,0.605358898639679
translation,145,148,experimental-setup,1000 steps,with,batch size,1000 steps with batch size,0.6536218523979187
translation,145,148,experimental-setup,batch size,of,64,batch size of 64,0.6741159558296204
translation,145,148,experimental-setup,experimental setup,optimize,gpt - 2 model,experimental setup optimize gpt - 2 model,0.7069230675697327
translation,145,149,experimental-setup,batch size,set as,128,batch size set as 128,0.6918348073959351
translation,145,149,experimental-setup,learning rates,for,"g( u , d ) and gpt - 2","learning rates for g( u , d ) and gpt - 2",0.6446206569671631
translation,145,149,experimental-setup,"g( u , d ) and gpt - 2",set as,5e ? 6 and 5e ? 5,"g( u , d ) and gpt - 2 set as 5e ? 6 and 5e ? 5",0.6317586898803711
translation,145,149,experimental-setup,joint optimization,has,batch size,joint optimization has batch size,0.551348090171814
translation,145,149,experimental-setup,joint optimization,has,learning rates,joint optimization has learning rates,0.5355370044708252
translation,145,149,experimental-setup,experimental setup,In,joint optimization,experimental setup In joint optimization,0.49394360184669495
translation,145,151,experimental-setup,parameter p,of,bernoulli distribution,parameter p of bernoulli distribution,0.583314836025238
translation,145,151,experimental-setup,parameter p,of,anneals,parameter p of anneals,0.6668916344642639
translation,145,151,experimental-setup,parameter p,initially set as,1.0,parameter p initially set as 1.0,0.6619993448257446
translation,145,151,experimental-setup,bernoulli distribution,in,curriculum step,bernoulli distribution in curriculum step,0.5568757057189941
translation,145,151,experimental-setup,anneals,with,rate,anneals with rate,0.6672667860984802
translation,145,151,experimental-setup,rate,of,1e ? 5,rate of 1e ? 5,0.6634196043014526
translation,145,151,experimental-setup,experimental setup,has,parameter p,experimental setup has parameter p,0.5376415252685547
translation,145,152,experimental-setup,early stopping,on,validation,early stopping on validation,0.595452606678009
translation,145,152,experimental-setup,validation,adopted as,regularization strategy,validation adopted as regularization strategy,0.6081844568252563
translation,145,152,experimental-setup,experimental setup,has,early stopping,experimental setup has early stopping,0.5249444842338562
translation,145,5,model,response generation,defined by,pretrained language model,response generation defined by pretrained language model,0.5202029347419739
translation,145,5,model,response generation,defined by,unsupervised approach,response generation defined by unsupervised approach,0.5875988602638245
translation,145,5,model,response generation,with,unlabeled dialogues,response generation with unlabeled dialogues,0.6240705847740173
translation,145,5,model,pretrained language model,with,knowledge selection module,pretrained language model with knowledge selection module,0.5911886692047119
translation,145,5,model,unsupervised approach,to jointly optimizing,knowledge selection,unsupervised approach to jointly optimizing knowledge selection,0.7411114573478699
translation,145,5,model,unsupervised approach,to jointly optimizing,response generation,unsupervised approach to jointly optimizing response generation,0.7280842661857605
translation,145,5,model,response generation,with,unlabeled dialogues,response generation with unlabeled dialogues,0.6240705847740173
translation,145,29,model,pre-trained response generation model,with,knowledge selection module,pre-trained response generation model with knowledge selection module,0.6079289317131042
translation,145,29,model,knowledge selection module,whereby,redundant knowledge input,knowledge selection module whereby redundant knowledge input,0.640045166015625
translation,145,29,model,redundant knowledge input,slimmed with,relevant information,redundant knowledge input slimmed with relevant information,0.6342173218727112
translation,145,29,model,relevant information,regarding to,conversation contexts ),relevant information regarding to conversation contexts ),0.6516302227973938
translation,145,29,model,relevant information,kept to meet,capacity constraint,relevant information kept to meet capacity constraint,0.6464466452598572
translation,145,29,model,model,equipping,pre-trained response generation model,model equipping pre-trained response generation model,0.7234573364257812
translation,145,31,model,unsupervised approach,learning of,knowledge selection,unsupervised approach learning of knowledge selection,0.7582884430885315
translation,145,31,model,unsupervised approach,learning of,fine-tuning,unsupervised approach learning of fine-tuning,0.7880659103393555
translation,145,31,model,fine-tuning,of,response generation,fine-tuning of response generation,0.5521458983421326
translation,145,31,model,fine-tuning,jointly conducted with,unlabeled dialogues,fine-tuning jointly conducted with unlabeled dialogues,0.5898478031158447
translation,145,31,model,response generation,jointly conducted with,unlabeled dialogues,response generation jointly conducted with unlabeled dialogues,0.6154158115386963
translation,145,31,model,model,propose,unsupervised approach,model propose unsupervised approach,0.7285106778144836
translation,145,32,model,knowledge selection module,on the basis of,bert,knowledge selection module on the basis of bert,0.7149751782417297
translation,145,32,model,knowledge selection module,formalize,knowledge selection,knowledge selection module formalize knowledge selection,0.6107764840126038
translation,145,32,model,knowledge selection,as,sequence prediction process,knowledge selection as sequence prediction process,0.5158992409706116
translation,145,32,model,model,build,knowledge selection module,model build knowledge selection module,0.7498502731323242
translation,145,32,model,model,formalize,knowledge selection,model formalize knowledge selection,0.670472264289856
translation,145,33,model,learning algorithm,starts from,training,learning algorithm starts from training,0.7286482453346252
translation,145,33,model,training,with,pseudo ground - truth,training with pseudo ground - truth,0.6013511419296265
translation,145,33,model,pseudo ground - truth,updates,knowledge selection model,pseudo ground - truth updates knowledge selection model,0.6964229345321655
translation,145,33,model,pseudo ground - truth,updates,response generation model,pseudo ground - truth updates response generation model,0.7166890501976013
translation,145,33,model,responses,as,alternation,responses as alternation,0.5915822982788086
translation,145,33,model,alternation,of,human annotations,alternation of human annotations,0.5667930245399475
translation,145,33,model,knowledge selection model,through,reinforcement learning approach,knowledge selection model through reinforcement learning approach,0.6534093618392944
translation,145,33,model,knowledge selection model,through,curriculum learning approach,knowledge selection model through curriculum learning approach,0.6890764832496643
translation,145,33,model,response generation model,through,reinforcement learning approach,response generation model through reinforcement learning approach,0.6683196425437927
translation,145,33,model,response generation model,through,curriculum learning approach,response generation model through curriculum learning approach,0.7156000733375549
translation,145,33,model,model,has,learning algorithm,model has learning algorithm,0.562118411064148
translation,145,34,model,knowledge selection,with,feedback,knowledge selection with feedback,0.6544547080993652
translation,145,34,model,further optimized,with,feedback,further optimized with feedback,0.6892327070236206
translation,145,34,model,feedback,from,response generation,feedback from response generation,0.589963436126709
translation,145,34,model,feedback,from,response generation model,feedback from response generation model,0.5829486846923828
translation,145,34,model,knowledge,used for,fine-tuning,knowledge used for fine-tuning,0.655168890953064
translation,145,34,model,gradually moves,from,pseudo ground -truth,gradually moves from pseudo ground -truth,0.5631710886955261
translation,145,34,model,pseudo ground -truth,to,prediction,pseudo ground -truth to prediction,0.5567259788513184
translation,145,34,model,prediction,of,knowledge selection module,prediction of knowledge selection module,0.5785117745399475
translation,145,34,model,fine-tuning,has,response generation model,fine-tuning has response generation model,0.5151103138923645
translation,145,34,model,response generation model,has,gradually moves,response generation model has gradually moves,0.5513062477111816
translation,145,34,model,model,has,knowledge selection,model has knowledge selection,0.5760982632637024
translation,145,46,model,pre-trained language models,with,external knowledge,pre-trained language models with external knowledge,0.6108272075653076
translation,145,126,model,responses,with,deliberation technique,responses with deliberation technique,0.649948239326477
translation,145,126,model,model,has,incremental transformer with deliberation decoder ( itdd ),model has incremental transformer with deliberation decoder ( itdd ),0.6046577095985413
translation,145,146,model,greedy search,in,response decoding,greedy search in response decoding,0.5603323578834534
translation,145,146,model,model,employ,greedy search,model employ greedy search,0.5932345390319824
translation,145,177,model,knowledge selection module,propose,unsupervised approach,knowledge selection module propose unsupervised approach,0.6837093234062195
translation,145,177,model,unsupervised approach,to jointly optimizing,knowledge selection,unsupervised approach to jointly optimizing knowledge selection,0.7411114573478699
translation,145,177,model,unsupervised approach,to jointly optimizing,response generation,unsupervised approach to jointly optimizing response generation,0.7280842661857605
translation,145,177,model,model,devise,knowledge selection module,model devise knowledge selection module,0.7294803857803345
translation,145,177,model,model,propose,unsupervised approach,model propose unsupervised approach,0.7285106778144836
translation,145,43,results,pre-trained language models,achieve,new stateof - the - art,pre-trained language models achieve new stateof - the - art,0.5848875641822815
translation,145,155,results,knowledgpt,achieves,new state - of - the - art,knowledgpt achieves new state - of - the - art,0.6558063626289368
translation,145,155,results,new state - of - the - art,on,most metrics,new state - of - the - art on most metrics,0.522750198841095
translation,145,155,results,new state - of - the - art,in,datasets,new state - of - the - art in datasets,0.5008478164672852
translation,145,155,results,most metrics,in,datasets,most metrics in datasets,0.5207850337028503
translation,145,155,results,results,has,knowledgpt,results has knowledgpt,0.5968797206878662
translation,145,156,results,gpt - 2 trunc,worse than,knowledgpt,gpt - 2 trunc worse than knowledgpt,0.7649677395820618
translation,145,156,results,knowledge loss,in,53 % test examples,knowledge loss in 53 % test examples,0.49120503664016724
translation,145,156,results,53 % test examples,has,groundtruth knowledge,53 % test examples has groundtruth knowledge,0.5642005801200867
translation,145,156,results,results,has,gpt - 2 trunc,results has gpt - 2 trunc,0.49071168899536133
translation,145,158,results,comparable,on,fluency,comparable on fluency,0.5569005608558655
translation,145,158,results,knowl-edgpt,superior to,others,knowl-edgpt superior to others,0.7866533398628235
translation,145,158,results,others,on,context coherence,others on context coherence,0.5139533877372742
translation,145,158,results,others,on,knowledge relevance,others on knowledge relevance,0.5186833143234253
translation,145,178,results,two benchmarks,indicate,our model,two benchmarks indicate our model,0.5958791971206665
translation,145,178,results,our model,has,significantly outperform,our model has significantly outperform,0.6035624742507935
translation,145,178,results,significantly outperform,has,state - of - the - art methods,significantly outperform has state - of - the - art methods,0.5536825656890869
translation,145,187,results,ground - truth,forced to be,kept,ground - truth forced to be kept,0.7103819251060486
translation,145,187,results,ground - truth,randomly mixed with,other candidates,ground - truth randomly mixed with other candidates,0.691526472568512
translation,145,187,results,ground - truth,has,gpt - 2 trunc,ground - truth has gpt - 2 trunc,0.5756311416625977
translation,145,187,results,results,when,ground - truth,results when ground - truth,0.596250593662262
translation,145,190,results,length,limited to,128 tokens,length limited to 128 tokens,0.6413041353225708
translation,145,190,results,ppl,of,model,ppl of model,0.634839653968811
translation,145,190,results,model,is,not good,model is not good,0.5960519909858704
translation,145,190,results,length,has,ppl,length has ppl,0.5887138843536377
translation,145,190,results,128 tokens,has,ppl,128 tokens has ppl,0.6283362507820129
translation,145,190,results,results,When,length,results When length,0.6001988649368286
translation,145,215,results,ev- idence,is,gpt - 2 trunc,ev- idence is gpt - 2 trunc,0.6501470804214478
translation,145,215,results,gpt - 2 trunc,worse than,knowl - edgpt,gpt - 2 trunc worse than knowl - edgpt,0.7517392039299011
translation,145,215,results,knowl - edgpt,on,cmu dog,knowl - edgpt on cmu dog,0.6230436563491821
translation,145,215,results,results,has,ev- idence,results has ev- idence,0.5250019431114197
translation,145,216,results,outper -,forms,skt + gpt - 2,outper - forms skt + gpt - 2,0.7325924038887024
translation,145,216,results,skt + gpt - 2,on,wizard,skt + gpt - 2 on wizard,0.590778112411499
translation,145,216,results,knowledgpt,has,outper -,knowledgpt has outper -,0.7066544890403748
translation,145,216,results,results,has,knowledgpt,results has knowledgpt,0.5968797206878662
translation,146,142,experiments,human user,emulated by,agenda- based user simulator with error model,human user emulated by agenda- based user simulator with error model,0.8004751205444336
translation,146,142,experiments,human teacher,emulated by,pre-trained policy model,human teacher emulated by pre-trained policy model,0.745794415473938
translation,146,142,experiments,pre-trained policy model,with,success rate,pre-trained policy model with success rate,0.617575466632843
translation,146,142,experiments,success rate,of,0.78,success rate of 0.78,0.5328511595726013
translation,146,142,experiments,success rate,about,0.78,success rate about 0.78,0.5373097062110901
translation,146,142,experiments,0.78,through,multitask dqn approach,0.78 through multitask dqn approach,0.6204372644424438
translation,146,142,experiments,multitask dqn approach,without,teaching,multitask dqn approach without teaching,0.677575409412384
translation,146,30,model,novel failure prognosis based teaching heuristic,where,mul-titask learning ( mtl ),novel failure prognosis based teaching heuristic where mul-titask learning ( mtl ),0.6192666292190552
translation,146,30,model,mul-titask learning ( mtl ),to predict,dialogue success reward,mul-titask learning ( mtl ) to predict dialogue success reward,0.6902204751968384
translation,146,70,model,novel failure prognosis based teaching heuristic ( fpt ),for,on - line policy learning,novel failure prognosis based teaching heuristic ( fpt ) for on - line policy learning,0.5983309745788574
translation,146,70,model,on - line policy learning,to reduce,unnecessary advice,on - line policy learning to reduce unnecessary advice,0.6671631932258606
translation,146,70,model,model,propose,novel failure prognosis based teaching heuristic ( fpt ),model propose novel failure prognosis based teaching heuristic ( fpt ),0.6763105392456055
translation,146,74,model,multitask learning ( mtl ),for,policy model,multitask learning ( mtl ) for policy model,0.5896430611610413
translation,146,74,model,multitask learning ( mtl ),compatible with,various rl algorithms,multitask learning ( mtl ) compatible with various rl algorithms,0.6938062310218811
translation,146,74,model,policy model,to estimate,future dialogue success reward,policy model to estimate future dialogue success reward,0.6696168780326843
translation,146,75,model,policy model,with,deep q-network ( dqn ),policy model with deep q-network ( dqn ),0.6301254630088806
translation,146,75,model,policy model,in which,neural network function approximator,policy model in which neural network function approximator,0.6004663705825806
translation,146,75,model,neural network function approximator,named,q-network,neural network function approximator named q-network,0.6893484592437744
translation,146,75,model,neural network function approximator,to estimate,action - value function,neural network function approximator to estimate action - value function,0.7104753851890564
translation,146,75,model,model,implement,policy model,model implement policy model,0.6859935522079468
translation,146,180,results,eapc + sut,is,safest teaching scheme,eapc + sut is safest teaching scheme,0.5898984670639038
translation,146,180,results,safest teaching scheme,reduces,about 78 % risk,safest teaching scheme reduces about 78 % risk,0.6796123385429382
translation,146,180,results,about 78 % risk,of,no-teaching learning,about 78 % risk of no-teaching learning,0.5718896389007568
translation,146,180,results,all 18 teaching schemes,has,eapc + sut,all 18 teaching schemes has eapc + sut,0.6175739765167236
translation,146,180,results,results,Among,all 18 teaching schemes,results Among all 18 teaching schemes,0.5774754881858826
translation,146,209,results,fpt - based heuristics,combined with,eapc strategy,fpt - based heuristics combined with eapc strategy,0.7150353789329529
translation,146,209,results,fpt - based heuristics,achieved,promising performance,fpt - based heuristics achieved promising performance,0.7092759013175964
translation,146,209,results,fpt - based heuristics,required,relatively slight workload,fpt - based heuristics required relatively slight workload,0.7044860124588013
translation,146,209,results,promising performance,on,ri and ht,promising performance on ri and ht,0.5600317716598511
translation,146,209,results,teaching schemes,has,fpt - based heuristics,teaching schemes has fpt - based heuristics,0.5927484631538391
translation,147,110,ablation-analysis,sign tests,show that,improvements,sign tests show that improvements,0.5111735463142395
translation,147,110,ablation-analysis,improvements,of,ssvn,improvements of ssvn,0.6341219544410706
translation,147,110,ablation-analysis,ssvn,are,statistically significant ( p- value < 0.01 ),ssvn are statistically significant ( p- value < 0.01 ),0.5770624279975891
translation,147,110,ablation-analysis,ablation analysis,has,sign tests,ablation analysis has sign tests,0.5666570663452148
translation,147,123,ablation-analysis,slightly lower,than,ssvn,slightly lower than ssvn,0.6051105260848999
translation,147,123,ablation-analysis,distinct ratios and numbers,has,drop,distinct ratios and numbers has drop,0.5751176476478577
translation,147,123,ablation-analysis,drop,has,dramatically,drop has dramatically,0.6238924264907837
translation,147,123,ablation-analysis,ablation analysis,Removing,extractor,ablation analysis Removing extractor,0.7449770569801331
translation,147,135,ablation-analysis,vmf,in,extractor,vmf in extractor,0.5321911573410034
translation,147,135,ablation-analysis,vmf,mitigate,kl,vmf mitigate kl,0.668683648109436
translation,147,135,ablation-analysis,vmf,capture,more meaningful personal features,vmf capture more meaningful personal features,0.737051248550415
translation,147,135,ablation-analysis,kl,-,vanishing,kl - vanishing,0.7560577392578125
translation,147,135,ablation-analysis,kl,has,vanishing,kl has vanishing,0.7018224000930786
translation,147,135,ablation-analysis,ablation analysis,indicate,vmf,ablation analysis indicate vmf,0.5981623530387878
translation,147,136,ablation-analysis,kl loss,presents,upward trend,kl loss presents upward trend,0.6752740740776062
translation,147,136,ablation-analysis,upward trend,in,"gau-generators ( i.e. , ssvn gau and ssvn gau ? g )","upward trend in gau-generators ( i.e. , ssvn gau and ssvn gau ? g )",0.5354117751121521
translation,147,136,ablation-analysis,generator kl,has,kl loss,generator kl has kl loss,0.5539672374725342
translation,147,136,ablation-analysis,ablation analysis,in,generator kl,ablation analysis in generator kl,0.5470391511917114
translation,147,138,ablation-analysis,gau-generators,in,generator kl,gau-generators in generator kl,0.5456188321113586
translation,147,138,ablation-analysis,ablation analysis,Compared with,gau-generators,ablation analysis Compared with gau-generators,0.6870923042297363
translation,147,148,ablation-analysis,performances,of,average and diversity,performances of average and diversity,0.5947771668434143
translation,147,148,ablation-analysis,second stage,has,performances,second stage has performances,0.5885063409805298
translation,147,149,ablation-analysis,larger ?,not bring,improvement,larger ? not bring improvement,0.6975079774856567
translation,147,149,ablation-analysis,larger ?,increases,diversity,larger ? increases diversity,0.7771965861320496
translation,147,149,ablation-analysis,improvement,of,average,improvement of average,0.6000789403915405
translation,147,149,ablation-analysis,reconstruction rout stage,has,larger ?,reconstruction rout stage has larger ?,0.6438915133476257
translation,147,149,ablation-analysis,ablation analysis,For,reconstruction rout stage,ablation analysis For reconstruction rout stage,0.6107615232467651
translation,147,161,ablation-analysis,logic condradiction scenes,make up,most of the errors,logic condradiction scenes make up most of the errors,0.7292146682739258
translation,147,161,ablation-analysis,ablation analysis,find that,logic condradiction scenes,ablation analysis find that logic condradiction scenes,0.650152862071991
translation,147,162,ablation-analysis,personal features,from,replier,personal features from replier,0.5599709153175354
translation,147,162,ablation-analysis,personal features,exists,31.49 % replier-inconsistent cases,personal features exists 31.49 % replier-inconsistent cases,0.654523491859436
translation,147,162,ablation-analysis,ablation analysis,considering,personal features,ablation analysis considering personal features,0.7203316688537598
translation,147,163,ablation-analysis,consistency,of,replier 's features,consistency of replier 's features,0.5881147980690002
translation,147,163,ablation-analysis,replier 's features,has,improve,replier 's features has improve,0.630520224571228
translation,147,163,ablation-analysis,improve,has,of response diversity,improve has of response diversity,0.5386555194854736
translation,147,163,ablation-analysis,of response diversity,has,significantly,of response diversity has significantly,0.5741125345230103
translation,147,84,baselines,standard seq2seq model,with,attention mechanism,standard seq2seq model with attention mechanism,0.5979354381561279
translation,147,84,baselines,hierarchical encoder framework,to model,multi-turn dialogs,hierarchical encoder framework to model multi-turn dialogs,0.6692209243774414
translation,147,84,baselines,s2sa,has,standard seq2seq model,s2sa has standard seq2seq model,0.5379343032836914
translation,147,84,baselines,hred,has,hierarchical encoder framework,hred has hierarchical encoder framework,0.5646829009056091
translation,147,85,baselines,hierarchical encoder-decoder,with,latent stochastic variable,hierarchical encoder-decoder with latent stochastic variable,0.625377357006073
translation,147,85,baselines,encoder-decoder network,containing,hierarchical structure,encoder-decoder network containing hierarchical structure,0.6644191145896912
translation,147,85,baselines,encoder-decoder network,containing,variational memory,encoder-decoder network containing variational memory,0.6211811304092407
translation,147,85,baselines,vhred,has,hierarchical encoder-decoder,vhred has hierarchical encoder-decoder,0.6065942645072937
translation,147,85,baselines,hvmn,has,encoder-decoder network,hvmn has encoder-decoder network,0.5798155069351196
translation,147,85,baselines,baselines,has,vhred,baselines has vhred,0.6250126957893372
translation,147,87,experimental-setup,experimental setup,implemented using,tensorflow framework,experimental setup implemented using tensorflow framework,0.6224326491355896
translation,147,88,experimental-setup,word embeddings,to,size,word embeddings to size,0.5143236517906189
translation,147,88,experimental-setup,word embeddings,initialize them,randomly,word embeddings initialize them randomly,0.6591594815254211
translation,147,88,experimental-setup,size,of,200,size of 200,0.679601788520813
translation,147,88,experimental-setup,experimental setup,set,word embeddings,experimental setup set word embeddings,0.6023686528205872
translation,147,89,experimental-setup,shared utterance encoder,is,2 - layer bidirectional gru structure,shared utterance encoder is 2 - layer bidirectional gru structure,0.5272692441940308
translation,147,89,experimental-setup,2 - layer bidirectional gru structure,with,600 hidden neurons,2 - layer bidirectional gru structure with 600 hidden neurons,0.6150968670845032
translation,147,89,experimental-setup,600 hidden neurons,for,each layer,600 hidden neurons for each layer,0.5858322978019714
translation,147,89,experimental-setup,unidirectional ones,with,hidden size,unidirectional ones with hidden size,0.6806585192680359
translation,147,89,experimental-setup,hidden size,of,600,hidden size of 600,0.6702679395675659
translation,147,89,experimental-setup,experimental setup,has,shared utterance encoder,experimental setup has shared utterance encoder,0.5396847724914551
translation,147,90,experimental-setup,dimensions,of,latent variable z and z r,dimensions of latent variable z and z r,0.6004587411880493
translation,147,90,experimental-setup,latent variable z and z r,set to,50,latent variable z and z r set to 50,0.7080693244934082
translation,147,90,experimental-setup,experimental setup,has,dimensions,experimental setup has dimensions,0.4716714024543762
translation,147,91,experimental-setup,adam algorithm,to update,parameters,adam algorithm to update parameters,0.7106003165245056
translation,147,91,experimental-setup,parameters,with,initial learning rate,parameters with initial learning rate,0.5836310386657715
translation,147,91,experimental-setup,initial learning rate,of,0.001,initial learning rate of 0.001,0.5703312754631042
translation,147,91,experimental-setup,experimental setup,use,adam algorithm,experimental setup use adam algorithm,0.6145118474960327
translation,147,92,experimental-setup,early - stop strategy,to select,best models,early - stop strategy to select best models,0.7107124328613281
translation,147,92,experimental-setup,best models,using,variational lower - bound,best models using variational lower - bound,0.5794464349746704
translation,147,92,experimental-setup,variational lower - bound,on,validation set,variational lower - bound on validation set,0.548384964466095
translation,147,8,model,unit hypersperical distribution,namely,von mises - fisher ( vmf ),unit hypersperical distribution namely von mises - fisher ( vmf ),0.6992496252059937
translation,147,8,model,unit hypersperical distribution,namely,latent space,unit hypersperical distribution namely latent space,0.7094652652740479
translation,147,8,model,unit hypersperical distribution,as,latent space,unit hypersperical distribution as latent space,0.5698285102844238
translation,147,8,model,unit hypersperical distribution,can obtain,stable kl performance,unit hypersperical distribution can obtain stable kl performance,0.6308591961860657
translation,147,8,model,latent space,of,semi-supervised model,latent space of semi-supervised model,0.5454666614532471
translation,147,8,model,stable kl performance,by setting,fixed variance,stable kl performance by setting fixed variance,0.6903136968612671
translation,147,8,model,stable kl performance,enhance,global information representation,stable kl performance enhance global information representation,0.6030892729759216
translation,147,8,model,model,use,unit hypersperical distribution,model use unit hypersperical distribution,0.6559752225875854
translation,147,9,model,unsupervised extractor,to automatically distill,replier-tailored feature,unsupervised extractor to automatically distill replier-tailored feature,0.7000274658203125
translation,147,9,model,unsupervised extractor,injected into,supervised generator,unsupervised extractor injected into supervised generator,0.6147341132164001
translation,147,9,model,supervised generator,to encourage,replier-consistency,supervised generator to encourage replier-consistency,0.7029445767402649
translation,147,9,model,model,has,unsupervised extractor,model has unsupervised extractor,0.5582181215286255
translation,147,23,model,variational auto-encoder ( vae ),propose,semi-supervised stable variational network ( ssvn ) framework,variational auto-encoder ( vae ) propose semi-supervised stable variational network ( ssvn ) framework,0.6191617846488953
translation,147,23,model,model,propose,semi-supervised stable variational network ( ssvn ) framework,model propose semi-supervised stable variational network ( ssvn ) framework,0.6560696363449097
translation,147,24,model,unsupervised personal feature extractor,has,a vae with vmf ),unsupervised personal feature extractor has a vae with vmf ),0.5748677253723145
translation,147,24,model,model,consists of,unsupervised personal feature extractor,model consists of unsupervised personal feature extractor,0.6294593811035156
translation,147,24,model,model,consists of,supervised information - enhanced generator ( a cvae with vmf ),model consists of supervised information - enhanced generator ( a cvae with vmf ),0.6686961054801941
translation,147,25,model,consistency,of,replier features,consistency of replier features,0.5755077600479126
translation,147,25,model,extractor,encodes,previous utterances,extractor encodes previous utterances,0.7614619135856628
translation,147,25,model,extractor,produces,personally tailored latent variable,extractor produces personally tailored latent variable,0.6198022961616516
translation,147,25,model,previous utterances,from,replier,previous utterances from replier,0.5826672911643982
translation,147,25,model,consistency,has,extractor,consistency has extractor,0.5630919933319092
translation,147,25,model,replier features,has,extractor,replier features has extractor,0.5634665489196777
translation,147,25,model,model,To maintain,consistency,model To maintain consistency,0.7409985661506653
translation,147,108,model,hvmn,introduces,memory network,hvmn introduces memory network,0.6586982011795044
translation,147,108,model,hvmn,obtains,best performance,hvmn obtains best performance,0.6692065000534058
translation,147,108,model,memory network,to enhance,long-term memory,memory network to enhance long-term memory,0.6055366396903992
translation,147,108,model,best performance,among,baseline models,best performance among baseline models,0.6168348789215088
translation,147,108,model,vhred,has,hvmn,vhred has hvmn,0.6654478907585144
translation,147,108,model,model,On the top of,vhred,model On the top of vhred,0.7022001147270203
translation,147,166,model,semi-supervised stable variational network,for addressing,latent space futility,semi-supervised stable variational network for addressing latent space futility,0.7001809477806091
translation,147,166,model,semi-supervised stable variational network,for addressing,replier-consistency decay issues,semi-supervised stable variational network for addressing replier-consistency decay issues,0.6703383326530457
translation,147,166,model,model,propose,semi-supervised stable variational network,model propose semi-supervised stable variational network,0.6348533630371094
translation,147,167,model,vmf,as,prior and posterior,vmf as prior and posterior,0.5089002847671509
translation,147,167,model,prior and posterior,to resolve,latent space futility issue,prior and posterior to resolve latent space futility issue,0.720842182636261
translation,147,167,model,unsupervised extractor,to obtain,replier-tailored personal features,unsupervised extractor to obtain replier-tailored personal features,0.5693208575248718
translation,147,167,model,replier-tailored personal features,to ensure,replier-consistency,replier-tailored personal features to ensure replier-consistency,0.6691216826438904
translation,147,105,results,hred,performs,better,hred performs better,0.6922153234481812
translation,147,105,results,better,than,s2sa,better than s2sa,0.6268478035926819
translation,147,105,results,results,observe,hred,results observe hred,0.6070396304130554
translation,147,106,results,hred,on,all metrics,hred on all metrics,0.5764995813369751
translation,147,106,results,hred,on,cornell,hred on cornell,0.7254233360290527
translation,147,106,results,all metrics,on,cornell,all metrics on cornell,0.5691684484481812
translation,147,106,results,vhred,has,outperforms,vhred has outperforms,0.6710678935050964
translation,147,106,results,outperforms,has,hred,outperforms has hred,0.675796627998352
translation,147,106,results,results,has,vhred,results has vhred,0.5513537526130676
translation,147,107,results,worse performance,than,hred,worse performance than hred,0.614655077457428
translation,147,107,results,hred,in terms of,three embedding - based metrics,hred in terms of three embedding - based metrics,0.7303774356842041
translation,147,107,results,three embedding - based metrics,on,ubuntu,three embedding - based metrics on ubuntu,0.520389974117279
translation,147,107,results,vhred,has,worse performance,vhred has worse performance,0.6089594960212708
translation,147,107,results,results,has,vhred,results has vhred,0.5513537526130676
translation,147,109,results,our ssvn model,achieves,highest scores,our ssvn model achieves highest scores,0.6622269749641418
translation,147,109,results,highest scores,in terms of,all metrics,highest scores in terms of all metrics,0.6352481842041016
translation,147,109,results,all metrics,on,two datasets,all metrics on two datasets,0.5051544904708862
translation,147,109,results,all the baselines,has,our ssvn model,all the baselines has our ssvn model,0.561903715133667
translation,147,109,results,results,Compared with,all the baselines,results Compared with all the baselines,0.6420896649360657
translation,147,111,results,models,trained on,ubuntu,models trained on ubuntu,0.6703121066093445
translation,147,111,results,ubuntu,consistently have,more distinct n-grams,ubuntu consistently have more distinct n-grams,0.5718985795974731
translation,147,111,results,more distinct n-grams,than,same models,more distinct n-grams than same models,0.590152382850647
translation,147,111,results,same models,trained on,cornell,same models trained on cornell,0.7953391075134277
translation,147,111,results,results,has,models,results has models,0.5335168838500977
translation,147,115,results,"replier-specific responses ( i.e. , the grade ' 4 ' )",of,ssvn model,"replier-specific responses ( i.e. , the grade ' 4 ' ) of ssvn model",0.5880390405654907
translation,147,115,results,ssvn model,is,22.69 %,ssvn model is 22.69 %,0.5672687292098999
translation,147,115,results,22.69 %,much higher than,baselines,22.69 % much higher than baselines,0.6443368792533875
translation,147,116,results,ssvn model,generates,much more informative responses,ssvn model generates much more informative responses,0.6496389508247375
translation,147,116,results,much more informative responses,"much less generic responses ( i.e. , 20.28 % labeled as",2,"much more informative responses much less generic responses ( i.e. , 20.28 % labeled as 2",0.6987696886062622
translation,147,116,results,results,has,ssvn model,results has ssvn model,0.5347487926483154
translation,147,117,results,results,has,results,results has results,0.48582205176353455
translation,147,118,results,kappa scores,of,models,kappa scores of models,0.6142564415931702
translation,147,118,results,models,higher than,0.4,models higher than 0.4,0.6943799257278442
translation,147,118,results,results,has,kappa scores,results has kappa scores,0.5036689639091492
translation,147,119,results,sign tests,show that,human evaluation improvements,sign tests show that human evaluation improvements,0.4462029039859772
translation,147,119,results,human evaluation improvements,of,ssvn,human evaluation improvements of ssvn,0.5850747227668762
translation,147,119,results,ssvn,to,baselines,ssvn to baselines,0.5982382297515869
translation,147,119,results,significant,on,cornell dataset,significant on cornell dataset,0.5417125821113586
translation,147,119,results,results,has,sign tests,results has sign tests,0.5107909440994263
translation,147,125,results,better performance,than,svn,better performance than svn,0.5844295024871826
translation,147,125,results,worse one,than,ssvn,worse one than ssvn,0.6584449410438538
translation,147,125,results,ssvn gau ?e,has,better performance,ssvn gau ?e has better performance,0.619646430015564
translation,147,125,results,results,has,ssvn gau ?e,results has ssvn gau ?e,0.6188284754753113
translation,147,130,results,gau-generator,tends to produce,informative but very short responses,gau-generator tends to produce informative but very short responses,0.7198812961578369
translation,147,130,results,results,indicate,gau-generator,results indicate gau-generator,0.5529844760894775
translation,148,91,experimental-setup,c = 4,for,number of the receptors,c = 4 for number of the receptors,0.6457153558731079
translation,148,91,experimental-setup,number of the receptors,for,each slot,number of the receptors for each slot,0.6248522400856018
translation,148,92,experimental-setup,word embeddings,used by,semantically specialised paragram - sl999 vectors,word embeddings used by semantically specialised paragram - sl999 vectors,0.6586839556694031
translation,148,92,experimental-setup,word embeddings,is,semantically specialised paragram - sl999 vectors,word embeddings is semantically specialised paragram - sl999 vectors,0.555560290813446
translation,148,92,experimental-setup,semantically specialised paragram - sl999 vectors,with,dimension,semantically specialised paragram - sl999 vectors with dimension,0.6402860879898071
translation,148,92,experimental-setup,semantically specialised paragram - sl999 vectors,contain,richer semantic contents,semantically specialised paragram - sl999 vectors contain richer semantic contents,0.6422986388206482
translation,148,92,experimental-setup,dimension,of,300,dimension of 300,0.6947354674339294
translation,148,92,experimental-setup,richer semantic contents,compared to,other kinds of word embeddings,richer semantic contents compared to other kinds of word embeddings,0.6210024952888489
translation,148,92,experimental-setup,experimental setup,has,word embeddings,experimental setup has word embeddings,0.4932878315448761
translation,148,93,experimental-setup,model,trained with,batch size,model trained with batch size,0.7479425072669983
translation,148,93,experimental-setup,batch size,of,32,batch size of 32,0.6741614937782288
translation,148,93,experimental-setup,32,for,150 epochs,32 for 150 epochs,0.6069816946983337
translation,148,93,experimental-setup,mxnet deep learning framework,has,model,mxnet deep learning framework has model,0.49832993745803833
translation,148,93,experimental-setup,1.1.0,has,model,1.1.0 has model,0.5670629143714905
translation,148,93,experimental-setup,experimental setup,Implemented with,mxnet deep learning framework,experimental setup Implemented with mxnet deep learning framework,0.6464319229125977
translation,148,93,experimental-setup,experimental setup,has,model,experimental setup has model,0.5338840484619141
translation,148,99,experiments,statenet psi,with,different pre-trained models,statenet psi with different pre-trained models,0.6308340430259705
translation,148,8,model,statenet,has,universal dialogue state tracker,statenet has universal dialogue state tracker,0.5759886503219604
translation,148,8,model,model,propose,statenet,model propose statenet,0.6976816058158875
translation,148,34,model,universal dialogue state tracker,has,statenet,universal dialogue state tracker has statenet,0.5644349455833435
translation,148,34,model,model,propose,universal dialogue state tracker,model propose universal dialogue state tracker,0.6665436029434204
translation,148,95,results,statenet psi,performs,best,statenet psi performs best,0.5911552906036377
translation,148,95,results,best,among,all 3 models,best among all 3 models,0.5990620255470276
translation,148,95,results,statenet ps,has,outperforms,statenet ps has outperforms,0.6418894529342651
translation,148,95,results,outperforms,has,statenet,outperforms has statenet,0.631118893623352
translation,148,95,results,results,has,statenet ps,results has statenet ps,0.5620022416114807
translation,148,98,results,statenet psi,beats,all the mod-dst models,statenet psi beats all the mod-dst models,0.7283364534378052
translation,148,98,results,all the mod-dst models,has,joint acc. dstc2,all the mod-dst models has joint acc. dstc2,0.593640148639679
translation,148,98,results,neural belief tracker,has,nbt,neural belief tracker has nbt,0.544499933719635
translation,148,98,results,neural belief tracker,has,neural belief tracker,neural belief tracker has neural belief tracker,0.5780827403068542
translation,148,98,results,results,has,statenet psi,results has statenet psi,0.5304076671600342
translation,149,115,experimental-setup,experimental setup,has,truncation or zeropadding,experimental setup has truncation or zeropadding,0.5416317582130432
translation,149,116,experimental-setup,word embedding,pre-trained with,"word2vec ( mikolov et al. , 2013 )","word embedding pre-trained with word2vec ( mikolov et al. , 2013 )",0.720696747303009
translation,149,116,experimental-setup,"word2vec ( mikolov et al. , 2013 )",on,training sets,"word2vec ( mikolov et al. , 2013 ) on training sets",0.4543750584125519
translation,149,116,experimental-setup,training sets,of,douban and ecd,training sets of douban and ecd,0.5548161864280701
translation,149,116,experimental-setup,dimension,of,word vectors,dimension of word vectors,0.6073312163352966
translation,149,116,experimental-setup,word vectors,is,200,word vectors is 200,0.6032853722572327
translation,149,116,experimental-setup,experimental setup,has,word embedding,experimental setup has word embedding,0.5124366283416748
translation,149,117,experimental-setup,co-teaching framework,implemented with,tensorflow,co-teaching framework implemented with tensorflow,0.6973075270652771
translation,149,117,experimental-setup,experimental setup,has,co-teaching framework,experimental setup has co-teaching framework,0.5510251522064209
translation,149,118,experimental-setup,learning rates,in,dynamic margins,learning rates in dynamic margins,0.5313180685043335
translation,149,118,experimental-setup,learning rates,in,dynamic instance weighting,learning rates in dynamic instance weighting,0.4690660536289215
translation,149,118,experimental-setup,learning rates,in,dynamic data curriculum,learning rates in dynamic data curriculum,0.542549729347229
translation,149,118,experimental-setup,learning rates,set as,"0.001 , 0.0001 , and 0.0001","learning rates set as 0.001 , 0.0001 , and 0.0001",0.6270649433135986
translation,149,118,experimental-setup,dynamic data curriculum,set as,"0.001 , 0.0001 , and 0.0001","dynamic data curriculum set as 0.001 , 0.0001 , and 0.0001",0.6375573873519897
translation,149,118,experimental-setup,co-teaching,has,learning rates,co-teaching has learning rates,0.582693874835968
translation,149,118,experimental-setup,experimental setup,In,co-teaching,experimental setup In co-teaching,0.5611042380332947
translation,149,119,experimental-setup,200,in,co-teaching,200 in co-teaching,0.5866084098815918
translation,149,119,experimental-setup,co-teaching,with,smn,co-teaching with smn,0.6717823147773743
translation,149,119,experimental-setup,50,in,co-teaching,50 in co-teaching,0.57896888256073
translation,149,119,experimental-setup,co-teaching,with,dam,co-teaching with dam,0.6922171711921692
translation,149,119,experimental-setup,size,of,mini-batches,size of mini-batches,0.6529055833816528
translation,149,119,experimental-setup,experimental setup,choose,200,experimental setup choose 200,0.674637496471405
translation,149,119,experimental-setup,experimental setup,choose,50,experimental setup choose 50,0.6279971599578857
translation,149,120,experimental-setup,optimization,conducted using,stochastic gradient descent,optimization conducted using stochastic gradient descent,0.5140354037284851
translation,149,120,experimental-setup,stochastic gradient descent,with,adam algorithm,stochastic gradient descent with adam algorithm,0.6115962266921997
translation,149,120,experimental-setup,experimental setup,has,optimization,experimental setup has optimization,0.5174741744995117
translation,149,126,experiments,best strategy,for,smn,best strategy for smn,0.6419762969017029
translation,149,126,experiments,best strategy,teaching with,dynamic data curriculum,best strategy teaching with dynamic data curriculum,0.7738862037658691
translation,149,126,experiments,smn,teaching with,dynamic margins,smn teaching with dynamic margins,0.7258784174919128
translation,149,126,experiments,best strategy,teaching with,dynamic data curriculum,best strategy teaching with dynamic data curriculum,0.7738862037658691
translation,149,126,experiments,douban,has,best strategy,douban has best strategy,0.6199737191200256
translation,149,126,experiments,douban,has,best strategy,douban has best strategy,0.6199737191200256
translation,149,126,experiments,ecd,has,best strategy,ecd has best strategy,0.5774970054626465
translation,149,6,model,robust matching model,from,noisy training data,robust matching model from noisy training data,0.5345579981803894
translation,149,6,model,robust matching model,propose,general co-teaching framework,robust matching model propose general co-teaching framework,0.6066545248031616
translation,149,6,model,general co-teaching framework,with,three specific teaching strategies,general co-teaching framework with three specific teaching strategies,0.6135592460632324
translation,149,6,model,three specific teaching strategies,cover,teaching with loss functions,three specific teaching strategies cover teaching with loss functions,0.7088419198989868
translation,149,6,model,three specific teaching strategies,cover,teaching with data curriculum,three specific teaching strategies cover teaching with data curriculum,0.7383002042770386
translation,149,6,model,model,To learn,robust matching model,model To learn robust matching model,0.5732588768005371
translation,149,7,model,framework,simultaneously learn,two matching models,framework simultaneously learn two matching models,0.6190881729125977
translation,149,7,model,two matching models,with,independent training sets,two matching models with independent training sets,0.5845832228660583
translation,149,7,model,model,Under,framework,model Under framework,0.6366250514984131
translation,149,7,model,model,simultaneously learn,two matching models,model simultaneously learn two matching models,0.6462323069572449
translation,149,21,model,effectively learn existing matching models,from,noisy training data,effectively learn existing matching models from noisy training data,0.5467888712882996
translation,149,22,model,matching model,under,general co-teaching framework,matching model under general co-teaching framework,0.6249637007713318
translation,149,22,model,learning,has,matching model,learning has matching model,0.542110800743103
translation,149,22,model,model,propose,learning,model propose learning,0.7013421654701233
translation,149,23,model,two peer models,on,two i.i.d. training sets,two peer models on two i.i.d. training sets,0.5031280517578125
translation,149,23,model,two peer models,lets,two models,two peer models lets two models,0.7100299596786499
translation,149,23,model,two models,teach,each other,two models teach each other,0.6274664998054504
translation,149,23,model,each other,during,learning,each other during learning,0.7043463587760925
translation,149,106,model,smn,lets,each utterance,smn lets each utterance,0.7043265104293823
translation,149,106,model,smn,forms,matching vector,smn forms matching vector,0.6188257336616516
translation,149,106,model,each utterance,in,context,each utterance in context,0.503588080406189
translation,149,106,model,each utterance,interact with,response,each utterance interact with response,0.6734420657157898
translation,149,106,model,matching vector,for,pair,matching vector for pair,0.6712838411331177
translation,149,106,model,pair,through,cnns,pair through cnns,0.7067329287528992
translation,149,106,model,model,has,smn,model has smn,0.6165951490402222
translation,149,108,model,dam,performs,matching,dam performs matching,0.6483232975006104
translation,149,108,model,dam,represents,context,dam represents context,0.6910861134529114
translation,149,108,model,dam,represents,a response,dam represents a response,0.707736074924469
translation,149,108,model,matching,under,representation - matching -aggregation framework,matching under representation - matching -aggregation framework,0.6328664422035217
translation,149,108,model,a response,with,stacked self-attention and crossattention,a response with stacked self-attention and crossattention,0.5596757531166077
translation,149,108,model,model,has,dam,model has dam,0.6231600046157837
translation,149,30,results,most effective strategy,teaching with,dynamic margins,most effective strategy teaching with dynamic margins,0.7398014068603516
translation,149,30,results,most effective strategy,teaching with,dynamic data curriculum,most effective strategy teaching with dynamic data curriculum,0.7767921686172485
translation,149,30,results,dynamic margins,brings,2.8 % absolute improvement,dynamic margins brings 2.8 % absolute improvement,0.5780965685844421
translation,149,30,results,dynamic margins,brings,2.5 % absolute improvement,dynamic margins brings 2.5 % absolute improvement,0.5837454795837402
translation,149,30,results,2.8 % absolute improvement,to,smn,2.8 % absolute improvement to smn,0.5684213638305664
translation,149,30,results,2.8 % absolute improvement,to,smn,2.8 % absolute improvement to smn,0.5684213638305664
translation,149,30,results,2.5 % absolute improvement,to,dam,2.5 % absolute improvement to dam,0.5329252481460571
translation,149,30,results,2.5 % absolute improvement,to,dam,2.5 % absolute improvement to dam,0.5329252481460571
translation,149,30,results,dam,on,p@1,dam on p@1,0.6462566256523132
translation,149,30,results,dam,on,p@1,dam on p@1,0.6462566256523132
translation,149,30,results,dam,on,p@1,dam on p@1,0.6462566256523132
translation,149,30,results,dam,on,p@1,dam on p@1,0.6462566256523132
translation,149,30,results,best strategy,teaching with,dynamic data curriculum,best strategy teaching with dynamic data curriculum,0.7738862037658691
translation,149,30,results,dynamic data curriculum,brings,2.4 % absolute improvement,dynamic data curriculum brings 2.4 % absolute improvement,0.586065948009491
translation,149,30,results,dynamic data curriculum,brings,3.2 % absolute improvement,dynamic data curriculum brings 3.2 % absolute improvement,0.5848066806793213
translation,149,30,results,2.4 % absolute improvement,to,smn,2.4 % absolute improvement to smn,0.5647911429405212
translation,149,30,results,3.2 % absolute improvement,to,dam,3.2 % absolute improvement to dam,0.5372717380523682
translation,149,30,results,dam,on,p@1,dam on p@1,0.6462566256523132
translation,149,30,results,douban data,has,most effective strategy,douban data has most effective strategy,0.5813775062561035
translation,149,30,results,douban data,has,best strategy,douban data has best strategy,0.5936662554740906
translation,149,30,results,e-commerce data,has,best strategy,e-commerce data has best strategy,0.5453056693077087
translation,149,30,results,results,On,douban data,results On douban data,0.5180622935295105
translation,149,30,results,results,on,e-commerce data,results on e-commerce data,0.5425590872764587
translation,149,125,results,all teaching strategies,improve,original models,all teaching strategies improve original models,0.6645779609680176
translation,149,125,results,original models,on,both data sets,original models on both data sets,0.5278353691101074
translation,149,125,results,improvement,from,best strategy,improvement from best strategy,0.5327360033988953
translation,149,125,results,best strategy,is,statistically significant,best strategy is statistically significant,0.5220847129821777
translation,149,125,results,statistically significant,on,most metrics,statistically significant on most metrics,0.5102144479751587
translation,149,125,results,results,see that,all teaching strategies,results see that all teaching strategies,0.5634634494781494
translation,149,140,results,co-teaching,is,still effective,co-teaching is still effective,0.5622142553329468
translation,149,140,results,still effective,when starting from,two networks,still effective when starting from two networks,0.7167443633079529
translation,149,140,results,results,find that,co-teaching,results find that co-teaching,0.6114261150360107
translation,150,176,ablation-analysis,words,in,queries,words in queries,0.5317262411117554
translation,150,176,ablation-analysis,dist - 2,of,mmi and cas,dist - 2 of mmi and cas,0.6678629517555237
translation,150,176,ablation-analysis,mmi and cas,become,0.710 and 0.751,mmi and cas become 0.710 and 0.751,0.6052446365356445
translation,150,176,ablation-analysis,words,has,dist - 2,words has dist - 2,0.6728590726852417
translation,150,176,ablation-analysis,queries,has,dist - 2,queries has dist - 2,0.6723542213439941
translation,150,176,ablation-analysis,ablation analysis,removing,words,ablation analysis removing words,0.8003085851669312
translation,150,153,baselines,mmi seq2seq,with,maximum mutual information ( mmi ) objective in decoding,mmi seq2seq with maximum mutual information ( mmi ) objective in decoding,0.6249291896820068
translation,150,153,baselines,seq2seq,has,standard attention - based rnn encoder-decoder model,seq2seq has standard attention - based rnn encoder-decoder model,0.5394753217697144
translation,150,179,experiments,best model ( cas ),compared with,strong ir system ( ir - rerank ),best model ( cas ) compared with strong ir system ( ir - rerank ),0.6761595606803894
translation,150,179,experiments,best model ( cas ),compared with,previous state - of- the- art ( editvec ),best model ( cas ) compared with previous state - of- the- art ( editvec ),0.6222403049468994
translation,150,146,hyperparameters,skeleton generator,based on,bidirectional recurrent neural network,skeleton generator based on bidirectional recurrent neural network,0.6302759051322937
translation,150,146,hyperparameters,bidirectional recurrent neural network,with,500 lstm units,bidirectional recurrent neural network with 500 lstm units,0.6211350560188293
translation,150,146,hyperparameters,hyperparameters,implement,skeleton generator,hyperparameters implement skeleton generator,0.6127436757087708
translation,150,148,hyperparameters,word embedding size,set to,300,word embedding size set to 300,0.7209631204605103
translation,150,148,hyperparameters,hyperparameters,has,word embedding size,hyperparameters has word embedding size,0.494825541973114
translation,150,149,hyperparameters,response generator,encoder for,queries,response generator encoder for queries,0.7696505188941956
translation,150,149,hyperparameters,response generator,encoder for,skeletons,response generator encoder for skeletons,0.773825466632843
translation,150,149,hyperparameters,decoder,are,three two -layer recurrent neural networks,decoder are three two -layer recurrent neural networks,0.6196308732032776
translation,150,149,hyperparameters,three two -layer recurrent neural networks,with,500 lstm units,three two -layer recurrent neural networks with 500 lstm units,0.6330646276473999
translation,150,149,hyperparameters,three two -layer recurrent neural networks,where,both encoders,three two -layer recurrent neural networks where both encoders,0.6317814588546753
translation,150,149,hyperparameters,500 lstm units,where,both encoders,500 lstm units where both encoders,0.5618759989738464
translation,150,149,hyperparameters,both encoders,are,bidirectional,both encoders are bidirectional,0.6210798025131226
translation,150,149,hyperparameters,hyperparameters,For,response generator,hyperparameters For response generator,0.5751087069511414
translation,150,150,hyperparameters,"dropout ( srivastava et al. , 2014 )",to alleviate,overfitting,"dropout ( srivastava et al. , 2014 ) to alleviate overfitting",0.6264107823371887
translation,150,151,hyperparameters,dropout rate,set to,0.3,dropout rate set to 0.3,0.6678073406219482
translation,150,151,hyperparameters,0.3,across,different layers,0.3 across different layers,0.7092939615249634
translation,150,151,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,150,9,model,new framework,exploits,retrieval results,new framework exploits retrieval results,0.7132285237312317
translation,150,9,model,retrieval results,via,skeleton- to- response paradigm,retrieval results via skeleton- to- response paradigm,0.6121874451637268
translation,150,9,model,model,propose,new framework,model propose new framework,0.7318839430809021
translation,150,10,model,skeleton,extracted from,retrieved dialogues,skeleton extracted from retrieved dialogues,0.6156071424484253
translation,150,38,model,skeleton generator,extracts,response skeleton,skeleton generator extracts response skeleton,0.6320745348930359
translation,150,38,model,response skeleton,by detecting and removing,unwanted words,response skeleton by detecting and removing unwanted words,0.7297049760818481
translation,150,38,model,unwanted words,in,retrieved response,unwanted words in retrieved response,0.5359655022621155
translation,150,38,model,model,has,skeleton generator,model has skeleton generator,0.5610936880111694
translation,150,43,model,skeleton generation and the response generation,in,multi-task learning fashion,skeleton generation and the response generation in multi-task learning fashion,0.5303446054458618
translation,150,46,model,response skeleton,results of,ir systems,response skeleton results of ir systems,0.734749436378479
translation,150,46,model,response skeleton,for guiding,response generation,response skeleton for guiding response generation,0.7558660507202148
translation,150,46,model,model,construct,response skeleton,model construct response skeleton,0.7483450174331665
translation,150,47,model,skeleton- to- response paradigm,helps reduce,search space,skeleton- to- response paradigm helps reduce search space,0.7335025072097778
translation,150,47,model,skeleton- to- response paradigm,provides,useful elements,skeleton- to- response paradigm provides useful elements,0.6222947239875793
translation,150,47,model,search space,of,possible responses,search space of possible responses,0.5801423192024231
translation,150,47,model,useful elements,missing in,given query,useful elements missing in given query,0.7036242485046387
translation,150,47,model,model,has,skeleton- to- response paradigm,model has skeleton- to- response paradigm,0.5169604420661926
translation,150,147,model,hidden states,from,both directions,hidden states from both directions,0.5754882097244263
translation,150,147,model,model,concatenate,hidden states,model concatenate hidden states,0.7452241778373718
translation,150,171,results,of our models,surpass,all other methods,of our models surpass all other methods,0.6418374180793762
translation,150,171,results,our cascaded model ( cas ),gives,best performance,our cascaded model ( cas ) gives best performance,0.5810039639472961
translation,150,171,results,best performance,according to,human evaluation,best performance according to human evaluation,0.6261820197105408
translation,150,172,results,skp model,use of,skeletons,skp model use of skeletons,0.6508650779724121
translation,150,172,results,skeletons,brings,significant performance gain,skeletons brings significant performance gain,0.6649159789085388
translation,150,173,results,generative models,achieve,significantly better diversity,generative models achieve significantly better diversity,0.5896609425544739
translation,150,173,results,significantly better diversity,by,retrieval results,significantly better diversity by retrieval results,0.5677066445350647
translation,150,173,results,significantly better diversity,use of,retrieval results,significantly better diversity use of retrieval results,0.6271053552627563
translation,150,173,results,dist - 1&2 metrics,has,generative models,dist - 1&2 metrics has generative models,0.5647156238555908
translation,150,174,results,retrieval method,yields,highest diversity,retrieval method yields highest diversity,0.7087907195091248
translation,150,174,results,results,has,retrieval method,results has retrieval method,0.5900374054908752
translation,150,175,results,model,of,mmi,model of mmi,0.6393855810165405
translation,150,175,results,model,gives,strong diversity,model gives strong diversity,0.6518239974975586
translation,150,175,results,mmi,gives,strong diversity,mmi gives strong diversity,0.6129714846611023
translation,150,175,results,results,has,model,results has model,0.5339115858078003
translation,150,180,results,cas model,significantly boosts,performance,cas model significantly boosts performance,0.7371589541435242
translation,150,180,results,performance,when,query similarity,performance when query similarity,0.6103500723838806
translation,150,180,results,query similarity,is,relatively low,query similarity is relatively low,0.5719208121299744
translation,150,180,results,results,has,cas model,results has cas model,0.5382773876190186
translation,151,8,ablation-analysis,un-annotated data,in,meta-way,un-annotated data in meta-way,0.5406450033187866
translation,151,8,ablation-analysis,un-annotated data,amount of,dialogue state annotations,un-annotated data amount of dialogue state annotations,0.6812960505485535
translation,151,8,ablation-analysis,ablation analysis,by leveraging,un-annotated data,ablation analysis by leveraging un-annotated data,0.6977654099464417
translation,151,75,ablation-analysis,w/o entropy,has,same regularization loss function,w/o entropy has same regularization loss function,0.528132975101471
translation,151,75,ablation-analysis,acc,increases,8.98 %,acc increases 8.98 %,0.6885815262794495
translation,151,75,ablation-analysis,same regularization loss function,as,sedst,same regularization loss function as sedst,0.5347768068313599
translation,151,75,ablation-analysis,sedst,in,meta-training,sedst in meta-training,0.5661482214927673
translation,151,75,ablation-analysis,w/o entropy,has,acc,w/o entropy has acc,0.6085675954818726
translation,151,75,ablation-analysis,w/o entropy,has,same regularization loss function,w/o entropy has same regularization loss function,0.528132975101471
translation,151,75,ablation-analysis,emr,has,8.0 %,emr has 8.0 %,0.5774598717689514
translation,151,75,ablation-analysis,emr,has,same regularization loss function,emr has same regularization loss function,0.5150110125541687
translation,151,75,ablation-analysis,ablation analysis,has,w/o entropy,ablation analysis has w/o entropy,0.5581454038619995
translation,151,88,baselines,maml algorithm and entropy regularization,on top of,sedst,maml algorithm and entropy regularization on top of sedst,0.7289174795150757
translation,151,88,baselines,sedst,for,lowresource dialogue tasks,sedst for lowresource dialogue tasks,0.579318642616272
translation,151,88,baselines,baselines,investigate,maml algorithm and entropy regularization,baselines investigate maml algorithm and entropy regularization,0.5925742387771606
translation,151,64,experimental-setup,single - layer gru networks,with,hidden size,single - layer gru networks with hidden size,0.60595703125
translation,151,64,experimental-setup,hidden size,of,50,hidden size of 50,0.673846423625946
translation,151,64,experimental-setup,hidden size,to be,encoder,hidden size to be encoder,0.5827797651290894
translation,151,64,experimental-setup,hidden size,to be,decoder,hidden size to be decoder,0.6137944459915161
translation,151,64,experimental-setup,experimental setup,choose,single - layer gru networks,experimental setup choose single - layer gru networks,0.6711639165878296
translation,151,65,experimental-setup,embeddings,initialized by,"glove ( pennington et al. , 2014 )","embeddings initialized by glove ( pennington et al. , 2014 )",0.6935442090034485
translation,151,65,experimental-setup,size,of,word embedding,size of word embedding,0.5581256747245789
translation,151,65,experimental-setup,word embedding,set to,50,word embedding set to 50,0.6318463683128357
translation,151,65,experimental-setup,experimental setup,has,embeddings,experimental setup has embeddings,0.5157045722007751
translation,151,65,experimental-setup,experimental setup,has,size,experimental setup has size,0.5329226851463318
translation,151,66,experimental-setup,"adam ( kingma and ba , 2015 )",with,learning rate,"adam ( kingma and ba , 2015 ) with learning rate",0.5996310710906982
translation,151,66,experimental-setup,learning rate,of,0.003,learning rate of 0.003,0.6038756370544434
translation,151,66,experimental-setup,learning rate,of,0.0015,learning rate of 0.0015,0.5938100218772888
translation,151,66,experimental-setup,0.003,in,meta-training,0.003 in meta-training,0.4317718744277954
translation,151,66,experimental-setup,0.0015,in,metatesting,0.0015 in metatesting,0.45979735255241394
translation,151,66,experimental-setup,experimental setup,optimized using,"adam ( kingma and ba , 2015 )","experimental setup optimized using adam ( kingma and ba , 2015 )",0.6984754204750061
translation,151,67,experimental-setup,experimental setup,implemented in,pytorch,experimental setup implemented in pytorch,0.7101436853408813
translation,151,68,experiments,medst 's semi-supervision,randomly choose,720 unlabelled dialogues,medst 's semi-supervision randomly choose 720 unlabelled dialogues,0.63975590467453
translation,151,68,experiments,720 unlabelled dialogues,for,each domain,720 unlabelled dialogues for each domain,0.5412607192993164
translation,151,68,experiments,720 unlabelled dialogues,with,batch size,720 unlabelled dialogues with batch size,0.5615750551223755
translation,151,68,experiments,each domain,in,meta-training,each domain in meta-training,0.5254611968994141
translation,151,68,experiments,batch size,of,support dataset 32,batch size of support dataset 32,0.6456964015960693
translation,151,68,experiments,batch size,of,query dataset 8,batch size of query dataset 8,0.6299949288368225
translation,151,4,model,based semi-supervised explicit dialogue state tracker ( sedst ),for,neural dialogue generation,based semi-supervised explicit dialogue state tracker ( sedst ) for neural dialogue generation,0.5820072293281555
translation,151,6,model,meta-training,with,adequate unlabelled data,meta-training with adequate unlabelled data,0.5837247371673584
translation,151,6,model,meta-testing,with,few annotated data,meta-testing with few annotated data,0.6229948997497559
translation,151,6,model,few annotated data,by,supervised learning,few annotated data by supervised learning,0.5157193541526794
translation,151,6,model,medst,has,two core steps,medst has two core steps,0.6101880073547363
translation,151,6,model,model,has,medst,model has medst,0.6186000108718872
translation,151,7,model,sedst,via,entropy regularization,sedst via entropy regularization,0.6719119548797607
translation,151,7,model,semi-supervised learning frameworks,based on,model- agnostic meta-learning ( maml ),semi-supervised learning frameworks based on model- agnostic meta-learning ( maml ),0.6594234108924866
translation,151,7,model,reduce,has,amount of required intermediate state labelling,reduce has amount of required intermediate state labelling,0.5628625154495239
translation,151,7,model,model,enhance,sedst,model enhance sedst,0.6979727149009705
translation,151,7,model,model,investigate,semi-supervised learning frameworks,model investigate semi-supervised learning frameworks,0.6039292812347412
translation,151,20,model,"maml - based ( finn et al. , 2017 ) semisupervision architecture",for,low-resource,"maml - based ( finn et al. , 2017 ) semisupervision architecture for low-resource",0.5558006167411804
translation,151,20,model,model,propose,"maml - based ( finn et al. , 2017 ) semisupervision architecture","model propose maml - based ( finn et al. , 2017 ) semisupervision architecture",0.6365487575531006
translation,151,73,results,medst,achieves,great improvement,medst achieves great improvement,0.6850144863128662
translation,151,73,results,great improvement,with,different proportions,great improvement with different proportions,0.6647395491600037
translation,151,73,results,different proportions,of,labelled data,different proportions of labelled data,0.6197195649147034
translation,151,76,results,improvement,benefits from,maml algorithm,improvement benefits from maml algorithm,0.6632602214813232
translation,151,76,results,improvement,maximize,sensitivity,improvement maximize sensitivity,0.7537890672683716
translation,151,76,results,maml algorithm,tries to build,internal representation,maml algorithm tries to build internal representation,0.6595579981803894
translation,151,76,results,maml algorithm,maximize,sensitivity,maml algorithm maximize sensitivity,0.7721178531646729
translation,151,76,results,internal representation,of,multiple tasks,internal representation of multiple tasks,0.5729145407676697
translation,151,76,results,sensitivity,of,loss function,sensitivity of loss function,0.6049503684043884
translation,151,76,results,sensitivity,applied to,new tasks,sensitivity applied to new tasks,0.7026649713516235
translation,151,76,results,results,has,improvement,results has improvement,0.6248279809951782
translation,151,77,results,maml,remove,acc,maml remove acc,0.6889621615409851
translation,151,77,results,maml,has,same framework,maml has same framework,0.6252867579460144
translation,151,77,results,acc,increases,1.83 %,acc increases 1.83 %,0.6897298693656921
translation,151,77,results,one-stage training procedure,with,sedst,one-stage training procedure with sedst,0.6484561562538147
translation,151,77,results,maml,has,acc,maml has acc,0.6512900590896606
translation,151,77,results,results,w/o,maml,results w/o maml,0.3352181613445282
translation,151,77,results,results,has,maml,results has maml,0.5715363621711731
translation,151,78,results,improvement,due to,entropy regularization,improvement due to entropy regularization,0.6744785904884338
translation,151,78,results,entropy regularization,takes account of,uncertainty,entropy regularization takes account of uncertainty,0.6303706765174866
translation,151,78,results,uncertainty,has,of unlabelled data,uncertainty has of unlabelled data,0.5491273403167725
translation,151,78,results,results,shows,improvement,results shows improvement,0.7450517416000366
translation,151,84,results,medst,improves,ability,medst improves ability,0.7847118973731995
translation,151,84,results,ability,of,new domain adaption,ability of new domain adaption,0.595775306224823
translation,151,84,results,results,see,medst,results see medst,0.6294096112251282
translation,151,85,results,three new domains,achieve,18.97 % higher,three new domains achieve 18.97 % higher,0.6167993545532227
translation,151,85,results,three new domains,achieve,13.93 % higher,three new domains achieve 13.93 % higher,0.6168978214263916
translation,151,85,results,18.97 % higher,in,acc,18.97 % higher in acc,0.5284019708633423
translation,151,85,results,13.93 % higher,in,emr,13.93 % higher in emr,0.5406501293182373
translation,151,85,results,results,has,three new domains,results has three new domains,0.5364131331443787
translation,151,112,results,medst,greatly improve,ability of expanding to new domains,medst greatly improve ability of expanding to new domains,0.7216399312019348
translation,151,112,results,results,has,medst,results has medst,0.5907823443412781
translation,152,88,ablation-analysis,layers,to,seq2seq,layers to seq2seq,0.5688885450363159
translation,152,88,ablation-analysis,drop,in,performance,drop in performance,0.5523719787597656
translation,152,88,ablation-analysis,drop,suggesting,overly powerful model,drop suggesting overly powerful model,0.7362463474273682
translation,152,88,ablation-analysis,overly powerful model,for,small dataset size,overly powerful model for small dataset size,0.5595388412475586
translation,152,88,ablation-analysis,ablation analysis,Adding,layers,ablation analysis Adding layers,0.7080048322677612
translation,152,59,hyperparameters,adam optimizer,applying,"dropout ( hinton et al. , 2012 )","adam optimizer applying dropout ( hinton et al. , 2012 )",0.5569362640380859
translation,152,59,hyperparameters,"dropout ( hinton et al. , 2012 )",as,regularizer,"dropout ( hinton et al. , 2012 ) as regularizer",0.4591834843158722
translation,152,59,hyperparameters,regularizer,to,input and output,regularizer to input and output,0.5808210372924805
translation,152,59,hyperparameters,input and output,of,lstm,input and output of lstm,0.6064214110374451
translation,152,59,hyperparameters,hyperparameters,trained using,cross-entropy loss,hyperparameters trained using cross-entropy loss,0.6893611550331116
translation,152,61,hyperparameters,dropout keep rates,ranged from,0.75 to 0.95,dropout keep rates ranged from 0.75 to 0.95,0.6102406978607178
translation,152,61,hyperparameters,hyperparameters,has,dropout keep rates,hyperparameters has dropout keep rates,0.504219651222229
translation,152,62,hyperparameters,word embeddings,size,300,word embeddings size 300,0.7333933711051941
translation,152,62,hyperparameters,cell sizes,set to,353,cell sizes set to 353,0.6910582780838013
translation,152,62,hyperparameters,hyperparameters,used,word embeddings,hyperparameters used word embeddings,0.5552517175674438
translation,152,63,hyperparameters,gradient clipping,with,clipvalue,gradient clipping with clipvalue,0.6434056162834167
translation,152,63,hyperparameters,clipvalue,of,10,clipvalue of 10,0.6689322590827942
translation,152,63,hyperparameters,gradient explosions,during,training,gradient explosions during training,0.7004767656326294
translation,152,63,hyperparameters,hyperparameters,applied,gradient clipping,hyperparameters applied gradient clipping,0.6689514517784119
translation,152,64,hyperparameters,randomly initialized,from,uniform unit-scaled distribution,randomly initialized from uniform unit-scaled distribution,0.5762899518013
translation,152,64,hyperparameters,uniform unit-scaled distribution,in,style,uniform unit-scaled distribution in style,0.5184751749038696
translation,152,64,hyperparameters,hyperparameters,has,"attention , output parameters , word embeddings , and lstm weights","hyperparameters has attention , output parameters , word embeddings , and lstm weights",0.48716431856155396
translation,152,6,model,selective attention,to,dialogue history,selective attention to dialogue history,0.5824606418609619
translation,152,6,model,learning,has,selective attention,learning has selective attention,0.5582193732261658
translation,152,6,model,model,bypassing,explicit representation,model bypassing explicit representation,0.7163915634155273
translation,152,93,model,class of neural models,for,task - oriented dialogue,class of neural models for task - oriented dialogue,0.5941699147224426
translation,152,93,model,able to outperform,has,other more intricately designed neural architectures,able to outperform has other more intricately designed neural architectures,0.5758794546127319
translation,152,93,model,model,iteratively built out,class of neural models,model iteratively built out class of neural models,0.7474543452262878
translation,152,87,results,1 - layer vanilla encoder-decoder,able to,significantly outperform,1 - layer vanilla encoder-decoder able to significantly outperform,0.5095955729484558
translation,152,87,results,memnn,in,per-response and per-dialogue accuracies,memnn in per-response and per-dialogue accuracies,0.5450290441513062
translation,152,87,results,significantly outperform,has,memnn,significantly outperform has memnn,0.5973766446113586
translation,152,87,results,results,see that,1 - layer vanilla encoder-decoder,results see that 1 - layer vanilla encoder-decoder,0.6007124781608582
translation,152,89,results,attention - based decoding,to,vanilla model,attention - based decoding to vanilla model,0.545359194278717
translation,152,89,results,attention - based decoding,increases,bleu,attention - based decoding increases bleu,0.6870190501213074
translation,152,89,results,vanilla model,increases,bleu,vanilla model increases bleu,0.7304217219352722
translation,152,89,results,results,Adding,attention - based decoding,results Adding attention - based decoding,0.5748652815818787
translation,152,90,results,our attention - based entity copy mechanism,achieves,substantial increases,our attention - based entity copy mechanism achieves substantial increases,0.7267900109291077
translation,152,90,results,substantial increases,in,perresponse accuracies,substantial increases in perresponse accuracies,0.5685610175132751
translation,152,90,results,substantial increases,in,entity f 1,substantial increases in entity f 1,0.6034414172172546
translation,152,90,results,results,Adding,our attention - based entity copy mechanism,results Adding our attention - based entity copy mechanism,0.6337859034538269
translation,152,91,results,entity class features to + copy,achieves,our bestperforming model,entity class features to + copy achieves our bestperforming model,0.6480662226676941
translation,152,91,results,our bestperforming model,in terms of,per-response accuracy,our bestperforming model in terms of per-response accuracy,0.6852139234542847
translation,152,91,results,our bestperforming model,in terms of,entity f 1,our bestperforming model in terms of entity f 1,0.7333959341049194
translation,152,92,results,6.9 % increase,in,per-response accuracy,6.9 % increase in per-response accuracy,0.5320291519165039
translation,152,92,results,per-response accuracy,on,dstc2,per-response accuracy on dstc2,0.5595514178276062
translation,152,92,results,dstc2,over,memnn,dstc2 over memnn,0.692426860332489
translation,152,92,results,on par,with,performance,on par with performance,0.6817876696586609
translation,152,92,results,performance,of,gmemnn,performance of gmemnn,0.5978856086730957
translation,153,165,ablation-analysis,benefit,of,snapshot learning,benefit of snapshot learning,0.5769508481025696
translation,153,165,ablation-analysis,snapshot learning,mainly comes from,more discriminative and robust subspace representation,snapshot learning mainly comes from more discriminative and robust subspace representation,0.638102114200592
translation,153,165,ablation-analysis,more discriminative and robust subspace representation,learned from,heuristically labelled companion signals,more discriminative and robust subspace representation learned from heuristically labelled companion signals,0.6788707971572876
translation,153,165,ablation-analysis,heuristically labelled companion signals,facilitates,optimisation,heuristically labelled companion signals facilitates optimisation,0.5251147747039795
translation,153,165,ablation-analysis,optimisation,of,final target objective,optimisation of final target objective,0.5741266012191772
translation,153,165,ablation-analysis,ablation analysis,suggested that,benefit,ablation analysis suggested that benefit,0.6936594843864441
translation,153,137,baselines,first block belief state representation,compare,effect,first block belief state representation compare effect,0.6582457423210144
translation,153,117,hyperparameters,gradient clipping,set to,1,gradient clipping set to 1,0.626135528087616
translation,153,117,hyperparameters,hyperparameters,has,gradient clipping,hyperparameters has gradient clipping,0.5166950225830078
translation,153,118,hyperparameters,hidden layer sizes,set to,50,hidden layer sizes set to 50,0.721225917339325
translation,153,118,hyperparameters,randomly initialised,between,- 0.3 and 0.3,randomly initialised between - 0.3 and 0.3,0.6773866415023804
translation,153,118,hyperparameters,- 0.3 and 0.3,including,word embeddings,- 0.3 and 0.3 including word embeddings,0.7198673486709595
translation,153,118,hyperparameters,hyperparameters,has,hidden layer sizes,hyperparameters has hidden layer sizes,0.48641785979270935
translation,153,118,hyperparameters,hyperparameters,has,weights,hyperparameters has weights,0.5201958417892456
translation,153,119,hyperparameters,vocabulary size,is,500,vocabulary size is 500,0.61527019739151
translation,153,119,hyperparameters,vocabulary size,around,500,vocabulary size around 500,0.7043612003326416
translation,153,119,hyperparameters,500,for,both input and output,500 for both input and output,0.6681917905807495
translation,153,119,hyperparameters,both input and output,in which,rare words and words,both input and output in which rare words and words,0.5979041457176208
translation,153,119,hyperparameters,rare words and words,can be,delexicalised,rare words and words can be delexicalised,0.6934695243835449
translation,153,119,hyperparameters,hyperparameters,has,vocabulary size,hyperparameters has vocabulary size,0.5002733469009399
translation,153,121,hyperparameters,the trained models,with,average log probability,the trained models with average log probability,0.6712508797645569
translation,153,121,hyperparameters,average log probability,of,tokens,average log probability of tokens,0.6432296633720398
translation,153,121,hyperparameters,tokens,in,sentence,tokens in sentence,0.5188359618186951
translation,153,121,hyperparameters,hyperparameters,decode,the trained models,hyperparameters decode the trained models,0.6831824779510498
translation,153,5,model,different ways,to represent and aggregate,source information,different ways to represent and aggregate source information,0.744856595993042
translation,153,5,model,source information,in,endto-end neural dialogue system framework,source information in endto-end neural dialogue system framework,0.5128048658370972
translation,153,5,model,model,study,various model architectures,model study various model architectures,0.5700898170471191
translation,153,5,model,model,study,different ways,model study different ways,0.5561780333518982
translation,153,6,model,snapshot learning,facilitate,learning,snapshot learning facilitate learning,0.6219807863235474
translation,153,6,model,learning,applying,companion cross-entropy objective function,learning applying companion cross-entropy objective function,0.6573754549026489
translation,153,6,model,from supervised sequential signals,applying,companion cross-entropy objective function,from supervised sequential signals applying companion cross-entropy objective function,0.634486734867096
translation,153,6,model,companion cross-entropy objective function,to,conditioning vector,companion cross-entropy objective function to conditioning vector,0.5354390144348145
translation,153,6,model,learning,has,from supervised sequential signals,learning has from supervised sequential signals,0.5377293825149536
translation,153,14,model,conditional lstms,in,generation component,conditional lstms in generation component,0.502515435218811
translation,153,14,model,generation component,of,neural network ( nn ) - based dialogue systems,generation component of neural network ( nn ) - based dialogue systems,0.5586756467819214
translation,153,14,model,model,study,conditional lstms,model study conditional lstms,0.5491904020309448
translation,153,14,model,model,use of,conditional lstms,model use of conditional lstms,0.6047952175140381
translation,153,23,model,snapshot learning,heuristically applying,companion supervision signals,snapshot learning heuristically applying companion supervision signals,0.7724463939666748
translation,153,23,model,companion supervision signals,to,subset of the conditioning vector,companion supervision signals to subset of the conditioning vector,0.5799484848976135
translation,153,53,model,dialogue,as,source to target sequence transduction problem,dialogue as source to target sequence transduction problem,0.5510420799255371
translation,153,53,model,source to target sequence transduction problem,modelled by,sequence - to-sequence architecture,source to target sequence transduction problem modelled by sequence - to-sequence architecture,0.7143872380256653
translation,153,53,model,dialogue history,modelled by,belief tracker,dialogue history modelled by belief tracker,0.7341536283493042
translation,153,53,model,current database search outcome,modelled by,database operator,current database search outcome modelled by database operator,0.6848528981208801
translation,153,53,model,model,casts,dialogue,model casts dialogue,0.748975932598114
translation,153,25,results,snapshot learning,offers,several benefits,snapshot learning offers several benefits,0.7099849581718445
translation,153,25,results,snapshot learning,learns,discriminative and robust feature representations,snapshot learning learns discriminative and robust feature representations,0.6841521859169006
translation,153,25,results,snapshot learning,alleviates,vanishing gradient problem,snapshot learning alleviates vanishing gradient problem,0.6104069948196411
translation,153,25,results,snapshot learning,appears to learn,transparent and interpretable subspaces,snapshot learning appears to learn transparent and interpretable subspaces,0.6625357866287231
translation,153,25,results,transparent and interpretable subspaces,of,conditioning vector,transparent and interpretable subspaces of conditioning vector,0.5651882290840149
translation,153,25,results,consistently improves,has,performance,consistently improves has performance,0.5987896919250488
translation,153,134,results,first observation,is that,snapshot learning,first observation is that snapshot learning,0.6672947406768799
translation,153,134,results,consistently improves,on,most metrics,consistently improves on most metrics,0.5199145078659058
translation,153,134,results,most metrics,regardless of,model architecture,most metrics regardless of model architecture,0.6635608077049255
translation,153,134,results,snapshot learning,has,consistently improves,snapshot learning has consistently improves,0.6039560437202454
translation,153,134,results,results,has,first observation,results has first observation,0.5561463832855225
translation,153,138,results,succinct representation,is,better ( summary > full ),succinct representation is better ( summary > full ),0.567259669303894
translation,153,141,results,language model type ( lm ) and memory type ( mem ) networks,perform,better,language model type ( lm ) and memory type ( mem ) networks perform better,0.6406739950180054
translation,153,141,results,better,in terms of,bleu score and slot matching rate,better in terms of bleu score and slot matching rate,0.6578658223152161
translation,153,141,results,hybrid type ( hybrid ) networks,achieve,higher task success,hybrid type ( hybrid ) networks achieve higher task success,0.6552655100822449
translation,153,141,results,results,shows that,language model type ( lm ) and memory type ( mem ) networks,results shows that language model type ( lm ) and memory type ( mem ) networks,0.6308985948562622
translation,153,145,results,attention mechanism,improves,all the three architectures,attention mechanism improves all the three architectures,0.6281806230545044
translation,153,145,results,all the three architectures,on,task success rate,all the three architectures on task success rate,0.49402549862861633
translation,153,145,results,all the three architectures,not,bleu scores,all the three architectures not bleu scores,0.6410626173019409
translation,153,145,results,results,found,attention mechanism,results found attention mechanism,0.6376927495002747
translation,153,163,results,hybrid type model,did not rank,highest,hybrid type model did not rank highest,0.7268072962760925
translation,153,163,results,highest,on,all metrics,highest on all metrics,0.5193137526512146
translation,153,164,results,snapshot learning,provided,gains,snapshot learning provided gains,0.6703389286994934
translation,153,164,results,gains,on,virtually all metrics,gains on virtually all metrics,0.5255568027496338
translation,153,164,results,virtually all metrics,regardless of,architecture,virtually all metrics regardless of architecture,0.724031388759613
translation,153,164,results,results,has,snapshot learning,results has snapshot learning,0.5539976358413696
translation,154,100,experimental-setup,single tesla p100 gpu,with,batch size,single tesla p100 gpu with batch size,0.6260975003242493
translation,154,100,experimental-setup,batch size,of,512,batch size of 512,0.6329059600830078
translation,154,105,experimental-setup,decoding,use,beam size,decoding use beam size,0.66057288646698
translation,154,105,experimental-setup,decoding,apply,trigram avoidance,decoding apply trigram avoidance,0.634983479976654
translation,154,105,experimental-setup,beam size,of,2,beam size of 2,0.690122127532959
translation,154,105,experimental-setup,2,to search for,optimal results,2 to search for optimal results,0.528304398059845
translation,154,105,experimental-setup,trigram avoidance,to fight,trigram-level repetition,trigram avoidance to fight trigram-level repetition,0.586172878742218
translation,154,105,experimental-setup,experimental setup,For,decoding,experimental setup For decoding,0.5741471648216248
translation,154,105,experimental-setup,experimental setup,apply,trigram avoidance,experimental setup apply trigram avoidance,0.623239278793335
translation,154,106,experimental-setup,training,train,act generator,training train act generator,0.736397922039032
translation,154,106,experimental-setup,training,optimize,uncertainty loss,training optimize uncertainty loss,0.7107676863670349
translation,154,106,experimental-setup,act generator,for,10 epochs,act generator for 10 epochs,0.61713045835495
translation,154,106,experimental-setup,act generator,optimize,uncertainty loss,act generator optimize uncertainty loss,0.7384266257286072
translation,154,106,experimental-setup,10 epochs,for,warmup,10 epochs for warmup,0.6064676642417908
translation,154,106,experimental-setup,uncertainty loss,with,adam optimizer,uncertainty loss with adam optimizer,0.5472975373268127
translation,154,106,experimental-setup,experimental setup,During,training,experimental setup During training,0.6835477948188782
translation,154,9,model,neural co-generation model,that generates,dialogue acts and responses concurrently,neural co-generation model that generates dialogue acts and responses concurrently,0.6769809126853943
translation,154,9,model,model,propose,neural co-generation model,model propose neural co-generation model,0.6435933709144592
translation,154,10,model,act generation module,preserves,semantic structures,act generation module preserves semantic structures,0.6858339905738831
translation,154,10,model,semantic structures,of,multi-domain dialogue acts,semantic structures of multi-domain dialogue acts,0.5253472924232483
translation,154,10,model,response generation module,dynamically attends to,different acts,response generation module dynamically attends to different acts,0.7388033866882324
translation,154,11,model,two modules,using,uncertainty loss,two modules using uncertainty loss,0.6999498009681702
translation,154,11,model,jointly,using,uncertainty loss,jointly using uncertainty loss,0.7110490202903748
translation,154,11,model,uncertainty loss,to adjust,task weights,uncertainty loss to adjust task weights,0.6765038371086121
translation,154,11,model,two modules,has,jointly,two modules has jointly,0.6280980110168457
translation,154,11,model,task weights,has,adaptively,task weights has adaptively,0.5721844434738159
translation,154,11,model,model,train,two modules,model train two modules,0.6968069076538086
translation,154,104,model,3 - layer transformer,with,4 heads,3 - layer transformer with 4 heads,0.6545451879501343
translation,154,104,model,4 heads,for,multi-head attention layer,4 heads for multi-head attention layer,0.5680656433105469
translation,154,104,model,model,use,3 - layer transformer,model use 3 - layer transformer,0.6213507652282715
translation,154,135,model,dynamic act attention mechanism,helps,response generator,dynamic act attention mechanism helps response generator,0.608478307723999
translation,154,135,model,response generator,capture,local information,response generator capture local information,0.7920433282852173
translation,154,135,model,local information,by attending to,different acts,local information by attending to different acts,0.7076626420021057
translation,154,135,model,sequential acts,has,dynamic act attention mechanism,sequential acts has dynamic act attention mechanism,0.5640278458595276
translation,154,135,model,model,by utilizing,sequential acts,model by utilizing sequential acts,0.6710700392723083
translation,154,139,model,joint training,makes,two stages,joint training makes two stages,0.6208800673484802
translation,154,139,model,two stages,interact with,each other,two stages interact with each other,0.6743208169937134
translation,154,139,model,each other,easing,error propagation,each other easing error propagation,0.6726517081260681
translation,154,139,model,error propagation,of,pipeline systems,error propagation of pipeline systems,0.6028999090194702
translation,154,139,model,model,has,joint training,model has joint training,0.559281051158905
translation,154,116,results,success,of,marco,success of marco,0.6391592621803284
translation,154,116,results,marco,by modeling,act prediction,marco by modeling act prediction,0.8075480461120605
translation,154,116,results,marco,by modeling,training it,marco by modeling training it,0.8114768862724304
translation,154,116,results,act prediction,as,generation problem,act prediction as generation problem,0.5365548729896545
translation,154,116,results,training it,jointly with,response generation,training it jointly with response generation,0.5867772698402405
translation,154,116,results,results,confirm,success,results confirm success,0.5282280445098877
translation,154,117,results,marco,shows,inferior bleu performance,marco shows inferior bleu performance,0.6836010217666626
translation,154,117,results,inferior bleu performance,to,two hdsa models,inferior bleu performance to two hdsa models,0.5367482304573059
translation,154,126,results,separate generator,performs,much better,separate generator performs much better,0.6290398240089417
translation,154,126,results,separate generator,comparable with,transformer,separate generator comparable with transformer,0.7047564387321472
translation,154,126,results,much better,than,bilstm and word - cnn,much better than bilstm and word - cnn,0.5719857811927795
translation,154,126,results,separate generator,has,transformer ( gen ),separate generator has transformer ( gen ),0.567263126373291
translation,154,126,results,results,notice that,separate generator,results notice that separate generator,0.6365615129470825
translation,154,127,results,marco,manages to show,best performance,marco manages to show best performance,0.7682600021362305
translation,154,127,results,response generator,has,marco,response generator has marco,0.5818906426429749
translation,154,133,results,pipeline 2,with,dynamic act attention,pipeline 2 with dynamic act attention,0.6099551916122437
translation,154,133,results,pipeline 2,without it in,all metrics,pipeline 2 without it in all metrics,0.6594498753547668
translation,154,133,results,dynamic act attention,superior to,pipeline 1,dynamic act attention superior to pipeline 1,0.6284602880477905
translation,154,133,results,pipeline 1,without it in,all metrics,pipeline 1 without it in all metrics,0.6580711603164673
translation,154,134,results,our joint model,surpasses,currently state - of - the - art pipeline system hdsa,our joint model surpasses currently state - of - the - art pipeline system hdsa,0.6278992891311646
translation,154,134,results,results,has,our joint model,results has our joint model,0.5684905052185059
translation,154,149,results,marco,ties,94 %,marco ties 94 %,0.6413317322731018
translation,154,149,results,hdsa,in,readability,hdsa in readability,0.5052431225776672
translation,154,149,results,hdsa,in,readability,hdsa in readability,0.5052431225776672
translation,154,149,results,human response,in,completion,human response in completion,0.5267192721366882
translation,154,149,results,94 %,with,hdsa,94 % with hdsa,0.6826202273368835
translation,154,149,results,hdsa,in,readability,hdsa in readability,0.5052431225776672
translation,154,149,results,marco,has,outperforms,marco has outperforms,0.6549976468086243
translation,154,149,results,outperforms,has,hdsa,outperforms has hdsa,0.5960028767585754
translation,154,149,results,outperforms,has,human response,outperforms has human response,0.5958957672119141
translation,154,149,results,underperforming,has,human response,underperforming has human response,0.5756317973136902
translation,154,149,results,results,note,marco,results note marco,0.5742971897125244
translation,154,150,results,marco,superior to,hdsa,marco superior to hdsa,0.678225576877594
translation,154,150,results,marco,comparable with,human response,marco comparable with human response,0.6947336196899414
translation,154,150,results,results,has,marco,results has marco,0.5312316417694092
translation,155,128,ablation-analysis,ablations,on,no / small / big persona subsets,ablations on no / small / big persona subsets,0.5851655602455139
translation,155,128,ablation-analysis,no / small / big persona subsets,of,test samples,no / small / big persona subsets of test samples,0.6042995452880859
translation,155,128,ablation-analysis,no / small / big persona subsets,see that,relative improvements,no / small / big persona subsets see that relative improvements,0.6050045490264893
translation,155,128,ablation-analysis,relative improvements,obtained by,"p exp , random","relative improvements obtained by p exp , random",0.6122667789459229
translation,155,128,ablation-analysis,relative improvements,proportional to,persona size,relative improvements proportional to persona size,0.6890773773193359
translation,155,128,ablation-analysis,"p exp , random",proportional to,persona size,"p exp , random proportional to persona size",0.7666967511177063
translation,155,128,ablation-analysis,ablation analysis,from,ablations,ablation analysis from ablations,0.5593735575675964
translation,155,128,ablation-analysis,ablation analysis,on,no / small / big persona subsets,ablation analysis on no / small / big persona subsets,0.5744321346282959
translation,155,149,ablation-analysis,persona,related to,topic,persona related to topic,0.6690056324005127
translation,155,149,ablation-analysis,benefits,to,model,benefits to model,0.5756431818008423
translation,155,149,ablation-analysis,ablation analysis,more,persona,ablation analysis more persona,0.6556565165519714
translation,155,114,baselines,t5 - small,is,text generation model,t5 - small is text generation model,0.5658422708511353
translation,155,114,baselines,text generation model,with,stateof - the - art results,text generation model with stateof - the - art results,0.6161251068115234
translation,155,114,baselines,stateof - the - art results,on,challenging,stateof - the - art results on challenging,0.5275329947471619
translation,155,114,baselines,challenging,has,language understanding tasks,challenging has language understanding tasks,0.5161992907524109
translation,155,114,baselines,baselines,has,t5 - small,baselines has t5 - small,0.6242335438728333
translation,155,16,experiments,new conversational dataset,from,social platform,new conversational dataset from social platform,0.5364159345626831
translation,155,16,experiments,stance - based personas,with,varying degrees of abstraction,stance - based personas with varying degrees of abstraction,0.6116501688957214
translation,155,7,model,stance - based persona,grasp,more profound characteristics,stance - based persona grasp more profound characteristics,0.7214218378067017
translation,155,7,model,model,investigate,stance - based persona,model investigate stance - based persona,0.6200364828109741
translation,155,15,model,representing persona,with,different approaches and levels of abstraction,representing persona with different approaches and levels of abstraction,0.5340865850448608
translation,155,126,results,clear benefit,from adding,persona information,clear benefit from adding persona information,0.6117244362831116
translation,155,126,results,results,show,clear benefit,results show clear benefit,0.6458030343055725
translation,155,127,results,persona information,ingested at,training time,persona information ingested at training time,0.6817359924316406
translation,155,127,results,training time,allows,"p exp , random","training time allows p exp , random",0.6748483777046204
translation,155,127,results,"p exp , random",to perform,significantly better,"p exp , random to perform significantly better",0.7045015096664429
translation,155,127,results,significantly better,than,baseline model,significantly better than baseline model,0.5733217000961304
translation,155,127,results,  no persona   subset,has,persona information,  no persona   subset has persona information,0.5630465149879456
translation,155,127,results,samples,has,persona information,samples has persona information,0.5389163494110107
translation,155,127,results,results,even on,  no persona   subset,results even on   no persona   subset,0.673976719379425
translation,155,145,results,slightly improves,over,baseline results,slightly improves over baseline results,0.6771889328956604
translation,155,145,results,input,has,implicit persona p imp,input has implicit persona p imp,0.6093065142631531
translation,155,145,results,implicit persona p imp,has,slightly improves,implicit persona p imp has slightly improves,0.6076829433441162
translation,155,145,results,results,Adding to,input,results Adding to input,0.6977182030677795
translation,155,148,results,larger bleu and rouge gains,with,explicit persona,larger bleu and rouge gains with explicit persona,0.6728490591049194
translation,155,148,results,larger bleu and rouge gains,has,increasing gradually,larger bleu and rouge gains has increasing gradually,0.6099305152893066
translation,155,148,results,results,observe,larger bleu and rouge gains,results observe larger bleu and rouge gains,0.5906661748886108
translation,155,155,results,better,than,random persona,better than random persona,0.6472048759460449
translation,155,155,results,outperforms,has,p exp,outperforms has p exp,0.7087615132331848
translation,155,158,results,largest improvements,come for,persona size,largest improvements come for persona size,0.6239327192306519
translation,155,158,results,persona size,superior to,5,persona size superior to 5,0.781891405582428
translation,155,158,results,results,observe,largest improvements,results observe largest improvements,0.6200369000434875
translation,155,163,results,hybrid,achieves,best performance,hybrid achieves best performance,0.7304835319519043
translation,155,163,results,best performance,thanks to,more diverse vocabulary,best performance thanks to more diverse vocabulary,0.4534706175327301
translation,155,163,results,rouge and bleu metrics,has,p exp,rouge and bleu metrics has p exp,0.613618016242981
translation,155,163,results,rouge and bleu metrics,has,hybrid,rouge and bleu metrics has hybrid,0.6130075454711914
translation,155,163,results,p exp,has,hybrid,p exp has hybrid,0.7157056331634521
translation,155,176,results,hybrid,performs,best,hybrid performs best,0.6643561124801636
translation,155,176,results,baseline,scores,poorly,baseline scores poorly,0.8523375391960144
translation,155,176,results,poorly,for,relevance,poorly for relevance,0.651138424873352
translation,155,176,results,relevance,toward,persona and the parent claim,relevance toward persona and the parent claim,0.6538649201393127
translation,155,176,results,p exp,has,hybrid,p exp has hybrid,0.7157056331634521
translation,155,177,results,"p exp , dynamic",achieves,similar results,"p exp , dynamic achieves similar results",0.687076210975647
translation,155,177,results,similar results,than,"p exp , hybrid","similar results than p exp , hybrid",0.6091358065605164
translation,155,177,results,"p exp , hybrid",for,persona score,"p exp , hybrid for persona score",0.6024635434150696
translation,155,177,results,underperforms,w.r.t.,parent claim,underperforms w.r.t. parent claim,0.5620043277740479
translation,155,177,results,underperforms,to,parent claim,underperforms to parent claim,0.5931230187416077
translation,155,177,results,results,observe,"p exp , dynamic","results observe p exp , dynamic",0.6339333653450012
translation,156,129,ablation-analysis,accuracy,of,disease identification,accuracy of disease identification,0.5990021228790283
translation,156,129,ablation-analysis,disease identification,for,all the four diseases,disease identification for all the four diseases,0.6201963424682617
translation,156,129,ablation-analysis,greatly improve,has,accuracy,greatly improve has accuracy,0.5548149943351746
translation,156,115,baselines,baselines,include,svm,baselines include svm,0.6056399345397949
translation,156,121,baselines,baselines,has,rule- based agent,baselines has rule- based agent,0.5689266920089722
translation,156,107,hyperparameters,of - greedy strategy,set to,0.1,of - greedy strategy set to 0.1,0.6618730425834656
translation,156,107,hyperparameters,0.1,for,effective action space exploration,0.1 for effective action space exploration,0.5921112895011902
translation,156,107,hyperparameters,0.1,is,0.9,0.1 is 0.9,0.5649217963218689
translation,156,107,hyperparameters,? in bellman equation,is,0.9,? in bellman equation is 0.9,0.5844932198524475
translation,156,107,hyperparameters,hyperparameters,has,of - greedy strategy,hyperparameters has of - greedy strategy,0.5679943561553955
translation,156,107,hyperparameters,hyperparameters,has,? in bellman equation,hyperparameters has ? in bellman equation,0.5523196458816528
translation,156,108,hyperparameters,buffer d,is,10000,buffer d is 10000,0.6325172781944275
translation,156,108,hyperparameters,batch size,is,30,batch size is 30,0.6487055420875549
translation,156,108,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,156,110,hyperparameters,learning rate,is,0.001,learning rate is 0.001,0.5655806064605713
translation,156,110,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,156,60,model,dm,for,automatic diagnosis,dm for automatic diagnosis,0.5981762409210205
translation,156,60,model,two sub-modules,namely,dialogue state tracker ( dst ),two sub-modules namely dialogue state tracker ( dst ),0.702316164970398
translation,156,60,model,two sub-modules,namely,policy learning,two sub-modules namely policy learning,0.6869300007820129
translation,156,60,model,model,focus on,dm,model focus on dm,0.768639862537384
translation,156,109,model,neural network,of,dqn,neural network of dqn,0.5671444535255432
translation,156,109,model,dqn,is,single layer network,dqn is single layer network,0.5559171438217163
translation,156,109,model,model,has,neural network,model has neural network,0.5323531031608582
translation,156,116,model,automatic diagnosis,as,multi-class classification problem,automatic diagnosis as multi-class classification problem,0.5316724181175232
translation,156,116,model,model,treats,automatic diagnosis,model treats automatic diagnosis,0.6863588094711304
translation,156,128,results,accuracy,of,two svm - based models,accuracy of two svm - based models,0.5997753143310547
translation,156,132,results,random agent,in,large margin,random agent in large margin,0.55499267578125
translation,156,132,results,rule- based agent,has,outperforms,rule- based agent has outperforms,0.6205328702926636
translation,156,132,results,outperforms,has,random agent,outperforms has random agent,0.6231421232223511
translation,156,132,results,results,has,rule- based agent,results has rule- based agent,0.5704967975616455
translation,156,134,results,rl - based dqn agent,has,outperforms,rl - based dqn agent has outperforms,0.5864477753639221
translation,156,134,results,outperforms,has,rule-based agent,outperforms has rule-based agent,0.6112922430038452
translation,156,134,results,rule-based agent,has,significantly,rule-based agent has significantly,0.589663028717041
translation,156,134,results,results,see that,rl - based dqn agent,results see that rl - based dqn agent,0.6546074748039246
translation,156,135,results,outperforms,by collecting,additional implicit symptoms,outperforms by collecting additional implicit symptoms,0.7342963814735413
translation,156,135,results,svm - ex,by collecting,additional implicit symptoms,svm - ex by collecting additional implicit symptoms,0.7011723518371582
translation,156,135,results,additional implicit symptoms,conversing with,patients,additional implicit symptoms conversing with patients,0.641460657119751
translation,156,135,results,dqn agent,has,outperforms,dqn agent has outperforms,0.6423438787460327
translation,156,135,results,outperforms,has,svm - ex,outperforms has svm - ex,0.6256816387176514
translation,156,135,results,results,has,dqn agent,results has dqn agent,0.5667219161987305
translation,157,200,ablation-analysis,question duration,along with,bag-of-words features,question duration along with bag-of-words features,0.5902258157730103
translation,157,200,ablation-analysis,question duration,helped boost,model performance,question duration helped boost model performance,0.7328511476516724
translation,157,200,ablation-analysis,ablation analysis,Using,question duration,ablation analysis Using question duration,0.62648606300354
translation,157,106,baselines,only textual features,from,tutor questions,only textual features from tutor questions,0.538062572479248
translation,157,106,baselines,simple unigram bag-of-words model,for,feature representation,simple unigram bag-of-words model for feature representation,0.5642858743667603
translation,157,146,experiments,"keras ( chollet et al. , 2015 )",to run,our deep learning experiments,"keras ( chollet et al. , 2015 ) to run our deep learning experiments",0.7026990652084351
translation,157,143,hyperparameters,model,for,maximum of 20 epochs,model for maximum of 20 epochs,0.5900745391845703
translation,157,143,hyperparameters,model,optimizing,cross-entropy loss,model optimizing cross-entropy loss,0.6806650757789612
translation,157,143,hyperparameters,maximum of 20 epochs,optimizing,cross-entropy loss,maximum of 20 epochs optimizing cross-entropy loss,0.6549065709114075
translation,157,143,hyperparameters,hyperparameters,train,model,hyperparameters train model,0.7064553499221802
translation,157,142,model,encoded question,fed into,densely - connected layer,encoded question fed into densely - connected layer,0.6906837821006775
translation,157,142,model,densely - connected layer,with,sigmoid activation function,densely - connected layer with sigmoid activation function,0.6144023537635803
translation,157,142,model,model,has,encoded question,model has encoded question,0.6582397818565369
translation,157,119,results,question word count,add,question word count,question word count add question word count,0.6386174559593201
translation,157,119,results,question word count,as,feature,question word count as feature,0.5276161432266235
translation,157,119,results,feature,along with,default bag-of-words features,feature along with default bag-of-words features,0.5980073809623718
translation,157,119,results,results,Experiment 1 :,question word count,results Experiment 1 : question word count,0.5774709582328796
translation,157,119,results,results,add,question word count,results add question word count,0.5724949240684509
translation,157,175,results,difference,between,best model and the best human performance,difference between best model and the best human performance,0.6587816476821899
translation,157,175,results,best model and the best human performance,is,statistically significant,best model and the best human performance is statistically significant,0.5530644655227661
translation,157,175,results,statistically significant,has,. 62 vs .55,statistically significant has . 62 vs .55,0.5026955008506775
translation,157,175,results,results,has,difference,results has difference,0.5636705756187439
translation,157,186,results,best human performance,is,statistically significant ( .63 vs .67 ),best human performance is statistically significant ( .63 vs .67 ),0.5032942295074463
translation,157,189,results,best performance,obtained with,l2 penalties,best performance obtained with l2 penalties,0.6461408734321594
translation,157,189,results,svm and logistic regres-sion classifiers,has,best performance,svm and logistic regres-sion classifiers has best performance,0.5700097680091858
translation,157,189,results,results,find that,svm and logistic regres-sion classifiers,results find that svm and logistic regres-sion classifiers,0.5673413872718811
translation,157,189,results,results,for,svm and logistic regres-sion classifiers,results for svm and logistic regres-sion classifiers,0.5433833599090576
translation,157,190,results,squared hinge loss,found to work better,hinge loss,squared hinge loss found to work better hinge loss,0.6305594444274902
translation,157,190,results,svm,has,squared hinge loss,svm has squared hinge loss,0.5411388278007507
translation,157,190,results,results,For,svm,results For svm,0.667313277721405
translation,157,191,results,no significant difference,in,performance,no significant difference in performance,0.5489620566368103
translation,157,191,results,performance,between,svm and logistic regression,performance between svm and logistic regression,0.6443106532096863
translation,157,191,results,results,find,no significant difference,results find no significant difference,0.5300222635269165
translation,157,194,results,question word count,Adding,question length,question word count Adding question length,0.6400570869445801
translation,157,194,results,question length,improves,performance,question length improves performance,0.6706086993217468
translation,157,194,results,results,Experiment 1 :,question word count,results Experiment 1 : question word count,0.5774709582328796
translation,157,195,results,longer questions,result in,higher response times,longer questions result in higher response times,0.6437211036682129
translation,157,195,results,results,has,longer questions,results has longer questions,0.5328959822654724
translation,157,205,results,previous dialogue times,helps improve,model performance,previous dialogue times helps improve model performance,0.6822544932365417
translation,157,205,results,model performance,in,dev and test sets,model performance in dev and test sets,0.5438097715377808
translation,157,205,results,model performance,both,dev and test sets,model performance both dev and test sets,0.7003323435783386
translation,157,205,results,results,adding,previous dialogue times,results adding previous dialogue times,0.610292911529541
translation,157,206,results,best results,while using,5 turns,best results while using 5 turns,0.6975785493850708
translation,157,206,results,5 turns,of,previous dialogue,5 turns of previous dialogue,0.5785208940505981
translation,157,208,results,word entrainment,seems to have,no effect,word entrainment seems to have no effect,0.662204384803772
translation,157,208,results,no effect,on,model performance,no effect on model performance,0.5722158551216125
translation,157,208,results,results,has,word entrainment,results has word entrainment,0.507670521736145
translation,157,218,results,rnn,achieves,performance,rnn achieves performance,0.6815159320831299
translation,157,218,results,performance,which is,statistically significant better,performance which is statistically significant better,0.6116689443588257
translation,157,218,results,statistically significant better,than,baseline,statistically significant better than baseline,0.5658645629882812
translation,157,218,results,baseline,with,same feature,baseline with same feature,0.684490442276001
translation,157,218,results,results,has,rnn,results has rnn,0.5777176022529602
translation,158,144,experimental-setup,na?ve bayes implementation,provided in,scikit,na?ve bayes implementation provided in scikit,0.6231542825698853
translation,158,144,experimental-setup,experimental setup,has,na?ve bayes implementation,experimental setup has na?ve bayes implementation,0.553796648979187
translation,158,145,experimental-setup,svm,use,svm - light,svm use svm - light,0.5877618789672852
translation,158,145,experimental-setup,experimental setup,For,svm,experimental setup For svm,0.6140734553337097
translation,158,5,experiments,manuallylabeled dataset,of,dialogue,manuallylabeled dataset of dialogue,0.5698000192642212
translation,158,5,experiments,dialogue,from,tv series ' friends ',dialogue from tv series ' friends ',0.6144531965255737
translation,158,9,results,f-score,of,around 4 %,f-score of around 4 %,0.5575323700904846
translation,158,9,results,two sequence labeling algorithms,has,svm - hmm and searn,two sequence labeling algorithms has svm - hmm and searn,0.5893970727920532
translation,158,9,results,two sequence labeling algorithms,has,outperform,two sequence labeling algorithms has outperform,0.5820527076721191
translation,158,9,results,svm - hmm and searn,has,outperform,svm - hmm and searn has outperform,0.6451836228370667
translation,158,9,results,outperform,has,three classification algorithms,outperform has three classification algorithms,0.5715996026992798
translation,158,9,results,three classification algorithms,has,naive bayes ),three classification algorithms has naive bayes ),0.5532050132751465
translation,158,9,results,results,has,two sequence labeling algorithms,results has two sequence labeling algorithms,0.5329503417015076
translation,158,34,results,novel features,has,sequence labeling algorithms,novel features has sequence labeling algorithms,0.5361886620521545
translation,158,34,results,sequence labeling algorithms,has,outperform,sequence labeling algorithms has outperform,0.5980391502380371
translation,158,34,results,outperform,has,classification algorithms,outperform has classification algorithms,0.5694552659988403
translation,158,35,results,improvement,of,3 - 4 %,improvement of 3 - 4 %,0.6079158186912537
translation,158,35,results,3 - 4 %,in,f-score,3 - 4 % in f-score,0.5366892218589783
translation,158,35,results,3 - 4 %,when,sequence labelers,3 - 4 % when sequence labelers,0.6724516153335571
translation,158,35,results,f-score,when,sequence labelers,f-score when sequence labelers,0.6007366180419922
translation,158,35,results,sequence labelers,compared to,classifiers,sequence labelers compared to classifiers,0.6207845211029053
translation,158,35,results,classifiers,for,sarcasm detection,classifiers for sarcasm detection,0.5892133712768555
translation,158,35,results,sarcasm detection,in,our dialogue dataset,sarcasm detection in our dialogue dataset,0.4563286304473877
translation,158,165,results,precision,for,sarcastic class,precision for sarcastic class,0.5707950592041016
translation,158,165,results,sarcastic class,obtained in case of,svm - hmm,sarcastic class obtained in case of svm - hmm,0.6426193714141846
translation,158,165,results,results,best value of,precision,results best value of precision,0.719430148601532
translation,158,166,results,best f-score,for,sarcastic class,best f-score for sarcastic class,0.5954811573028564
translation,158,166,results,best f-score,for,nonsarcastic class,best f-score for nonsarcastic class,0.5999997854232788
translation,158,166,results,sarcastic class,in the case of,svm ( o ) ( 29 % ),sarcastic class in the case of svm ( o ) ( 29 % ),0.6863163709640503
translation,158,166,results,sarcastic class,in the case of,svm - hmm,sarcastic class in the case of svm - hmm,0.6358798742294312
translation,158,166,results,nonsarcastic class,in the case of,svm - hmm,nonsarcastic class in the case of svm - hmm,0.6804026961326599
translation,158,166,results,svm - hmm,has,93.6 % ),svm - hmm has 93.6 % ),0.5642046928405762
translation,158,166,results,results,has,best f-score,results has best f-score,0.5508406162261963
translation,158,167,results,perform better,than,classification techniques,perform better than classification techniques,0.5698946714401245
translation,158,167,results,high recall,has,sequence labeling techniques,high recall has sequence labeling techniques,0.5559110045433044
translation,158,167,results,sequence labeling techniques,has,perform better,sequence labeling techniques has perform better,0.5907296538352966
translation,158,174,results,best sequence labeling algorithm ( svm - hmm ),gives,fscore,best sequence labeling algorithm ( svm - hmm ) gives fscore,0.583756685256958
translation,158,174,results,fscore,of,84.4 %,fscore of 84.4 %,0.5330333709716797
translation,158,174,results,f-score,of,81.2 %,f-score of 81.2 %,0.5178791880607605
translation,158,174,results,best classifier ( svm ( o ) ),has,f-score,best classifier ( svm ( o ) ) has f-score,0.5282576084136963
translation,158,174,results,results,has,best sequence labeling algorithm ( svm - hmm ),results has best sequence labeling algorithm ( svm - hmm ),0.5360778570175171
translation,158,183,results,best recall,for,classification techniques,best recall for classification techniques,0.620519757270813
translation,158,183,results,classification techniques,is,77.8 %,classification techniques is 77.8 %,0.530352771282196
translation,158,183,results,77.8 %,for,svm ( o ),77.8 % for svm ( o ),0.6096710562705994
translation,158,183,results,results,has,best recall,results has best recall,0.5691875219345093
translation,158,261,results,best fscore,obtained with,svm ( o ) ( i.e. 79.8 % ),best fscore obtained with svm ( o ) ( i.e. 79.8 % ),0.5965174436569214
translation,158,261,results,best fscore,obtained with,svm - hmm,best fscore obtained with svm - hmm,0.6083911657333374
translation,158,261,results,best f-score,for,sequence labeling techniques,best f-score for sequence labeling techniques,0.5699418187141418
translation,158,261,results,best f-score,obtained using,svm - hmm,best f-score obtained using svm - hmm,0.5670763850212097
translation,158,261,results,our classifiers,has,best fscore,our classifiers has best fscore,0.5493952631950378
translation,158,261,results,svm - hmm,has,i.e. 84.2 % ),svm - hmm has i.e. 84.2 % ),0.5529523491859436
translation,158,261,results,results,For,our classifiers,results For our classifiers,0.594133734703064
translation,158,262,results,best combinations,has,both sequence labeling techniques,best combinations has both sequence labeling techniques,0.5534306168556213
translation,158,262,results,features,has,both sequence labeling techniques,features has both sequence labeling techniques,0.5734471082687378
translation,158,262,results,algorithm,has,both sequence labeling techniques,algorithm has both sequence labeling techniques,0.583221971988678
translation,158,262,results,both sequence labeling techniques,has,outperformed,both sequence labeling techniques has outperformed,0.5799517035484314
translation,158,262,results,outperformed,has,classifiers,outperformed has classifiers,0.6224817037582397
translation,158,262,results,results,case of,best combinations,results case of best combinations,0.7092231512069702
translation,158,269,results,sarcasm detection,of,our dataset,sarcasm detection of our dataset,0.5267940163612366
translation,158,269,results,sarcasm detection,in case of,different feature configurations,sarcasm detection in case of different feature configurations,0.691493034362793
translation,158,269,results,performs better,than,classification,performs better than classification,0.5816745162010193
translation,158,269,results,sarcasm detection,has,sequence labeling,sarcasm detection has sequence labeling,0.5353533029556274
translation,158,269,results,our dataset,has,sequence labeling,our dataset has sequence labeling,0.5555878281593323
translation,158,269,results,different feature configurations,has,sequence labeling,different feature configurations has sequence labeling,0.5750441551208496
translation,158,269,results,sequence labeling,has,performs better,sequence labeling has performs better,0.5988264679908752
translation,158,269,results,results,for,sarcasm detection,results for sarcasm detection,0.5901129841804504
translation,159,146,experimental-setup,bidirectional lstm,with,300 hidden units,bidirectional lstm with 300 hidden units,0.6051872968673706
translation,159,146,experimental-setup,300 hidden units,as,sentence encoder,300 hidden units as sentence encoder,0.4964502155780792
translation,159,146,experimental-setup,forward lstm,for,decoding,forward lstm for decoding,0.5652844905853271
translation,159,146,experimental-setup,experimental setup,use,bidirectional lstm,experimental setup use bidirectional lstm,0.569962739944458
translation,159,147,experimental-setup,300 - dimensional word embeddings,initialized with,glove word embedding,300 - dimensional word embeddings initialized with glove word embedding,0.7176932096481323
translation,159,147,experimental-setup,experimental setup,has,300 - dimensional word embeddings,experimental setup has 300 - dimensional word embeddings,0.5049955248832703
translation,159,148,experimental-setup,dropout rate,of,0.5,dropout rate of 0.5,0.6072384119033813
translation,159,148,experimental-setup,dropout rate,adopted during,training,dropout rate adopted during training,0.7090640068054199
translation,159,148,experimental-setup,0.5,adopted during,training,0.5 adopted during training,0.6756642460823059
translation,159,148,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,159,149,experimental-setup,bowloss weight,to be,0.5,bowloss weight to be 0.5,0.5674459934234619
translation,159,149,experimental-setup,experimental setup,set,bowloss weight,experimental setup set bowloss weight,0.6157253980636597
translation,159,150,experimental-setup,whole network,trained with,adam optimizer,whole network trained with adam optimizer,0.7362538576126099
translation,159,150,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,159,150,experimental-setup,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,159,150,experimental-setup,0.001,on,gtx titan x gpus,0.001 on gtx titan x gpus,0.5186579823493958
translation,159,150,experimental-setup,gtx titan x gpus,for,60 epochs,gtx titan x gpus for 60 epochs,0.5385954976081848
translation,159,150,experimental-setup,experimental setup,has,whole network,experimental setup has whole network,0.5650103688240051
translation,159,175,experimental-setup,lstms and bert,as,sentence embedding encoder,lstms and bert as sentence embedding encoder,0.5697855949401855
translation,159,175,experimental-setup,lstms and bert,as,"two gru ( chung et al. , 2014 ) layers","lstms and bert as two gru ( chung et al. , 2014 ) layers",0.5323231816291809
translation,159,175,experimental-setup,"two gru ( chung et al. , 2014 ) layers",with,300 hidden units,"two gru ( chung et al. , 2014 ) layers with 300 hidden units",0.5774635076522827
translation,159,175,experimental-setup,300 hidden units,as,decoder,300 hidden units as decoder,0.5559996962547302
translation,159,175,experimental-setup,experimental setup,use,lstms and bert,experimental setup use lstms and bert,0.6083194613456726
translation,159,175,experimental-setup,experimental setup,use,"two gru ( chung et al. , 2014 ) layers","experimental setup use two gru ( chung et al. , 2014 ) layers",0.5788264274597168
translation,159,176,experimental-setup,model,converges,100 epochs,model converges 100 epochs,0.753809928894043
translation,159,176,experimental-setup,100 epochs,on,gtx titan x gpus,100 epochs on gtx titan x gpus,0.4854038953781128
translation,159,176,experimental-setup,experimental setup,has,model,experimental setup has model,0.5338840484619141
translation,159,177,experimental-setup,training procedure,takes,about 54 hours,training procedure takes about 54 hours,0.5916603207588196
translation,159,177,experimental-setup,experimental setup,has,training procedure,experimental setup has training procedure,0.4894740879535675
translation,159,37,experiments,proposed vrnn - linearcrf,learns,better structures,proposed vrnn - linearcrf learns better structures,0.6714786887168884
translation,159,37,experiments,better structures,than,baseline vrnn,better structures than baseline vrnn,0.5567046403884888
translation,159,37,experiments,better structures,for,semantic structure learning,better structures for semantic structure learning,0.6252833604812622
translation,159,37,experiments,baseline vrnn,on,simdial dataset,baseline vrnn on simdial dataset,0.49857744574546814
translation,159,37,experiments,simdial dataset,for,semantic structure learning,simdial dataset for semantic structure learning,0.5488318800926208
translation,159,37,experiments,semantic structure learning,in,two -party dialogues,semantic structure learning in two -party dialogues,0.48754286766052246
translation,159,7,model,structured attention layers,into,variational recurrent neural network ( vrnn ) model,structured attention layers into variational recurrent neural network ( vrnn ) model,0.5683566927909851
translation,159,7,model,variational recurrent neural network ( vrnn ) model,with,discrete latent states,variational recurrent neural network ( vrnn ) model with discrete latent states,0.6095713376998901
translation,159,7,model,discrete latent states,to learn,dialogue structure,discrete latent states to learn dialogue structure,0.6208945512771606
translation,159,7,model,dialogue structure,in,unsupervised fashion,dialogue structure in unsupervised fashion,0.5166724324226379
translation,159,7,model,model,incorporate,structured attention layers,model incorporate structured attention layers,0.6742982864379883
translation,159,8,model,structured attention,enables,model,structured attention enables model,0.6482715606689453
translation,159,8,model,model,to focus on,different parts,model to focus on different parts,0.7162793278694153
translation,159,8,model,different parts,of,source sentence embeddings,different parts of source sentence embeddings,0.49980202317237854
translation,159,8,model,different parts,while enforcing,structural inductive bias,different parts while enforcing structural inductive bias,0.6745373606681824
translation,159,27,model,combination of structured attention and unsupervised generative model,to infer,latent structure,combination of structured attention and unsupervised generative model to infer latent structure,0.7529768347740173
translation,159,27,model,latent structure,in,dialogue,latent structure in dialogue,0.5886927247047424
translation,159,27,model,model,use,combination of structured attention and unsupervised generative model,model use combination of structured attention and unsupervised generative model,0.5860531330108643
translation,159,32,model,linear conditional random field ( crf ) attention layer,used in,two -party dialogues,linear conditional random field ( crf ) attention layer used in two -party dialogues,0.6278142929077148
translation,159,32,model,two -party dialogues,to discover,semantic structures,two -party dialogues to discover semantic structures,0.5931133031845093
translation,159,32,model,model,has,linear conditional random field ( crf ) attention layer,model has linear conditional random field ( crf ) attention layer,0.5328879356384277
translation,159,33,model,non-projective dependency tree attention layer,embedded to learn,interactive structure,non-projective dependency tree attention layer embedded to learn interactive structure,0.7293551564216614
translation,159,33,model,interactive structure,could help identify,speaker / addressee information,interactive structure could help identify speaker / addressee information,0.7481588125228882
translation,159,33,model,model,has,non-projective dependency tree attention layer,model has non-projective dependency tree attention layer,0.5357712507247925
translation,159,38,model,interactive structure learning,combine,vrnn,interactive structure learning combine vrnn,0.6717481017112732
translation,159,38,model,vrnn,with,non-projective dependency tree attention layer,vrnn with non-projective dependency tree attention layer,0.587230920791626
translation,159,38,model,interactive structure learning,has,in multi-party dialogues,interactive structure learning has in multi-party dialogues,0.5115029215812683
translation,159,38,model,model,For,interactive structure learning,model For interactive structure learning,0.6284628510475159
translation,159,45,model,structured attention,to explore,dialogue structures,structured attention to explore dialogue structures,0.6864168047904968
translation,159,45,model,model,utilize,structured attention,model utilize structured attention,0.5905929803848267
translation,159,46,model,two types,of,dialogue structures,two types of dialogue structures,0.5945322513580322
translation,159,46,model,two types,of,semantic structures ( dialogue intent transitions ),two types of semantic structures ( dialogue intent transitions ),0.5618425607681274
translation,159,46,model,two types,of,interactive structures ( addressee / speaker changes ),two types of interactive structures ( addressee / speaker changes ),0.5824235081672668
translation,159,46,model,model,work on,two types,model work on two types,0.756200909614563
translation,159,63,model,gsn,builds,conversation graph,gsn builds conversation graph,0.6394355893135071
translation,159,63,model,conversation graph,utilizing,explicit speaker / addressee information,conversation graph utilizing explicit speaker / addressee information,0.65062016248703
translation,159,63,model,explicit speaker / addressee information,in,"ubuntu chat corpus ( uthus and aha , 2013 )","explicit speaker / addressee information in ubuntu chat corpus ( uthus and aha , 2013 )",0.4881262481212616
translation,159,63,model,explicit speaker / addressee information,to improve,dialogue generation performance,explicit speaker / addressee information to improve dialogue generation performance,0.6552402973175049
translation,159,63,model,model,has,gsn,model has gsn,0.6074990630149841
translation,159,163,model,vrnn - linearcrf,observes,entire history,vrnn - linearcrf observes entire history,0.7231686115264893
translation,159,163,model,vrnn - linearcrf,ignores,redundant transitions,vrnn - linearcrf ignores redundant transitions,0.7055819034576416
translation,159,163,model,entire history,of,latent states,entire history of latent states,0.6110541820526123
translation,159,163,model,redundant transitions,due to,structure attention,redundant transitions due to structure attention,0.6675045490264893
translation,159,163,model,model,has,vrnn - linearcrf,model has vrnn - linearcrf,0.5783230662345886
translation,159,157,results,our method,generates,similar structure,our method generates similar structure,0.6371766924858093
translation,159,157,results,similar structure,compared to,ground truth,similar structure compared to ground truth,0.6370492577552795
translation,159,157,results,ground truth,in,bus domain,ground truth in bus domain,0.516845703125
translation,159,157,results,results,find,our method,results find our method,0.5806980133056641
translation,159,160,results,our method,with,bert,our method with bert,0.6817998886108398
translation,159,160,results,vrnn - linearcrf - bert,performs,best,vrnn - linearcrf - bert performs best,0.6377077102661133
translation,159,160,results,our method,has,vrnn - linearcrf - bert,our method has vrnn - linearcrf - bert,0.6339725852012634
translation,159,160,results,bert,has,vrnn - linearcrf - bert,bert has vrnn - linearcrf - bert,0.6541069746017456
translation,159,160,results,results,has,our method,results has our method,0.5589964985847473
translation,159,161,results,k-means clustering,performs,worse,k-means clustering performs worse,0.6432095170021057
translation,159,161,results,worse,than,vrnn - based models,worse than vrnn - based models,0.547484815120697
translation,159,161,results,results,has,k-means clustering,results has k-means clustering,0.5461338758468628
translation,159,188,results,proposed vrnn - dependency - tree model,without using,any speaker annotation,proposed vrnn - dependency - tree model without using any speaker annotation,0.7347927093505859
translation,159,188,results,proposed vrnn - dependency - tree model,without using,speaker annotation,proposed vrnn - dependency - tree model without using speaker annotation,0.6870527267456055
translation,159,188,results,proposed vrnn - dependency - tree model,achieves,similar generation performance,proposed vrnn - dependency - tree model achieves similar generation performance,0.6640023589134216
translation,159,188,results,similar generation performance,compared to,state- ofthe - art method,similar generation performance compared to state- ofthe - art method,0.6339980363845825
translation,159,188,results,gsn,with,speaker annotation,gsn with speaker annotation,0.599128007888794
translation,159,188,results,results,observe,proposed vrnn - dependency - tree model,results observe proposed vrnn - dependency - tree model,0.5868305563926697
translation,160,167,ablation-analysis,bidirectional attention flow mechanism,improves,performance,bidirectional attention flow mechanism improves performance,0.6195350289344788
translation,160,167,ablation-analysis,performance,in,all recall@k scores,performance in all recall@k scores,0.49995851516723633
translation,160,167,ablation-analysis,ablation analysis,addition of,bidirectional attention flow mechanism,ablation analysis addition of bidirectional attention flow mechanism,0.5502785444259644
translation,160,9,baselines,several discriminative and generative models,based on,tridirectional attention flow ( tridaf ),several discriminative and generative models based on tridirectional attention flow ( tridaf ),0.6214917302131653
translation,160,25,baselines,several strong discriminative and generative baselines,learn to retrieve and generate,bimodal - relevant responses,several strong discriminative and generative baselines learn to retrieve and generate bimodal - relevant responses,0.7181190848350525
translation,160,25,baselines,baselines,present,several strong discriminative and generative baselines,baselines present several strong discriminative and generative baselines,0.5865423083305359
translation,160,22,experiments,dynamically - visual multimodal dialogue models,introduce,"new ' manyspeaker , video- context chat ' testbed","dynamically - visual multimodal dialogue models introduce new ' manyspeaker , video- context chat ' testbed",0.5463565587997437
translation,160,26,model,triple- encoder discriminative model,to encode,video,triple- encoder discriminative model to encode video,0.688875138759613
translation,160,26,model,triple- encoder discriminative model,to encode,"chat history , and response","triple- encoder discriminative model to encode chat history , and response",0.766391396522522
translation,160,26,model,triple- encoder discriminative model,classify,relevance label,triple- encoder discriminative model classify relevance label,0.6501597762107849
translation,160,26,model,relevance label,of,response,relevance label of response,0.5859703421592712
translation,160,26,model,model,present,triple- encoder discriminative model,model present triple- encoder discriminative model,0.6004524230957031
translation,160,27,model,model,via,tridirectional attention flow ( tridaf ),model via tridirectional attention flow ( tridaf ),0.6803339719772339
translation,160,28,model,generative models,model,bidirectional attention flow,generative models model bidirectional attention flow,0.6791223883628845
translation,160,28,model,bidirectional attention flow,between,video and textual chat context encoders,bidirectional attention flow between video and textual chat context encoders,0.6300170421600342
translation,160,28,model,bidirectional attention flow,decodes,response,bidirectional attention flow decodes response,0.7177538871765137
translation,160,28,model,model,For,generative models,model For generative models,0.5730097889900208
translation,160,43,model,new video- chat dataset,dialogue models need to generate,next response,new video- chat dataset dialogue models need to generate next response,0.753007709980011
translation,160,43,model,next response,in,sequence of chats,next response in sequence of chats,0.5790814757347107
translation,160,43,model,next response,conditioned both on,raw video features,next response conditioned both on raw video features,0.6513543128967285
translation,160,43,model,next response,conditioned both on,previous textual chat history,next response conditioned both on previous textual chat history,0.6622374653816223
translation,160,43,model,model,propose,new video- chat dataset,model propose new video- chat dataset,0.6599411964416504
translation,160,153,results,' most- frequent - response ' baseline,ranks,10 - sized response retrieval list,' most- frequent - response ' baseline ranks 10 - sized response retrieval list,0.7360146641731262
translation,160,153,results,' most- frequent - response ' baseline,only around,10 % recall@1,' most- frequent - response ' baseline only around 10 % recall@1,0.718899130821228
translation,160,153,results,10 - sized response retrieval list,based on,frequency,10 - sized response retrieval list based on frequency,0.6422449350357056
translation,160,153,results,frequency,in,training data,frequency in training data,0.538145124912262
translation,160,153,results,results,has,' most- frequent - response ' baseline,results has ' most- frequent - response ' baseline,0.5265094637870789
translation,160,155,results,our simple trained baselines ( logistic regression and nearest neighbor ),achieve,relatively low scores,our simple trained baselines ( logistic regression and nearest neighbor ) achieve relatively low scores,0.6138580441474915
translation,160,155,results,results,show that,our simple trained baselines ( logistic regression and nearest neighbor ),results show that our simple trained baselines ( logistic regression and nearest neighbor ),0.5014557242393494
translation,160,165,results,simple sequenceto-sequence attention model,with,video only,simple sequenceto-sequence attention model with video only,0.6313639879226685
translation,160,165,results,simple sequenceto-sequence attention model,with,chat only,simple sequenceto-sequence attention model with chat only,0.6315129399299622
translation,160,165,results,simple sequenceto-sequence attention model,with,both video and chat encoders,simple sequenceto-sequence attention model with both video and chat encoders,0.6427147388458252
translation,160,165,results,recall@k scores,are,better,recall@k scores are better,0.5828932523727417
translation,160,165,results,better,than,all the simple baselines,better than all the simple baselines,0.6043811440467834
translation,160,165,results,simple sequenceto-sequence attention model,has,recall@k scores,simple sequenceto-sequence attention model has recall@k scores,0.5563540458679199
translation,160,166,results,both video + chat context,is,better,both video + chat context is better,0.5744123458862305
translation,160,166,results,better,than using,only one context modality,better than using only one context modality,0.7142801284790039
translation,160,166,results,results,using,both video + chat context,results using both video + chat context,0.6131060123443604
translation,160,176,results,bidaf based generative model,performs,better,bidaf based generative model performs better,0.6121187210083008
translation,160,176,results,better,than,non-bidaf one,better than non-bidaf one,0.6537576913833618
translation,160,187,results,performs better,than,classification loss,performs better than classification loss,0.524837076663971
translation,160,187,results,max-margin loss,has,performs better,max-margin loss has performs better,0.5750874876976013
translation,160,187,results,results,observe,max-margin loss,results observe max-margin loss,0.5774810910224915
translation,160,189,results,best generative model ( bidaf ),using,joint loss,best generative model ( bidaf ) using joint loss,0.6236822605133057
translation,160,189,results,joint loss,of,cross-entropy and max-margin,joint loss of cross-entropy and max-margin,0.5096850395202637
translation,160,189,results,better,using,only cross-entropy loss optimization,better using only cross-entropy loss optimization,0.6365088224411011
translation,160,189,results,results,For,best generative model ( bidaf ),results For best generative model ( bidaf ),0.5752536654472351
translation,161,230,ablation-analysis,extra effort,leads to,more comprehensible res,extra effort leads to more comprehensible res,0.6536155939102173
translation,161,230,ablation-analysis,more comprehensible res,with,3.9 % improvement,more comprehensible res with 3.9 % improvement,0.6155167818069458
translation,161,230,ablation-analysis,more comprehensible res,with,6.3 % improvement,more comprehensible res with 6.3 % improvement,0.6144211292266846
translation,161,230,ablation-analysis,3.9 % improvement,under,perfect perception,3.9 % improvement under perfect perception,0.5893879532814026
translation,161,230,ablation-analysis,3.9 % improvement,under,imperfect perception,3.9 % improvement under imperfect perception,0.5995081663131714
translation,161,230,ablation-analysis,3.9 % improvement,under,imperfect perception,3.9 % improvement under imperfect perception,0.5995081663131714
translation,161,230,ablation-analysis,6.3 % improvement,under,imperfect perception,6.3 % improvement under imperfect perception,0.5962954163551331
translation,161,230,ablation-analysis,ablation analysis,has,extra effort,ablation analysis has extra effort,0.5408028364181519
translation,161,77,model,low perceptual capabilities of artificial agents,introduce,hypergraphs,low perceptual capabilities of artificial agents introduce hypergraphs,0.5489630103111267
translation,161,77,model,hypergraphs,to represent,shared environment,hypergraphs to represent shared environment,0.6124173998832703
translation,161,77,model,model,to address,low perceptual capabilities of artificial agents,model to address low perceptual capabilities of artificial agents,0.6170282959938049
translation,161,77,model,model,introduce,hypergraphs,model introduce hypergraphs,0.5336184501647949
translation,161,37,results,approache,performs,effectively,approache performs effectively,0.7178317308425903
translation,161,37,results,approache,agent has,"perfect knowledge or perception of the environment ( e.g. , 84 % )","approache agent has perfect knowledge or perception of the environment ( e.g. , 84 % )",0.6522627472877502
translation,161,37,results,approache,performs,poorly,approache performs poorly,0.7445220351219177
translation,161,37,results,effectively,agent has,"perfect knowledge or perception of the environment ( e.g. , 84 % )","effectively agent has perfect knowledge or perception of the environment ( e.g. , 84 % )",0.6261362433433533
translation,161,37,results,poorly,under,"mismatched perceptual basis ( e.g. , 45 % )","poorly under mismatched perceptual basis ( e.g. , 45 % )",0.5593782067298889
translation,161,191,results,perceived world,is full of,uncertainties,perceived world is full of uncertainties,0.6901021003723145
translation,161,191,results,spatial relations,tend to be,more reliable,spatial relations tend to be more reliable,0.6639249920845032
translation,161,191,results,perceived world,has,spatial relations,perceived world has spatial relations,0.5554235577583313
translation,161,191,results,uncertainties,has,spatial relations,uncertainties has spatial relations,0.5796186327934265
translation,161,191,results,results,when,perceived world,results when perceived world,0.58568274974823
translation,161,192,results,hypergraphs,enables generating,group - based relations,hypergraphs enables generating group - based relations,0.6786875128746033
translation,161,192,results,hypergraphs,results in,significantly better performance,hypergraphs results in significantly better performance,0.6503688097000122
translation,161,192,results,significantly better performance,compared to,regular graphs,significantly better performance compared to regular graphs,0.6721690893173218
translation,161,192,results,45.2 % ),compared to,regular graphs,45.2 % ) compared to regular graphs,0.637639582157135
translation,161,192,results,significantly better performance,has,45.2 % ),significantly better performance has 45.2 % ),0.5516539216041565
translation,161,192,results,regular graphs,has,36.7 % ),regular graphs has 36.7 % ),0.5599315166473389
translation,161,192,results,results,using,hypergraphs,results using hypergraphs,0.6152823567390442
translation,161,202,results,perfect knowledge,of,environment,perfect knowledge of environment,0.5901517271995544
translation,161,202,results,hypergraphs,perform,marginally better,hypergraphs perform marginally better,0.556570291519165
translation,161,202,results,marginally better,than,regular graphs,marginally better than regular graphs,0.6191536784172058
translation,161,202,results,perfect knowledge,has,hypergraphs,perfect knowledge has hypergraphs,0.5194743275642395
translation,161,202,results,environment,has,hypergraphs,environment has hypergraphs,0.49308711290359497
translation,161,202,results,results,shows,perfect knowledge,results shows perfect knowledge,0.6979293823242188
translation,161,202,results,results,given,perfect knowledge,results given perfect knowledge,0.705093264579773
translation,161,203,results,significantly outperforms,by taking advantage of,spatial grouping information,significantly outperforms by taking advantage of spatial grouping information,0.7532377243041992
translation,161,203,results,environment,has,hypergraphs,environment has hypergraphs,0.49308711290359497
translation,161,203,results,hypergraphs,has,significantly outperforms,hypergraphs has significantly outperforms,0.6213305592536926
translation,161,203,results,significantly outperforms,has,regular graphs,significantly outperforms has regular graphs,0.6131137013435364
translation,161,207,results,graphbased approaches,perform,quite competitively,graphbased approaches perform quite competitively,0.5519574284553528
translation,161,207,results,quite competitively,under,perfect knowledge and perception,quite competitively under perfect knowledge and perception,0.6382862329483032
translation,161,207,results,quite competitively,condition of,perfect knowledge and perception,quite competitively condition of perfect knowledge and perception,0.6780539751052856
translation,161,207,results,results,shows that,graphbased approaches,results shows that graphbased approaches,0.6658942103385925
translation,161,209,results,graph - based approaches,perform,well,graph - based approaches perform well,0.5989777445793152
translation,161,209,results,well,when,agent,well when agent,0.6736751794815063
translation,161,209,results,agent,has,perfect knowledge of the environment,agent has perfect knowledge of the environment,0.5107244849205017
translation,162,21,baselines,two methods,for improving,model 's strategic reasoning skillsboth,two methods for improving model 's strategic reasoning skillsboth,0.7862062454223633
translation,162,151,baselines,likelihood,uses,supervised training and decoding,likelihood uses supervised training and decoding,0.6136072874069214
translation,162,151,baselines,fine-tuned,with,goal - based selfplay,fine-tuned with goal - based selfplay,0.5776911973953247
translation,162,151,baselines,rollouts,uses,supervised training,rollouts uses supervised training,0.6356032490730286
translation,162,151,baselines,supervised training,combined with,goal - based decoding,supervised training combined with goal - based decoding,0.7155494689941406
translation,162,151,baselines,goal - based decoding,using,rollouts,goal - based decoding using rollouts,0.6421312689781189
translation,162,151,baselines,rollouts,with,base model,rollouts with base model,0.6628444194793701
translation,162,151,baselines,base model,trained with,reinforcement learning,base model trained with reinforcement learning,0.7291519045829773
translation,162,135,experimental-setup,our models,using,pytorch,our models using pytorch,0.7268629670143127
translation,162,135,experimental-setup,experimental setup,implement,our models,experimental setup implement our models,0.6547874808311462
translation,162,140,experimental-setup,supervised training,optimise,stochastic gradient descent,supervised training optimise stochastic gradient descent,0.6951358318328857
translation,162,140,experimental-setup,stochastic gradient descent,with,minibatch size,stochastic gradient descent with minibatch size,0.5984631180763245
translation,162,140,experimental-setup,stochastic gradient descent,with,initial learning rate,stochastic gradient descent with initial learning rate,0.5994112491607666
translation,162,140,experimental-setup,stochastic gradient descent,with,nesterov momentum,stochastic gradient descent with nesterov momentum,0.574812114238739
translation,162,140,experimental-setup,minibatch size,of,16,minibatch size of 16,0.6327798366546631
translation,162,140,experimental-setup,initial learning rate,of,1.0,initial learning rate of 1.0,0.5808937549591064
translation,162,140,experimental-setup,nesterov momentum,with,?=0.1,nesterov momentum with ?=0.1,0.615424633026123
translation,162,140,experimental-setup,clipping gradients,whose,l 2 norm,clipping gradients whose l 2 norm,0.6223680377006531
translation,162,140,experimental-setup,l 2 norm,exceeds,0.5,l 2 norm exceeds 0.5,0.6500688791275024
translation,162,140,experimental-setup,experimental setup,During,supervised training,experimental setup During supervised training,0.6867899894714355
translation,162,141,experimental-setup,model,for,30 epochs,model for 30 epochs,0.6260956525802612
translation,162,141,experimental-setup,model,pick,snapshot,model pick snapshot,0.7428925633430481
translation,162,141,experimental-setup,snapshot,of,model,snapshot of model,0.6155698299407959
translation,162,141,experimental-setup,model,with,best validation perplexity,model with best validation perplexity,0.6195087432861328
translation,162,141,experimental-setup,experimental setup,train,model,experimental setup train model,0.6514950394630432
translation,162,142,experimental-setup,learning rate,by,factor of 5,learning rate by factor of 5,0.6077352166175842
translation,162,142,experimental-setup,factor of 5,has,each epoch,factor of 5 has each epoch,0.5766064524650574
translation,162,142,experimental-setup,experimental setup,annealed,learning rate,experimental setup annealed learning rate,0.6742097735404968
translation,162,146,experimental-setup,reinforcement learning,use,learning rate,reinforcement learning use learning rate,0.5663551092147827
translation,162,146,experimental-setup,reinforcement learning,use,discount factor,reinforcement learning use discount factor,0.5741384625434875
translation,162,146,experimental-setup,reinforcement learning,use,discount factor,reinforcement learning use discount factor,0.5741384625434875
translation,162,146,experimental-setup,learning rate,of,0.1,learning rate of 0.1,0.6136453747749329
translation,162,146,experimental-setup,gradients,above,1.0,gradients above 1.0,0.6941365003585815
translation,162,146,experimental-setup,discount factor,of,?=0.95,discount factor of ?=0.95,0.5832595229148865
translation,162,146,experimental-setup,experimental setup,During,reinforcement learning,experimental setup During reinforcement learning,0.6196222901344299
translation,162,147,experimental-setup,every 4 reinforcement learning updates,make,supervised update,every 4 reinforcement learning updates make supervised update,0.6545739769935608
translation,162,147,experimental-setup,every 4 reinforcement learning updates,clip,gradients,every 4 reinforcement learning updates clip gradients,0.6695073843002319
translation,162,147,experimental-setup,supervised update,with,mini-batch size,supervised update with mini-batch size,0.6181144118309021
translation,162,147,experimental-setup,supervised update,with,learning rate,supervised update with learning rate,0.6291791796684265
translation,162,147,experimental-setup,supervised update,clip,gradients,supervised update clip gradients,0.7029956579208374
translation,162,147,experimental-setup,gradients,at,1.0,gradients at 1.0,0.5713656544685364
translation,162,147,experimental-setup,mini-batch size,has,16,mini-batch size has 16,0.6320573091506958
translation,162,147,experimental-setup,learning rate,has,0.5,learning rate has 0.5,0.5396801829338074
translation,162,147,experimental-setup,experimental setup,After,every 4 reinforcement learning updates,experimental setup After every 4 reinforcement learning updates,0.6595333218574524
translation,162,147,experimental-setup,experimental setup,clip,gradients,experimental setup clip gradients,0.6564173698425293
translation,162,14,experiments,first large dataset,of,natural language negotiations,first large dataset of natural language negotiations,0.5414178371429443
translation,162,14,experiments,first large dataset,show,end-to - end neural models,first large dataset show end-to - end neural models,0.612602710723877
translation,162,14,experiments,natural language negotiations,between,two people,natural language negotiations between two people,0.6260764598846436
translation,162,14,experiments,end-to - end neural models,trained to,negotiate,end-to - end neural models trained to negotiate,0.7396650314331055
translation,162,14,experiments,negotiate,by maximizing,likelihood of human actions,negotiate by maximizing likelihood of human actions,0.6988943815231323
translation,162,7,model,end-to - end models,for,negotiation,end-to - end models for negotiation,0.6414647698402405
translation,162,23,model,reinforcement learning updates,with,supervised updates,reinforcement learning updates with supervised updates,0.6033074259757996
translation,162,23,model,model,models diverging from,human language,model models diverging from human language,0.6609789729118347
translation,162,8,results,dialogue rollouts,in which,model,dialogue rollouts in which model,0.6466380953788757
translation,162,8,results,plans ahead,by simulating,possible complete continuations,plans ahead by simulating possible complete continuations,0.7416601181030273
translation,162,8,results,possible complete continuations,of,conversation,possible complete continuations of conversation,0.6394125819206238
translation,162,8,results,model,has,plans ahead,model has plans ahead,0.5803992748260498
translation,162,22,results,likelihood,show that,our agents,likelihood show that our agents,0.5787824988365173
translation,162,22,results,considerably improved,using,self play,considerably improved using self play,0.6966171264648438
translation,162,22,results,self play,in which,pre-trained models,self play in which pre-trained models,0.5759793519973755
translation,162,22,results,pre-trained models,practice,negotiating,pre-trained models practice negotiating,0.6651431322097778
translation,162,22,results,negotiating,with,each other,negotiating with each other,0.6124054789543152
translation,162,22,results,each other,to optimise,performance,each other to optimise performance,0.7078846096992493
translation,162,22,results,optimise,has,likelihood,optimise has likelihood,0.6178393363952637
translation,162,169,results,rl and rollouts models,achieve,significantly better results,rl and rollouts models achieve significantly better results,0.6136117577552795
translation,162,169,results,likelihood model,particularly,rl+rollouts model,likelihood model particularly rl+rollouts model,0.6253542900085449
translation,162,169,results,results,see that,rl and rollouts models,results see that rl and rollouts models,0.621440052986145
translation,162,170,results,percentage,of,pareto optimal solutions,percentage of pareto optimal solutions,0.5933948755264282
translation,162,170,results,increases,showing,better exploration,increases showing better exploration,0.7405897974967957
translation,162,170,results,better exploration,of,solution space,better exploration of solution space,0.545142650604248
translation,162,170,results,pareto optimal solutions,has,increases,pareto optimal solutions has increases,0.6017035841941833
translation,162,170,results,results,has,percentage,results has percentage,0.5217120051383972
translation,162,171,results,best models,achieve,higher agreement rate,best models achieve higher agreement rate,0.6035333871841431
translation,162,171,results,best models,achieve,better scores,best models achieve better scores,0.6137204766273499
translation,162,171,results,best models,achieve,similar pareto efficiency,best models achieve similar pareto efficiency,0.6226744651794434
translation,162,171,results,human-human negotiations,has,best models,human-human negotiations has best models,0.602108895778656
translation,162,171,results,results,Compared to,human-human negotiations,results Compared to human-human negotiations,0.6336647868156433
translation,162,173,results,goal - based reasoning,has,outperforming,goal - based reasoning has outperforming,0.5364788174629211
translation,162,173,results,outperforming,has,imitation learning,outperforming has imitation learning,0.5481928586959839
translation,162,174,results,rollouts model,achieves,comparable scores,rollouts model achieves comparable scores,0.679762065410614
translation,162,174,results,comparable scores,to,human partners,comparable scores to human partners,0.5545702576637268
translation,162,174,results,rl+rollouts model,achieves,higher scores,rl+rollouts model achieves higher scores,0.6496573686599731
translation,162,174,results,results,has,rollouts model,results has rollouts model,0.565502405166626
translation,162,175,results,significantly more cases,of,goal - based models,significantly more cases of goal - based models,0.575291097164154
translation,162,175,results,failing to agree a deal,with,humans,failing to agree a deal with humans,0.5922144055366516
translation,162,175,results,goal - based models,has,failing to agree a deal,goal - based models has failing to agree a deal,0.5246695280075073
translation,162,175,results,results,find,significantly more cases,results find significantly more cases,0.569589376449585
translation,163,125,hyperparameters,each data configuration,fine- tune,doublehead gpt - 2 model,each data configuration fine- tune doublehead gpt - 2 model,0.7803657054901123
translation,163,125,hyperparameters,doublehead gpt - 2 model,has,117 m - parameter version,doublehead gpt - 2 model has 117 m - parameter version,0.557644248008728
translation,163,125,hyperparameters,hyperparameters,for,each data configuration,hyperparameters for each data configuration,0.556825578212738
translation,163,131,hyperparameters,"nucleus sampling ( holtzman et al. , 2020 )",for,decoding step,"nucleus sampling ( holtzman et al. , 2020 ) for decoding step",0.5808897614479065
translation,163,131,hyperparameters,decoding step,to keep,only the top tokens,decoding step to keep only the top tokens,0.660234272480011
translation,163,131,hyperparameters,only the top tokens,with,cumulative probability,only the top tokens with cumulative probability,0.6621900200843811
translation,163,131,hyperparameters,hyperparameters,use,"nucleus sampling ( holtzman et al. , 2020 )","hyperparameters use nucleus sampling ( holtzman et al. , 2020 )",0.5890206098556519
translation,163,151,results,models,trained only with,spolin,models trained only with spolin,0.7597602605819702
translation,163,151,results,models,trained only with,spolin and another dialogue dataset,models trained only with spolin and another dialogue dataset,0.7640563249588013
translation,163,151,results,models,with,spolin and another dialogue dataset,models with spolin and another dialogue dataset,0.6227699518203735
translation,163,151,results,models,trained only with,another dialogue dataset,models trained only with another dialogue dataset,0.7619227766990662
translation,163,151,results,spolin and another dialogue dataset,preferred to,models,spolin and another dialogue dataset preferred to models,0.5899804830551147
translation,163,151,results,models,trained only with,another dialogue dataset,models trained only with another dialogue dataset,0.7619227766990662
translation,163,151,results,improves,by,at most 0.06,improves by at most 0.06,0.6582015156745911
translation,163,151,results,at most 0.06,after,fine-tuning,at most 0.06 after fine-tuning,0.6634118556976318
translation,163,151,results,results,show that,models,results show that models,0.4730059802532196
translation,163,152,results,responses,generated by,models,responses generated by models,0.6953391432762146
translation,163,152,results,models,trained with,spolin,models trained with spolin,0.7451788187026978
translation,163,152,results,actual responses,in,development set,actual responses in development set,0.5667597055435181
translation,164,118,experimental-setup,retrieval system,adopted,publicly available chatbot api,retrieval system adopted publicly available chatbot api,0.6034170985221863
translation,164,118,experimental-setup,retrieval system,is,publicly available chatbot api,retrieval system is publicly available chatbot api,0.4964401423931122
translation,164,151,experimental-setup,encoders and decoders,implemented by,lstm networks,encoders and decoders implemented by lstm networks,0.6794437170028687
translation,164,151,experimental-setup,number of layers and hidden size,equal to,2 and 500,number of layers and hidden size equal to 2 and 500,0.7175102829933167
translation,164,151,experimental-setup,lstm networks,has,bidirectional for encoders,lstm networks has bidirectional for encoders,0.607994019985199
translation,164,151,experimental-setup,experimental setup,For,encoders and decoders,experimental setup For encoders and decoders,0.5959098935127258
translation,164,152,experimental-setup,word embeddings,are,randomly initialized,word embeddings are randomly initialized,0.5598148107528687
translation,164,152,experimental-setup,word embeddings,of which,dimension,word embeddings of which dimension,0.5272491574287415
translation,164,152,experimental-setup,dimension,is,300,dimension is 300,0.659263551235199
translation,164,152,experimental-setup,experimental setup,has,word embeddings,experimental setup has word embeddings,0.4932878315448761
translation,164,154,experimental-setup,skeleton extractor,implemented by,2 - layer transformer encoder,skeleton extractor implemented by 2 - layer transformer encoder,0.6847368478775024
translation,164,154,experimental-setup,number of heads and hidden size,is,8 and 512,number of heads and hidden size is 8 and 512,0.5663728713989258
translation,164,154,experimental-setup,experimental setup,has,skeleton extractor,experimental setup has skeleton extractor,0.49966755509376526
translation,164,8,model,skeleton extraction,made by,interpretable matching model,skeleton extraction made by interpretable matching model,0.5981716513633728
translation,164,8,model,skeleton - guided response generation,accomplished by,separately trained generator,skeleton - guided response generation accomplished by separately trained generator,0.6810256242752075
translation,164,27,model,interpretable matching model,for,skeleton extraction,interpretable matching model for skeleton extraction,0.5643758177757263
translation,164,27,model,skeleton - guided response generator,for,response generation,skeleton - guided response generator for response generation,0.613757848739624
translation,164,44,model,skeleton- guided response generator,can handle,skeletons,skeleton- guided response generator can handle skeletons,0.7226842045783997
translation,164,44,model,skeletons,with,different qualities,skeletons with different qualities,0.6484297513961792
translation,164,44,model,model,train,skeleton- guided response generator,model train skeleton- guided response generator,0.7219361662864685
translation,164,92,model,model,has,matching model and the response generator,model has matching model and the response generator,0.5315649509429932
translation,164,169,results,our generation model,is,only one,our generation model is only one,0.5890941619873047
translation,164,169,results,only one,achieves,close performance,only one achieves close performance,0.7214912176132202
translation,164,169,results,close performance,to,retrieval system,close performance to retrieval system,0.5544152855873108
translation,164,169,results,automatic metrics ( dist - 1 and dist - 2 ),has,our generation model,automatic metrics ( dist - 1 and dist - 2 ) has our generation model,0.5796549320220947
translation,164,170,results,retrievalindependent seq2seq - mmi,establishes,strong baseline,retrievalindependent seq2seq - mmi establishes strong baseline,0.5670111179351807
translation,164,170,results,relevance score,has,retrievalindependent seq2seq - mmi,relevance score has retrievalindependent seq2seq - mmi,0.584882378578186
translation,164,170,results,results,For,relevance score,results For relevance score,0.5997902750968933
translation,164,171,results,skeleton - guided methods,better than,who use completely retrieved responses,skeleton - guided methods better than who use completely retrieved responses,0.7241411805152893
translation,164,171,results,retrieval - guided generation,has,skeleton - guided methods,retrieval - guided generation has skeleton - guided methods,0.5725980401039124
translation,164,172,results,our method,advances,performance,our method advances performance,0.6793009042739868
translation,164,172,results,performance,of,skeleton - lex,performance of skeleton - lex,0.595331609249115
translation,164,172,results,skeleton - lex,by,large margin,skeleton - lex by large margin,0.6038647294044495
translation,164,172,results,results,has,our method,results has our method,0.5589964985847473
translation,164,173,results,fluency,achieves,much better performance,fluency achieves much better performance,0.6955317258834839
translation,164,173,results,our method,achieves,much better performance,our method achieves much better performance,0.6303183436393738
translation,164,173,results,much better performance,than,all baseline methods,much better performance than all baseline methods,0.5414262413978577
translation,164,173,results,fluency,has,our method,fluency has our method,0.6255914568901062
translation,164,173,results,results,For,fluency,results For fluency,0.5258268713951111
translation,164,196,results,both two learnable skeleton extractors,give,better results,both two learnable skeleton extractors give better results,0.6137465834617615
translation,164,196,results,better results,than,non-parametric methods,better results than non-parametric methods,0.5951929688453674
translation,164,196,results,results,has,both two learnable skeleton extractors,results has both two learnable skeleton extractors,0.49230074882507324
translation,164,197,results,semantic -inspired model,far ahead of,others,semantic -inspired model far ahead of others,0.6810450553894043
translation,164,197,results,others,in,all aspects,others in all aspects,0.5463129281997681
translation,164,197,results,notable improvement,in,informativeness,notable improvement in informativeness,0.4773128032684326
translation,164,197,results,informativeness,compared to,statistical methods,informativeness compared to statistical methods,0.5995075106620789
translation,164,197,results,results,has,semantic -inspired model,results has semantic -inspired model,0.5476531982421875
translation,164,199,results,pmi and keywords,give,almost the same performance,pmi and keywords give almost the same performance,0.6247622966766357
translation,164,199,results,almost the same performance,on,all three metrics,almost the same performance on all three metrics,0.5190401673316956
translation,164,199,results,results,see that,pmi and keywords,results see that pmi and keywords,0.6608448028564453
translation,164,205,results,clear decline,in,performance,clear decline in performance,0.5686289668083191
translation,164,205,results,clear decline,after switching to,response generator,clear decline after switching to response generator,0.7570394277572632
translation,164,205,results,performance,after switching to,response generator,performance after switching to response generator,0.7440725564956665
translation,164,205,results,response generator,of,skeleton - lex,response generator of skeleton - lex,0.6227595806121826
translation,164,205,results,results,see,clear decline,results see clear decline,0.6468095183372498
translation,164,215,results,our matching model,shows,superior capability,our matching model shows superior capability,0.6579754948616028
translation,164,215,results,superior capability,in selecting,best response,superior capability in selecting best response,0.701769232749939
translation,164,215,results,superior capability,especially in terms of,relevance,superior capability especially in terms of relevance,0.6938310265541077
translation,164,215,results,results,has,our matching model,results has our matching model,0.5875341296195984
translation,165,191,baselines,baselines,has,counterpart data augmentation ( cda ),baselines has counterpart data augmentation ( cda ),0.5831397175788879
translation,165,168,experimental-setup,both the encoder and decoder,implemented as,one - layer gru networks,both the encoder and decoder implemented as one - layer gru networks,0.6736212372779846
translation,165,168,experimental-setup,one - layer gru networks,with,hidden size,one - layer gru networks with hidden size,0.6170798540115356
translation,165,168,experimental-setup,hidden size,of,"1,000","hidden size of 1,000",0.6529634594917297
translation,165,168,experimental-setup,autoencoder,has,both the encoder and decoder,autoencoder has both the encoder and decoder,0.5840482115745544
translation,165,168,experimental-setup,experimental setup,In,autoencoder,experimental setup In autoencoder,0.5097758769989014
translation,165,169,experimental-setup,word embedding size,set as,300,word embedding size set as 300,0.6770156025886536
translation,165,169,experimental-setup,experimental setup,has,word embedding size,experimental setup has word embedding size,0.5138497352600098
translation,165,170,experimental-setup,sizes,of,unbiased gender features and the semantic features,sizes of unbiased gender features and the semantic features,0.5946730375289917
translation,165,170,experimental-setup,sizes,set as,200 and 800,sizes set as 200 and 800,0.6873272657394409
translation,165,170,experimental-setup,unbiased gender features and the semantic features,set as,200 and 800,unbiased gender features and the semantic features set as 200 and 800,0.632779598236084
translation,165,170,experimental-setup,experimental setup,has,sizes,experimental setup has sizes,0.4978969097137451
translation,165,171,experimental-setup,vocab size,is,"30,000","vocab size is 30,000",0.6245003342628479
translation,165,171,experimental-setup,experimental setup,has,vocab size,experimental setup has vocab size,0.5390894412994385
translation,165,172,experimental-setup,experimental setup,set,"k 1 = 10 , k 2 = 1 , k 3 = 1 and k 4 = 3","experimental setup set k 1 = 10 , k 2 = 1 , k 3 = 1 and k 4 = 3",0.6338897347450256
translation,165,197,experimental-setup,weight,of,regularization term,weight of regularization term,0.5223682522773743
translation,165,197,experimental-setup,regularization term,as,k = 0.25,regularization term as k = 0.25,0.5159823894500732
translation,165,197,experimental-setup,experimental setup,empirically set,weight,experimental setup empirically set weight,0.7374498844146729
translation,165,198,experimental-setup,encoder and the decoder,implemented by,three - layer lstm networks,encoder and the decoder implemented by three - layer lstm networks,0.6776831746101379
translation,165,198,experimental-setup,three - layer lstm networks,with,hidden size,three - layer lstm networks with hidden size,0.6016185283660889
translation,165,198,experimental-setup,hidden size,of,"1,024","hidden size of 1,024",0.6437975764274597
translation,165,198,experimental-setup,seq2seq dialogue models,has,encoder and the decoder,seq2seq dialogue models has encoder and the decoder,0.5880987048149109
translation,165,198,experimental-setup,experimental setup,For,seq2seq dialogue models,experimental setup For seq2seq dialogue models,0.5601882338523865
translation,165,199,experimental-setup,word embedding size,set as,300,word embedding size set as 300,0.6770156025886536
translation,165,199,experimental-setup,vocab size,is,"30,000","vocab size is 30,000",0.6245003342628479
translation,165,199,experimental-setup,experimental setup,has,word embedding size,experimental setup has word embedding size,0.5138497352600098
translation,165,199,experimental-setup,experimental setup,has,vocab size,experimental setup has vocab size,0.5390894412994385
translation,165,200,experimental-setup,original model,trained using,standard stochastic gradient descent ( sgd ) algorithm,original model trained using standard stochastic gradient descent ( sgd ) algorithm,0.697032630443573
translation,165,200,experimental-setup,standard stochastic gradient descent ( sgd ) algorithm,with,learning rate,standard stochastic gradient descent ( sgd ) algorithm with learning rate,0.5891495943069458
translation,165,200,experimental-setup,learning rate,of,1.0,learning rate of 1.0,0.6061668395996094
translation,165,200,experimental-setup,experimental setup,has,original model,experimental setup has original model,0.5469253063201904
translation,165,201,experimental-setup,adversarial training process,of,debiased - chat,adversarial training process of debiased - chat,0.5802325010299683
translation,165,201,experimental-setup,adversarial training process,both,dialogue model and the discriminators,adversarial training process both dialogue model and the discriminators,0.6330870389938354
translation,165,201,experimental-setup,dialogue model and the discriminators,trained by,"adam optimizer ( kingma and ba , 2014 )","dialogue model and the discriminators trained by adam optimizer ( kingma and ba , 2014 )",0.760332465171814
translation,165,201,experimental-setup,"adam optimizer ( kingma and ba , 2014 )",with,initial learning rate,"adam optimizer ( kingma and ba , 2014 ) with initial learning rate",0.5920190811157227
translation,165,201,experimental-setup,initial learning rate,of,0.001,initial learning rate of 0.001,0.5703312754631042
translation,165,201,experimental-setup,experimental setup,In,adversarial training process,experimental setup In adversarial training process,0.507844090461731
translation,165,202,experimental-setup,temperature value,for,gumbel - softmax,temperature value for gumbel - softmax,0.6089777946472168
translation,165,202,experimental-setup,gumbel - softmax,initialized as,1.0,gumbel - softmax initialized as 1.0,0.6348854899406433
translation,165,202,experimental-setup,decreases,dividing by,1.1,decreases dividing by 1.1,0.5854805111885071
translation,165,202,experimental-setup,1.1,every,200 iterations,1.1 every 200 iterations,0.6399392485618591
translation,165,202,experimental-setup,experimental setup,has,temperature value,experimental setup has temperature value,0.4827934205532074
translation,165,204,experimental-setup,hyperparameters,empirically set as,k 1 = k 2 = 1,hyperparameters empirically set as k 1 = k 2 = 1,0.7113938927650452
translation,165,204,experimental-setup,hyperparameters,empirically set as,"d steps = 2 , g steps = 2 , g teach steps = 1","hyperparameters empirically set as d steps = 2 , g steps = 2 , g teach steps = 1",0.6900238990783691
translation,165,204,experimental-setup,experimental setup,has,hyperparameters,experimental setup has hyperparameters,0.4710215926170349
translation,165,205,experimental-setup,experimental setup,trained on,nvidia tesla k80 gpus,experimental setup trained on nvidia tesla k80 gpus,0.7035052180290222
translation,165,10,model,novel adversarial learning framework debiased - chat,to train,dialogue models,novel adversarial learning framework debiased - chat to train dialogue models,0.6482681035995483
translation,165,10,model,dialogue models,free from,gender bias,dialogue models free from gender bias,0.5116065740585327
translation,165,10,model,model,propose,novel adversarial learning framework debiased - chat,model propose novel adversarial learning framework debiased - chat,0.6773493885993958
translation,165,45,model,novel framework debiased - chat,to train,bias - free generative dialogue models,novel framework debiased - chat to train bias - free generative dialogue models,0.648451566696167
translation,165,45,model,model,propose,novel framework debiased - chat,model propose novel framework debiased - chat,0.702673077583313
translation,165,46,model,concepts,of,unbiased and biased gender features,concepts of unbiased and biased gender features,0.5742994546890259
translation,165,46,model,unbiased and biased gender features,in,dialogues,unbiased and biased gender features in dialogues,0.5617510080337524
translation,165,46,model,model,introduce,concepts,model introduce concepts,0.7252336740493774
translation,165,48,model,disentanglement model,learns to separate,unbiased gender features,disentanglement model learns to separate unbiased gender features,0.7965199947357178
translation,165,48,model,unbiased gender features,from,biased gender features,unbiased gender features from biased gender features,0.5480099320411682
translation,165,48,model,biased gender features,of,gender-related utterance,biased gender features of gender-related utterance,0.5873132348060608
translation,165,48,model,model,propose,disentanglement model,model propose disentanglement model,0.6216863989830017
translation,165,181,results,classifier,based on,unbiased gender features,classifier based on unbiased gender features,0.6279491186141968
translation,165,181,results,classifier,achieves,very high accuracy,classifier achieves very high accuracy,0.678725004196167
translation,165,181,results,classifier,based on,semantic features,classifier based on semantic features,0.6259073615074158
translation,165,181,results,very high accuracy,of,over 95 %,very high accuracy of over 95 %,0.5700876712799072
translation,165,181,results,performance,of,classifier,performance of classifier,0.6358745098114014
translation,165,181,results,classifier,based on,semantic features,classifier based on semantic features,0.6259073615074158
translation,165,181,results,just slightly higher,than,random guess,just slightly higher than random guess,0.6543359160423279
translation,165,181,results,results,find that,classifier,results find that classifier,0.6631225943565369
translation,165,233,results,our model,behaves,comparably,our model behaves comparably,0.6942963004112244
translation,165,233,results,comparably,with,original model,comparably with original model,0.673305094242096
translation,165,233,results,relevance,has,our model,relevance has our model,0.5950853228569031
translation,165,235,results,responses,to be,reasonably different,responses to be reasonably different,0.6124797463417053
translation,165,235,results,reasonably different,for,male - and female- related messages,reasonably different for male - and female- related messages,0.631141722202301
translation,165,235,results,our model,achieves,better performance,our model achieves better performance,0.6816908121109009
translation,165,235,results,better performance,than,original model and the baseline models,better performance than original model and the baseline models,0.5151971578598022
translation,166,201,ablation-analysis,other components,of,tscp,other components of tscp,0.5574632883071899
translation,166,201,ablation-analysis,tscp,are,important,tscp are important,0.610331654548645
translation,166,201,ablation-analysis,ablation analysis,has,other components,ablation analysis has other components,0.5199902057647705
translation,166,202,ablation-analysis,vanilla attention - based rnn,instead of,copynet,vanilla attention - based rnn instead of copynet,0.635400652885437
translation,166,202,ablation-analysis,vanilla attention - based rnn,instead of,all metrics,vanilla attention - based rnn instead of all metrics,0.6358754634857178
translation,166,202,ablation-analysis,all metrics,for,model effectiveness,all metrics for model effectiveness,0.5204805731773376
translation,166,202,ablation-analysis,vanilla attention - based rnn,has,all metrics,vanilla attention - based rnn has all metrics,0.5842321515083313
translation,166,202,ablation-analysis,copynet,has,all metrics,copynet has all metrics,0.5854436159133911
translation,166,202,ablation-analysis,model effectiveness,has,decrease,model effectiveness has decrease,0.5692873597145081
translation,166,202,ablation-analysis,ablation analysis,use,vanilla attention - based rnn,ablation analysis use vanilla attention - based rnn,0.5907496809959412
translation,166,31,experimental-setup,"copynet ( gu et al. , 2016 )",to instantiate,sequicity framework,"copynet ( gu et al. , 2016 ) to instantiate sequicity framework",0.7622925639152527
translation,166,31,experimental-setup,experimental setup,improve on,"copynet ( gu et al. , 2016 )","experimental setup improve on copynet ( gu et al. , 2016 )",0.6658512949943542
translation,166,170,hyperparameters,hidden size and the embedding size d,set to,50,hidden size and the embedding size d set to 50,0.7429988980293274
translation,166,171,hyperparameters,| v |,is,800,| v | is 800,0.6104863286018372
translation,166,171,hyperparameters,| v |,is,1400,| v | is 1400,0.6596755981445312
translation,166,171,hyperparameters,800,for,camres676,800 for camres676,0.6557353734970093
translation,166,171,hyperparameters,1400,for,kvret,1400 for kvret,0.6829554438591003
translation,166,171,hyperparameters,hyperparameters,has,| v |,hyperparameters has | v |,0.49679604172706604
translation,166,172,hyperparameters,our model,with,"adam optimizer ( kingma and ba , 2015 )","our model with adam optimizer ( kingma and ba , 2015 )",0.6167454123497009
translation,166,172,hyperparameters,our model,with,learning rate,our model with learning rate,0.6211487054824829
translation,166,172,hyperparameters,"adam optimizer ( kingma and ba , 2015 )",with,learning rate,"adam optimizer ( kingma and ba , 2015 ) with learning rate",0.5968794822692871
translation,166,172,hyperparameters,"adam optimizer ( kingma and ba , 2015 )",with,0.0001,"adam optimizer ( kingma and ba , 2015 ) with 0.0001",0.5981927514076233
translation,166,172,hyperparameters,learning rate,of,0.003,learning rate of 0.003,0.6038756370544434
translation,166,172,hyperparameters,learning rate,of,0.0001,learning rate of 0.0001,0.5900294780731201
translation,166,172,hyperparameters,0.003,for,supervised training,0.003 for supervised training,0.5797774791717529
translation,166,172,hyperparameters,0.0001,for,reinforcement learning,0.0001 for reinforcement learning,0.5823118686676025
translation,166,172,hyperparameters,hyperparameters,train,our model,hyperparameters train our model,0.6687315702438354
translation,166,173,hyperparameters,early stopping,performed on,developing set,early stopping performed on developing set,0.7056814432144165
translation,166,173,hyperparameters,hyperparameters,has,early stopping,hyperparameters has early stopping,0.5431009531021118
translation,166,174,hyperparameters,decay parameter,set to,0.8,decay parameter set to 0.8,0.7052109241485596
translation,166,174,hyperparameters,reinforcement learning,has,decay parameter,reinforcement learning has decay parameter,0.4967537224292755
translation,166,174,hyperparameters,hyperparameters,In,reinforcement learning,hyperparameters In reinforcement learning,0.47564077377319336
translation,166,175,hyperparameters,beam search strategy,for,decoding,beam search strategy for decoding,0.6298984885215759
translation,166,175,hyperparameters,beam search strategy,with,beam size,beam search strategy with beam size,0.6281864047050476
translation,166,175,hyperparameters,decoding,with,beam size,decoding with beam size,0.6321919560432434
translation,166,175,hyperparameters,beam size,of,10,beam size of 10,0.7020634412765503
translation,166,5,model,"novel , holistic , extendable framework",based on,single sequence - to-sequence ( seq2seq ) model,"novel , holistic , extendable framework based on single sequence - to-sequence ( seq2seq ) model",0.6652995347976685
translation,166,5,model,single sequence - to-sequence ( seq2seq ) model,optimized with,supervised or reinforcement learning,single sequence - to-sequence ( seq2seq ) model optimized with supervised or reinforcement learning,0.6984025239944458
translation,166,5,model,model,propose,"novel , holistic , extendable framework","model propose novel , holistic , extendable framework",0.6789401769638062
translation,166,6,model,text spans,named,belief spans,text spans named belief spans,0.7361081838607788
translation,166,6,model,text spans,to track,dialogue believes,text spans to track dialogue believes,0.7278814315795898
translation,166,6,model,belief spans,to track,dialogue believes,belief spans to track dialogue believes,0.7084531784057617
translation,166,6,model,dialogue believes,allowing,task - oriented dialogue systems,dialogue believes allowing task - oriented dialogue systems,0.6550394296646118
translation,166,6,model,task - oriented dialogue systems,to be modeled in,seq2seq way,task - oriented dialogue systems to be modeled in seq2seq way,0.6710273623466492
translation,166,6,model,model,design,text spans,model design text spans,0.6121793985366821
translation,166,7,model,simplistic two stage copynet instantiation,demonstrates,good scalability,simplistic two stage copynet instantiation demonstrates good scalability,0.6078650951385498
translation,166,7,model,simplistic two stage copynet instantiation,reducing,model complexity,simplistic two stage copynet instantiation reducing model complexity,0.6752236485481262
translation,166,7,model,significantly,reducing,model complexity,significantly reducing model complexity,0.7457814812660217
translation,166,7,model,model complexity,in terms of,number of parameters,model complexity in terms of number of parameters,0.7000307440757751
translation,166,7,model,model complexity,by,an order of magnitude,model complexity by an order of magnitude,0.5690170526504517
translation,166,7,model,training time,by,an order of magnitude,training time by an order of magnitude,0.565326452255249
translation,166,7,model,good scalability,has,significantly,good scalability has significantly,0.5769765377044678
translation,166,7,model,model,propose,simplistic two stage copynet instantiation,model propose simplistic two stage copynet instantiation,0.6472792625427246
translation,166,22,model,text span,tracks,belief states,text span tracks belief states,0.7811307907104492
translation,166,22,model,belief span ( bspan ),has,text span,belief span ( bspan ) has text span,0.5606665015220642
translation,166,22,model,model,concept of,belief span ( bspan ),model concept of belief span ( bspan ),0.722952127456665
translation,166,23,model,new framework,named,sequicity,new framework named sequicity,0.718859076499939
translation,166,23,model,sequicity,with,single seq2seq model,sequicity with single seq2seq model,0.7033238410949707
translation,166,23,model,model,leads to,new framework,model leads to new framework,0.7104935050010681
translation,166,24,model,sequicity,decomposes,task- oriented dialogue problem,sequicity decomposes task- oriented dialogue problem,0.7293771505355835
translation,166,24,model,task- oriented dialogue problem,into,generation of bspans and machine responses,task- oriented dialogue problem into generation of bspans and machine responses,0.5071091055870056
translation,166,24,model,model,has,sequicity,model has sequicity,0.6498809456825256
translation,166,25,model,sequicity,decodes in,two stages,sequicity decodes in two stages,0.8498494029045105
translation,166,25,model,sequicity,in,second,sequicity in second,0.6496858596801758
translation,166,25,model,sequicity,decodes,bspan,sequicity decodes bspan,0.7712977528572083
translation,166,25,model,sequicity,decodes,machine response,sequicity decodes machine response,0.7913994789123535
translation,166,25,model,two stages,in,second,two stages in second,0.6250032186508179
translation,166,25,model,first stage,decodes,bspan,first stage decodes bspan,0.7856031656265259
translation,166,25,model,bspan,to facilitate,knowledge base ( kb ) search,bspan to facilitate knowledge base ( kb ) search,0.7038519978523254
translation,166,25,model,second,decodes,machine response,second decodes machine response,0.7756479382514954
translation,166,25,model,machine response,on,bspan,machine response on bspan,0.5802414417266846
translation,166,25,model,machine response,condition of,knowledge base search result,machine response condition of knowledge base search result,0.6548261046409607
translation,166,25,model,machine response,condition of,bspan,machine response condition of bspan,0.6654479503631592
translation,166,25,model,model,has,sequicity,model has sequicity,0.6498809456825256
translation,166,27,model,sequicity,employs,single seq2seq model,sequicity employs single seq2seq model,0.616266667842865
translation,166,27,model,single seq2seq model,resulting in,vastly simplified architecture,single seq2seq model resulting in vastly simplified architecture,0.6681633591651917
translation,166,27,model,model,has,sequicity,model has sequicity,0.6498809456825256
translation,166,28,results,sequicity,achieves,much less train-ing time,sequicity achieves much less train-ing time,0.6933255791664124
translation,166,28,results,sequicity,achieves,better performance,sequicity achieves better performance,0.7158747315406799
translation,166,28,results,better performance,on,larger a dataset,better performance on larger a dataset,0.5018574595451355
translation,166,28,results,better performance,on,exceptional ability,better performance on exceptional ability,0.5174136757850647
translation,166,28,results,exceptional ability,to handle,oov cases,exceptional ability to handle oov cases,0.7638128995895386
translation,166,196,results,outperforms,in,task completion,outperforms in task completion,0.5303379893302917
translation,166,196,results,outperforms,in,language quality ( bleu ),outperforms in language quality ( bleu ),0.5068533420562744
translation,166,196,results,all baselines,in,task completion,all baselines in task completion,0.4634450078010559
translation,166,196,results,all baselines,in,language quality ( bleu ),all baselines in language quality ( bleu ),0.4577447474002838
translation,166,196,results,tscp,has,outperforms,tscp has outperforms,0.6515331268310547
translation,166,196,results,outperforms,has,all baselines,outperforms has all baselines,0.5888515114784241
translation,166,196,results,results,has,tscp,results has tscp,0.5389677882194519
translation,166,198,results,sophisticated models,such as,attention copynets,sophisticated models such as attention copynets,0.5942735075950623
translation,166,198,results,sophisticated models,in,kvret,sophisticated models in kvret,0.5699589848518372
translation,166,198,results,results,With,bspans,results With bspans,0.676190197467804
translation,166,199,results,tscp\b t,in,all aspects,tscp\b t in all aspects,0.5842472910881042
translation,166,199,results,tscp,has,outperforms,tscp has outperforms,0.6515331268310547
translation,166,199,results,outperforms,has,tscp\b t,outperforms has tscp\b t,0.698766827583313
translation,166,199,results,results,has,tscp,results has tscp,0.5389677882194519
translation,166,205,results,reinforcement learning,effectively helps,bleu and success f 1,reinforcement learning effectively helps bleu and success f 1,0.62251216173172
translation,166,205,results,results,has,reinforcement learning,results has reinforcement learning,0.5438361167907715
translation,167,4,model,novel training / decoding strategy,for,sequence labeling,novel training / decoding strategy for sequence labeling,0.6345343589782715
translation,167,4,model,model,introduces,novel training / decoding strategy,model introduces novel training / decoding strategy,0.6867799162864685
translation,167,5,model,label,using it for,next prediction,label using it for next prediction,0.6592742204666138
translation,167,5,model,next prediction,retain,probability distribution,next prediction retain probability distribution,0.6595714092254639
translation,167,5,model,next prediction,pass,distribution,next prediction pass distribution,0.7208310961723328
translation,167,5,model,probability distribution,over,current label,probability distribution over current label,0.6705308556556702
translation,167,5,model,probability distribution,to,next prediction,probability distribution to next prediction,0.6151441931724548
translation,167,5,model,distribution,to,next prediction,distribution to next prediction,0.6266151070594788
translation,167,5,model,model,retain,probability distribution,model retain probability distribution,0.686541736125946
translation,167,69,results,our two models,has,outperform,our two models has outperform,0.5965777635574341
translation,167,69,results,outperform,has,all the baselines,outperform has all the baselines,0.6026061177253723
translation,167,69,results,results,has,our two models,results has our two models,0.5413253903388977
translation,167,70,results,uncertainty propagation,has,outperforms,uncertainty propagation has outperforms,0.628128707408905
translation,167,70,results,outperforms,has,average embedding model,outperforms has average embedding model,0.5783910155296326
translation,167,71,results,true current label,during,training,true current label during training,0.6824380159378052
translation,167,71,results,true current label,seems to,degrade,true current label seems to degrade,0.7259997725486755
translation,167,71,results,performance,compared to using,predicted label,performance compared to using predicted label,0.6819577217102051
translation,167,71,results,degrade,has,performance,degrade has performance,0.5986349582672119
translation,167,71,results,results,Using,true current label,results Using true current label,0.7038201093673706
translation,167,72,results,scheduled sampling method,performs,similarly,scheduled sampling method performs similarly,0.6676734089851379
translation,167,72,results,scheduled sampling method,performs,outperforms,scheduled sampling method performs outperforms,0.6328058242797852
translation,167,72,results,similarly,to,predicted - label method,similarly to predicted - label method,0.5655392408370972
translation,167,72,results,predicted - label method,for,maptask corpus,predicted - label method for maptask corpus,0.5692301392555237
translation,167,72,results,outperforms,for,switchboard corpus,outperforms for switchboard corpus,0.6193754076957703
translation,167,72,results,results,has,scheduled sampling method,results has scheduled sampling method,0.5327832102775574
translation,167,79,results,simple model,with,no current label,simple model with no current label,0.6470699906349182
translation,167,79,results,simple model,performing,worse,simple model performing worse,0.6868823170661926
translation,167,79,results,worse,than,all other models,worse than all other models,0.5948051810264587
translation,167,79,results,worse,does not suffer from,label - bias problem,worse does not suffer from label - bias problem,0.7606565356254578
translation,167,79,results,all other models,in,accuracy,all other models in accuracy,0.5278493762016296
translation,167,79,results,results,has,simple model,results has simple model,0.5724096298217773
translation,167,80,results,uncertainty propagation,suffers,least,uncertainty propagation suffers least,0.7234985828399658
translation,167,80,results,least,from,label bias,least from label bias,0.5737422108650208
translation,167,80,results,current label information,has,uncertainty propagation,current label information has uncertainty propagation,0.532105565071106
translation,167,81,results,outperforms,on,maptask,outperforms on maptask,0.5592165589332581
translation,167,81,results,model,with,no current label,model with no current label,0.6294693350791931
translation,167,81,results,model,on,maptask,model on maptask,0.5882394909858704
translation,167,81,results,no current label,on,switchboard,no current label on switchboard,0.5601604580879211
translation,167,81,results,no current label,for,all values of n,no current label for all values of n,0.6001445055007935
translation,167,81,results,maptask,for,n = 2,maptask for n = 2,0.6601043343544006
translation,167,81,results,outperforms,has,model,outperforms has model,0.6832752227783203
translation,168,190,ablation-analysis,positional embedding,for,images,positional embedding for images,0.6662502884864807
translation,168,190,ablation-analysis,positional embedding,boost,performance,positional embedding boost performance,0.6771661043167114
translation,168,190,ablation-analysis,performance,of,text generation,performance of text generation,0.5907790064811707
translation,168,190,ablation-analysis,text generation,by,( + 0.94 % ),text generation by ( + 0.94 % ),0.5732378959655762
translation,168,190,ablation-analysis,text generation,by,+ 0.58 % ),text generation by + 0.58 % ),0.5825554132461548
translation,168,190,ablation-analysis,( + 0.94 % ),in,bleu score,( + 0.94 % ) in bleu score,0.5355201363563538
translation,168,190,ablation-analysis,+ 0.58 % ),in,rouge -l,+ 0.58 % ) in rouge -l,0.5819565653800964
translation,168,192,ablation-analysis,efficiency,of,position - aware and attribute - aware attention mechanism,efficiency of position - aware and attribute - aware attention mechanism,0.5558947324752808
translation,168,192,ablation-analysis,position - aware and attribute - aware attention mechanism,seen in,increased performance,position - aware and attribute - aware attention mechanism seen in increased performance,0.668972909450531
translation,168,192,ablation-analysis,model 6 ),seen in,increased performance,model 6 ) seen in increased performance,0.699501633644104
translation,168,192,ablation-analysis,increased performance,of,model,increased performance of model,0.628963828086853
translation,168,192,ablation-analysis,increased performance,with respect to,model 5,increased performance with respect to model 5,0.7287412881851196
translation,168,192,ablation-analysis,increased performance,with,improvement,increased performance with improvement,0.647747278213501
translation,168,192,ablation-analysis,model,with respect to,model 4,model with respect to model 4,0.7133116126060486
translation,168,192,ablation-analysis,improvement,of,0.68 % and 0.6 %,improvement of 0.68 % and 0.6 %,0.5999342799186707
translation,168,192,ablation-analysis,0.68 % and 0.6 %,in,rouge -l metric,0.68 % and 0.6 % in rouge -l metric,0.5608611702919006
translation,168,192,ablation-analysis,position - aware and attribute - aware attention mechanism,has,model 6 ),position - aware and attribute - aware attention mechanism has model 6 ),0.544755220413208
translation,168,192,ablation-analysis,ablation analysis,has,efficiency,ablation analysis has efficiency,0.5153630971908569
translation,168,193,ablation-analysis,mfb based fusion technique,helps to improve,performance,mfb based fusion technique helps to improve performance,0.6645761728286743
translation,168,193,ablation-analysis,performance,of,generation model ( model 8 ),performance of generation model ( model 8 ),0.5778538584709167
translation,168,193,ablation-analysis,performance,with,improvement,performance with improvement,0.6677234768867493
translation,168,193,ablation-analysis,improvement,of,3.82 %,improvement of 3.82 %,0.5640873312950134
translation,168,193,ablation-analysis,3.82 %,in,bleu score,3.82 % in bleu score,0.49486488103866577
translation,168,193,ablation-analysis,3.82 %,in,bleu score,3.82 % in bleu score,0.49486488103866577
translation,168,193,ablation-analysis,bleu score,with respect to,baseline model,bleu score with respect to baseline model,0.570514440536499
translation,168,193,ablation-analysis,0.26 % improvement,in,bleu score,0.26 % improvement in bleu score,0.5158976912498474
translation,168,193,ablation-analysis,bleu score,in comparison to,model 6,bleu score in comparison to model 6,0.625289797782898
translation,168,193,ablation-analysis,ablation analysis,has,mfb based fusion technique,ablation analysis has mfb based fusion technique,0.5504804253578186
translation,168,84,baselines,multimodal hred ( mhred ),built upon,hred,multimodal hred ( mhred ) built upon hred,0.6691827178001404
translation,168,84,baselines,hred,to include,text and image modalities,hred to include text and image modalities,0.6520406603813171
translation,168,84,baselines,baselines,has,multimodal hred ( mhred ),baselines has multimodal hred ( mhred ),0.5876293182373047
translation,168,131,baselines,baselines,has,model 2 ( mhred + a ),baselines has model 2 ( mhred + a ),0.5946897864341736
translation,168,133,baselines,baselines,has,model 3 ( mhred + a + pe ),baselines has model 3 ( mhred + a + pe ),0.5999969244003296
translation,168,135,baselines,self-attention,on,text representations,self-attention on text representations,0.5682811737060547
translation,168,135,baselines,text representations,with respect to,position information,text representations with respect to position information,0.6783093214035034
translation,168,135,baselines,model 4 ( mhred + pa ),has,self-attention,model 4 ( mhred + pa ) has self-attention,0.5370948314666748
translation,168,135,baselines,baselines,has,model 4 ( mhred + pa ),baselines has model 4 ( mhred + pa ),0.5945412516593933
translation,168,158,experimental-setup,implementations,done using,py- torch 1 framework,implementations done using py- torch 1 framework,0.5899409055709839
translation,168,158,experimental-setup,experimental setup,has,implementations,experimental setup has implementations,0.5001025199890137
translation,168,159,experimental-setup,experimental setup,use,512 - dimensional word embedding,experimental setup use 512 - dimensional word embedding,0.5661892890930176
translation,168,159,experimental-setup,experimental setup,use,10 - dimensional position embedding,experimental setup use 10 - dimensional position embedding,0.5856096744537354
translation,168,160,experimental-setup,"dropout ( srivastava et al. , 2014 )",with,probability 0.45,"dropout ( srivastava et al. , 2014 ) with probability 0.45",0.6059021949768066
translation,168,160,experimental-setup,experimental setup,use,"dropout ( srivastava et al. , 2014 )","experimental setup use dropout ( srivastava et al. , 2014 )",0.5588595867156982
translation,168,161,experimental-setup,decoding,use,beam search,decoding use beam search,0.6366719603538513
translation,168,161,experimental-setup,beam search,with,beam size 10,beam search with beam size 10,0.7035357356071472
translation,168,161,experimental-setup,experimental setup,During,decoding,experimental setup During decoding,0.6438420414924622
translation,168,162,experimental-setup,model parameters randomly,using,gaussian distribution,model parameters randomly using gaussian distribution,0.6785755157470703
translation,168,162,experimental-setup,gaussian distribution,with,"xavier scheme ( glorot and bengio , 2010 )","gaussian distribution with xavier scheme ( glorot and bengio , 2010 )",0.6375000476837158
translation,168,162,experimental-setup,experimental setup,initialize,model parameters randomly,experimental setup initialize model parameters randomly,0.7876346111297607
translation,168,163,experimental-setup,hidden size,for,all the layers,hidden size for all the layers,0.6047607064247131
translation,168,163,experimental-setup,all the layers,is,512,all the layers is 512,0.5852378606796265
translation,168,163,experimental-setup,experimental setup,has,hidden size,experimental setup has hidden size,0.5616794228553772
translation,168,164,experimental-setup,"amsgrad ( reddi et al. , 2019 )",as,optimizer,"amsgrad ( reddi et al. , 2019 ) as optimizer",0.4987943470478058
translation,168,164,experimental-setup,"amsgrad ( reddi et al. , 2019 )",to mitigate,slow convergence issues,"amsgrad ( reddi et al. , 2019 ) to mitigate slow convergence issues",0.6442592144012451
translation,168,164,experimental-setup,optimizer,for,model training,optimizer for model training,0.6057636141777039
translation,168,164,experimental-setup,model training,to mitigate,slow convergence issues,model training to mitigate slow convergence issues,0.6496013402938843
translation,168,164,experimental-setup,experimental setup,employ,"amsgrad ( reddi et al. , 2019 )","experimental setup employ amsgrad ( reddi et al. , 2019 )",0.5254977345466614
translation,168,165,experimental-setup,uniform label smoothing,with,= 0.1,uniform label smoothing with = 0.1,0.6197128295898438
translation,168,165,experimental-setup,uniform label smoothing,perform,gradient clipping,uniform label smoothing perform gradient clipping,0.5290144681930542
translation,168,165,experimental-setup,gradient clipping,when,gradient norm,gradient clipping when gradient norm,0.629986584186554
translation,168,165,experimental-setup,experimental setup,use,uniform label smoothing,experimental setup use uniform label smoothing,0.5540266036987305
translation,168,165,experimental-setup,experimental setup,perform,gradient clipping,experimental setup perform gradient clipping,0.5520387291908264
translation,168,166,experimental-setup,fc6( 4096 dimension ) layer representation,of,vgg - 19,fc6( 4096 dimension ) layer representation of vgg - 19,0.5593244433403015
translation,168,166,experimental-setup,fc6( 4096 dimension ) layer representation,pretrained on,imagenet,fc6( 4096 dimension ) layer representation pretrained on imagenet,0.7699846625328064
translation,168,166,experimental-setup,image representation,has,fc6( 4096 dimension ) layer representation,image representation has fc6( 4096 dimension ) layer representation,0.5648349523544312
translation,168,166,experimental-setup,experimental setup,For,image representation,experimental setup For image representation,0.5551928877830505
translation,168,8,model,novel position and attribute aware attention mechanism,to learn,enhanced image representation,novel position and attribute aware attention mechanism to learn enhanced image representation,0.5597906708717346
translation,168,8,model,enhanced image representation,conditioned on,user utterance,enhanced image representation conditioned on user utterance,0.7012186646461487
translation,168,8,model,model,propose,novel position and attribute aware attention mechanism,model propose novel position and attribute aware attention mechanism,0.6641051769256592
translation,168,30,model,entirely data-driven response generation model,in,multi-modal setup,entirely data-driven response generation model in multi-modal setup,0.5517570972442627
translation,168,30,model,model,propose,entirely data-driven response generation model,model propose entirely data-driven response generation model,0.6709063649177551
translation,168,188,results,mhred model,is,decent baseline,mhred model is decent baseline,0.5625230669975281
translation,168,188,results,decent baseline,with,"good scores ( 0.6725 rouge -l , 0.4454 bleu )","decent baseline with good scores ( 0.6725 rouge -l , 0.4454 bleu )",0.6068696975708008
translation,168,188,results,results,has,mhred model,results has mhred model,0.5347924828529358
translation,168,189,results,attention,over,text and image representations,attention over text and image representations,0.6966927647590637
translation,168,189,results,attention,provides,absolute improvement,attention provides absolute improvement,0.6481844782829285
translation,168,189,results,absolute improvement,of,( + 0.85 % ),absolute improvement of ( + 0.85 % ),0.5493583083152771
translation,168,189,results,absolute improvement,of,other metrics,absolute improvement of other metrics,0.5055443048477173
translation,168,189,results,absolute improvement,in,other metrics,absolute improvement in other metrics,0.46038320660591125
translation,168,189,results,( + 0.85 % ),in,meteor,( + 0.85 % ) in meteor,0.6023522019386292
translation,168,189,results,results,application of,attention,results application of attention,0.6558489203453064
translation,168,194,results,"final proposed model ( mhred + pa + aa + mfb ( i , t, c )",after incorporating,position and attribute aware attention mechanisms,"final proposed model ( mhred + pa + aa + mfb ( i , t, c ) after incorporating position and attribute aware attention mechanisms",0.699665904045105
translation,168,194,results,position and attribute aware attention mechanisms,along with,mfb fusion,position and attribute aware attention mechanisms along with mfb fusion,0.6083645820617676
translation,168,194,results,state - of - theart performance,with,improvement,state - of - theart performance with improvement,0.6526469588279724
translation,168,194,results,improvement,of,3.23 %,improvement of 3.23 %,0.5677269101142883
translation,168,194,results,improvement,of,3.31 %,improvement of 3.31 %,0.569061279296875
translation,168,194,results,improvement,of,2.34 %,improvement of 2.34 %,0.5635799765586853
translation,168,194,results,3.23 %,in,bleu score,3.23 % in bleu score,0.49898412823677063
translation,168,194,results,3.31 %,in,rouge -l,3.31 % in rouge -l,0.6124773025512695
translation,168,194,results,2.34 %,in,meteor,2.34 % in meteor,0.6064801812171936
translation,168,194,results,2.34 %,in comparison to,existing approaches,2.34 % in comparison to existing approaches,0.6269809603691101
translation,168,194,results,results,has,"final proposed model ( mhred + pa + aa + mfb ( i , t, c )","results has final proposed model ( mhred + pa + aa + mfb ( i , t, c )",0.5361142754554749
translation,168,199,results,proposed model,shown,better performance,proposed model shown better performance,0.6628475785255432
translation,168,199,results,better performance,with,improvement,better performance with improvement,0.6294130682945251
translation,168,199,results,improvement,of,7.47 %,improvement of 7.47 %,0.5647045969963074
translation,168,199,results,7.47 %,in generating,correct responses,7.47 % in generating correct responses,0.6273677349090576
translation,168,199,results,relevance metric,has,proposed model,relevance metric has proposed model,0.5597049593925476
translation,168,199,results,results,for,relevance metric,results for relevance metric,0.5804930329322815
translation,169,216,ablation-analysis,24.1 % and 6.8 %,in,transfer setting,24.1 % and 6.8 % in transfer setting,0.5740925669670105
translation,169,216,ablation-analysis,roberta -mc and roberta,has,drops,roberta -mc and roberta has drops,0.6437286734580994
translation,169,216,ablation-analysis,drops,has,24.1 % and 6.8 %,drops has 24.1 % and 6.8 %,0.5966479778289795
translation,169,238,ablation-analysis,performance,of,roberta and roberta - mc,performance of roberta and roberta - mc,0.6784089803695679
translation,169,238,ablation-analysis,ablation utterance,has,increases,ablation utterance has increases,0.6172899603843689
translation,169,238,ablation-analysis,ablation utterance,has,performance,ablation utterance has performance,0.5722318291664124
translation,169,238,ablation-analysis,increases,has,performance,increases has performance,0.5952932834625244
translation,169,238,ablation-analysis,performance,has,significantly decreases,performance has significantly decreases,0.6196367144584656
translation,169,238,ablation-analysis,roberta and roberta - mc,has,significantly decreases,roberta and roberta - mc has significantly decreases,0.5885175466537476
translation,169,238,ablation-analysis,ablation analysis,As,ablation utterance,ablation analysis As ablation utterance,0.570532500743866
translation,169,240,ablation-analysis,sequence,of,utterance,sequence of utterance,0.6170269250869751
translation,169,240,ablation-analysis,performance,of,roberta - mc,performance of roberta - mc,0.639281153678894
translation,169,240,ablation-analysis,performance,drops by,3.8 %,performance drops by 3.8 %,0.7330220937728882
translation,169,240,ablation-analysis,roberta - mc,drops by,3.8 %,roberta - mc drops by 3.8 %,0.735055148601532
translation,169,240,ablation-analysis,sequence,has,performance,sequence has performance,0.5717343091964722
translation,169,240,ablation-analysis,utterance,has,performance,utterance has performance,0.5822509527206421
translation,169,240,ablation-analysis,ablation analysis,shuffle,sequence,ablation analysis shuffle sequence,0.8025595545768738
translation,169,168,baselines,two lstms,to encode,context and response,two lstms to encode context and response,0.7305291891098022
translation,169,168,baselines,"dual lstm ( lowe et al. , 2015 )",has,two lstms,"dual lstm ( lowe et al. , 2015 ) has two lstms",0.524830162525177
translation,169,168,baselines,baselines,has,"dual lstm ( lowe et al. , 2015 )","baselines has dual lstm ( lowe et al. , 2015 )",0.4945921301841736
translation,169,172,baselines,deep attention matching network,adopt,self attention module,deep attention matching network adopt self attention module,0.5657930970191956
translation,169,172,baselines,self attention module,to encode,response and each utterance,self attention module to encode response and each utterance,0.7626259922981262
translation,169,172,baselines,baselines,has,deep attention matching network,baselines has deep attention matching network,0.4956202208995819
translation,169,45,results,best method,gives,r@1,best method gives r@1,0.6117379665374756
translation,169,45,results,r@1,of,71 %,r@1 of 71 %,0.6502992510795593
translation,169,45,results,significantly underperforms,has,human performance ( 94 % ),significantly underperforms has human performance ( 94 % ),0.6072450876235962
translation,169,45,results,results,has,best method,results has best method,0.567950963973999
translation,169,165,results,correct response,tends to share,more words,correct response tends to share more words,0.6777575612068176
translation,169,165,results,more words,with,context,more words with context,0.6507130861282349
translation,169,165,results,more words,than,incorrect ones,more words than incorrect ones,0.5757430791854858
translation,169,165,results,tf - idf,has,correct response,tf - idf has correct response,0.5746254920959473
translation,169,165,results,results,has,tf - idf,results has tf - idf,0.5418165922164917
translation,169,197,results,tf -idf,achieves,54.98 % r@1 score,tf -idf achieves 54.98 % r@1 score,0.6265953183174133
translation,169,197,results,54.98 % r@1 score,on,ubuntu corpus,54.98 % r@1 score on ubuntu corpus,0.4753282368183136
translation,169,197,results,results,has,tf -idf,results has tf -idf,0.5418165922164917
translation,169,203,results,roberta,gets,71.3 %,roberta gets 71.3 %,0.5924426913261414
translation,169,203,results,roberta,achieves,surprising number,roberta achieves surprising number,0.6356736421585083
translation,169,203,results,71.3 %,on,r@1,71.3 % on r@1,0.6048838496208191
translation,169,203,results,71.3 %,on,r@2,71.3 % on r@2,0.6176933646202087
translation,169,203,results,71.3 %,on,r@2,71.3 % on r@2,0.6176933646202087
translation,169,203,results,89.2 %,on,r@2,89.2 % on r@2,0.6188279986381531
translation,169,204,results,bert -mc and roberta - mc,obtain,similar results,bert -mc and roberta - mc obtain similar results,0.6042148470878601
translation,169,204,results,similar results,with,bert and roberta,similar results with bert and roberta,0.7028481960296631
translation,169,204,results,results,has,bert -mc and roberta - mc,results has bert -mc and roberta - mc,0.5574213266372681
translation,169,209,results,all models,perform,worse,all models perform worse,0.6306739449501038
translation,169,209,results,worse,on,mutual plus,worse on mutual plus,0.5962008237838745
translation,169,209,results,results,has,all models,results has all models,0.5029959678649902
translation,169,210,results,performance,of,multichoice method,performance of multichoice method,0.60536789894104
translation,169,210,results,performance,is,significantly better,performance is significantly better,0.577980101108551
translation,169,210,results,multichoice method,is,significantly better,multichoice method is significantly better,0.5461422801017761
translation,169,210,results,significantly better,than,individual scoring method,significantly better than individual scoring method,0.5797327160835266
translation,169,210,results,results,find that,performance,results find that performance,0.6555676460266113
translation,169,213,results,roberta - mc,showing,outstanding performance,roberta - mc showing outstanding performance,0.7278267741203308
translation,169,213,results,outperforms,by,large margin,outperforms by large margin,0.6290680766105652
translation,169,213,results,outperforms,showing,outstanding performance,outperforms showing outstanding performance,0.70880526304245
translation,169,213,results,others,by,large margin,others by large margin,0.5819982886314392
translation,169,213,results,outstanding performance,on,reasoning problems,outstanding performance on reasoning problems,0.4900265038013458
translation,169,213,results,roberta - mc,has,outperforms,roberta - mc has outperforms,0.6612902283668518
translation,169,213,results,outperforms,has,others,outperforms has others,0.6126620769500732
translation,169,213,results,results,has,roberta - mc,results has roberta - mc,0.5298922657966614
translation,169,217,results,outperforms,showing that,individual scoring method,outperforms showing that individual scoring method,0.6637336015701294
translation,169,217,results,roberta - mc,showing that,individual scoring method,roberta - mc showing that individual scoring method,0.6496469378471375
translation,169,217,results,individual scoring method,is,more robust,individual scoring method is more robust,0.5485570430755615
translation,169,217,results,more robust,when,safe response,more robust when safe response,0.6755632758140564
translation,169,217,results,safe response,not fed during,training,safe response not fed during training,0.7434737086296082
translation,169,217,results,individual scoring roberta,has,outperforms,individual scoring roberta has outperforms,0.5908725261688232
translation,169,217,results,outperforms,has,roberta - mc,outperforms has roberta - mc,0.6731129288673401
translation,169,217,results,results,has,individual scoring roberta,results has individual scoring roberta,0.5587567687034607
translation,169,222,results,significantly outperforms,in,attitude reasoning,significantly outperforms in attitude reasoning,0.5532551407814026
translation,169,222,results,significantly outperforms,in,multi-fact reasoning,significantly outperforms in multi-fact reasoning,0.5283779501914978
translation,169,222,results,bert - mc,in,attitude reasoning,bert - mc in attitude reasoning,0.5433413982391357
translation,169,222,results,bert - mc,in,multi-fact reasoning,bert - mc in multi-fact reasoning,0.5594416260719299
translation,169,222,results,significantly outperforms,has,bert - mc,significantly outperforms has bert - mc,0.6084380149841309
translation,169,222,results,results,has,roberta,results has roberta,0.530095100402832
translation,169,239,results,roberta and roberta - mc,achieve,only 43.7 % and 47.7 %,roberta and roberta - mc achieve only 43.7 % and 47.7 %,0.6177598834037781
translation,169,239,results,only 43.7 % and 47.7 %,after ablating,all utterances,only 43.7 % and 47.7 % after ablating all utterances,0.7908700704574585
translation,169,239,results,all utterances,in,context,all utterances in context,0.5271069407463074
translation,169,239,results,results,has,roberta and roberta - mc,results has roberta and roberta - mc,0.5588083863258362
translation,170,137,ablation-analysis,all the features,except,adjective unigram probability,all the features except adjective unigram probability,0.6171045303344727
translation,170,137,ablation-analysis,positively,to,model learning,positively to model learning,0.5727576017379761
translation,170,137,ablation-analysis,ablation analysis,found,all the features,ablation analysis found all the features,0.6494871973991394
translation,170,151,ablation-analysis,semantic features,significantly contribute to,decision tree learning,semantic features significantly contribute to decision tree learning,0.5599897503852844
translation,170,151,ablation-analysis,ablation analysis,shows that,semantic features,ablation analysis shows that semantic features,0.6522139310836792
translation,170,152,ablation-analysis,feature of bigram probability,of,adjective -noun pair,feature of bigram probability of adjective -noun pair,0.5362806916236877
translation,170,152,ablation-analysis,feature of bigram probability,contributes,most,feature of bigram probability contributes most,0.6418836116790771
translation,170,152,ablation-analysis,most,to,model learning,most to model learning,0.5880360007286072
translation,170,152,ablation-analysis,ablation analysis,show,feature of bigram probability,ablation analysis show feature of bigram probability,0.5700857043266296
translation,170,153,ablation-analysis,precision,drops by,21.3 % and 10.6 %,precision drops by 21.3 % and 10.6 %,0.7032170295715332
translation,170,153,ablation-analysis,21.3 % and 10.6 %,reaching,lowest precision,21.3 % and 10.6 % reaching lowest precision,0.6974779963493347
translation,170,153,ablation-analysis,lowest precision,among,all the leave- one - out experiments,lowest precision among all the leave- one - out experiments,0.5574281215667725
translation,170,157,ablation-analysis,precision,drops to,63.4 % and 66.6 %,precision drops to 63.4 % and 66.6 %,0.6984795928001404
translation,170,157,ablation-analysis,63.4 % and 66.6 %,on,two annotations,63.4 % and 66.6 % on two annotations,0.5430455803871155
translation,170,157,ablation-analysis,63.4 % and 66.6 %,decreasing by,14.5 % and 7.9 %,63.4 % and 66.6 % decreasing by 14.5 % and 7.9 %,0.7640608549118042
translation,170,157,ablation-analysis,sentiment features,has,precision,sentiment features has precision,0.5510790348052979
translation,170,157,ablation-analysis,ablation analysis,Without,sentiment features,ablation analysis Without sentiment features,0.6956855058670044
translation,170,158,ablation-analysis,sentiment features,contribute significantly,classification,sentiment features contribute significantly classification,0.6200737357139587
translation,170,158,ablation-analysis,ablation analysis,shows that,sentiment features,ablation analysis shows that sentiment features,0.6493721604347229
translation,170,47,model,automatic approach,to classifying,high / low informative phrases,automatic approach to classifying high / low informative phrases,0.6419147849082947
translation,170,47,model,model,propose,automatic approach,model propose automatic approach,0.7228855490684509
translation,170,144,results,statistical models,trained by,svms,statistical models trained by svms,0.7134345769882202
translation,170,144,results,statistical models,trained by,decision tree algorithms,statistical models trained by decision tree algorithms,0.6534759998321533
translation,170,144,results,outperform,has,baseline,outperform has baseline,0.6223028898239136
translation,170,144,results,baseline,has,significantly,baseline has significantly,0.5920042991638184
translation,170,145,results,outperforms,by,10.5 % and 11.9 %,outperforms by 10.5 % and 11.9 %,0.6204180121421814
translation,170,145,results,baseline,by,10.5 % and 11.9 %,baseline by 10.5 % and 11.9 %,0.5657365918159485
translation,170,145,results,baseline,on,two annotation sets,baseline on two annotation sets,0.5161300897598267
translation,170,145,results,10.5 % and 11.9 %,on,two annotation sets,10.5 % and 11.9 % on two annotation sets,0.5392028093338013
translation,170,145,results,svm model,has,outperforms,svm model has outperforms,0.6251876354217529
translation,170,145,results,outperforms,has,baseline,outperforms has baseline,0.6131853461265564
translation,170,145,results,results,has,svm model,results has svm model,0.5709033012390137
translation,170,146,results,outperforms,by,16.4 % and 23.2 %,outperforms by 16.4 % and 23.2 %,0.626472532749176
translation,170,146,results,baseline,by,16.4 % and 23.2 %,baseline by 16.4 % and 23.2 %,0.5604331493377686
translation,170,146,results,average relative improvement,of,36 %,average relative improvement of 36 %,0.5968326330184937
translation,170,146,results,average relative improvement,of,13 %,average relative improvement of 13 %,0.6010512709617615
translation,170,146,results,average relative improvement,of,13 %,average relative improvement of 13 %,0.6010512709617615
translation,170,146,results,svm model,by,5.9 % and 11.3 %,svm model by 5.9 % and 11.3 %,0.5638232231140137
translation,170,146,results,average relative improvement,of,13 %,average relative improvement of 13 %,0.6010512709617615
translation,170,146,results,decision tree model,has,outperforms,decision tree model has outperforms,0.6210178732872009
translation,170,146,results,outperforms,has,baseline,outperforms has baseline,0.6131853461265564
translation,170,146,results,outperforms,has,svm model,outperforms has svm model,0.5915688872337341
translation,170,146,results,5.9 % and 11.3 %,has,average relative improvement,5.9 % and 11.3 % has average relative improvement,0.569715678691864
translation,170,146,results,results,has,decision tree model,results has decision tree model,0.5502873063087463
translation,170,147,results,classification model,using,decision tree algorithm,classification model using decision tree algorithm,0.6749396920204163
translation,170,147,results,classification model,achieve,precision,classification model achieve precision,0.6319196224212646
translation,170,147,results,precision,of,77.9 % and 74.5 %,precision of 77.9 % and 74.5 %,0.5564801692962646
translation,170,147,results,77.9 % and 74.5 %,compared with,ground truth,77.9 % and 74.5 % compared with ground truth,0.6723366379737854
translation,170,147,results,results,has,classification model,results has classification model,0.5731441974639893
translation,170,150,results,precision,is,70.6 % and 65.4 %,precision is 70.6 % and 65.4 %,0.5576044917106628
translation,170,150,results,70.6 % and 65.4 %,on,two annotation sets,70.6 % and 65.4 % on two annotation sets,0.5210753083229065
translation,170,150,results,70.6 % and 65.4 %,lower by,7.3 % and 9.1 %,70.6 % and 65.4 % lower by 7.3 % and 9.1 %,0.709067702293396
translation,170,150,results,model,with,all the features ( 77.9 % and 74.5 % ),model with all the features ( 77.9 % and 74.5 % ),0.621314525604248
translation,170,150,results,semantic features,has,precision,semantic features has precision,0.5307118892669678
translation,170,150,results,training,has,model,training has model,0.5824191570281982
translation,170,156,results,sentiment,of,phrases,sentiment of phrases,0.5900672078132629
translation,170,156,results,sentiment,plays,important role,sentiment plays important role,0.7374776601791382
translation,170,156,results,results,has,sentiment,results has sentiment,0.5444703698158264
translation,170,160,results,experimental results,show,decision tree algorithm,experimental results show decision tree algorithm,0.6269479393959045
translation,170,160,results,decision tree algorithm,has,outperforms,decision tree algorithm has outperforms,0.6227176189422607
translation,170,160,results,outperforms,has,svms,outperforms has svms,0.6074677109718323
translation,170,160,results,outperforms,has,heuristic rule baseline,outperforms has heuristic rule baseline,0.5999661087989807
translation,170,160,results,heuristic rule baseline,has,significantly,heuristic rule baseline has significantly,0.6387571692466736
translation,170,160,results,results,show,decision tree algorithm,results show decision tree algorithm,0.6620612144470215
translation,170,160,results,results,has,experimental results,results has experimental results,0.5342857837677002
translation,171,86,model,inverse reinforcement learning,to train,"general , automated navigation model","inverse reinforcement learning to train general , automated navigation model",0.6417443752288818
translation,171,86,model,"general , automated navigation model",from,manual human demonstrations,"general , automated navigation model from manual human demonstrations",0.5469847321510315
translation,171,86,model,model,implements,inverse reinforcement learning,model implements inverse reinforcement learning,0.6704344153404236
translation,172,8,baselines,structured dialogue management,dynamically generate,series of questions,structured dialogue management dynamically generate series of questions,0.7315465211868286
translation,172,8,baselines,series of questions,based on,personal kg,series of questions based on personal kg,0.7226289510726929
translation,172,8,baselines,personal kg,to ask,applicants,personal kg to ask applicants,0.657241940498352
translation,172,174,baselines,hierarchical rule,uses,hand-crafted hierarchical policy,hierarchical rule uses hand-crafted hierarchical policy,0.5935994386672974
translation,172,174,baselines,hand-crafted hierarchical policy,to unfold,dialogues,hand-crafted hierarchical policy to unfold dialogues,0.6156288981437683
translation,172,174,baselines,hierarchical rule,has,rule- based system,hierarchical rule has rule- based system,0.5999489426612854
translation,172,174,baselines,baselines,has,hierarchical rule,baselines has hierarchical rule,0.6022455096244812
translation,172,177,baselines,hp -s,does not use,message passing,hp -s does not use message passing,0.6575002074241638
translation,172,177,baselines,neural dialogue system,uses,hierarchical policy,neural dialogue system uses hierarchical policy,0.5417883396148682
translation,172,177,baselines,hierarchical policy,to unfold,dialogues,hierarchical policy to unfold dialogues,0.6306658387184143
translation,172,177,baselines,message passing,to infer,dialogue states,message passing to infer dialogue states,0.7421249151229858
translation,172,177,baselines,hp -s,has,neural dialogue system,hp -s has neural dialogue system,0.6032482385635376
translation,172,177,baselines,baselines,has,hp -s,baselines has hp -s,0.6362096667289734
translation,172,181,experiments,32 applicants ' information,for,simulation,32 applicants ' information for simulation,0.552082359790802
translation,172,187,experiments,mp -s,with,flat rule,mp -s with flat rule,0.6803103089332581
translation,172,188,experiments,hp -s and full -s,with,hierarchical rule,hp -s and full -s with hierarchical rule,0.6656820178031921
translation,172,182,hyperparameters,maximum interaction turns,of,system and the worker,maximum interaction turns of system and the worker,0.5908653736114502
translation,172,182,hyperparameters,system and the worker,are,40 and 10,system and the worker are 40 and 10,0.6296932101249695
translation,172,182,hyperparameters,hyperparameters,has,maximum interaction turns,hyperparameters has maximum interaction turns,0.5112295150756836
translation,172,183,hyperparameters,iteration depth k,is,2,iteration depth k is 2,0.6079734563827515
translation,172,183,hyperparameters,2,in,message passing,2 in message passing,0.5062963962554932
translation,172,183,hyperparameters,hyperparameters,has,iteration depth k,hyperparameters has iteration depth k,0.5177463293075562
translation,172,185,hyperparameters,discount factors,are,0.999 and 0.99,discount factors are 0.999 and 0.99,0.5839230418205261
translation,172,185,hyperparameters,0.999 and 0.99,for,manager and worker,0.999 and 0.99 for manager and worker,0.6464231610298157
translation,172,185,hyperparameters,hyperparameters,has,discount factors,hyperparameters has discount factors,0.5109073519706726
translation,172,186,hyperparameters,neural dialogue systems,pre-trained with,rule- based systems,neural dialogue systems pre-trained with rule- based systems,0.702243983745575
translation,172,186,hyperparameters,rule- based systems,for,20 epochs,rule- based systems for 20 epochs,0.6364322900772095
translation,172,186,hyperparameters,hyperparameters,has,neural dialogue systems,hyperparameters has neural dialogue systems,0.5506572127342224
translation,172,189,hyperparameters,neural dialogue systems,trained for,300 epochs,neural dialogue systems trained for 300 epochs,0.7447850108146667
translation,172,189,hyperparameters,rl stage,has,neural dialogue systems,rl stage has neural dialogue systems,0.5914971828460693
translation,172,189,hyperparameters,hyperparameters,In,rl stage,hyperparameters In rl stage,0.4873306155204773
translation,172,6,model,novel interactive dialogue system,consists of,two modules,novel interactive dialogue system consists of two modules,0.6494884490966797
translation,172,6,model,identity fraud detection,has,in loan applications,identity fraud detection has in loan applications,0.5227649807929993
translation,172,7,model,knowledge graph ( kg ) constructor,organizing,personal information,knowledge graph ( kg ) constructor organizing personal information,0.6655644774436951
translation,172,7,model,personal information,for,each loan applicant,personal information for each loan applicant,0.5399413704872131
translation,172,38,model,structured dialogue management,to explore,optimal dialogue strategy,structured dialogue management to explore optimal dialogue strategy,0.6937685012817383
translation,172,38,model,optimal dialogue strategy,with,reinforcement learning,optimal dialogue strategy with reinforcement learning,0.6088992953300476
translation,172,39,model,dialogue management,consists of,kg - based dialogue state tracker ( kg - dst ),dialogue management consists of kg - based dialogue state tracker ( kg - dst ),0.6305167078971863
translation,172,39,model,dialogue management,consists of,hierarchical dialogue policy ( hdp ),dialogue management consists of hierarchical dialogue policy ( hdp ),0.6528530716896057
translation,172,39,model,kg - based dialogue state tracker ( kg - dst ),that treats,embeddings,kg - based dialogue state tracker ( kg - dst ) that treats embeddings,0.6067761182785034
translation,172,39,model,embeddings,of,nodes,embeddings of nodes,0.5912385582923889
translation,172,39,model,nodes,in,kg,nodes in kg,0.5539510846138
translation,172,39,model,hierarchical dialogue policy ( hdp ),where,high - level and low-level agents unfold,hierarchical dialogue policy ( hdp ) where high - level and low-level agents unfold,0.6283175349235535
translation,172,39,model,high - level and low-level agents unfold,has,dialogue together,high - level and low-level agents unfold has dialogue together,0.5870427489280701
translation,172,39,model,model,has,dialogue management,model has dialogue management,0.6085423827171326
translation,172,194,results,accuracy,of,flat rule,accuracy of flat rule,0.6186766624450684
translation,172,194,results,accuracy,of,data-driven counterpart,accuracy of data-driven counterpart,0.5752447843551636
translation,172,194,results,accuracy,of,data-driven counterpart,accuracy of data-driven counterpart,0.5752447843551636
translation,172,194,results,flat rule,lower than,hierarchical rule,flat rule lower than hierarchical rule,0.6838364005088806
translation,172,194,results,accuracy,of,data-driven counterpart,accuracy of data-driven counterpart,0.5752447843551636
translation,172,194,results,data-driven counterpart,of,flat rule ( mp - s ),data-driven counterpart of flat rule ( mp - s ),0.5736232995986938
translation,172,194,results,data-driven counterpart,is,slightly 1768 higher,data-driven counterpart is slightly 1768 higher,0.5916123986244202
translation,172,194,results,slightly 1768 higher,than,randomly guessing,slightly 1768 higher than randomly guessing,0.6521991491317749
translation,172,194,results,results,see that,accuracy,results see that accuracy,0.6826137900352478
translation,172,196,results,hp -s,achieves,higher accuracy,hp -s achieves higher accuracy,0.690475583076477
translation,172,196,results,higher accuracy,than,rule- based counterpart ( hierarchical rule ),higher accuracy than rule- based counterpart ( hierarchical rule ),0.5803096294403076
translation,172,196,results,higher accuracy,within,much fewer turns,higher accuracy within much fewer turns,0.6623842716217041
translation,172,196,results,results,has,hp -s,results has hp -s,0.5576576590538025
translation,172,198,results,full - s,achieves,best accuracy,full - s achieves best accuracy,0.6919843554496765
translation,172,198,results,message passing,has,full - s,message passing has full - s,0.6269600987434387
translation,172,198,results,hierarchical policy,has,full - s,hierarchical policy has full - s,0.6486912369728088
translation,172,198,results,results,equipped with,message passing,results equipped with message passing,0.6449956893920898
translation,172,208,results,convergence speed,of,full - s,convergence speed of full - s,0.6371937990188599
translation,172,208,results,full - s,faster than,hp -s,full - s faster than hp -s,0.7986958026885986
translation,172,208,results,hp -s,in,pre-training and the rl stages,hp -s in pre-training and the rl stages,0.5576255321502686
translation,172,208,results,hp -s,both,pre-training and the rl stages,hp -s both pre-training and the rl stages,0.6922440528869629
translation,172,208,results,results,see that,convergence speed,results see that convergence speed,0.687335729598999
translation,173,66,baselines,dip,based on,deep-q networks ( dqn ),dip based on deep-q networks ( dqn ),0.6930952072143555
translation,173,61,experimental-setup,experimental setup,implemented using,pydial toolkit,experimental setup implemented using pydial toolkit,0.6438909769058228
translation,173,6,model,novel dialogue management architecture,based on,feudal rl,novel dialogue management architecture based on feudal rl,0.6811261177062988
translation,173,6,model,decision,into,two steps,decision into two steps,0.6350046396255493
translation,173,6,model,first step,where,master policy,first step where master policy,0.6489347815513611
translation,173,6,model,first step,where,primitive action,first step where primitive action,0.6269939541816711
translation,173,6,model,master policy,selects,subset of primitive actions,master policy selects subset of primitive actions,0.7031981348991394
translation,173,6,model,second step,where,primitive action,second step where primitive action,0.6349835991859436
translation,173,6,model,primitive action,chosen from,selected subset,primitive action chosen from selected subset,0.6515839695930481
translation,173,6,model,model,propose,novel dialogue management architecture,model propose novel dialogue management architecture,0.6830825805664062
translation,173,7,model,structural information,included in,domain ontology,structural information included in domain ontology,0.5837670564651489
translation,173,7,model,structural information,to,abstract,structural information to abstract,0.5413209199905396
translation,173,7,model,domain ontology,to,abstract,domain ontology to abstract,0.4403505027294159
translation,173,7,model,different parts,of,abstracted state,different parts of abstracted state,0.6007043123245239
translation,173,7,model,abstract,has,dialogue state space,abstract has dialogue state space,0.5211847424507141
translation,173,7,model,model,has,structural information,model has structural information,0.5285148620605469
translation,173,24,model,feudal dialogue policy,decomposes,decision,feudal dialogue policy decomposes decision,0.7367675304412842
translation,173,24,model,model,introduce,feudal dialogue policy,model introduce feudal dialogue policy,0.6152908802032471
translation,173,25,model,policy,takes,slot independent or slot dependent action,policy takes slot independent or slot dependent action,0.660101592540741
translation,173,25,model,first step,has,policy,first step has policy,0.5864288806915283
translation,173,95,model,novel dialogue management architecture,based on,feudal rl,novel dialogue management architecture based on feudal rl,0.6811261177062988
translation,173,95,model,substantially outperforms,has,previous state of the art,substantially outperforms has previous state of the art,0.5582051277160645
translation,173,95,model,model,presented,novel dialogue management architecture,model presented novel dialogue management architecture,0.672585129737854
translation,173,96,model,set of slot dependent policies,with,shared parameters,set of slot dependent policies with shared parameters,0.6438065767288208
translation,173,96,model,scalability,to,large domains,scalability to large domains,0.558276355266571
translation,173,96,model,model,defining,set of slot dependent policies,model defining set of slot dependent policies,0.6973995566368103
translation,173,83,results,every other other policy,in,all the environments,every other other policy in all the environments,0.5429370403289795
translation,173,83,results,all the environments,except,env.,all the environments except env.,0.7270888090133667
translation,173,83,results,fdqn policy,has,substantially outperforms,fdqn policy has substantially outperforms,0.6041935086250305
translation,173,83,results,substantially outperforms,has,every other other policy,substantially outperforms has every other other policy,0.5442042946815491
translation,173,83,results,results,has,fdqn policy,results has fdqn policy,0.5417234897613525
translation,173,84,results,more considerable,in,two largest domains ( sfr and lap ),more considerable in two largest domains ( sfr and lap ),0.5724728107452393
translation,173,84,results,more considerable,with,gains,more considerable with gains,0.6923140287399292
translation,173,84,results,up to 5 points,in,accumulated reward,up to 5 points in accumulated reward,0.5649996995925903
translation,173,84,results,accumulated reward,in,most challenging environments ( e.g. env. 4 lap ),accumulated reward in most challenging environments ( e.g. env. 4 lap ),0.4808689057826996
translation,173,84,results,gains,has,up to 5 points,gains has up to 5 points,0.580620527267456
translation,173,84,results,results,has,performance increase,results has performance increase,0.5924491882324219
translation,173,85,results,handcrafted policy ( hdc. ),in,environments 2 to 6,handcrafted policy ( hdc. ) in environments 2 to 6,0.558214008808136
translation,173,85,results,fdqn,has,consistently outperforms,fdqn has consistently outperforms,0.6151248812675476
translation,173,85,results,consistently outperforms,has,handcrafted policy ( hdc. ),consistently outperforms has handcrafted policy ( hdc. ),0.5867424607276917
translation,173,85,results,results,has,fdqn,results has fdqn,0.5285598635673523
translation,173,86,results,results,for,fdqn and dip - dqn,results for fdqn and dip - dqn,0.6083866357803345
translation,173,86,results,fdqn and dip - dqn,are,rather low,fdqn and dip - dqn are rather low,0.6259059309959412
translation,173,86,results,rather low,specially for,dip - dqn,rather low specially for dip - dqn,0.743736743927002
translation,173,86,results,results,for,fdqn and dip - dqn,results for fdqn and dip - dqn,0.6083866357803345
translation,173,87,results,differs,absence of,action masks,differs absence of action masks,0.7389339208602905
translation,173,87,results,outperform,has,every other algorithm,outperform has every other algorithm,0.5740036368370056
translation,174,26,model,two novel modelindepedent metrics,to evaluate,dataset quality,two novel modelindepedent metrics to evaluate dataset quality,0.6571242213249207
translation,174,26,model,model,propose,two novel modelindepedent metrics,model propose two novel modelindepedent metrics,0.6918521523475647
translation,174,78,model,model,consider,recently developed neural network approach,model consider recently developed neural network approach,0.7094561457633972
translation,174,136,results,both scenario and paraphrase jobs,mixture of both,generic and specific prompts,both scenario and paraphrase jobs mixture of both generic and specific prompts,0.7662851214408875
translation,174,136,results,generic and specific prompts,yields,training data,generic and specific prompts yields training data,0.7138845324516296
translation,174,136,results,training data,with,higher coverage,training data with higher coverage,0.6355448961257935
translation,174,136,results,models,with,higher accuracy,models with higher accuracy,0.6275522112846375
translation,174,136,results,higher accuracy,than using,only generic or specific prompts,higher accuracy than using only generic or specific prompts,0.6501924395561218
translation,174,136,results,results,For,both scenario and paraphrase jobs,results For both scenario and paraphrase jobs,0.5805484056472778
translation,174,155,results,number of prompts,from,1 to 2,number of prompts from 1 to 2,0.5611658096313477
translation,174,155,results,number of prompts,from,4 to 5,number of prompts from 4 to 5,0.571038544178009
translation,174,155,results,number of prompts,increases,accuracy,number of prompts increases accuracy,0.7456325888633728
translation,174,155,results,number of prompts,increases,accuracy,number of prompts increases accuracy,0.7456325888633728
translation,174,155,results,number of prompts,from,4 to 5,number of prompts from 4 to 5,0.571038544178009
translation,174,155,results,1 to 2,increases,accuracy,1 to 2 increases accuracy,0.739332914352417
translation,174,155,results,1 to 2,increases,accuracy,1 to 2 increases accuracy,0.739332914352417
translation,174,155,results,accuracy,by,6.9 % and 7.6 %,accuracy by 6.9 % and 7.6 %,0.5834430456161499
translation,174,155,results,accuracy,by,only 1.6 % and 1.3 %,accuracy by only 1.6 % and 1.3 %,0.5689206123352051
translation,174,155,results,accuracy,increasing,number of prompts,accuracy increasing number of prompts,0.7071725130081177
translation,174,155,results,accuracy,by,only 1.6 % and 1.3 %,accuracy by only 1.6 % and 1.3 %,0.5689206123352051
translation,174,155,results,6.9 % and 7.6 %,for,svm and fasttext,6.9 % and 7.6 % for svm and fasttext,0.6281675696372986
translation,174,155,results,number of prompts,from,4 to 5,number of prompts from 4 to 5,0.571038544178009
translation,174,155,results,number of prompts,improves,accuracy,number of prompts improves accuracy,0.7010511755943298
translation,174,155,results,4 to 5,improves,accuracy,4 to 5 improves accuracy,0.7439642548561096
translation,174,155,results,accuracy,by,only 1.6 % and 1.3 %,accuracy by only 1.6 % and 1.3 %,0.5689206123352051
translation,174,155,results,results,Increasing,number of prompts,results Increasing number of prompts,0.6726585626602173
translation,175,180,experimental-setup,pytorch,trained with,learning rate,pytorch trained with learning rate,0.7348036170005798
translation,175,180,experimental-setup,pytorch,trained with,batch size,pytorch trained with batch size,0.7343775033950806
translation,175,180,experimental-setup,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,175,180,experimental-setup,batch size,of,512,batch size of 512,0.6329059600830078
translation,175,180,experimental-setup,experimental setup,implemented in,pytorch,experimental setup implemented in pytorch,0.7101436853408813
translation,175,181,experimental-setup,dimension,of,word embeddings,dimension of word embeddings,0.5582399964332581
translation,175,181,experimental-setup,dimension,set to,512,dimension set to 512,0.7443968057632446
translation,175,181,experimental-setup,hidden dimensions,of,lstm units,hidden dimensions of lstm units,0.5491944551467896
translation,175,181,experimental-setup,lstm units,set to,512,lstm units set to 512,0.6410451531410217
translation,175,181,experimental-setup,experimental setup,set to,512,experimental setup set to 512,0.6521086692810059
translation,175,181,experimental-setup,experimental setup,has,dimension,experimental setup has dimension,0.4825572967529297
translation,175,182,experimental-setup,parameters,optimised using,"adam ( kingma and ba , 2014 )","parameters optimised using adam ( kingma and ba , 2014 )",0.707689642906189
translation,175,182,experimental-setup,experimental setup,has,parameters,experimental setup has parameters,0.4818422794342041
translation,175,4,experiments,photobook dataset,large-scale collection of,"visually - grounded , task - oriented dialogues in english","photobook dataset large-scale collection of visually - grounded , task - oriented dialogues in english",0.6857957243919373
translation,175,4,experiments,"visually - grounded , task - oriented dialogues in english",to investigate,shared dialogue history,"visually - grounded , task - oriented dialogues in english to investigate shared dialogue history",0.5924562215805054
translation,175,4,experiments,shared dialogue history,has,accumulating during conversation,shared dialogue history has accumulating during conversation,0.5613570213317871
translation,175,188,results,resolution capabilities,of,our model,resolution capabilities of our model,0.5898066163063049
translation,175,188,results,resolution capabilities,well above,baseline,resolution capabilities well above baseline,0.7159947752952576
translation,175,188,results,results,show that,resolution capabilities,results show that resolution capabilities,0.5208039879798889
translation,175,189,results,history model,achieves,higher recall and f-score,history model achieves higher recall and f-score,0.6705443263053894
translation,175,189,results,higher recall and f-score,than,no-history model,higher recall and f-score than no-history model,0.5874758958816528
translation,175,189,results,results,has,history model,results has history model,0.5241687893867493
translation,175,196,results,history model,yields,higher results,history model yields higher results,0.7165493965148926
translation,175,196,results,higher results,than,no-history model,higher results than no-history model,0.6022728085517883
translation,175,196,results,no-history model,when it comes to,resolving,no-history model when it comes to resolving,0.7533054947853088
translation,175,196,results,segments,refer to,image,segments refer to image,0.6196605563163757
translation,175,196,results,already been referred to earlier,within,dialogue ( positions > 1 ),already been referred to earlier within dialogue ( positions > 1 ),0.6695716381072998
translation,175,196,results,resolving,has,segments,resolving has segments,0.606431245803833
translation,175,196,results,results,has,history model,results has history model,0.5241687893867493
translation,175,200,results,  blind   model,by about,21 points,  blind   model by about 21 points,0.6669080853462219
translation,175,200,results,  blind   model,by about,14 points,  blind   model by about 14 points,0.6645556092262268
translation,175,200,results,21 points,in,precision,21 points in precision,0.5860859751701355
translation,175,200,results,14 points,in,recall,14 points in recall,0.5953186750411987
translation,175,200,results,history model,has,outperforms,history model has outperforms,0.6567355990409851
translation,175,200,results,outperforms,has,  blind   model,outperforms has   blind   model,0.6125141978263855
translation,175,200,results,results,has,history model,results has history model,0.5241687893867493
translation,176,186,experiments,best submitted systems,managed to match,performance,best submitted systems managed to match performance,0.6618736386299133
translation,176,186,experiments,performance,of,baseline system,performance of baseline system,0.589571475982666
translation,176,186,experiments,tourism english subtask,has,best submitted systems,tourism english subtask has best submitted systems,0.5852225422859192
translation,176,12,results,submissions,manage to,significantly beat,submissions manage to significantly beat,0.7095568776130676
translation,176,12,results,submissions,achieving,f-measure,submissions achieving f-measure,0.6494797468185425
translation,176,12,results,significantly beat,of,baseline,significantly beat of baseline,0.5865095853805542
translation,176,12,results,significantly beat,in comparison to,baseline,significantly beat in comparison to baseline,0.7046626210212708
translation,176,12,results,significantly beat,for,baseline,significantly beat for baseline,0.67132967710495
translation,176,12,results,f-measure,of,0.69,f-measure of 0.69,0.5626325011253357
translation,176,12,results,0.69,in comparison to,0.56,0.69 in comparison to 0.56,0.6349037885665894
translation,176,12,results,0.56,for,baseline,0.56 for baseline,0.6398574113845825
translation,176,12,results,significantly beat,has,baseline,significantly beat has baseline,0.5890014171600342
translation,176,12,results,results,has,submissions,results has submissions,0.5178678035736084
translation,176,183,results,weighted f-measure,see that,all domains but,weighted f-measure see that all domains but,0.7388317584991455
translation,176,183,results,weighted f-measure,in,all domains but,weighted f-measure in all domains but,0.5546576380729675
translation,176,183,results,manages to outperform,provided by,organizers,manages to outperform provided by organizers,0.6630386710166931
translation,176,183,results,baseline,provided by,organizers,baseline provided by organizers,0.7016690373420715
translation,176,183,results,weighted f-measure,has,at least one submission,weighted f-measure has at least one submission,0.5640305876731873
translation,176,183,results,all domains but,has,tourism english,all domains but has tourism english,0.6027130484580994
translation,176,183,results,all domains but,has,at least one submission,all domains but has at least one submission,0.6042413115501404
translation,176,183,results,tourism english,has,at least one submission,tourism english has at least one submission,0.5307300090789795
translation,176,183,results,at least one submission,has,manages to outperform,at least one submission has manages to outperform,0.6010538935661316
translation,176,183,results,manages to outperform,has,baseline,manages to outperform has baseline,0.6200125217437744
translation,176,183,results,results,Focusing on,weighted f-measure,results Focusing on weighted f-measure,0.726662278175354
translation,176,184,results,travel english,achieves,0.51 weighted f-measure,travel english achieves 0.51 weighted f-measure,0.6018831729888916
translation,176,184,results,baseline system,achieves,0.51 weighted f-measure,baseline system achieves 0.51 weighted f-measure,0.5869765281677246
translation,176,184,results,two out of the three systems,achieving,0.68 and 0.58,two out of the three systems achieving 0.68 and 0.58,0.6176796555519104
translation,176,184,results,travel english,has,baseline system,travel english has baseline system,0.5757363438606262
translation,176,184,results,results,In,travel english,results In travel english,0.5188270211219788
translation,176,185,results,improvement,over,baseline,improvement over baseline,0.7266895771026611
translation,176,185,results,baseline,greater for,travel greek subtask,baseline greater for travel greek subtask,0.6828703284263611
translation,176,185,results,results,has,improvement,results has improvement,0.6248279809951782
translation,177,199,baselines,lstm - based encoder-encoder,with,attention layers,lstm - based encoder-encoder with attention layers,0.6259576082229614
translation,177,199,baselines,attention layers,between,user queries,attention layers between user queries,0.6410087943077087
translation,177,199,baselines,attention layers,between,temporal - level visual and audio features,attention layers between temporal - level visual and audio features,0.5997765064239502
translation,177,200,baselines,baselines,has,baseline + gru + hierattn,baselines has baseline + gru + hierattn,0.5770164132118225
translation,177,202,baselines,fa + hred,adopts,film neural blocks,fa + hred adopts film neural blocks,0.6868389844894409
translation,177,202,baselines,film neural blocks,for,language-vision dependency learning,film neural blocks for language-vision dependency learning,0.6222572326660156
translation,177,202,baselines,baselines,has,fa + hred,baselines has fa + hred,0.5990971326828003
translation,177,203,baselines,baselines,has,video summarization,baselines has video summarization,0.5253332257270813
translation,177,204,baselines,student - teacher,adopts,dual network architecture,student - teacher adopts dual network architecture,0.6446107029914856
translation,177,204,baselines,dual network architecture,in which,student network,dual network architecture in which student network,0.6443575024604797
translation,177,204,baselines,student network,trained to mimic,teacher network,student network trained to mimic teacher network,0.6657612323760986
translation,177,204,baselines,teacher network,trained with,additional video-dependent text input,teacher network trained with additional video-dependent text input,0.7434543371200562
translation,177,204,baselines,baselines,has,student - teacher,baselines has student - teacher,0.589889407157898
translation,177,205,baselines,mtn,fuses,temporal features,mtn fuses temporal features,0.6904857754707336
translation,177,205,baselines,temporal features,of,different modalities sequentially,temporal features of different modalities sequentially,0.5479286313056946
translation,177,205,baselines,different modalities sequentially,through,transformer decoder architecture,different modalities sequentially through transformer decoder architecture,0.6689102053642273
translation,177,205,baselines,mtn,has,"le et al. , 2019 b","mtn has le et al. , 2019 b",0.6241775751113892
translation,177,205,baselines,baselines,has,mtn,baselines has mtn,0.559651255607605
translation,177,206,baselines,fga,consists of,attention networks,fga consists of attention networks,0.635007917881012
translation,177,206,baselines,"et al. , 2019 )",consists of,attention networks,"et al. , 2019 ) consists of attention networks",0.5719894170761108
translation,177,206,baselines,attention networks,between,all pairs of modalities,attention networks between all pairs of modalities,0.6357971429824829
translation,177,206,baselines,attention scores,along,edges,attention scores along edges,0.659785270690918
translation,177,206,baselines,edges,of,attention graph,edges of attention graph,0.5467455983161926
translation,177,206,baselines,baselines,has,fga,baselines has fga,0.5526036024093628
translation,177,178,experiments,avsd experiments,train,our models,avsd experiments train our models,0.6674870252609253
translation,177,178,experiments,our models,by applying,"label smoothing ( szegedy et al. , 2016 )","our models by applying label smoothing ( szegedy et al. , 2016 )",0.6079224348068237
translation,177,178,experiments,"label smoothing ( szegedy et al. , 2016 )",on,target system responses y,"label smoothing ( szegedy et al. , 2016 ) on target system responses y",0.5060890316963196
translation,177,239,experiments,multi-head attention mechanism,suitable for,tasks,multi-head attention mechanism suitable for tasks,0.6793771982192993
translation,177,239,experiments,tasks,dealing with,information - intensive media,tasks dealing with information - intensive media,0.7059286832809448
translation,177,239,experiments,information - intensive media,such as,video and dialogues,information - intensive media such as video and dialogues,0.6465830206871033
translation,177,165,hyperparameters,visual and audio features,used,3d - cnn resnext - 101,visual and audio features used 3d - cnn resnext - 101,0.5409435629844666
translation,177,165,hyperparameters,3d - cnn resnext - 101,pretrained on,"kinetics ( hara et al. , 2018 )","3d - cnn resnext - 101 pretrained on kinetics ( hara et al. , 2018 )",0.7507622241973877
translation,177,165,hyperparameters,"kinetics ( hara et al. , 2018 )",to obtain,spatio-temporal visual features,"kinetics ( hara et al. , 2018 ) to obtain spatio-temporal visual features",0.5787022709846497
translation,177,165,hyperparameters,hyperparameters,To extract,visual and audio features,hyperparameters To extract visual and audio features,0.6734697818756104
translation,177,172,hyperparameters,learning rate warm - up steps,equivalent to,5 epochs,learning rate warm - up steps equivalent to 5 epochs,0.6514589786529541
translation,177,172,hyperparameters,models,up to,50 epochs,models up to 50 epochs,0.6151988506317139
translation,177,172,hyperparameters,hyperparameters,set,learning rate warm - up steps,hyperparameters set learning rate warm - up steps,0.630652904510498
translation,177,172,hyperparameters,hyperparameters,train,models,hyperparameters train models,0.6666569709777832
translation,177,173,hyperparameters,best models,based on,average loss per epoch,best models based on average loss per epoch,0.6056265830993652
translation,177,173,hyperparameters,average loss per epoch,in,validation set,average loss per epoch in validation set,0.5254945158958435
translation,177,173,hyperparameters,hyperparameters,select,best models,hyperparameters select best models,0.6050817370414734
translation,177,174,hyperparameters,all model parameters,with,"uniform distribution ( glorot and bengio , 2010 )","all model parameters with uniform distribution ( glorot and bengio , 2010 )",0.595175564289093
translation,177,174,hyperparameters,hyperparameters,initialize,all model parameters,hyperparameters initialize all model parameters,0.6968234777450562
translation,177,175,hyperparameters,training,adopt,auxiliary auto-encoder loss function,training adopt auxiliary auto-encoder loss function,0.6030877232551575
translation,177,175,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
translation,177,176,hyperparameters,blocks,in,multimodal reasoning and decoder networks,blocks in multimodal reasoning and decoder networks,0.5866883993148804
translation,177,176,hyperparameters,hyperparameters,adopt,"transformer attention ( vaswani et al. , 2017 )","hyperparameters adopt transformer attention ( vaswani et al. , 2017 )",0.5872014164924622
translation,177,176,hyperparameters,hyperparameters,select,h att = 8,hyperparameters select h att = 8,0.6485415697097778
translation,177,6,model,vision - language neural framework,for,high-resolution queries,vision - language neural framework for high-resolution queries,0.5630525946617126
translation,177,6,model,high-resolution queries,in,videos,high-resolution queries in videos,0.5073659420013428
translation,177,6,model,high-resolution queries,based on,textual cues,high-resolution queries based on textual cues,0.6363102197647095
translation,177,6,model,bi-directional spatio-temporal learning ( bist ),has,vision - language neural framework,bi-directional spatio-temporal learning ( bist ) has vision - language neural framework,0.5560321807861328
translation,177,6,model,model,propose,bi-directional spatio-temporal learning ( bist ),model propose bi-directional spatio-temporal learning ( bist ),0.6772962808609009
translation,177,7,model,dynamic information diffusion,between,two feature spaces,dynamic information diffusion between two feature spaces,0.6127119064331055
translation,177,7,model,dynamic information diffusion,through,spatial - to -temporal and temporal -tospatial reasoning,dynamic information diffusion through spatial - to -temporal and temporal -tospatial reasoning,0.6354962587356567
translation,177,8,model,bidirectional strategy,aims to tackle,evolving semantics,bidirectional strategy aims to tackle evolving semantics,0.6367692351341248
translation,177,8,model,evolving semantics,of,user queries,evolving semantics of user queries,0.5107786655426025
translation,177,8,model,user queries,in,dialogue setting,user queries in dialogue setting,0.521077573299408
translation,177,8,model,model,has,bidirectional strategy,model has bidirectional strategy,0.5769558548927307
translation,177,29,model,two parallel networks,to learn,relevant visual signals,two parallel networks to learn relevant visual signals,0.5649049878120422
translation,177,29,model,relevant visual signals,from,input video,relevant visual signals from input video,0.552440881729126
translation,177,29,model,language signals,from,user utterances,language signals from user utterances,0.5581768751144409
translation,177,30,model,language - based features,to,three - dimensional tensor,language - based features to three - dimensional tensor,0.5337308049201965
translation,177,30,model,three - dimensional tensor,used to,independently learn,three - dimensional tensor used to independently learn,0.572259247303009
translation,177,30,model,video signals,following,reasoning direction,video signals following reasoning direction,0.6537415385246277
translation,177,30,model,independently learn,has,video signals,independently learn has video signals,0.5589770078659058
translation,177,31,model,output,from,each network,output from each network,0.6209622025489807
translation,177,31,model,output,dynamically combined by,importance scores,output dynamically combined by importance scores,0.7582197785377502
translation,177,31,model,importance scores,computed based on,language and visual features,importance scores computed based on language and visual features,0.6938547492027283
translation,177,31,model,model,has,output,model has output,0.5534584522247314
translation,177,35,model,spatial and temporal features,of,videos,spatial and temporal features of videos,0.5921762585639954
translation,177,35,model,spatial and temporal features,for,higher - resolution queries,spatial and temporal features for higher - resolution queries,0.6075721979141235
translation,177,35,model,higher - resolution queries,of,visual cues,higher - resolution queries of visual cues,0.5778667330741882
translation,177,36,model,diverse queried information,from,conversational queries,diverse queried information from conversational queries,0.5452706217765808
translation,177,36,model,diverse queried information,propose,bidirectional strategy,diverse queried information propose bidirectional strategy,0.6358596086502075
translation,177,36,model,bidirectional strategy,denoted,spatial ? temporal,bidirectional strategy denoted spatial ? temporal,0.6966019868850708
translation,177,36,model,bidirectional strategy,to enable,comprehensive information diffusion,bidirectional strategy to enable comprehensive information diffusion,0.6591408252716064
translation,177,36,model,comprehensive information diffusion,between,two visual feature spaces,comprehensive information diffusion between two visual feature spaces,0.6088114976882935
translation,177,36,model,model,To tackle,diverse queried information,model To tackle diverse queried information,0.6792469620704651
translation,177,37,results,our models,achieve,competitive performance,our models achieve competitive performance,0.6367077231407166
translation,177,37,results,competitive performance,on,  avsd   ( audio- visual scene aware dialogues ) benchmark,competitive performance on   avsd   ( audio- visual scene aware dialogues ) benchmark,0.5206071734428406
translation,177,37,results,  avsd   ( audio- visual scene aware dialogues ) benchmark,from,7 th dialogue system technology challenge ( dstc7 ),  avsd   ( audio- visual scene aware dialogues ) benchmark from 7 th dialogue system technology challenge ( dstc7 ),0.5616352558135986
translation,177,37,results,results,has,our models,results has our models,0.5733726620674133
translation,177,208,results,our models,has,outperform,our models has outperform,0.6103132367134094
translation,177,208,results,outperform,has,existing approaches,outperform has existing approaches,0.6087539196014404
translation,177,209,results,performance,of,our models,performance of our models,0.5822860598564148
translation,177,209,results,our models,in,visual-only setting,our models in visual-only setting,0.5275364518165588
translation,177,209,results,our models,shows,performance gain,our models shows performance gain,0.6965627074241638
translation,177,209,results,performance gain,coming from,our bidirectional language -vision reasoning approach,performance gain coming from our bidirectional language -vision reasoning approach,0.59846031665802
translation,177,209,results,results,has,performance,results has performance,0.5972660779953003
translation,177,210,results,performance boost,whenever,text feature,performance boost whenever text feature,0.5953752994537354
translation,177,210,results,text feature,from,video,text feature from video,0.5395587086677551
translation,177,211,results,performance gain,is,not significant,performance gain is not significant,0.5689372420310974
translation,177,211,results,audio features,has,performance gain,audio features has performance gain,0.5398674011230469
translation,177,213,results,fga,reports,cider score,fga reports cider score,0.5901055335998535
translation,177,213,results,cider score,of,0.806,cider score of 0.806,0.5397586226463318
translation,177,213,results,0.806,in,visual-only setting,0.806 in visual-only setting,0.4795849025249481
translation,177,213,results,results,has,fga,results has fga,0.5442795157432556
translation,177,214,results,our performance gain,indicates,efficacy,our performance gain indicates efficacy,0.6873968243598938
translation,177,214,results,efficacy,of learning,fine- grained dependencies,efficacy of learning fine- grained dependencies,0.6763397455215454
translation,177,214,results,fine- grained dependencies,between,query and visual features,fine- grained dependencies between query and visual features,0.6386674642562866
translation,177,214,results,query and visual features,at both,spatial and temporal levels,query and visual features at both spatial and temporal levels,0.6644349098205566
translation,177,214,results,relevant information,from,video,relevant information from video,0.574043333530426
translation,177,214,results,fga,has,our performance gain,fga has our performance gain,0.5547071099281311
translation,177,214,results,results,Compared to,fga,results Compared to fga,0.6511723399162292
translation,177,215,results,results,has,tgif - qa results,results has tgif - qa results,0.575677752494812
translation,177,217,results,outperforms,across,all qa tasks,outperforms across all qa tasks,0.6680630445480347
translation,177,217,results,outperforms,using,framelevel ( appearance ) feature,outperforms using framelevel ( appearance ) feature,0.6668297648429871
translation,177,217,results,outperforms,using,resnet,outperforms using resnet,0.6915411949157715
translation,177,217,results,outperforms,using,sequencelevel feature,outperforms using sequencelevel feature,0.6930636167526245
translation,177,217,results,outperforms,using,resnext,outperforms using resnext,0.7035478949546814
translation,177,217,results,existing approaches,across,all qa tasks,existing approaches across all qa tasks,0.6871228218078613
translation,177,217,results,existing approaches,using,sequencelevel feature,existing approaches using sequencelevel feature,0.6544493436813354
translation,177,217,results,existing approaches,using,resnext,existing approaches using resnext,0.7310647964477539
translation,177,217,results,model,has,outperforms,model has outperforms,0.675282895565033
translation,177,217,results,outperforms,has,existing approaches,outperforms has existing approaches,0.5970985293388367
translation,177,218,results,our models,perform,better,our models perform better,0.6135782599449158
translation,177,218,results,better,with,resnext,better with resnext,0.7450042366981506
translation,177,218,results,results,has,our models,results has our models,0.5733726620674133
translation,177,223,results,model,with,temporal ? spatial,model with temporal ? spatial,0.6470280885696411
translation,177,223,results,model,performs,better,model performs better,0.6795132756233215
translation,177,223,results,better,than,reverse reasoning direction,better than reverse reasoning direction,0.5805382132530212
translation,177,223,results,single reasoning direction,has,model,single reasoning direction has model,0.5931271314620972
translation,177,223,results,results,when using,single reasoning direction,results when using single reasoning direction,0.6670553684234619
translation,177,229,results,improves,when,both reasoning directions,improves when both reasoning directions,0.6326819658279419
translation,177,229,results,improves,use,both reasoning directions,improves use both reasoning directions,0.6283397078514099
translation,177,229,results,both reasoning directions,rather than,only one of them,both reasoning directions rather than only one of them,0.6963477730751038
translation,177,229,results,our model performance,has,improves,our model performance has improves,0.6219343543052673
translation,177,229,results,results,observe that,our model performance,results observe that our model performance,0.5689132213592529
translation,177,232,results,spatio-temporal features,is,better,spatio-temporal features is better,0.5924057364463806
translation,177,232,results,better,than,only using one of them,better than only using one of them,0.617514967918396
translation,177,238,results,more than 3 reasoning steps,has,model performance,more than 3 reasoning steps has model performance,0.5603609681129456
translation,177,238,results,model performance,has,improves slightly,model performance has improves slightly,0.6060904860496521
translation,178,130,baselines,plain sa,consider,dialogue,plain sa consider dialogue,0.7859151363372803
translation,178,130,baselines,plain sa,ignore,utterance matching,plain sa ignore utterance matching,0.7585001587867737
translation,178,130,baselines,dialogue,as,plain text,dialogue as plain text,0.559044361114502
translation,178,130,baselines,baselines,has,plain sa,baselines has plain sa,0.5778590440750122
translation,178,131,baselines,lstm,use,word vectors,lstm use word vectors,0.5963193774223328
translation,178,131,baselines,lstm,feed,last hidden state,lstm feed last hidden state,0.6816098093986511
translation,178,131,baselines,word vectors,as,input,word vectors as input,0.5503232479095459
translation,178,131,baselines,input,of,standard lstm,input of standard lstm,0.5607337355613708
translation,178,131,baselines,last hidden state,into,softmax layer,last hidden state into softmax layer,0.5642862319946289
translation,178,131,baselines,softmax layer,for,satisfaction prediction,softmax layer for satisfaction prediction,0.6030393242835999
translation,178,133,baselines,baselines,has,hrn,baselines has hrn,0.5727208852767944
translation,178,135,baselines,baselines,has,milnet,baselines has milnet,0.5741172432899475
translation,178,139,baselines,hmn,uses,question - answer bidirectional matching layer,hmn uses question - answer bidirectional matching layer,0.5311321020126343
translation,178,139,baselines,hierarchical matching network,for,sentiment analysis,hierarchical matching network for sentiment analysis,0.5870821475982666
translation,178,139,baselines,question - answer bidirectional matching layer,to learn,matching vector,question - answer bidirectional matching layer to learn matching vector,0.5991669297218323
translation,178,139,baselines,question - answer bidirectional matching layer,characterizes,importance,question - answer bidirectional matching layer characterizes importance,0.6306614875793457
translation,178,139,baselines,matching vector,of,"each qa pair ( i.e. , customer utterance , server utterance )","matching vector of each qa pair ( i.e. , customer utterance , server utterance )",0.5662286281585693
translation,178,139,baselines,matching vector,characterizes,importance,matching vector characterizes importance,0.6606602072715759
translation,178,139,baselines,importance,of,generated matching vectors,importance of generated matching vectors,0.5960553884506226
translation,178,139,baselines,generated matching vectors,via,self-matching attention layer,generated matching vectors via self-matching attention layer,0.6507961750030518
translation,178,139,baselines,hmn,has,hierarchical matching network,hmn has hierarchical matching network,0.5259749293327332
translation,178,139,baselines,baselines,has,hmn,baselines has hmn,0.6033270955085754
translation,178,142,baselines,baselines,has,"camil s , camil r and camil f ull","baselines has camil s , camil r and camil f ull",0.5913680195808411
translation,178,123,experimental-setup,other trainable model parameters,initialized by,sampling values,other trainable model parameters initialized by sampling values,0.7171239852905273
translation,178,123,experimental-setup,sampling values,from,"uniform distribution u(?0.01 , 0.01 )","sampling values from uniform distribution u(?0.01 , 0.01 )",0.56153404712677
translation,178,123,experimental-setup,experimental setup,has,other trainable model parameters,experimental setup has other trainable model parameters,0.4930664300918579
translation,178,124,experimental-setup,size,of,lstm hidden states k,size of lstm hidden states k,0.5756258964538574
translation,178,124,experimental-setup,lstm hidden states k,set as,128,lstm hidden states k set as 128,0.6162890791893005
translation,178,124,experimental-setup,experimental setup,has,size,experimental setup has size,0.5329226851463318
translation,178,126,experimental-setup,initial learning rate,fixed as,0.1,initial learning rate fixed as 0.1,0.6709678769111633
translation,178,126,experimental-setup,dropout rate,is,0.2,dropout rate is 0.2,0.5355412364006042
translation,178,126,experimental-setup,batch size,is,32,batch size is 32,0.6284153461456299
translation,178,126,experimental-setup,number of epochs,is,20,number of epochs is 20,0.6013389825820923
translation,178,126,experimental-setup,experimental setup,has,initial learning rate,experimental setup has initial learning rate,0.49018073081970215
translation,178,126,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,178,144,experimental-setup,methods,run on,server,methods run on server,0.5236797332763672
translation,178,144,experimental-setup,server,configured with,tesla v100 gpu,server configured with tesla v100 gpu,0.6697618961334229
translation,178,144,experimental-setup,server,configured with,2 cpu,server configured with 2 cpu,0.7235422730445862
translation,178,144,experimental-setup,server,configured with,32g memory,server configured with 32g memory,0.7086019515991211
translation,178,144,experimental-setup,experimental setup,run on,server,experimental setup run on server,0.7603636980056763
translation,178,144,experimental-setup,experimental setup,has,methods,experimental setup has methods,0.44329318404197693
translation,178,175,experiments,camil f ull,is,best,camil f ull is best,0.6951934695243835
translation,178,175,experiments,camil f ull,to,accurate context clue matching,camil f ull to accurate context clue matching,0.5595366358757019
translation,178,7,model,extensible context-assisted multiple instance learning ( camil ) model,to predict,sentiments,extensible context-assisted multiple instance learning ( camil ) model to predict sentiments,0.6960658431053162
translation,178,7,model,extensible context-assisted multiple instance learning ( camil ) model,aggregate,sentiments,extensible context-assisted multiple instance learning ( camil ) model aggregate sentiments,0.7680540084838867
translation,178,7,model,sentiments,of,all the customer utterances,sentiments of all the customer utterances,0.5723894238471985
translation,178,7,model,sentiments,into,service satisfaction polarity,sentiments into service satisfaction polarity,0.5807161927223206
translation,178,7,model,sentiments,into,service satisfaction polarity,sentiments into service satisfaction polarity,0.5807161927223206
translation,178,7,model,model,propose,extensible context-assisted multiple instance learning ( camil ) model,model propose extensible context-assisted multiple instance learning ( camil ) model,0.6469377875328064
translation,178,8,model,novel context clue matching mechanism ( ccmm ),to enhance,representations,novel context clue matching mechanism ( ccmm ) to enhance representations,0.6894510984420776
translation,178,8,model,representations,of,all customer utterances,representations of all customer utterances,0.5993812084197998
translation,178,8,model,all customer utterances,with,matched context clues,all customer utterances with matched context clues,0.6589785814285278
translation,178,8,model,matched context clues,i.e.,sentiment and reasoning clues,matched context clues i.e. sentiment and reasoning clues,0.6123837828636169
translation,178,8,model,model,propose,novel context clue matching mechanism ( ccmm ),model propose novel context clue matching mechanism ( ccmm ),0.6695570945739746
translation,178,41,model,novel and extensible context -assisted multiple instance learning ( camil ) model,for,new ssa task,novel and extensible context -assisted multiple instance learning ( camil ) model for new ssa task,0.58189457654953
translation,178,41,model,novel and extensible context -assisted multiple instance learning ( camil ) model,for,dialogue -level satisfaction classification,novel and extensible context -assisted multiple instance learning ( camil ) model for dialogue -level satisfaction classification,0.5639523267745972
translation,178,41,model,supervision,of,satisfaction labels,supervision of satisfaction labels,0.5684924125671387
translation,178,41,model,model,propose,novel and extensible context -assisted multiple instance learning ( camil ) model,model propose novel and extensible context -assisted multiple instance learning ( camil ) model,0.6497791409492493
translation,178,60,model,position - guided automatic context clue matching mechanism ( ccmm ),to conduct,customer utterance and context clues alignments,position - guided automatic context clue matching mechanism ( ccmm ) to conduct customer utterance and context clues alignments,0.603710949420929
translation,178,60,model,customer utterance and context clues alignments,for,better sentiment classification,customer utterance and context clues alignments for better sentiment classification,0.6278161406517029
translation,178,60,model,better sentiment classification,to boost,satisfaction classification,better sentiment classification to boost satisfaction classification,0.6317699551582336
translation,178,63,model,sentiments,of,all customer utterances,sentiments of all customer utterances,0.5825687646865845
translation,178,63,model,sentiments,propose,camil model,sentiments propose camil model,0.6856561899185181
translation,178,63,model,all customer utterances,with,available satisfaction labels,all customer utterances with available satisfaction labels,0.6365193128585815
translation,178,63,model,camil model,based on,multiple instance learning approach,camil model based on multiple instance learning approach,0.6689355373382568
translation,178,63,model,model,to predict,service satisfaction,model to predict service satisfaction,0.6996115446090698
translation,178,63,model,model,propose,camil model,model propose camil model,0.6593868732452393
translation,178,132,model,han,feeding it into,softmax layer,han feeding it into softmax layer,0.6380144953727722
translation,178,132,model,hierarchical attention network,for,document classification,hierarchical attention network for document classification,0.5471378564834595
translation,178,132,model,two levels of attention mechanisms,applied at,word - and utterance - level,two levels of attention mechanisms applied at word - and utterance - level,0.6487618088722229
translation,178,132,model,more and less important content,when constructing,dialogue representation,more and less important content when constructing dialogue representation,0.7095856666564941
translation,178,132,model,softmax layer,for,classification,softmax layer for classification,0.6276646852493286
translation,178,132,model,han,has,hierarchical attention network,han has hierarchical attention network,0.5107623934745789
translation,178,132,model,model,has,han,model has han,0.6128441691398621
translation,178,134,model,bi-directional lstm,to represent,utterances,bi-directional lstm to represent utterances,0.6486810445785522
translation,178,134,model,bi-directional lstm,fed into,standard lstm,bi-directional lstm fed into standard lstm,0.6595425605773926
translation,178,134,model,standard lstm,for,dialogue representation,standard lstm for dialogue representation,0.5756994485855103
translation,178,134,model,softmax layer,for,classification,softmax layer for classification,0.6276646852493286
translation,178,134,model,model,uses,bi-directional lstm,model uses bi-directional lstm,0.5592467188835144
translation,178,136,model,original method,designed for,plain textual data,original method designed for plain textual data,0.7354038953781128
translation,178,136,model,model,has,original method,model has original method,0.5481769442558289
translation,178,143,model,camil models,with,only sentiment clues,camil models with only sentiment clues,0.6706037521362305
translation,178,143,model,camil models,with,only reasoning clues,camil models with only reasoning clues,0.6721913814544678
translation,178,143,model,camil models,by setting,masking function,camil models by setting masking function,0.6910336017608643
translation,178,143,model,only sentiment clues,by setting,masking function,only sentiment clues by setting masking function,0.6431148648262024
translation,178,143,model,model,has,camil models,model has camil models,0.5864661931991577
translation,178,206,model,basic mil approach,inputs of,context-matched customer utterances,basic mil approach inputs of context-matched customer utterances,0.7268765568733215
translation,178,206,model,basic mil approach,predict,utterancelevel sentiment polarities and dialogue -level satisfaction polarities simultaneously,basic mil approach predict utterancelevel sentiment polarities and dialogue -level satisfaction polarities simultaneously,0.633500874042511
translation,178,206,model,model,propose,basic mil approach,model propose basic mil approach,0.6775826811790466
translation,178,207,model,context clue matching mechanism ( ccmm ),to match,any customer utterance,context clue matching mechanism ( ccmm ) to match any customer utterance,0.7060756683349609
translation,178,207,model,any customer utterance,with,most related server utterances,any customer utterance with most related server utterances,0.646418035030365
translation,178,207,model,model,propose,context clue matching mechanism ( ccmm ),model propose context clue matching mechanism ( ccmm ),0.6493157744407654
translation,178,121,results,fine-tuning,for,word vectors,fine-tuning for word vectors,0.5902180671691895
translation,178,121,results,fine-tuning,can improve,performance,fine-tuning can improve performance,0.7600679993629456
translation,178,156,results,clothes corpus,compared to,met class,clothes corpus compared to met class,0.6255698800086975
translation,178,156,results,performances,of,all models,performances of all models,0.5580928921699524
translation,178,156,results,all models,on,satisfied class,all models on satisfied class,0.5875563025474548
translation,178,156,results,satisfied class,are,much worse,satisfied class are much worse,0.610575258731842
translation,178,156,results,clothes corpus,has,performances,clothes corpus has performances,0.5751047730445862
translation,178,156,results,met class,has,performances,met class has performances,0.6087510585784912
translation,178,156,results,results,On,clothes corpus,results On clothes corpus,0.5138763785362244
translation,178,166,results,customer,has,outperforms,customer has outperforms,0.6624400615692139
translation,178,166,results,results,observe that,customer,results observe that customer,0.5826486349105835
translation,178,169,results,nopos,performs,well but worse,nopos performs well but worse,0.648036003112793
translation,178,169,results,well but worse,than,camil f ull,well but worse than camil f ull,0.689132571220398
translation,178,169,results,results,has,nopos,results has nopos,0.5995913147926331
translation,178,174,results,camil r and camil s,perform,worse,camil r and camil s perform worse,0.5684329271316528
translation,178,174,results,worse,than,camil f ull,worse than camil f ull,0.7010274529457092
translation,178,174,results,results,has,camil r and camil s,results has camil r and camil s,0.5528266429901123
translation,179,5,model,divide- and - conquer approach,discovers and exploits,hidden structure,divide- and - conquer approach discovers and exploits hidden structure,0.7464173436164856
translation,179,5,model,hidden structure,of,task,hidden structure of task,0.6182807683944702
translation,179,5,model,hidden structure,to enable,efficient policy learning,hidden structure to enable efficient policy learning,0.7174044847488403
translation,179,5,model,model,propose,divide- and - conquer approach,model propose divide- and - conquer approach,0.7190815806388855
translation,179,6,model,subgoal discovery network ( sdn ),to divide,complex goal-oriented task,subgoal discovery network ( sdn ) to divide complex goal-oriented task,0.6399193406105042
translation,179,6,model,complex goal-oriented task,into,set of simpler subgoals,complex goal-oriented task into set of simpler subgoals,0.541010856628418
translation,179,6,model,set of simpler subgoals,in,unsupervised fashion,set of simpler subgoals in unsupervised fashion,0.48350733518600464
translation,179,22,model,simple yet effective subgoal discovery network ( sdn ),discovers,useful subgoals,simple yet effective subgoal discovery network ( sdn ) discovers useful subgoals,0.7116971611976624
translation,179,22,model,useful subgoals,for,rl - based dialogue agent,useful subgoals for rl - based dialogue agent,0.59278804063797
translation,179,22,model,automatically,for,rl - based dialogue agent,automatically for rl - based dialogue agent,0.6014209985733032
translation,179,22,model,useful subgoals,has,automatically,useful subgoals has automatically,0.5672633051872253
translation,179,22,model,model,propose,simple yet effective subgoal discovery network ( sdn ),model propose simple yet effective subgoal discovery network ( sdn ),0.6727385520935059
translation,179,23,model,sdn,takes as input,collection of successful conversations,sdn takes as input collection of successful conversations,0.6671878099441528
translation,179,23,model,sdn,identifies,  hub   states,sdn identifies   hub   states,0.6684989929199219
translation,179,23,model,  hub   states,as,subgoals,  hub   states as subgoals,0.4962388873100281
translation,179,23,model,model,has,sdn,model has sdn,0.5853769183158875
translation,180,172,ablation-analysis,knowledge base,affects,tppa knowledge selection,knowledge base affects tppa knowledge selection,0.6781214475631714
translation,180,172,ablation-analysis,knowledge base,for,holl -e data set,knowledge base for holl -e data set,0.6256823539733887
translation,180,172,ablation-analysis,tppa knowledge selection,for,wiz data set,tppa knowledge selection for wiz data set,0.5909996628761292
translation,180,172,ablation-analysis,tppa knowledge selection,for,holl -e data set,tppa knowledge selection for holl -e data set,0.5952558517456055
translation,180,172,ablation-analysis,fixed number of 10 prk,increasing,number of neg items,fixed number of 10 prk increasing number of neg items,0.7048395276069641
translation,180,172,ablation-analysis,number of neg items,improves,performance,number of neg items improves performance,0.6890319585800171
translation,180,172,ablation-analysis,performance,until reaching,plateau,performance until reaching plateau,0.6829207539558411
translation,180,172,ablation-analysis,plateau,at,1rrk - 30neg - 10prk,plateau at 1rrk - 30neg - 10prk,0.5977029204368591
translation,180,172,ablation-analysis,plateau,at,1rrk - 10neg - 10prk,plateau at 1rrk - 10neg - 10prk,0.5982651710510254
translation,180,172,ablation-analysis,tppa knowledge selection,has,best combination,tppa knowledge selection has best combination,0.5637763738632202
translation,180,172,ablation-analysis,wiz data set,has,best combination,wiz data set has best combination,0.5644911527633667
translation,180,172,ablation-analysis,holl -e data set,has,best combination,holl -e data set has best combination,0.5754515528678894
translation,180,172,ablation-analysis,ablation analysis,for,wiz data set,ablation analysis for wiz data set,0.6301745176315308
translation,180,172,ablation-analysis,ablation analysis,for,holl -e data set,ablation analysis for holl -e data set,0.6164271235466003
translation,180,181,ablation-analysis,ted performance,suffers from,increased knowledge injection,ted performance suffers from increased knowledge injection,0.6909236311912537
translation,180,181,ablation-analysis,ablation analysis,has,ted performance,ablation analysis has ted performance,0.5766268968582153
translation,180,199,ablation-analysis,tppa,for,wiz,tppa for wiz,0.7128477692604065
translation,180,199,ablation-analysis,number of additionally added useful words,comparable to,post,number of additionally added useful words comparable to post,0.6833723783493042
translation,180,199,ablation-analysis,number of additionally added useful words,comparable to,post,number of additionally added useful words comparable to post,0.6833723783493042
translation,180,199,ablation-analysis,retrieved knowledge,more than double,useful words,retrieved knowledge more than double useful words,0.60625159740448
translation,180,199,ablation-analysis,useful words,than,post,useful words than post,0.5980817675590515
translation,180,199,ablation-analysis,tppa,has,number of additionally added useful words,tppa has number of additionally added useful words,0.6066807508468628
translation,180,199,ablation-analysis,tppa,has,retrieved knowledge,tppa has retrieved knowledge,0.5943548679351807
translation,180,199,ablation-analysis,wiz,has,number of additionally added useful words,wiz has number of additionally added useful words,0.5849230885505676
translation,180,199,ablation-analysis,wiz,has,retrieved knowledge,wiz has retrieved knowledge,0.642098069190979
translation,180,199,ablation-analysis,post,has,brings,post has brings,0.6550049781799316
translation,180,199,ablation-analysis,brings,has,10.25 % vs. 14.6 %,brings has 10.25 % vs. 14.6 %,0.5816771388053894
translation,180,199,ablation-analysis,holl -e,has,retrieved knowledge,holl -e has retrieved knowledge,0.632189154624939
translation,180,199,ablation-analysis,post,has,15.98 % vs. 7.52 %,post has 15.98 % vs. 7.52 %,0.572433590888977
translation,180,199,ablation-analysis,ablation analysis,Considering,tppa,ablation analysis Considering tppa,0.7003517150878906
translation,180,8,baselines,tppa method,processes,posts,tppa method processes posts,0.7440444231033325
translation,180,8,baselines,tppa method,processes,post related knowledge,tppa method processes post related knowledge,0.7323309183120728
translation,180,8,baselines,tppa method,processes,response related knowledge,tppa method processes response related knowledge,0.7410337924957275
translation,180,8,baselines,response related knowledge,at,word and sentence level,response related knowledge at word and sentence level,0.5111737251281738
translation,180,8,baselines,baselines,has,tppa method,baselines has tppa method,0.5636698007583618
translation,180,40,baselines,new method,for,knowledge selection,new method for knowledge selection,0.5776960849761963
translation,180,40,baselines,new method,includes,transformer - based representations,new method includes transformer - based representations,0.6051213145256042
translation,180,40,baselines,knowledge selection,includes,transformer - based representations,knowledge selection includes transformer - based representations,0.6185272336006165
translation,180,40,baselines,tppa ),includes,transformer - based representations,tppa ) includes transformer - based representations,0.6281731724739075
translation,180,40,baselines,transformer - based representations,of,posts and post related knowledge,transformer - based representations of posts and post related knowledge,0.5706919431686401
translation,180,40,baselines,relevant knowledge,processed with,word embedding and maxpooling,relevant knowledge processed with word embedding and maxpooling,0.6500344276428223
translation,180,40,baselines,knowledge selection,has,tppa ),knowledge selection has tppa ),0.6042316555976868
translation,180,139,baselines,tppa knowledge selection,on,retrieval performance,tppa knowledge selection on retrieval performance,0.4905658960342407
translation,180,139,baselines,"bm25 ( robertson and walker , 1994 )",is,unsupervised probabilistic retrieval algorithm,"bm25 ( robertson and walker , 1994 ) is unsupervised probabilistic retrieval algorithm",0.5212120413780212
translation,180,139,baselines,unsupervised probabilistic retrieval algorithm,robust for,short document ( sentence ) retrieval,unsupervised probabilistic retrieval algorithm robust for short document ( sentence ) retrieval,0.7321319580078125
translation,180,139,baselines,baselines,compare,tppa knowledge selection,baselines compare tppa knowledge selection,0.6803882122039795
translation,180,140,baselines,drqa,uses,bigram hashing,drqa uses bigram hashing,0.6010194420814514
translation,180,140,baselines,drqa,uses,tf - idf matching,drqa uses tf - idf matching,0.5771400332450867
translation,180,140,baselines,tf - idf matching,with,multi-layer recurrent neural network model,tf - idf matching with multi-layer recurrent neural network model,0.5834466814994812
translation,180,140,baselines,baselines,has,drqa,baselines has drqa,0.5787320733070374
translation,180,143,baselines,tppa output,with,three models,tppa output with three models,0.6636540293693542
translation,180,143,baselines,"wseq ( tian et al. , 2017 )",uses,weighted sum and concatenation,"wseq ( tian et al. , 2017 ) uses weighted sum and concatenation",0.5608817338943481
translation,180,143,baselines,"wseq ( tian et al. , 2017 )",obtain,representations,"wseq ( tian et al. , 2017 ) obtain representations",0.5869449377059937
translation,180,143,baselines,weighted sum and concatenation,of,post and its contextual utter-ances,weighted sum and concatenation of post and its contextual utter-ances,0.5563567876815796
translation,180,143,baselines,representations,through,rnn,representations through rnn,0.6725142598152161
translation,180,144,baselines,memnet,leverages,multi-task learning framework,memnet leverages multi-task learning framework,0.6815124750137329
translation,180,144,baselines,multi-task learning framework,to jointly train,"post- to- response ' , ' knowledge-to- response ' and ' knowledge- to- knowledge ' tasks","multi-task learning framework to jointly train post- to- response ' , ' knowledge-to- response ' and ' knowledge- to- knowledge ' tasks",0.6764124035835266
translation,180,144,baselines,"post- to- response ' , ' knowledge-to- response ' and ' knowledge- to- knowledge ' tasks",for,response generation,"post- to- response ' , ' knowledge-to- response ' and ' knowledge- to- knowledge ' tasks for response generation",0.6038926839828491
translation,180,144,baselines,baselines,has,memnet,baselines has memnet,0.5386731028556824
translation,180,146,baselines,two methods,jointly train,knowledge selection model,two methods jointly train knowledge selection model,0.644798755645752
translation,180,146,baselines,two methods,jointly train,dialogue generation model,two methods jointly train dialogue generation model,0.6455743312835693
translation,180,146,baselines,two methods,jointly train,knowledge selection model,two methods jointly train knowledge selection model,0.644798755645752
translation,180,146,baselines,"post - ks ( lian et al. , 2019 )",approximates,posterior-distribution of knowledge,"post - ks ( lian et al. , 2019 ) approximates posterior-distribution of knowledge",0.7816897034645081
translation,180,146,baselines,posterior-distribution of knowledge,jointly train,knowledge selection model,posterior-distribution of knowledge jointly train knowledge selection model,0.6600043177604675
translation,180,146,baselines,posterior-distribution of knowledge,jointly train,dialogue generation model,posterior-distribution of knowledge jointly train dialogue generation model,0.6569257378578186
translation,180,146,baselines,baselines,consider,two methods,baselines consider two methods,0.6570055484771729
translation,180,149,experimental-setup,dimension,of,word embedding,dimension of word embedding,0.5654559135437012
translation,180,149,experimental-setup,word embedding,is,300,word embedding is 300,0.5882549285888672
translation,180,149,experimental-setup,multihead number,of,transformer,multihead number of transformer,0.6471209526062012
translation,180,149,experimental-setup,transformer,is,4,transformer is 4,0.6691712737083435
translation,180,150,experimental-setup,vocabulary,obtained by,ranking,vocabulary obtained by ranking,0.5836148262023926
translation,180,150,experimental-setup,training data,by,word frequency,training data by word frequency,0.5292552709579468
translation,180,150,experimental-setup,ranking,has,training data,ranking has training data,0.5630699992179871
translation,180,150,experimental-setup,experimental setup,has,vocabulary,experimental setup has vocabulary,0.5132392048835754
translation,180,153,experimental-setup,model training,use,mini-batch size,model training use mini-batch size,0.6254695057868958
translation,180,153,experimental-setup,mini-batch size,has,64,mini-batch size has 64,0.6270067095756531
translation,180,153,experimental-setup,experimental setup,During,model training,experimental setup During model training,0.6773790717124939
translation,180,154,experimental-setup,adam optimiser,used for,optimisation,adam optimiser used for optimisation,0.6459234356880188
translation,180,154,experimental-setup,experimental setup,has,adam optimiser,experimental setup has adam optimiser,0.5539454817771912
translation,180,155,experimental-setup,initial learning rate,set to,0.001,initial learning rate set to 0.001,0.6823219656944275
translation,180,155,experimental-setup,initial learning rate,halved when reaching,plateau,initial learning rate halved when reaching plateau,0.6808598637580872
translation,180,155,experimental-setup,experimental setup,has,initial learning rate,experimental setup has initial learning rate,0.49018073081970215
translation,180,156,experimental-setup,experiments,run on,single titan v gpu,experiments run on single titan v gpu,0.7598764300346375
translation,180,156,experimental-setup,experimental setup,has,experiments,experimental setup has experiments,0.5502888560295105
translation,180,137,experiments,holl -e data,contains,"9,071 conversations","holl -e data contains 9,071 conversations",0.6790853142738342
translation,180,137,experiments,"9,071 conversations",covering,921 movies,"9,071 conversations covering 921 movies",0.7412711381912231
translation,180,7,model,"novel method , transformer & post based posterior approximation ( tppa )",to select,knowledge,"novel method , transformer & post based posterior approximation ( tppa ) to select knowledge",0.6990687251091003
translation,180,7,model,"novel method , transformer & post based posterior approximation ( tppa )",use,transformer with expanded decoder ( ted ) model,"novel method , transformer & post based posterior approximation ( tppa ) use transformer with expanded decoder ( ted ) model",0.5727269053459167
translation,180,7,model,transformer with expanded decoder ( ted ) model,to generate,responses,transformer with expanded decoder ( ted ) model to generate responses,0.6995192766189575
translation,180,7,model,responses,from both,post and the knowledge,responses from both post and the knowledge,0.7399808764457703
translation,180,7,model,model,apply,"novel method , transformer & post based posterior approximation ( tppa )","model apply novel method , transformer & post based posterior approximation ( tppa )",0.629973828792572
translation,180,34,model,transformer & post based posterior approximation ( tppa ) method,by applying,multi-stage processing,transformer & post based posterior approximation ( tppa ) method by applying multi-stage processing,0.7328891754150391
translation,180,34,model,multi-stage processing,of,posts,multi-stage processing of posts,0.6173309683799744
translation,180,34,model,multi-stage processing,of,response related knowledge,multi-stage processing of response related knowledge,0.5652898550033569
translation,180,34,model,response related knowledge,to capture,word and sentence level characteristics,response related knowledge to capture word and sentence level characteristics,0.6749842166900635
translation,180,34,model,word and sentence level characteristics,through,word embeddings,word and sentence level characteristics through word embeddings,0.6067870259284973
translation,180,34,model,word and sentence level characteristics,through,transformer and max-pooling,word and sentence level characteristics through transformer and max-pooling,0.6475707292556763
translation,180,34,model,word and sentence level characteristics,useful for,ranking,word and sentence level characteristics useful for ranking,0.6820028424263
translation,180,34,model,of new posts,during,test phase,of new posts during test phase,0.6926975846290588
translation,180,34,model,model,present,transformer & post based posterior approximation ( tppa ) method,model present transformer & post based posterior approximation ( tppa ) method,0.6377925872802734
translation,180,145,model,ted,adopts,transformer,ted adopts transformer,0.6977795362472534
translation,180,145,model,transformer,as,backbone framework,transformer as backbone framework,0.5848376750946045
translation,180,145,model,knowledge,by assigning,weights,knowledge by assigning weights,0.7593791484832764
translation,180,145,model,knowledge,from,multiple sources,knowledge from multiple sources,0.5829300880432129
translation,180,145,model,weights,to,knowledge,weights to knowledge,0.5700562000274658
translation,180,145,model,knowledge,from,multiple sources,knowledge from multiple sources,0.5829300880432129
translation,180,145,model,model,has,ted,model has ted,0.6839973330497742
translation,180,152,model,knowledge item,represented by,sentence,knowledge item represented by sentence,0.6793721318244934
translation,180,152,model,model,has,knowledge item,model has knowledge item,0.5319691896438599
translation,180,218,model,knowledge relevance,to,posts and responses,knowledge relevance to posts and responses,0.5811381340026855
translation,180,218,model,knowledge relevance,do not leverage,post-retrieved knowledge,knowledge relevance do not leverage post-retrieved knowledge,0.6701682806015015
translation,180,218,model,posts and responses,during,training,posts and responses during training,0.7371202707290649
translation,180,218,model,post-retrieved knowledge,during,testing,post-retrieved knowledge during testing,0.6634645462036133
translation,180,219,model,transformer & post based posterior approximation ( tppa ) model,applying,pseudo relevance feedback approach,transformer & post based posterior approximation ( tppa ) model applying pseudo relevance feedback approach,0.6751406788825989
translation,180,219,model,response related knowledge,into,training,response related knowledge into training,0.6090050339698792
translation,180,219,model,pseudo relevance feedback approach,by training,autopointer vector,pseudo relevance feedback approach by training autopointer vector,0.7487906813621521
translation,180,219,model,autopointer vector,to identify,potentially the most relevant knowledge,autopointer vector to identify potentially the most relevant knowledge,0.7106977105140686
translation,180,219,model,model,applying,pseudo relevance feedback approach,model applying pseudo relevance feedback approach,0.6784220933914185
translation,180,37,results,outperforms,has,set of strong baseline systems,outperforms has set of strong baseline systems,0.5955945253372192
translation,180,37,results,skt,has,sequential latentknowledge selection,skt has sequential latentknowledge selection,0.512072741985321
translation,180,173,results,fixed number of neg,vary,number of prks items,fixed number of neg vary number of prks items,0.7613639235496521
translation,180,173,results,fixed number of neg,find that,wiz and n=30,fixed number of neg find that wiz and n=30,0.7200976610183716
translation,180,173,results,fixed number of neg,find that,optimal prk number,fixed number of neg find that optimal prk number,0.6693975329399109
translation,180,173,results,fixed number of neg,find that,holl -e and neg =10,fixed number of neg find that holl -e and neg =10,0.7142254710197449
translation,180,173,results,fixed number of neg,find that,optimal prk number,fixed number of neg find that optimal prk number,0.6693975329399109
translation,180,173,results,fixed number of neg,for,wiz and n=30,fixed number of neg for wiz and n=30,0.6531168818473816
translation,180,173,results,fixed number of neg,for,holl -e and neg =10,fixed number of neg for holl -e and neg =10,0.6532533764839172
translation,180,173,results,optimal prk number,is,1,optimal prk number is 1,0.6022815108299255
translation,180,173,results,optimal prk number,is,10,optimal prk number is 10,0.6249464750289917
translation,180,173,results,fixed number of neg,has,optimal prk number,fixed number of neg has optimal prk number,0.5994536280632019
translation,180,173,results,wiz and n=30,has,optimal prk number,wiz and n=30 has optimal prk number,0.6165310144424438
translation,180,173,results,wiz and n=30,has,optimal prk number,wiz and n=30 has optimal prk number,0.6165310144424438
translation,180,173,results,holl -e and neg =10,has,optimal prk number,holl -e and neg =10 has optimal prk number,0.6123899221420288
translation,180,173,results,results,For,fixed number of neg,results For fixed number of neg,0.6684691905975342
translation,180,180,results,injecting knowledge,from,skt,injecting knowledge from skt,0.6194000840187073
translation,180,180,results,post only selection,using,bm25,post only selection using bm25,0.6857889294624329
translation,180,180,results,wiz and holl -e data sets,in terms of,bleu -4,wiz and holl -e data sets in terms of bleu -4,0.6927611231803894
translation,180,180,results,wiz and holl -e data sets,in terms of,meteor,wiz and holl -e data sets in terms of meteor,0.7486085891723633
translation,180,180,results,wiz and holl -e data sets,in terms of,bert-score,wiz and holl -e data sets in terms of bert-score,0.6833822131156921
translation,180,180,results,injecting knowledge,has,cnn - dssm and tppa,injecting knowledge has cnn - dssm and tppa,0.6138226985931396
translation,180,180,results,cnn - dssm and tppa,has,outperforms,cnn - dssm and tppa has outperforms,0.6222731471061707
translation,180,180,results,outperforms,has,post only selection,outperforms has post only selection,0.6323888301849365
translation,180,180,results,results,observe,injecting knowledge,results observe injecting knowledge,0.5389841794967651
translation,180,184,results,knowledge selection methods,with,better retrieval performance,knowledge selection methods with better retrieval performance,0.6127141714096069
translation,180,184,results,better retrieval performance,achieve,better response generation metrics,better retrieval performance achieve better response generation metrics,0.6331450343132019
translation,180,184,results,results,has,knowledge selection methods,results has knowledge selection methods,0.5060791373252869
translation,180,186,results,tppa with 1rrk - 30neg - 1 prk,achieves,best retrieval performance,tppa with 1rrk - 30neg - 1 prk achieves best retrieval performance,0.7112951278686523
translation,180,186,results,tppa with 1rrk - 30neg - 1 prk,achieves,better results,tppa with 1rrk - 30neg - 1 prk achieves better results,0.7034379243850708
translation,180,186,results,better results,on,generative models ( ted and wseq ),better results on generative models ( ted and wseq ),0.5380468964576721
translation,180,186,results,wiz data set,has,tppa with 1rrk - 30neg - 1 prk,wiz data set has tppa with 1rrk - 30neg - 1 prk,0.6328235864639282
translation,180,186,results,results,For,wiz data set,results For wiz data set,0.637505292892456
translation,180,187,results,holl -e data set,where,tppa,holl -e data set where tppa,0.6505222916603088
translation,180,187,results,other models,including,post - ks,other models including post - ks,0.7470604777336121
translation,180,187,results,other models,including,skt,other models including skt,0.7143501043319702
translation,180,187,results,tppa,has,outperforms,tppa has outperforms,0.6250160336494446
translation,180,187,results,outperforms,has,other models,outperforms has other models,0.5875559449195862
translation,180,187,results,results,confirmed on,holl -e data set,results confirmed on holl -e data set,0.7319031953811646
translation,180,241,results,5 knowledge-injection results,on,wizard of wikipedia data set,5 knowledge-injection results on wizard of wikipedia data set,0.48840054869651794
translation,182,6,model,novel approach,to,incremental decision making,novel approach to incremental decision making,0.5698648691177368
translation,182,6,model,incremental decision making,based on,hierarchical reinforcement learning,incremental decision making based on hierarchical reinforcement learning,0.5774854421615601
translation,182,6,model,hierarchical reinforcement learning,to achieve,interactive optimisation of information presentation ( ip ) strategies,hierarchical reinforcement learning to achieve interactive optimisation of information presentation ( ip ) strategies,0.6320309042930603
translation,182,6,model,barge-ins,by employing,recent psycholinguistic hypothesis of information density ( id ),barge-ins by employing recent psycholinguistic hypothesis of information density ( id ),0.679388701915741
translation,182,6,model,comprehend,has,backchannels,comprehend has backchannels,0.6149229407310486
translation,182,17,model,novel approach,to,incremental decision making,novel approach to incremental decision making,0.5698648691177368
translation,182,17,model,incremental decision making,for,output planning,incremental decision making for output planning,0.6219233870506287
translation,182,17,model,model,present,novel approach,model present novel approach,0.6989931464195251
translation,182,18,model,comprehend backchannels and bargeins,based on,partially data-driven reward function,comprehend backchannels and bargeins based on partially data-driven reward function,0.6459203958511353
translation,182,226,results,absolute comparison,of,last 1000 episodes,absolute comparison of last 1000 episodes,0.599321722984314
translation,182,226,results,behaviour,shows,improvement,behaviour shows improvement,0.5901816487312317
translation,182,226,results,improvement,of,hrl agent,improvement of hrl agent,0.5922925472259521
translation,182,226,results,improvement,corresponds to,23.42 %,improvement corresponds to 23.42 %,0.6830231547355652
translation,182,226,results,hrl agent,over,baseline 1,hrl agent over baseline 1,0.7041664719581604
translation,183,48,results,results,of,sail - grs system,results of sail - grs system,0.6024956107139587
translation,183,48,results,baseline,in,all dataset categories,baseline in all dataset categories,0.49724113941192627
translation,183,48,results,all dataset categories,except,tourism domain,all dataset categories except tourism domain,0.6505932807922363
translation,183,48,results,sail - grs system,has,outperform,sail - grs system has outperform,0.6497021913528442
translation,183,48,results,outperform,has,baseline,outperform has baseline,0.6223028898239136
translation,183,48,results,results,of,sail - grs system,results of sail - grs system,0.6024956107139587
translation,183,48,results,results,has,results,results has results,0.48582205176353455
translation,183,49,results,both systems,present,highest scores,both systems present highest scores,0.6586582064628601
translation,183,49,results,highest scores,compared to,other domains,highest scores compared to other domains,0.523786187171936
translation,183,50,results,high results,in,travel domain,high results in travel domain,0.5371055603027344
translation,183,50,results,high results,due to,high data overlap,high results due to high data overlap,0.6568571925163269
translation,183,50,results,high data overlap,between,train and the test data,high data overlap between train and the test data,0.6825501918792725
translation,183,50,results,results,has,high results,results has high results,0.619356632232666
translation,183,52,results,overall higher f measures,of,sail - grs system,overall higher f measures of sail - grs system,0.6008908152580261
translation,183,52,results,overall higher f measures,due to,higher precision scores,overall higher f measures due to higher precision scores,0.6853174567222595
translation,183,52,results,sail - grs system,in,travel and finance domains,sail - grs system in travel and finance domains,0.5725238919258118
translation,183,52,results,baseline system,displays,higher recall,baseline system displays higher recall,0.7073991894721985
translation,183,52,results,baseline system,displays,lower precision scores,baseline system displays lower precision scores,0.7056726217269897
translation,183,52,results,baseline system,displays,lower f-measure,baseline system displays lower f-measure,0.6913968920707703
translation,183,52,results,results,observe,overall higher f measures,results observe overall higher f measures,0.5758649706840515
translation,183,53,results,overall lowest scores,for,both systems,overall lowest scores for both systems,0.5827157497406006
translation,183,53,results,both systems,reached in,travel domain,both systems reached in travel domain,0.7155863046646118
translation,183,53,results,travel domain,for,greek,travel domain for greek,0.6060547828674316
translation,183,53,results,results,has,overall lowest scores,results has overall lowest scores,0.5134630799293518
translation,183,54,results,performance,of,sail - grs system,performance of sail - grs system,0.6188331842422485
translation,183,54,results,performance,of,sail - grs system,performance of sail - grs system,0.6188331842422485
translation,183,54,results,does not deteriorate,to,same extent,does not deteriorate to same extent,0.6219184994697571
translation,183,54,results,same extent,as,baseline,same extent as baseline,0.5936489105224609
translation,183,54,results,sail - grs system,has,does not deteriorate,sail - grs system has does not deteriorate,0.6526758670806885
translation,183,54,results,results,has,performance,results has performance,0.5972660779953003
translation,184,7,model,dynamically computed corpuslevel statistics,to determine,conversational participants,dynamically computed corpuslevel statistics to determine conversational participants,0.6149983406066895
translation,184,7,model,model,uses,dynamically computed corpuslevel statistics,model uses dynamically computed corpuslevel statistics,0.6223747730255127
translation,184,16,model,novel method,to collect and determine,more diverse data,novel method to collect and determine more diverse data,0.6157498955726624
translation,184,16,model,more diverse data,to train,models,more diverse data to train models,0.6744346022605896
translation,184,16,model,model,propose,novel method,model propose novel method,0.7230806350708008
translation,184,22,model,more diverse sub-corpus,from,existing larger collection,more diverse sub-corpus from existing larger collection,0.5628764033317566
translation,184,26,model,model,has,diversity -informed data collection,model has diversity -informed data collection,0.5037454962730408
translation,184,8,results,diversity -informed data collection,produces,significantly more diverse data,diversity -informed data collection produces significantly more diverse data,0.6115055680274963
translation,184,8,results,diversity -informed data collection,produces,better results,diversity -informed data collection produces better results,0.6334799528121948
translation,184,8,results,significantly more diverse data,than,baseline data collection methods,significantly more diverse data than baseline data collection methods,0.5423885583877563
translation,184,8,results,better results,on,downstream tasks,better results on downstream tasks,0.5511670112609863
translation,184,8,results,results,has,diversity -informed data collection,results has diversity -informed data collection,0.4568769037723541
translation,185,7,ablation-analysis,annotation,of,supporting span,annotation of supporting span,0.5682206153869629
translation,185,7,ablation-analysis,supporting span,for,multiwoz 2.1,supporting span for multiwoz 2.1,0.6269588470458984
translation,185,7,ablation-analysis,supporting span,is,shortest span,supporting span is shortest span,0.519538164138794
translation,185,7,ablation-analysis,shortest span,to support,labeled value,shortest span to support labeled value,0.6583817005157471
translation,185,7,ablation-analysis,ablation analysis,supplement,annotation,ablation analysis supplement annotation,0.6867009997367859
translation,185,61,ablation-analysis,vn,gain,positive performance,vn gain positive performance,0.8184196949005127
translation,185,61,ablation-analysis,positive performance,for,savn,positive performance for savn,0.6747702956199646
translation,185,61,ablation-analysis,savn,as long as,integrity of ontology,savn as long as integrity of ontology,0.5513310432434082
translation,185,61,ablation-analysis,integrity of ontology,is,more than 30 %,integrity of ontology is more than 30 %,0.5545284152030945
translation,185,61,ablation-analysis,ablation analysis,show,vn,ablation analysis show vn,0.5967699289321899
translation,185,154,ablation-analysis,vn,prove that,vn,vn prove that vn,0.14339673519134521
translation,185,154,ablation-analysis,vn,has,enough performance,vn has enough performance,0.6239632964134216
translation,185,154,ablation-analysis,vn,has,enough performance,vn has enough performance,0.6239632964134216
translation,185,154,ablation-analysis,enough performance,for,multiwoz dataset,enough performance for multiwoz dataset,0.6170179843902588
translation,185,154,ablation-analysis,multiwoz dataset,with,only one transformer layer,multiwoz dataset with only one transformer layer,0.6366215348243713
translation,185,154,ablation-analysis,vn,has,enough performance,vn has enough performance,0.6239632964134216
translation,185,154,ablation-analysis,ablation analysis,comparing,vn,ablation analysis comparing vn,0.6642506718635559
translation,185,176,ablation-analysis,attraction - name,mainly due to,repair of spelling mistakes,attraction - name mainly due to repair of spelling mistakes,0.6662483811378479
translation,185,176,ablation-analysis,attraction - type,benefits from,normalization of varied expressions,attraction - type benefits from normalization of varied expressions,0.7169178128242493
translation,185,176,ablation-analysis,ablation analysis,improvement of,attraction - name,ablation analysis improvement of attraction - name,0.7043757438659668
translation,185,176,ablation-analysis,ablation analysis,improvement of,attraction - type,ablation analysis improvement of attraction - type,0.7505176663398743
translation,185,131,experiments,sa and vn,trained with,adam optimizer,sa and vn trained with adam optimizer,0.7396547198295593
translation,185,131,experiments,adam optimizer,),learning rate,adam optimizer ) learning rate,0.5770988464355469
translation,185,131,experiments,adam optimizer,in which,learning rate,adam optimizer in which learning rate,0.5739453434944153
translation,185,131,experiments,linearly decreases,from,5e ?5 and 1e ?4,linearly decreases from 5e ?5 and 1e ?4,0.6393256187438965
translation,185,131,experiments,learning rate,has,linearly decreases,learning rate has linearly decreases,0.5732414722442627
translation,185,128,hyperparameters,pre-trained bert - base-uncased model,as,utterance encoder,pre-trained bert - base-uncased model as utterance encoder,0.5162320137023926
translation,185,128,hyperparameters,utterance encoder,in,sa,utterance encoder in sa,0.5695739984512329
translation,185,128,hyperparameters,12 hid - den layers,with,768 units,12 hid - den layers with 768 units,0.6730654835700989
translation,185,128,hyperparameters,hyperparameters,use,pre-trained bert - base-uncased model,hyperparameters use pre-trained bert - base-uncased model,0.5980822443962097
translation,185,129,hyperparameters,limitation,of,maximum sequence length,limitation of maximum sequence length,0.5776112079620361
translation,185,129,hyperparameters,maximum sequence length,set,m,maximum sequence length set m,0.6834648847579956
translation,185,129,hyperparameters,m,to,9,m to 9,0.7010708451271057
translation,185,129,hyperparameters,hyperparameters,For,limitation,hyperparameters For limitation,0.581321656703949
translation,185,129,hyperparameters,hyperparameters,set,m,hyperparameters set m,0.6662734150886536
translation,185,132,hyperparameters,sa,with,3 epochs,sa with 3 epochs,0.7174953818321228
translation,185,132,hyperparameters,sa,with,5 epochs,sa with 5 epochs,0.6970416903495789
translation,185,132,hyperparameters,5 epochs,on,multi-woz 2.0 and multiwoz 2.1,5 epochs on multi-woz 2.0 and multiwoz 2.1,0.5463049411773682
translation,185,132,hyperparameters,hyperparameters,trained,sa,hyperparameters trained sa,0.6959959268569946
translation,185,6,model,new architecture,to cleverly exploit,ontology,new architecture to cleverly exploit ontology,0.6513254642486572
translation,185,6,model,model,propose,new architecture,model propose new architecture,0.7437537312507629
translation,185,53,model,model,of,dst,model of dst,0.628861665725708
translation,185,53,model,model,into,slot attention ( sa ),model into slot attention ( sa ),0.6071060299873352
translation,185,53,model,model,divide,model,model divide model,0.679488480091095
translation,185,53,model,model,of,dst,model of dst,0.628861665725708
translation,185,10,results,empirical results,demonstrate,savn,empirical results demonstrate savn,0.6363860964775085
translation,185,10,results,savn,achieves,state - of - the - art joint accuracy,savn achieves state - of - the - art joint accuracy,0.6443062424659729
translation,185,10,results,state - of - the - art joint accuracy,of,54.52 %,state - of - the - art joint accuracy of 54.52 %,0.5223239660263062
translation,185,10,results,state - of - the - art joint accuracy,of,54.86 %,state - of - the - art joint accuracy of 54.86 %,0.52567458152771
translation,185,10,results,54.52 %,on,multiwoz 2.0,54.52 % on multiwoz 2.0,0.5609003901481628
translation,185,10,results,54.52 %,on,multiwoz 2.1,54.52 % on multiwoz 2.1,0.5593281388282776
translation,185,10,results,54.86 %,on,multiwoz 2.1,54.86 % on multiwoz 2.1,0.5543564558029175
translation,185,10,results,results,has,empirical results,results has empirical results,0.4882138967514038
translation,185,149,results,our model,achieves,highest performance,our model achieves highest performance,0.6964774131774902
translation,185,149,results,54.52 %,of,joint accuracy,54.52 % of joint accuracy,0.5348426699638367
translation,185,149,results,multiwoz 2.0,has,our model,multiwoz 2.0 has our model,0.5888165831565857
translation,185,149,results,highest performance,has,54.52 %,highest performance has 54.52 %,0.5474784970283508
translation,185,149,results,results,On,multiwoz 2.0,results On multiwoz 2.0,0.5503654479980469
translation,185,152,results,performance,of,sa,performance of sa,0.6376556754112244
translation,185,152,results,performance,of,sa,performance of sa,0.6376556754112244
translation,185,152,results,sa,similar to,trade,sa similar to trade,0.6473763585090637
translation,185,152,results,about 5 % higher absolute performance,than,dst - span,about 5 % higher absolute performance than dst - span,0.6039013266563416
translation,185,152,results,span-based method,using,bert,span-based method using bert,0.7011062502861023
translation,185,152,results,sa,has,about 5 % higher absolute performance,sa has about 5 % higher absolute performance,0.5815553069114685
translation,185,153,results,sa sp,by,vn,sa sp by vn,0.6395987272262573
translation,185,153,results,similar,without,vn,similar without vn,0.8016691207885742
translation,185,153,results,similar,without,vn,similar without vn,0.8016691207885742
translation,185,153,results,improvement,of,sa sp performance,improvement of sa sp performance,0.5913991928100586
translation,185,153,results,sa sp performance,obviously greater than,sa raw,sa sp performance obviously greater than sa raw,0.6455308794975281
translation,185,153,results,sa raw,by,vn,sa raw by vn,0.6467005610466003
translation,185,165,results,performance,of,vn,performance of vn,0.6736805438995361
translation,185,165,results,steadily improved,with,increased usage rate,steadily improved with increased usage rate,0.6883136630058289
translation,185,165,results,increased usage rate,of,ontology,increased usage rate of ontology,0.6348694562911987
translation,185,165,results,performance,has,steadily improved,performance has steadily improved,0.6046807169914246
translation,185,165,results,vn,has,steadily improved,vn has steadily improved,0.6511275768280029
translation,185,165,results,results,demonstrate that,performance,results demonstrate that performance,0.689225971698761
translation,186,152,baselines,hyst,employs,hierarchical rnn encoder,hyst employs hierarchical rnn encoder,0.5853887796401978
translation,186,152,baselines,hyst,takes,hybrid approach,hyst takes hybrid approach,0.6794120669364929
translation,186,152,baselines,hybrid approach,incorporates,predefined ontology - based setting,hybrid approach incorporates predefined ontology - based setting,0.7176361083984375
translation,186,152,baselines,hybrid approach,incorporates,open vocabulary - based setting,hybrid approach incorporates open vocabulary - based setting,0.7058372497558594
translation,186,152,baselines,baselines,has,hyst,baselines has hyst,0.6272464990615845
translation,186,156,baselines,comer,uses,bert - large,comer uses bert - large,0.6682814359664917
translation,186,156,baselines,comer,uses,hierarchical lstm decoder,comer uses hierarchical lstm decoder,0.5998055338859558
translation,186,156,baselines,comer,as,hierarchical lstm decoder,comer as hierarchical lstm decoder,0.5429743528366089
translation,186,156,baselines,bert - large,as,feature extractor,bert - large as feature extractor,0.5571086406707764
translation,186,156,baselines,bert - large,as,hierarchical lstm decoder,bert - large as hierarchical lstm decoder,0.582127571105957
translation,186,156,baselines,hierarchical lstm decoder,to generate,current turn dialogue state,hierarchical lstm decoder to generate current turn dialogue state,0.6226786375045776
translation,186,156,baselines,current turn dialogue state,as,target sequence,current turn dialogue state as target sequence,0.5465389490127563
translation,186,156,baselines,nadst,uses,transformer - based nonautoregressive decoder,nadst uses transformer - based nonautoregressive decoder,0.5821417570114136
translation,186,156,baselines,transformer - based nonautoregressive decoder,to generate,current turn dialogue state,transformer - based nonautoregressive decoder to generate current turn dialogue state,0.6633421182632446
translation,186,156,baselines,ml - bst,uses,transformer - based architecture,ml - bst uses transformer - based architecture,0.6219063997268677
translation,186,156,baselines,ml - bst,combines,outputs,ml - bst combines outputs,0.7624287009239197
translation,186,156,baselines,transformer - based architecture,to encode,dialogue context,transformer - based architecture to encode dialogue context,0.7165408134460449
translation,186,156,baselines,dialogue context,with,domain and slot information,dialogue context with domain and slot information,0.6193602085113525
translation,186,156,baselines,outputs,in,late fusion approach,outputs in late fusion approach,0.6006786227226257
translation,186,156,baselines,baselines,has,comer,baselines has comer,0.6290825605392456
translation,186,131,experiments,bertadam,as,optimizer,bertadam as optimizer,0.5818175673484802
translation,186,128,hyperparameters,pretrained bert - base-uncased model,for,state operation predictor,pretrained bert - base-uncased model for state operation predictor,0.6351230144500732
translation,186,128,hyperparameters,one gru,for,slot value generator,one gru for slot value generator,0.6312172412872314
translation,186,128,hyperparameters,hyperparameters,employ,pretrained bert - base-uncased model,hyperparameters employ pretrained bert - base-uncased model,0.49540919065475464
translation,186,129,hyperparameters,hidden size,of,decoder,hidden size of decoder,0.624654233455658
translation,186,129,hyperparameters,hidden size,of,encoder,hidden size of encoder,0.6120144724845886
translation,186,129,hyperparameters,decoder,same as,encoder,decoder same as encoder,0.6426581144332886
translation,186,129,hyperparameters,hyperparameters,has,hidden size,hyperparameters has hidden size,0.535134494304657
translation,186,132,hyperparameters,greedy decoding,for,slot value generator,greedy decoding for slot value generator,0.6063086986541748
translation,186,132,hyperparameters,hyperparameters,use,greedy decoding,hyperparameters use greedy decoding,0.6568648219108582
translation,186,135,hyperparameters,peak learning rate and warmup proportion,to,4e - 5 and 0.1,peak learning rate and warmup proportion to 4e - 5 and 0.1,0.5732753872871399
translation,186,135,hyperparameters,peak learning rate and warmup proportion,to,1e - 4 and 0.1,peak learning rate and warmup proportion to 1e - 4 and 0.1,0.5748229622840881
translation,186,135,hyperparameters,4e - 5 and 0.1,for,encoder,4e - 5 and 0.1 for encoder,0.6066544651985168
translation,186,135,hyperparameters,4e - 5 and 0.1,for,decoder,4e - 5 and 0.1 for decoder,0.6112204790115356
translation,186,135,hyperparameters,4e - 5 and 0.1,for,decoder,4e - 5 and 0.1 for decoder,0.6112204790115356
translation,186,135,hyperparameters,1e - 4 and 0.1,for,decoder,1e - 4 and 0.1 for decoder,0.6373339295387268
translation,186,135,hyperparameters,hyperparameters,set,peak learning rate and warmup proportion,hyperparameters set peak learning rate and warmup proportion,0.644281804561615
translation,186,136,hyperparameters,batch size,of,32,batch size of 32,0.6741614937782288
translation,186,136,hyperparameters,"dropout ( srivastava et al. , 2014 ) rate",to,0.1,"dropout ( srivastava et al. , 2014 ) rate to 0.1",0.505247950553894
translation,186,136,hyperparameters,hyperparameters,set,"dropout ( srivastava et al. , 2014 ) rate","hyperparameters set dropout ( srivastava et al. , 2014 ) rate",0.5708841681480408
translation,186,137,hyperparameters,"word dropout ( bowman et al. , 2016 )",by randomly replacing,input tokens,"word dropout ( bowman et al. , 2016 ) by randomly replacing input tokens",0.6865060329437256
translation,186,137,hyperparameters,input tokens,with,special [ unk ] token,input tokens with special [ unk ] token,0.5976485013961792
translation,186,137,hyperparameters,special [ unk ] token,probability of,0.1,special [ unk ] token probability of 0.1,0.6958329081535339
translation,186,137,hyperparameters,hyperparameters,utilize,"word dropout ( bowman et al. , 2016 )","hyperparameters utilize word dropout ( bowman et al. , 2016 )",0.4899750351905823
translation,186,138,hyperparameters,max sequence length,for,all inputs,max sequence length for all inputs,0.5954176187515259
translation,186,138,hyperparameters,all inputs,fixed to,256,all inputs fixed to 256,0.7416814565658569
translation,186,138,hyperparameters,hyperparameters,has,max sequence length,hyperparameters has max sequence length,0.4825800061225891
translation,186,139,hyperparameters,jointly,for,30 epochs,jointly for 30 epochs,0.6456647515296936
translation,186,139,hyperparameters,best performance,on,validation set,best performance on validation set,0.5691216588020325
translation,186,139,hyperparameters,state operation predictor and slot value generator,has,jointly,state operation predictor and slot value generator has jointly,0.5876655578613281
translation,186,139,hyperparameters,hyperparameters,train,state operation predictor and slot value generator,hyperparameters train state operation predictor and slot value generator,0.7051317095756531
translation,186,144,hyperparameters,50 % of the time,to train,decoder,50 % of the time to train decoder,0.7087679505348206
translation,186,144,hyperparameters,teacher forcing,has,50 % of the time,teacher forcing has 50 % of the time,0.5861451029777527
translation,186,7,model,predicting state operation,on,each of the memory slots,predicting state operation on each of the memory slots,0.5568615794181824
translation,186,7,model,memory,with,new values,memory with new values,0.6200257539749146
translation,186,7,model,overwriting,has,memory,overwriting has memory,0.5833626985549927
translation,186,8,model,dst,into,two sub-tasks,dst into two sub-tasks,0.6024299263954163
translation,186,28,model,som - dst,has,selectively overwriting memory for dialogue state tracking,som - dst has selectively overwriting memory for dialogue state tracking,0.5542129278182983
translation,186,28,model,model,propose,som - dst,model propose som - dst,0.6898749470710754
translation,186,29,model,dst,into,two sub-tasks,dst into two sub-tasks,0.6024299263954163
translation,186,29,model,state operation prediction,decides,types of the operations to be performed,state operation prediction decides types of the operations to be performed,0.7151023149490356
translation,186,130,model,token embedding matrix,of,slot value generator,token embedding matrix of slot value generator,0.5520985126495361
translation,186,130,model,slot value generator,shared with,state operation predictor,slot value generator shared with state operation predictor,0.6182958483695984
translation,186,130,model,model,has,token embedding matrix,model has token embedding matrix,0.5482039451599121
translation,186,133,model,state operation predictor,makes use of,pretrained model,state operation predictor makes use of pretrained model,0.6798717379570007
translation,186,133,model,slot value generator,to be trained from,scratch,slot value generator to be trained from scratch,0.7963609099388123
translation,186,134,model,different learning rate schemes,for,encoder and the decoder,different learning rate schemes for encoder and the decoder,0.6297153234481812
translation,186,134,model,model,use,different learning rate schemes,model use different learning rate schemes,0.6789327263832092
translation,186,155,model,trade,encodes,whole dialogue context,trade encodes whole dialogue context,0.7632924914360046
translation,186,155,model,trade,decodes,value,trade decodes value,0.713292121887207
translation,186,155,model,whole dialogue context,with,bidirectional gru,whole dialogue context with bidirectional gru,0.6760395169258118
translation,186,155,model,value,for,every slot,value for every slot,0.6559250950813293
translation,186,155,model,every slot,using,copy-augmented gru decoder,every slot using copy-augmented gru decoder,0.7067205905914307
translation,186,155,model,model,encodes,whole dialogue context,model encodes whole dialogue context,0.7193001508712769
translation,186,155,model,model,decodes,value,model decodes value,0.7499917149543762
translation,186,155,model,model,has,trade,model has trade,0.5830050706863403
translation,186,34,results,proposed som - dst,achieves,state - of- theart joint goal accuracy,proposed som - dst achieves state - of- theart joint goal accuracy,0.6619386076927185
translation,186,34,results,state - of- theart joint goal accuracy,in,open vocabulary - based dst setting,state - of- theart joint goal accuracy in open vocabulary - based dst setting,0.5136442184448242
translation,186,34,results,state - of- theart joint goal accuracy,on,two of the most actively studied datasets,state - of- theart joint goal accuracy on two of the most actively studied datasets,0.47364068031311035
translation,186,34,results,open vocabulary - based dst setting,on,two of the most actively studied datasets,open vocabulary - based dst setting on two of the most actively studied datasets,0.4805430769920349
translation,186,34,results,results,has,proposed som - dst,results has proposed som - dst,0.5912491679191589
translation,186,160,results,joint goal accuracy,of,som - dst and other models,joint goal accuracy of som - dst and other models,0.5712417960166931
translation,186,160,results,joint goal accuracy,on,test set,joint goal accuracy on test set,0.5141354203224182
translation,186,160,results,som - dst and other models,on,test set,som - dst and other models on test set,0.5365760326385498
translation,186,160,results,test set,of,multiwoz 2.0 and multiwoz 2.1,test set of multiwoz 2.0 and multiwoz 2.1,0.597270667552948
translation,186,160,results,results,shows,joint goal accuracy,results shows joint goal accuracy,0.6469613313674927
translation,186,162,results,som - dst,achieves,state - of - the - art performance,som - dst achieves state - of - the - art performance,0.6641196012496948
translation,186,162,results,state - of - the - art performance,in,open vocabularybased setting,state - of - the - art performance in open vocabularybased setting,0.4759848117828369
translation,186,162,results,results,has,som - dst,results has som - dst,0.5259467363357544
translation,186,163,results,our model,achieves,higher performance,our model achieves higher performance,0.7029496431350708
translation,186,163,results,higher performance,on,multiwoz 2.1,higher performance on multiwoz 2.1,0.5596599578857422
translation,186,163,results,multiwoz 2.1,than on,multiwoz 2.0,multiwoz 2.1 than on multiwoz 2.0,0.6201228499412537
translation,187,5,model,  two - teacher one - student   learning framework ( ttos ),for,task - oriented dialogue,  two - teacher one - student   learning framework ( ttos ) for task - oriented dialogue,0.6118524670600891
translation,187,5,model,model,propose,  two - teacher one - student   learning framework ( ttos ),model propose   two - teacher one - student   learning framework ( ttos ),0.6632559895515442
translation,187,6,model,ttos,amalgamates,knowledge,ttos amalgamates knowledge,0.7202907800674438
translation,187,6,model,knowledge,from,two teacher networks,knowledge from two teacher networks,0.6120673418045044
translation,187,6,model,knowledge,provide,comprehensive guidance,knowledge provide comprehensive guidance,0.6577033400535583
translation,187,6,model,comprehensive guidance,to build,highquality task - oriented dialogue system ( student network ),comprehensive guidance to build highquality task - oriented dialogue system ( student network ),0.6829385161399841
translation,187,6,model,model,has,ttos,model has ttos,0.641048014163971
translation,187,7,model,teacher network,trained via,reinforcement learning,teacher network trained via reinforcement learning,0.755927562713623
translation,187,7,model,reinforcement learning,with,goal-specific reward,reinforcement learning with goal-specific reward,0.5791957974433899
translation,187,7,model,goal-specific reward,viewed as,expert,goal-specific reward viewed as expert,0.5846754908561707
translation,187,7,model,expert,towards,goal,expert towards goal,0.719898521900177
translation,187,7,model,expert,transfers,professional characteristic,expert transfers professional characteristic,0.652208685874939
translation,187,7,model,professional characteristic,to,student network,professional characteristic to student network,0.5404766201972961
translation,187,7,model,model,has,teacher network,model has teacher network,0.5630258917808533
translation,187,8,model,two discriminators,as in,generative adversarial network ( gan ),two discriminators as in generative adversarial network ( gan ),0.6031770706176758
translation,187,8,model,knowledge,from,two teachers,knowledge from two teachers,0.6243478655815125
translation,187,8,model,model,introduce,two discriminators,model introduce two discriminators,0.6424264311790466
translation,187,9,model,discriminators,relaxes,rigid coupling,discriminators relaxes rigid coupling,0.6930716037750244
translation,187,9,model,rigid coupling,between,student and teachers,rigid coupling between student and teachers,0.7332221865653992
translation,187,9,model,model,usage of,discriminators,model usage of discriminators,0.744448721408844
translation,187,28,model,  two - teacher one - student   learning framework ( ttos ),for building,high-quality tds ( student ),  two - teacher one - student   learning framework ( ttos ) for building high-quality tds ( student ),0.7208874225616455
translation,187,28,model,high-quality tds ( student ),where,student network,high-quality tds ( student ) where student network,0.6085885167121887
translation,187,28,model,student network,encouraged to integrate,knowledge,student network encouraged to integrate knowledge,0.7636322379112244
translation,187,28,model,knowledge,from,two expert teacher networks,knowledge from two expert teacher networks,0.6073969006538391
translation,187,28,model,model,propose,  two - teacher one - student   learning framework ( ttos ),model propose   two - teacher one - student   learning framework ( ttos ),0.6632559895515442
translation,187,32,model,generative adversarial network ( gan ),to transfer,knowledge,generative adversarial network ( gan ) to transfer knowledge,0.6737882494926453
translation,187,32,model,knowledge,from,two teachers,knowledge from two teachers,0.6243478655815125
translation,187,32,model,model,employ,generative adversarial network ( gan ),model employ generative adversarial network ( gan ),0.552105188369751
translation,187,38,results,in - car assistant and camrest datasets,demonstrate,ttos,in - car assistant and camrest datasets demonstrate ttos,0.6071767807006836
translation,187,38,results,ttos,achieves,impressive results,ttos achieves impressive results,0.6979549527168274
translation,187,38,results,impressive results,compared to,baseline methods,impressive results compared to baseline methods,0.6037958264350891
translation,187,38,results,baseline methods,across,multiple evaluation metrics,baseline methods across multiple evaluation metrics,0.6571276783943176
translation,187,38,results,results,on,in - car assistant and camrest datasets,results on in - car assistant and camrest datasets,0.5011688470840454
translation,188,36,ablation-analysis,discrete,identify,autoaugment,discrete identify autoaugment,0.6057705879211426
translation,188,36,ablation-analysis,autoaugment,as,more proper base model to use,autoaugment as more proper base model to use,0.5369234085083008
translation,188,36,ablation-analysis,autoaugment,adapt it,linguistically,autoaugment adapt it linguistically,0.7054280042648315
translation,188,36,ablation-analysis,linguistically,challenging task of,generative dialogue tasks,linguistically challenging task of generative dialogue tasks,0.7566716074943542
translation,188,36,ablation-analysis,ablation analysis,identify,autoaugment,ablation analysis identify autoaugment,0.6520051956176758
translation,188,36,ablation-analysis,ablation analysis,adapt it,linguistically,ablation analysis adapt it linguistically,0.6921802759170532
translation,188,71,baselines,variational hierarchical encoder-decoder ( vhred ),on,troubleshooting ubuntu dialogue task,variational hierarchical encoder-decoder ( vhred ) on troubleshooting ubuntu dialogue task,0.5426626205444336
translation,188,70,hyperparameters,search,use,best policy,search use best policy,0.7055798172950745
translation,188,70,hyperparameters,best policy,to train,target model,best policy to train target model,0.7453283071517944
translation,188,70,hyperparameters,best policy,evaluate on,test set,best policy evaluate on test set,0.6997829079627991
translation,188,70,hyperparameters,target model,has,from scratch,target model has from scratch,0.5693731904029846
translation,188,70,hyperparameters,hyperparameters,end of,search,hyperparameters end of search,0.6619683504104614
translation,188,29,model,some operations,require,source inputs,some operations require source inputs,0.6681250333786011
translation,188,29,model,some operations,explore,controller,some operations explore controller,0.7170763611793518
translation,188,29,model,source inputs,to have,certain linguistic features,source inputs to have certain linguistic features,0.5949666500091553
translation,188,29,model,source inputs,of,target dataset,source inputs of target dataset,0.5680184960365295
translation,188,29,model,controller,conditions on,source inputs,controller conditions on source inputs,0.7588090300559998
translation,188,29,model,source inputs,of,target dataset,source inputs of target dataset,0.5680184960365295
translation,188,29,model,source inputs,via,sequence - to-sequence ( seq2seq ) controller,source inputs via sequence - to-sequence ( seq2seq ) controller,0.6577882766723633
translation,188,29,model,model,observing,some operations,model observing some operations,0.7154188752174377
translation,188,29,model,model,explore,controller,model explore controller,0.7017835974693298
translation,188,91,results,all dataaugmentation approaches,improve,statistically significantly ( p < 0.01 ),all dataaugmentation approaches improve statistically significantly ( p < 0.01 ),0.6535175442695618
translation,188,91,results,statistically significantly ( p < 0.01 ),over,strongest baseline,statistically significantly ( p < 0.01 ) over strongest baseline,0.6547330617904663
translation,188,91,results,strongest baseline,obtained,significantly more net wins,strongest baseline obtained significantly more net wins,0.6535952091217041
translation,188,91,results,significantly more net wins,than,vhred - attn baseline,significantly more net wins than vhred - attn baseline,0.5895625352859497
translation,188,91,results,results,shows,all dataaugmentation approaches,results shows all dataaugmentation approaches,0.6220402121543884
translation,189,83,baselines,3 types of classification models,implemented via,three packages,3 types of classification models implemented via three packages,0.7119948267936707
translation,189,83,baselines,3 types of classification models,implemented via,j48,3 types of classification models implemented via j48,0.6907111406326294
translation,189,83,baselines,j48,from,"weka ( hall et al. , 2009 )","j48 from weka ( hall et al. , 2009 )",0.5687144994735718
translation,189,83,baselines,j48,from,libsvm,j48 from libsvm,0.5505999326705933
translation,189,83,baselines,baselines,experimented with,3 types of classification models,baselines experimented with 3 types of classification models,0.730852484703064
translation,189,102,results,set,is,limited,set is limited,0.647258460521698
translation,189,102,results,all the classification models,perform,better,all the classification models perform better,0.6024354100227356
translation,189,102,results,better,than,baseline,better than baseline,0.6157954335212708
translation,189,102,results,biggest improvement,is,14.4 %,biggest improvement is 14.4 %,0.5394008755683899
translation,189,102,results,biggest improvement,solving,25 more pronouns and deictics,biggest improvement solving 25 more pronouns and deictics,0.6174706816673279
translation,189,102,results,14.4 %,in,f-score,14.4 % in f-score,0.5216541290283203
translation,189,102,results,limited,has,all the classification models,limited has all the classification models,0.5918031930923462
translation,190,56,baselines,two approaches,applying,two different statistical classification methods,two approaches applying two different statistical classification methods,0.6638833284378052
translation,190,56,baselines,two different statistical classification methods,for,showcase corpus,two different statistical classification methods for showcase corpus,0.637361466884613
translation,190,56,baselines,baselines,present,two approaches,baselines present two approaches,0.6615917682647705
translation,190,10,model,relationship,between,user and expert ratings,relationship between user and expert ratings,0.6648896932601929
translation,190,10,model,model,analyze,relationship,model analyze relationship,0.6557666659355164
translation,190,176,results,action-independent nor actiondependent belief - based sequential recognition,has,outperform,action-independent nor actiondependent belief - based sequential recognition has outperform,0.5792294144630432
translation,190,176,results,outperform,has,reference experiment,outperform has reference experiment,0.642469584941864
translation,190,176,results,results,illustrate,action-independent nor actiondependent belief - based sequential recognition,results illustrate action-independent nor actiondependent belief - based sequential recognition,0.5598265528678894
translation,190,185,results,action-dependent,to,action- independent variant,action-dependent to action- independent variant,0.5814656615257263
translation,190,185,results,action-dependent,to,actiondependent variant,action-dependent to actiondependent variant,0.5807497501373291
translation,190,185,results,action- independent variant,of,belief - based sequential recognition,action- independent variant of belief - based sequential recognition,0.5767748951911926
translation,190,185,results,slightly better results,for,actiondependent variant,slightly better results for actiondependent variant,0.5835222601890564
translation,190,185,results,action-dependent,has,better iq performance,action-dependent has better iq performance,0.5614704489707947
translation,190,185,results,action- independent variant,has,better iq performance,action- independent variant has better iq performance,0.5671718716621399
translation,190,185,results,belief - based sequential recognition,has,better iq performance,belief - based sequential recognition has better iq performance,0.5698210000991821
translation,190,185,results,results,comparing,action-dependent,results comparing action-dependent,0.6231786012649536
translation,190,198,results,iq recognition,is,much more reliable and accurate,iq recognition is much more reliable and accurate,0.5409203171730042
translation,190,198,results,much more reliable and accurate,than,us recognition,much more reliable and accurate than us recognition,0.6013327240943909
translation,190,198,results,results,has,iq recognition,results has iq recognition,0.5641894340515137
translation,191,170,ablation-analysis,gce,simplifies,glad,gce simplifies glad,0.6787813901901245
translation,191,170,ablation-analysis,gce,simplifies,glad,gce simplifies glad,0.6787813901901245
translation,191,170,ablation-analysis,excellent performance,of,glad,excellent performance of glad,0.5915690660476685
translation,191,170,ablation-analysis,gce,has,efficiently improves,gce has efficiently improves,0.6254449486732483
translation,191,170,ablation-analysis,ablation analysis,has,gce,ablation analysis has gce,0.5389404296875
translation,191,194,ablation-analysis,slot information sharing,enhances,performance,slot information sharing enhances performance,0.610518217086792
translation,191,194,ablation-analysis,performance,by,1.04 %,performance by 1.04 %,0.5728057622909546
translation,191,194,ablation-analysis,ablation analysis,has,slot information sharing,ablation analysis has slot information sharing,0.5505849123001099
translation,191,166,baselines,baselines,compare,sas,baselines compare sas,0.7635341286659241
translation,191,159,hyperparameters,batch size,of,32,batch size of 32,0.6741614937782288
translation,191,159,hyperparameters,hyperparameters,trained with,"adam optimizer ( kingma and ba , 2014 )","hyperparameters trained with adam optimizer ( kingma and ba , 2014 )",0.7295531034469604
translation,191,160,hyperparameters,both the encoder and the decoder,use,400 hidden dimensions,both the encoder and the decoder use 400 hidden dimensions,0.6379274129867554
translation,191,160,hyperparameters,hyperparameters,has,both the encoder and the decoder,hyperparameters has both the encoder and the decoder,0.5773769021034241
translation,191,161,hyperparameters,learning rate,initially set to,0.001,learning rate initially set to 0.001,0.6653388142585754
translation,191,161,hyperparameters,learning rate,once,joint goal accuracy,learning rate once joint goal accuracy,0.6444714069366455
translation,191,161,hyperparameters,network,automatically decrease,learning rate,network automatically decrease learning rate,0.7942773103713989
translation,191,161,hyperparameters,learning rate,to improve,performance,learning rate to improve performance,0.7012313008308411
translation,191,161,hyperparameters,joint goal accuracy,has,does not rise,joint goal accuracy has does not rise,0.6113869547843933
translation,191,161,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,191,162,hyperparameters,dropout,with,0.2 dropout rate,dropout with 0.2 dropout rate,0.5649941563606262
translation,191,162,hyperparameters,0.2 dropout rate,for,regularization,0.2 dropout rate for regularization,0.5490408539772034
translation,191,162,hyperparameters,hyperparameters,apply,dropout,hyperparameters apply dropout,0.6019902229309082
translation,191,164,hyperparameters,k-means clustering algorithm,implemented with,sklearn module,k-means clustering algorithm implemented with sklearn module,0.6931730508804321
translation,191,164,hyperparameters,k-means clustering algorithm,set,all the hyperparameter,k-means clustering algorithm set all the hyperparameter,0.6568783521652222
translation,191,164,hyperparameters,all the hyperparameter,in,k-means algorithm,all the hyperparameter in k-means algorithm,0.5081891417503357
translation,191,164,hyperparameters,all the hyperparameter,as,default,all the hyperparameter as default,0.5198108553886414
translation,191,164,hyperparameters,hyperparameters,set,all the hyperparameter,hyperparameters set all the hyperparameter,0.6538718342781067
translation,191,164,hyperparameters,hyperparameters,has,k-means clustering algorithm,hyperparameters has k-means clustering algorithm,0.5434169769287109
translation,191,6,model,dialogue state tracker,with,slot attention and slot information sharing ( sas ),dialogue state tracker with slot attention and slot information sharing ( sas ),0.652857780456543
translation,191,6,model,dialogue state tracker,to reduce,redundant information 's interference,dialogue state tracker to reduce redundant information 's interference,0.6598988175392151
translation,191,6,model,dialogue state tracker,improve,long dialogue context tracking,dialogue state tracker improve long dialogue context tracking,0.6327422261238098
translation,191,6,model,model,propose,dialogue state tracker,model propose dialogue state tracker,0.6719363331794739
translation,191,32,model,sas,has,new multi-domain dialogue state tracking model,sas has new multi-domain dialogue state tracking model,0.5298164486885071
translation,191,32,model,model,propose,sas,model propose sas,0.6861782073974609
translation,191,33,model,slot attention,to localize,key features,slot attention to localize key features,0.7348766922950745
translation,191,33,model,key features,from,original information,key features from original information,0.5539029836654663
translation,191,33,model,slot infor-mation sharing,to improve,models ability,slot infor-mation sharing to improve models ability,0.7077914476394653
translation,191,33,model,models ability,to deduce,value,models ability to deduce value,0.6941659450531006
translation,191,33,model,value,from,related slots,value from related slots,0.5864737033843994
translation,191,33,model,model,use,slot attention,model use slot attention,0.6839529871940613
translation,191,169,model,glad,relies on,global modules,glad relies on global modules,0.7631717920303345
translation,191,169,model,glad,relies on,local modules,glad relies on local modules,0.7307915687561035
translation,191,169,model,global modules,to learn,general information,global modules to learn general information,0.6253284811973572
translation,191,169,model,global modules,to catch,slotspecific information,global modules to catch slotspecific information,0.6143978238105774
translation,191,169,model,local modules,to catch,slotspecific information,local modules to catch slotspecific information,0.6192589998245239
translation,191,169,model,slotspecific information,from,dialogues,slotspecific information from dialogues,0.5928640961647034
translation,191,169,model,model,has,glad,model has glad,0.6027166843414307
translation,191,9,results,our model,yields,significantly improved performance,our model yields significantly improved performance,0.7227380871772766
translation,191,9,results,significantly improved performance,compared to,previous state - of - the - art models,significantly improved performance compared to previous state - of - the - art models,0.6394386887550354
translation,191,9,results,previous state - of - the - art models,on,multi-woz dataset,previous state - of - the - art models on multi-woz dataset,0.5155171155929565
translation,191,9,results,results,has,our model,results has our model,0.5871725678443909
translation,191,35,results,multi-domain multiwoz dataset,shows,sas,multi-domain multiwoz dataset shows sas,0.696137547492981
translation,191,35,results,sas,achieve,51.03 % joint goal accuracy,sas achieve 51.03 % joint goal accuracy,0.6438629627227783
translation,191,35,results,sas,achieve,outperform,sas achieve outperform,0.745425283908844
translation,191,35,results,previous state - of- the - art model,by,2.41 %,previous state - of- the - art model by 2.41 %,0.517329752445221
translation,191,35,results,outperform,has,previous state - of- the - art model,outperform has previous state - of- the - art model,0.5598663687705994
translation,191,36,results,single domain dataset,contains,restaurant domain,single domain dataset contains restaurant domain,0.576124906539917
translation,191,36,results,single domain dataset,achieve,67.34 % joint goal accuracy,single domain dataset achieve 67.34 % joint goal accuracy,0.5923652052879333
translation,191,36,results,best,by,1.99 %,best by 1.99 %,0.5675169229507446
translation,191,36,results,outperforming,has,prior,outperforming has prior,0.6251639127731323
translation,191,36,results,prior,has,best,prior has best,0.6034232378005981
translation,191,36,results,results,On,single domain dataset,results On single domain dataset,0.5596676468849182
translation,191,180,results,our model,achieves,best performance,our model achieves best performance,0.684498131275177
translation,191,180,results,best performance,in,most important metric,best performance in most important metric,0.5048379898071289
translation,191,180,results,results,has,our model,results has our model,0.5871725678443909
translation,191,181,results,previous state - of- the - art model,TRADE by,2.41 % absolute score,previous state - of- the - art model TRADE by 2.41 % absolute score,0.6304914951324463
translation,191,181,results,2.41 % absolute score,on,joint goal accuracy,2.41 % absolute score on joint goal accuracy,0.48276159167289734
translation,191,181,results,our model,has,outperformed,our model has outperformed,0.5903543829917908
translation,191,181,results,outperformed,has,previous state - of- the - art model,outperformed has previous state - of- the - art model,0.5699883699417114
translation,191,181,results,results,has,our model,results has our model,0.5871725678443909
translation,191,182,results,slight increase,of,slot accuracy,slight increase of slot accuracy,0.6234716773033142
translation,191,182,results,slot accuracy,compared to,trade,slot accuracy compared to trade,0.6699675917625427
translation,191,182,results,results,observe,slight increase,results observe slight increase,0.6337372660636902
translation,191,186,results,sas,achieved,1.99 % improvement,sas achieved 1.99 % improvement,0.6936172842979431
translation,191,186,results,1.99 % improvement,over,trade,1.99 % improvement over trade,0.6639509201049805
translation,191,186,results,trade,on,joint goal accuracy,trade on joint goal accuracy,0.5616795420646667
translation,191,186,results,results,has,sas,results has sas,0.48594075441360474
translation,191,192,results,slot attention,improves,state tracking results,slot attention improves state tracking results,0.6778720617294312
translation,191,192,results,state tracking results,by,1.37 %,state tracking results by 1.37 %,0.5666502118110657
translation,191,192,results,1.37 %,on,multiwoz,1.37 % on multiwoz,0.5797721743583679
translation,191,192,results,results,adding,slot attention,results adding slot attention,0.6571593880653381
translation,191,204,results,performance,of,k-means sharing method,performance of k-means sharing method,0.5867908596992493
translation,191,204,results,performance,close to,one,performance close to one,0.7009287476539612
translation,191,204,results,k-means sharing method,close to,one,k-means sharing method close to one,0.6987528800964355
translation,191,204,results,k-means sharing method,close to,human constructed method,k-means sharing method close to human constructed method,0.6798468232154846
translation,191,204,results,one,has,human constructed method,one has human constructed method,0.5768370032310486
translation,191,206,results,fix combination model,has,usually outperforms,fix combination model has usually outperforms,0.619488000869751
translation,191,206,results,usually outperforms,has,human constructed method,usually outperforms has human constructed method,0.568662166595459
translation,191,206,results,results,notice,fix combination model,results notice fix combination model,0.7194724678993225
translation,192,8,ablation-analysis,multiwoz corpus,covering,range of domains and topics,multiwoz corpus covering range of domains and topics,0.7095958590507507
translation,192,8,ablation-analysis,multiwoz corpus,finds that,annotations,multiwoz corpus finds that annotations,0.6410638093948364
translation,192,8,ablation-analysis,annotations,reduced by,up to 30 %,annotations reduced by up to 30 %,0.7006075978279114
translation,192,8,ablation-analysis,ablation analysis,on,multiwoz corpus,ablation analysis on multiwoz corpus,0.5405738949775696
translation,192,87,ablation-analysis,labelling model,not producing,pseudo training points,labelling model not producing pseudo training points,0.6776511669158936
translation,192,87,ablation-analysis,pseudo training points,help improve,dst predictions,pseudo training points help improve dst predictions,0.6727772355079651
translation,192,87,ablation-analysis,scarce data levels ( 10 % and 30 % ),has,pseudo-,scarce data levels ( 10 % and 30 % ) has pseudo-,0.6096704006195068
translation,192,87,ablation-analysis,scarce data levels ( 10 % and 30 % ),has,labelling model,scarce data levels ( 10 % and 30 % ) has labelling model,0.6044945120811462
translation,192,87,ablation-analysis,pseudo-,has,labelling model,pseudo- has labelling model,0.5956701636314392
translation,192,87,ablation-analysis,ablation analysis,At,scarce data levels ( 10 % and 30 % ),ablation analysis At scarce data levels ( 10 % and 30 % ),0.5418345332145691
translation,192,98,ablation-analysis,improvement,on,few-shot slots,improvement on few-shot slots,0.605633020401001
translation,192,98,ablation-analysis,few-shot slots,improvement of,joint accuracy,few-shot slots improvement of joint accuracy,0.7042487263679504
translation,192,98,ablation-analysis,ablation analysis,has,improvement,ablation analysis has improvement,0.5466791987419128
translation,192,20,baselines,improved training signal,to,dialogue state tracking component,improved training signal to dialogue state tracking component,0.5459024906158447
translation,192,20,baselines,dialogue state tracking component,in,end-to - end dialogue system,dialogue state tracking component in end-to - end dialogue system,0.5235181450843811
translation,192,19,model,reliance,of,taskoriented dialogue systems,reliance of taskoriented dialogue systems,0.5810511112213135
translation,192,19,model,reliance,on,data collection,reliance on data collection,0.49065515398979187
translation,192,19,model,reliance,leveraging,semi-supervised training,reliance leveraging semi-supervised training,0.7442576885223389
translation,192,19,model,taskoriented dialogue systems,on,data collection,taskoriented dialogue systems on data collection,0.5097580552101135
translation,192,19,model,taskoriented dialogue systems,leveraging,semi-supervised training,taskoriented dialogue systems leveraging semi-supervised training,0.6709917783737183
translation,192,19,model,model,reduce,reliance,model reduce reliance,0.7436922788619995
translation,192,88,model,unlabelled data,to enhance,performance,unlabelled data to enhance performance,0.6825196743011475
translation,192,88,model,performance,over,baseline,performance over baseline,0.6994789242744446
translation,192,7,results,un-annotated data,amount of,turn- level annotations,un-annotated data amount of turn- level annotations,0.7271349430084229
translation,192,7,results,turn- level annotations,of,dialogue state,turn- level annotations of dialogue state,0.5670665502548218
translation,192,7,results,turn- level annotations,can be,significantly reduced,turn- level annotations can be significantly reduced,0.664086639881134
translation,192,7,results,significantly reduced,when building,neural dialogue system,significantly reduced when building neural dialogue system,0.6967189908027649
translation,192,7,results,results,by leveraging,un-annotated data,results by leveraging un-annotated data,0.6753462553024292
translation,192,86,results,labelling model,performs,better,labelling model performs better,0.6467083692550659
translation,192,86,results,better,than,baseline,better than baseline,0.6157954335212708
translation,192,86,results,better,when,more than 50 %,better when more than 50 %,0.6672924160957336
translation,192,86,results,baseline,when,more than 50 %,baseline when more than 50 %,0.6495678424835205
translation,192,86,results,more than 50 %,is,labelled,more than 50 % is labelled,0.6590686440467834
translation,192,86,results,more than 50 %,has,of the dataset,more than 50 % has of the dataset,0.5918956398963928
translation,192,86,results,results,pseudo-,labelling model,results pseudo- labelling model,0.7179878354072571
translation,192,86,results,results,has,labelling model,results has labelling model,0.5710693001747131
translation,192,89,results,improvements,are,consistently more than 5 %,improvements are consistently more than 5 %,0.5879313349723816
translation,192,89,results,improvements,reach,performance,improvements reach performance,0.8190574049949646
translation,192,89,results,consistently more than 5 %,training with,30 to 90 %,consistently more than 5 % training with 30 to 90 %,0.7800226807594299
translation,192,89,results,30 to 90 %,of,labelled data,30 to 90 % of labelled data,0.6250214576721191
translation,192,89,results,performance,of,fully trained baseline model,performance of fully trained baseline model,0.532105565071106
translation,192,89,results,fully trained baseline model,with,only 70 % labelled data,fully trained baseline model with only 70 % labelled data,0.5738309621810913
translation,192,89,results,results,has,improvements,results has improvements,0.615561842918396
translation,192,91,results,pseudo-,not able to improve,performance,pseudo- not able to improve performance,0.7061598896980286
translation,192,91,results,labelling method,not able to improve,performance,labelling method not able to improve performance,0.6705062389373779
translation,192,91,results,performance,over,baseline,performance over baseline,0.6994789242744446
translation,192,91,results,pseudo-,has,labelling method,pseudo- has labelling method,0.5842923521995544
translation,192,91,results,results,has,pseudo-,results has pseudo-,0.52322918176651
translation,192,91,results,results,has,labelling method,results has labelling method,0.550545334815979
translation,192,92,results,?-model,capable of improving,success rate,?-model capable of improving success rate,0.7372329235076904
translation,192,92,results,?-model,manages to reach,performance,?-model manages to reach performance,0.7031939029693604
translation,192,92,results,performance,of,fully trained model,performance of fully trained model,0.5694246888160706
translation,192,92,results,fully trained model,with,only 50 %,fully trained model with only 50 %,0.6121645569801331
translation,192,92,results,only 50 %,of,intermediate dst signal,only 50 % of intermediate dst signal,0.5983425974845886
translation,192,92,results,results,has,?-model,results has ?-model,0.6019361615180969
translation,192,97,results,accuracy,by,5 %,accuracy by 5 %,0.6282424926757812
translation,192,97,results,accuracy,when,slot-value pair,accuracy when slot-value pair,0.6724593043327332
translation,192,97,results,5 %,when,slot-value pair,5 % when slot-value pair,0.7146144509315491
translation,192,97,results,slot-value pair,rarely ( 1 - 10 times ) seen during,training,slot-value pair rarely ( 1 - 10 times ) seen during training,0.6854251623153687
translation,192,102,results,performance,over,baseline,performance over baseline,0.6994789242744446
translation,192,102,results,baseline,in,all domains,baseline in all domains,0.4771929383277893
translation,192,102,results,all domains,except for,taxi domain,all domains except for taxi domain,0.6414659023284912
translation,192,102,results,both semi-supervised models,has,improve,both semi-supervised models has improve,0.5858681201934814
translation,192,102,results,improve,has,performance,improve has performance,0.5578044652938843
translation,192,102,results,results,has,both semi-supervised models,results has both semi-supervised models,0.5004952549934387
translation,193,137,ablation-analysis,dah -crf + lda utt,removes,dual-attention component,dah -crf + lda utt removes dual-attention component,0.6230076551437378
translation,193,137,ablation-analysis,dual-attention component,from,dah - crf + lda utt,dual-attention component from dah - crf + lda utt,0.574982762336731
translation,193,137,ablation-analysis,dual-attention component,from,dah -crf + lda utt,dual-attention component from dah -crf + lda utt,0.574982762336731
translation,193,137,ablation-analysis,dual-attention component,from,dah -crf + lda utt,dual-attention component from dah -crf + lda utt,0.574982762336731
translation,193,137,ablation-analysis,dah + lda utt,removes,crf,dah + lda utt removes crf,0.6667331457138062
translation,193,137,ablation-analysis,dah + lda utt,retaining,dual-attention component,dah + lda utt retaining dual-attention component,0.7006222009658813
translation,193,137,ablation-analysis,crf,from,dah -crf + lda utt,crf from dah -crf + lda utt,0.5642254948616028
translation,193,137,ablation-analysis,crf,retaining,dual-attention component,crf retaining dual-attention component,0.7561938762664795
translation,193,137,ablation-analysis,ablation analysis,has,dah -crf + lda utt,ablation analysis has dah -crf + lda utt,0.5691472887992859
translation,193,140,ablation-analysis,dual-attention mechanism and the crf component,are,beneficial,dual-attention mechanism and the crf component are beneficial,0.4924261271953583
translation,193,140,ablation-analysis,dual-attention mechanism and the crf component,are,more effective,dual-attention mechanism and the crf component are more effective,0.5121228098869324
translation,193,140,ablation-analysis,more effective,on,swda and dyda datasets,more effective on swda and dyda datasets,0.5123996734619141
translation,193,140,ablation-analysis,swda and dyda datasets,than,mrda,swda and dyda datasets than mrda,0.5575975775718689
translation,193,140,ablation-analysis,ablation analysis,observed that,dual-attention mechanism and the crf component,ablation analysis observed that dual-attention mechanism and the crf component,0.6328648328781128
translation,193,140,ablation-analysis,ablation analysis,both,dual-attention mechanism and the crf component,ablation analysis both dual-attention mechanism and the crf component,0.6441220641136169
translation,193,141,ablation-analysis,biggest gain,obtained by,jointly modelling,biggest gain obtained by jointly modelling,0.6543887853622437
translation,193,141,ablation-analysis,jointly modelling,has,das and topics,jointly modelling has das and topics,0.5808021426200867
translation,193,125,baselines,proposed dah - crf model,incorporating,utterance - level topic labels,proposed dah - crf model incorporating utterance - level topic labels,0.6385875940322876
translation,193,125,baselines,utterance - level topic labels,extracted by,lda,utterance - level topic labels extracted by lda,0.6572250127792358
translation,193,125,baselines,"generative joint , additive , sequential model",of,topics and speech acts,"generative joint , additive , sequential model of topics and speech acts",0.5551953911781311
translation,193,125,baselines,topics and speech acts,in,patient- doctor communication,topics and speech acts in patient- doctor communication,0.5379465818405151
translation,193,125,baselines,latent variable recurrent neural network,for,da classification,latent variable recurrent neural network for da classification,0.5963357090950012
translation,193,125,baselines,hierarchical bi-lstm,with,crf,hierarchical bi-lstm with crf,0.6594632267951965
translation,193,125,baselines,crf,to classify,das,crf to classify das,0.7197175621986389
translation,193,125,baselines,jas,has,"generative joint , additive , sequential model","jas has generative joint , additive , sequential model",0.5533168315887451
translation,193,125,baselines,drlm - cond,has,latent variable recurrent neural network,drlm - cond has latent variable recurrent neural network,0.5892318487167358
translation,193,125,baselines,bi-lstm -crf,has,hierarchical bi-lstm,bi-lstm -crf has hierarchical bi-lstm,0.605859100818634
translation,193,125,baselines,baselines,compare,proposed dah - crf model,baselines compare proposed dah - crf model,0.6561185717582703
translation,193,97,hyperparameters,each conversation,as,document,each conversation as document,0.5851069092750549
translation,193,97,hyperparameters,topic models,using,gensim,topic models using gensim,0.6383009552955627
translation,193,97,hyperparameters,gensim,with,topic number settings,gensim with topic number settings,0.6251479387283325
translation,193,97,hyperparameters,topic number settings,ranging from,10 to 100,topic number settings ranging from 10 to 100,0.6785078048706055
translation,193,97,hyperparameters,hyperparameters,treat,each conversation,hyperparameters treat each conversation,0.5787757039070129
translation,193,98,hyperparameters,gibbs sampling,to estimate,model posterior,gibbs sampling to estimate model posterior,0.7041928172111511
translation,193,98,hyperparameters,hyperparameters,has,gibbs sampling,hyperparameters has gibbs sampling,0.524420440196991
translation,193,120,hyperparameters,input data,with,300 - dimensional glove word embeddings,input data with 300 - dimensional glove word embeddings,0.5861907005310059
translation,193,120,hyperparameters,input data,with,50 - dimensional character embeddings,input data with 50 - dimensional character embeddings,0.5841922163963318
translation,193,120,hyperparameters,50 - dimensional character embeddings,has,"ma and hovy , 2016 )","50 - dimensional character embeddings has ma and hovy , 2016 )",0.5558133125305176
translation,193,120,hyperparameters,hyperparameters,represented,input data,hyperparameters represented input data,0.591947078704834
translation,193,121,hyperparameters,dimension,of,"hidlayers ( i.e. , h i t , g t and s t )","dimension of hidlayers ( i.e. , h i t , g t and s t )",0.6137786507606506
translation,193,121,hyperparameters,"hidlayers ( i.e. , h i t , g t and s t )",to,256,"hidlayers ( i.e. , h i t , g t and s t ) to 256",0.5965131521224976
translation,193,121,hyperparameters,dropout layer,to,shared encoder and the sequence tagger,dropout layer to shared encoder and the sequence tagger,0.559898853302002
translation,193,121,hyperparameters,dropout layer,both,shared encoder and the sequence tagger,dropout layer both shared encoder and the sequence tagger,0.6435961723327637
translation,193,121,hyperparameters,dropout layer,at,rate,dropout layer at rate,0.5365698337554932
translation,193,121,hyperparameters,rate,of,0.2,rate of 0.2,0.6393477320671082
translation,193,121,hyperparameters,hyperparameters,set,dimension,hyperparameters set dimension,0.6708630919456482
translation,193,122,hyperparameters,adam optimiser,used for,training,adam optimiser used for training,0.6436636447906494
translation,193,122,hyperparameters,training,with,initial learning rate,training with initial learning rate,0.6178848743438721
translation,193,122,hyperparameters,training,with,weight decay,training with weight decay,0.6287098526954651
translation,193,122,hyperparameters,initial learning rate,of,0.001,initial learning rate of 0.001,0.5703312754631042
translation,193,122,hyperparameters,weight decay,of,0.0001,weight decay of 0.0001,0.5422717332839966
translation,193,122,hyperparameters,hyperparameters,has,adam optimiser,hyperparameters has adam optimiser,0.5281989574432373
translation,193,123,hyperparameters,each utterance,in,minibatch,each utterance in minibatch,0.5196709632873535
translation,193,123,hyperparameters,maximum length,for,batch,maximum length for batch,0.6598762273788452
translation,193,123,hyperparameters,maximum batch - size allowed,was,50,maximum batch - size allowed was 50,0.6539552807807922
translation,193,123,hyperparameters,hyperparameters,has,each utterance,hyperparameters has each utterance,0.5407543778419495
translation,193,123,hyperparameters,hyperparameters,has,maximum batch - size allowed,hyperparameters has maximum batch - size allowed,0.49154478311538696
translation,193,5,model,dualattention hierarchical recurrent neural network,for,da classification,dualattention hierarchical recurrent neural network for da classification,0.5933343172073364
translation,193,5,model,model,propose,dualattention hierarchical recurrent neural network,model propose dualattention hierarchical recurrent neural network,0.6583859324455261
translation,193,25,model,dual-attention hierarchical recurrent neural network,with,crf ( dah - crf ),dual-attention hierarchical recurrent neural network with crf ( dah - crf ),0.6668264269828796
translation,193,25,model,crf ( dah - crf ),for,da classification,crf ( dah - crf ) for da classification,0.6148704886436462
translation,193,25,model,model,propose,dual-attention hierarchical recurrent neural network,model propose dual-attention hierarchical recurrent neural network,0.6418821811676025
translation,193,26,model,rich context information,with,developed dual-attention mechanism,rich context information with developed dual-attention mechanism,0.6153340935707092
translation,193,26,model,rich context information,capture,information,rich context information capture information,0.7859296202659607
translation,193,26,model,information,about,topics and das,information about topics and das,0.6776621341705322
translation,193,26,model,model,account for,rich context information,model account for rich context information,0.7179788947105408
translation,193,28,model,hierarchical recurrent neural network,represent,input,hierarchical recurrent neural network represent input,0.6466061472892761
translation,193,28,model,input,at,"character , word , utterance , and conversation levels","input at character , word , utterance , and conversation levels",0.5434319376945496
translation,193,28,model,input,preserving,natural hierarchical structure,input preserving natural hierarchical structure,0.75997394323349
translation,193,28,model,natural hierarchical structure,of,conversation,natural hierarchical structure of conversation,0.6136775612831116
translation,193,29,model,topic information,of,conversations,topic information of conversations,0.5746234655380249
translation,193,29,model,topic information,propose,simple automatic utterance - level topic labelling mechanism,topic information propose simple automatic utterance - level topic labelling mechanism,0.6443178057670593
translation,193,29,model,simple automatic utterance - level topic labelling mechanism,based on,"lda ( blei et al. , 2003 )","simple automatic utterance - level topic labelling mechanism based on lda ( blei et al. , 2003 )",0.5786888003349304
translation,193,29,model,model,To capture,topic information,model To capture topic information,0.7071239948272705
translation,193,33,model,topic information,of,utterances,topic information of utterances,0.6024008989334106
translation,193,33,model,dual-attention hierarchical recurrent neural network,with,crf,dual-attention hierarchical recurrent neural network with crf,0.6582816243171692
translation,193,33,model,crf,respects,natural hierarchical structure,crf respects natural hierarchical structure,0.6220279335975647
translation,193,33,model,natural hierarchical structure,of,conversation,natural hierarchical structure of conversation,0.6136775612831116
translation,193,33,model,rich context information,for,da classification,rich context information for da classification,0.5687212347984314
translation,193,33,model,rich context information,achieving,better or comparable performance,rich context information achieving better or comparable performance,0.6456888914108276
translation,193,33,model,better or comparable performance,develop,simple topic labelling mechanism,better or comparable performance develop simple topic labelling mechanism,0.5831305384635925
translation,193,33,model,model,leverage,topic information,model leverage topic information,0.7764108777046204
translation,193,33,model,model,propose,dual-attention hierarchical recurrent neural network,model propose dual-attention hierarchical recurrent neural network,0.6418821811676025
translation,193,33,model,model,develop,simple topic labelling mechanism,model develop simple topic labelling mechanism,0.5988833904266357
translation,193,131,results,crf - asn,around,1 %,crf - asn around 1 %,0.72200608253479
translation,193,131,results,1 %,on,mrda dataset,1 % on mrda dataset,0.5559540390968323
translation,193,131,results,da and topics,has,dah - crf + lda utt,da and topics has dah - crf + lda utt,0.6178383231163025
translation,193,131,results,dah - crf + lda utt,has,outperforms,dah - crf + lda utt has outperforms,0.6047938466072083
translation,193,131,results,outperforms,has,two best baseline models selfatt-crf,outperforms has two best baseline models selfatt-crf,0.5438231825828552
translation,193,131,results,results,jointly modelling,da and topics,results jointly modelling da and topics,0.6485881805419922
translation,193,132,results,our model,gives,similar performance,our model gives similar performance,0.624527633190155
translation,193,132,results,similar performance,to,selfatt - crf,similar performance to selfatt - crf,0.5669338703155518
translation,193,132,results,results,has,our model,results has our model,0.5871725678443909
translation,193,133,results,manually annotated and automatically acquired topic labels,are,effective,manually annotated and automatically acquired topic labels are effective,0.5524520874023438
translation,193,133,results,effective,see that,dah - crf + lda utt,effective see that dah - crf + lda utt,0.6261880397796631
translation,193,133,results,outperforms,with,over 1.6 % gain,outperforms with over 1.6 % gain,0.69871586561203
translation,193,133,results,outperforms,with,over 1.4 %,outperforms with over 1.4 %,0.6898015141487122
translation,193,133,results,dah -crf + manual conv and dah -crf + lda conv,with,over 1.6 % gain,dah -crf + manual conv and dah -crf + lda conv with over 1.6 % gain,0.6179854273796082
translation,193,133,results,dah -crf + manual conv and dah -crf + lda conv,with,over 1.4 %,dah -crf + manual conv and dah -crf + lda conv with over 1.4 %,0.6131487488746643
translation,193,133,results,over 1.6 % gain,on,dyda,over 1.6 % gain on dyda,0.5948618650436401
translation,193,133,results,over 1.6 % gain,on,swda,over 1.6 % gain on swda,0.581768274307251
translation,193,133,results,over 1.4 %,on,swda,over 1.4 % on swda,0.5998391509056091
translation,193,133,results,dah - crf + lda utt,has,outperforms,dah - crf + lda utt has outperforms,0.6047938466072083
translation,193,133,results,outperforms,has,dah -crf + manual conv and dah -crf + lda conv,outperforms has dah -crf + manual conv and dah -crf + lda conv,0.5885695815086365
translation,193,134,results,dah -crf + manual conv,perform,very similar,dah -crf + manual conv perform very similar,0.5691268444061279
translation,193,134,results,dah -crf + lda conv,perform,very similar,dah -crf + lda conv perform very similar,0.5742502212524414
translation,193,139,results,dah + lda utt,achieves,over 3 % averaged gain,dah + lda utt achieves over 3 % averaged gain,0.727281928062439
translation,193,139,results,over 3 % averaged gain,on,all datasets,over 3 % averaged gain on all datasets,0.5349830985069275
translation,193,139,results,over 3 % averaged gain,compared to,sah,over 3 % averaged gain compared to sah,0.6978919506072998
translation,193,146,results,dah -crf + lda utt,yields,improvement,dah -crf + lda utt yields improvement,0.6900132894515991
translation,193,146,results,improvement,on,recall,improvement on recall,0.5474287867546082
translation,193,146,results,recall,for,many da classes,recall for many da classes,0.6927399635314941
translation,193,146,results,many da classes,compared to,sah - crf,many da classes compared to sah - crf,0.6438839435577393
translation,193,146,results,results,observed that,dah -crf + lda utt,results observed that dah -crf + lda utt,0.6492624878883362
translation,193,151,results,dah - crf + lda utt,better handle,confusion cases,dah - crf + lda utt better handle confusion cases,0.6905973553657532
translation,193,151,results,dah - crf + lda utt,improve,prediction,dah - crf + lda utt improve prediction,0.6946180462837219
translation,193,151,results,prediction,for,bk,prediction for bk,0.6936109066009521
translation,193,151,results,topic information,has,dah - crf + lda utt,topic information has dah - crf + lda utt,0.596827507019043
translation,193,151,results,bk,has,significantly,bk has significantly,0.6664747595787048
translation,193,151,results,results,leveraging,topic information,results leveraging topic information,0.6308205127716064
translation,193,156,results,sah - crf,gives,quite decent performance,sah - crf gives quite decent performance,0.5803430676460266
translation,193,156,results,quite decent performance,in distinguishing,qw and qo classes,quite decent performance in distinguishing qw and qo classes,0.6973748803138733
translation,193,156,results,results,has,sah - crf,results has sah - crf,0.5305931568145752
translation,194,32,model,beetle ii,uses,deep parser,beetle ii uses deep parser,0.5680161714553833
translation,194,32,model,beetle ii,uses,deep generator,beetle ii uses deep generator,0.6291678547859192
translation,194,32,model,deep parser,together with,domain-specific diagnoser,deep parser together with domain-specific diagnoser,0.6151861548423767
translation,194,32,model,deep parser,together with,deep generator,deep parser together with deep generator,0.599864661693573
translation,194,32,model,domain-specific diagnoser,to process,student input,domain-specific diagnoser to process student input,0.7388660311698914
translation,194,32,model,deep generator,to produce,tutorial feedback,deep generator to produce tutorial feedback,0.6938907504081726
translation,194,32,model,tutorial feedback,depending on,current tutorial policy,tutorial feedback depending on current tutorial policy,0.7329583168029785
translation,194,32,model,automatically,depending on,current tutorial policy,automatically depending on current tutorial policy,0.7201863527297974
translation,194,32,model,tutorial feedback,has,automatically,tutorial feedback has automatically,0.5792597532272339
translation,194,32,model,model,has,beetle ii,model has beetle ii,0.5798324942588806
translation,194,71,results,average tutor evaluation score,for,full,average tutor evaluation score for full,0.6039854288101196
translation,194,71,results,full,was,2.56 out of 5 ( sd = 0.65 ),full was 2.56 out of 5 ( sd = 0.65 ),0.5793567299842834
translation,194,71,results,3.32 ( sd = 0.65 ),in,base,3.32 ( sd = 0.65 ) in base,0.4638512134552002
translation,194,71,results,students liked base,has,better,students liked base has better,0.6108032464981079
translation,194,71,results,students liked base,has,average tutor evaluation score,students liked base has average tutor evaluation score,0.5723734498023987
translation,194,71,results,better,has,average tutor evaluation score,better has average tutor evaluation score,0.5360600352287292
translation,194,71,results,results,has,students liked base,results has students liked base,0.568771243095398
translation,195,37,ablation-analysis,open-ended user utterances,characterize,chat detection,open-ended user utterances characterize chat detection,0.6397235989570618
translation,195,37,ablation-analysis,chat detection,by using,unlabeled external resources,chat detection by using unlabeled external resources,0.5935742855072021
translation,195,37,ablation-analysis,ablation analysis,address,open-ended user utterances,ablation analysis address open-ended user utterances,0.5314744710922241
translation,195,196,ablation-analysis,external resources,are,effective,external resources are effective,0.5504825115203857
translation,195,196,ablation-analysis,almost 1 points,in,svm and cnn,almost 1 points in svm and cnn,0.5484477877616882
translation,195,196,ablation-analysis,ablation analysis,represents,external resources,ablation analysis represents external resources,0.5429807305335999
translation,195,133,hyperparameters,rectified linear unit ( relu ),as,non-linear activation function,rectified linear unit ( relu ) as non-linear activation function,0.544946014881134
translation,195,133,hyperparameters,hyperparameters,use,rectified linear unit ( relu ),hyperparameters use rectified linear unit ( relu ),0.604617178440094
translation,195,156,hyperparameters,"80 % , 10 % , and 10 %",of,data,"80 % , 10 % , and 10 % of data",0.6384722590446472
translation,195,156,hyperparameters,data,for,"training , development , and evaluation","data for training , development , and evaluation",0.5822759866714478
translation,195,157,hyperparameters,word2vec,to learn,300 dimensional word embeddings,word2vec to learn 300 dimensional word embeddings,0.5861098170280457
translation,195,157,hyperparameters,hyperparameters,used,word2vec,hyperparameters used word2vec,0.5635432004928589
translation,195,158,hyperparameters,additional 300 features,for,svm,additional 300 features for svm,0.6477319002151489
translation,195,158,hyperparameters,hyperparameters,to induce,additional 300 features,hyperparameters to induce additional 300 features,0.6110115647315979
translation,195,159,hyperparameters,pre-trained word embeddings,for,cnn,pre-trained word embeddings for cnn,0.5574199557304382
translation,195,159,hyperparameters,hyperparameters,used as,pre-trained word embeddings,hyperparameters used as pre-trained word embeddings,0.4755277633666992
translation,195,160,hyperparameters,faster-rnn toolkit,to train,gru language models,faster-rnn toolkit to train gru language models,0.6606104969978333
translation,195,160,hyperparameters,hyperparameters,used,faster-rnn toolkit,hyperparameters used faster-rnn toolkit,0.5804676413536072
translation,195,161,hyperparameters,size,of,embedding and hidden layer,size of embedding and hidden layer,0.5791632533073425
translation,195,161,hyperparameters,embedding and hidden layer,set to,256,embedding and hidden layer set to 256,0.7185221314430237
translation,195,161,hyperparameters,hyperparameters,has,size,hyperparameters has size,0.5313372611999512
translation,195,162,hyperparameters,noise contrastive estimation,train,soft-max function,noise contrastive estimation train soft-max function,0.6343660354614258
translation,195,162,hyperparameters,number of noise samples,set to,50,number of noise samples set to 50,0.7199922800064087
translation,195,162,hyperparameters,hyperparameters,has,noise contrastive estimation,hyperparameters has noise contrastive estimation,0.47525662183761597
translation,195,171,hyperparameters,number of feature maps,over,"{ 100 , 150 }","number of feature maps over { 100 , 150 }",0.6550920009613037
translation,195,171,hyperparameters,filter region sizes,over,"{ { 2 } , { 3 } , { 1 , 2 } , { 2 , 3 } , { 3 , 4 } , { 1 , 2 , 3 } , { 2 , 3 , 4 }}.","filter region sizes over { { 2 } , { 3 } , { 1 , 2 } , { 2 , 3 } , { 3 , 4 } , { 1 , 2 , 3 } , { 2 , 3 , 4 }}.",0.6156280040740967
translation,195,171,hyperparameters,filter region sizes,over,mini-batch size,filter region sizes over mini-batch size,0.6540764570236206
translation,195,171,hyperparameters,filter region sizes,over,32,filter region sizes over 32,0.6761127710342407
translation,195,171,hyperparameters,mini-batch size,set to,32,mini-batch size set to 32,0.7183046340942383
translation,195,171,hyperparameters,hyperparameters,tuned,number of feature maps,hyperparameters tuned number of feature maps,0.6954686045646667
translation,195,171,hyperparameters,hyperparameters,tuned,filter region sizes,hyperparameters tuned filter region sizes,0.7027567028999329
translation,195,172,hyperparameters,dropout rate,set to,0.5,dropout rate set to 0.5,0.6910414695739746
translation,195,172,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,195,173,hyperparameters,"adam ( ? = 0.001 , ?1 = 0.9 , ?2 = 0.999 , and ? = 10 ?8 )",to perform,stochastic gradient descent,"adam ( ? = 0.001 , ?1 = 0.9 , ?2 = 0.999 , and ? = 10 ?8 ) to perform stochastic gradient descent",0.6184244751930237
translation,195,36,model,supervised binary classifiers,to perform,chat detection,supervised binary classifiers to perform chat detection,0.657272458076477
translation,195,36,model,model,develop,supervised binary classifiers,model develop supervised binary classifiers,0.6124153137207031
translation,195,132,model,simple cnn,has,single convolution and max-pooling layer,simple cnn has single convolution and max-pooling layer,0.525671124458313
translation,195,132,model,single convolution and max-pooling layer,followed by,soft-max layer,single convolution and max-pooling layer followed by soft-max layer,0.6492932438850403
translation,195,132,model,model,to develop,simple cnn,model to develop simple cnn,0.5987302660942078
translation,195,187,results,both of the classifiers,perform,accurately,both of the classifiers perform accurately,0.6058155298233032
translation,195,187,results,results,represents,both of the classifiers,results represents both of the classifiers,0.5865916609764099
translation,195,191,results,best performing method,achieves,92 % accuracy,best performing method achieves 92 % accuracy,0.6753016710281372
translation,195,191,results,best performing method,achieves,87 % f 1 - score,best performing method achieves 87 % f 1 - score,0.6439002752304077
translation,195,191,results,best performing method,has,svm + embed .+ tweet -query,best performing method has svm + embed .+ tweet -query,0.5752042531967163
translation,195,191,results,outperforming,has,all of the baselines,outperforming has all of the baselines,0.5781440138816833
translation,195,191,results,results,has,best performing method,results has best performing method,0.5347415208816528
translation,195,199,results,chat detection results,when,each of the three features,chat detection results when each of the three features,0.5842695236206055
translation,195,199,results,each of the three features,derived from,external resources,each of the three features derived from external resources,0.633432149887085
translation,195,199,results,each of the three features,added to,svm + embed,each of the three features added to svm + embed,0.6990048289299011
translation,195,199,results,results,shows,chat detection results,results shows chat detection results,0.7055510878562927
translation,195,205,results,all metrics,get,higher,all metrics get higher,0.6047046780586243
translation,195,205,results,higher,as,number of agreement,higher as number of agreement,0.5443571209907532
translation,195,205,results,number of agreement,among,crowd workers,number of agreement among crowd workers,0.600003719329834
translation,195,205,results,number of agreement,becomes,larger,number of agreement becomes larger,0.6608500480651855
translation,195,205,results,results,see that,all metrics,results see that all metrics,0.5759257078170776
translation,196,174,baselines,cnns-bi-rnns-crf,Compared with,bi-rnns - crf,cnns-bi-rnns-crf Compared with bi-rnns - crf,0.6528826951980591
translation,196,174,baselines,character level information,with,cnn,character level information with cnn,0.6448686122894287
translation,196,174,baselines,cnn,for,encoder,cnn for encoder,0.5969973206520081
translation,196,174,baselines,cnns-bi-rnns-crf,has,cnns-bi-rnns,cnns-bi-rnns-crf has cnns-bi-rnns,0.5870652198791504
translation,196,174,baselines,bi-rnns - crf,has,cnns-bi-rnns,bi-rnns - crf has cnns-bi-rnns,0.6071733236312866
translation,196,174,baselines,baselines,has,cnns-bi-rnns-crf,baselines has cnns-bi-rnns-crf,0.550385057926178
translation,196,176,baselines,bi-lstm - crf model,incorporates,corpus-level features,bi-lstm - crf model incorporates corpus-level features,0.6695502996444702
translation,196,176,baselines,corpus-level features,via,our corpus-level attention,corpus-level features via our corpus-level attention,0.65520179271698
translation,196,145,hyperparameters,200 - dimensional chinese embeddings,trained on,wikipedia,200 - dimensional chinese embeddings trained on wikipedia,0.6618773937225342
translation,196,145,hyperparameters,200 - dimensional chinese embeddings,during,model training,200 - dimensional chinese embeddings during model training,0.5436214208602905
translation,196,145,hyperparameters,model training,by back - propagating,gradients,model training by back - propagating gradients,0.7514870762825012
translation,196,145,hyperparameters,hyperparameters,use,200 - dimensional chinese embeddings,hyperparameters use 200 - dimensional chinese embeddings,0.5611485242843628
translation,196,146,hyperparameters,parameters,of,weight matrix,parameters of weight matrix,0.5570167899131775
translation,196,146,hyperparameters,parameters,initialized by,xavier method,parameters initialized by xavier method,0.6834689378738403
translation,196,146,hyperparameters,weight matrix,initialized by,xavier method,weight matrix initialized by xavier method,0.6972260475158691
translation,196,146,hyperparameters,hyperparameters,has,parameters,hyperparameters has parameters,0.4783959984779358
translation,196,147,hyperparameters,stochastic gradient descent ( sgd ) method,with,momentum,stochastic gradient descent ( sgd ) method with momentum,0.6353885531425476
translation,196,147,hyperparameters,momentum,of,0.9,momentum of 0.9,0.6296281814575195
translation,196,147,hyperparameters,momentum,used for,optimization,momentum used for optimization,0.6708073019981384
translation,196,147,hyperparameters,hyperparameters,has,stochastic gradient descent ( sgd ) method,hyperparameters has stochastic gradient descent ( sgd ) method,0.5500671863555908
translation,196,148,hyperparameters,gradually decreases,with,increasing epoch,gradually decreases with increasing epoch,0.707995593547821
translation,196,148,hyperparameters,initial learning rate,has,? 0,initial learning rate has ? 0,0.569202184677124
translation,196,148,hyperparameters,initial learning rate,has,learning rate,initial learning rate has learning rate,0.539470374584198
translation,196,148,hyperparameters,learning rate,has,gradually decreases,learning rate has gradually decreases,0.5766164064407349
translation,196,148,hyperparameters,hyperparameters,has,initial learning rate,hyperparameters has initial learning rate,0.4469831883907318
translation,196,148,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,196,150,hyperparameters,gradient clipping,set to,5,gradient clipping set to 5,0.6119866371154785
translation,196,150,hyperparameters,5,to avoid,gradient exploding,5 to avoid gradient exploding,0.6500341296195984
translation,196,150,hyperparameters,hyperparameters,has,gradient clipping,hyperparameters has gradient clipping,0.5166950225830078
translation,196,151,hyperparameters,dropout rate,is,0.5,dropout rate is 0.5,0.5554119348526001
translation,196,151,hyperparameters,bi-lstm hidden dimension,is,200,bi-lstm hidden dimension is 200,0.5749582648277283
translation,196,35,model,approach,embeds,global attention and symptom graph,approach embeds global attention and symptom graph,0.698569655418396
translation,196,35,model,global attention and symptom graph,to improve,performance,global attention and symptom graph to improve performance,0.6319918036460876
translation,196,35,model,performance,of,dialogue symptom diagnosis,performance of dialogue symptom diagnosis,0.5827845931053162
translation,196,36,model,global attention,incorporating,more related information,global attention incorporating more related information,0.6858383417129517
translation,196,36,model,more related information,from,whole dialogue and corpus,more related information from whole dialogue and corpus,0.5926172137260437
translation,196,36,model,whole dialogue and corpus,for,better,whole dialogue and corpus for better,0.6351789236068726
translation,196,36,model,symptom entity representation,used for,symptom recognition and inference,symptom entity representation used for symptom recognition and inference,0.6533803939819336
translation,196,36,model,better,has,symptom entity representation,better has symptom entity representation,0.543961763381958
translation,196,36,model,model,has,global attention,model has global attention,0.5330514311790466
translation,196,80,model,word sequence,by,bi-lstm,word sequence by bi-lstm,0.5528907179832458
translation,196,80,model,model,encode,word sequence,model encode word sequence,0.7703935503959656
translation,196,173,model,rnns,for,sentence encoder,rnns for sentence encoder,0.583675742149353
translation,196,173,model,crf layer,for,decoder,crf layer for decoder,0.5976055264472961
translation,196,173,model,crf layer,yields,tagging prediction,crf layer yields tagging prediction,0.6357294321060181
translation,196,173,model,tagging prediction,for,each token,tagging prediction for each token,0.6036586761474609
translation,196,173,model,model,use,rnns,model use rnns,0.6377373933792114
translation,196,173,model,model,use,crf layer,model use crf layer,0.6195247769355774
translation,196,210,model,global attention mechanism,consists of,document- level 5041,global attention mechanism consists of document- level 5041,0.6528867483139038
translation,196,210,model,global attention mechanism,consists of,corpus-level attentions,global attention mechanism consists of corpus-level attentions,0.6561145782470703
translation,196,210,model,corpus-level attentions,select,supporting sentences,corpus-level attentions select supporting sentences,0.637129008769989
translation,196,210,model,supporting sentences,from,current dialogue and corpus,supporting sentences from current dialogue and corpus,0.5649367570877075
translation,196,210,model,model,has,global attention mechanism,model has global attention mechanism,0.5221193432807922
translation,196,165,results,accuracy,of,symptom normalization,accuracy of symptom normalization,0.5639973282814026
translation,196,165,results,symptom normalization,on,test set,symptom normalization on test set,0.5653955340385437
translation,196,165,results,symptom normalization,is,97.04 %,symptom normalization is 97.04 %,0.5114873051643372
translation,196,165,results,test set,is,97.04 %,test set is 97.04 %,0.5598159432411194
translation,196,165,results,results,has,accuracy,results has accuracy,0.5888755321502686
translation,196,182,results,bi-rnns models,including,bi-gru and bi-lstm,bi-rnns models including bi-gru and bi-lstm,0.6970067620277405
translation,196,182,results,bi-gru and bi-lstm,have,similar performance,bi-gru and bi-lstm have similar performance,0.5680873394012451
translation,196,182,results,results,observe,bi-rnns models,results observe bi-rnns models,0.5560523867607117
translation,196,183,results,bi-rnns - crf models,perform,much better,bi-rnns - crf models perform much better,0.5605877637863159
translation,196,183,results,much better,than,bi-rnns,much better than bi-rnns,0.6132487654685974
translation,196,183,results,results,has,bi-rnns - crf models,results has bi-rnns - crf models,0.5444412231445312
translation,196,184,results,slightly improved,by incorporating,character level information,slightly improved by incorporating character level information,0.6881236433982849
translation,196,184,results,character level information,with,cnn,character level information with cnn,0.6448686122894287
translation,196,185,results,document- level attention,into,existing models,document- level attention into existing models,0.560319185256958
translation,196,185,results,performance,can be,significantly boosted,performance can be significantly boosted,0.6861007809638977
translation,196,185,results,our corpus-level attention,has,performance,our corpus-level attention has performance,0.5588594079017639
translation,196,185,results,document- level attention,has,performance,document- level attention has performance,0.5321023464202881
translation,196,185,results,existing models,has,performance,existing models has performance,0.578241765499115
translation,196,186,results,our model,with,global attention,our model with global attention,0.6098377108573914
translation,196,186,results,global attention,achieves,best performance,global attention achieves best performance,0.6879312992095947
translation,196,186,results,best performance,in terms of,all metrics,best performance in terms of all metrics,0.662115752696991
translation,196,186,results,results,has,our model,results has our model,0.5871725678443909
translation,196,188,results,symptom inference,presents,symptom inference results,symptom inference presents symptom inference results,0.6268944144248962
translation,196,188,results,symptom inference results,of,classical bi-lstm crf - inference model,symptom inference results of classical bi-lstm crf - inference model,0.5395422577857971
translation,196,188,results,symptom inference results,of,our proposed joint model,symptom inference results of our proposed joint model,0.5677321553230286
translation,196,188,results,results,Performance of,symptom inference,results Performance of symptom inference,0.6791247725486755
translation,196,188,results,results,presents,symptom inference results,results presents symptom inference results,0.6540928483009338
translation,196,189,results,our proposed model,with,global attention,our proposed model with global attention,0.6186344623565674
translation,196,189,results,bi-lstm crfinference model,for,symptom inference,bi-lstm crfinference model for symptom inference,0.5916368961334229
translation,196,189,results,symptom inference,across,all the categories,symptom inference across all the categories,0.7178978323936462
translation,196,189,results,global attention,has,significantly outperforms,global attention has significantly outperforms,0.6143760681152344
translation,196,189,results,significantly outperforms,has,bi-lstm crfinference model,significantly outperforms has bi-lstm crfinference model,0.5869006514549255
translation,196,189,results,results,show,our proposed model,results show our proposed model,0.6705440282821655
translation,196,190,results,substantial improvements,for inferring,false and uncertain categories of symptoms,substantial improvements for inferring false and uncertain categories of symptoms,0.6833038926124573
translation,196,190,results,false and uncertain categories of symptoms,by utilizing,global information,false and uncertain categories of symptoms by utilizing global information,0.6388134360313416
translation,196,190,results,global information,in,current dialogue,global information in current dialogue,0.5268885493278503
translation,196,190,results,global information,in,whole corpus,global information in whole corpus,0.5227682590484619
translation,196,190,results,results,achieve,substantial improvements,results achieve substantial improvements,0.6258140206336975
translation,196,191,results,symptom graph,for,symptom inference,symptom graph for symptom inference,0.5967715978622437
translation,196,191,results,symptom graph,compare,models,symptom graph compare models,0.63751620054245
translation,196,191,results,models,with and without,symptom graphs,models with and without symptom graphs,0.6555086970329285
translation,196,191,results,results,effect of,symptom graph,results effect of symptom graph,0.6978716254234314
translation,196,192,results,symptom graphs,for,inference,symptom graphs for inference,0.6151507496833801
translation,196,192,results,performance,of,each model,performance of each model,0.6105527281761169
translation,196,192,results,symptom graphs,has,performance,symptom graphs has performance,0.5836719870567322
translation,196,192,results,inference,has,performance,inference has performance,0.5827714204788208
translation,196,192,results,results,when incorporating,symptom graphs,results when incorporating symptom graphs,0.6384302377700806
translation,196,207,results,our model,with,graph,our model with graph,0.6537144780158997
translation,196,207,results,our model,achieves,larger improvements,our model achieves larger improvements,0.6409273147583008
translation,196,207,results,graph,for inferring,highly associated symptoms,graph for inferring highly associated symptoms,0.7426118850708008
translation,196,207,results,larger improvements,than,without graph,larger improvements than without graph,0.6382353901863098
translation,196,207,results,highly associated symptoms,such as,cough   and   sputum,highly associated symptoms such as cough   and   sputum,0.647969126701355
translation,196,207,results,results,show,our model,results show our model,0.6888449192047119
translation,197,6,model,novel asynchronous method,for collecting,tutoring dialogue,novel asynchronous method for collecting tutoring dialogue,0.6096146106719971
translation,197,6,model,tutoring dialogue,via,crowdworkers,tutoring dialogue via crowdworkers,0.6332272291183472
translation,197,6,model,model,propose,novel asynchronous method,model propose novel asynchronous method,0.7199592590332031
translation,197,232,results,rule- based baseline,performs,slightly better,rule- based baseline performs slightly better,0.5635064840316772
translation,197,232,results,slightly better,than,gm,slightly better than gm,0.6186068058013916
translation,197,232,results,slightly better,than,gm,slightly better than gm,0.6186068058013916
translation,197,232,results,gm,on,bleu score,gm on bleu score,0.5297995805740356
translation,197,232,results,gm,performs,higher,gm performs higher,0.6581612825393677
translation,197,232,results,gm,performs,higher,gm performs higher,0.6581612825393677
translation,197,232,results,higher,on,bert f1 score,higher on bert f1 score,0.5703798532485962
translation,197,232,results,results,note,rule- based baseline,results note rule- based baseline,0.575286328792572
translation,198,148,model,data-driven dialogue acts,created by,spectral clustering algorithm,data-driven dialogue acts created by spectral clustering algorithm,0.6518113017082214
translation,198,148,model,spectral clustering algorithm,applied on,vectors of sentences,spectral clustering algorithm applied on vectors of sentences,0.690497636795044
translation,198,148,model,vectors of sentences,represented by,derivation rules,vectors of sentences represented by derivation rules,0.7121336460113525
translation,198,148,model,model,has,data-driven dialogue acts,model has data-driven dialogue acts,0.5601038932800293
translation,198,110,results,average word accuracy,of,asr module,average word accuracy of asr module,0.5886932015419006
translation,198,110,results,asr module,is,86.1 %,asr module is 86.1 %,0.5490105152130127
translation,198,110,results,86.1 %,with,lexicon,86.1 % with lexicon,0.6570945382118225
translation,198,110,results,lexicon,of,297 words,lexicon of 297 words,0.5809608697891235
translation,198,110,results,results,has,average word accuracy,results has average word accuracy,0.5546354651451111
translation,198,129,results,strong improvements,when,nec,strong improvements when nec,0.7323925495147705
translation,198,129,results,nec,from,49.6 % to 56.8 % ),nec from 49.6 % to 56.8 % ),0.526583731174469
translation,198,129,results,pst,from,56.8 % to 76.2 % ),pst from 56.8 % to 76.2 % ),0.5172056555747986
translation,198,130,results,dr and dr - da representations,lead to,significant improvements,dr and dr - da representations lead to significant improvements,0.701190710067749
translation,198,130,results,significant improvements,achieving,81.6 %,significant improvements achieving 81.6 %,0.6142295598983765
translation,198,130,results,81.6 %,has,to 82.9 %,81.6 % has to 82.9 %,0.5366185307502747
translation,198,130,results,results,has,dr and dr - da representations,results has dr and dr - da representations,0.5087171196937561
translation,198,131,results,other conditions,of,"40 % - sim , 60 % - sim , and ref","other conditions of 40 % - sim , 60 % - sim , and ref",0.5966943502426147
translation,198,131,results,other conditions,of,similar improvements,other conditions of similar improvements,0.5615061521530151
translation,198,131,results,similar improvements,of using,nec and pst,similar improvements of using nec and pst,0.7164663672447205
translation,198,131,results,other conditions,has,similar improvements,other conditions has similar improvements,0.5605277419090271
translation,198,131,results,"40 % - sim , 60 % - sim , and ref",has,similar improvements,"40 % - sim , 60 % - sim , and ref has similar improvements",0.5594803094863892
translation,198,131,results,results,For,other conditions,results For other conditions,0.6289997696876526
translation,198,132,results,dr - da,suffers from,performance degradation,dr - da suffers from performance degradation,0.7237269282341003
translation,198,132,results,performance degradation,when,keywords,performance degradation when keywords,0.5785121321678162
translation,198,150,results,fully integrated system,find that,proposed approach,fully integrated system find that proposed approach,0.6249686479568481
translation,198,150,results,proposed approach,achieves,84.3 % detection accuracy,proposed approach achieves 84.3 % detection accuracy,0.6464285850524902
translation,198,150,results,results,For,fully integrated system,results For fully integrated system,0.640980064868927
translation,199,172,ablation-analysis,large decrease,in,performance,large decrease in performance,0.5610359311103821
translation,199,172,ablation-analysis,large decrease,relative to,full model,large decrease relative to full model,0.7680595517158508
translation,199,172,ablation-analysis,ablation analysis,Without,response encoder,ablation analysis Without response encoder,0.7355566024780273
translation,199,118,hyperparameters,feature representation,use,300 dimensional word embeddings,feature representation use 300 dimensional word embeddings,0.526857316493988
translation,199,118,hyperparameters,300 dimensional word embeddings,initialized with,glove vectors,300 dimensional word embeddings initialized with glove vectors,0.718801736831665
translation,199,118,hyperparameters,300 dimensional word embeddings,jointly optimized with,rest of the network,300 dimensional word embeddings jointly optimized with rest of the network,0.7509319186210632
translation,199,118,hyperparameters,hyperparameters,As,feature representation,hyperparameters As feature representation,0.46484798192977905
translation,199,138,hyperparameters,hidden sizes,of,"1024 , 256 , 256 , and 512","hidden sizes of 1024 , 256 , 256 , and 512",0.6262784004211426
translation,199,138,hyperparameters,hyperparameters,has,"inference , acoustic , linguistic , and master lstms","hyperparameters has inference , acoustic , linguistic , and master lstms",0.5437887907028198
translation,199,139,hyperparameters,latent variable size,of,4,latent variable size of 4,0.6242216229438782
translation,199,139,hyperparameters,batch size,of,128,batch size of 128,0.6836684346199036
translation,199,139,hyperparameters,l2 regularization,of,1e - 05,l2 regularization of 1e - 05,0.5754185318946838
translation,199,140,hyperparameters,adam optimizer,with,initial learning rate,adam optimizer with initial learning rate,0.5838022828102112
translation,199,140,hyperparameters,initial learning rate,of,5e - 04,initial learning rate of 5e - 04,0.6188014149665833
translation,199,140,hyperparameters,hyperparameters,used,adam optimizer,hyperparameters used adam optimizer,0.5959904789924622
translation,199,141,hyperparameters,each model,for,15000 iterations,each model for 15000 iterations,0.5954722762107849
translation,199,141,hyperparameters,each model,with,learning rate,each model with learning rate,0.6202495098114014
translation,199,141,hyperparameters,reductions,by,factor of 0.1,reductions by factor of 0.1,0.6255176663398743
translation,199,141,hyperparameters,factor of 0.1,after,"9000 , 11000 , 13000 , and 14000 iterations","factor of 0.1 after 9000 , 11000 , 13000 , and 14000 iterations",0.6714006066322327
translation,199,141,hyperparameters,learning rate,has,reductions,learning rate has reductions,0.5827985405921936
translation,199,141,hyperparameters,hyperparameters,trained,each model,hyperparameters trained each model,0.7187178730964661
translation,199,30,model,extension of rtnet,uses,variational autoencoder ( vae ),extension of rtnet uses variational autoencoder ( vae ),0.5677964687347412
translation,199,30,model,variational autoencoder ( vae ),to train,interpretable latent space,variational autoencoder ( vae ) to train interpretable latent space,0.6790740489959717
translation,199,30,model,interpretable latent space,to bypass,encoding process,interpretable latent space to bypass encoding process,0.6883131861686707
translation,199,30,model,encoding process,at,inference-time,encoding process at inference-time,0.5703067183494568
translation,199,30,model,model,propose,extension of rtnet,model propose extension of rtnet,0.6360794901847839
translation,199,31,model,- vae ),allows,benefit,- vae ) allows benefit,0.7109957933425903
translation,199,31,model,benefit,of having,data-driven neural representation,benefit of having data-driven neural representation,0.6132729053497314
translation,199,31,model,data-driven neural representation,of,response encodings,data-driven neural representation of response encodings,0.5792797207832336
translation,199,31,model,data-driven neural representation,manipulated without,overhead,data-driven neural representation manipulated without overhead,0.6965079307556152
translation,199,31,model,overhead,of,encoding process,overhead of encoding process,0.5839315056800842
translation,199,173,results,encoders,with,acoustic and linguistic modalities,encoders with acoustic and linguistic modalities,0.6021965146064758
translation,199,173,results,encoders,see that,results,encoders see that results,0.6935446858406067
translation,199,173,results,results,benefit more from,acoustic modality,results benefit more from acoustic modality,0.6077224016189575
translation,199,173,results,acoustic modality,than,linguistic modality,acoustic modality than linguistic modality,0.5519839525222778
translation,199,173,results,results,looking at,encoders,results looking at encoders,0.6403375864028931
translation,199,173,results,results,benefit more from,acoustic modality,results benefit more from acoustic modality,0.6077224016189575
translation,199,179,results,acoustic or linguistic features,from,user 's features,acoustic or linguistic features from user 's features,0.5139018893241882
translation,199,179,results,acoustic or linguistic features,detrimental to,results,acoustic or linguistic features detrimental to results,0.6780008673667908
translation,199,179,results,results,removing,acoustic or linguistic features,results removing acoustic or linguistic features,0.6682894825935364
translation,199,180,results,interesting irregular - ity,observed in,results,interesting irregular - ity observed in results,0.6900842189788818
translation,199,180,results,results,for,model,results for model,0.6094648838043213
translation,199,180,results,model,uses,only acoustic features,model uses only acoustic features,0.6207094192504883
translation,199,180,results,mae,is,unusually high,mae is unusually high,0.6153352856636047
translation,199,180,results,unusually high,relative to,l bce,unusually high relative to l bce,0.763065755367279
translation,199,180,results,results,has,mae,results has mae,0.6067304015159607
translation,199,180,results,model,has,mae,model has mae,0.6281838417053223
translation,199,180,results,only acoustic features,has,mae,only acoustic features has mae,0.6194376945495605
translation,199,180,results,results,for,model,results for model,0.6094648838043213
translation,199,180,results,results,has,interesting irregular - ity,results has interesting irregular - ity,0.5949995517730713
translation,200,6,model,annotation tool,designed specifically for,conversation data,annotation tool designed specifically for conversation data,0.6410155296325684
translation,200,6,model,lida,has,annotation tool,lida has annotation tool,0.6041178107261658
translation,200,6,model,model,introduce,lida,model introduce lida,0.6627473831176758
translation,200,18,model,web application,to make,dialogue dataset creation and annotation,web application to make dialogue dataset creation and annotation,0.5420681834220886
translation,200,18,model,lida,has,web application,lida has web application,0.5965645909309387
translation,200,18,model,dialogue dataset creation and annotation,has,as easy and fast as possible,dialogue dataset creation and annotation has as easy and fast as possible,0.568109393119812
translation,200,18,model,model,introduce,lida,model introduce lida,0.6627473831176758
translation,200,19,results,best practices,from,other state - of - the - art annotation tools,best practices from other state - of - the - art annotation tools,0.5456016063690186
translation,200,19,results,best practices,allowing,arbitrary ml models,best practices allowing arbitrary ml models,0.7077928185462952
translation,200,19,results,arbitrary ml models,to be integrated as,annotation recommenders,arbitrary ml models to be integrated as annotation recommenders,0.6448987722396851
translation,200,19,results,annotation recommenders,to suggest,annotations,annotation recommenders to suggest annotations,0.6093723773956299
translation,200,19,results,annotations,for,data,annotations for data,0.6114296913146973
translation,201,39,model,structure,of,noun phrases,structure of noun phrases,0.5173891186714172
translation,201,39,model,structure,to facilitate,specific grounding,structure to facilitate specific grounding,0.7031654715538025
translation,201,39,model,specific grounding,within,artifact,specific grounding within artifact,0.6801217198371887
translation,201,37,results,model,achieves,71.3 % accuracy,model achieves 71.3 % accuracy,0.6430060863494873
translation,202,18,model,hierarchical deep neural network,to model,different levels of utterance and dialogue act semantics,hierarchical deep neural network to model different levels of utterance and dialogue act semantics,0.6834389567375183
translation,202,18,model,model,propose,hierarchical deep neural network,model propose hierarchical deep neural network,0.669481098651886
translation,202,68,results,state - of- the - art methods,by,1.6 %,state - of- the - art methods by 1.6 %,0.5252515077590942
translation,202,68,results,state - of- the - art methods,comes within,0.6 %,state - of- the - art methods comes within 0.6 %,0.5546022057533264
translation,202,68,results,1.6 %,on,swda,1.6 % on swda,0.5702872276306152
translation,202,68,results,0.6 %,on,mrda,0.6 % on mrda,0.568878710269928
translation,202,68,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,202,68,results,outperforms,has,state - of- the - art methods,outperforms has state - of- the - art methods,0.5579615831375122
translation,202,68,results,results,has,our model,results has our model,0.5871725678443909
translation,202,69,results,tf-idf glove baseline,by,16.4 % and 12.2 %,tf-idf glove baseline by 16.4 % and 12.2 %,0.5699820518493652
translation,202,70,results,model,able to,make,model able to make,0.6885123252868652
translation,202,70,results,model,over,other methods,model over other methods,0.6846504807472229
translation,202,70,results,make,over,other methods,make over other methods,0.664030909538269
translation,202,70,results,other methods,are,significant,other methods are significant,0.5979616641998291
translation,202,70,results,gains,on,mrda,gains on mrda,0.6352502107620239
translation,202,70,results,mrda,fall short of,state - of - the - art,mrda fall short of state - of - the - art,0.6442198157310486
translation,202,70,results,state - of - the - art,by,0.6 %,state - of - the - art by 0.6 %,0.5466565489768982
translation,202,70,results,results,has,improvements,results has improvements,0.615561842918396
translation,202,87,results,self-attention mechanism ( bi- rnn + selfattention,leads to,even greater overall improvements,self-attention mechanism ( bi- rnn + selfattention leads to even greater overall improvements,0.6367105841636658
translation,202,87,results,results,has,self-attention mechanism ( bi- rnn + selfattention,results has self-attention mechanism ( bi- rnn + selfattention,0.5227023363113403
translation,202,88,results,context information ( previous recurrent state of the conversation,boosts,performance,context information ( previous recurrent state of the conversation boosts performance,0.6942627429962158
translation,202,88,results,performance,has,significantly,performance has significantly,0.6174713373184204
translation,202,88,results,results,Adding,context information ( previous recurrent state of the conversation,results Adding context information ( previous recurrent state of the conversation,0.6444239020347595
translation,202,90,results,conversation - level contextual states,for,utterancerepresentation learning ( + context ),conversation - level contextual states for utterancerepresentation learning ( + context ),0.5910091400146484
translation,202,90,results,crf,at,conversation level,crf at conversation level,0.5447853803634644
translation,202,90,results,conversation level,to further inform,conversation sequence modeling,conversation level to further inform conversation sequence modeling,0.6499293446540833
translation,202,90,results,results,combination of,conversation - level contextual states,results combination of conversation - level contextual states,0.575361967086792
translation,202,91,results,swda dataset,two variants of,context-aware attention models,swda dataset two variants of context-aware attention models,0.6294112205505371
translation,202,91,results,context-aware attention models,have,significant performance gains,context-aware attention models have significant performance gains,0.5501838326454163
translation,203,192,ablation-analysis,plausibility reranking,improving,plausibility,plausibility reranking improving plausibility,0.7185544967651367
translation,203,192,ablation-analysis,plausibility,by,3.6-6.5 %,plausibility by 3.6-6.5 %,0.5557029247283936
translation,203,192,ablation-analysis,ablation analysis,has,plausibility reranking,ablation analysis has plausibility reranking,0.5668628811836243
translation,203,200,ablation-analysis,coherence,led to,increased diversity,coherence led to increased diversity,0.6469789147377014
translation,203,200,ablation-analysis,specificity,led to,decrease,specificity led to decrease,0.6490390300750732
translation,203,200,ablation-analysis,opensubtitles,has,coherence,opensubtitles has coherence,0.6124536395072937
translation,203,200,ablation-analysis,ablation analysis,On,opensubtitles,ablation analysis On opensubtitles,0.5778034925460815
translation,203,124,experiments,classifiers,targeting,various types of errors,classifiers targeting various types of errors,0.6856225728988647
translation,203,124,experiments,various types of errors,using,synthetically generated data,various types of errors using synthetically generated data,0.6498522162437439
translation,203,167,experiments,chit-chat dataset,collected via,crowdsourcing,chit-chat dataset collected via crowdsourcing,0.6207024455070496
translation,203,167,experiments,personachat,has,chit-chat dataset,personachat has chit-chat dataset,0.5924761891365051
translation,203,175,experiments,15 candidates,for,reranking,15 candidates for reranking,0.5927315950393677
translation,203,175,experiments,reranking,per,input sentence,reranking per input sentence,0.5707736611366272
translation,203,173,hyperparameters,lstms,with,hidden layers,lstms with hidden layers,0.6113386750221252
translation,203,173,hyperparameters,lstms,with,learning rate,lstms with learning rate,0.6105855107307434
translation,203,173,hyperparameters,lstms,with,dropout rate,lstms with dropout rate,0.5987489223480225
translation,203,173,hyperparameters,hidden layers,of size,500,hidden layers of size 500,0.7216807007789612
translation,203,173,hyperparameters,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,203,173,hyperparameters,0.2,for,training and testing,0.2 for training and testing,0.6144993305206299
translation,203,173,hyperparameters,learning rate,has,"0.001 , ? 1 = 0.9 , ? 2 = 0.999","learning rate has 0.001 , ? 1 = 0.9 , ? 2 = 0.999",0.5581499338150024
translation,203,173,hyperparameters,dropout rate,has,0.2,dropout rate has 0.2,0.508906900882721
translation,203,173,hyperparameters,metric embedding dimension,has,300,metric embedding dimension has 300,0.5912296175956726
translation,203,173,hyperparameters,hyperparameters,use,lstms,hyperparameters use lstms,0.6021344661712646
translation,203,173,hyperparameters,hyperparameters,use,adam optimizer,hyperparameters use adam optimizer,0.6090166568756104
translation,203,174,hyperparameters,randomly initialized word embeddings,of size,500,randomly initialized word embeddings of size 500,0.6547372937202454
translation,203,174,hyperparameters,500,for,dialog model,500 for dialog model,0.6303039789199829
translation,203,174,hyperparameters,embeddings,for,reranking classifiers,embeddings for reranking classifiers,0.5734461545944214
translation,203,174,hyperparameters,300 dimentional glove,has,embeddings,300 dimentional glove has embeddings,0.5978655219078064
translation,203,174,hyperparameters,hyperparameters,train,randomly initialized word embeddings,hyperparameters train randomly initialized word embeddings,0.5584268569946289
translation,203,18,model,sequence - to-sequence dialogue model,factors out,specificity,sequence - to-sequence dialogue model factors out specificity,0.7134116291999817
translation,203,18,model,sequence - to-sequence dialogue model,explicitly conditions on,generating,sequence - to-sequence dialogue model explicitly conditions on generating,0.723244845867157
translation,203,18,model,generating,has,response,generating has response,0.6202003359794617
translation,203,18,model,model,present,sequence - to-sequence dialogue model,model present sequence - to-sequence dialogue model,0.6481049060821533
translation,203,19,model,decoder,takes as input,categorized values,decoder takes as input categorized values,0.6656180620193481
translation,203,19,model,decoder,uses them at,each stage,decoder uses them at each stage,0.7174660563468933
translation,203,19,model,categorized values,of,several specificity metrics,categorized values of several specificity metrics,0.6199265718460083
translation,203,19,model,categorized values,uses them at,each stage,categorized values uses them at each stage,0.7394985556602478
translation,203,19,model,each stage,of,decoding,each stage of decoding,0.6393808722496033
translation,203,19,model,model,has,decoder,model has decoder,0.6226420402526855
translation,203,57,model,seq2seq model,consisting of,encoder and decoder,seq2seq model consisting of encoder and decoder,0.713386595249176
translation,203,57,model,encoder and decoder,are,lstms,encoder and decoder are lstms,0.5795313715934753
translation,203,57,model,model,based on,seq2seq model,model based on seq2seq model,0.6690468788146973
translation,203,123,model,semantic plausibility issues,propose,reranking method,semantic plausibility issues propose reranking method,0.6058472990989685
translation,203,123,model,reranking method,so that,more plausible sentences,reranking method so that more plausible sentences,0.5477129220962524
translation,203,123,model,ranked higher,among,candidates,ranked higher among candidates,0.6789187788963318
translation,203,123,model,model,To mitigate,semantic plausibility issues,model To mitigate semantic plausibility issues,0.6939387917518616
translation,203,31,results,our plausibility reranking method,improved,"informativeness , relevance , and grammaticality","our plausibility reranking method improved informativeness , relevance , and grammaticality",0.6527717709541321
translation,203,31,results,semantic plausibility,of,responses,semantic plausibility of responses,0.5640873908996582
translation,203,31,results,successfully improved,has,semantic plausibility,successfully improved has semantic plausibility,0.5786690711975098
translation,203,31,results,results,has,our plausibility reranking method,results has our plausibility reranking method,0.5662631392478943
translation,203,188,results,our full model,with,plausibility reranking,our full model with plausibility reranking,0.6934611797332764
translation,203,188,results,plausibility reranking,generates,"most informative , relevant and plausible responses","plausibility reranking generates most informative , relevant and plausible responses",0.6011961102485657
translation,203,188,results,both datasets,has,our full model,both datasets has our full model,0.5880351066589355
translation,203,188,results,results,shows,both datasets,results shows both datasets,0.6079030632972717
translation,203,188,results,results,for,both datasets,results for both datasets,0.5571818351745605
translation,203,190,results,specificity,led to,more interesting responses,specificity led to more interesting responses,0.6995270848274231
translation,203,190,results,more interesting responses,with,6 - 10 % improvement,more interesting responses with 6 - 10 % improvement,0.6176137328147888
translation,203,190,results,more interesting responses,with,3 - 7 % improvement,more interesting responses with 3 - 7 % improvement,0.609462320804596
translation,203,190,results,6 - 10 % improvement,in,informativeness,6 - 10 % improvement in informativeness,0.4948303997516632
translation,203,190,results,3 - 7 % improvement,in,topic relevance,3 - 7 % improvement in topic relevance,0.48764777183532715
translation,203,190,results,results,Incorporating,specificity,results Incorporating specificity,0.6465474367141724
translation,203,198,results,joint approach,gave,stable performance,joint approach gave stable performance,0.6540706753730774
translation,203,198,results,stable performance,on,both datasets,stable performance on both datasets,0.47146883606910706
translation,203,198,results,results,has,joint approach,results has joint approach,0.5723766088485718
translation,203,199,results,our coherence,led to,improvements,our coherence led to improvements,0.6780596971511841
translation,203,199,results,improvements,in,topic relevance and cosine similarity,improvements in topic relevance and cosine similarity,0.5018088817596436
translation,203,199,results,specificity,improved,topic relevance and diversity,specificity improved topic relevance and diversity,0.7186909317970276
translation,203,199,results,personachat,has,our coherence,personachat has our coherence,0.6124953031539917
translation,203,199,results,results,On,personachat,results On personachat,0.6002495288848877
translation,203,203,results,reranking,did improve,plausibility,reranking did improve plausibility,0.6162045001983643
translation,203,203,results,notable improvement,in,informativeness,notable improvement in informativeness,0.4773128032684326
translation,203,207,results,informativeness and diversity ( distinct - 2 ),on,personachat,informativeness and diversity ( distinct - 2 ) on personachat,0.6307516694068909
translation,203,207,results,personachat,by,0.90 and 0.53,personachat by 0.90 and 0.53,0.6113882660865784
translation,203,207,results,median,has,significantly improved,median has significantly improved,0.5909315943717957
translation,203,207,results,significantly improved,has,informativeness and diversity ( distinct - 2 ),significantly improved has informativeness and diversity ( distinct - 2 ),0.5944344997406006
translation,203,207,results,results,Using,median,results Using median,0.6637553572654724
translation,203,219,results,mmi,gave,best grammaticality,mmi gave best grammaticality,0.5969305634498596
translation,203,219,results,results,has,mmi,results has mmi,0.5508152842521667
translation,204,88,experimental-setup,standard backpropagation training,with,momentum,standard backpropagation training with momentum,0.6029842495918274
translation,204,88,experimental-setup,standard backpropagation training,with,initial learning rate,standard backpropagation training with initial learning rate,0.5692348480224609
translation,204,88,experimental-setup,momentum,set to,0.9,momentum set to 0.9,0.6571952700614929
translation,204,88,experimental-setup,initial learning rate,to,10 ?5,initial learning rate to 10 ?5,0.5987541675567627
translation,204,88,experimental-setup,experimental setup,used,standard backpropagation training,experimental setup used standard backpropagation training,0.6080459356307983
translation,204,103,experimental-setup,lenovo x250 laptop,with,intel i5 cpu,lenovo x250 laptop with intel i5 cpu,0.5402243137359619
translation,204,103,experimental-setup,lenovo x250 laptop,with,8 gb ram,lenovo x250 laptop with 8 gb ram,0.5339763760566711
translation,204,103,experimental-setup,lenovo x250 laptop,with,ssd hard disk,lenovo x250 laptop with ssd hard disk,0.588883638381958
translation,204,103,experimental-setup,lenovo x250 laptop,running,linux ubuntu 16.04,lenovo x250 laptop running linux ubuntu 16.04,0.6246718168258667
translation,204,103,experimental-setup,experimental setup,has,system,experimental setup has system,0.535061240196228
translation,204,41,hyperparameters,layer - wise training,of,restricted boltzmann machines ( rbm ),layer - wise training of restricted boltzmann machines ( rbm ),0.5634441375732422
translation,204,41,hyperparameters,layer - wise training,of,frame cross-entropy training,layer - wise training of frame cross-entropy training,0.5559184551239014
translation,204,41,hyperparameters,layer - wise training,of,sequence discriminative training,layer - wise training of sequence discriminative training,0.5587747097015381
translation,204,41,hyperparameters,frame cross-entropy training,with,mini-batch stochastic gradient descent ( sgd ),frame cross-entropy training with mini-batch stochastic gradient descent ( sgd ),0.6160762310028076
translation,204,41,hyperparameters,sequence discriminative training,using,state minimum bayes risk ( smbr ) criterion,sequence discriminative training using state minimum bayes risk ( smbr ) criterion,0.6546834707260132
translation,204,41,hyperparameters,hyperparameters,apply,layer - wise training,hyperparameters apply layer - wise training,0.6068967580795288
translation,204,41,hyperparameters,hyperparameters,apply,frame cross-entropy training,hyperparameters apply frame cross-entropy training,0.6029475927352905
translation,204,64,hyperparameters,convolution window,size,200,convolution window size 200,0.8024306297302246
translation,204,64,hyperparameters,convolution window,corresponds to,25 ms,convolution window corresponds to 25 ms,0.6947122812271118
translation,204,64,hyperparameters,200,corresponds to,25 ms,200 corresponds to 25 ms,0.6920106410980225
translation,204,64,hyperparameters,overlapping step size,of,50,overlapping step size of 50,0.6604968309402466
translation,204,64,hyperparameters,hyperparameters,set,convolution window,hyperparameters set convolution window,0.6474704146385193
translation,204,64,hyperparameters,hyperparameters,set,overlapping step size,hyperparameters set overlapping step size,0.6381856799125671
translation,204,71,hyperparameters,cnn,with,one layer of convolution and max pooling,cnn with one layer of convolution and max pooling,0.6267097592353821
translation,204,71,hyperparameters,cnn,on top of,word embedding vectors,cnn on top of word embedding vectors,0.6268818974494934
translation,204,71,hyperparameters,one layer of convolution and max pooling,on top of,word embedding vectors,one layer of convolution and max pooling on top of word embedding vectors,0.6681138277053833
translation,204,71,hyperparameters,word embedding vectors,trained on,google news corpus,word embedding vectors trained on google news corpus,0.6809000968933105
translation,204,71,hyperparameters,word embedding vectors,of size,300,word embedding vectors of size 300,0.7150178551673889
translation,204,71,hyperparameters,hyperparameters,train,cnn,hyperparameters train cnn,0.7105877995491028
translation,204,72,hyperparameters,convolutional sliding window,of size,"3 , 4 and 5","convolutional sliding window of size 3 , 4 and 5",0.7276924252510071
translation,204,72,hyperparameters,convolutional sliding window,to represent,multiple features,convolutional sliding window to represent multiple features,0.6499084830284119
translation,204,72,hyperparameters,"3 , 4 and 5",to represent,multiple features,"3 , 4 and 5 to represent multiple features",0.6717664003372192
translation,204,72,hyperparameters,word vectors,has,convolutional sliding window,word vectors has convolutional sliding window,0.5662805438041687
translation,204,75,hyperparameters,hidden layer dimensions,set to,100,hidden layer dimensions set to 100,0.7278248071670532
translation,204,75,hyperparameters,hyperparameters,has,hidden layer dimensions,hyperparameters has hidden layer dimensions,0.4960375130176544
translation,204,4,model,interactive dialogue system,to recognize,user emotion and sentiment in realtime,interactive dialogue system to recognize user emotion and sentiment in realtime,0.7199565768241882
translation,204,7,model,cnn model,to extract,emotion,cnn model to extract emotion,0.6353720426559448
translation,204,7,model,emotion,from,raw speech input,emotion from raw speech input,0.5467569828033447
translation,204,7,model,model,describe,cnn model,model describe cnn model,0.6395071744918823
translation,204,9,model,"separate , cnn - based sentiment analysis module",recognizes,sentiments,"separate , cnn - based sentiment analysis module recognizes sentiments",0.6346433758735657
translation,204,9,model,sentiments,from,speech recognition results,sentiments from speech recognition results,0.5835732221603394
translation,204,9,model,sentiments,with,82.5 fmeasure,sentiments with 82.5 fmeasure,0.6444345116615295
translation,204,9,model,82.5 fmeasure,on,human-machine dialogues,82.5 fmeasure on human-machine dialogues,0.5408405661582947
translation,204,9,model,human-machine dialogues,when trained with,out - of- domain data,human-machine dialogues when trained with out - of- domain data,0.7192193269729614
translation,204,9,model,model,has,"separate , cnn - based sentiment analysis module","model has separate , cnn - based sentiment analysis module",0.5465304851531982
translation,204,15,model,module,of,emotion and sentiment recognition,module of emotion and sentiment recognition,0.5935168862342834
translation,204,15,model,emotion and sentiment recognition,for,interactive dialogue system,emotion and sentiment recognition for interactive dialogue system,0.6149980425834656
translation,204,15,model,model,propose,module,model propose module,0.7126700282096863
translation,204,40,model,deep neural network hidden markov models ( dnn - hmms ),using,raw audio,deep neural network hidden markov models ( dnn - hmms ) using raw audio,0.6533291935920715
translation,204,40,model,raw audio,together with,encode-decode parallel audio,raw audio together with encode-decode parallel audio,0.6280998587608337
translation,204,40,model,model,train,deep neural network hidden markov models ( dnn - hmms ),model train deep neural network hidden markov models ( dnn - hmms ),0.6827691793441772
translation,204,63,model,cnn,designed with,single filter,cnn designed with single filter,0.6548987030982971
translation,204,63,model,single filter,for,real-time processing,single filter for real-time processing,0.6537650227546692
translation,204,63,model,model,has,cnn,model has cnn,0.577612578868866
translation,204,65,model,convolution layer,performs,feature extraction,convolution layer performs feature extraction,0.6009758114814758
translation,204,65,model,convolution layer,models,variations,convolution layer models variations,0.7686961889266968
translation,204,65,model,variations,among,"neighboring , overlapping frames","variations among neighboring , overlapping frames",0.5814743638038635
translation,204,65,model,model,has,convolution layer,model has convolution layer,0.5562974810600281
translation,204,73,model,max-pooling operation,over,output vectors,max-pooling operation over output vectors,0.6225103735923767
translation,204,73,model,output vectors,of,convolutional layer,output vectors of convolutional layer,0.5701336860656738
translation,204,73,model,model,to pick up,most valuable information,model to pick up most valuable information,0.7267971634864807
translation,204,73,model,most valuable information,output,fixed - length sentence encoding vector,most valuable information output fixed - length sentence encoding vector,0.7521345615386963
translation,204,73,model,model,apply,max-pooling operation,model apply max-pooling operation,0.6551768183708191
translation,204,74,model,two distinct cnn channels,uses,word embedding vectors,two distinct cnn channels uses word embedding vectors,0.5741415619850159
translation,204,74,model,word embedding vectors,directly as,input,word embedding vectors directly as input,0.6438398957252502
translation,204,74,model,model,employ,two distinct cnn channels,model employ two distinct cnn channels,0.5788890719413757
translation,204,97,results,sometimes marginally,in,angry and sad classes,sometimes marginally in angry and sad classes,0.5105485320091248
translation,204,97,results,all the emotion classes considered,has,cnn model,all the emotion classes considered has cnn model,0.5929369330406189
translation,204,97,results,cnn model,has,outperformed,cnn model has outperformed,0.6267691850662231
translation,204,97,results,outperformed,has,svm baseline,outperformed has svm baseline,0.5953543186187744
translation,204,97,results,outperformed,has,sometimes marginally,outperformed has sometimes marginally,0.640819787979126
translation,204,97,results,svm baseline,has,sometimes marginally,svm baseline has sometimes marginally,0.5955660343170166
translation,204,97,results,more significantly,has,happy and criticism classes,more significantly has happy and criticism classes,0.5776865482330322
translation,204,97,results,results,In,all the emotion classes considered,results In all the emotion classes considered,0.5145348906517029
translation,204,110,results,our cnn model,got,6.1 % relative improvement,our cnn model got 6.1 % relative improvement,0.5665314197540283
translation,204,110,results,6.1 % relative improvement,on,f-score,6.1 % relative improvement on f-score,0.5254185199737549
translation,204,110,results,6.1 % relative improvement,over,baseline,6.1 % relative improvement over baseline,0.6597911715507507
translation,204,110,results,6.1 % relative improvement,trained with,larger twitter dataset,6.1 % relative improvement trained with larger twitter dataset,0.692693293094635
translation,204,110,results,results,has,our cnn model,results has our cnn model,0.5471631288528442
translation,205,221,ablation-analysis,reduced,from,2 to 0,reduced from 2 to 0,0.5442091226577759
translation,205,221,ablation-analysis,number of message passing iterations,has,k,number of message passing iterations has k,0.5901088118553162
translation,205,221,ablation-analysis,number of message passing iterations,has,loss,number of message passing iterations has loss,0.5762447714805603
translation,205,221,ablation-analysis,loss,has,consistently increases,loss has consistently increases,0.6330500245094299
translation,205,221,ablation-analysis,ablation analysis,When,number of message passing iterations,ablation analysis When number of message passing iterations,0.6530696749687195
translation,205,222,ablation-analysis,degrades,has,performance,degrades has performance,0.5834147930145264
translation,205,222,ablation-analysis,ablation analysis,Removing,entity abstraction,ablation analysis Removing entity abstraction,0.6975467801094055
translation,205,142,hyperparameters,data,into,"train , dev , and test sets ( 8:1:1 )","data into train , dev , and test sets ( 8:1:1 )",0.5572165846824646
translation,205,142,hyperparameters,hyperparameters,randomly split,data,hyperparameters randomly split data,0.7655662298202515
translation,205,143,hyperparameters,one- layer lstm,with,100 hidden units,one- layer lstm with 100 hidden units,0.606386125087738
translation,205,143,hyperparameters,one- layer lstm,with,100 - dimensional word vectors,one- layer lstm with 100 - dimensional word vectors,0.6102449893951416
translation,205,143,hyperparameters,100 - dimensional word vectors,for,both the encoder and the decoder,100 - dimensional word vectors for both the encoder and the decoder,0.5896461606025696
translation,205,143,hyperparameters,hyperparameters,use,one- layer lstm,hyperparameters use one- layer lstm,0.5983721017837524
translation,205,146,hyperparameters,parameters,optimized by,"adagrad ( duchi et al. , 2010 )","parameters optimized by adagrad ( duchi et al. , 2010 )",0.585448682308197
translation,205,146,hyperparameters,"adagrad ( duchi et al. , 2010 )",with,initial learning rate,"adagrad ( duchi et al. , 2010 ) with initial learning rate",0.5286109447479248
translation,205,146,hyperparameters,initial learning rate,of,0.5,initial learning rate of 0.5,0.6004254221916199
translation,205,146,hyperparameters,hyperparameters,has,parameters,hyperparameters has parameters,0.4783959984779358
translation,205,147,hyperparameters,stops,if,no improvement,stops if no improvement,0.6623260974884033
translation,205,147,hyperparameters,no improvement,on,dev set,no improvement on dev set,0.6058999300003052
translation,205,147,hyperparameters,dev set,for,5 epochs,dev set for 5 epochs,0.6183580756187439
translation,205,147,hyperparameters,training,has,stops,training has stops,0.6109644770622253
translation,205,147,hyperparameters,hyperparameters,trained for,at least 10 epochs,hyperparameters trained for at least 10 epochs,0.7267897129058838
translation,205,148,hyperparameters,k = 2 iterations,of,message passing,k = 2 iterations of message passing,0.5958714485168457
translation,205,148,hyperparameters,message passing,to compute,node embeddings,message passing to compute node embeddings,0.7280598878860474
translation,205,148,hyperparameters,hyperparameters,perform,k = 2 iterations,hyperparameters perform k = 2 iterations,0.5651199221611023
translation,205,149,hyperparameters,decoding,sequentially sample,output distribution,decoding sequentially sample output distribution,0.7837530374526978
translation,205,149,hyperparameters,output distribution,with,softmax temperature,output distribution with softmax temperature,0.6567450761795044
translation,205,149,hyperparameters,softmax temperature,of,0.5,softmax temperature of 0.5,0.6274405121803284
translation,205,149,hyperparameters,hyperparameters,For,decoding,hyperparameters For decoding,0.5911394953727722
translation,205,22,model,structured and open-ended context,propose,dynamic knowledge graph network ( dynonet ),structured and open-ended context propose dynamic knowledge graph network ( dynonet ),0.6256718039512634
translation,205,22,model,dynamic knowledge graph network ( dynonet ),in which,dialogue state,dynamic knowledge graph network ( dynonet ) in which dialogue state,0.6052084565162659
translation,205,22,model,dialogue state,modeled as,knowledge graph,dialogue state modeled as knowledge graph,0.6388499140739441
translation,205,22,model,knowledge graph,with,embedding,knowledge graph with embedding,0.6488707661628723
translation,205,22,model,embedding,for,each node,embedding for each node,0.6243482232093811
translation,205,22,model,model,To model,structured and open-ended context,model To model structured and open-ended context,0.7649934887886047
translation,205,26,model,knowledge graphs,captures,grounding capability,knowledge graphs captures grounding capability,0.7144547700881958
translation,205,26,model,grounding capability,of,classic task - oriented systems,grounding capability of classic task - oriented systems,0.591701090335846
translation,205,26,model,graph embedding,provides,representational flexibility,graph embedding provides representational flexibility,0.6373303532600403
translation,205,26,model,representational flexibility,of,neural models,representational flexibility of neural models,0.5722382664680481
translation,205,26,model,model,Our model 's use of,knowledge graphs,model Our model 's use of knowledge graphs,0.6539233922958374
translation,205,161,results,dynonet,has,lowest test loss,dynonet has lowest test loss,0.5730285048484802
translation,205,161,results,results,has,dynonet,results has dynonet,0.5576302409172058
translation,205,200,results,dynonet,achieves,better partner satisfaction,dynonet achieves better partner satisfaction,0.6202458739280701
translation,205,200,results,better partner satisfaction,in,cooperation,better partner satisfaction in cooperation,0.49144673347473145
translation,205,200,results,results,has,dynonet,results has dynonet,0.5576302409172058
translation,205,212,results,best partner satisfaction,in terms of,fluency ( flnt ),best partner satisfaction in terms of fluency ( flnt ),0.6734843254089355
translation,205,212,results,best partner satisfaction,in terms of,correctness ( crct ),best partner satisfaction in terms of correctness ( crct ),0.676216721534729
translation,205,212,results,best partner satisfaction,in terms of,cooperation ( coop ),best partner satisfaction in terms of cooperation ( coop ),0.6837708353996277
translation,205,212,results,best partner satisfaction,in terms of,human likeness,best partner satisfaction in terms of human likeness,0.6260198354721069
translation,205,212,results,best partner satisfaction,in terms of,human ),best partner satisfaction in terms of human ),0.6712298393249512
translation,205,212,results,dynonet,has,best partner satisfaction,dynonet has best partner satisfaction,0.5466873645782471
translation,205,212,results,human likeness,has,human ),human likeness has human ),0.6088098287582397
translation,205,212,results,results,has,dynonet,results has dynonet,0.5576302409172058
translation,206,233,ablation-analysis,both mie models and mie - classifier models,overwhelm,plain - classifier model,both mie models and mie - classifier models overwhelm plain - classifier model,0.6870079040527344
translation,206,233,ablation-analysis,ablation analysis,has,both mie models and mie - classifier models,ablation analysis has both mie models and mie - classifier models,0.5651065111160278
translation,206,221,baselines,mie - classifier -multi,considers,turn-interaction,mie - classifier -multi considers turn-interaction,0.6837932467460632
translation,206,221,baselines,turn-interaction,as,mie -multi,turn-interaction as mie -multi,0.5939621925354004
translation,206,221,baselines,baselines,has,mie - classifier -multi,baselines has mie - classifier -multi,0.6247963905334473
translation,206,7,experiments,online medical consultation dialogues,in,window- sliding style,online medical consultation dialogues in window- sliding style,0.5221875905990601
translation,206,201,hyperparameters,size,of,hidden states,size of hidden states,0.6102981567382812
translation,206,201,hyperparameters,hidden states,of,feed -forward network,hidden states of feed -forward network,0.583204448223114
translation,206,201,hyperparameters,hidden states,of,bi-lstm,hidden states of bi-lstm,0.5694854259490967
translation,206,201,hyperparameters,bi-lstm,is,400,bi-lstm is 400,0.5830090045928955
translation,206,201,hyperparameters,hyperparameters,has,size,hyperparameters has size,0.5313372611999512
translation,206,202,hyperparameters,"dropout ( srivastava et al. , 2014 )",with,0.2 drop rate,"dropout ( srivastava et al. , 2014 ) with 0.2 drop rate",0.5442216396331787
translation,206,202,hyperparameters,"dropout ( srivastava et al. , 2014 )",with,hidden states,"dropout ( srivastava et al. , 2014 ) with hidden states",0.5807865858078003
translation,206,202,hyperparameters,0.2 drop rate,to,output,0.2 drop rate to output,0.561059832572937
translation,206,202,hyperparameters,0.2 drop rate,to,hidden states,0.2 drop rate to hidden states,0.5412972569465637
translation,206,202,hyperparameters,output,of,each module,output of each module,0.6362781524658203
translation,206,202,hyperparameters,hidden states,of,feed -forward network,hidden states of feed -forward network,0.583204448223114
translation,206,202,hyperparameters,feed -forward network,for,regularization,feed -forward network for regularization,0.6368937492370605
translation,206,202,hyperparameters,hyperparameters,apply,"dropout ( srivastava et al. , 2014 )","hyperparameters apply dropout ( srivastava et al. , 2014 )",0.5427748560905457
translation,206,203,hyperparameters,early stopping,using,f1 score,early stopping using f1 score,0.6676729917526245
translation,206,203,hyperparameters,f1 score,on,development set,f1 score on development set,0.5765734910964966
translation,206,203,hyperparameters,hyperparameters,adopt,early stopping,hyperparameters adopt early stopping,0.6519354581832886
translation,206,8,model,medical information extractor ( mie ),towards,medical dialogues,medical information extractor ( mie ) towards medical dialogues,0.674862802028656
translation,206,8,model,model,propose,medical information extractor ( mie ),model propose medical information extractor ( mie ),0.6769938468933105
translation,206,37,model,medical information extractor,constructed on,deep matching model,medical information extractor constructed on deep matching model,0.6009514331817627
translation,206,37,model,mie,has,medical information extractor,mie has medical information extractor,0.5274559259414673
translation,206,37,model,model,propose,mie,model propose mie,0.7146005630493164
translation,206,45,model,mie,constructed on,novel neural matching model,mie constructed on novel neural matching model,0.6400089859962463
translation,206,45,model,model,develop,mie,model develop mie,0.7290681600570679
translation,206,46,model,mie model,consists of,four main components,mie model consists of four main components,0.6689571142196655
translation,206,46,model,four main components,namely,encoder module,four main components namely encoder module,0.7043125033378601
translation,206,46,model,four main components,namely,matching module,four main components namely matching module,0.7159290909767151
translation,206,46,model,four main components,namely,aggregate module,four main components namely aggregate module,0.7119064927101135
translation,206,46,model,four main components,namely,scorer module,four main components namely scorer module,0.6808022856712341
translation,206,46,model,model,has,mie model,model has mie model,0.5818130970001221
translation,206,269,model,corpus,for,medical information extraction task,corpus for medical information extraction task,0.5736964344978333
translation,206,269,model,corpus,including,annotation methods,corpus including annotation methods,0.6130737066268921
translation,206,269,model,corpus,including,evaluation metrics,corpus including evaluation metrics,0.6125118732452393
translation,206,270,model,deep neural matching model,tailored for,task,deep neural matching model tailored for task,0.6908369660377502
translation,206,270,model,mie,has,deep neural matching model,mie has deep neural matching model,0.46171846985816956
translation,206,270,model,model,propose,mie,model propose mie,0.7146005630493164
translation,206,9,results,mie,able to extract,mentioned symptoms,mie able to extract mentioned symptoms,0.6995278596878052
translation,206,9,results,mie,able to extract,surgeries,mie able to extract surgeries,0.7495341300964355
translation,206,9,results,mie,able to extract,tests,mie able to extract tests,0.7352545857429504
translation,206,9,results,mie,able to extract,other information,mie able to extract other information,0.6217644810676575
translation,206,9,results,mie,able to extract,corresponding status,mie able to extract corresponding status,0.6737570762634277
translation,206,9,results,results,has,mie,results has mie,0.6101365685462952
translation,206,50,results,mie,achieves,promising overall f-score,mie achieves promising overall f-score,0.6513903737068176
translation,206,50,results,promising overall f-score,of,69.28,promising overall f-score of 69.28,0.5304372310638428
translation,206,50,results,significantly surpassing,has,several competitive baselines,significantly surpassing has several competitive baselines,0.5771490335464478
translation,206,50,results,results,has,mie,results has mie,0.6101365685462952
translation,206,224,results,mie - multi,achieves,best f-score,mie - multi achieves best f-score,0.66962069272995
translation,206,224,results,best f-score,on,window-level and dialogue - level full evaluation metric,best f-score on window-level and dialogue - level full evaluation metric,0.48837485909461975
translation,206,224,results,results,has,mie - multi,results has mie - multi,0.6051118969917297
translation,206,225,results,f-score,reaches,66.40 and 69.28,f-score reaches 66.40 and 69.28,0.6674757599830627
translation,206,225,results,results,has,f-score,results has f-score,0.5543299913406372
translation,206,226,results,both of the models,using,multi-turn interactions,both of the models using multi-turn interactions,0.6751431822776794
translation,206,226,results,both of the models,models solely using,single utterance information,both of the models models solely using single utterance information,0.7678576111793518
translation,206,226,results,multi-turn interactions,has,perform better,multi-turn interactions has perform better,0.5911383628845215
translation,206,226,results,results,has,both of the models,results has both of the models,0.5242513418197632
translation,206,228,results,mie - multi,achieves,2.01 % f-score improvement,mie - multi achieves 2.01 % f-score improvement,0.6484693884849548
translation,206,228,results,2.01 % f-score improvement,in,dialogue - level full evaluation,2.01 % f-score improvement in dialogue - level full evaluation,0.4833831489086151
translation,206,228,results,results,has,mie - multi,results has mie - multi,0.6051118969917297
translation,206,229,results,matching - based methods,surpass,classifier models,matching - based methods surpass classifier models,0.710267186164856
translation,206,229,results,classifier models,in,full evaluation,classifier models in full evaluation,0.4846976399421692
translation,206,229,results,results,has,matching - based methods,results has matching - based methods,0.5372671484947205
translation,206,232,results,mie - classifiers,are,better at times,mie - classifiers are better at times,0.6172884702682495
translation,206,232,results,mie - classifiers,are,fail,mie - classifiers are fail,0.6337499022483826
translation,206,232,results,fail,to,correctly predict,fail to correctly predict,0.6359409093856812
translation,206,232,results,category and item metrics,has,mie - classifiers,category and item metrics has mie - classifiers,0.5667888522148132
translation,206,232,results,correctly predict,has,status information,correctly predict has status information,0.598787248134613
translation,206,232,results,results,Note,category and item metrics,results Note category and item metrics,0.5677182674407959
translation,206,234,results,dialogue - level performance,not always better than,window - level performance,dialogue - level performance not always better than window - level performance,0.7292658090591431
translation,206,234,results,window - level performance,in,full evalua- tion,window - level performance in full evalua- tion,0.5409560799598694
translation,206,234,results,results,has,dialogue - level performance,results has dialogue - level performance,0.539566159248352
translation,206,235,results,classifier - based models,perform,better,classifier - based models perform better,0.6262316107749939
translation,206,235,results,better,in,window - level,better in window - level,0.5396796464920044
translation,206,235,results,better,in,full evaluation,better in full evaluation,0.53763747215271
translation,206,235,results,window - level,than,dialogue - level,window - level than dialogue - level,0.6163771152496338
translation,207,6,model,computational model,for,automatically annotating text,computational model for automatically annotating text,0.5757707953453064
translation,207,6,model,computational model,using,supervised learning,computational model using supervised learning,0.5964277386665344
translation,207,6,model,automatically annotating text,using,coding scheme,automatically annotating text using coding scheme,0.6908991932868958
translation,207,6,model,automatically annotating text,using,supervised learning,automatically annotating text using supervised learning,0.5785370469093323
translation,207,6,model,supervised learning,enhanced by,constraints,supervised learning enhanced by constraints,0.6949459910392761
translation,207,6,model,constraints,implemented with,integer linear programming,constraints implemented with integer linear programming,0.6530242562294006
translation,207,210,results,kappa,of,0.465 ),kappa of 0.465 ),0.5814766883850098
translation,207,210,results,random chance,has,kappa,random chance has kappa,0.6299798488616943
translation,207,210,results,results,observe,baseline bag-of-words model,results observe baseline bag-of-words model,0.5654593706130981
translation,207,211,results,significant improvement,when,ilp constraints,significant improvement when ilp constraints,0.6661189794540405
translation,207,211,results,results,observe,significant improvement,results observe significant improvement,0.6078537106513977
translation,207,212,results,contextual model,performs,better,contextual model performs better,0.6595280766487122
translation,207,212,results,better,than,our baseline constrained model,better than our baseline constrained model,0.5495668649673462
translation,207,212,results,results,has,contextual model,results has contextual model,0.5356138348579407
translation,207,215,results,baseline,to,contextual features,baseline to contextual features,0.5054488778114319
translation,207,215,results,baseline,observe,improvement,baseline observe improvement,0.6117035150527954
translation,207,215,results,improvement,in,accuracy,improvement in accuracy,0.49491560459136963
translation,207,215,results,accuracy,of,2.6 %,accuracy of 2.6 %,0.5660911202430725
translation,207,215,results,results,Switching from,baseline,results Switching from baseline,0.6733338236808777
translation,207,235,results,human analyses,of,authoritativeness,human analyses of authoritativeness,0.5750925540924072
translation,207,235,results,r 2 coefficient,of,0.947,r 2 coefficient of 0.947,0.5171435475349426
translation,207,235,results,authoritativeness,has,very well,authoritativeness has very well,0.494195818901062
translation,208,91,ablation-analysis,increasing entropy,course of,whole dialogue,increasing entropy course of whole dialogue,0.692025899887085
translation,208,91,ablation-analysis,ablation analysis,find,increasing entropy,ablation analysis find increasing entropy,0.6067792773246765
translation,208,88,results,fixed effects,of,global position,fixed effects of global position,0.5831394195556641
translation,208,88,results,global position,are,significant,global position are significant,0.5969318151473999
translation,208,88,results,significant,for,both measures,significant for both measures,0.5532152056694031
translation,208,88,results,both measures,in,both corpora,both measures in both corpora,0.5291901230812073
translation,208,88,results,entropy,in,switchboard,entropy in switchboard,0.5441964864730835
translation,208,88,results,entropy,in,switchboard,entropy in switchboard,0.5441964864730835
translation,208,88,results,entropy,in,bnc,entropy in bnc,0.5510824918746948
translation,208,88,results,entropy,in,bnc,entropy in bnc,0.5510824918746948
translation,208,88,results,entropy,in,switchboard,entropy in switchboard,0.5441964864730835
translation,208,88,results,entropy,in,bnc,entropy in bnc,0.5510824918746948
translation,208,88,results,normalized entropy,in,switchboard,normalized entropy in switchboard,0.5358219146728516
translation,208,88,results,entropy,in,bnc,entropy in bnc,0.5510824918746948
translation,208,88,results,normalized entropy,in,bnc,normalized entropy in bnc,0.5595410466194153
translation,208,88,results,results,show,fixed effects,results show fixed effects,0.5262935161590576
translation,208,107,results,significant fixed effect,of,within-episode position,significant fixed effect of within-episode position,0.5652084946632385
translation,208,107,results,within-episode position,on,both measures,within-episode position on both measures,0.4576072096824646
translation,208,107,results,both measures,for,both corpora,both measures for both corpora,0.602156400680542
translation,208,107,results,entropy,in,switchboard,entropy in switchboard,0.5441964864730835
translation,208,107,results,entropy,in,switchboard,entropy in switchboard,0.5441964864730835
translation,208,107,results,entropy,in,bnc,entropy in bnc,0.5510824918746948
translation,208,107,results,entropy,in,bnc,entropy in bnc,0.5510824918746948
translation,208,107,results,normalized entropy,in,switchboard,normalized entropy in switchboard,0.5358219146728516
translation,208,107,results,normalized entropy,in,? = 4.5 ? 10 ?3,normalized entropy in ? = 4.5 ? 10 ?3,0.5500102043151855
translation,208,107,results,entropy,in,bnc,entropy in bnc,0.5510824918746948
translation,208,107,results,normalized entropy,in,bnc,normalized entropy in bnc,0.5595410466194153
translation,208,107,results,normalized entropy,has,? = 3.0 ? 10 ?3,normalized entropy has ? = 3.0 ? 10 ?3,0.587512731552124
translation,208,107,results,results,find,significant fixed effect,results find significant fixed effect,0.5560887455940247
translation,209,7,model,reinforcement learning ( rl ) framework,in which,system,reinforcement learning ( rl ) framework in which system,0.6370011568069458
translation,209,7,model,system,learns,reg policies,system learns reg policies,0.714813232421875
translation,209,7,model,reg policies,adapt to,unknown users online,reg policies adapt to unknown users online,0.7088745832443237
translation,209,8,model,effective adaptive policies,be learned from,small dialogue corpus,effective adaptive policies be learned from small dialogue corpus,0.6482294797897339
translation,209,8,model,small dialogue corpus,of,non-adaptive human-machine interaction,small dialogue corpus of non-adaptive human-machine interaction,0.48936495184898376
translation,209,8,model,small dialogue corpus,by using,rl framework,small dialogue corpus by using rl framework,0.5931482315063477
translation,209,8,model,small dialogue corpus,by using,statistical user simulation,small dialogue corpus by using statistical user simulation,0.5709778070449829
translation,209,8,model,model,show,effective adaptive policies,model show effective adaptive policies,0.6955843567848206
translation,209,24,model,corpus-driven framework,using which,user-adaptive reg policy,corpus-driven framework using which user-adaptive reg policy,0.6251917481422424
translation,209,24,model,user-adaptive reg policy,be learned using,rl,user-adaptive reg policy be learned using rl,0.7850819230079651
translation,209,24,model,rl,from,small corpus,rl from small corpus,0.5433389544487
translation,209,24,model,small corpus,of,non-adaptive humanmachine interaction,small corpus of non-adaptive humanmachine interaction,0.5218629240989685
translation,209,24,model,model,present,corpus-driven framework,model present corpus-driven framework,0.6249052882194519
translation,209,258,results,learned ds policy,is,"most accurate ( mean = 79.70 , sd = 10.46 )","learned ds policy is most accurate ( mean = 79.70 , sd = 10.46 )",0.5313875079154968
translation,209,258,results,"most accurate ( mean = 79.70 , sd = 10.46 )",in terms of,adaptation,"most accurate ( mean = 79.70 , sd = 10.46 ) in terms of adaptation",0.6833109855651855
translation,209,258,results,adaptation,to,each user 's initial state of domain knowledge,adaptation to each user 's initial state of domain knowledge,0.5188862085342407
translation,209,258,results,results,found that,learned ds policy,results found that learned ds policy,0.6815336346626282
translation,209,261,results,learned ds policy,has,outperforms,learned ds policy has outperforms,0.6253833770751953
translation,209,261,results,outperforms,has,all other policies,outperforms has all other policies,0.5656366348266602
translation,209,261,results,learned hs,has,"mean = 69.67 , sd = 14.18 )","learned hs has mean = 69.67 , sd = 14.18 )",0.5647115111351013
translation,209,261,results,switching,has,"mean = 62.47 , sd = 14.18 )","switching has mean = 62.47 , sd = 14.18 )",0.5633621215820312
translation,209,261,results,jargon,has,"mean = 74.54 , sd = 17.9 )","jargon has mean = 74.54 , sd = 17.9 )",0.5224934220314026
translation,209,261,results,descriptive,has,"mean = 46.15 , sd = 33.29 )","descriptive has mean = 46.15 , sd = 33.29 )",0.5156769156455994
translation,209,261,results,results,has,learned ds policy,results has learned ds policy,0.6014915108680725
translation,209,267,results,jargon policy,performs,better,jargon policy performs better,0.6267985701560974
translation,209,267,results,better,than,learned hs and switching policies,better than learned hs and switching policies,0.5668371319770813
translation,209,267,results,results,has,jargon policy,results has jargon policy,0.5588337779045105
translation,209,273,results,jargon policy,produces,more user learning gain,jargon policy produces more user learning gain,0.6222376823425293
translation,209,273,results,more user learning gain,because of,more jargon expressions,more user learning gain because of more jargon expressions,0.661044180393219
translation,209,273,results,more user learning gain,use of,more jargon expressions,more user learning gain use of more jargon expressions,0.5962077379226685
translation,209,273,results,results,has,jargon policy,results has jargon policy,0.5588337779045105
translation,210,26,results,eye gaze,with,word confusion networks,eye gaze with word confusion networks,0.6226037740707397
translation,210,26,results,eye gaze,improves,performance,eye gaze improves performance,0.653927206993103
translation,210,26,results,results,incorporating,eye gaze,results incorporating eye gaze,0.6516793966293335
translation,210,166,results,lenient reference resolution,incorporating,eye gaze information,lenient reference resolution incorporating eye gaze information,0.7215330600738525
translation,210,169,results,eye gaze information,significantly ( p < 0.0024 ) improves,reference resolution performance,eye gaze information significantly ( p < 0.0024 ) improves reference resolution performance,0.6353460550308228
translation,210,169,results,reference resolution performance,when using,transcription,reference resolution performance when using transcription,0.6952798366546631
translation,210,169,results,marginally ( p < 0.113 ),case of,wcn,marginally ( p < 0.113 ) case of wcn,0.7205117344856262
translation,210,169,results,wcn,optimized for,strict evaluation,wcn optimized for strict evaluation,0.752769410610199
translation,210,169,results,results,incorporating,eye gaze information,results incorporating eye gaze information,0.6267455220222473
translation,210,177,results,significantly improved,for,all input configurations,significantly improved for all input configurations,0.6317154765129089
translation,210,177,results,all input configurations,when,eye gaze information,all input configurations when eye gaze information,0.6032841205596924
translation,210,177,results,lenient evaluation,has,reference resolution performance,lenient evaluation has reference resolution performance,0.5256861448287964
translation,210,177,results,results,In,lenient evaluation,results In lenient evaluation,0.548669159412384
translation,210,178,results,closely coupled utterances,achieve,higher performance,closely coupled utterances achieve higher performance,0.6611049175262451
translation,210,178,results,higher performance,than,entire set of utterances,higher performance than entire set of utterances,0.576586902141571
translation,210,179,results,wcn,optimized for,strict evaluation,wcn optimized for strict evaluation,0.752769410610199
translation,210,183,results,significant improvement,when using,wcns,significant improvement when using wcns,0.7286698222160339
translation,210,183,results,wcns,rather than,1 - best hypotheses,wcns rather than 1 - best hypotheses,0.689029335975647
translation,210,183,results,wcns,for,with ( p < 0.015 ) and without ( p < 0.0012 ) eye gaze configurations,wcns for with ( p < 0.015 ) and without ( p < 0.0012 ) eye gaze configurations,0.5777052044868469
translation,210,183,results,results,shows,significant improvement,results shows significant improvement,0.6989579796791077
translation,210,184,results,significant improvement,in,strict evaluation,significant improvement in strict evaluation,0.5362805724143982
translation,210,184,results,strict evaluation,when using,wcns,strict evaluation when using wcns,0.6907358169555664
translation,210,184,results,wcns,rather than,1 - best hypotheses,wcns rather than 1 - best hypotheses,0.689029335975647
translation,210,184,results,results,shows,significant improvement,results shows significant improvement,0.6989579796791077
translation,210,185,results,word confusion networks,improves,lenient and strict reference resolution,word confusion networks improves lenient and strict reference resolution,0.6402349472045898
translation,210,185,results,results,indicate,word confusion networks,results indicate word confusion networks,0.5418798327445984
translation,210,185,results,results,using,word confusion networks,results using word confusion networks,0.6272466778755188
translation,210,204,results,multiple speech recognition hypotheses,in the form of,word confusion network,multiple speech recognition hypotheses in the form of word confusion network,0.668442964553833
translation,210,204,results,multiple speech recognition hypotheses,improves,reference resolution performance,multiple speech recognition hypotheses improves reference resolution performance,0.6289417743682861
translation,210,204,results,results,incorporating,multiple speech recognition hypotheses,results incorporating multiple speech recognition hypotheses,0.6104677319526672
translation,211,117,baselines,baselines,has,tm - hmms,baselines has tm - hmms,0.5615828633308411
translation,211,162,hyperparameters,all models,with,80 %,all models with 80 %,0.6469834446907043
translation,211,162,hyperparameters,all models,use,rest,all models use rest,0.6686866283416748
translation,211,162,hyperparameters,80 %,of,entire dataset,80 % of entire dataset,0.5824031829833984
translation,211,162,hyperparameters,rest,for,testing,rest for testing,0.6204051375389099
translation,211,162,hyperparameters,hyperparameters,train,all models,hyperparameters train all models,0.6430407166481018
translation,211,163,hyperparameters,gibbs samplers,for,1000 iterations,gibbs samplers for 1000 iterations,0.5537481307983398
translation,211,163,hyperparameters,gibbs samplers,update,all hyper-parameters,gibbs samplers update all hyper-parameters,0.7453870177268982
translation,211,163,hyperparameters,all hyper-parameters,using,"slice sampling ( neal , 2003 ; wallach , 2008 )","all hyper-parameters using slice sampling ( neal , 2003 ; wallach , 2008 )",0.6446628570556641
translation,211,163,hyperparameters,"slice sampling ( neal , 2003 ; wallach , 2008 )",every,10 iterations,"slice sampling ( neal , 2003 ; wallach , 2008 ) every 10 iterations",0.6712077856063843
translation,211,163,hyperparameters,hyperparameters,run,gibbs samplers,hyperparameters run gibbs samplers,0.6771657466888428
translation,211,164,hyperparameters,training likelihood,suggest,all models,training likelihood suggest all models,0.6180930137634277
translation,211,164,hyperparameters,converge,within,500? 800 iterations,converge within 500? 800 iterations,0.6375011205673218
translation,211,164,hyperparameters,all models,has,converge,all models has converge,0.6151766180992126
translation,211,164,hyperparameters,hyperparameters,has,training likelihood,hyperparameters has training likelihood,0.4860915541648865
translation,211,5,model,three new unsupervised models,to discover,latent structures,three new unsupervised models to discover latent structures,0.6571362614631653
translation,211,5,model,latent structures,in,task - oriented dialogues,latent structures in task - oriented dialogues,0.5166785717010498
translation,211,5,model,model,propose,three new unsupervised models,model propose three new unsupervised models,0.6928853988647461
translation,211,29,model,three model variants,link,topics and states,three model variants link topics and states,0.6838857531547546
translation,211,29,model,model,propose,three model variants,model propose three model variants,0.6498361825942993
translation,211,99,model,words,from,topic model ( tm ),words from topic model ( tm ),0.5877657532691956
translation,211,99,model,model,generate,words,model generate words,0.6759447455406189
translation,211,31,results,decoupling,of,states and topics,decoupling of states and topics,0.5998981595039368
translation,211,31,results,decoupling,gives,our models,decoupling gives our models,0.6359794735908508
translation,211,31,results,states and topics,gives,our models,states and topics gives our models,0.6120588779449463
translation,211,31,results,our models,has,more expressive power,our models has more expressive power,0.5467596650123596
translation,211,31,results,potential,has,to be more data efficient,potential has to be more data efficient,0.575715184211731
translation,211,209,results,our models,yield,good or better likelihood,our models yield good or better likelihood,0.7341317534446716
translation,211,209,results,good or better likelihood,than,lm - hmm and lm - hmms models,good or better likelihood than lm - hmm and lm - hmms models,0.5304996371269226
translation,211,209,results,good or better likelihood,on,both datasets,good or better likelihood on both datasets,0.4863578677177429
translation,211,209,results,results,has,our models,results has our models,0.5733726620674133
translation,211,210,results,perform better,than,tm - hmm,perform better than tm - hmm,0.6288278698921204
translation,211,210,results,tm - hmm,on,techsupport,tm - hmm on techsupport,0.6145193576812744
translation,211,210,results,proposed models,has,tm - hmms and tm - hmmss,proposed models has tm - hmms and tm - hmmss,0.6068487763404846
translation,211,210,results,tm - hmms and tm - hmmss,has,perform better,tm - hmms and tm - hmmss has perform better,0.6143542528152466
translation,211,211,results,marginal benefit,of,tm - hmmss,marginal benefit of tm - hmmss,0.626895010471344
translation,211,211,results,tm - hmmss,over,tm - hmm,tm - hmmss over tm - hmm,0.6355618238449097
translation,211,211,results,greater,on,tech - support dataset,greater on tech - support dataset,0.5584254264831543
translation,211,231,results,tm - hmm,out - performs,all other models,tm - hmm out - performs all other models,0.7422295212745667
translation,211,231,results,results,has,tm - hmm,results has tm - hmm,0.5577008724212646
translation,211,233,results,"lm - hmms , tm -hmms and tm - hmmss models",perform,considerably well,"lm - hmms , tm -hmms and tm - hmmss models perform considerably well",0.569682240486145
translation,211,233,results,considerably well,on,bustime,considerably well on bustime,0.5509118437767029
translation,211,233,results,considerably well,not on,techsupport data,considerably well not on techsupport data,0.6600582003593445
translation,211,233,results,results,has,"lm - hmms , tm -hmms and tm - hmmss models","results has lm - hmms , tm -hmms and tm - hmmss models",0.5231831073760986
translation,211,236,results,both quantitative evaluation measures,has,our models,both quantitative evaluation measures has our models,0.5929276347160339
translation,211,236,results,our models,has,advance,our models has advance,0.5858029723167419
translation,211,236,results,advance,has,state - of - the - art,advance has state - of - the - art,0.5577608942985535
translation,211,236,results,results,under,both quantitative evaluation measures,results under both quantitative evaluation measures,0.5544394254684448
translation,212,183,ablation-analysis,findings,suggest,deceptive behavior,findings suggest deceptive behavior,0.59725421667099
translation,212,183,ablation-analysis,findings,suggest,deceptive language,findings suggest deceptive language,0.6066429018974304
translation,212,183,ablation-analysis,deceptive language,affected by,person 's individual characteristics,deceptive language affected by person 's individual characteristics,0.7338963150978088
translation,212,183,ablation-analysis,person 's individual characteristics,including,gender,person 's individual characteristics including gender,0.6610252261161804
translation,212,183,ablation-analysis,person 's individual characteristics,including,native language,person 's individual characteristics including native language,0.6098578572273254
translation,212,183,ablation-analysis,ablation analysis,suggest,deceptive behavior,ablation analysis suggest deceptive behavior,0.6146694421768188
translation,212,183,ablation-analysis,ablation analysis,suggest,deceptive language,ablation analysis suggest deceptive language,0.6073842644691467
translation,212,183,ablation-analysis,ablation analysis,has,findings,ablation analysis has findings,0.5422948002815247
translation,212,203,ablation-analysis,ablation analysis,Adding,individual traits information,ablation analysis Adding individual traits information,0.6886239647865295
translation,212,208,ablation-analysis,individual traits,including,gender,individual traits including gender,0.6677607297897339
translation,212,208,ablation-analysis,individual traits,including,native language,individual traits including native language,0.6114095449447632
translation,212,208,ablation-analysis,individual traits,including,personality scores,individual traits including personality scores,0.5632157325744629
translation,212,208,ablation-analysis,individual traits,helpful in,deception classification,individual traits helpful in deception classification,0.628752589225769
translation,212,208,ablation-analysis,deception classification,condition where,contextual information,deception classification condition where contextual information,0.719953179359436
translation,212,208,ablation-analysis,contextual information,has,is not available,contextual information has is not available,0.5957488417625427
translation,212,208,ablation-analysis,ablation analysis,seems that,individual traits,ablation analysis seems that individual traits,0.6874552369117737
translation,212,209,ablation-analysis,contextual information,is,available,contextual information is available,0.5515949726104736
translation,212,209,ablation-analysis,additional lexical content,is,more useful,additional lexical content is more useful,0.5468801259994507
translation,212,209,ablation-analysis,more useful,than,individual traits,more useful than individual traits,0.5600071549415588
translation,212,209,ablation-analysis,contextual information,has,additional lexical content,contextual information has additional lexical content,0.5679053664207458
translation,212,209,ablation-analysis,available,has,additional lexical content,available has additional lexical content,0.6138584017753601
translation,212,209,ablation-analysis,ablation analysis,When,contextual information,ablation analysis When contextual information,0.6478672027587891
translation,212,196,results,results,has,random for-est classifier,results has random for-est classifier,0.5813246369361877
translation,212,199,results,best performance,obtained using,liwc features,best performance obtained using liwc features,0.601885974407196
translation,212,199,results,liwc features,extracted from,multiple turns,liwc features extracted from multiple turns,0.555217444896698
translation,212,199,results,best performance,has,72.74 f1 -score,best performance has 72.74 f1 -score,0.5225554704666138
translation,212,199,results,results,has,best performance,results has best performance,0.5759831070899963
translation,212,201,results,performance,of,classifiers,performance of classifiers,0.6148607730865479
translation,212,201,results,classifiers,trained on,multiple turns,classifiers trained on multiple turns,0.7606949806213379
translation,212,201,results,classifiers,trained on,single turns,classifiers trained on single turns,0.7731704711914062
translation,212,201,results,consistently better,than,all feature sets,consistently better than all feature sets,0.586195170879364
translation,212,201,results,single turns,for,all feature sets,single turns for all feature sets,0.6263546347618103
translation,212,201,results,results,has,performance,results has performance,0.5972660779953003
translation,212,202,results,multiple turns,combining,lexical,multiple turns combining lexical,0.7917128801345825
translation,212,202,results,liwc features,better than,lexical feature set,liwc features better than lexical feature set,0.6567882299423218
translation,212,202,results,liwc features,combining,lexical,liwc features combining lexical,0.695955753326416
translation,212,202,results,lexical,with,liwc features,lexical with liwc features,0.6342149972915649
translation,212,202,results,lexical,with,liwc features,lexical with liwc features,0.6342149972915649
translation,212,202,results,did not improve,performance of,liwc features,did not improve performance of liwc features,0.68428635597229
translation,212,202,results,multiple turns,has,liwc features,multiple turns has liwc features,0.5883004069328308
translation,212,202,results,liwc features,has,did not improve,liwc features has did not improve,0.6057469844818115
translation,212,202,results,results,For,multiple turns,results For multiple turns,0.6136594414710999
translation,212,204,results,best results ( 70.87 f1 -score ),obtained using,combination of liwc + lexical + individual features,best results ( 70.87 f1 -score ) obtained using combination of liwc + lexical + individual features,0.5948074460029602
translation,212,204,results,first turn,has,best results ( 70.87 f1 -score ),first turn has best results ( 70.87 f1 -score ),0.5284152626991272
translation,212,204,results,results,considering,first turn,results considering first turn,0.6544910669326782
translation,212,205,results,lexical features,slightly better than,liwc features,lexical features slightly better than liwc features,0.6725184321403503
translation,212,205,results,lexical features,adding,individual traits,lexical features adding individual traits,0.6523177027702332
translation,212,205,results,individual traits,helped,both feature sets,individual traits helped both feature sets,0.5481333136558533
translation,212,205,results,first turns segmentation,has,lexical features,first turns segmentation has lexical features,0.5508392453193665
translation,212,205,results,results,Using,first turns segmentation,results Using first turns segmentation,0.6348574161529541
translation,212,206,results,liwc and lexical features,better than,each,liwc and lexical features better than each,0.7252787947654724
translation,212,206,results,liwc and lexical features,better than,its own,liwc and lexical features better than its own,0.7255027890205383
translation,213,6,model,affect-enriched dialogue act classifier,models,facial expressions of users,affect-enriched dialogue act classifier models facial expressions of users,0.6751736998558044
translation,213,21,model,affect-enriched dialogue act classification,leverages,knowledge of users ' facial expressions,affect-enriched dialogue act classification leverages knowledge of users ' facial expressions,0.6959067583084106
translation,213,21,model,knowledge of users ' facial expressions,during,computer -mediated textual human-human dialogue,knowledge of users ' facial expressions during computer -mediated textual human-human dialogue,0.6513449549674988
translation,213,122,results,accuracy,of,model,accuracy of model,0.628264307975769
translation,213,122,results,model,across,ten folds,model across ten folds,0.7512586116790771
translation,213,122,results,two of the affect-enriched classifiers,exhibited,statistically significantly better performance,two of the affect-enriched classifiers exhibited statistically significantly better performance,0.6462296843528748
translation,213,122,results,accuracy,has,two of the affect-enriched classifiers,accuracy has two of the affect-enriched classifiers,0.5625428557395935
translation,213,122,results,model,has,two of the affect-enriched classifiers,model has two of the affect-enriched classifiers,0.5651458501815796
translation,213,122,results,results,considering,accuracy,results considering accuracy,0.6801744699478149
translation,213,123,results,facial expression features,significantly improved,classification accuracy,facial expression features significantly improved classification accuracy,0.6500972509384155
translation,213,123,results,classification accuracy,compared to,model,classification accuracy compared to model,0.6340481042861938
translation,213,123,results,model,learned without,affective features,model learned without affective features,0.663640558719635
translation,213,123,results,grounding,has,request for feedback ( rf ),grounding has request for feedback ( rf ),0.6062139272689819
translation,213,123,results,grounding,has,facial expression features,grounding has facial expression features,0.5382642149925232
translation,213,123,results,request for feedback ( rf ),has,facial expression features,request for feedback ( rf ) has facial expression features,0.5517809391021729
translation,213,123,results,results,For,grounding,results For grounding,0.6647322177886963
translation,213,138,results,leveraging knowledge,of,user affect,leveraging knowledge of user affect,0.5579221844673157
translation,213,138,results,leveraging knowledge,may,improve,leveraging knowledge may improve,0.6457383632659912
translation,213,138,results,performance,of,dialogue act classification models,performance of dialogue act classification models,0.5537704229354858
translation,213,138,results,improve,has,performance,improve has performance,0.5578044652938843
translation,213,138,results,results,demonstrate,leveraging knowledge,results demonstrate leveraging knowledge,0.5691080689430237
translation,214,34,baselines,dialogue pl task,as,multi-label classification problem,dialogue pl task as multi-label classification problem,0.4875353276729584
translation,214,97,baselines,supervised learning,where,dialogue action selection task,supervised learning where dialogue action selection task,0.5284410119056702
translation,214,97,baselines,dialogue action selection task,regarded as,multi-label classification problem,dialogue action selection task regarded as multi-label classification problem,0.5110939741134644
translation,214,98,baselines,reinforcement learning ( rl ),where,reward function,reinforcement learning ( rl ) where reward function,0.6114436388015747
translation,214,98,baselines,reward function,is,handcrafted,reward function is handcrafted,0.5679724216461182
translation,214,98,baselines,end,of,dialogue,end of dialogue,0.6500382423400879
translation,214,98,baselines,dialogue agent,accomplishes,task,dialogue agent accomplishes task,0.6933566331863403
translation,214,98,baselines,task,within,t turns,task within t turns,0.6985461115837097
translation,214,98,baselines,baselines,has,reinforcement learning ( rl ),baselines has reinforcement learning ( rl ),0.5537581443786621
translation,214,101,baselines,three methods,including,gp - mbcm,three methods including gp - mbcm,0.674550473690033
translation,214,101,baselines,three methods,including,"acer ( wang et al. , 2016 )","three methods including acer ( wang et al. , 2016 )",0.6178467273712158
translation,214,101,baselines,three methods,including,ppo,three methods including ppo,0.7010038495063782
translation,214,101,baselines,three methods,including,adversarial learning,three methods including adversarial learning,0.5979123711585999
translation,214,101,baselines,adversarial learning,where,dialogue agent,adversarial learning where dialogue agent,0.6154131293296814
translation,214,101,baselines,dialogue agent,trained with,user simulator,dialogue agent trained with user simulator,0.7172639966011047
translation,214,101,baselines,ppo,has,"schulman et al. , 2017 )","ppo has schulman et al. , 2017 )",0.5734841227531433
translation,214,172,baselines,method diaadv,regarded as,extension,method diaadv regarded as extension,0.6164388060569763
translation,214,172,baselines,extension,of,diamultidense,extension of diamultidense,0.6449137926101685
translation,214,172,baselines,diamultidense,by adding,classifier,diamultidense by adding classifier,0.7599071860313416
translation,214,172,baselines,classifier,to provide,stronger training signal,classifier to provide stronger training signal,0.662754237651825
translation,214,172,baselines,baselines,has,method diaadv,baselines has method diaadv,0.5807609558105469
translation,214,151,experiments,remarkable performance,with respect to,different metrics,remarkable performance with respect to different metrics,0.6616904139518738
translation,214,151,experiments,supervised learning method,has,diamulti-,supervised learning method has diamulti-,0.6197078824043274
translation,214,151,experiments,diamulti-,has,dense,diamulti- has dense,0.6553784012794495
translation,214,109,hyperparameters,beam search,to predict,action combinations,beam search to predict action combinations,0.7234382629394531
translation,214,109,hyperparameters,beam size,set to,6,beam size set to 6,0.7686007022857666
translation,214,109,hyperparameters,hyperparameters,use,beam search,hyperparameters use beam search,0.6655463576316833
translation,214,110,hyperparameters,action embedding size,set to,30,action embedding size set to 30,0.716011106967926
translation,214,110,hyperparameters,hidden size,of,gru,hidden size of gru,0.612912118434906
translation,214,110,hyperparameters,gru,is,50,gru is 50,0.6837201118469238
translation,214,110,hyperparameters,hyperparameters,has,action embedding size,hyperparameters has action embedding size,0.5007083415985107
translation,214,110,hyperparameters,hyperparameters,has,hidden size,hyperparameters has hidden size,0.535134494304657
translation,214,113,hyperparameters,discrete action representation,implemented,  straight - through   gumbel -softmax estimator,discrete action representation implemented   straight - through   gumbel -softmax estimator,0.6807811856269836
translation,214,113,hyperparameters,hyperparameters,To sample,discrete action representation,hyperparameters To sample discrete action representation,0.6338250637054443
translation,214,9,model,traditional multilabel classification solution,for,dialogue policy learning,traditional multilabel classification solution for dialogue policy learning,0.5795404314994812
translation,214,9,model,traditional multilabel classification solution,extended by adding,dense layers,traditional multilabel classification solution extended by adding dense layers,0.6945974230766296
translation,214,9,model,dense layers,to improve,dialogue agent performance,dense layers to improve dialogue agent performance,0.6267880797386169
translation,214,9,model,model,has,traditional multilabel classification solution,model has traditional multilabel classification solution,0.5380940437316895
translation,214,33,model,first method,utilizes,action decoder,first method utilizes action decoder,0.6082733273506165
translation,214,33,model,action decoder,to predict,dialogue combinations,action decoder to predict dialogue combinations,0.6928286552429199
translation,214,33,model,model,has,first method,model has first method,0.5816154479980469
translation,214,35,model,dense layer,to,each action label,dense layer to each action label,0.5078096985816956
translation,214,35,model,each action label,in,action space,each action label in action space,0.4680973291397095
translation,214,35,model,model,assign,dense layer,model assign dense layer,0.734810471534729
translation,214,36,model,adversarial learning method,for,dialogue pl,adversarial learning method for dialogue pl,0.586811900138855
translation,214,36,model,dialogue pl,without utilizing,rl,dialogue pl without utilizing rl,0.6962947249412537
translation,214,36,model,model,propose,adversarial learning method,model propose adversarial learning method,0.6560080647468567
translation,214,37,model,loss,from,reward model,loss from reward model,0.5523602366447449
translation,214,37,model,loss,utilize,gumbel - softmax,loss utilize gumbel - softmax,0.5790820121765137
translation,214,37,model,reward model,to,policy model,reward model to policy model,0.5273003578186035
translation,214,37,model,reward model,to,policy model and the reward model,reward model to policy model and the reward model,0.5806002020835876
translation,214,37,model,gumbel - softmax,to connect,policy model and the reward model,gumbel - softmax to connect policy model and the reward model,0.5878621935844421
translation,214,37,model,model,To backpropagate,loss,model To backpropagate loss,0.7566814422607422
translation,214,37,model,model,utilize,gumbel - softmax,model utilize gumbel - softmax,0.596230149269104
translation,214,112,model,policy network,of,diaadv,policy network of diaadv,0.6051554679870605
translation,214,112,model,twolayer mlp,to extract,state features,twolayer mlp to extract state features,0.6745233535766602
translation,214,112,model,state features,followed by,166 dense layers,state features followed by 166 dense layers,0.6434295773506165
translation,214,112,model,policy network,has,twolayer mlp,policy network has twolayer mlp,0.5707342624664307
translation,214,112,model,diaadv,has,twolayer mlp,diaadv has twolayer mlp,0.6036914587020874
translation,214,112,model,model,For,policy network,model For policy network,0.6093758344650269
translation,214,141,results,diaadv,manages to achieve,highest performance,diaadv manages to achieve highest performance,0.6870503425598145
translation,214,141,results,highest performance,by,6 %,highest performance by 6 %,0.5893685221672058
translation,214,141,results,highest performance,compared to,second highest method gdpl,highest performance compared to second highest method gdpl,0.6420853734016418
translation,214,141,results,success rate,has,diaadv,success rate has diaadv,0.5904108881950378
translation,214,141,results,results,With respect to,success rate,results With respect to success rate,0.5886430144309998
translation,214,142,results,diaadv,not able to beat,gdpl,diaadv not able to beat gdpl,0.674680769443512
translation,214,142,results,gdpl,in terms of,average turns,gdpl in terms of average turns,0.6817594766616821
translation,214,142,results,results,has,diaadv,results has diaadv,0.5162973403930664
translation,214,152,results,diamulticlass,joining of,dense layers,diamulticlass joining of dense layers,0.6884692311286926
translation,214,152,results,diamulticlass,on,all the metrics,diamulticlass on all the metrics,0.5576395988464355
translation,214,152,results,dense layers,as in,diamultidense,dense layers as in diamultidense,0.6482614278793335
translation,214,152,results,diamulticlass,on,all the metrics,diamulticlass on all the metrics,0.5576395988464355
translation,214,152,results,traditional solution,has,diamulticlass,traditional solution has diamulticlass,0.6207289099693298
translation,214,152,results,results,Compared to,traditional solution,results Compared to traditional solution,0.7026735544204712
translation,214,153,results,higher f1 score,than,diaadv,higher f1 score than diaadv,0.5903197526931763
translation,214,156,results,diamultidense,achieves,highest performance,diamultidense achieves highest performance,0.6996592283248901
translation,214,156,results,diamultidense,using,fewest parameters,diamultidense using fewest parameters,0.712578535079956
translation,214,156,results,highest performance,among,three methods,highest performance among three methods,0.5874106287956238
translation,214,156,results,results,has,diamultidense,results has diamultidense,0.5558724999427795
translation,214,181,results,diaseq,is,completely supervised learning method,diaseq is completely supervised learning method,0.5896652936935425
translation,214,181,results,diaseq,has,outperforms,diaseq has outperforms,0.6464789509773254
translation,214,181,results,completely supervised learning method,has,outperforms,completely supervised learning method has outperforms,0.6061050891876221
translation,214,181,results,outperforms,has,gdpl,outperforms has gdpl,0.6128548383712769
translation,214,181,results,results,has,diaseq,results has diaseq,0.5194017291069031
translation,214,189,results,diamulticlass,gets,half the success rate,diamulticlass gets half the success rate,0.6511268019676208
translation,214,189,results,half the success rate,compared to,original performance,half the success rate compared to original performance,0.6647220849990845
translation,214,189,results,supervised learning agents,has,diamulticlass,supervised learning agents has diamulticlass,0.5968766808509827
translation,214,189,results,expert dialogue pairs,has,diamulticlass,expert dialogue pairs has diamulticlass,0.6363714337348938
translation,214,189,results,results,With respect to,supervised learning agents,results With respect to supervised learning agents,0.658021092414856
translation,214,190,results,30 % more dialogue pairs,to,training set,30 % more dialogue pairs to training set,0.5333484411239624
translation,214,190,results,diamulticlass,achieve,same performance,diamulticlass achieve same performance,0.697886049747467
translation,214,190,results,59 %,with,original success rate,59 % with original success rate,0.609168291091919
translation,214,190,results,30 % more dialogue pairs,has,diamulticlass,30 % more dialogue pairs has diamulticlass,0.6048856377601624
translation,214,190,results,same performance,has,59 %,same performance has 59 %,0.5861965417861938
translation,214,190,results,original success rate,has,57.2 %,original success rate has 57.2 %,0.5269198417663574
translation,214,190,results,results,adding,30 % more dialogue pairs,results adding 30 % more dialogue pairs,0.6571575999259949
translation,214,193,results,diamultidense,achieves,best,diamultidense achieves best,0.7432634830474854
translation,214,193,results,diamultidense,achieves,diaadv,diamultidense achieves diaadv,0.7207087874412537
translation,214,193,results,results,has,diamultidense,results has diamultidense,0.5558724999427795
translation,214,194,results,diaadv,help to reduce,dialogue turns,diaadv help to reduce dialogue turns,0.6864920854568481
translation,214,194,results,improving,has,success rate,improving has success rate,0.580021321773529
translation,214,194,results,results,has,diaadv,results has diaadv,0.5162973403930664
translation,215,199,results,analysis of variance,with,wizard,analysis of variance with wizard,0.6205536723136902
translation,215,199,results,wizard,as,predictor and accuracy,wizard as predictor and accuracy,0.5528669357299805
translation,215,199,results,predictor and accuracy,as,dependent variable,predictor and accuracy as dependent variable,0.4905608296394348
translation,215,199,results,dependent variable,is,highly significant ( p=0.0006 ),dependent variable is highly significant ( p=0.0006 ),0.4644268751144409
translation,215,199,results,results,has,analysis of variance,results has analysis of variance,0.4773741364479065
translation,215,205,results,results,in,modeling wizards ' actions,results in modeling wizards ' actions,0.4824717044830322
translation,215,205,results,uniform,across,three learning methods,uniform across three learning methods,0.6979732513427734
translation,215,205,results,uniform,gauged by,accuracy and f measure,uniform gauged by accuracy and f measure,0.7714537382125854
translation,215,205,results,results,in,modeling wizards ' actions,results in modeling wizards ' actions,0.4824717044830322
translation,215,205,results,results,has,results,results has results,0.48582205176353455
translation,215,206,results,accuracy,of,75.2 %,accuracy of 75.2 %,0.5569097399711609
translation,215,206,results,f measures,of,0.83,f measures of 0.83,0.5538804531097412
translation,215,206,results,f measures,of,0.72,f measures of 0.72,0.5490903854370117
translation,215,206,results,f measures,for,firm versus tentative choices,f measures for firm versus tentative choices,0.578075647354126
translation,215,206,results,0.83,for,firm choices,0.83 for firm choices,0.5436248779296875
translation,215,206,results,0.72,for,tentative choices,0.72 for tentative choices,0.5920637249946594
translation,215,206,results,decision tree accuracy,was,82.2 %,decision tree accuracy was 82.2 %,0.5797000527381897
translation,215,206,results,f measures,for,firm versus tentative choices,f measures for firm versus tentative choices,0.578075647354126
translation,215,206,results,combined wizard data,has,logistic regression,combined wizard data has logistic regression,0.5540764331817627
translation,215,206,results,results,For,combined wizard data,results For combined wizard data,0.5969170331954956
translation,215,246,results,uniformly good,at choosing,correct title,uniformly good at choosing correct title,0.683750331401825
translation,215,246,results,correct title,when,present,correct title when present,0.6516504287719727
translation,215,246,results,most were overly eager,to identify,title,most were overly eager to identify title,0.7240729928016663
translation,215,246,results,title,not among,candidates,title not among candidates,0.7081189751625061
translation,215,246,results,results,has,our wizards,results has our wizards,0.5895347595214844
translation,216,118,baselines,"data processing , model training and decoding",employ,few strategies,"data processing , model training and decoding employ few strategies",0.5753195285797119
translation,216,118,baselines,few strategies,that improve,response quality,few strategies that improve response quality,0.6523298025131226
translation,216,118,baselines,few strategies,Remove,training examples,few strategies Remove training examples,0.6754523515701294
translation,216,118,baselines,training examples,with,length of responses,training examples with length of responses,0.5660209059715271
translation,216,118,baselines,length of responses,shorter than,threshold,length of responses shorter than threshold,0.8046558499336243
translation,216,176,baselines,hierarchical encoder,with,structure,hierarchical encoder with structure,0.6779603362083435
translation,216,176,baselines,neural representations,of,dialogues,neural representations of dialogues,0.5998148322105408
translation,216,176,baselines,dialogues,obtained by,neural model,dialogues obtained by neural model,0.6561090350151062
translation,216,176,baselines,hierarchical neural,has,hierarchical encoder,hierarchical neural has hierarchical encoder,0.5951752066612244
translation,216,176,baselines,svm + neural + multi-lex -features,has,svm model,svm + neural + multi-lex -features has svm model,0.5707852244377136
translation,216,176,baselines,baselines,has,hierarchical neural,baselines has hierarchical neural,0.602315366268158
translation,216,183,baselines,standard seq2seq models,using,greedy decoding ( mlegreedy ),standard seq2seq models using greedy decoding ( mlegreedy ),0.6477910280227661
translation,216,183,baselines,standard seq2seq models,using,beam-search ( mle + bs ),standard seq2seq models using beam-search ( mle + bs ),0.6577596664428711
translation,216,183,baselines,standard seq2seq models,using,sampling,standard seq2seq models using sampling,0.669113039970398
translation,216,183,baselines,mutual information reranking model,with,two algorithmic variations,mutual information reranking model with two algorithmic variations,0.621180534362793
translation,216,183,baselines,mmi + p(,in which,large n-best list,mmi + p( in which large n-best list,0.5925553441047668
translation,216,113,experiments,seq2seq model,with,attention mechanism,seq2seq model with attention mechanism,0.6091710329055786
translation,216,113,experiments,attention mechanism,on,opensubtitles dataset,attention mechanism on opensubtitles dataset,0.5031523704528809
translation,216,117,hyperparameters,negative examples,generated using,beamsearch,negative examples generated using beamsearch,0.6804450750350952
translation,216,117,hyperparameters,beamsearch,with,mutual information reranking,beamsearch with mutual information reranking,0.6327705383300781
translation,216,117,hyperparameters,other half,generated from,sampling,other half generated from sampling,0.6821172833442688
translation,216,134,hyperparameters,hyperparameters,has,stop words,hyperparameters has stop words,0.5180882811546326
translation,216,5,model,reinforcement learning ( rl ) problem,jointly train,two systems,reinforcement learning ( rl ) problem jointly train two systems,0.6414825320243835
translation,216,5,model,generative model,to produce,response sequences,generative model to produce response sequences,0.7324424982070923
translation,216,5,model,discriminator -analagous,to,human evaluator,discriminator -analagous to human evaluator,0.5315316319465637
translation,216,5,model,human evaluator,in,turing test,human evaluator in turing test,0.5261768698692322
translation,216,5,model,human evaluator,to distinguish between,machine - generated ones,human evaluator to distinguish between machine - generated ones,0.6596923470497131
translation,216,5,model,two systems,has,generative model,two systems has generative model,0.576092004776001
translation,216,6,model,outputs,from,discriminator,outputs from discriminator,0.5995899438858032
translation,216,6,model,outputs,used as,rewards,outputs used as rewards,0.5695458650588989
translation,216,6,model,discriminator,used as,rewards,discriminator used as rewards,0.6271204948425293
translation,216,6,model,rewards,for,generative model,rewards for generative model,0.589403510093689
translation,216,6,model,rewards,pushing,system,rewards pushing system,0.7565006613731384
translation,216,6,model,system,to generate,dialogues,system to generate dialogues,0.702350378036499
translation,216,6,model,dialogues,mostly resemble,human dialogues,dialogues mostly resemble human dialogues,0.6637248992919922
translation,216,6,model,model,has,outputs,model has outputs,0.5564337968826294
translation,216,7,model,adversarial evaluation,uses,success,adversarial evaluation uses success,0.6053800582885742
translation,216,7,model,success,in fooling,adversary,success in fooling adversary,0.704426646232605
translation,216,205,model,adversarial training approach,for,response generation,adversarial training approach for response generation,0.590063214302063
translation,216,206,model,model,in,reinforcement learning,model in reinforcement learning,0.5015341639518738
translation,216,206,model,model,framework of,reinforcement learning,model framework of reinforcement learning,0.6232540011405945
translation,216,206,model,model,train,generator,model train generator,0.7315921187400818
translation,216,206,model,generator,based on,signal,generator based on signal,0.673744261264801
translation,216,206,model,signal,from,discriminator,signal from discriminator,0.5943869352340698
translation,216,206,model,response sequences,indistinguishable from,human- generated dialogues,response sequences indistinguishable from human- generated dialogues,0.6731457114219666
translation,216,206,model,model,cast,model,model cast model,0.7025041580200195
translation,216,206,model,model,framework of,reinforcement learning,model framework of reinforcement learning,0.6232540011405945
translation,216,206,model,model,train,generator,model train generator,0.7315921187400818
translation,216,178,results,hierarchical neural evaluator,is,more reliable,hierarchical neural evaluator is more reliable,0.537004292011261
translation,216,178,results,more reliable,concatenating,sentence - level representations,more reliable concatenating sentence - level representations,0.682417094707489
translation,216,178,results,hierarchical neural evaluator,has,),hierarchical neural evaluator has ),0.6148186922073364
translation,216,186,results,decoding,using,sampling,decoding using sampling,0.7197922468185425
translation,216,186,results,significantly higher adversuc number,than,all the rest models,significantly higher adversuc number than all the rest models,0.5563939809799194
translation,216,190,results,different baselines,find that,mmi + p ( t|s ),different baselines find that mmi + p ( t|s ),0.6611637473106384
translation,216,190,results,mmi + p ( t|s ),better than,mle - greedy,mmi + p ( t|s ) better than mle - greedy,0.7456561923027039
translation,216,190,results,results,comparing,different baselines,results comparing different baselines,0.723951518535614
translation,216,192,results,two proposed adversarial algorithms,achieve,better performance,two proposed adversarial algorithms achieve better performance,0.6241012811660767
translation,216,192,results,better performance,than,baselines,better performance than baselines,0.559979259967804
translation,216,192,results,results,has,two proposed adversarial algorithms,results has two proposed adversarial algorithms,0.5647813081741333
translation,217,96,ablation-analysis,cross entropy,leads to,high ejce,cross entropy leads to high ejce,0.6918114423751831
translation,217,96,ablation-analysis,ablation analysis,has,cross entropy,ablation analysis has cross entropy,0.5659502148628235
translation,217,97,ablation-analysis,label smoothing,reduces,ejce,label smoothing reduces ejce,0.6946406364440918
translation,217,97,ablation-analysis,label smoothing,leading to,negligible drop,label smoothing leading to negligible drop,0.7228468656539917
translation,217,97,ablation-analysis,negligible drop,in,accuracy,negligible drop in accuracy,0.5475136041641235
translation,217,97,ablation-analysis,ablation analysis,has,label smoothing,ablation analysis has label smoothing,0.5314321517944336
translation,217,107,ablation-analysis,l2 norm,see that,uncertainty estimates,l2 norm see that uncertainty estimates,0.6222323775291443
translation,217,107,ablation-analysis,uncertainty estimates,of,belief tracking models,uncertainty estimates of belief tracking models,0.5255321860313416
translation,217,107,ablation-analysis,uncertainty estimates,compensate for,lower joint goal accuracy,uncertainty estimates compensate for lower joint goal accuracy,0.6528530716896057
translation,217,107,ablation-analysis,ablation analysis,analysing,l2 norm,ablation analysis analysing l2 norm,0.6883533000946045
translation,217,71,baselines,baseline belief tracker,use,sumbt model architecture,baseline belief tracker use sumbt model architecture,0.5693485140800476
translation,217,71,baselines,sumbt model architecture,uses,"bert ( devlin et al. , 2018 )","sumbt model architecture uses bert ( devlin et al. , 2018 )",0.5558674931526184
translation,217,71,baselines,"bert ( devlin et al. , 2018 )",as,turn encoder,"bert ( devlin et al. , 2018 ) as turn encoder",0.5396523475646973
translation,217,71,baselines,"bert ( devlin et al. , 2018 )",as,multi-head attention,"bert ( devlin et al. , 2018 ) as multi-head attention",0.4934789836406708
translation,217,71,baselines,multi-head attention,for,slot candidate matching,multi-head attention for slot candidate matching,0.5930027365684509
translation,217,71,baselines,baselines,For,baseline belief tracker,baselines For baseline belief tracker,0.5566374063491821
translation,217,73,hyperparameters,training,using,bayesian matching,training using bayesian matching,0.6840519309043884
translation,217,73,hyperparameters,training,use,scaling coefficient,training use scaling coefficient,0.6737384796142578
translation,217,73,hyperparameters,bayesian matching,use,scaling coefficient,bayesian matching use scaling coefficient,0.6349154114723206
translation,217,73,hyperparameters,bayesian matching,use,smoothing coefficient,bayesian matching use smoothing coefficient,0.6251986026763916
translation,217,73,hyperparameters,scaling coefficient,of,? = 0.003,scaling coefficient of ? = 0.003,0.5778598785400391
translation,217,73,hyperparameters,scaling coefficient,for,label smoothing,scaling coefficient for label smoothing,0.6102451086044312
translation,217,73,hyperparameters,smoothing coefficient,of,? = 0.05,smoothing coefficient of ? = 0.05,0.5817628502845764
translation,217,73,hyperparameters,label smoothing,has,smoothing coefficient,label smoothing has smoothing coefficient,0.5306433439254761
translation,217,73,hyperparameters,hyperparameters,When,training,hyperparameters When training,0.6092633008956909
translation,217,73,hyperparameters,hyperparameters,for,label smoothing,hyperparameters for label smoothing,0.540507972240448
translation,217,74,hyperparameters,ensemble belief tracker,train,10 identical independent models,ensemble belief tracker train 10 identical independent models,0.5946292877197266
translation,217,74,hyperparameters,10 identical independent models,with,sub-sample,10 identical independent models with sub-sample,0.6338229775428772
translation,217,74,hyperparameters,sub-sample,of,7500 dialogues,sub-sample of 7500 dialogues,0.6150026321411133
translation,217,74,hyperparameters,hyperparameters,For,ensemble belief tracker,hyperparameters For ensemble belief tracker,0.5404877662658691
translation,217,76,hyperparameters,bert - base-uncased model,from,"pytorch transformers ( wolf et al. , 2019 )","bert - base-uncased model from pytorch transformers ( wolf et al. , 2019 )",0.5609506964683533
translation,217,76,hyperparameters,bert - base-uncased model,for,turn embedding,bert - base-uncased model for turn embedding,0.663882851600647
translation,217,77,hyperparameters,gated recurrent unit,with,hidden dimension,gated recurrent unit with hidden dimension,0.6362117528915405
translation,217,77,hyperparameters,gated recurrent unit,with,euclidean distance,gated recurrent unit with euclidean distance,0.6469860076904297
translation,217,77,hyperparameters,300,for,latent tracking,300 for latent tracking,0.6440193057060242
translation,217,77,hyperparameters,euclidean distance,for,value candidate scoring,euclidean distance for value candidate scoring,0.6181142926216125
translation,217,77,hyperparameters,hidden dimension,has,300,hidden dimension has 300,0.618621289730072
translation,217,77,hyperparameters,hyperparameters,use,gated recurrent unit,hyperparameters use gated recurrent unit,0.5986553430557251
translation,217,78,hyperparameters,training,use,learning rate,training use learning rate,0.6668137311935425
translation,217,78,hyperparameters,learning rate,of,5e ? 5,learning rate of 5e ? 5,0.6546747088432312
translation,217,78,hyperparameters,learning rate,in combination with,linear learning rate scheduler,learning rate in combination with linear learning rate scheduler,0.5672224760055542
translation,217,78,hyperparameters,5e ? 5,in combination with,linear learning rate scheduler,5e ? 5 in combination with linear learning rate scheduler,0.6190047264099121
translation,217,78,hyperparameters,warm - up proportion,set to,0.1,warm - up proportion set to 0.1,0.6531150937080383
translation,217,78,hyperparameters,linear learning rate scheduler,has,warm - up proportion,linear learning rate scheduler has warm - up proportion,0.5381671786308289
translation,217,78,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
translation,217,79,hyperparameters,dropout rate,of,0.3,dropout rate of 0.3,0.5798346996307373
translation,217,79,hyperparameters,training,performed for,100 epochs,training performed for 100 epochs,0.6080447435379028
translation,217,79,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,217,79,hyperparameters,hyperparameters,has,training,hyperparameters has training,0.519983172416687
translation,217,98,results,bayesian matching,has,underperformed,bayesian matching has underperformed,0.5854060649871826
translation,217,98,results,results,has,bayesian matching,results has bayesian matching,0.5094051957130432
translation,217,100,results,ensemble methods,produce,very promising results,ensemble methods produce very promising results,0.6100130677223206
translation,217,100,results,very promising results,for,accuracy and calibration of the model,very promising results for accuracy and calibration of the model,0.6063793897628784
translation,217,100,results,results,has,ensemble methods,results has ensemble methods,0.4917691648006439
translation,218,69,baselines,hyst,is,hybrid approach,hyst is hybrid approach,0.6185634732246399
translation,218,69,baselines,hybrid approach,combines,joint tracking fixed vocabulary approach,hybrid approach combines joint tracking fixed vocabulary approach,0.6552301645278931
translation,218,69,baselines,hybrid approach,combines,open vocabulary approach,hybrid approach combines open vocabulary approach,0.7256698608398438
translation,218,69,baselines,baselines,has,hyst,baselines has hyst,0.6272464990615845
translation,218,70,baselines,"comer ( ren et al. , 2019 )",adopts,three hierarchically stacked decoders,"comer ( ren et al. , 2019 ) adopts three hierarchically stacked decoders",0.6167647242546082
translation,218,70,baselines,three hierarchically stacked decoders,to generate,dialogue states,three hierarchically stacked decoders to generate dialogue states,0.6731271743774414
translation,218,70,baselines,baselines,has,"comer ( ren et al. , 2019 )","baselines has comer ( ren et al. , 2019 )",0.545454204082489
translation,218,74,hyperparameters,glove,to initialize,word embeddings,glove to initialize word embeddings,0.6981847286224365
translation,218,74,hyperparameters,character embeddings,to initialize,word embeddings,character embeddings to initialize word embeddings,0.5854353308677673
translation,218,74,hyperparameters,"hashimoto et al. , 2017 )",to initialize,word embeddings,"hashimoto et al. , 2017 ) to initialize word embeddings",0.6541038155555725
translation,218,74,hyperparameters,character embeddings,has,"hashimoto et al. , 2017 )","character embeddings has hashimoto et al. , 2017 )",0.5339025855064392
translation,218,74,hyperparameters,hyperparameters,used,glove,hyperparameters used glove,0.636309802532196
translation,218,75,hyperparameters,hidden sizes,of,all gru layers,hidden sizes of all gru layers,0.569953441619873
translation,218,75,hyperparameters,all gru layers,set to,400,all gru layers set to 400,0.7022528648376465
translation,218,75,hyperparameters,hyperparameters,has,hidden sizes,hyperparameters has hidden sizes,0.5404813885688782
translation,218,76,hyperparameters,training phase,used,ground truth,training phase used ground truth,0.6365130543708801
translation,218,76,hyperparameters,prior-turn dialogue states,in,slot connecting mechanism,prior-turn dialogue states in slot connecting mechanism,0.5351101160049438
translation,218,76,hyperparameters,ground truth,has,prior-turn dialogue states,ground truth has prior-turn dialogue states,0.5865903496742249
translation,218,76,hyperparameters,hyperparameters,In,training phase,hyperparameters In training phase,0.4707139730453491
translation,218,77,hyperparameters,"adam optimizer ( kingma and ba , 2015 )",applied with,0.001 learning rate,"adam optimizer ( kingma and ba , 2015 ) applied with 0.001 learning rate",0.6339959502220154
translation,218,77,hyperparameters,hyperparameters,has,"adam optimizer ( kingma and ba , 2015 )","hyperparameters has adam optimizer ( kingma and ba , 2015 )",0.5152386426925659
translation,218,78,hyperparameters,learning rate,reduced by,factor,learning rate reduced by factor,0.7101114392280579
translation,218,78,hyperparameters,factor,of,0.2,factor of 0.2,0.6461678743362427
translation,218,78,hyperparameters,training,stopped,early,training stopped early,0.7952266931533813
translation,218,78,hyperparameters,early,when,performance in validation set,early when performance in validation set,0.660135805606842
translation,218,78,hyperparameters,not improved,for,6 consecutive epochs,not improved for 6 consecutive epochs,0.6180706024169922
translation,218,78,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,218,78,hyperparameters,hyperparameters,has,training,hyperparameters has training,0.519983172416687
translation,218,79,hyperparameters,batch size,of,32,batch size of 32,0.6741614937782288
translation,218,79,hyperparameters,dropout rate,of,0.2,dropout rate of 0.2,0.5832480192184448
translation,218,80,hyperparameters,greedy search strategy,used for,decoding,greedy search strategy used for decoding,0.6747421622276306
translation,218,80,hyperparameters,decoding,with,maximum 10 decoded tokens,decoding with maximum 10 decoded tokens,0.6423177123069763
translation,218,80,hyperparameters,decoding,with,50 % probability,decoding with 50 % probability,0.6924008131027222
translation,218,80,hyperparameters,50 % probability,of,teacher forcing,50 % probability of teacher forcing,0.6082393527030945
translation,218,80,hyperparameters,hyperparameters,has,greedy search strategy,hyperparameters has greedy search strategy,0.5277536511421204
translation,218,6,model,dialogue state tracking with slot connections ( dst - sc ) model,to explicitly consider,slot correlations,dialogue state tracking with slot connections ( dst - sc ) model to explicitly consider slot correlations,0.6884246468544006
translation,218,6,model,slot correlations,across,different domains,slot correlations across different domains,0.7244970202445984
translation,218,6,model,model,propose,dialogue state tracking with slot connections ( dst - sc ) model,model propose dialogue state tracking with slot connections ( dst - sc ) model,0.6498854756355286
translation,218,7,model,slot connecting mechanism,in,dst - sc,slot connecting mechanism in dst - sc,0.554390549659729
translation,218,7,model,slot connecting mechanism,infer,source slot,slot connecting mechanism infer source slot,0.6205644607543945
translation,218,7,model,source slot,copy,source slot value,source slot copy source slot value,0.6711249947547913
translation,218,7,model,target slot,has,slot connecting mechanism,target slot has slot connecting mechanism,0.5724932551383972
translation,218,7,model,source slot value,has,directly,source slot value has directly,0.5974749326705933
translation,218,7,model,significantly reducing,has,difficulty of learning and reasoning,significantly reducing has difficulty of learning and reasoning,0.5735660195350647
translation,218,7,model,model,Given,target slot,model Given target slot,0.7544159889221191
translation,218,24,model,model,propose,novel model dst - sc,model propose novel model dst - sc,0.6944796442985535
translation,218,87,results,multiwoz 2.0 and multiwoz 2.1,with,joint goal accuracy,multiwoz 2.0 and multiwoz 2.1 with joint goal accuracy,0.6267004013061523
translation,218,87,results,joint goal accuracy,of,52.24 % and 49.58 %,joint goal accuracy of 52.24 % and 49.58 %,0.5311486124992371
translation,219,243,baselines,strong context- free benchmark model,uses,similar multimodal approach,strong context- free benchmark model uses similar multimodal approach,0.5756544470787048
translation,219,243,baselines,similar multimodal approach,on,ensemble of trees,similar multimodal approach on ensemble of trees,0.5660253167152405
translation,219,243,baselines,svm - ensemble,has,strong context- free benchmark model,svm - ensemble has strong context- free benchmark model,0.5268258452415466
translation,219,243,baselines,baselines,compare,cmn,baselines compare cmn,0.6692062616348267
translation,219,244,baselines,each node,represents,binary support vector machines ( svm ),each node represents binary support vector machines ( svm ),0.6358968615531921
translation,219,244,baselines,bi-directional lstm,equipped with,hierarchical fusion,bi-directional lstm equipped with hierarchical fusion,0.7002734541893005
translation,219,244,baselines,bc- lstm,has,bi-directional lstm,bc- lstm has bi-directional lstm,0.5827131867408752
translation,219,244,baselines,baselines,has,each node,baselines has each node,0.5839501619338989
translation,219,249,baselines,memn2n,has,original memory network,memn2n has original memory network,0.5748773217201233
translation,219,249,baselines,baselines,has,memn2n,baselines has memn2n,0.5788412690162659
translation,219,255,baselines,single layer variant,of,cmn,single layer variant of cmn,0.6237713098526001
translation,219,255,baselines,cmn,with,no attention module,cmn with no attention module,0.6187685132026672
translation,219,255,baselines,cmn n a,has,single layer variant,cmn n a has single layer variant,0.6308976411819458
translation,219,255,baselines,baselines,has,cmn n a,baselines has cmn n a,0.516059398651123
translation,219,108,hyperparameters,- pooling,employed on,feature maps,- pooling employed on feature maps,0.6349045038223267
translation,219,108,hyperparameters,feature maps,with,pooling window,feature maps with pooling window,0.6336556673049927
translation,219,108,hyperparameters,max,has,- pooling,max has - pooling,0.5694697499275208
translation,219,108,hyperparameters,pooling window,has,of size 2,pooling window has of size 2,0.5744995474815369
translation,219,108,hyperparameters,hyperparameters,has,max,hyperparameters has max,0.5175120234489441
translation,219,108,hyperparameters,hyperparameters,has,- pooling,hyperparameters has - pooling,0.5423876047134399
translation,219,130,hyperparameters,pooling,set,m p,pooling set m p,0.6179007887840271
translation,219,130,hyperparameters,m p,to be,3,m p to be 3,0.590193510055542
translation,219,130,hyperparameters,output,fed to,fully connected layer,output fed to fully connected layer,0.7016726732254028
translation,219,130,hyperparameters,fully connected layer,with,100 neurons,fully connected layer with 100 neurons,0.5666177868843079
translation,219,130,hyperparameters,hyperparameters,For,pooling,hyperparameters For pooling,0.5819546580314636
translation,219,236,hyperparameters,annealing approach,halves,lr,annealing approach halves lr,0.5993608832359314
translation,219,236,hyperparameters,lr,every,20 epochs,lr every 20 epochs,0.6960946917533875
translation,219,236,hyperparameters,termination,decided using,early - stop measure,termination decided using early - stop measure,0.7317793369293213
translation,219,236,hyperparameters,early - stop measure,with,patience,early - stop measure with patience,0.6594017148017883
translation,219,236,hyperparameters,patience,of,12,patience of 12,0.6759308576583862
translation,219,236,hyperparameters,12,by monitoring,validation loss,12 by monitoring validation loss,0.7377326488494873
translation,219,236,hyperparameters,hyperparameters,has,annealing approach,hyperparameters has annealing approach,0.5672199130058289
translation,219,237,hyperparameters,gradient clipping,used for,regularization,gradient clipping used for regularization,0.5959029793739319
translation,219,237,hyperparameters,regularization,with,norm,regularization with norm,0.6162556409835815
translation,219,237,hyperparameters,norm,set to,40,norm set to 40,0.7387048006057739
translation,219,237,hyperparameters,hyperparameters,has,gradient clipping,hyperparameters has gradient clipping,0.5166950225830078
translation,219,238,hyperparameters,hyperparameters,decided using,random search,hyperparameters decided using random search,0.7209319472312927
translation,219,239,hyperparameters,context window length k,set to be,40,context window length k set to be 40,0.6830331087112427
translation,219,239,hyperparameters,number of hops r,fixed at,3 hops,number of hops r fixed at 3 hops,0.7002493143081665
translation,219,239,hyperparameters,validation performance,has,context window length k,validation performance has context window length k,0.5425633788108826
translation,219,239,hyperparameters,validation performance,has,number of hops r,validation performance has number of hops r,0.5544673800468445
translation,219,239,hyperparameters,hyperparameters,Based on,validation performance,hyperparameters Based on validation performance,0.6155572533607483
translation,219,241,hyperparameters,dimension size,of,memory cells d,dimension size of memory cells d,0.6154080033302307
translation,219,241,hyperparameters,memory cells d,set as,50,memory cells d set as 50,0.6306106448173523
translation,219,241,hyperparameters,hyperparameters,has,dimension size,hyperparameters has dimension size,0.5081862211227417
translation,219,7,model,deep neural framework,termed,conversational memory network,deep neural framework termed conversational memory network,0.6229162812232971
translation,219,7,model,deep neural framework,leverages,contextual information,deep neural framework leverages contextual information,0.7620801329612732
translation,219,7,model,contextual information,from,conversation history,contextual information from conversation history,0.508476197719574
translation,219,7,model,model,propose,deep neural framework,model propose deep neural framework,0.6719881296157837
translation,219,8,model,multimodal approach,comprising,"audio , visual and textual features","multimodal approach comprising audio , visual and textual features",0.6331297755241394
translation,219,8,model,"audio , visual and textual features",with,gated recurrent units,"audio , visual and textual features with gated recurrent units",0.6442909240722656
translation,219,8,model,gated recurrent units,to model,past utterances,gated recurrent units to model past utterances,0.6649419665336609
translation,219,8,model,past utterances,of,each speaker,past utterances of each speaker,0.5731832385063171
translation,219,8,model,each speaker,into,memories,each speaker into memories,0.6271853446960449
translation,219,29,model,memory cells,of,cmn,memory cells of cmn,0.6107180118560791
translation,219,29,model,cmn,are,continuous vectors,cmn are continuous vectors,0.6054225564002991
translation,219,29,model,continuous vectors,store,context information,continuous vectors store context information,0.7425636649131775
translation,219,29,model,context information,found in,utterance histories,context information found in utterance histories,0.5930724740028381
translation,219,29,model,model,has,memory cells,model has memory cells,0.5599194169044495
translation,219,30,model,cmn,models,interplay,cmn models interplay,0.7750471830368042
translation,219,30,model,interplay,of,memories,interplay of memories,0.621451199054718
translation,219,30,model,memories,to capture,interspeaker dependencies,memories to capture interspeaker dependencies,0.704500675201416
translation,219,30,model,model,has,cmn,model has cmn,0.6319617629051208
translation,219,106,model,sentence representation,use,simple cnn,sentence representation use simple cnn,0.5717206001281738
translation,219,106,model,simple cnn,with,one convolutional layer,simple cnn with one convolutional layer,0.6219902038574219
translation,219,106,model,one convolutional layer,followed by,max-pooling,one convolutional layer followed by max-pooling,0.5959819555282593
translation,219,106,model,model,To get,sentence representation,model To get sentence representation,0.5560083389282227
translation,219,107,model,convolution layer,consists,filters,convolution layer consists filters,0.6557859778404236
translation,219,107,model,filters,with,50 feature maps each,filters with 50 feature maps each,0.6574095487594604
translation,219,107,model,"of sizes 3 , 4 and 5",with,50 feature maps each,"of sizes 3 , 4 and 5 with 50 feature maps each",0.6261889338493347
translation,219,107,model,filters,has,"of sizes 3 , 4 and 5","filters has of sizes 3 , 4 and 5",0.6348280310630798
translation,219,107,model,model,has,convolution layer,model has convolution layer,0.5562974810600281
translation,219,28,results,speakerbased emotion modeling,by using,memory networks,speakerbased emotion modeling by using memory networks,0.5654148459434509
translation,219,28,results,memory networks,efficient in capturing,long-term dependencies,memory networks efficient in capturing long-term dependencies,0.6832945942878723
translation,219,28,results,task -specific details,using,attention models,task -specific details using attention models,0.6405194997787476
translation,219,28,results,results,improves,speakerbased emotion modeling,results improves speakerbased emotion modeling,0.5701698660850525
translation,219,43,results,cmn,provides,significant increase,cmn provides significant increase,0.6647322773933411
translation,219,43,results,significant increase,in,accuracy,significant increase in accuracy,0.5362258553504944
translation,219,43,results,accuracy,of,3 ? 4 %,accuracy of 3 ? 4 %,0.5998469591140747
translation,219,43,results,3 ? 4 %,over,previous state - of - the - art networks,3 ? 4 % over previous state - of - the - art networks,0.652033805847168
translation,219,43,results,results,has,cmn,results has cmn,0.5536463260650635
translation,219,265,results,improvement,in,performance,improvement in performance,0.5151869058609009
translation,219,265,results,performance,seen,all emotions,performance seen all emotions,0.6790362000465393
translation,219,265,results,performance,for,all emotions,performance for all emotions,0.604042649269104
translation,219,265,results,all emotions,over,ensemble - svm based method,all emotions over ensemble - svm based method,0.6835147738456726
translation,219,265,results,results,has,improvement,results has improvement,0.6248279809951782
translation,219,270,results,cmn self,uses,only single history channel,cmn self uses only single history channel,0.6463184952735901
translation,219,270,results,cmn self,provides,lesser performance,cmn self provides lesser performance,0.6364713907241821
translation,219,270,results,lesser performance,compared to,cmn,lesser performance compared to cmn,0.6864354610443115
translation,219,270,results,results,has,cmn self,results has cmn self,0.5535567402839661
translation,219,272,results,predictions,on,valence and arousal levels,predictions on valence and arousal levels,0.5851840376853943
translation,219,272,results,valence and arousal levels,show,similar results,valence and arousal levels show similar results,0.6308241486549377
translation,219,272,results,results,has,predictions,results has predictions,0.5450208783149719
translation,219,296,results,unimodal variants,justifying,design of cmn,unimodal variants justifying design of cmn,0.6561408042907715
translation,219,296,results,design of cmn,as,multimodal system,design of cmn as multimodal system,0.5547714829444885
translation,219,296,results,multimodal systems,has,outperform,multimodal systems has outperform,0.6209559440612793
translation,219,296,results,outperform,has,unimodal variants,outperform has unimodal variants,0.6125638484954834
translation,219,296,results,results,has,multimodal systems,results has multimodal systems,0.5684642195701599
translation,219,297,results,superiority,of,cmn and its variants,superiority of cmn and its variants,0.5965102910995483
translation,219,297,results,cmn and its variants,over,bc - lstm,cmn and its variants over bc - lstm,0.6697100400924683
translation,219,297,results,results,showcases,superiority,results showcases superiority,0.7363500595092773
translation,219,298,results,proposed model,achieves,better performance,proposed model achieves better performance,0.6802239418029785
translation,219,298,results,better performance,over,state of the art,better performance over state of the art,0.6627853512763977
translation,219,298,results,state of the art,in,all the unimodal and multimodal segments,state of the art in all the unimodal and multimodal segments,0.5060348510742188
translation,219,298,results,results,has,proposed model,results has proposed model,0.5938616394996643
translation,220,115,ablation-analysis,cross validation,on,training corpus,cross validation on training corpus,0.5484178066253662
translation,220,115,ablation-analysis,lemmatisation,increases,accuracy,lemmatisation increases accuracy,0.7512267827987671
translation,220,115,ablation-analysis,accuracy,by,up to 3.42 points,accuracy by up to 3.42 points,0.5769270062446594
translation,220,115,ablation-analysis,cross validation,has,lemmatisation,cross validation has lemmatisation,0.5903372764587402
translation,220,115,ablation-analysis,training corpus,has,lemmatisation,training corpus has lemmatisation,0.5558575987815857
translation,220,120,ablation-analysis,increase,of,up to 8.27 points,increase of up to 8.27 points,0.5781217217445374
translation,220,120,ablation-analysis,statistically significant impact,for,all configurations involving lemmatisation,statistically significant impact for all configurations involving lemmatisation,0.6240592002868652
translation,220,120,ablation-analysis,ablation analysis,yields,increase,ablation analysis yields increase,0.7209053039550781
translation,220,121,ablation-analysis,data expansion,best used in combination with,lemmatisation,data expansion best used in combination with lemmatisation,0.652958333492279
translation,220,121,ablation-analysis,data expansion,creating,"better , more balanced and more general training data","data expansion creating better , more balanced and more general training data",0.6388131380081177
translation,220,121,ablation-analysis,combination,creating,"better , more balanced and more general training data","combination creating better , more balanced and more general training data",0.6429862976074219
translation,220,121,ablation-analysis,ablation analysis,has,data expansion,ablation analysis has data expansion,0.5182466506958008
translation,220,38,baselines,baselines,has,tf*idf,baselines has tf*idf,0.5322921872138977
translation,220,12,model,training data,of,supervised qa character,training data of supervised qa character,0.6132894158363342
translation,220,12,model,different ways of improving and complementing,has,training data,different ways of improving and complementing has training data,0.5576943755149841
translation,220,12,model,model,explore,different ways of improving and complementing,model explore different ways of improving and complementing,0.6517824530601501
translation,220,13,results,size and the quality ( less skewed data ),of,training corpus,size and the quality ( less skewed data ) of training corpus,0.587452232837677
translation,220,13,results,size and the quality ( less skewed data ),using,paraphrase generation techniques,size and the quality ( less skewed data ) using paraphrase generation techniques,0.6898584961891174
translation,220,13,results,results,expand,size and the quality ( less skewed data ),results expand size and the quality ( less skewed data ),0.6478778719902039
translation,220,97,results,largest performance gain,obtained by,combination of the three techniques,largest performance gain obtained by combination of the three techniques,0.6132554411888123
translation,220,97,results,combination of the three techniques,namely,data expansion,combination of the three techniques namely data expansion,0.6697596311569214
translation,220,97,results,combination of the three techniques,namely,synonym handling,combination of the three techniques namely synonym handling,0.6494059562683105
translation,220,97,results,combination of the three techniques,namely,lemmatisation,combination of the three techniques namely lemmatisation,0.6708579063415527
translation,220,97,results,+ 8.9 points,for,cross-validation experiment,+ 8.9 points for cross-validation experiment,0.6019629836082458
translation,220,97,results,+ 2.3,for,h-c evaluation,+ 2.3 for h-c evaluation,0.6390912532806396
translation,220,97,results,lemmatisation,has,+ 8.9 points,lemmatisation has + 8.9 points,0.5923691987991333
translation,220,97,results,results,has,largest performance gain,results has largest performance gain,0.5871927738189697
translation,220,111,results,paraphrases,improves,synonym handling,paraphrases improves synonym handling,0.6913454532623291
translation,220,111,results,results,show,paraphrases,results show paraphrases,0.6235834360122681
translation,220,112,results,synonym handling,helps most,words,synonym handling helps most words,0.6018075942993164
translation,220,112,results,synonym handling,helps most,unknown words,synonym handling helps most unknown words,0.6056615114212036
translation,220,112,results,words,are,lemmatised,words are lemmatised,0.6702642440795898
translation,220,112,results,unknown words,can be,at least partially,unknown words can be at least partially,0.6553833484649658
translation,220,112,results,results,has,synonym handling,results has synonym handling,0.5413652062416077
translation,220,113,results,synonym handling,with,data expansion,synonym handling with data expansion,0.5954195261001587
translation,220,113,results,synonym handling,improves,accuracy,synonym handling improves accuracy,0.71551114320755
translation,220,113,results,results,combining,synonym handling,results combining synonym handling,0.661856472492218
translation,220,119,results,data expansion,has,no significant impact,data expansion has no significant impact,0.5781437158584595
translation,220,119,results,h-h corpus,has,data expansion,h-h corpus has data expansion,0.5542262196540833
translation,220,119,results,data expansion,has,no significant impact,data expansion has no significant impact,0.5781437158584595
translation,220,119,results,results,On,h-h corpus,results On h-h corpus,0.5697252154350281
translation,221,83,hyperparameters,entity words,use,string matching methods,entity words use string matching methods,0.5877923369407654
translation,221,83,hyperparameters,string matching methods,to extract,tokens,string matching methods to extract tokens,0.7067033052444458
translation,221,83,hyperparameters,tokens,for,dialogue,tokens for dialogue,0.6864996552467346
translation,221,83,hyperparameters,hyperparameters,To replace,entity words,hyperparameters To replace entity words,0.6204141974449158
translation,221,115,hyperparameters,hyperparameters,has,encoder,hyperparameters has encoder,0.5368890762329102
translation,221,116,hyperparameters,adagrad optimizer,with,learning rate,adagrad optimizer with learning rate,0.5825135707855225
translation,221,116,hyperparameters,adagrad optimizer,randomly initialized,128 - dimensional word embeddings,adagrad optimizer randomly initialized 128 - dimensional word embeddings,0.68817538022995
translation,221,116,hyperparameters,learning rate,of,0.15,learning rate of 0.15,0.6145498156547546
translation,221,116,hyperparameters,hyperparameters,used,adagrad optimizer,hyperparameters used adagrad optimizer,0.551213264465332
translation,221,117,hyperparameters,word embeddings,shared between,encoder lstm,word embeddings shared between encoder lstm,0.6252046823501587
translation,221,117,hyperparameters,word embeddings,shared between,the decoder lstm,word embeddings shared between the decoder lstm,0.6734009981155396
translation,221,117,hyperparameters,hyperparameters,has,word embeddings,hyperparameters has word embeddings,0.4784160554409027
translation,221,119,hyperparameters,model,for,20 epochs,model for 20 epochs,0.6305356025695801
translation,221,119,hyperparameters,model,with,early stopping,model with early stopping,0.6294175386428833
translation,221,119,hyperparameters,early stopping,on,validation set,early stopping on validation set,0.5784603357315063
translation,221,119,hyperparameters,hyperparameters,trained,model,hyperparameters trained model,0.723837673664093
translation,221,7,model,dialogue context - aware user query reformulation task,has,dialog state,dialogue context - aware user query reformulation task has dialog state,0.5301501750946045
translation,221,8,model,our model,for,query reformulation,our model for query reformulation,0.6404257416725159
translation,221,8,model,query reformulation,using,pointer - generator network,query reformulation using pointer - generator network,0.6745845079421997
translation,221,8,model,query reformulation,using,novel multi-task learning setup,query reformulation using novel multi-task learning setup,0.607612133026123
translation,221,8,model,model,develop,our model,model develop our model,0.6248242259025574
translation,221,8,model,model,develop,query reformulation,model develop query reformulation,0.6379411220550537
translation,221,8,model,model,for,query reformulation,model for query reformulation,0.6265031695365906
translation,221,24,model,novel approach,for enabling,seamless interaction,novel approach for enabling seamless interaction,0.7837465405464172
translation,221,24,model,seamless interaction,between,agents,seamless interaction between agents,0.64322429895401
translation,221,24,model,natural language,as,api,natural language as api,0.4785056412220001
translation,221,24,model,model,propose,novel approach,model propose novel approach,0.7168048620223999
translation,221,25,model,model,build upon,pointergenerator network ( pgn ),model build upon pointergenerator network ( pgn ),0.613417387008667
translation,221,26,model,pgn,without requiring,extra manually annotated training data,pgn without requiring extra manually annotated training data,0.6973251700401306
translation,221,26,model,model,describe,new multi-task learning ( mtl ) objective,model describe new multi-task learning ( mtl ) objective,0.6599591970443726
translation,221,36,model,pgn,is,hybrid architecture,pgn is hybrid architecture,0.5999307632446289
translation,221,36,model,hybrid architecture,combines,sequence - to-sequence model,hybrid architecture combines sequence - to-sequence model,0.7427692413330078
translation,221,36,model,sequence - to-sequence model,with,pointer networks,sequence - to-sequence model with pointer networks,0.6396070122718811
translation,221,36,model,model,has,pgn,model has pgn,0.6361218690872192
translation,221,129,results,our reimplementation,achieves,response entity f1,our reimplementation achieves response entity f1,0.6438155770301819
translation,221,129,results,mem2seq *,achieves,response entity f1,mem2seq * achieves response entity f1,0.6497669219970703
translation,221,129,results,response entity f1,of,33.6,response entity f1 of 33.6,0.5255120992660522
translation,221,129,results,33.6,higher than,best overall entity f1 score,33.6 higher than best overall entity f1 score,0.6611577868461609
translation,221,129,results,best overall entity f1 score,of,33.4,best overall entity f1 score of 33.4,0.5372348427772522
translation,221,129,results,our reimplementation,has,mem2seq *,our reimplementation has mem2seq *,0.5764671564102173
translation,221,134,results,internal dataset,show,cqr,internal dataset show cqr,0.6551823019981384
translation,221,134,results,internal dataset,show,cqr,internal dataset show cqr,0.6551823019981384
translation,221,134,results,cqr,significantly improves over,"naik et al. , 2018 )","cqr significantly improves over naik et al. , 2018 )",0.6944980621337891
translation,221,134,results,cqr,improves,f1,cqr improves f1,0.7129026055335999
translation,221,134,results,cqr,improves,f1,cqr improves f1,0.7129026055335999
translation,221,134,results,f1,for,current turn slots,f1 for current turn slots,0.6088751554489136
translation,221,134,results,improve,has,slu,improve has slu,0.6346521377563477
translation,221,134,results,results,On,internal dataset,results On internal dataset,0.5724051594734192
translation,221,135,results,most improvements,upon,baseline pgn model ( m0 ),most improvements upon baseline pgn model ( m0 ),0.6095758080482483
translation,221,135,results,baseline pgn model ( m0 ),come from,pre-processing steps,baseline pgn model ( m0 ) come from pre-processing steps,0.6077401041984558
translation,221,135,results,pre-processing steps,like,canonicalizing entities,pre-processing steps like canonicalizing entities,0.6297073364257812
translation,221,135,results,results,see that,most improvements,results see that most improvements,0.6136292815208435
translation,221,137,results,proposed multi-task learning model ( cqr ),improves,bleu,proposed multi-task learning model ( cqr ) improves bleu,0.6858571171760559
translation,221,137,results,proposed multi-task learning model ( cqr ),improves,entityf1,proposed multi-task learning model ( cqr ) improves entityf1,0.695805549621582
translation,221,137,results,entityf1,at,most distances,entityf1 at most distances,0.5098761320114136
translation,221,137,results,results,has,proposed multi-task learning model ( cqr ),results has proposed multi-task learning model ( cqr ),0.5737002491950989
translation,221,138,results,improvement,of,4.2 %,improvement of 4.2 %,0.5684617161750793
translation,221,138,results,4.2 %,over,m2,4.2 % over m2,0.6383554339408875
translation,221,138,results,m2,for,slots,m2 for slots,0.6775447130203247
translation,221,138,results,results,see,improvement,results see improvement,0.6593043208122253
translation,221,143,results,navigation domain,observe,significant improvement,navigation domain observe significant improvement,0.608361005783081
translation,221,143,results,results,On,navigation domain,results On navigation domain,0.5552932620048523
translation,221,146,results,cqr model,performs,better,cqr model performs better,0.6401413679122925
translation,221,146,results,better,than,mem2seq * model,better than mem2seq * model,0.5614765882492065
translation,221,146,results,results,see that,cqr model,results see that cqr model,0.6685300469398499
translation,222,172,experimental-setup,dialsql,in,"tensorflow ( abadi et al. , 2016 )","dialsql in tensorflow ( abadi et al. , 2016 )",0.4931080639362335
translation,222,172,experimental-setup,dialsql,using,adam optimizer,dialsql using adam optimizer,0.6518963575363159
translation,222,172,experimental-setup,adam optimizer,for,training,adam optimizer for training,0.6055397391319275
translation,222,172,experimental-setup,training,with,learning rate,training with learning rate,0.6329779624938965
translation,222,172,experimental-setup,learning rate,of,1e ?4,learning rate of 1e ?4,0.6287522315979004
translation,222,172,experimental-setup,experimental setup,implement,dialsql,experimental setup implement dialsql,0.6479164361953735
translation,222,173,experimental-setup,embedding size,of,300,embedding size of 300,0.6510111689567566
translation,222,173,experimental-setup,rnn state size,of,50,rnn state size of 50,0.6480998396873474
translation,222,173,experimental-setup,batch size,of,64,batch size of 64,0.6741159558296204
translation,222,173,experimental-setup,experimental setup,use,embedding size,experimental setup use embedding size,0.6037160158157349
translation,222,173,experimental-setup,experimental setup,use,rnn state size,experimental setup use rnn state size,0.6031572818756104
translation,222,173,experimental-setup,experimental setup,use,batch size,experimental setup use batch size,0.6168987154960632
translation,222,174,experimental-setup,embeddings,initialized from,pretrained glove embeddings,embeddings initialized from pretrained glove embeddings,0.6665152907371521
translation,222,174,experimental-setup,embeddings,fine-tuned during,training,embeddings fine-tuned during training,0.692444384098053
translation,222,174,experimental-setup,experimental setup,has,embeddings,experimental setup has embeddings,0.5157045722007751
translation,222,176,experimental-setup,stanford corenlp tokenizer,to,parse,stanford corenlp tokenizer to parse,0.48680123686790466
translation,222,176,experimental-setup,parse,has,questions and column names,parse has questions and column names,0.5869886875152588
translation,222,176,experimental-setup,experimental setup,has,stanford corenlp tokenizer,experimental setup has stanford corenlp tokenizer,0.5358015894889832
translation,222,6,model,dialoguebased structured query generation framework,leverages,human intelligence,dialoguebased structured query generation framework leverages human intelligence,0.6146894097328186
translation,222,6,model,human intelligence,to boost,performance,human intelligence to boost performance,0.6372610330581665
translation,222,6,model,performance,of,existing algorithms,performance of existing algorithms,0.5757234692573547
translation,222,6,model,performance,via,user interaction,performance via user interaction,0.6307747960090637
translation,222,6,model,dialsql,has,dialoguebased structured query generation framework,dialsql has dialoguebased structured query generation framework,0.5468860864639282
translation,222,6,model,model,introduce,dialsql,model introduce dialsql,0.6653734445571899
translation,222,175,model,bidirectional rnn encoders,with,two layers,bidirectional rnn encoders with two layers,0.6063694357872009
translation,222,175,model,two layers,for,"questions , column names , and queries","two layers for questions , column names , and queries",0.6130579113960266
translation,222,175,model,model,use,bidirectional rnn encoders,model use bidirectional rnn encoders,0.6191173195838928
translation,222,7,results,dialsql,capable of identifying,potential errors,dialsql capable of identifying potential errors,0.6967957615852356
translation,222,7,results,dialsql,asking users for,validation,dialsql asking users for validation,0.5999788045883179
translation,222,7,results,potential errors,in,generated sql query,potential errors in generated sql query,0.5382727384567261
translation,222,7,results,validation,via,simple multi-choice questions,validation via simple multi-choice questions,0.6477658748626709
translation,222,7,results,results,has,dialsql,results has dialsql,0.5027616620063782
translation,222,182,results,dialsql model,with,number of 5 choices,dialsql model with number of 5 choices,0.6617422699928284
translation,222,182,results,number of 5 choices,improves,performance,number of 5 choices improves performance,0.7124781608581543
translation,222,182,results,performance,of,sqlnet and seq2sql,performance of sqlnet and seq2sql,0.5861391425132751
translation,222,182,results,performance,by,7.7 % and 9.4 %,performance by 7.7 % and 9.4 %,0.5948708653450012
translation,222,182,results,sqlnet and seq2sql,by,7.7 % and 9.4 %,sqlnet and seq2sql by 7.7 % and 9.4 %,0.5810455679893494
translation,222,182,results,results,observe,dialsql model,results observe dialsql model,0.5880869626998901
translation,222,195,results,performance,of,dialsql,performance of dialsql,0.5992509722709656
translation,222,195,results,improves,in,all the cases,improves in all the cases,0.5550943613052368
translation,222,195,results,number of choices,has,increases,number of choices has increases,0.6112694144248962
translation,222,195,results,number of choices,has,performance,number of choices has performance,0.5929335355758667
translation,222,195,results,increases,has,performance,increases has performance,0.5952932834625244
translation,222,195,results,dialsql,has,improves,dialsql has improves,0.6419004201889038
translation,222,196,results,sqlnet- dialsql model,observe,more accuray gain,sqlnet- dialsql model observe more accuray gain,0.6583709716796875
translation,222,196,results,results,for,sqlnet- dialsql model,results for sqlnet- dialsql model,0.6292046308517456
translation,222,212,results,real users,interact with,our system,real users interact with our system,0.6949701309204102
translation,222,212,results,overall performance,of,generated queries,overall performance of generated queries,0.5298757553100586
translation,222,212,results,overall performance,of,strong nlidb system,overall performance of strong nlidb system,0.5760008096694946
translation,222,212,results,generated queries,are,better,generated queries are better,0.5800931453704834
translation,222,212,results,better,than,sqlnet model,better than sqlnet model,0.5850163102149963
translation,222,212,results,better,improve,performance,better improve performance,0.7383228540420532
translation,222,212,results,sqlnet model,showing that,dialsql,sqlnet model showing that dialsql,0.6111158728599548
translation,222,212,results,dialsql,improve,performance,dialsql improve performance,0.6555947661399841
translation,222,212,results,performance,of,strong nlidb system,performance of strong nlidb system,0.5856444835662842
translation,222,212,results,real users,has,overall performance,real users has overall performance,0.5477667450904846
translation,222,212,results,our system,has,overall performance,our system has overall performance,0.5666546821594238
translation,222,212,results,results,observe,real users,results observe real users,0.6054354310035706
translation,222,212,results,results,when,real users,results when real users,0.6468709707260132
translation,223,5,model,dialogue agent,with the ability to extract,new training examples,dialogue agent with the ability to extract new training examples,0.6224547624588013
translation,223,5,model,new training examples,from,conversations,new training examples from conversations,0.5880152583122253
translation,223,5,model,self-feeding chatbot,has,dialogue agent,self-feeding chatbot has dialogue agent,0.5825000405311584
translation,223,5,model,model,propose,self-feeding chatbot,model propose self-feeding chatbot,0.68012934923172
translation,223,24,model,dialogue agent,with the ability to extract,new examples,dialogue agent with the ability to extract new examples,0.6407588720321655
translation,223,24,model,new examples,from,conversations,new examples from conversations,0.5883139371871948
translation,223,24,model,self-feeding chatbot,has,dialogue agent,self-feeding chatbot has dialogue agent,0.5825000405311584
translation,223,24,model,model,propose,self-feeding chatbot,model propose self-feeding chatbot,0.68012934923172
translation,223,9,results,personachat chitchat dataset,with,131 k training examples,personachat chitchat dataset with 131 k training examples,0.5778969526290894
translation,223,9,results,personachat chitchat dataset,over,131 k training examples,personachat chitchat dataset over 131 k training examples,0.6256163120269775
translation,223,9,results,personachat chitchat dataset,learning from,dialogue,personachat chitchat dataset learning from dialogue,0.7460795044898987
translation,223,9,results,dialogue,with,selffeeding chatbot,dialogue with selffeeding chatbot,0.6593017578125
translation,223,9,results,selffeeding chatbot,has,significantly improves,selffeeding chatbot has significantly improves,0.6134619116783142
translation,223,9,results,significantly improves,has,performance,significantly improves has performance,0.5962982177734375
translation,223,9,results,results,On,personachat chitchat dataset,results On personachat chitchat dataset,0.5501452684402466
translation,224,98,ablation-analysis,negative training,effectively reduce,hit rate,negative training effectively reduce hit rate,0.5839899182319641
translation,224,98,ablation-analysis,hit rate,on,training target list,hit rate on training target list,0.5513474345207214
translation,224,98,ablation-analysis,training target list,to,less than 5 %,training target list to less than 5 %,0.5720966458320618
translation,224,98,ablation-analysis,less than 5 %,with,little or no degradation,less than 5 % with little or no degradation,0.6058162450790405
translation,224,98,ablation-analysis,little or no degradation,on,perplexity,little or no degradation on perplexity,0.5731510519981384
translation,224,98,ablation-analysis,all data-sets,has,negative training,all data-sets has negative training,0.6044545769691467
translation,224,98,ablation-analysis,ablation analysis,for,all data-sets,ablation analysis for all data-sets,0.5775190591812134
translation,224,79,hyperparameters,lstm based lm and attention based seq2seq models,with,one hidden layer,lstm based lm and attention based seq2seq models with one hidden layer,0.6166422963142395
translation,224,79,hyperparameters,one hidden layer,of size,600,one hidden layer of size 600,0.7398183941841125
translation,224,79,hyperparameters,embedding size,set to,300,embedding size set to 300,0.7361504435539246
translation,224,80,hyperparameters,dropout layer,rate,0.3,dropout layer rate 0.3,0.7140063047409058
translation,224,80,hyperparameters,dropout layer,added to,model,dropout layer added to model,0.6190335154533386
translation,224,80,hyperparameters,switchboard,has,dropout layer,switchboard has dropout layer,0.5753288865089417
translation,224,80,hyperparameters,hyperparameters,For,switchboard,hyperparameters For switchboard,0.5779423117637634
translation,224,81,hyperparameters,mini-,apply,sgd training,mini- apply sgd training,0.6067948937416077
translation,224,81,hyperparameters,batch size,set to,64,batch size set to 64,0.7451491355895996
translation,224,81,hyperparameters,sgd training,with,fixed starting learning rate ( lr ),sgd training with fixed starting learning rate ( lr ),0.6193438768386841
translation,224,81,hyperparameters,fixed starting learning rate ( lr ),for,10 iterations,fixed starting learning rate ( lr ) for 10 iterations,0.592129111289978
translation,224,81,hyperparameters,10 iterations,with,lr halving,10 iterations with lr halving,0.6578517556190491
translation,224,81,hyperparameters,mini-,has,batch size,mini- has batch size,0.6139686107635498
translation,224,81,hyperparameters,hyperparameters,apply,sgd training,hyperparameters apply sgd training,0.5725660920143127
translation,224,81,hyperparameters,hyperparameters,has,mini-,hyperparameters has mini-,0.5373920202255249
translation,224,82,hyperparameters,starting lr,is,1,starting lr is 1,0.6732286810874939
translation,224,82,hyperparameters,starting lr,of,0.1,starting lr of 0.1,0.6195300221443176
translation,224,82,hyperparameters,0.1,used for,opensubtitles,0.1 used for opensubtitles,0.5971077084541321
translation,224,82,hyperparameters,ubuntu and switchboard,has,starting lr,ubuntu and switchboard has starting lr,0.5828791260719299
translation,224,82,hyperparameters,ubuntu and switchboard,has,starting lr,ubuntu and switchboard has starting lr,0.5828791260719299
translation,224,82,hyperparameters,hyperparameters,For,ubuntu and switchboard,hyperparameters For ubuntu and switchboard,0.5275900363922119
translation,224,92,hyperparameters,negative training,executed on,( trained ) baseline model,negative training executed on ( trained ) baseline model,0.6955589056015015
translation,224,92,hyperparameters,( trained ) baseline model,for,20 iterations,( trained ) baseline model for 20 iterations,0.5855265259742737
translation,224,92,hyperparameters,20 iterations,over,training target list,20 iterations over training target list,0.6419646739959717
translation,224,92,hyperparameters,all data-sets,has,negative training,all data-sets has negative training,0.6044545769691467
translation,224,92,hyperparameters,hyperparameters,For,all data-sets,hyperparameters For all data-sets,0.533582866191864
translation,224,93,hyperparameters,fixed learning rate,of,0.01,fixed learning rate of 0.01,0.5930482745170593
translation,224,93,hyperparameters,mini-batch size,of,100,mini-batch size of 100,0.6536015868186951
translation,224,93,hyperparameters,hyperparameters,has,fixed learning rate,hyperparameters has fixed learning rate,0.4959334433078766
translation,224,109,hyperparameters,negative training,executed for,20 iterations,negative training executed for 20 iterations,0.6941707134246826
translation,224,109,hyperparameters,20 iterations,on,mle trained model,20 iterations on mle trained model,0.5574971437454224
translation,224,109,hyperparameters,mle trained model,over,training data,mle trained model over training data,0.6725091338157654
translation,224,109,hyperparameters,all datasets,has,negative training,all datasets has negative training,0.6048474311828613
translation,224,109,hyperparameters,hyperparameters,For,all datasets,hyperparameters For all datasets,0.4999330937862396
translation,224,110,hyperparameters,fixed learning rate,of,0.001,fixed learning rate of 0.001,0.5881882309913635
translation,224,110,hyperparameters,0.001,used for,all three data-sets,0.001 used for all three data-sets,0.6440834999084473
translation,224,110,hyperparameters,batch size,set to,64,batch size set to 64,0.7451491355895996
translation,224,110,hyperparameters,? pos,set to,1,? pos set to 1,0.7275962829589844
translation,224,110,hyperparameters,mini-,has,batch size,mini- has batch size,0.6139686107635498
translation,224,110,hyperparameters,hyperparameters,has,fixed learning rate,hyperparameters has fixed learning rate,0.4959334433078766
translation,224,11,model,negative training framework,to correct,unwanted behaviors,negative training framework to correct unwanted behaviors,0.6463061571121216
translation,224,11,model,unwanted behaviors,of,dialogue response generator,unwanted behaviors of dialogue response generator,0.5625408887863159
translation,224,11,model,model,propose and explore,negative training framework,model propose and explore negative training framework,0.665865957736969
translation,224,159,model,training list,be,effectively augmented,training list be effectively augmented,0.5579203367233276
translation,224,159,model,effectively augmented,by utilizing,paraphrase model,effectively augmented by utilizing paraphrase model,0.6965435743331909
translation,224,159,model,model,show that,training list,model show that training list,0.5245022177696228
translation,224,160,model,definition,for,frequent response problem,definition for frequent response problem,0.6028804183006287
translation,224,160,model,model,propose,definition,model propose definition,0.6219309568405151
translation,224,112,results,beam search,gives far worse response diversity,greedy decoding,beam search gives far worse response diversity greedy decoding,0.7508794665336609
translation,224,127,results,large gap,on,diversity measures,large gap on diversity measures,0.5575549602508545
translation,224,127,results,diversity measures,between,baseline models and the test set,diversity measures between baseline models and the test set,0.6391983032226562
translation,224,127,results,baseline models and the test set,especially on,switchboard and opensubtitles data,baseline models and the test set especially on switchboard and opensubtitles data,0.6113616824150085
translation,224,127,results,results,observe,large gap,results observe large gap,0.6455563306808472
translation,224,146,results,negative training,wins by,significant margin,negative training wins by significant margin,0.7367115616798401
translation,224,146,results,significant margin,for,all three data-sets,significant margin for all three data-sets,0.603524923324585
translation,225,99,ablation-analysis,all three methods,reduces,noise,all three methods reduces noise,0.6493530869483948
translation,225,99,ablation-analysis,noise,in,inputs,noise in inputs,0.5329480171203613
translation,225,99,ablation-analysis,noise,makes,model,noise makes model,0.6618862152099609
translation,225,99,ablation-analysis,model,more robust to,noise,model more robust to noise,0.7929815649986267
translation,225,99,ablation-analysis,noise,help,lvm,noise help lvm,0.763132631778717
translation,225,99,ablation-analysis,all three methods,has,gaussian noise injection,all three methods has gaussian noise injection,0.5611000657081604
translation,225,99,ablation-analysis,more easily approximate,has,distribution,more easily approximate has distribution,0.5798630714416504
translation,225,99,ablation-analysis,ablation analysis,Jointly incorporating,all three methods,ablation analysis Jointly incorporating all three methods,0.6995257139205933
translation,225,101,ablation-analysis,lvm,with,mlp,lvm with mlp,0.6815471649169922
translation,225,101,ablation-analysis,lvm,see,clear performance gains,lvm see clear performance gains,0.6177687048912048
translation,225,101,ablation-analysis,clear performance gains,by using,lvm,clear performance gains by using lvm,0.6405467987060547
translation,225,101,ablation-analysis,ablation analysis,removing or replacing,lvm,ablation analysis removing or replacing lvm,0.6924090385437012
translation,225,81,baselines,baselines,has,latent variable model ( lvm ),baselines has latent variable model ( lvm ),0.5622681379318237
translation,225,22,hyperparameters,gaussian noise,to further compensate,imperfect alignment,gaussian noise to further compensate imperfect alignment,0.6259976625442505
translation,225,22,hyperparameters,imperfect alignment,of,cross-lingual embeddings,imperfect alignment of cross-lingual embeddings,0.5333276391029358
translation,225,22,hyperparameters,hyperparameters,add,gaussian noise,hyperparameters add gaussian noise,0.6136342883110046
translation,225,62,hyperparameters,bi-directional lstm model,with,hidden dimension size,bi-directional lstm model with hidden dimension size,0.5839214324951172
translation,225,62,hyperparameters,hidden dimension size,of,250,hidden dimension size of 250,0.6432896256446838
translation,225,62,hyperparameters,latent variable model,with,mean and variance,latent variable model with mean and variance,0.5617783069610596
translation,225,62,hyperparameters,latent variable model,both,mean and variance,latent variable model both mean and variance,0.6264656782150269
translation,225,62,hyperparameters,mean and variance,in,size,mean and variance in size,0.5341445207595825
translation,225,62,hyperparameters,size,of,100,size of 100,0.683699369430542
translation,225,62,hyperparameters,hyperparameters,use,bi-directional lstm model,hyperparameters use bi-directional lstm model,0.5906049609184265
translation,225,62,hyperparameters,hyperparameters,use,latent variable model,hyperparameters use latent variable model,0.6132977604866028
translation,225,6,model,set of very few parallel word pairs,to refine,aligned cross-lingual wordlevel representations,set of very few parallel word pairs to refine aligned cross-lingual wordlevel representations,0.6718665361404419
translation,225,6,model,model,use,set of very few parallel word pairs,model use set of very few parallel word pairs,0.6003632545471191
translation,225,7,model,similar sentences,across,different languages,similar sentences across different languages,0.6784403920173645
translation,225,7,model,model,employ,latent variable model,model employ latent variable model,0.5674797892570496
translation,225,21,model,cross-lingual embeddings,with,?10 seed word -pairs,cross-lingual embeddings with ?10 seed word -pairs,0.6045958399772644
translation,225,21,model,?10 seed word -pairs,related to,dialogue domains,?10 seed word -pairs related to dialogue domains,0.6654208302497864
translation,225,21,model,model,first refine,cross-lingual embeddings,model first refine cross-lingual embeddings,0.6381984353065491
translation,225,79,model,model,has,conditional random fields ( crf ),model has conditional random fields ( crf ),0.5781365633010864
translation,225,82,model,crf module,with,latent variables,crf module with latent variables,0.5881207585334778
translation,225,82,model,crf module,apply it to,intent prediction,crf module apply it to intent prediction,0.6024660468101501
translation,225,82,model,model,replace,crf module,model replace crf module,0.650067925453186
translation,225,19,results,latent variables,does not help,model,latent variables does not help model,0.5575109124183655
translation,225,19,results,improve much,in,slot filling and intent prediction,improve much in slot filling and intent prediction,0.5200572609901428
translation,225,19,results,model,has,improve much,model has improve much,0.5948878526687622
translation,225,19,results,results,naively using,latent variables,results naively using latent variables,0.5276221632957458
translation,225,92,results,lvm,outperforms,crf models,lvm outperforms crf models,0.7130811214447021
translation,225,95,results,only gaussian noise,to,vanilla bilstm,only gaussian noise to vanilla bilstm,0.4875273108482361
translation,225,95,results,only gaussian noise,improves,our prediction performance,only gaussian noise improves our prediction performance,0.6883910894393921
translation,225,95,results,our prediction performance,has,significantly,our prediction performance has significantly,0.5805613994598389
translation,225,95,results,results,adding,only gaussian noise,results adding only gaussian noise,0.7068896293640137
translation,225,96,results,crosslingual embeddings refinement,is,more effective,crosslingual embeddings refinement is more effective,0.5285172462463379
translation,225,96,results,more effective,in,spanish,more effective in spanish,0.5289570689201355
translation,225,96,results,spanish,than,thai,spanish than thai,0.6417766809463501
translation,225,98,results,spanish,much more lexically and grammatically similar to,english,spanish much more lexically and grammatically similar to english,0.6659053564071655
translation,225,98,results,english,than,thai,english than thai,0.6280592679977417
translation,225,98,results,word - level embedding refinement,is,reasonably good,word - level embedding refinement is reasonably good,0.5187602043151855
translation,225,98,results,results,has,spanish,results has spanish,0.49849560856819153
translation,226,183,ablation-analysis,two model - based entailment ratios,calculated on,whole test set,two model - based entailment ratios calculated on whole test set,0.6257179379463196
translation,226,183,ablation-analysis,two model - based entailment ratios,prove,effectiveness,two model - based entailment ratios prove effectiveness,0.58774334192276
translation,226,183,ablation-analysis,effectiveness,of,our gdr model,effectiveness of our gdr model,0.6107800006866455
translation,226,183,ablation-analysis,ablation analysis,has,two model - based entailment ratios,ablation analysis has two model - based entailment ratios,0.5546274185180664
translation,226,202,ablation-analysis,observable improvement,in,const.,observable improvement in const.,0.5175444483757019
translation,226,202,ablation-analysis,"t , g , gr to gdr",has,every step,"t , g , gr to gdr has every step",0.6007943153381348
translation,226,202,ablation-analysis,every step,has,observable improvement,every step has observable improvement,0.5588698983192444
translation,226,202,ablation-analysis,ablation analysis,from,"t , g , gr to gdr","ablation analysis from t , g , gr to gdr",0.5625815391540527
translation,226,203,ablation-analysis,tuple-interaction,in,g,tuple-interaction in g,0.5822878479957581
translation,226,203,ablation-analysis,rewriting process,in,r,rewriting process in r,0.5420560240745544
translation,226,203,ablation-analysis,rewriting process,contribute to,improvements,rewriting process contribute to improvements,0.7249073386192322
translation,226,203,ablation-analysis,improvements,of,persona consistency,improvements of persona consistency,0.5596394538879395
translation,226,203,ablation-analysis,ablation analysis,Both,tuple-interaction,ablation analysis Both tuple-interaction,0.639712929725647
translation,226,206,ablation-analysis,one-stage transformer,approaches,g,one-stage transformer approaches g,0.629077672958374
translation,226,206,ablation-analysis,perplexity,higher than,26,perplexity higher than 26,0.6562641263008118
translation,226,207,ablation-analysis,perplexity,of,all approaches,perplexity of all approaches,0.5745999217033386
translation,226,207,ablation-analysis,perplexity,has,significant decline,perplexity has significant decline,0.6006749272346497
translation,226,207,ablation-analysis,all approaches,has,significant decline,all approaches has significant decline,0.6088302731513977
translation,226,207,ablation-analysis,rewriter r,has,perplexity,rewriter r has perplexity,0.5440986156463623
translation,226,207,ablation-analysis,all approaches,has,significant decline,all approaches has significant decline,0.6088302731513977
translation,226,207,ablation-analysis,ablation analysis,add,rewriter r,ablation analysis add rewriter r,0.6589827537536621
translation,226,213,ablation-analysis,deleting and rewriting,designed for,improving consistency,deleting and rewriting designed for improving consistency,0.6442070007324219
translation,226,213,ablation-analysis,improving consistency,have,positive effect,improving consistency have positive effect,0.5281830430030823
translation,226,213,ablation-analysis,positive effect,on,dialogue quality,positive effect on dialogue quality,0.5584338903427124
translation,226,147,experimental-setup,all the rnn - based baseline models,implemented by,two - layer lstm networks,all the rnn - based baseline models implemented by two - layer lstm networks,0.6731961369514465
translation,226,147,experimental-setup,two - layer lstm networks,with,hidden size 512,two - layer lstm networks with hidden size 512,0.6196834444999695
translation,226,147,experimental-setup,experimental setup,For,all the rnn - based baseline models,experimental setup For all the rnn - based baseline models,0.5423715710639954
translation,226,148,experimental-setup,hidden size,set to,512,hidden size set to 512,0.7357071042060852
translation,226,148,experimental-setup,of both encoder and decoder,are,3,of both encoder and decoder are 3,0.6057586073875427
translation,226,148,experimental-setup,layers,has,of both encoder and decoder,layers has of both encoder and decoder,0.5620736479759216
translation,226,149,experimental-setup,number of heads,in,multi-head attention,number of heads in multi-head attention,0.5287878513336182
translation,226,149,experimental-setup,multi-head attention,is,8,multi-head attention is 8,0.5655282139778137
translation,226,149,experimental-setup,inner-layer size,of,feedforward network,inner-layer size of feedforward network,0.5948638319969177
translation,226,149,experimental-setup,feedforward network,is,2048,feedforward network is 2048,0.6326902508735657
translation,226,149,experimental-setup,experimental setup,has,number of heads,experimental setup has number of heads,0.5273870229721069
translation,226,149,experimental-setup,experimental setup,has,inner-layer size,experimental setup has inner-layer size,0.4913395643234253
translation,226,150,experimental-setup,word embeddings,are,randomly initialized,word embeddings are randomly initialized,0.5598148107528687
translation,226,150,experimental-setup,embedding dimension,of,models,embedding dimension of models,0.6021998524665833
translation,226,150,experimental-setup,models,set to,512,models set to 512,0.7472295761108398
translation,226,150,experimental-setup,experimental setup,has,word embeddings,experimental setup has word embeddings,0.4932878315448761
translation,226,150,experimental-setup,experimental setup,has,embedding dimension,experimental setup has embedding dimension,0.5115625262260437
translation,226,152,experimental-setup,experimental setup,has,number of layers,experimental setup has number of layers,0.5300881266593933
translation,226,154,experimental-setup,token - level batching,size,4096,token - level batching size 4096,0.7353858947753906
translation,226,155,experimental-setup,adam,used for,optimization,adam used for optimization,0.6671109795570374
translation,226,155,experimental-setup,warm - up steps,set to,"10,000","warm - up steps set to 10,000",0.7075178027153015
translation,226,155,experimental-setup,experimental setup,has,adam,experimental setup has adam,0.4964992105960846
translation,226,156,experimental-setup,model,in,opennmt - py,model in opennmt - py,0.5917279720306396
translation,226,156,experimental-setup,experimental setup,implemented,model,experimental setup implemented model,0.6763837933540344
translation,226,7,model,three - stage framework,employs,generate - delete-rewrite mechanism,three - stage framework employs generate - delete-rewrite mechanism,0.5544692277908325
translation,226,7,model,generate - delete-rewrite mechanism,to delete,inconsistent words,generate - delete-rewrite mechanism to delete inconsistent words,0.7740740180015564
translation,226,7,model,inconsistent words,from,generated response prototype,inconsistent words from generated response prototype,0.5295469760894775
translation,226,7,model,model,introduce,three - stage framework,model introduce three - stage framework,0.5788894295692444
translation,226,21,model,persona- based dialogue models,adopt,encoder-decoder architecture,persona- based dialogue models adopt encoder-decoder architecture,0.6265717148780823
translation,226,21,model,persona- based dialogue models,enhanced with,persona encoding components,persona- based dialogue models enhanced with persona encoding components,0.676626980304718
translation,226,21,model,persona encoding components,such as,memory network,persona encoding components such as memory network,0.6257550716400146
translation,226,21,model,persona encoding components,such as,"latent variable ( kingma and welling , 2013 )","persona encoding components such as latent variable ( kingma and welling , 2013 )",0.5919539928436279
translation,226,21,model,model,has,persona- based dialogue models,model has persona- based dialogue models,0.5671820640563965
translation,226,27,model,generate - delete- rewrite framework,to mitigate,generation of inconsistent personas,generate - delete- rewrite framework to mitigate generation of inconsistent personas,0.6116442680358887
translation,226,27,model,generate - delete- rewrite framework,has,gdr,generate - delete- rewrite framework has gdr,0.6141265034675598
translation,226,27,model,model,present,generate - delete- rewrite framework,model present generate - delete- rewrite framework,0.6475474238395691
translation,226,28,model,model,design,three stages,model design three stages,0.6693403124809265
translation,226,65,model,model,has,consistency matching model d,model has consistency matching model d,0.58014976978302
translation,226,222,model,three -stage framework,for,persona consistent dialogue generation,three -stage framework for persona consistent dialogue generation,0.6152610182762146
translation,226,222,model,generate - delete-rewrite,for,persona consistent dialogue generation,generate - delete-rewrite for persona consistent dialogue generation,0.6217129230499268
translation,226,222,model,model,presented,three -stage framework,model presented three -stage framework,0.6033763885498047
translation,226,223,model,matching model,to delete,inconsistent words,matching model to delete inconsistent words,0.7181096076965332
translation,226,182,results,target responses,in,sampled data,target responses in sampled data,0.5155971646308899
translation,226,182,results,65.4 %,of them,expressed persona information,65.4 % of them expressed persona information,0.5519981384277344
translation,226,182,results,results,has,target responses,results has target responses,0.4801417589187622
translation,226,184,results,gdr model,ranks,first,gdr model ranks first,0.7660736441612244
translation,226,184,results,first,under,evaluation,first under evaluation,0.6380977034568787
translation,226,184,results,evaluation,has,of both diin and bert,evaluation has of both diin and bert,0.5898269414901733
translation,226,185,results,remarkably lower perplexity,of,16.7,remarkably lower perplexity of 16.7,0.5578681826591492
translation,226,185,results,16.7,than,all other baseline methods,16.7 than all other baseline methods,0.5197407007217407
translation,226,185,results,dialogue quality,has,our proposed model,dialogue quality has our proposed model,0.5695770978927612
translation,226,185,results,our proposed model,has,remarkably lower perplexity,our proposed model has remarkably lower perplexity,0.5616620182991028
translation,226,185,results,results,For,dialogue quality,results For dialogue quality,0.5374194383621216
translation,226,187,results,distinct - 2 metric,is,even significantly better,distinct - 2 metric is even significantly better,0.5726150870323181
translation,226,187,results,even significantly better,than,per-cvae model,even significantly better than per-cvae model,0.5838570594787598
translation,226,187,results,per-cvae model,to generate,diverse responses,per-cvae model to generate diverse responses,0.7051904201507568
translation,226,187,results,results,has,distinct - 2 metric,results has distinct - 2 metric,0.5815862417221069
translation,226,190,results,gdr model,generate,high-quality responses,gdr model generate high-quality responses,0.663301944732666
translation,226,190,results,significantly improves,generate,high-quality responses,significantly improves generate high-quality responses,0.6418559551239014
translation,226,190,results,persona consistency,generate,high-quality responses,persona consistency generate high-quality responses,0.5943396687507629
translation,226,190,results,high-quality responses,like,transformer and gpmn,high-quality responses like transformer and gpmn,0.6420948505401611
translation,226,190,results,gdr model,has,significantly improves,gdr model has significantly improves,0.5908212065696716
translation,226,190,results,significantly improves,has,persona consistency,significantly improves has persona consistency,0.58729088306427
translation,226,204,results,grdr approach,serves as,foil,grdr approach serves as foil,0.6381352543830872
translation,226,204,results,foil,to,our gdr model,foil to our gdr model,0.6063027381896973
translation,226,204,results,results,has,grdr approach,results has grdr approach,0.5704206228256226
translation,226,210,results,relevance scores,of,"gr , grdr , and g","relevance scores of gr , grdr , and g",0.5890081524848938
translation,226,210,results,"gr , grdr , and g",are a little inferior to,t,"gr , grdr , and g are a little inferior to t",0.7117656469345093
translation,226,210,results,"gr , grdr , and g",are a little inferior to,t,"gr , grdr , and g are a little inferior to t",0.7117656469345093
translation,226,210,results,t,on,relevance score,t on relevance score,0.5282663702964783
translation,226,210,results,not significantly better,than,t,not significantly better than t,0.6775215268135071
translation,226,210,results,not significantly better,on,relevance score,not significantly better on relevance score,0.5039803385734558
translation,226,210,results,t,on,relevance score,t on relevance score,0.5282663702964783
translation,226,210,results,results,observed that,relevance scores,results observed that relevance scores,0.6247431635856628
translation,227,172,ablation-analysis,improvements,especially,improvements,improvements especially improvements,0.6831343770027161
translation,227,172,ablation-analysis,improvements,on,ellipsis resolution,improvements on ellipsis resolution,0.534369707107544
translation,227,172,ablation-analysis,improvements,on,ellipsis resolution,improvements on ellipsis resolution,0.534369707107544
translation,227,172,ablation-analysis,ellipsis resolution,higher than,co-reference resolution,ellipsis resolution higher than co-reference resolution,0.6959561705589294
translation,227,172,ablation-analysis,ellipsis resolution,indicate,copy mechanism,ellipsis resolution indicate copy mechanism,0.5602823495864868
translation,227,172,ablation-analysis,copy mechanism,crucial for,recovery,copy mechanism crucial for recovery,0.7491615414619446
translation,227,172,ablation-analysis,recovery,of,ellipsis and co-reference,recovery of ellipsis and co-reference,0.5856282114982605
translation,227,172,ablation-analysis,ablation analysis,has,improvements,ablation analysis has improvements,0.5440875291824341
translation,227,31,baselines,new dataset,based on,cam-rest676,new dataset based on cam-rest676,0.691290557384491
translation,227,31,baselines,cam-rest676,for,ellipsis and co-reference resolution,cam-rest676 for ellipsis and co-reference resolution,0.624298095703125
translation,227,31,baselines,ellipsis and co-reference resolution,in the context of,task - oriented dialogue,ellipsis and co-reference resolution in the context of task - oriented dialogue,0.6135833859443665
translation,227,149,baselines,gecor 1/2,with,copy / gated copy mechanism,gecor 1/2 with copy / gated copy mechanism,0.6709601283073425
translation,227,149,baselines,gecor model,with,copy / gated copy mechanism,gecor model with copy / gated copy mechanism,0.6411669850349426
translation,227,149,baselines,gecor 1/2,has,gecor model,gecor 1/2 has gecor model,0.5531553626060486
translation,227,149,baselines,baselines,has,gecor 1/2,baselines has gecor 1/2,0.563365638256073
translation,227,160,baselines,resolution task,compared,gecor model,resolution task compared gecor model,0.6015514135360718
translation,227,160,baselines,gecor model,with,baseline model,gecor model with baseline model,0.5947144031524658
translation,227,160,baselines,seq2seq neural network model,identifies and recovers,ellipsis,seq2seq neural network model identifies and recovers ellipsis,0.6541203856468201
translation,227,160,baselines,ellipsis,for,short texts,ellipsis for short texts,0.5116025805473328
translation,227,75,experimental-setup,gecor,tokenize,input user utterance,gecor tokenize input user utterance,0.7101715803146362
translation,227,75,experimental-setup,gecor,tokenize,the dialogue context,gecor tokenize the dialogue context,0.735340416431427
translation,227,75,experimental-setup,experimental setup,In,gecor,experimental setup In gecor,0.5757619142532349
translation,227,161,experiments,dialogue task,compared,multitask learning framework,dialogue task compared multitask learning framework,0.6171245574951172
translation,227,161,experiments,multitask learning framework,with,baseline model tscp,multitask learning framework with baseline model tscp,0.619600236415863
translation,227,161,experiments,baseline model tscp,is,seq2seq model,baseline model tscp is seq2seq model,0.5498189926147461
translation,227,161,experiments,seq2seq model,enhanced with,reinforcement learning,seq2seq model enhanced with reinforcement learning,0.6307171583175659
translation,227,155,hyperparameters,size of hidden states and word embeddings,set to,50,size of hidden states and word embeddings set to 50,0.6616835594177246
translation,227,156,hyperparameters,| v |,set to,800,| v | set to 800,0.6714462637901306
translation,227,156,hyperparameters,batch size,set to,32,batch size set to 32,0.733751654624939
translation,227,156,hyperparameters,vocabulary size,has,| v |,vocabulary size has | v |,0.5577567219734192
translation,227,156,hyperparameters,vocabulary size,has,batch size,vocabulary size has batch size,0.5674017667770386
translation,227,156,hyperparameters,hyperparameters,has,vocabulary size,hyperparameters has vocabulary size,0.5002733469009399
translation,227,156,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,227,157,hyperparameters,our models,via,adam optimizer,our models via adam optimizer,0.6668140292167664
translation,227,157,hyperparameters,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,227,157,hyperparameters,adam optimizer,with,decay parameter,adam optimizer with decay parameter,0.5803123116493225
translation,227,157,hyperparameters,learning rate,of,0.003,learning rate of 0.003,0.6038756370544434
translation,227,157,hyperparameters,decay parameter,of,0.5,decay parameter of 0.5,0.6376051306724548
translation,227,157,hyperparameters,hyperparameters,trained,our models,hyperparameters trained our models,0.7182732224464417
translation,227,158,hyperparameters,early stopping and dropout,to prevent,overfitting,early stopping and dropout to prevent overfitting,0.6117830872535706
translation,227,158,hyperparameters,dropout rate,set to,0.5,dropout rate set to 0.5,0.6910414695739746
translation,227,158,hyperparameters,hyperparameters,has,early stopping and dropout,hyperparameters has early stopping and dropout,0.5186657309532166
translation,227,158,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,227,6,model,unified end-to- end generative ellipsis,CO,reference resolution model ( gecor ),unified end-to- end generative ellipsis CO reference resolution model ( gecor ),0.5029836893081665
translation,227,6,model,model,propose,unified end-to- end generative ellipsis,model propose unified end-to- end generative ellipsis,0.6034026145935059
translation,227,6,model,model,CO,reference resolution model ( gecor ),model CO reference resolution model ( gecor ),0.5247713327407837
translation,227,17,model,endto-end generative ellipsis and co-reference resolution model ( gecor ),for,task - oriented dialogue,endto-end generative ellipsis and co-reference resolution model ( gecor ) for task - oriented dialogue,0.5842520594596863
translation,227,17,model,model,propose,endto-end generative ellipsis and co-reference resolution model ( gecor ),model propose endto-end generative ellipsis and co-reference resolution model ( gecor ),0.6374921202659607
translation,227,22,model,gecor,into,end-toend task - oriented dialogue system,gecor into end-toend task - oriented dialogue system,0.5347879528999329
translation,227,22,model,end-toend task - oriented dialogue system,in,multi-task learning framework,end-toend task - oriented dialogue system in multi-task learning framework,0.5084295272827148
translation,227,22,model,model,incorporate,gecor,model incorporate gecor,0.7476984262466431
translation,227,23,model,two encoders,one for,user utterance,two encoders one for user utterance,0.6609572768211365
translation,227,23,model,two encoders,other for,dialogue context,two encoders other for dialogue context,0.6241041421890259
translation,227,23,model,one decoder,for predicting,dialogue states,one decoder for predicting dialogue states,0.7079795598983765
translation,227,23,model,third decoder,for generating,system responses,third decoder for generating system responses,0.7055646181106567
translation,227,23,model,second decoder,has,for generating complete user utterances,second decoder has for generating complete user utterances,0.5555013418197632
translation,227,80,model,forward and backward hidden states,over,input embeddings,forward and backward hidden states over input embeddings,0.6417840719223022
translation,227,80,model,input embeddings,from,embedding layer,input embeddings from embedding layer,0.5237916707992554
translation,227,80,model,input embeddings,concatenated to form,hidden states,input embeddings concatenated to form hidden states,0.6743404269218445
translation,227,80,model,hidden states,of,two encoders,hidden states of two encoders,0.5835345983505249
translation,227,80,model,model,has,forward and backward hidden states,model has forward and backward hidden states,0.56721031665802
translation,227,227,model,unified end-to - end generative model,for both,ellipsis and co-reference resolution,unified end-to - end generative model for both ellipsis and co-reference resolution,0.6193321347236633
translation,227,227,model,ellipsis and co-reference resolution,in,multi-turn dialogues,ellipsis and co-reference resolution in multi-turn dialogues,0.4821148216724396
translation,227,168,results,gecor model,with,copy mechanism ( gecor 1 ),gecor model with copy mechanism ( gecor 1 ),0.6560055017471313
translation,227,168,results,copy mechanism ( gecor 1 ),improves,exact match rate ( em ),copy mechanism ( gecor 1 ) improves exact match rate ( em ),0.6941290497779846
translation,227,168,results,exact match rate ( em ),by,more than 17 points,exact match rate ( em ) by more than 17 points,0.5742878913879395
translation,227,168,results,exact match rate ( em ),by,more than 15 points,exact match rate ( em ) by more than 15 points,0.5731598138809204
translation,227,168,results,exact match rate ( em ),by,more than 18 points,exact match rate ( em ) by more than 18 points,0.5748669505119324
translation,227,168,results,more than 17 points,on,ellipsis version data,more than 17 points on ellipsis version data,0.5288593173027039
translation,227,168,results,more than 15 points,on,co-reference data,more than 15 points on co-reference data,0.5592160224914551
translation,227,168,results,more than 18 points,on,mixed data,more than 18 points on mixed data,0.5428271889686584
translation,227,170,results,gecor model,achieves,consistent and significant improvements,gecor model achieves consistent and significant improvements,0.6447085738182068
translation,227,170,results,consistent and significant improvements,over,baseline,consistent and significant improvements over baseline,0.7379213571548462
translation,227,170,results,consistent and significant improvements,in terms of,f 1,consistent and significant improvements in terms of f 1,0.7714408040046692
translation,227,170,results,consistent and significant improvements,in terms of,resolution f 1,consistent and significant improvements in terms of resolution f 1,0.788507878780365
translation,227,170,results,baseline,in terms of,bleu,baseline in terms of bleu,0.5934461355209351
translation,227,170,results,results,has,gecor model,results has gecor model,0.4962286353111267
translation,227,173,results,effect of the two copy mechanisms,Comparing,gecor 1,effect of the two copy mechanisms Comparing gecor 1,0.6510509848594666
translation,227,173,results,effect of the two copy mechanisms,find that,gating,effect of the two copy mechanisms find that gating,0.644548773765564
translation,227,173,results,gecor 1,against,gecor 2,gecor 1 against gecor 2,0.659828782081604
translation,227,173,results,gecor 1,against,gated copy mechanism,gecor 1 against gated copy mechanism,0.7021188735961914
translation,227,173,results,gecor 1,against,gating,gecor 1 against gating,0.7305217385292053
translation,227,173,results,gecor 1,against,helpful,gecor 1 against helpful,0.7179743051528931
translation,227,173,results,gecor 1,against,fragment,gecor 1 against fragment,0.7151265144348145
translation,227,173,results,gecor 1,against,sequence - based metrics,gecor 1 against sequence - based metrics,0.6400430798530579
translation,227,173,results,gecor 2,with,gated copy mechanism,gecor 2 with gated copy mechanism,0.6770273447036743
translation,227,173,results,gating,between,copy and generation,gating between copy and generation,0.6408700942993164
translation,227,173,results,helpful,in terms of,word- level quality ( f 1 and resolution f 1 score ),helpful in terms of word- level quality ( f 1 and resolution f 1 score ),0.7125864624977112
translation,227,173,results,helpful,in terms of,sequence - based metrics,helpful in terms of sequence - based metrics,0.7229320406913757
translation,227,173,results,results,has,effect of the two copy mechanisms,results has effect of the two copy mechanisms,0.5360739827156067
translation,227,180,results,gecor model,beats,baseline model,gecor model beats baseline model,0.7015731334686279
translation,227,180,results,results,find that,gecor model,results find that gecor model,0.6068624258041382
translation,227,184,results,input user utterances,are,complete,input user utterances are complete,0.574722170829773
translation,227,184,results,gecor model,amazingly generate,92.03 % utterances,gecor model amazingly generate 92.03 % utterances,0.6798665523529053
translation,227,184,results,92.03 % utterances,that exactly match,input utterances,92.03 % utterances that exactly match input utterances,0.6454168558120728
translation,227,184,results,input user utterances,has,gecor model,input user utterances has gecor model,0.5343019962310791
translation,227,184,results,complete,has,gecor model,complete has gecor model,0.560248613357544
translation,227,217,results,baseline,see that,our model,baseline see that our model,0.6472579836845398
translation,227,217,results,our model,improves,success f 1 score,our model improves success f 1 score,0.6762903928756714
translation,227,217,results,success f 1 score,by,nearly 4 points,success f 1 score by nearly 4 points,0.5831519365310669
translation,227,217,results,nearly 4 points,on,co-reference dataset,nearly 4 points on co-reference dataset,0.5353478789329529
translation,227,217,results,results,In comparison to,baseline,results In comparison to baseline,0.6747597455978394
translation,227,217,results,results,see that,our model,results see that our model,0.6820751428604126
translation,227,218,results,our model,achieves,2.7 points and 0.8 points,our model achieves 2.7 points and 0.8 points,0.6563690304756165
translation,227,218,results,2.7 points and 0.8 points,of,success f 1 score improvements,2.7 points and 0.8 points of success f 1 score improvements,0.5522355437278748
translation,227,218,results,mixed and ellipsis dataset,has,our model,mixed and ellipsis dataset has our model,0.6013955473899841
translation,227,218,results,results,On,mixed and ellipsis dataset,results On mixed and ellipsis dataset,0.503910481929779
translation,227,219,results,resolution performance,of,integrated gecor,resolution performance of integrated gecor,0.6334106922149658
translation,227,219,results,results,has,resolution performance,results has resolution performance,0.5608176589012146
translation,227,223,results,proposed multi-task learning framework,for,end-to - end dialogue,proposed multi-task learning framework for end-to - end dialogue,0.593207597732544
translation,227,223,results,task completion rate,by incorporating,auxiliary ellipsis / co-reference resolution task,task completion rate by incorporating auxiliary ellipsis / co-reference resolution task,0.6199473738670349
translation,227,223,results,results,demonstrate,proposed multi-task learning framework,results demonstrate proposed multi-task learning framework,0.6057289242744446
translation,228,6,model,agglomerative convolutional neural network,takes,groups of features,agglomerative convolutional neural network takes groups of features,0.6410620808601379
translation,228,6,model,agglomerative convolutional neural network,learns,mention and mention - pair embeddings,agglomerative convolutional neural network learns mention and mention - pair embeddings,0.6636181473731995
translation,228,6,model,mention and mention - pair embeddings,for,coreference resolution,mention and mention - pair embeddings for coreference resolution,0.5504802465438843
translation,228,6,model,model,introduce,agglomerative convolutional neural network,model introduce agglomerative convolutional neural network,0.6080620884895325
translation,228,26,model,existing corpus,for,character identification,existing corpus for character identification,0.6104062795639038
translation,228,26,model,end-to - end deep- learning system,combines,neural models,end-to - end deep- learning system combines neural models,0.7196307182312012
translation,228,26,model,neural models,for,coreference resolution,neural models for coreference resolution,0.5275750160217285
translation,228,26,model,neural models,for,entity linking,neural models for entity linking,0.5679468512535095
translation,228,26,model,model,augment and correct,existing corpus,model augment and correct existing corpus,0.7099863290786743
translation,228,26,model,model,propose,end-to - end deep- learning system,model propose end-to - end deep- learning system,0.6470071077346802
translation,228,30,model,coreference resolution,to learn,"mention , mention - pair , and cluster embeddings","coreference resolution to learn mention , mention - pair , and cluster embeddings",0.5401281118392944
translation,228,8,results,coreference resolution model,shows,comparable results,coreference resolution model shows comparable results,0.6779442429542542
translation,228,8,results,comparable results,to,other state - of - the - art systems,comparable results to other state - of - the - art systems,0.5491811037063599
translation,228,8,results,results,has,coreference resolution model,results has coreference resolution model,0.546944260597229
translation,229,5,model,multi-domain wizard - of - oz dataset ( multiwoz ),fully - labeled collection of,human-human written conversations,multi-domain wizard - of - oz dataset ( multiwoz ) fully - labeled collection of human-human written conversations,0.748223602771759
translation,229,5,model,human-human written conversations,spanning over,multiple domains and topics,human-human written conversations spanning over multiple domains and topics,0.7181952595710754
translation,229,5,model,model,introduce,multi-domain wizard - of - oz dataset ( multiwoz ),model introduce multi-domain wizard - of - oz dataset ( multiwoz ),0.6370818614959717
translation,229,205,model,data-collection pipeline,based on,crowd- sourcing,data-collection pipeline based on crowd- sourcing,0.6401954293251038
translation,229,205,model,"large scale , linguistically rich corpus",of,human-human conversations,"large scale , linguistically rich corpus of human-human conversations",0.5297157764434814
translation,229,205,model,model,established,data-collection pipeline,model established data-collection pipeline,0.7136672139167786
translation,229,157,results,compares favorably,to,all other data sets,compares favorably to all other data sets,0.5684983134269714
translation,229,157,results,all other data sets,on,most of the metrics,all other data sets on most of the metrics,0.4840884804725647
translation,229,157,results,most of the metrics,with,number of total dialogues,most of the metrics with number of total dialogues,0.6147335171699524
translation,229,157,results,most of the metrics,with,average number of tokens per turn,most of the metrics with average number of tokens per turn,0.6262702345848083
translation,229,157,results,most of the metrics,with,total number of unique tokens,most of the metrics with total number of unique tokens,0.5957016944885254
translation,229,157,results,total number of unique tokens,as,most prominent ones,total number of unique tokens as most prominent ones,0.4916960597038269
translation,229,157,results,our corpus,has,compares favorably,our corpus has compares favorably,0.6176867485046387
translation,229,157,results,results,shows,our corpus,results shows our corpus,0.6853793263435364
translation,229,170,results,performance,of,model,performance of model,0.6080846190452576
translation,229,170,results,performance,is,consecutively poorer,performance is consecutively poorer,0.5911804437637329
translation,229,170,results,consecutively poorer,on,new dataset,consecutively poorer on new dataset,0.5853142142295837
translation,229,170,results,consecutively poorer,compared to,woz2.0,consecutively poorer compared to woz2.0,0.6935223340988159
translation,229,182,results,best results,on,cam676 corpus,best results on cam676 corpus,0.5095129013061523
translation,229,182,results,cam676 corpus,obtained with,bidirectional gru cell,cam676 corpus obtained with bidirectional gru cell,0.6631492972373962
translation,229,182,results,results,has,best results,results has best results,0.542218804359436
translation,229,183,results,lstm cell,serving as,decoder,lstm cell serving as decoder,0.6112877130508423
translation,229,183,results,lstm cell,serving as,encoder,lstm cell serving as encoder,0.6068469285964966
translation,229,183,results,encoder,achieved,highest score,encoder achieved highest score,0.7574911117553711
translation,229,183,results,highest score,with,global type of attention,highest score with global type of attention,0.5129783153533936
translation,229,183,results,multiwoz dataset,has,lstm cell,multiwoz dataset has lstm cell,0.6000424027442932
translation,229,183,results,results,case of,multiwoz dataset,results case of multiwoz dataset,0.7105297446250916
translation,229,185,results,model,achieves,almost perfect score,model achieves almost perfect score,0.7141677141189575
translation,229,185,results,almost perfect score,on,inform metric,almost perfect score on inform metric,0.5511714816093445
translation,229,185,results,almost perfect score,taking the advantage of,oracle belief state signal,almost perfect score taking the advantage of oracle belief state signal,0.6771402955055237
translation,229,185,results,inform metric,on,cam676 dataset,inform metric on cam676 dataset,0.5979623794555664
translation,229,185,results,results,has,model,results has model,0.5339115858078003
translation,229,186,results,baseline models,obtain,almost 30 % lower score,baseline models obtain almost 30 % lower score,0.5851498246192932
translation,229,186,results,almost 30 % lower score,on,inform metric,almost 30 % lower score on inform metric,0.5282660126686096
translation,229,186,results,inform metric,on,new corpus,inform metric on new corpus,0.6045060753822327
translation,229,187,results,attention,improves,score,attention improves score,0.6809841394424438
translation,229,187,results,score,on,success metric,score on success metric,0.500166654586792
translation,229,187,results,success metric,on,new dataset,success metric on new dataset,0.5126380324363708
translation,229,187,results,results,addition of,attention,results addition of attention,0.5785763263702393
translation,229,188,results,best model,on,multiwoz,best model on multiwoz,0.6028790473937988
translation,229,188,results,falling behind,by,large margin,falling behind by large margin,0.6392030119895935
translation,229,188,results,large margin,in comparison to,results,large margin in comparison to results,0.6344194412231445
translation,229,188,results,results,on,cam676 corpus,results on cam676 corpus,0.5336583852767944
translation,229,188,results,results,on,cam676 corpus,results on cam676 corpus,0.5336583852767944
translation,230,8,model,method,to embed,kb,method to embed kb,0.6491836905479431
translation,230,8,model,kb,directly into,model parameters,kb directly into model parameters,0.6378352046012878
translation,230,8,model,model,propose,method,model propose method,0.6280754208564758
translation,230,9,model,kb,via,finetuning,kb via finetuning,0.7099869251251221
translation,230,28,model,method,to store,kb,method to store kb,0.6224727630615234
translation,230,28,model,kb,directly into,model parameters,kb directly into model parameters,0.6378352046012878
translation,230,28,model,kb,using,novel knowledge embedded ( ke ) approach,kb using novel knowledge embedded ( ke ) approach,0.6624625325202942
translation,230,28,model,model,propose,method,model propose method,0.6280754208564758
translation,230,29,model,dynamically changing kbs,via,fine-tuning,dynamically changing kbs via fine-tuning,0.6788577437400818
translation,230,109,results,gpt2,just on,training dialogues,gpt2 just on training dialogues,0.68419349193573
translation,230,109,results,gpt2,does not perform,well,gpt2 does not perform well,0.7074059247970581
translation,230,109,results,training,has,gpt2,training has gpt2,0.6032498478889465
translation,230,109,results,results,has,training,results has training,0.5440090298652649
translation,230,110,results,ke dialogues,in,training,ke dialogues in training,0.5900821089744568
translation,230,110,results,gpt2,consistently generates,correct response,gpt2 consistently generates correct response,0.7244294285774231
translation,230,110,results,correct response,in,both test sets,correct response in both test sets,0.477440744638443
translation,230,110,results,ke dialogues,has,gpt2,ke dialogues has gpt2,0.6608574986457825
translation,230,110,results,training,has,gpt2,training has gpt2,0.6032498478889465
translation,230,110,results,results,by using,ke dialogues,results by using ke dialogues,0.6264992356300354
translation,230,119,results,gpt2 + ke,able to achieve,better performance,gpt2 + ke able to achieve better performance,0.6472972631454468
translation,230,119,results,better performance,than,current state - of - the- art,better performance than current state - of - the- art,0.5558580756187439
translation,230,119,results,1 % improvement,with,much shorter input sequence ( 156 vs 393 ),1 % improvement with much shorter input sequence ( 156 vs 393 ),0.6262520551681519
translation,230,119,results,results,has,gpt2 + ke,results has gpt2 + ke,0.5377054214477539
translation,230,120,results,human evaluation,notice,significant improvement,human evaluation notice significant improvement,0.6825678944587708
translation,230,120,results,significant improvement,in favor of,gpt2 models,significant improvement in favor of gpt2 models,0.6764212250709534
translation,230,120,results,results,From,human evaluation,results From human evaluation,0.49978944659233093
translation,230,128,results,gpt2,performs,better,gpt2 performs better,0.6779499053955078
translation,230,128,results,better,than,existing baselines,better than existing baselines,0.6160018444061279
translation,230,128,results,results,notice that,gpt2,results notice that gpt2,0.5965799689292908
translation,230,129,results,gpt2,with,kb as,gpt2 with kb as,0.6847190856933594
translation,230,129,results,input,does not perform,as well,input does not perform as well,0.7177132964134216
translation,230,129,results,as well,as,other baselines,as well as other baselines,0.6479501128196716
translation,230,129,results,kb as,has,input,kb as has input,0.6296879053115845
translation,230,129,results,results,has,gpt2,results has gpt2,0.5226525664329529
translation,230,130,results,gpt2,fine-tuned with,ke dialogues,gpt2 fine-tuned with ke dialogues,0.8165993094444275
translation,230,130,results,gpt2,performs,almost as well,gpt2 performs almost as well,0.6528317928314209
translation,230,130,results,almost as well,as,"dff ( qin et al. , 2020 )","almost as well as dff ( qin et al. , 2020 )",0.5883937478065491
translation,230,130,results,"dff ( qin et al. , 2020 )",in terms of,f1 - score,"dff ( qin et al. , 2020 ) in terms of f1 - score",0.6539132595062256
translation,230,130,results,"dff ( qin et al. , 2020 )",from,human judgments,"dff ( qin et al. , 2020 ) from human judgments",0.5535178780555725
translation,230,130,results,- based models,perform,significantly better,- based models perform significantly better,0.582004189491272
translation,230,130,results,significantly better,in terms of,humanness and correctness,significantly better in terms of humanness and correctness,0.6486120820045471
translation,230,130,results,human judgments,has,- based models,human judgments has - based models,0.5598532557487488
translation,230,130,results,results,has,gpt2,results has gpt2,0.5226525664329529
translation,230,144,results,gpt,trained with,ke dialogues,gpt trained with ke dialogues,0.7602148056030273
translation,230,144,results,ke dialogues,as well as,damd,ke dialogues as well as damd,0.6746300458908081
translation,230,144,results,damd,trained using,dst and template responses,damd trained using dst and template responses,0.7312889099121094
translation,230,144,results,damd,in,automatic and human evaluation,damd in automatic and human evaluation,0.5338066816329956
translation,230,145,results,damd,achieved,85.40 inform and 70.40 success score,damd achieved 85.40 inform and 70.40 success score,0.7154659628868103
translation,230,145,results,responses,are,relexicalize,responses are relexicalize,0.6461700797080994
translation,230,145,results,responses,use,our scorer,responses use our scorer,0.7001875042915344
translation,230,145,results,results,are,significantly lower,results are significantly lower,0.566371738910675
translation,230,145,results,original scorer,has,damd,original scorer has damd,0.6005520820617676
translation,230,145,results,original scorer,has,results,original scorer has results,0.516650915145874
translation,230,145,results,responses,has,results,responses has results,0.5603074431419373
translation,230,145,results,our scorer,has,results,our scorer has results,0.6073575019836426
translation,230,145,results,results,Using,original scorer,results Using original scorer,0.6722202301025391
translation,230,145,results,results,are,significantly lower,results are significantly lower,0.566371738910675
translation,230,169,results,blue score,linearly increase with,number of templates,blue score linearly increase with number of templates,0.6746680736541748
translation,230,169,results,number of templates,used in,training,number of templates used in training,0.676215410232544
translation,230,169,results,results,observe that,blue score,results observe that blue score,0.5925090312957764
translation,231,4,baselines,chat-oriented dialogue system,based on,vector space model framework,chat-oriented dialogue system based on vector space model framework,0.6241702437400818
translation,231,4,baselines,iris ( informal response interactive system ),has,chat-oriented dialogue system,iris ( informal response interactive system ) has chat-oriented dialogue system,0.5607395768165588
translation,231,5,model,chat capabilities,on,dual search strategy,chat capabilities on dual search strategy,0.5987982749938965
translation,231,5,model,dual search strategy,over,large collection of dialogue samples,dual search strategy over large collection of dialogue samples,0.6754169464111328
translation,231,5,model,model,builds,chat capabilities,model builds chat capabilities,0.7155326008796692
translation,231,15,model,chat-oriented dialogue system,based on,vector space model framework,chat-oriented dialogue system based on vector space model framework,0.6241702437400818
translation,231,15,model,iris ( informal response interactive system ),has,chat-oriented dialogue system,iris ( informal response interactive system ) has chat-oriented dialogue system,0.5607395768165588
translation,231,15,model,model,introduce,iris ( informal response interactive system ),model introduce iris ( informal response interactive system ),0.639005720615387
translation,231,127,model,chat-oriented dialogue system,based on,vector space model framework,chat-oriented dialogue system based on vector space model framework,0.6241702437400818
translation,231,127,model,iris ( informal response interactive system ),has,chat-oriented dialogue system,iris ( informal response interactive system ) has chat-oriented dialogue system,0.5607395768165588
translation,231,127,model,model,presented,iris ( informal response interactive system ),model presented iris ( informal response interactive system ),0.6857965588569641
translation,231,128,model,chat capabilities,on,dual search strategy,chat capabilities on dual search strategy,0.5987982749938965
translation,231,128,model,dual search strategy,over,large collection of movie dialogues,dual search strategy over large collection of movie dialogues,0.6760002970695496
translation,231,128,model,model,builds,chat capabilities,model builds chat capabilities,0.7155326008796692
translation,232,151,ablation-analysis,prototype -kr,select,higherquality knowledge facts,prototype -kr select higherquality knowledge facts,0.6663305759429932
translation,232,151,ablation-analysis,effectiveness,of,prototype-krg,effectiveness of prototype-krg,0.5958835482597351
translation,232,158,ablation-analysis,improvement,of,relevance,improvement of relevance,0.6014711856842041
translation,232,158,ablation-analysis,improvement,of,overall performance,improvement of overall performance,0.5743282437324524
translation,232,158,ablation-analysis,ablation analysis,seen that,diversity and informativeness,ablation analysis seen that diversity and informativeness,0.6616080403327942
translation,232,167,ablation-analysis,knowledge facts,retrieved by,pkr and enm,knowledge facts retrieved by pkr and enm,0.5962229371070862
translation,232,167,ablation-analysis,similar performance,find,metric,similar performance find metric,0.595114529132843
translation,232,167,ablation-analysis,metric,has,dist2,metric has dist2,0.6375086903572083
translation,232,168,ablation-analysis,com-,pare,pkr and enm,com- pare pkr and enm,0.6659778356552124
translation,232,168,ablation-analysis,performance,more than,- enm,performance more than - enm,0.6248646378517151
translation,232,169,ablation-analysis,most metrics,has,drop,most metrics has drop,0.61003577709198
translation,232,169,ablation-analysis,drop,has,dramatically,drop has dramatically,0.6238924264907837
translation,232,169,ablation-analysis,ablation analysis,has,pkr - enm,ablation analysis has pkr - enm,0.5589010119438171
translation,232,132,baselines,s2s,implement them based on,pytorch seq2seq framework,s2s implement them based on pytorch seq2seq framework,0.6884310841560364
translation,232,132,baselines,approach,implement them based on,pytorch seq2seq framework,approach implement them based on pytorch seq2seq framework,0.7399957776069641
translation,232,152,baselines,baselines,has,vs. ir - based baselines,baselines has vs. ir - based baselines,0.5670388340950012
translation,232,133,experimental-setup,gends,use,tensorflow implementation,gends use tensorflow implementation,0.6291029453277588
translation,232,133,experimental-setup,experimental setup,For,gends,experimental setup For gends,0.6319554448127747
translation,232,135,experimental-setup,vocabulary,set to,"30,000","vocabulary set to 30,000",0.6806634068489075
translation,232,135,experimental-setup,word embedding dimension,is,300,word embedding dimension is 300,0.5878181457519531
translation,232,135,experimental-setup,entity / relation embedding,initialized from,100 - dimensional pre-trained embedding,entity / relation embedding initialized from 100 - dimensional pre-trained embedding,0.6582334637641907
translation,232,135,experimental-setup,100 - dimensional pre-trained embedding,learned by,"transe ( bordes et al. , 2013 )","100 - dimensional pre-trained embedding learned by transe ( bordes et al. , 2013 )",0.6599400639533997
translation,232,135,experimental-setup,rnns,implemented as,1024 - dimensional grus,rnns implemented as 1024 - dimensional grus,0.6534916162490845
translation,232,135,experimental-setup,adam,optimizing,model,adam optimizing model,0.7453351616859436
translation,232,135,experimental-setup,model,with,initial learning rate,model with initial learning rate,0.5982702374458313
translation,232,135,experimental-setup,model,with,batch size,model with batch size,0.6316491365432739
translation,232,135,experimental-setup,halved,if,perplexity,halved if perplexity,0.642487108707428
translation,232,135,experimental-setup,halved,if,perplexity,halved if perplexity,0.642487108707428
translation,232,135,experimental-setup,halved,if,perplexity,halved if perplexity,0.642487108707428
translation,232,135,experimental-setup,perplexity,on,validation set,perplexity on validation set,0.5367031693458557
translation,232,135,experimental-setup,perplexity,on,validation set,perplexity on validation set,0.5367031693458557
translation,232,135,experimental-setup,perplexity,starts to,increase,perplexity starts to increase,0.6669461727142334
translation,232,135,experimental-setup,perplexity,on,validation set,perplexity on validation set,0.5367031693458557
translation,232,135,experimental-setup,stopped,if,perplexity,stopped if perplexity,0.6432392597198486
translation,232,135,experimental-setup,perplexity,on,validation set,perplexity on validation set,0.5367031693458557
translation,232,135,experimental-setup,perplexity,increases in,two successive epochs,perplexity increases in two successive epochs,0.6459808349609375
translation,232,135,experimental-setup,initial learning rate,has,0.0001,initial learning rate has 0.0001,0.5173112750053406
translation,232,135,experimental-setup,batch size,has,64,batch size has 64,0.6416366696357727
translation,232,136,experimental-setup,beam width,set to,10,beam width set to 10,0.7460395097732544
translation,232,136,experimental-setup,inference,has,beam width,inference has beam width,0.5555850863456726
translation,232,136,experimental-setup,experimental setup,In,inference,experimental setup In inference,0.5383538603782654
translation,232,7,model,model,proposes,novel knowledge selection approach,model proposes novel knowledge selection approach,0.7010608911514282
translation,232,7,model,model,proposes,knowledge - aware generative model,model proposes knowledge - aware generative model,0.6810964345932007
translation,232,7,model,model,proposes,prototype - krg,model proposes prototype - krg,0.7138364911079407
translation,232,8,model,query,first retrieves,set of prototype dialogues,query first retrieves set of prototype dialogues,0.7586241364479065
translation,232,8,model,set of prototype dialogues,relevant to,query,set of prototype dialogues relevant to query,0.6700751185417175
translation,232,8,model,model,Given,query,model Given query,0.7787366509437561
translation,232,20,model,structured open-domain commonsense knowledge graph,into,single-turn dialogue response generation,structured open-domain commonsense knowledge graph into single-turn dialogue response generation,0.5510081648826599
translation,232,20,model,model,introducing,structured open-domain commonsense knowledge graph,model introducing structured open-domain commonsense knowledge graph,0.6592661142349243
translation,232,194,model,model,propose,novel knowledge selection method,model propose novel knowledge selection method,0.6409825682640076
translation,232,194,model,model,for,open-domain knowledgeaware dialogue generation,model for open-domain knowledgeaware dialogue generation,0.5785709619522095
translation,232,12,results,responses,generated by,our approach,responses generated by our approach,0.6718106269836426
translation,232,12,results,responses,more relevant to,context,responses more relevant to context,0.7464234828948975
translation,232,12,results,responses,have,comparable informativeness,responses have comparable informativeness,0.5127514600753784
translation,232,12,results,our approach,more relevant to,context,our approach more relevant to context,0.6991764307022095
translation,232,12,results,ir ( retrieval ) - based baselines,has,responses,ir ( retrieval ) - based baselines has responses,0.5974817276000977
translation,232,12,results,results,compared to,ir ( retrieval ) - based baselines,results compared to ir ( retrieval ) - based baselines,0.6616357564926147
translation,232,148,results,leadership,in terms of,entropy,leadership in terms of entropy,0.7302331328392029
translation,232,148,results,entropy,compared to,transformer,entropy compared to transformer,0.7144071459770203
translation,232,148,results,slightly loses,has,leadership,slightly loses has leadership,0.5905815362930298
translation,232,148,results,results,has,prototype -krg,results has prototype -krg,0.5736082196235657
translation,232,149,results,advantages,of,previous sota ccm and our prototype - krg,advantages of previous sota ccm and our prototype - krg,0.6022942066192627
translation,232,149,results,previous sota ccm and our prototype - krg,show,knowledge,previous sota ccm and our prototype - krg show knowledge,0.6034589409828186
translation,232,149,results,knowledge,help,dialogue generation,knowledge help dialogue generation,0.6434695720672607
translation,232,149,results,results,has,advantages,results has advantages,0.5523803234100342
translation,232,150,results,prototype -krg,is,notably better,prototype -krg is notably better,0.5948856472969055
translation,232,150,results,notably better,in terms of,knowledge utilization,notably better in terms of knowledge utilization,0.6701194643974304
translation,232,150,results,notably better,in terms of,diversity,notably better in terms of diversity,0.687419593334198
translation,232,150,results,notably better,in terms of,informativeness,notably better in terms of informativeness,0.6692764163017273
translation,232,150,results,two knowledge- aware baselines,has,prototype -krg,two knowledge- aware baselines has prototype -krg,0.5959910750389099
translation,232,150,results,gends and gends,has,prototype -krg,gends and gends has prototype -krg,0.6703027486801147
translation,232,150,results,results,Compared with,two knowledge- aware baselines,results Compared with two knowledge- aware baselines,0.6219666004180908
translation,232,154,results,other models,in terms of,dist - 1/2 and the entropy,other models in terms of dist - 1/2 and the entropy,0.674301028251648
translation,232,154,results,ir and ir - rerank,has,significantly outperform,ir and ir - rerank has significantly outperform,0.5001022815704346
translation,232,154,results,significantly outperform,has,other models,significantly outperform has other models,0.5593274831771851
translation,232,154,results,results,has,ir and ir - rerank,results has ir and ir - rerank,0.5703012347221375
translation,232,159,results,comparable performance,in terms of,diversity,comparable performance in terms of diversity,0.7192018628120422
translation,232,159,results,notably better performance,in,remaining aspects,notably better performance in remaining aspects,0.5538058280944824
translation,232,159,results,protoedit,has,prototype -krg,protoedit has prototype -krg,0.6338638663291931
translation,232,159,results,prototype -krg,has,comparable performance,prototype -krg has comparable performance,0.5979326367378235
translation,232,159,results,results,Compared to,protoedit,results Compared to protoedit,0.6921185255050659
translation,232,195,results,prototype -kr,retrieves,knowledge facts,prototype -kr retrieves knowledge facts,0.7396314144134521
translation,232,195,results,knowledge facts,from,human-written prototype dialogues,knowledge facts from human-written prototype dialogues,0.5791352391242981
translation,232,195,results,knowledge facts,is,"fast , accurate","knowledge facts is fast , accurate",0.5478706955909729
translation,232,195,results,results,has,prototype -kr,results has prototype -kr,0.5587952136993408
translation,233,14,model,automatically derived,from,annotated parallel monologue / dialogue corpus,automatically derived from annotated parallel monologue / dialogue corpus,0.5119917988777161
translation,234,159,ablation-analysis,budget allocation algorithm and active sampling strategy,helpful for improving,dialogue policy,budget allocation algorithm and active sampling strategy helpful for improving dialogue policy,0.7260998487472534
translation,234,159,ablation-analysis,dialogue policy,in,limited budget setting,dialogue policy in limited budget setting,0.5218084454536438
translation,234,159,ablation-analysis,ablation analysis,shows,budget allocation algorithm and active sampling strategy,ablation analysis shows budget allocation algorithm and active sampling strategy,0.6090015172958374
translation,234,160,ablation-analysis,active sampling strategy,is,more important,active sampling strategy is more important,0.5667021870613098
translation,234,160,ablation-analysis,active sampling strategy,without which,performance,active sampling strategy without which performance,0.6079537272453308
translation,234,160,ablation-analysis,more important,without which,performance,more important without which performance,0.5838311314582825
translation,234,160,ablation-analysis,performance,has,drops,performance has drops,0.5993483662605286
translation,234,160,ablation-analysis,drops,has,significantly,drops has significantly,0.6735619306564331
translation,234,160,ablation-analysis,ablation analysis,has,active sampling strategy,ablation analysis has active sampling strategy,0.526036262512207
translation,234,94,hyperparameters,mlp,to parameterize,function q ( ? ),mlp to parameterize function q ( ? ),0.7511532306671143
translation,234,94,hyperparameters,mlp,with,hidden layer size,mlp with hidden layer size,0.6338373422622681
translation,234,94,hyperparameters,function q ( ? ),in,"all the dialogue agents ( sl , dqn , ddq and bcs - ddq )","function q ( ? ) in all the dialogue agents ( sl , dqn , ddq and bcs - ddq )",0.5075816512107849
translation,234,94,hyperparameters,function q ( ? ),with,hidden layer size,function q ( ? ) with hidden layer size,0.6251295208930969
translation,234,94,hyperparameters,hidden layer size,set to,80,hidden layer size set to 80,0.7381187081336975
translation,234,94,hyperparameters,hyperparameters,use,mlp,hyperparameters use mlp,0.6667225956916809
translation,234,96,hyperparameters,hyperparameters,set,discount factor,hyperparameters set discount factor,0.5682007074356079
translation,234,98,hyperparameters,world model,contains,one shared hidden layer,world model contains one shared hidden layer,0.5719268321990967
translation,234,98,hyperparameters,world model,contains,three task -specific hidden layers,world model contains three task -specific hidden layers,0.5442073941230774
translation,234,98,hyperparameters,three task -specific hidden layers,size,80,three task -specific hidden layers size 80,0.6432204842567444
translation,234,98,hyperparameters,hyperparameters,has,world model,hyperparameters has world model,0.5400346517562866
translation,234,99,hyperparameters,number of planning steps,set to,5,number of planning steps set to 5,0.7370021343231201
translation,234,99,hyperparameters,5,for using,world model,5 for using world model,0.6547811031341553
translation,234,99,hyperparameters,world model,to improve,agent 's policy,world model to improve agent 's policy,0.6726557612419128
translation,234,99,hyperparameters,agent 's policy,in,ddq and bcs - ddq frameworks,agent 's policy in ddq and bcs - ddq frameworks,0.5478187203407288
translation,234,99,hyperparameters,hyperparameters,has,number of planning steps,hyperparameters has number of planning steps,0.5264536142349243
translation,234,101,hyperparameters,bcs - ddq,set as,"l = 128 , d = 10","bcs - ddq set as l = 128 , d = 10",0.6763350963592529
translation,234,103,hyperparameters,parameters,of,neural networks,parameters of neural networks,0.5617836713790894
translation,234,103,hyperparameters,parameters,initialized using,normal distribution,parameters initialized using normal distribution,0.6917715072631836
translation,234,103,hyperparameters,neural networks,initialized using,normal distribution,neural networks initialized using normal distribution,0.6830618977546692
translation,234,103,hyperparameters,normal distribution,with,mean of 0,normal distribution with mean of 0,0.6884322166442871
translation,234,103,hyperparameters,hyperparameters,has,parameters,hyperparameters has parameters,0.4783959984779358
translation,234,105,hyperparameters,batch size,set to,16,batch size set to 16,0.7520541548728943
translation,234,105,hyperparameters,mini-,has,batch size,mini- has batch size,0.6139686107635498
translation,234,105,hyperparameters,initial learn,has,"{ 50 , 100 , 150 , 200 , 250 , 300 }","initial learn has { 50 , 100 , 150 , 200 , 250 , 300 }",0.5645418763160706
translation,234,105,hyperparameters,hyperparameters,has,mini-,hyperparameters has mini-,0.5373920202255249
translation,234,108,hyperparameters,buffer sizes,of,b r and b s,buffer sizes of b r and b s,0.6469427347183228
translation,234,108,hyperparameters,b r and b s,set to,3000,b r and b s set to 3000,0.7698014974594116
translation,234,108,hyperparameters,hyperparameters,has,buffer sizes,hyperparameters has buffer sizes,0.5310488939285278
translation,234,109,hyperparameters,agents,utilized,variant of imitation learning,agents utilized variant of imitation learning,0.6238682866096497
translation,234,109,hyperparameters,variant of imitation learning,to pre-train,all agent variants,variant of imitation learning to pre-train all agent variants,0.6785109043121338
translation,234,109,hyperparameters,all agent variants,at,starting stage,all agent variants at starting stage,0.5292541980743408
translation,234,109,hyperparameters,agents,has,more efficiently,agents has more efficiently,0.5978712439537048
translation,234,109,hyperparameters,hyperparameters,to train,agents,hyperparameters to train agents,0.6875041723251343
translation,234,109,hyperparameters,hyperparameters,utilized,variant of imitation learning,hyperparameters utilized variant of imitation learning,0.602728545665741
translation,234,4,model,deep dyna - q ( ddq ),by incorporating,budget-conscious scheduling ( bcs ),deep dyna - q ( ddq ) by incorporating budget-conscious scheduling ( bcs ),0.6977113485336304
translation,234,4,model,budget-conscious scheduling ( bcs ),to best utilize,"fixed , small amount of user interactions ( budget )","budget-conscious scheduling ( bcs ) to best utilize fixed , small amount of user interactions ( budget )",0.6992873549461365
translation,234,4,model,"fixed , small amount of user interactions ( budget )",for learning,task - oriented dialogue agents,"fixed , small amount of user interactions ( budget ) for learning task - oriented dialogue agents",0.6937470436096191
translation,234,5,model,bcs,consists of,poissonbased global scheduler,bcs consists of poissonbased global scheduler,0.678604781627655
translation,234,5,model,bcs,consists of,controller,bcs consists of controller,0.6901187896728516
translation,234,5,model,bcs,consists of,user goal sampling module,bcs consists of user goal sampling module,0.6839961409568787
translation,234,5,model,poissonbased global scheduler,to allocate,budget,poissonbased global scheduler to allocate budget,0.7650849223136902
translation,234,5,model,budget,over,different stages of training,budget over different stages of training,0.6747801303863525
translation,234,5,model,controller,to decide,each training step,controller to decide each training step,0.7326754331588745
translation,234,5,model,each training step,agent is trained using,real or simulated experiences,each training step agent is trained using real or simulated experiences,0.7190182209014893
translation,234,5,model,user goal sampling module,to generate,experiences,user goal sampling module to generate experiences,0.7053403854370117
translation,234,5,model,experiences,most effective for,policy learning,experiences most effective for policy learning,0.6658969521522522
translation,234,5,model,model,has,bcs,model has bcs,0.6286662817001343
translation,234,25,model,framework,called,budget-conscious scheduling - based deep dyna - q ( bcs - ddq ),framework called budget-conscious scheduling - based deep dyna - q ( bcs - ddq ),0.6512137651443481
translation,234,25,model,framework,to best utilize,"fixed , small number of human interactions ( budget )","framework to best utilize fixed , small number of human interactions ( budget )",0.7122825980186462
translation,234,25,model,"fixed , small number of human interactions ( budget )",for,taskoriented dialogue policy learning,"fixed , small number of human interactions ( budget ) for taskoriented dialogue policy learning",0.5897247195243835
translation,234,25,model,model,propose,framework,model propose framework,0.666053295135498
translation,234,26,model,framework,extends,ddq,framework extends ddq,0.7844488024711609
translation,234,26,model,ddq,by incorporating,budget-conscious scheduling ( bcs ),ddq by incorporating budget-conscious scheduling ( bcs ),0.7146202325820923
translation,234,26,model,ddq,improve,ddq 's sample efficiency,ddq improve ddq 's sample efficiency,0.6851298213005066
translation,234,26,model,budget-conscious scheduling ( bcs ),to control,budget,budget-conscious scheduling ( bcs ) to control budget,0.7210692167282104
translation,234,26,model,ddq 's sample efficiency,leveraging,active learning,ddq 's sample efficiency leveraging active learning,0.729882001876831
translation,234,26,model,model,has,framework,model has framework,0.5441871285438538
translation,234,27,model,bcs module,consists of,poisson - based global scheduler,bcs module consists of poisson - based global scheduler,0.6876517534255981
translation,234,27,model,bcs module,consists of,user goal sampling module,bcs module consists of user goal sampling module,0.6728308796882629
translation,234,27,model,bcs module,consists of,controller,bcs module consists of controller,0.696629524230957
translation,234,27,model,poisson - based global scheduler,to allocate,budget,poisson - based global scheduler to allocate budget,0.7757236361503601
translation,234,27,model,budget,over,different stages of training,budget over different stages of training,0.6747801303863525
translation,234,27,model,user goal sampling module,to select,previously failed or unexplored user goals,user goal sampling module to select previously failed or unexplored user goals,0.671370804309845
translation,234,27,model,previously failed or unexplored user goals,to generate,experiences,previously failed or unexplored user goals to generate experiences,0.6738203763961792
translation,234,27,model,experiences,effective for,dialogue policy learning,experiences effective for dialogue policy learning,0.667837917804718
translation,234,27,model,controller,decides,humanagent interactions,controller decides humanagent interactions,0.684635579586029
translation,234,27,model,controller,decides,simulated experiences,controller decides simulated experiences,0.69247967004776
translation,234,27,model,controller,collect,human-human conversation,controller collect human-human conversation,0.6862069368362427
translation,234,27,model,controller,conduct,humanagent interactions,controller conduct humanagent interactions,0.6566631197929382
translation,234,27,model,simulated experiences,through interaction with,world model,simulated experiences through interaction with world model,0.7223069667816162
translation,234,27,model,model,has,bcs module,model has bcs module,0.6101080775260925
translation,234,95,model,- greedy policy,adopted for,exploration,- greedy policy adopted for exploration,0.7101200819015503
translation,234,95,model,model,has,- greedy policy,model has - greedy policy,0.6073862910270691
translation,234,97,model,model,has,target value function q ( ? ),model has target value function q ( ? ),0.5781378149986267
translation,234,121,results,consistently outperforms,by,statistically significant margin,consistently outperforms by statistically significant margin,0.6413445472717285
translation,234,121,results,other baseline agents,by,statistically significant margin,other baseline agents by statistically significant margin,0.5430047512054443
translation,234,121,results,bcs - ddq agent,has,consistently outperforms,bcs - ddq agent has consistently outperforms,0.6168578863143921
translation,234,121,results,consistently outperforms,has,other baseline agents,consistently outperforms has other baseline agents,0.5713440179824829
translation,234,126,results,our method,achieves,better performance,our method achieves better performance,0.6458921432495117
translation,234,126,results,better performance,than,ddq,better performance than ddq,0.586583137512207
translation,234,126,results,results,has,our method,results has our method,0.5589964985847473
translation,235,97,ablation-analysis,hmm connection,brings an,increase,hmm connection brings an increase,0.695353627204895
translation,235,97,ablation-analysis,increase,of,3.63 %,increase of 3.63 %,0.5899122357368469
translation,235,97,ablation-analysis,increase,of,2.58 %,increase of 2.58 %,0.5831485986709595
translation,235,97,ablation-analysis,3.63 %,with,gated bias hmm configuration,3.63 % with gated bias hmm configuration,0.6504451036453247
translation,235,97,ablation-analysis,2.58 %,with,fully gated hmm configuration,2.58 % with fully gated hmm configuration,0.6436453461647034
translation,235,97,ablation-analysis,attention mechanism,has,hmm connection,attention mechanism has hmm connection,0.550712525844574
translation,235,97,ablation-analysis,ablation analysis,Without,attention mechanism,ablation analysis Without attention mechanism,0.6923165917396545
translation,235,99,ablation-analysis,gated attention,in,place,gated attention in place,0.5752502679824829
translation,235,99,ablation-analysis,two hmm configurations,improve,accuracy,two hmm configurations improve accuracy,0.6430087685585022
translation,235,99,ablation-analysis,accuracy,by,3.73 %,accuracy by 3.73 %,0.5682898163795471
translation,235,99,ablation-analysis,gated attention,has,two hmm configurations,gated attention has two hmm configurations,0.578088641166687
translation,235,99,ablation-analysis,place,has,two hmm configurations,place has two hmm configurations,0.6013324856758118
translation,235,99,ablation-analysis,ablation analysis,with,gated attention,ablation analysis with gated attention,0.6451179385185242
translation,235,61,baselines,baselines,has,switchboard dialog act corpus,baselines has switchboard dialog act corpus,0.49482211470603943
translation,235,72,experimental-setup,model variants,implemented with,cnn package,model variants implemented with cnn package,0.7110546827316284
translation,235,72,experimental-setup,model variants,trained with,"adagrad ( duchi et al. , 2011 )","model variants trained with adagrad ( duchi et al. , 2011 )",0.6708066463470459
translation,235,72,experimental-setup,"adagrad ( duchi et al. , 2011 )",using,"dropout ( srivastava et al. , 2014 )","adagrad ( duchi et al. , 2011 ) using dropout ( srivastava et al. , 2014 )",0.5815497636795044
translation,235,72,experimental-setup,experimental setup,trained with,"adagrad ( duchi et al. , 2011 )","experimental setup trained with adagrad ( duchi et al. , 2011 )",0.6806050539016724
translation,235,72,experimental-setup,experimental setup,has,model variants,experimental setup has model variants,0.5390467047691345
translation,235,81,experimental-setup,experiments,executed on,intel xeon e5 - 2667 cpu,experiments executed on intel xeon e5 - 2667 cpu,0.6942527890205383
translation,235,81,experimental-setup,intel xeon e5 - 2667 cpu,with,16gb,intel xeon e5 - 2667 cpu with 16gb,0.5967223644256592
translation,235,81,experimental-setup,16gb,of,ram,16gb of ram,0.5300720930099487
translation,235,81,experimental-setup,experimental setup,executed on,intel xeon e5 - 2667 cpu,experimental setup executed on intel xeon e5 - 2667 cpu,0.7119176387786865
translation,235,81,experimental-setup,experimental setup,has,experiments,experimental setup has experiments,0.5502888560295105
translation,235,5,model,label - to - label connection,for,sequence learning,label - to - label connection for sequence learning,0.6054819226264954
translation,235,5,model,model,Building upon,recurrent neural network framework,model Building upon recurrent neural network framework,0.6403936743736267
translation,235,5,model,model,incorporates,new attentional technique,model incorporates new attentional technique,0.755214273929596
translation,235,11,model,generative model,of,utterances and dialogue acts,generative model of utterances and dialogue acts,0.5502182841300964
translation,235,11,model,model,present,generative model,model present generative model,0.6634361147880554
translation,235,12,model,model,use,attention mechanism,model use attention mechanism,0.6464574933052063
translation,235,15,model,gated attention mechanism,where,attention signal,gated attention mechanism where attention signal,0.5929518342018127
translation,235,15,model,attention signal,represented as,gate,attention signal represented as gate,0.6368407607078552
translation,235,15,model,gate,over,input vector,gate over input vector,0.6491534113883972
translation,235,15,model,model,propose,gated attention mechanism,model propose gated attention mechanism,0.65959233045578
translation,235,102,model,novel hmm - like connection,in,generative model,novel hmm - like connection in generative model,0.5195486545562744
translation,235,102,model,generative model,of,utterances and dialogue acts,generative model of utterances and dialogue acts,0.5502182841300964
translation,235,102,model,model,proposed,new gated attention mechanism,model proposed new gated attention mechanism,0.7225209474563599
translation,235,102,model,model,proposed,novel hmm - like connection,model proposed novel hmm - like connection,0.7031716704368591
translation,235,89,results,strong baselines,for,both corpora,strong baselines for both corpora,0.5378097891807556
translation,235,89,results,best models,has,outperform,best models has outperform,0.6202653646469116
translation,235,89,results,outperform,has,strong baselines,outperform has strong baselines,0.5938366651535034
translation,235,91,results,attention mechanism,is,beneficial,attention mechanism is beneficial,0.5525509715080261
translation,235,91,results,results,adding,attention mechanism,results adding attention mechanism,0.6606267094612122
translation,235,92,results,outperform,with,traditional attention mechanism,outperform with traditional attention mechanism,0.6368556618690491
translation,235,92,results,traditional attention mechanism,by,0.49 % - 1.21 %,traditional attention mechanism by 0.49 % - 1.21 %,0.5620348453521729
translation,235,92,results,results,has,gated attention configurations,results has gated attention configurations,0.5361750721931458
translation,235,93,results,accuracy,of,shen and lee's ( 2016 ) classifier,accuracy of shen and lee's ( 2016 ) classifier,0.5978496670722961
translation,235,93,results,shen and lee's ( 2016 ) classifier,employs,attention mechanism,shen and lee's ( 2016 ) classifier employs attention mechanism,0.5883356332778931
translation,235,93,results,attention mechanism,lower than,blunsom ( 2013 ),attention mechanism lower than blunsom ( 2013 ),0.6970508694648743
translation,235,93,results,results,has,accuracy,results has accuracy,0.5888755321502686
translation,235,96,results,performance gain,from,hmm connection,performance gain from hmm connection,0.6041697859764099
translation,235,96,results,hmm connection,larger than,gain,hmm connection larger than gain,0.7049005627632141
translation,235,96,results,gain,from,attention mechanism,gain from attention mechanism,0.5777989625930786
translation,235,98,results,improvement,is,3.01 %,improvement is 3.01 %,0.578338086605072
translation,235,98,results,improvement,is,3.47 %,improvement is 3.47 %,0.574000358581543
translation,235,98,results,3.01 %,for,bias hmm configuration,3.01 % for bias hmm configuration,0.6369878053665161
translation,235,98,results,3.47 %,for,gated hmm configuration,3.47 % for gated hmm configuration,0.6324372291564941
translation,235,98,results,traditional attention,has,improvement,traditional attention has improvement,0.5621243119239807
translation,235,98,results,results,With the use of,traditional attention,results With the use of traditional attention,0.6312010288238525
translation,236,172,ablation-analysis,increase,from,0.43 to 0.82,increase from 0.43 to 0.82,0.5635528564453125
translation,236,172,ablation-analysis,0.43 to 0.82,in,pearson correlation,0.43 to 0.82 in pearson correlation,0.4624570310115814
translation,236,172,ablation-analysis,enhancement,from,0.32 to 0.81,enhancement from 0.32 to 0.81,0.53610759973526
translation,236,172,ablation-analysis,0.32 to 0.81,in,spearman correlation,0.32 to 0.81 in spearman correlation,0.4737551808357239
translation,236,172,ablation-analysis,ablation analysis,observe,increase,ablation analysis observe increase,0.6632914543151855
translation,236,172,ablation-analysis,ablation analysis,observe,enhancement,ablation analysis observe enhancement,0.6268028616905212
translation,236,194,ablation-analysis,response fluency,correlates with,context coherence,response fluency correlates with context coherence,0.6675686836242676
translation,236,194,ablation-analysis,ablation analysis,has,response fluency,ablation analysis has response fluency,0.5346816778182983
translation,236,39,baselines,two effective approaches,to generate,augmented utterances,two effective approaches to generate augmented utterances,0.6949354410171509
translation,236,39,baselines,text generator,with,k-best decoder,text generator with k-best decoder,0.6240913271903992
translation,236,113,baselines,conditional text generator ( ctg ),to augment,queries,conditional text generator ( ctg ) to augment queries,0.7081897258758545
translation,236,113,baselines,queries,in,multi-turn dialogue,queries in multi-turn dialogue,0.5549371242523193
translation,236,133,baselines,seq2seq,consists of,2 - layer lstm,seq2seq consists of 2 - layer lstm,0.6199893951416016
translation,236,133,baselines,2 - layer lstm,with,500 hidden units,2 - layer lstm with 500 hidden units,0.5855761766433716
translation,236,133,baselines,baselines,has,seq2seq,baselines has seq2seq,0.5571820139884949
translation,236,35,experiments,context coherence,of,dialogue,context coherence of dialogue,0.5491611361503601
translation,236,35,experiments,language fluency,of,generated responses,language fluency of generated responses,0.5517026782035828
translation,236,134,hyperparameters,learning rate,of,1,learning rate of 1,0.6358351111412048
translation,236,134,hyperparameters,hyperparameters,trained with,sgd,hyperparameters trained with sgd,0.7188319563865662
translation,236,134,hyperparameters,hyperparameters,trained with,learning rate,hyperparameters trained with learning rate,0.6679986715316772
translation,236,8,model,holistic evaluation metrics,capture,different aspects,holistic evaluation metrics capture different aspects,0.7013993263244629
translation,236,8,model,different aspects,of,open-domain dialogues,different aspects of open-domain dialogues,0.5640358924865723
translation,236,8,model,model,propose,holistic evaluation metrics,model propose holistic evaluation metrics,0.6485078930854797
translation,236,34,model,model,propose,holistic metrics,model propose holistic metrics,0.688827395439148
translation,236,40,model,n-gram based entropy,to capture,response diversity,n-gram based entropy to capture response diversity,0.6915835738182068
translation,236,40,model,n-gram based entropy,to capture,entailment based approach,n-gram based entropy to capture entailment based approach,0.6628924608230591
translation,236,40,model,n-gram based entropy,to capture,logical self -consistency,n-gram based entropy to capture logical self -consistency,0.6843900084495544
translation,236,40,model,entailment based approach,to capture,logical self -consistency,entailment based approach to capture logical self -consistency,0.6733807325363159
translation,236,40,model,model,utilize,n-gram based entropy,model utilize n-gram based entropy,0.6291632056236267
translation,236,156,results,our language model based coherence metric,shows,higher correlation,our language model based coherence metric shows higher correlation,0.6440777778625488
translation,236,156,results,higher correlation,with,human judgments,higher correlation with human judgments,0.6432051062583923
translation,236,156,results,human judgments,than,classifier - based metric,human judgments than classifier - based metric,0.5524116158485413
translation,236,156,results,results,has,our language model based coherence metric,results has our language model based coherence metric,0.5589628219604492
translation,236,158,results,fine-tuned version,improved,results,fine-tuned version improved results,0.7178968787193298
translation,236,158,results,results,has,fine-tuned version,results has fine-tuned version,0.5802456736564636
translation,236,159,results,metric,based on,language model,metric based on language model,0.6211287379264832
translation,236,159,results,metric,correlated with,human ratings,metric correlated with human ratings,0.678586483001709
translation,236,159,results,language model,correlated with,human ratings,language model correlated with human ratings,0.6507936716079712
translation,236,159,results,human ratings,stronger than,ruber,human ratings stronger than ruber,0.755988359451294
translation,236,159,results,language model,has,without fine-tuning,language model has without fine-tuning,0.5282754302024841
translation,236,178,results,human ratings,based on,paraphrasing augmented datasets,human ratings based on paraphrasing augmented datasets,0.5862734913825989
translation,236,178,results,paraphrasing augmented datasets,show,high inter-rater correlations,paraphrasing augmented datasets show high inter-rater correlations,0.5758745670318604
translation,236,178,results,paraphrasing augmented datasets,show,lower variance,paraphrasing augmented datasets show lower variance,0.636613667011261
translation,236,178,results,results,has,human ratings,results has human ratings,0.5122916102409363
translation,236,184,results,metric,based on,ctg augmentation,metric based on ctg augmentation,0.6932031512260437
translation,236,184,results,metric,aligns with,human judgments,metric aligns with human judgments,0.7397254109382629
translation,236,184,results,ctg augmentation,aligns with,human judgments,ctg augmentation aligns with human judgments,0.7105422019958496
translation,236,184,results,human judgments,has,closet,human judgments has closet,0.5848217010498047
translation,236,184,results,results,has,metric,results has metric,0.5532228350639343
translation,237,232,ablation-analysis,uncorrelated part,there is,obvious decrease,uncorrelated part there is obvious decrease,0.6406799554824829
translation,237,232,ablation-analysis,obvious decrease,in,relevance,obvious decrease in relevance,0.5149281620979309
translation,237,232,ablation-analysis,ablation analysis,without,uncorrelated part,ablation analysis without uncorrelated part,0.7452102303504944
translation,237,140,baselines,"vanilla seq2seq model with attention ( luong et al. , 2015 )",trained using,cross entropy loss,"vanilla seq2seq model with attention ( luong et al. , 2015 ) trained using cross entropy loss",0.708882212638855
translation,237,140,baselines,"vanilla seq2seq model with attention ( luong et al. , 2015 )",trained using,"nucleus sampling ( holtzman et al. , 2020 )","vanilla seq2seq model with attention ( luong et al. , 2015 ) trained using nucleus sampling ( holtzman et al. , 2020 )",0.7096650004386902
translation,237,140,baselines,"vanilla seq2seq model with attention ( luong et al. , 2015 )",decoded using,beam search,"vanilla seq2seq model with attention ( luong et al. , 2015 ) decoded using beam search",0.7456222176551819
translation,237,140,baselines,"vanilla seq2seq model with attention ( luong et al. , 2015 )",decoded using,"nucleus sampling ( holtzman et al. , 2020 )","vanilla seq2seq model with attention ( luong et al. , 2015 ) decoded using nucleus sampling ( holtzman et al. , 2020 )",0.727176308631897
translation,237,141,baselines,mmi - anti ( li an approximation,use,10 dimensions,mmi - anti ( li an approximation use 10 dimensions,0.6618803143501282
translation,237,141,baselines,10 dimensions,for,uncorrelated part,10 dimensions for uncorrelated part,0.6147291660308838
translation,237,149,experimental-setup,1 layer gru,for,all encoders and decoders,1 layer gru for all encoders and decoders,0.567715048789978
translation,237,149,experimental-setup,experimental setup,use,1 layer gru,experimental setup use 1 layer gru,0.6172777414321899
translation,237,150,experimental-setup,correlated representation size,is,512,correlated representation size is 512,0.5945888757705688
translation,237,150,experimental-setup,uncorrelated representation size,is,10,uncorrelated representation size is 10,0.6348156332969666
translation,237,150,experimental-setup,experimental setup,has,correlated representation size,experimental setup has correlated representation size,0.5363960862159729
translation,237,150,experimental-setup,experimental setup,has,uncorrelated representation size,experimental setup has uncorrelated representation size,0.5286475419998169
translation,237,152,experimental-setup,hidden layer size,has,522,hidden layer size has 522,0.602931022644043
translation,237,153,experimental-setup,word embedding dimension,is,128,word embedding dimension is 128,0.5770930647850037
translation,237,153,experimental-setup,experimental setup,has,word embedding dimension,experimental setup has word embedding dimension,0.49582281708717346
translation,237,155,experimental-setup,batch size,is,64,batch size is 64,0.6388692259788513
translation,237,155,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,237,156,experimental-setup,"{? 1 , ? 2 , ? 3 , ? 4 , ? 5 , ? 6 }",set to,"{ 3.9,6.25,0.05,2,2,0.1 }","{? 1 , ? 2 , ? 3 , ? 4 , ? 5 , ? 6 } set to { 3.9,6.25,0.05,2,2,0.1 }",0.6643739938735962
translation,237,156,experimental-setup,experimental setup,has,"{? 1 , ? 2 , ? 3 , ? 4 , ? 5 , ? 6 }","experimental setup has {? 1 , ? 2 , ? 3 , ? 4 , ? 5 , ? 6 }",0.5430784225463867
translation,237,157,experimental-setup,our model,with,attention,our model with attention,0.6930736303329468
translation,237,157,experimental-setup,attention bottleneck,dimension,10,attention bottleneck dimension 10,0.7162933349609375
translation,237,157,experimental-setup,our model,has,attention bottleneck,our model has attention bottleneck,0.5438241958618164
translation,237,157,experimental-setup,attention,has,attention bottleneck,attention has attention bottleneck,0.5620117783546448
translation,237,157,experimental-setup,experimental setup,For,our model,experimental setup For our model,0.5664494037628174
translation,237,158,experimental-setup,experimental setup,trained on,one tesla m40 gpu,experimental setup trained on one tesla m40 gpu,0.6867728233337402
translation,237,154,experiments,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,237,154,experiments,learning rate,has,"0.001 , ? 1 = 0.9 , ? 2 = 0.999","learning rate has 0.001 , ? 1 = 0.9 , ? 2 = 0.999",0.5581499338150024
translation,237,8,model,pair relationship,between,prompts and responses,pair relationship between prompts and responses,0.6455861330032349
translation,237,8,model,pair relationship,as,regression task,pair relationship as regression task,0.5320767164230347
translation,237,8,model,regression task,on,latent space,regression task on latent space,0.5564503073692322
translation,237,8,model,model,learn,pair relationship,model learn pair relationship,0.6729320287704468
translation,237,36,model,beam search,for,most probable sequence,beam search for most probable sequence,0.5989643335342407
translation,237,36,model,most probable sequence,given,semantic vector,most probable sequence given semantic vector,0.6529489755630493
translation,237,36,model,semantic vector,during,inference,semantic vector during inference,0.648586094379425
translation,237,36,model,inference,without preferring,generic responses,inference without preferring generic responses,0.7757222652435303
translation,237,36,model,model,perform,beam search,model perform beam search,0.6429034471511841
translation,237,131,model,bottleneck,is,fully connected layer,bottleneck is fully connected layer,0.5454551577568054
translation,237,131,model,fully connected layer,reduces,attention output vector,fully connected layer reduces attention output vector,0.625585675239563
translation,237,131,model,attention output vector,into,low dimension,attention output vector into low dimension,0.5434466004371643
translation,237,131,model,model,has,bottleneck,model has bottleneck,0.568199872970581
translation,237,39,results,crowdworkers,showed,latent space method,crowdworkers showed latent space method,0.6640123128890991
translation,237,39,results,significantly outperforms,using,end-to - end cross entropy classification,significantly outperforms using end-to - end cross entropy classification,0.6900616884231567
translation,237,39,results,baselines,using,end-to - end cross entropy classification,baselines using end-to - end cross entropy classification,0.6474986672401428
translation,237,39,results,responses,that are both,relevant and informative,responses that are both relevant and informative,0.5886236429214478
translation,237,39,results,latent space method,has,significantly outperforms,latent space method has significantly outperforms,0.6131057143211365
translation,237,39,results,significantly outperforms,has,baselines,significantly outperforms has baselines,0.6014122366905212
translation,237,122,results,uncorrelated representation,allows,cca loss,uncorrelated representation allows cca loss,0.6359042525291443
translation,237,122,results,uncorrelated representation,allows,the autoencoder reconstruction loss,uncorrelated representation allows the autoencoder reconstruction loss,0.6094865798950195
translation,237,122,results,uncorrelated representation,both,cca loss,uncorrelated representation both cca loss,0.6114540100097656
translation,237,122,results,uncorrelated representation,both,the autoencoder reconstruction loss,uncorrelated representation both the autoencoder reconstruction loss,0.6003442406654358
translation,237,122,results,uncorrelated representation,to converge to,significantly lower value,uncorrelated representation to converge to significantly lower value,0.6935278177261353
translation,237,122,results,results,adding,uncorrelated representation,results adding uncorrelated representation,0.6682645082473755
translation,237,199,results,spacefusion and mle + sampling,could generate,very informative responses,spacefusion and mle + sampling could generate very informative responses,0.6805054545402527
translation,237,199,results,relevance score,is,low,relevance score is low,0.5201359987258911
translation,237,199,results,personachat dataset,has,spacefusion and mle + sampling,personachat dataset has spacefusion and mle + sampling,0.5685925483703613
translation,237,199,results,results,On,personachat dataset,results On personachat dataset,0.5518697500228882
translation,237,200,results,mmi,on,relevance and informativeness,mmi on relevance and informativeness,0.5501856207847595
translation,237,200,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,237,200,results,outperforms,has,mle + beam search,outperforms has mle + beam search,0.6035937666893005
translation,237,200,results,results,has,our model,results has our model,0.5871725678443909
translation,237,201,results,mle + sampling,scores,highest,mle + sampling scores highest,0.7422738671302795
translation,237,201,results,highest,on,informativeness,highest on informativeness,0.4951944351196289
translation,237,201,results,dai-lydailog dataset,has,mle + sampling,dai-lydailog dataset has mle + sampling,0.5706012845039368
translation,237,201,results,results,On,dai-lydailog dataset,results On dai-lydailog dataset,0.5306484699249268
translation,237,202,results,mmi and mle + beam search,are,relevant,mmi and mle + beam search are relevant,0.6272363066673279
translation,237,202,results,mmi and mle + beam search,prone to,generic responses,mmi and mle + beam search prone to generic responses,0.7491108775138855
translation,237,202,results,results,has,mmi and mle + beam search,results has mmi and mle + beam search,0.5017011165618896
translation,237,204,results,attention,to,our model,attention to our model,0.5795972347259521
translation,237,204,results,attention,im-proves,both relevance and informativeness,attention im-proves both relevance and informativeness,0.7441710233688354
translation,237,204,results,our model,im-proves,both relevance and informativeness,our model im-proves both relevance and informativeness,0.7248833775520325
translation,237,204,results,our model,im-proves,harms,our model im-proves harms,0.810741126537323
translation,237,204,results,both relevance and informativeness,on,dailydialog dataset,both relevance and informativeness on dailydialog dataset,0.5673924684524536
translation,237,204,results,informativeness,on,persona chat dataset,informativeness on persona chat dataset,0.4851354956626892
translation,237,204,results,harms,has,informativeness,harms has informativeness,0.5793449878692627
translation,237,204,results,results,Adding,attention,results Adding attention,0.5996842980384827
translation,237,205,results,our models,performs,best,our models performs best,0.6299703121185303
translation,237,205,results,best,on,ui score,best on ui score,0.5232958793640137
translation,237,205,results,ui score,for,both datasets,ui score for both datasets,0.5565033555030823
translation,237,205,results,results,has,our models,results has our models,0.5733726620674133
translation,237,206,results,bootstrapping significance test,found,our improvements,bootstrapping significance test found our improvements,0.6607809662818909
translation,237,206,results,our improvements,are,statistically significant,our improvements are statistically significant,0.58494633436203
translation,237,206,results,results,performed,bootstrapping significance test,results performed bootstrapping significance test,0.3019231855869293
translation,237,222,results,model,gets,higher bleu,model gets higher bleu,0.5975553393363953
translation,237,222,results,higher bleu,than,most of the baselines,higher bleu than most of the baselines,0.5739926695823669
translation,237,222,results,higher bleu,showing,effectiveness,higher bleu showing effectiveness,0.737118661403656
translation,237,222,results,bleu scores,has,model,bleu scores has model,0.5437372922897339
translation,237,223,results,embedding similarity score,has,our model,embedding similarity score has our model,0.5493597388267517
translation,237,223,results,our model,has,consistently outperform,our model has consistently outperform,0.5980361700057983
translation,237,223,results,consistently outperform,has,other compared methods,consistently outperform has other compared methods,0.535953164100647
translation,237,223,results,results,For,embedding similarity score,results For embedding similarity score,0.5939347147941589
translation,237,235,results,decreased,except,dist,decreased except dist,0.699521541595459
translation,237,235,results,automatic metrics,has,decreased,automatic metrics has decreased,0.6311611533164978
translation,237,235,results,dist,has,2,dist has 2,0.7231589555740356
translation,237,235,results,results,has,automatic metrics,results has automatic metrics,0.5785090327262878
translation,238,87,experiments,"adam optimizer ( kingma and ba , 2014 )",with,learning rate,"adam optimizer ( kingma and ba , 2014 ) with learning rate",0.5956289768218994
translation,238,87,experiments,learning rate,of,0.0001,learning rate of 0.0001,0.5900294780731201
translation,238,86,hyperparameters,dimensions,of,lstm hidden units,dimensions of lstm hidden units,0.532248854637146
translation,238,86,hyperparameters,lstm hidden units,set to,600,lstm hidden units set to 600,0.6752987504005432
translation,238,86,hyperparameters,hyperparameters,has,dimensions,hyperparameters has dimensions,0.5247315764427185
translation,238,88,hyperparameters,size,of,vocabulary space,size of vocabulary space,0.5758693218231201
translation,238,88,hyperparameters,vocabulary space,set to,"25,000","vocabulary space set to 25,000",0.7010477185249329
translation,238,88,hyperparameters,hyperparameters,has,size,hyperparameters has size,0.5313372611999512
translation,238,89,hyperparameters,"fasttext ( joulin et al. , 2016 ) pre-trained word embedding",shared by,lstms,"fasttext ( joulin et al. , 2016 ) pre-trained word embedding shared by lstms",0.6114006638526917
translation,238,89,hyperparameters,"fasttext ( joulin et al. , 2016 ) pre-trained word embedding",set to,trainable,"fasttext ( joulin et al. , 2016 ) pre-trained word embedding set to trainable",0.5943873524665833
translation,238,7,model,three models,concatenate,desired emotion,three models concatenate desired emotion,0.6946653127670288
translation,238,7,model,three models,push,emotion,three models push emotion,0.7199795246124268
translation,238,7,model,desired emotion,with,source input,desired emotion with source input,0.6371196508407593
translation,238,7,model,source input,during,learning,source input during learning,0.7141125202178955
translation,238,7,model,emotion,in,decoder,emotion in decoder,0.5379112362861633
translation,238,7,model,model,present,three models,model present three models,0.6858277320861816
translation,238,7,model,model,concatenate,desired emotion,model concatenate desired emotion,0.6939727663993835
translation,238,30,model,seq2seq,learns to generate,sequence of words,seq2seq learns to generate sequence of words,0.7456696033477783
translation,238,30,model,sequence of words,from,another sequence of words,sequence of words from another sequence of words,0.5821591019630432
translation,238,30,model,model,has,seq2seq,model has seq2seq,0.5933537483215332
translation,238,99,results,better overall average accuracies,than,enc-bef,better overall average accuracies than enc-bef,0.5906075239181519
translation,238,99,results,better overall average accuracies,than,encaft,better overall average accuracies than encaft,0.6226198673248291
translation,238,99,results,dec,has,better overall average accuracies,dec has better overall average accuracies,0.571434736251831
translation,238,99,results,results,observe that,dec,results observe that dec,0.6087027788162231
translation,240,134,ablation-analysis,doc2vec,used,"doc2vec ( le and mikolov , 2014 )","doc2vec used doc2vec ( le and mikolov , 2014 )",0.5598265528678894
translation,240,134,ablation-analysis,doc2vec,to generate,sentence vectors,doc2vec to generate sentence vectors,0.6671525239944458
translation,240,134,ablation-analysis,"doc2vec ( le and mikolov , 2014 )",to generate,sentence vectors,"doc2vec ( le and mikolov , 2014 ) to generate sentence vectors",0.6261493563652039
translation,240,134,ablation-analysis,ablation analysis,has,doc2vec,ablation analysis has doc2vec,0.556117594242096
translation,240,76,experiments,best performance,with,20 topics,best performance with 20 topics,0.6481408476829529
translation,240,135,experiments,primary submission,for,subtasks a and b,primary submission for subtasks a and b,0.5734067559242249
translation,240,135,experiments,primary submission,uses,svm,primary submission uses svm,0.6532256007194519
translation,240,135,experiments,subtasks a and b,uses,svm,subtasks a and b uses svm,0.6576941609382629
translation,240,135,experiments,svm,with,rbf kernel,svm with rbf kernel,0.647553026676178
translation,240,135,experiments,rbf kernel,for,classification,rbf kernel for classification,0.6425283551216125
translation,240,5,model,support vector machine ( svm ) based system,makes use of,"textual , domain-specific , wordembedding and topic-modeling features","support vector machine ( svm ) based system makes use of textual , domain-specific , wordembedding and topic-modeling features",0.6631850004196167
translation,240,5,model,model,develop,support vector machine ( svm ) based system,model develop support vector machine ( svm ) based system,0.6077802181243896
translation,240,6,model,novel method,for,dialogue chain identification,novel method for dialogue chain identification,0.6249048113822937
translation,240,6,model,dialogue chain identification,in,comment threads,dialogue chain identification in comment threads,0.5203891396522522
translation,240,6,model,model,propose,novel method,model propose novel method,0.7230806350708008
translation,240,13,model,model,propose,rich feature - based system,model propose rich feature - based system,0.6640385985374451
translation,240,14,model,architecture,integrates,"textual , semantic and domain-specific features","architecture integrates textual , semantic and domain-specific features",0.6319026947021484
translation,240,14,model,"textual , semantic and domain-specific features",to achieve,good results,"textual , semantic and domain-specific features to achieve good results",0.6123014688491821
translation,240,14,model,model,create,architecture,model create architecture,0.6587472558021545
translation,240,15,model,customized preprocessing pipeline,rather than using,standard tools,customized preprocessing pipeline rather than using standard tools,0.634846568107605
translation,240,15,model,model,develop,customized preprocessing pipeline,model develop customized preprocessing pipeline,0.6770055294036865
translation,240,19,model,novel method,identification of,dialogue groups,novel method identification of dialogue groups,0.6714997291564941
translation,240,19,model,dialogue groups,in,comment thread,dialogue groups in comment thread,0.5273541212081909
translation,240,19,model,dialogue groups,by constructing,user interaction graph,dialogue groups by constructing user interaction graph,0.7319076061248779
translation,240,19,model,model,propose,novel method,model propose novel method,0.7230806350708008
translation,240,118,results,comparable re-sults,with,linear kernel,comparable re-sults with linear kernel,0.6240846514701843
translation,240,118,results,comparable re-sults,with,l2 - regularized logistic regression,comparable re-sults with l2 - regularized logistic regression,0.630090057849884
translation,240,118,results,results,achieve,comparable re-sults,results achieve comparable re-sults,0.5776261687278748
translation,241,7,model,model,propose,new annotation scheme,model propose new annotation scheme,0.6858028173446655
translation,241,27,model,model,propose,new annotation scheme,model propose new annotation scheme,0.6858028173446655
translation,242,16,results,troublesome definitions,of,rules and templates,troublesome definitions of rules and templates,0.5626767873764038
translation,242,16,results,troublesome definitions,reduced,significantly,troublesome definitions reduced significantly,0.7603477239608765
translation,242,16,results,rules and templates,for,sds prototyping,rules and templates for sds prototyping,0.6508362293243408
translation,242,16,results,significantly,compared with,ordinary rdbbased approach,significantly compared with ordinary rdbbased approach,0.6848665475845337
translation,242,16,results,csrs and the extended rails framework,has,troublesome definitions,csrs and the extended rails framework has troublesome definitions,0.5743346810340881
translation,242,16,results,results,using,csrs and the extended rails framework,results using csrs and the extended rails framework,0.6399262547492981
translation,243,188,ablation-analysis,vhred and nexus - f,not,help much,vhred and nexus - f not help much,0.724598228931427
translation,243,188,ablation-analysis,degrade,has,performance,degrade has performance,0.5986349582672119
translation,243,188,ablation-analysis,ablation analysis,has,vhred and nexus - f,ablation analysis has vhred and nexus - f,0.54302978515625
translation,243,208,ablation-analysis,nexus -h,contributes,nexus - f,nexus -h contributes nexus - f,0.7143347859382629
translation,243,208,ablation-analysis,nexus -h,more to,coher-ence,nexus -h more to coher-ence,0.6884249448776245
translation,243,208,ablation-analysis,nexus -h,more to,diversity,nexus -h more to diversity,0.6688341498374939
translation,243,208,ablation-analysis,nexus - f,more to,diversity,nexus - f more to diversity,0.6584596037864685
translation,243,208,ablation-analysis,ablation analysis,has,nexus -h,ablation analysis has nexus -h,0.5647467374801636
translation,243,138,baselines,seq2seq,has,seq2seq model,seq2seq has seq2seq model,0.5813721418380737
translation,243,138,baselines,baselines,has,seq2seq,baselines has seq2seq,0.5571820139884949
translation,243,140,baselines,mmi,implemented,bidirectional - mmi decoder,mmi implemented bidirectional - mmi decoder,0.6719150543212891
translation,243,140,baselines,bidirectional - mmi decoder,showed,better performance,bidirectional - mmi decoder showed better performance,0.6894532442092896
translation,243,140,baselines,better performance,over,anti-lm model,better performance over anti-lm model,0.6924096941947937
translation,243,140,baselines,baselines,has,mmi,baselines has mmi,0.5730272531509399
translation,243,143,baselines,model vhred,is essentially,conditional variational autoencoder,model vhred is essentially conditional variational autoencoder,0.6369043588638306
translation,243,143,baselines,conditional variational autoencoder,with,hierarchical encoders,conditional variational autoencoder with hierarchical encoders,0.5863546133041382
translation,243,143,baselines,baselines,has,model vhred,baselines has model vhred,0.6061884164810181
translation,243,148,baselines,nexus-h,maximizing,mutual information,nexus-h maximizing mutual information,0.7908106446266174
translation,243,148,baselines,mutual information,with,history ( ? 2 = 0 ),mutual information with history ( ? 2 = 0 ),0.660435676574707
translation,243,148,baselines,nexus-h,has,nexus network,nexus-h has nexus network,0.6304215788841248
translation,243,148,baselines,baselines,has,nexus-h,baselines has nexus-h,0.6065970659255981
translation,243,149,baselines,nexus -f,maximizing,mutual information,nexus -f maximizing mutual information,0.8013928532600403
translation,243,149,baselines,nexus network,maximizing,mutual information,nexus network maximizing mutual information,0.7843959331512451
translation,243,149,baselines,mutual information,with,future ( ? 1 = 0 ),mutual information with future ( ? 1 = 0 ),0.6368519067764282
translation,243,149,baselines,nexus -f,has,nexus network,nexus -f has nexus network,0.6312812566757202
translation,243,149,baselines,baselines,has,nexus -f,baselines has nexus -f,0.5987221002578735
translation,243,127,experimental-setup,vocabulary dictionary,based on,"most frequent 20,000 words","vocabulary dictionary based on most frequent 20,000 words",0.6177501082420349
translation,243,127,experimental-setup,vocabulary dictionary,map,other words,vocabulary dictionary map other words,0.6694828271865845
translation,243,127,experimental-setup,"most frequent 20,000 words",for,both corpus,"most frequent 20,000 words for both corpus",0.5918955206871033
translation,243,127,experimental-setup,other words,to,unk token,other words to unk token,0.5831418037414551
translation,243,127,experimental-setup,experimental setup,build,vocabulary dictionary,experimental setup build vocabulary dictionary,0.6828868389129639
translation,243,128,experimental-setup,dimensionality,of,code space c,dimensionality of code space c,0.5897374749183655
translation,243,128,experimental-setup,code space c,is,100,code space c is 100,0.6226221323013306
translation,243,128,experimental-setup,experimental setup,has,dimensionality,experimental setup has dimensionality,0.4790812134742737
translation,243,129,experimental-setup,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,243,129,experimental-setup,learning rate,of,0.0002,learning rate of 0.0002,0.5917128324508667
translation,243,129,experimental-setup,0.001,for,dailydialog,0.001 for dailydialog,0.5950209498405457
translation,243,129,experimental-setup,0.0002,for,twitter corpus,0.0002 for twitter corpus,0.573637068271637
translation,243,130,experimental-setup,batch size,fixed to,128,batch size fixed to 128,0.7752795219421387
translation,243,130,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,243,131,experimental-setup,word vector dimension,is,300,word vector dimension is 300,0.5992984771728516
translation,243,131,experimental-setup,word vector dimension,initialized with,"public word2vec ( mikolov et al. , 2013 ) embeddings","word vector dimension initialized with public word2vec ( mikolov et al. , 2013 ) embeddings",0.702578067779541
translation,243,131,experimental-setup,"public word2vec ( mikolov et al. , 2013 ) embeddings",trained on,google news corpus,"public word2vec ( mikolov et al. , 2013 ) embeddings trained on google news corpus",0.6787811517715454
translation,243,131,experimental-setup,experimental setup,has,word vector dimension,experimental setup has word vector dimension,0.5111497640609741
translation,243,132,experimental-setup,probability estimators,for,gaussian distributions,probability estimators for gaussian distributions,0.5328013300895691
translation,243,132,experimental-setup,probability estimators,implemented as,3 - layer perceptrons,probability estimators implemented as 3 - layer perceptrons,0.5806199312210083
translation,243,132,experimental-setup,3 - layer perceptrons,with,hyperbolic tangent activation function,3 - layer perceptrons with hyperbolic tangent activation function,0.5729311108589172
translation,243,132,experimental-setup,experimental setup,has,probability estimators,experimental setup has probability estimators,0.5278184413909912
translation,243,135,experimental-setup,models,with,open-sourced python library pytorch,models with open-sourced python library pytorch,0.6372950673103333
translation,243,135,experimental-setup,models,optimized using,adam optimizer,models optimized using adam optimizer,0.6584382057189941
translation,243,135,experimental-setup,experimental setup,implemented,models,experimental setup implemented models,0.6744210720062256
translation,243,135,experimental-setup,experimental setup,optimized using,adam optimizer,experimental setup optimized using adam optimizer,0.7224153280258179
translation,243,139,experimental-setup,"beam search ( graves , 2012 )",with,beam size,"beam search ( graves , 2012 ) with beam size",0.59697425365448
translation,243,139,experimental-setup,beam size,set to,5,beam size set to 5,0.7865644693374634
translation,243,139,experimental-setup,5,when,testing,5 when testing,0.6098998785018921
translation,243,139,experimental-setup,experimental setup,try,greedy decoding,experimental setup try greedy decoding,0.6519377827644348
translation,243,139,experimental-setup,experimental setup,try,"beam search ( graves , 2012 )","experimental setup try beam search ( graves , 2012 )",0.6151100397109985
translation,243,141,experimental-setup,hyperparameter,set to,0.5,hyperparameter set to 0.5,0.6915715336799622
translation,243,141,experimental-setup,experimental setup,has,hyperparameter,experimental setup has hyperparameter,0.4927394390106201
translation,243,144,experimental-setup,posterior collapsing problem,apply,kl - annealing trick,posterior collapsing problem apply kl - annealing trick,0.5952609777450562
translation,243,144,experimental-setup,posterior collapsing problem,apply,early stop,posterior collapsing problem apply early stop,0.6316289305686951
translation,243,144,experimental-setup,early stop,with,step,early stop with step,0.6677518486976624
translation,243,144,experimental-setup,step,set as,"12,000","step set as 12,000",0.6892502307891846
translation,243,144,experimental-setup,step,set as,"75,000","step set as 75,000",0.681790828704834
translation,243,144,experimental-setup,"12,000",for,dailydialog,"12,000 for dailydialog",0.6586140990257263
translation,243,144,experimental-setup,"75,000",for,twitter corpus,"75,000 for twitter corpus",0.5912448167800903
translation,243,144,experimental-setup,experimental setup,To alleviate,posterior collapsing problem,experimental setup To alleviate posterior collapsing problem,0.6644567847251892
translation,243,151,experimental-setup,different components,in,our model,different components in our model,0.5342305898666382
translation,243,151,experimental-setup,experimental setup,has,nexus -h and nexus - f,experimental setup has nexus -h and nexus - f,0.5506794452667236
translation,243,145,experiments,rl,has,deep reinforcement learning chatbot,rl has deep reinforcement learning chatbot,0.5429149270057678
translation,243,200,experiments,nexus network,dominates,most fields,nexus network dominates most fields,0.7535125017166138
translation,243,7,model,connection,through,mutual information maximization,connection through mutual information maximization,0.6749957203865051
translation,243,7,model,model,strengthen,connection,model strengthen connection,0.7263432145118713
translation,243,8,model,nondifferentiability,of,discrete natural language tokens,nondifferentiability of discrete natural language tokens,0.47807374596595764
translation,243,8,model,nondifferentiability,introduce,auxiliary continuous code space,nondifferentiability introduce auxiliary continuous code space,0.6079827547073364
translation,243,8,model,code space,to,learnable prior distribution,code space to learnable prior distribution,0.4715918004512787
translation,243,8,model,learnable prior distribution,for,generation purpose,learnable prior distribution for generation purpose,0.5387733578681946
translation,243,8,model,model,To sidestep,nondifferentiability,model To sidestep nondifferentiability,0.7388239502906799
translation,243,8,model,model,map,code space,model map code space,0.7737550735473633
translation,243,24,model,nexus network,producing,more on -topic responses,nexus network producing more on -topic responses,0.7314897179603577
translation,243,24,model,more on -topic responses,to maintain,interactive conversation flow,more on -topic responses to maintain interactive conversation flow,0.6598640084266663
translation,243,24,model,model,propose,nexus network,model propose nexus network,0.6463262438774109
translation,243,43,model,"current , past and future utterances",trained to maximize,mutual information,"current , past and future utterances trained to maximize mutual information",0.7602024078369141
translation,243,43,model,mutual information,with,code space,mutual information with code space,0.6531698703765869
translation,243,43,model,each time step,has,"current , past and future utterances","each time step has current , past and future utterances",0.5955377817153931
translation,243,43,model,model,At,each time step,model At each time step,0.559150218963623
translation,243,44,model,simultaneously optimized,to predict,corresponding code space,simultaneously optimized to predict corresponding code space,0.7116057872772217
translation,243,44,model,efficient sampling,in,testing phase,efficient sampling in testing phase,0.5503926873207092
translation,243,44,model,model,has,learnable prior distribution,model has learnable prior distribution,0.522408127784729
translation,243,150,model,nexus,maximizing,mutual information,nexus maximizing mutual information,0.7939557433128357
translation,243,150,model,nexus network,maximizing,mutual information,nexus network maximizing mutual information,0.7843959331512451
translation,243,150,model,mutual information,with both,history and future,mutual information with both history and future,0.7382376194000244
translation,243,150,model,nexus,has,nexus network,nexus has nexus network,0.61726313829422
translation,243,150,model,model,has,nexus,model has nexus,0.5631052851676941
translation,243,160,results,best baseline model,in,most cases,best baseline model in most cases,0.5033124685287476
translation,243,160,results,nexus network,has,significantly outperforms,nexus network has significantly outperforms,0.6167795658111572
translation,243,160,results,significantly outperforms,has,best baseline model,significantly outperforms has best baseline model,0.5735251903533936
translation,243,160,results,results,has,nexus network,results has nexus network,0.5640871524810791
translation,243,161,results,nexus,absorb,advantages,nexus absorb advantages,0.7299670577049255
translation,243,161,results,advantages,from,nexus -h,advantages from nexus -h,0.5840915441513062
translation,243,161,results,advantages,from,nexus -f,advantages from nexus -f,0.5840135216712952
translation,243,161,results,results,has,nexus,results has nexus,0.547726571559906
translation,243,173,results,nexus network,achieves,best or near-best performances,nexus network achieves best or near-best performances,0.6856208443641663
translation,243,173,results,best or near-best performances,with,only greedy decoders,best or near-best performances with only greedy decoders,0.6183656454086304
translation,243,173,results,results,has,nexus network,results has nexus network,0.5640871524810791
translation,243,174,results,nexus -h,has,generally outperforms,nexus -h has generally outperforms,0.6266863346099854
translation,243,174,results,generally outperforms,has,nexus -f,generally outperforms has nexus -f,0.5798318982124329
translation,243,174,results,results,has,nexus -h,results has nexus -h,0.5162239074707031
translation,243,175,results,mmi and vhred,bring,minor improvements,mmi and vhred bring minor improvements,0.6684501767158508
translation,243,175,results,minor improvements,over,seq2seq model,minor improvements over seq2seq model,0.6546246409416199
translation,243,175,results,results,has,mmi and vhred,results has mmi and vhred,0.5071729421615601
translation,243,186,results,nexus -h,leads to,most significant improvement,nexus -h leads to most significant improvement,0.6301119327545166
translation,243,186,results,results,has,nexus -h,results has nexus -h,0.5162239074707031
translation,243,187,results,mmi model,performs,remarkably well,mmi model performs remarkably well,0.589920699596405
translation,243,187,results,results,has,mmi model,results has mmi model,0.5355782508850098
translation,243,189,results,history context,when computing,posterior distribution,history context when computing posterior distribution,0.6560672521591187
translation,243,189,results,posterior distribution,in,vhred,posterior distribution in vhred,0.5354935526847839
translation,243,189,results,similar performance,among,all metrics,similar performance among all metrics,0.5564864277839661
translation,243,189,results,results,removing,history context,results removing history context,0.6783639788627625
translation,243,190,results,rl,explicitly set,coherence score,rl explicitly set coherence score,0.671551525592804
translation,243,190,results,coherence score,as,reward function,coherence score as reward function,0.5096685886383057
translation,243,190,results,performance,far from,satisfying,performance far from satisfying,0.7336804866790771
translation,243,190,results,rl,has,performance,rl has performance,0.5967791080474854
translation,243,190,results,coherence score,has,performance,coherence score has performance,0.5651602745056152
translation,243,201,results,nexus - f,brings,more impact,nexus - f brings more impact,0.6546852588653564
translation,243,201,results,more impact,than,nexus - h,more impact than nexus - h,0.6336348056793213
translation,243,201,results,results,has,nexus - f,results has nexus - f,0.5234320759773254
translation,243,202,results,seq2seq models,fail to provide,informative response,seq2seq models fail to provide informative response,0.6730495691299438
translation,243,202,results,informative response,in,first turn,informative response in first turn,0.5177040696144104
translation,243,202,results,results,has,seq2seq models,results has seq2seq models,0.5086690783500671
translation,243,205,results,best diversity score,in,both datasets,best diversity score in both datasets,0.4716598689556122
translation,243,206,results,nexus -h,improves over,baselines,nexus -h improves over baselines,0.7777242660522461
translation,243,206,results,not as significantly,as,nexus -f,not as significantly as nexus -f,0.6482822895050049
translation,243,207,results,nexus network,generates,higher -quality responses,nexus network generates higher -quality responses,0.613734245300293
translation,243,207,results,higher -quality responses,in both,coherence and diversity,higher -quality responses in both coherence and diversity,0.6018179655075073
translation,243,207,results,results,has,nexus network,results has nexus network,0.5640871524810791
translation,243,218,results,pri and post human scores,highly correlated with,automatic evaluation metric,pri and post human scores highly correlated with automatic evaluation metric,0.6762735247612
translation,243,218,results,automatic evaluation metric,has,coherence,automatic evaluation metric has coherence,0.5402788519859314
translation,243,220,results,fluency,is,not a major problem,fluency is not a major problem,0.5851722955703735
translation,243,220,results,all models,produce,mostly well - formed sentences,all models produce mostly well - formed sentences,0.6102597713470459
translation,243,221,results,nexus network,produce,responses,nexus network produce responses,0.5954619646072388
translation,243,221,results,more acceptable,to,human judges,more acceptable to human judges,0.559263288974762
translation,243,221,results,results,has,nexus network,results has nexus network,0.5640871524810791
translation,244,171,ablation-analysis,our synthesis technique,derived from,multiwoz dataset,our synthesis technique derived from multiwoz dataset,0.6374576687812805
translation,244,171,ablation-analysis,our synthesis technique,adds,no value,our synthesis technique adds no value,0.6485323905944824
translation,244,171,ablation-analysis,ablation analysis,observe,our synthesis technique,ablation analysis observe our synthesis technique,0.6342154145240784
translation,244,138,baselines,trade,uses,soft copy mechanism,trade uses soft copy mechanism,0.6826013326644897
translation,244,138,baselines,transferable dialogue state generator ( trade ),uses,soft copy mechanism,transferable dialogue state generator ( trade ) uses soft copy mechanism,0.6117875576019287
translation,244,138,baselines,soft copy mechanism,to either copy,slot-values,soft copy mechanism to either copy slot-values,0.6737448573112488
translation,244,138,baselines,soft copy mechanism,generate,recurrent neural network ( rnn ),soft copy mechanism generate recurrent neural network ( rnn ),0.6553779244422913
translation,244,138,baselines,slot-values,from,utterance pairs,slot-values from utterance pairs,0.5621570944786072
translation,244,138,baselines,trade,has,transferable dialogue state generator ( trade ),trade has transferable dialogue state generator ( trade ),0.6257792115211487
translation,244,138,baselines,baselines,has,trade,baselines has trade,0.6254712343215942
translation,244,111,experimental-setup,all possible transitions uniformly,to maximize,variety and coverage,all possible transitions uniformly to maximize variety and coverage,0.7025025486946106
translation,244,111,experimental-setup,experimental setup,sample,all possible transitions uniformly,experimental setup sample all possible transitions uniformly,0.7204515337944031
translation,244,5,model,dialogue state tracking,where,in-domain training data,dialogue state tracking where in-domain training data,0.5515218377113342
translation,244,5,model,in-domain training data,synthesized from,abstract dialogue model,in-domain training data synthesized from abstract dialogue model,0.566992461681366
translation,244,5,model,in-domain training data,synthesized from,ontology of the domain,in-domain training data synthesized from ontology of the domain,0.6018180847167969
translation,244,55,model,novel dialogue model,suitable for,synthesis,novel dialogue model suitable for synthesis,0.7305237650871277
translation,244,55,model,model,present,novel dialogue model,model present novel dialogue model,0.6524322032928467
translation,244,141,model,sumbt,uses,attention mechanism,sumbt uses attention mechanism,0.6444171071052551
translation,244,141,model,slot-utterance matching belief tracker ( sumbt ),uses,attention mechanism,slot-utterance matching belief tracker ( sumbt ) uses attention mechanism,0.5756068825721741
translation,244,141,model,attention mechanism,over,user-agent utterances,attention mechanism over user-agent utterances,0.6596391797065735
translation,244,141,model,user-agent utterances,at,each turn,user-agent utterances at each turn,0.5629175901412964
translation,244,141,model,each turn,to extract,slot-value information,each turn to extract slot-value information,0.7333696484565735
translation,244,141,model,sumbt,has,slot-utterance matching belief tracker ( sumbt ),sumbt has slot-utterance matching belief tracker ( sumbt ),0.5895315408706665
translation,244,141,model,model,has,sumbt,model has sumbt,0.6577731370925903
translation,244,148,model,dialogue model and template library,into,new version of the tool,dialogue model and template library into new version of the tool,0.539337694644928
translation,244,148,model,model,incorporated,dialogue model and template library,model incorporated dialogue model and template library,0.6932092905044556
translation,244,6,results,data augmentation,through,synthesized data,data augmentation through synthesized data,0.7111981511116028
translation,244,6,results,synthesized data,improve,accuracy,synthesized data improve accuracy,0.6975553035736084
translation,244,6,results,accuracy,of,zero-shot learning,accuracy of zero-shot learning,0.5557842254638672
translation,244,6,results,zero-shot learning,for,trade model,zero-shot learning for trade model,0.6275448203086853
translation,244,6,results,zero-shot learning,for,the bert - based sumbt model,zero-shot learning for the bert - based sumbt model,0.6118200421333313
translation,244,6,results,the bert - based sumbt model,on,multiwoz 2.1 dataset,the bert - based sumbt model on multiwoz 2.1 dataset,0.4124782681465149
translation,244,6,results,results,show,data augmentation,results show data augmentation,0.6122227311134338
translation,244,7,results,only synthesized in- domain data,on,sumbt model,only synthesized in- domain data on sumbt model,0.5428494215011597
translation,244,7,results,2/3,of,accuracy,2/3 of accuracy,0.6405856609344482
translation,244,7,results,accuracy,obtained with,full training dataset,accuracy obtained with full training dataset,0.5996425747871399
translation,244,7,results,results,training with,only synthesized in- domain data,results training with only synthesized in- domain data,0.7744200825691223
translation,244,43,results,approach,improves over,previous state - of - the - art result,approach improves over previous state - of - the - art result,0.7667644023895264
translation,244,43,results,previous state - of - the - art result,on,zero-shot transfer learning,previous state - of - the - art result on zero-shot transfer learning,0.5125687718391418
translation,244,43,results,zero-shot transfer learning,for,multiwoz 2.1 tasks,zero-shot transfer learning for multiwoz 2.1 tasks,0.48256248235702515
translation,244,43,results,multiwoz 2.1 tasks,by,21 %,multiwoz 2.1 tasks by 21 %,0.5769007205963135
translation,244,43,results,21 %,across,domains,21 % across domains,0.7473068237304688
translation,244,43,results,results,has,approach,results has approach,0.5518386363983154
translation,244,172,results,our joint accuracy,within,usual margin of error,our joint accuracy within usual margin of error,0.6672307252883911
translation,244,172,results,usual margin of error,compared to,training,usual margin of error compared to training,0.6582105159759521
translation,244,172,results,training,with,original dataset,training with original dataset,0.6037127375602722
translation,244,172,results,results,obtain,almost identical slot accuracy,results obtain almost identical slot accuracy,0.582064151763916
translation,244,179,results,joint accuracy,for,trade,joint accuracy for trade,0.6627141237258911
translation,244,179,results,trade,varies from,domain to domain,trade varies from domain to domain,0.7589303255081177
translation,244,179,results,50.5 %,for,hotel,50.5 % for hotel,0.6778824329376221
translation,244,179,results,74.0 %,for,train,74.0 % for train,0.6659704446792603
translation,244,179,results,results,shows,joint accuracy,results shows joint accuracy,0.668055534362793
translation,244,180,results,domain accuracy,with,sumbt model,domain accuracy with sumbt model,0.6376886367797852
translation,244,180,results,sumbt model,better than,trade,sumbt model better than trade,0.7533793449401855
translation,244,180,results,trade,between,1 % and 4 %,trade between 1 % and 4 %,0.7117578983306885
translation,244,180,results,1 % and 4 %,for,all domains,1 % and 4 % for all domains,0.6355201005935669
translation,244,180,results,drops,by,about 4.5 %,drops by about 4.5 %,0.6689845323562622
translation,244,180,results,results,has,domain accuracy,results has domain accuracy,0.5644609928131104
translation,244,202,results,dialogue -model based zero-shot result,shows,our synthesized data,dialogue -model based zero-shot result shows our synthesized data,0.6502488255500793
translation,244,202,results,our synthesized data,improves,zero-shot accuracy,our synthesized data improves zero-shot accuracy,0.6753134727478027
translation,244,202,results,zero-shot accuracy,on,all domains,zero-shot accuracy on all domains,0.5060482025146484
translation,244,202,results,results,has,dialogue -model based zero-shot result,results has dialogue -model based zero-shot result,0.569616973400116
translation,244,203,results,trade,for,sumbt,trade for sumbt,0.6968824863433838
translation,244,203,results,6 %,on,taxi  ,6 % on taxi  ,0.6274053454399109
translation,244,203,results,6 %,on,restaurant  ,6 % on restaurant  ,0.5592265129089355
translation,244,203,results,19 %,on,restaurant  ,19 % on restaurant  ,0.5466119050979614
translation,244,203,results,3 %,on,taxi,3 % on taxi,0.630439281463623
translation,244,203,results,30 %,on,attraction,30 % on attraction,0.6149940490722656
translation,244,203,results,trade,has,joint accuracy,trade has joint accuracy,0.6059677004814148
translation,244,203,results,sumbt,has,joint accuracy,sumbt has joint accuracy,0.5655264854431152
translation,244,203,results,results,For,trade,results For trade,0.5904760360717773
translation,244,203,results,results,for,sumbt,results for sumbt,0.6721022129058838
translation,244,204,results,outperforms,by,large margin,outperforms by large margin,0.6290680766105652
translation,244,204,results,trade,by,large margin,trade by large margin,0.6518891453742981
translation,244,204,results,synthesis,has,sumbt,synthesis has sumbt,0.6707964539527893
translation,244,204,results,sumbt,has,outperforms,sumbt has outperforms,0.665898859500885
translation,244,204,results,outperforms,has,trade,outperforms has trade,0.6733927130699158
translation,244,204,results,results,With,synthesis,results With synthesis,0.5304616689682007
translation,244,205,results,high joint accuracy,of,65 %,high joint accuracy of 65 %,0.5778228044509888
translation,244,205,results,trade,from,8 % to 18 %,trade from 8 % to 18 %,0.5906982421875
translation,244,205,results,65 %,has,sumbt,65 % has sumbt,0.6752092242240906
translation,244,205,results,sumbt,has,outperforms,sumbt has outperforms,0.665898859500885
translation,244,205,results,outperforms,has,trade,outperforms has trade,0.6733927130699158
translation,244,230,results,synthesized training data,decreases as,percent of real data,synthesized training data decreases as percent of real data,0.6006976366043091
translation,244,230,results,percent of real data,has,increases,percent of real data has increases,0.6140472292900085
translation,244,231,results,more pronounced,for,sumbt,more pronounced for sumbt,0.709297776222229
translation,244,231,results,sumbt,than,trade,sumbt than trade,0.6832988858222961
translation,244,231,results,trade,for,all domains,trade for all domains,0.667117714881897
translation,244,231,results,all domains,even with,5 % real data,all domains even with 5 % real data,0.6691451668739319
translation,244,231,results,significant,for,  attraction   domain,significant for   attraction   domain,0.6022679209709167
translation,244,231,results,  attraction   domain,with,10 % real data,  attraction   domain with 10 % real data,0.6145870685577393
translation,245,18,baselines,open - dial,adopts,hybrid approach,open - dial adopts hybrid approach,0.6696420907974243
translation,245,18,baselines,hybrid approach,benefits of,logical and statistical methods,hybrid approach benefits of logical and statistical methods,0.6802955269813538
translation,245,18,baselines,logical and statistical methods,to dialogue modelling,single framework,logical and statistical methods to dialogue modelling single framework,0.622225821018219
translation,245,18,baselines,baselines,has,open - dial,baselines has open - dial,0.5716379880905151
translation,245,5,model,information -state architecture,where,dialogue state,information -state architecture where dialogue state,0.5663450956344604
translation,245,5,model,dialogue state,represented as,bayesian network,dialogue state represented as bayesian network,0.5912065505981445
translation,245,5,model,dialogue state,acts as,shared memory,dialogue state acts as shared memory,0.6678987741470337
translation,245,5,model,shared memory,for,all system modules,shared memory for all system modules,0.5709103345870972
translation,245,17,model,open-dial,has,"java-based , open-source software toolkit","open-dial has java-based , open-source software toolkit",0.5236126780509949
translation,247,160,baselines,reinforcement learning,to,mention - ranking models,reinforcement learning to mention - ranking models,0.5036406517028809
translation,247,160,baselines,mention - ranking models,to form,coreference clusters,mention - ranking models to form coreference clusters,0.570131778717041
translation,247,146,experimental-setup,"elmo ( peters et al. , 2018 ) embedding",as,initial word representations,"elmo ( peters et al. , 2018 ) embedding as initial word representations",0.46231454610824585
translation,247,146,experimental-setup,experimental setup,concatenation of,300d glove embedding,experimental setup concatenation of 300d glove embedding,0.7091600894927979
translation,247,147,experimental-setup,out - of-vocabulary words,initialized with,zero vectors,out - of-vocabulary words initialized with zero vectors,0.779390275478363
translation,247,147,experimental-setup,experimental setup,has,out - of-vocabulary words,experimental setup has out - of-vocabulary words,0.5583083629608154
translation,247,148,experimental-setup,ssd resnet 50 fpn coco,from,tensorflow detection model,ssd resnet 50 fpn coco from tensorflow detection model,0.49810290336608887
translation,247,148,experimental-setup,model,from,tensorflow detection model,model from tensorflow detection model,0.5226808190345764
translation,247,148,experimental-setup,ssd resnet 50 fpn coco,has,model,ssd resnet 50 fpn coco has model,0.5322306156158447
translation,247,148,experimental-setup,experimental setup,adopt,ssd resnet 50 fpn coco,experimental setup adopt ssd resnet 50 fpn coco,0.587247371673584
translation,247,149,experimental-setup,size,of,hidden states,size of hidden states,0.6102981567382812
translation,247,149,experimental-setup,size,of,projected embedding,size of projected embedding,0.5660908818244934
translation,247,149,experimental-setup,hidden states,in,lstm module,hidden states in lstm module,0.5192142128944397
translation,247,149,experimental-setup,lstm module,set to,200,lstm module set to 200,0.7022380828857422
translation,247,149,experimental-setup,projected embedding,for computing,similarity,projected embedding for computing similarity,0.6787391901016235
translation,247,149,experimental-setup,similarity,between,text spans and object labels,similarity between text spans and object labels,0.6463176012039185
translation,247,149,experimental-setup,similarity,is,512,similarity is 512,0.6165364980697632
translation,247,149,experimental-setup,experimental setup,size of,projected embedding,experimental setup size of projected embedding,0.6981300115585327
translation,247,149,experimental-setup,experimental setup,has,size,experimental setup has size,0.5329226851463318
translation,247,150,experimental-setup,feed -forward networks,for,contextual scoring and visual scoring,feed -forward networks for contextual scoring and visual scoring,0.6387327313423157
translation,247,150,experimental-setup,feed -forward networks,have,two 150 - dimension hidden layers,feed -forward networks have two 150 - dimension hidden layers,0.5764081478118896
translation,247,150,experimental-setup,feed -forward networks,have,one 100 - dimension hidden layer,feed -forward networks have one 100 - dimension hidden layer,0.5685635805130005
translation,247,150,experimental-setup,contextual scoring and visual scoring,have,two 150 - dimension hidden layers,contextual scoring and visual scoring have two 150 - dimension hidden layers,0.5567481517791748
translation,247,150,experimental-setup,contextual scoring and visual scoring,have,one 100 - dimension hidden layer,contextual scoring and visual scoring have one 100 - dimension hidden layer,0.5446359515190125
translation,247,150,experimental-setup,experimental setup,has,feed -forward networks,experimental setup has feed -forward networks,0.5419938564300537
translation,247,151,experimental-setup,model training,use,cross-entropy,model training use cross-entropy,0.6290634870529175
translation,247,151,experimental-setup,model training,use,"adam ( kingma and ba , 2015 )","model training use adam ( kingma and ba , 2015 )",0.5913255214691162
translation,247,151,experimental-setup,cross-entropy,as,loss function,cross-entropy as loss function,0.5249319076538086
translation,247,151,experimental-setup,"adam ( kingma and ba , 2015 )",as,optimizer,"adam ( kingma and ba , 2015 ) as optimizer",0.5069490075111389
translation,247,151,experimental-setup,experimental setup,For,model training,experimental setup For model training,0.5682768225669861
translation,247,152,experimental-setup,experimental setup,has,parameters,experimental setup has parameters,0.4818422794342041
translation,247,31,experiments,vispro,has,large-scale visual - supported pcr dataset,vispro has large-scale visual - supported pcr dataset,0.5487529635429382
translation,247,8,model,model,propose,novel visual - aware pcr model,model propose novel visual - aware pcr model,0.6431606411933899
translation,247,34,model,novel visual - aware pcr model viscoref,effectively extract,information,novel visual - aware pcr model viscoref effectively extract information,0.7754818201065063
translation,247,34,model,pronouns,in,dialogues,pronouns in dialogues,0.5246292948722839
translation,247,34,model,model,design,novel visual - aware pcr model viscoref,model design novel visual - aware pcr model viscoref,0.5705710649490356
translation,247,35,model,mentions,in,dialogue,mentions in dialogue,0.6040000915527344
translation,247,35,model,mentions,with,objects,mentions with objects,0.6220671534538269
translation,247,35,model,mentions,jointly use,contextual and visual information,mentions jointly use contextual and visual information,0.7127318382263184
translation,247,35,model,objects,in,image,objects in image,0.5408445596694946
translation,247,35,model,contextual and visual information,for,final prediction,contextual and visual information for final prediction,0.6001937985420227
translation,247,35,model,model,align,mentions,model align mentions,0.7173982262611389
translation,247,105,results,final iaa score,is,72.4,final iaa score is 72.4,0.506710410118103
translation,247,105,results,results,has,final iaa score,results has final iaa score,0.5190261006355286
translation,247,169,results,proposed model,has,viscoref,proposed model has viscoref,0.5801746845245361
translation,247,169,results,proposed model,has,outperforms,proposed model has outperforms,0.642342746257782
translation,247,169,results,viscoref,has,outperforms,viscoref has outperforms,0.6596987843513489
translation,247,169,results,outperforms,has,all the baseline models,outperforms has all the baseline models,0.5834654569625854
translation,247,169,results,all the baseline models,has,significantly,all the baseline models has significantly,0.5793172717094421
translation,247,169,results,results,has,proposed model,results has proposed model,0.5938616394996643
translation,247,170,results,  not discussed   pronouns,antecedents are absent in,dialogues,  not discussed   pronouns antecedents are absent in dialogues,0.7766351699829102
translation,247,170,results,more challenging,than,  discussed   pronouns,more challenging than   discussed   pronouns,0.6029548048973083
translation,247,170,results,  discussed   pronouns,whose antecedents appear in,dialogue context,  discussed   pronouns whose antecedents appear in dialogue context,0.7261254787445068
translation,247,170,results,models,has,  not discussed   pronouns,models has   not discussed   pronouns,0.5915917158126831
translation,247,173,results,visual feature,to,contextual feature,visual feature to contextual feature,0.49353915452957153
translation,247,173,results,visual feature,resolve,  discussed   pronouns,visual feature resolve   discussed   pronouns,0.6635163426399231
translation,247,173,results,performance,on,  not discussed   pronouns,performance on   not discussed   pronouns,0.5159459114074707
translation,247,173,results,model,on,  not discussed   pronouns,model on   not discussed   pronouns,0.5356445908546448
translation,247,173,results,hurt,has,performance,hurt has performance,0.6066785454750061
translation,247,174,results,proposed viscoref,improve,resolution,proposed viscoref improve resolution,0.6474937200546265
translation,247,174,results,resolution,of,  discussed   and   not discussed   pronouns,resolution of   discussed   and   not discussed   pronouns,0.6076958179473877
translation,247,174,results,resolution,both,  discussed   and   not discussed   pronouns,resolution both   discussed   and   not discussed   pronouns,0.6832118630409241
translation,247,179,results,our model,observe,huge gap,our model observe huge gap,0.5910565257072449
translation,247,179,results,huge gap,between,our model,huge gap between our model,0.6870778203010559
translation,247,179,results,huge gap,between,human being,huge gap between human being,0.7075918316841125
translation,247,179,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,247,179,results,outperforms,has,all the baseline methods,outperforms has all the baseline methods,0.5675629377365112
translation,248,5,experiments,three language modeling tasks,pre-train,transformers,three language modeling tasks pre-train transformers,0.5972342491149902
translation,248,5,experiments,three language modeling tasks,pre-train,token - and utterance - level language modeling,three language modeling tasks pre-train token - and utterance - level language modeling,0.6491159200668335
translation,248,5,experiments,three language modeling tasks,pre-train,utterance order prediction,three language modeling tasks pre-train utterance order prediction,0.6562805771827698
translation,248,5,experiments,utterance order prediction,that learn,token and utterance embeddings,utterance order prediction that learn token and utterance embeddings,0.5994850397109985
translation,248,5,experiments,token and utterance embeddings,for better understanding,dialogue contexts,token and utterance embeddings for better understanding dialogue contexts,0.654779314994812
translation,248,6,model,multitask learning,between,utterance prediction and the token span prediction,multitask learning between utterance prediction and the token span prediction,0.556099534034729
translation,248,6,model,utterance prediction and the token span prediction,applied to,finetune,utterance prediction and the token span prediction applied to finetune,0.7010753154754639
translation,248,6,model,finetune,for,span-based question answering ( qa ),finetune for span-based question answering ( qa ),0.6103769540786743
translation,248,6,model,model,has,multitask learning,model has multitask learning,0.5049652457237244
translation,248,14,model,fine- tuning,for,span- based qa,fine- tuning for span- based qa,0.5890229344367981
translation,248,14,model,every utterance,separated encoded,multi-head attentions,every utterance separated encoded multi-head attentions,0.8185956478118896
translation,248,14,model,additional transformers,built on,token and utterance embeddings,additional transformers built on token and utterance embeddings,0.7156162858009338
translation,248,14,model,fine- tuning,has,every utterance,fine- tuning has every utterance,0.6019938588142395
translation,248,14,model,span- based qa,has,every utterance,span- based qa has every utterance,0.5995338559150696
translation,248,76,results,* pre models,show,marginal improvement,* pre models show marginal improvement,0.6858673095703125
translation,248,76,results,marginal improvement,over,base models,marginal improvement over base models,0.7137187719345093
translation,248,76,results,results,has,* pre models,results has * pre models,0.5600688457489014
translation,248,77,results,models,using,our approach,models using our approach,0.6676508784294128
translation,248,77,results,models,perform,noticeably better,models perform noticeably better,0.5571387410163879
translation,248,77,results,noticeably better,than,baseline models,noticeably better than baseline models,0.5705884695053101
translation,248,77,results,noticeably better,showing,3.8 % and 1.4 % improvements,noticeably better showing 3.8 % and 1.4 % improvements,0.7169328331947327
translation,248,77,results,3.8 % and 1.4 % improvements,on,sm,3.8 % and 1.4 % improvements on sm,0.5375638008117676
translation,248,77,results,sm,from,bert and roberta,sm from bert and roberta,0.6359761953353882
translation,248,77,results,results,has,models,results has models,0.5335168838500977
translation,248,78,results,two dialogue-specific lm approaches,give,very marginal improvement,two dialogue-specific lm approaches give very marginal improvement,0.561398446559906
translation,248,78,results,very marginal improvement,over,baseline models,very marginal improvement over baseline models,0.668452262878418
translation,248,78,results,results,has,two dialogue-specific lm approaches,results has two dialogue-specific lm approaches,0.4709396958351135
translation,248,81,results,improvement,on,um,improvement on um,0.659523606300354
translation,248,81,results,um,giving,2 % and 1 % boosts,um giving 2 % and 1 % boosts,0.747188925743103
translation,248,81,results,2 % and 1 % boosts,to,bert pre and roberta pre,2 % and 1 % boosts to bert pre and roberta pre,0.641488790512085
translation,248,81,results,results,has,improvement,results has improvement,0.6248279809951782
translation,249,170,ablation-analysis,features,induced from,addressee 's utterance,features induced from addressee 's utterance,0.6615850329399109
translation,249,170,ablation-analysis,prediction performance,for,emotions,prediction performance for emotions,0.6478659510612488
translation,249,170,ablation-analysis,features,has,significantly improved,features has significantly improved,0.5823861360549927
translation,249,170,ablation-analysis,addressee 's utterance,has,significantly improved,addressee 's utterance has significantly improved,0.6110059022903442
translation,249,170,ablation-analysis,significantly improved,has,prediction performance,significantly improved has prediction performance,0.5508537888526917
translation,249,170,ablation-analysis,emotions,has,other than fear,emotions has other than fear,0.5163535475730896
translation,249,170,ablation-analysis,ablation analysis,see that,features,ablation analysis see that features,0.670304000377655
translation,249,43,baselines,standard classifiers,for predicting,emotion,standard classifiers for predicting emotion,0.7498733401298523
translation,249,43,baselines,emotion,of,addressee,emotion of addressee,0.5888243913650513
translation,249,114,experiments,srilm,for learning,translation model,srilm for learning translation model,0.6943410038948059
translation,249,114,experiments,srilm,for learning,5 - gram language model,srilm for learning 5 - gram language model,0.6487959027290344
translation,249,39,model,data-driven approach,to performing,two tasks,data-driven approach to performing two tasks,0.6434223651885986
translation,249,39,model,model,explores,data-driven approach,model explores data-driven approach,0.7040281891822815
translation,249,48,model,method,for controlling,contents,method for controlling contents,0.6207146644592285
translation,249,48,model,contents,of,response,contents of response,0.6613144874572754
translation,249,48,model,model,investigate,method,model investigate method,0.5994312167167664
translation,249,50,model,multiple models,adapted for,eliciting,multiple models adapted for eliciting,0.6758896112442017
translation,249,50,model,eliciting,has,one specific emotion,eliciting has one specific emotion,0.5051890015602112
translation,249,50,model,model,learn,multiple models,model learn multiple models,0.6810702085494995
translation,249,51,model,model interpolation,for addressing,data sparseness,model interpolation for addressing data sparseness,0.6802325248718262
translation,249,51,model,model,perform,model interpolation,model perform model interpolation,0.6036876440048218
translation,251,182,baselines,woz 2.0,compare,nbt models,woz 2.0 compare nbt models,0.6964235901832581
translation,251,182,baselines,nbt models,to,more sophisticated belief tracking model,nbt models to more sophisticated belief tracking model,0.5419299602508545
translation,251,209,baselines,random ' word vectors,initialised using,xavier initialisation,random ' word vectors initialised using xavier initialisation,0.7928887605667114
translation,251,209,baselines,distributional glove vectors,trained using,co-occurrence information,distributional glove vectors trained using co-occurrence information,0.7324889898300171
translation,251,209,baselines,co-occurrence information,in,large textual corpora,co-occurrence information in large textual corpora,0.4640316963195801
translation,251,209,baselines,semantically specialised paragram - sl999 vectors,obtained by injecting,semantic similarity constraints,semantically specialised paragram - sl999 vectors obtained by injecting semantic similarity constraints,0.6467003226280212
translation,251,209,baselines,semantic similarity constraints,from,paraphrase database,semantic similarity constraints from paraphrase database,0.5183244943618774
translation,251,162,experiments,woz 2.0,performed,wizard of oz style experiment,woz 2.0 performed wizard of oz style experiment,0.2893211543560028
translation,251,162,experiments,wizard of oz style experiment,in which,amazon mechanical turk users,wizard of oz style experiment in which amazon mechanical turk users,0.580629289150238
translation,251,162,experiments,amazon mechanical turk users,assumed,role,amazon mechanical turk users assumed role,0.6781352758407593
translation,251,162,experiments,role,of,system,role of system,0.6067988872528076
translation,251,162,experiments,task - oriented dialogue system,based on,dstc2 ontology,task - oriented dialogue system based on dstc2 ontology,0.6304988861083984
translation,251,162,experiments,2,has,woz 2.0,2 has woz 2.0,0.5592793226242065
translation,251,185,hyperparameters,initial adam learning rate,set to,0.001,initial adam learning rate set to 0.001,0.6719345450401306
translation,251,185,hyperparameters,1 8 th,of,positive examples,1 8 th of positive examples,0.6000689268112183
translation,251,185,hyperparameters,1 8 th,included in,each mini-batch,1 8 th included in each mini-batch,0.6931635737419128
translation,251,185,hyperparameters,positive examples,included in,each mini-batch,positive examples included in each mini-batch,0.5690704584121704
translation,251,187,hyperparameters,gradient clipping,to,"[ ?2.0 , 2.0 ]","gradient clipping to [ ?2.0 , 2.0 ]",0.5288860201835632
translation,251,187,hyperparameters,gradient clipping,to handle,exploding gradients,gradient clipping to handle exploding gradients,0.6969633102416992
translation,251,187,hyperparameters,hyperparameters,has,gradient clipping,hyperparameters has gradient clipping,0.5166950225830078
translation,251,7,model,model,propose,novel neural belief tracking ( nbt ) framework,model propose novel neural belief tracking ( nbt ) framework,0.6648657321929932
translation,251,8,model,nbt,reason over,pre-trained word vectors,nbt reason over pre-trained word vectors,0.6441476941108704
translation,251,8,model,distributed representations,of,user utterances and dialogue context,distributed representations of user utterances and dialogue context,0.5559418797492981
translation,251,8,model,model,has,nbt,model has nbt,0.5911585688591003
translation,251,35,model,model,called,neural belief tracker ( nbt ),model called neural belief tracker ( nbt ),0.6542852520942688
translation,251,36,model,efficiently learning,to handle,variation,efficiently learning to handle variation,0.7350862622261047
translation,251,36,model,model,couple,slu and dst,model couple slu and dst,0.7772682309150696
translation,251,70,model,nbt model,efficiently learns from,avail-able data,nbt model efficiently learns from avail-able data,0.6831159591674805
translation,251,70,model,nbt model,leveraging,semantic information,nbt model leveraging semantic information,0.6427673697471619
translation,251,70,model,semantic information,from,pre-trained word vectors,semantic information from pre-trained word vectors,0.5009750127792358
translation,251,70,model,pre-trained word vectors,to resolve,lexical / morphological ambiguity,pre-trained word vectors to resolve lexical / morphological ambiguity,0.6637139916419983
translation,251,70,model,number of parameters,shared across,ontology values,number of parameters shared across ontology values,0.6816467046737671
translation,251,70,model,flexibility,to learn,domainspecific paraphrasings,flexibility to learn domainspecific paraphrasings,0.6115745306015015
translation,251,70,model,model,has,nbt model,model has nbt model,0.583085298538208
translation,251,41,results,two datasets,show,nbt models,two datasets show nbt models,0.6263535618782043
translation,251,41,results,nbt models,match the performance,delexicalisation - based models,nbt models match the performance delexicalisation - based models,0.7074189186096191
translation,251,41,results,delexicalisation - based models,which make use of,hand- crafted semantic lexicons,delexicalisation - based models which make use of hand- crafted semantic lexicons,0.7362669110298157
translation,251,41,results,resources,are,not available,resources are not available,0.5204662084579468
translation,251,41,results,nbt models,has,significantly outperform,nbt models has significantly outperform,0.6048780679702759
translation,251,62,results,outperformed,all systems that only used,external slu features,outperformed all systems that only used external slu features,0.7114264965057373
translation,251,62,results,dstc2,has,systems which used no external slu module,dstc2 has systems which used no external slu module,0.6080324649810791
translation,251,62,results,systems which used no external slu module,has,outperformed,systems which used no external slu module has outperformed,0.600726306438446
translation,251,200,results,outperformed,in terms of,joint goal and request accuracies,outperformed in terms of joint goal and request accuracies,0.7159613370895386
translation,251,200,results,nbt models,has,outperformed,nbt models has outperformed,0.5901476740837097
translation,251,200,results,outperformed,has,baseline models,outperformed has baseline models,0.6163802146911621
translation,251,200,results,results,has,nbt models,results has nbt models,0.49179694056510925
translation,251,204,results,improvement,over,baseline,improvement over baseline,0.7266895771026611
translation,251,204,results,baseline,greater on,woz 2.0,baseline greater on woz 2.0,0.6756771206855774
translation,251,204,results,results,has,improvement,results has improvement,0.6248279809951782
translation,251,205,results,language of the subjects,in,dstc2 dataset,language of the subjects in dstc2 dataset,0.5077903270721436
translation,251,205,results,asr errors,is,hurdle,asr errors is hurdle,0.6081324815750122
translation,251,205,results,accuracy,rises to,0.96,accuracy rises to 0.96,0.6400802731513977
translation,251,210,results,semantically specialised word vectors,leads to,considerable performance gains,semantically specialised word vectors leads to considerable performance gains,0.6462050080299377
translation,251,210,results,xavier vectors,for,goal tracking,xavier vectors for goal tracking,0.6134710311889648
translation,251,210,results,goal tracking,on,both datasets,goal tracking on both datasets,0.46179863810539246
translation,251,210,results,considerable performance gains,has,paragram -sl999 vectors,considerable performance gains has paragram -sl999 vectors,0.5887768268585205
translation,251,210,results,paragram -sl999 vectors,has,significantly ),paragram -sl999 vectors has significantly ),0.6226648092269897
translation,251,210,results,paragram -sl999 vectors,has,outperformed,paragram -sl999 vectors has outperformed,0.590977132320404
translation,251,210,results,significantly ),has,outperformed,significantly ) has outperformed,0.6590545773506165
translation,251,210,results,results,use of,semantically specialised word vectors,results use of semantically specialised word vectors,0.6194254755973816
translation,251,211,results,gains,particularly robust for,noisy dstc2 data,gains particularly robust for noisy dstc2 data,0.7450059056282043
translation,251,211,results,noisy dstc2 data,where,both collections of pre-trained vectors,noisy dstc2 data where both collections of pre-trained vectors,0.5787739753723145
translation,251,211,results,outperformed,has,random initialisation,outperformed has random initialisation,0.6366233229637146
translation,251,211,results,results,has,gains,results has gains,0.5357105135917664
translation,251,212,results,gains,are,weaker,gains are weaker,0.6161507368087769
translation,251,212,results,weaker,for,noise- free woz 2.0 dataset,weaker for noise- free woz 2.0 dataset,0.6011565327644348
translation,251,212,results,results,has,gains,results has gains,0.5357105135917664
translation,251,213,results,do not improve,over,randomly initialised ones,do not improve over randomly initialised ones,0.6521177291870117
translation,251,213,results,glove vectors,has,do not improve,glove vectors has do not improve,0.619059145450592
translation,252,114,ablation-analysis,selection module,boost,system 's performance significanlty,selection module boost system 's performance significanlty,0.6724655628204346
translation,252,114,ablation-analysis,high performance,in,different domains,high performance in different domains,0.5477502346038818
translation,252,114,ablation-analysis,high performance,promising indicator for,domain and language portability,high performance promising indicator for domain and language portability,0.6547567844390869
translation,252,113,experiments,best configuration,of,our system,best configuration of our system,0.5839351415634155
translation,252,113,experiments,our system,achieved,highest performance,our system achieved highest performance,0.7435455322265625
translation,252,113,experiments,run 1,achieved,highest performance,run 1 achieved highest performance,0.7492688298225403
translation,252,113,experiments,highest performance,compared to,other submissions,highest performance compared to other submissions,0.6208308935165405
translation,252,113,experiments,other submissions,in,almost all domains,other submissions in almost all domains,0.4924602210521698
translation,252,27,model,candidate selection model posterior,fused with,phrase -level semantic similarity metric,candidate selection model posterior fused with phrase -level semantic similarity metric,0.6635538935661316
translation,252,27,model,model,has,candidate selection model posterior,model has candidate selection model posterior,0.5519418716430664
translation,252,112,model,supervised grammar induction system,fusion of,grammar fragment selection and similarity estimation modules,supervised grammar induction system fusion of grammar fragment selection and similarity estimation modules,0.6453573107719421
translation,252,112,model,model,proposed,supervised grammar induction system,model proposed supervised grammar induction system,0.6634746789932251
translation,252,104,results,systems,suggests,our submission,systems suggests our submission,0.6857022047042847
translation,252,104,results,our submission,achieved,highest performance,our submission achieved highest performance,0.7146051526069641
translation,252,104,results,highest performance,for,almost all domains and languages,highest performance for almost all domains and languages,0.5353710055351257
translation,252,107,results,performance,is,consistently worse,performance is consistently worse,0.5934910178184509
translation,252,107,results,consistently worse,for,runs 2 and 3,consistently worse for runs 2 and 3,0.6339255571365356
translation,252,107,results,consistently worse,with the exception of,travel english dataset,consistently worse with the exception of travel english dataset,0.6902949810028076
translation,252,107,results,runs 2 and 3,with the exception of,travel english dataset,runs 2 and 3 with the exception of travel english dataset,0.6505950689315796
translation,252,107,results,results,observe,performance,results observe performance,0.6366938948631287
translation,252,108,results,performance,of,runs 1 and 2,performance of runs 1 and 2,0.6149551868438721
translation,252,108,results,runs 1 and 2,observe that,character bigram metric,runs 1 and 2 observe that character bigram metric,0.5867918729782104
translation,252,108,results,character bigram metric,has,consistently outperforms,character bigram metric has consistently outperforms,0.6267213821411133
translation,252,108,results,consistently outperforms,has,context - based one,consistently outperforms has context - based one,0.6044233441352844
translation,252,108,results,results,Comparing,performance,results Comparing performance,0.7179481983184814
translation,252,109,results,underperforms,for,finance,underperforms for finance,0.6367099285125732
translation,252,109,results,underperforms,for,the tourism domain,underperforms for the tourism domain,0.590395987033844
translation,252,109,results,individual datasets,has,our system,individual datasets has our system,0.5891051888465881
translation,252,109,results,our system,has,underperforms,our system has underperforms,0.6134560704231262
translation,252,109,results,results,For,individual datasets,results For individual datasets,0.5700223445892334
translation,253,4,model,browsing long list,using,haptic input,browsing long list using haptic input,0.6873295307159424
translation,253,4,model,browsing long list,using,spoken output,browsing long list using spoken output,0.6877124309539795
translation,253,4,model,dico ii +,has,in-vehicle dialogue system,dico ii + has in-vehicle dialogue system,0.5987587571144104
translation,253,10,model,dico ii + demonstrator,introduces,novel combination of flexible multimodal menu-based dialogue,dico ii + demonstrator introduces novel combination of flexible multimodal menu-based dialogue,0.5590251088142395
translation,253,10,model,speech cursor,enable,flexible multimodal interaction,speech cursor enable flexible multimodal interaction,0.7404881715774536
translation,254,199,ablation-analysis,kvbert reranking,improves,profile consistency,kvbert reranking improves profile consistency,0.724496603012085
translation,254,199,ablation-analysis,profile consistency,increasing,rate of entailment,profile consistency increasing rate of entailment,0.7021762728691101
translation,254,199,ablation-analysis,decreasing,has,rate of contradiction,decreasing has rate of contradiction,0.6004952788352966
translation,254,219,ablation-analysis,structural information,contributes to,final performance,structural information contributes to final performance,0.7054646611213684
translation,254,36,baselines,annotated kvpi dataset,set up,different baseline models,annotated kvpi dataset set up different baseline models,0.6071426272392273
translation,254,36,baselines,annotated kvpi dataset,propose,key-value structure information enriched bert ( kvbert ) model,annotated kvpi dataset propose key-value structure information enriched bert ( kvbert ) model,0.6109219193458557
translation,254,36,baselines,key-value structure information enriched bert ( kvbert ) model,leverages,dependency structures,key-value structure information enriched bert ( kvbert ) model leverages dependency structures,0.694732129573822
translation,254,36,baselines,dependency structures,in,profiles,dependency structures in profiles,0.5323126316070557
translation,254,36,baselines,dependency structures,to enrich,contextual representations,dependency structures to enrich contextual representations,0.6702040433883667
translation,254,147,experimental-setup,tree -lstm,firstly pretrained on,kvpi dataset,tree -lstm firstly pretrained on kvpi dataset,0.8228556513786316
translation,254,147,experimental-setup,tree -lstm,jointly finetuned with,bert representations,tree -lstm jointly finetuned with bert representations,0.7551842927932739
translation,254,147,experimental-setup,kvpi dataset,for,13 epochs,kvpi dataset for 13 epochs,0.5810949802398682
translation,254,147,experimental-setup,bert representations,for,3 epochs,bert representations for 3 epochs,0.7029111981391907
translation,254,147,experimental-setup,experimental setup,has,tree -lstm,experimental setup has tree -lstm,0.562681257724762
translation,254,148,experimental-setup,kvbert model,implemented in,pytorch,kvbert model implemented in pytorch,0.718425452709198
translation,254,148,experimental-setup,experimental setup,has,kvbert model,experimental setup has kvbert model,0.5430745482444763
translation,254,34,experiments,three representative domains,involved in,human annotation,three representative domains involved in human annotation,0.6087509989738464
translation,254,34,experiments,"gender , location , and constellation",involved in,human annotation,"gender , location , and constellation involved in human annotation",0.6765073537826538
translation,254,32,model,structured profile consistency,need,new dataset,structured profile consistency need new dataset,0.6931242942810059
translation,254,32,model,new dataset,with,fine- grained labels,new dataset with fine- grained labels,0.5860682129859924
translation,254,32,model,fine- grained labels,between,response and profile,fine- grained labels between response and profile,0.673130214214325
translation,254,32,model,model,leverage,structure information,model leverage structure information,0.768267035484314
translation,254,32,model,structure information,in,profile,structure information in profile,0.5277878642082214
translation,254,173,results,explicit modeling,of,profile structures,explicit modeling of profile structures,0.5942160487174988
translation,254,173,results,our kvbert,achieves,best performance,our kvbert achieves best performance,0.7052730917930603
translation,254,173,results,best performance,on,all metrics,best performance on all metrics,0.5029283165931702
translation,254,173,results,all metrics,across,all domains,all metrics across all domains,0.7081018090248108
translation,254,173,results,explicit modeling,has,our kvbert,explicit modeling has our kvbert,0.6332777142524719
translation,254,173,results,profile structures,has,our kvbert,profile structures has our kvbert,0.6507971286773682
translation,254,173,results,results,With,explicit modeling,results With explicit modeling,0.6164187788963318
translation,254,174,results,kvbert,is,only model,kvbert is only model,0.6005866527557373
translation,254,174,results,only model,whose,all metrics,only model whose all metrics,0.6382727026939392
translation,254,174,results,all metrics,are over 90 %,kvpi test set,all metrics are over 90 % kvpi test set,0.722937822341919
translation,254,174,results,results,has,kvbert,results has kvbert,0.5938493609428406
translation,254,176,results,interesting phenomenon,between,bert - kv and bert - template,interesting phenomenon between bert - kv and bert - template,0.6690415740013123
translation,254,176,results,performance,of,bert - template,performance of bert - template,0.6257091760635376
translation,254,176,results,bert - template,on,all three individual domains,bert - template on all three individual domains,0.5584748387336731
translation,254,176,results,all three individual domains,are,better,all three individual domains are better,0.5558049082756042
translation,254,176,results,better,than,bert - kv's,better than bert - kv's,0.6760378479957581
translation,254,176,results,bert - kv and bert - template,has,performance,bert - kv and bert - template has performance,0.5982586145401001
translation,254,176,results,results,noticed,interesting phenomenon,results noticed interesting phenomenon,0.7177711725234985
translation,254,211,results,kvbert,obtains,good agreements,kvbert obtains good agreements,0.6665414571762085
translation,254,211,results,good agreements,with,humans,good agreements with humans,0.6732690930366516
translation,254,211,results,results,has,kvbert,results has kvbert,0.5938493609428406
translation,254,220,results,treelstm,is,low accuracy,treelstm is low accuracy,0.5663741827011108
translation,254,220,results,treelstm,at,low accuracy,treelstm at low accuracy,0.5209263563156128
translation,254,220,results,performance,of,kvbert model,performance of kvbert model,0.6268361806869507
translation,254,220,results,kvbert model,inferior to,baseline model,kvbert model inferior to baseline model,0.6920177936553955
translation,254,220,results,treelstm,has,performance,treelstm has performance,0.6035221219062805
translation,254,220,results,low accuracy,has,performance,low accuracy has performance,0.5766597986221313
translation,254,220,results,results,When,treelstm,results When treelstm,0.626395046710968
translation,255,47,baselines,word2vec,extract,each token,word2vec extract each token,0.6713630557060242
translation,255,47,baselines,word vector,from,word2vec,word vector from word2vec,0.5175144076347351
translation,255,47,baselines,each token,has,word vector,each token has word vector,0.5520427227020264
translation,255,47,baselines,baselines,has,word2vec,baselines has word2vec,0.5186407566070557
translation,255,78,hyperparameters,neural network,set,size,neural network set size,0.6976587176322937
translation,255,78,hyperparameters,size,to,100,size to 100,0.6643491387367249
translation,255,78,hyperparameters,all the hidden layers,of,cnn and the lstm,all the hidden layers of cnn and the lstm,0.5406932234764099
translation,255,78,hyperparameters,all the hidden layers,of,convolutional window,all the hidden layers of convolutional window,0.5670413970947266
translation,255,78,hyperparameters,hyperparameters,In,neural network,hyperparameters In neural network,0.5068250298500061
translation,255,79,hyperparameters,dropout regularization layer,after,output,dropout regularization layer after output,0.5787081718444824
translation,255,79,hyperparameters,output,of,lstm,output of lstm,0.5861949920654297
translation,255,79,hyperparameters,l2 regularization,on,softmax output layer,l2 regularization on softmax output layer,0.4656713306903839
translation,255,79,hyperparameters,hyperparameters,applied,dropout regularization layer,hyperparameters applied dropout regularization layer,0.5902434587478638
translation,255,79,hyperparameters,hyperparameters,applied,l2 regularization,hyperparameters applied l2 regularization,0.60664963722229
translation,255,80,hyperparameters,network,trained with,standard backpropagation,network trained with standard backpropagation,0.7182908058166504
translation,255,80,hyperparameters,standard backpropagation,using,each scene,standard backpropagation using each scene,0.6479800343513489
translation,255,80,hyperparameters,each scene,as,training unit,each scene as training unit,0.531076967716217
translation,255,80,hyperparameters,hyperparameters,has,network,hyperparameters has network,0.5504763126373291
translation,255,4,model,memory based framework,to predict,humor,memory based framework to predict humor,0.7167274355888367
translation,255,4,model,humor,has,in dialogues,humor has in dialogues,0.5492799282073975
translation,255,7,results,out neural network framework,able to improve,f-score,out neural network framework able to improve f-score,0.6794925332069397
translation,255,7,results,f-score,of,8 %,f-score of 8 %,0.5944671630859375
translation,255,7,results,8 %,over,conditional random field baseline,8 % over conditional random field baseline,0.6837720274925232
translation,255,7,results,results,has,out neural network framework,results has out neural network framework,0.6138671040534973
translation,255,89,results,lstm,with the aid of,high level feature vector,lstm with the aid of high level feature vector,0.6430767178535461
translation,255,89,results,outperformed,with,highest accuracy,outperformed with highest accuracy,0.7087578177452087
translation,255,89,results,all the crf baselines,with,highest accuracy,all the crf baselines with highest accuracy,0.590294361114502
translation,255,89,results,all the crf baselines,with,highest f-score,all the crf baselines with highest f-score,0.6000151038169861
translation,255,89,results,highest accuracy,of,70.0 %,highest accuracy of 70.0 %,0.5398064255714417
translation,255,89,results,highest f-score,of,62.9 %,highest f-score of 62.9 %,0.5121618509292603
translation,255,89,results,high level feature vector,has,outperformed,high level feature vector has outperformed,0.6261438727378845
translation,255,89,results,outperformed,has,all the crf baselines,outperformed has all the crf baselines,0.5977761745452881
translation,255,89,results,results,has,lstm,results has lstm,0.5593706965446472
translation,255,90,results,biggest improvement,of,lstm,biggest improvement of lstm,0.5554229021072388
translation,255,90,results,biggest improvement,is,improvement,biggest improvement is improvement,0.5457286238670349
translation,255,90,results,lstm,is,improvement,lstm is improvement,0.608665406703949
translation,255,90,results,improvement,of,recall,improvement of recall,0.6037074327468872
translation,255,90,results,improvement,without affecting,too much,improvement without affecting too much,0.6856169104576111
translation,255,90,results,improvement,without affecting,precision,improvement without affecting precision,0.6955575346946716
translation,255,90,results,recall,without affecting,too much,recall without affecting too much,0.735034704208374
translation,255,90,results,recall,without affecting,precision,recall without affecting precision,0.6664246320724487
translation,255,90,results,too much,has,precision,too much has precision,0.5862375497817993
translation,255,90,results,results,has,biggest improvement,results has biggest improvement,0.552254855632782
translation,255,91,results,lexical features,given by,n-gram,lexical features given by n-gram,0.6137766242027283
translation,255,91,results,lexical features,yield,many false positives,lexical features yield many false positives,0.719602108001709
translation,255,91,results,n-gram,from,context window,n-gram from context window,0.5618976950645447
translation,255,91,results,n-gram,are,very useful,n-gram are very useful,0.5759344100952148
translation,255,91,results,very useful,to recognize,more punchlines,very useful to recognize more punchlines,0.6790194511413574
translation,255,91,results,more punchlines,in,our baseline experiment,more punchlines in our baseline experiment,0.5210425853729248
translation,255,91,results,many false positives,when,same n-gram,many false positives when same n-gram,0.6335347294807434
translation,255,91,results,same n-gram,used in,different contexts,same n-gram used in different contexts,0.6483295559883118
translation,255,91,results,results,has,lexical features,results has lexical features,0.519563615322113
translation,255,94,results,cnn,is,more effective,cnn is more effective,0.5773662328720093
translation,255,94,results,cnn,obtaining,recall,cnn obtaining recall,0.6311451196670532
translation,255,94,results,more effective,obtaining,recall,more effective obtaining recall,0.6823844313621521
translation,255,94,results,recall,of,10 % higher and 6 % more,recall of 10 % higher and 6 % more,0.6329144835472107
translation,255,94,results,10 % higher and 6 % more,in,fscore,10 % higher and 6 % more in fscore,0.580780565738678
translation,255,94,results,results,has,cnn,results has cnn,0.5897790193557739
translation,255,102,results,our neural network,particularly effective in,increasing,our neural network particularly effective in increasing,0.7314503788948059
translation,255,102,results,f-score,to,62.9 %,f-score to 62.9 %,0.5149645805358887
translation,255,102,results,62.9 %,over,conditional random field baseline,62.9 % over conditional random field baseline,0.6394039988517761
translation,255,102,results,conditional random field baseline,of,58.1 %,conditional random field baseline of 58.1 %,0.5354307293891907
translation,255,102,results,increasing,has,f-score,increasing has f-score,0.6114411354064941
translation,255,102,results,results,showed,our neural network,results showed our neural network,0.6899467706680298
translation,256,87,ablation-analysis,small radius,show,less diversity,small radius show less diversity,0.6600644588470459
translation,256,87,ablation-analysis,less diversity,in terms of,unique outputs,less diversity in terms of unique outputs,0.668413519859314
translation,256,48,baselines,encoder-decoder dialogue model,with,ml decoding ( dial - mle ),encoder-decoder dialogue model with ml decoding ( dial - mle ),0.6598078012466431
translation,256,49,baselines,anti-lm decoder,on top of,encoder-decoder,anti-lm decoder on top of encoder-decoder,0.7003369331359863
translation,256,18,model,latent variable model,for,one - shot dialogue response,latent variable model for one - shot dialogue response,0.5952402353286743
translation,256,18,model,model,present,latent variable model,model present latent variable model,0.6619983315467834
translation,256,69,results,lexical diversity,of,dial -lv,lexical diversity of dial -lv,0.596184492111206
translation,256,69,results,dial -lv,almost identical to,dial - mle,dial -lv almost identical to dial - mle,0.7344005703926086
translation,256,69,results,results,has,lexical diversity,results has lexical diversity,0.5232128500938416
translation,256,70,results,dial -lv,beats,dial - samp,dial -lv beats dial - samp,0.7864238619804382
translation,256,70,results,rivals,beats,dial - samp,rivals beats dial - samp,0.7677397727966309
translation,256,70,results,dial - samp,in terms of,sentential diversity,dial - samp in terms of sentential diversity,0.7204892039299011
translation,256,70,results,dial - samp,in terms of,lexical diversity,dial - samp in terms of lexical diversity,0.7239528298377991
translation,256,70,results,dial - samp,in terms of,lexical diversity,dial - samp in terms of lexical diversity,0.7239528298377991
translation,256,70,results,dial - samp,in terms of,lexical diversity,dial - samp in terms of lexical diversity,0.7239528298377991
translation,256,70,results,rivals,has,dial - samp,rivals has dial - samp,0.6601940393447876
translation,256,70,results,results,note,dial -lv,results note dial -lv,0.582533597946167
translation,256,70,results,results,beats,dial - samp,results beats dial - samp,0.655779242515564
translation,256,92,results,ml decoding,not seem to be,best objective,ml decoding not seem to be best objective,0.5660224556922913
translation,256,92,results,best objective,for generating,diverse dialogue,best objective for generating diverse dialogue,0.7189386487007141
translation,257,113,baselines,baseline models,include,"tgen ( du?ek and jur?ek , 2016 )","baseline models include tgen ( du?ek and jur?ek , 2016 )",0.5525650978088379
translation,257,113,baselines,baseline models,include,"sc -lstm ( wen et al. , 2015 b )","baseline models include sc -lstm ( wen et al. , 2015 b )",0.514367401599884
translation,257,113,baselines,baseline models,include,"ralstm ( tran and nguyen , 2017 )","baseline models include ralstm ( tran and nguyen , 2017 )",0.5279744863510132
translation,257,113,baselines,baseline models,include,"slug ( juraska et al. , 2018 )","baseline models include slug ( juraska et al. , 2018 )",0.5832581520080566
translation,257,113,baselines,baselines,has,baseline models,baselines has baseline models,0.5690722465515137
translation,257,134,baselines,novel multi-task learning method,has,nlg -lm,novel multi-task learning method has nlg -lm,0.5607128739356995
translation,257,150,experimental-setup,machine,has,intel xeon cpu e5,machine has intel xeon cpu e5,0.47364991903305054
translation,257,150,experimental-setup,experimental setup,has,machine,experimental setup has machine,0.5680505633354187
translation,257,33,experiments,nlg task,employ,sequence - to-sequence framework,nlg task employ sequence - to-sequence framework,0.5700567960739136
translation,257,111,experiments,restaurant dataset,delexicalize,all slots,restaurant dataset delexicalize all slots,0.7002962827682495
translation,257,111,experiments,all slots,except,kidsallowed and request,all slots except kidsallowed and request,0.7307963371276855
translation,257,125,experiments,nlg - lm,greatly improves,best result,nlg - lm greatly improves best result,0.7629448175430298
translation,257,125,experiments,best result,by,"7.6 % , 6.1 % , 4.1 % , and 1.6 %","best result by 7.6 % , 6.1 % , 4.1 % , and 1.6 %",0.5237668752670288
translation,257,125,experiments,"tv , laptop , hotel and restaurant datasets",has,nlg - lm,"tv , laptop , hotel and restaurant datasets has nlg - lm",0.566044807434082
translation,257,115,hyperparameters,"adamax ( kingma and ba , 2014 )",as,optimizer,"adamax ( kingma and ba , 2014 ) as optimizer",0.5137593150138855
translation,257,116,hyperparameters,teacher forcing,has,decoder,teacher forcing has decoder,0.601974606513977
translation,257,118,hyperparameters,inference,uses,beam search,inference uses beam search,0.6050080060958862
translation,257,118,hyperparameters,hyperparameters,has,inference,hyperparameters has inference,0.5389398336410522
translation,257,119,hyperparameters,hyperparameters,use,multi-task coefficient ? = 0.5,hyperparameters use multi-task coefficient ? = 0.5,0.5975626707077026
translation,257,135,model,language model task,into,response generation process,language model task into response generation process,0.539519727230072
translation,257,135,model,response generation process,as,unconditioned complementary process,response generation process as unconditioned complementary process,0.5284510254859924
translation,257,135,model,unconditioned complementary process,to boost,naturalness,unconditioned complementary process to boost naturalness,0.6827078461647034
translation,257,135,model,naturalness,of,generated utterances,naturalness of generated utterances,0.5869494676589966
translation,257,135,model,model,incorporates,language model task,model incorporates language model task,0.6707364916801453
translation,257,136,model,both tasks,into,sequence - to-sequence structure,both tasks into sequence - to-sequence structure,0.588238537311554
translation,257,136,model,sequence - to-sequence structure,under,multi-task learning scheme,sequence - to-sequence structure under multi-task learning scheme,0.616721510887146
translation,257,136,model,model,fit,both tasks,model fit both tasks,0.6956748366355896
translation,257,8,results,naturalness,in,generated responses,naturalness in generated responses,0.5185166597366333
translation,257,8,results,generated responses,via,unconditioned language model,generated responses via unconditioned language model,0.648492693901062
translation,257,123,results,baseline models,in,all 5 datasets,baseline models in all 5 datasets,0.46600407361984253
translation,257,123,results,our model,has,nlg - lm,our model has nlg - lm,0.6308997869491577
translation,257,123,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,257,123,results,nlg - lm,has,outperforms,nlg - lm has outperforms,0.6305233240127563
translation,257,123,results,outperforms,has,baseline models,outperforms has baseline models,0.5940393209457397
translation,257,123,results,results,has,our model,results has our model,0.5871725678443909
translation,257,124,results,e2e-nlg dataset,achieves,2.2 % higher bleu score,e2e-nlg dataset achieves 2.2 % higher bleu score,0.6364763379096985
translation,257,124,results,e2e-nlg dataset,achieves,0.013 higher nist score,e2e-nlg dataset achieves 0.013 higher nist score,0.6541586518287659
translation,257,124,results,0.013 higher nist score,than,slug,0.013 higher nist score than slug,0.5943910479545593
translation,257,124,results,results,In,e2e-nlg dataset,results In e2e-nlg dataset,0.5147053599357605
translation,257,127,results,language modeling,improve,result,language modeling improve result,0.6497109532356262
translation,257,127,results,result,by,0.8 % to 6.1 %,result by 0.8 % to 6.1 %,0.5761093497276306
translation,257,127,results,result,by,2.4 %,result by 2.4 %,0.5657752156257629
translation,257,127,results,results,has,language modeling,results has language modeling,0.5308453440666199
translation,258,16,model,model,maps,what people say,model maps what people say,0.7037789225578308
translation,258,16,model,what people say,in,win-lose game,what people say in win-lose game,0.5519645810127258
translation,258,16,model,what people say,into,prediction of exactly which players,what people say into prediction of exactly which players,0.5526041984558105
translation,258,16,model,prediction of exactly which players,trade with,each other,prediction of exactly which players trade with each other,0.8051410913467407
translation,258,16,model,model,design,model,model design model,0.6813608407974243
translation,258,16,model,model,maps,what people say,model maps what people say,0.7037789225578308
translation,258,17,model,statistics and logic,use,corpus,statistics and logic use corpus,0.6403170824050903
translation,258,17,model,statistics and logic,develop,symbolic algorithm,statistics and logic develop symbolic algorithm,0.6243226528167725
translation,258,17,model,corpus,of,negotiation dialogues,corpus of negotiation dialogues,0.5718068480491638
translation,258,17,model,negotiation dialogues,to learn,classifiers,negotiation dialogues to learn classifiers,0.6591259837150574
translation,258,17,model,classifiers,map,each utterance,classifiers map each utterance,0.7603387236595154
translation,258,17,model,each utterance,to,other acts,each utterance to other acts,0.5762973427772522
translation,258,17,model,other acts,pertinent to,bargaining,other acts pertinent to bargaining,0.6824079751968384
translation,258,17,model,symbolic algorithm,from,classifiers,symbolic algorithm from classifiers,0.6243234872817993
translation,258,17,model,symbolic algorithm,dynamically constructs,model,symbolic algorithm dynamically constructs model,0.7783385515213013
translation,258,17,model,model,of,each player 's preferences,model of each player 's preferences,0.5360971093177795
translation,258,17,model,each player 's preferences,as,conversation,each player 's preferences as conversation,0.5901952981948853
translation,258,17,model,each utterance,has,to its speech act,each utterance has to its speech act,0.602899968624115
translation,258,17,model,conversation,has,proceeds,conversation has proceeds,0.6091391444206238
translation,258,17,model,model,use,statistics and logic,model use statistics and logic,0.6859257221221924
translation,258,17,model,model,develop,symbolic algorithm,model develop symbolic algorithm,0.6558620929718018
translation,258,85,results,our model,has,significantly outperforms,our model has significantly outperforms,0.6134821176528931
translation,258,85,results,significantly outperforms,has,frequency - based baseline,significantly outperforms has frequency - based baseline,0.5887643694877625
translation,258,85,results,frequency - based baseline,has,maf = 14.5 ; accuracy = 56.8 ),frequency - based baseline has maf = 14.5 ; accuracy = 56.8 ),0.5787293910980225
translation,258,85,results,results,has,our model,results has our model,0.5871725678443909
translation,258,86,results,least good results,for,two least frequent classes,least good results for two least frequent classes,0.6164838075637817
translation,258,86,results,two least frequent classes,in,our data,two least frequent classes in our data,0.5017338991165161
translation,258,86,results,results,has,least good results,results has least good results,0.5897995829582214
translation,258,179,results,mcnemar 's test,shows,our model,mcnemar 's test shows our model,0.6691581010818481
translation,258,179,results,our model,has,significantly outperforms,our model has significantly outperforms,0.6134821176528931
translation,258,179,results,significantly outperforms,has,all the baselines ( p < 0.05 ),significantly outperforms has all the baselines ( p < 0.05 ),0.5821607112884521
translation,258,179,results,results,has,mcnemar 's test,results has mcnemar 's test,0.5049999952316284
translation,258,193,results,cp - nets,has,significantly outperforms,cp - nets has significantly outperforms,0.6287564039230347
translation,258,193,results,significantly outperforms,has,all baselines,significantly outperforms has all baselines,0.5895546674728394
translation,258,195,results,decreases,due to,classifiers ' errors,decreases due to classifiers ' errors,0.657581627368927
translation,258,195,results,classifiers ' errors,mainly on,"type of resources ( givable , etc. )","classifiers ' errors mainly on type of resources ( givable , etc. )",0.598796010017395
translation,258,195,results,performance,has,decreases,performance has decreases,0.599746823310852
translation,258,195,results,results,has,performance,results has performance,0.5972660779953003
translation,258,196,results,significantly outperforms,with,accuracy,significantly outperforms with accuracy,0.7071527242660522
translation,258,196,results,all the baselines,with,accuracy,all the baselines with accuracy,0.6229162812232971
translation,258,196,results,accuracy,of,73.4 %,accuracy of 73.4 %,0.5547864437103271
translation,258,196,results,73.4 %,when,baselines,73.4 % when baselines,0.625289261341095
translation,258,196,results,baselines,obtain,values,baselines obtain values,0.541581928730011
translation,258,196,results,values,between,60.9 % and 68.4 %,values between 60.9 % and 68.4 %,0.6074693202972412
translation,258,196,results,our method,has,significantly outperforms,our method has significantly outperforms,0.6120707988739014
translation,258,196,results,significantly outperforms,has,all the baselines,significantly outperforms has all the baselines,0.5936449766159058
translation,258,196,results,results,has,our method,results has our method,0.5589964985847473
translation,259,124,ablation-analysis,significant impact,on,agent 's performance,significant impact on agent 's performance,0.5520071387290955
translation,259,124,ablation-analysis,quality of the world model,has,significant impact,quality of the world model has significant impact,0.5572603344917297
translation,259,124,ablation-analysis,ablation analysis,shows,quality of the world model,ablation analysis shows quality of the world model,0.6630159020423889
translation,259,146,ablation-analysis,larger k,leads to,more aggressive planning,larger k leads to more aggressive planning,0.6760960221290588
translation,259,146,ablation-analysis,larger k,leads to,better results,larger k leads to better results,0.6761825680732727
translation,259,97,baselines,variant of imitation learning,called,reply buffer spiking ( rbs ),variant of imitation learning called reply buffer spiking ( rbs ),0.6879333853721619
translation,259,90,hyperparameters,greedy,applied for,exploration,greedy applied for exploration,0.719866931438446
translation,259,90,hyperparameters,hyperparameters,has,greedy,hyperparameters has greedy,0.5732554793357849
translation,259,91,hyperparameters,hyperparameters,set,discount factor,hyperparameters set discount factor,0.5682007074356079
translation,259,92,hyperparameters,buffer sizes,of,d u and d s,buffer sizes of d u and d s,0.6472083926200867
translation,259,92,hyperparameters,d u and d s,set to,5000,d u and d s set to 5000,0.7742310166358948
translation,259,92,hyperparameters,hyperparameters,has,buffer sizes,hyperparameters has buffer sizes,0.5310488939285278
translation,259,96,hyperparameters,maximum length,of,simulated dialogue,maximum length of simulated dialogue,0.6102182865142822
translation,259,96,hyperparameters,simulated dialogue,is,40 ( l = 40 ),simulated dialogue is 40 ( l = 40 ),0.6066402792930603
translation,259,96,hyperparameters,planning,has,maximum length,planning has maximum length,0.6059095859527588
translation,259,96,hyperparameters,hyperparameters,In,planning,hyperparameters In planning,0.5124067664146423
translation,259,7,model,deep dyna - q,is,first deep rl framework,deep dyna - q is first deep rl framework,0.5310730338096619
translation,259,7,model,first deep rl framework,integrates,planning for task - completion dialogue policy learning,first deep rl framework integrates planning for task - completion dialogue policy learning,0.5940728187561035
translation,259,7,model,model,present,deep dyna - q,model present deep dyna - q,0.7146701812744141
translation,259,8,model,dialogue agent,generate,simulated experience,dialogue agent generate simulated experience,0.6275084614753723
translation,259,8,model,model of the environment,referred to as,world model,model of the environment referred to as world model,0.6010721325874329
translation,259,8,model,model of the environment,to mimic,real user response,model of the environment to mimic real user response,0.6511561870574951
translation,259,8,model,dialogue agent,has,model of the environment,dialogue agent has model of the environment,0.5624651908874512
translation,259,8,model,model,incorporate into,dialogue agent,model incorporate into dialogue agent,0.6499385237693787
translation,259,9,model,world model,constantly updated with,real user experience,world model constantly updated with real user experience,0.6505131125450134
translation,259,9,model,real user experience,to approach,real user behavior,real user experience to approach real user behavior,0.7205931544303894
translation,259,9,model,dialogue agent,optimized using,real experience and simulated experience,dialogue agent optimized using real experience and simulated experience,0.7072771191596985
translation,259,9,model,dialogue policy learning,has,world model,dialogue policy learning has world model,0.5608083009719849
translation,259,9,model,dialogue policy learning,has,dialogue agent,dialogue policy learning has dialogue agent,0.5717508792877197
translation,259,9,model,model,During,dialogue policy learning,model During dialogue policy learning,0.6574655175209045
translation,259,26,model,new strategy,of learning,dialogue policy,new strategy of learning dialogue policy,0.6347892880439758
translation,259,26,model,dialogue policy,by interacting with,real users,dialogue policy by interacting with real users,0.7143290042877197
translation,259,26,model,model,propose,new strategy,model propose new strategy,0.7295149564743042
translation,259,28,model,dyna - q framework,where,planning,dyna - q framework where planning,0.646556556224823
translation,259,28,model,planning,integrated into,policy learning,planning integrated into policy learning,0.6878383159637451
translation,259,28,model,policy learning,for,task - completion dialogue,policy learning for task - completion dialogue,0.6161307692527771
translation,259,28,model,model,based on,dyna - q framework,model based on dyna - q framework,0.7050812244415283
translation,259,29,model,model of the environment,into,dialogue agent,model of the environment into dialogue agent,0.531879186630249
translation,259,29,model,dialogue agent,generates,simulated user experience,dialogue agent generates simulated user experience,0.5670020580291748
translation,259,29,model,model,incorporate,model of the environment,model incorporate model of the environment,0.663428544998169
translation,259,38,model,deep dyna - q ( ddq ),by combining,dyna - q with deep learning approaches,deep dyna - q ( ddq ) by combining dyna - q with deep learning approaches,0.7359878420829773
translation,259,38,model,dyna - q with deep learning approaches,to representing,state-action space,dyna - q with deep learning approaches to representing state-action space,0.6612480878829956
translation,259,38,model,state-action space,by,neural networks ( nn ),state-action space by neural networks ( nn ),0.5922424793243408
translation,259,38,model,model,propose,deep dyna - q ( ddq ),model propose deep dyna - q ( ddq ),0.7017331123352051
translation,259,93,model,target value function,updated at,end,target value function updated at end,0.5455225110054016
translation,259,93,model,end,of,each epoch,end of each epoch,0.5975684523582458
translation,259,93,model,model,has,target value function,model has target value function,0.5398483872413635
translation,259,94,model,q ( . ) and m ( . ),refined,one-step ( z = 1 ) 16 - tupleminibatch update,q ( . ) and m ( . ) refined one-step ( z = 1 ) 16 - tupleminibatch update,0.712165117263794
translation,259,94,model,epoch,has,q ( . ) and m ( . ),epoch has q ( . ) and m ( . ),0.6018974781036377
translation,259,27,results,dialogue agent,learns,much more efficient way,dialogue agent learns much more efficient way,0.6143369674682617
translation,259,30,results,dialogue policy learning,via,rl,dialogue policy learning via rl,0.634975254535675
translation,259,30,results,real user experience,plays,two pivotal roles,real user experience plays two pivotal roles,0.6844797730445862
translation,259,30,results,improve,make it behave more like,real users,improve make it behave more like real users,0.7725814580917358
translation,259,30,results,world model,make it behave more like,real users,world model make it behave more like real users,0.6537131071090698
translation,259,30,results,world model,via,supervised learning,world model via supervised learning,0.6469371318817139
translation,259,30,results,dialogue policy,via,rl,dialogue policy via rl,0.7043012976646423
translation,259,30,results,dialogue policy learning,has,real user experience,dialogue policy learning has real user experience,0.5403119325637817
translation,259,30,results,improve,has,world model,improve has world model,0.6121993064880371
translation,259,30,results,directly improve,has,dialogue policy,directly improve has dialogue policy,0.6029126644134521
translation,259,30,results,results,During,dialogue policy learning,results During dialogue policy learning,0.6120853424072266
translation,259,113,results,dqn,with,statistically significant margin,dqn with statistically significant margin,0.5544239282608032
translation,259,113,results,ddq agents,has,consistently outperform,ddq agents has consistently outperform,0.6198731660842896
translation,259,113,results,consistently outperform,has,dqn,consistently outperform has dqn,0.5843106508255005
translation,259,113,results,results,show,ddq agents,results show ddq agents,0.610735297203064
translation,259,145,results,ddq agent,trained with,100 real dialogues,ddq agent trained with 100 real dialogues,0.7332524061203003
translation,259,145,results,ddq agent,can complete,50 %,ddq agent can complete 50 %,0.7724665999412537
translation,259,145,results,100 real dialogues,is,more robust,100 real dialogues is more robust,0.5611206293106079
translation,259,145,results,100 real dialogues,much,more robust,100 real dialogues much more robust,0.691861093044281
translation,259,145,results,50 %,of,user tasks,50 % of user tasks,0.5973041653633118
translation,259,145,results,planning,has,ddq agent,planning has ddq agent,0.612809956073761
translation,259,145,results,user tasks,has,successfully,user tasks has successfully,0.5845436453819275
translation,259,145,results,results,with,planning,results with planning,0.6122254133224487
translation,259,147,results,world model,with,human con-versational data,world model with human con-versational data,0.5872086882591248
translation,259,147,results,human con-versational data,improves,learning efficiency,human con-versational data improves learning efficiency,0.6823221445083618
translation,259,147,results,human con-versational data,improves,agent 's performance,human con-versational data improves agent 's performance,0.649150013923645
translation,259,147,results,ddq,has,"5 ) vs. ddq ( 5 , rand- init ? m )","ddq has 5 ) vs. ddq ( 5 , rand- init ? m )",0.6280111074447632
translation,259,147,results,results,Pre-training,world model,results Pre-training world model,0.7201591730117798
translation,260,151,ablation-analysis,team - level entrainment,on,jitter,team - level entrainment on jitter,0.5491065382957458
translation,260,151,ablation-analysis,significant,for,only the second game,significant for only the second game,0.6471253633499146
translation,260,151,ablation-analysis,ablation analysis,has,team - level entrainment,ablation analysis has team - level entrainment,0.5110886096954346
translation,260,7,results,team entrainment,developing and evaluating,teamlevel acoustic -prosodic entrainment measures,team entrainment developing and evaluating teamlevel acoustic -prosodic entrainment measures,0.5840693116188049
translation,260,7,results,team entrainment,investigating,relationships,team entrainment investigating relationships,0.7024949789047241
translation,260,7,results,teamlevel acoustic -prosodic entrainment measures,extend,existing dyad measures,teamlevel acoustic -prosodic entrainment measures extend existing dyad measures,0.6334816217422485
translation,261,155,ablation-analysis,simulation,helps to,improve,simulation helps to improve,0.7237119078636169
translation,261,155,ablation-analysis,performance,in,most cases,performance in most cases,0.5704897046089172
translation,261,155,ablation-analysis,most cases,of,p > 0 and < 1,most cases of p > 0 and < 1,0.6004602313041687
translation,261,155,ablation-analysis,improve,has,performance,improve has performance,0.5578044652938843
translation,261,155,ablation-analysis,ablation analysis,adding,simulation,ablation analysis adding simulation,0.6785648465156555
translation,261,162,ablation-analysis,audio feature,adding,auto-encoder component,audio feature adding auto-encoder component,0.6655558347702026
translation,261,162,ablation-analysis,auto-encoder component,increases,bleu4 and cider measures,auto-encoder component increases bleu4 and cider measures,0.6617971062660217
translation,261,162,ablation-analysis,ablation analysis,with,audio feature,ablation analysis with audio feature,0.6523925065994263
translation,261,133,experimental-setup,each model,up to,17 epochs,each model up to 17 epochs,0.6387767791748047
translation,261,133,experimental-setup,experimental setup,trained,each model,experimental setup trained each model,0.7156051397323608
translation,261,134,experimental-setup,experimental setup,used,adam optimizer,experimental setup used adam optimizer,0.5961911082267761
translation,261,135,experimental-setup,varied,over,course of training,varied over course of training,0.6961992383003235
translation,261,135,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,261,136,experimental-setup,warmup steps,as,9660,warmup steps as 9660,0.5609540343284607
translation,261,136,experimental-setup,experimental setup,used,warmup steps,experimental setup used warmup steps,0.5970718264579773
translation,261,137,experimental-setup,"dropout ( srivastava et al. , 2014 )",of,0.1,"dropout ( srivastava et al. , 2014 ) of 0.1",0.5779061317443848
translation,261,137,experimental-setup,0.1,at,all sub-layers and embeddings,0.1 at all sub-layers and embeddings,0.49110639095306396
translation,261,137,experimental-setup,experimental setup,employed,"dropout ( srivastava et al. , 2014 )","experimental setup employed dropout ( srivastava et al. , 2014 )",0.6210466027259827
translation,261,138,experimental-setup,label smoothing,applied during,training,label smoothing applied during training,0.6929877400398254
translation,261,138,experimental-setup,experimental setup,has,label smoothing,experimental setup has label smoothing,0.4983872175216675
translation,261,140,experimental-setup,beam search,with,beam size,beam search with beam size,0.6320963501930237
translation,261,140,experimental-setup,beam size,an,a length penalty 1.0,beam size an a length penalty 1.0,0.4679281711578369
translation,261,140,experimental-setup,5,an,a length penalty 1.0,5 an a length penalty 1.0,0.4719146192073822
translation,261,140,experimental-setup,beam size,has,5,beam size has 5,0.6528496742248535
translation,261,140,experimental-setup,beam size,has,a length penalty 1.0,beam size has a length penalty 1.0,0.5171213746070862
translation,261,140,experimental-setup,5,has,a length penalty 1.0,5 has a length penalty 1.0,0.4973413348197937
translation,261,140,experimental-setup,experimental setup,used,beam search,experimental setup used beam search,0.5615512132644653
translation,261,141,experimental-setup,maximum output length,during,inference,maximum output length during inference,0.6685625314712524
translation,261,141,experimental-setup,inference,is,30 tokens,inference is 30 tokens,0.5746088027954102
translation,261,141,experimental-setup,experimental setup,has,maximum output length,experimental setup has maximum output length,0.48396629095077515
translation,261,142,experimental-setup,experimental setup,implemented using,pytorch,experimental setup implemented using pytorch,0.6359934210777283
translation,261,6,model,multimodal transformer networks ( mtn ),to encode,videos,multimodal transformer networks ( mtn ) to encode videos,0.719814658164978
translation,261,6,model,multimodal transformer networks ( mtn ),incorporate,information,multimodal transformer networks ( mtn ) incorporate information,0.6907219290733337
translation,261,6,model,information,from,different modalities,information from different modalities,0.5465307235717773
translation,261,6,model,model,propose,multimodal transformer networks ( mtn ),model propose multimodal transformer networks ( mtn ),0.6788105964660645
translation,261,7,model,queryaware attention,through,auto-encoder,queryaware attention through auto-encoder,0.6203964352607727
translation,261,7,model,auto-encoder,to extract,query - aware features,auto-encoder to extract query - aware features,0.7177960276603699
translation,261,7,model,query - aware features,from,non-text modalities,query - aware features from non-text modalities,0.5384469628334045
translation,261,7,model,model,propose,queryaware attention,model propose queryaware attention,0.6662667989730835
translation,261,23,model,multimodal transformer networks ( mtn ),model,complex sequential information,multimodal transformer networks ( mtn ) model complex sequential information,0.6984837651252747
translation,261,23,model,multimodal transformer networks ( mtn ),incorporate,information,multimodal transformer networks ( mtn ) incorporate information,0.6907219290733337
translation,261,23,model,complex sequential information,from,video frames,complex sequential information from video frames,0.5757118463516235
translation,261,23,model,information,from,different modalities,information from different modalities,0.5465307235717773
translation,261,23,model,model,propose,multimodal transformer networks ( mtn ),model propose multimodal transformer networks ( mtn ),0.6788105964660645
translation,261,24,model,mtns,allow for,complex reasoning,mtns allow for complex reasoning,0.647010862827301
translation,261,24,model,complex reasoning,over,multimodal data,complex reasoning over multimodal data,0.6159655451774597
translation,261,24,model,complex reasoning,by jointly attending to,information,complex reasoning by jointly attending to information,0.6559849381446838
translation,261,24,model,information,in,different representation subspaces,information in different representation subspaces,0.48715728521347046
translation,261,24,model,information,from,different modalities,information from different modalities,0.5465307235717773
translation,261,24,model,information,from,different modalities,information from different modalities,0.5465307235717773
translation,261,24,model,model,has,mtns,model has mtns,0.5925377011299133
translation,261,25,model,novel neural architectures,for,vgds,novel neural architectures for vgds,0.6545699238777161
translation,261,25,model,model,propose,novel neural architectures,model propose novel neural architectures,0.7236433625221252
translation,261,27,model,multihead attention,applied across,"several modalities ( visual , audio , captions )","multihead attention applied across several modalities ( visual , audio , captions )",0.6845839023590088
translation,261,27,model,model,has,multihead attention,model has multihead attention,0.5442439913749695
translation,261,28,model,memory network,to allow,models,memory network to allow models,0.687606930732727
translation,261,28,model,memory network,to allow,models,memory network to allow models,0.687606930732727
translation,261,28,model,memory network,propose,autoencoder component,memory network propose autoencoder component,0.6307442784309387
translation,261,28,model,models,to comprehensively reason over,video,models to comprehensively reason over video,0.7100029587745667
translation,261,28,model,video,to answer,human queries,video to answer human queries,0.6245819926261902
translation,261,28,model,autoencoder component,designed as,query - aware attention layer,autoencoder component designed as query - aware attention layer,0.46722766757011414
translation,261,28,model,autoencoder component,to further improve,reasoning capability,autoencoder component to further improve reasoning capability,0.6636704802513123
translation,261,28,model,reasoning capability,of,models,reasoning capability of models,0.5785065293312073
translation,261,28,model,models,on,non-text features,models on non-text features,0.479012131690979
translation,261,28,model,non-text features,of,input videos,non-text features of input videos,0.5143654942512512
translation,261,28,model,model,works like,memory network,model works like memory network,0.6761524677276611
translation,261,28,model,model,propose,autoencoder component,model propose autoencoder component,0.6617670059204102
translation,261,29,model,training approach,to improve,generated responses,training approach to improve generated responses,0.707580029964447
translation,261,29,model,generated responses,by simulating,token - level decoding,generated responses by simulating token - level decoding,0.7078649997711182
translation,261,29,model,token - level decoding,during,training,token - level decoding during training,0.6314973831176758
translation,261,29,model,model,employ,training approach,model employ training approach,0.5905766487121582
translation,261,164,model,auto-encoder structure,as,encoder,auto-encoder structure as encoder,0.5547672510147095
translation,261,164,model,model,consider,auto-encoder structure,model consider auto-encoder structure,0.7069819569587708
translation,261,164,model,model,using,auto-encoder structure,model using auto-encoder structure,0.6709004640579224
translation,261,147,results,outperform,in,all metrics,outperform in all metrics,0.5308061838150024
translation,261,147,results,baseline,in,all metrics,baseline in all metrics,0.46490955352783203
translation,261,147,results,base - and large - mtn models,has,outperform,base - and large - mtn models has outperform,0.5785881280899048
translation,261,147,results,outperform,has,baseline,outperform has baseline,0.6223028898239136
translation,261,147,results,results,both,base - and large - mtn models,results both base - and large - mtn models,0.6423792839050293
translation,261,148,results,outperforms,in,challenge,outperforms in challenge,0.6022800207138062
translation,261,148,results,best previously reported models,in,challenge,best previously reported models in challenge,0.5387267470359802
translation,261,148,results,best previously reported models,across,all the metrics,best previously reported models across all the metrics,0.6712879538536072
translation,261,148,results,large model,has,outperforms,large model has outperforms,0.6229472160339355
translation,261,148,results,outperforms,has,best previously reported models,outperforms has best previously reported models,0.601542055606842
translation,261,148,results,results,has,large model,results has large model,0.5789811015129089
translation,261,149,results,our base model,with,smaller parameters,our base model with smaller parameters,0.6498978734016418
translation,261,149,results,most of the previous results,except for,entry1,most of the previous results except for entry1,0.7142631411552429
translation,261,149,results,smaller parameters,has,outperforms,smaller parameters has outperforms,0.6478822827339172
translation,261,149,results,outperforms,has,most of the previous results,outperforms has most of the previous results,0.6018695831298828
translation,261,149,results,results,has,our base model,results has our base model,0.5756781101226807
translation,261,159,results,text -only input,using,encoder layers,text -only input using encoder layers,0.6226820945739746
translation,261,159,results,encoder layers,with,selfattention blocks,encoder layers with selfattention blocks,0.6517585515975952
translation,261,159,results,selfattention blocks,does not perform,well,selfattention blocks does not perform well,0.7044171690940857
translation,261,159,results,results,With,text -only input,results With text -only input,0.6059015393257141
translation,261,161,results,video caption,from,input,video caption from input,0.5333129167556763
translation,261,161,results,video caption,use,visual or audio video features,video caption use visual or audio video features,0.6304700374603271
translation,261,161,results,visual or audio video features,observe,proposed auto-encoder,visual or audio video features observe proposed auto-encoder,0.5790560841560364
translation,261,161,results,proposed auto-encoder,with,query - aware attention,proposed auto-encoder with query - aware attention,0.6170771718025208
translation,261,161,results,results,remove,video caption,results remove video caption,0.5953298211097717
translation,261,163,results,proposed auto-encoder,improves,all metrics,proposed auto-encoder improves all metrics,0.6707739233970642
translation,261,163,results,all metrics,from,decoderonly model,all metrics from decoderonly model,0.5507618188858032
translation,261,163,results,both caption and video features,has,proposed auto-encoder,both caption and video features has proposed auto-encoder,0.5601124167442322
translation,261,163,results,results,When using,both caption and video features,results When using both caption and video features,0.6179646253585815
translation,261,165,results,auto-encoder structure,superior to,stacked encoder layers,auto-encoder structure superior to stacked encoder layers,0.6899698376655579
translation,261,165,results,results,show,auto-encoder structure,results show auto-encoder structure,0.6373436450958252
translation,261,166,results,our architecture,is,better,our architecture is better,0.6015111804008484
translation,261,166,results,better,in terms of,computation speed,better in terms of computation speed,0.6794195175170898
translation,261,166,results,computation speed,as,decoder and auto-encoder,computation speed as decoder and auto-encoder,0.5421079993247986
translation,261,166,results,decoder and auto-encoder,processed in,parallel,decoder and auto-encoder processed in parallel,0.6955584287643433
translation,261,166,results,results,has,our architecture,results has our architecture,0.5819753408432007
translation,261,181,results,our proposed mtn,able to,generalize,our proposed mtn able to generalize,0.6930851340293884
translation,261,181,results,generalize,to,visually grounded dialogue setting,generalize to visually grounded dialogue setting,0.5789523720741272
translation,261,181,results,results,shows,our proposed mtn,results shows our proposed mtn,0.6929185390472412
translation,261,182,results,outperforms,in,ndcg,outperforms in ndcg,0.5492883324623108
translation,261,182,results,outperforms,without,task -specific fine-tuning,outperforms without task -specific fine-tuning,0.6589306592941284
translation,261,182,results,other retrieval - based approaches,in,ndcg,other retrieval - based approaches in ndcg,0.4910331070423126
translation,261,182,results,other retrieval - based approaches,without,task -specific fine-tuning,other retrieval - based approaches without task -specific fine-tuning,0.6790391206741333
translation,261,182,results,our generative model,has,outperforms,our generative model has outperforms,0.6282193660736084
translation,261,182,results,outperforms,has,other retrieval - based approaches,outperforms has other retrieval - based approaches,0.5882551670074463
translation,261,182,results,results,interesting that,our generative model,results interesting that our generative model,0.656939685344696
translation,261,188,results,our generated responses,are,more accurate,our generated responses are more accurate,0.600425124168396
translation,261,188,results,more accurate,than,baseline,more accurate than baseline,0.5747393369674683
translation,261,188,results,more accurate,to answer,human queries,more accurate to answer human queries,0.6528246998786926
translation,261,188,results,results,has,our generated responses,results has our generated responses,0.5607577562332153
translation,261,194,results,mtn,perform,better,mtn perform better,0.6412938237190247
translation,261,194,results,better,at,shorter dialogue lengths,better at shorter dialogue lengths,0.5586001873016357
translation,261,194,results,shorter dialogue lengths,e.g.,1- turn,shorter dialogue lengths e.g. 1- turn,0.6423226594924927
translation,261,194,results,shorter dialogue lengths,e.g.,2 - turn and 3 - turn,shorter dialogue lengths e.g. 2 - turn and 3 - turn,0.6526817679405212
translation,261,194,results,results,shows,mtn,results shows mtn,0.7020562887191772
translation,262,45,baselines,temporal memory linkage,records tracks,consecutively written memory locations,temporal memory linkage records tracks consecutively written memory locations,0.7810736298561096
translation,262,45,baselines,consecutively written memory locations,to be able to,read,consecutively written memory locations to be able to read,0.5812966227531433
translation,262,45,baselines,sequences,order of,locations,sequences order of locations,0.7215333580970764
translation,262,45,baselines,read,has,sequences,read has sequences,0.5764805674552917
translation,262,45,baselines,baselines,has,temporal memory linkage,baselines has temporal memory linkage,0.5567699074745178
translation,262,94,hyperparameters,models,using,one layer lstm,models using one layer lstm,0.6265586018562317
translation,262,94,hyperparameters,one layer lstm,with,hidden layer,one layer lstm with hidden layer,0.6474570035934448
translation,262,94,hyperparameters,hidden layer,of size,256,hidden layer of size 256,0.7583318948745728
translation,262,94,hyperparameters,learning rate,of,1 ? 10 ?4,learning rate of 1 ? 10 ?4,0.6436555981636047
translation,262,94,hyperparameters,context memory dimensions,of,256 ? 64,context memory dimensions of 256 ? 64,0.6041120290756226
translation,262,94,hyperparameters,context memory dimensions,of,512 ? 128,context memory dimensions of 512 ? 128,0.6100614666938782
translation,262,94,hyperparameters,context memory dimensions,of,512 ? 128,context memory dimensions of 512 ? 128,0.6100614666938782
translation,262,94,hyperparameters,knowledge memory dimensions,of,512 ? 128,knowledge memory dimensions of 512 ? 128,0.6008455157279968
translation,262,94,hyperparameters,hyperparameters,trained,models,hyperparameters trained models,0.6957098245620728
translation,262,95,hyperparameters,rmsprop optimizer,with,momentum,rmsprop optimizer with momentum,0.5936768054962158
translation,262,95,hyperparameters,momentum,of,0.9.,momentum of 0.9.,0.6370923519134521
translation,262,95,hyperparameters,rsdnc models,have,dropout probability,rsdnc models have dropout probability,0.5148853063583374
translation,262,95,hyperparameters,dropout probability,of,10 %,dropout probability of 10 %,0.5607074499130249
translation,262,95,hyperparameters,0.9.,has,rsdnc models,0.9. has rsdnc models,0.5469503402709961
translation,262,95,hyperparameters,hyperparameters,used,rmsprop optimizer,hyperparameters used rmsprop optimizer,0.5917237997055054
translation,262,96,hyperparameters,"transe ( bordes et al. , 2013 )",for,kb 's word embeddings,"transe ( bordes et al. , 2013 ) for kb 's word embeddings",0.548456609249115
translation,262,96,hyperparameters,"transe ( bordes et al. , 2013 )",for,words,"transe ( bordes et al. , 2013 ) for words",0.6681182384490967
translation,262,96,hyperparameters,"glove embeddings ( pennington et al. , 2014 )",for,words,"glove embeddings ( pennington et al. , 2014 ) for words",0.5836856961250305
translation,262,96,hyperparameters,words,do not appear in,kb,words do not appear in kb,0.6793572306632996
translation,262,96,hyperparameters,words,appear in,dialogue,words appear in dialogue,0.6602587103843689
translation,262,96,hyperparameters,hyperparameters,used,"transe ( bordes et al. , 2013 )","hyperparameters used transe ( bordes et al. , 2013 )",0.5536080598831177
translation,262,97,hyperparameters,dimension,of,each word embedding vector,dimension of each word embedding vector,0.5553562045097351
translation,262,97,hyperparameters,each word embedding vector,is,200,each word embedding vector is 200,0.5599393248558044
translation,262,97,hyperparameters,hyperparameters,has,dimension,hyperparameters has dimension,0.514893651008606
translation,262,7,model,architecture,for,knowledge,architecture for knowledge,0.5902519822120667
translation,262,7,model,knowledge,into,dnc models,knowledge into dnc models,0.6213579773902893
translation,262,7,model,dnc models,i.e.,dnc,dnc models i.e. dnc,0.6667079329490662
translation,262,7,model,dnc models,i.e.,rsdnc,dnc models i.e. rsdnc,0.6917644143104553
translation,262,7,model,dnc models,i.e.,dnc - dms,dnc models i.e. dnc - dms,0.6493881344795227
translation,262,7,model,ability,to generate,correct responses,ability to generate correct responses,0.5986518859863281
translation,262,7,model,correct responses,using,contextual information,correct responses using contextual information,0.6373080611228943
translation,262,7,model,correct responses,using,structured knowledge,correct responses using structured knowledge,0.6477422118186951
translation,262,7,model,model,incorporate,architecture,model incorporate architecture,0.6944834589958191
translation,262,29,model,single-memory unit dnc models,to,multiple -memory architecture,single-memory unit dnc models to multiple -memory architecture,0.5469599962234497
translation,262,29,model,multiple -memory architecture,leverages,contextual and structured knowledge information,multiple -memory architecture leverages contextual and structured knowledge information,0.700057864189148
translation,262,29,model,model,extend,single-memory unit dnc models,model extend single-memory unit dnc models,0.6879426836967468
translation,262,30,model,extra memory unit,encodes,knowledge,extra memory unit encodes knowledge,0.7251721620559692
translation,262,30,model,knowledge,from,knowledge bases,knowledge from knowledge bases,0.5634422302246094
translation,262,30,model,model,add,extra memory unit,model add extra memory unit,0.635125994682312
translation,262,31,model,complex data structures,over,long time -scale,complex data structures over long time -scale,0.6280578970909119
translation,262,59,model,three models,by adding,extra memory architecture,three models by adding extra memory architecture,0.7089989185333252
translation,262,59,model,extra memory architecture,to store,structured knowledge,extra memory architecture to store structured knowledge,0.7178205251693726
translation,262,59,model,model,expand,three models,model expand three models,0.6397833824157715
translation,262,59,model,model,expand,dnc,model expand dnc,0.6912978291511536
translation,262,60,model,proposed model,consists of,control unit,proposed model consists of control unit,0.6767789721488953
translation,262,60,model,proposed model,consists of,two memory units,proposed model consists of two memory units,0.6789146065711975
translation,262,60,model,model,consists of,control unit,model consists of control unit,0.7114687561988831
translation,262,60,model,model,has,proposed model,model has proposed model,0.566501796245575
translation,262,8,results,improved rsdnc model,improves,mean accuracy,improved rsdnc model improves mean accuracy,0.7064668536186218
translation,262,8,results,by approximately 20 %,to,original rsdnc,by approximately 20 % to original rsdnc,0.6284105181694031
translation,262,8,results,original rsdnc,on,tasks,original rsdnc on tasks,0.5370686054229736
translation,262,8,results,tasks,requiring,knowledge,tasks requiring knowledge,0.6686007976531982
translation,262,8,results,knowledge,in,dialog babi tasks,knowledge in dialog babi tasks,0.5312495827674866
translation,262,8,results,mean accuracy,has,by approximately 20 %,mean accuracy has by approximately 20 %,0.5640496611595154
translation,262,8,results,results,has,improved rsdnc model,results has improved rsdnc model,0.5550710558891296
translation,262,112,results,significant improvement,of,14.45 %,significant improvement of 14.45 %,0.5380010008811951
translation,262,112,results,14.45 %,on,w/ kb facts task,14.45 % on w/ kb facts task,0.5372466444969177
translation,262,112,results,dnc and dnc + km models,obtained,accuracy scores,dnc and dnc + km models obtained accuracy scores,0.6381717324256897
translation,262,112,results,accuracy scores,of,29.53 % and 43.98 %,accuracy scores of 29.53 % and 43.98 %,0.5620247721672058
translation,262,113,results,rsdnc + km,achieved,best performance,rsdnc + km achieved best performance,0.7342569828033447
translation,262,113,results,rsdnc + km,improved,results,rsdnc + km improved results,0.7198612689971924
translation,262,113,results,results,on,w/ kb facts task,results on w/ kb facts task,0.5139051079750061
translation,262,113,results,w/ kb facts task,by,19.52 %,w/ kb facts task by 19.52 %,0.5660464763641357
translation,262,113,results,results,has,rsdnc + km,results has rsdnc + km,0.536773681640625
translation,262,152,results,hits@1 score,of,our model,hits@1 score of our model,0.5728229284286499
translation,262,152,results,our model,with,knowledge memory,our model with knowledge memory,0.6248986721038818
translation,262,152,results,knowledge memory,was,higher,knowledge memory was higher,0.6003416180610657
translation,262,152,results,higher,in,every dnc model,higher in every dnc model,0.5741931796073914
translation,262,152,results,response 3 tasks,has,hits@1 score,response 3 tasks has hits@1 score,0.5940818786621094
translation,262,152,results,results,In,response 3 tasks,results In response 3 tasks,0.45815613865852356
translation,262,153,results,rsdnc models,both,hits@1 and hits@10,rsdnc models both hits@1 and hits@10,0.719516932964325
translation,262,153,results,hits@1 and hits@10,of,rs-dnc + km,hits@1 and hits@10 of rs-dnc + km,0.6424214839935303
translation,262,153,results,rs-dnc + km,improved on,all three tasks,rs-dnc + km improved on all three tasks,0.7496129870414734
translation,262,153,results,results,In,rsdnc models,results In rsdnc models,0.49508169293403625
translation,263,104,baselines,di-alogpt,is,large-scale dialog response generation model,di-alogpt is large-scale dialog response generation model,0.514616072177887
translation,263,104,baselines,large-scale dialog response generation model,pre-trained on,147m reddit conversations,large-scale dialog response generation model pre-trained on 147m reddit conversations,0.7823867797851562
translation,263,104,baselines,baselines,has,di-alogpt,baselines has di-alogpt,0.610547661781311
translation,263,7,experiments,social media feedback data ( number of replies and upvotes ),to build,large-scale training dataset,social media feedback data ( number of replies and upvotes ) to build large-scale training dataset,0.6426622271537781
translation,263,7,experiments,large-scale training dataset,for,feedback prediction,large-scale training dataset for feedback prediction,0.5772189497947693
translation,263,103,model,12 - layer transformer model,based on,"gpt - 2 ( radford et al. , 2019 ) architecture","12 - layer transformer model based on gpt - 2 ( radford et al. , 2019 ) architecture",0.658644437789917
translation,263,103,model,model,is,12 - layer transformer model,model is 12 - layer transformer model,0.5137498378753662
translation,263,9,results,set of gpt - 2 based models,on,133 m pairs,set of gpt - 2 based models on 133 m pairs,0.5610580444335938
translation,263,9,results,133 m pairs,of,human feedback data,133 m pairs of human feedback data,0.5599575042724609
translation,263,9,results,dialogrpt,has,set of gpt - 2 based models,dialogrpt has set of gpt - 2 based models,0.5978826284408569
translation,263,9,results,results,trained,dialogrpt,results trained dialogrpt,0.6940137147903442
translation,263,130,results,responses,receive,fewer replies or upvotes,responses receive fewer replies or upvotes,0.6102575659751892
translation,263,130,results,fewer replies or upvotes,tend to be,less contentful,fewer replies or upvotes tend to be less contentful,0.6927760243415833
translation,263,147,results,dialogrpt,shows,highest test performance,dialogrpt shows highest test performance,0.6845372319221497
translation,263,147,results,highest test performance,on,both measurements,highest test performance on both measurements,0.48838189244270325
translation,263,147,results,reverse dialog perplexity,performs,better,reverse dialog perplexity performs better,0.6025342345237732
translation,263,147,results,better,than,forward dialog perplexity,better than forward dialog perplexity,0.5960045456886292
translation,263,147,results,highest test performance,has,reverse dialog perplexity,highest test performance has reverse dialog perplexity,0.5632293224334717
translation,263,147,results,both measurements,has,reverse dialog perplexity,both measurements has reverse dialog perplexity,0.6000757217407227
translation,263,147,results,results,has,dialogrpt,results has dialogrpt,0.6195732951164246
translation,263,148,results,simple bow baseline,has,outperforms,simple bow baseline has outperforms,0.5863822102546692
translation,263,148,results,outperforms,has,dialog models,outperforms has dialog models,0.5824452042579651
translation,263,169,results,model,trained only on,human- vs- rand data,model trained only on human- vs- rand data,0.7551524043083191
translation,263,169,results,model,performs,poorly,model performs poorly,0.7047662138938904
translation,263,171,results,feedback prediction models,show,much higher accuracy,feedback prediction models show much higher accuracy,0.6470718383789062
translation,263,171,results,much higher accuracy,in,human- vs- generated task,much higher accuracy in human- vs- generated task,0.5250301957130432
translation,263,171,results,results,has,feedback prediction models,results has feedback prediction models,0.5644233822822571
translation,263,173,results,model,trained with,random and generated responses,model trained with random and generated responses,0.7329778671264648
translation,263,173,results,model,perform,well,model perform well,0.7173547148704529
translation,263,173,results,model,perform,not well,model perform not well,0.6492457985877991
translation,263,173,results,random and generated responses,perform,well,random and generated responses perform well,0.6344420313835144
translation,263,173,results,random and generated responses,perform,not well,random and generated responses perform not well,0.5871025323867798
translation,263,173,results,well,on,human - vs . - fake tasks,well on human - vs . - fake tasks,0.5461388826370239
translation,263,173,results,well,on,humanvs .- human feedback ranking tasks,well on humanvs .- human feedback ranking tasks,0.5095667243003845
translation,263,173,results,not well,on,humanvs .- human feedback ranking tasks,not well on humanvs .- human feedback ranking tasks,0.5263027548789978
translation,263,173,results,results,has,model,results has model,0.5339115858078003
translation,263,179,results,ensemble model 's accuracy,is,not the highest,ensemble model 's accuracy is not the highest,0.5841563940048218
translation,263,179,results,ensemble model 's accuracy,performs,reasonably well,ensemble model 's accuracy performs reasonably well,0.5788776278495789
translation,263,179,results,not the highest,for,any of the test sets,not the highest for any of the test sets,0.6474456191062927
translation,263,182,results,human- like model ? 0,improves,model performance,human- like model ? 0 improves model performance,0.7088363766670227
translation,263,182,results,results,adding,human- like model ? 0,results adding human- like model ? 0,0.6923935413360596
translation,264,121,ablation-analysis,nexus and vhred + bow,enrich,latent variable,nexus and vhred + bow enrich latent variable,0.6496375203132629
translation,264,121,ablation-analysis,nexus and vhred + bow,bring,more diversity and relevance,nexus and vhred + bow bring more diversity and relevance,0.6753621697425842
translation,264,121,ablation-analysis,nexus and vhred + bow,show,distinct decline,nexus and vhred + bow show distinct decline,0.6838440895080566
translation,264,121,ablation-analysis,more diversity and relevance,show,distinct decline,more diversity and relevance show distinct decline,0.6579384803771973
translation,264,121,ablation-analysis,distinct decline,in,ppl,distinct decline in ppl,0.6119204163551331
translation,264,121,ablation-analysis,ablation analysis,has,nexus and vhred + bow,ablation analysis has nexus and vhred + bow,0.5640929937362671
translation,264,89,baselines,lstm,including,seq2seq + att,lstm including seq2seq + att,0.6849551200866699
translation,264,89,baselines,seq2seq + att,contains,vanilla seq2seq model,seq2seq + att contains vanilla seq2seq model,0.6245555877685547
translation,264,89,baselines,vanilla seq2seq model,with,attention mechanism,vanilla seq2seq model with attention mechanism,0.6211159825325012
translation,264,89,baselines,vhred + bow,introduces,continuous latent variable,vhred + bow introduces continuous latent variable,0.643074095249176
translation,264,89,baselines,continuous latent variable,attached to,response information,continuous latent variable attached to response information,0.6026058197021484
translation,264,89,baselines,continuous latent variable,applies,bow loss,continuous latent variable applies bow loss,0.5918325781822205
translation,264,89,baselines,response information,into,hred,response information into hred,0.6422669887542725
translation,264,89,baselines,future conversation,to incorporate,more scenario information,future conversation to incorporate more scenario information,0.6643528938293457
translation,264,89,baselines,more scenario information,into,latent variable,more scenario information into latent variable,0.5286436676979065
translation,264,94,hyperparameters,vocabulary sizes,for,"opensubtitles , dailydialog , personachat , and multiwoz","vocabulary sizes for opensubtitles , dailydialog , personachat , and multiwoz",0.5811190009117126
translation,264,94,hyperparameters,"opensubtitles , dailydialog , personachat , and multiwoz",set to,"50k , 20k , 20k , and 18 k","opensubtitles , dailydialog , personachat , and multiwoz set to 50k , 20k , 20k , and 18 k",0.6857237815856934
translation,264,95,hyperparameters,separate word embeddings,for,encoder and the decoder,separate word embeddings for encoder and the decoder,0.6277799010276794
translation,264,95,hyperparameters,word embedding dimension,is,256,word embedding dimension is 256,0.5879477262496948
translation,264,96,hyperparameters,initialized randomly,from,"normal distribution n ( 0 , 0.0001 )","initialized randomly from normal distribution n ( 0 , 0.0001 )",0.5512955188751221
translation,264,96,hyperparameters,hyperparameters,has,parameters,hyperparameters has parameters,0.4783959984779358
translation,264,97,hyperparameters,"adam ( kingma and ba , 2015 )",with,learning rate,"adam ( kingma and ba , 2015 ) with learning rate",0.5996310710906982
translation,264,97,hyperparameters,"adam ( kingma and ba , 2015 )",with,gradient clipping,"adam ( kingma and ba , 2015 ) with gradient clipping",0.6115773916244507
translation,264,97,hyperparameters,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,264,97,hyperparameters,gradient clipping,at,2.0,gradient clipping at 2.0,0.5267893075942993
translation,264,97,hyperparameters,hyperparameters,trained using,"adam ( kingma and ba , 2015 )","hyperparameters trained using adam ( kingma and ba , 2015 )",0.7323557138442993
translation,264,98,hyperparameters,batch size,is,128,batch size is 128,0.6402363181114197
translation,264,98,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,264,99,hyperparameters,hyper-parameters,in,proposed appraoch,hyper-parameters in proposed appraoch,0.5512083172798157
translation,264,99,hyperparameters,proposed appraoch,set as,? = 0.01 and ? 1 = 2.0,proposed appraoch set as ? = 0.01 and ? 1 = 2.0,0.6680120229721069
translation,264,99,hyperparameters,hyperparameters,has,hyper-parameters,hyperparameters has hyper-parameters,0.526509702205658
translation,264,5,model,scenario perspective,where,dialogue history and future conversation,scenario perspective where dialogue history and future conversation,0.5790902972221375
translation,264,5,model,dialogue history and future conversation,to implicitly reconstruct,scenario knowledge,dialogue history and future conversation to implicitly reconstruct scenario knowledge,0.7417091727256775
translation,264,5,model,taken into account,to implicitly reconstruct,scenario knowledge,taken into account to implicitly reconstruct scenario knowledge,0.7504017353057861
translation,264,19,model,conversation scenario,employ,future conversations,conversation scenario employ future conversations,0.5615824460983276
translation,264,19,model,future conversations,together with,dialogue histories,future conversations together with dialogue histories,0.6543293595314026
translation,264,19,model,dialogue histories,to learn,implicit conversation scenarios,dialogue histories to learn implicit conversation scenarios,0.584686815738678
translation,264,19,model,implicit conversation scenarios,provide,more semantic constraints,implicit conversation scenarios provide more semantic constraints,0.5924476981163025
translation,264,19,model,more semantic constraints,to guide,response generation,more semantic constraints to guide response generation,0.6676898002624512
translation,264,19,model,model,to enrich,conversation scenario,model to enrich conversation scenario,0.6864808797836304
translation,264,20,model,data,consisting of,"{ implicit scenario , response} pairs","data consisting of { implicit scenario , response} pairs",0.748343825340271
translation,264,20,model,model,propose,novel model,model propose novel model,0.6891457438468933
translation,264,22,model,imitation learning framework,to drive,conventional dialogue model,imitation learning framework to drive conventional dialogue model,0.6466847658157349
translation,264,22,model,conventional dialogue model,to absorb,corresponding scenario knowledge,conventional dialogue model to absorb corresponding scenario knowledge,0.6748629808425903
translation,264,22,model,corresponding scenario knowledge,from,scenario- based dialogue model,corresponding scenario knowledge from scenario- based dialogue model,0.5413510799407959
translation,264,22,model,model,propose,imitation learning framework,model propose imitation learning framework,0.6290730834007263
translation,264,23,model,scenario- based dialogue model,serves as,teacher,scenario- based dialogue model serves as teacher,0.662293016910553
translation,264,23,model,conventional dialogue model,relies solely on,dialogue history,conventional dialogue model relies solely on dialogue history,0.7273209095001221
translation,264,23,model,dialogue history,serves as,student,dialogue history serves as student,0.7047165036201477
translation,264,23,model,dialogue history,mimics,outputs,dialogue history mimics outputs,0.750377893447876
translation,264,23,model,student,mimics,outputs,student mimics outputs,0.7674345970153809
translation,264,23,model,model,has,scenario- based dialogue model,model has scenario- based dialogue model,0.5509269833564758
translation,264,40,model,conversation,introduce,implicit conversation scenario,conversation introduce implicit conversation scenario,0.5321933627128601
translation,264,40,model,implicit conversation scenario,into,existing dialogue models,implicit conversation scenario into existing dialogue models,0.5660602450370789
translation,264,40,model,existing dialogue models,to provide,more semantic constraints,existing dialogue models to provide more semantic constraints,0.5847403407096863
translation,264,40,model,existing dialogue models,reduce,difficulty,existing dialogue models reduce difficulty,0.6900137662887573
translation,264,40,model,difficulty,of,prediction,difficulty of prediction,0.5908464789390564
translation,264,40,model,model,combining,dialogue history,model combining dialogue history,0.7501270771026611
translation,264,117,results,significantly outperforms,on,all datasets,significantly outperforms on all datasets,0.5289040803909302
translation,264,117,results,all baselines,on,all datasets,all baselines on all datasets,0.4716669023036957
translation,264,117,results,proposed model,has,significantly outperforms,proposed model has significantly outperforms,0.6167242527008057
translation,264,117,results,significantly outperforms,has,all baselines,significantly outperforms has all baselines,0.5895546674728394
translation,264,117,results,results,has,proposed model,results has proposed model,0.5938616394996643
translation,264,118,results,lstmbased baselines,obtain,better performance,lstmbased baselines obtain better performance,0.550147294998169
translation,264,118,results,better performance,than,transformer - based baselines,better performance than transformer - based baselines,0.5896977186203003
translation,264,118,results,better performance,in terms of,diversity,better performance in terms of diversity,0.6645956635475159
translation,264,118,results,better performance,in terms of,relevance,better performance in terms of relevance,0.6468005776405334
translation,264,118,results,transformer - based baselines,in terms of,relevance,transformer - based baselines in terms of relevance,0.7174457311630249
translation,264,118,results,results,has,lstmbased baselines,results has lstmbased baselines,0.5419703722000122
translation,264,120,results,re- cosa,achieves,higher scores,re- cosa achieves higher scores,0.6277880072593689
translation,264,120,results,re- cosa,achieves,weaker results,re- cosa achieves weaker results,0.6991903185844421
translation,264,120,results,higher scores,on,bleu and embedding metrics,higher scores on bleu and embedding metrics,0.4963575005531311
translation,264,120,results,weaker results,on,"dist - { 1,2,3 }","weaker results on dist - { 1,2,3 }",0.5502257347106934
translation,264,120,results,weaker results,on,kl divergence,weaker results on kl divergence,0.5345422625541687
translation,264,120,results,chmam,has,re- cosa,chmam has re- cosa,0.7078808546066284
translation,264,120,results,results,Compared with,chmam,results Compared with chmam,0.6990949511528015
translation,264,123,results,improvements,of,our model,improvements of our model,0.5960047841072083
translation,264,123,results,our model,on,all datasets,our model on all datasets,0.5026571154594421
translation,264,123,results,our model,are,significant,our model are significant,0.6257864236831665
translation,264,123,results,significant,with,p,significant with p,0.7275827527046204
translation,264,123,results,results,has,improvements,results has improvements,0.615561842918396
translation,264,127,results,our model,performs,better,our model performs better,0.6546649932861328
translation,264,127,results,better,than,baselines,better than baselines,0.6307952404022217
translation,264,127,results,baselines,in,all datasets,baselines in all datasets,0.4695027768611908
translation,264,127,results,results,observe that,our model,results observe that our model,0.6364359259605408
translation,264,128,results,our model,obtains,most significant win-lose difference,our model obtains most significant win-lose difference,0.5842209458351135
translation,264,128,results,most significant win-lose difference,on,diversity,most significant win-lose difference on diversity,0.5537785887718201
translation,264,128,results,results,has,our model,results has our model,0.5871725678443909
translation,264,144,results,all metric improvements,of,transformer - if,all metric improvements of transformer - if,0.5744513273239136
translation,264,144,results,all metric improvements,on,uninformative set,all metric improvements on uninformative set,0.5224494934082031
translation,264,144,results,transformer - if,on,uninformative set,transformer - if on uninformative set,0.5636824369430542
translation,264,144,results,uninformative set,lower than,other set,uninformative set lower than other set,0.7116897702217102
translation,264,147,results,both models,on,dailydialog ( 3 - 1 - 3 ),both models on dailydialog ( 3 - 1 - 3 ),0.618503987789154
translation,264,147,results,both models,achieve,overall improvements,both models achieve overall improvements,0.6074851751327515
translation,264,147,results,dailydialog ( 3 - 1 - 3 ),achieve,overall improvements,dailydialog ( 3 - 1 - 3 ) achieve overall improvements,0.662891149520874
translation,264,147,results,results,on,dailydialog ( 1 - 1 - 1 ),results on dailydialog ( 1 - 1 - 1 ),0.5813932418823242
translation,264,148,results,absolute improvements,in,multi-turn conversation,absolute improvements in multi-turn conversation,0.5342951416969299
translation,264,148,results,single-turn conversation,means,transformer - if,single-turn conversation means transformer - if,0.6498583555221558
translation,264,148,results,transformer - if,performs,better,transformer - if performs better,0.7051656246185303
translation,264,148,results,better,when,implicit conversation scenario,better when implicit conversation scenario,0.6563441157341003
translation,264,148,results,implicit conversation scenario,contains,rich semantic information,implicit conversation scenario contains rich semantic information,0.5642822980880737
translation,264,148,results,results,has,absolute improvements,results has absolute improvements,0.5706809759140015
translation,264,157,results,perplexity,of,all baselines,perplexity of all baselines,0.5245617628097534
translation,264,157,results,perplexity,of,our student model,perplexity of our student model,0.5769113302230835
translation,264,157,results,perplexity,of,our student model,perplexity of our student model,0.5769113302230835
translation,264,157,results,perplexity,grows,slowly,perplexity grows slowly,0.730644941329956
translation,264,157,results,all baselines,has,rapidly increases,all baselines has rapidly increases,0.6272114515304565
translation,264,157,results,results,show that,perplexity,results show that perplexity,0.4910401403903961
translation,264,162,results,our model,has,significantly outperforms,our model has significantly outperforms,0.6134821176528931
translation,264,162,results,significantly outperforms,has,plain transformer and other baselines,significantly outperforms has plain transformer and other baselines,0.5922102928161621
translation,264,169,results,results,of,dai-lydialog,results of dai-lydialog,0.5805033445358276
translation,264,169,results,dai-lydialog,With the help of,pre-trained lm,dai-lydialog With the help of pre-trained lm,0.750348687171936
translation,264,169,results,performance,improved,consistently,performance improved consistently,0.8033753633499146
translation,264,169,results,results,has,performance,results has performance,0.5972660779953003
translation,264,169,results,dai-lydialog,has,performance,dai-lydialog has performance,0.570641279220581
translation,264,169,results,pre-trained lm,has,performance,pre-trained lm has performance,0.5494406819343567
translation,264,169,results,results,of,dai-lydialog,results of dai-lydialog,0.5805033445358276
translation,264,169,results,results,has,results,results has results,0.48582205176353455
translation,264,174,results,do multiple teachers work,To take advantage of,more diverse and richer prior knowledge,do multiple teachers work To take advantage of more diverse and richer prior knowledge,0.6110087633132935
translation,264,174,results,extending the teacher,has,from one to many,extending the teacher has from one to many,0.6013327836990356
translation,264,174,results,results,has,do multiple teachers work,results has do multiple teachers work,0.5464915037155151
translation,264,177,results,student,further improves on,all metrics,student further improves on all metrics,0.684208869934082
translation,264,177,results,weak decline,in,relevance,weak decline in relevance,0.5297641158103943
translation,264,177,results,language model,has,student,language model has student,0.5928905010223389
translation,264,177,results,results,with the help of,language model,results with the help of language model,0.652669370174408
translation,265,126,ablation-analysis,fine-tuning,benefit,main task of response generation,fine-tuning benefit main task of response generation,0.6710649728775024
translation,265,126,ablation-analysis,pretrained model,with,spatial - temporal information and multi-task objectives,pretrained model with spatial - temporal information and multi-task objectives,0.5961858630180359
translation,265,126,ablation-analysis,fine-tuning,has,pretrained model,fine-tuning has pretrained model,0.5594874024391174
translation,265,126,ablation-analysis,ablation analysis,shows,fine-tuning,ablation analysis shows fine-tuning,0.6336027979850769
translation,265,131,ablation-analysis,mvm objective function,increase,cider score,mvm objective function increase cider score,0.674839973449707
translation,265,131,ablation-analysis,cider score,by,0.043 absolute score,cider score by 0.043 absolute score,0.5453681945800781
translation,265,131,ablation-analysis,cider score,has,most,cider score has most,0.6103736162185669
translation,265,131,ablation-analysis,ablation analysis,adding,mvm objective function,ablation analysis adding mvm objective function,0.6487210988998413
translation,265,91,baselines,masked multi-modal modeling,explore,two loss functions,masked multi-modal modeling explore two loss functions,0.6404151916503906
translation,265,91,baselines,baselines,has,masked multi-modal modeling,baselines has masked multi-modal modeling,0.5641458630561829
translation,265,99,baselines,matching video - text pair ( mvt ),adapt,pretrained language model,matching video - text pair ( mvt ) adapt pretrained language model,0.7577542066574097
translation,265,99,baselines,pretrained language model,to,dialogue domain,pretrained language model to dialogue domain,0.5061872005462646
translation,265,99,baselines,dialogue domain,by replacing,original input,dialogue domain by replacing original input,0.6844973564147949
translation,265,99,baselines,original input,with,incorrect dialogue,original input with incorrect dialogue,0.6339582800865173
translation,265,99,baselines,original input,with,video input,original input with video input,0.6173269152641296
translation,265,99,baselines,video input,at,random,video input at random,0.6026143431663513
translation,265,99,baselines,baselines,has,matching video - text pair ( mvt ),baselines has matching video - text pair ( mvt ),0.5720655918121338
translation,265,105,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,265,105,experimental-setup,learning rate,of,5e - 5,learning rate of 5e - 5,0.6587256789207458
translation,265,105,experimental-setup,5e - 5,based on,grid search,5e - 5 based on grid search,0.6625295281410217
translation,265,105,experimental-setup,experimental setup,use,adam optimizer,experimental setup use adam optimizer,0.5987385511398315
translation,265,106,experimental-setup,learning rate decay schedule,set,weight,learning rate decay schedule set weight,0.6264125108718872
translation,265,106,experimental-setup,weight,on,response generation loss,weight on response generation loss,0.5756502747535706
translation,265,106,experimental-setup,response generation loss,to be,1.5 times higher,response generation loss to be 1.5 times higher,0.5789130330085754
translation,265,106,experimental-setup,1.5 times higher,than,other losses,1.5 times higher than other losses,0.566046953201294
translation,265,106,experimental-setup,experimental setup,adopt,learning rate decay schedule,experimental setup adopt learning rate decay schedule,0.6267268061637878
translation,265,106,experimental-setup,experimental setup,set,weight,experimental setup set weight,0.6465502381324768
translation,265,110,experimental-setup,visual features,used,3d cnnbased resnext - 101,visual features used 3d cnnbased resnext - 101,0.5328145623207092
translation,265,110,experimental-setup,3d cnnbased resnext - 101,pre-trained on,"kinetics ( hara et al. , 2018 )","3d cnnbased resnext - 101 pre-trained on kinetics ( hara et al. , 2018 )",0.7788697481155396
translation,265,110,experimental-setup,"kinetics ( hara et al. , 2018 )",to obtain,spatiotemporal video features,"kinetics ( hara et al. , 2018 ) to obtain spatiotemporal video features",0.5833236575126648
translation,265,110,experimental-setup,experimental setup,To extract,visual features,experimental setup To extract visual features,0.7132585644721985
translation,265,111,experimental-setup,batch size,to,16,batch size to 16,0.6355306506156921
translation,265,111,experimental-setup,maximum sequence length,compatible with,gpt2 models,maximum sequence length compatible with gpt2 models,0.7131866812705994
translation,265,111,experimental-setup,experimental setup,fixed,batch size,experimental setup fixed batch size,0.6885966062545776
translation,265,111,experimental-setup,experimental setup,fixed,maximum sequence length,experimental setup fixed maximum sequence length,0.6901282072067261
translation,265,112,experimental-setup,video features,every,16 frames,video features every 16 frames,0.6208383440971375
translation,265,112,experimental-setup,16 frames,without,overlapping,16 frames without overlapping,0.7410838007926941
translation,265,112,experimental-setup,experimental setup,sampled,video features,experimental setup sampled video features,0.6803569197654724
translation,265,113,experimental-setup,up to 50 epochs,on,4,up to 50 epochs on 4,0.5722241401672363
translation,265,113,experimental-setup,up to 50 epochs,on,gpus,up to 50 epochs on gpus,0.5254924297332764
translation,265,113,experimental-setup,4,has,gpus,4 has gpus,0.5346742868423462
translation,265,113,experimental-setup,experimental setup,trained,up to 50 epochs,experimental setup trained up to 50 epochs,0.689530611038208
translation,265,107,experiments,video-grounded dialogue task,in,large-scale avsd benchmark,video-grounded dialogue task in large-scale avsd benchmark,0.4924333095550537
translation,265,130,experiments,mlm and mvm,to improve,learning,mlm and mvm to improve learning,0.6900815963745117
translation,265,130,experiments,mlm and mvm,to improve,learning,mlm and mvm to improve learning,0.6900815963745117
translation,265,130,experiments,learning,of,local dependencies,learning of local dependencies,0.5208359360694885
translation,265,130,experiments,local dependencies,in,token and spatial levels,local dependencies in token and spatial levels,0.5685385465621948
translation,265,130,experiments,mvt,to support,learning,mvt to support learning,0.6957220435142517
translation,265,130,experiments,global dependencies,between,text and visual modalities,global dependencies between text and visual modalities,0.6783028244972229
translation,265,130,experiments,learning,has,global dependencies,learning has global dependencies,0.542989194393158
translation,265,6,model,models,formulating,videogrounded dialogue tasks,models formulating videogrounded dialogue tasks,0.6212943196296692
translation,265,6,model,videogrounded dialogue tasks,as,sequence - tosequence task,videogrounded dialogue tasks as sequence - tosequence task,0.5174524784088135
translation,265,6,model,videogrounded dialogue tasks,combining,visual and textual representation,videogrounded dialogue tasks combining visual and textual representation,0.6784036755561829
translation,265,6,model,visual and textual representation,into,structured sequence,visual and textual representation into structured sequence,0.5809175372123718
translation,265,7,model,fine-tuning language models,to capture,dependencies,fine-tuning language models to capture dependencies,0.6357687711715698
translation,265,7,model,dependencies,across,multiple modalities,dependencies across multiple modalities,0.7323861718177795
translation,265,7,model,multiple modalities,over,different levels of information,multiple modalities over different levels of information,0.6459394693374634
translation,265,7,model,spatio-temporal level,in,video,spatio-temporal level in video,0.542684018611908
translation,265,7,model,token - sentence level,in,dialogue context,token - sentence level in dialogue context,0.5265743732452393
translation,265,21,model,power,of,pre-trained models,power of pre-trained models,0.6053030490875244
translation,265,21,model,pre-trained models,to obtain,linguistic and visual representations,pre-trained models to obtain linguistic and visual representations,0.5633653998374939
translation,265,21,model,linguistic and visual representations,in,dialogues and videos,linguistic and visual representations in dialogues and videos,0.539516031742096
translation,265,21,model,model,fully lever - age,power,model fully lever - age power,0.7486106157302856
translation,265,23,model,dialogue agent,create,responses,dialogue agent create responses,0.6427260637283325
translation,265,23,model,responses,address,user questions,responses address user questions,0.5918234586715698
translation,265,23,model,match,has,dialogue flow,match has dialogue flow,0.6297890543937683
translation,265,23,model,model,has,dialogue agent,model has dialogue agent,0.577430009841919
translation,265,24,model,input components,of,video-grounded dialogue,input components of video-grounded dialogue,0.5086154937744141
translation,265,24,model,downstream task,of,pre-trained language models,downstream task of pre-trained language models,0.5270370244979858
translation,265,24,model,model,formulate,input components,model formulate input components,0.6286981105804443
translation,265,27,model,dependencies,between,each token,dependencies between each token,0.6549068093299866
translation,265,27,model,dependencies,between,each spatial feature,dependencies between each spatial feature,0.6856547594070435
translation,265,27,model,each token,in,text data,each token in text data,0.534574568271637
translation,265,27,model,each spatial feature,along,temporal dimension,each spatial feature along temporal dimension,0.6517462730407715
translation,265,27,model,temporal dimension,of,input video,temporal dimension of input video,0.5630378127098083
translation,265,27,model,model,capture,dependencies,model capture dependencies,0.7985786199569702
translation,265,28,model,multi-task learning framework,includes,additional learning objectives,multi-task learning framework includes additional learning objectives,0.6190571188926697
translation,265,28,model,additional learning objectives,in addition to,dialogue response generation objective,additional learning objectives in addition to dialogue response generation objective,0.5886989235877991
translation,265,28,model,model,present,multi-task learning framework,model present multi-task learning framework,0.5891478061676025
translation,265,51,model,multi-turn dialogue input,combined with,video input,multi-turn dialogue input combined with video input,0.6451216340065002
translation,265,51,model,video input,with,spatialtemporal variations,video input with spatialtemporal variations,0.6309407353401184
translation,265,58,model,gpt - 2 model,based on,transformer network,gpt - 2 model based on transformer network,0.6637243628501892
translation,265,58,model,12 to 24 layers,of,masked multi-head attention,12 to 24 layers of masked multi-head attention,0.5565282702445984
translation,265,58,model,masked multi-head attention,on,very large text data,masked multi-head attention on very large text data,0.5079315304756165
translation,265,58,model,model,has,gpt - 2 model,model has gpt - 2 model,0.5862849950790405
translation,265,59,model,models,to generate,video-grounded dialogue responses,models to generate video-grounded dialogue responses,0.699378252029419
translation,265,119,results,outperforms,across,all the automated metrics,outperforms across all the automated metrics,0.6741637587547302
translation,265,119,results,existing approaches,across,all the automated metrics,existing approaches across all the automated metrics,0.6910010576248169
translation,265,119,results,vgd - gpt2 model,has,outperforms,vgd - gpt2 model has outperforms,0.6210123896598816
translation,265,119,results,outperforms,has,existing approaches,outperforms has existing approaches,0.5970985293388367
translation,265,119,results,results,has,vgd - gpt2 model,results has vgd - gpt2 model,0.5195605158805847
translation,265,120,results,language model,with,video-grounded dialogues,language model with video-grounded dialogues,0.6019747853279114
translation,265,120,results,fine-tuning,has,language model,fine-tuning has language model,0.5249429941177368
translation,265,120,results,results,show,fine-tuning,results show fine-tuning,0.5946627855300903
translation,265,121,results,our models,with,language model,our models with language model,0.6160839796066284
translation,265,121,results,language model,pre-trained on,massive text data,language model pre-trained on massive text data,0.6887704730033875
translation,265,121,results,language model,obtain,richer feature representations,language model obtain richer feature representations,0.5287895798683167
translation,265,121,results,richer feature representations,that capture,more complex dependencies,richer feature representations that capture more complex dependencies,0.6563078761100769
translation,265,121,results,more complex dependencies,between,inputs,more complex dependencies between inputs,0.6975794434547424
translation,265,121,results,results,initializing,our models,results initializing our models,0.7183706164360046
translation,265,122,results,our model,treats,visual and text features,our model treats visual and text features,0.6153990030288696
translation,265,122,results,visual and text features,with,equal importance,visual and text features with equal importance,0.6528009176254272
translation,265,122,results,equal importance,at,different levels,equal importance at different levels,0.580057680606842
translation,265,122,results,different levels,of,different dimensions,different levels of different dimensions,0.6166532039642334
translation,265,128,results,cider,learning,dependencies,cider learning dependencies,0.7314799427986145
translation,265,128,results,dependencies,in,spatial and temporal dimensions,dependencies in spatial and temporal dimensions,0.588866651058197
translation,265,128,results,dependencies,both,spatial and temporal dimensions,dependencies both spatial and temporal dimensions,0.7638599276542664
translation,265,128,results,dependencies,improve,performance,dependencies improve performance,0.7181175947189331
translation,265,128,results,performance,by,0.01 absolute score,performance by 0.01 absolute score,0.5788195133209229
translation,265,128,results,performance,by,0.008 absolute score,performance by 0.008 absolute score,0.5663522481918335
translation,265,128,results,0.01 absolute score,from,spatial -only feature,0.01 absolute score from spatial -only feature,0.48673829436302185
translation,265,128,results,0.008 absolute score,from,temporal-only feature,0.008 absolute score from temporal-only feature,0.4711800813674927
translation,265,128,results,results,Considering,cider,results Considering cider,0.6465057134628296
translation,265,129,results,proposed auxiliary objectives,help to improve,model performance,proposed auxiliary objectives help to improve model performance,0.7025655508041382
translation,265,129,results,model performance,by adapting,pretrained model,model performance by adapting pretrained model,0.7034320831298828
translation,265,129,results,pretrained model,to,current data domain,pretrained model to current data domain,0.5124142169952393
translation,265,129,results,results,has,proposed auxiliary objectives,results has proposed auxiliary objectives,0.5584479570388794
translation,265,132,results,moderate performance improvements,in,"bleu3 , bleu4 , and rouge -l","moderate performance improvements in bleu3 , bleu4 , and rouge -l",0.49416229128837585
translation,265,132,results,moderate performance improvements,when increasing,gpt - 2,moderate performance improvements when increasing gpt - 2,0.7383292317390442
translation,265,132,results,gpt - 2,from,small to medium size,gpt - 2 from small to medium size,0.5584739446640015
translation,265,132,results,results,found,moderate performance improvements,results found moderate performance improvements,0.6457409262657166
translation,266,190,ablation-analysis,hotel domain,has,high error rate,hotel domain has high error rate,0.5727869272232056
translation,266,190,ablation-analysis,ablation analysis,type slot of,hotel domain,ablation analysis type slot of hotel domain,0.7947795391082764
translation,266,147,baselines,multiple bi-lstms,to encode,system and user utterances,multiple bi-lstms to encode system and user utterances,0.6970492601394653
translation,266,152,baselines,gce,is,current state - of- the - art model,gce is current state - of- the - art model,0.5436920523643494
translation,266,152,baselines,spanptr,is,first model,spanptr is first model,0.5896856188774109
translation,266,152,baselines,current state - of- the - art model,has,spanptr,current state - of- the - art model has spanptr,0.5442662239074707
translation,266,152,baselines,baselines,has,gce,baselines has gce,0.5680229663848877
translation,266,187,experiments,name slots,in,"restaurant , attraction , and hotel domains","name slots in restaurant , attraction , and hotel domains",0.48887714743614197
translation,266,132,hyperparameters,model,trained,end-to - end,model trained end-to - end,0.7512461543083191
translation,266,132,hyperparameters,model,using,adam optimizer,model using adam optimizer,0.6418686509132385
translation,266,132,hyperparameters,end-to - end,using,adam optimizer,end-to - end using adam optimizer,0.6591702699661255
translation,266,132,hyperparameters,adam optimizer,with,batch size,adam optimizer with batch size,0.606801450252533
translation,266,132,hyperparameters,batch size,of,32,batch size of 32,0.6741614937782288
translation,266,132,hyperparameters,hyperparameters,trained,end-to - end,hyperparameters trained end-to - end,0.7198280096054077
translation,266,132,hyperparameters,hyperparameters,has,model,hyperparameters has model,0.5282720923423767
translation,266,133,hyperparameters,learning rate annealing,in the range of,"[ 0.001 , 0.0001 ]","learning rate annealing in the range of [ 0.001 , 0.0001 ]",0.6735932230949402
translation,266,133,hyperparameters,learning rate annealing,with,dropout ratio,learning rate annealing with dropout ratio,0.6394553780555725
translation,266,133,hyperparameters,dropout ratio,of,0.2,dropout ratio of 0.2,0.6002745628356934
translation,266,133,hyperparameters,hyperparameters,has,learning rate annealing,hyperparameters has learning rate annealing,0.5017721652984619
translation,266,135,hyperparameters,embeddings,initialized by,concatenating glove embeddings,embeddings initialized by concatenating glove embeddings,0.6123888492584229
translation,266,135,hyperparameters,dimension,is,400,dimension is 400,0.6541664600372314
translation,266,135,hyperparameters,400,for,each vocabulary word,400 for each vocabulary word,0.6546388268470764
translation,266,135,hyperparameters,character embeddings,has,"hashimoto et al. , 2016 )","character embeddings has hashimoto et al. , 2016 )",0.532998263835907
translation,266,135,hyperparameters,hyperparameters,has,embeddings,hyperparameters has embeddings,0.548468291759491
translation,266,136,hyperparameters,greedy search decoding strategy,used for,our state generator,greedy search decoding strategy used for our state generator,0.647617757320404
translation,266,136,hyperparameters,hyperparameters,has,greedy search decoding strategy,hyperparameters has greedy search decoding strategy,0.5221691727638245
translation,266,141,hyperparameters,gem,set,memory sizes k,gem set memory sizes k,0.6967154741287231
translation,266,141,hyperparameters,memory sizes k,to,1 %,memory sizes k to 1 %,0.5721499919891357
translation,266,141,hyperparameters,1 %,of,source domains,1 % of source domains,0.5959804654121399
translation,266,141,hyperparameters,hyperparameters,in,gem,hyperparameters in gem,0.5219300985336304
translation,266,6,model,transferable dialogue state generator ( trade ),generates,dialogue states,transferable dialogue state generator ( trade ) generates dialogue states,0.6861598491668701
translation,266,6,model,dialogue states,from,utterances,dialogue states from utterances,0.6232897043228149
translation,266,6,model,utterances,using,copy mechanism,utterances using copy mechanism,0.7511549592018127
translation,266,6,model,copy mechanism,facilitating,knowledge transfer,copy mechanism facilitating knowledge transfer,0.7792959213256836
translation,266,6,model,knowledge transfer,has,"when predicting ( domain , slot , value ) triplets","knowledge transfer has when predicting ( domain , slot , value ) triplets",0.5639902949333191
translation,266,6,model,model,propose,transferable dialogue state generator ( trade ),model propose transferable dialogue state generator ( trade ),0.6809195280075073
translation,266,7,model,utterance encoder,shared across,domains,utterance encoder shared across domains,0.7427172064781189
translation,266,7,model,state generator,shared across,domains,state generator shared across domains,0.6887083649635315
translation,266,7,model,model,composed of,utterance encoder,model composed of utterance encoder,0.6605850458145142
translation,266,7,model,model,composed of,state generator,model composed of state generator,0.708981990814209
translation,266,52,model,transferable dialogue state generator ( trade ),for,multi-domain taskoriented dialogue state tracking,transferable dialogue state generator ( trade ) for multi-domain taskoriented dialogue state tracking,0.5914825201034546
translation,266,52,model,model,propose,transferable dialogue state generator ( trade ),model propose transferable dialogue state generator ( trade ),0.6809195280075073
translation,266,122,model,two specific continual learning techniques,to fine - tune,our model,two specific continual learning techniques to fine - tune our model,0.7216516137123108
translation,266,122,model,gradient episodic memory ( gem ),to fine - tune,our model,gradient episodic memory ( gem ) to fine - tune our model,0.7056613564491272
translation,266,122,model,model,employ,two specific continual learning techniques,model employ two specific continual learning techniques,0.6356610059738159
translation,266,137,model,word dropout,utilized with,utterance encoder,word dropout utilized with utterance encoder,0.6324695348739624
translation,266,137,model,utterance encoder,by,randomly masking,utterance encoder by randomly masking,0.5971867442131042
translation,266,137,model,outof-vocabulary setting,has,word dropout,outof-vocabulary setting has word dropout,0.5504063367843628
translation,266,137,model,randomly masking,has,small amount of input tokens,randomly masking has small amount of input tokens,0.584703266620636
translation,266,137,model,model,in-crease,model generalization,model in-crease model generalization,0.7441788911819458
translation,266,137,model,model,simulate,outof-vocabulary setting,model simulate outof-vocabulary setting,0.6451740860939026
translation,266,154,results,trade,achieves,highest performance,trade achieves highest performance,0.7297481894493103
translation,266,154,results,highest performance,on,multiwoz,highest performance on multiwoz,0.5707004070281982
translation,266,154,results,highest performance,on,multiwoz,highest performance on multiwoz,0.5707004070281982
translation,266,154,results,highest performance,on,multiwoz,highest performance on multiwoz,0.5707004070281982
translation,266,154,results,48.62 %,on,joint goal accuracy,48.62 % on joint goal accuracy,0.47320082783699036
translation,266,154,results,96.92 %,on,slot accuracy,96.92 % on slot accuracy,0.4944492280483246
translation,266,154,results,highest performance,has,48.62 %,highest performance has 48.62 %,0.5499602556228638
translation,266,154,results,results,has,trade,results has trade,0.505282998085022
translation,266,166,results,taxi domain,achieves,highest zero-shot performance,taxi domain achieves highest zero-shot performance,0.7175757884979248
translation,266,166,results,60.58 %,on,joint goal accuracy,60.58 % on joint goal accuracy,0.4695529639720917
translation,266,166,results,highest zero-shot performance,has,60.58 %,highest zero-shot performance has 60.58 %,0.5453978180885315
translation,266,167,results,performances,on,other zero-shot domains,performances on other zero-shot domains,0.5459886193275452
translation,266,167,results,other zero-shot domains,are,not especially promising,other zero-shot domains are not especially promising,0.5798941254615784
translation,266,167,results,50 to 65 % slot accuracy,without using,in- domain samples,50 to 65 % slot accuracy without using in- domain samples,0.7288516759872437
translation,266,167,results,results,has,performances,results has performances,0.5711642503738403
translation,266,174,results,naive and ewc fine - tuning,in terms of overcoming,catastrophic forgetting,naive and ewc fine - tuning in terms of overcoming catastrophic forgetting,0.681867241859436
translation,266,174,results,outperforms,has,naive and ewc fine - tuning,outperforms has naive and ewc fine - tuning,0.5918400883674622
translation,266,175,results,pre-training,followed by,fine-tuning,pre-training followed by fine-tuning,0.6619076132774353
translation,266,175,results,training from scratch,on,single domain,training from scratch on single domain,0.5693881511688232
translation,266,175,results,fine-tuning,has,outperforms,fine-tuning has outperforms,0.6195870041847229
translation,266,175,results,outperforms,has,training from scratch,outperforms has training from scratch,0.6049229502677917
translation,266,175,results,results,find that,pre-training,results find that pre-training,0.6078681349754333
translation,266,176,results,fine-tuning trade,with,gem,fine-tuning trade with gem,0.627191424369812
translation,266,176,results,fine-tuning trade,maintains,higher performance,fine-tuning trade maintains higher performance,0.7040285468101501
translation,266,176,results,gem,maintains,higher performance,gem maintains higher performance,0.7271789312362671
translation,266,176,results,higher performance,on,original four domains,higher performance on original four domains,0.5197551250457764
translation,266,176,results,results,has,fine-tuning trade,results has fine-tuning trade,0.5610789656639099
translation,266,178,results,trade,from,four domains,trade from four domains,0.6560627818107605
translation,266,178,results,trade,achieves,better performance,trade achieves better performance,0.7225508689880371
translation,266,178,results,four domains,to,new domain,four domains to new domain,0.5818154811859131
translation,266,178,results,better performance,than,training from scratch,better performance than training from scratch,0.5749625563621521
translation,266,178,results,training from scratch,on,new domain,training from scratch on new domain,0.5484956502914429
translation,266,178,results,results,Expanding,trade,results Expanding trade,0.6389685273170471
translation,266,182,results,gem,obtains,34.73 % joint accuracy,gem obtains 34.73 % joint accuracy,0.6065155267715454
translation,266,182,results,34.73 % joint accuracy,on,attraction domain,34.73 % joint accuracy on attraction domain,0.5042657256126404
translation,266,182,results,naive finetuning,achieve,29.39 %,naive finetuning achieve 29.39 %,0.6001601219177246
translation,266,182,results,results,has,gem,results has gem,0.4178132116794586
translation,267,125,ablation-analysis,data counterfeiting ( counterfeit ),brings,substantial reduction,data counterfeiting ( counterfeit ) brings substantial reduction,0.6015015840530396
translation,267,125,ablation-analysis,substantial reduction,in,slot error rate,substantial reduction in slot error rate,0.561924159526825
translation,267,125,ablation-analysis,ablation analysis,has,data counterfeiting ( counterfeit ),ablation analysis has data counterfeiting ( counterfeit ),0.5557824969291687
translation,267,150,ablation-analysis,limited amount of in-domain data,is available,efficient adaptation,limited amount of in-domain data is available efficient adaptation,0.6784565448760986
translation,267,150,ablation-analysis,efficient adaptation,is,critical,efficient adaptation is critical,0.6095353364944458
translation,267,150,ablation-analysis,critical,has,dt - 10 % & ml - 10 % > scr -10 %,critical has dt - 10 % & ml - 10 % > scr -10 %,0.6371563673019409
translation,267,150,ablation-analysis,ablation analysis,if,limited amount of in-domain data,ablation analysis if limited amount of in-domain data,0.5601968169212341
translation,267,153,ablation-analysis,adaptation methods ( dt - 10 % & ml - 10 % ),crucial to bridge,gap,adaptation methods ( dt - 10 % & ml - 10 % ) crucial to bridge gap,0.6374948024749756
translation,267,153,ablation-analysis,gap,between,domains,gap between domains,0.7436578273773193
translation,267,153,ablation-analysis,domains,when,target domain data,domains when target domain data,0.6289666295051575
translation,267,153,ablation-analysis,target domain data,is,scarce,target domain data is scarce,0.592586100101471
translation,267,153,ablation-analysis,ablation analysis,has,preference test,ablation analysis has preference test,0.5585103034973145
translation,267,110,experimental-setup,generators,implemented using,theano library,generators implemented using theano library,0.7319589853286743
translation,267,110,experimental-setup,generators,trained by partitioning,each of the collected corpora,generators trained by partitioning each of the collected corpora,0.7782610058784485
translation,267,110,experimental-setup,each of the collected corpora,into,"training , validation , and testing set","each of the collected corpora into training , validation , and testing set",0.5328993797302246
translation,267,110,experimental-setup,"training , validation , and testing set",in,ratio,"training , validation , and testing set in ratio",0.5063410401344299
translation,267,110,experimental-setup,ratio,has,3:1:1,ratio has 3:1:1,0.5884220004081726
translation,267,110,experimental-setup,experimental setup,has,generators,experimental setup has generators,0.4874105453491211
translation,267,111,experimental-setup,generators,trained by treating,each sentence,generators trained by treating each sentence,0.7653738856315613
translation,267,111,experimental-setup,each sentence,as,mini-batch,each sentence as mini-batch,0.517674446105957
translation,267,111,experimental-setup,experimental setup,has,generators,experimental setup has generators,0.4874105453491211
translation,267,112,experimental-setup,l 2 regularisation term,added to,objective function,l 2 regularisation term added to objective function,0.6296073198318481
translation,267,112,experimental-setup,objective function,for,every 10 training examples,objective function for every 10 training examples,0.5537101030349731
translation,267,112,experimental-setup,experimental setup,has,l 2 regularisation term,experimental setup has l 2 regularisation term,0.482504278421402
translation,267,113,experimental-setup,hidden layer size,set to be,100,hidden layer size set to be 100,0.728607714176178
translation,267,113,experimental-setup,experimental setup,has,hidden layer size,experimental setup has hidden layer size,0.5019465684890747
translation,267,114,experimental-setup,back propagation,to,optimise,back propagation to optimise,0.582828938961029
translation,267,114,experimental-setup,"through time ( werbos , 1990 )",to,optimise,"through time ( werbos , 1990 ) to optimise",0.5857694745063782
translation,267,114,experimental-setup,back propagation,has,"through time ( werbos , 1990 )","back propagation has through time ( werbos , 1990 )",0.5885629057884216
translation,267,114,experimental-setup,optimise,has,parameters,optimise has parameters,0.6024158596992493
translation,267,114,experimental-setup,experimental setup,has,stochastic gradient descent,experimental setup has stochastic gradient descent,0.49394887685775757
translation,267,134,experimental-setup,each da,applied,our generator,each da applied our generator,0.7545167207717896
translation,267,134,experimental-setup,50 times,to generate,candidate sentences,50 times to generate candidate sentences,0.6549036502838135
translation,267,134,experimental-setup,our generator,has,50 times,our generator has 50 times,0.5888777375221252
translation,267,134,experimental-setup,experimental setup,For,each da,experimental setup For each da,0.662233293056488
translation,267,6,model,procedure,to train,"multi-domain , recurrent neural network - based ( rnn ) language generators","procedure to train multi-domain , recurrent neural network - based ( rnn ) language generators",0.678126335144043
translation,267,6,model,"multi-domain , recurrent neural network - based ( rnn ) language generators",via,multiple adaptation steps,"multi-domain , recurrent neural network - based ( rnn ) language generators via multiple adaptation steps",0.6293901801109314
translation,267,6,model,model,propose,procedure,model propose procedure,0.5745797157287598
translation,267,7,model,model,trained on,counterfeited data,model trained on counterfeited data,0.7723358273506165
translation,267,7,model,counterfeited data,synthesised from,out-of- domain dataset,counterfeited data synthesised from out-of- domain dataset,0.6287872791290283
translation,267,7,model,small set of in-domain utterances,with,discriminative objective function,small set of in-domain utterances with discriminative objective function,0.6138787865638733
translation,267,7,model,model,trained on,counterfeited data,model trained on counterfeited data,0.7723358273506165
translation,267,20,model,dt,enable,generator,dt enable generator,0.7042873501777649
translation,267,20,model,generator,to learn more efficiently,in-domain data,generator to learn more efficiently in-domain data,0.7569807767868042
translation,267,20,model,in-domain data,has,is scarce,in-domain data has is scarce,0.575667679309845
translation,267,20,model,model,show,dt,model show dt,0.7229129076004028
translation,267,21,model,incremental recipe,based on,rnn - based generation model,incremental recipe based on rnn - based generation model,0.6250247359275818
translation,267,156,model,procedure,for,"training multi-domain , rnn - based language generators","procedure for training multi-domain , rnn - based language generators",0.5545564293861389
translation,267,156,model,procedure,by,data counterfeiting,procedure by data counterfeiting,0.5800018310546875
translation,267,156,model,procedure,by,discriminative training,procedure by discriminative training,0.5661596059799194
translation,267,156,model,model,proposed,procedure,model proposed procedure,0.6156147122383118
translation,267,122,results,model fine- tuning ( tune ),achieves,better bleu score,model fine- tuning ( tune ) achieves better bleu score,0.6627250909805298
translation,267,122,results,better bleu score,than,training from scratch ( scratch ),better bleu score than training from scratch ( scratch ),0.5579681396484375
translation,267,122,results,better bleu score,when,target domain data,better bleu score when target domain data,0.5785484910011292
translation,267,122,results,training from scratch ( scratch ),when,target domain data,training from scratch ( scratch ) when target domain data,0.6024821400642395
translation,267,122,results,target domain data,is,limited,target domain data is limited,0.5741276741027832
translation,267,123,results,data counterfeiting ( counterfeit ) method,obtain,even greater bleu score gain,data counterfeiting ( counterfeit ) method obtain even greater bleu score gain,0.5523030161857605
translation,267,123,results,results,apply,data counterfeiting ( counterfeit ) method,results apply data counterfeiting ( counterfeit ) method,0.6355156302452087
translation,267,128,results,data counterfeiting ( counterfeit ) method,superior to,other methods,data counterfeiting ( counterfeit ) method superior to other methods,0.6876084208488464
translation,267,128,results,results,has,data counterfeiting ( counterfeit ) method,results has data counterfeiting ( counterfeit ) method,0.5494241118431091
translation,267,138,results,dt,consistently improves,generator performance,dt consistently improves generator performance,0.7089508175849915
translation,267,138,results,generator performance,on,both metrics,generator performance on both metrics,0.48097488284111023
translation,267,149,results,target domain data,is,available,target domain data is available,0.5878214836120605
translation,267,149,results,training everything from scratch ( scrall ),achieves,very good performance,training everything from scratch ( scrall ) achieves very good performance,0.6341703534126282
translation,267,151,results,judges,preferred,dt trained generator ( dt - 10 % ),judges preferred dt trained generator ( dt - 10 % ),0.7021923661231995
translation,267,151,results,dt trained generator ( dt - 10 % ),compared to,ml trained generator ( ml - 10 % ),dt trained generator ( dt - 10 % ) compared to ml trained generator ( ml - 10 % ),0.6980848908424377
translation,267,151,results,dt trained generator ( dt - 10 % ),for,informativeness,dt trained generator ( dt - 10 % ) for informativeness,0.6070545315742493
translation,267,151,results,ml trained generator ( ml - 10 % ),for,informativeness,ml trained generator ( ml - 10 % ) for informativeness,0.6292031407356262
translation,267,151,results,results,has,judges,results has judges,0.4722744822502136
translation,267,154,results,preferred,compared to,ml training ( ml - 10 % ),preferred compared to ml training ( ml - 10 % ),0.7060482501983643
translation,267,154,results,results,suggest that,dt training approach ( dt - 10 % ),results suggest that dt training approach ( dt - 10 % ),0.654634416103363
translation,268,199,ablation-analysis,model,becomes better at,game,model becomes better at game,0.6826792359352112
translation,268,112,experimental-setup,tokens,of,textual inputs,tokens of textual inputs,0.5683239698410034
translation,268,112,experimental-setup,textual inputs,are,lower -cased,textual inputs are lower -cased,0.5989006161689758
translation,268,112,experimental-setup,tokenized,using,bytepair-encoding ( bpe ),tokenized using bytepair-encoding ( bpe ),0.7126895785331726
translation,268,112,experimental-setup,experimental setup,has,tokens,experimental setup has tokens,0.5007272958755493
translation,268,113,experimental-setup,seq-to-seq model,uses,300 - dimensional word embeddings,seq-to-seq model uses 300 - dimensional word embeddings,0.5726695656776428
translation,268,113,experimental-setup,300 - dimensional word embeddings,initialized with,glove,300 - dimensional word embeddings initialized with glove,0.7449949383735657
translation,268,113,experimental-setup,300 - dimensional word embeddings,with,0.1 dropout ratio,300 - dimensional word embeddings with 0.1 dropout ratio,0.5956906080245972
translation,268,113,experimental-setup,"[ 1 , 2 ] layers",of,"[ 256 , 512 ] - dimensional uni/bi-directional lstms","[ 1 , 2 ] layers of [ 256 , 512 ] - dimensional uni/bi-directional lstms",0.5484210848808289
translation,268,113,experimental-setup,"[ 1 , 2 ] layers",of,0.1 dropout ratio,"[ 1 , 2 ] layers of 0.1 dropout ratio",0.5754992961883545
translation,268,113,experimental-setup,"[ 1 , 2 ] layers",of,soft attention,"[ 1 , 2 ] layers of soft attention",0.5471727252006531
translation,268,113,experimental-setup,"[ 1 , 2 ] layers",with,0.1 dropout ratio,"[ 1 , 2 ] layers with 0.1 dropout ratio",0.597348153591156
translation,268,113,experimental-setup,experimental setup,has,seq-to-seq model,experimental setup has seq-to-seq model,0.541225016117096
translation,268,114,experimental-setup,decoding,use,beam search,decoding use beam search,0.6366719603538513
translation,268,114,experimental-setup,decoding,choose,maximum likelihood output,decoding choose maximum likelihood output,0.7355210185050964
translation,268,114,experimental-setup,beam search,with,beam of size 3,beam search with beam of size 3,0.6784133911132812
translation,268,114,experimental-setup,experimental setup,At,decoding,experimental setup At decoding,0.5393803119659424
translation,268,115,experimental-setup,all previous dialogue turns,including,seeker 's and expert 's replies,all previous dialogue turns including seeker 's and expert 's replies,0.6636207699775696
translation,268,115,experimental-setup,seeker 's and expert 's replies,concatenated as,input,seeker 's and expert 's replies concatenated as input,0.699756383895874
translation,268,115,experimental-setup,input,to,models,input to models,0.5779573917388916
translation,268,115,experimental-setup,each turn,has,initial movie text,each turn has initial movie text,0.5633696913719177
translation,268,115,experimental-setup,experimental setup,For,each turn,experimental setup For each turn,0.6284903883934021
translation,268,117,experimental-setup,softmax layers,is,"[ 1 , 2 ]","softmax layers is [ 1 , 2 ]",0.4982052743434906
translation,268,117,experimental-setup,experimental setup,number of,softmax layers,experimental setup number of softmax layers,0.6506301164627075
translation,268,119,experimental-setup,movie textual description,truncated at,50 words,movie textual description truncated at 50 words,0.34477663040161133
translation,268,119,experimental-setup,experimental setup,has,movie textual description,experimental setup has movie textual description,0.47474443912506104
translation,268,120,experimental-setup,annealing,to balance,different supervised objectives,annealing to balance different supervised objectives,0.6032333374023438
translation,268,120,experimental-setup,annealing,optimize,generate loss,annealing optimize generate loss,0.8084872961044312
translation,268,120,experimental-setup,annealing,optimize,gradually increase,annealing optimize gradually increase,0.7844712734222412
translation,268,120,experimental-setup,generate loss,for,first 5 epochs,generate loss for first 5 epochs,0.5814271569252014
translation,268,120,experimental-setup,weights,for,predict and decide losses,weights for predict and decide losses,0.6286238431930542
translation,268,120,experimental-setup,gradually increase,has,weights,gradually increase has weights,0.5813056230545044
translation,268,120,experimental-setup,experimental setup,use,annealing,experimental setup use annealing,0.5827125310897827
translation,268,122,experimental-setup,experimental setup,implemented using,pytorch and par-lai,experimental setup implemented using pytorch and par-lai,0.6758129000663757
translation,268,6,experiments,goal-driven recommendation dialogue dataset ( gorecdial ),consists of,"9,125 dialogue games","goal-driven recommendation dialogue dataset ( gorecdial ) consists of 9,125 dialogue games",0.5640603303909302
translation,268,6,experiments,goal-driven recommendation dialogue dataset ( gorecdial ),consists of,"81,260 conversation turns","goal-driven recommendation dialogue dataset ( gorecdial ) consists of 81,260 conversation turns",0.5691549777984619
translation,268,6,experiments,"81,260 conversation turns",between,pairs of human workers,"81,260 conversation turns between pairs of human workers",0.595626175403595
translation,268,6,experiments,pairs of human workers,recommending,movies to each other,pairs of human workers recommending movies to each other,0.6428108811378479
translation,268,22,experiments,corpus,of,goal-driven dialogues,corpus of goal-driven dialogues,0.5385040640830994
translation,268,22,experiments,corpus,show,models,corpus show models,0.6286725401878357
translation,268,22,experiments,goal-driven dialogues,grounded in,real user movie preferences,goal-driven dialogues grounded in real user movie preferences,0.6754835247993469
translation,268,116,experiments,supervised and bot-play learning,use,"adam ( kingma and ba , 2015 ) optimizer","supervised and bot-play learning use adam ( kingma and ba , 2015 ) optimizer",0.6047264337539673
translation,268,116,experiments,supervised and bot-play learning,with,0.1 gradient clipping,supervised and bot-play learning with 0.1 gradient clipping,0.587223470211029
translation,268,116,experiments,"adam ( kingma and ba , 2015 ) optimizer",with,batch size,"adam ( kingma and ba , 2015 ) optimizer with batch size",0.616158127784729
translation,268,116,experiments,"adam ( kingma and ba , 2015 ) optimizer",with,0.1 gradient clipping,"adam ( kingma and ba , 2015 ) optimizer with 0.1 gradient clipping",0.5694693326950073
translation,268,116,experiments,"adam ( kingma and ba , 2015 ) optimizer",with,0.1 gradient clipping,"adam ( kingma and ba , 2015 ) optimizer with 0.1 gradient clipping",0.5694693326950073
translation,268,116,experiments,learning rates,of,"[ 0.1 , 0.01 , 0.001 ]","learning rates of [ 0.1 , 0.01 , 0.001 ]",0.5729448795318604
translation,268,116,experiments,learning rates,of,0.1 gradient clipping,learning rates of 0.1 gradient clipping,0.546316921710968
translation,268,116,experiments,learning rates,with,0.1 gradient clipping,learning rates with 0.1 gradient clipping,0.5988903045654297
translation,268,116,experiments,batch size,has,32,batch size has 32,0.630264163017273
translation,268,10,model,our models,on,simulated bot-bot conversations,our models on simulated bot-bot conversations,0.5582584142684937
translation,268,10,model,simulated bot-bot conversations,between,two paired pre-trained models ( bot - play ),simulated bot-bot conversations between two paired pre-trained models ( bot - play ),0.6439340114593506
translation,268,10,model,model,finetune,our models,model finetune our models,0.650771975517273
translation,268,23,model,training,conducted in,two stages,training conducted in two stages,0.7343807816505432
translation,268,23,model,supervised phase,trains,model,supervised phase trains model,0.7786290645599365
translation,268,23,model,supervised phase,trains,model,supervised phase trains model,0.7786290645599365
translation,268,23,model,model,to mimic,human behavior,model to mimic human behavior,0.6528815031051636
translation,268,23,model,human behavior,on,task,human behavior on task,0.5949397683143616
translation,268,23,model,bot-play phase,improves,goal-directed strategy,bot-play phase improves goal-directed strategy,0.636157751083374
translation,268,23,model,goal-directed strategy,of,model,goal-directed strategy of model,0.5398796200752258
translation,268,23,model,two stages,has,supervised phase,two stages has supervised phase,0.5989324450492859
translation,268,23,model,model,has,training,model has training,0.5495393872261047
translation,268,118,model,all previous dialogue utterances,from,seeker and the expert,all previous dialogue utterances from seeker and the expert,0.5906651020050049
translation,268,118,model,input text,to,other modules,input text to other modules,0.5718207955360413
translation,268,118,model,each turn,has,initial movie description,each turn has initial movie description,0.5421603918075562
translation,268,118,model,model,For,each turn,model For each turn,0.6544380784034729
translation,268,11,results,our experiments,show,models,our experiments show models,0.6826819777488708
translation,268,11,results,our experiments,show,models,our experiments show models,0.6826819777488708
translation,268,11,results,models,finetuned with,bot-play,models finetuned with bot-play,0.7342802882194519
translation,268,11,results,models,finetuned with,bot-play,models finetuned with bot-play,0.7342802882194519
translation,268,11,results,models,rated as,more consistent,models rated as more consistent,0.6642272472381592
translation,268,11,results,models,trained without,bot-play,models trained without bot-play,0.7253748178482056
translation,268,11,results,bot-play,learn,improved dialogue strategies,bot-play learn improved dialogue strategies,0.6407045722007751
translation,268,11,results,dialogue goal,when paired with,human,dialogue goal when paired with human,0.6298812627792358
translation,268,11,results,more often,when paired with,human,more often when paired with human,0.7165049910545349
translation,268,11,results,more consistent,by,humans,more consistent by humans,0.5950307250022888
translation,268,11,results,more consistent,compared to,models,more consistent compared to models,0.6452895402908325
translation,268,11,results,models,trained without,bot-play,models trained without bot-play,0.7253748178482056
translation,268,11,results,dialogue goal,has,more often,dialogue goal has more often,0.6099050641059875
translation,268,143,results,our prediction predict modules,show,significant improvements,our prediction predict modules show significant improvements,0.6787937879562378
translation,268,143,results,significant improvements,over,recommendation baselines,significant improvements over recommendation baselines,0.7283550500869751
translation,268,143,results,significant improvements,on,per-turn and per-chat recommendations,significant improvements on per-turn and per-chat recommendations,0.5834095478057861
translation,268,143,results,52 %,on,turn@1,52 % on turn@1,0.6369273066520691
translation,268,143,results,34 %,on,turn@3,34 % on turn@3,0.6346527338027954
translation,268,143,results,recommendation -only models,has,our prediction predict modules,recommendation -only models has our prediction predict modules,0.5918867588043213
translation,268,143,results,results,Compared to,recommendation -only models,results Compared to recommendation -only models,0.6617789268493652
translation,268,145,results,decide module,yields,additional improvements,decide module yields additional improvements,0.7248111963272095
translation,268,145,results,additional improvements,over,predict model,additional improvements over predict model,0.7004665732383728
translation,268,145,results,predict model,in,generation and recommendation,predict model in generation and recommendation,0.560350775718689
translation,268,145,results,predict model,with,67.6 % decision accuracy,predict model with 67.6 % decision accuracy,0.6096788048744202
translation,268,145,results,results,has,decide module,results has decide module,0.623164713382721
translation,268,146,results,our proposed models,show,comparable performance,our proposed models show comparable performance,0.6669789552688599
translation,268,146,results,comparable performance,as,ir baseline models,comparable performance as ir baseline models,0.531419575214386
translation,268,146,results,results,has,our proposed models,results has our proposed models,0.5751316547393799
translation,268,147,results,+ decide model,improves on,f1 generation score,+ decide model improves on f1 generation score,0.7369005084037781
translation,268,147,results,results,has,+ decide model,results has + decide model,0.6119722127914429
translation,268,148,results,most metrics,of,supervised evaluation,most metrics of supervised evaluation,0.5285620093345642
translation,268,148,results,slightly hurts,has,most metrics,slightly hurts has most metrics,0.5858182311058044
translation,268,173,results,self-supervised model,fine-tuned by,seeker models,self-supervised model fine-tuned by seeker models,0.7546457052230835
translation,268,173,results,self-supervised model,shows,significant improvements,self-supervised model shows significant improvements,0.6699129343032837
translation,268,173,results,significant improvements,in,gamerelated measures,significant improvements in gamerelated measures,0.44648709893226624
translation,268,173,results,supervised model,has,self-supervised model,supervised model has self-supervised model,0.5875252485275269
translation,268,173,results,results,Compared to,supervised model,results Compared to supervised model,0.716423511505127
translation,268,174,results,bert -r model,shows,+ 27.7 % improvement,bert -r model shows + 27.7 % improvement,0.6600486636161804
translation,268,174,results,+ 27.7 % improvement,in,goal success ratio,+ 27.7 % improvement in goal success ratio,0.5163530707359314
translation,268,174,results,results,has,bert -r model,results has bert -r model,0.5341694951057434
translation,268,176,results,dialogue games,with,human seeker players,dialogue games with human seeker players,0.6168861985206604
translation,268,176,results,dialogue games,has,bot-play model,dialogue games has bot-play model,0.5859035849571228
translation,268,176,results,human seeker players,has,bot-play model,human seeker players has bot-play model,0.5759758949279785
translation,268,176,results,bot-play model,has,outperforms,bot-play model has outperforms,0.6395300030708313
translation,268,176,results,outperforms,has,supervised one,outperforms has supervised one,0.5982592105865479
translation,268,176,results,results,In,dialogue games,results In dialogue games,0.4832950532436371
translation,268,197,results,linearly increases,as,more dialogue context,linearly increases as more dialogue context,0.5457069873809814
translation,268,197,results,recommendation performance,has,linearly increases,recommendation performance has linearly increases,0.5902032256126404
translation,269,164,baselines,scoring model,used in,qa,scoring model used in qa,0.7222525477409363
translation,269,164,baselines,ir,has,basic tf - idf match model,ir has basic tf - idf match model,0.5399636030197144
translation,269,164,baselines,supervised embedding model ( sem ),has,supervised word embedding model,supervised embedding model ( sem ) has supervised word embedding model,0.5554858446121216
translation,269,164,baselines,memory networks ( memn2n ),has,scoring model,memory networks ( memn2n ) has scoring model,0.5673335194587708
translation,269,174,hyperparameters,word embeddings,are,randomly initialized,word embeddings are randomly initialized,0.5598148107528687
translation,269,174,hyperparameters,hyperparameters,has,word embeddings,hyperparameters has word embeddings,0.4784160554409027
translation,269,175,hyperparameters,dimensions,of,word embeddings and gru hidden units,dimensions of word embeddings and gru hidden units,0.5292198061943054
translation,269,175,hyperparameters,dimensions,are,32,dimensions are 32,0.6359957456588745
translation,269,175,hyperparameters,word embeddings and gru hidden units,are,32,word embeddings and gru hidden units are 32,0.5523868799209595
translation,269,175,hyperparameters,hyperparameters,has,dimensions,hyperparameters has dimensions,0.5247315764427185
translation,269,176,hyperparameters,size,of,latent variable z,size of latent variable z,0.5983812212944031
translation,269,176,hyperparameters,latent variable z,is,20,latent variable z is 20,0.630005419254303
translation,269,176,hyperparameters,hyperparameters,has,size,hyperparameters has size,0.5313372611999512
translation,269,177,hyperparameters,repetition time k,is,50,repetition time k is 50,0.6652846336364746
translation,269,177,hyperparameters,uncertainty estimation,has,repetition time k,uncertainty estimation has repetition time k,0.5646266937255859
translation,269,177,hyperparameters,hyperparameters,In,uncertainty estimation,hyperparameters In uncertainty estimation,0.4958474338054657
translation,269,178,hyperparameters,response probability threshold ? 2,set to,0.3,response probability threshold ? 2 set to 0.3,0.681513249874115
translation,269,179,hyperparameters,number of monte carlo sampling,is,50,number of monte carlo sampling is 50,0.5983380079269409
translation,269,179,hyperparameters,online learning,has,number of monte carlo sampling,online learning has number of monte carlo sampling,0.5501173734664917
translation,269,179,hyperparameters,hyperparameters,In,online learning,hyperparameters In online learning,0.4611380100250244
translation,269,180,hyperparameters,learning rate,is,0.001,learning rate is 0.001,0.5655806064605713
translation,269,180,hyperparameters,adam optimizer,has,learning rate,adam optimizer has learning rate,0.5073457360267639
translation,269,180,hyperparameters,hyperparameters,use,adam optimizer,hyperparameters use adam optimizer,0.6090166568756104
translation,269,181,hyperparameters,all models,in,mini-batches,all models in mini-batches,0.5571229457855225
translation,269,181,hyperparameters,mini-batches,of size,32,mini-batches of size 32,0.7157005667686462
translation,269,181,hyperparameters,hyperparameters,train,all models,hyperparameters train all models,0.6430407166481018
translation,269,7,model,novel incremental learning framework,to design,task - oriented dialogue systems,novel incremental learning framework to design task - oriented dialogue systems,0.5921409726142883
translation,269,7,model,short incremental dialogue system ( ids ),pre-defining,exhaustive list of user needs,short incremental dialogue system ( ids ) pre-defining exhaustive list of user needs,0.7408878207206726
translation,269,7,model,model,propose,novel incremental learning framework,model propose novel incremental learning framework,0.6377102136611938
translation,269,8,model,uncertainty estimation module,to evaluate,confidence,uncertainty estimation module to evaluate confidence,0.6849804520606995
translation,269,8,model,confidence,has,of giving correct responses,confidence has of giving correct responses,0.5820999145507812
translation,269,8,model,model,introduce,uncertainty estimation module,model introduce uncertainty estimation module,0.6639293432235718
translation,269,31,model,model,propose,novel incremental dialogue system ( ids ),model propose novel incremental dialogue system ( ids ),0.6648259162902832
translation,269,208,results,ids ?,achieves,better performance,ids ? achieves better performance,0.7040495872497559
translation,269,208,results,better performance,with,much less training data,better performance with much less training data,0.6392626166343689
translation,269,208,results,all baselines,has,ids ?,all baselines has ids ?,0.6772468090057373
translation,269,208,results,results,compared with,all baselines,results compared with all baselines,0.6300603747367859
translation,270,7,model,opposite behavior aware framework,for,policy learning,opposite behavior aware framework for policy learning,0.6014020442962646
translation,270,7,model,policy learning,in,goal-oriented dialogues,policy learning in goal-oriented dialogues,0.5378904342651367
translation,270,7,model,model,propose,opposite behavior aware framework,model propose opposite behavior aware framework,0.695660412311554
translation,270,28,model,new dialogue policy learning method,with,opposite agent awareness ( oppa ),new dialogue policy learning method with opposite agent awareness ( oppa ),0.6465561389923096
translation,270,28,model,new dialogue policy learning method,where,agent,new dialogue policy learning method where agent,0.5821220874786377
translation,270,28,model,agent,maintains,explicit modeling,agent maintains explicit modeling,0.7090641260147095
translation,270,28,model,explicit modeling,of,opposite agent or user,explicit modeling of opposite agent or user,0.5756086707115173
translation,270,28,model,explicit modeling,for facilitating,own policy learning,explicit modeling for facilitating own policy learning,0.7284535765647888
translation,270,28,model,model,propose,new dialogue policy learning method,model propose new dialogue policy learning method,0.6699908375740051
translation,270,29,model,estimated user model,not utilized as,simulator,estimated user model not utilized as simulator,0.7240896821022034
translation,270,29,model,estimated user model,as,auxiliary component,estimated user model as auxiliary component,0.5202646851539612
translation,270,29,model,simulator,to produce,simulated experiences,simulator to produce simulated experiences,0.7285157442092896
translation,270,29,model,auxiliary component,of,target agent 's policy,auxiliary component of target agent 's policy,0.5624033808708191
translation,270,29,model,target agent 's policy,to guide,next action,target agent 's policy to guide next action,0.6553467512130737
translation,271,96,ablation-analysis,gating based mechanism,to augment,user utterance,gating based mechanism to augment user utterance,0.6978343725204468
translation,271,96,ablation-analysis,user utterance,with,relevant information,user utterance with relevant information,0.6014871001243591
translation,271,96,ablation-analysis,relevant information,from,previous system utterance,relevant information from previous system utterance,0.5548337697982788
translation,271,96,ablation-analysis,joint goal accuracy,by,1.0 %,joint goal accuracy by 1.0 %,0.532885730266571
translation,271,96,ablation-analysis,ablation analysis,has,gating based mechanism,ablation analysis has gating based mechanism,0.5494407415390015
translation,271,102,ablation-analysis,relevant context,results in,significant reduction,relevant context results in significant reduction,0.6269223690032959
translation,271,102,ablation-analysis,significant reduction,in,number of learnable parameters,significant reduction in number of learnable parameters,0.514968991279602
translation,271,102,ablation-analysis,number of learnable parameters,in,model,number of learnable parameters in model,0.5082216262817383
translation,271,102,ablation-analysis,ablation analysis,utilization of,relevant context,ablation analysis utilization of relevant context,0.6994472742080688
translation,271,86,hyperparameters,hyperparameters,use,pretrained glove word embeddings,hyperparameters use pretrained glove word embeddings,0.5339311361312866
translation,271,89,hyperparameters,bi-lstms,use,200 hidden dimensions,bi-lstms use 200 hidden dimensions,0.5642656683921814
translation,271,89,hyperparameters,hyperparameters,Each of,bi-lstms,hyperparameters Each of bi-lstms,0.5706532597541809
translation,271,90,hyperparameters,models,trained using,"adam optimizer ( kingma and ba , 2014 )","models trained using adam optimizer ( kingma and ba , 2014 )",0.7549477219581604
translation,271,90,hyperparameters,"adam optimizer ( kingma and ba , 2014 )",with,initial learning rate,"adam optimizer ( kingma and ba , 2014 ) with initial learning rate",0.5920190811157227
translation,271,90,hyperparameters,initial learning rate,of,0.001,initial learning rate of 0.001,0.5703312754631042
translation,271,90,hyperparameters,hyperparameters,trained using,"adam optimizer ( kingma and ba , 2014 )","hyperparameters trained using adam optimizer ( kingma and ba , 2014 )",0.7194001078605652
translation,271,90,hyperparameters,hyperparameters,has,models,hyperparameters has models,0.5447477102279663
translation,271,91,hyperparameters,dropout rate,set to,0.2,dropout rate set to 0.2,0.6595621705055237
translation,271,91,hyperparameters,0.2,for,all bilstm modules and the embedding layer,0.2 for all bilstm modules and the embedding layer,0.5767470002174377
translation,271,91,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,271,92,hyperparameters,maximum of 100 epochs,with,batch size,maximum of 100 epochs with batch size,0.6039434671401978
translation,271,92,hyperparameters,batch size,of,50,batch size of 50,0.6921922564506531
translation,271,92,hyperparameters,hyperparameters,trained for,maximum of 100 epochs,hyperparameters trained for maximum of 100 epochs,0.6913177967071533
translation,271,93,hyperparameters,validation data,used for,early stopping,validation data used for early stopping,0.6787852048873901
translation,271,93,hyperparameters,validation data,used for,hyperparameter tuning,validation data used for hyperparameter tuning,0.6731724739074707
translation,271,93,hyperparameters,hyperparameters,has,validation data,hyperparameters has validation data,0.5335173606872559
translation,271,8,model,novel framework,for,dst,novel framework for dst,0.66347736120224
translation,271,8,model,novel framework,identifies,relevant historical context,novel framework identifies relevant historical context,0.6345636248588562
translation,271,8,model,relevant historical context,by referring to,past utterances,relevant historical context by referring to past utterances,0.6341726779937744
translation,271,8,model,past utterances,where,particular slot-value changes,past utterances where particular slot-value changes,0.555512011051178
translation,271,8,model,system utterance,to identify,relevant context,system utterance to identify relevant context,0.6549232602119446
translation,271,8,model,model,propose,novel framework,model propose novel framework,0.720393180847168
translation,271,9,model,current user utterance and the most recent system utterance,to determine,relevance,current user utterance and the most recent system utterance to determine relevance,0.6478973031044006
translation,271,9,model,relevance,of,system utterance,relevance of system utterance,0.6430264711380005
translation,271,9,model,model,use,current user utterance and the most recent system utterance,model use current user utterance and the most recent system utterance,0.6287817358970642
translation,271,12,model,dst,to identify,set of goals,dst to identify set of goals,0.6624494791030884
translation,271,12,model,requests,represented as,slot-value pairs,requests represented as slot-value pairs,0.6341832876205444
translation,271,12,model,model,has,dst,model has dst,0.597681999206543
translation,271,35,results,our novel model,discerns,important details,our novel model discerns important details,0.7177952527999878
translation,271,35,results,our novel model,discerns,previous system utterance,our novel model discerns previous system utterance,0.7371704578399658
translation,271,35,results,important details,in,non-adjacent dialogue turns,important details in non-adjacent dialogue turns,0.5180469751358032
translation,271,35,results,previous system utterance,from,dialog history,previous system utterance from dialog history,0.5466068983078003
translation,271,35,results,dialog history,able to improve,"previous state - of- the - art glad ( zhong et al. , 2018 ) model","dialog history able to improve previous state - of- the - art glad ( zhong et al. , 2018 ) model",0.6476255059242249
translation,271,35,results,results,has,our novel model,results has our novel model,0.5586313605308533
translation,271,36,results,simple self-attention based bilstm model,using,one - third,simple self-attention based bilstm model using one - third,0.6485487222671509
translation,271,36,results,one - third,of,number of parameters,one - third of number of parameters,0.6302044987678528
translation,271,36,results,one - third,as,glad,one - third as glad,0.5824907422065735
translation,271,36,results,glad,by identifying and incorporating,relevant context,glad by identifying and incorporating relevant context,0.7056543827056885
translation,271,36,results,glad,by identifying and incorporating,relevant context,glad by identifying and incorporating relevant context,0.7056543827056885
translation,271,36,results,simple self-attention based bilstm model,has,outperforms,simple self-attention based bilstm model has outperforms,0.5900751352310181
translation,271,36,results,outperforms,has,glad,outperforms has glad,0.6362531781196594
translation,271,36,results,results,empirically show,simple self-attention based bilstm model,results empirically show simple self-attention based bilstm model,0.7454321980476379
translation,271,94,results,appropriate context,from,previous turns,appropriate context from previous turns,0.6046363115310669
translation,271,94,results,appropriate context,improves,overall performance,appropriate context improves overall performance,0.7193230986595154
translation,271,94,results,overall performance,of,dialogue state tracker,overall performance of dialogue state tracker,0.5300130248069763
translation,271,94,results,dialogue state tracker,on,joint goal and turn request accuracies,dialogue state tracker on joint goal and turn request accuracies,0.49969229102134705
translation,271,94,results,joint goal and turn request accuracies,on,woz 2.0 dataset,joint goal and turn request accuracies on woz 2.0 dataset,0.48840245604515076
translation,271,95,results,relevant referential utterances,to identify,implicitly mentioned slot-value,relevant referential utterances to identify implicitly mentioned slot-value,0.6044905185699463
translation,271,95,results,implicitly mentioned slot-value,improves,accuracy,implicitly mentioned slot-value improves accuracy,0.6806069016456604
translation,271,95,results,accuracy,of,global bilstm based gle model,accuracy of global bilstm based gle model,0.5083035230636597
translation,271,95,results,global bilstm based gle model,on,joint goal task,global bilstm based gle model on joint goal task,0.47968462109565735
translation,271,95,results,joint goal task,by,2.4 %,joint goal task by 2.4 %,0.5497435927391052
translation,271,95,results,results,incorporating,relevant referential utterances,results incorporating relevant referential utterances,0.5957269072532654
translation,271,97,results,joint goal and request accuracy,of,global bilstm based gle model,joint goal and request accuracy of global bilstm based gle model,0.5107094049453735
translation,271,97,results,global bilstm based gle model,by,3.4 % and 0.2 %,global bilstm based gle model by 3.4 % and 0.2 %,0.5455492734909058
translation,271,98,results,performance,on,multiwoz 2.0 dataset,performance on multiwoz 2.0 dataset,0.5151218771934509
translation,271,98,results,overall improvement,of,2.36 % and 1.77 %,overall improvement of 2.36 % and 1.77 %,0.5366753935813904
translation,271,98,results,2.36 % and 1.77 %,on,joint goal and turn inform accuracies,2.36 % and 1.77 % on joint goal and turn inform accuracies,0.4833296537399292
translation,271,98,results,both referential context and fused system utterance,has,proportionally improve,both referential context and fused system utterance has proportionally improve,0.5905472636222839
translation,271,98,results,proportionally improve,has,performance,proportionally improve has performance,0.5678238868713379
translation,271,99,results,performances,of,all models,performances of all models,0.5580928921699524
translation,271,99,results,all models,on,mul-tiwoz 2.0,all models on mul-tiwoz 2.0,0.5885908007621765
translation,271,99,results,mul-tiwoz 2.0,are,significantly inferior,mul-tiwoz 2.0 are significantly inferior,0.6055535674095154
translation,271,99,results,significantly inferior,compared to,woz 2.0,significantly inferior compared to woz 2.0,0.6570821404457092
translation,271,99,results,woz 2.0,owing to,higher complexity,woz 2.0 owing to higher complexity,0.6721038222312927
translation,271,99,results,higher complexity,with,richer and longer utterances,higher complexity with richer and longer utterances,0.6541793346405029
translation,271,99,results,considerably more slotvalues,in,former dataset,considerably more slotvalues in former dataset,0.5115624070167542
translation,271,99,results,results,has,performances,results has performances,0.5711642503738403
translation,272,6,model,model,propose,method,model propose method,0.6280754208564758
translation,272,118,results,s c+r,achieved,highest correlation,s c+r achieved highest correlation,0.7271917462348938
translation,272,118,results,highest correlation,with,human scores,highest correlation with human scores,0.6351457238197327
translation,272,165,results,model,trained on,data,model trained on data,0.7572044730186462
translation,272,165,results,model,trained on,non-filtered data,model trained on non-filtered data,0.7838608622550964
translation,272,165,results,model,trained on,non-filtered data,model trained on non-filtered data,0.7838608622550964
translation,272,165,results,data,filtered using,proposed method s c+r,data filtered using proposed method s c+r,0.7429304718971252
translation,272,165,results,proposed method s c+r,produced,"more than three times as many distinct { 1 , 2 } - grams","proposed method s c+r produced more than three times as many distinct { 1 , 2 } - grams",0.6356368660926819
translation,272,165,results,"more than three times as many distinct { 1 , 2 } - grams",as,model,"more than three times as many distinct { 1 , 2 } - grams as model",0.5881125926971436
translation,272,165,results,model,trained on,non-filtered data,model trained on non-filtered data,0.7838608622550964
translation,272,165,results,results,has,model,results has model,0.5339115858078003
translation,272,166,results,model,trained on,non-filtered data,model trained on non-filtered data,0.7838608622550964
translation,272,166,results,model,achieving,highest percentage of acceptable responses,model achieving highest percentage of acceptable responses,0.696891188621521
translation,272,166,results,non-filtered data,in,human evaluation,non-filtered data in human evaluation,0.5255523324012756
translation,272,166,results,highest percentage of acceptable responses,of,85 %,highest percentage of acceptable responses of 85 %,0.5838006734848022
translation,272,166,results,outperformed,has,model,outperformed has model,0.6591501832008362
translation,272,166,results,results,has,outperformed,results has outperformed,0.636634886264801
translation,272,167,results,results,of,s c+r,results of s c+r,0.5989983677864075
translation,272,167,results,s c+r,better than,other baselines,s c+r better than other baselines,0.7672585248947144
translation,272,167,results,results,of,s c+r,results of s c+r,0.5989983677864075
translation,272,167,results,results,has,results,results has results,0.48582205176353455
translation,272,193,results,filtered data,generated by,s c+r,filtered data generated by s c+r,0.6884880661964417
translation,272,193,results,s c+r,provided,best results,s c+r provided best results,0.6613621115684509
translation,272,193,results,best results,in terms of,almost all the metrics,best results in terms of almost all the metrics,0.651389479637146
translation,272,193,results,best results,including,human evaluation,best results including human evaluation,0.6772653460502625
translation,272,193,results,results,has,filtered data,results has filtered data,0.5765068531036377
translation,273,208,ablation-analysis,joint optimization,boosts,performance,joint optimization boosts performance,0.6914966702461243
translation,273,208,ablation-analysis,of both these tasks,boosts,performance,of both these tasks boosts performance,0.6575769186019897
translation,273,208,ablation-analysis,performance,of,dac,performance of dac,0.6384608745574951
translation,273,208,ablation-analysis,joint optimization,has,of both these tasks,joint optimization has of both these tasks,0.5432084202766418
translation,273,208,ablation-analysis,ablation analysis,shows,joint optimization,ablation analysis shows joint optimization,0.6332972049713135
translation,273,146,baselines,dac features,tuned in accordance with,emotion features,dac features tuned in accordance with emotion features,0.6003542542457581
translation,273,34,experiments,dac,assisted by,emotion recognition ( er ),dac assisted by emotion recognition ( er ),0.7202597260475159
translation,273,231,experiments,of dialogues,collected from,various open-source datasets,of dialogues collected from various open-source datasets,0.638211190700531
translation,273,231,experiments,various open-source datasets,manually annotated with,das,various open-source datasets manually annotated with das,0.7714143991470337
translation,273,231,experiments,emotion - rich videos,has,of dialogues,emotion - rich videos has of dialogues,0.544518232345581
translation,273,128,hyperparameters,"pretrained glove ( pennington et al. , 2014 ) embeddings",of,dimension,"pretrained glove ( pennington et al. , 2014 ) embeddings of dimension",0.5181398987770081
translation,273,128,hyperparameters,"pretrained glove ( pennington et al. , 2014 ) embeddings",to obtain,representation of words,"pretrained glove ( pennington et al. , 2014 ) embeddings to obtain representation of words",0.5351737141609192
translation,273,128,hyperparameters,representation of words,as,word vectors,representation of words as word vectors,0.5709871649742126
translation,273,128,hyperparameters,textual features,has,"pretrained glove ( pennington et al. , 2014 ) embeddings","textual features has pretrained glove ( pennington et al. , 2014 ) embeddings",0.49193429946899414
translation,273,128,hyperparameters,dimension,has,300,dimension has 300,0.6396656632423401
translation,273,128,hyperparameters,hyperparameters,extract,textual features,hyperparameters extract textual features,0.6541652083396912
translation,273,186,hyperparameters,bi-lstm layer,with,200 memory cells,bi-lstm layer with 200 memory cells,0.634842038154602
translation,273,186,hyperparameters,bi-lstm layer,followed by,dropout rate,bi-lstm layer followed by dropout rate,0.6150931715965271
translation,273,186,hyperparameters,200 memory cells,followed by,dropout rate,200 memory cells followed by dropout rate,0.6749757528305054
translation,273,186,hyperparameters,dropout rate,of,0.1,dropout rate of 0.1,0.5755324363708496
translation,273,186,hyperparameters,textual modality,has,bi-lstm layer,textual modality has bi-lstm layer,0.5473753809928894
translation,273,186,hyperparameters,hyperparameters,encoding,textual modality,hyperparameters encoding textual modality,0.7358512282371521
translation,273,187,hyperparameters,fully - connected layer,dimension,300,fully - connected layer dimension 300,0.7195190191268921
translation,273,187,hyperparameters,fully - connected layer,used in,all the subsequent layers,fully - connected layer used in all the subsequent layers,0.6390058994293213
translation,273,187,hyperparameters,hyperparameters,has,fully - connected layer,hyperparameters has fully - connected layer,0.48157066106796265
translation,273,189,hyperparameters,hyperparameters,has,categorical crossentropy loss function,hyperparameters has categorical crossentropy loss function,0.5055696368217468
translation,273,190,hyperparameters,learning rate,of,0.01,learning rate of 0.01,0.6152973175048828
translation,273,190,hyperparameters,0.01,found to be,optimum,0.01 found to be optimum,0.6129844188690186
translation,273,190,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,273,191,hyperparameters,adam optimizer,used in,final experimental setting,adam optimizer used in final experimental setting,0.64727383852005
translation,273,191,hyperparameters,hyperparameters,has,adam optimizer,hyperparameters has adam optimizer,0.5012347102165222
translation,273,12,model,"attention based ( self , inter-modal , inter-task ) multi-modal , multi-task deep neural network ( dnn )",for,joint learning,"attention based ( self , inter-modal , inter-task ) multi-modal , multi-task deep neural network ( dnn ) for joint learning",0.5751792788505554
translation,273,12,model,joint learning,of,das and emotions,joint learning of das and emotions,0.5882591009140015
translation,273,33,model,emotion,on,identification of das,emotion on identification of das,0.5801947116851807
translation,273,33,model,identification of das,by utilizing,com-bination,identification of das by utilizing com-bination,0.6728225946426392
translation,273,33,model,com-bination,of,text,com-bination of text,0.653778612613678
translation,273,33,model,com-bination,of,vocal modulations and facial expressions,com-bination of vocal modulations and facial expressions,0.5658566951751709
translation,273,33,model,com-bination,for,task - independent conversations,com-bination for task - independent conversations,0.6175819635391235
translation,273,33,model,vocal modulations and facial expressions,for,task - independent conversations,vocal modulations and facial expressions for task - independent conversations,0.5632171630859375
translation,273,33,model,model,influence of,emotion,model influence of emotion,0.6526365280151367
translation,273,147,model,features,for,er task,features for er task,0.6384634375572205
translation,273,147,model,features,regulated by,learning of da features,features regulated by learning of da features,0.6439595818519592
translation,273,147,model,er task,regulated by,learning of da features,er task regulated by learning of da features,0.6621630787849426
translation,273,147,model,model,learns,features,model learns features,0.6743517518043518
translation,273,188,model,first and the second channel,contain,12 and 10 output neurons,first and the second channel contain 12 and 10 output neurons,0.5722388625144958
translation,273,188,model,12 and 10 output neurons,for,da and the emotion tags,12 and 10 output neurons for da and the emotion tags,0.6622732281684875
translation,273,188,model,model,has,first and the second channel,model has first and the second channel,0.5779181718826294
translation,273,200,results,textual modality,provides,best results,textual modality provides best results,0.6451455950737
translation,273,200,results,best results,amongst,uni-modal variants,best results amongst uni-modal variants,0.5626131296157837
translation,273,200,results,results,has,textual modality,results has textual modality,0.533868670463562
translation,273,201,results,audio and visual features,individually improves,uni-modal baseline,audio and visual features individually improves uni-modal baseline,0.7177311182022095
translation,273,202,results,combination of visual and textual features,achieves,best score,combination of visual and textual features achieves best score,0.6573787927627563
translation,273,202,results,best score,throughout,all the combinations of the dataset,best score throughout all the combinations of the dataset,0.5722683072090149
translation,273,202,results,results,has,combination of visual and textual features,results has combination of visual and textual features,0.5529935956001282
translation,273,203,results,tri-modal variant,not able to attain,best score,tri-modal variant not able to attain best score,0.6317696571350098
translation,273,203,results,results,has,tri-modal variant,results has tri-modal variant,0.5724025964736938
translation,273,206,results,multi-task variant,performs,consistently well,multi-task variant performs consistently well,0.5969120264053345
translation,273,206,results,consistently well,throughout,all the experiments,consistently well throughout all the experiments,0.6453869938850403
translation,273,206,results,consistently well,compared to,single task dac variant,consistently well compared to single task dac variant,0.6768538951873779
translation,273,207,results,emotion,as,feature,emotion as feature,0.5077855587005615
translation,273,207,results,feature,in,single task dac counterpart,feature in single task dac counterpart,0.5429370999336243
translation,273,207,results,single task dac counterpart,has,does n't outperform,single task dac counterpart has does n't outperform,0.6205511093139648
translation,273,207,results,does n't outperform,has,proposed multi-task variant,does n't outperform has proposed multi-task variant,0.5939822196960449
translation,273,210,results,all three attention mechanisms,i.e.,sa,all three attention mechanisms i.e. sa,0.6737207770347595
translation,273,210,results,all three attention mechanisms,i.e.,ima,all three attention mechanisms i.e. ima,0.6715008616447449
translation,273,210,results,all three attention mechanisms,i.e.,ita,all three attention mechanisms i.e. ita,0.6463522911071777
translation,273,210,results,all three attention mechanisms,yields,best results,all three attention mechanisms yields best results,0.6814863085746765
translation,273,210,results,results,combinations of,all three attention mechanisms,results combinations of all three attention mechanisms,0.578183650970459
translation,273,220,results,dyadic conversations,attain,better results,dyadic conversations attain better results,0.615294873714447
translation,273,220,results,better results,compared to,multi-party conversations,better results compared to multi-party conversations,0.6458847522735596
translation,273,220,results,results,experiments with,dyadic conversations,results experiments with dyadic conversations,0.6044713854789734
translation,275,26,experiments,our model,predict,identity of pronouns,our model predict identity of pronouns,0.6670013666152954
translation,275,26,experiments,identity of pronouns,at a granularity of,"person attribute : first , second , or third person-","identity of pronouns at a granularity of person attribute : first , second , or third person-",0.5996636748313904
translation,275,26,experiments,identity of pronouns,based on,variety of features,identity of pronouns based on variety of features,0.6562520265579224
translation,275,26,experiments,variety of features,of,utterance context,variety of features of utterance context,0.5932000875473022
translation,275,26,experiments,chinese sms (   texting   ) dialogues,has,hypothesis,chinese sms (   texting   ) dialogues has hypothesis,0.6060540676116943
translation,275,141,hyperparameters,75 %,of,training data,75 % of training data,0.6062865853309631
translation,275,141,hyperparameters,25 %,as,development data,25 % as development data,0.5390856266021729
translation,275,141,hyperparameters,hyperparameters,train on,75 %,hyperparameters train on 75 %,0.7515794038772583
translation,275,141,hyperparameters,hyperparameters,retain,25 %,hyperparameters retain 25 %,0.6775942444801331
translation,275,4,model,novel approach,to,zero pronoun resolution,novel approach to zero pronoun resolution,0.5414806604385376
translation,275,4,model,zero pronoun resolution,in,chinese,zero pronoun resolution in chinese,0.4828564524650574
translation,275,4,model,model,take,novel approach,model take novel approach,0.6876962184906006
translation,275,25,model,novel sequential model,for,zero pronoun resolution,novel sequential model for zero pronoun resolution,0.5736249089241028
translation,275,25,model,zero pronoun resolution,explicitly tracks,conversation focus,zero pronoun resolution explicitly tracks conversation focus,0.7284319400787354
translation,275,25,model,conversation focus,in,dialogue,conversation focus in dialogue,0.5278264284133911
translation,275,25,model,model,develop,novel sequential model,model develop novel sequential model,0.6387885808944702
translation,275,28,model,zero pronoun resolution system,using,supervision,zero pronoun resolution system using supervision,0.6534616351127625
translation,275,28,model,supervision,coming from,english translations,supervision coming from english translations,0.5728507041931152
translation,275,28,model,english translations,of,chinese text,english translations of chinese text,0.5745320320129395
translation,275,28,model,model,train,zero pronoun resolution system,model train zero pronoun resolution system,0.6815735697746277
translation,275,179,results,our full model,able to,substantially outperform,our full model able to substantially outperform,0.6761151552200317
translation,275,179,results,sms data,has,our full model,sms data has our full model,0.5968093276023865
translation,275,179,results,ontonotes data,has,our full model,ontonotes data has our full model,0.6175399422645569
translation,275,179,results,substantially outperform,has,baselines,substantially outperform has baselines,0.5976693630218506
translation,275,179,results,results,on,sms data,results on sms data,0.5095138549804688
translation,275,180,results,our full model,was,only baseline,our full model was only baseline,0.5976142287254333
translation,275,180,results,only baseline,to beat,random guessing,only baseline to beat random guessing,0.6312033534049988
translation,275,180,results,ontonotes,has,our full model,ontonotes has our full model,0.6082093119621277
translation,275,180,results,results,on,ontonotes,results on ontonotes,0.5512500405311584
translation,275,182,results,our model,achieves,14 % relative improvement,our model achieves 14 % relative improvement,0.6627933979034424
translation,275,182,results,our model,on,ontonotes data,our model on ontonotes data,0.5936744809150696
translation,275,182,results,14 % relative improvement,over,best baseline,14 % relative improvement over best baseline,0.6907385587692261
translation,275,182,results,sms / chat data,has,our model,sms / chat data has our model,0.5602483153343201
translation,275,182,results,ontonotes data,has,50 % relative improvement,ontonotes data has 50 % relative improvement,0.5887817740440369
translation,275,182,results,results,On,sms / chat data,results On sms / chat data,0.5190941095352173
translation,275,186,results,our model,tends to have,much higher precision,our model tends to have much higher precision,0.7070132493972778
translation,275,186,results,much higher precision,at the expense of,recall,much higher precision at the expense of recall,0.7464069724082947
translation,275,186,results,much higher precision,across,board,much higher precision across board,0.716101884841919
translation,275,186,results,much higher precision,leading to,14 % relative improvement,much higher precision leading to 14 % relative improvement,0.6850950717926025
translation,275,186,results,board,on,sms data,board on sms data,0.5339707732200623
translation,275,186,results,14 % relative improvement,over,subject continuation baseline,14 % relative improvement over subject continuation baseline,0.6858261227607727
translation,275,186,results,results,has,our model,results has our model,0.5871725678443909
translation,275,195,results,verb feature,creates,greatest improvement,verb feature creates greatest improvement,0.6386376619338989
translation,275,195,results,greatest improvement,over,minimal model,greatest improvement over minimal model,0.6608367562294006
translation,275,195,results,minimal model,followed by,bag of pos,minimal model followed by bag of pos,0.6130115389823914
translation,275,195,results,sms data,has,verb feature,sms data has verb feature,0.5979499816894531
translation,275,195,results,results,for,sms data,results for sms data,0.569692850112915
translation,275,197,results,bag of words feature,performs,best,bag of words feature performs best,0.5899310111999512
translation,275,197,results,best,by,large margin,best by large margin,0.5930963158607483
translation,275,197,results,ontonotes corpus,has,bag of words feature,ontonotes corpus has bag of words feature,0.5239579081535339
translation,275,197,results,results,For,ontonotes corpus,results For ontonotes corpus,0.5767267346382141
translation,276,131,ablation-analysis,accuracy drop,of,1.95 %,accuracy drop of 1.95 %,0.5494267344474792
translation,276,131,ablation-analysis,1.95 %,after removing,residual connections,1.95 % after removing residual connections,0.738142728805542
translation,276,131,ablation-analysis,1.95 %,after removing,hierarchical stack,1.95 % after removing hierarchical stack,0.7109673023223877
translation,276,131,ablation-analysis,hierarchical stack,of,our attention modules,hierarchical stack of our attention modules,0.5826835632324219
translation,276,131,ablation-analysis,ablation analysis,observe,accuracy drop,ablation analysis observe accuracy drop,0.6235467195510864
translation,276,132,ablation-analysis,effectiveness,of,hierarchical attention design,effectiveness of hierarchical attention design,0.590813159942627
translation,276,132,ablation-analysis,ablation analysis,proves,effectiveness,ablation analysis proves effectiveness,0.6730262041091919
translation,276,139,ablation-analysis,drops,by,3.45 %,drops by 3.45 %,0.643265426158905
translation,276,139,ablation-analysis,hidden size 512,has,accuracy,hidden size 512 has accuracy,0.6110052466392517
translation,276,139,ablation-analysis,relu activation function,has,accuracy,relu activation function has accuracy,0.55556720495224
translation,276,139,ablation-analysis,accuracy,has,drops,accuracy has drops,0.6229715347290039
translation,276,139,ablation-analysis,ablation analysis,of,hidden size 512,ablation analysis of hidden size 512,0.6273755431175232
translation,276,30,baselines,comer,applies,"bert contextualized word embeddings ( devlin et al. , 2018 )","comer applies bert contextualized word embeddings ( devlin et al. , 2018 )",0.5564466118812561
translation,276,30,baselines,comer,applies,"bpe ( sennrich et al. , 2016 )","comer applies bpe ( sennrich et al. , 2016 )",0.6159805655479431
translation,276,30,baselines,"bpe ( sennrich et al. , 2016 )",for,sequence encoding,"bpe ( sennrich et al. , 2016 ) for sequence encoding",0.561313271522522
translation,276,30,baselines,sequence encoding,to ensure,uniqueness,sequence encoding to ensure uniqueness,0.7084306478500366
translation,276,30,baselines,uniqueness,of,representations,uniqueness of representations,0.6311728954315186
translation,276,30,baselines,representations,of,unseen words,representations of unseen words,0.5949892401695251
translation,276,30,baselines,baselines,has,comer,baselines has comer,0.6290825605392456
translation,276,106,experimental-setup,relu non-linearity,used for,activation function,relu non-linearity used for activation function,0.6760419011116028
translation,276,106,experimental-setup,experimental setup,has,relu non-linearity,experimental setup has relu non-linearity,0.532162070274353
translation,276,108,experimental-setup,training,on,woz2.0,training on woz2.0,0.5915838479995728
translation,276,108,experimental-setup,model,trained with,batch size,model trained with batch size,0.7479425072669983
translation,276,108,experimental-setup,model,trained with,adam optimizer,model trained with adam optimizer,0.7251604199409485
translation,276,108,experimental-setup,model,trained with,batch size,model trained with batch size,0.7479425072669983
translation,276,108,experimental-setup,batch size,of,32,batch size of 32,0.6741614937782288
translation,276,108,experimental-setup,adam optimizer,),150 epochs,adam optimizer ) 150 epochs,0.5274516344070435
translation,276,108,experimental-setup,adam optimizer,for,150 epochs,adam optimizer for 150 epochs,0.5516048073768616
translation,276,108,experimental-setup,150 epochs,for,multiwoz,150 epochs for multiwoz,0.6531417369842529
translation,276,108,experimental-setup,batch size,of,16,batch size of 16,0.6842944622039795
translation,276,108,experimental-setup,16,adopted for,15 epochs,16 adopted for 15 epochs,0.6792242527008057
translation,276,108,experimental-setup,15 epochs,of,training,15 epochs of training,0.5833079218864441
translation,276,108,experimental-setup,training,has,model,training has model,0.5824191570281982
translation,276,108,experimental-setup,woz2.0,has,model,woz2.0 has model,0.624701201915741
translation,276,108,experimental-setup,multiwoz,has,amsgrad optimizer,multiwoz has amsgrad optimizer,0.5945635437965393
translation,276,108,experimental-setup,experimental setup,For,training,experimental setup For training,0.5809131264686584
translation,276,108,experimental-setup,experimental setup,for,multiwoz,experimental setup for multiwoz,0.6378808617591858
translation,276,109,experimental-setup,learning rate,of,0.0005,learning rate of 0.0005,0.6050455570220947
translation,276,109,experimental-setup,0.0005,with,gradient clip,0.0005 with gradient clip,0.6307951807975769
translation,276,109,experimental-setup,gradient clip,of,2.0,gradient clip of 2.0,0.5904988050460815
translation,276,110,experimental-setup,all weights,in,our model,all weights in our model,0.5386183261871338
translation,276,110,experimental-setup,all weights,with,kaiming initialization,all weights with kaiming initialization,0.5748324990272522
translation,276,110,experimental-setup,zero initialization,for,bias,zero initialization for bias,0.6160590648651123
translation,276,110,experimental-setup,experimental setup,initialize,all weights,experimental setup initialize all weights,0.7935352325439453
translation,276,110,experimental-setup,experimental setup,adopt,zero initialization,experimental setup adopt zero initialization,0.6282605528831482
translation,276,111,experimental-setup,experiments,conducted on,single nvidia gtx 1080 ti gpu,experiments conducted on single nvidia gtx 1080 ti gpu,0.6189778447151184
translation,276,111,experimental-setup,experimental setup,has,experiments,experimental setup has experiments,0.5502888560295105
translation,276,16,experiments,dst models,has,itc,dst models has itc,0.5845744609832764
translation,276,115,experiments,validation set,of,woz2.0,validation set of woz2.0,0.5747515559196472
translation,276,115,experiments,validation set,obtain,runtime,validation set obtain runtime,0.5837750434875488
translation,276,115,experiments,validation set,on,multiwoz,validation set on multiwoz,0.5954704880714417
translation,276,115,experiments,woz2.0,obtain,runtime,woz2.0 obtain runtime,0.6360070705413818
translation,276,115,experiments,woz2.0,obtain,runtime,woz2.0 obtain runtime,0.6360070705413818
translation,276,115,experiments,woz2.0,on,multiwoz,woz2.0 on multiwoz,0.6106173396110535
translation,276,115,experiments,runtime,of,65.6 seconds,runtime of 65.6 seconds,0.5195377469062805
translation,276,115,experiments,runtime,is,835.2 seconds,runtime is 835.2 seconds,0.5360130071640015
translation,276,115,experiments,runtime,is,835.2 seconds,runtime is 835.2 seconds,0.5360130071640015
translation,276,115,experiments,multiwoz,has,runtime,multiwoz has runtime,0.6087304949760437
translation,276,121,experiments,dst models,has,joint acc. woz 2.0,dst models has joint acc. woz 2.0,0.5659176707267761
translation,276,121,experiments,joint acc. woz 2.0,has,joint acc,joint acc. woz 2.0 has joint acc,0.5507933497428894
translation,276,114,hyperparameters,batch size,to,1,batch size to 1,0.6180254220962524
translation,276,114,hyperparameters,batch size,to avoid,influence,batch size to avoid influence,0.6817091703414917
translation,276,7,model,dst,using,generation framework,dst using generation framework,0.6673901081085205
translation,276,7,model,generation framework,without,pre-defined ontology list,generation framework without pre-defined ontology list,0.680651068687439
translation,276,7,model,model,approach,dst,model approach dst,0.6644587516784668
translation,276,8,model,each turn,of,user utterance and system response,each turn of user utterance and system response,0.6210963726043701
translation,276,8,model,belief states,by applying,hierarchical encoder-decoder structure,belief states by applying hierarchical encoder-decoder structure,0.6947814226150513
translation,276,8,model,model,Given,each turn,model Given each turn,0.6867771744728088
translation,276,12,model,dst module,takes,user utterance and the dialogue history,dst module takes user utterance and the dialogue history,0.5976771116256714
translation,276,12,model,dst module,outputs,belief estimate,dst module outputs belief estimate,0.7626835107803345
translation,276,12,model,user utterance and the dialogue history,as,input,user utterance and the dialogue history as input,0.5225602984428406
translation,276,12,model,belief estimate,of,dialogue state,belief estimate of dialogue state,0.5685233473777771
translation,276,12,model,each dialogue turn,has,dst module,each dialogue turn has dst module,0.596867024898529
translation,276,12,model,model,For,each dialogue turn,model For each dialogue turn,0.6362709403038025
translation,276,26,model,dialogue state tracking task,as,sequence generation problem,dialogue state tracking task as sequence generation problem,0.46666496992111206
translation,276,26,model,model,formulate,dialogue state tracking task,model formulate dialogue state tracking task,0.6535962820053101
translation,276,27,model,scalable and accurate,has,constant inference time complexity,scalable and accurate has constant inference time complexity,0.5371513366699219
translation,276,27,model,dialogue state tracker,has,constant inference time complexity,dialogue state tracker has constant inference time complexity,0.5211114287376404
translation,276,27,model,conditional memory relation network ( comer ),has,scalable and accurate,conditional memory relation network ( comer ) has scalable and accurate,0.5710880756378174
translation,276,27,model,scalable and accurate,has,dialogue state tracker,scalable and accurate has dialogue state tracker,0.5154597163200378
translation,276,27,model,model,propose,conditional memory relation network ( comer ),model propose conditional memory relation network ( comer ),0.6829659342765808
translation,276,28,model,encoderdecoder network,with,hierarchically stacked decoder,encoderdecoder network with hierarchically stacked decoder,0.6280638575553894
translation,276,28,model,hierarchically stacked decoder,to first generate,slot sequences,hierarchically stacked decoder to first generate slot sequences,0.686302661895752
translation,276,28,model,slot sequences,in,belief state,slot sequences in belief state,0.5322614312171936
translation,276,28,model,each slot,generate,corresponding value sequences,each slot generate corresponding value sequences,0.6893691420555115
translation,276,28,model,model,consists of,encoderdecoder network,model consists of encoderdecoder network,0.6641542911529541
translation,276,125,model,- mlp,further replace,mlp,- mlp further replace mlp,0.6725983023643494
translation,276,125,model,mlp,with,single linear layer,mlp with single linear layer,0.651300847530365
translation,276,125,model,single linear layer,with,nonlinear activation,single linear layer with nonlinear activation,0.6222060322761536
translation,276,125,model,model,For,- mlp,model For - mlp,0.7146015167236328
translation,276,133,model,mlp,replaced with,linear layer,mlp replaced with linear layer,0.7708927392959595
translation,276,138,model,- blockgrad,remove,gradient blocking mechanism,- blockgrad remove gradient blocking mechanism,0.7362977266311646
translation,276,138,model,gradient blocking mechanism,in,cmr decoder,gradient blocking mechanism in cmr decoder,0.5186895728111267
translation,276,138,model,model,For,- blockgrad,model For - blockgrad,0.6850576400756836
translation,276,120,results,woz2.0 dataset,maintain,performance,woz2.0 dataset maintain performance,0.6261369585990906
translation,276,120,results,performance,level of,state- of -,performance level of state- of -,0.4475751221179962
translation,276,120,results,results,For,woz2.0 dataset,results For woz2.0 dataset,0.579222559928894
translation,276,128,results,our model,achieves,joint goal accuracy,our model achieves joint goal accuracy,0.6471179723739624
translation,276,128,results,joint goal accuracy,of,45.72 %,joint goal accuracy of 45.72 %,0.5155491828918457
translation,276,128,results,45.72 %,significant better than,most of the previous models,45.72 % significant better than most of the previous models,0.7111223340034485
translation,276,128,results,muli-domain dataset,has,mul-tiwoz,muli-domain dataset has mul-tiwoz,0.6213934421539307
translation,276,128,results,muli-domain dataset,has,our model,muli-domain dataset has our model,0.5817388296127319
translation,276,128,results,mul-tiwoz,has,our model,mul-tiwoz has our model,0.6267263889312744
translation,276,128,results,results,On,muli-domain dataset,results On muli-domain dataset,0.5586019158363342
translation,276,147,results,absolute boost,of,5.48 %,absolute boost of 5.48 %,0.5527199506759644
translation,276,147,results,5.48 %,switch from,combined slot representation,5.48 % switch from combined slot representation,0.6932152509689331
translation,276,147,results,combined slot representation,to,nested tuple representation,combined slot representation to nested tuple representation,0.5194312334060669
translation,276,147,results,jds acc.,has,absolute boost,jds acc. has absolute boost,0.6076803803443909
translation,276,147,results,results,see that,jds acc.,results see that jds acc.,0.6286413073539734
translation,277,18,baselines,four negative sampling strategies,namely,minimum sampling,four negative sampling strategies namely minimum sampling,0.6801019310951233
translation,277,18,baselines,four negative sampling strategies,namely,maximum sampling,four negative sampling strategies namely maximum sampling,0.6806952357292175
translation,277,18,baselines,four negative sampling strategies,namely,semi-hard sampling,four negative sampling strategies namely semi-hard sampling,0.6826035380363464
translation,277,18,baselines,four negative sampling strategies,namely,decay - hard sampling,four negative sampling strategies namely decay - hard sampling,0.6972938776016235
translation,277,18,baselines,baselines,consider,four negative sampling strategies,baselines consider four negative sampling strategies,0.6940165758132935
translation,277,60,baselines,response candidate,with,lstms,response candidate with lstms,0.6508017182350159
translation,277,60,baselines,matching score,final states of,two lstms,matching score final states of two lstms,0.6854793429374695
translation,277,67,baselines,static strategy,where,negative examples,static strategy where negative examples,0.5822945833206177
translation,277,67,baselines,negative examples,fixed in,entire learning procedure,negative examples fixed in entire learning procedure,0.6625909209251404
translation,277,15,model,improve the performance,of,existing matching models,improve the performance of existing matching models,0.5622454285621643
translation,277,15,model,model,investigate,improve the performance,model investigate improve the performance,0.6579659581184387
translation,277,64,model,dam,performs,matching,dam performs matching,0.6483232975006104
translation,277,64,model,manner,as,smn,manner as smn,0.654189944267273
translation,277,64,model,response candidate,with,stacked self-attention and cross-attention,response candidate with stacked self-attention and cross-attention,0.6618669629096985
translation,277,64,model,model,has,dam,model has dam,0.6231600046157837
translation,277,103,model,decay - hard sampling,as,four model adaptive negative sampling strategies,decay - hard sampling as four model adaptive negative sampling strategies,0.5367563366889954
translation,277,103,model,four model adaptive negative sampling strategies,to learn,matching model,four model adaptive negative sampling strategies to learn matching model,0.6033371090888977
translation,277,103,model,matching model,for,retrievalbased dialogue systems,matching model for retrievalbased dialogue systems,0.6177566051483154
translation,277,103,model,minimum sampling,has,maximum sampling,minimum sampling has maximum sampling,0.5914467573165894
translation,277,103,model,model,present,minimum sampling,model present minimum sampling,0.6903852820396423
translation,277,103,model,model,present,maximum sampling,model present maximum sampling,0.6919050216674805
translation,277,103,model,model,present,semi-hard sampling,model present semi-hard sampling,0.6640466451644897
translation,277,103,model,model,present,decay - hard sampling,model present decay - hard sampling,0.6564959287643433
translation,277,85,results,three matching models,on,both data sets,three matching models on both data sets,0.5222088694572449
translation,277,85,results,improve,has,three matching models,improve has three matching models,0.5664374828338623
translation,277,85,results,results,see that,semi-hard sampling and decay - hard sampling,results see that semi-hard sampling and decay - hard sampling,0.6653850078582764
translation,277,86,results,minimum sampling and maximum sampling,consistently worse than,random sampling,minimum sampling and maximum sampling consistently worse than random sampling,0.6602029800415039
translation,277,86,results,results,has,minimum sampling and maximum sampling,results has minimum sampling and maximum sampling,0.5435261726379395
translation,278,117,hyperparameters,training process,carried out using,adam optimizer,training process carried out using adam optimizer,0.6307322978973389
translation,278,117,hyperparameters,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,278,117,hyperparameters,learning rate,of,2e - 4,learning rate of 2e - 4,0.6387818455696106
translation,278,117,hyperparameters,hyperparameters,has,training process,hyperparameters has training process,0.5041748881340027
translation,278,118,hyperparameters,conversation turns t,set to,8,conversation turns t set to 8,0.7285695672035217
translation,278,118,hyperparameters,conversation turns t,set to,8,conversation turns t set to 8,0.7285695672035217
translation,278,118,hyperparameters,batch size,set to,8,batch size set to 8,0.770182192325592
translation,278,118,hyperparameters,monte carlo sampling times k,set to,16,monte carlo sampling times k set to 16,0.6731544137001038
translation,278,118,hyperparameters,hyperparameters,has,conversation turns t,hyperparameters has conversation turns t,0.5335624814033508
translation,278,118,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,278,4,model,novel generation - evaluation framework,developed for,multi-turn conversations,novel generation - evaluation framework developed for multi-turn conversations,0.6154561638832092
translation,278,5,model,dialogue strategy,controls,knowledge selection,dialogue strategy controls knowledge selection,0.6571435928344727
translation,278,5,model,instantiated and continuously adapted,via,reinforcement learning,instantiated and continuously adapted via reinforcement learning,0.6274856328964233
translation,278,5,model,rational knowledge utilization,has,dialogue strategy,rational knowledge utilization has dialogue strategy,0.5456184148788452
translation,278,5,model,coherent conversation flow,has,dialogue strategy,coherent conversation flow has dialogue strategy,0.5890609622001648
translation,278,5,model,model,sake of,rational knowledge utilization,model sake of rational knowledge utilization,0.6208910346031189
translation,278,144,results,our method,is,consistently and significantly better,our method is consistently and significantly better,0.5523959994316101
translation,278,144,results,consistently and significantly better,than,other state - of - the - art approaches,consistently and significantly better than other state - of - the - art approaches,0.5435304641723633
translation,278,144,results,results,demonstrate,our method,results demonstrate our method,0.6011868715286255
translation,279,152,ablation-analysis,all the extension models,exhibit,higher time cost,all the extension models exhibit higher time cost,0.626176655292511
translation,279,152,ablation-analysis,higher time cost,than,original seq2seq model,higher time cost than original seq2seq model,0.5676170587539673
translation,279,152,ablation-analysis,auxiliary components,has,all the extension models,auxiliary components has all the extension models,0.5803267359733582
translation,279,152,ablation-analysis,ablation analysis,Augmented with,auxiliary components,ablation analysis Augmented with auxiliary components,0.7147694826126099
translation,279,106,baselines,ta-seq2seq,incorporates,outsourcing topic information,ta-seq2seq incorporates outsourcing topic information,0.7140903472900391
translation,279,106,baselines,outsourcing topic information,into,response generation,outsourcing topic information into response generation,0.5546915531158447
translation,279,106,baselines,response generation,where,topics,response generation where topics,0.6562978029251099
translation,279,106,baselines,seq2seq,for,response generation,seq2seq for response generation,0.6039499044418335
translation,279,106,baselines,baselines,has,ta-seq2seq,baselines has ta-seq2seq,0.5592435002326965
translation,279,131,baselines,cvae and laed,inject,seq2seq,cvae and laed inject seq2seq,0.6820436120033264
translation,279,131,baselines,seq2seq,with,stochastic latent variable,seq2seq with stochastic latent variable,0.6224608421325684
translation,279,131,baselines,better performance,on,"distinct - { 1 , 2 , 3 }.","better performance on distinct - { 1 , 2 , 3 }.",0.5383664965629578
translation,279,131,baselines,ta-seq2seq,incorporates,seq2seq,ta-seq2seq incorporates seq2seq,0.6931003928184509
translation,279,131,baselines,seq2seq,with,outsourcing topic information,seq2seq with outsourcing topic information,0.615944504737854
translation,279,131,baselines,outsourcing topic information,from,lda,outsourcing topic information from lda,0.5856853127479553
translation,279,131,baselines,baselines,has,cvae and laed,baselines has cvae and laed,0.5646317601203918
translation,279,115,experimental-setup,our model,with,parlai,our model with parlai,0.7448940277099609
translation,279,115,experimental-setup,parlai,has,"miller et al. , 2017 )","parlai has miller et al. , 2017 )",0.5809506177902222
translation,279,115,experimental-setup,experimental setup,implemented,our model,experimental setup implemented our model,0.7312729358673096
translation,279,116,experimental-setup,sequence lengths,truncated at,50,sequence lengths truncated at 50,0.38227343559265137
translation,279,116,experimental-setup,experimental setup,has,sequence lengths,experimental setup has sequence lengths,0.4742785692214966
translation,279,117,experimental-setup,"adam ( kingma and ba , 2014 )",with,initial learning rate,"adam ( kingma and ba , 2014 ) with initial learning rate",0.5918858647346497
translation,279,117,experimental-setup,initial learning rate,of,0.001,initial learning rate of 0.001,0.5703312754631042
translation,279,117,experimental-setup,experimental setup,used,"adam ( kingma and ba , 2014 )","experimental setup used adam ( kingma and ba , 2014 )",0.6106398105621338
translation,279,118,experimental-setup,2 - layer bidirectional lstm,as,encoder,2 - layer bidirectional lstm as encoder,0.5305489301681519
translation,279,118,experimental-setup,unidirectional one,as,decoder,unidirectional one as decoder,0.6102378368377686
translation,279,119,experimental-setup,hidden size,set to,300,hidden size set to 300,0.7481361627578735
translation,279,119,experimental-setup,the word embedding dimension,set to,300,the word embedding dimension set to 300,0.7033465504646301
translation,279,119,experimental-setup,experimental setup,has,hidden size,experimental setup has hidden size,0.5616794228553772
translation,279,119,experimental-setup,experimental setup,has,the word embedding dimension,experimental setup has the word embedding dimension,0.5407721996307373
translation,279,120,experimental-setup,latent variable size,set to,64,latent variable size set to 64,0.6794230937957764
translation,279,120,experimental-setup,experimental setup,has,latent variable size,experimental setup has latent variable size,0.525607705116272
translation,279,121,experimental-setup,topic number k,set to,5,topic number k set to 5,0.7739011645317078
translation,279,121,experimental-setup,"most frequent 3,159 words",taken as,topical words vocabulary,"most frequent 3,159 words taken as topical words vocabulary",0.6287965178489685
translation,279,121,experimental-setup,topical words vocabulary,by,stemming,topical words vocabulary by stemming,0.5240722894668579
translation,279,121,experimental-setup,topical words vocabulary,filtering,stop-words,topical words vocabulary filtering stop-words,0.6763908267021179
translation,279,121,experimental-setup,stop-words,from,training set,stop-words from training set,0.5269755721092224
translation,279,121,experimental-setup,experimental setup,has,topic number k,experimental setup has topic number k,0.5555629730224609
translation,279,122,experimental-setup,batch size,set to,128,batch size set to 128,0.7535710334777832
translation,279,122,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,279,124,experimental-setup,dropout,of,0.1,dropout of 0.1,0.6162222623825073
translation,279,124,experimental-setup,weight decay,set to,3 ? 10 ?5,weight decay set to 3 ? 10 ?5,0.7116039395332336
translation,279,124,experimental-setup,regularization,has,dropout,regularization has dropout,0.5779775977134705
translation,279,124,experimental-setup,preventing over-fitting,has,dropout,preventing over-fitting has dropout,0.5115253329277039
translation,279,124,experimental-setup,experimental setup,For,regularization,experimental setup For regularization,0.5391446948051453
translation,279,125,experimental-setup,"pretrained word embeddings ( pennington et al. , 2014 )",of,300 dimensions,"pretrained word embeddings ( pennington et al. , 2014 ) of 300 dimensions",0.48505544662475586
translation,279,125,experimental-setup,vocabulary size,set to,"20,000","vocabulary size set to 20,000",0.7229076027870178
translation,279,125,experimental-setup,experimental setup,used,"pretrained word embeddings ( pennington et al. , 2014 )","experimental setup used pretrained word embeddings ( pennington et al. , 2014 )",0.5247020721435547
translation,279,7,model,adand,manages,various conversations,adand manages various conversations,0.6579782962799072
translation,279,7,model,various conversations,with,conversation -specific parameterization,various conversations with conversation -specific parameterization,0.5940881371498108
translation,279,7,model,adaptive neural dialogue generation model,has,adand,adaptive neural dialogue generation model has adand,0.6062886714935303
translation,279,7,model,model,propose,adaptive neural dialogue generation model,model propose adaptive neural dialogue generation model,0.6488009095191956
translation,279,8,model,parameters,of,encoder-decoder,parameters of encoder-decoder,0.6097143292427063
translation,279,8,model,encoder-decoder,by referring to,input context,encoder-decoder by referring to input context,0.6568168997764587
translation,279,8,model,model,For,each conversation,model For each conversation,0.6050987839698792
translation,279,9,model,model,propose,two adaptive parameterization mechanisms,model propose two adaptive parameterization mechanisms,0.6606627106666565
translation,279,10,model,context - aware parameterization,directly generates,parameters,context - aware parameterization directly generates parameters,0.7465442419052124
translation,279,10,model,parameters,by capturing,local semantics,parameters by capturing local semantics,0.6702607274055481
translation,279,10,model,local semantics,of,given context,local semantics of given context,0.5589948892593384
translation,279,10,model,model,has,context - aware parameterization,model has context - aware parameterization,0.5222085118293762
translation,279,11,model,topic-aware parameterization,enables,parameter sharing,topic-aware parameterization enables parameter sharing,0.6539829969406128
translation,279,11,model,topic-aware parameterization,generating,parameters,topic-aware parameterization generating parameters,0.6900157928466797
translation,279,11,model,parameter sharing,conversations with,similar topics,parameter sharing conversations with similar topics,0.6335480213165283
translation,279,11,model,parameter sharing,inferring,latent topics,parameter sharing inferring latent topics,0.73282390832901
translation,279,11,model,parameter sharing,generating,parameters,parameter sharing generating parameters,0.733668327331543
translation,279,11,model,latent topics,of,given context,latent topics of given context,0.5554004907608032
translation,279,11,model,parameters,with respect to,distributional topics,parameters with respect to distributional topics,0.6841265559196472
translation,279,11,model,model,has,topic-aware parameterization,model has topic-aware parameterization,0.5135422945022583
translation,279,24,model,adaptive neural dialogue generation model,utilizes,single encoderdecoder,adaptive neural dialogue generation model utilizes single encoderdecoder,0.5737192630767822
translation,279,24,model,single encoderdecoder,for,diverse conversations,single encoderdecoder for diverse conversations,0.6148234009742737
translation,279,24,model,encoder-decoder,specifically parameterized according to,each conversation,encoder-decoder specifically parameterized according to each conversation,0.7220919132232666
translation,279,24,model,model,propose,adaptive neural dialogue generation model,model propose adaptive neural dialogue generation model,0.6488009095191956
translation,279,25,model,context - aware parameterization,directly generates,parameters,context - aware parameterization directly generates parameters,0.7465442419052124
translation,279,25,model,context - aware parameterization,directly generates,parameters,context - aware parameterization directly generates parameters,0.7465442419052124
translation,279,25,model,parameters,of,encoder-decoder model,parameters of encoder-decoder model,0.5814051032066345
translation,279,25,model,parameters,by capturing,local semantics,parameters by capturing local semantics,0.6702607274055481
translation,279,25,model,parameters,with respect to,inferred latent topics,parameters with respect to inferred latent topics,0.6304980516433716
translation,279,25,model,local semantics,of,given context,local semantics of given context,0.5589948892593384
translation,279,25,model,topic-aware parameterization,enables,parameter sharing,topic-aware parameterization enables parameter sharing,0.6539829969406128
translation,279,25,model,parameter sharing,among,conversations,parameter sharing among conversations,0.6087042689323425
translation,279,25,model,conversations,with,similar topic distributions,conversations with similar topic distributions,0.5971664190292358
translation,279,25,model,similar topic distributions,by first inferring,latent topics,similar topic distributions by first inferring latent topics,0.6316453814506531
translation,279,25,model,latent topics,of,input context,latent topics of input context,0.4968187212944031
translation,279,25,model,parameters,with respect to,inferred latent topics,parameters with respect to inferred latent topics,0.6304980516433716
translation,279,25,model,two adaptive parameterization mechanisms,has,context - aware parameterization,two adaptive parameterization mechanisms has context - aware parameterization,0.527703583240509
translation,279,25,model,model,propose,two adaptive parameterization mechanisms,model propose two adaptive parameterization mechanisms,0.6606627106666565
translation,279,26,model,responses,for,diverse conversations,responses for diverse conversations,0.5964658260345459
translation,279,26,model,diverse conversations,with,single encoder-decoder,diverse conversations with single encoder-decoder,0.6557872295379639
translation,279,26,model,single encoder-decoder,through,more flexible and efficient approach,single encoder-decoder through more flexible and efficient approach,0.6604328155517578
translation,279,26,model,model,Equipped with both,context-aware and topic-aware parameterization mechanisms,model Equipped with both context-aware and topic-aware parameterization mechanisms,0.6829465627670288
translation,279,26,model,model,capable of generating,responses,model capable of generating responses,0.7183637022972107
translation,279,27,model,model,trained in,end-to - end fashion,model trained in end-to - end fashion,0.7573421001434326
translation,279,27,model,end-to - end fashion,without,costly external or labeled topic annotations,end-to - end fashion without costly external or labeled topic annotations,0.7012027502059937
translation,279,27,model,model,trained in,end-to - end fashion,model trained in end-to - end fashion,0.7573421001434326
translation,279,27,model,model,has,model,model has model,0.5623406171798706
translation,279,133,model,dom -seq2seq,builds,multiple domain-specific encoder-decoders,dom -seq2seq builds multiple domain-specific encoder-decoders,0.6745325326919556
translation,279,133,model,model,has,dom -seq2seq,model has dom -seq2seq,0.6037378311157227
translation,279,134,results,improvements,on,relevance metrics,improvements on relevance metrics,0.5063212513923645
translation,279,134,results,improvements,on,informativeness metrics,improvements on informativeness metrics,0.508694052696228
translation,279,134,results,improvements,both,relevance metrics,improvements both relevance metrics,0.6069562435150146
translation,279,134,results,improvements,both,informativeness metrics,improvements both informativeness metrics,0.6375160217285156
translation,279,134,results,results,gains,improvements,results gains improvements,0.8265023231506348
translation,279,135,results,all the competitive baselines,in terms of,response relevance and informativeness,all the competitive baselines in terms of response relevance and informativeness,0.6685526967048645
translation,279,135,results,context- aware and topic-aware parameterization,has,our model,context- aware and topic-aware parameterization has our model,0.5568743348121643
translation,279,135,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,279,135,results,outperforms,has,all the competitive baselines,outperforms has all the competitive baselines,0.5892216563224792
translation,279,135,results,results,with both,context- aware and topic-aware parameterization,results with both context- aware and topic-aware parameterization,0.7257798910140991
translation,279,136,results,parameters sharing,has,among conversations with similar topics,parameters sharing has among conversations with similar topics,0.5338169932365417
translation,279,136,results,results,has,context -aware vs topic-aware parameterization,results has context -aware vs topic-aware parameterization,0.5548802018165588
translation,279,137,results,both parameterization mechanisms,perform,much better,both parameterization mechanisms perform much better,0.6143534779548645
translation,279,137,results,much better,than,original seq2seq model,much better than original seq2seq model,0.5438770055770874
translation,279,137,results,context - aware parameterization,is,slightly better,context - aware parameterization is slightly better,0.5348283648490906
translation,279,137,results,slightly better,in terms of,informativeness,slightly better in terms of informativeness,0.6051071286201477
translation,279,138,results,context-aware and topic-aware parameterization mechanisms,observe,best performance,context-aware and topic-aware parameterization mechanisms observe best performance,0.579120934009552
translation,279,148,results,adand,enjoys,large margin,adand enjoys large margin,0.6281924843788147
translation,279,148,results,outperforms,enjoys,large margin,outperforms enjoys large margin,0.6078923344612122
translation,279,148,results,large margin,over,existing models,large margin over existing models,0.7058505415916443
translation,279,148,results,adand,has,outperforms,adand has outperforms,0.6632006764411926
translation,279,148,results,outperforms,has,other baselines,outperforms has other baselines,0.5879674553871155
translation,279,148,results,results,has,adand,results has adand,0.5415932536125183
translation,279,149,results,relative performance,of,competitors,relative performance of competitors,0.621609091758728
translation,279,149,results,competitors,consistent with,quantitative evaluation results,competitors consistent with quantitative evaluation results,0.5331154465675354
translation,279,149,results,quantitative evaluation results,confirming,superior performance,quantitative evaluation results confirming superior performance,0.6834518313407898
translation,279,149,results,superior performance,of,our proposed method,superior performance of our proposed method,0.590394139289856
translation,279,149,results,results,has,relative performance,results has relative performance,0.5923845767974854
translation,279,153,results,decoding speeds,of,cvae and laed,decoding speeds of cvae and laed,0.5908418893814087
translation,279,153,results,cvae and laed,are,relatively comparable,cvae and laed are relatively comparable,0.6094209551811218
translation,279,153,results,relatively comparable,with,our model,relatively comparable with our model,0.6738675236701965
translation,279,153,results,results,observe,decoding speeds,results observe decoding speeds,0.5553289651870728
translation,279,154,results,ta-seq2seq and dom -seq2seq,elaborately and explicitly model,conversations,ta-seq2seq and dom -seq2seq elaborately and explicitly model conversations,0.7115241289138794
translation,279,154,results,conversations,with,diverse topics or themes,conversations with diverse topics or themes,0.5593114495277405
translation,279,154,results,adand,shows,clear superiority,adand shows clear superiority,0.7025115489959717
translation,279,154,results,clear superiority,in,decoding speed,clear superiority in decoding speed,0.5061361789703369
translation,279,154,results,ta-seq2seq and dom -seq2seq,has,adand,ta-seq2seq and dom -seq2seq has adand,0.6260494589805603
translation,279,154,results,results,comparing with,ta-seq2seq and dom -seq2seq,results comparing with ta-seq2seq and dom -seq2seq,0.6350793838500977
translation,279,157,results,time complexity,of,multiple topic / theme-specific encoderdecoders,time complexity of multiple topic / theme-specific encoderdecoders,0.5697817206382751
translation,279,157,results,multiple topic / theme-specific encoderdecoders,much higher than,all - other comparison models,multiple topic / theme-specific encoderdecoders much higher than all - other comparison models,0.5574815273284912
translation,279,157,results,results,For,dom -seq2seq,results For dom -seq2seq,0.5860885977745056
translation,279,170,results,latent variable conversation,generate,more diverse but sometimes irrelevant responses,latent variable conversation generate more diverse but sometimes irrelevant responses,0.62649005651474
translation,279,170,results,cvae and laed ),generate,more diverse but sometimes irrelevant responses,cvae and laed ) generate more diverse but sometimes irrelevant responses,0.6506426930427551
translation,279,170,results,ta-seq2seq,tends to produce,short responses,ta-seq2seq tends to produce short responses,0.7448391914367676
translation,279,170,results,dom -seq2seq,not perform,obviously better,dom -seq2seq not perform obviously better,0.6705823540687561
translation,279,170,results,obviously better,than,ta - seq2seq,obviously better than ta - seq2seq,0.6111336350440979
translation,279,170,results,results,has,latent variable conversation,results has latent variable conversation,0.4798057973384857
translation,280,136,ablation-analysis,adversarial training,successfully brings down,both f1 's ( stat. significantly ),adversarial training successfully brings down both f1 's ( stat. significantly ),0.6889806389808655
translation,280,136,ablation-analysis,both f1 's ( stat. significantly ),for,each model,both f1 's ( stat. significantly ) for each model,0.5809443593025208
translation,280,5,baselines,adversarial training,with,each strategy,adversarial training with each strategy,0.6487380862236023
translation,280,5,baselines,adversarial training,employing,maxmargin approach,adversarial training employing maxmargin approach,0.5899394154548645
translation,280,5,baselines,maxmargin approach,for,negative generative examples,maxmargin approach for negative generative examples,0.5468602180480957
translation,280,69,baselines,baselines,has,stopword dropout,baselines has stopword dropout,0.5225881934165955
translation,280,17,experiments,three state - of - the - art models,on,two task - oriented dialogue datasets,three state - of - the - art models on two task - oriented dialogue datasets,0.4788416028022766
translation,280,18,experiments,should - not - change and five should - change adversarial strategies,on,vhred ( variational hierarchical encoder - decoder ) model,should - not - change and five should - change adversarial strategies on vhred ( variational hierarchical encoder - decoder ) model,0.5298128724098206
translation,280,18,experiments,should - not - change and five should - change adversarial strategies,on,rl ( reinforcement learning ) model,should - not - change and five should - change adversarial strategies on rl ( reinforcement learning ) model,0.5263242721557617
translation,280,18,experiments,rl ( reinforcement learning ) model,with,ubuntu dialogue cor-pus,rl ( reinforcement learning ) model with ubuntu dialogue cor-pus,0.6404919624328613
translation,280,18,experiments,dynamic knowledge graph network,with,collaborative communicating agents ( cocoa ) dataset,dynamic knowledge graph network with collaborative communicating agents ( cocoa ) dataset,0.6291345953941345
translation,280,18,experiments,five naturally motivated and increasingly complex,has,should - not - change and five should - change adversarial strategies,five naturally motivated and increasingly complex has should - not - change and five should - change adversarial strategies,0.5777263641357422
translation,280,19,experiments,should - not - change side,for,ubuntu task,should - not - change side for ubuntu task,0.6058178544044495
translation,280,19,experiments,should - not - change side,introduce,adversarial strategies,should - not - change side introduce adversarial strategies,0.6741874814033508
translation,280,19,experiments,adversarial strategies,of,increasing linguistic -unit complexity,adversarial strategies of increasing linguistic -unit complexity,0.5702580809593201
translation,280,19,experiments,increasing linguistic -unit complexity,from,shallow word - level errors,increasing linguistic -unit complexity from shallow word - level errors,0.5570836663246155
translation,280,4,model,model- agnostic adversarial strategies,reveal,weaknesses,model- agnostic adversarial strategies reveal weaknesses,0.6585696935653687
translation,280,4,model,weaknesses,of,"several generative , task - oriented dialogue models","weaknesses of several generative , task - oriented dialogue models",0.5660533308982849
translation,280,4,model,should - not - change strategies,evaluate,over-sensitivity,should - not - change strategies evaluate over-sensitivity,0.740045428276062
translation,280,4,model,over-sensitivity,to,small and semantics - preserving edits,over-sensitivity to small and semantics - preserving edits,0.5424942970275879
translation,280,4,model,model,two categories of,model- agnostic adversarial strategies,model two categories of model- agnostic adversarial strategies,0.6977284550666809
translation,280,20,model,two rule-based perturbations,to,source dialogue context,two rule-based perturbations to source dialogue context,0.5304965376853943
translation,280,20,model,two rule-based perturbations,namely,random swap,two rule-based perturbations namely random swap,0.7082957625389099
translation,280,20,model,two rule-based perturbations,namely,stopword dropout,two rule-based perturbations namely stopword dropout,0.6582741141319275
translation,280,20,model,stopword dropout,has,randomly removing stopwords,stopword dropout has randomly removing stopwords,0.5632127523422241
translation,280,20,model,model,propose,two rule-based perturbations,model propose two rule-based perturbations,0.6916415691375732
translation,280,21,model,two data- level strategies,that leverage,existing parallel datasets,two data- level strategies that leverage existing parallel datasets,0.7036039233207703
translation,280,21,model,"more realistic , diverse noises",namely,data-level paraphrasing,"more realistic , diverse noises namely data-level paraphrasing",0.6695807576179504
translation,280,21,model,"more realistic , diverse noises",namely,grammar errors,"more realistic , diverse noises namely grammar errors",0.6865711808204651
translation,280,21,model,data-level paraphrasing,replacing,words,data-level paraphrasing replacing words,0.663827657699585
translation,280,21,model,model,propose,two data- level strategies,model propose two data- level strategies,0.6894527673721313
translation,280,32,model,robustness issue,directly at,model- level,robustness issue directly at model- level,0.6452407836914062
translation,280,32,model,subword units,derived from,byte pair encoding ( bpe ) algorithm,subword units derived from byte pair encoding ( bpe ) algorithm,0.6390590667724609
translation,280,32,model,subword units,to,vhred model,subword units to vhred model,0.5719259977340698
translation,280,49,model,vhred model,apply,additive attention mechanism,vhred model apply additive attention mechanism,0.595535397529602
translation,280,49,model,additive attention mechanism,to,source sequence,additive attention mechanism to source sequence,0.5275333523750305
translation,280,49,model,remaining architecture,has,unchanged,remaining architecture has unchanged,0.5786382555961609
translation,280,49,model,model,For,vhred model,model For vhred model,0.6799323558807373
translation,280,8,results,robustness,feeding it,subword units,robustness feeding it subword units,0.6384378671646118
translation,280,8,results,resulting model,is,equally competitive,resulting model is equally competitive,0.5680426359176636
translation,280,8,results,subword units,has,as both inputs and outputs,subword units has as both inputs and outputs,0.5782817602157593
translation,280,31,results,vhred,on,all of the perturbed data,vhred on all of the perturbed data,0.5835013389587402
translation,280,31,results,all of the perturbed data,from,each adversarial strategy,all of the perturbed data from each adversarial strategy,0.5554815530776978
translation,280,31,results,performance,on,original task,performance on original task,0.5060971975326538
translation,280,31,results,performance,improves,even further,performance improves even further,0.7664743661880493
translation,280,31,results,state - of - the - art result,by,significant margin,state - of - the - art result by significant margin,0.5649917721748352
translation,280,31,results,vhred,has,performance,vhred has performance,0.610036313533783
translation,280,31,results,all of the perturbed data,has,performance,all of the perturbed data has performance,0.5812652707099915
translation,280,31,results,results,train,vhred,results train vhred,0.5890777707099915
translation,280,33,results,resulting model,reduces,vocabulary size,resulting model reduces vocabulary size,0.6622210144996643
translation,280,33,results,resulting model,obtains,results,resulting model obtains results,0.6469231843948364
translation,280,33,results,vocabulary size,by around,75 %,vocabulary size by around 75 %,0.7313423156738281
translation,280,33,results,results,comparable to,original vhred,results comparable to original vhred,0.6985602974891663
translation,280,33,results,results,show,resulting model,results show resulting model,0.632773220539093
translation,280,122,results,vhred,robust to,none of the should - not - change strategies,vhred robust to none of the should - not - change strategies,0.7381292581558228
translation,280,122,results,vhred,robust to,none of the should - not - change strategies,vhred robust to none of the should - not - change strategies,0.7381292581558228
translation,280,122,results,none of the should - not - change strategies,other than,random swap,none of the should - not - change strategies other than random swap,0.7137699127197266
translation,280,122,results,none of the should - not - change strategies,other than,stopword dropout,none of the should - not - change strategies other than stopword dropout,0.7367609143257141
translation,280,122,results,reranking - rl,robust to,none of the should - not - change strategies,reranking - rl robust to none of the should - not - change strategies,0.7526929378509521
translation,280,122,results,none of the should - not - change strategies,other than,stopword dropout,none of the should - not - change strategies other than stopword dropout,0.7367609143257141
translation,280,122,results,results,shows,vhred,results shows vhred,0.6565968990325928
translation,280,122,results,results,shows that,reranking - rl,results shows that reranking - rl,0.6607692241668701
translation,280,155,results,normal inputs,with,confusing entities strategy,normal inputs with confusing entities strategy,0.6558210253715515
translation,280,155,results,dynonet,able to,finish,dynonet able to finish,0.6839147210121155
translation,280,155,results,normal inputs,has,dynonet,normal inputs has dynonet,0.6125954985618591
translation,280,155,results,confusing entities strategy,has,dynonet,confusing entities strategy has dynonet,0.6284481883049011
translation,280,155,results,finish,has,task,finish has task,0.5675331950187683
translation,280,155,results,task,has,77 % of the time,task has 77 % of the time,0.569256067276001
translation,280,155,results,results,even with,normal inputs,results even with normal inputs,0.7199695110321045
translation,281,198,ablation-analysis,linguistic modality,is,weakest,linguistic modality is weakest,0.5383192300796509
translation,281,198,ablation-analysis,ablation analysis,has,linguistic modality,ablation analysis has linguistic modality,0.520964503288269
translation,281,201,ablation-analysis,performance,of,dialog features,performance of dialog features,0.5723772644996643
translation,281,201,ablation-analysis,performance,having evidence of,previous behavior,performance having evidence of previous behavior,0.6353232264518738
translation,281,201,ablation-analysis,dialog features,having evidence of,previous behavior,dialog features having evidence of previous behavior,0.5923596620559692
translation,281,201,ablation-analysis,previous behavior,can be,useful,previous behavior can be useful,0.6277000904083252
translation,281,201,ablation-analysis,useful,when modeling,deceptive behavior,useful when modeling deceptive behavior,0.7592032551765442
translation,281,201,ablation-analysis,deceptive behavior,of,single individual,deceptive behavior of single individual,0.5683599710464478
translation,281,201,ablation-analysis,ablation analysis,has,performance,ablation analysis has performance,0.5053174495697021
translation,281,173,experimental-setup,all the classification experiments,with,python package scikit-learn,all the classification experiments with python package scikit-learn,0.5809375643730164
translation,281,173,experimental-setup,python package scikit-learn,using,standard settings,python package scikit-learn using standard settings,0.5936402082443237
translation,281,173,experimental-setup,standard settings,for,model parameters,standard settings for model parameters,0.5520492196083069
translation,281,173,experimental-setup,experimental setup,perform,all the classification experiments,experimental setup perform all the classification experiments,0.591826319694519
translation,281,187,results,results,for,"verbal , nonverbal , dialog features","results for verbal , nonverbal , dialog features",0.563503623008728
translation,281,188,results,our experiments,show,benefit,our experiments show benefit,0.6965559720993042
translation,281,188,results,benefit,combining,multiple sources of information,benefit combining multiple sources of information,0.7514752745628357
translation,281,188,results,multiple sources of information,with,accuracies,multiple sources of information with accuracies,0.5991083979606628
translation,281,188,results,multiple sources of information,with,noticeable accuracy improvement,multiple sources of information with noticeable accuracy improvement,0.6225618124008179
translation,281,188,results,accuracies,well above,baseline,accuracies well above baseline,0.7623820304870605
translation,281,188,results,noticeable accuracy improvement,when using,all feature sets,noticeable accuracy improvement when using all feature sets,0.6707539558410645
translation,281,190,results,different classification performances,show,adding information,different classification performances show adding information,0.6339167952537537
translation,281,190,results,adding information,from,several modalities,adding information from several modalities,0.5514928102493286
translation,281,190,results,adding information,helps to increase,accuracy,adding information helps to increase accuracy,0.7815018892288208
translation,281,190,results,accuracy,of,detection system,accuracy of detection system,0.5989142060279846
translation,281,190,results,results,has,different classification performances,results has different classification performances,0.5277126431465149
translation,281,191,results,linguistic modality,shows,best performance,linguistic modality shows best performance,0.6694544553756714
translation,281,191,results,best performance,among,single modalities,best performance among single modalities,0.5964968800544739
translation,281,191,results,results,has,linguistic modality,results has linguistic modality,0.5071799159049988
translation,281,192,results,non-verbal modality,second best indicator of,deception,non-verbal modality second best indicator of deception,0.7486954927444458
translation,281,192,results,results,has,non-verbal modality,results has non-verbal modality,0.5143958926200867
translation,282,70,baselines,pointer lstm,implemented,pointer network,pointer lstm implemented pointer network,0.6763634085655212
translation,282,70,baselines,pointer network,for,qa,pointer network for qa,0.6819836497306824
translation,282,72,hyperparameters,bi-daf,implemented,bi-directional attention flow network,bi-daf implemented bi-directional attention flow network,0.6487761735916138
translation,282,72,hyperparameters,hyperparameters,has,bi-daf,hyperparameters has bi-daf,0.5724939703941345
translation,282,73,hyperparameters,training configuration,Pre-trained word embeddings from,glove,training configuration Pre-trained word embeddings from glove,0.6869853734970093
translation,282,73,hyperparameters,hyperparameters,has,training configuration,hyperparameters has training configuration,0.5111837387084961
translation,282,74,hyperparameters,out - of- vocabulary words,replaced with,[ unk ] token,out - of- vocabulary words replaced with [ unk ] token,0.71957927942276
translation,282,74,hyperparameters,hyperparameters,has,out - of- vocabulary words,hyperparameters has out - of- vocabulary words,0.5295700430870056
translation,282,75,hyperparameters,hidden size and embedding dimension,set to,300,hidden size and embedding dimension set to 300,0.7214565873146057
translation,282,75,hyperparameters,hyperparameters,has,hidden size and embedding dimension,hyperparameters has hidden size and embedding dimension,0.5109702348709106
translation,282,76,hyperparameters,"adam optimizer ( kingma and ba , 2015 )",with,batch size,"adam optimizer ( kingma and ba , 2015 ) with batch size",0.6134414076805115
translation,282,76,hyperparameters,"adam optimizer ( kingma and ba , 2015 )",with,learning rate,"adam optimizer ( kingma and ba , 2015 ) with learning rate",0.5968794822692871
translation,282,76,hyperparameters,batch size,of,64,batch size of 64,0.6741159558296204
translation,282,76,hyperparameters,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,282,76,hyperparameters,hyperparameters,has,"adam optimizer ( kingma and ba , 2015 )","hyperparameters has adam optimizer ( kingma and ba , 2015 )",0.5152386426925659
translation,282,77,hyperparameters,dropout rate,set to,0.2,dropout rate set to 0.2,0.6595621705055237
translation,282,77,hyperparameters,modeling layers,has,dropout rate,modeling layers has dropout rate,0.5107760429382324
translation,282,77,hyperparameters,hyperparameters,For,modeling layers,hyperparameters For modeling layers,0.5470958948135376
translation,282,78,hyperparameters,weight,in,loss function,weight in loss function,0.5226495862007141
translation,282,78,hyperparameters,weight,set to,1.0,weight set to 1.0,0.7055991888046265
translation,282,78,hyperparameters,hyperparameters,has,weight,hyperparameters has weight,0.5009066462516785
translation,282,6,model,hierarchical attention neural network architecture,combining,turnlevel and word-level attention mechanisms,hierarchical attention neural network architecture combining turnlevel and word-level attention mechanisms,0.6654728651046753
translation,282,6,model,hierarchical attention neural network architecture,to improve,spoken dialogue comprehension performance,hierarchical attention neural network architecture to improve spoken dialogue comprehension performance,0.5966647863388062
translation,282,6,model,turnlevel and word-level attention mechanisms,to improve,spoken dialogue comprehension performance,turnlevel and word-level attention mechanisms to improve spoken dialogue comprehension performance,0.5879979133605957
translation,282,6,model,model,propose,hierarchical attention neural network architecture,model propose hierarchical attention neural network architecture,0.6508312821388245
translation,282,17,model,hierarchical attention mechanism,for,dialogue comprehension,hierarchical attention mechanism for dialogue comprehension,0.5268378853797913
translation,282,17,model,model,utilize,hierarchical attention mechanism,model utilize hierarchical attention mechanism,0.5833791494369507
translation,282,23,model,hierarchical neu - ral attention architecture,integrating,turn- level attention,hierarchical neu - ral attention architecture integrating turn- level attention,0.6516891717910767
translation,282,23,model,turn- level attention,with,word- level attention,turn- level attention with word- level attention,0.6079984903335571
translation,282,23,model,word- level attention,for,multi-turn dialogue comprehension,word- level attention for multi-turn dialogue comprehension,0.5362635850906372
translation,282,23,model,multi-turn dialogue comprehension,in,question - answering manner,multi-turn dialogue comprehension in question - answering manner,0.5115096569061279
translation,282,23,model,model,introduce,hierarchical neu - ral attention architecture,model introduce hierarchical neu - ral attention architecture,0.591913640499115
translation,282,24,results,challenges,from,limited training data scenarios,challenges from limited training data scenarios,0.4993442893028259
translation,282,24,results,challenges,from,lengthy and out -of- distribution test samples,challenges from lengthy and out -of- distribution test samples,0.5706972479820251
translation,282,83,results,network,on par with,established baselines,network on par with established baselines,0.7265545725822449
translation,282,83,results,proposed turn - based ha model,obtains,more gains,proposed turn - based ha model obtains more gains,0.6634336113929749
translation,282,83,results,proposed turn - based ha model,achieving,best em and f1 scores,proposed turn - based ha model achieving best em and f1 scores,0.6681050062179565
translation,282,83,results,more gains,achieving,best em and f1 scores,more gains achieving best em and f1 scores,0.6866759061813354
translation,282,87,results,all other models,when,training set,all other models when training set,0.5893138647079468
translation,282,87,results,training set,is,smaller than 20k,training set is smaller than 20k,0.5731271505355835
translation,282,87,results,turn - based ha model,has,outperforms,turn - based ha model has outperforms,0.5930196046829224
translation,282,87,results,outperforms,has,all other models,outperforms has all other models,0.5782700181007385
translation,282,87,results,all other models,has,significantly,all other models has significantly,0.5514129400253296
translation,282,96,results,proposed turn - based ha model,is,most robust,proposed turn - based ha model is most robust,0.5867993831634521
translation,282,96,results,proposed turn - based ha model,performing,well,proposed turn - based ha model performing well,0.678147554397583
translation,282,96,results,most robust,in,answering questions,most robust in answering questions,0.4901578426361084
translation,282,96,results,answering questions,related to,unseen symptoms / topics,answering questions related to unseen symptoms / topics,0.7284670472145081
translation,282,96,results,well,on,in- domain symptoms,well on in- domain symptoms,0.5449584126472473
translation,282,97,results,proposed hierarchical method,achieves,higher learning efficiency,proposed hierarchical method achieves higher learning efficiency,0.6437072157859802
translation,282,97,results,higher learning efficiency,with,robust performance,higher learning efficiency with robust performance,0.6276060342788696
translation,282,97,results,results,demonstrate,proposed hierarchical method,results demonstrate proposed hierarchical method,0.6420125961303711
translation,282,98,results,turn- based model,has,significantly outperforms,turn- based model has significantly outperforms,0.6066540479660034
translation,282,98,results,significantly outperforms,has,utterance - based one,significantly outperforms has utterance - based one,0.597779393196106
translation,282,98,results,results,has,turn- based model,results has turn- based model,0.536207377910614
translation,283,146,ablation-analysis,lev,with,belief span,lev with belief span,0.6302161812782288
translation,283,146,ablation-analysis,lev,shows,effectiveness,lev shows effectiveness,0.7001187801361084
translation,283,146,ablation-analysis,overall performance,shows,effectiveness,overall performance shows effectiveness,0.665878415107727
translation,283,146,ablation-analysis,belief span,has,hurts,belief span has hurts,0.5973696708679199
translation,283,146,ablation-analysis,hurts,has,overall performance,hurts has overall performance,0.6045477390289307
translation,283,146,ablation-analysis,ablation analysis,Replacing,lev,ablation analysis Replacing lev,0.6612469553947449
translation,283,147,ablation-analysis,lev,greatly reduces,inference latency,lev greatly reduces inference latency,0.7438673377037048
translation,283,147,ablation-analysis,ablation analysis,show,lev,ablation analysis show lev,0.6006275415420532
translation,283,5,baselines,mintl,is,simple yet effective transfer learning framework,mintl is simple yet effective transfer learning framework,0.5603475570678711
translation,283,5,baselines,mintl,plug-and - play,pre-trained seq2seq models,mintl plug-and - play pre-trained seq2seq models,0.74298095703125
translation,283,5,baselines,mintl,jointly learn,dialogue state tracking,mintl jointly learn dialogue state tracking,0.6660134196281433
translation,283,5,baselines,pre-trained seq2seq models,jointly learn,dialogue response generation,pre-trained seq2seq models jointly learn dialogue response generation,0.771313488483429
translation,283,5,baselines,baselines,has,mintl,baselines has mintl,0.6449890732765198
translation,283,100,baselines,five different document corruption methods,in,pre-training,five different document corruption methods in pre-training,0.4749660789966583
translation,283,100,baselines,five different document corruption methods,including,token masking,five different document corruption methods including token masking,0.6561081409454346
translation,283,100,baselines,five different document corruption methods,including,token deletion,five different document corruption methods including token deletion,0.6447215676307678
translation,283,100,baselines,five different document corruption methods,including,text infilling,five different document corruption methods including text infilling,0.6368629932403564
translation,283,100,baselines,five different document corruption methods,including,sentence permutation,five different document corruption methods including sentence permutation,0.6632556319236755
translation,283,100,baselines,five different document corruption methods,including,document rotation,five different document corruption methods including document rotation,0.6260027885437012
translation,283,100,baselines,token masking,has,"devlin et al. , 2019","token masking has devlin et al. , 2019",0.588878870010376
translation,283,100,baselines,text infilling,has,"joshi et al. , 2020 )","text infilling has joshi et al. , 2020 )",0.5742596983909607
translation,283,118,baselines,end-to-end modeling,with,oracle dialogue state,end-to-end modeling with oracle dialogue state,0.6635017991065979
translation,283,118,baselines,oracle dst,fine - tuned,gpt2 -small,oracle dst fine - tuned gpt2 -small,0.7094462513923645
translation,283,118,baselines,oracle dst,fine - tuned,"gpt2 - medium ( radford et al. , 2019 )","oracle dst fine - tuned gpt2 - medium ( radford et al. , 2019 )",0.7513064742088318
translation,283,118,baselines,oracle dst,with,oracle dialogue state,oracle dst with oracle dialogue state,0.6137092113494873
translation,283,118,baselines,"gpt2 - medium ( radford et al. , 2019 )",with,oracle dialogue state,"gpt2 - medium ( radford et al. , 2019 ) with oracle dialogue state",0.639451265335083
translation,283,118,baselines,end-to-end modeling,has,oracle dst,end-to-end modeling has oracle dst,0.5736888647079468
translation,283,118,baselines,baselines,has,end-to-end modeling,baselines has end-to-end modeling,0.5489822030067444
translation,283,119,baselines,hred - ts,with,hierarchical recurrent encoder-decoder backbone,hred - ts with hierarchical recurrent encoder-decoder backbone,0.6582357883453369
translation,283,119,baselines,teacher -student framework,with,hierarchical recurrent encoder-decoder backbone,teacher -student framework with hierarchical recurrent encoder-decoder backbone,0.637204110622406
translation,283,119,baselines,seq2seq network,comprised of,several pre-trained dialogue modules,seq2seq network comprised of several pre-trained dialogue modules,0.5763969421386719
translation,283,119,baselines,several pre-trained dialogue modules,connected through,hidden states,several pre-trained dialogue modules connected through hidden states,0.7223495841026306
translation,283,119,baselines,hred - ts,has,teacher -student framework,hred - ts has teacher -student framework,0.5937855243682861
translation,283,119,baselines,sfn + rl,has,seq2seq network,sfn + rl has seq2seq network,0.5933958292007446
translation,283,119,baselines,baselines,has,hred - ts,baselines has hred - ts,0.6128941178321838
translation,283,120,baselines,reinforcement fine tuning,to train,model,reinforcement fine tuning to train model,0.7078563570976257
translation,283,120,baselines,md - sequicity,extension of,sequicity,md - sequicity extension of sequicity,0.7183985710144043
translation,283,120,baselines,md - sequicity,extension of,) framework,md - sequicity extension of ) framework,0.7645518183708191
translation,283,120,baselines,md - sequicity,for,multi-domain task - oriented dialogue,md - sequicity for multi-domain task - oriented dialogue,0.601163923740387
translation,283,120,baselines,) framework,for,multi-domain task - oriented dialogue,) framework for multi-domain task - oriented dialogue,0.624494194984436
translation,283,120,baselines,sequicity,has,) framework,sequicity has ) framework,0.6264976263046265
translation,283,120,baselines,baselines,has,reinforcement fine tuning,baselines has reinforcement fine tuning,0.5598889589309692
translation,283,121,baselines,damd,has,domain- aware multi-decoder network,damd has domain- aware multi-decoder network,0.5297423005104065
translation,283,121,baselines,baselines,has,damd,baselines has damd,0.5979146957397461
translation,283,124,baselines,"sequicity ( lei et al. , 2018 ) framework",with,t5 backbone model,"sequicity ( lei et al. , 2018 ) framework with t5 backbone model",0.6062749028205872
translation,283,124,baselines,sequicity + t5,has,"sequicity ( lei et al. , 2018 ) framework","sequicity + t5 has sequicity ( lei et al. , 2018 ) framework",0.5868216753005981
translation,283,124,baselines,baselines,has,sequicity + t5,baselines has sequicity + t5,0.609849214553833
translation,283,7,experiments,learning framework,with,two pretrained backbones,learning framework with two pretrained backbones,0.5786471962928772
translation,283,103,experiments,dialogue state tracking module,of,our framework,dialogue state tracking module of our framework,0.5378968715667725
translation,283,103,experiments,dialogue state tracking module,on,both datasets,dialogue state tracking module on both datasets,0.49174508452415466
translation,283,103,experiments,end-to - end models,on,multi-woz 2.0,end-to - end models on multi-woz 2.0,0.5260586142539978
translation,283,129,experiments,former,includes,mdbt,former includes mdbt,0.6523369550704956
translation,283,129,experiments,former,includes,"glad ( zhong et al. , 2018 )","former includes glad ( zhong et al. , 2018 )",0.6359952092170715
translation,283,129,experiments,former,includes,"gce ( nouri and hosseini , 2018 )","former includes gce ( nouri and hosseini , 2018 )",0.6224186420440674
translation,283,129,experiments,former,includes,fjst,former includes fjst,0.6816355586051941
translation,283,129,experiments,former,includes,"eric et al. , 2019 )","former includes eric et al. , 2019 )",0.627066433429718
translation,283,129,experiments,former,includes,hyst,former includes hyst,0.6643584370613098
translation,283,129,experiments,former,includes,sumbt,former includes sumbt,0.6965599656105042
translation,283,129,experiments,former,includes,sst,former includes sst,0.650638222694397
translation,283,129,experiments,former,includes,"tod - bert ( wu et al. , 2020 )","former includes tod - bert ( wu et al. , 2020 )",0.6262860894203186
translation,283,129,experiments,former,includes,"dst - picklist ( zhang et al. , 2019a )","former includes dst - picklist ( zhang et al. , 2019a )",0.6251600384712219
translation,283,129,experiments,former,includes,latter,former includes latter,0.6783299446105957
translation,283,129,experiments,former,includes,neural reading,former includes neural reading,0.6338708996772766
translation,283,129,experiments,former,includes,som - dst,former includes som - dst,0.6448484659194946
translation,283,129,experiments,former,includes,"dstqa ( zhou and small , 2019 )","former includes dstqa ( zhou and small , 2019 )",0.6376219987869263
translation,283,129,experiments,former,includes,nadst,former includes nadst,0.6819912195205688
translation,283,129,experiments,former,includes,"le et al. , 2020 )","former includes le et al. , 2020 )",0.6155039668083191
translation,283,129,experiments,former,includes,neural reading,former includes neural reading,0.6338708996772766
translation,283,129,experiments,latter,includes,neural reading,latter includes neural reading,0.6701722145080566
translation,283,129,experiments,latter,includes,trade,latter includes trade,0.6663826704025269
translation,283,129,experiments,latter,includes,"comer ( ren et al. , 2019 )","latter includes comer ( ren et al. , 2019 )",0.6318007111549377
translation,283,129,experiments,latter,includes,som - dst,latter includes som - dst,0.6527343988418579
translation,283,129,experiments,latter,includes,"dstqa ( zhou and small , 2019 )","latter includes dstqa ( zhou and small , 2019 )",0.6496739387512207
translation,283,129,experiments,latter,includes,nadst,latter includes nadst,0.6432121396064758
translation,283,129,experiments,latter,includes,"le et al. , 2020 )","latter includes le et al. , 2020 )",0.6243358254432678
translation,283,129,experiments,fjst,has,"eric et al. , 2019 )","fjst has eric et al. , 2019 )",0.5942492485046387
translation,283,129,experiments,nadst,has,"le et al. , 2020 )","nadst has le et al. , 2020 )",0.5897312760353088
translation,283,132,experiments,generated dialogue states,used for,knowledge base search,generated dialogue states used for knowledge base search,0.6569446921348572
translation,283,132,experiments,generated dialogue states,used for,response generation,generated dialogue states used for response generation,0.6452791094779968
translation,283,105,hyperparameters,framework,with,three pre-trained models,framework with three pre-trained models,0.614878237247467
translation,283,105,hyperparameters,8headed attention,with,hidden size d model = 512,8headed attention with hidden size d model = 512,0.6366511583328247
translation,283,105,hyperparameters,8headed attention,with,hidden size d model = 768,8headed attention with hidden size d model = 768,0.658829927444458
translation,283,105,hyperparameters,8headed attention,with,hidden size d model = 1024,8headed attention with hidden size d model = 1024,0.6433291435241699
translation,283,105,hyperparameters,12,-,headed attention,12 - headed attention,0.6677116751670837
translation,283,105,hyperparameters,headed attention,with,hidden size d model = 768,headed attention with hidden size d model = 768,0.6672725081443787
translation,283,105,hyperparameters,headed attention,with,hidden size d model = 1024,headed attention with hidden size d model = 1024,0.638232946395874
translation,283,105,hyperparameters,- headed attention,with,hidden size d model = 1024,- headed attention with hidden size d model = 1024,0.6325238943099976
translation,283,105,hyperparameters,t5 - small ( 60 m parameters ),has,6 encoder-decoder layers,t5 - small ( 60 m parameters ) has 6 encoder-decoder layers,0.585321843624115
translation,283,105,hyperparameters,each layer,has,8headed attention,each layer has 8headed attention,0.5885316133499146
translation,283,105,hyperparameters,t5 - base ( 220 m parameters ),has,12,t5 - base ( 220 m parameters ) has 12,0.6214925050735474
translation,283,105,hyperparameters,t5 - base ( 220 m parameters ),has,encoderdecoder layers,t5 - base ( 220 m parameters ) has encoderdecoder layers,0.577531635761261
translation,283,105,hyperparameters,12,has,encoderdecoder layers,12 has encoderdecoder layers,0.5853852033615112
translation,283,105,hyperparameters,12,has,headed attention,12 has headed attention,0.596263587474823
translation,283,105,hyperparameters,bartlarge ( 400 m parameters ),has,12 encoder- decoder layers,bartlarge ( 400 m parameters ) has 12 encoder- decoder layers,0.5761147141456604
translation,283,105,hyperparameters,each layer,has,16,each layer has 16,0.6095540523529053
translation,283,105,hyperparameters,each layer,has,- headed attention,each layer has - headed attention,0.5863394737243652
translation,283,105,hyperparameters,16,has,- headed attention,16 has - headed attention,0.6161223649978638
translation,283,105,hyperparameters,hyperparameters,set up,framework,hyperparameters set up framework,0.6796087622642517
translation,283,108,hyperparameters,fine-tuned,with,batch size,fine-tuned with batch size,0.6441797018051147
translation,283,108,hyperparameters,fine-tuned,with,early stop,fine-tuned with early stop,0.6354618072509766
translation,283,108,hyperparameters,batch size,of,64,batch size of 64,0.6741159558296204
translation,283,108,hyperparameters,early stop,according to,performance,early stop according to performance,0.6810544729232788
translation,283,108,hyperparameters,performance,on,validation set,performance on validation set,0.5804513692855835
translation,283,108,hyperparameters,hyperparameters,has,models,hyperparameters has models,0.5447477102279663
translation,283,4,model,minimalist transfer learning ( mintl ),to simplify,system design process,minimalist transfer learning ( mintl ) to simplify system design process,0.6627918481826782
translation,283,4,model,system design process,of,task - oriented dialogue systems,system design process of task - oriented dialogue systems,0.5549771785736084
translation,283,4,model,model,propose,minimalist transfer learning ( mintl ),model propose minimalist transfer learning ( mintl ),0.6540265679359436
translation,283,6,model,levenshtein belief spans ( lev ),allows,efficient dialogue state tracking,levenshtein belief spans ( lev ) allows efficient dialogue state tracking,0.6562412977218628
translation,283,6,model,efficient dialogue state tracking,with,minimal generation length,efficient dialogue state tracking with minimal generation length,0.5546138286590576
translation,283,6,model,model,introduce,levenshtein belief spans ( lev ),model introduce levenshtein belief spans ( lev ),0.6338819861412048
translation,283,20,model,prior knowledge of pre-trained language models,for improving,task - oriented dialogue systems,prior knowledge of pre-trained language models for improving task - oriented dialogue systems,0.6341947317123413
translation,283,21,model,plug-and - play,jointly learn,dialogue state tracking ( dst ),plug-and - play jointly learn dialogue state tracking ( dst ),0.6987177133560181
translation,283,21,model,plug-and - play,jointly learn,dialogue response generation,plug-and - play jointly learn dialogue response generation,0.6907296180725098
translation,283,21,model,pre-trained sequence - to-sequence ( seq2seq ) models,jointly learn,dialogue response generation,pre-trained sequence - to-sequence ( seq2seq ) models jointly learn dialogue response generation,0.7232112884521484
translation,283,21,model,plug-and - play,has,pre-trained sequence - to-sequence ( seq2seq ) models,plug-and - play has pre-trained sequence - to-sequence ( seq2seq ) models,0.5599460601806641
translation,283,21,model,model,propose,minimalist transfer learning ( mintl ),model propose minimalist transfer learning ( mintl ),0.6540265679359436
translation,283,22,model,levenshtein belief spans ( lev ),models,difference,levenshtein belief spans ( lev ) models difference,0.7228949666023254
translation,283,22,model,difference,between,old states,difference between old states,0.7318760752677917
translation,283,22,model,difference,between,states,difference between states,0.7447935342788696
translation,283,22,model,model,introduce,levenshtein belief spans ( lev ),model introduce levenshtein belief spans ( lev ),0.6338819861412048
translation,283,98,model,standard encoderdecoder transformer,with,bidirectional encoder,standard encoderdecoder transformer with bidirectional encoder,0.6149333119392395
translation,283,98,model,standard encoderdecoder transformer,with,autoregressive decoder,standard encoderdecoder transformer with autoregressive decoder,0.644843339920044
translation,283,106,model,kb state embeddings,to,pretrained models,kb state embeddings to pretrained models,0.45957326889038086
translation,283,106,model,pretrained models,by extending,token embeddings,pretrained models by extending token embeddings,0.62314772605896
translation,283,106,model,model,add,special segment token embeddings,model add special segment token embeddings,0.5907115936279297
translation,283,122,model,multi-action data augmentation method,by leveraging,system act and user act annotations,multi-action data augmentation method by leveraging system act and user act annotations,0.70072340965271
translation,283,29,results,our framework,with,two different pre-trained backbones,our framework with two different pre-trained backbones,0.5941883325576782
translation,283,29,results,them,improve,sota results,them improve sota results,0.7121663689613342
translation,283,29,results,sota results,by,large margin,sota results by large margin,0.6200010180473328
translation,283,29,results,results,instantiate,our framework,results instantiate our framework,0.6911035180091858
translation,283,133,results,mintl - based systems,achieve,best performance,mintl - based systems achieve best performance,0.6438591480255127
translation,283,133,results,best performance,in terms of,inform rate,best performance in terms of inform rate,0.7432718276977539
translation,283,133,results,best performance,in terms of,success rate,best performance in terms of success rate,0.7086392641067505
translation,283,133,results,best performance,in terms of,bleu,best performance in terms of bleu,0.6566612124443054
translation,283,134,results,our models,improve,previous sota model,our models improve previous sota model,0.6622063517570496
translation,283,134,results,previous sota model,by around,10 % success rate,previous sota model by around 10 % success rate,0.7005996108055115
translation,283,134,results,fewer human annotations,has,our models,fewer human annotations has our models,0.5376601815223694
translation,283,134,results,results,With,fewer human annotations,results With fewer human annotations,0.5997233390808105
translation,283,135,results,t5 - small,as,backbone,t5 - small as backbone,0.5534400343894958
translation,283,135,results,overall performance,of,sequicity,overall performance of sequicity,0.6719325184822083
translation,283,135,results,backbone,has,barely improves,backbone has barely improves,0.6441987156867981
translation,283,135,results,barely improves,has,overall performance,barely improves has overall performance,0.6013892292976379
translation,283,135,results,results,Using,t5 - small,results Using t5 - small,0.6449168920516968
translation,283,136,results,our approach,achieves,around 11 % higher success rate,our approach achieves around 11 % higher success rate,0.6623870134353638
translation,283,136,results,around 11 % higher success rate,with,same backbone model,around 11 % higher success rate with same backbone model,0.6850569248199463
translation,283,136,results,sequicity framework,has,our approach,sequicity framework has our approach,0.6308005452156067
translation,283,136,results,results,Compared to,sequicity framework,results Compared to sequicity framework,0.6773005723953247
translation,284,177,ablation-analysis,all representations,are,useful,all representations are useful,0.6170985102653503
translation,284,177,ablation-analysis,useful,in representing,information flow,useful in representing information flow,0.7201205492019653
translation,284,177,ablation-analysis,information flow,along,chain of interaction blocks,information flow along chain of interaction blocks,0.6379169821739197
translation,284,177,ablation-analysis,matching information,between,utterance - response pair,matching information between utterance - response pair,0.6405113935470581
translation,284,177,ablation-analysis,utterance - response pair,within,blocks,utterance - response pair within blocks,0.6464420557022095
translation,284,177,ablation-analysis,ablation analysis,conclude,all representations,ablation analysis conclude all representations,0.6796560287475586
translation,284,123,baselines,"match-lstm ( wang and jiang , 2016 )",calculating,matching score,"match-lstm ( wang and jiang , 2016 ) calculating matching score",0.58543461561203
translation,284,123,baselines,context- response matching,by concatenating,all utterances in a context,context- response matching by concatenating all utterances in a context,0.7168974876403809
translation,284,123,baselines,context- response matching,calculating,matching score,context- response matching calculating matching score,0.642695963382721
translation,284,123,baselines,all utterances in a context,into,single long document,all utterances in a context into single long document,0.5199496150016785
translation,284,123,baselines,matching score,between,document and a response candidate,matching score between document and a response candidate,0.6268264055252075
translation,284,125,baselines,dl2r,reformulates,last utterance,dl2r reformulates last utterance,0.6798071265220642
translation,284,125,baselines,last utterance,with,previous turns,last utterance with previous turns,0.6498495936393738
translation,284,125,baselines,previous turns,in,context,previous turns in context,0.5351031422615051
translation,284,125,baselines,context,with,different approaches,context with different approaches,0.6085944175720215
translation,284,125,baselines,baselines,has,dl2r,baselines has dl2r,0.5659240484237671
translation,284,130,baselines,relationship,among,utterances,relationship among utterances,0.6361433267593384
translation,284,130,baselines,relationship,by exploiting,deep utterance aggregation,relationship by exploiting deep utterance aggregation,0.7015596628189087
translation,284,130,baselines,utterances,within,context,utterances within context,0.6384322047233582
translation,284,130,baselines,deep utterance aggregation,to form,fine-grained context representation,deep utterance aggregation to form fine-grained context representation,0.5521122813224792
translation,284,130,baselines,dua,has,"zhang et al. , 2018 b","dua has zhang et al. , 2018 b",0.5535790324211121
translation,284,130,baselines,baselines,has,dua,baselines has dua,0.5969741940498352
translation,284,138,hyperparameters,ioi,set,size of word embedding,ioi set size of word embedding,0.6509217619895935
translation,284,138,hyperparameters,size of word embedding,as,200,size of word embedding as 200,0.5869948863983154
translation,284,138,hyperparameters,hyperparameters,In,ioi,hyperparameters In ioi,0.5196021199226379
translation,284,139,hyperparameters,cnn,in,matching aggregation,cnn in matching aggregation,0.5053083300590515
translation,284,139,hyperparameters,cnn,set,strides,cnn set strides,0.6629190444946289
translation,284,139,hyperparameters,window size,of,convolution and pooling kernels,window size of convolution and pooling kernels,0.5483770370483398
translation,284,139,hyperparameters,convolution and pooling kernels,as,"( 3 , 3 )","convolution and pooling kernels as ( 3 , 3 )",0.5396977066993713
translation,284,139,hyperparameters,strides,as,"( 1 , 1 ) and ( 3 , 3 )","strides as ( 1 , 1 ) and ( 3 , 3 )",0.5782774090766907
translation,284,139,hyperparameters,hyperparameters,For,cnn,hyperparameters For cnn,0.5553604364395142
translation,284,140,hyperparameters,number of convolution kernels,is,32,number of convolution kernels is 32,0.5458498001098633
translation,284,140,hyperparameters,number of convolution kernels,is,16,number of convolution kernels is 16,0.5620041489601135
translation,284,140,hyperparameters,32,in,first layer,32 in first layer,0.5446420907974243
translation,284,140,hyperparameters,16,in,second layer,16 in second layer,0.574103832244873
translation,284,140,hyperparameters,hyperparameters,has,number of convolution kernels,hyperparameters has number of convolution kernels,0.4937756061553955
translation,284,141,hyperparameters,dimension,of,hidden states,dimension of hidden states,0.5819122791290283
translation,284,141,hyperparameters,hidden states,of,gru,hidden states of gru,0.6313064694404602
translation,284,141,hyperparameters,gru,set as,200,gru set as 200,0.6302684545516968
translation,284,141,hyperparameters,hyperparameters,has,dimension,hyperparameters has dimension,0.514893651008606
translation,284,142,hyperparameters,length,of,context,length of context,0.5951420068740845
translation,284,142,hyperparameters,length,of,length of an utterance,length of length of an utterance,0.5632270574569702
translation,284,142,hyperparameters,context,to,10 turns,context to 10 turns,0.6302911043167114
translation,284,142,hyperparameters,length of an utterance,to,50 words,length of an utterance to 50 words,0.5605769157409668
translation,284,142,hyperparameters,hyperparameters,limit,length,hyperparameters limit length,0.6949666738510132
translation,284,142,hyperparameters,hyperparameters,limit,length of an utterance,hyperparameters limit length of an utterance,0.6202137470245361
translation,284,143,hyperparameters,truncation or zero-padding,applied to,context,truncation or zero-padding applied to context,0.7114933133125305
translation,284,143,hyperparameters,truncation or zero-padding,applied to,response candidate,truncation or zero-padding applied to response candidate,0.6944786906242371
translation,284,143,hyperparameters,hyperparameters,has,truncation or zero-padding,hyperparameters has truncation or zero-padding,0.5570148229598999
translation,284,144,hyperparameters,"number of interaction blocks ( i.e. , l )",in,ioi,"number of interaction blocks ( i.e. , l ) in ioi",0.5621593594551086
translation,284,144,hyperparameters,l = 7,in comparison with,baseline models,l = 7 in comparison with baseline models,0.688159167766571
translation,284,144,hyperparameters,hyperparameters,gradually increase,"number of interaction blocks ( i.e. , l )","hyperparameters gradually increase number of interaction blocks ( i.e. , l )",0.7050827145576477
translation,284,144,hyperparameters,hyperparameters,set,l = 7,hyperparameters set l = 7,0.649642825126648
translation,284,145,hyperparameters,optimization,choose,0.2,optimization choose 0.2,0.6861036419868469
translation,284,145,hyperparameters,optimization,choose,50,optimization choose 50,0.7049772143363953
translation,284,145,hyperparameters,0.2,as,dropout rate,0.2 as dropout rate,0.5029371380805969
translation,284,145,hyperparameters,50,as,size of mini-batches,50 as size of mini-batches,0.5666813850402832
translation,284,145,hyperparameters,hyperparameters,In,optimization,hyperparameters In optimization,0.4939781427383423
translation,284,146,hyperparameters,learning rate,initialized as,0.0005,learning rate initialized as 0.0005,0.6708728671073914
translation,284,146,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,284,146,hyperparameters,hyperparameters,has,exponentially decayed,hyperparameters has exponentially decayed,0.5365432500839233
translation,284,7,model,go deep,by proposing,interaction - over-interaction network ( ioi ),go deep by proposing interaction - over-interaction network ( ioi ),0.7168463468551636
translation,284,7,model,utterance - response interaction,has,go deep,utterance - response interaction has go deep,0.6069985628128052
translation,284,7,model,model,let,utterance - response interaction,model let utterance - response interaction,0.6436628103256226
translation,284,8,model,matching,by stacking,multiple interaction blocks,matching by stacking multiple interaction blocks,0.7753658294677734
translation,284,8,model,multiple interaction blocks,in which,residual information,multiple interaction blocks in which residual information,0.6265740990638733
translation,284,8,model,residual information,from,one time of interaction,residual information from one time of interaction,0.5228870511054993
translation,284,8,model,one time of interaction,initiates,interaction process,one time of interaction initiates interaction process,0.7061759233474731
translation,284,8,model,model,performs,matching,model performs matching,0.6559018492698669
translation,284,20,model,context- response matching,with,multiple steps of interaction,context- response matching with multiple steps of interaction,0.6202903389930725
translation,284,20,model,multiple steps of interaction,where,residual information from one time of interaction,multiple steps of interaction where residual information from one time of interaction,0.5556880235671997
translation,284,20,model,residual information from one time of interaction,leveraged for,additional interactions,residual information from one time of interaction leveraged for additional interactions,0.6680809855461121
translation,284,20,model,model,consider,context- response matching,model consider context- response matching,0.697040855884552
translation,284,22,model,interaction - over-interaction network ( ioi ),for,context- response matching,interaction - over-interaction network ( ioi ) for context- response matching,0.6027623414993286
translation,284,22,model,model,propose,interaction - over-interaction network ( ioi ),model propose interaction - over-interaction network ( ioi ),0.6796649098396301
translation,284,23,model,key component,in,ioi,key component in ioi,0.6036332249641418
translation,284,23,model,model,has,key component,model has key component,0.5345369577407837
translation,284,124,model,matching degree,between,context and a response candidate,matching degree between context and a response candidate,0.6495001316070557
translation,284,124,model,context and a response candidate,from both,word sequence view and an utterance sequence view,context and a response candidate from both word sequence view and an utterance sequence view,0.6596879959106445
translation,284,124,model,model,has,"multi-view ( zhou et al. , 2016 )","model has multi-view ( zhou et al. , 2016 )",0.5091800689697266
translation,284,128,model,smn,transforms,interaction matrices,smn transforms interaction matrices,0.7479318976402283
translation,284,128,model,each utterance,in,context,each utterance in context,0.503588080406189
translation,284,128,model,each utterance,interact with,response candidate,each utterance interact with response candidate,0.6762744188308716
translation,284,128,model,response candidate,at,beginning,response candidate at beginning,0.589627206325531
translation,284,128,model,interaction matrices,into,matching vector,interaction matrices into matching vector,0.5467509627342224
translation,284,128,model,matching vector,with,cnn,matching vector with cnn,0.6195349097251892
translation,284,128,model,model,has,smn,model has smn,0.6165951490402222
translation,284,132,model,each utterance,in,context,each utterance in context,0.503588080406189
translation,284,132,model,each utterance,interact with,response candidate,each utterance interact with response candidate,0.6762744188308716
translation,284,132,model,response candidate,at,different levels,response candidate at different levels,0.5994584560394287
translation,284,132,model,representations,obtained by,stacked self-attention module,representations obtained by stacked self-attention module,0.612270176410675
translation,284,132,model,representations,obtained by,a cross-attention module,representations obtained by a cross-attention module,0.647432804107666
translation,284,132,model,model,lets,each utterance,model lets each utterance,0.6391531825065613
translation,284,132,model,model,has,dam,model has dam,0.6231600046157837
translation,284,30,results,stateof - the - art methods,with,7 interaction blocks,stateof - the - art methods with 7 interaction blocks,0.6062512397766113
translation,284,30,results,7 interaction blocks,over,all metrics,7 interaction blocks over all metrics,0.6205662488937378
translation,284,30,results,ioi,has,significantly outperform,ioi has significantly outperform,0.6277660727500916
translation,284,30,results,significantly outperform,has,stateof - the - art methods,significantly outperform has stateof - the - art methods,0.5549909472465515
translation,284,31,results,best performing,on,all the three data sets,best performing on all the three data sets,0.515033483505249
translation,284,31,results,ioi,achieves,2.9 % absolute improvement,ioi achieves 2.9 % absolute improvement,0.6549813747406006
translation,284,31,results,ioi,achieves,2.3 % absolute improvement,ioi achieves 2.3 % absolute improvement,0.6588113903999329
translation,284,31,results,ioi,achieves,3.7 % absolute improvement,ioi achieves 3.7 % absolute improvement,0.6598759889602661
translation,284,31,results,2.9 % absolute improvement,on,r 10 @1,2.9 % absolute improvement on r 10 @1,0.5871933698654175
translation,284,31,results,2.9 % absolute improvement,on,r 10 @1,2.9 % absolute improvement on r 10 @1,0.5871933698654175
translation,284,31,results,2.9 % absolute improvement,on,r 10 @1,2.9 % absolute improvement on r 10 @1,0.5871933698654175
translation,284,31,results,r 10 @1,on,ubuntu data,r 10 @1 on ubuntu data,0.539049506187439
translation,284,31,results,2.3 % absolute improvement,on,map,2.3 % absolute improvement on map,0.5666802525520325
translation,284,31,results,map,on,douban data,map on douban data,0.5677334666252136
translation,284,31,results,3.7 % absolute improvement,on,r 10 @1,3.7 % absolute improvement on r 10 @1,0.5901228785514832
translation,284,31,results,r 10 @1,on,ecommerce data,r 10 @1 on ecommerce data,0.5793383717536926
translation,284,31,results,deep attention matching network ( dam ),has,best performing,deep attention matching network ( dam ) has best performing,0.5446545481681824
translation,284,31,results,deep attention matching network ( dam ),has,ioi,deep attention matching network ( dam ) has ioi,0.5901560187339783
translation,284,31,results,results,Compared with,deep attention matching network ( dam ),results Compared with deep attention matching network ( dam ),0.653645932674408
translation,284,32,results,depth,brings,improvement,depth brings improvement,0.6520970463752747
translation,284,32,results,improvement,to,performance,improvement to performance,0.5461621880531311
translation,284,32,results,performance,of,ioi,performance of ioi,0.6502758860588074
translation,284,32,results,performance,of,ioi,performance of ioi,0.6502758860588074
translation,284,32,results,performance,of,ioi,performance of ioi,0.6502758860588074
translation,284,32,results,ioi,with,1 interaction block,ioi with 1 interaction block,0.6737782955169678
translation,284,32,results,ioi,with,1 interaction block,ioi with 1 interaction block,0.6737782955169678
translation,284,32,results,1 interaction block,performs,worse,1 interaction block performs worse,0.6930065155029297
translation,284,32,results,worse,than,dam,worse than dam,0.7085736989974976
translation,284,32,results,worse,than,the e-commerce data,worse than the e-commerce data,0.5350117087364197
translation,284,32,results,worse,on,the e-commerce data,worse on the e-commerce data,0.5295664668083191
translation,284,32,results,worse,on,ubuntu data,worse on ubuntu data,0.5424800515174866
translation,284,32,results,dam,on,douban data,dam on douban data,0.5828474760055542
translation,284,32,results,dam,on,the e-commerce data,dam on the e-commerce data,0.5047508478164673
translation,284,32,results,dam,on,ubuntu data,dam on ubuntu data,0.5764296650886536
translation,284,32,results,gap,on,r 10 @1,gap on r 10 @1,0.6553739309310913
translation,284,32,results,r 10 @1,between,ioi,r 10 @1 between ioi,0.6589401960372925
translation,284,153,results,improvements,from,ioi -local,improvements from ioi -local,0.5870751142501831
translation,284,153,results,improvements,from,ioi - global,improvements from ioi - global,0.5673502087593079
translation,284,153,results,ioi -local,on,all metrics,ioi -local on all metrics,0.5526143908500671
translation,284,153,results,ioi -local,on,few metrics,ioi -local on few metrics,0.5774828791618347
translation,284,153,results,ioi -local,on,few metrics,ioi -local on few metrics,0.5774828791618347
translation,284,153,results,ioi - global,on,few metrics,ioi - global on few metrics,0.5735668540000916
translation,284,153,results,ioi - global,are,statistically significant,ioi - global are statistically significant,0.577900767326355
translation,284,153,results,few metrics,are,statistically significant,few metrics are statistically significant,0.5617719888687134
translation,284,153,results,both ioilocal and ioi -global,has,outperform,both ioilocal and ioi -global has outperform,0.5998362898826599
translation,284,153,results,outperform,has,best performing baseline,outperform has best performing baseline,0.6194249391555786
translation,284,153,results,results,see that,both ioilocal and ioi -global,results see that both ioilocal and ioi -global,0.6307926774024963
translation,284,154,results,ioi -local,consistently better than,ioiglobal,ioi -local consistently better than ioiglobal,0.7675522565841675
translation,284,154,results,ioiglobal,over,all metrics,ioiglobal over all metrics,0.7122020125389099
translation,284,154,results,all metrics,on,all the three data sets,all metrics on all the three data sets,0.5392348170280457
translation,284,154,results,results,has,ioi -local,results has ioi -local,0.5642999410629272
translation,284,163,results,improvement,to,dam,improvement to dam,0.6609265804290771
translation,284,163,results,dam,from,deep model ( l = 7 ),dam from deep model ( l = 7 ),0.597918689250946
translation,284,163,results,dam,from,shallow model ( l = 1 ),dam from shallow model ( l = 1 ),0.5769566893577576
translation,284,163,results,dam,more than twice as much as,shallow model ( l = 1 ),dam more than twice as much as shallow model ( l = 1 ),0.6154069900512695
translation,284,163,results,ubuntu data,has,improvement,ubuntu data has improvement,0.5710502862930298
translation,284,163,results,results,On,ubuntu data,results On ubuntu data,0.5762377381324768
translation,284,170,results,i oi,good at dealing with,contexts,i oi good at dealing with contexts,0.7084110975265503
translation,284,170,results,i oi,good at dealing with,contexts,i oi good at dealing with contexts,0.7084110975265503
translation,284,170,results,contexts,with,long utterances,contexts with long utterances,0.6032839417457581
translation,284,170,results,ioi,performs,well,ioi performs well,0.713650107383728
translation,284,170,results,well,on,contexts,well on contexts,0.5646288394927979
translation,284,170,results,deep form of our model,always better than,shallow form,deep form of our model always better than shallow form,0.7548467516899109
translation,284,170,results,results,find that,i oi,results find that i oi,0.6147778630256653
translation,285,147,hyperparameters,batch sgd,with,batch size,batch sgd with batch size,0.6497457027435303
translation,285,147,hyperparameters,batch sgd,with,adam optimizer,batch sgd with adam optimizer,0.6227790117263794
translation,285,147,hyperparameters,batch size,of,50,batch size of 50,0.6921922564506531
translation,285,147,hyperparameters,adam optimizer,used for,training,adam optimizer used for training,0.6588284969329834
translation,285,147,hyperparameters,hyperparameters,Mini-,batch sgd,hyperparameters Mini- batch sgd,0.6256963610649109
translation,285,6,model,e2e architecture,based on,pointer network ( ptrnet ),e2e architecture based on pointer network ( ptrnet ),0.6833590269088745
translation,285,6,model,e2e architecture,effectively extract,unknown slot values,e2e architecture effectively extract unknown slot values,0.7040435075759888
translation,285,6,model,model,describe,e2e architecture,model describe e2e architecture,0.6080267429351807
translation,285,28,model,e2e tracker,effectively handle,unknown value sets,e2e tracker effectively handle unknown value sets,0.7015454769134521
translation,285,28,model,model,introduce,e2e tracker,model introduce e2e tracker,0.632233202457428
translation,285,29,model,proposed solution,based on,recently introduced pointer network ( ptrnet ),proposed solution based on recently introduced pointer network ( ptrnet ),0.6718236207962036
translation,285,29,model,recently introduced pointer network ( ptrnet ),performs,state tracking,recently introduced pointer network ( ptrnet ) performs state tracking,0.6180450916290283
translation,285,29,model,state tracking,in,extractive fashion,state tracking in extractive fashion,0.5699358582496643
translation,285,29,model,model,based on,recently introduced pointer network ( ptrnet ),model based on recently introduced pointer network ( ptrnet ),0.6994192600250244
translation,285,29,model,model,has,proposed solution,model has proposed solution,0.6040842533111572
translation,285,146,model,joint architecture,trained separately for,each slot type,joint architecture trained separately for each slot type,0.7404659986495972
translation,285,146,model,each slot type,by minimizing,sum of the cross entropy loss,each slot type by minimizing sum of the cross entropy loss,0.7169646620750427
translation,285,146,model,sum of the cross entropy loss,from,ptrnet and the classifier,sum of the cross entropy loss from ptrnet and the classifier,0.5693869590759277
translation,285,146,model,model,has,joint architecture,model has joint architecture,0.5556433796882629
translation,285,174,results,our ptrnet model,holds,various advantages,our ptrnet model holds various advantages,0.6489243507385254
translation,285,174,results,various advantages,against,all baseline models,various advantages against all baseline models,0.6254252791404724
translation,285,177,results,standard training process,with,targeted dropout,standard training process with targeted dropout,0.6233410835266113
translation,285,177,results,standard training process,performs,poorly,standard training process performs poorly,0.6770886778831482
translation,285,177,results,poorly,when,food types,poorly when food types,0.6785148978233337
translation,285,177,results,results,has,standard training process,results has standard training process,0.544805109500885
translation,285,178,results,small dropout probability,of,5 %,small dropout probability of 5 %,0.5253243446350098
translation,285,178,results,accuracy,on,unknown values,accuracy on unknown values,0.5065889358520508
translation,285,178,results,accuracy,on,other values,accuracy on other values,0.4754537045955658
translation,285,178,results,accuracy,on,other values,accuracy on other values,0.4754537045955658
translation,285,178,results,unknown values,essentially increases by,three times,unknown values essentially increases by three times,0.6499138474464417
translation,285,178,results,three times,from,11.6 % to 34.4 %,three times from 11.6 % to 34.4 %,0.5357283353805542
translation,285,178,results,accuracy,on,other values,accuracy on other values,0.4754537045955658
translation,285,178,results,small dropout probability,has,accuracy,small dropout probability has accuracy,0.5371619462966919
translation,285,178,results,5 %,has,accuracy,5 % has accuracy,0.6003517508506775
translation,285,178,results,results,With,small dropout probability,results With small dropout probability,0.5988680720329285
translation,286,99,ablation-analysis,indepth analysis,of,ctx and psa planner output,indepth analysis of ctx and psa planner output,0.5921561121940613
translation,286,99,ablation-analysis,ctx and psa planner output,on,entire testing set,ctx and psa planner output on entire testing set,0.5497621893882751
translation,286,99,ablation-analysis,ablation analysis,conducted,indepth analysis,ablation analysis conducted indepth analysis,0.696294903755188
translation,286,63,baselines,context attention planner ( ctx ),based on,encoder / decoder architecture,context attention planner ( ctx ) based on encoder / decoder architecture,0.6539479494094849
translation,286,63,baselines,baselines,has,context attention planner ( ctx ),baselines has context attention planner ( ctx ),0.5575873851776123
translation,286,83,experimental-setup,"publicly available gpt - 2 model ( radford et al. , 2019 )",with,117 m parameters,"publicly available gpt - 2 model ( radford et al. , 2019 ) with 117 m parameters",0.5580710172653198
translation,286,83,experimental-setup,"publicly available gpt - 2 model ( radford et al. , 2019 )",with,12 layers,"publicly available gpt - 2 model ( radford et al. , 2019 ) with 12 layers",0.6049298048019409
translation,286,83,experimental-setup,"publicly available gpt - 2 model ( radford et al. , 2019 )",with,12 heads,"publicly available gpt - 2 model ( radford et al. , 2019 ) with 12 heads",0.6182811856269836
translation,286,83,experimental-setup,experimental setup,use,"publicly available gpt - 2 model ( radford et al. , 2019 )","experimental setup use publicly available gpt - 2 model ( radford et al. , 2019 )",0.5792651176452637
translation,286,84,experimental-setup,tokenized,using,byte-pair encoding,tokenized using byte-pair encoding,0.6691793203353882
translation,286,84,experimental-setup,byte-pair encoding,to reduce,vocabulary size,byte-pair encoding to reduce vocabulary size,0.6456800103187561
translation,286,84,experimental-setup,experimental setup,has,input utterances and the plans,experimental setup has input utterances and the plans,0.5565420389175415
translation,286,89,experimental-setup,roughly 15 hours,to train,planner and realization component,roughly 15 hours to train planner and realization component,0.6960890889167786
translation,286,88,model,decoding,use,nucleus sam-pling,decoding use nucleus sam-pling,0.6799184679985046
translation,286,88,model,nucleus sam-pling,both in,planning and realization phase,nucleus sam-pling both in planning and realization phase,0.6768848299980164
translation,286,88,model,model,During,decoding,model During decoding,0.7364474534988403
translation,286,98,results,psa,able to achieve,higher word overlap metrics,psa able to achieve higher word overlap metrics,0.6634336709976196
translation,286,98,results,higher word overlap metrics,with respect to,silver standard,higher word overlap metrics with respect to silver standard,0.6382684707641602
translation,286,98,results,results,find,psa,results find psa,0.5424491763114929
translation,286,110,results,ctx planner,is,better suited,ctx planner is better suited,0.5946776866912842
translation,286,110,results,better suited,to generate,appropriate response,better suited to generate appropriate response,0.6581655740737915
translation,286,110,results,appropriate response,over,psa planner,appropriate response over psa planner,0.7293095588684082
translation,286,111,results,better able,to generate,"appropriate ask / framing types , actions , and targets","better able to generate appropriate ask / framing types , actions , and targets",0.6742236018180847
translation,286,118,results,experts,prefer,plans,experts prefer plans,0.7559670209884644
translation,286,118,results,plans,produced by,symbolic planner,plans produced by symbolic planner,0.64372318983078
translation,286,118,results,plans,not over,psa planner output,plans not over psa planner output,0.6611329317092896
translation,286,118,results,symbolic planner,over,ctx output,symbolic planner over ctx output,0.6391083002090454
translation,286,118,results,results,find that,experts,results find that experts,0.6176390051841736
translation,286,121,results,more faithful,to,ground truth,more faithful to ground truth,0.5567675828933716
translation,286,124,results,outperforms,on,automated metrics,outperforms on automated metrics,0.5528535842895508
translation,286,124,results,ctx planner,on,automated metrics,ctx planner on automated metrics,0.5426021814346313
translation,286,124,results,psa,has,outperforms,psa has outperforms,0.6623282432556152
translation,286,124,results,outperforms,has,ctx planner,outperforms has ctx planner,0.6496469378471375
translation,286,124,results,results,find,psa,results find psa,0.5424491763114929
translation,286,126,results,find that both the planners,able to generate,appropriate plans,find that both the planners able to generate appropriate plans,0.748894453048706
translation,286,126,results,appropriate plans,with,"appropriate ask / framing type , action , and target","appropriate plans with appropriate ask / framing type , action , and target",0.5900749564170837
translation,286,126,results,"appropriate ask / framing type , action , and target",for,realization phase,"appropriate ask / framing type , action , and target for realization phase",0.6088772416114807
translation,286,127,results,psa planner output,preferred over,ctx planner,psa planner output preferred over ctx planner,0.7258201837539673
translation,286,127,results,silver standard plans,has,psa planner output,silver standard plans has psa planner output,0.5720688700675964
translation,286,134,results,diversity,incorporating,plans,diversity incorporating plans,0.766921877861023
translation,286,134,results,plans,as,additional input,plans as additional input,0.5140010118484497
translation,286,134,results,additional input,to,realization phase,additional input to realization phase,0.593411386013031
translation,286,134,results,realization phase,helps achieve,higher score,realization phase helps achieve higher score,0.702639102935791
translation,286,134,results,higher score,than,no planner,higher score than no planner,0.6393246054649353
translation,286,134,results,results,on,both corpora,results on both corpora,0.48045825958251953
translation,286,135,results,realizer,able to achieve,higher diversity,realizer able to achieve higher diversity,0.6971132159233093
translation,286,135,results,without any plans,able to achieve,higher diversity,without any plans able to achieve higher diversity,0.6811655163764954
translation,286,135,results,realizer,has,without any plans,realizer has without any plans,0.6177374720573425
translation,286,135,results,results,find that,realizer,results find that realizer,0.7006688714027405
translation,286,147,results,plans,generated by,ctx planner,plans generated by ctx planner,0.7122995853424072
translation,286,147,results,plans,helps generate,better responses,plans helps generate better responses,0.7715389728546143
translation,286,148,results,quality and usefulness,incorporating,planning,quality and usefulness incorporating planning,0.7220363616943359
translation,286,148,results,planning,as,additional input,planning as additional input,0.5267901420593262
translation,286,148,results,performs better,than,no plan,performs better than no plan,0.645359456539154
translation,286,148,results,additional input,has,performs better,additional input has performs better,0.6280086040496826
translation,286,156,results,e.g. 53 % vs. 26 %,on,appropriateness metric,e.g. 53 % vs. 26 % on appropriateness metric,0.5799816846847534
translation,286,156,results,significant,has,e.g. 53 % vs. 26 %,significant has e.g. 53 % vs. 26 %,0.5501711368560791
translation,286,156,results,results,has,proportion of time,results has proportion of time,0.5351849794387817
translation,286,157,results,responses,generated using,ctx and psa plans,responses generated using ctx and psa plans,0.6952809691429138
translation,286,157,results,responses,comparable to,responses,responses comparable to responses,0.6997135281562805
translation,286,157,results,responses,produced by,humans,responses produced by humans,0.645673394203186
translation,286,157,results,responses,comparable to,responses,responses comparable to responses,0.6997135281562805
translation,286,157,results,responses,produced by,humans,responses produced by humans,0.645673394203186
translation,286,157,results,responses,produced by,humans,responses produced by humans,0.645673394203186
translation,286,158,results,psa planner - based responses,perform,better,psa planner - based responses perform better,0.6017424464225769
translation,286,158,results,psa planner - based responses,perform,persuasion,psa planner - based responses perform persuasion,0.5753461122512817
translation,286,158,results,psa planner - based responses,on,persuasion,psa planner - based responses on persuasion,0.5522356629371643
translation,286,158,results,persuasion,for,social good,persuasion for social good,0.5883864760398865
translation,286,158,results,results,find that,psa planner - based responses,results find that psa planner - based responses,0.5924170613288879
translation,286,159,results,ctx planner based responses,performs,better,ctx planner based responses performs better,0.6555940508842468
translation,286,159,results,better,than,ground truth utterances,better than ground truth utterances,0.5759563446044922
translation,286,159,results,ground truth utterances,for,anti-scam corpus,ground truth utterances for anti-scam corpus,0.588013768196106
translation,286,159,results,ground truth response,has,"% , 37 % and 37 %","ground truth response has % , 37 % and 37 %",0.5693782567977905
translation,286,159,results,results,has,ctx planner based responses,results has ctx planner based responses,0.5433968305587769
translation,286,162,results,"ctx , psa , and no planner output",on,automated metrics,"ctx , psa , and no planner output on automated metrics",0.5354750156402588
translation,286,162,results,automated metrics,of,bleu and bert - score,automated metrics of bleu and bert - score,0.5157107710838318
translation,286,162,results,symbolic planner - realized output,has,outperforms,symbolic planner - realized output has outperforms,0.628129780292511
translation,286,162,results,outperforms,has,"ctx , psa , and no planner output","outperforms has ctx , psa , and no planner output",0.593779444694519
translation,286,162,results,results,find that,symbolic planner - realized output,results find that symbolic planner - realized output,0.6428072452545166
translation,286,164,results,human-generated utterances ( ground truth ),are,preferred overall,human-generated utterances ( ground truth ) are preferred overall,0.5500714778900146
translation,286,164,results,preferred overall,than,model outputs,preferred overall than model outputs,0.5454598665237427
translation,286,164,results,results,find that,human-generated utterances ( ground truth ),results find that human-generated utterances ( ground truth ),0.6147704720497131
translation,286,197,results,two separate human crowdsourced studies,is,"decoupling realization , and planning phases","two separate human crowdsourced studies is decoupling realization , and planning phases",0.5501266717910767
translation,286,197,results,outperforms,across,three metrics,outperforms across three metrics,0.6839244365692139
translation,286,197,results,"decoupling realization , and planning phases",has,outperforms,"decoupling realization , and planning phases has outperforms",0.5874214768409729
translation,286,197,results,outperforms,has,endto-end no planner system,outperforms has endto-end no planner system,0.6249291300773621
translation,287,19,baselines,an open-source library,fast development of,dialogue systems,an open-source library fast development of dialogue systems,0.7481096386909485
translation,287,19,baselines,deeppavlov,has,an open-source library,deeppavlov has an open-source library,0.5388473868370056
translation,287,19,baselines,baselines,create,deeppavlov,baselines create deeppavlov,0.6234776377677917
translation,288,156,ablation-analysis,f-measure and recall,for,med,f-measure and recall for med,0.6595703959465027
translation,288,156,ablation-analysis,rise considerably,by,3.0 and 7.8,rise considerably by 3.0 and 7.8,0.617480456829071
translation,288,156,ablation-analysis,med,has,rise considerably,med has rise considerably,0.6679590344429016
translation,288,156,ablation-analysis,ablation analysis,has,f-measure and recall,ablation analysis has f-measure and recall,0.5881449580192566
translation,288,183,ablation-analysis,anaphoricity features,do not contribute,positively,anaphoricity features do not contribute positively,0.7414130568504333
translation,288,183,ablation-analysis,positively,to,overall performance,positively to overall performance,0.5158240795135498
translation,288,183,ablation-analysis,structured features,has,anaphoricity features,structured features has anaphoricity features,0.551626443862915
translation,288,7,model,) machine learning approach,to,information -status determination,) machine learning approach to information -status determination,0.5509073138237
translation,288,7,model,information -status determination,with,lexical and structured features,information -status determination with lexical and structured features,0.6233536005020142
translation,288,7,model,learned knowledge,of,information status,learned knowledge of information status,0.5362451076507568
translation,288,7,model,information status,of,each discourse entity,information status of each discourse entity,0.527064859867096
translation,288,7,model,each discourse entity,for,coreference resolution,each discourse entity for coreference resolution,0.5981311798095703
translation,288,7,model,model,exploit,learned knowledge,model exploit learned knowledge,0.7563819289207458
translation,288,22,model,structured features,based on,syntactic parse trees,structured features based on syntactic parse trees,0.5713069438934326
translation,288,22,model,model,describe,learning approach,model describe learning approach,0.6484172940254211
translation,288,155,results,baseline and baseline + lexical,augmenting,nissim 's feature set,baseline and baseline + lexical augmenting nissim 's feature set,0.6447596549987793
translation,288,155,results,nissim 's feature set,with,lexical features,nissim 's feature set with lexical features,0.5979952216148376
translation,288,155,results,f-measure scores,on,all three classes,f-measure scores on all three classes,0.5217356085777283
translation,288,155,results,lexical features,has,improves,lexical features has improves,0.6277179718017578
translation,288,155,results,improves,has,f-measure scores,improves has f-measure scores,0.5885160565376282
translation,288,155,results,results,Comparing,baseline and baseline + lexical,results Comparing baseline and baseline + lexical,0.6803749799728394
translation,288,158,results,baseline + lexical and baseline + both,addition of,structured features,baseline + lexical and baseline + both addition of structured features,0.6131352186203003
translation,288,158,results,further boost,to,performance,further boost to performance,0.5835223197937012
translation,288,158,results,f-measure,increases by,2.8-3.9,f-measure increases by 2.8-3.9,0.705207347869873
translation,288,158,results,2.8-3.9,for,three classes,2.8-3.9 for three classes,0.6409226059913635
translation,288,158,results,results,Comparing,baseline + lexical and baseline + both,results Comparing baseline + lexical and baseline + both,0.6216133832931519
translation,288,159,results,results,substantiate,hypothesis,results substantiate hypothesis,0.6430118083953857
translation,288,159,results,results,employing,richer representation,results employing richer representation,0.6469513177871704
translation,288,159,results,hypothesis,employing,richer representation,hypothesis employing richer representation,0.6710505485534668
translation,288,159,results,richer representation,of,syntactic context,richer representation of syntactic context,0.545407772064209
translation,288,159,results,richer representation,beneficial to,information -status classification,richer representation beneficial to information -status classification,0.7202467918395996
translation,288,159,results,results,substantiate,hypothesis,results substantiate hypothesis,0.6430118083953857
translation,288,159,results,results,employing,richer representation,results employing richer representation,0.6469513177871704
translation,288,159,results,results,has,results,results has results,0.48582205176353455
translation,288,160,results,baseline and baseline + both,see that,f-measure,baseline and baseline + both see that f-measure,0.6459494829177856
translation,288,160,results,improves considerably,by,5 - 6.9,improves considerably by 5 - 6.9,0.6313914060592651
translation,288,160,results,5 - 6.9,for,three classes,5 - 6.9 for three classes,0.6270064115524292
translation,288,160,results,f-measure,has,improves considerably,f-measure has improves considerably,0.5972784757614136
translation,288,160,results,results,Comparing,baseline and baseline + both,results Comparing baseline and baseline + both,0.6479063034057617
translation,288,161,results,results,provide,suggestive evidence,results provide suggestive evidence,0.6137669682502747
translation,288,161,results,both types of features,effective at improving,information -status classifier,both types of features effective at improving information -status classifier,0.7232896685600281
translation,288,161,results,information -status classifier,employs,nissim 's features,information -status classifier employs nissim 's features,0.6124663949012756
translation,288,161,results,results,provide,suggestive evidence,results provide suggestive evidence,0.6137669682502747
translation,288,161,results,results,has,results,results has results,0.48582205176353455
translation,288,163,results,lexical features,to,baseline features,lexical features to baseline features,0.4924291968345642
translation,288,163,results,lexical features,improves,accuracy,lexical features improves accuracy,0.6970894932746887
translation,288,163,results,lexical features,improves,accuracy,lexical features improves accuracy,0.6970894932746887
translation,288,163,results,accuracy,by,2.2 %,accuracy by 2.2 %,0.5730752348899841
translation,288,163,results,accuracy,by,5.9 %,accuracy by 5.9 %,0.5807486176490784
translation,288,163,results,accuracy,by,5.9 %,accuracy by 5.9 %,0.5807486176490784
translation,288,163,results,structured features,improves,accuracy,structured features improves accuracy,0.7390068769454956
translation,288,163,results,accuracy,by,5.9 %,accuracy by 5.9 %,0.5807486176490784
translation,288,163,results,results,adding,lexical features,results adding lexical features,0.5886479020118713
translation,288,163,results,results,adding,structured features,results adding structured features,0.6352025270462036
translation,288,164,results,our two types of features,used in combination with,nissim 's features,our two types of features used in combination with nissim 's features,0.6268932819366455
translation,288,164,results,our two types of features,improve,baseline,our two types of features improve baseline,0.6278007626533508
translation,288,164,results,our two types of features,improve,substantially,our two types of features improve substantially,0.6400308012962341
translation,288,164,results,baseline,by,accuracy,baseline by accuracy,0.5643240809440613
translation,288,164,results,accuracy,of,8.1 %,accuracy of 8.1 %,0.5711699724197388
translation,288,164,results,baseline,has,substantially,baseline has substantially,0.6289440393447876
translation,288,164,results,results,has,our two types of features,results has our two types of features,0.5620328783988953
translation,288,165,results,systems,are,consistent,systems are consistent,0.6472156047821045
translation,288,165,results,consistent,in terms of,relative performance,consistent in terms of relative performance,0.7627878785133362
translation,288,165,results,relative performance,for,three classes,relative performance for three classes,0.6279197931289673
translation,288,181,results,results,obtained using,baseline features,results obtained using baseline features,0.5967931747436523
translation,288,182,results,baseline + ana and baseline + lexical + ana,addition of,anaphoricity features,baseline + ana and baseline + lexical + ana addition of anaphoricity features,0.6065478324890137
translation,288,182,results,results,Comparing,baseline + ana and baseline + lexical + ana,results Comparing baseline + ana and baseline + lexical + ana,0.61949223279953
translation,288,255,results,increases,in comparison to,baseline,increases in comparison to baseline,0.7043711543083191
translation,288,255,results,b 3 precision,has,increases,b 3 precision has increases,0.6474438905715942
translation,288,255,results,results,has,b 3 precision,results has b 3 precision,0.5685015916824341
translation,288,256,results,knowledge of information status,improves,coreference performance,knowledge of information status improves coreference performance,0.6880185604095459
translation,288,256,results,f-measure scores,increase by,1.0- 1.1 % ( b 3 ),f-measure scores increase by 1.0- 1.1 % ( b 3 ),0.7343782186508179
translation,288,256,results,f-measure scores,increase by,0.3-0.7 % ( ceaf ),f-measure scores increase by 0.3-0.7 % ( ceaf ),0.7113533020019531
translation,288,256,results,f-measure scores,increase by,0.9- 1.1 % ( b 3 ) and 1.2-2.6 % ( ceaf ),f-measure scores increase by 0.9- 1.1 % ( b 3 ) and 1.2-2.6 % ( ceaf ),0.7018954157829285
translation,288,256,results,0.3-0.7 % ( ceaf ),for,mp model,0.3-0.7 % ( ceaf ) for mp model,0.6333887577056885
translation,288,256,results,0.9- 1.1 % ( b 3 ) and 1.2-2.6 % ( ceaf ),for,cr model,0.9- 1.1 % ( b 3 ) and 1.2-2.6 % ( ceaf ) for cr model,0.6674126386642456
translation,288,256,results,coreference performance,has,f-measure scores,coreference performance has f-measure scores,0.566585898399353
translation,288,256,results,results,employing,knowledge of information status,results employing knowledge of information status,0.5444488525390625
translation,288,257,results,three information -status classifiers,achieved,level of accuracy,three information -status classifiers achieved level of accuracy,0.6944881677627563
translation,288,257,results,level of accuracy,needed for,coreference models,level of accuracy needed for coreference models,0.6333531737327576
translation,288,257,results,results,suggest,three information -status classifiers,results suggest three information -status classifiers,0.4990493357181549
translation,288,265,results,perfect information -status knowledge,yields,coreference system,perfect information -status knowledge yields coreference system,0.6944723725318909
translation,288,265,results,automatically acquired information -status knowledge,by,1.8-4.1 % ( mp ),automatically acquired information -status knowledge by 1.8-4.1 % ( mp ),0.5715397596359253
translation,288,265,results,automatically acquired information -status knowledge,by,2.7-3.1 % ( cr ),automatically acquired information -status knowledge by 2.7-3.1 % ( cr ),0.5674192309379578
translation,288,265,results,2.7-3.1 % ( cr ),in,f-measure,2.7-3.1 % ( cr ) in f-measure,0.520539402961731
translation,288,265,results,results,using,perfect information -status knowledge,results using perfect information -status knowledge,0.6601990461349487
translation,289,146,baselines,augmented pairs,generated by,back translating,augmented pairs generated by back translating,0.7109401822090149
translation,289,146,baselines,post sentences,of,dialogue pairs,post sentences of dialogue pairs,0.6104465126991272
translation,289,146,baselines,bt,has,augmented pairs,bt has augmented pairs,0.6209836006164551
translation,289,146,baselines,bt,has,post sentences,bt has post sentences,0.6308805346488953
translation,289,146,baselines,back translating,has,post sentences,back translating has post sentences,0.6420578360557556
translation,289,175,baselines,teacher,Training,dialogue models,teacher Training dialogue models,0.7442055344581604
translation,289,175,baselines,dialogue models,on,paired data d p,dialogue models on paired data d p,0.615042507648468
translation,289,175,baselines,paired data d p,with,nll loss,paired data d p with nll loss,0.7053611278533936
translation,289,175,baselines,baselines,has,teacher,baselines has teacher,0.6185755729675293
translation,289,178,baselines,np + ml,Sampling,300 k pairs,np + ml Sampling 300 k pairs,0.6272339820861816
translation,289,178,baselines,np + ml,use,noisy pairs,np + ml use noisy pairs,0.6420301795005798
translation,289,178,baselines,300 k pairs,set of,weibo dialogues,300 k pairs set of weibo dialogues,0.6570466160774231
translation,289,178,baselines,noisy pairs,as,augmented pairs,noisy pairs as augmented pairs,0.5184355974197388
translation,289,178,baselines,baselines,has,np + ml,baselines has np + ml,0.5834107398986816
translation,289,127,experimental-setup,retrieval module,using,lucene library,retrieval module using lucene library,0.6415241360664368
translation,289,127,experimental-setup,retrieval module,set,value of both n and m,retrieval module set value of both n and m,0.6668940782546997
translation,289,127,experimental-setup,value of both n and m,to,5,value of both n and m to 5,0.6242600679397583
translation,289,127,experimental-setup,experimental setup,implement,retrieval module,experimental setup implement retrieval module,0.6498583555221558
translation,289,127,experimental-setup,experimental setup,set,value of both n and m,experimental setup set value of both n and m,0.6418639421463013
translation,289,128,experimental-setup,matching model,fine - tuned with,d p,matching model fine - tuned with d p,0.8089479804039001
translation,289,128,experimental-setup,d p,for,three epochs,d p for three epochs,0.6361685991287231
translation,289,128,experimental-setup,d p,based on,pretrained bert - base model,d p based on pretrained bert - base model,0.6968492865562439
translation,289,128,experimental-setup,pretrained bert - base model,has,"devlin et al. , 2019 )","pretrained bert - base model has devlin et al. , 2019 )",0.5377125144004822
translation,289,128,experimental-setup,experimental setup,has,matching model,experimental setup has matching model,0.5340561270713806
translation,289,133,experimental-setup,encoder and decoder,share,same set of parameters,encoder and decoder share same set of parameters,0.6655874848365784
translation,289,133,experimental-setup,same set of parameters,initialized using,pretrained gpt model,same set of parameters initialized using pretrained gpt model,0.7639895081520081
translation,289,133,experimental-setup,generation - based dialogue model,has,encoder and decoder,generation - based dialogue model has encoder and decoder,0.5459920167922974
translation,289,133,experimental-setup,experimental setup,For,generation - based dialogue model,experimental setup For generation - based dialogue model,0.5906816124916077
translation,289,134,experimental-setup,teacher model,uses,same architecture,teacher model uses same architecture,0.6399886012077332
translation,289,134,experimental-setup,fine-tuned,using,paired dataset d p,fine-tuned using paired dataset d p,0.6849697828292847
translation,289,134,experimental-setup,paired dataset d p,for,15 epochs,paired dataset d p for 15 epochs,0.6337178945541382
translation,289,134,experimental-setup,paired dataset d p,on,nll loss,paired dataset d p on nll loss,0.5899183750152588
translation,289,134,experimental-setup,15 epochs,on,nll loss,15 epochs on nll loss,0.49561694264411926
translation,289,134,experimental-setup,experimental setup,has,teacher model,experimental setup has teacher model,0.5482322573661804
translation,289,135,experimental-setup,final generative dialogue model,first initialized using,pre-trained gpt weights,final generative dialogue model first initialized using pre-trained gpt weights,0.7084715366363525
translation,289,135,experimental-setup,final generative dialogue model,fine-tuned using,loss,final generative dialogue model fine-tuned using loss,0.7367483973503113
translation,289,135,experimental-setup,loss,for,50 epochs,loss for 50 epochs,0.5534118413925171
translation,289,135,experimental-setup,loss,on,d p,loss on d p,0.7152926921844482
translation,289,135,experimental-setup,50 epochs,on,d p,50 epochs on d p,0.6376346349716187
translation,289,135,experimental-setup,experimental setup,has,final generative dialogue model,experimental setup has final generative dialogue model,0.5415892601013184
translation,289,171,experiments,300 k augmented dialogue pairs,generated,model- level distillation process,300 k augmented dialogue pairs generated model- level distillation process,0.6432644128799438
translation,289,171,experiments,model- level distillation process,to train,dialogue models,model- level distillation process to train dialogue models,0.6828840374946594
translation,289,6,model,novel data augmentation method,for training,opendomain dialogue models,novel data augmentation method for training opendomain dialogue models,0.6521210074424744
translation,289,6,model,opendomain dialogue models,by utilizing,unpaired data,opendomain dialogue models by utilizing unpaired data,0.6324589252471924
translation,289,6,model,model,propose,novel data augmentation method,model propose novel data augmentation method,0.6812246441841125
translation,289,25,model,novel data augmentation method,to improve,performance,novel data augmentation method to improve performance,0.6616340279579163
translation,289,25,model,performance,of,open-domain dialogue models,performance of open-domain dialogue models,0.5736980438232422
translation,289,25,model,performance,by utilizing,unpaired data,performance by utilizing unpaired data,0.6256346106529236
translation,289,25,model,open-domain dialogue models,by utilizing,unpaired data,open-domain dialogue models by utilizing unpaired data,0.6342385411262512
translation,289,25,model,novel data augmentation method,has,dialogue distillation,novel data augmentation method has dialogue distillation,0.5604797005653381
translation,289,25,model,model,propose,novel data augmentation method,model propose novel data augmentation method,0.6812246441841125
translation,289,31,model,second phase,at,model- level,second phase at model- level,0.5662997364997864
translation,289,31,model,model- level,distills,teacher model,model- level distills teacher model,0.7378737926483154
translation,289,31,model,teacher model,using,augmented data,teacher model using augmented data,0.7004503607749939
translation,289,31,model,model,has,second phase,model has second phase,0.5953893661499023
translation,289,32,model,knowledge distillation,to first train,teacher model,knowledge distillation to first train teacher model,0.7613437175750732
translation,289,32,model,knowledge distillation,to first train,teacher model,knowledge distillation to first train teacher model,0.7613437175750732
translation,289,32,model,teacher model,set of,high-quality dialogue pairs,teacher model set of high-quality dialogue pairs,0.6421566009521484
translation,289,32,model,teacher model,distill,dialogue model,teacher model distill dialogue model,0.6882840991020203
translation,289,32,model,dialogue model,by mimicking,distribution,dialogue model by mimicking distribution,0.7235944271087646
translation,289,32,model,distribution,produced by,teacher model,distribution produced by teacher model,0.6811843514442444
translation,289,32,model,teacher model,on,augmented data,teacher model on augmented data,0.5667009353637695
translation,289,32,model,teacher model,on,augmented data,teacher model on augmented data,0.5667009353637695
translation,289,35,model,data- level distillation,constructs,new post-response pairs,data- level distillation constructs new post-response pairs,0.6670003533363342
translation,289,35,model,data- level distillation,distills,teacher model,data- level distillation distills teacher model,0.7403406500816345
translation,289,35,model,new post-response pairs,where,post and response,new post-response pairs where post and response,0.5688523054122925
translation,289,35,model,post and response,retrieved from,unpaired data,post and response retrieved from unpaired data,0.4897186756134033
translation,289,35,model,model- level distillation,distills,teacher model,model- level distillation distills teacher model,0.7573770880699158
translation,289,35,model,teacher model,trained on,high quality paired data,teacher model trained on high quality paired data,0.7456527948379517
translation,289,35,model,high quality paired data,to,augmented pairs,high quality paired data to augmented pairs,0.5897824168205261
translation,289,35,model,model,has,data- level distillation,model has data- level distillation,0.5599828958511353
translation,289,37,results,automatic and manual evaluation,shows,augmented pairs,automatic and manual evaluation shows augmented pairs,0.6636717319488525
translation,289,37,results,augmented pairs,produced by,our method,augmented pairs produced by our method,0.6635305881500244
translation,289,37,results,our method,are,content-rich,our method are content-rich,0.5531601905822754
translation,289,37,results,performance,of,generationbased and retrieval - based dialogue models,performance of generationbased and retrieval - based dialogue models,0.5645557045936584
translation,289,37,results,results,has,automatic and manual evaluation,results has automatic and manual evaluation,0.5337609648704529
translation,289,163,results,augmented dialogue data,produced by,our method,augmented dialogue data produced by our method,0.6302945017814636
translation,289,163,results,all the baselines,in,almost all the metrics,all the baselines in almost all the metrics,0.4831666350364685
translation,289,163,results,our method,has,outperform,our method has outperform,0.6298079490661621
translation,289,163,results,outperform,has,all the baselines,outperform has all the baselines,0.6026061177253723
translation,289,164,results,our method,obtains,similar scores,our method obtains similar scores,0.5601963996887207
translation,289,164,results,similar scores,on,all the metrics,similar scores on all the metrics,0.5165162682533264
translation,289,164,results,similar scores,compared to,human-produced and filtered dialogue pairs,similar scores compared to human-produced and filtered dialogue pairs,0.6487948894500732
translation,289,164,results,results,observe,our method,results observe our method,0.5780253410339355
translation,289,193,results,all the baselines,in,almost all the metrics,all the baselines in almost all the metrics,0.4831666350364685
translation,289,193,results,outperforms,has,all the baselines,outperforms has all the baselines,0.596747100353241
translation,289,194,results,dialogue models,that utilize,"unpaired data d u ( e.g. dl + ml , dl + pret , up + pret )","dialogue models that utilize unpaired data d u ( e.g. dl + ml , dl + pret , up + pret )",0.6561551094055176
translation,289,194,results,"unpaired data d u ( e.g. dl + ml , dl + pret , up + pret )",outperform,models,"unpaired data d u ( e.g. dl + ml , dl + pret , up + pret ) outperform models",0.6137312054634094
translation,289,194,results,models,that are only trained on,"d p ( e.g. , teacher , cvae + ml )","models that are only trained on d p ( e.g. , teacher , cvae + ml )",0.6997330784797668
translation,289,194,results,results,observe,dialogue models,results observe dialogue models,0.5494580268859863
translation,289,196,results,d p d a,without utilizing,"model- level distillation ( i.e. , w/ o ml )","d p d a without utilizing model- level distillation ( i.e. , w/ o ml )",0.7012510299682617
translation,289,196,results,d p d a,brings,little or no performance improvements,d p d a brings little or no performance improvements,0.6032857298851013
translation,289,196,results,little or no performance improvements,compared to,"directly training on d p ( i.e. , teacher )","little or no performance improvements compared to directly training on d p ( i.e. , teacher )",0.6546270847320557
translation,289,196,results,merged data,has,d p d a,merged data has d p d a,0.5824289321899414
translation,289,196,results,results,on,merged data,results on merged data,0.535347580909729
translation,289,197,results,effectiveness,of,model- level distillation process,effectiveness of model- level distillation process,0.5170863270759583
translation,289,197,results,modellevel distillation,is,augmented data,modellevel distillation is augmented data,0.5885412096977234
translation,289,197,results,modellevel distillation,employed,augmented data,modellevel distillation employed augmented data,0.6562986373901367
translation,289,197,results,augmented data,produced by,"our data-level distillation process ( i.e. , dl + ml )","augmented data produced by our data-level distillation process ( i.e. , dl + ml )",0.6251907348632812
translation,289,197,results,augmented data,produced by,other data augmentation methods,augmented data produced by other data augmentation methods,0.597579836845398
translation,289,197,results,performance,of,dialogue models,performance of dialogue models,0.5658886432647705
translation,289,197,results,dialogue models,compared to,augmented data,dialogue models compared to augmented data,0.603330671787262
translation,289,197,results,augmented data,produced by,other data augmentation methods,augmented data produced by other data augmentation methods,0.597579836845398
translation,289,197,results,"our data-level distillation process ( i.e. , dl + ml )",has,better improve,"our data-level distillation process ( i.e. , dl + ml ) has better improve",0.5494762659072876
translation,289,197,results,better improve,has,performance,better improve has performance,0.5790328979492188
translation,289,197,results,results,verifies,effectiveness,results verifies effectiveness,0.6279590129852295
translation,289,197,results,results,When,modellevel distillation,results When modellevel distillation,0.6054204702377319
translation,290,58,experiments,multiwoz,contains,"10,438 dialogues","multiwoz contains 10,438 dialogues",0.6460482478141785
translation,290,58,experiments,"10,438 dialogues",about,cambridge hotels and restaurants,"10,438 dialogues about cambridge hotels and restaurants",0.6277900338172913
translation,290,217,hyperparameters,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,290,217,hyperparameters,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,290,217,hyperparameters,0.001,to train,lstm and elmo models,0.001 to train lstm and elmo models,0.6620199084281921
translation,290,217,hyperparameters,lstm and elmo models,for,50 epochs,lstm and elmo models for 50 epochs,0.5462620258331299
translation,290,217,hyperparameters,lstm and elmo models,using,batch sizes,lstm and elmo models using batch sizes,0.6395477056503296
translation,290,217,hyperparameters,batch sizes,has,256 and 128,batch sizes has 256 and 128,0.6099535226821899
translation,290,217,hyperparameters,hyperparameters,use,adam optimizer,hyperparameters use adam optimizer,0.6090166568756104
translation,290,218,hyperparameters,early stopping,on,validation loss,early stopping on validation loss,0.5295768976211548
translation,290,218,hyperparameters,validation loss,with,tolerance,validation loss with tolerance,0.6387253999710083
translation,290,218,hyperparameters,tolerance,of,10 epochs,tolerance of 10 epochs,0.6392611861228943
translation,290,218,hyperparameters,10 epochs,to prevent,over fitting,10 epochs to prevent over fitting,0.5689188241958618
translation,290,218,hyperparameters,hyperparameters,employ,early stopping,hyperparameters employ early stopping,0.5583352446556091
translation,290,230,results,our models,achieve,similar ic f1 and sl f1 scores,our models achieve similar ic f1 and sl f1 scores,0.620387077331543
translation,290,230,results,similar ic f1 and sl f1 scores,on,turn and sentence level annotations,similar ic f1 and sl f1 scores on turn and sentence level annotations,0.536314845085144
translation,290,230,results,results,has,our models,results has our models,0.5733726620674133
translation,290,238,results,headroom,for,performance improvement,headroom for performance improvement,0.6275641322135925
translation,290,238,results,performance improvement,especially for,sl task,performance improvement especially for sl task,0.64289391040802
translation,290,238,results,sl task,across,all domains,sl task across all domains,0.7107445001602173
translation,290,238,results,results,still,headroom,results still headroom,0.7284477949142456
translation,291,171,ablation-analysis,new sample,of,hsc,new sample of hsc,0.6112081408500671
translation,291,171,ablation-analysis,new sample,enriched with,new annotation layers pop -hs-it,new sample enriched with new annotation layers pop -hs-it,0.7267360687255859
translation,291,171,ablation-analysis,ablation analysis,named,new sample,ablation analysis named new sample,0.731911838054657
translation,291,20,model,extension of the italian twitter corpus of hate speech ( hs ),against,immigrants,extension of the italian twitter corpus of hate speech ( hs ) against immigrants,0.6262816786766052
translation,291,20,model,model,proposes,extension of the italian twitter corpus of hate speech ( hs ),model proposes extension of the italian twitter corpus of hate speech ( hs ),0.634028971195221
translation,292,137,experimental-setup,each sentence,into,sub-word,each sentence into sub-word,0.5702040791511536
translation,292,137,experimental-setup,sub-word,using,gpt2tokenizer,sub-word using gpt2tokenizer,0.6854481101036072
translation,292,137,experimental-setup,experimental setup,tokenized,each sentence,experimental setup tokenized each sentence,0.7452592253684998
translation,292,138,experimental-setup,gpt - 2,with,batch size 2,gpt - 2 with batch size 2,0.685999870300293
translation,292,138,experimental-setup,batch size 2,for,4 epochs,batch size 2 for 4 epochs,0.5482996702194214
translation,292,138,experimental-setup,4 epochs,over,multiwoz training dataset,4 epochs over multiwoz training dataset,0.5819712281227112
translation,292,138,experimental-setup,experimental setup,fine- tuned,gpt - 2,experimental setup fine- tuned gpt - 2,0.7236436605453491
translation,292,139,experimental-setup,maximum history size,of,each dialogue,maximum history size of each dialogue,0.5906169414520264
translation,292,139,experimental-setup,each dialogue,set to,15,each dialogue set to 15,0.7152536511421204
translation,292,139,experimental-setup,experimental setup,has,maximum history size,experimental setup has maximum history size,0.5406368374824524
translation,292,140,experimental-setup,adam optimizer,with,"? 1 = 0.9 , ? 2 = 0.999","adam optimizer with ? 1 = 0.9 , ? 2 = 0.999",0.6175872087478638
translation,292,140,experimental-setup,adam optimizer,with,learning late,adam optimizer with learning late,0.6427842378616333
translation,292,140,experimental-setup,learning late,of,6.25e - 5,learning late of 6.25e - 5,0.5700462460517883
translation,292,140,experimental-setup,experimental setup,used,adam optimizer,experimental setup used adam optimizer,0.5961911082267761
translation,292,141,experimental-setup,lm and the nc losses,set to,2.0 and 1.0,lm and the nc losses set to 2.0 and 1.0,0.7253118753433228
translation,292,141,experimental-setup,experimental setup,coefficients of,lm and the nc losses,experimental setup coefficients of lm and the nc losses,0.6848008036613464
translation,292,9,model,end-to - end neural architecture,for,dialogue systems,end-to - end neural architecture for dialogue systems,0.6279841065406799
translation,292,9,model,model,present,end-to - end neural architecture,model present end-to - end neural architecture,0.6615651845932007
translation,292,19,model,end-to - end neural architecture,for,dialogue systems,end-to - end neural architecture for dialogue systems,0.6279841065406799
translation,292,19,model,model,present,end-to - end neural architecture,model present end-to - end neural architecture,0.6615651845932007
translation,292,38,model,traditional dialogue management pipeline,making,monolithic neural model,traditional dialogue management pipeline making monolithic neural model,0.6433077454566956
translation,292,38,model,integratable,with,external systems,integratable with external systems,0.6605093479156494
translation,292,38,model,end-to - end fashion,with,simple gradient descent,end-to - end fashion with simple gradient descent,0.6450071334838867
translation,292,38,model,monolithic neural model,has,more interpretable,monolithic neural model has more interpretable,0.5522497892379761
translation,292,38,model,model,trained in,end-to - end fashion,model trained in end-to - end fashion,0.7573421001434326
translation,292,38,model,model,leverages,gpt - 2,model leverages gpt - 2,0.7310748100280762
translation,292,37,results,our model,is,competitive,our model is competitive,0.6213095188140869
translation,292,37,results,competitive,to,other state - of - the - art models,competitive to other state - of - the - art models,0.5322827696800232
translation,292,37,results,other state - of - the - art models,specialized for,two sub-tasks,other state - of - the - art models specialized for two sub-tasks,0.7028957009315491
translation,292,37,results,two sub-tasks,in,dialogue management,two sub-tasks in dialogue management,0.5128254294395447
translation,292,37,results,two sub-tasks,in,dialogue state tracking,two sub-tasks in dialogue state tracking,0.5110981464385986
translation,292,37,results,two sub-tasks,in,dialogue -context - to - text generation tasks,two sub-tasks in dialogue -context - to - text generation tasks,0.5143616795539856
translation,292,37,results,two sub-tasks,i.e.,dialogue state tracking,two sub-tasks i.e. dialogue state tracking,0.6229408383369446
translation,292,37,results,two sub-tasks,i.e.,dialogue -context - to - text generation tasks,two sub-tasks i.e. dialogue -context - to - text generation tasks,0.6604094505310059
translation,292,37,results,results,show,our model,results show our model,0.6888449192047119
translation,292,156,results,proposed model,with,greedy decoding strategy,proposed model with greedy decoding strategy,0.6625103950500488
translation,292,156,results,greedy decoding strategy,achieved,success rate,greedy decoding strategy achieved success rate,0.6991786956787109
translation,292,156,results,greedy decoding strategy,achieved,avg return,greedy decoding strategy achieved avg return,0.7142530679702759
translation,292,156,results,greedy decoding strategy,achieved,avg turns,greedy decoding strategy achieved avg turns,0.7422384023666382
translation,292,156,results,greedy decoding strategy,achieved,book rate,greedy decoding strategy achieved book rate,0.714824914932251
translation,292,156,results,greedy decoding strategy,achieved,precision,greedy decoding strategy achieved precision,0.7102988958358765
translation,292,156,results,greedy decoding strategy,achieved,recall,greedy decoding strategy achieved recall,0.7040456533432007
translation,292,156,results,greedy decoding strategy,achieved,f1 score,greedy decoding strategy achieved f1 score,0.7111490368843079
translation,292,156,results,success rate,of,78.60 %,success rate of 78.60 %,0.5157403945922852
translation,292,156,results,avg return,of,48.92,avg return of 48.92,0.5426925420761108
translation,292,156,results,avg turns,of,7.40,avg turns of 7.40,0.6113296747207642
translation,292,156,results,book rate,of,86.34 %,book rate of 86.34 %,0.5325509905815125
translation,292,156,results,precision,of,0.87,precision of 0.87,0.5496881008148193
translation,292,156,results,precision,of,0.87,precision of 0.87,0.5496881008148193
translation,292,156,results,recall,of,0.89,recall of 0.89,0.5743798613548279
translation,292,156,results,f1 score,of,0.87,f1 score of 0.87,0.5401912331581116
translation,292,156,results,0.87,in,automatic evaluation,0.87 in automatic evaluation,0.47378072142601013
translation,292,156,results,automatic evaluation,using,500 simulated dialogues,automatic evaluation using 500 simulated dialogues,0.6073216199874878
translation,292,156,results,results,has,proposed model,results has proposed model,0.5938616394996643
translation,292,157,results,our model,failed to perform,best,our model failed to perform best,0.7296682596206665
translation,292,157,results,best,among,submitted systems,best among submitted systems,0.6249880790710449
translation,292,157,results,our model,has,outperformed,our model has outperformed,0.5903543829917908
translation,292,157,results,outperformed,has,baseline system,outperformed has baseline system,0.6210705637931824
translation,292,157,results,results,has,our model,results has our model,0.5871725678443909
translation,292,163,results,our proposed model,with,top-p sampling ( p=0.8 ),our proposed model with top-p sampling ( p=0.8 ),0.6184332370758057
translation,292,163,results,top-p sampling ( p=0.8 ),ranked in,first place,top-p sampling ( p=0.8 ) ranked in first place,0.7493776679039001
translation,292,163,results,first place,with,success rate,first place with success rate,0.6212754845619202
translation,292,163,results,success rate,of,68.32 %,success rate of 68.32 %,0.5211758613586426
translation,292,163,results,average turns,of,19.507,average turns of 19.507,0.5844796895980835
translation,292,163,results,language understanding score,of,4.149,language understanding score of 4.149,0.4667864143848419
translation,292,163,results,response appropriateness score,has,4.287,response appropriateness score has 4.287,0.5159767866134644
translation,292,163,results,results,has,our proposed model,results has our proposed model,0.5871988534927368
translation,292,164,results,our model,showed,2.51 % improvement,our model showed 2.51 % improvement,0.660050094127655
translation,292,164,results,2.51 % improvement,in,success rate,2.51 % improvement in success rate,0.5128642916679382
translation,292,164,results,2nd- ranked model,has,our model,2nd- ranked model has our model,0.5650575757026672
translation,292,164,results,results,Compared to,2nd- ranked model,results Compared to 2nd- ranked model,0.6788957118988037
translation,292,165,results,more significant,in,human language metrics,more significant in human language metrics,0.4970068037509918
translation,292,165,results,higher,than,2nd- ranked model,higher than 2nd- ranked model,0.5948922038078308
translation,292,165,results,2nd- ranked model,in,language understanding score,2nd- ranked model in language understanding score,0.48198163509368896
translation,292,165,results,2nd- ranked model,in,response appropriateness score,2nd- ranked model in response appropriateness score,0.4964800477027893
translation,292,165,results,human language metrics,has,0.365 points,human language metrics has 0.365 points,0.5630559921264648
translation,292,165,results,human language metrics,has,0.458 points,human language metrics has 0.458 points,0.5755578279495239
translation,292,165,results,human language metrics,has,higher,human language metrics has higher,0.5927929878234863
translation,292,165,results,0.458 points,has,higher,0.458 points has higher,0.5664215087890625
translation,292,165,results,results,has,performance gap,results has performance gap,0.5742626190185547
translation,292,174,results,dialogue state tracking,compares,dialogue state tracking accuracy,dialogue state tracking compares dialogue state tracking accuracy,0.585156261920929
translation,292,174,results,results,compares,dialogue state tracking accuracy,results compares dialogue state tracking accuracy,0.642120897769928
translation,292,174,results,results,has,dialogue state tracking,results has dialogue state tracking,0.5458087921142578
translation,292,180,results,competitive,to,state - of - the - art models,competitive to state - of - the - art models,0.5296013355255127
translation,292,180,results,state - of - the - art models,except for,bleu score,state - of - the - art models except for bleu score,0.5569663047790527
translation,292,180,results,results,has,our model,results has our model,0.5871725678443909
translation,293,67,baselines,cmn,has,"hazarika et al. , 2018 b )","cmn has hazarika et al. , 2018 b )",0.5762950778007507
translation,293,7,model,fine-grained extraction and reasoning network ( fernet ),to generate,target-specific historical utterance representations,fine-grained extraction and reasoning network ( fernet ) to generate target-specific historical utterance representations,0.675507128238678
translation,293,7,model,model,propose,fine-grained extraction and reasoning network ( fernet ),model propose fine-grained extraction and reasoning network ( fernet ),0.6589186191558838
translation,293,8,model,reasoning module,handles,local and global sequential dependencies,reasoning module handles local and global sequential dependencies,0.6892484426498413
translation,293,8,model,reasoning module,updates,target utterance representations,reasoning module updates target utterance representations,0.6898542642593384
translation,293,8,model,local and global sequential dependencies,to reason over,context,local and global sequential dependencies to reason over context,0.7069884538650513
translation,293,8,model,target utterance representations,to,more informed vectors,target utterance representations to more informed vectors,0.522891104221344
translation,293,8,model,model,has,reasoning module,model has reasoning module,0.6088245511054993
translation,293,20,model,fine-grained extraction and reasoning network ( fernet ),to generate,target-specific historical utterance representations,fine-grained extraction and reasoning network ( fernet ) to generate target-specific historical utterance representations,0.675507128238678
translation,293,20,model,target-specific historical utterance representations,conditioned on,content of target utterances,target-specific historical utterance representations conditioned on content of target utterances,0.6930731534957886
translation,293,20,model,target-specific historical utterance representations,extracting,"more fine- grained , relevant and contributing information","target-specific historical utterance representations extracting more fine- grained , relevant and contributing information",0.641177773475647
translation,293,20,model,content of target utterances,by using,multi-head attention mechanism,content of target utterances by using multi-head attention mechanism,0.6509148478507996
translation,293,20,model,multi-head attention mechanism,extracting,"more fine- grained , relevant and contributing information","multi-head attention mechanism extracting more fine- grained , relevant and contributing information",0.6668723821640015
translation,293,20,model,"more fine- grained , relevant and contributing information",for,emotion recognition,"more fine- grained , relevant and contributing information for emotion recognition",0.5883755087852478
translation,293,20,model,model,propose,fine-grained extraction and reasoning network ( fernet ),model propose fine-grained extraction and reasoning network ( fernet ),0.6589186191558838
translation,293,21,model,reasoning module,employs,historical utterances,reasoning module employs historical utterances,0.5551005005836487
translation,293,21,model,reasoning module,updates,representation,reasoning module updates representation,0.7512331604957581
translation,293,21,model,historical utterances,as,sequence of triggers,historical utterances as sequence of triggers,0.5346454977989197
translation,293,21,model,historical utterances,through,time,historical utterances through time,0.6661076545715332
translation,293,21,model,representation,of,target utterance,representation of target utterance,0.5705461502075195
translation,293,21,model,target utterance,to,more informed vector,target utterance to more informed vector,0.5558639168739319
translation,293,21,model,more informed vector,as it observes,historical utterances,more informed vector as it observes historical utterances,0.7055602669715881
translation,293,21,model,historical utterances,through,time,historical utterances through time,0.6661076545715332
translation,293,21,model,model,devise,reasoning module,model devise reasoning module,0.734743595123291
translation,293,22,model,module,models,short-term and long-term sequential dependencies,module models short-term and long-term sequential dependencies,0.7587019205093384
translation,293,22,model,reasoning process,has,module,reasoning process has module,0.6111978888511658
translation,293,22,model,short-term and long-term sequential dependencies,has,effectively,short-term and long-term sequential dependencies has effectively,0.5557129383087158
translation,293,22,model,model,In,reasoning process,model In reasoning process,0.5278329849243164
translation,293,72,results,outperforms,on,all evaluation metrics,outperforms on all evaluation metrics,0.4785044193267822
translation,293,72,results,baselines,on,all evaluation metrics,baselines on all evaluation metrics,0.448621928691864
translation,293,72,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,293,72,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
translation,293,72,results,baselines,has,significantly,baselines has significantly,0.6135930418968201
translation,293,72,results,results,see that,our model,results see that our model,0.6820751428604126
translation,293,73,results,our model,surpasses,dialoguernn,our model surpasses dialoguernn,0.6060506105422974
translation,293,73,results,dialoguernn,by,1.69 %,dialoguernn by 1.69 %,0.6007921695709229
translation,293,73,results,dialoguernn,on,weighted average f1score,dialoguernn on weighted average f1score,0.5259957313537598
translation,293,73,results,results,has,our model,results has our model,0.5871725678443909
translation,293,74,results,our model,lower,mean absolute error,our model lower mean absolute error,0.7219690084457397
translation,293,74,results,mean absolute error,by,"0.03 , 0.027 , 0.009 and 0.31","mean absolute error by 0.03 , 0.027 , 0.009 and 0.31",0.5700589418411255
translation,293,74,results,"0.03 , 0.027 , 0.009 and 0.31",for,"valence , arousal , expectancy and power","0.03 , 0.027 , 0.009 and 0.31 for valence , arousal , expectancy and power",0.5576779246330261
translation,293,74,results,avec dataset,has,our model,avec dataset has our model,0.5952469110488892
translation,293,74,results,results,For,avec dataset,results For avec dataset,0.6244845986366272
translation,295,25,experiments,statistical word-overlap similar - ity metrics,such as,bleu,statistical word-overlap similar - ity metrics such as bleu,0.6026453375816345
translation,295,25,experiments,statistical word-overlap similar - ity metrics,such as,"meteor , and rouge","statistical word-overlap similar - ity metrics such as meteor , and rouge",0.6076264381408691
translation,295,25,experiments,word embedding metrics,derived from,word embedding models,word embedding metrics derived from word embedding models,0.5932328701019287
translation,295,25,experiments,word embedding models,such as,"word2vec ( mikolov et al. , 2013 )","word embedding models such as word2vec ( mikolov et al. , 2013 )",0.49369046092033386
translation,296,111,baselines,three out of the eight models,used as,baseline models,three out of the eight models used as baseline models,0.6181495189666748
translation,296,111,baselines,baseline models,are,gradient boosting regression ( g.boost ) model,baseline models are gradient boosting regression ( g.boost ) model,0.4867627024650574
translation,296,111,baselines,gradient boosting regression ( g.boost ) model,trained using,features,gradient boosting regression ( g.boost ) model trained using features,0.7387195229530334
translation,296,111,baselines,features,derived from,entire dialogue context ( t 1:n ),features derived from entire dialogue context ( t 1:n ),0.6301189064979553
translation,296,111,baselines,features,including,hand -crafted turn-level and temporal features,features including hand -crafted turn-level and temporal features,0.6695310473442078
translation,296,111,baselines,features,except for,embeddings,features except for embeddings,0.6150549650192261
translation,296,111,baselines,two -layer bil-stm model ( bilst m f eatures ),trained with,all turn-level features,two -layer bil-stm model ( bilst m f eatures ) trained with all turn-level features,0.715023934841156
translation,296,111,baselines,all turn-level features,except for,embeddings,all turn-level features except for embeddings,0.6365900635719299
translation,296,111,baselines,baselines,has,three out of the eight models,baselines has three out of the eight models,0.5622294545173645
translation,296,118,experiments,deep neural models,experimented with,"adam ( kingma and ba , 2014 ) optimizer","deep neural models experimented with adam ( kingma and ba , 2014 ) optimizer",0.700995922088623
translation,296,118,experiments,"adam ( kingma and ba , 2014 ) optimizer",with,learning rate,"adam ( kingma and ba , 2014 ) optimizer with learning rate",0.6116575002670288
translation,296,118,experiments,"adam ( kingma and ba , 2014 ) optimizer",with,hidden vector size,"adam ( kingma and ba , 2014 ) optimizer with hidden vector size",0.6121392250061035
translation,296,118,experiments,"adam ( kingma and ba , 2014 ) optimizer",mini-,batch size,"adam ( kingma and ba , 2014 ) optimizer mini- batch size",0.6327173113822937
translation,296,118,experiments,"adam ( kingma and ba , 2014 ) optimizer",mini-,hidden vector size,"adam ( kingma and ba , 2014 ) optimizer mini- hidden vector size",0.6389870643615723
translation,296,118,experiments,batch size,of,64,batch size of 64,0.6741159558296204
translation,296,118,experiments,learning rate,has,0.0001,learning rate has 0.0001,0.5380781888961792
translation,296,118,experiments,hidden vector size,has,512,hidden vector size has 512,0.6134202480316162
translation,296,33,hyperparameters,features,derived from,pre-trained universal sentence encoder ( use ) embeddings,features derived from pre-trained universal sentence encoder ( use ) embeddings,0.5907182097434998
translation,296,33,hyperparameters,pre-trained universal sentence encoder ( use ) embeddings,of,utterance and system response texts,pre-trained universal sentence encoder ( use ) embeddings of utterance and system response texts,0.5490537881851196
translation,296,33,hyperparameters,utterance and system response texts,to train,model,utterance and system response texts to train model,0.7242239117622375
translation,296,60,hyperparameters,feature sets,derived from,use pre-trained ( 512 dimensional ) embeddings,feature sets derived from use pre-trained ( 512 dimensional ) embeddings,0.64676833152771
translation,296,60,hyperparameters,use pre-trained ( 512 dimensional ) embeddings,from,transformer variant,use pre-trained ( 512 dimensional ) embeddings from transformer variant,0.567010223865509
translation,296,119,hyperparameters,"dropout ( srivastava et al. , 2014 ) regularization techniques",to avoid,overfitting,"dropout ( srivastava et al. , 2014 ) regularization techniques to avoid overfitting",0.5909941792488098
translation,296,119,hyperparameters,early stopping criteria,has,"dropout ( srivastava et al. , 2014 ) regularization techniques","early stopping criteria has dropout ( srivastava et al. , 2014 ) regularization techniques",0.47727158665657043
translation,296,119,hyperparameters,0.5 ),has,"dropout ( srivastava et al. , 2014 ) regularization techniques","0.5 ) has dropout ( srivastava et al. , 2014 ) regularization techniques",0.4778904318809509
translation,296,6,model,novel user satisfaction estimation approach,minimizes,adaptive multi-task loss function,novel user satisfaction estimation approach minimizes adaptive multi-task loss function,0.6017230749130249
translation,296,6,model,adaptive multi-task loss function,jointly predict,turn- level response quality labels,adaptive multi-task loss function jointly predict turn- level response quality labels,0.7610782980918884
translation,296,6,model,adaptive multi-task loss function,jointly predict,explicit dialogue - level ratings,adaptive multi-task loss function jointly predict explicit dialogue - level ratings,0.744243323802948
translation,296,6,model,turn- level response quality labels,provided by,experts,turn- level response quality labels provided by experts,0.6709690690040588
translation,296,6,model,explicit dialogue - level ratings,provided by,end users,explicit dialogue - level ratings provided by end users,0.6563154458999634
translation,296,6,model,model,propose,novel user satisfaction estimation approach,model propose novel user satisfaction estimation approach,0.6617423892021179
translation,296,32,model,turn- level rq estimation model,implicitly encodes,temporal dependencies,turn- level rq estimation model implicitly encodes temporal dependencies,0.7901243567466736
translation,296,32,model,hand - crafting,of,turn and temporal features,hand - crafting of turn and temporal features,0.5709410309791565
translation,296,93,model,a,uses,pipelined modular dialogue agent,a uses pipelined modular dialogue agent,0.4603218734264374
translation,296,93,model,pipelined modular dialogue agent,comprising of,asr,pipelined modular dialogue agent comprising of asr,0.6589544415473938
translation,296,93,model,pipelined modular dialogue agent,comprising of,nlu,pipelined modular dialogue agent comprising of nlu,0.6505557298660278
translation,296,93,model,pipelined modular dialogue agent,comprising of,state tracker,pipelined modular dialogue agent comprising of state tracker,0.6532668471336365
translation,296,93,model,pipelined modular dialogue agent,comprising of,dialogue policy,pipelined modular dialogue agent comprising of dialogue policy,0.6492711901664734
translation,296,93,model,pipelined modular dialogue agent,comprising of,natural language generation components,pipelined modular dialogue agent comprising of natural language generation components,0.6504151225090027
translation,296,93,model,model,uses,pipelined modular dialogue agent,model uses pipelined modular dialogue agent,0.5660223960876465
translation,296,93,model,model,has,a,model has a,0.5863970518112183
translation,296,95,model,b,is,end-to - end neural model,b is end-to - end neural model,0.525162935256958
translation,296,95,model,end-to - end neural model,shares,only the asr component with system a,end-to - end neural model shares only the asr component with system a,0.7160037755966187
translation,296,95,model,model,is,end-to - end neural model,model is end-to - end neural model,0.5422965884208679
translation,296,95,model,model,has,b,model has b,0.5726120471954346
translation,296,133,results,proposed lstm based turn-level quality estimation model,removed,need to hand -craft features,proposed lstm based turn-level quality estimation model removed need to hand -craft features,0.6474091410636902
translation,296,133,results,proposed lstm based turn-level quality estimation model,has,outperformed,proposed lstm based turn-level quality estimation model has outperformed,0.591725766658783
translation,296,133,results,outperformed,has,benchmark gradient boosting regression model,outperformed has benchmark gradient boosting regression model,0.5502634644508362
translation,296,134,results,nlu features,on,dialogues,nlu features on dialogues,0.5345064997673035
translation,296,134,results,dialogues,from,both dialogue systems,dialogues from both dialogue systems,0.5945358276367188
translation,296,134,results,best- performing ( lst m embedding f eatures ) model,achieved,?3 % relative improvement,best- performing ( lst m embedding f eatures ) model achieved ?3 % relative improvement,0.6830771565437317
translation,296,134,results,best- performing ( lst m embedding f eatures ) model,statistically significant ( at,95 % boostrapconfidence interval ) relative improvement,best- performing ( lst m embedding f eatures ) model statistically significant ( at 95 % boostrapconfidence interval ) relative improvement,0.5907854437828064
translation,296,134,results,?3 % relative improvement,in,correlation ( 0.74 ? 0.76 ),?3 % relative improvement in correlation ( 0.74 ? 0.76 ),0.5186160802841187
translation,296,134,results,8.3 % ( 0.72 ? 0.78 ),in,f-score,8.3 % ( 0.72 ? 0.78 ) in f-score,0.4767364263534546
translation,296,134,results,8.3 % ( 0.72 ? 0.78 ),over,benchmark model,8.3 % ( 0.72 ? 0.78 ) over benchmark model,0.6152498126029968
translation,296,134,results,f-score,on,dissatisfactory class performance,f-score on dissatisfactory class performance,0.537287175655365
translation,296,134,results,nlu features,has,best- performing ( lst m embedding f eatures ) model,nlu features has best- performing ( lst m embedding f eatures ) model,0.5529299378395081
translation,296,134,results,95 % boostrapconfidence interval ) relative improvement,has,8.3 % ( 0.72 ? 0.78 ),95 % boostrapconfidence interval ) relative improvement has 8.3 % ( 0.72 ? 0.78 ),0.4702591300010681
translation,296,146,results,turn- level model 's performance,on,new domain,turn- level model 's performance on new domain,0.5501585006713867
translation,296,146,results,improves,with,similarity score,improves with similarity score,0.6333402991294861
translation,296,238,results,best- performing joint-model,achieved,up to 27 % absolute significant improvement,best- performing joint-model achieved up to 27 % absolute significant improvement,0.6994861960411072
translation,296,238,results,up to 27 % absolute significant improvement,in,correla- tion ( pearson ' s -r ) performance,up to 27 % absolute significant improvement in correla- tion ( pearson ' s -r ) performance,0.5752217769622803
translation,296,238,results,7 % absolute improvement,over,baseline deep neural network and the benchmark g.boost models,7 % absolute improvement over baseline deep neural network and the benchmark g.boost models,0.6426677107810974
translation,296,238,results,results,has,best- performing joint-model,results has best- performing joint-model,0.5450049042701721
translation,297,179,ablation-analysis,significant jump,in,performance,significant jump in performance,0.4790453016757965
translation,297,179,ablation-analysis,performance,entitles,images,performance entitles images,0.661543071269989
translation,297,179,ablation-analysis,ablation analysis,has,significant jump,ablation analysis has significant jump,0.525173008441925
translation,297,129,baselines,baseline models,has,model 1 ( hred ),baseline models has model 1 ( hred ),0.5220078229904175
translation,297,129,baselines,baselines,has,baseline models,baselines has baseline models,0.5690722465515137
translation,297,130,baselines,first baseline,is,simple hierarchical encoder-decoder framework,first baseline is simple hierarchical encoder-decoder framework,0.5263913869857788
translation,297,130,baselines,simple hierarchical encoder-decoder framework,makes use of,textual information,simple hierarchical encoder-decoder framework makes use of textual information,0.6717207431793213
translation,297,130,baselines,textual information,for generating,responses,textual information for generating responses,0.7032137513160706
translation,297,130,baselines,baselines,has,first baseline,baselines has first baseline,0.595583438873291
translation,297,131,baselines,baselines,has,model 2 ( mhred ),baselines has model 2 ( mhred ),0.5952718257904053
translation,297,132,baselines,extension of the hred framework,incorporate,multi-modal information,extension of the hred framework incorporate multi-modal information,0.6339834332466125
translation,297,132,baselines,multi-modal information,i.e.,images,multi-modal information i.e. images,0.6382038593292236
translation,297,132,baselines,images,for,generation of coherent responses,images for generation of coherent responses,0.6237342953681946
translation,297,133,baselines,baselines,has,model 3 ( hred + aspect ),baselines has model 3 ( hred + aspect ),0.6100752353668213
translation,297,135,baselines,baselines,has,model 4 ( mhred + aspect ),baselines has model 4 ( mhred + aspect ),0.6014874577522278
translation,297,140,experimental-setup,implementations,done using,pytorch 2 framework,implementations done using pytorch 2 framework,0.5893166065216064
translation,297,140,experimental-setup,experimental setup,done using,pytorch 2 framework,experimental setup done using pytorch 2 framework,0.602667510509491
translation,297,140,experimental-setup,experimental setup,has,implementations,experimental setup has implementations,0.5001025199890137
translation,297,141,experimental-setup,batch size,set to,32,batch size set to 32,0.733751654624939
translation,297,141,experimental-setup,baselines,has,batch size,baselines has batch size,0.5746903419494629
translation,297,143,experimental-setup,"dropout ( srivastava et al. , 2014 )",with,probability 0.45,"dropout ( srivastava et al. , 2014 ) with probability 0.45",0.6059021949768066
translation,297,143,experimental-setup,experimental setup,use,"dropout ( srivastava et al. , 2014 )","experimental setup use dropout ( srivastava et al. , 2014 )",0.5588595867156982
translation,297,144,experimental-setup,decoding,use,beam search,decoding use beam search,0.6366719603538513
translation,297,144,experimental-setup,beam search,with,beam size 10,beam search with beam size 10,0.7035357356071472
translation,297,144,experimental-setup,experimental setup,During,decoding,experimental setup During decoding,0.6438420414924622
translation,297,145,experimental-setup,model,initialized with,parameters,model initialized with parameters,0.6890477538108826
translation,297,145,experimental-setup,gaussian distribution,with,xavier scheme,gaussian distribution with xavier scheme,0.6638971567153931
translation,297,145,experimental-setup,parameters,has,chosen randomly,parameters has chosen randomly,0.549701988697052
translation,297,145,experimental-setup,experimental setup,initialized with,parameters,experimental setup initialized with parameters,0.672275722026825
translation,297,145,experimental-setup,experimental setup,has,model,experimental setup has model,0.5338840484619141
translation,297,146,experimental-setup,hidden size,for,all the layers,hidden size for all the layers,0.6047607064247131
translation,297,146,experimental-setup,all the layers,is,512,all the layers is 512,0.5852378606796265
translation,297,146,experimental-setup,experimental setup,has,hidden size,experimental setup has hidden size,0.5616794228553772
translation,297,147,experimental-setup,"amsgrad ( reddi et al. , 2019 )",used as,optimizer,"amsgrad ( reddi et al. , 2019 ) used as optimizer",0.6090565919876099
translation,297,147,experimental-setup,optimizer,for,model training,optimizer for model training,0.6057636141777039
translation,297,147,experimental-setup,model training,to mitigate,slow convergence issues,model training to mitigate slow convergence issues,0.6496013402938843
translation,297,147,experimental-setup,experimental setup,has,"amsgrad ( reddi et al. , 2019 )","experimental setup has amsgrad ( reddi et al. , 2019 )",0.5041541457176208
translation,297,148,experimental-setup,uniform label smoothing,with,= 0.1,uniform label smoothing with = 0.1,0.6197128295898438
translation,297,148,experimental-setup,uniform label smoothing,perform,gradient clipping,uniform label smoothing perform gradient clipping,0.5290144681930542
translation,297,148,experimental-setup,gradient clipping,when,gradient norm,gradient clipping when gradient norm,0.629986584186554
translation,297,148,experimental-setup,gradient norm,has,is above 5,gradient norm has is above 5,0.604973316192627
translation,297,148,experimental-setup,experimental setup,use,uniform label smoothing,experimental setup use uniform label smoothing,0.5540266036987305
translation,297,148,experimental-setup,experimental setup,perform,gradient clipping,experimental setup perform gradient clipping,0.5520387291908264
translation,297,149,experimental-setup,300 - dimensional word-embedding,initialized with,glove,300 - dimensional word-embedding initialized with glove,0.7509647011756897
translation,297,149,experimental-setup,experimental setup,use,300 - dimensional word-embedding,experimental setup use 300 - dimensional word-embedding,0.570549488067627
translation,297,150,experimental-setup,previous 2 turns,for,dialogue history,previous 2 turns for dialogue history,0.6017584204673767
translation,297,150,experimental-setup,maximum utterance length,set to,50,maximum utterance length set to 50,0.7316628098487854
translation,297,150,experimental-setup,experimental setup,consider,previous 2 turns,experimental setup consider previous 2 turns,0.6560980081558228
translation,297,150,experimental-setup,experimental setup,has,maximum utterance length,experimental setup has maximum utterance length,0.5173882246017456
translation,297,151,experimental-setup,fc6( 4096 dimension ) layer representation,of,vgg - 19,fc6( 4096 dimension ) layer representation of vgg - 19,0.5593244433403015
translation,297,151,experimental-setup,fc6( 4096 dimension ) layer representation,pre-trained on,imagenet,fc6( 4096 dimension ) layer representation pre-trained on imagenet,0.751017689704895
translation,297,151,experimental-setup,image representation,has,fc6( 4096 dimension ) layer representation,image representation has fc6( 4096 dimension ) layer representation,0.5648349523544312
translation,297,151,experimental-setup,experimental setup,For,image representation,experimental setup For image representation,0.5551928877830505
translation,297,29,experiments,multi-domain multi-modal dialogue ( mdmmd ) dataset,comprising,text and images,multi-domain multi-modal dialogue ( mdmmd ) dataset comprising text and images,0.6460830569267273
translation,297,29,experiments,text and images,having,conversations,text and images having conversations,0.6990699172019958
translation,297,29,experiments,conversations,belonging to,three different domains,conversations belonging to three different domains,0.7549363970756531
translation,297,25,model,multi-modal graph convolutional network ( gcn ),incorporates,information,multi-modal graph convolutional network ( gcn ) incorporates information,0.702521800994873
translation,297,25,model,information,from,textual and visual modalities,information from textual and visual modalities,0.518826425075531
translation,297,25,model,textual and visual modalities,to generate,aspect-guided responses,textual and visual modalities to generate aspect-guided responses,0.7006234526634216
translation,297,25,model,model,present,multi-modal graph convolutional network ( gcn ),model present multi-modal graph convolutional network ( gcn ),0.6464790105819702
translation,297,142,model,utterance encoder,is,bidirectional gru,utterance encoder is bidirectional gru,0.5782443881034851
translation,297,142,model,bidirectional gru,with,600 hidden units,bidirectional gru with 600 hidden units,0.6285520792007446
translation,297,142,model,600 hidden units,in,each direction,600 hidden units in each direction,0.5026475787162781
translation,297,142,model,model,has,utterance encoder,model has utterance encoder,0.5678896903991699
translation,297,214,model,gcn based method,to capture,textual representation,gcn based method to capture textual representation,0.6877844333648682
translation,297,214,model,gcn based method,use,vgg - 19,gcn based method use vgg - 19,0.6181543469429016
translation,297,214,model,vgg - 19,for,image representation,vgg - 19 for image representation,0.6260569095611572
translation,297,214,model,model,develop,gcn based method,model develop gcn based method,0.6309893131256104
translation,297,214,model,model,use,vgg - 19,model use vgg - 19,0.6487459540367126
translation,297,215,model,context encoder,captures,multi-modal information,context encoder captures multi-modal information,0.7271268367767334
translation,297,215,model,multi-modal information,from,utterances,multi-modal information from utterances,0.5552214980125427
translation,297,215,model,model,has,context encoder,model has context encoder,0.5556648969650269
translation,297,216,model,representation,from,context encoder,representation from context encoder,0.5539109706878662
translation,297,216,model,representation,fed to,decoder,representation fed to decoder,0.746424674987793
translation,297,216,model,context encoder,along with,aspect vector,context encoder along with aspect vector,0.5849729180335999
translation,297,216,model,decoder,for generating,aspect-guided responses,decoder for generating aspect-guided responses,0.7239712476730347
translation,297,216,model,model,has,representation,model has representation,0.5781156420707703
translation,297,8,results,responses,guided by,aspect information,responses guided by aspect information,0.6840935349464417
translation,297,8,results,aspect information,provide,more interactive and informative responses,aspect information provide more interactive and informative responses,0.6280958652496338
translation,297,8,results,more interactive and informative responses,for,better communication,more interactive and informative responses for better communication,0.5782913565635681
translation,297,8,results,better communication,between,agent and the user,better communication between agent and the user,0.6405186057090759
translation,297,8,results,results,show,responses,results show responses,0.5609707832336426
translation,297,172,results,proposed approach,outperforms,all the baseline models,proposed approach outperforms all the baseline models,0.7419829368591309
translation,297,172,results,improvements,are,statistically significant,improvements are statistically significant,0.5855386853218079
translation,297,173,results,perplexity better,is,generated responses,perplexity better is generated responses,0.5780647397041321
translation,297,173,results,perplexity scores,of,proposed m- gcn + aspect model,perplexity scores of proposed m- gcn + aspect model,0.5412681102752686
translation,297,173,results,proposed m- gcn + aspect model,are,lowest,proposed m- gcn + aspect model are lowest,0.5665013194084167
translation,297,173,results,lowest,among,all the baseline models,lowest among all the baseline models,0.6600481271743774
translation,297,173,results,lower,has,perplexity better,lower has perplexity better,0.5700496435165405
translation,297,173,results,results,has,lower,results has lower,0.5253013968467712
translation,297,174,results,lower scores,for,perplexity,lower scores for perplexity,0.6167404651641846
translation,297,176,results,bleu - 4 metric,see that,proposed model m-gcn + aspect,bleu - 4 metric see that proposed model m-gcn + aspect,0.590362012386322
translation,297,176,results,proposed model m-gcn + aspect,having,ability,proposed model m-gcn + aspect having ability,0.6923560500144958
translation,297,176,results,proposed model m-gcn + aspect,achieves,higher scores,proposed model m-gcn + aspect achieves higher scores,0.6802184581756592
translation,297,176,results,ability,to generate,responses,ability to generate responses,0.6673519015312195
translation,297,176,results,ability,achieves,higher scores,ability achieves higher scores,0.5801265835762024
translation,297,176,results,responses,according to,specified aspect information,responses according to specified aspect information,0.6205359697341919
translation,297,176,results,responses,achieves,higher scores,responses achieves higher scores,0.5910482406616211
translation,297,176,results,higher scores,with,improvement,higher scores with improvement,0.638762354850769
translation,297,176,results,improvement,of,6.2 %,improvement of 6.2 %,0.5742694735527039
translation,297,176,results,6.2 %,from,mhred + aspect baseline model,6.2 % from mhred + aspect baseline model,0.5438634157180786
translation,297,178,results,rouge -l,increase of,6.12 %,rouge -l increase of 6.12 %,0.729188859462738
translation,297,178,results,6.12 %,in comparison to,multimodal hred framework,6.12 % in comparison to multimodal hred framework,0.6477115750312805
translation,297,178,results,results,in the case of,rouge -l,results in the case of rouge -l,0.6984221339225769
translation,297,180,results,frameworks,having,aspect information,frameworks having aspect information,0.6356124877929688
translation,297,185,results,fluency scores,of,baseline hred model,fluency scores of baseline hred model,0.5511007308959961
translation,297,185,results,baseline hred model,are,lowest,baseline hred model are lowest,0.578359067440033
translation,297,185,results,lowest,for,grammatically correct responses,lowest for grammatically correct responses,0.5487602353096008
translation,297,185,results,grammatically correct responses,due to,repetition and incomplete responses,grammatically correct responses due to repetition and incomplete responses,0.6424417495727539
translation,297,185,results,results,has,fluency scores,results has fluency scores,0.4856947362422943
translation,297,187,results,proposed framework,generates,responses,proposed framework generates responses,0.6177855730056763
translation,297,187,results,responses,appropriate to,specified aspects,responses appropriate to specified aspects,0.7307878732681274
translation,297,187,results,specified aspects,with,improvement,specified aspects with improvement,0.669177234172821
translation,297,187,results,improvement,of,8.46 %,improvement of 8.46 %,0.5678538680076599
translation,297,187,results,8.46 %,from,mhred + aspect based baseline,8.46 % from mhred + aspect based baseline,0.5184458494186401
translation,297,187,results,results,evident,proposed framework,results evident proposed framework,0.6726282238960266
translation,297,188,results,proposed model,with,aspect information,proposed model with aspect information,0.6234597563743591
translation,297,188,results,aspect information,provided,additionally,aspect information provided additionally,0.6789296269416809
translation,297,188,results,significantly higher,compared to,other methods,significantly higher compared to other methods,0.6681948304176331
translation,297,188,results,results,improvement in,proposed model,results improvement in proposed model,0.6983372569084167
translation,298,6,baselines,"qlearning , policy hill - climbing ( phc )",varying,scenario complexity ( state space size ),"qlearning , policy hill - climbing ( phc ) varying scenario complexity ( state space size )",0.7020913362503052
translation,298,6,baselines,"qlearning , policy hill - climbing ( phc )",varying,number of training episodes,"qlearning , policy hill - climbing ( phc ) varying number of training episodes",0.6914308667182922
translation,298,6,baselines,"qlearning , policy hill - climbing ( phc )",varying,learning rate,"qlearning , policy hill - climbing ( phc ) varying learning rate",0.7231690287590027
translation,298,6,baselines,"qlearning , policy hill - climbing ( phc )",varying,exploration rate,"qlearning , policy hill - climbing ( phc ) varying exploration rate",0.736083447933197
translation,298,6,baselines,fast policy hill - climbing ( phc - wolf ) algorithms,varying,scenario complexity ( state space size ),fast policy hill - climbing ( phc - wolf ) algorithms varying scenario complexity ( state space size ),0.67977374792099
translation,298,6,baselines,fast policy hill - climbing ( phc - wolf ) algorithms,varying,learning rate,fast policy hill - climbing ( phc - wolf ) algorithms varying learning rate,0.7084280252456665
translation,298,6,baselines,fast policy hill - climbing ( phc - wolf ) algorithms,varying,exploration rate,fast policy hill - climbing ( phc - wolf ) algorithms varying exploration rate,0.7139757871627808
translation,298,6,baselines,baselines,compare,"qlearning , policy hill - climbing ( phc )","baselines compare qlearning , policy hill - climbing ( phc )",0.6812146902084351
translation,298,36,baselines,q-learning,has,rl algorithm,q-learning has rl algorithm,0.5629547238349915
translation,298,36,baselines,baselines,compare,q-learning,baselines compare q-learning,0.6742931604385376
translation,298,4,experiments,single-agent and multi-agent reinforcement learning ( rl ),for learning,dialogue policies,single-agent and multi-agent reinforcement learning ( rl ) for learning dialogue policies,0.7208611965179443
translation,298,4,experiments,dialogue policies,in,resource allocation negotiation scenario,dialogue policies in resource allocation negotiation scenario,0.5246396660804749
translation,298,5,model,two agents,learn,concurrently,two agents learn concurrently,0.6331456303596497
translation,298,5,model,concurrently,by interacting with,each other,concurrently by interacting with each other,0.7472863793373108
translation,298,5,model,model,has,two agents,model has two agents,0.5567236542701721
translation,298,7,results,q-learning,fails to,converge,q-learning fails to converge,0.7851011157035828
translation,298,7,results,q-learning,fails to,converge,q-learning fails to converge,0.7851011157035828
translation,298,7,results,phc and phc - wolf,perform,similarly,phc and phc - wolf perform similarly,0.6612754464149475
translation,298,7,results,results,show,q-learning,results show q-learning,0.5869504809379578
translation,298,7,results,results,show,phc and phc - wolf,results show phc and phc - wolf,0.5823718309402466
translation,299,6,baselines,"fully automatic , uncertainty - aware evaluation method",for,open-domain dialogue systems,"fully automatic , uncertainty - aware evaluation method for open-domain dialogue systems",0.5428908467292786
translation,299,22,model,neural network,judges,quality,neural network judges quality,0.8179298043251038
translation,299,22,model,quality,of,responses,quality of responses,0.5949519276618958
translation,299,22,model,responses,by using,training data,responses by using training data,0.6544274091720581
translation,299,22,model,training data,automatically generated from,utterances,training data automatically generated from utterances,0.7511935234069824
translation,299,22,model,utterances,with,multiple responses,utterances with multiple responses,0.6901077032089233
translation,299,22,model,model,train,neural network,model train neural network,0.6953878402709961
translation,299,135,results,reference scorer,in,ruber,reference scorer in ruber,0.5170983672142029
translation,299,135,results,reference scorer,with,our ? bleu,reference scorer with our ? bleu,0.6171321272850037
translation,299,135,results,reference scorer,obtained,best overall correlations,reference scorer obtained best overall correlations,0.6690756678581238
translation,299,135,results,results,replacing,reference scorer,results replacing reference scorer,0.6857509016990662
translation,300,110,hyperparameters,forward and backward networks,structured to share,same set of word embeddings,forward and backward networks structured to share same set of word embeddings,0.7144652009010315
translation,300,110,hyperparameters,forward and backward networks,initialised with,pre-trained word vectors,forward and backward networks initialised with pre-trained word vectors,0.6449164748191833
translation,300,110,hyperparameters,hyperparameters,has,forward and backward networks,hyperparameters has forward and backward networks,0.5642982125282288
translation,300,111,hyperparameters,hidden layer size,set to be,80,hidden layer size set to be 80,0.733054518699646
translation,300,111,hyperparameters,deep networks,trained with,two hidden layers,deep networks trained with two hidden layers,0.7078280448913574
translation,300,111,hyperparameters,deep networks,trained with,50 % dropout rate,deep networks trained with 50 % dropout rate,0.6823384165763855
translation,300,111,hyperparameters,hyperparameters,has,hidden layer size,hyperparameters has hidden layer size,0.4991928040981293
translation,300,111,hyperparameters,hyperparameters,has,deep networks,hyperparameters has deep networks,0.5210888981819153
translation,300,7,model,statistical language generator,based on,semantically controlled long short-term memory ( lstm ) structure,statistical language generator based on semantically controlled long short-term memory ( lstm ) structure,0.6034902334213257
translation,300,26,model,statistical nlg,based on,semantically controlled long short-term memory ( lstm ) recurrent network,statistical nlg based on semantically controlled long short-term memory ( lstm ) recurrent network,0.6332916021347046
translation,300,162,model,neural networkbased generator,capable of generating,natural linguistically varied responses,neural networkbased generator capable of generating natural linguistically varied responses,0.7197282314300537
translation,300,162,model,natural linguistically varied responses,based on,"deep , semantically controlled lstm architecture","natural linguistically varied responses based on deep , semantically controlled lstm architecture",0.628148078918457
translation,300,162,model,"deep , semantically controlled lstm architecture",call,sc - lstm,"deep , semantically controlled lstm architecture call sc - lstm",0.5955148935317993
translation,300,162,model,model,proposed,neural networkbased generator,model proposed neural networkbased generator,0.7535687685012817
translation,300,163,model,generator,trained on,unaligned data,generator trained on unaligned data,0.7616185545921326
translation,300,163,model,unaligned data,by jointly optimising,sentence planning and surface realisation components,unaligned data by jointly optimising sentence planning and surface realisation components,0.7429189682006836
translation,300,163,model,sentence planning and surface realisation components,using,simple cross entropy criterion,sentence planning and surface realisation components using simple cross entropy criterion,0.6382645964622498
translation,300,163,model,simple cross entropy criterion,without,heuristics or handcrafting,simple cross entropy criterion without heuristics or handcrafting,0.7457813024520874
translation,300,163,model,model,has,generator,model has generator,0.5837957262992859
translation,300,8,results,lstm generator,learn from,unaligned data,lstm generator learn from unaligned data,0.6704722046852112
translation,300,8,results,unaligned data,by jointly optimising,sentence planning and surface realisation,unaligned data by jointly optimising sentence planning and surface realisation,0.732302725315094
translation,300,8,results,sentence planning and surface realisation,using,simple cross entropy training criterion,sentence planning and surface realisation using simple cross entropy training criterion,0.633563220500946
translation,300,8,results,sampling,from,output candidates,sampling from output candidates,0.6069033741950989
translation,300,8,results,results,has,lstm generator,results has lstm generator,0.5462320446968079
translation,300,27,results,unaligned data,by jointly optimising,sentence planning and surface realisation components,unaligned data by jointly optimising sentence planning and surface realisation components,0.7429189682006836
translation,300,27,results,sentence planning and surface realisation components,using,simple cross entropy training criterion,sentence planning and surface realisation components using simple cross entropy training criterion,0.639011800289154
translation,300,27,results,simple cross entropy training criterion,without,heuristics,simple cross entropy training criterion without heuristics,0.7158095836639404
translation,300,27,results,simple cross entropy training criterion,any,heuristics,simple cross entropy training criterion any heuristics,0.6744756698608398
translation,300,27,results,good quality language variation,obtained,randomly sampling,good quality language variation obtained randomly sampling,0.6249284148216248
translation,300,27,results,good quality language variation,by,randomly sampling,good quality language variation by randomly sampling,0.5562748312950134
translation,300,27,results,randomly sampling,has,network outputs,randomly sampling has network outputs,0.5655149817466736
translation,300,27,results,results,learn from,unaligned data,results learn from unaligned data,0.5871318578720093
translation,300,145,results,gates,gave,much lower slot error rates,gates gave much lower slot error rates,0.6465793251991272
translation,300,145,results,results,using,gates,results using gates,0.5179011821746826
translation,300,156,results,sc - lstm systems,has,),sc - lstm systems has ),0.6129632592201233
translation,300,156,results,),has,outperform,) has outperform,0.6833404302597046
translation,300,156,results,outperform,has,class - based lms ( classlm ),outperform has class - based lms ( classlm ),0.61598140001297
translation,300,156,results,results,has,sc - lstm systems,results has sc - lstm systems,0.5606744885444641
translation,300,157,results,deep sc - lstm system ( + deep ),significantly better than,class lms ( classlm ),deep sc - lstm system ( + deep ) significantly better than class lms ( classlm ),0.6831375360488892
translation,300,157,results,deep sc - lstm system ( + deep ),better than,rnn with heuristic gates ( rnn w / ),deep sc - lstm system ( + deep ) better than rnn with heuristic gates ( rnn w / ),0.7696514129638672
translation,300,157,results,class lms ( classlm ),in terms of,informativeness,class lms ( classlm ) in terms of informativeness,0.6650776863098145
translation,300,157,results,rnn with heuristic gates ( rnn w / ),in terms of,naturalness,rnn with heuristic gates ( rnn w / ) in terms of naturalness,0.6739454865455627
translation,300,157,results,results,has,deep sc - lstm system ( + deep ),results has deep sc - lstm system ( + deep ),0.5375538468360901
translation,300,164,results,sc - lstm model,achieved,best overall performance,sc - lstm model achieved best overall performance,0.6980035901069641
translation,300,164,results,best overall performance,on,two objective metrics,best overall performance on two objective metrics,0.48824647068977356
translation,300,164,results,two objective metrics,across,two different domains,two objective metrics across two different domains,0.6978869438171387
translation,300,164,results,results,found,sc - lstm model,results found sc - lstm model,0.6039571166038513
translation,301,2,experiments,movie-dic,has,movie dialogue corpus,movie-dic has movie dialogue corpus,0.5711167454719543
translation,302,9,experiments,cohesive answers,measure of,rhetoric agreement,cohesive answers measure of rhetoric agreement,0.6799030900001526
translation,302,9,experiments,rhetoric agreement,between,question and an answer,rhetoric agreement between question and an answer,0.6402114033699036
translation,302,9,experiments,question and an answer,by,tree kernel learning,question and an answer by tree kernel learning,0.551562488079071
translation,302,9,experiments,tree kernel learning,of,dts,tree kernel learning of dts,0.5618491172790527
translation,302,4,model,chat bot,with,iterative content exploration,chat bot with iterative content exploration,0.629977822303772
translation,302,4,model,iterative content exploration,leads,user,iterative content exploration leads user,0.6070833802223206
translation,302,4,model,user,through,personalized knowledge acquisition session,user through personalized knowledge acquisition session,0.6437727808952332
translation,302,4,model,model,build,chat bot,model build chat bot,0.765750527381897
translation,302,5,model,chat bot,designed as,automated customer support,chat bot designed as automated customer support,0.47040241956710815
translation,302,5,model,chat bot,designed as,product recommendation agent,chat bot designed as product recommendation agent,0.476218581199646
translation,302,5,model,product recommendation agent,assisting,user,product recommendation agent assisting user,0.5561919808387756
translation,302,5,model,user,in,learning,user in learning,0.5600560903549194
translation,302,5,model,user,in,product usability,user in product usability,0.4984295070171356
translation,302,5,model,user,in,suitability,user in suitability,0.5229681134223938
translation,302,5,model,user,in,troubleshooting,user in troubleshooting,0.4966806173324585
translation,302,5,model,user,in,other related tasks,user in other related tasks,0.5212259292602539
translation,302,5,model,learning,has,product features,learning has product features,0.5782643556594849
translation,302,5,model,model,has,chat bot,model has chat bot,0.6099886298179626
translation,302,6,model,linguistic discourse tree ( dt ),towards,set of documents,linguistic discourse tree ( dt ) towards set of documents,0.6190811991691589
translation,302,6,model,set of documents,with,multiple sections,set of documents with multiple sections,0.6381245851516724
translation,302,6,model,multiple sections,covering,topic,multiple sections covering topic,0.8085737228393555
translation,302,7,model,dt,built by,dt parsers,dt built by dt parsers,0.7535035610198975
translation,303,214,ablation-analysis,cl aad,boost,efficiency,cl aad boost efficiency,0.7957554459571838
translation,303,214,ablation-analysis,efficiency,benefits from,agent- aware experience replay,efficiency benefits from agent- aware experience replay,0.6859973669052124
translation,303,214,ablation-analysis,safety,has,cl aad,safety has cl aad,0.6281620860099792
translation,303,214,ablation-analysis,ablation analysis,except for,safety,ablation analysis except for safety,0.6878147125244141
translation,303,44,baselines,baselines,has,deep reinforcement learning ( drl ),baselines has deep reinforcement learning ( drl ),0.5500544905662537
translation,303,196,baselines,vanilla deep q-network,has,two hidden layers,vanilla deep q-network has two hidden layers,0.5711351633071899
translation,303,196,baselines,two hidden layers,each with,128 nodes,two hidden layers each with 128 nodes,0.6611324548721313
translation,303,196,baselines,dqn,has,vanilla deep q-network,dqn has vanilla deep q-network,0.5562481880187988
translation,303,197,baselines,advantage actor-critic policy,consists of,actor network,advantage actor-critic policy consists of actor network,0.6283490657806396
translation,303,197,baselines,advantage actor-critic policy,consists of,a critic network,advantage actor-critic policy consists of a critic network,0.651469886302948
translation,303,197,baselines,dropout layer,after,each hidden layer,dropout layer after each hidden layer,0.6471642851829529
translation,303,197,baselines,a2c,has,advantage actor-critic policy,a2c has advantage actor-critic policy,0.5781654119491577
translation,303,197,baselines,baselines,has,a2c,baselines has a2c,0.5546706914901733
translation,303,210,baselines,continuously optimized,with,a2c algorithm,continuously optimized with a2c algorithm,0.63251793384552
translation,303,210,baselines,cl aad,with,aad - dqn,cl aad with aad - dqn,0.6663382649421692
translation,303,210,baselines,full cl,with,aad - dqn,full cl with aad - dqn,0.6515864729881287
translation,303,210,baselines,pre-training,has,policy,pre-training has policy,0.5961285829544067
translation,303,210,baselines,cl aad,has,full cl,cl aad has full cl,0.643778383731842
translation,303,210,baselines,cl d,has,cl,cl d has cl,0.698941171169281
translation,303,210,baselines,cl d,has,without agent- aware experience repay,cl d has without agent- aware experience repay,0.6222348213195801
translation,303,210,baselines,cl,has,without agent- aware experience repay,cl has without agent- aware experience repay,0.6041076183319092
translation,303,198,hyperparameters,dropout rate,is,0.2,dropout rate is 0.2,0.5355412364006042
translation,303,198,hyperparameters,hyperparameters,has,dropout rate,hyperparameters has dropout rate,0.4790858328342438
translation,303,6,model,companion learning framework,to integrate,two approaches,companion learning framework to integrate two approaches,0.6725593209266663
translation,303,6,model,two approaches,for,on - line dialogue policy learning,two approaches for on - line dialogue policy learning,0.5628274083137512
translation,303,6,model,predefined rule- based policy,acts as,teacher,predefined rule- based policy acts as teacher,0.7245965003967285
translation,303,6,model,predefined rule- based policy,guides,data-driven rl system,predefined rule- based policy guides data-driven rl system,0.6642587780952454
translation,303,6,model,data-driven rl system,by giving,example actions,data-driven rl system by giving example actions,0.6570078730583191
translation,303,6,model,data-driven rl system,by giving,additional rewards,data-driven rl system by giving additional rewards,0.6553493738174438
translation,303,6,model,example actions,as,additional rewards,example actions as additional rewards,0.4610191881656647
translation,303,6,model,model,employ,companion learning framework,model employ companion learning framework,0.5666114687919617
translation,303,7,model,model,has,novel agent-aware dropout deep q-network ( aad - dqn ),model has novel agent-aware dropout deep q-network ( aad - dqn ),0.5603684186935425
translation,303,37,model,agent-aware dropout deep q-network ( aad - dqn ),as,student statistical policy,agent-aware dropout deep q-network ( aad - dqn ) as student statistical policy,0.49777358770370483
translation,303,37,model,student statistical policy,provides,two separate experience replay pools,student statistical policy provides two separate experience replay pools,0.6316898465156555
translation,303,37,model,two separate experience replay pools,for,student and teacher,two separate experience replay pools for student and teacher,0.6611247658729553
translation,303,37,model,uncertainty,estimated by,dropout,uncertainty estimated by dropout,0.6778789758682251
translation,303,37,model,uncertainty,to control,timing of consultation and learning,uncertainty to control timing of consultation and learning,0.7701773643493652
translation,303,37,model,model,propose,agent-aware dropout deep q-network ( aad - dqn ),model propose agent-aware dropout deep q-network ( aad - dqn ),0.6662694215774536
translation,303,39,model,agent- aware dropout deep q-network ( aad - dqn ),proposed as,statistical student policy,agent- aware dropout deep q-network ( aad - dqn ) proposed as statistical student policy,0.7006013989448547
translation,303,40,results,supervised pre-training,using,static dialogue corpus,supervised pre-training using static dialogue corpus,0.6242708563804626
translation,303,40,results,cl,with,aad - dqn,cl with aad - dqn,0.6758521795272827
translation,303,40,results,cl,achieve,better performance,cl achieve better performance,0.6679369211196899
translation,303,40,results,aad - dqn,achieve,better performance,aad - dqn achieve better performance,0.6480050683021545
translation,303,40,results,other companion teaching approaches,has,cl,other companion teaching approaches has cl,0.5819613337516785
translation,303,40,results,results,Compared with,other companion teaching approaches,results Compared with other companion teaching approaches,0.6210355758666992
translation,304,209,ablation-analysis,underperform,to classify,utterances,underperform to classify utterances,0.7570390105247498
translation,304,209,ablation-analysis,utterances,which belong to,rarely occurred classes,utterances which belong to rarely occurred classes,0.6947751045227051
translation,304,209,ablation-analysis,correctly,which belong to,rarely occurred classes,correctly which belong to rarely occurred classes,0.710734486579895
translation,304,209,ablation-analysis,support vector machines,has,underperform,support vector machines has underperform,0.5807329416275024
translation,304,209,ablation-analysis,utterances,has,correctly,utterances has correctly,0.617806077003479
translation,304,211,ablation-analysis,client target utterances,have,context,client target utterances have context,0.5376865267753601
translation,304,211,ablation-analysis,client target utterances,have,client context utterances,client target utterances have client context utterances,0.5463812947273254
translation,304,211,ablation-analysis,client target utterances,depend on,counselor 's preceding utterances,client target utterances depend on counselor 's preceding utterances,0.6502466797828674
translation,304,211,ablation-analysis,counselor 's preceding utterances,using,preceding counselor utterance,counselor 's preceding utterances using preceding counselor utterance,0.6310661435127258
translation,304,211,ablation-analysis,preceding counselor utterance,as well as,client context utterances,preceding counselor utterance as well as client context utterances,0.6059507727622986
translation,304,211,ablation-analysis,client context utterances,helps to improve,classification performance,client context utterances helps to improve classification performance,0.6740228533744812
translation,304,211,ablation-analysis,ablation analysis,has,client target utterances,ablation analysis has client target utterances,0.5605692863464355
translation,304,212,ablation-analysis,information,using,simple ( 6 ) seq2seq model,information using simple ( 6 ) seq2seq model,0.6802332997322083
translation,304,212,ablation-analysis,better performance,than,) ulmfit,better performance than ) ulmfit,0.5739908814430237
translation,304,212,ablation-analysis,.530 ),than,) ulmfit,.530 ) than ) ulmfit,0.6422284841537476
translation,304,212,ablation-analysis,hred,adds,higher - level rnn,hred adds higher - level rnn,0.592621922492981
translation,304,212,ablation-analysis,higher - level rnn,to,seq2seq models,higher - level rnn to seq2seq models,0.5391901731491089
translation,304,212,ablation-analysis,better performance,has,.530 ),better performance has .530 ),0.5705311894416809
translation,304,212,ablation-analysis,ablation analysis,integrating,information,ablation analysis integrating information,0.7243525981903076
translation,304,217,ablation-analysis,ablation study,applying,pre-trained components,ablation study applying pre-trained components,0.747126579284668
translation,304,217,ablation-analysis,pre-trained components,step by step,our model,pre-trained components step by step our model,0.7169782519340515
translation,304,217,ablation-analysis,ablation analysis,With,ablation study,ablation analysis With ablation study,0.5995650291442871
translation,304,217,ablation-analysis,ablation analysis,applying,pre-trained components,ablation analysis applying pre-trained components,0.7452651858329773
translation,304,220,ablation-analysis,task -specific seq2seq layers,to ensure,model 's sufficient capacity,task -specific seq2seq layers to ensure model 's sufficient capacity,0.6879481077194214
translation,304,220,ablation-analysis,task -specific seq2seq layers,applying,gradual unfreezing,task -specific seq2seq layers applying gradual unfreezing,0.7138133645057678
translation,304,220,ablation-analysis,model 's sufficient capacity,for capturing,relevant features,model 's sufficient capacity for capturing relevant features,0.7401170134544373
translation,304,220,ablation-analysis,gradual unfreezing,lead to,performance improvement,gradual unfreezing lead to performance improvement,0.6828756928443909
translation,304,220,ablation-analysis,ablation analysis,adding,task -specific seq2seq layers,ablation analysis adding task -specific seq2seq layers,0.6631641983985901
translation,304,223,ablation-analysis,model,uses,pre-trained word vectors,model uses pre-trained word vectors,0.5114022493362427
translation,304,223,ablation-analysis,model,helps to increase,performance,model helps to increase performance,0.7363418340682983
translation,304,223,ablation-analysis,performance,has,slightly,performance has slightly,0.6279520392417908
translation,304,223,ablation-analysis,ablation analysis,uses,pre-trained word vectors,ablation analysis uses pre-trained word vectors,0.5556907057762146
translation,304,223,ablation-analysis,ablation analysis,has,model,ablation analysis has model,0.4912438988685608
translation,304,224,ablation-analysis,two pre-trained lms,with,gradual unfreezing,two pre-trained lms with gradual unfreezing,0.6668038964271545
translation,304,224,ablation-analysis,two pre-trained lms,resulting in,substantial performance increase,two pre-trained lms resulting in substantial performance increase,0.6972724795341492
translation,304,224,ablation-analysis,substantial performance increase,has,.626,substantial performance increase has .626,0.573376476764679
translation,304,224,ablation-analysis,ablation analysis,adds,two pre-trained lms,ablation analysis adds two pre-trained lms,0.6233454942703247
translation,304,227,ablation-analysis,attention and dense layers,are,not sufficient,attention and dense layers are not sufficient,0.5876349210739136
translation,304,227,ablation-analysis,not sufficient,to learn,task -specific features,not sufficient to learn task -specific features,0.6519194841384888
translation,304,227,ablation-analysis,more capacity,improving,performance,more capacity improving performance,0.7291172742843628
translation,304,227,ablation-analysis,model,has,more capacity,model has more capacity,0.595614492893219
translation,304,227,ablation-analysis,ablation analysis,adding,attention and dense layers,ablation analysis adding attention and dense layers,0.7455461025238037
translation,304,228,ablation-analysis,decreased f1 score,has,.563 ),decreased f1 score has .563 ),0.5928934216499329
translation,304,228,ablation-analysis,ablation analysis,Without,taskspecific seq2seq layers,ablation analysis Without taskspecific seq2seq layers,0.6989533305168152
translation,304,175,baselines,svm,with,rbf kernel,svm with rbf kernel,0.647553026676178
translation,304,175,baselines,baselines,has,random forest,baselines has random forest,0.5333009958267212
translation,304,176,baselines,utterance,by,average,utterance by average,0.5884119868278503
translation,304,176,baselines,utterance,feed it to,classifier input,utterance feed it to classifier input,0.6349993944168091
translation,304,176,baselines,cnn,for,text classification,cnn for text classification,0.5836858153343201
translation,304,185,baselines,gradual unfreezing,applied during,training,gradual unfreezing applied during training,0.739426851272583
translation,304,185,baselines,baselines,has,gradual unfreezing,baselines has gradual unfreezing,0.6034387350082397
translation,304,213,baselines,convmfit,employs,pre-trained conversation model,convmfit employs pre-trained conversation model,0.5339064002037048
translation,304,213,baselines,pre-trained conversation model,based on,pre-trained lms,pre-trained conversation model based on pre-trained lms,0.6584307551383972
translation,304,213,baselines,outperforms,has,all other baseline models,outperforms has all other baseline models,0.566451907157898
translation,304,213,baselines,all other baseline models,has,),all other baseline models has ),0.6023508906364441
translation,304,213,baselines,baselines,has,convmfit,baselines has convmfit,0.5371137261390686
translation,304,177,hyperparameters,convolution layer,set,filter size,convolution layer set filter size,0.6822842359542847
translation,304,177,hyperparameters,convolution layer,set,30 filters,convolution layer set 30 filters,0.6633209586143494
translation,304,177,hyperparameters,convolution layer,apply,max-over - time pooling,convolution layer apply max-over - time pooling,0.5879250168800354
translation,304,177,hyperparameters,filter size,to,1 - 10,filter size to 1 - 10,0.5989853143692017
translation,304,177,hyperparameters,hyperparameters,In,convolution layer,hyperparameters In convolution layer,0.46136194467544556
translation,304,6,model,proper anonymization,collect,counselor-client dialogues,proper anonymization collect counselor-client dialogues,0.6519507765769958
translation,304,6,model,proper anonymization,develop,novel neural network model,proper anonymization develop novel neural network model,0.6240702271461487
translation,304,6,model,counselor-client dialogues,define,meaningful categories,counselor-client dialogues define meaningful categories,0.6023932695388794
translation,304,6,model,counselor-client dialogues,develop,novel neural network model,counselor-client dialogues develop novel neural network model,0.5965227484703064
translation,304,6,model,meaningful categories,of,client utterances,meaningful categories of client utterances,0.5984039902687073
translation,304,6,model,meaningful categories,of,client utterances,meaningful categories of client utterances,0.5984039902687073
translation,304,6,model,client utterances,with,professional counselors,client utterances with professional counselors,0.617546796798706
translation,304,6,model,novel neural network model,for classifying,client utterances,novel neural network model for classifying client utterances,0.7402775287628174
translation,304,6,model,model,With,proper anonymization,model With proper anonymization,0.6143977642059326
translation,304,6,model,model,develop,novel neural network model,model develop novel neural network model,0.6238305568695068
translation,304,25,model,conversation model fine-tuning ( convmfit ),to classify,utterances,conversation model fine-tuning ( convmfit ) to classify utterances,0.7153059840202332
translation,304,25,model,new model,has,conversation model fine-tuning ( convmfit ),new model has conversation model fine-tuning ( convmfit ),0.5781311988830566
translation,304,25,model,model,propose,new model,model propose new model,0.6939241886138916
translation,304,26,model,conversation model,to take advantage of,pre-trained knowledge,conversation model to take advantage of pre-trained knowledge,0.6428967118263245
translation,304,26,model,pre-trained knowledge,in,our model,pre-trained knowledge in our model,0.48528289794921875
translation,304,26,model,pre-trained language -specific word embeddings,has,language models,pre-trained language -specific word embeddings has language models,0.5374306440353394
translation,304,26,model,pre-trained language -specific word embeddings,has,conversation model,pre-trained language -specific word embeddings has conversation model,0.5405222177505493
translation,304,26,model,model,explicitly integrate,pre-trained language -specific word embeddings,model explicitly integrate pre-trained language -specific word embeddings,0.668942391872406
translation,304,109,model,pre-trained seq2seq based conversation model,to classify,client 's utterances,pre-trained seq2seq based conversation model to classify client 's utterances,0.6668066382408142
translation,304,109,model,model,introduce,convmfit ( conversation model fine-tuning ),model introduce convmfit ( conversation model fine-tuning ),0.5835655331611633
translation,304,8,results,classification result,shows,convmfit,classification result shows convmfit,0.6347358226776123
translation,304,8,results,convmfit,has,outperforms,convmfit has outperforms,0.651024580001831
translation,304,8,results,outperforms,has,state - of- the - art comparison models,outperforms has state - of- the - art comparison models,0.5355826020240784
translation,304,8,results,results,has,classification result,results has classification result,0.5694932341575623
translation,304,199,results,performance,of,convmfit,performance of convmfit,0.5890550017356873
translation,304,199,results,results,leverages,counselor and client language models,results leverages counselor and client language models,0.5828866362571716
translation,304,210,results,cnn and ( 4 ) rnn,show,better performance,cnn and ( 4 ) rnn show better performance,0.6720904111862183
translation,304,210,results,better performance,than,cnn,better performance than cnn,0.5840674042701721
translation,304,210,results,rnn,shows,slightly better performance,rnn shows slightly better performance,0.6949635744094849
translation,304,210,results,slightly better performance,than,cnn,slightly better performance than cnn,0.5895074009895325
translation,304,210,results,others,by using,pre-trained client language models,others by using pre-trained client language models,0.6307804584503174
translation,304,210,results,ulmfit,has,outperforms,ulmfit has outperforms,0.6546809077262878
translation,304,210,results,outperforms,has,others,outperforms has others,0.6126620769500732
translation,304,210,results,pre-trained client language models,has,.455 ),pre-trained client language models has .455 ),0.5678886771202087
translation,304,215,results,higher,for,class ( fact . ) and ( chan . ),higher for class ( fact . ) and ( chan . ),0.6428849101066589
translation,304,215,results,higher,which have,small numbers of examples,higher which have small numbers of examples,0.6517729759216309
translation,304,215,results,class ( fact . ) and ( chan . ),which have,small numbers of examples,class ( fact . ) and ( chan . ) which have small numbers of examples,0.664091169834137
translation,304,215,results,results,has,improvement,results has improvement,0.6248279809951782
translation,304,219,results,seq2seq layers,of,conversation model,seq2seq layers of conversation model,0.5302536487579346
translation,304,219,results,pre-trained word vectors,has,seq2seq layers,pre-trained word vectors has seq2seq layers,0.5239480137825012
translation,304,219,results,results,Adding,pre-trained word vectors,results Adding pre-trained word vectors,0.595075249671936
translation,304,293,results,models,use,client 's utterances,models use client 's utterances,0.6476072072982788
translation,304,293,results,client 's utterances,to predict,categories,client 's utterances to predict categories,0.7028587460517883
translation,304,293,results,ulmfit,show,better performance,ulmfit show better performance,0.6734667420387268
translation,304,293,results,better performance,to,others,better performance to others,0.5401281118392944
translation,304,293,results,models,has,ulmfit,models has ulmfit,0.6140580773353577
translation,304,293,results,results,Among,models,results Among models,0.6251620650291443
translation,305,182,ablation-analysis,memory component and pseudo parallel corpus,vary in,different ratios of training data,memory component and pseudo parallel corpus vary in different ratios of training data,0.7283182740211487
translation,305,182,ablation-analysis,ablation analysis,contribution of,memory component and pseudo parallel corpus,ablation analysis contribution of memory component and pseudo parallel corpus,0.6700499057769775
translation,305,183,ablation-analysis,memory component,brings,11.9 % and 3.0 % improvements,memory component brings 11.9 % and 3.0 % improvements,0.6104772090911865
translation,305,183,ablation-analysis,11.9 % and 3.0 % improvements,compared to,posthoc saliency,11.9 % and 3.0 % improvements compared to posthoc saliency,0.6494989991188049
translation,305,183,ablation-analysis,posthoc saliency,under,inform,posthoc saliency under inform,0.7071542739868164
translation,305,183,ablation-analysis,posthoc saliency,when,ratio,posthoc saliency when ratio,0.6614990830421448
translation,305,183,ablation-analysis,inform,when,ratio,inform when ratio,0.7503032684326172
translation,305,183,ablation-analysis,ratio,is,50 % and 20 %,ratio is 50 % and 20 %,0.6633528470993042
translation,305,183,ablation-analysis,pseudo parallel corpus,brings,3.4 % and 8.5 % improvements,pseudo parallel corpus brings 3.4 % and 8.5 % improvements,0.5908858180046082
translation,305,183,ablation-analysis,3.4 % and 8.5 % improvements,compared to,memory - based saliency,3.4 % and 8.5 % improvements compared to memory - based saliency,0.6265668869018555
translation,305,183,ablation-analysis,ablation analysis,has,memory component,ablation analysis has memory component,0.5187697410583496
translation,305,200,ablation-analysis,pseudo parallel corpus,might contribute more to,action learning,pseudo parallel corpus might contribute more to action learning,0.6435648202896118
translation,305,200,ablation-analysis,target domains,has,that do not have much advantage,target domains has that do not have much advantage,0.582159161567688
translation,305,200,ablation-analysis,target domains,has,pseudo parallel corpus,target domains has pseudo parallel corpus,0.5709578394889832
translation,305,200,ablation-analysis,that do not have much advantage,has,pseudo parallel corpus,that do not have much advantage has pseudo parallel corpus,0.5854801535606384
translation,305,200,ablation-analysis,ablation analysis,for,target domains,ablation analysis for target domains,0.5980986952781677
translation,305,206,ablation-analysis,performance,of,conditioned response generation,performance of conditioned response generation,0.4979584515094757
translation,305,206,ablation-analysis,effectively improve,has,performance,effectively improve has performance,0.5753288269042969
translation,305,206,ablation-analysis,ablation analysis,reusing,memory component,ablation analysis reusing memory component,0.7030752301216125
translation,305,172,baselines,"seqto-seq ( budzianowski et al. , 2018 )",implemented based on,"transformer ( vaswani et al. , 2017 )","seqto-seq ( budzianowski et al. , 2018 ) implemented based on transformer ( vaswani et al. , 2017 )",0.7001289129257202
translation,305,172,baselines,latent action learning,for,conditioned generation,latent action learning for conditioned generation,0.5680489540100098
translation,305,172,baselines,tscp,has,"lei et al. , 2018 )","tscp has lei et al. , 2018 )",0.585842490196228
translation,305,172,baselines,larl,has,"zhao et al. , 2019 )","larl has zhao et al. , 2019 )",0.6015753149986267
translation,305,172,baselines,mala,has,"huang et al. , 2020a )","mala has huang et al. , 2020a )",0.5874412059783936
translation,305,172,baselines,baselines,not consider,conditioned generation,baselines not consider conditioned generation,0.6910212635993958
translation,305,172,baselines,baselines,adopt,latent action learning,baselines adopt latent action learning,0.621048092842102
translation,305,174,baselines,post-hoc saliency,obtains,action representations,post-hoc saliency obtains action representations,0.5787359476089478
translation,305,174,baselines,action representations,via,importance attribution technique,action representations via importance attribution technique,0.6663566827774048
translation,305,174,baselines,memory - based saliency,trained without,pseudo parallel corpus,memory - based saliency trained without pseudo parallel corpus,0.7589380145072937
translation,305,203,baselines,content planning model,works on,natural language actions,content planning model works on natural language actions,0.6879312992095947
translation,305,203,baselines,classifier,denoted,act- dec,classifier denoted act- dec,0.7456098794937134
translation,305,203,baselines,classifier,denoted,act - cls,classifier denoted act - cls,0.7679827213287354
translation,305,166,hyperparameters,grid search,to find,best hyperparameters,grid search to find best hyperparameters,0.5902838706970215
translation,305,166,hyperparameters,best hyperparameters,for,models,best hyperparameters for models,0.5897504687309265
translation,305,166,hyperparameters,best hyperparameters,use,combination,best hyperparameters use combination,0.6619192361831665
translation,305,166,hyperparameters,models,based on,validation performance,models based on validation performance,0.656038224697113
translation,305,166,hyperparameters,combination,of,"inform , success and bleu scores","combination of inform , success and bleu scores",0.5584782958030701
translation,305,166,hyperparameters,hyperparameters,use,grid search,hyperparameters use grid search,0.67574143409729
translation,305,167,hyperparameters,embedding dimensionality d,among,"{ 50 , 75 , 100 , 150 , 200 }","embedding dimensionality d among { 50 , 75 , 100 , 150 , 200 }",0.5972720980644226
translation,305,167,hyperparameters,embedding dimensionality d,in,"[ 0.01 , 1.0 ]","embedding dimensionality d in [ 0.01 , 1.0 ]",0.5064941048622131
translation,305,167,hyperparameters,hyperparameters,choose,embedding dimensionality d,hyperparameters choose embedding dimensionality d,0.6384006142616272
translation,305,171,hyperparameters,one domain,as,low-resource target domain,one domain as low-resource target domain,0.5445640087127686
translation,305,171,hyperparameters,others,as,source domains,others as source domains,0.49952396750450134
translation,305,171,hyperparameters,hyperparameters,use,one domain,hyperparameters use one domain,0.6477727293968201
translation,305,12,model,unsupervised approach,learns,memory component,unsupervised approach learns memory component,0.6850903034210205
translation,305,12,model,memory component,to summarize,system utterances,memory component to summarize system utterances,0.7381691932678223
translation,305,12,model,system utterances,into,short span of words,system utterances into short span of words,0.5289674997329712
translation,305,13,model,compact action representation,propose,auxiliary task,compact action representation propose auxiliary task,0.6414328813552856
translation,305,13,model,auxiliary task,restores,state annotations,auxiliary task restores state annotations,0.710493803024292
translation,305,13,model,state annotations,as,summarized dialogue context,state annotations as summarized dialogue context,0.5038741827011108
translation,305,13,model,summarized dialogue context,using,memory component,summarized dialogue context using memory component,0.6851308345794678
translation,305,13,model,model,To further promote,compact action representation,model To further promote compact action representation,0.6566428542137146
translation,305,42,model,memoryaugmented saliency approach,identifies,salient words,memoryaugmented saliency approach identifies salient words,0.5898159146308899
translation,305,42,model,salient words,from,broader vocabulary,salient words from broader vocabulary,0.5564042329788208
translation,305,42,model,model,proposing,memoryaugmented saliency approach,model proposing memoryaugmented saliency approach,0.7269718647003174
translation,305,43,model,vocabulary,consists of,all the words,vocabulary consists of all the words,0.637856125831604
translation,305,43,model,vocabulary,consists of,each word,vocabulary consists of each word,0.6506237983703613
translation,305,43,model,all the words,could compose,natural language actions,all the words could compose natural language actions,0.6918200850486755
translation,305,43,model,each word,stored as,slot,each word stored as slot,0.6866885423660278
translation,305,43,model,slot,in,memory component,slot in memory component,0.5612338781356812
translation,305,43,model,model,has,vocabulary,model has vocabulary,0.592056393623352
translation,305,204,model,action decoder,generates,text span,action decoder generates text span,0.6411876678466797
translation,305,204,model,action decoder,feed it to,language generation model,action decoder feed it to language generation model,0.5739021897315979
translation,305,204,model,text span,feed it to,language generation model,text span feed it to language generation model,0.619718611240387
translation,305,204,model,action classifier,conducts,classification,action classifier conducts classification,0.6471980214118958
translation,305,204,model,classification,to select,one action,classification to select one action,0.6734908223152161
translation,305,204,model,one action,from,action set,one action from action set,0.6037028431892395
translation,305,204,model,action set,given by,training set,action set given by training set,0.6316004395484924
translation,305,204,model,model,has,action decoder,model has action decoder,0.5499139428138733
translation,305,205,model,planning model,with,action embeddings,planning model with action embeddings,0.5530973672866821
translation,305,205,model,planning model,with,memory component,planning model with memory component,0.5847846269607544
translation,305,205,model,model,enhance,planning model,model enhance planning model,0.6328805685043335
translation,305,225,model,our proposed model masp,learns,unified and compact action representations,our proposed model masp learns unified and compact action representations,0.6763057708740234
translation,305,225,model,model,has,our proposed model masp,model has our proposed model masp,0.6361216902732849
translation,305,226,model,memory component,summarizes,system utterances,memory component summarizes system utterances,0.7274328470230103
translation,305,226,model,system utterances,into,natural language actions,system utterances into natural language actions,0.534841775894165
translation,305,226,model,system utterances,i.e.,spans,system utterances i.e. spans,0.6802902221679688
translation,305,226,model,of words,from,unified vocabulary,of words from unified vocabulary,0.4859349727630615
translation,305,226,model,spans,has,of words,spans has of words,0.6291431188583374
translation,305,226,model,model,propose,memory component,model propose memory component,0.6788484454154968
translation,305,14,results,latent action baselines,on,multiwoz,latent action baselines on multiwoz,0.5330590605735779
translation,305,14,results,our proposed approach,has,outperforms,our proposed approach has outperforms,0.6332746148109436
translation,305,14,results,outperforms,has,latent action baselines,outperforms has latent action baselines,0.5875802040100098
translation,305,14,results,results,has,our proposed approach,results has our proposed approach,0.5936056971549988
translation,305,176,results,outperforms,in,multi-domain joint training setting,outperforms in multi-domain joint training setting,0.5088914632797241
translation,305,176,results,baselines,in,multi-domain joint training setting,baselines in multi-domain joint training setting,0.5096843242645264
translation,305,176,results,masp,has,outperforms,masp has outperforms,0.664983868598938
translation,305,176,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
translation,305,176,results,results,shows,masp,results shows masp,0.6743320822715759
translation,305,177,results,masp,achieves,better,masp achieves better,0.7455152273178101
translation,305,177,results,masp,achieves,language quality,masp achieves language quality,0.6274762153625488
translation,305,177,results,dialogue task completion,measured by,inform and success,dialogue task completion measured by inform and success,0.7148731350898743
translation,305,177,results,language quality,measured by,bleu,language quality measured by bleu,0.640122652053833
translation,305,177,results,language quality,especially in,low resource scenarios,language quality especially in low resource scenarios,0.6030798554420471
translation,305,177,results,better,has,dialogue task completion,better has dialogue task completion,0.5551353693008423
translation,305,177,results,results,has,masp,results has masp,0.5623499751091003
translation,305,187,results,masp,has,significantly outperforms,masp has significantly outperforms,0.6256610751152039
translation,305,187,results,significantly outperforms,has,baselines,significantly outperforms has baselines,0.6014122366905212
translation,305,187,results,results,show,masp,results show masp,0.6392921805381775
translation,305,198,results,memory - based saliency,more comparable to,mala,memory - based saliency more comparable to mala,0.6987478137016296
translation,305,198,results,mala,when using,train,mala when using train,0.7231302261352539
translation,305,198,results,train,as,target domain,train as target domain,0.5281468629837036
translation,305,198,results,results,see that,memory - based saliency,results see that memory - based saliency,0.6503990292549133
translation,305,207,results,perform better,than,action decoder,perform better than action decoder,0.5823318958282471
translation,305,207,results,results,find that,action classifier,results find that action classifier,0.6257374882698059
translation,306,4,baselines,multi-turn dialogue agent,helps,users,multi-turn dialogue agent helps users,0.598115861415863
translation,306,4,baselines,kb - infobot,has,multi-turn dialogue agent,kb - infobot has multi-turn dialogue agent,0.5743895769119263
translation,306,4,baselines,users,has,search knowledge bases ( kbs ),users has search knowledge bases ( kbs ),0.5841230750083923
translation,306,22,model,probabilistic framework,for computing,posterior distribution,probabilistic framework for computing posterior distribution,0.6464784145355225
translation,306,22,model,posterior distribution,of,user target,posterior distribution of user target,0.5838086605072021
translation,306,22,model,user target,over,knowledge base,user target over knowledge base,0.6219825148582458
translation,306,22,model,knowledge base,term,soft - kb lookup,knowledge base term soft - kb lookup,0.7099400162696838
translation,306,22,model,model,propose,probabilistic framework,model propose probabilistic framework,0.6710799336433411
translation,307,8,baselines,evaluation model ( adem ),learns to predict,human-like scores,evaluation model ( adem ) learns to predict human-like scores,0.7700796723365784
translation,307,8,baselines,human-like scores,to,input responses,human-like scores to input responses,0.5634396076202393
translation,307,8,baselines,new dataset,of,human response scores,new dataset of human response scores,0.5620463490486145
translation,307,145,baselines,adem,using,tweet2vec embeddings,adem using tweet2vec embeddings,0.6300768256187439
translation,307,44,experimental-setup,model,to predict,human scores,model to predict human scores,0.7235509157180786
translation,307,44,experimental-setup,human scores,to,dialogue responses,human scores to dialogue responses,0.5672233700752258
translation,307,44,experimental-setup,dataset of human judgements ( scores ),of,twitter responses,dataset of human judgements ( scores ) of twitter responses,0.5617475509643555
translation,307,44,experimental-setup,twitter responses,using,crowdsourcing platform amazon mechanical turk ( amt ),twitter responses using crowdsourcing platform amazon mechanical turk ( amt ),0.6341818571090698
translation,307,44,experimental-setup,experimental setup,collect,dataset of human judgements ( scores ),experimental setup collect dataset of human judgements ( scores ),0.6616600155830383
translation,307,113,hyperparameters,effective vocabulary size,use,byte pair encoding ( bpe ),effective vocabulary size use byte pair encoding ( bpe ),0.6310595870018005
translation,307,113,hyperparameters,byte pair encoding ( bpe ),splits,each word,byte pair encoding ( bpe ) splits each word,0.7002153396606445
translation,307,113,hyperparameters,each word,into,sub-words or characters,each word into sub-words or characters,0.5807778835296631
translation,307,113,hyperparameters,hyperparameters,to reduce,effective vocabulary size,hyperparameters to reduce effective vocabulary size,0.594688892364502
translation,307,114,hyperparameters,layer normalization,for,hierarchical encoder,layer normalization for hierarchical encoder,0.593614399433136
translation,307,114,hyperparameters,hyperparameters,use,layer normalization,hyperparameters use layer normalization,0.6045794486999512
translation,307,115,hyperparameters,words,in,decoder,words in decoder,0.5806415677070618
translation,307,115,hyperparameters,words,with,fixed rate,words with fixed rate,0.6710909008979797
translation,307,115,hyperparameters,fixed rate,of,25 %,fixed rate of 25 %,0.6176683902740479
translation,307,115,hyperparameters,kl,-,divergence term,kl - divergence term,0.5366606116294861
translation,307,115,hyperparameters,divergence term,linearly from,0 to 1,divergence term linearly from 0 to 1,0.7056519389152527
translation,307,115,hyperparameters,0 to 1,over,"first 60,000 batches","0 to 1 over first 60,000 batches",0.6784895062446594
translation,307,115,hyperparameters,kl,has,divergence term,kl has divergence term,0.49832674860954285
translation,307,115,hyperparameters,hyperparameters,To train,vhred model,hyperparameters To train vhred model,0.6669409871101379
translation,307,115,hyperparameters,hyperparameters,drop,words,hyperparameters drop words,0.7536042928695679
translation,307,116,hyperparameters,adam,as,optimizer,adam as optimizer,0.5250696539878845
translation,307,116,hyperparameters,hyperparameters,use,adam,hyperparameters use adam,0.6479569673538208
translation,307,117,hyperparameters,adem,employ,subsampling procedure,adem employ subsampling procedure,0.58592689037323
translation,307,117,hyperparameters,subsampling procedure,based on,model response length,subsampling procedure based on model response length,0.623275101184845
translation,307,117,hyperparameters,hyperparameters,When training,adem,hyperparameters When training adem,0.7262833714485168
translation,307,7,model,model,formulate,automatic dialogue evaluation,model formulate automatic dialogue evaluation,0.621930718421936
translation,307,143,results,correlates far better,with,human judgement,correlates far better with human judgement,0.6671743988990784
translation,307,143,results,human judgement,than,wordoverlap baselines,human judgement than wordoverlap baselines,0.5604991316795349
translation,307,143,results,adem,has,correlates far better,adem has correlates far better,0.6026808619499207
translation,307,143,results,results,see,adem,results see adem,0.5802513957023621
translation,307,163,results,adem model,able to,generalize,adem model able to generalize,0.6813288331031799
translation,307,163,results,generalize,for,all models,generalize for all models,0.6477581262588501
translation,307,163,results,all models,except,dual encoder,all models except dual encoder,0.6882213950157166
translation,307,163,results,results,observe,adem model,results observe adem model,0.5865452289581299
translation,308,215,ablation-analysis,planning step,increases in,d3q,planning step increases in d3q,0.6853020787239075
translation,308,215,ablation-analysis,performance,shows,apparent drop,performance shows apparent drop,0.725402295589447
translation,308,215,ablation-analysis,planning step,has,performance,planning step has performance,0.6036663055419922
translation,308,215,ablation-analysis,d3q,has,performance,d3q has performance,0.5899290442466736
translation,308,215,ablation-analysis,ablation analysis,When,planning step,ablation analysis When planning step,0.6558374762535095
translation,308,162,baselines,methods,for,dialogue learning,methods for dialogue learning,0.5383179783821106
translation,308,162,baselines,regs,cal,lstm,regs cal lstm,0.6513580083847046
translation,308,162,baselines,monte carlo search,implemented to obtain,rewards,monte carlo search implemented to obtain rewards,0.7079300880432129
translation,308,162,baselines,rewards,for,every generation step,rewards for every generation step,0.5890331268310547
translation,308,162,baselines,every generation step,to update,generator g.,every generation step to update generator g.,0.7706118226051331
translation,308,166,baselines,generator model g,adopt,same seq2seq model,generator model g adopt same seq2seq model,0.6453919410705566
translation,308,166,baselines,same seq2seq model,with,attention mechanism,same seq2seq model with attention mechanism,0.6329975724220276
translation,308,203,baselines,ssn - based discriminator,within,state - of- the - art task - oriented dialogue policy learning approach,ssn - based discriminator within state - of- the - art task - oriented dialogue policy learning approach,0.6126734614372253
translation,308,159,experiments,open-domain dialogue learning,choose,widely - used opensubtitles,open-domain dialogue learning choose widely - used opensubtitles,0.7017713785171509
translation,308,159,experiments,widely - used opensubtitles,has,) dataset,widely - used opensubtitles has ) dataset,0.5477274656295776
translation,308,127,hyperparameters,seq2seq model,for,response generation,seq2seq model for response generation,0.6118772029876709
translation,308,127,hyperparameters,response generation,as,generator g,response generation as generator g,0.5790998935699463
translation,308,127,hyperparameters,hyperparameters,use,seq2seq model,hyperparameters use seq2seq model,0.6122431755065918
translation,308,169,hyperparameters,monte carlo search,to give,rewards,monte carlo search to give rewards,0.6777446866035461
translation,308,169,hyperparameters,rewards,for,each generation step,rewards for each generation step,0.6051958799362183
translation,308,169,hyperparameters,hyperparameters,implement,monte carlo search,hyperparameters implement monte carlo search,0.6509057283401489
translation,308,172,hyperparameters,dimension,of,utterance embeddings,dimension of utterance embeddings,0.558549165725708
translation,308,172,hyperparameters,utterance embeddings,is,128,utterance embeddings is 128,0.5305754542350769
translation,308,172,hyperparameters,hyperparameters,has,dimension,hyperparameters has dimension,0.514893651008606
translation,308,173,hyperparameters,hidden size,is,256,hidden size is 256,0.60945725440979
translation,308,173,hyperparameters,hidden size,is,1024,hidden size is 1024,0.6171846985816956
translation,308,173,hyperparameters,256,for,utterance encoding bi-lstm,256 for utterance encoding bi-lstm,0.5947238802909851
translation,308,173,hyperparameters,1024,for,triple reasoning bi-lstm,1024 for triple reasoning bi-lstm,0.5973584055900574
translation,308,173,hyperparameters,hyperparameters,has,hidden size,hyperparameters has hidden size,0.535134494304657
translation,308,174,hyperparameters,single hidden layer,of size,512,single hidden layer of size 512,0.7379506230354309
translation,308,174,hyperparameters,mlp,has,single hidden layer,mlp has single hidden layer,0.5844205617904663
translation,308,174,hyperparameters,hyperparameters,has,mlp,hyperparameters has mlp,0.5759816765785217
translation,308,206,hyperparameters,conventional discriminator d,of,d3q,conventional discriminator d of d3q,0.6284287571907043
translation,308,206,hyperparameters,d3q,with,our ssn,d3q with our ssn,0.7343161106109619
translation,308,206,hyperparameters,hyperparameters,replace,conventional discriminator d,hyperparameters replace conventional discriminator d,0.5693371891975403
translation,308,208,hyperparameters,dimension,of,utterance embeddings,dimension of utterance embeddings,0.558549165725708
translation,308,208,hyperparameters,utterance embeddings,is,80,utterance embeddings is 80,0.5492789149284363
translation,308,208,hyperparameters,self-supervised network,has,dimension,self-supervised network has dimension,0.5674962401390076
translation,308,208,hyperparameters,hyperparameters,In,self-supervised network,hyperparameters In self-supervised network,0.4910147488117218
translation,308,209,hyperparameters,hidden size,is,128,hidden size is 128,0.6033065915107727
translation,308,209,hyperparameters,hidden size,is,512,hidden size is 512,0.6110948920249939
translation,308,209,hyperparameters,128,for,utterance encoding bi-lstm,128 for utterance encoding bi-lstm,0.6015011668205261
translation,308,209,hyperparameters,512,for,triple reasoning bi-lstm,512 for triple reasoning bi-lstm,0.6023625135421753
translation,308,209,hyperparameters,hyperparameters,has,hidden size,hyperparameters has hidden size,0.535134494304657
translation,308,210,hyperparameters,single hidden layer,of size,128,single hidden layer of size 128,0.7298716902732849
translation,308,210,hyperparameters,mlp,has,single hidden layer,mlp has single hidden layer,0.5844205617904663
translation,308,210,hyperparameters,hyperparameters,has,mlp,hyperparameters has mlp,0.5759816765785217
translation,308,211,hyperparameters,simulator,to generate,user utterances,simulator to generate user utterances,0.715100884437561
translation,308,211,hyperparameters,threshold interval,set to,range,threshold interval set to range,0.6983648538589478
translation,308,211,hyperparameters,range,between,0.45 and 0.55,range between 0.45 and 0.55,0.6420290470123291
translation,308,211,hyperparameters,simulator,has,threshold interval,simulator has threshold interval,0.5486156344413757
translation,308,211,hyperparameters,hyperparameters,use,simulator,hyperparameters use simulator,0.696266233921051
translation,308,8,model,samplingbased self-supervised network ssn,to perform,prediction,samplingbased self-supervised network ssn to perform prediction,0.6683467626571655
translation,308,8,model,prediction,with,sampled triple references,prediction with sampled triple references,0.6708958148956299
translation,308,8,model,sampled triple references,from,previous dialogue history,sampled triple references from previous dialogue history,0.559376060962677
translation,308,8,model,model,propose,samplingbased self-supervised network ssn,model propose samplingbased self-supervised network ssn,0.6249150633811951
translation,308,9,model,joint learning framework,where,ssn,joint learning framework where ssn,0.615718424320221
translation,308,9,model,ssn,guide,dialogue systems,ssn guide dialogue systems,0.6586052179336548
translation,308,9,model,dialogue systems,towards,more coherent and relevant dialogue learning,dialogue systems towards more coherent and relevant dialogue learning,0.6303505897521973
translation,308,9,model,more coherent and relevant dialogue learning,through,adversarial training,more coherent and relevant dialogue learning through adversarial training,0.6335006356239319
translation,308,9,model,model,design,joint learning framework,model design joint learning framework,0.5381903648376465
translation,308,20,model,sequential order,within,dialogue,sequential order within dialogue,0.6680538058280945
translation,308,20,model,dialogue,as,self-supervised signal,dialogue as self-supervised signal,0.5749967098236084
translation,308,20,model,self-supervised signal,to guide,meaningful and coherent dialogue learning,self-supervised signal to guide meaningful and coherent dialogue learning,0.6498629450798035
translation,308,20,model,model,explore,sequential order,model explore sequential order,0.6990136504173279
translation,308,30,model,ssn and the dialogue model,via,alternative training,ssn and the dialogue model via alternative training,0.6405342221260071
translation,308,30,model,alternative training,where,output probability,alternative training where output probability,0.6105844974517822
translation,308,30,model,output probability,of,ssn,output probability of ssn,0.627115786075592
translation,308,30,model,output probability,treated as,order signal,output probability treated as order signal,0.7163769006729126
translation,308,30,model,order signal,to evaluate,generated utterance,order signal to evaluate generated utterance,0.7314981818199158
translation,308,163,model,discriminator d,encodes,currently generated utterance,discriminator d encodes currently generated utterance,0.695531964302063
translation,308,163,model,discriminator d,encodes,generator g,discriminator d encodes generator g,0.7221822738647461
translation,308,163,model,currently generated utterance,by,cnn model,currently generated utterance by cnn model,0.560985803604126
translation,308,163,model,generator g,optimized using,approximate embedding layer,generator g optimized using approximate embedding layer,0.6329700350761414
translation,308,163,model,model,has,discriminator d,model has discriminator d,0.561442494392395
translation,308,170,model,sampling process,use,multiple gpus,sampling process use multiple gpus,0.6197230219841003
translation,308,170,model,multiple gpus,to parallelize and distribute,jobs,multiple gpus to parallelize and distribute jobs,0.7270717620849609
translation,308,170,model,model,To accelerate,sampling process,model To accelerate sampling process,0.7330319881439209
translation,308,171,model,ssn,first gets,pre-trained,ssn first gets pre-trained,0.6539856791496277
translation,308,171,model,ssn,first gets,iteratively updated,ssn first gets iteratively updated,0.7359004616737366
translation,308,171,model,pre-trained,using,sampled data,pre-trained using sampled data,0.7058326005935669
translation,308,171,model,sampled data,from,open-subtitiles,sampled data from open-subtitiles,0.5617614984512329
translation,308,171,model,iteratively updated,during,min-max adversarial training process,iteratively updated during min-max adversarial training process,0.6283741593360901
translation,308,171,model,model,first gets,pre-trained,model first gets pre-trained,0.651412308216095
translation,308,152,results,marginal improvement,compared with,strategy,marginal improvement compared with strategy,0.7482597827911377
translation,308,152,results,strategy,considers,target triple,strategy considers target triple,0.7238801717758179
translation,308,152,results,target triple,without,any history,target triple without any history,0.784428060054779
translation,308,152,results,results,has,conventional hierarchical lstm,results has conventional hierarchical lstm,0.5370303392410278
translation,308,154,results,reference triples,can be,tremendous supplement,reference triples can be tremendous supplement,0.6435045599937439
translation,308,154,results,tremendous supplement,to,inconsistent order detection,tremendous supplement to inconsistent order detection,0.5437050461769104
translation,308,156,results,ordered and misordered references,has,ssn,ordered and misordered references has ssn,0.6015006899833679
translation,308,156,results,ssn,has,highest classification accuracy,ssn has highest classification accuracy,0.558564305305481
translation,308,156,results,results,when having,ordered and misordered references,results when having ordered and misordered references,0.6561238765716553
translation,308,181,results,our trained generator,achieve,higher adversuc,our trained generator achieve higher adversuc,0.6089649796485901
translation,308,181,results,higher adversuc,in,three discriminators,higher adversuc in three discriminators,0.514615535736084
translation,308,181,results,higher adversuc,shows that,generator,higher adversuc shows that generator,0.7415757775306702
translation,308,181,results,generator,generate,more humanlike utterance responses,generator generate more humanlike utterance responses,0.7054343223571777
translation,308,181,results,results,observe,our trained generator,results observe our trained generator,0.5961856842041016
translation,308,182,results,other two methods,have,noticeable drop,other two methods have noticeable drop,0.5703429579734802
translation,308,182,results,noticeable drop,in,adversuc,noticeable drop in adversuc,0.5586531162261963
translation,308,182,results,adversuc,when evaluating,ssn - based discriminator,adversuc when evaluating ssn - based discriminator,0.6638415455818176
translation,308,182,results,results,generators of,other two methods,results generators of other two methods,0.6733836531639099
translation,308,184,results,regs method,with,full dialogue history encoded,regs method with full dialogue history encoded,0.6251548528671265
translation,308,184,results,regs method,performs,worse,regs method performs worse,0.6607227921485901
translation,308,184,results,worse,than,ael,worse than ael,0.7127612233161926
translation,308,184,results,ael,only considers,current utterances,ael only considers current utterances,0.7380461692810059
translation,308,184,results,results,has,regs method,results has regs method,0.5537568926811218
translation,308,190,results,distinct - 1 and distinct - 2 metrics,generator trained in,our approach,distinct - 1 and distinct - 2 metrics generator trained in our approach,0.7444234490394592
translation,308,190,results,results,based on,distinct - 1 and distinct - 2 metrics,results based on distinct - 1 and distinct - 2 metrics,0.6498022675514221
translation,308,190,results,results,generator trained in,our approach,results generator trained in our approach,0.7168328762054443
translation,308,195,results,significant improvement,in,quality,significant improvement in quality,0.4926450252532959
translation,308,195,results,quality,of,generated sentences,quality of generated sentences,0.5869962573051453
translation,308,195,results,results,generator trained in,our method,results generator trained in our method,0.710887610912323
translation,308,214,results,d3q,in,most of cases,d3q in most of cases,0.5453709363937378
translation,308,214,results,our ssn - based discriminator,improve,ability,our ssn - based discriminator improve ability,0.6813868284225464
translation,308,214,results,ability,to recognize,high-quality stimulated user experiences,ability to recognize high-quality stimulated user experiences,0.7167056798934937
translation,308,214,results,d3q - ssn,has,outperform,d3q - ssn has outperform,0.6753902435302734
translation,308,214,results,outperform,has,d3q,outperform has d3q,0.7157228589057922
translation,308,217,results,ssn,see,some performance improvement,ssn see some performance improvement,0.6028135418891907
translation,308,217,results,some performance improvement,when using,10 - step planning,some performance improvement when using 10 - step planning,0.7099375128746033
translation,309,79,experimental-setup,batch size,of,3072 sequences,batch size of 3072 sequences,0.6223183274269104
translation,309,79,experimental-setup,3072 sequences,for,3 m updates,3072 sequences for 3 m updates,0.6746320128440857
translation,309,79,experimental-setup,3 m updates,using,learning rate,3 m updates using learning rate,0.6278395056724548
translation,309,79,experimental-setup,3 m updates,using,inverse square root scheduler,3 m updates using inverse square root scheduler,0.6780367493629456
translation,309,79,experimental-setup,learning rate,of,5e - 4,learning rate of 5e - 4,0.6392122507095337
translation,309,79,experimental-setup,experimental setup,trained with,batch size,experimental setup trained with batch size,0.7112688422203064
translation,309,80,experimental-setup,pre-training,took,approximately two weeks,pre-training took approximately two weeks,0.6076361536979675
translation,309,80,experimental-setup,approximately two weeks,using,64 nvidia v100s,approximately two weeks using 64 nvidia v100s,0.6932711005210876
translation,309,80,experimental-setup,experimental setup,has,pre-training,experimental setup has pre-training,0.527898907661438
translation,309,7,model,model,extending,"recently introduced unlikelihood loss ( welleck et al. , 2019a )","model extending recently introduced unlikelihood loss ( welleck et al. , 2019a )",0.6244428157806396
translation,309,25,model,inference ( nli ) data,as,supervision,inference ( nli ) data as supervision,0.4973706901073456
translation,309,25,model,supervision,against,poor quality generations,supervision against poor quality generations,0.6641007661819458
translation,309,25,model,models,assign,low probability,models assign low probability,0.6944787502288818
translation,309,25,model,low probability,to generating,incoherent and contradictory text,low probability to generating incoherent and contradictory text,0.6459606289863586
translation,309,25,model,model,train,models,model train models,0.6628757119178772
translation,309,78,model,transformer model,consists of,8 layer encoder,transformer model consists of 8 layer encoder,0.6673361659049988
translation,309,78,model,transformer model,consists of,8 layer decoder,transformer model consists of 8 layer decoder,0.6653228402137756
translation,309,78,model,8 layer decoder,with,512 - dimensional embeddings,8 layer decoder with 512 - dimensional embeddings,0.582149088382721
translation,309,78,model,8 layer decoder,with,16 attention heads,8 layer decoder with 16 attention heads,0.6132367849349976
translation,309,78,model,model,has,transformer model,model has transformer model,0.5662795305252075
translation,309,8,results,appropriate loss functions,which regularize,generated outputs,appropriate loss functions which regularize generated outputs,0.708154559135437
translation,309,8,results,generated outputs,to match,human distributions,generated outputs to match human distributions,0.6819141507148743
translation,309,8,results,human distributions,are,effective,human distributions are effective,0.6401588320732117
translation,309,8,results,results,show that,appropriate loss functions,results show that appropriate loss functions,0.5044374465942383
translation,309,90,results,training unlikelihood,using,only -contexts or only - labels,training unlikelihood using only -contexts or only - labels,0.6733444929122925
translation,309,90,results,training unlikelihood,reduces,corresponding metrics,training unlikelihood reduces corresponding metrics,0.6715044975280762
translation,309,90,results,corresponding metrics,compared to,mle baseline,corresponding metrics compared to mle baseline,0.6238783001899719
translation,309,90,results,dramatically,compared to,mle baseline,dramatically compared to mle baseline,0.7132750153541565
translation,309,90,results,corresponding metrics,has,dramatically,corresponding metrics has dramatically,0.5880059599876404
translation,309,90,results,results,see that,training unlikelihood,results see that training unlikelihood,0.6470173597335815
translation,309,91,results,training with both context - and label- repetition unlikelihood,reduced,context repetitions,training with both context - and label- repetition unlikelihood reduced context repetitions,0.6141924262046814
translation,309,91,results,training with both context - and label- repetition unlikelihood,reduced,label repetitions,training with both context - and label- repetition unlikelihood reduced label repetitions,0.6253727078437805
translation,309,91,results,context repetitions,by,"69 % , .0352 vs. .1131","context repetitions by 69 % , .0352 vs. .1131",0.5494824647903442
translation,309,91,results,context repetitions,by,"89 % , .0023 vs .0210","context repetitions by 89 % , .0023 vs .0210",0.5477812886238098
translation,309,91,results,context repetitions,compared to,mle baseline,context repetitions compared to mle baseline,0.6348462700843811
translation,309,91,results,label repetitions,by,"89 % , .0023 vs .0210","label repetitions by 89 % , .0023 vs .0210",0.5722208023071289
translation,309,91,results,label repetitions,compared to,mle baseline,label repetitions compared to mle baseline,0.6410304307937622
translation,309,91,results,results,has,training with both context - and label- repetition unlikelihood,results has training with both context - and label- repetition unlikelihood,0.49491211771965027
translation,309,93,results,eli5,show,especially large problem,eli5 show especially large problem,0.6673522591590881
translation,309,93,results,eli5,show,label - unlikelihood,eli5 show label - unlikelihood,0.639158308506012
translation,309,93,results,especially large problem,with,label repetition,especially large problem with label repetition,0.6340135335922241
translation,309,93,results,label - unlikelihood,able to,reduce,label - unlikelihood able to reduce,0.6235269904136658
translation,309,93,results,repetitions,by,91 % ( .055 vs .617 ),repetitions by 91 % ( .055 vs .617 ),0.5644931793212891
translation,309,93,results,reduce,has,repetitions,reduce has repetitions,0.5521181225776672
translation,309,93,results,significantly boosting,has,f1 ( .130 to .182 ),significantly boosting has f1 ( .130 to .182 ),0.4974101483821869
translation,309,93,results,results,for,eli5,results for eli5,0.6421816945075989
translation,309,139,results,mle baseline,obtains,perplexity,mle baseline obtains perplexity,0.5828666687011719
translation,309,139,results,perplexity,of,11.4,perplexity of 11.4,0.5706902742385864
translation,309,139,results,results,has,mle baseline,results has mle baseline,0.5346214175224304
translation,309,150,results,nli unlikelihood training,yields,large improvements,nli unlikelihood training yields large improvements,0.7039511203765869
translation,309,150,results,large improvements,on,all coherence metrics,large improvements on all coherence metrics,0.529589056968689
translation,309,150,results,minimally increasing,has,overall perplexity,minimally increasing has overall perplexity,0.5983218550682068
translation,309,150,results,results,has,nli unlikelihood training,results has nli unlikelihood training,0.5053635239601135
translation,310,121,experiments,seq2seq model,with,"attention ( bahdanau et al. , 2015 )","seq2seq model with attention ( bahdanau et al. , 2015 )",0.584800124168396
translation,310,121,experiments,"attention ( bahdanau et al. , 2015 )",on,opensubtitles dataset,"attention ( bahdanau et al. , 2015 ) on opensubtitles dataset",0.5138041377067566
translation,310,121,experiments,opensubtitles dataset,consists of,roughly 80 million,opensubtitles dataset consists of roughly 80 million,0.6262676119804382
translation,310,119,hyperparameters,first stage of training,build on,prior work,first stage of training build on prior work,0.7291377782821655
translation,310,119,hyperparameters,prior work,of predicting,generated target sequence,prior work of predicting generated target sequence,0.6956523656845093
translation,310,119,hyperparameters,generated target sequence,given,dialogue history,generated target sequence given dialogue history,0.7216611504554749
translation,310,119,hyperparameters,dialogue history,using,supervised seq2seq model,dialogue history using supervised seq2seq model,0.6373467445373535
translation,310,119,hyperparameters,hyperparameters,For,first stage of training,hyperparameters For first stage of training,0.5574323534965515
translation,310,6,model,deep reinforcement learning,to model,future reward,deep reinforcement learning to model future reward,0.6294919848442078
translation,310,6,model,future reward,in,chatbot dialogue,future reward in chatbot dialogue,0.5438055992126465
translation,310,7,model,dialogues,between,two virtual agents,dialogues between two virtual agents,0.6008543372154236
translation,310,7,model,dialogues,using,policy gradient methods,dialogues using policy gradient methods,0.6835877895355225
translation,310,7,model,policy gradient methods,to reward,sequences,policy gradient methods to reward sequences,0.7082691192626953
translation,310,7,model,sequences,display,three useful conversational properties,sequences display three useful conversational properties,0.7062566876411438
translation,310,7,model,model,simulates,dialogues,model simulates dialogues,0.7539584040641785
translation,310,51,model,neural reinforcement learning ( rl ) generation method,optimize,long-term rewards,neural reinforcement learning ( rl ) generation method optimize long-term rewards,0.7189199924468994
translation,310,51,model,model,introduce,neural reinforcement learning ( rl ) generation method,model introduce neural reinforcement learning ( rl ) generation method,0.6244248151779175
translation,310,52,model,encoderdecoder architecture,as,backbone,encoderdecoder architecture as backbone,0.5409859418869019
translation,310,52,model,conversation,between,two virtual agents,conversation between two virtual agents,0.6199608445167542
translation,310,52,model,conversation,to explore,space of possible actions,conversation to explore space of possible actions,0.7091714143753052
translation,310,52,model,space of possible actions,while,learning,space of possible actions while learning,0.578541100025177
translation,310,52,model,learning,to maximize,expected reward,learning to maximize expected reward,0.7132645845413208
translation,310,52,model,model,uses,encoderdecoder architecture,model uses encoderdecoder architecture,0.5704888701438904
translation,310,52,model,model,simulates,conversation,model simulates conversation,0.7644425630569458
translation,310,171,model,proposed rl model,first trained,mutual information objective,proposed rl model first trained mutual information objective,0.7008914351463318
translation,310,171,model,model,has,proposed rl model,model has proposed rl model,0.5638203620910645
translation,310,170,results,mutual information,leads to,more sustained conversations,mutual information leads to more sustained conversations,0.6384912133216858
translation,310,170,results,more sustained conversations,between,two agents,more sustained conversations between two agents,0.7009627819061279
translation,310,170,results,results,using,mutual information,results using mutual information,0.6530759334564209
translation,310,172,results,rl model,with,dialogue simulation,rl model with dialogue simulation,0.6267803907394409
translation,310,172,results,rl model,achieves,best evaluation score,rl model achieves best evaluation score,0.624324381351471
translation,310,172,results,results,observe,rl model,results observe rl model,0.6135700345039368
translation,310,180,results,proposed rl model,generates,more diverse outputs,proposed rl model generates more diverse outputs,0.6511366367340088
translation,310,180,results,more diverse outputs,when,com,more diverse outputs when com,0.7122005820274353
translation,310,180,results,results,find that,proposed rl model,results find that proposed rl model,0.6588054895401001
translation,310,199,results,proposed rl system,does not introduce,significant boost,proposed rl system does not introduce significant boost,0.712769627571106
translation,310,199,results,significant boost,in,single - turn response quality,significant boost in single - turn response quality,0.5405219197273254
translation,310,199,results,single - turn response quality,has,winning,single - turn response quality has winning,0.5677800178527832
translation,310,199,results,winning,has,40 percent of time,winning has 40 percent of time,0.6057426929473877
translation,310,199,results,losing,has,36 percent of time,losing has 36 percent of time,0.6142421960830688
translation,310,199,results,results,has,proposed rl system,results has proposed rl system,0.6081292033195496
translation,310,201,results,rl system,produces,responses,rl system produces responses,0.6482117772102356
translation,310,201,results,significantly easier,to,answer,significantly easier to answer,0.5832880139350891
translation,310,201,results,answer,than does,mutual information system,answer than does mutual information system,0.6656110882759094
translation,310,201,results,results,has,rl system,results has rl system,0.5916411876678467
translation,310,204,results,tendency,to,end,tendency to end,0.6514031887054443
translation,310,204,results,sentence,with,another question,sentence with another question,0.6734349727630615
translation,310,204,results,sentence,hand,conversation over,sentence hand conversation over,0.7308284640312195
translation,310,204,results,conversation over,to,user,conversation over to user,0.6148836612701416
translation,310,204,results,rl model,has,tendency,rl model has tendency,0.6060637831687927
translation,310,204,results,end,has,sentence,end has sentence,0.5388645529747009
translation,310,204,results,results,find that,rl model,results find that rl model,0.6632708311080933
translation,310,205,results,rl model,manages to produce,more interactive and sustained conversations,rl model manages to produce more interactive and sustained conversations,0.7827410101890564
translation,310,205,results,more interactive and sustained conversations,than,mutual information model,more interactive and sustained conversations than mutual information model,0.6014401316642761
translation,310,205,results,results,observe that,rl model,results observe that rl model,0.6226529479026794
translation,311,180,ablation-analysis,schema graph,improve,som - dst,schema graph improve som - dst,0.6408217549324036
translation,311,180,ablation-analysis,som - dst,achieves,52.23 % and 53.19 % joint accuracies,som - dst achieves 52.23 % and 53.19 % joint accuracies,0.639320969581604
translation,311,180,ablation-analysis,52.23 % and 53.19 % joint accuracies,on,multiwoz 2.0 and 2.1,52.23 % and 53.19 % joint accuracies on multiwoz 2.0 and 2.1,0.553730845451355
translation,311,125,hyperparameters,hidden size,of,"csfn , d model","hidden size of csfn , d model",0.580558717250824
translation,311,125,hyperparameters,hidden size,as,400,hidden size as 400,0.5915289521217346
translation,311,125,hyperparameters,"csfn , d model",as,400,"csfn , d model as 400",0.6414098739624023
translation,311,125,hyperparameters,400,with,4 heads,400 with 4 heads,0.6701463460922241
translation,311,125,hyperparameters,hyperparameters,set,hidden size,hyperparameters set hidden size,0.6568804979324341
translation,311,126,hyperparameters,token embeddings,with,400 dimensions,token embeddings with 400 dimensions,0.5929186344146729
translation,311,126,hyperparameters,token embeddings,initialized by,concatenating glove embeddings,token embeddings initialized by concatenating glove embeddings,0.6301240921020508
translation,311,126,hyperparameters,hyperparameters,has,token embeddings,hyperparameters has token embeddings,0.49778297543525696
translation,311,7,model,relations,among,different domain-slots,relations among different domain-slots,0.6041947603225708
translation,311,7,model,schema graph,involving,prior knowledge,schema graph involving prior knowledge,0.5639225840568542
translation,311,7,model,relations,has,schema graph,relations has schema graph,0.537152111530304
translation,311,7,model,different domain-slots,has,schema graph,different domain-slots has schema graph,0.5499370098114014
translation,311,7,model,model,To consider,relations,model To consider relations,0.7272676229476929
translation,311,8,model,novel context and schema fusion network,encode,dialogue context and schema graph,novel context and schema fusion network encode dialogue context and schema graph,0.7588935494422913
translation,311,8,model,dialogue context and schema graph,by using,internal and external attention mechanisms,dialogue context and schema graph by using internal and external attention mechanisms,0.6280598640441895
translation,311,26,model,multi-domain dialogue state tracker,with,context and schema fusion networks ( csfn - dst ),multi-domain dialogue state tracker with context and schema fusion networks ( csfn - dst ),0.6331274509429932
translation,311,26,model,model,propose,multi-domain dialogue state tracker,model propose multi-domain dialogue state tracker,0.6344255805015564
translation,311,27,model,fusion network,jointly encode,previous dialogue state,fusion network jointly encode previous dialogue state,0.7561193704605103
translation,311,27,model,fusion network,jointly encode,current turn dialogue,fusion network jointly encode current turn dialogue,0.7761933207511902
translation,311,27,model,fusion network,jointly encode,schema graph,fusion network jointly encode schema graph,0.7270721793174744
translation,311,27,model,schema graph,by,internal and external attention mechanisms,schema graph by internal and external attention mechanisms,0.5412095785140991
translation,311,27,model,model,has,fusion network,model has fusion network,0.5919990539550781
translation,311,28,model,multiple layers,of,attention networks,multiple layers of attention networks,0.5415534377098083
translation,311,28,model,final representation,of,each domain-slot node,final representation of each domain-slot node,0.5934127569198608
translation,311,28,model,each domain-slot node,to predict,corresponding value,each domain-slot node to predict corresponding value,0.6790516972541809
translation,311,28,model,corresponding value,involving,context and schema information,corresponding value involving context and schema information,0.637275755405426
translation,311,28,model,multiple layers,has,final representation,multiple layers has final representation,0.5946797132492065
translation,311,28,model,attention networks,has,final representation,attention networks has final representation,0.5548532009124756
translation,311,28,model,model,After,multiple layers,model After multiple layers,0.6814646124839783
translation,311,127,model,inference,use,greedy search strategy,inference use greedy search strategy,0.6774469017982483
translation,311,127,model,predicted dialogue state,of,last turn,predicted dialogue state of last turn,0.5961312651634216
translation,311,127,model,greedy search strategy,in,decoding process,greedy search strategy in decoding process,0.5250148773193359
translation,311,127,model,decoding process,of,value decoder,decoding process of value decoder,0.5820448398590088
translation,311,127,model,inference,has,predicted dialogue state,inference has predicted dialogue state,0.5781262516975403
translation,311,127,model,model,In,inference,model In inference,0.5416132807731628
translation,311,34,results,our approach,surpasses,strong baselines,our approach surpasses strong baselines,0.6284676790237427
translation,311,34,results,improved,by,our proposed schema graph,improved by our proposed schema graph,0.5566099286079407
translation,311,34,results,results,show,our approach,results show our approach,0.6604568362236023
translation,311,133,results,proposed csfn - dst,can outperform,other models,proposed csfn - dst can outperform other models,0.7722128629684448
translation,311,133,results,other models,except for,som - dst,other models except for som - dst,0.6492172479629517
translation,311,134,results,our schema graphs,with,som - dst,our schema graphs with som - dst,0.6147668957710266
translation,311,134,results,our schema graphs,achieve,state - of - the - art performances,our schema graphs achieve state - of - the - art performances,0.6083073616027832
translation,311,134,results,state - of - the - art performances,on,multiwoz 2.0 and 2.1,state - of - the - art performances on multiwoz 2.0 and 2.1,0.5604469776153564
translation,311,134,results,multiwoz 2.0 and 2.1,in,open-vocabulary setting,multiwoz 2.0 and 2.1 in open-vocabulary setting,0.5239256620407104
translation,311,134,results,results,combining,our schema graphs,results combining our schema graphs,0.6879504323005676
translation,311,135,results,our method,using,bert ( bert- base-uncased ),our method using bert ( bert- base-uncased ),0.7268775701522827
translation,311,135,results,bert ( bert- base-uncased ),can obtain,very competitive performance,bert ( bert- base-uncased ) can obtain very competitive performance,0.6026760339736938
translation,311,135,results,very competitive performance,with,best systems,very competitive performance with best systems,0.6746759414672852
translation,311,135,results,best systems,in,predefined ontology - based setting,best systems in predefined ontology - based setting,0.5307018160820007
translation,311,135,results,results,has,our method,results has our method,0.5589964985847473
translation,311,160,results,joint accuracy,can be,nearly 80 %,joint accuracy can be nearly 80 %,0.6443448662757874
translation,311,160,results,nearly 80 %,with,ground truth,nearly 80 % with ground truth,0.6683331727981567
translation,311,160,results,ground truth,of,previous dialogue state,ground truth of previous dialogue state,0.5372164845466614
translation,311,160,results,results,show,joint accuracy,results show joint accuracy,0.6351638436317444
translation,311,185,results,strong baselines,on,multiwoz 2.0 and 2.1 benchmarks,strong baselines on multiwoz 2.0 and 2.1 benchmarks,0.49311116337776184
translation,311,185,results,our approach,has,outperforms,our approach has outperforms,0.6385829448699951
translation,311,185,results,outperforms,has,strong baselines,outperforms has strong baselines,0.6115288734436035
translation,311,185,results,results,has,our approach,results has our approach,0.6050099730491638
translation,313,19,baselines,demo system,based on,alternative approach to task - oriented dialogue,demo system based on alternative approach to task - oriented dialogue,0.6656969785690308
translation,313,4,experiments,polyresponse,supports,task - oriented dialogue,polyresponse supports task - oriented dialogue,0.6346563696861267
translation,313,20,experiments,non-generative response retrieval,describe,polyresponse conversational search engine,non-generative response retrieval describe polyresponse conversational search engine,0.6521316766738892
translation,313,5,model,retrieval - based approach,bypasses,complex multi-component design,retrieval - based approach bypasses complex multi-component design,0.7114511728286743
translation,313,5,model,retrieval - based approach,use of,explicit semantics,retrieval - based approach use of explicit semantics,0.6246475577354431
translation,313,5,model,complex multi-component design,of,traditional task - oriented dialogue systems,complex multi-component design of traditional task - oriented dialogue systems,0.5557858347892761
translation,313,5,model,explicit semantics,in the form of,task -specific ontologies,explicit semantics in the form of task -specific ontologies,0.6849396824836731
translation,313,5,model,model,is,retrieval - based approach,model is retrieval - based approach,0.6055240035057068
translation,313,6,model,polyresponse engine,trained on,hundreds of millions of examples,polyresponse engine trained on hundreds of millions of examples,0.7031062841415405
translation,313,6,model,hundreds of millions of examples,extracted from,real conversations,hundreds of millions of examples extracted from real conversations,0.5517769455909729
translation,313,6,model,model,has,polyresponse engine,model has polyresponse engine,0.5818167328834534
translation,313,8,model,restaurant search and booking system,powered by,polyresponse engine,restaurant search and booking system powered by polyresponse engine,0.5638907551765442
translation,313,8,model,model,introduce,restaurant search and booking system,model introduce restaurant search and booking system,0.6128225326538086
translation,313,55,model,photos,represented using,convolutional neural net ( cnn ) models,photos represented using convolutional neural net ( cnn ) models,0.7099538445472717
translation,313,55,model,convolutional neural net ( cnn ) models,pretrained on,imagenet,convolutional neural net ( cnn ) models pretrained on imagenet,0.7507076859474182
translation,313,55,model,model,has,photos,model has photos,0.5873267650604248
translation,314,177,ablation-analysis,removal,of,slot consistency reward r sc,removal of slot consistency reward r sc,0.575589656829834
translation,314,177,ablation-analysis,removal,of,distant supervision reward r ds,removal of distant supervision reward r ds,0.560979425907135
translation,314,177,ablation-analysis,distant supervision reward r ds,from,advantage function,distant supervision reward r ds from advantage function,0.5170539021492004
translation,314,177,ablation-analysis,advantage function,has,dramatically degrades,advantage function has dramatically degrades,0.6031073927879333
translation,314,177,ablation-analysis,dramatically degrades,has,ser performance,dramatically degrades has ser performance,0.5695985555648804
translation,314,177,ablation-analysis,ablation analysis,show,removal,ablation analysis show removal,0.6289095878601074
translation,314,177,ablation-analysis,ablation analysis,show,distant supervision reward r ds,ablation analysis show distant supervision reward r ds,0.6143467426300049
translation,314,181,ablation-analysis,candidates,from,irn,candidates from irn,0.6431741118431091
translation,314,181,ablation-analysis,irn,is,important,irn is important,0.7111073136329651
translation,314,181,ablation-analysis,ablation analysis,shows,candidates,ablation analysis shows candidates,0.6692082285881042
translation,314,181,ablation-analysis,ablation analysis,incorporating,candidates,ablation analysis incorporating candidates,0.7480190992355347
translation,314,191,ablation-analysis,irn,helps to improve,performances,irn helps to improve performances,0.7221581935882568
translation,314,191,ablation-analysis,performances,of,tgen,performances of tgen,0.6821369528770447
translation,314,191,ablation-analysis,tgen,by,5.12 %,tgen by 5.12 %,0.5761094689369202
translation,314,191,ablation-analysis,tgen,by,3.23 %,tgen by 3.23 %,0.5721653699874878
translation,314,191,ablation-analysis,5.12 %,on,informativeness,5.12 % on informativeness,0.4882850646972656
translation,314,191,ablation-analysis,3.23 %,on,naturalness,3.23 % on naturalness,0.5168448090553284
translation,314,191,ablation-analysis,ablation analysis,has,irn,ablation analysis has irn,0.526465892791748
translation,314,26,baselines,irn,consists of,pointer rewriter,irn consists of pointer rewriter,0.5987954139709473
translation,314,26,baselines,irn,consists of,experience replay buffer,irn consists of experience replay buffer,0.6728397011756897
translation,314,26,baselines,baselines,has,irn,baselines has irn,0.5770946741104126
translation,314,8,model,iterative rectification network ( irn ),for improving,general nlg systems,iterative rectification network ( irn ) for improving general nlg systems,0.683103621006012
translation,314,8,model,iterative rectification network ( irn ),to produce,correct and fluent responses,iterative rectification network ( irn ) to produce correct and fluent responses,0.6882882714271545
translation,314,8,model,model,propose,iterative rectification network ( irn ),model propose iterative rectification network ( irn ),0.6577194929122925
translation,314,25,model,iterative rectification network ( irn ),to improve,slot consistency,iterative rectification network ( irn ) to improve slot consistency,0.6790273189544678
translation,314,25,model,slot consistency,for,general nlg systems,slot consistency for general nlg systems,0.6469458341598511
translation,314,25,model,model,propose,iterative rectification network ( irn ),model propose iterative rectification network ( irn ),0.6577194929122925
translation,314,31,results,extensive experiments,show,proposed model,extensive experiments show proposed model,0.6138390302658081
translation,314,31,results,extensive experiments,has,significantly outperforms,extensive experiments has significantly outperforms,0.6033377647399902
translation,314,31,results,proposed model,has,knn + irn,proposed model has knn + irn,0.6199155449867249
translation,314,31,results,proposed model,has,significantly outperforms,proposed model has significantly outperforms,0.6167242527008057
translation,314,31,results,knn + irn,has,significantly outperforms,knn + irn has significantly outperforms,0.6148996949195862
translation,314,31,results,significantly outperforms,has,all previous strong approaches,significantly outperforms has all previous strong approaches,0.597298800945282
translation,314,166,results,significantly outperforms,on,bleu score and err,significantly outperforms on bleu score and err,0.5962215662002563
translation,314,166,results,proposed model,has,significantly outperforms,proposed model has significantly outperforms,0.6167242527008057
translation,314,166,results,significantly outperforms,has,previous baselines,significantly outperforms has previous baselines,0.5914339423179626
translation,314,166,results,results,shows,proposed model,results shows proposed model,0.7134876251220703
translation,314,167,results,current state - of- the - art model,achieves,reductions,current state - of- the - art model achieves reductions,0.633877694606781
translation,314,167,results,ralstm,achieves,reductions,ralstm achieves reductions,0.7161539196968079
translation,314,167,results,reductions,of,"1.45 , 1.38 , 1.45 and 1.80 times","reductions of 1.45 , 1.38 , 1.45 and 1.80 times",0.6212050318717957
translation,314,167,results,"1.45 , 1.38 , 1.45 and 1.80 times",for,"sf restaurant , sf hotel , laptop , and television datasets","1.45 , 1.38 , 1.45 and 1.80 times for sf restaurant , sf hotel , laptop , and television datasets",0.5815375447273254
translation,314,167,results,current state - of- the - art model,has,ralstm,current state - of- the - art model has ralstm,0.5844473242759705
translation,314,167,results,results,Compared with,current state - of- the - art model,results Compared with current state - of- the - art model,0.6257089376449585
translation,314,168,results,"3.59 % , 1.45 % , 2.29 % and 3.33 %",of,bleu scores,"3.59 % , 1.45 % , 2.29 % and 3.33 % of bleu scores",0.544265866279602
translation,314,168,results,results,improves,"3.59 % , 1.45 % , 2.29 % and 3.33 %","results improves 3.59 % , 1.45 % , 2.29 % and 3.33 %",0.5890865921974182
translation,314,173,results,irn ( + knn ),contribute to,improvements,irn ( + knn ) contribute to improvements,0.6586495041847229
translation,314,173,results,improvements,of,slot consistency,improvements of slot consistency,0.5960191488265991
translation,314,173,results,slot consistency,for,general nlg systems,slot consistency for general nlg systems,0.6469458341598511
translation,314,173,results,our model,has,irn ( + knn ),our model has irn ( + knn ),0.6269682049751282
translation,314,178,results,language fluency related information,from,baseline bleu and reward r lm,language fluency related information from baseline bleu and reward r lm,0.510669469833374
translation,314,178,results,positive impact,on,bleu and ser,positive impact on bleu and ser,0.6166285872459412
translation,314,178,results,results,has,language fluency related information,results has language fluency related information,0.5046286582946777
translation,314,180,results,candidates,from,baselines,candidates from baselines,0.6247516870498657
translation,314,180,results,candidates,degrades,performance,candidates degrades performance,0.7653642296791077
translation,314,180,results,performance,to approximately that of,baseline sclstm,performance to approximately that of baseline sclstm,0.5799373984336853
translation,314,180,results,results,Using,candidates,results Using candidates,0.586313009262085
translation,314,182,results,worse performance,than,sclstm,worse performance than sclstm,0.5751160979270935
translation,314,182,results,model,has,without bootstrapping,model has without bootstrapping,0.5888981223106384
translation,314,182,results,model,has,worse performance,model has worse performance,0.5863714814186096
translation,314,182,results,without bootstrapping,has,worse performance,without bootstrapping has worse performance,0.5744174718856812
translation,314,182,results,results,has,model,results has model,0.5339115858078003
translation,314,189,results,outperforms,notably in,informativeness,outperforms notably in informativeness,0.5446569919586182
translation,314,189,results,ralstm,notably in,informativeness,ralstm notably in informativeness,0.5948410034179688
translation,314,189,results,informativeness,relatively by,4.97 %,informativeness relatively by 4.97 %,0.6210650205612183
translation,314,189,results,ralstm + irn,has,outperforms,ralstm + irn has outperforms,0.6250999569892883
translation,314,189,results,outperforms,has,ralstm,outperforms has ralstm,0.6028347611427307
translation,314,189,results,results,shows,ralstm + irn,results shows ralstm + irn,0.617603600025177
translation,314,190,results,improvement,from,4.01 to 4.07,improvement from 4.01 to 4.07,0.5542600154876709
translation,314,190,results,4.01 to 4.07,relative by,1.50 %,4.01 to 4.07 relative by 1.50 %,0.6852863430976868
translation,314,190,results,naturalness,has,improvement,naturalness has improvement,0.5647821426391602
translation,314,190,results,results,In terms of,naturalness,results In terms of naturalness,0.6894197463989258
translation,315,131,ablation-analysis,each self-supervision signal,presented,useful in its degree,each self-supervision signal presented useful in its degree,0.706814706325531
translation,315,131,ablation-analysis,each self-supervision signal,is,useful in its degree,each self-supervision signal is useful in its degree,0.6230513453483582
translation,315,131,ablation-analysis,useful in its degree,especially for,1 % and 5 % labeled data scenarios,useful in its degree especially for 1 % and 5 % labeled data scenarios,0.7084304094314575
translation,315,131,ablation-analysis,ablation analysis,find that,each self-supervision signal,ablation analysis find that each self-supervision signal,0.6480137705802917
translation,315,133,ablation-analysis,self-supervision,becomes,less dominant and less effective,self-supervision becomes less dominant and less effective,0.5978471636772156
translation,315,133,ablation-analysis,less dominant and less effective,as,number of labeled data,less dominant and less effective as number of labeled data,0.5539131760597229
translation,315,133,ablation-analysis,number of labeled data,has,increases,number of labeled data has increases,0.5989896655082703
translation,315,133,ablation-analysis,ablation analysis,found that,self-supervision,ablation analysis found that self-supervision,0.6110763549804688
translation,315,137,ablation-analysis,conversational behavior objective,seems to be,more effective,conversational behavior objective seems to be more effective,0.6651891469955444
translation,315,137,ablation-analysis,more effective,than,consistency objective,more effective than consistency objective,0.5408624410629272
translation,315,137,ablation-analysis,ablation analysis,modeling,conversational behavior objective,ablation analysis modeling conversational behavior objective,0.7439924478530884
translation,315,139,ablation-analysis,semisupervised learning,achieve,highest joint goal accuracy,semisupervised learning achieve highest joint goal accuracy,0.5879909992218018
translation,315,139,ablation-analysis,20.41 %,in,1 % setting,20.41 % in 1 % setting,0.6151400804519653
translation,315,139,ablation-analysis,33.67 %,in,5 % setting,33.67 % in 5 % setting,0.6173855662345886
translation,315,139,ablation-analysis,highest joint goal accuracy,has,20.41 %,highest joint goal accuracy has 20.41 %,0.5389274954795837
translation,315,139,ablation-analysis,ablation analysis,leverage,remaining dialogue data,ablation analysis leverage remaining dialogue data,0.7708614468574524
translation,315,149,ablation-analysis,performance,of,1 % labeled data setting,performance of 1 % labeled data setting,0.5553825497627258
translation,315,149,ablation-analysis,1 % labeled data setting,drops from,18.31 % to 11.07 %,1 % labeled data setting drops from 18.31 % to 11.07 %,0.7343392372131348
translation,315,149,ablation-analysis,context vector c ij,has,performance,context vector c ij has performance,0.5365750789642334
translation,315,149,ablation-analysis,ablation analysis,remove,context vector c ij,ablation analysis remove context vector c ij,0.6593599319458008
translation,315,29,baselines,baselines,has,modeling conversational behavior,baselines has modeling conversational behavior,0.5274754762649536
translation,315,179,experiments,self- supervision,are,less helpful,self- supervision are less helpful,0.5211411118507385
translation,315,179,experiments,less helpful,to,pairs,less helpful to pairs,0.5570983290672302
translation,315,179,experiments,less helpful,to,all the pairs,less helpful to all the pairs,0.566683828830719
translation,315,179,experiments,less helpful,such as,all the pairs,less helpful such as all the pairs,0.5899813771247864
translation,315,179,experiments,pairs,such as,"hotel , parking )","pairs such as hotel , parking )",0.6520804166793823
translation,315,179,experiments,pairs,such as,"hotel , internet )","pairs such as hotel , internet )",0.6786681413650513
translation,315,179,experiments,pairs,such as,"restaurant , name )","pairs such as restaurant , name )",0.5971362590789795
translation,315,179,experiments,pairs,such as,all the pairs,pairs such as all the pairs,0.6285824179649353
translation,315,179,experiments,pairs,such as,taxi domain,pairs such as taxi domain,0.6401838064193726
translation,315,179,experiments,pairs,in,taxi domain,pairs in taxi domain,0.5706383585929871
translation,315,179,experiments,all the pairs,in,taxi domain,all the pairs in taxi domain,0.5449927449226379
translation,315,118,hyperparameters,model,trained,end-to - end,model trained end-to - end,0.7512461543083191
translation,315,118,hyperparameters,model,trained,adam optimizer,model trained adam optimizer,0.7078166007995605
translation,315,118,hyperparameters,model,using,adam optimizer,model using adam optimizer,0.6418686509132385
translation,315,118,hyperparameters,end-to - end,using,adam optimizer,end-to - end using adam optimizer,0.6591702699661255
translation,315,118,hyperparameters,adam optimizer,),batch size,adam optimizer ) batch size,0.5308153033256531
translation,315,118,hyperparameters,adam optimizer,with,batch size,adam optimizer with batch size,0.606801450252533
translation,315,118,hyperparameters,batch size,of,8 or 32,batch size of 8 or 32,0.6459572315216064
translation,315,118,hyperparameters,hyperparameters,trained,end-to - end,hyperparameters trained end-to - end,0.7198280096054077
translation,315,118,hyperparameters,hyperparameters,has,model,hyperparameters has model,0.5282720923423767
translation,315,120,hyperparameters,learning rate annealing,used with,0.2 dropout ratio,learning rate annealing used with 0.2 dropout ratio,0.636102020740509
translation,315,120,hyperparameters,hyperparameters,has,learning rate annealing,hyperparameters has learning rate annealing,0.5017721652984619
translation,315,121,hyperparameters,word embeddings,have,400 dimensions,word embeddings have 400 dimensions,0.5391427278518677
translation,315,121,hyperparameters,word embeddings,by concatenating,300 glove embeddings,word embeddings by concatenating 300 glove embeddings,0.6248342394828796
translation,315,121,hyperparameters,word embeddings,by concatenating,100 character embeddings,word embeddings by concatenating 100 character embeddings,0.6399930715560913
translation,315,121,hyperparameters,400 dimensions,by concatenating,100 character embeddings,400 dimensions by concatenating 100 character embeddings,0.6677274703979492
translation,315,121,hyperparameters,hyperparameters,has,word embeddings,hyperparameters has word embeddings,0.4784160554409027
translation,315,122,hyperparameters,greedy decoding strategy,used for,state generator,greedy decoding strategy used for state generator,0.6401527523994446
translation,315,122,hyperparameters,hyperparameters,has,greedy decoding strategy,hyperparameters has greedy decoding strategy,0.5284808874130249
translation,315,123,hyperparameters,20 % - 50 %,of,input tokens,20 % - 50 % of input tokens,0.6051618456840515
translation,315,123,hyperparameters,20 % - 50 %,to strengthen,prediction consistency,20 % - 50 % to strengthen prediction consistency,0.7240575551986694
translation,315,123,hyperparameters,hyperparameters,mask out,20 % - 50 %,hyperparameters mask out 20 % - 50 %,0.7975243330001831
translation,315,8,model,dst model,to have,consistent latent distributions,dst model to have consistent latent distributions,0.6105634570121765
translation,315,8,model,consistent latent distributions,given,perturbed input,consistent latent distributions given perturbed input,0.7318320274353027
translation,315,8,model,model,encourage,dst model,model encourage dst model,0.6449589729309082
translation,315,24,model,dst problem,using,copy - augmented ontology - free models,dst problem using copy - augmented ontology - free models,0.6414341926574707
translation,315,24,model,copy - augmented ontology - free models,from,rarely discussed perspective,copy - augmented ontology - free models from rarely discussed perspective,0.6034092307090759
translation,315,24,model,model,approach,dst problem,model approach dst problem,0.6867960691452026
translation,315,26,model,model,has,preserving latent consistency,model has preserving latent consistency,0.5750194787979126
translation,315,10,results,our proposed self-supervised signals,improve,joint goal accuracy,our proposed self-supervised signals improve joint goal accuracy,0.6853190660476685
translation,315,10,results,joint goal accuracy,by,8.95 %,joint goal accuracy by 8.95 %,0.5313534140586853
translation,315,10,results,joint goal accuracy,when,only 1 % labeled data,joint goal accuracy when only 1 % labeled data,0.549382746219635
translation,315,10,results,8.95 %,when,only 1 % labeled data,8.95 % when only 1 % labeled data,0.620360255241394
translation,315,10,results,only 1 % labeled data,used on,multiwoz dataset,only 1 % labeled data used on multiwoz dataset,0.6732510328292847
translation,315,10,results,results,show,our proposed self-supervised signals,results show our proposed self-supervised signals,0.654131293296814
translation,315,39,results,1 % data setting,improve,joint goal accuracy,1 % data setting improve joint goal accuracy,0.677232563495636
translation,315,39,results,joint goal accuracy,by,4.5 %,joint goal accuracy by 4.5 %,0.5383831262588501
translation,315,39,results,joint goal accuracy,with,additional 4.43 % improvement,joint goal accuracy with additional 4.43 % improvement,0.5992051362991333
translation,315,39,results,joint goal accuracy,with,additional 4.43 % improvement,joint goal accuracy with additional 4.43 % improvement,0.5992051362991333
translation,315,39,results,4.5 %,with,proposed consistency objective,4.5 % with proposed consistency objective,0.6171627640724182
translation,315,39,results,additional 4.43 % improvement,add,behavior modeling objective,additional 4.43 % improvement add behavior modeling objective,0.6156028509140015
translation,315,39,results,results,of,1 % data setting,results of 1 % data setting,0.5268058776855469
translation,315,132,results,modeling conversational behavior,seems to be more effective,preserving prediction consistency,modeling conversational behavior seems to be more effective preserving prediction consistency,0.6181443929672241
translation,315,132,results,results,has,modeling conversational behavior,results has modeling conversational behavior,0.5033128261566162
translation,315,134,results,labeled data,with,self-supervision,labeled data with self-supervision,0.6347000598907471
translation,315,134,results,joint goal accuracy,compared to,original reported 48.62 %,joint goal accuracy compared to original reported 48.62 %,0.605711042881012
translation,315,134,results,100 %,has,labeled data,100 % has labeled data,0.5757007002830505
translation,315,134,results,48.72 %,has,joint goal accuracy,48.72 % has joint goal accuracy,0.5276877284049988
translation,315,134,results,results,try,100 %,results try 100 %,0.6580476760864258
translation,315,135,results,consistency,has,4.52 % ( or 4.03 % fuzzy ) improvement,consistency has 4.52 % ( or 4.03 % fuzzy ) improvement,0.5428207516670227
translation,315,135,results,4.52 % ( or 4.03 % fuzzy ) improvement,for,1 % scenario,4.52 % ( or 4.03 % fuzzy ) improvement for 1 % scenario,0.6187593936920166
translation,315,135,results,consistency,has,4.52 % ( or 4.03 % fuzzy ) improvement,consistency has 4.52 % ( or 4.03 % fuzzy ) improvement,0.5428207516670227
translation,315,135,results,results,preserving,consistency,results preserving consistency,0.6717517971992493
translation,315,136,results,labeled data,increases to,25 % ( 2105 dialogues ),labeled data increases to 25 % ( 2105 dialogues ),0.703003466129303
translation,315,136,results,no difference,with or without,consistency objective,no difference with or without consistency objective,0.6381286382675171
translation,315,138,results,small improvement,jointly train,end-to-end,small improvement jointly train end-to-end,0.6463664174079895
translation,315,138,results,results,has,small improvement,results has small improvement,0.596271276473999
translation,315,153,results,gate accuracy,of,1 % labeled data,gate accuracy of 1 % labeled data,0.5791805982589722
translation,315,153,results,1 % labeled data,improves by,around 3 %,1 % labeled data improves by around 3 %,0.7463204860687256
translation,315,153,results,around 3 %,with,self-supervision,around 3 % with self-supervision,0.6048787236213684
translation,315,153,results,results,has,gate accuracy,results has gate accuracy,0.5741962194442749
translation,315,181,results,all the slots,perform,relatively well,all the slots perform relatively well,0.5883233547210693
translation,315,181,results,relatively well,with,1 % labeled data,relatively well with 1 % labeled data,0.6450566649436951
translation,315,181,results,taxi domain,has,all the slots,taxi domain has all the slots,0.6315139532089233
translation,315,181,results,results,worth mentioning,taxi domain,results worth mentioning taxi domain,0.6106854677200317
translation,315,181,results,results,in,taxi domain,results in taxi domain,0.5412610769271851
translation,316,13,model,"data-driven , automated system",control,simulated robot,"data-driven , automated system control simulated robot",0.7250078320503235
translation,316,13,model,scoutbot,control,simulated robot,scoutbot control simulated robot,0.7330692410469055
translation,316,13,model,simulated robot,with,"verbally issued , natural language instructions","simulated robot with verbally issued , natural language instructions",0.6017196178436279
translation,316,13,model,"verbally issued , natural language instructions",within,simulated environment,"verbally issued , natural language instructions within simulated environment",0.5996304154396057
translation,316,59,model,vhmsg and ros modules,to interact with,each other,vhmsg and ros modules to interact with each other,0.7174478769302368
translation,316,59,model,each other,created,ros2vhmsg,each other created ros2vhmsg,0.6396421790122986
translation,316,59,model,model,To allow,vhmsg and ros modules,model To allow vhmsg and ros modules,0.6047426462173462
translation,316,59,model,model,created,ros2vhmsg,model created ros2vhmsg,0.6285707950592041
translation,317,159,ablation-analysis,kdbts,brings about,significant improvement,kdbts brings about significant improvement,0.7343153357505798
translation,317,159,ablation-analysis,significant improvement,on,generation quality,significant improvement on generation quality,0.5457420349121094
translation,317,159,ablation-analysis,significant improvement,by removing,exposure bias,significant improvement by removing exposure bias,0.5947256684303284
translation,317,159,ablation-analysis,exposure bias,of,knowledge selection,exposure bias of knowledge selection,0.5168136358261108
translation,317,159,ablation-analysis,ablation analysis,see that,kdbts,ablation analysis see that kdbts,0.6442850232124329
translation,317,178,ablation-analysis,sequence information,in,y seq,sequence information in y seq,0.5271328091621399
translation,317,178,ablation-analysis,y seq,contributes to,knowledge selection,y seq contributes to knowledge selection,0.7787902355194092
translation,317,178,ablation-analysis,ablation analysis,see that,sequence information,ablation analysis see that sequence information,0.6157628297805786
translation,317,186,ablation-analysis,skt ?,achieves,lower accuracy ( 32.8 vs 52.0 ),skt ? achieves lower accuracy ( 32.8 vs 52.0 ),0.6090056300163269
translation,317,186,ablation-analysis,skt ?,achieves,lower kl divergence,skt ? achieves lower kl divergence,0.6390566825866699
translation,317,186,ablation-analysis,l kl,has,skt ?,l kl has skt ?,0.6815891265869141
translation,317,186,ablation-analysis,lower kl divergence,has,0.31 vs 1.41 ),lower kl divergence has 0.31 vs 1.41 ),0.5243867039680481
translation,317,186,ablation-analysis,ablation analysis,using,l kl,ablation analysis using l kl,0.6750725507736206
translation,317,189,ablation-analysis,our kdbts,does not suffer from,exposure bias,our kdbts does not suffer from exposure bias,0.7297235727310181
translation,317,189,ablation-analysis,exposure bias,of,knowledge selection,exposure bias of knowledge selection,0.5168136358261108
translation,317,189,ablation-analysis,generation quality,improved,significantly,generation quality improved significantly,0.78721022605896
translation,317,189,ablation-analysis,ablation analysis,has,our kdbts,ablation analysis has our kdbts,0.6143526434898376
translation,317,132,baselines,"skt ( kim et al. , 2020 )",is,current state- ofthe - art latent variable model,"skt ( kim et al. , 2020 ) is current state- ofthe - art latent variable model",0.5247292518615723
translation,317,132,baselines,baselines,has,"skt ( kim et al. , 2020 )","baselines has skt ( kim et al. , 2020 )",0.554519534111023
translation,317,133,baselines,skt,leverages,posterior distribution,skt leverages posterior distribution,0.7025254368782043
translation,317,133,baselines,posterior distribution,by,sequential latent modeling,posterior distribution by sequential latent modeling,0.580558717250824
translation,317,133,baselines,tmn bert + postks cp,has,skt,tmn bert + postks cp has skt,0.6383894085884094
translation,317,137,hyperparameters,ffn,with,512 hidden dims,ffn with 512 hidden dims,0.6510623693466187
translation,317,137,hyperparameters,512 hidden dims,to generate,posterior information,512 hidden dims to generate posterior information,0.6870714426040649
translation,317,137,hyperparameters,posterior information,in,bow formats,posterior information in bow formats,0.5079908967018127
translation,317,137,hyperparameters,hyperparameters,use,ffn,hyperparameters use ffn,0.6216461062431335
translation,317,138,hyperparameters,hidden size d,is,768,hidden size d is 768,0.6087965369224548
translation,317,138,hyperparameters,vocabulary size | v |,is,30,vocabulary size | v | is 30,0.5973257422447205
translation,317,138,hyperparameters,vocabulary size | v |,is,522,vocabulary size | v | is 522,0.5913764834403992
translation,317,138,hyperparameters,30,",",522,"30 , 522",0.6935141682624817
translation,317,138,hyperparameters,hyperparameters,has,hidden size d,hyperparameters has hidden size d,0.5154374837875366
translation,317,138,hyperparameters,hyperparameters,has,vocabulary size | v |,hyperparameters has vocabulary size | v |,0.4708179533481598
translation,317,139,hyperparameters,batch,consists of,dialogues,batch consists of dialogues,0.6567279696464539
translation,317,139,hyperparameters,dialogues,rather than,individual turns,dialogues rather than individual turns,0.6907663345336914
translation,317,139,hyperparameters,batch size,is,1,batch size is 1,0.6298584342002869
translation,317,139,hyperparameters,hyperparameters,has,batch,hyperparameters has batch,0.5509300827980042
translation,317,139,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,317,141,hyperparameters,fix   operation,implemented by,gradient stoping,fix   operation implemented by gradient stoping,0.7061074376106262
translation,317,141,hyperparameters,hyperparameters,has,fix   operation,hyperparameters has fix   operation,0.5039042234420776
translation,317,142,hyperparameters,models,trained,end-to - end,models trained end-to - end,0.7560179233551025
translation,317,142,hyperparameters,models,using,adam optimizer,models using adam optimizer,0.6263576149940491
translation,317,142,hyperparameters,end-to - end,using,adam optimizer,end-to - end using adam optimizer,0.6591702699661255
translation,317,142,hyperparameters,end-to - end,),gradient clipping,end-to - end ) gradient clipping,0.5782397985458374
translation,317,142,hyperparameters,end-to - end,with,learning rate,end-to - end with learning rate,0.641472339630127
translation,317,142,hyperparameters,adam optimizer,),gradient clipping,adam optimizer ) gradient clipping,0.5596022605895996
translation,317,142,hyperparameters,adam optimizer,with,gradient clipping,adam optimizer with gradient clipping,0.6027368903160095
translation,317,142,hyperparameters,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,317,142,hyperparameters,gradient clipping,at,0.4,gradient clipping at 0.4,0.5317643284797668
translation,317,142,hyperparameters,learning rate,is,0.00002,learning rate is 0.00002,0.5624144077301025
translation,317,142,hyperparameters,hyperparameters,trained,end-to - end,hyperparameters trained end-to - end,0.7198280096054077
translation,317,142,hyperparameters,hyperparameters,has,models,hyperparameters has models,0.5447477102279663
translation,317,143,hyperparameters,label smoothing,set,0.1,label smoothing set 0.1,0.6200072765350342
translation,317,143,hyperparameters,label smoothing,set,0.05,label smoothing set 0.05,0.6303500533103943
translation,317,143,hyperparameters,0.1,for,knowledge selection,0.1 for knowledge selection,0.5932929515838623
translation,317,143,hyperparameters,0.05,for,response generation,0.05 for response generation,0.615609347820282
translation,317,143,hyperparameters,hyperparameters,apply,label smoothing,hyperparameters apply label smoothing,0.5724738836288452
translation,317,145,hyperparameters,our teacher,with,5 epochs,our teacher with 5 epochs,0.6889669299125671
translation,317,145,hyperparameters,our teacher,select,teacher,our teacher select teacher,0.6922171711921692
translation,317,145,hyperparameters,teacher,to teach,student,teacher to teach student,0.6981246471405029
translation,317,145,hyperparameters,student,according to,prior knowledge selection accuracy,student according to prior knowledge selection accuracy,0.605888307094574
translation,317,145,hyperparameters,prior knowledge selection accuracy,rather than,posterior selection accuracy,prior knowledge selection accuracy rather than posterior selection accuracy,0.6155533194541931
translation,317,145,hyperparameters,hyperparameters,train,our teacher,hyperparameters train our teacher,0.695534884929657
translation,317,9,model,prior selection module,with,necessary posterior information,prior selection module with necessary posterior information,0.6098254919052124
translation,317,9,model,necessary posterior information,obtained from,specially designed posterior information prediction module ( pipm ),necessary posterior information obtained from specially designed posterior information prediction module ( pipm ),0.6229814291000366
translation,317,9,model,knowledge distillation based training strategy ( kdbts ),to train,decoder,knowledge distillation based training strategy ( kdbts ) to train decoder,0.7395991086959839
translation,317,9,model,decoder,with,knowledge,decoder with knowledge,0.6797893643379211
translation,317,9,model,knowledge,selected from,prior distribution,knowledge selected from prior distribution,0.5951921343803406
translation,317,9,model,model,enhance,prior selection module,model enhance prior selection module,0.7051907777786255
translation,317,9,model,model,propose,knowledge distillation based training strategy ( kdbts ),model propose knowledge distillation based training strategy ( kdbts ),0.6706728935241699
translation,317,46,model,prior and posterior knowledge selection,for,knowledge - grounded dialogue generation,prior and posterior knowledge selection for knowledge - grounded dialogue generation,0.5993012189865112
translation,317,47,model,prior selection module,with,necessary posterior information,prior selection module with necessary posterior information,0.6098254919052124
translation,317,47,model,necessary posterior information,obtained by,specially designed posterior information prediction module ( pipm ),necessary posterior information obtained by specially designed posterior information prediction module ( pipm ),0.631366491317749
translation,317,47,model,model,enhance,prior selection module,model enhance prior selection module,0.7051907777786255
translation,317,48,model,knowledge distillation based training strategy ( kdbts ),to train,decoder,knowledge distillation based training strategy ( kdbts ) to train decoder,0.7395991086959839
translation,317,48,model,decoder,with,knowledge,decoder with knowledge,0.6797893643379211
translation,317,48,model,knowledge,selected by,prior module,knowledge selected by prior module,0.6458904147148132
translation,317,48,model,knowledge,removing,exposure bias,knowledge removing exposure bias,0.7021018862724304
translation,317,48,model,model,design,knowledge distillation based training strategy ( kdbts ),model design knowledge distillation based training strategy ( kdbts ),0.5872677564620972
translation,317,136,model,5 - layer transformer decoder,with,copy mechanism,5 - layer transformer decoder with copy mechanism,0.6182317733764648
translation,317,136,model,copy mechanism,to generate,response,copy mechanism to generate response,0.7098037600517273
translation,317,54,results,both pipm and kdbts,bring,performance improvement,both pipm and kdbts bring performance improvement,0.6499417424201965
translation,317,54,results,combination,achieves,state - of - the - art performance,combination achieves state - of - the - art performance,0.6703071594238281
translation,317,54,results,results,show,both pipm and kdbts,results show both pipm and kdbts,0.5810531377792358
translation,317,158,results,pipm,provides,some necessary posterior information,pipm provides some necessary posterior information,0.626756489276886
translation,317,158,results,some necessary posterior information,helpful for,knowledge selection,some necessary posterior information helpful for knowledge selection,0.7020072340965271
translation,317,158,results,results,on,wizard of wikipedia dataset,results on wizard of wikipedia dataset,0.5551702976226807
translation,317,158,results,results,see that,pipm,results see that pipm,0.6939408183097839
translation,317,161,results,combination of pipm and kdbts,achieves,further improvement,combination of pipm and kdbts achieves further improvement,0.6349483132362366
translation,317,161,results,further improvement,on,most metrics,further improvement on most metrics,0.4992936849594116
translation,317,161,results,further improvement,except,knowledge selection accuracy,further improvement except knowledge selection accuracy,0.6977847814559937
translation,317,161,results,results,has,combination of pipm and kdbts,results has combination of pipm and kdbts,0.5460022687911987
translation,317,166,results,our approach,brings about,consistent improvement,our approach brings about consistent improvement,0.7129199504852295
translation,317,166,results,consistent improvement,over,state - of - the - art model skt,consistent improvement over state - of - the - art model skt,0.6573716402053833
translation,317,166,results,results,see that,our approach,results see that our approach,0.6574503183364868
translation,317,187,results,skt ( klfix ),performs,worse in generation,skt ( klfix ) performs worse in generation,0.624235987663269
translation,317,187,results,worse in generation,than,skt,worse in generation than skt,0.654930830001831
translation,318,60,baselines,bidirectional encoder representations from transformers ( bert ),employs,bi-lstm model,bidirectional encoder representations from transformers ( bert ) employs bi-lstm model,0.5656906366348267
translation,318,64,baselines,"bert ( devlin et al. , 2018 )",exploits,"multilayer bidirectional transformers model ( vaswani et al. , 2017 )","bert ( devlin et al. , 2018 ) exploits multilayer bidirectional transformers model ( vaswani et al. , 2017 )",0.7012556195259094
translation,318,64,baselines,"multilayer bidirectional transformers model ( vaswani et al. , 2017 )",to learn,pre-trained universal representations,"multilayer bidirectional transformers model ( vaswani et al. , 2017 ) to learn pre-trained universal representations",0.5983653664588928
translation,318,64,baselines,pre-trained universal representations,of,text,pre-trained universal representations of text,0.5763989686965942
translation,318,64,baselines,only a plain text corpus,from,wikipedia,only a plain text corpus from wikipedia,0.5847097039222717
translation,318,64,baselines,baselines,has,"bert ( devlin et al. , 2018 )","baselines has bert ( devlin et al. , 2018 )",0.5257770419120789
translation,318,86,baselines,conversational datasets,using,optimization,conversational datasets using optimization,0.6651168465614319
translation,318,86,baselines,seq2seq with attention mechanism,predicts,next response,seq2seq with attention mechanism predicts next response,0.7627376914024353
translation,318,86,baselines,next response,given,previous utterance,next response given previous utterance,0.7298969626426697
translation,318,86,baselines,previous utterance,using,encoder-decoder model,previous utterance using encoder-decoder model,0.6336323022842407
translation,318,86,baselines,convergence,has,seq2seq with attention mechanism,convergence has seq2seq with attention mechanism,0.5739918947219849
translation,318,87,baselines,hred,extends,seq2seq model,hred extends seq2seq model,0.7109972834587097
translation,318,87,baselines,seq2seq model,by adding,context - rnn layer,seq2seq model by adding context - rnn layer,0.688800036907196
translation,318,87,baselines,context - rnn layer,accounts for,contextual information,context - rnn layer accounts for contextual information,0.6728421449661255
translation,318,87,baselines,baselines,has,hred,baselines has hred,0.6393665075302124
translation,318,88,baselines,ta - seq2seq,extends,seq2seq model,ta - seq2seq extends seq2seq model,0.7347188591957092
translation,318,88,baselines,seq2seq model,by biasing,overall distribution,seq2seq model by biasing overall distribution,0.6968318223953247
translation,318,88,baselines,overall distribution,towards,leveraging,overall distribution towards leveraging,0.7512789368629456
translation,318,88,baselines,topic words,in,response,topic words in response,0.48886942863464355
translation,318,88,baselines,leveraging,has,topic words,leveraging has topic words,0.5378552675247192
translation,318,88,baselines,baselines,has,ta - seq2seq,baselines has ta - seq2seq,0.5592435002326965
translation,318,89,baselines,thred,builds upon,ta - seq2seq model,thred builds upon ta - seq2seq model,0.65718674659729
translation,318,89,baselines,ta - seq2seq model,by levering,topic words,ta - seq2seq model by levering topic words,0.66085284948349
translation,318,89,baselines,topic words,in,response,topic words in response,0.48886942863464355
translation,318,89,baselines,response,in,multi-turn dialogue system,response in multi-turn dialogue system,0.549723207950592
translation,318,89,baselines,baselines,has,thred,baselines has thred,0.6628758907318115
translation,318,96,baselines,two state - of- the - art nli models,i.e.,esim,two state - of- the - art nli models i.e. esim,0.7246525883674622
translation,318,96,baselines,two state - of- the - art nli models,i.e.,"bert ( devlin et al. , 2018 )","two state - of- the - art nli models i.e. bert ( devlin et al. , 2018 )",0.7061825394630432
translation,318,8,model,interpretable metrics,for evaluating,topic coherence,interpretable metrics for evaluating topic coherence,0.6909809708595276
translation,318,8,model,topic coherence,making use of,distributed sentence representations,topic coherence making use of distributed sentence representations,0.6256127953529358
translation,318,8,model,model,present,interpretable metrics,model present interpretable metrics,0.672244668006897
translation,318,9,model,calculable approximations,of,human judgment,calculable approximations of human judgment,0.5595412254333496
translation,318,9,model,human judgment,based on,conversational coherence,human judgment based on conversational coherence,0.5603291392326355
translation,318,9,model,conversational coherence,by adopting,state - of - the - art entailment techniques,conversational coherence by adopting state - of - the - art entailment techniques,0.671952486038208
translation,318,9,model,model,introduce,calculable approximations,model introduce calculable approximations,0.6343930959701538
translation,318,63,model,esim,incorporating,contextualized word embeddings,esim incorporating contextualized word embeddings,0.5934929847717285
translation,318,63,model,contextualized word embeddings,namely,"elmo ( peters et al. , 2018 )","contextualized word embeddings namely elmo ( peters et al. , 2018 )",0.5864464044570923
translation,318,63,model,contextualized word embeddings,into,inference model,contextualized word embeddings into inference model,0.5227308869361877
translation,318,63,model,model,boost,esim,model boost esim,0.7495540380477905
translation,318,65,results,results,has,bert,results has bert,0.43097156286239624
translation,318,100,results,both models,reach,reasonable performance,both models reach reasonable performance,0.7565392851829529
translation,318,100,results,bert,has,outperforms,bert has outperforms,0.7181920409202576
translation,318,100,results,outperforms,has,esim,outperforms has esim,0.6667474508285522
translation,318,100,results,results,has,both models,results has both models,0.5060139894485474
translation,318,114,results,three metrics,correlate weakly with,human judgment,three metrics correlate weakly with human judgment,0.6965613961219788
translation,318,114,results,e,correlate weakly with,human judgment,e correlate weakly with human judgment,0.7355948090553284
translation,318,114,results,human judgment,in,both datasets,human judgment in both datasets,0.4562755525112152
translation,318,114,results,three metrics,has,e,three metrics has e,0.5975725054740906
translation,318,114,results,results,notice,three metrics,results notice three metrics,0.7028365135192871
translation,318,123,results,ss metrics,correlate better than,word- level metrics,ss metrics correlate better than word- level metrics,0.6820393800735474
translation,318,123,results,word interactions,to represent,utterances,word interactions to represent utterances,0.7001979351043701
translation,318,123,results,results,demonstrate,ss metrics,results demonstrate ss metrics,0.5911966562271118
translation,318,126,results,ss metric,reaches,pearson correlation,ss metric reaches pearson correlation,0.7236928939819336
translation,318,126,results,pearson correlation,of,- 0.404,pearson correlation of - 0.404,0.5362710356712341
translation,318,126,results,- 0.404,with respect to,most recent utterance,- 0.404 with respect to most recent utterance,0.6287286877632141
translation,318,126,results,most recent utterance,on,open-subtitles,most recent utterance on open-subtitles,0.5759252309799194
translation,318,126,results,results,has,ss metric,results has ss metric,0.5988979339599609
Blogs,0,260,baselines,dstc1,features,conversations,dstc1 features conversations,0.6772362589836121
Blogs,0,260,baselines,conversations,with,automated bus information interface,conversations with automated bus information interface,0.6320997476577759
Blogs,0,260,baselines,automated bus information interface,where,users,automated bus information interface where users,0.5789831280708313
Blogs,0,260,baselines,users,request,bus routes,users request bus routes,0.7243291735649109
Blogs,0,260,baselines,bus routes,from,system,bus routes from system,0.6080560088157654
Blogs,0,260,baselines,bus routes,from,system,bus routes from system,0.6080560088157654
Blogs,0,260,baselines,baselines,has,dstc1,baselines has dstc1,0.5917455554008484
Blogs,0,261,experiments,dstc2,introduces,changing user goals,dstc2 introduces changing user goals,0.6570504307746887
Blogs,0,261,experiments,dstc2,provide,desired reservation,dstc2 provide desired reservation,0.6651750802993774
Blogs,0,261,experiments,changing user goals,in,restaurant booking system,changing user goals in restaurant booking system,0.5252567529678345
Blogs,0,261,experiments,small amount of labelled data,in,domain of tourist information,small amount of labelled data in domain of tourist information,0.4782451093196869
Blogs,1,33,experiments,corpus,of,150k human-human dialogues,corpus of 150k human-human dialogues,0.5648444294929504
Blogs,1,33,experiments,150k human-human dialogues,collected via,recently introduced guesswhat ?!,150k human-human dialogues collected via recently introduced guesswhat ?!,0.6841539740562439
Blogs,1,179,hyperparameters,our environment,with,pre-trained models,our environment with pre-trained models,0.6004354357719421
Blogs,1,179,hyperparameters,qgen,with,reinforce,qgen with reinforce,0.742692232131958
Blogs,1,179,hyperparameters,qgen,with,plain stochastic gradient descent ( sgd ),qgen with plain stochastic gradient descent ( sgd ),0.6465671062469482
Blogs,1,179,hyperparameters,reinforce,for,80 epochs,reinforce for 80 epochs,0.6367480158805847
Blogs,1,179,hyperparameters,plain stochastic gradient descent ( sgd ),with,learning rate,plain stochastic gradient descent ( sgd ) with learning rate,0.6281343698501587
Blogs,1,179,hyperparameters,plain stochastic gradient descent ( sgd ),with,batch size,plain stochastic gradient descent ( sgd ) with batch size,0.6292403340339661
Blogs,1,179,hyperparameters,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
Blogs,1,179,hyperparameters,batch size,of,64,batch size of 64,0.6741159558296204
Blogs,1,179,hyperparameters,hyperparameters,initialize,our environment,hyperparameters initialize our environment,0.7579895257949829
Blogs,1,179,hyperparameters,hyperparameters,train,qgen,hyperparameters train qgen,0.6883615255355835
Blogs,1,180,hyperparameters,each epoch,sample,each training images once,each epoch sample each training images once,0.6457351446151733
Blogs,1,180,hyperparameters,object,as,target,object as target,0.5568116903305054
Blogs,1,180,hyperparameters,hyperparameters,For,each epoch,hyperparameters For each epoch,0.5405371189117432
Blogs,1,181,hyperparameters,baseline parameters,with,sgd,baseline parameters with sgd,0.5671696066856384
Blogs,1,181,hyperparameters,sgd,with,learning rate,sgd with learning rate,0.6203937530517578
Blogs,1,181,hyperparameters,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
Blogs,1,181,hyperparameters,hyperparameters,optimize,baseline parameters,hyperparameters optimize baseline parameters,0.6702424883842468
Blogs,1,7,model,deep reinforcement learning method,to optimize,visually grounded task - oriented dialogues,deep reinforcement learning method to optimize visually grounded task - oriented dialogues,0.6550509929656982
Blogs,1,7,model,deep reinforcement learning method,based on,policy gradient algorithm,deep reinforcement learning method based on policy gradient algorithm,0.6171609163284302
Blogs,1,7,model,model,introduce,deep reinforcement learning method,model introduce deep reinforcement learning method,0.6030301451683044
Blogs,1,32,model,end-to - end rl optimization,of,task - oriented question generator,end-to - end rl optimization of task - oriented question generator,0.566978394985199
Blogs,1,187,results,test set,training with,ce,test set training with ce,0.76902836561203
Blogs,1,187,results,test set,training with,reinforce,test set training with reinforce,0.7673925757408142
Blogs,1,187,results,ce,obtains,38.0 % accuracy,ce obtains 38.0 % accuracy,0.6096565127372742
Blogs,1,187,results,improves,to,52.3 %,improves to 52.3 %,0.613669216632843
Blogs,1,187,results,reinforce,has,improves,reinforce has improves,0.6717612147331238
Blogs,1,187,results,results,On,test set,results On test set,0.582119882106781
Blogs,1,188,results,significant improvement,over,beam-search ce,significant improvement over beam-search ce,0.7018950581550598
Blogs,1,188,results,beam-search ce,achieves,44.8 %,beam-search ce achieves 44.8 %,0.6543612480163574
Blogs,1,188,results,44.8 %,on,test-set,44.8 % on test-set,0.529982328414917
Blogs,1,189,results,proposed framework,closes,gap,proposed framework closes gap,0.6984941959381104
Blogs,1,189,results,gap,towards,human-performance ( 84.4 % ),gap towards human-performance ( 84.4 % ),0.5930178165435791
Blogs,1,189,results,human-performance ( 84.4 % ),with,more than 10 %,human-performance ( 84.4 % ) with more than 10 %,0.6312949061393738
Blogs,1,189,results,results,has,proposed framework,results has proposed framework,0.590894877910614
Blogs,1,190,results,beam-search procedure,improves over,sampling,beam-search procedure improves over sampling,0.7004209160804749
Blogs,1,190,results,sampling,from,ce,sampling from ce,0.6469513177871704
Blogs,1,190,results,score,for,reinforce,score for reinforce,0.6695670485496521
Blogs,1,190,results,lowers,has,score,lowers has score,0.6546049118041992
Blogs,1,190,results,results,has,beam-search procedure,results has beam-search procedure,0.5203544497489929
Blogs,2,172,ablation-analysis,multi-turn ( self ) repetition,incorporated alongside,other attribute control methods,multi-turn ( self ) repetition incorporated alongside other attribute control methods,0.7716754078865051
Blogs,2,172,ablation-analysis,ablation analysis,controlling for,multi-turn ( self ) repetition,ablation analysis controlling for multi-turn ( self ) repetition,0.7063102722167969
Blogs,2,6,baselines,two controllable neural text generation methods,to control,four important attributes,two controllable neural text generation methods to control four important attributes,0.7270514369010925
Blogs,2,6,baselines,four important attributes,for,chitchat dialogue,four important attributes for chitchat dialogue,0.6506489515304565
Blogs,2,6,baselines,two controllable neural text generation methods,has,conditional training,two controllable neural text generation methods has conditional training,0.5453901886940002
Blogs,2,6,baselines,question -,has,asking,question - has asking,0.5852800011634827
Blogs,2,6,baselines,baselines,examine,two controllable neural text generation methods,baselines examine two controllable neural text generation methods,0.5968101620674133
Blogs,2,35,baselines,baselines,study,two control methods,baselines study two control methods,0.6448937654495239
Blogs,2,45,experimental-setup,beam search,with,beam size 20,beam search with beam size 20,0.6913294196128845
Blogs,2,46,experimental-setup,word embedding matrix,with,300 - dimensional glove embeddings,word embedding matrix with 300 - dimensional glove embeddings,0.6007586121559143
Blogs,2,46,experimental-setup,experimental setup,initialized,word embedding matrix,experimental setup initialized word embedding matrix,0.7185723185539246
Blogs,2,47,experimental-setup,parlai framework,pretrained,model,parlai framework pretrained model,0.636178731918335
Blogs,2,47,experimental-setup,model,on,dataset,model on dataset,0.5746496319770813
Blogs,2,47,experimental-setup,dataset,of,2.5 million twitter message - response pairs,dataset of 2.5 million twitter message - response pairs,0.5009551048278809
Blogs,2,47,experimental-setup,experimental setup,Using,parlai framework,experimental setup Using parlai framework,0.6506912708282471
Blogs,2,16,model,conditional training,in which,neural model,conditional training in which neural model,0.624004065990448
Blogs,2,16,model,neural model,conditioned on,additional control features,neural model conditioned on additional control features,0.7338448166847229
Blogs,2,16,model,weighted decoding,in which,control features,weighted decoding in which control features,0.6467408537864685
Blogs,2,16,model,weighted decoding,added to,decoding scoring function,weighted decoding added to decoding scoring function,0.6282394528388977
Blogs,2,16,model,control features,added to,decoding scoring function,control features added to decoding scoring function,0.6605799198150635
Blogs,2,16,model,decoding scoring function,at,test time,decoding scoring function at test time,0.545366108417511
Blogs,2,18,results,personachat task,obtain,significantly higher engagingness scores,personachat task obtain significantly higher engagingness scores,0.5565394163131714
Blogs,2,18,results,significantly higher engagingness scores,than,baseline,significantly higher engagingness scores than baseline,0.5674593448638916
Blogs,2,18,results,significantly higher engagingness scores,by optimizing,control,significantly higher engagingness scores by optimizing control,0.7342508435249329
Blogs,2,18,results,control,of,repetition,control of repetition,0.6436887383460999
Blogs,2,18,results,control,of,specificity,control of specificity,0.6311853528022766
Blogs,2,18,results,control,of,question - asking,control of question - asking,0.5954136252403259
Blogs,2,18,results,question - asking,over,multiple turns,question - asking over multiple turns,0.6817680597305298
Blogs,2,48,results,perplexity,of,26.83,perplexity of 26.83,0.5190354585647583
Blogs,2,48,results,f1,of,17.02,f1 of 17.02,0.5677592158317566
Blogs,2,48,results,per-sona chat validation set,has,baseline model,per-sona chat validation set has baseline model,0.573833703994751
Blogs,2,48,results,baseline model,has,perplexity,baseline model has perplexity,0.5270923376083374
Blogs,2,48,results,results,On,per-sona chat validation set,results On per-sona chat validation set,0.5938144326210022
Blogs,2,173,results,no improvement,by controlling,response-relatedness,no improvement by controlling response-relatedness,0.6609885096549988
Blogs,2,173,results,results,found,no improvement,results found no improvement,0.6767414212226868
Blogs,2,175,results,repetition,leads to,improvements,repetition leads to improvements,0.7065107226371765
Blogs,2,175,results,improvements,across,all our aspects of conversational quality,improvements across all our aspects of conversational quality,0.6851074695587158
Blogs,2,175,results,results,find,repetition,results find repetition,0.515371561050415
Blogs,2,175,results,results,reducing,repetition,results reducing repetition,0.5836597084999084
Blogs,2,176,results,specificity,improvements in,interestingness and listening ability,specificity improvements in interestingness and listening ability,0.7206045985221863
Blogs,2,176,results,interestingness and listening ability,over,repetition - controlled baseline,interestingness and listening ability over repetition - controlled baseline,0.6459789872169495
Blogs,2,176,results,interestingness and listening ability,over,repetition - controlled baseline,interestingness and listening ability over repetition - controlled baseline,0.6459789872169495
Blogs,2,176,results,interestingness and listening ability,over,repetition - controlled baseline,interestingness and listening ability over repetition - controlled baseline,0.6459789872169495
Blogs,2,176,results,question - asking,improvements in,inquisitiveness and interestingness,question - asking improvements in inquisitiveness and interestingness,0.6991466879844666
Blogs,2,176,results,inquisitiveness and interestingness,over,repetition - controlled baseline,inquisitiveness and interestingness over repetition - controlled baseline,0.6517155766487122
Blogs,2,176,results,increasing,has,specificity,increasing has specificity,0.5753806233406067
Blogs,2,176,results,increasing,has,question - asking,increasing has question - asking,0.5681975483894348
Blogs,2,176,results,results,has,increasing,results has increasing,0.5362156629562378
Blogs,2,180,results,low-level attributes,over,multiple turns,low-level attributes over multiple turns,0.6787208914756775
Blogs,2,180,results,low-level attributes,leads to,improved overall quality,low-level attributes leads to improved overall quality,0.6279582381248474
Blogs,2,180,results,results,controlling,low-level attributes,results controlling low-level attributes,0.6553159952163696
Blogs,2,181,results,effect of controlled attributes repetition,has,wd ),effect of controlled attributes repetition has wd ),0.5944725275039673
Blogs,2,181,results,results,has,effect of controlled attributes repetition,results has effect of controlled attributes repetition,0.5573638081550598
Blogs,2,182,results,selfrepetition,across,utterances ( external repetition ),selfrepetition across utterances ( external repetition ),0.648759126663208
Blogs,2,182,results,selfrepetition,most severe form of,repetition,selfrepetition most severe form of repetition,0.6718220710754395
Blogs,2,182,results,utterances ( external repetition ),most severe form of,repetition,utterances ( external repetition ) most severe form of repetition,0.6370030641555786
Blogs,2,182,results,repetition,in,our beam search baseline model,repetition in our beam search baseline model,0.5575152635574341
Blogs,2,182,results,results,observe,selfrepetition,results observe selfrepetition,0.6002333760261536
Blogs,2,183,results,several settings,of,extrep bigram weighted decoding feature,several settings of extrep bigram weighted decoding feature,0.5550313591957092
Blogs,2,183,results,aggressive repetition - reduction setting,reducing,bigram repetition rate,aggressive repetition - reduction setting reducing bigram repetition rate,0.7124438881874084
Blogs,2,183,results,aggressive repetition - reduction setting,rated,best,aggressive repetition - reduction setting rated best,0.7220740914344788
Blogs,2,183,results,results,evaluate,several settings,results evaluate several settings,0.610429048538208
Blogs,2,184,results,blocking,improves,avoiding repetition score,blocking improves avoiding repetition score,0.6488631367683411
Blogs,2,184,results,repeated content words,improves,avoiding repetition score,repeated content words improves avoiding repetition score,0.7092685699462891
Blogs,2,184,results,blocking,has,repeated content words,blocking has repeated content words,0.627936065196991
Blogs,2,184,results,results,find,blocking,results find blocking,0.48984119296073914
Blogs,2,186,results,beam search baseline,in,all metrics,beam search baseline in all metrics,0.49438875913619995
Blogs,2,186,results,close-to-human scores,on,all metrics,close-to-human scores on all metrics,0.4705517590045929
Blogs,2,186,results,close-to-human scores,except,humanness,close-to-human scores except humanness,0.6603427529335022
Blogs,2,191,results,extreme settings ( very generic and very specific ),score,poorly,extreme settings ( very generic and very specific ) score poorly,0.7210339903831482
Blogs,2,191,results,poorly,in,engagingness,poorly in engagingness,0.5436596870422363
Blogs,2,191,results,engagingness,due to,frequent presence of degenerate output,engagingness due to frequent presence of degenerate output,0.6895931363105774
Blogs,2,191,results,weighted decoding models,has,extreme settings ( very generic and very specific ),weighted decoding models has extreme settings ( very generic and very specific ),0.5611165761947632
Blogs,2,191,results,results,For,weighted decoding models,results For weighted decoding models,0.5716858506202698
Blogs,2,192,results,weight = 4 setting,maximizes,engagingness,weight = 4 setting maximizes engagingness,0.775123119354248
Blogs,2,192,results,results,find,weight = 4 setting,results find weight = 4 setting,0.6516022086143494
Blogs,2,193,results,more-specific model,is,better listener,more-specific model is better listener,0.5473163723945618
Blogs,2,193,results,more-specific model,rated,"more interesting , engaging","more-specific model rated more interesting , engaging",0.6983625292778015
Blogs,2,193,results,more-specific model,rated,better listener,more-specific model rated better listener,0.7450314164161682
Blogs,2,193,results,better listener,than,repetition - controlled baseline,better listener than repetition - controlled baseline,0.6156414747238159
Blogs,2,194,results,our ct model,with,z = 7,our ct model with z = 7,0.6804958581924438
Blogs,2,194,results,our ct model,shows,similar results,our ct model shows similar results,0.6991973519325256
Blogs,2,194,results,results,has,our ct model,results has our ct model,0.5233845114707947
Blogs,2,214,results,rated significantly more interesting,than,repetitioncontrolled baseline,rated significantly more interesting than repetitioncontrolled baseline,0.5994051098823547
Blogs,3,81,baselines,r+t+h model,trained on,all dialogs,r+t+h model trained on all dialogs,0.7209679484367371
Blogs,3,81,baselines,all dialogs,related to,"restaurants , hotels , pubs and coffee shops","all dialogs related to restaurants , hotels , pubs and coffee shops",0.6622403264045715
Blogs,3,81,baselines,r+t+h+l model,has,most general model,r+t+h+l model has most general model,0.5254300832748413
Blogs,3,81,baselines,baselines,has,all restaurants model,baselines has all restaurants model,0.5418745279312134
Blogs,3,6,model,training procedure,uses,out -of- domain data,training procedure uses out -of- domain data,0.6257718205451965
Blogs,3,6,model,out -of- domain data,to initialise,belief tracking models,out -of- domain data to initialise belief tracking models,0.7167653441429138
Blogs,3,6,model,belief tracking models,for,entirely new domains,belief tracking models for entirely new domains,0.5866299867630005
Blogs,3,6,model,model,propose,training procedure,model propose training procedure,0.5949419736862183
Blogs,3,11,model,model,build,dialog state tracking models,model build dialog state tracking models,0.6699817776679993
Blogs,3,12,model,dialog system,updating,system 's belief state,dialog system updating system 's belief state,0.7067674994468689
Blogs,3,12,model,interpreting,has,users ' utterances,interpreting has users ' utterances,0.543023943901062
Blogs,3,12,model,system 's belief state,has,probability distribution,system 's belief state has probability distribution,0.5644381642341614
Blogs,3,12,model,model,has,state tracking component,model has state tracking component,0.5715351700782776
Blogs,3,17,model,method,for training,multi-domain rnn dialog state tracking models,method for training multi-domain rnn dialog state tracking models,0.6676596403121948
Blogs,3,18,model,hierarchical training procedure,first uses,all the data,hierarchical training procedure first uses all the data,0.7383820414543152
Blogs,3,18,model,all the data,to train,very general belief tracking model,all the data to train very general belief tracking model,0.6925008296966553
Blogs,3,18,model,model,has,hierarchical training procedure,model has hierarchical training procedure,0.5523334741592407
Blogs,3,20,model,general model,specialised for,each domain,general model specialised for each domain,0.6818982362747192
Blogs,3,20,model,general model,learning,domain-specific behaviour,general model learning domain-specific behaviour,0.7427461743354797
Blogs,3,20,model,domain-specific behaviour,while retaining,cross-domain dialog patterns,domain-specific behaviour while retaining cross-domain dialog patterns,0.6320270299911499
Blogs,3,20,model,cross-domain dialog patterns,learned during,initial training stages,cross-domain dialog patterns learned during initial training stages,0.7150980234146118
Blogs,3,20,model,model,has,general model,model has general model,0.5565866827964783
Blogs,3,5,results,dialog data,drawn from,different dialog domains,dialog data drawn from different dialog domains,0.5904592275619507
Blogs,3,5,results,dialog data,to,train,dialog data to train,0.5694891810417175
Blogs,3,5,results,general belief tracking model,exhibiting,superior performance,general belief tracking model exhibiting superior performance,0.6233948469161987
Blogs,3,5,results,superior performance,to,each of the domainspecific models,superior performance to each of the domainspecific models,0.5323482155799866
Blogs,3,5,results,train,has,general belief tracking model,train has general belief tracking model,0.5497673749923706
Blogs,3,5,results,results,shows,dialog data,results shows dialog data,0.6677075624465942
Blogs,3,92,results,of them,improve over,domain-specific model,of them improve over domain-specific model,0.7021824717521667
Blogs,3,92,results,domain-specific model,for,all but one of their constituent domains,domain-specific model for all but one of their constituent domains,0.6029263734817505
Blogs,3,92,results,results,has,of them,results has of them,0.5687906742095947
Blogs,3,93,results,r+t+h+l model,across,four domains,r+t+h+l model across four domains,0.699726402759552
Blogs,3,93,results,performance slightly,across,other more closely related domains,performance slightly across other more closely related domains,0.7070531845092773
Blogs,3,93,results,r+t+h model,has,outperforms,r+t+h model has outperforms,0.6139615774154663
Blogs,3,93,results,outperforms,has,r+t+h+l model,outperforms has r+t+h+l model,0.6128550171852112
Blogs,3,93,results,decreases,has,performance slightly,decreases has performance slightly,0.5760924220085144
Blogs,3,93,results,results,has,r+t+h model,results has r+t+h model,0.5451583862304688
Blogs,3,99,results,all three slotspecialised general models,has,outperformed,all three slotspecialised general models has outperformed,0.5747438073158264
Blogs,3,99,results,outperformed,has,rnn model 's performance,outperformed has rnn model 's performance,0.5687668323516846
Blogs,3,99,results,results,has,all three slotspecialised general models,results has all three slotspecialised general models,0.495053768157959
Blogs,3,109,results,performance,of,two differently initialised models,performance of two differently initialised models,0.5988419055938721
Blogs,3,109,results,improves,as,additional in-domain dialogs,improves as additional in-domain dialogs,0.5444442629814148
Blogs,3,109,results,results,shows,performance,results shows performance,0.7266349792480469
Blogs,3,110,results,out - of- domain data,helps to initialise,model,out - of- domain data helps to initialise model,0.7552971243858337
Blogs,3,110,results,model,to,much better starting point,model to much better starting point,0.5832255482673645
Blogs,3,110,results,much better starting point,when,in-domain training data set,much better starting point when in-domain training data set,0.6068650484085083
Blogs,3,110,results,in-domain training data set,is,small,in-domain training data set is small,0.5150627493858337
Blogs,3,110,results,results,use of,out - of- domain data,results use of out - of- domain data,0.6651124954223633
Blogs,3,111,results,entire in- domain dataset,becomes available to,training procedure,entire in- domain dataset becomes available to training procedure,0.6681510806083679
Blogs,3,111,results,out -of- domain initialisation,has,consistently improves,out -of- domain initialisation has consistently improves,0.5974788665771484
Blogs,3,111,results,consistently improves,has,performance,consistently improves has performance,0.5987896919250488
Blogs,3,111,results,performance,has,joint goal accuracy,performance has joint goal accuracy,0.5542365908622742
Blogs,3,111,results,results,has,out -of- domain initialisation,results has out -of- domain initialisation,0.5321898460388184
Blogs,3,115,results,relatively minor performance gains,over,domain-specific one,relatively minor performance gains over domain-specific one,0.6783828735351562
Blogs,3,115,results,results,has,additional restaurants dialogs,results has additional restaurants dialogs,0.5918183922767639
Blogs,5,239,baselines,best performing meena model,is,evolved transformer ( et ),best performing meena model is evolved transformer ( et ),0.5665327310562134
Blogs,5,239,baselines,) seq2seq model,with,2.6b parameters,) seq2seq model with 2.6b parameters,0.6154777407646179
Blogs,5,239,baselines,) seq2seq model,includes,1 et encoder block,) seq2seq model includes 1 et encoder block,0.6124237775802612
Blogs,5,239,baselines,) seq2seq model,includes,13 et decoder blocks,) seq2seq model includes 13 et decoder blocks,0.6510384678840637
Blogs,5,239,baselines,2.6b parameters,includes,13 et decoder blocks,2.6b parameters includes 13 et decoder blocks,0.63856440782547
Blogs,5,239,baselines,evolved transformer ( et ),has,) seq2seq model,evolved transformer ( et ) has ) seq2seq model,0.5902525782585144
Blogs,5,239,baselines,baselines,has,best performing meena model,baselines has best performing meena model,0.5401448607444763
Blogs,5,23,experimental-setup,seq2seq model,with,"evolved transformer ( so et al. , 2019 )","seq2seq model with evolved transformer ( so et al. , 2019 )",0.6093432307243347
Blogs,5,23,experimental-setup,"evolved transformer ( so et al. , 2019 )",as,main architecture,"evolved transformer ( so et al. , 2019 ) as main architecture",0.5135304927825928
Blogs,5,23,experimental-setup,experimental setup,use,seq2seq model,experimental setup use seq2seq model,0.6060530543327332
Blogs,5,246,experimental-setup,maximum length,of,128 tokens,maximum length of 128 tokens,0.5920374989509583
Blogs,5,246,experimental-setup,experimental setup,has,encoder and decoder,experimental setup has encoder and decoder,0.5583252906799316
Blogs,5,252,experimental-setup,adafactor optimizer,with,0.01,adafactor optimizer with 0.01,0.6309744119644165
Blogs,5,252,experimental-setup,0.01,as,initial learning rate,0.01 as initial learning rate,0.5245258808135986
Blogs,5,252,experimental-setup,0.01,keeping it,constant,0.01 keeping it constant,0.5495700836181641
Blogs,5,252,experimental-setup,0.01,decaying with,inverse square root,0.01 decaying with inverse square root,0.688732385635376
Blogs,5,252,experimental-setup,constant,for,first 10 k steps,constant for first 10 k steps,0.659351646900177
Blogs,5,252,experimental-setup,inverse square root,of,number of steps,inverse square root of number of steps,0.6176457405090332
Blogs,5,253,experimental-setup,"tensor2tensor code-base ( vaswani et al. , 2018 )",for training,meena,"tensor2tensor code-base ( vaswani et al. , 2018 ) for training meena",0.7057667970657349
Blogs,5,253,experimental-setup,experimental setup,use,"tensor2tensor code-base ( vaswani et al. , 2018 )","experimental setup use tensor2tensor code-base ( vaswani et al. , 2018 )",0.5671793818473816
Blogs,5,254,experimental-setup,tpu - v3 core,16GB of,high - bandwidth memory,tpu - v3 core 16GB of high - bandwidth memory,0.7344464063644409
Blogs,5,254,experimental-setup,experimental setup,has,tpu - v3 core,experimental setup has tpu - v3 core,0.5665559768676758
Blogs,5,255,experimental-setup,memory usage,for,model parameters,memory usage for model parameters,0.5370039939880371
Blogs,5,255,experimental-setup,8 training examples,per,core,8 training examples per core,0.605009913444519
Blogs,5,255,experimental-setup,experimental setup,maximized,memory usage,experimental setup maximized memory usage,0.6582551002502441
Blogs,5,4,experiments,multi-turn open-domain chatbot,trained,end-to - end,multi-turn open-domain chatbot trained end-to - end,0.6801589131355286
Blogs,5,4,experiments,end-to - end,on,data,end-to - end on data,0.5938222408294678
Blogs,5,4,experiments,data,mined and filtered from,public domain social media conversations,data mined and filtered from public domain social media conversations,0.6396252512931824
Blogs,5,4,experiments,meena,has,multi-turn open-domain chatbot,meena has multi-turn open-domain chatbot,0.6024839878082275
Blogs,5,24,experiments,multi-turn conversations,where,input sequence,multi-turn conversations where input sequence,0.596646785736084
Blogs,5,24,experiments,multi-turn conversations,where,output sequence,multi-turn conversations where output sequence,0.6101347208023071
Blogs,5,24,experiments,input sequence,all turns of,context ( up to 7 ),input sequence all turns of context ( up to 7 ),0.6936137676239014
Blogs,5,24,experiments,output sequence,is,response,output sequence is response,0.6113768219947815
Blogs,5,21,model,generative chatbot model,trained,end-to -end,generative chatbot model trained end-to -end,0.6961816549301147
Blogs,5,21,model,end-to -end,on,40b words,end-to -end on 40b words,0.5772637128829956
Blogs,5,21,model,40b words,mined and filtered from,public domain social media conversations,40b words mined and filtered from public domain social media conversations,0.6932148337364197
Blogs,5,21,model,meena,has,generative chatbot model,meena has generative chatbot model,0.5648692846298218
Blogs,5,21,model,model,present,meena,model present meena,0.7561080455780029
Blogs,5,240,model,evolved transformer,is,evolutionary nas architecture,evolved transformer is evolutionary nas architecture,0.6009524464607239
Blogs,5,240,model,evolutionary nas architecture,based on,transformer,evolutionary nas architecture based on transformer,0.6875801086425781
Blogs,5,240,model,model,has,evolved transformer,model has evolved transformer,0.5766139030456543
Blogs,5,245,model,embeddings,across,encoder,embeddings across encoder,0.7058364748954773
Blogs,5,245,model,embeddings,across,decoder,embeddings across decoder,0.7437927722930908
Blogs,5,245,model,embeddings,across,softmax layer,embeddings across softmax layer,0.6360242962837219
Blogs,5,245,model,model,share,embeddings,model share embeddings,0.7217400670051575
Blogs,5,9,results,full version of meena,with,filtering mechanism,full version of meena with filtering mechanism,0.6186627149581909
Blogs,5,9,results,full version of meena,with,tuned decoding,full version of meena with tuned decoding,0.6400811672210693
Blogs,5,9,results,full version of meena,scores,79 % ssa,full version of meena scores 79 % ssa,0.7116901874542236
Blogs,5,9,results,higher,in,absolute ssa,higher in absolute ssa,0.5607447028160095
Blogs,5,9,results,absolute ssa,than,existing chatbots,absolute ssa than existing chatbots,0.5897865295410156
Blogs,5,9,results,23 %,has,higher,23 % has higher,0.5875585675239563
Blogs,5,9,results,results,has,full version of meena,results has full version of meena,0.569191575050354
Blogs,5,22,results,meena,push,limits,meena push limits,0.7704432606697083
Blogs,5,22,results,meena,show that,largescale low-perplexity model,meena show that largescale low-perplexity model,0.5090560913085938
Blogs,5,22,results,limits,of,end-to - end approach,limits of end-to - end approach,0.6077728867530823
Blogs,5,22,results,largescale low-perplexity model,can be,good conversationalist,largescale low-perplexity model can be good conversationalist,0.6294926404953003
Blogs,5,22,results,results,With,meena,results With meena,0.6450912356376648
Blogs,5,44,results,best end-to - end learned model,average of,72 % ssa,best end-to - end learned model average of 72 % ssa,0.7156574130058289
Blogs,5,44,results,results,has,best end-to - end learned model,results has best end-to - end learned model,0.5261165499687195
Blogs,5,370,results,n = 20,provides,significant improvement,n = 20 provides significant improvement,0.6101865768432617
Blogs,5,370,results,significant improvement,over,n = 1,significant improvement over n = 1,0.6381925940513611
Blogs,5,370,results,significant improvement,with,absolute improvement,significant improvement with absolute improvement,0.5764414668083191
Blogs,5,370,results,absolute improvement,in,ssa,absolute improvement in ssa,0.5976672172546387
Blogs,5,370,results,ssa,of,?10 %,ssa of ?10 %,0.620086669921875
Blogs,5,370,results,results,show,n = 20,results show n = 20,0.604446530342102
Blogs,6,251,ablation-analysis,crop and category information,are,redundant,crop and category information are redundant,0.6156622171401978
Blogs,6,251,ablation-analysis,( question + category ) and ( question + crop ) model,achieve,29.2 % and 25.7 % error,( question + category ) and ( question + crop ) model achieve 29.2 % and 25.7 % error,0.6042695641517639
Blogs,6,251,ablation-analysis,combined model ( question + category + crop ),achieves,24.7 %,combined model ( question + category + crop ) achieves 24.7 %,0.6285178065299988
Blogs,6,251,ablation-analysis,ablation analysis,find that,crop and category information,ablation analysis find that crop and category information,0.6288418769836426
Blogs,6,299,ablation-analysis,oracle 's answers,while training,question generator,oracle 's answers while training question generator,0.7709630727767944
Blogs,6,299,ablation-analysis,question generator,introduces,additional errors,question generator introduces additional errors,0.6475090384483337
Blogs,6,299,ablation-analysis,significantly deteriorates,has,performance,significantly deteriorates has performance,0.5933961272239685
Blogs,6,299,ablation-analysis,ablation analysis,using,oracle 's answers,ablation analysis using oracle 's answers,0.6753945350646973
Blogs,6,239,hyperparameters,training,keep,parameters,training keep parameters,0.6240848302841187
Blogs,6,239,hyperparameters,training,optimize,lstm,training optimize lstm,0.7382363080978394
Blogs,6,239,hyperparameters,training,optimize,mlp parameters,training optimize mlp parameters,0.7110339403152466
Blogs,6,239,hyperparameters,parameters,of,vgg network,parameters of vgg network,0.5704600214958191
Blogs,6,239,hyperparameters,lstm,by minimizing,negative log-likelihood,lstm by minimizing negative log-likelihood,0.6947293877601624
Blogs,6,239,hyperparameters,mlp parameters,by minimizing,negative log-likelihood,mlp parameters by minimizing negative log-likelihood,0.6909685730934143
Blogs,6,239,hyperparameters,negative log-likelihood,of,correct answer,negative log-likelihood of correct answer,0.5876075625419617
Blogs,6,239,hyperparameters,vgg network,has,fixed,vgg network has fixed,0.5832900404930115
Blogs,6,239,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
Blogs,6,240,hyperparameters,adam,for,optimization,adam for optimization,0.6337777972221375
Blogs,6,240,hyperparameters,adam,train for,at most 15 epochs,adam train for at most 15 epochs,0.7066036462783813
Blogs,6,241,hyperparameters,early stopping,on,validation set,early stopping on validation set,0.5784603357315063
Blogs,6,241,hyperparameters,hyperparameters,report,"train , valid and test error","hyperparameters report train , valid and test error",0.6069615483283997
Blogs,6,30,model,novel goal- directed task,for,multi-modal dialogue,novel goal- directed task for multi-modal dialogue,0.5644753575325012
Blogs,6,30,model,model,propose,novel goal- directed task,model propose novel goal- directed task,0.6648051142692566
Blogs,6,255,model,best performing model,combines,object category and its spatial features,best performing model combines object category and its spatial features,0.675894021987915
Blogs,6,255,model,object category and its spatial features,along with,question,object category and its spatial features along with question,0.6243165731430054
Blogs,6,255,model,model,has,best performing model,model has best performing model,0.5509435534477234
Blogs,6,248,results,question,slightly improves,error rate,question slightly improves error rate,0.6994810104370117
Blogs,6,248,results,error rate,to,41.2 %,error rate to 41.2 %,0.5409724712371826
Blogs,6,248,results,results,using,question,results using question,0.6031928062438965
Blogs,6,253,results,object category,has,outperforms,object category has outperforms,0.6268097162246704
Blogs,6,253,results,outperforms,has,object crop embedding,outperforms has object crop embedding,0.5745108127593994
Blogs,6,253,results,results,find that,object category,results find that object category,0.6098222136497498
Blogs,6,278,results,vgg features,does not improve,performance,vgg features does not improve performance,0.6614406108856201
Blogs,6,278,results,performance,of,lstm and hred models,performance of lstm and hred models,0.5683088302612305
Blogs,6,278,results,results,find that,vgg features,results find that vgg features,0.580809473991394
Blogs,6,278,results,results,including,vgg features,results including vgg features,0.583983302116394
Blogs,6,280,results,lstms,to perform,slightly better,lstms to perform slightly better,0.6218841671943665
Blogs,6,280,results,slightly better,than,sophisticated hred models,slightly better than sophisticated hred models,0.5989726781845093
Blogs,6,280,results,results,find,lstms,results find lstms,0.5299681425094604
Blogs,6,297,results,guesser,based on,human generated dialogues,guesser based on human generated dialogues,0.6673079133033752
Blogs,6,297,results,guesser,achieves,38.7 % error,guesser achieves 38.7 % error,0.6466398239135742
Blogs,6,297,results,results,has,guesser,results has guesser,0.6022589206695557
Blogs,6,298,results,question generator models,achieve,reasonable performance,question generator models achieve reasonable performance,0.6220930218696594
Blogs,6,298,results,reasonable performance,lies in between,random performance,reasonable performance lies in between random performance,0.6456789374351501
Blogs,6,298,results,results,has,question generator models,results has question generator models,0.5188761949539185
Blogs,7,204,baselines,baselines,has,linear crf team3 entry0,baselines has linear crf team3 entry0,0.5903571248054504
Blogs,7,181,results,machine - learned methods,for,dst,machine - learned methods for dst,0.5916548371315002
Blogs,7,181,results,machine - learned methods,performed,top,machine - learned methods performed top,0.31716567277908325
Blogs,7,181,results,top,in,dstc1 and dstc3 evaluations,top in dstc1 and dstc3 evaluations,0.5576714277267456
Blogs,7,181,results,results,has,machine - learned methods,results has machine - learned methods,0.4929344654083252
Blogs,7,189,results,dstc1,has,sequential crf model,dstc1 has sequential crf model,0.5712734460830688
Blogs,7,189,results,outperformed,has,static classifier approaches,outperformed has static classifier approaches,0.6310768127441406
Blogs,7,189,results,results,In,dstc1,results In dstc1,0.5163173079490662
