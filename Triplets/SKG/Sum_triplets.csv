topic,paper_ID,sentence_ID,info-unit,sub,pred,obj,triplets,pred_weights
translation,0,108,baselines,pos n-grams,use,part- of-speech tags,pos n-grams use part- of-speech tags,0.5906521677970886
translation,0,108,baselines,baselines,has,pos n-grams,baselines has pos n-grams,0.5503397583961487
translation,0,31,model,summarization,generate,multiple compression candidates,summarization generate multiple compression candidates,0.6627424955368042
translation,0,31,model,summarization,use,ilp framework,summarization use ilp framework,0.5703021883964539
translation,0,31,model,multiple compression candidates,for,each sentence,multiple compression candidates for each sentence,0.6302015781402588
translation,0,31,model,ilp framework,to select,compressed summary sentences,ilp framework to select compressed summary sentences,0.6487351059913635
translation,0,31,model,model,During,summarization,model During summarization,0.6929118633270264
translation,0,31,model,model,use,ilp framework,model use ilp framework,0.6088116765022278
translation,0,56,model,novel guided compression method,catered to,summarization task,novel guided compression method catered to summarization task,0.6927028298377991
translation,0,56,model,model,adopt,pipeline - based compressive summarization framework,model adopt pipeline - based compressive summarization framework,0.5989312529563904
translation,0,59,model,sentence pre-selection step,to further accelerate,processing,sentence pre-selection step to further accelerate processing,0.6912800669670105
translation,0,59,model,sentence pre-selection step,enhance,performance,sentence pre-selection step enhance performance,0.6568630337715149
translation,0,59,model,model,apply,sentence pre-selection step,model apply sentence pre-selection step,0.6231167912483215
translation,0,96,model,summarization system,consists of,three key components,summarization system consists of three key components,0.6949561834335327
translation,0,96,model,three key components,train,supervised guided compression model,three key components train supervised guided compression model,0.670466423034668
translation,0,96,model,supervised guided compression model,using,created compression data,supervised guided compression model using created compression data,0.6986274719238281
translation,0,96,model,model,has,summarization system,model has summarization system,0.5953480005264282
translation,0,97,model,model,to generate,n-best compressions,model to generate n-best compressions,0.6984363198280334
translation,0,97,model,n-best compressions,for,each sentence,n-best compressions for each sentence,0.5855704545974731
translation,0,97,model,n-best compressions,feed,multiple compressed sentences,n-best compressions feed multiple compressed sentences,0.678177535533905
translation,0,97,model,multiple compressed sentences,to,ilp framework,multiple compressed sentences to ilp framework,0.5186049938201904
translation,0,97,model,ilp framework,to select,best summary sentences,ilp framework to select best summary sentences,0.6600947380065918
translation,0,97,model,model,use,model,model use model,0.6699982285499573
translation,0,97,model,model,to generate,n-best compressions,model to generate n-best compressions,0.6984363198280334
translation,0,97,model,model,feed,multiple compressed sentences,model feed multiple compressed sentences,0.6469528675079346
translation,0,98,model,speed up,has,summarization system,speed up has summarization system,0.5974866151809692
translation,0,98,model,improve,has,performance,improve has performance,0.5578044652938843
translation,0,98,model,model,propose,sentence pre-selection step,model propose sentence pre-selection step,0.6170579195022583
translation,0,146,model,simple supervised support vector regression ( svr ) model,to predict,salience score,simple supervised support vector regression ( svr ) model to predict salience score,0.6918779015541077
translation,0,146,model,simple supervised support vector regression ( svr ) model,select,top ranked sentences,simple supervised support vector regression ( svr ) model select top ranked sentences,0.6551986336708069
translation,0,146,model,salience score,for,each sentence,salience score for each sentence,0.5871537923812866
translation,0,146,model,top ranked sentences,for,further processing,top ranked sentences for further processing,0.6146187782287598
translation,0,146,model,further processing,has,compression and summarization ),further processing has compression and summarization ),0.6004527807235718
translation,0,146,model,model,propose to use,simple supervised support vector regression ( svr ) model,model propose to use simple supervised support vector regression ( svr ) model,0.7245643734931946
translation,0,212,model,pipeline summarization approach,combines,novel guided compression model,pipeline summarization approach combines novel guided compression model,0.6782447099685669
translation,0,212,model,novel guided compression model,with,ilp - based summary sentence selection,novel guided compression model with ilp - based summary sentence selection,0.5875503420829773
translation,0,212,model,model,propose,pipeline summarization approach,model propose pipeline summarization approach,0.6755444407463074
translation,0,213,model,guided compression corpus,where,human annotators,guided compression corpus where human annotators,0.5669155716896057
translation,0,213,model,human annotators,explicitly informed about,important summary words,human annotators explicitly informed about important summary words,0.6529316306114197
translation,0,213,model,important summary words,during,compression annotation,important summary words during compression annotation,0.6352081894874573
translation,0,213,model,model,create,guided compression corpus,model create guided compression corpus,0.6421606540679932
translation,0,214,model,supervised compression model,to capture,guided compression process,supervised compression model to capture guided compression process,0.7220235466957092
translation,0,214,model,supervised compression model,set of,"word - , sentence - , and document- level features","supervised compression model set of word - , sentence - , and document- level features",0.6226105690002441
translation,0,214,model,guided compression process,set of,"word - , sentence - , and document- level features","guided compression process set of word - , sentence - , and document- level features",0.6539142727851868
translation,0,214,model,model,train,supervised compression model,model train supervised compression model,0.6889259815216064
translation,0,165,results,our system,achieves,considerably better results,our system achieves considerably better results,0.654616117477417
translation,0,165,results,considerably better results,compared to,state - of - the - art,considerably better results compared to state - of - the - art,0.6327361464500427
translation,0,165,results,state - of - the - art,on,tac 2008 and 2011 data sets,state - of - the - art on tac 2008 and 2011 data sets,0.5169239044189453
translation,0,166,results,our system,on,tac 2011 data set,our system on tac 2011 data set,0.5280033946037292
translation,0,166,results,our system,yields,considerable performance gain,our system yields considerable performance gain,0.6935840845108032
translation,0,166,results,best reported result,at,95 % significance level,best reported result at 95 % significance level,0.4689198434352875
translation,0,166,results,our system,yields,considerable performance gain,our system yields considerable performance gain,0.6935840845108032
translation,0,166,results,tac 2008 data set,has,our system,tac 2008 data set has our system,0.5591336488723755
translation,0,166,results,our system,has,outperforms,our system has outperforms,0.6423544883728027
translation,0,166,results,outperforms,has,best reported result,outperforms has best reported result,0.6395747065544128
translation,0,166,results,tac 2011 data set,has,our system,tac 2011 data set has our system,0.5589584112167358
translation,0,166,results,results,On,tac 2008 data set,results On tac 2008 data set,0.5474139451980591
translation,0,166,results,results,on,tac 2011 data set,results on tac 2011 data set,0.5362268686294556
translation,0,172,results,sentence pre-selection,removes,some sentences,sentence pre-selection removes some sentences,0.6814356446266174
translation,0,172,results,some sentences,from consideration,later summarization step,some sentences from consideration later summarization step,0.6913214921951294
translation,0,172,results,some sentences,in,later summarization step,some sentences in later summarization step,0.49199092388153076
translation,0,172,results,significantly improves,has,system performance,significantly improves has system performance,0.5920281410217285
translation,0,191,results,r,-,2 score,r - 2 score,0.6523276567459106
translation,0,191,results,more compression candidates,has,r,more compression candidates has r,0.5637633800506592
translation,0,191,results,more compression candidates,has,2 score,more compression candidates has 2 score,0.5741539597511292
translation,0,191,results,r,has,2 score,r has 2 score,0.5851866006851196
translation,0,191,results,results,given,more compression candidates,results given more compression candidates,0.7210704684257507
translation,0,201,results,system,with,generic compression model,system with generic compression model,0.671351969242096
translation,0,201,results,system,performs,worse,system performs worse,0.7055599689483643
translation,0,201,results,worse,than,ours,worse than ours,0.637956976890564
translation,0,201,results,inferior,to,tac best performing system,inferior to tac best performing system,0.5686338543891907
translation,0,201,results,results,see that,system,results see that system,0.6715076565742493
translation,0,209,results,compression training data,improves,summarization performance,compression training data improves summarization performance,0.6468327641487122
translation,0,209,results,results,increasing,compression training data,results increasing compression training data,0.671905517578125
translation,1,153,ablation-analysis,external graph,seems to help,summarization process,external graph seems to help summarization process,0.5788825750350952
translation,1,153,ablation-analysis,ablation analysis,Adding,external graph,ablation analysis Adding external graph,0.7099175453186035
translation,1,160,ablation-analysis,our experiments,confirmed,encoding paragraph position,our experiments confirmed encoding paragraph position,0.6397021412849426
translation,1,160,ablation-analysis,encoding paragraph position,in addition to,token position,encoding paragraph position in addition to token position,0.5875154137611389
translation,1,160,ablation-analysis,token position,within,each paragraph,token position within each paragraph,0.6062170267105103
translation,1,137,baselines,proposed hierarchical transformer,set,k,proposed hierarchical transformer set k,0.690501868724823
translation,1,137,baselines,lead,is,simple baseline,lead is simple baseline,0.603202223777771
translation,1,137,baselines,lead,extracts,first k tokens,lead extracts first k tokens,0.6501572132110596
translation,1,137,baselines,simple baseline,concatenates,title and ranked paragraphs,simple baseline concatenates title and ranked paragraphs,0.788750410079956
translation,1,137,baselines,k,to,length,k to length,0.6009944081306458
translation,1,137,baselines,length,of,ground -truth target,length of ground -truth target,0.5593069791793823
translation,1,138,baselines,lexrank,build,graph,lexrank build graph,0.7245469689369202
translation,1,138,baselines,lexrank,run,pagerank - like algorithm,lexrank run pagerank - like algorithm,0.716074526309967
translation,1,138,baselines,graph,with,paragraphs,graph with paragraphs,0.7073993682861328
translation,1,138,baselines,graph,with,paragraphs,graph with paragraphs,0.7073993682861328
translation,1,138,baselines,graph,rank and select,paragraphs,graph rank and select paragraphs,0.7217458486557007
translation,1,138,baselines,paragraphs,as,nodes and edges,paragraphs as nodes and edges,0.5622860193252563
translation,1,138,baselines,paragraphs,until,length,paragraphs until length,0.6834081411361694
translation,1,138,baselines,pagerank - like algorithm,rank and select,paragraphs,pagerank - like algorithm rank and select paragraphs,0.6797289252281189
translation,1,138,baselines,paragraphs,until,length,paragraphs until length,0.6834081411361694
translation,1,138,baselines,length,of,ground - truth summary,length of ground - truth summary,0.5492429733276367
translation,1,138,baselines,ground - truth summary,is,reached,ground - truth summary is reached,0.57976233959198
translation,1,138,baselines,baselines,has,lexrank,baselines has lexrank,0.5459157228469849
translation,1,139,baselines,flat transformer ( ft ),applies,transformer - based encoder - decoder model,flat transformer ( ft ) applies transformer - based encoder - decoder model,0.6122121214866638
translation,1,139,baselines,transformer - based encoder - decoder model,to,flat token sequence,transformer - based encoder - decoder model to flat token sequence,0.5573021173477173
translation,1,139,baselines,baselines,has,flat transformer ( ft ),baselines has flat transformer ( ft ),0.5606473684310913
translation,1,54,experimental-setup,length penalty,in,decoding process,length penalty in decoding process,0.5175997018814087
translation,1,54,experimental-setup,", 2016 )",in,decoding process,", 2016 ) in decoding process",0.5546984672546387
translation,1,54,experimental-setup,decoding process,to generate,more fluent and longer summaries,decoding process to generate more fluent and longer summaries,0.6823875904083252
translation,1,120,experimental-setup,hidden size,of,two lstms,hidden size of two lstms,0.5664369463920593
translation,1,120,experimental-setup,two lstms,set to,256,two lstms set to 256,0.6668161749839783
translation,1,120,experimental-setup,dropout,with,dropout probability,dropout with dropout probability,0.5986770987510681
translation,1,120,experimental-setup,dropout,used before,all linear layers,dropout used before all linear layers,0.669087827205658
translation,1,120,experimental-setup,dropout probability,of,0.2,dropout probability of 0.2,0.6002066731452942
translation,1,120,experimental-setup,experimental setup,has,hidden size,experimental setup has hidden size,0.5616794228553772
translation,1,120,experimental-setup,experimental setup,has,dropout,experimental setup has dropout,0.5067690014839172
translation,1,121,experimental-setup,"adagrad ( duchi et al. , 2011 )",with,learning rate,"adagrad ( duchi et al. , 2011 ) with learning rate",0.53758704662323
translation,1,121,experimental-setup,learning rate,used for,optimization,learning rate used for optimization,0.6693194508552551
translation,1,121,experimental-setup,0.15,used for,optimization,0.15 used for optimization,0.679355800151825
translation,1,121,experimental-setup,learning rate,has,0.15,learning rate has 0.15,0.5453540682792664
translation,1,121,experimental-setup,experimental setup,has,"adagrad ( duchi et al. , 2011 )","experimental setup has adagrad ( duchi et al. , 2011 )",0.46130964159965515
translation,1,128,experimental-setup,abstractive models,apply,dropout,abstractive models apply dropout,0.5705495476722717
translation,1,128,experimental-setup,dropout,before,all linear layers,dropout before all linear layers,0.6243862509727478
translation,1,128,experimental-setup,with probability of 0.1 ),before,all linear layers,with probability of 0.1 ) before all linear layers,0.6255180239677429
translation,1,128,experimental-setup,label smoothing,with,smoothing factor 0.1,label smoothing with smoothing factor 0.1,0.6028391718864441
translation,1,128,experimental-setup,"szegedy et al. , 2016 )",with,smoothing factor 0.1,"szegedy et al. , 2016 ) with smoothing factor 0.1",0.5872542262077332
translation,1,128,experimental-setup,dropout,has,with probability of 0.1 ),dropout has with probability of 0.1 ),0.5995839238166809
translation,1,128,experimental-setup,label smoothing,has,"szegedy et al. , 2016 )","label smoothing has szegedy et al. , 2016 )",0.532588005065918
translation,1,128,experimental-setup,experimental setup,In,abstractive models,experimental setup In abstractive models,0.5178495645523071
translation,1,130,experimental-setup,optimizer,was,"adam ( kingma and ba , 2014 )","optimizer was adam ( kingma and ba , 2014 )",0.5348350405693054
translation,1,130,experimental-setup,"adam ( kingma and ba , 2014 )",with,learning rate,"adam ( kingma and ba , 2014 ) with learning rate",0.5989180207252502
translation,1,130,experimental-setup,"adam ( kingma and ba , 2014 )",with,decay,"adam ( kingma and ba , 2014 ) with decay",0.6575497388839722
translation,1,130,experimental-setup,learning rate,of,"2 , ? 1 = 0.9 , and ? 2 = 0.998","learning rate of 2 , ? 1 = 0.9 , and ? 2 = 0.998",0.6118789911270142
translation,1,130,experimental-setup,learning rate warmup,over,"first 8 , 000 steps","learning rate warmup over first 8 , 000 steps",0.6533567905426025
translation,1,130,experimental-setup,experimental setup,has,optimizer,experimental setup has optimizer,0.5528271794319153
translation,1,131,experimental-setup,transformer - based models,had,256 hidden units,transformer - based models had 256 hidden units,0.6258583068847656
translation,1,131,experimental-setup,feed-forward hidden size,was,1,feed-forward hidden size was 1,0.6112061738967896
translation,1,131,experimental-setup,feed-forward hidden size,was,024,feed-forward hidden size was 024,0.6635110974311829
translation,1,131,experimental-setup,1,",",024,"1 , 024",0.7358691096305847
translation,1,131,experimental-setup,024,for,all layers,024 for all layers,0.6903228759765625
translation,1,131,experimental-setup,transformer - based models,has,256 hidden units,transformer - based models has 256 hidden units,0.570981502532959
translation,1,131,experimental-setup,experimental setup,has,transformer - based models,experimental setup has transformer - based models,0.5325118899345398
translation,1,131,experimental-setup,experimental setup,has,feed-forward hidden size,experimental setup has feed-forward hidden size,0.5573694705963135
translation,1,132,experimental-setup,4 gpus ( nvidia titan xp ),for,500,4 gpus ( nvidia titan xp ) for 500,0.5921014547348022
translation,1,132,experimental-setup,4 gpus ( nvidia titan xp ),for,000 steps,4 gpus ( nvidia titan xp ) for 000 steps,0.5815173983573914
translation,1,132,experimental-setup,500,",",000 steps,"500 , 000 steps",0.5950824022293091
translation,1,132,experimental-setup,experimental setup,trained on,4 gpus ( nvidia titan xp ),experimental setup trained on 4 gpus ( nvidia titan xp ),0.6997883915901184
translation,1,133,experimental-setup,gradient accumulation,to keep,training time,gradient accumulation to keep training time,0.5635850429534912
translation,1,133,experimental-setup,training time,for,all models,training time for all models,0.5512842535972595
translation,1,133,experimental-setup,all models,has,approximately consistent,all models has approximately consistent,0.5511525869369507
translation,1,133,experimental-setup,experimental setup,used,gradient accumulation,experimental setup used gradient accumulation,0.58647221326828
translation,1,135,experimental-setup,decoding,use,beam search,decoding use beam search,0.6366719603538513
translation,1,135,experimental-setup,decoding,until,end-of-sequence token,decoding until end-of-sequence token,0.6660791039466858
translation,1,135,experimental-setup,beam search,with,beam size 5,beam search with beam size 5,0.7065784335136414
translation,1,135,experimental-setup,beam search,with,length penalty,beam search with length penalty,0.6435361504554749
translation,1,135,experimental-setup,length penalty,with,? = 0.4,length penalty with ? = 0.4,0.6269850134849548
translation,1,135,experimental-setup,end-of-sequence token,has,is reached,end-of-sequence token has is reached,0.6067472696304321
translation,1,135,experimental-setup,experimental setup,During,decoding,experimental setup During decoding,0.6438420414924622
translation,1,140,experimental-setup,experimental setup,used,6 - layer transformer,experimental setup used 6 - layer transformer,0.570415735244751
translation,1,4,model,neural summarization model,effectively process,multiple input documents,neural summarization model effectively process multiple input documents,0.6005194783210754
translation,1,4,model,neural summarization model,distill,abstractive summaries,neural summarization model distill abstractive summaries,0.5884011387825012
translation,1,4,model,model,develop,neural summarization model,model develop neural summarization model,0.5878228545188904
translation,1,5,model,previously proposed transformer architecture,with,ability,previously proposed transformer architecture with ability,0.6716651916503906
translation,1,5,model,ability,to encode,documents,ability to encode documents,0.7557600736618042
translation,1,5,model,documents,in,hierarchical manner,documents in hierarchical manner,0.5504407286643982
translation,1,5,model,model,augments,previously proposed transformer architecture,model augments previously proposed transformer architecture,0.6959146857261658
translation,1,6,model,cross-document relationships,via,attention mechanism,cross-document relationships via attention mechanism,0.6557683944702148
translation,1,6,model,attention mechanism,share,information,attention mechanism share information,0.6851430535316467
translation,1,6,model,model,represent,cross-document relationships,model represent cross-document relationships,0.604417622089386
translation,1,7,model,latent dependencies,among,textual units,latent dependencies among textual units,0.5535645484924316
translation,1,7,model,explicit graph representations,focusing on,similarity or discourse relations,explicit graph representations focusing on similarity or discourse relations,0.6563987731933594
translation,1,7,model,model,learns,latent dependencies,model learns latent dependencies,0.7044114470481873
translation,1,26,model,neural summarization model,effectively process,multiple input documents,neural summarization model effectively process multiple input documents,0.6005194783210754
translation,1,26,model,neural summarization model,distill,abstractive summaries,neural summarization model distill abstractive summaries,0.5884011387825012
translation,1,26,model,model,develop,neural summarization model,model develop neural summarization model,0.5878228545188904
translation,1,27,model,previously proposed transformer architecture,ability to encode,multiple documents,previously proposed transformer architecture ability to encode multiple documents,0.7390145063400269
translation,1,27,model,multiple documents,in,hierarchical manner,multiple documents in hierarchical manner,0.5412675738334656
translation,1,27,model,model,augments,previously proposed transformer architecture,model augments previously proposed transformer architecture,0.6959146857261658
translation,1,28,model,cross-document relationships,via,attention mechanism,cross-document relationships via attention mechanism,0.6557683944702148
translation,1,28,model,attention mechanism,allows to share,information,attention mechanism allows to share information,0.7263853549957275
translation,1,28,model,information,across,multiple documents,information across multiple documents,0.6655192375183105
translation,1,28,model,model,represent,cross-document relationships,model represent cross-document relationships,0.604417622089386
translation,1,29,model,richer structural dependencies,among,textual units,richer structural dependencies among textual units,0.5511929988861084
translation,1,29,model,richer structural dependencies,incorporating,well - established insights,richer structural dependencies incorporating well - established insights,0.6976740956306458
translation,1,29,model,model,automatically learns,richer structural dependencies,model automatically learns richer structural dependencies,0.7454397082328796
translation,1,129,model,training,is in,traditional sequence - to-sequence manner,training is in traditional sequence - to-sequence manner,0.5768858194351196
translation,1,129,model,training,with,maximum likelihood estimation,training with maximum likelihood estimation,0.6427416205406189
translation,1,129,model,traditional sequence - to-sequence manner,with,maximum likelihood estimation,traditional sequence - to-sequence manner with maximum likelihood estimation,0.6460523009300232
translation,1,129,model,model,has,training,model has training,0.5495393872261047
translation,1,147,model,model architecture,is,7 - layer network,model architecture is 7 - layer network,0.5807145833969116
translation,1,147,model,7 - layer network,with,5 localattention layers,7 - layer network with 5 localattention layers,0.6102325320243835
translation,1,147,model,7 - layer network,with,2 global attention layers,7 - layer network with 2 global attention layers,0.6120242476463318
translation,1,147,model,5 localattention layers,at,bottom,5 localattention layers at bottom,0.551084041595459
translation,1,147,model,2 global attention layers,at,top,2 global attention layers at top,0.5741722583770752
translation,1,147,model,model,has,model architecture,model has model architecture,0.5430979132652283
translation,1,31,results,proposed architecture,brings,substantial improvements,proposed architecture brings substantial improvements,0.6235128045082092
translation,1,31,results,substantial improvements,over,several strong baselines,substantial improvements over several strong baselines,0.6738615036010742
translation,1,126,results,our ranker,effectively extracts,related paragraphs,our ranker effectively extracts related paragraphs,0.7544379830360413
translation,1,126,results,our ranker,produces,more informative input,our ranker produces more informative input,0.6795090436935425
translation,1,126,results,more informative input,for,downstream summarization task,more informative input for downstream summarization task,0.6059406995773315
translation,1,126,results,results,see that,our ranker,results see that our ranker,0.7086120843887329
translation,1,154,results,similarity graph,does,obvious influence,similarity graph does obvious influence,0.2919824421405792
translation,1,154,results,similarity graph,not have,obvious influence,similarity graph not have obvious influence,0.6243569850921631
translation,1,154,results,obvious influence,on,results,obvious influence on results,0.5548160672187805
translation,1,154,results,discourse graph,boosts,rouge -l,discourse graph boosts rouge -l,0.7345001697540283
translation,1,154,results,rouge -l,by,0.16,rouge -l by 0.16,0.6084180474281311
translation,1,154,results,results,has,similarity graph,results has similarity graph,0.5754130482673645
translation,1,155,results,performance,of,hierarchical transformer,performance of hierarchical transformer,0.6206218600273132
translation,1,155,results,hierarchical transformer,further improves,model,hierarchical transformer further improves model,0.6766357421875
translation,1,155,results,model,presented with,longer input at test time,model presented with longer input at test time,0.6707537174224854
translation,1,155,results,results,found that,performance,results found that performance,0.7056670188903809
translation,1,157,results,3,",",000 input tokens,"3 , 000 input tokens",0.6114229559898376
translation,1,157,results,improves,across,board,improves across board,0.6682805418968201
translation,1,157,results,3,has,000 input tokens,3 has 000 input tokens,0.6162632703781128
translation,1,157,results,3,has,summarization quality,3 has summarization quality,0.5107328295707703
translation,1,157,results,000 input tokens,has,summarization quality,000 input tokens has summarization quality,0.5131234526634216
translation,1,157,results,summarization quality,has,improves,summarization quality has improves,0.5663559436798096
translation,1,179,results,participants,overwhelmingly prefer,our model ( ht ),participants overwhelmingly prefer our model ( ht ),0.6208176016807556
translation,1,179,results,both evaluations,has,participants,both evaluations has participants,0.5438516736030579
translation,1,179,results,results,on,both evaluations,results on both evaluations,0.5109100937843323
translation,1,246,results,outperform,has,extractive ones,outperform has extractive ones,0.5962715744972229
translation,2,6,model,submodular optimization problem,on,topic hierarchy,submodular optimization problem on topic hierarchy,0.5223748683929443
translation,2,6,model,topic hierarchy,using,documents,topic hierarchy using documents,0.6800593137741089
translation,2,6,model,documents,as,features,documents as features,0.526165783405304
translation,2,6,model,model,as,submodular optimization problem,model as submodular optimization problem,0.5074564814567566
translation,2,9,model,large-margin framework,to learn,convex mixtures,large-margin framework to learn convex mixtures,0.5528078675270081
translation,2,9,model,convex mixtures,over,set of submodular components,convex mixtures over set of submodular components,0.6397362351417542
translation,2,9,model,model,use,large-margin framework,model use large-margin framework,0.6139281988143921
translation,3,121,ablation-analysis,redundancy and topic similarity,are,correlated,redundancy and topic similarity are correlated,0.5978836417198181
translation,3,121,ablation-analysis,ablation analysis,observe,redundancy and topic similarity,ablation analysis observe redundancy and topic similarity,0.6023629903793335
translation,3,121,ablation-analysis,ablation analysis,observe,abstractivity,ablation analysis observe abstractivity,0.5882689952850342
translation,3,111,results,scientific summaries,which are,abstracts of published papers ),scientific summaries which are abstracts of published papers ),0.6545215249061584
translation,3,111,results,scientific summaries,are,more coherent,scientific summaries are more coherent,0.5830003023147583
translation,3,111,results,scientific summaries,are,concatenated bullet-point summaries,scientific summaries are concatenated bullet-point summaries,0.6130524277687073
translation,3,111,results,scientific summaries,clearly,more coherent,scientific summaries clearly more coherent,0.6448982954025269
translation,3,111,results,abstracts of published papers ),are,more coherent,abstracts of published papers ) are more coherent,0.5684830546379089
translation,3,111,results,abstracts of published papers ),clearly,more coherent,abstracts of published papers ) clearly more coherent,0.6584985852241516
translation,3,111,results,more coherent,than,author- generated summaries,more coherent than author- generated summaries,0.5533539652824402
translation,3,111,results,more coherent,than,concatenated bullet-point summaries,more coherent than concatenated bullet-point summaries,0.536429226398468
translation,3,111,results,author- generated summaries,in,tl,author- generated summaries in tl,0.5680755376815796
translation,3,111,results,fragmented summaries,in,ami,fragmented summaries in ami,0.6291067600250244
translation,3,111,results,concatenated bullet-point summaries,in,cnn -dm,concatenated bullet-point summaries in cnn -dm,0.5453240871429443
translation,3,111,results,results,observe,scientific summaries,results observe scientific summaries,0.5839071869850159
translation,4,6,model,model,iteratively polishes,document representation,model iteratively polishes document representation,0.6665030717849731
translation,4,6,model,document representation,on,many passes,document representation on many passes,0.5636765360832214
translation,4,6,model,many passes,through,document,many passes through document,0.7307002544403076
translation,4,6,model,model,introduce,model,model introduce model,0.6799194812774658
translation,4,6,model,model,iteratively polishes,document representation,model iteratively polishes document representation,0.6665030717849731
translation,4,7,model,selective reading mechanism,decides,more accurately,selective reading mechanism decides more accurately,0.6596737504005432
translation,4,7,model,more accurately,extent to which,each sentence,more accurately extent to which each sentence,0.6339324116706848
translation,4,7,model,each sentence,in,model,each sentence in model,0.5084435343742371
translation,4,7,model,model,introduce,selective reading mechanism,model introduce selective reading mechanism,0.6210695505142212
translation,4,24,model,iterative text summarization ( its ),consisting of,novel   iteration mechanism,iterative text summarization ( its ) consisting of novel   iteration mechanism,0.7224747538566589
translation,4,24,model,iterative text summarization ( its ),consisting of,selective reading module,iterative text summarization ( its ) consisting of selective reading module,0.7547616362571716
translation,4,24,model,model,call,iterative text summarization ( its ),model call iterative text summarization ( its ),0.6217030882835388
translation,4,25,model,iterative process,reading through,document,iterative process reading through document,0.7467613220214844
translation,4,25,model,document,has,many times,document has many times,0.6058955192565918
translation,4,25,model,model,is,iterative process,model is iterative process,0.6009126305580139
translation,4,29,model,selective reading module,design,modi-fied version,selective reading module design modi-fied version,0.5927808880805969
translation,4,29,model,selective reading module,is,modi-fied version,selective reading module is modi-fied version,0.5694995522499084
translation,4,29,model,modi-fied version,of,gated recurrent unit ( gru ) network,modi-fied version of gated recurrent unit ( gru ) network,0.5764360427856445
translation,4,29,model,hidden state,of,each sentence should be retained or updated,hidden state of each sentence should be retained or updated,0.5698420405387878
translation,4,29,model,hidden state,relationship with,document,hidden state relationship with document,0.6628212332725525
translation,4,29,model,model,has,selective reading module,model has selective reading module,0.5651887059211731
translation,4,30,model,iteration based summary generator,uses,sequence classifier,iteration based summary generator uses sequence classifier,0.6095372438430786
translation,4,30,model,sequence classifier,to extract,salient sentences,sequence classifier to extract salient sentences,0.6769620776176453
translation,4,30,model,salient sentences,from,documents,salient sentences from documents,0.5486743450164795
translation,4,30,model,iterative text summarization,has,its ),iterative text summarization has its ),0.6230436563491821
translation,4,30,model,iterative text summarization,has,iteration based summary generator,iterative text summarization has iteration based summary generator,0.5614545345306396
translation,4,30,model,its ),has,iteration based summary generator,its ) has iteration based summary generator,0.5898964405059814
translation,4,30,model,model,propose,iterative text summarization,model propose iterative text summarization,0.6680012941360474
translation,4,31,model,novel iterative neural network model,repeatedly polishes,distributed representation of document,novel iterative neural network model repeatedly polishes distributed representation of document,0.7254268527030945
translation,4,31,model,model,introduce,novel iterative neural network model,model introduce novel iterative neural network model,0.6026782393455505
translation,4,32,model,selective reading mechanism,decides,information,selective reading mechanism decides information,0.6835288405418396
translation,4,32,model,information,should be updated,each sentence,information should be updated each sentence,0.6169450879096985
translation,4,32,model,each sentence,based on,relationship,each sentence based on relationship,0.704789936542511
translation,4,32,model,relationship,with,polished document representation,relationship with polished document representation,0.634113609790802
translation,4,32,model,model,propose,selective reading mechanism,model propose selective reading mechanism,0.6322012543678284
translation,5,164,baselines,gmds,selects,important sentences,gmds selects important sentences,0.6931995153427124
translation,5,164,baselines,sentence connectivity graph,based on,cosine similarity,sentence connectivity graph based on cosine similarity,0.6481387615203857
translation,5,164,baselines,important sentences,based on,concept of eigenvector centrality,important sentences based on concept of eigenvector centrality,0.6389627456665039
translation,5,164,baselines,baselines,has,gmds,baselines has gmds,0.5464202165603638
translation,5,165,baselines,baselines,has,chieu,baselines has chieu,0.6105822324752808
translation,5,166,baselines,etts,has,etts,etts has etts,0.5958828330039978
translation,5,166,baselines,baselines,has,etts,baselines has etts,0.5453791618347168
translation,5,166,baselines,baselines,has,etts,baselines has etts,0.5453791618347168
translation,5,6,model,etts,return,news evolution,etts return news evolution,0.7353909611701965
translation,5,6,model,news evolution,along,timeline,news evolution along timeline,0.6271135210990906
translation,5,6,model,news evolution,consisting of,individual but correlated summaries,news evolution consisting of individual but correlated summaries,0.6682659387588501
translation,5,6,model,individual but correlated summaries,on,each date,individual but correlated summaries on each date,0.5671240091323853
translation,5,69,model,trans-temporal summarization,based on,global biased graph,trans-temporal summarization based on global biased graph,0.6023169159889221
translation,5,69,model,trans-temporal summarization,based on,local biased graph,trans-temporal summarization based on local biased graph,0.5932334661483765
translation,5,69,model,global biased graph,using,inter-date dependency,global biased graph using inter-date dependency,0.6715509295463562
translation,5,69,model,global biased graph,using,intra-date dependency,global biased graph using intra-date dependency,0.6544909477233887
translation,5,69,model,global biased graph,using,intra-date dependency,global biased graph using intra-date dependency,0.6544909477233887
translation,5,69,model,local biased graph,using,intra-date dependency,local biased graph using intra-date dependency,0.6420747637748718
translation,5,69,model,model,conduct,trans-temporal summarization,model conduct trans-temporal summarization,0.6387661695480347
translation,5,167,results,reftl,used,multiple human timelines,reftl used multiple human timelines,0.639703094959259
translation,5,167,results,multiple human timelines,of,human timelines,multiple human timelines of human timelines,0.5870417356491089
translation,5,167,results,multiple human timelines,provides,good indicator,multiple human timelines provides good indicator,0.6514626741409302
translation,5,167,results,evaluations,of,competing systems,evaluations of competing systems,0.6047686338424683
translation,5,167,results,human timelines,against,each other,human timelines against each other,0.6984662413597107
translation,5,167,results,human timelines,provides,good indicator,human timelines provides good indicator,0.6545227766036987
translation,5,167,results,results,has,reftl,results has reftl,0.6181426644325256
translation,5,172,results,random,result of,chieu,random result of chieu,0.7250384092330933
translation,5,172,results,centroid,better than,random,centroid better than random,0.7537818551063538
translation,5,172,results,chieu,better than,centroid,chieu better than centroid,0.7623868584632874
translation,5,172,results,chieu,worse than,gmds,chieu worse than gmds,0.7388814091682434
translation,5,172,results,random,has,worst performance,random has worst performance,0.5852498412132263
translation,5,179,results,both balanced algorithms,achieve,comparable performance,both balanced algorithms achieve comparable performance,0.6207123398780823
translation,5,179,results,much faster,than,ets,much faster than ets,0.6423145532608032
translation,5,179,results,comparable performance,has,0.386,comparable performance has 0.386,0.5633469223976135
translation,5,179,results,results,notice,both balanced algorithms,results notice both balanced algorithms,0.7133693099021912
translation,5,194,results,window kernel,is,worst,window kernel is worst,0.6067953109741211
translation,5,194,results,gaussian kernel,has,outperforms,gaussian kernel has outperforms,0.6199965476989746
translation,5,194,results,outperforms,has,others,outperforms has others,0.6126620769500732
translation,5,194,results,results,has,gaussian kernel,results has gaussian kernel,0.5580993890762329
translation,5,195,results,window kernel,fails to distinguish,different weights,window kernel fails to distinguish different weights,0.7229657173156738
translation,5,195,results,different weights,of,neighboring sets,different weights of neighboring sets,0.6207703948020935
translation,5,195,results,neighboring sets,by,temporal proximity,neighboring sets by temporal proximity,0.5612683296203613
translation,5,195,results,results,has,window kernel,results has window kernel,0.5549744367599487
translation,6,106,baselines,c s iiith3,is,graph - based system,c s iiith3 is graph - based system,0.6572502851486206
translation,6,106,baselines,graph - based system,assess,summaries,graph - based system assess summaries,0.6000853776931763
translation,6,106,baselines,summaries,differences in,word co-locations,summaries differences in word co-locations,0.6705167889595032
translation,6,106,baselines,word co-locations,between,generated summaries and model summaries,word co-locations between generated summaries and model summaries,0.6354081034660339
translation,6,106,baselines,baselines,has,c s iiith3,baselines has c s iiith3,0.5964348316192627
translation,6,107,baselines,be - hm,is,be system,be - hm is be system,0.6766375303268433
translation,6,107,baselines,be system,where,basic elements,be system where basic elements,0.6511696577072144
translation,6,107,baselines,basic elements,identified using,head-modifier criterion,basic elements identified using head-modifier criterion,0.6143794059753418
translation,6,107,baselines,head-modifier criterion,on,parse results,head-modifier criterion on parse results,0.4908590614795685
translation,6,107,baselines,parse results,from,minipar,parse results from minipar,0.5276028513908386
translation,6,107,baselines,baselines,has,be - hm,baselines has be - hm,0.6549987196922302
translation,6,95,results,two bigram based variants,observe that,rouge -we - 2,two bigram based variants observe that rouge -we - 2,0.5539662837982178
translation,6,95,results,rouge -we - 2,improves on,rouge - 2,rouge -we - 2 improves on rouge - 2,0.7628482580184937
translation,6,95,results,rouge - 2,has,most of the time,rouge - 2 has most of the time,0.6224378347396851
translation,6,109,results,rouge -we - 1,displays,better correlations,rouge -we - 1 displays better correlations,0.6781900525093079
translation,6,109,results,better correlations,with,pyramid scores,better correlations with pyramid scores,0.6343119740486145
translation,6,109,results,pyramid scores,than,top system,pyramid scores than top system,0.5703248381614685
translation,6,109,results,top system,in,aesop 2011,top system in aesop 2011,0.5616148710250854
translation,6,109,results,results,see that,rouge -we - 1,results see that rouge -we - 1,0.6486640572547913
translation,7,5,model,confusion networks,to improve,summarization performance,confusion networks to improve summarization performance,0.6408419609069824
translation,7,5,model,summarization performance,on,asr condition,summarization performance on asr condition,0.5583660006523132
translation,7,5,model,asr condition,under,unsupervised framework,asr condition under unsupervised framework,0.7153034806251526
translation,7,5,model,unsupervised framework,considering,more word candidates,unsupervised framework considering more word candidates,0.658284604549408
translation,7,5,model,unsupervised framework,considering,confidence scores,unsupervised framework considering confidence scores,0.6726875901222229
translation,7,18,model,feasibility,of using,confusion networks,feasibility of using confusion networks,0.7268244028091431
translation,7,18,model,confusion networks,under,unsupervised mmr ( maximum marginal relevance ) framework,confusion networks under unsupervised mmr ( maximum marginal relevance ) framework,0.5910376310348511
translation,7,18,model,unsupervised mmr ( maximum marginal relevance ) framework,to improve,summarization performance,unsupervised mmr ( maximum marginal relevance ) framework to improve summarization performance,0.6434816718101501
translation,7,18,model,model,demonstrate,feasibility,model demonstrate feasibility,0.6521939635276794
translation,7,126,results,confusion networks,include,much more correct words,confusion networks include much more correct words,0.5864571332931519
translation,7,126,results,much more correct words,than,1 - best hypotheses,much more correct words than 1 - best hypotheses,0.5726526379585266
translation,7,126,results,results,see that,confusion networks,results see that confusion networks,0.6345071196556091
translation,7,139,results,fewer word candidates,are,pruned,fewer word candidates are pruned,0.5937890410423279
translation,7,139,results,higher node density,has,pruned cns,higher node density has pruned cns,0.5986133217811584
translation,7,139,results,results,has,fewer word candidates,results has fewer word candidates,0.5585359930992126
translation,7,141,results,dev set,using,1 - best hypothesis,dev set using 1 - best hypothesis,0.7134180068969727
translation,7,141,results,dev set,using,human transcripts,dev set using human transcripts,0.6698982119560242
translation,7,141,results,results,on,dev set,results on dev set,0.6656746864318848
translation,7,148,results,two testing conditions,see,performance degradation,two testing conditions see performance degradation,0.6064708828926086
translation,7,148,results,performance degradation,due to,recognition errors,performance degradation due to recognition errors,0.7102609276771545
translation,7,148,results,results,for,two testing conditions,results for two testing conditions,0.5629398822784424
translation,7,150,results,dev set,using,cns,dev set using cns,0.7186970114707947
translation,7,150,results,results,on,dev set,results on dev set,0.6656746864318848
translation,7,151,results,results,has,effect of segmentation representation,results has effect of segmentation representation,0.5817410945892334
translation,7,158,results,confusion networks,improves,summarization performance,confusion networks improves summarization performance,0.6548129320144653
translation,7,158,results,summarization performance,comparing with,baseline,summarization performance comparing with baseline,0.6776465773582458
translation,7,158,results,results,using,confusion networks,results using confusion networks,0.6285943984985352
translation,7,160,results,more improvement,using,our proposed method,more improvement using our proposed method,0.6780523061752319
translation,7,160,results,our proposed method,for,tfidf calculation,our proposed method for tfidf calculation,0.6148489117622375
translation,7,160,results,tfidf calculation,based on,posterior probabilities,tfidf calculation based on posterior probabilities,0.6840434074401855
translation,7,160,results,posterior probabilities,especially,rouge - 2 scores,posterior probabilities especially rouge - 2 scores,0.6360630393028259
translation,7,161,results,1,-,best hypotheses,1 - best hypotheses,0.5956432223320007
translation,7,161,results,best hypotheses,obtain,very competitive results,best hypotheses obtain very competitive results,0.542462170124054
translation,7,161,results,posteriors,obtain,very competitive results,posteriors obtain very competitive results,0.5386823415756226
translation,7,161,results,1,has,best hypotheses,1 has best hypotheses,0.5091912746429443
translation,7,163,results,posteriors,help,rouge - 1 and rouge - 2 scores,posteriors help rouge - 1 and rouge - 2 scores,0.6668212413787842
translation,7,163,results,posteriors,when using,confusion networks,posteriors when using confusion networks,0.6848406195640564
translation,7,163,results,posterior probabilities,improves,rouge - 2 results,posterior probabilities improves rouge - 2 results,0.6886464953422546
translation,7,163,results,posterior probabilities,not,rouge -1,posterior probabilities not rouge -1,0.7129983901977539
translation,7,163,results,top hypotheses representation,has,posteriors,top hypotheses representation has posteriors,0.535067617893219
translation,7,163,results,results,When using,top hypotheses representation,results When using top hypotheses representation,0.7291972637176514
translation,7,164,results,more candidates,in,vector representation,more candidates in vector representation,0.5097523331642151
translation,7,164,results,more candidates,does not necessarily help,summarization,more candidates does not necessarily help summarization,0.679037868976593
translation,7,164,results,results,adding,more candidates,results adding more candidates,0.7516127228736877
translation,7,165,results,pruned cns,yields,better performance,pruned cns yields better performance,0.7328831553459167
translation,7,165,results,better performance,than,non-pruned ones,better performance than non-pruned ones,0.606580913066864
translation,7,165,results,results,Using,pruned cns,results Using pruned cns,0.6664290428161621
translation,7,167,results,best results,achieved by,pruned cns,best results achieved by pruned cns,0.7324045300483704
translation,7,167,results,best rouge - 1 result,without using,posterior probabilities,best rouge - 1 result without using posterior probabilities,0.7125696539878845
translation,7,167,results,best rouge - 2 scores,when using,posteriors,best rouge - 2 scores when using posteriors,0.6811200976371765
translation,7,167,results,pruned cns,has,best rouge - 1 result,pruned cns has best rouge - 1 result,0.5975799560546875
translation,7,167,results,pruned cns,has,best rouge - 2 scores,pruned cns has best rouge - 2 scores,0.5944509506225586
translation,7,167,results,results,has,best results,results has best results,0.542218804359436
translation,7,176,results,summaries,formed using,human transcripts,summaries formed using human transcripts,0.6542096138000488
translation,7,176,results,human transcripts,are,much better,human transcripts are much better,0.5931850671768188
translation,7,176,results,results,see that,summaries,results see that summaries,0.6203921437263489
translation,7,179,results,difference,between using,1 - best hypothesis,difference between using 1 - best hypothesis,0.7500593662261963
translation,7,179,results,difference,between using,human transcripts,difference between using human transcripts,0.7873163819313049
translation,7,179,results,greatly reduced,using,new summary formulation,greatly reduced using new summary formulation,0.6978128552436829
translation,7,179,results,results,notice,difference,results notice difference,0.7241901159286499
translation,7,181,results,best hypotheses,with,posterior probabilities,best hypotheses with posterior probabilities,0.6107683181762695
translation,7,181,results,best hypotheses,obtain,similar rouge - 1 score,best hypotheses obtain similar rouge - 1 score,0.5324222445487976
translation,7,181,results,best hypotheses,obtain,little higher rouge - 2 score,best hypotheses obtain little higher rouge - 2 score,0.5328489542007446
translation,7,181,results,results,Using,best hypotheses,results Using best hypotheses,0.6553879976272583
translation,7,181,results,results,using,human transcripts,results using human transcripts,0.6141425967216492
translation,7,183,results,posterior probabilities,for,term weighting,posterior probabilities for term weighting,0.600579023361206
translation,7,183,results,rouge scores,worse than,most of other conditions,rouge scores worse than most of other conditions,0.77641361951828
translation,7,183,results,non-pruned cns,has,rouge scores,non-pruned cns has rouge scores,0.5909008383750916
translation,7,183,results,posterior probabilities,has,rouge scores,posterior probabilities has rouge scores,0.5695615410804749
translation,7,183,results,term weighting,has,rouge scores,term weighting has rouge scores,0.5458508729934692
translation,7,183,results,results,when using,non-pruned cns,results when using non-pruned cns,0.6936882734298706
translation,7,203,results,confidence scores and confusion networks,improve,summarization performance,confidence scores and confusion networks improve summarization performance,0.6543203592300415
translation,7,203,results,summarization performance,comparing with,baseline,summarization performance comparing with baseline,0.6776465773582458
translation,7,203,results,results,Using,confidence scores and confusion networks,results Using confidence scores and confusion networks,0.6328476071357727
translation,7,204,results,performance improvements,from,  best hyp  ,performance improvements from   best hyp  ,0.5430907011032104
translation,7,204,results,performance improvements,from,best hyp ( wp )  ,performance improvements from best hyp ( wp )  ,0.5025472044944763
translation,7,204,results,performance improvements,from,to   pruned cns  ,performance improvements from to   pruned cns  ,0.6121187210083008
translation,7,204,results,rouge - 1 and rouge - 2 measures,are,statistically significant,rouge - 1 and rouge - 2 measures are statistically significant,0.5847999453544617
translation,7,204,results,statistically significant,according to,paired t-test ( p < 0.05 ),statistically significant according to paired t-test ( p < 0.05 ),0.7005499601364136
translation,7,204,results,  best hyp  ,has,to   best hyp ( wp ),  best hyp   has to   best hyp ( wp ),0.5956196188926697
translation,7,204,results,best hyp ( wp )  ,has,to   pruned cns  ,best hyp ( wp )   has to   pruned cns  ,0.6017054915428162
translation,7,204,results,results,has,performance improvements,results has performance improvements,0.5816104412078857
translation,7,205,results,final summary,presented using,human transcripts,final summary presented using human transcripts,0.6642609238624573
translation,7,205,results,final summary,presented using,human transcripts,final summary presented using human transcripts,0.6642609238624573
translation,7,205,results,final summary,observe,slightly better results,final summary observe slightly better results,0.584493100643158
translation,7,205,results,human transcripts,of,selected segments,human transcripts of selected segments,0.6047919988632202
translation,7,205,results,human transcripts,as input for,summarization,human transcripts as input for summarization,0.7125214338302612
translation,7,205,results,slightly better results,using,pruned cns,slightly better results using pruned cns,0.7192891240119934
translation,7,205,results,pruned cns,than using,human transcripts,pruned cns than using human transcripts,0.708415150642395
translation,7,205,results,human transcripts,as input for,summarization,human transcripts as input for summarization,0.7125214338302612
translation,7,211,results,summarization performance,on,rouge - 1 and rouge - 2 scores,summarization performance on rouge - 1 and rouge - 2 scores,0.5111644864082336
translation,7,211,results,results,on,icsi meeting corpus,results on icsi meeting corpus,0.5031477212905884
translation,7,212,results,pruned cns,obtain,further improvement,pruned cns obtain further improvement,0.6023256778717041
translation,7,212,results,results,using,pruned cns,results using pruned cns,0.6664290428161621
translation,7,213,results,more gain,in,rouge - 2 results,more gain in rouge - 2 results,0.5467710494995117
translation,7,213,results,rouge - 2 results,yielded by,our proposed soft term weighting method,rouge - 2 results yielded by our proposed soft term weighting method,0.6223011016845703
translation,7,213,results,our proposed soft term weighting method,based on,posterior probabilities,our proposed soft term weighting method based on posterior probabilities,0.6309999823570251
translation,7,213,results,results,found that,more gain,results found that more gain,0.7177062034606934
translation,8,165,ablation-analysis,automatic termination mechanism,with,fixed extracting strategy,automatic termination mechanism with fixed extracting strategy,0.592559278011322
translation,8,165,ablation-analysis,ablation analysis,replace,automatic termination mechanism,ablation analysis replace automatic termination mechanism,0.560882031917572
translation,8,136,baselines,nn - se,"and Lapata , 2016 )","sum-marunner ( nallapati et al. , 2017 )","nn - se and Lapata , 2016 ) sum-marunner ( nallapati et al. , 2017 )",0.7156093716621399
translation,8,136,baselines,"sum-marunner ( nallapati et al. , 2017 )",are,sequence labeling task,"sum-marunner ( nallapati et al. , 2017 ) are sequence labeling task",0.5303741097450256
translation,8,136,baselines,"sum-marunner ( nallapati et al. , 2017 )",trained with,cross-entropy loss,"sum-marunner ( nallapati et al. , 2017 ) trained with cross-entropy loss",0.7030293345451355
translation,8,136,baselines,baselines,has,nn - se,baselines has nn - se,0.536815345287323
translation,8,137,baselines,"rnes ( wu and hu , 2018 )",extract,summary,"rnes ( wu and hu , 2018 ) extract summary",0.716373860836029
translation,8,137,baselines,summary,via,reinforcement learning,summary via reinforcement learning,0.6003264784812927
translation,8,137,baselines,"refresh ( narayan et al. , 2018 )",has,"dqn ( yao et al. , 2018 )","refresh ( narayan et al. , 2018 ) has dqn ( yao et al. , 2018 )",0.5647528171539307
translation,8,137,baselines,baselines,has,"refresh ( narayan et al. , 2018 )","baselines has refresh ( narayan et al. , 2018 )",0.5194606781005859
translation,8,138,baselines,extractive summarization,as,contextual bandit,extractive summarization as contextual bandit,0.5270380973815918
translation,8,163,experiments,better,than,other baselines,better than other baselines,0.5905631184577942
translation,8,140,hyperparameters,word embeddings,with,100 - dimension glove embeddings,word embeddings with 100 - dimension glove embeddings,0.584903359413147
translation,8,140,hyperparameters,hyperparameters,initialize,word embeddings,hyperparameters initialize word embeddings,0.6877135038375854
translation,8,141,hyperparameters,encoder,is,hierarchical,encoder is hierarchical,0.6181342601776123
translation,8,141,hyperparameters,twostacked bilstm,with,hidden size,twostacked bilstm with hidden size,0.6226452589035034
translation,8,141,hyperparameters,hidden size,of,200,hidden size of 200,0.6639924049377441
translation,8,141,hyperparameters,rough reading,has,encoder,rough reading has encoder,0.5524535775184631
translation,8,141,hyperparameters,rough reading,has,each layer,rough reading has each layer,0.6055725812911987
translation,8,141,hyperparameters,hyperparameters,In,rough reading,hyperparameters In rough reading,0.5064271092414856
translation,8,143,hyperparameters,variant cnn,adopt,filter windows h,variant cnn adopt filter windows h,0.6722888350486755
translation,8,143,hyperparameters,filter windows h,in,"{ 1 , 2 , 3 }","filter windows h in { 1 , 2 , 3 }",0.551396369934082
translation,8,143,hyperparameters,filter windows h,with,100 feature maps each,filter windows h with 100 feature maps each,0.6245379447937012
translation,8,143,hyperparameters,k = 3 local representations,for,each document,k = 3 local representations for each document,0.5666062831878662
translation,8,143,hyperparameters,hyperparameters,For,variant cnn,hyperparameters For variant cnn,0.5582033395767212
translation,8,145,hyperparameters,minimum and maximum number,of,selected sentence,minimum and maximum number of selected sentence,0.603108823299408
translation,8,145,hyperparameters,selected sentence,to be,1 and 10,selected sentence to be 1 and 10,0.613091230392456
translation,8,145,hyperparameters,1 and 10,for,termination mechanism,1 and 10 for termination mechanism,0.6516284942626953
translation,8,145,hyperparameters,hyperparameters,bound,minimum and maximum number,hyperparameters bound minimum and maximum number,0.7383003234863281
translation,8,146,hyperparameters,training,use,"optimizer adam ( kingma and ba , 2014 )","training use optimizer adam ( kingma and ba , 2014 )",0.5793353319168091
translation,8,146,hyperparameters,"optimizer adam ( kingma and ba , 2014 )",with,learning rate,"optimizer adam ( kingma and ba , 2014 ) with learning rate",0.5900667905807495
translation,8,146,hyperparameters,"optimizer adam ( kingma and ba , 2014 )",with,beta parameters,"optimizer adam ( kingma and ba , 2014 ) with beta parameters",0.5861591696739197
translation,8,146,hyperparameters,"optimizer adam ( kingma and ba , 2014 )",with,weight decay,"optimizer adam ( kingma and ba , 2014 ) with weight decay",0.5890341997146606
translation,8,146,hyperparameters,learning rate,of,10 ?5,learning rate of 10 ?5,0.6583234071731567
translation,8,146,hyperparameters,beta parameters,as,"( 0 , 0.999 )","beta parameters as ( 0 , 0.999 )",0.4935036301612854
translation,8,146,hyperparameters,weight decay,of,10 ?6,weight decay of 10 ?6,0.6106958389282227
translation,8,146,hyperparameters,weight decay,to maximize,objective function,weight decay to maximize objective function,0.7009053826332092
translation,8,146,hyperparameters,10 ?6,to maximize,objective function,10 ?6 to maximize objective function,0.692582368850708
translation,8,146,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
translation,8,147,hyperparameters,gradient clipping,of,1,gradient clipping of 1,0.5709764957427979
translation,8,147,hyperparameters,gradient clipping,for,regularization,gradient clipping for regularization,0.5693403482437134
translation,8,147,hyperparameters,gradient clipping,sample,b = 20 times,gradient clipping sample b = 20 times,0.6391040086746216
translation,8,147,hyperparameters,1,for,regularization,1 for regularization,0.6135506629943848
translation,8,147,hyperparameters,b = 20 times,for,each document,b = 20 times for each document,0.6278284192085266
translation,8,147,hyperparameters,hyperparameters,employ,gradient clipping,hyperparameters employ gradient clipping,0.5360305309295654
translation,8,147,hyperparameters,hyperparameters,sample,b = 20 times,hyperparameters sample b = 20 times,0.6325705051422119
translation,8,148,hyperparameters,our model,within,two epochs,our model within two epochs,0.663029134273529
translation,8,148,hyperparameters,hyperparameters,train,our model,hyperparameters train our model,0.6687315702438354
translation,8,6,model,novel approach,for,extractive summarization,novel approach for extractive summarization,0.6077476143836975
translation,8,7,model,problem,as,contextualbandit problem,problem as contextualbandit problem,0.5184161067008972
translation,8,7,model,problem,solve it with,policy gradient,problem solve it with policy gradient,0.6895801424980164
translation,8,7,model,contextualbandit problem,solve it with,policy gradient,contextualbandit problem solve it with policy gradient,0.6345238089561462
translation,8,7,model,model,formulate,problem,model formulate problem,0.7224826812744141
translation,8,8,model,convolutional neural network,to encode,gist,convolutional neural network to encode gist,0.6863188743591309
translation,8,8,model,gist,of,paragraphs,gist of paragraphs,0.6546829342842102
translation,8,8,model,paragraphs,for,rough reading,paragraphs for rough reading,0.6485862731933594
translation,8,8,model,decision making policy,with,adapted termination mechanism,decision making policy with adapted termination mechanism,0.6211242079734802
translation,8,8,model,adapted termination mechanism,for,careful reading,adapted termination mechanism for careful reading,0.6036781072616577
translation,8,8,model,model,adopt,convolutional neural network,model adopt convolutional neural network,0.5579025149345398
translation,8,8,model,model,adopt,decision making policy,model adopt decision making policy,0.6984776854515076
translation,8,34,model,rough reading process,use,hierarchical neural network,rough reading process use hierarchical neural network,0.5347501039505005
translation,8,34,model,hierarchical neural network,to encode,sentence vectors,hierarchical neural network to encode sentence vectors,0.7188394665718079
translation,8,34,model,hierarchical neural network,derive,document representation,hierarchical neural network derive document representation,0.5341492295265198
translation,8,34,model,document representation,as,global feature,document representation as global feature,0.5004821419715881
translation,8,34,model,model,implement,rough reading process,model implement rough reading process,0.5815376043319702
translation,8,35,model,convolutional neural network ( cnn ),to encode,local features,convolutional neural network ( cnn ) to encode local features,0.711592435836792
translation,8,35,model,local features,from,different paragraphs,local features from different paragraphs,0.5358300805091858
translation,8,35,model,model,has,convolutional neural network ( cnn ),model has convolutional neural network ( cnn ),0.5491653084754944
translation,8,157,results,our model,performs,best,our model performs best,0.6359259486198425
translation,8,157,results,our model,even surpasses,42,our model even surpasses 42,0.7077839970588684
translation,8,157,results,42,on,rouge - 1 score,42 on rouge - 1 score,0.5526478290557861
translation,8,157,results,42,on,combined cnn / dailymail dataset,42 on combined cnn / dailymail dataset,0.47529590129852295
translation,8,157,results,results,has,our model,results has our model,0.5871725678443909
translation,8,158,results,better results,on,separate datasets,better results on separate datasets,0.531181275844574
translation,8,158,results,results,shows,better results,results shows better results,0.7093518376350403
translation,9,226,ablation-analysis,correlations,show,high,correlations show high,0.6728308200836182
translation,9,226,ablation-analysis,correlations,show,low compression ratio,correlations show low compression ratio,0.6830448508262634
translation,9,226,ablation-analysis,low compression ratio,w.r.t to,dates,low compression ratio w.r.t to dates,0.6676226854324341
translation,9,226,ablation-analysis,high,has,number of articles and publication dates,high has number of articles and publication dates,0.5013719201087952
translation,9,226,ablation-analysis,decreases,has,performance,decreases has performance,0.5981842875480652
translation,9,226,ablation-analysis,ablation analysis,has,correlations,ablation analysis has correlations,0.5515233874320984
translation,9,238,ablation-analysis,all metrics,has,decreases,all metrics has decreases,0.6171730160713196
translation,9,238,ablation-analysis,ablation analysis,forcing,datewise,ablation analysis forcing datewise,0.788271427154541
translation,9,19,baselines,direct summarization,treats,tls,direct summarization treats tls,0.669264554977417
translation,9,63,baselines,ranking dates,by,number of sentences that mention the date,ranking dates by number of sentences that mention the date,0.5269489288330078
translation,9,63,baselines,mentioncount,has,ranking dates,mentioncount has ranking dates,0.5860813856124878
translation,9,63,baselines,baselines,has,mentioncount,baselines has mentioncount,0.5749873518943787
translation,9,64,baselines,classification or regression,to predict,date,classification or regression to predict date,0.6575691103935242
translation,9,89,baselines,textrank,Runs,pagerank,textrank Runs pagerank,0.5976917147636414
translation,9,89,baselines,pagerank,on,graph,pagerank on graph,0.5950300097465515
translation,9,89,baselines,graph,of,pairwise sentences similarities,graph of pairwise sentences similarities,0.5684682130813599
translation,9,89,baselines,graph,of,similarity,graph of similarity,0.5865376591682434
translation,9,89,baselines,graph,to rank,sentences,graph to rank sentences,0.7492813467979431
translation,9,89,baselines,pairwise sentences similarities,to rank,sentences,pairwise sentences similarities to rank sentences,0.6275487542152405
translation,9,89,baselines,sentences,by,similarity,sentences by similarity,0.5729074478149414
translation,9,89,baselines,centroid-rank,Ranks,sentences,centroid-rank Ranks sentences,0.7236430644989014
translation,9,89,baselines,sentences,by,similarity,sentences by similarity,0.5729074478149414
translation,9,89,baselines,similarity,to,centroid of all sentences,similarity to centroid of all sentences,0.5360180139541626
translation,9,89,baselines,similarity,to,centroid of all sentences,similarity to centroid of all sentences,0.5360180139541626
translation,9,89,baselines,similarity,to,centroid of all sentences,similarity to centroid of all sentences,0.5360180139541626
translation,9,89,baselines,centroid-opt,Greedily optimises,summary,centroid-opt Greedily optimises summary,0.7463361620903015
translation,9,89,baselines,summary,to be,similar,summary to be similar,0.6463822722434998
translation,9,89,baselines,similar,to,centroid of all sentences,similar to centroid of all sentences,0.5621709823608398
translation,9,90,baselines,submodular,optimizes,summary,submodular optimizes summary,0.724789559841156
translation,9,90,baselines,summary,using,submodular objective functions,summary using submodular objective functions,0.5565283298492432
translation,9,90,baselines,submodular,has,greedily,submodular has greedily,0.6523844599723816
translation,9,90,baselines,baselines,has,submodular,baselines has submodular,0.5757169723510742
translation,9,122,baselines,datementioncount,Rank,how often,datementioncount Rank how often,0.6998030543327332
translation,9,122,baselines,datementioncount,Rank,cluster date,datementioncount Rank cluster date,0.7499571442604065
translation,9,122,baselines,cluster date,mentioned throughout,input collection,cluster date mentioned throughout input collection,0.6456037163734436
translation,9,122,baselines,how often,has,cluster date,how often has cluster date,0.5731452703475952
translation,9,122,baselines,baselines,has,datementioncount,baselines has datementioncount,0.5551773905754089
translation,9,123,baselines,rank,using,score,rank using score,0.6725935935974121
translation,9,123,baselines,score,by,regression model,score by regression model,0.604511559009552
translation,9,123,baselines,importance scores,of,clusters,importance scores of clusters,0.587762713432312
translation,9,123,baselines,regression,has,rank,regression has rank,0.6152742505073547
translation,9,123,baselines,baselines,has,regression,baselines has regression,0.593812882900238
translation,9,189,baselines,baselines,has,direct summarization,baselines has direct summarization,0.5547390580177307
translation,9,194,baselines,approach,using,regression,approach using regression,0.7832804322242737
translation,9,194,baselines,regression,for,date selection and summarization,regression for date selection and summarization,0.6346387267112732
translation,9,194,baselines,regression,both,date selection and summarization,regression both date selection and summarization,0.6983872652053833
translation,9,194,baselines,all sentences,of,date,all sentences of date,0.6101652979850769
translation,9,194,baselines,all sentences,as,candidate sentences,all sentences as candidate sentences,0.5089102983474731
translation,9,195,baselines,simple date- wise baseline,uses,publication count,simple date- wise baseline uses publication count,0.6277576684951782
translation,9,195,baselines,simple date- wise baseline,uses,all sentences,simple date- wise baseline uses all sentences,0.565427303314209
translation,9,195,baselines,publication count,to rank,dates,publication count to rank dates,0.5457420945167542
translation,9,195,baselines,all sentences,published on,date,all sentences published on date,0.4948943555355072
translation,9,195,baselines,date,for,candidate selection,date for candidate selection,0.6317135095596313
translation,9,195,baselines,pubcount,has,simple date- wise baseline,pubcount has simple date- wise baseline,0.5572829246520996
translation,9,195,baselines,baselines,has,pubcount,baselines has pubcount,0.5799317359924316
translation,9,196,baselines,centroid -opt,for,summarization,centroid -opt for summarization,0.62967848777771
translation,9,199,baselines,clustering,use,datementioncount,clustering use datementioncount,0.6629269123077393
translation,9,199,baselines,datementioncount,to rank,clusters,datementioncount to rank clusters,0.7191263437271118
translation,9,199,baselines,centroid -opt,for,summarization,centroid -opt for summarization,0.62967848777771
translation,9,203,baselines,text oracle,Uses,regression,text oracle Uses regression,0.6500148177146912
translation,9,203,baselines,text oracle,constructs,summary,text oracle constructs summary,0.6923997402191162
translation,9,203,baselines,regression,to select,dates,regression to select dates,0.7421571016311646
translation,9,203,baselines,summary,for,each date,summary for each date,0.639543890953064
translation,9,203,baselines,summary,by optimizing,rouge,summary by optimizing rouge,0.692771315574646
translation,9,203,baselines,each date,by optimizing,rouge,each date by optimizing rouge,0.7184895873069763
translation,9,203,baselines,rouge,to,groundtruth summaries,rouge to groundtruth summaries,0.536707878112793
translation,9,203,baselines,baselines,has,text oracle,baselines has text oracle,0.5681461095809937
translation,9,7,experiments,new tls dataset,spans,longer time periods,new tls dataset spans longer time periods,0.7012139558792114
translation,9,7,experiments,longer time periods,than,previous datasets,longer time periods than previous datasets,0.576091468334198
translation,9,22,model,simple method,to improve,date summarization,simple method to improve date summarization,0.6926720142364502
translation,9,22,model,date summarization,for,date-wise approach,date summarization for date-wise approach,0.6227399110794067
translation,9,22,model,model,propose,simple method,model propose simple method,0.6824520826339722
translation,9,6,results,different tls strategies,propose,simple and effective combination of methods,different tls strategies propose simple and effective combination of methods,0.577078104019165
translation,9,6,results,improves,over,stateof - the - art,improves over stateof - the - art,0.6470912098884583
translation,9,6,results,stateof - the - art,on,all tested benchmarks,stateof - the - art on all tested benchmarks,0.46968719363212585
translation,9,6,results,results,compare,different tls strategies,results compare different tls strategies,0.6054186224937439
translation,9,25,results,event-based approach,via,clustering,event-based approach via clustering,0.7018734216690063
translation,9,25,results,results,propose,event-based approach,results propose event-based approach,0.6485742926597595
translation,9,30,results,new tls dataset,larger than,previous datasets,new tls dataset larger than previous datasets,0.6682315468788147
translation,9,30,results,new tls dataset,spans,longer time ranges ( decades of news timelines ),new tls dataset spans longer time ranges ( decades of news timelines ),0.7116672396659851
translation,9,30,results,results,present,new tls dataset,results present new tls dataset,0.6474469304084778
translation,9,213,results,all other methods,on,all tested datasets,all other methods on all tested datasets,0.4280775189399719
translation,9,213,results,all tested datasets,in,alignment - based rouge metrics,all tested datasets in alignment - based rouge metrics,0.4837180972099304
translation,9,213,results,datewise,has,consistently outperforms,datewise has consistently outperforms,0.6202452182769775
translation,9,213,results,consistently outperforms,has,all other methods,consistently outperforms has all other methods,0.5493871569633484
translation,9,214,results,date - f1 metric,close to,other methods,date - f1 metric close to other methods,0.6771510243415833
translation,9,214,results,advantage,of,datewise,advantage of datewise,0.6369355320930481
translation,9,214,results,datewise,due to,sentence selection,datewise due to sentence selection,0.6576159000396729
translation,9,214,results,datewise,due to,summarization,datewise due to summarization,0.6807587742805481
translation,9,214,results,results,has,date - f1 metric,results has date - f1 metric,0.5923582315444946
translation,9,217,results,clust,performs,worse,clust performs worse,0.6645942330360413
translation,9,217,results,worse,than,datewise and martschat2018,worse than datewise and martschat2018,0.6450313925743103
translation,9,217,results,worse,except on,entities,worse except on entities,0.7500698566436768
translation,9,217,results,datewise and martschat2018,except on,entities,datewise and martschat2018 except on entities,0.669245183467865
translation,9,217,results,results,has,clust,results has clust,0.5847872495651245
translation,9,218,results,clust,merges,articles,clust merges articles,0.7991132736206055
translation,9,218,results,articles,from,close dates together,articles from close dates together,0.5581361055374146
translation,9,218,results,articles,would belong to,separate events,articles would belong to separate events,0.7024515271186829
translation,9,218,results,separate events,on,ground - truth timelines,separate events on ground - truth timelines,0.5338815450668335
translation,9,218,results,other two datasets,has,clust,other two datasets has clust,0.6266402006149292
translation,9,218,results,results,for,other two datasets,results for other two datasets,0.5898985862731934
translation,9,244,results,titles,increases,ar precision,titles increases ar precision,0.705964207649231
translation,9,244,results,ar precision,at the cost of,recall,ar precision at the cost of recall,0.6328951716423035
translation,9,244,results,results,Using,titles,results Using titles,0.5619589686393738
translation,10,115,ablation-analysis,both structured views ( topic view + stage view ),increased,rouge scores,both structured views ( topic view + stage view ) increased rouge scores,0.6541585326194763
translation,10,115,ablation-analysis,ablation analysis,utilizing,both structured views ( topic view + stage view ),ablation analysis utilizing both structured views ( topic view + stage view ),0.6784874796867371
translation,10,159,ablation-analysis,percentage,of,24 %,percentage of 24 %,0.6158798336982727
translation,10,159,ablation-analysis,24 %,with,worst rouge - 2,24 % with worst rouge - 2,0.67017662525177
translation,10,159,ablation-analysis,improper gendered pronouns,seemed to,severely decrease,improper gendered pronouns seemed to severely decrease,0.6615813970565796
translation,10,159,ablation-analysis,incorrect reasoning,has,percentage,incorrect reasoning has percentage,0.5687409043312073
translation,10,159,ablation-analysis,ablation analysis,has,incorrect reasoning,ablation analysis has incorrect reasoning,0.5654608607292175
translation,10,168,ablation-analysis,high correlations,between,role & language change,high correlations between role & language change,0.6136361360549927
translation,10,168,ablation-analysis,high correlations,between,referral & coreference,high correlations between referral & coreference,0.6363381743431091
translation,10,168,ablation-analysis,high correlations,between,incorrect reasoning,high correlations between incorrect reasoning,0.592705488204956
translation,10,168,ablation-analysis,incorrect reasoning,indicated,interactions,incorrect reasoning indicated interactions,0.7212850451469421
translation,10,168,ablation-analysis,between multiple participants,with,frequent co-references,between multiple participants with frequent co-references,0.6177775263786316
translation,10,168,ablation-analysis,role & language change,has,referral & coreference,role & language change has referral & coreference,0.5636692047119141
translation,10,168,ablation-analysis,interactions,has,between multiple participants,interactions has between multiple participants,0.5680303573608398
translation,10,168,ablation-analysis,ablation analysis,has,high correlations,ablation analysis has high correlations,0.5337845087051392
translation,10,94,baselines,pointer generator,added,separators,pointer generator added separators,0.6532207727432251
translation,10,94,baselines,separators,between,each utterance,separators between each utterance,0.649141788482666
translation,10,94,baselines,separators,used it as,input,separators used it as input,0.5866089463233948
translation,10,94,baselines,input,for,pointer generator model,input for pointer generator model,0.5846856832504272
translation,10,95,baselines,dynamicconv + gpt -2,/,news,dynamicconv + gpt -2 / news,0.6530939340591431
translation,10,95,baselines,gpt - 2,to initialize,token embeddings,gpt - 2 to initialize token embeddings,0.6983269453048706
translation,10,95,baselines,dynamicconv + gpt -2,has,news,dynamicconv + gpt -2 has news,0.6560547351837158
translation,10,97,baselines,fast abs rl,selects,salient sentences,fast abs rl selects salient sentences,0.7289263010025024
translation,10,97,baselines,fast abs rl,rewrites,abstractively,fast abs rl rewrites abstractively,0.7257165312767029
translation,10,97,baselines,abstractively,via,sentence - level policy gradient methods,abstractively via sentence - level policy gradient methods,0.5974616408348083
translation,10,97,baselines,baselines,has,fast abs rl,baselines has fast abs rl,0.6020229458808899
translation,10,104,baselines,multi-view bart,experimented with,different view combinations,multi-view bart experimented with different view combinations,0.7643900513648987
translation,10,104,baselines,best generic view - global view,combined with,two structured views ( stage and topic view ),best generic view - global view combined with two structured views ( stage and topic view ),0.6830887198448181
translation,10,104,baselines,different view combinations,has,best generic view - global view,different view combinations has best generic view - global view,0.5694082379341125
translation,10,104,baselines,baselines,has,multi-view bart,baselines has multi-view bart,0.5637631416320801
translation,10,102,hyperparameters,topic view,via,c99,topic view via c99,0.7300269603729248
translation,10,102,hyperparameters,topic view,set,window size,topic view set window size,0.6361809968948364
translation,10,102,hyperparameters,topic view,set,std coefficient,topic view set std coefficient,0.6200399398803711
translation,10,102,hyperparameters,window size,has,4,window size has 4,0.6379761099815369
translation,10,102,hyperparameters,std coefficient,has,1,std coefficient has 1,0.5541843175888062
translation,10,102,hyperparameters,hyperparameters,For extracting,topic view,hyperparameters For extracting topic view,0.6925475597381592
translation,10,103,hyperparameters,stage view,set,number of hidden states,stage view set number of hidden states,0.6652286052703857
translation,10,103,hyperparameters,number of hidden states,in,hmm,number of hidden states in hmm,0.5271549820899963
translation,10,103,hyperparameters,4,in,hmm,4 in hmm,0.5868125557899475
translation,10,103,hyperparameters,number of hidden states,has,4,number of hidden states has 4,0.5674239993095398
translation,10,103,hyperparameters,hyperparameters,For extracting,stage view,hyperparameters For extracting stage view,0.7202798128128052
translation,10,106,hyperparameters,one- layer lstm,for,encoding sections,one- layer lstm for encoding sections,0.617424488067627
translation,10,106,hyperparameters,hyperparameters,used,one- layer lstm,hyperparameters used one- layer lstm,0.5821813941001892
translation,10,107,hyperparameters,learning rate,for,section encoder and multi-view attention,learning rate for section encoder and multi-view attention,0.5857412219047546
translation,10,107,hyperparameters,section encoder and multi-view attention,set,3e - 3,section encoder and multi-view attention set 3e - 3,0.6711081862449646
translation,10,107,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,10,108,hyperparameters,temperature t,was,0.2,temperature t was 0.2,0.5578066110610962
translation,10,108,hyperparameters,hyperparameters,has,temperature t,hyperparameters has temperature t,0.5064308643341064
translation,10,109,hyperparameters,beam search size,during,inference,beam search size during inference,0.6951203942298889
translation,10,109,hyperparameters,inference,for,all the models,inference for all the models,0.6185235381126404
translation,10,109,hyperparameters,all the models,was,4,all the models was 4,0.6480462551116943
translation,10,109,hyperparameters,hyperparameters,has,beam search size,hyperparameters has beam search size,0.5214741826057434
translation,10,6,model,different views,to represent,conversations,different views to represent conversations,0.7012431025505066
translation,10,6,model,multi-view decoder,to incorporate,different views,multi-view decoder to incorporate different views,0.6907024383544922
translation,10,6,model,different views,to generate,dialogue summaries,different views to generate dialogue summaries,0.6642111539840698
translation,10,6,model,structures,has,of unstructured daily chats,structures has of unstructured daily chats,0.5557487607002258
translation,10,6,model,model,proposes,multi-view sequence - tosequence model,model proposes multi-view sequence - tosequence model,0.7027788758277893
translation,10,44,model,diverse conversational structures,including,topic segments,diverse conversational structures including topic segments,0.6868729591369629
translation,10,44,model,diverse conversational structures,including,conversational stages,diverse conversational structures including conversational stages,0.6825288534164429
translation,10,44,model,diverse conversational structures,including,dialogue overview,diverse conversational structures including dialogue overview,0.6776241064071655
translation,10,44,model,diverse conversational structures,including,utterances,diverse conversational structures including utterances,0.6568133234977722
translation,10,44,model,diverse conversational structures,to design,multiview model,diverse conversational structures to design multiview model,0.6235455870628357
translation,10,44,model,utterances,to design,multiview model,utterances to design multiview model,0.6203390955924988
translation,10,44,model,multiview model,for,dialogue summarization,multiview model for dialogue summarization,0.5467069149017334
translation,10,44,model,model,leverage,diverse conversational structures,model leverage diverse conversational structures,0.6914240717887878
translation,10,73,model,our base encoders and decoders,with,transformer based pre-trained model,our base encoders and decoders with transformer based pre-trained model,0.6410094499588013
translation,10,113,results,conversations,into,blocks,conversations into blocks,0.657062292098999
translation,10,113,results,blocks,from,structured views ( stage view and topic view ),blocks from structured views ( stage view and topic view ),0.5934021472930908
translation,10,113,results,blocks,boosted,performance,blocks boosted performance,0.7242988348007202
translation,10,114,results,any performance boost,when combining,generic global view,any performance boost when combining generic global view,0.6947707533836365
translation,10,114,results,generic global view,with,topic or conversational stage views,generic global view with topic or conversational stage views,0.6184651851654053
translation,10,114,results,results,not see,any performance boost,results not see any performance boost,0.658367395401001
translation,10,118,results,gains,from,multi-view bart ( topic + stage ),gains from multi-view bart ( topic + stage ),0.605634331703186
translation,10,118,results,multi-view bart ( topic + stage ),mainly from,precision scores,multi-view bart ( topic + stage ) mainly from precision scores,0.595475971698761
translation,10,118,results,multi-view bart ( topic + stage ),mainly from,recall scores,multi-view bart ( topic + stage ) mainly from recall scores,0.6119983196258545
translation,10,118,results,precision scores,while,recall scores,precision scores while recall scores,0.5591347217559814
translation,10,118,results,recall scores,kept,comparable,recall scores kept comparable,0.6899023652076721
translation,10,127,results,multi-view model,achieved,highest human annotation scores,multi-view model achieved highest human annotation scores,0.662381649017334
translation,10,127,results,highest human annotation scores,than,generic ( discrete or global ) view,highest human annotation scores than generic ( discrete or global ) view,0.540346622467041
translation,11,114,baselines,lead,implements,lead baseline method,lead implements lead baseline method,0.6550390124320984
translation,11,114,baselines,lead baseline method,takes,first k sentences,lead baseline method takes first k sentences,0.6187902092933655
translation,11,114,baselines,first k sentences,of,document,first k sentences of document,0.59919273853302
translation,11,114,baselines,first k sentences,until,length limit,first k sentences until length limit,0.6630507111549377
translation,11,114,baselines,length limit,is,reached,length limit is reached,0.5580995082855225
translation,11,115,baselines,textrank,uses,variation of the pagerank graph centrality algorithm,textrank uses variation of the pagerank graph centrality algorithm,0.5862928628921509
translation,11,115,baselines,variation of the pagerank graph centrality algorithm,to identify,most important sentences,variation of the pagerank graph centrality algorithm to identify most important sentences,0.6660946607589722
translation,11,115,baselines,most important sentences,in,document,most important sentences in document,0.4760347604751587
translation,11,5,model,summary space,of,each domain,summary space of each domain,0.5891978740692139
translation,11,5,model,summary space,via,exhaustive search strategy,summary space via exhaustive search strategy,0.658700168132782
translation,11,5,model,probability density function ( pdf ),of,rouge score distributions,probability density function ( pdf ) of rouge score distributions,0.5935098528862
translation,11,5,model,rouge score distributions,for,each domain,rouge score distributions for each domain,0.5627445578575134
translation,11,5,model,model,explores,summary space,model explores summary space,0.7013676166534424
translation,11,5,model,model,finds,probability density function ( pdf ),model finds probability density function ( pdf ),0.6099990010261536
translation,11,119,results,rouge recall scores,of,three systems,rouge recall scores of three systems,0.5689170956611633
translation,11,119,results,results,has,rouge recall scores,results has rouge recall scores,0.548926055431366
translation,11,120,results,literary and legal domains,has,"random , and the lead systems","literary and legal domains has random , and the lead systems",0.6065739393234253
translation,11,123,results,textrank,is,best system,textrank is best system,0.5245604515075684
translation,11,123,results,best system,for,"literary , scientific , and legal domains","best system for literary , scientific , and legal domains",0.5829523205757141
translation,11,123,results,outperformed,by,lead system,outperformed by lead system,0.6419121026992798
translation,11,123,results,lead system,on,newswire domain,lead system on newswire domain,0.5236020088195801
translation,12,205,baselines,sentences,based on,centrality,sentences based on centrality,0.6579540371894836
translation,12,205,baselines,centrality,in,sentence similarity graph,centrality in sentence similarity graph,0.50107342004776
translation,12,205,baselines,baselines,has,lexrank,baselines has lexrank,0.5459157228469849
translation,12,207,baselines,"kl and js ( haghighi and vanderwende , 2009 )",measure,divergence,"kl and js ( haghighi and vanderwende , 2009 ) measure divergence",0.6568248867988586
translation,12,207,baselines,divergence,between,distribution of words,divergence between distribution of words,0.6685817837715149
translation,12,207,baselines,distribution of words,in,summary and in the sources,distribution of words in summary and in the sources,0.5673806667327881
translation,12,207,baselines,baselines,has,"kl and js ( haghighi and vanderwende , 2009 )","baselines has kl and js ( haghighi and vanderwende , 2009 )",0.542438268661499
translation,12,208,baselines,two baselines,account for,background knowledge,two baselines account for background knowledge,0.6762086749076843
translation,12,208,baselines,two baselines,measure,divergence,two baselines measure divergence,0.6814175844192505
translation,12,211,results,correlate better,with,human judgments,correlate better with human judgments,0.6654916405677795
translation,12,211,results,human judgments,than,other quantities,human judgments than other quantities,0.6305435299873352
translation,12,211,results,relevance,has,correlate better,relevance has correlate better,0.5744490027427673
translation,12,211,results,results,modelizations of,relevance,results modelizations of relevance,0.6001978516578674
translation,12,212,results,metrics,accounting for,background knowledge,metrics accounting for background knowledge,0.5810236930847168
translation,12,212,results,background knowledge,work,better,background knowledge work better,0.6308783292770386
translation,12,212,results,better,in,update scenario,better in update scenario,0.5352869033813477
translation,12,212,results,results,has,metrics,results has metrics,0.5256302952766418
translation,12,214,results,js divergence,gives,slightly better results,js divergence gives slightly better results,0.6171642541885376
translation,12,214,results,slightly better results,than,kl,slightly better results than kl,0.6050073504447937
translation,12,214,results,results,observe,js divergence,results observe js divergence,0.5953718423843384
translation,12,216,results,all baselines,in both,generic and the update case,all baselines in both generic and the update case,0.6559567451477051
translation,12,216,results,significantly,has,outperforms,significantly has outperforms,0.6342213749885559
translation,12,216,results,outperforms,has,all baselines,outperforms has all baselines,0.5888515114784241
translation,12,216,results,results,has,significantly,results has significantly,0.5334484577178955
translation,13,199,ablation-analysis,to 3 m,with,p neg soft switch,to 3 m with p neg soft switch,0.6943537592887878
translation,13,199,ablation-analysis,p neg soft switch,improves,negation f1,p neg soft switch improves negation f1,0.7310711741447449
translation,13,199,ablation-analysis,2m - base,has,to 3 m,2m - base has to 3 m,0.6362283229827881
translation,13,199,ablation-analysis,negation f1,has,76.9 vs 70.1 ),negation f1 has 76.9 vs 70.1 ),0.5529288053512573
translation,13,199,ablation-analysis,ablation analysis,extending,2m - base,ablation analysis extending 2m - base,0.7462911009788513
translation,13,207,ablation-analysis,local snippets,observe that,increased copying ability,local snippets observe that increased copying ability,0.5682940483093262
translation,13,207,ablation-analysis,increased copying ability,adding,concept attention,increased copying ability adding concept attention,0.6859301924705505
translation,13,207,ablation-analysis,concept attention,reduce,performance,concept attention reduce performance,0.6818697452545166
translation,13,207,ablation-analysis,ablation analysis,On,local snippets,ablation analysis On local snippets,0.5705441236495972
translation,13,158,experimental-setup,vocabulary size,of,50 k,vocabulary size of 50 k,0.6240697503089905
translation,13,158,experimental-setup,50 k,with,128 dimensional embeddings,50 k with 128 dimensional embeddings,0.6139382719993591
translation,13,159,experimental-setup,training parameters,followed,adagrad,training parameters followed adagrad,0.6189442276954651
translation,13,159,experimental-setup,training parameters,with,learning rate,training parameters with learning rate,0.6025553345680237
translation,13,159,experimental-setup,training parameters,with,adagrad,training parameters with adagrad,0.5727906227111816
translation,13,159,experimental-setup,learning rate,of,0.15,learning rate of 0.15,0.6145498156547546
translation,13,159,experimental-setup,adagrad,as,optimizer,adagrad as optimizer,0.5433330535888672
translation,13,159,experimental-setup,experimental setup,has,training parameters,experimental setup has training parameters,0.49305152893066406
translation,13,161,experimental-setup,models,pretrained on,cnn - daily mail corpus,models pretrained on cnn - daily mail corpus,0.7469791769981384
translation,13,161,experimental-setup,models,finetuned on,conversational data,models finetuned on conversational data,0.6608467102050781
translation,13,161,experimental-setup,conversational data,from,our in- house chat - based telehealth platform,conversational data from our in- house chat - based telehealth platform,0.5733591914176941
translation,13,161,experimental-setup,experimental setup,pretrained on,cnn - daily mail corpus,experimental setup pretrained on cnn - daily mail corpus,0.7690479159355164
translation,13,161,experimental-setup,experimental setup,finetuned on,conversational data,experimental setup finetuned on conversational data,0.6838071346282959
translation,13,161,experimental-setup,experimental setup,has,models,experimental setup has models,0.5060054659843445
translation,13,162,experimental-setup,pretraining,took,approximately 2 days,pretraining took approximately 2 days,0.6308430433273315
translation,13,162,experimental-setup,approximately 2 days,on,single nvidia titan xp gpu,approximately 2 days on single nvidia titan xp gpu,0.5199660062789917
translation,13,162,experimental-setup,finetuning,took,under 2 hours,finetuning took under 2 hours,0.692064106464386
translation,13,162,experimental-setup,experimental setup,has,pretraining,experimental setup has pretraining,0.5295986533164978
translation,13,36,experiments,our own dataset,using,conversations,our own dataset using conversations,0.7466475367546082
translation,13,36,experiments,conversations,from,telemedicine platform,conversations from telemedicine platform,0.6270477175712585
translation,13,36,experiments,conversations,obtain,reference summaries,conversations obtain reference summaries,0.5848163962364197
translation,13,36,experiments,reference summaries,from,healthcare professionals,reference summaries from healthcare professionals,0.554607629776001
translation,13,7,model,variation,of,pointer generator network,variation of pointer generator network,0.5983049869537354
translation,13,7,model,variation,introduce,penalty,variation introduce penalty,0.6428331136703491
translation,13,7,model,variation,explicitly model,negations,variation explicitly model negations,0.7918868660926819
translation,13,7,model,pointer generator network,introduce,penalty,pointer generator network introduce penalty,0.6426807641983032
translation,13,7,model,pointer generator network,explicitly model,negations,pointer generator network explicitly model negations,0.7809565663337708
translation,13,7,model,penalty,on,generator distribution,penalty on generator distribution,0.5372071862220764
translation,13,41,model,baseline pointer generator network,by introducing,penalty,baseline pointer generator network by introducing penalty,0.6483853459358215
translation,13,41,model,penalty,to,generator distribution,penalty to generator distribution,0.5427328944206238
translation,13,41,model,generator distribution,to guarantee,network,generator distribution to guarantee network,0.6030209064483643
translation,13,41,model,network,to,extractive summarization,network to extractive summarization,0.5453566908836365
translation,13,41,model,network,has,defaults,network has defaults,0.6269239187240601
translation,13,41,model,model,extend,baseline pointer generator network,model extend baseline pointer generator network,0.6599330902099609
translation,13,8,results,important properties,of,medical conversations,important properties of medical conversations,0.5478575229644775
translation,13,8,results,medical conversations,such as,medical knowledge,medical conversations such as medical knowledge,0.6187164783477783
translation,13,8,results,medical knowledge,coming from,standardized medical ontologies,medical knowledge coming from standardized medical ontologies,0.5529365539550781
translation,13,8,results,standardized medical ontologies,has,better,standardized medical ontologies has better,0.5699430108070374
translation,13,8,results,results,captures,important properties,results captures important properties,0.7307708859443665
translation,13,170,results,2m - pgen,generates,better summaries,2m - pgen generates better summaries,0.7052656412124634
translation,13,200,results,3m - neg,by incorporating,negation attention,3m - neg by incorporating negation attention,0.6883375644683838
translation,13,200,results,negation attention,see,even larger improvement,negation attention see even larger improvement,0.5716935396194458
translation,13,200,results,even larger improvement,in,negation f1 ( 81.5 ),even larger improvement in negation f1 ( 81.5 ),0.5316166877746582
translation,13,200,results,results,extending,3m - neg,results extending 3m - neg,0.5950211882591248
translation,14,107,baselines,baselines,has,non-comparative model ( ncm ),baselines has non-comparative model ( ncm ),0.5707104802131653
translation,14,7,experiments,semantic-related cross-topic concept pairs,as,comparative evidences,semantic-related cross-topic concept pairs as comparative evidences,0.5220933556556702
translation,14,7,experiments,topic-related concepts,as,representative evidences,topic-related concepts as representative evidences,0.5089769959449768
translation,14,6,model,task,as,optimization problem,task as optimization problem,0.5162028074264526
translation,14,6,model,of selecting proper sentences,to maximize,comparativeness,of selecting proper sentences to maximize comparativeness,0.6398530602455139
translation,14,6,model,of selecting proper sentences,to maximize,representativeness,of selecting proper sentences to maximize representativeness,0.6481348276138306
translation,14,6,model,comparativeness,within,summary,comparativeness within summary,0.6776698231697083
translation,14,6,model,comparativeness,within,representativeness,comparativeness within representativeness,0.6259807348251343
translation,14,6,model,optimization problem,has,of selecting proper sentences,optimization problem has of selecting proper sentences,0.5201680660247803
translation,14,6,model,representativeness,has,to both news topics,representativeness has to both news topics,0.5394006967544556
translation,14,6,model,model,formulate,task,model formulate task,0.7247849702835083
translation,14,33,model,comparativeness and representativeness,as well as,redundancy,comparativeness and representativeness as well as redundancy,0.6217297911643982
translation,14,33,model,comparativeness and representativeness,solve,optimization problem,comparativeness and representativeness solve optimization problem,0.5895681977272034
translation,14,33,model,redundancy,in,objective function,redundancy in objective function,0.4938545525074005
translation,14,33,model,optimization problem,by using,linear programming,optimization problem by using linear programming,0.5843554735183716
translation,14,33,model,linear programming,to extract,proper comparable sentences,linear programming to extract proper comparable sentences,0.6246803402900696
translation,14,33,model,model,consider,comparativeness and representativeness,model consider comparativeness and representativeness,0.6657782793045044
translation,14,33,model,model,solve,optimization problem,model solve optimization problem,0.6856990456581116
translation,14,116,results,linear programming based comparative model,denoted as,lpcm ),linear programming based comparative model denoted as lpcm ),0.7315848469734192
translation,14,116,results,linear programming based comparative model,achieves,best scores,linear programming based comparative model achieves best scores,0.6762902736663818
translation,14,116,results,best scores,over,all metrics,best scores over all metrics,0.6256011724472046
translation,14,116,results,baseline models,has,linear programming based comparative model,baseline models has linear programming based comparative model,0.5328162312507629
translation,14,116,results,results,Compared with,baseline models,results Compared with baseline models,0.6812605857849121
translation,15,99,experimental-setup,training and hyperparameter tuning,used,"tensorflow ( abadi et al. , 2015 )","training and hyperparameter tuning used tensorflow ( abadi et al. , 2015 )",0.5176301002502441
translation,15,99,experimental-setup,experimental setup,has,training and hyperparameter tuning,experimental setup has training and hyperparameter tuning,0.4891800880432129
translation,15,100,experimental-setup,initialized,generating,values,initialized generating values,0.7243706583976746
translation,15,100,experimental-setup,values,from,uniform distribution,values from uniform distribution,0.5655192732810974
translation,15,100,experimental-setup,uniform distribution,in,"range [ - 1 , 1 )","uniform distribution in range [ - 1 , 1 )",0.5418336987495422
translation,15,100,experimental-setup,experimental setup,has,encoder embeddings,experimental setup has encoder embeddings,0.5210381746292114
translation,15,101,experimental-setup,other variables,initialized using,glorot uniform initialization,other variables initialized using glorot uniform initialization,0.7668028473854065
translation,15,101,experimental-setup,experimental setup,has,other variables,experimental setup has other variables,0.4991949200630188
translation,15,102,experimental-setup,each hyperparameter,by choosing,parameter,each hyperparameter by choosing parameter,0.6841060519218445
translation,15,102,experimental-setup,each hyperparameter,selected,model,each hyperparameter selected model,0.6393226981163025
translation,15,102,experimental-setup,model,with,best sbleu score,model with best sbleu score,0.5983031392097473
translation,15,102,experimental-setup,best sbleu score,in,validation set,best sbleu score in validation set,0.5092178583145142
translation,15,102,experimental-setup,best sbleu score,over,500 epochs,best sbleu score over 500 epochs,0.6600103378295898
translation,15,102,experimental-setup,experimental setup,tune,each hyperparameter,experimental setup tune each hyperparameter,0.6824434399604797
translation,15,102,experimental-setup,experimental setup,selected,model,experimental setup selected model,0.6014821529388428
translation,15,105,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,15,105,experimental-setup,learning rate,set to,0.0001,learning rate set to 0.0001,0.6949850916862488
translation,15,106,experiments,embedding size,of,100,embedding size of 100,0.6582188010215759
translation,15,106,experiments,gru size,of,400,gru size of 400,0.6696271896362305
translation,15,106,experiments,static record attention sizep,of,150,static record attention sizep of 150,0.6542798280715942
translation,15,106,experiments,150,to work best for,mham model,150 to work best for mham model,0.6629566550254822
translation,15,7,model,mixed hierarchical attention based encoderdecoder model,able to leverage,structure,mixed hierarchical attention based encoderdecoder model able to leverage structure,0.6601499915122986
translation,15,7,model,model,propose,mixed hierarchical attention based encoderdecoder model,model propose mixed hierarchical attention based encoderdecoder model,0.6684508919715881
translation,15,22,model,mixed hierarchical attention model,following,"encode-attend - decode ( cho et al. , 2015 ) paradigm","mixed hierarchical attention model following encode-attend - decode ( cho et al. , 2015 ) paradigm",0.6208928823471069
translation,15,22,model,model,cast,mixed hierarchical attention model,model cast mixed hierarchical attention model,0.6751819849014282
translation,15,22,model,model,into,mixed hierarchical attention model,model into mixed hierarchical attention model,0.5942487120628357
translation,15,23,model,static attention,on,attributes,static attention on attributes,0.5694208741188049
translation,15,23,model,attributes,to compute,row representation,attributes to compute row representation,0.7154651880264282
translation,15,23,model,row representation,followed by,dynamic attention,row representation followed by dynamic attention,0.6340531706809998
translation,15,23,model,dynamic attention,on,rows,dynamic attention on rows,0.5975844860076904
translation,15,23,model,dynamic attention,subsequently fed to,decoder,dynamic attention subsequently fed to decoder,0.6504386067390442
translation,15,41,model,mixed hierarchical attention based encoder,incorporates,static attention and dynamic attention,mixed hierarchical attention based encoder incorporates static attention and dynamic attention,0.6886786222457886
translation,15,41,model,static attention and dynamic attention,at,two different levels,static attention and dynamic attention at two different levels,0.5616449117660522
translation,15,41,model,model,call,mixed hierarchical attention based encoder,model call mixed hierarchical attention based encoder,0.6332662105560303
translation,15,127,model,model,proposed,novel mixed hierarchical attention based encoder-decoder approach,model proposed novel mixed hierarchical attention based encoder-decoder approach,0.7315785884857178
translation,15,27,results,proposed mixed hierarchi- cal attention model,provides,improvement,proposed mixed hierarchi- cal attention model provides improvement,0.6106841564178467
translation,15,27,results,improvement,of,around 18 bleu ( around 30 % ),improvement of around 18 bleu ( around 30 % ),0.5590900778770447
translation,15,27,results,improvement,over,current state - of - the - art result,improvement over current state - of - the - art result,0.6434492468833923
translation,15,27,results,around 18 bleu ( around 30 % ),over,current state - of - the - art result,around 18 bleu ( around 30 % ) over current state - of - the - art result,0.6571297645568848
translation,15,27,results,results,has,proposed mixed hierarchi- cal attention model,results has proposed mixed hierarchi- cal attention model,0.579083263874054
translation,15,113,results,significant performance improvement,of,16.6 cbleu score ( 24 % ),significant performance improvement of 16.6 cbleu score ( 24 % ),0.5249435901641846
translation,15,113,results,significant performance improvement,of,18.3 sbleu score ( 30 % ),significant performance improvement of 18.3 sbleu score ( 30 % ),0.5191643238067627
translation,15,113,results,significant performance improvement,compared to,current state - of - the - art model of mbw,significant performance improvement compared to current state - of - the - art model of mbw,0.6470836400985718
translation,15,113,results,results,observe,significant performance improvement,results observe significant performance improvement,0.6156408190727234
translation,15,114,results,mham,shows,improvement,mham shows improvement,0.6878026127815247
translation,15,114,results,improvement,over,nhm,improvement over nhm,0.7031967043876648
translation,15,114,results,nhm,in,all metrics,nhm in all metrics,0.538885772228241
translation,15,114,results,results,has,mham,results has mham,0.5561223030090332
translation,15,123,results,our model,able to capture,weak signals,our model able to capture weak signals,0.730170726776123
translation,15,123,results,weak signals,like,snow accumulation,weak signals like snow accumulation,0.6385401487350464
translation,15,123,results,results,has,our model,results has our model,0.5871725678443909
translation,16,194,ablation-analysis,supremacy,of,extractive models,supremacy of extractive models,0.5988721251487732
translation,16,194,ablation-analysis,not retained,in,all datasets,not retained in all datasets,0.4997557997703552
translation,16,194,ablation-analysis,ablation analysis,has,supremacy,ablation analysis has supremacy,0.5847307443618774
translation,16,105,results,cnndm,is,most extractive dataset,cnndm is most extractive dataset,0.5400798916816711
translation,16,107,results,relatively higher copy rate,in,summary,relatively higher copy rate in summary,0.5364606380462646
translation,16,107,results,shorter,than,cnndm,shorter than cnndm,0.6312559247016907
translation,16,108,results,"bigaptent b , xsum",obtain,higher sentence fusion score,"bigaptent b , xsum obtain higher sentence fusion score",0.575156569480896
translation,16,108,results,results,has,"bigaptent b , xsum","results has bigaptent b , xsum",0.5720139145851135
translation,16,109,results,xsum and reddit,obtain,more 3 - gram novel units,xsum and reddit obtain more 3 - gram novel units,0.6092609763145447
translation,16,109,results,more 3 - gram novel units,in,summary,more 3 - gram novel units in summary,0.5156968832015991
translation,16,109,results,results,has,xsum and reddit,results has xsum and reddit,0.5595723986625671
translation,16,152,results,architecture match based reranking,improves,stiffness significantly,architecture match based reranking improves stiffness significantly,0.7341123819351196
translation,16,152,results,architecture match based reranking,enhances,stiffness,architecture match based reranking enhances stiffness,0.706149160861969
translation,16,152,results,results,has,architecture match based reranking,results has architecture match based reranking,0.6008792519569397
translation,16,165,results,pointer network,brings,trivial improvement,pointer network brings trivial improvement,0.5938209295272827
translation,16,165,results,trivial improvement,tested in,xsum and reddit,trivial improvement tested in xsum and reddit,0.6914363503456116
translation,16,165,results,results,has,pointer network,results has pointer network,0.5656625628471375
translation,16,166,results,absolute model performance improvement,of,pointer network,absolute model performance improvement of pointer network,0.5564240217208862
translation,16,166,results,absolute model performance improvement,is,trival,absolute model performance improvement is trival,0.576335072517395
translation,16,166,results,pointer network,is,trival,pointer network is trival,0.6053006649017334
translation,16,166,results,trival,tested in,xsum and reddit,trival tested in xsum and reddit,0.7203665375709534
translation,16,166,results,results,has,absolute model performance improvement,results has absolute model performance improvement,0.562481164932251
translation,16,171,results,l2l cov ptr,gets,lower stiffness,l2l cov ptr gets lower stiffness,0.6099199056625366
translation,16,171,results,lower stiffness,compared with,l2l ptr,lower stiffness compared with l2l ptr,0.6462907195091248
translation,16,171,results,xsum,has,l2l cov ptr,xsum has l2l cov ptr,0.5908375978469849
translation,16,171,results,results,trained in,xsum,results trained in xsum,0.7000669836997986
translation,16,182,results,bert non,performs,worse,bert non performs worse,0.6566758751869202
translation,16,182,results,worse,than,trans non,worse than trans non,0.7244465351104736
translation,16,182,results,trans non,in,some cells,trans non in some cells,0.6070399880409241
translation,16,182,results,results,has,bert non,results has bert non,0.6433594226837158
translation,16,183,results,4 b bart,shows,superior performance,4 b bart shows superior performance,0.6399077773094177
translation,16,183,results,superior performance,in terms of,stiffness,superior performance in terms of stiffness,0.7078624367713928
translation,16,183,results,superior performance,in terms of,stableness,superior performance in terms of stableness,0.6630874276161194
translation,16,183,results,results,has,4 b bart,results has 4 b bart,0.5490767359733582
translation,16,184,results,bart,obtains,highest stiffness,bart obtains highest stiffness,0.6685540080070496
translation,16,184,results,bart,comparable with,bert match,bart comparable with bert match,0.697616457939148
translation,16,184,results,highest stiffness,among,all abstractive models,highest stiffness among all abstractive models,0.5997840166091919
translation,16,185,results,outstanding,in terms of,stableness,outstanding in terms of stableness,0.6471255421638489
translation,16,185,results,stableness,compared with,other abstractive models,stableness compared with other abstractive models,0.5870890021324158
translation,16,185,results,results,has,bart,results has bart,0.40022552013397217
translation,16,186,results,performance gap,between,bart and be2t,performance gap between bart and be2t,0.662918746471405
translation,16,186,results,bart and be2t,proves,abstractive models,bart and be2t proves abstractive models,0.7098231911659241
translation,16,186,results,bart and be2t,for,abstractive models,bart and be2t for abstractive models,0.6620190143585205
translation,16,186,results,abstractive models,pre-training,whole sequence to sequence model,abstractive models pre-training whole sequence to sequence model,0.7153926491737366
translation,16,186,results,works better,than using,pretrained model,works better than using pretrained model,0.6471152305603027
translation,16,186,results,pretrained model,in,either side of encoder or decoder,pretrained model in either side of encoder or decoder,0.5063955187797546
translation,16,186,results,whole sequence to sequence model,has,works better,whole sequence to sequence model has works better,0.5892384052276611
translation,16,186,results,results,has,performance gap,results has performance gap,0.5742626190185547
translation,16,187,results,generation ways extractive models,superior to,abstractive models,generation ways extractive models superior to abstractive models,0.7209458947181702
translation,16,187,results,abstractive models,in terms of,stiffness and robustness,abstractive models in terms of stiffness and robustness,0.6335697174072266
translation,16,187,results,results,has,generation ways extractive models,results has generation ways extractive models,0.5820501446723938
translation,16,188,results,extractive models,show,superior advantage,extractive models show superior advantage,0.6509774923324585
translation,16,188,results,superior advantage,of,absolute performance,superior advantage of absolute performance,0.6016806364059448
translation,16,188,results,results,has,extractive models,results has extractive models,0.5364269614219666
translation,16,193,results,abstractive systems,possess,comparable or even better performance,abstractive systems possess comparable or even better performance,0.6540761590003967
translation,16,193,results,xsum and reddit,has,abstractive systems,xsum and reddit has abstractive systems,0.5937015414237976
translation,16,196,results,higher stiffness scores,tested in,cnndm and pubmed,higher stiffness scores tested in cnndm and pubmed,0.7066588401794434
translation,16,196,results,higher or comparable stiffness scores,tested at,xsum and reddit,higher or comparable stiffness scores tested at xsum and reddit,0.7172574400901794
translation,16,199,results,all extractive models,achieve,higher factuality scores,all extractive models achieve higher factuality scores,0.6034771800041199
translation,16,199,results,all abstractive models,obtain,quite lower ones,all abstractive models obtain quite lower ones,0.5698374509811401
translation,16,199,results,results,has,all extractive models,results has all extractive models,0.5192540287971497
translation,16,200,results,not all factuality scores,under,in- dataset setting,not all factuality scores under in- dataset setting,0.6418861150741577
translation,16,200,results,not all factuality scores,are,100 %,not all factuality scores are 100 %,0.5782971978187561
translation,16,200,results,extractive models,has,not all factuality scores,extractive models has not all factuality scores,0.5919802188873291
translation,16,202,results,ability,to generate,factual summaries,ability to generate factual summaries,0.7088662385940552
translation,16,202,results,factual summaries,compared with,other abstractive models,factual summaries compared with other abstractive models,0.6309517621994019
translation,16,202,results,bart,has,significantly improve,bart has significantly improve,0.6362488865852356
translation,16,202,results,significantly improve,has,ability,significantly improve has ability,0.5641452074050903
translation,16,202,results,results,has,bart,results has bart,0.40022552013397217
translation,16,203,results,abstractive models,obtain,higher stableness,abstractive models obtain higher stableness,0.5040586590766907
translation,16,203,results,higher stableness,of,factuality scores,higher stableness of factuality scores,0.5293118953704834
translation,16,203,results,higher stableness,surpass,100 %,higher stableness surpass 100 %,0.7336617112159729
translation,16,203,results,results,has,abstractive models,results has abstractive models,0.5407604575157166
translation,17,40,baselines,rouge,replaces,exact lexical matches,rouge replaces exact lexical matches,0.6715829372406006
translation,17,40,baselines,exact lexical matches,with,soft semantic similarity measure,exact lexical matches with soft semantic similarity measure,0.6384168863296509
translation,17,40,baselines,soft semantic similarity measure,approximated with,cosine distances,soft semantic similarity measure approximated with cosine distances,0.715346097946167
translation,17,40,baselines,cosine distances,between,distributed representations,cosine distances between distributed representations,0.6544152498245239
translation,17,40,baselines,distributed representations,has,of tokens,distributed representations has of tokens,0.5420838594436646
translation,17,40,baselines,baselines,has,rouge,baselines has rouge,0.6319058537483215
translation,17,41,baselines,rouge 2.0,leverages,synonym dictionaries,rouge 2.0 leverages synonym dictionaries,0.7026985883712769
translation,17,41,baselines,rouge 2.0,considers,all synonyms,rouge 2.0 considers all synonyms,0.6402601599693298
translation,17,41,baselines,synonym dictionaries,such as,wordnet,synonym dictionaries such as wordnet,0.5966330170631409
translation,17,41,baselines,all synonyms,of,matched words,all synonyms of matched words,0.5541228652000427
translation,17,41,baselines,all synonyms,when computing,token overlap,all synonyms when computing token overlap,0.6602432131767273
translation,17,41,baselines,baselines,has,rouge 2.0,baselines has rouge 2.0,0.5766393542289734
translation,17,42,baselines,rouge,combines,lexical and semantic matching,rouge combines lexical and semantic matching,0.70810866355896
translation,17,42,baselines,lexical and semantic matching,by applying,graph analysis algorithms,lexical and semantic matching by applying graph analysis algorithms,0.6404842734336853
translation,17,42,baselines,graph analysis algorithms,to,wordnet semantic network,graph analysis algorithms to wordnet semantic network,0.5097549557685852
translation,17,42,baselines,baselines,has,rouge,baselines has rouge,0.6319058537483215
translation,17,184,baselines,lead - 3,is,strong baseline,lead - 3 is strong baseline,0.6273162364959717
translation,17,184,baselines,lead - 3,exploits,described layout bias,lead - 3 exploits described layout bias,0.7736585736274719
translation,17,184,baselines,strong baseline,exploits,described layout bias,strong baseline exploits described layout bias,0.7665116190910339
translation,17,184,baselines,baselines,has,lead - 3,baselines has lead - 3,0.6145645976066589
translation,17,15,results,summaries,by extracting,first three sentences,summaries by extracting first three sentences,0.7456533908843994
translation,17,15,results,slightly outperform,has,lead - 3 baseline,slightly outperform has lead - 3 baseline,0.5998196005821228
translation,17,182,results,all examined models,noticed,substantial increase,all examined models noticed substantial increase,0.6886602640151978
translation,17,182,results,substantial increase,of,overlap,substantial increase of overlap,0.6489185094833374
translation,17,182,results,overlap,across,all rouge variants,overlap across all rouge variants,0.7162677645683289
translation,17,182,results,results,For,all examined models,results For all examined models,0.5130766034126282
translation,17,183,results,performance,of,current models,performance of current models,0.6026906371116638
translation,17,183,results,current models,strongly affected by,layout bias,current models strongly affected by layout bias,0.6417418718338013
translation,17,183,results,layout bias,of,news corpora,layout bias of news corpora,0.5617051720619202
translation,17,183,results,results,suggest,performance,results suggest performance,0.644426167011261
translation,18,191,ablation-analysis,greedy - based sentence selection,can not prevent,redundancy,greedy - based sentence selection can not prevent redundancy,0.6769232749938965
translation,18,191,ablation-analysis,redundancy,has,effectively,redundancy has effectively,0.6325650811195374
translation,18,191,ablation-analysis,ablation analysis,has,greedy - based sentence selection,ablation analysis has greedy - based sentence selection,0.538723349571228
translation,18,169,baselines,two greedy algorithms,select,top ranked sentence,two greedy algorithms select top ranked sentence,0.6584452390670776
translation,18,169,baselines,top ranked sentence,by removing,10 redundant neighbor sentences,top ranked sentence by removing 10 redundant neighbor sentences,0.7001441121101379
translation,18,169,baselines,10 redundant neighbor sentences,from,sentence similarity graph,10 redundant neighbor sentences from sentence similarity graph,0.566385805606842
translation,18,169,baselines,10 redundant neighbor sentences,if,summary length,10 redundant neighbor sentences if summary length,0.5824064612388611
translation,18,169,baselines,sentence similarity graph,if,summary length,sentence similarity graph if summary length,0.5580713748931885
translation,18,169,baselines,summary length,less then,100 words,summary length less then 100 words,0.5865813493728638
translation,18,183,baselines,entityaspect model,to approximate,hiersum   model,entityaspect model to approximate hiersum   model,0.6826308369636536
translation,18,7,experiments,integer linear programming,for,sentence selection,integer linear programming for sentence selection,0.5598416328430176
translation,18,189,experiments,bl - 1,see that,lda - based sentence clustering,bl - 1 see that lda - based sentence clustering,0.6208591461181641
translation,18,189,experiments,lda - based sentence clustering,better then,k-means,lda - based sentence clustering better then k-means,0.6670436859130859
translation,18,190,experiments,bl - 2,see that,traditional ranking plus greedy selection summary generation framework,bl - 2 see that traditional ranking plus greedy selection summary generation framework,0.6202419400215149
translation,18,190,experiments,traditional ranking plus greedy selection summary generation framework,not suitable for,aspect-oriented summarization task,traditional ranking plus greedy selection summary generation framework not suitable for aspect-oriented summarization task,0.6985941529273987
translation,18,87,hyperparameters,100 burn - in iterations,through,all documents,100 burn - in iterations through all documents,0.6327395439147949
translation,18,87,hyperparameters,all documents,in,collection,all documents in collection,0.5389758348464966
translation,18,87,hyperparameters,distribution of z and y,before collecting,samples,distribution of z and y before collecting samples,0.6772894263267517
translation,18,87,hyperparameters,hyperparameters,run,100 burn - in iterations,hyperparameters run 100 burn - in iterations,0.7188725471496582
translation,18,161,hyperparameters,cluto,to do,k-means clustering,cluto to do k-means clustering,0.4440085291862488
translation,18,161,hyperparameters,hyperparameters,use,cluto,hyperparameters use cluto,0.645473062992096
translation,18,171,hyperparameters,similarity graph building threshold,is,0.3,similarity graph building threshold is 0.3,0.551545262336731
translation,18,171,hyperparameters,damping factor,is,0.2,damping factor is 0.2,0.5647537708282471
translation,18,171,hyperparameters,error tolerance,for,power method,error tolerance for power method,0.6368660926818848
translation,18,171,hyperparameters,power method,in,lexrank,power method in lexrank,0.4911547899246216
translation,18,171,hyperparameters,lexrank,is,0.1,lexrank is 0.1,0.5754020810127258
translation,18,171,hyperparameters,hyperparameters,has,similarity graph building threshold,hyperparameters has similarity graph building threshold,0.5165534019470215
translation,18,171,hyperparameters,hyperparameters,has,damping factor,hyperparameters has damping factor,0.4970376789569855
translation,18,5,model,event-aspect lda model,to cluster,sentences,event-aspect lda model to cluster sentences,0.6895970702171326
translation,18,5,model,sentences,into,aspects,sentences into aspects,0.6103043556213379
translation,18,5,model,model,develop,event-aspect lda model,model develop event-aspect lda model,0.6171358823776245
translation,18,6,model,extended lexrank algorithm,to rank,sentences,extended lexrank algorithm to rank sentences,0.7224833369255066
translation,18,6,model,sentences,in,each cluster,sentences in each cluster,0.5567487478256226
translation,18,9,model,new sentence compression algorithm,which use,dependency tree,new sentence compression algorithm which use dependency tree,0.6395124793052673
translation,18,9,model,dependency tree,instead of,parser tree,dependency tree instead of parser tree,0.6073870062828064
translation,18,9,model,model,implement,new sentence compression algorithm,model implement new sentence compression algorithm,0.6475539207458496
translation,18,162,model,entity - aspect model,to do,sentence clustering,entity - aspect model to do sentence clustering,0.36146387457847595
translation,18,162,model,model,try,entity - aspect model,model try entity - aspect model,0.5692387819290161
translation,18,8,results,automatic grouping,of,semantically related sentences,automatic grouping of semantically related sentences,0.5318850874900818
translation,18,8,results,sentence ranking,based on,extension of random walk model,sentence ranking based on extension of random walk model,0.6514405608177185
translation,18,188,results,our method,gives,better rouge recall measures,our method gives better rouge recall measures,0.5641432404518127
translation,18,188,results,better rouge recall measures,then,four baseline methods,better rouge recall measures then four baseline methods,0.5050286650657654
translation,18,188,results,better rouge recall measures,has,four baseline methods,better rouge recall measures has four baseline methods,0.5338721871376038
translation,18,188,results,results,see that,our method,results see that our method,0.631170928478241
translation,18,192,results,bl - 3 evaluation results,showed that,ilp - based sentence selection,bl - 3 evaluation results showed that ilp - based sentence selection,0.6755529046058655
translation,18,192,results,ilp - based sentence selection,better then,kl - divergence selection,ilp - based sentence selection better then kl - divergence selection,0.6667206287384033
translation,18,192,results,redundancy,across,different aspects,redundancy across different aspects,0.7205902934074402
translation,18,192,results,results,has,bl - 3 evaluation results,results has bl - 3 evaluation results,0.5712625980377197
translation,18,200,results,rouge - su4 score,of,our approach,rouge - su4 score of our approach,0.5700816512107849
translation,18,200,results,rouge - su4 score,achieve,0.10146,rouge - su4 score achieve 0.10146,0.6092148423194885
translation,18,200,results,our approach,achieve,0.10146,our approach achieve 0.10146,0.6012318730354309
translation,18,200,results,higher,than,mead 's 0.09112,higher than mead 's 0.09112,0.5904035568237305
translation,18,200,results,0.10146,has,higher,0.10146 has higher,0.6113631725311279
translation,18,200,results,results,has,rouge - su4 score,results has rouge - su4 score,0.555749237537384
translation,18,215,results,sentence compression,improve,system performance,sentence compression improve system performance,0.6686267256736755
translation,18,215,results,results,see that,sentence compression,results see that sentence compression,0.6537451148033142
translation,19,25,baselines,two graph variants,one,other,two graph variants one other,0.761462390422821
translation,19,25,baselines,two graph variants,one,topic shifts,two graph variants one topic shifts,0.7135854363441467
translation,19,25,baselines,two graph variants,mainly capturing,entities ' document - level interactions,two graph variants mainly capturing entities ' document - level interactions,0.6976150870323181
translation,19,25,baselines,other,reflecting,interactions,other reflecting interactions,0.6768456697463989
translation,19,25,baselines,interactions,within,each paragraph,interactions within each paragraph,0.7027192115783691
translation,19,25,baselines,interactions,plus,topic shifts,interactions plus topic shifts,0.6860834360122681
translation,19,25,baselines,topic shifts,across,paragraphs,topic shifts across paragraphs,0.7055824398994446
translation,19,172,baselines,pointer - generator model,with,coverage,pointer - generator model with coverage,0.625265896320343
translation,19,172,baselines,abstractive models,has,pointer - generator model,abstractive models has pointer - generator model,0.5213645100593567
translation,19,164,experimental-setup,base version,of,roberta model,base version of roberta model,0.5999670624732971
translation,19,164,experimental-setup,base version,to extract,token features,base version to extract token features,0.6899203658103943
translation,19,164,experimental-setup,roberta model,to extract,token features,roberta model to extract token features,0.6952909827232361
translation,19,164,experimental-setup,token features,for,all experiments,token features for all experiments,0.5504390001296997
translation,19,164,experimental-setup,experimental setup,use,base version,experimental setup use base version,0.6373863816261292
translation,19,165,experimental-setup,input articles,to,1024 ( nyt ),input articles to 1024 ( nyt ),0.6149259209632874
translation,19,165,experimental-setup,experimental setup,truncate,input articles,experimental setup truncate input articles,0.7267043590545654
translation,19,166,experimental-setup,lstm models,with,256 - dimensional hidden states,lstm models with 256 - dimensional hidden states,0.6170610785484314
translation,19,166,experimental-setup,256 - dimensional hidden states,for,document encoder,256 - dimensional hidden states for document encoder,0.5646453499794006
translation,19,166,experimental-setup,256 - dimensional hidden states,for,the decoder,256 - dimensional hidden states for the decoder,0.6136918663978577
translation,19,166,experimental-setup,experimental setup,employ,lstm models,experimental setup employ lstm models,0.5243270993232727
translation,19,167,experimental-setup,residual connection,of,graph encoder,residual connection of graph encoder,0.589137852191925
translation,19,167,experimental-setup,residual connection,use,4 heads,residual connection use 4 heads,0.6760202050209045
translation,19,167,experimental-setup,4 heads,with,dimension,4 heads with dimension,0.697702169418335
translation,19,167,experimental-setup,dimension,of,72,dimension of 72,0.6845538020133972
translation,19,167,experimental-setup,experimental setup,For,residual connection,experimental setup For residual connection,0.5823078751564026
translation,19,168,experimental-setup,docgraph training and inference,prune,isolated graphs,docgraph training and inference prune isolated graphs,0.7395583391189575
translation,19,168,experimental-setup,docgraph training and inference,reduce,redundancy,docgraph training and inference reduce redundancy,0.7216635346412659
translation,19,168,experimental-setup,isolated graphs,with,fewer than three nodes,isolated graphs with fewer than three nodes,0.6412203311920166
translation,19,168,experimental-setup,isolated graphs,to increase,robustness,isolated graphs to increase robustness,0.685651957988739
translation,19,168,experimental-setup,isolated graphs,reduce,redundancy,isolated graphs reduce redundancy,0.7080653309822083
translation,19,168,experimental-setup,experimental setup,For,docgraph training and inference,experimental setup For docgraph training and inference,0.5005140900611877
translation,19,7,model,dual encoders,maintain,global context and local characteristics,dual encoders maintain global context and local characteristics,0.6464290618896484
translation,19,7,model,global context and local characteristics,of,entities,global context and local characteristics of entities,0.5778909921646118
translation,19,7,model,dual encoders,has,sequential document encoder,dual encoders has sequential document encoder,0.5820266008377075
translation,19,7,model,model,propose,dual encoders,model propose dual encoders,0.6980929374694824
translation,19,7,model,model,use of,dual encoders,model use of dual encoders,0.648844838142395
translation,19,8,model,reward,based on,multiple choice cloze test,reward based on multiple choice cloze test,0.6385106444358826
translation,19,8,model,multiple choice cloze test,to drive,model,multiple choice cloze test to drive model,0.6891705989837646
translation,19,8,model,model,to better capture,entity interactions,model to better capture entity interactions,0.7226285934448242
translation,19,8,model,model,design,reward,model design reward,0.6370587944984436
translation,19,23,model,asgard,for,abstractive summarization,asgard for abstractive summarization,0.6128543615341187
translation,19,23,model,model,present,asgard,model present asgard,0.7299169898033142
translation,19,24,model,encoder-decoder framework,enhance,regular document encoder,encoder-decoder framework enhance regular document encoder,0.5966262221336365
translation,19,24,model,regular document encoder,with,separate graph-structured encoder,regular document encoder with separate graph-structured encoder,0.625629723072052
translation,19,24,model,global context and local characteristics,of,entities,global context and local characteristics of entities,0.5778909921646118
translation,19,24,model,global context and local characteristics,by using,outputs,global context and local characteristics by using outputs,0.6899523735046387
translation,19,24,model,outputs,from,open information extraction ( openie ) system,outputs from open information extraction ( openie ) system,0.5693907141685486
translation,19,24,model,model,Under,encoder-decoder framework,model Under encoder-decoder framework,0.646593451499939
translation,19,30,model,novel multi-choice cloze reward,to drive,model,novel multi-choice cloze reward to drive model,0.6675373315811157
translation,19,30,model,model,to acquire,semantic understanding,model to acquire semantic understanding,0.6096537113189697
translation,19,30,model,semantic understanding,over,input,semantic understanding over input,0.652847945690155
translation,19,30,model,model,propose,novel multi-choice cloze reward,model propose novel multi-choice cloze reward,0.6226823925971985
translation,19,32,model,cloze reward,facilitates,acquisition of global entity interactions,cloze reward facilitates acquisition of global entity interactions,0.6255306005477905
translation,19,32,model,acquisition of global entity interactions,with,reinforcement learning,acquisition of global entity interactions with reinforcement learning,0.5986425280570984
translation,19,32,model,knowledge,has,cloze reward,knowledge has cloze reward,0.5901057124137878
translation,19,36,results,human judges,confirm,our models,human judges confirm our models,0.5669223666191101
translation,19,36,results,our models,generate,more informative summaries,our models generate more informative summaries,0.6466549038887024
translation,19,36,results,more informative summaries,with,less unfaithful errors,more informative summaries with less unfaithful errors,0.6020418405532837
translation,19,36,results,less unfaithful errors,than,counterparts,less unfaithful errors than counterparts,0.6227162480354309
translation,19,36,results,results,has,human judges,results has human judges,0.5338005423545837
translation,19,162,results,qa model,achieves,accuracy,qa model achieves accuracy,0.7017210125923157
translation,19,162,results,accuracy,of,97 %,accuracy of 97 %,0.613802433013916
translation,19,162,results,accuracy,of,95 %,accuracy of 95 %,0.6179144978523254
translation,19,162,results,97 %,on,nyt,97 % on nyt,0.6248763203620911
translation,19,162,results,95 %,on,cnn,95 % on cnn,0.6276836395263672
translation,19,162,results,results,has,qa model,results has qa model,0.5610512495040894
translation,19,183,results,asgard - seg model,trained with,rouge and cloze rewards,asgard - seg model trained with rouge and cloze rewards,0.7707330584526062
translation,19,183,results,asgard - seg model,achieves,better rouge scores,asgard - seg model achieves better rouge scores,0.6536197066307068
translation,19,183,results,better rouge scores,than,all,better rouge scores than all,0.5884752869606018
translation,19,183,results,better rouge scores,than,other comparisons,better rouge scores than other comparisons,0.5849295258522034
translation,19,183,results,other comparisons,except,fine- tuned bart,other comparisons except fine- tuned bart,0.7143642902374268
translation,19,183,results,better rouge scores,has,"lin and hovy , 2003 )","better rouge scores has lin and hovy , 2003 )",0.5598288774490356
translation,19,183,results,all,has,other comparisons,all has other comparisons,0.5635269284248352
translation,19,184,results,asgard -seg 's rouge -l score,comparable to,bart,asgard -seg 's rouge -l score comparable to bart,0.7102866768836975
translation,19,184,results,results,has,asgard -seg 's rouge -l score,results has asgard -seg 's rouge -l score,0.5714318156242371
translation,19,186,results,both our asgard - doc and asgard - seg models,yield,significantly higher rouge scores,both our asgard - doc and asgard - seg models yield significantly higher rouge scores,0.6638790369033813
translation,19,186,results,significantly higher rouge scores,than,variant,significantly higher rouge scores than variant,0.5864513516426086
translation,19,186,results,results,has,both our asgard - doc and asgard - seg models,results has both our asgard - doc and asgard - seg models,0.5140712857246399
translation,19,188,results,asgard -doc and asgard - seg with cloze reward ( r cloze ),obtain,significantly higher scores,asgard -doc and asgard - seg with cloze reward ( r cloze ) obtain significantly higher scores,0.5832711458206177
translation,19,188,results,significantly higher scores,compared to,models,significantly higher scores compared to models,0.6974554657936096
translation,19,188,results,models,trained with,rouge reward only,models trained with rouge reward only,0.7309419512748718
translation,19,188,results,results,has,asgard -doc and asgard - seg with cloze reward ( r cloze ),results has asgard -doc and asgard - seg with cloze reward ( r cloze ),0.5352420210838318
translation,19,190,results,asgard - seg,has,outperforms,asgard - seg has outperforms,0.6255943775177002
translation,19,190,results,outperforms,has,asgard - doc,outperforms has asgard - doc,0.631060779094696
translation,19,190,results,results,find that,asgard - seg,results find that asgard - seg,0.6546022295951843
translation,19,191,results,results,on,cnn / dm,results on cnn / dm,0.548747181892395
translation,19,192,results,similar trends,on,cnn / dm articles,similar trends on cnn / dm articles,0.5640319585800171
translation,19,192,results,asgard - doc,trained with,combined rouge and cloze reward,asgard - doc trained with combined rouge and cloze reward,0.7530070543289185
translation,19,192,results,combined rouge and cloze reward,produces,better rouge scores,combined rouge and cloze reward produces better rouge scores,0.6377632021903992
translation,19,192,results,better rouge scores,than,bertsumextabs and unilm,better rouge scores than bertsumextabs and unilm,0.5799392461776733
translation,19,192,results,results,observe,similar trends,results observe similar trends,0.5820894837379456
translation,19,197,results,our models,with,graph encoders,our models with graph encoders,0.630182683467865
translation,19,197,results,our models,perform,better,our models perform better,0.6135782599449158
translation,19,197,results,better,than,variant without it,better than variant without it,0.6341612339019775
translation,19,197,results,results,see that,our models,results see that our models,0.6678143739700317
translation,19,205,results,our asgard - seg model,obtains,better scores,our asgard - seg model obtains better scores,0.6270646452903748
translation,19,205,results,better scores,in,informativeness and fluency,better scores in informativeness and fluency,0.46536964178085327
translation,19,205,results,better scores,compared to,variant,better scores compared to variant,0.7167787551879883
translation,19,205,results,variant,has,without the graph encoder,variant has without the graph encoder,0.6193075180053711
translation,19,205,results,results,see that,our asgard - seg model,results see that our asgard - seg model,0.6444110870361328
translation,19,212,results,our graph- enhanced model,produces,significantly fewer mistakes,our graph- enhanced model produces significantly fewer mistakes,0.6661045551300049
translation,19,212,results,significantly fewer mistakes,compared to,model,significantly fewer mistakes compared to model,0.736821711063385
translation,19,212,results,model,without,graph information,model without graph information,0.7355488538742065
translation,19,212,results,out - of- context and deletion or substitution errors,has,our graph- enhanced model,out - of- context and deletion or substitution errors has our graph- enhanced model,0.5698718428611755
translation,19,212,results,results,for,out - of- context and deletion or substitution errors,results for out - of- context and deletion or substitution errors,0.5784040689468384
translation,20,156,ablation-analysis,encoder,to,roberta - base,encoder to roberta - base,0.5668755769729614
translation,20,156,ablation-analysis,encoder,has,performance,encoder has performance,0.5466384291648865
translation,20,156,ablation-analysis,roberta - base,has,performance,roberta - base has performance,0.6190093755722046
translation,20,156,ablation-analysis,ablation analysis,change,encoder,ablation analysis change encoder,0.6512295007705688
translation,20,137,experimental-setup,base version of bert,to implement,our models,base version of bert to implement our models,0.7360849976539612
translation,20,137,experimental-setup,experimental setup,use,base version of bert,experimental setup use base version of bert,0.6363224983215332
translation,20,138,experimental-setup,adam optimizer,with,warming - up,adam optimizer with warming - up,0.6577191352844238
translation,20,138,experimental-setup,batch size,of,32,batch size of 32,0.6741614937782288
translation,20,138,experimental-setup,experimental setup,has,adam optimizer,experimental setup has adam optimizer,0.5293667316436768
translation,20,143,experimental-setup,siamese - bert model,on,cnn / dm,siamese - bert model on cnn / dm,0.5491845607757568
translation,20,143,experimental-setup,siamese - bert model,use,8 tesla- v100 - 16g gpus,siamese - bert model use 8 tesla- v100 - 16g gpus,0.6017186641693115
translation,20,143,experimental-setup,8 tesla- v100 - 16g gpus,for about 30 hours,training,8 tesla- v100 - 16g gpus for about 30 hours training,0.7521969676017761
translation,20,143,experimental-setup,experimental setup,To obtain,siamese - bert model,experimental setup To obtain siamese - bert model,0.582886278629303
translation,20,30,model,extractive summarization,as,semantic text matching problem,extractive summarization as semantic text matching problem,0.47854694724082947
translation,20,30,model,model,conceptualize,extractive summarization,model conceptualize extractive summarization,0.6511373519897461
translation,20,34,model,extractive summarization,propose,siamese - bert architecture,extractive summarization propose siamese - bert architecture,0.6283749938011169
translation,20,34,model,siamese - bert architecture,to compute,similarity,siamese - bert architecture to compute similarity,0.7369126677513123
translation,20,34,model,similarity,between,source document,similarity between source document,0.5861660838127136
translation,20,34,model,similarity,between,candidate summary,similarity between candidate summary,0.6703184843063354
translation,20,34,model,model,Specific to,extractive summarization,model Specific to extractive summarization,0.6726552248001099
translation,20,34,model,model,propose,siamese - bert architecture,model propose siamese - bert architecture,0.6838381290435791
translation,20,35,model,siamese bert,leverages,"pre-trained bert ( devlin et al. , 2019 )","siamese bert leverages pre-trained bert ( devlin et al. , 2019 )",0.663781464099884
translation,20,35,model,"pre-trained bert ( devlin et al. , 2019 )",in,siamese network structure,"pre-trained bert ( devlin et al. , 2019 ) in siamese network structure",0.48874184489250183
translation,20,35,model,semantically meaningful text embeddings,compared using,cosine-similarity,semantically meaningful text embeddings compared using cosine-similarity,0.6181989908218384
translation,20,35,model,model,has,siamese bert,model has siamese bert,0.5858060121536255
translation,20,45,results,state - of- the - art extractive result,on,cnn / dailymail ( 44.41 in rouge - 1 ),state - of- the - art extractive result on cnn / dailymail ( 44.41 in rouge - 1 ),0.4579637944698334
translation,20,45,results,cnn / dailymail ( 44.41 in rouge - 1 ),by only using,base version of bert,cnn / dailymail ( 44.41 in rouge - 1 ) by only using base version of bert,0.7676202058792114
translation,20,45,results,results,obtain,state - of- the - art extractive result,results obtain state - of- the - art extractive result,0.5579274892807007
translation,20,147,results,results,on,cnn / dm,results on cnn / dm,0.548747181892395
translation,20,152,results,trigram blocking,is,simple yet effective heuristic,trigram blocking is simple yet effective heuristic,0.5714223980903625
translation,20,152,results,simple yet effective heuristic,on,cnn / dm,simple yet effective heuristic on cnn / dm,0.5570305585861206
translation,20,152,results,all redundancy removal methods,based on,neural models,all redundancy removal methods based on neural models,0.6426535248756409
translation,20,152,results,results,has,trigram blocking,results has trigram blocking,0.538478434085846
translation,20,153,results,all competitors,by,large margin,all competitors by large margin,0.6121708154678345
translation,20,153,results,proposed matchsum,has,outperformed,proposed matchsum has outperformed,0.6342592835426331
translation,20,153,results,outperformed,has,all competitors,outperformed has all competitors,0.6222853660583496
translation,20,154,results,bertext,by,1.51 rouge - 1 score,bertext by 1.51 rouge - 1 score,0.5448479056358337
translation,20,154,results,bertext,when using,bert - base as the encoder,bertext when using bert - base as the encoder,0.7306499481201172
translation,20,172,results,trigram blocking,works,well,trigram blocking works well,0.6134951114654541
translation,20,172,results,trigram blocking,not always maintain,stable improvement,trigram blocking not always maintain stable improvement,0.7018967866897583
translation,20,172,results,well,on,cnn / dm,well on cnn / dm,0.580892026424408
translation,20,172,results,cnn / dm,not always maintain,stable improvement,cnn / dm not always maintain stable improvement,0.7070764303207397
translation,20,172,results,results,presents,trigram blocking,results presents trigram blocking,0.6357791423797607
translation,20,173,results,ngram blocking,causes,large performance drop,ngram blocking causes large performance drop,0.6545063257217407
translation,20,173,results,little effect,on,wikihow and multi-news,little effect on wikihow and multi-news,0.5616491436958313
translation,20,173,results,large performance drop,on,pubmed,large performance drop on pubmed,0.6132296323776245
translation,20,173,results,ngram blocking,has,little effect,ngram blocking has little effect,0.6361730694770813
translation,20,173,results,results,has,ngram blocking,results has ngram blocking,0.5447913408279419
translation,21,47,results,small gain,exclude,single worst assessor,small gain exclude single worst assessor,0.7115569114685059
translation,21,47,results,small gain,excluding,two assessors,small gain excluding two assessors,0.6889539361000061
translation,21,47,results,two assessors,results in,decreased correlation,two assessors results in decreased correlation,0.6449766159057617
translation,21,47,results,results,case of,pyramid,results case of pyramid,0.6379191279411316
translation,21,68,results,noisy topics,from,training data,noisy topics from training data,0.5343205332756042
translation,21,68,results,noisy topics,does improve,correlations,noisy topics does improve correlations,0.7221097350120544
translation,21,68,results,correlations,with,manual metrics,correlations with manual metrics,0.6221632361412048
translation,21,68,results,manual metrics,in,most cases,manual metrics in most cases,0.5382969975471497
translation,21,68,results,results,removing,noisy topics,results removing noisy topics,0.7061536312103271
translation,21,69,results,greatest increase,takes place in,classy 's correlations,greatest increase takes place in classy 's correlations,0.7159654498100281
translation,21,69,results,greatest increase,takes place in,correlations,greatest increase takes place in correlations,0.6967436671257019
translation,21,69,results,classy 's correlations,with,responsiveness,classy 's correlations with responsiveness,0.6751415133476257
translation,21,69,results,responsiveness,for,main summaries,responsiveness for main summaries,0.6388685703277588
translation,21,69,results,main summaries,in,allpeers case,main summaries in allpeers case,0.5330430269241333
translation,21,69,results,correlations,with,readability,correlations with readability,0.6556689143180847
translation,21,69,results,results,has,greatest increase,results has greatest increase,0.5799432992935181
translation,22,9,ablation-analysis,timelines,guided by,filtering,timelines guided by filtering,0.7435054183006287
translation,22,9,ablation-analysis,overall summarization performance,increased by,significant 5.9 %,overall summarization performance increased by significant 5.9 %,0.658232569694519
translation,22,9,ablation-analysis,timelines,has,overall summarization performance,timelines has overall summarization performance,0.5290412902832031
translation,22,9,ablation-analysis,filtering,has,overall summarization performance,filtering has overall summarization performance,0.5481387376785278
translation,22,9,ablation-analysis,ablation analysis,selectively using,timelines,ablation analysis selectively using timelines,0.7232571244239807
translation,22,184,ablation-analysis,improvement,increases to,5.9 %,improvement increases to 5.9 %,0.7208718657493591
translation,22,184,ablation-analysis,reliability filtering,has,improvement,reliability filtering has improvement,0.5757814645767212
translation,22,185,ablation-analysis,drop,in,r - 2,drop in r - 2,0.655142605304718
translation,22,185,ablation-analysis,r - 2,each time,feature,r - 2 each time feature,0.744564414024353
translation,22,185,ablation-analysis,ablation analysis,has,ablation,ablation analysis has ablation,0.511965274810791
translation,22,214,ablation-analysis,summary,on,left,summary on left,0.5909895896911621
translation,22,214,ablation-analysis,summary,on,right,summary on right,0.5830817222595215
translation,22,214,ablation-analysis,summary,achieved,r - 2 score,summary achieved r - 2 score,0.712058424949646
translation,22,214,ablation-analysis,left,achieved,r - 2 score,left achieved r - 2 score,0.6518129110336304
translation,22,214,ablation-analysis,r - 2 score,of,0.1215,r - 2 score of 0.1215,0.5470753312110901
translation,22,214,ablation-analysis,right,achieved,0.0861,right achieved 0.0861,0.6520245671272278
translation,22,214,ablation-analysis,ablation analysis,has,summary,ablation analysis has summary,0.5297985076904297
translation,22,176,experiments,classy and polycom,are,top performing systems,classy and polycom are top performing systems,0.6249589920043945
translation,22,176,experiments,top performing systems,at,tac - 2011,top performing systems at tac - 2011,0.5442991852760315
translation,22,5,model,fully automated temporal processing system,to generate,timeline,fully automated temporal processing system to generate timeline,0.6920582056045532
translation,22,5,model,timeline,for,each input document,timeline for each input document,0.6088447570800781
translation,22,5,model,model,employ,fully automated temporal processing system,model employ fully automated temporal processing system,0.6092463135719299
translation,22,7,model,modification,to,maximal marginal relevance,modification to maximal marginal relevance,0.5521053075790405
translation,22,7,model,maximal marginal relevance,promotes,temporal diversity,maximal marginal relevance promotes temporal diversity,0.618665874004364
translation,22,7,model,temporal diversity,by way of computing,time span similarity,temporal diversity by way of computing time span similarity,0.691909909248352
translation,22,7,model,timemmr,has,modification,timemmr has modification,0.6043243408203125
translation,22,7,model,summarizing,has,certain document sets,summarizing has certain document sets,0.4939654767513275
translation,22,7,model,model,propose,timemmr,model propose timemmr,0.6441447734832764
translation,22,8,model,filtering metric,to discard,noisy timelines,filtering metric to discard noisy timelines,0.7110850214958191
translation,22,8,model,noisy timelines,generated by,our automatic processes,noisy timelines generated by our automatic processes,0.6570253968238831
translation,22,8,model,noisy timelines,to purify,timeline input,noisy timelines to purify timeline input,0.647320032119751
translation,22,8,model,timeline input,for,summarization,timeline input for summarization,0.5968821048736572
translation,22,8,model,model,propose,filtering metric,model propose filtering metric,0.6841998100280762
translation,22,34,model,timelines,as a representation of,temporal information,timelines as a representation of temporal information,0.7139682173728943
translation,22,34,model,timelines,incorporate them into,state - of - the - art multidocument summarization system,timelines incorporate them into state - of - the - art multidocument summarization system,0.6645760536193848
translation,22,34,model,model,construct,timelines,model construct timelines,0.719532310962677
translation,22,35,model,three novel features,derived from,timelines,three novel features derived from timelines,0.674368679523468
translation,22,35,model,timemmr,modification to,traditional maximal marginal relevance ( mmr ),timemmr modification to traditional maximal marginal relevance ( mmr ),0.7087307572364807
translation,22,35,model,model,achieved with,three novel features,model achieved with three novel features,0.7015706300735474
translation,22,36,results,timemmr,promotes,diversity,timemmr promotes diversity,0.6621261239051819
translation,22,36,results,diversity,by additionally considering,temporal information,diversity by additionally considering temporal information,0.6936935186386108
translation,22,36,results,temporal information,instead of just,lexical similarities,temporal information instead of just lexical similarities,0.5926805734634399
translation,22,36,results,results,has,timemmr,results has timemmr,0.5675005316734314
translation,22,177,results,swing,is,very competitive baseline,swing is very competitive baseline,0.5981355905532837
translation,22,177,results,results,see that,swing,results see that swing,0.5334388017654419
translation,22,183,results,statistically significant improvement,of,4.1 %,statistically significant improvement of 4.1 %,0.5169409513473511
translation,22,183,results,4.1 %,use of,all three features,4.1 % use of all three features,0.6616840362548828
translation,22,183,results,all three features,over,swing,all three features over swing,0.6823115944862366
translation,22,183,results,results,has,statistically significant improvement,results has statistically significant improvement,0.5260407328605652
translation,23,6,model,multi-task learning framework,to take advantage of,existing data,multi-task learning framework to take advantage of existing data,0.6631326079368591
translation,23,6,model,existing data,for,extractive summarization,existing data for extractive summarization,0.5891650319099426
translation,23,6,model,existing data,for,sentence compression,existing data for sentence compression,0.6177603602409363
translation,23,6,model,model,propose,multi-task learning framework,model propose multi-task learning framework,0.6368643641471863
translation,23,24,model,model,derive,dual decomposition framework,model derive dual decomposition framework,0.5784404873847961
translation,23,25,model,orders of magnitude more efficient,than,ilp - based approaches,orders of magnitude more efficient than ilp - based approaches,0.5813299417495728
translation,23,25,model,orders of magnitude more efficient,allows,three well -known metrics,orders of magnitude more efficient allows three well -known metrics,0.6101575493812561
translation,23,25,model,three well -known metrics,of,summaries,three well -known metrics of summaries,0.5607375502586365
translation,23,25,model,three well -known metrics,of,informativeness,three well -known metrics of informativeness,0.5241890549659729
translation,23,25,model,three well -known metrics,be treated,separately,three well -known metrics be treated separately,0.6378705501556396
translation,23,25,model,separately,in,modular fashion,separately in modular fashion,0.606701135635376
translation,23,25,model,model,allows,three well -known metrics,model allows three well -known metrics,0.6620956659317017
translation,23,26,model,novel knapsack factor,along with,linear-time algorithm,novel knapsack factor along with linear-time algorithm,0.5672582387924194
translation,23,26,model,linear-time algorithm,for,dual decomposition subproblem,linear-time algorithm for dual decomposition subproblem,0.5663432478904724
translation,23,26,model,model,contribute with,novel knapsack factor,model contribute with novel knapsack factor,0.6908988356590271
translation,23,27,model,compressive summarizers,using,auxiliary data,compressive summarizers using auxiliary data,0.670516848564148
translation,23,27,model,auxiliary data,for,extractive summarization,auxiliary data for extractive summarization,0.5850206613540649
translation,23,27,model,auxiliary data,for,sentence compression,auxiliary data for sentence compression,0.6154888868331909
translation,23,27,model,model,propose,multi-task learning,model propose multi-task learning,0.6510700583457947
translation,23,28,model,structured predictors,that share,some of their parts,structured predictors that share some of their parts,0.7206646203994751
translation,23,28,model,model,adapt,framework of evgeniou and pontil ( 2004 ),model adapt framework of evgeniou and pontil ( 2004 ),0.7390602231025696
translation,23,28,model,model,to train,structured predictors,model to train structured predictors,0.7406725287437439
translation,23,49,model,alternating directions dual decomposition,for solving,linear relaxation,alternating directions dual decomposition for solving linear relaxation,0.6908729672431946
translation,23,49,model,alternating directions dual decomposition,resembles,subgradientbased algorithm,alternating directions dual decomposition resembles subgradientbased algorithm,0.5896783471107483
translation,23,49,model,ad 3,resembles,subgradientbased algorithm,ad 3 resembles subgradientbased algorithm,0.6397070288658142
translation,23,49,model,alternating directions dual decomposition,has,ad 3,alternating directions dual decomposition has ad 3,0.6320942044258118
translation,23,49,model,alternating directions dual decomposition,has,ad 3,alternating directions dual decomposition has ad 3,0.6320942044258118
translation,23,49,model,model,employ,alternating directions dual decomposition,model employ alternating directions dual decomposition,0.5913261771202087
translation,23,50,model,original problem,into,several components,original problem into several components,0.600631594657898
translation,23,50,model,independent local subproblems,at,each component,independent local subproblems at each component,0.5081559419631958
translation,23,50,model,adjusting multipliers,to promote,agreement,adjusting multipliers to promote agreement,0.6821568608283997
translation,23,50,model,solving,has,independent local subproblems,solving has independent local subproblems,0.5084918141365051
translation,23,124,model,multi-task learning framework,for,compressive summarization,multi-task learning framework for compressive summarization,0.5504137873649597
translation,23,124,model,model,put together,multi-task learning framework,model put together multi-task learning framework,0.5977647304534912
translation,23,209,model,fast and modular dual decomposition algorithm,which is,orders of magnitude faster,fast and modular dual decomposition algorithm which is orders of magnitude faster,0.5967825055122375
translation,23,209,model,orders of magnitude faster,than,ilp - based approaches,orders of magnitude faster than ilp - based approaches,0.5491169095039368
translation,23,209,model,ad 3,has,fast and modular dual decomposition algorithm,ad 3 has fast and modular dual decomposition algorithm,0.5772845149040222
translation,23,209,model,model,decode,ad 3,model decode ad 3,0.7291910648345947
translation,23,29,results,tac data ( ?5 ),yield,state - of - theart results,tac data ( ?5 ) yield state - of - theart results,0.716096818447113
translation,23,180,results,rouge and pyramid scores,show,compressive summarizers,rouge and pyramid scores show compressive summarizers,0.6549011468887329
translation,23,180,results,compressive summarizers,yield,considerable benefits,compressive summarizers yield considerable benefits,0.727015495300293
translation,23,180,results,considerable benefits,in,content coverage,considerable benefits in content coverage,0.5062096118927002
translation,23,180,results,content coverage,over,extractive systems,content coverage over extractive systems,0.6616224646568298
translation,23,180,results,results,has,rouge and pyramid scores,results has rouge and pyramid scores,0.5469376444816589
translation,23,181,results,clear benefit,by,training,clear benefit by training,0.598189651966095
translation,23,181,results,clear benefit,with,consistent gain,clear benefit with consistent gain,0.6190463304519653
translation,23,181,results,training,in,multi-task setting,training in multi-task setting,0.5417371392250061
translation,23,181,results,training,with,consistent gain,training with consistent gain,0.6926261782646179
translation,23,181,results,consistent gain,in both,coverage and linguistic quality,consistent gain in both coverage and linguistic quality,0.6322400569915771
translation,23,181,results,results,see,clear benefit,results see clear benefit,0.5817082524299622
translation,23,182,results,rouge - 2 score ( 12.30 % ),highest reported on,tac - 2008 dataset,rouge - 2 score ( 12.30 % ) highest reported on tac - 2008 dataset,0.6478849053382874
translation,23,182,results,tac - 2008 dataset,with,little harm,tac - 2008 dataset with little harm,0.6457473039627075
translation,23,182,results,little harm,in,grammaticality,little harm in grammaticality,0.49461033940315247
translation,23,182,results,results,has,rouge - 2 score ( 12.30 % ),results has rouge - 2 score ( 12.30 % ),0.5257114171981812
translation,23,195,results,proposed configuration ( ad 3 - 1000 ),is,orders of magnitude faster,proposed configuration ( ad 3 - 1000 ) is orders of magnitude faster,0.5701572895050049
translation,23,195,results,proposed configuration ( ad 3 - 1000 ),is,5 times faster,proposed configuration ( ad 3 - 1000 ) is 5 times faster,0.5635145306587219
translation,23,195,results,orders of magnitude faster,than,ilp solver,orders of magnitude faster than ilp solver,0.5055667161941528
translation,23,195,results,5 times faster,than,relaxed variant,5 times faster than relaxed variant,0.5363466143608093
translation,23,195,results,results,see that,proposed configuration ( ad 3 - 1000 ),results see that proposed configuration ( ad 3 - 1000 ),0.6855316758155823
translation,23,205,results,our system 's runtime,is,competitive,our system 's runtime is competitive,0.54410320520401
translation,23,205,results,results,see that,our system 's runtime,results see that our system 's runtime,0.6235383152961731
translation,24,176,ablation-analysis,abbreviations and morphological differences,appear to be,important factors,abbreviations and morphological differences appear to be important factors,0.5776810646057129
translation,24,176,ablation-analysis,ablation analysis,has,abbreviations and morphological differences,ablation analysis has abbreviations and morphological differences,0.46689990162849426
translation,24,123,baselines,third variant,use,weights,third variant use weights,0.7247860431671143
translation,24,123,baselines,weights,of,fine- tuned bert,weights of fine- tuned bert,0.5882902145385742
translation,24,123,baselines,fine- tuned bert,in,bertext,fine- tuned bert in bertext,0.5993567705154419
translation,24,123,baselines,fine- tuned bert,for,encoder,fine- tuned bert for encoder,0.6203609704971313
translation,24,129,baselines,model,with,coverage penalty ( ptgen + cov ),model with coverage penalty ( ptgen + cov ),0.6082934141159058
translation,24,132,baselines,lead - 2,is,best lead -n baseline,lead - 2 is best lead -n baseline,0.5731740593910217
translation,24,132,baselines,best lead -n baseline,for,liputan6,best lead -n baseline for liputan6,0.6034112572669983
translation,24,132,baselines,baselines,has,lead - 2,baselines has lead - 2,0.5995181202888489
translation,24,100,experimental-setup,indobert,using,huggingface framework,indobert using huggingface framework,0.6935473084449768
translation,24,100,experimental-setup,default configuration,of,bert - base ( uncased ),default configuration of bert - base ( uncased ),0.5842257738113403
translation,24,100,experimental-setup,hidden size,=,768d,hidden size = 768d,0.6768637299537659
translation,24,100,experimental-setup,experimental setup,implement,indobert,experimental setup implement indobert,0.6879242658615112
translation,24,101,experimental-setup,indobert,with,"31,923 word-pieces ( vocabulary )","indobert with 31,923 word-pieces ( vocabulary )",0.6143553256988525
translation,24,101,experimental-setup,"31,923 word-pieces ( vocabulary )",for,2 million steps,"31,923 word-pieces ( vocabulary ) for 2 million steps",0.581727921962738
translation,24,101,experimental-setup,experimental setup,train,indobert,experimental setup train indobert,0.7000207901000977
translation,24,107,experimental-setup,transformer encoder,configured,layers = 2,transformer encoder configured layers = 2,0.7490967512130737
translation,24,107,experimental-setup,transformer encoder,configured,hidden size,transformer encoder configured hidden size,0.711613655090332
translation,24,107,experimental-setup,transformer encoder,configured,feed -forward,transformer encoder configured feed -forward,0.7134889364242554
translation,24,107,experimental-setup,transformer encoder,configured,heads = 8,transformer encoder configured heads = 8,0.7334480881690979
translation,24,107,experimental-setup,hidden size,=,768,hidden size = 768,0.6986480355262756
translation,24,107,experimental-setup,feed -forward,=,"2,048","feed -forward = 2,048",0.6756148934364319
translation,24,107,experimental-setup,experimental setup,has,transformer encoder,experimental setup has transformer encoder,0.505271852016449
translation,24,109,experimental-setup,"50,000 steps",on,3 ? v100 16gb gpus,"50,000 steps on 3 ? v100 16gb gpus",0.5121698975563049
translation,24,109,experimental-setup,experimental setup,train for,"50,000 steps","experimental setup train for 50,000 steps",0.6949471235275269
translation,24,118,experimental-setup,transformer decoder,initialized with,random parameters,transformer decoder initialized with random parameters,0.7947223782539368
translation,24,118,experimental-setup,experimental setup,has,transformer decoder,experimental setup has transformer decoder,0.5693507790565491
translation,24,119,experimental-setup,transformer decoder,configured,layers = 6,transformer decoder configured layers = 6,0.7568254470825195
translation,24,119,experimental-setup,transformer decoder,configured,heads = 8,transformer decoder configured heads = 8,0.7704681158065796
translation,24,119,experimental-setup,transformer decoder,use,different learning rate,transformer decoder use different learning rate,0.6117660403251648
translation,24,119,experimental-setup,hidden size,=,768,hidden size = 768,0.6986480355262756
translation,24,119,experimental-setup,different learning rate,for,bert,different learning rate for bert,0.6324715614318848
translation,24,119,experimental-setup,different learning rate,when training,model,different learning rate when training model,0.7112084627151489
translation,24,119,experimental-setup,decoder,when training,model,decoder when training model,0.7156834006309509
translation,24,119,experimental-setup,lr = 2e,for,bert and the transformer decoder,lr = 2e for bert and the transformer decoder,0.6508602499961853
translation,24,119,experimental-setup,experimental setup,use,different learning rate,experimental setup use different learning rate,0.6201101541519165
translation,24,119,experimental-setup,experimental setup,has,transformer decoder,experimental setup has transformer decoder,0.5693507790565491
translation,24,120,experimental-setup,adam optimizer,for,"200,000 steps","adam optimizer for 200,000 steps",0.5982627868652344
translation,24,120,experimental-setup,adam optimizer,on,4 ? v100 16gb gpus,adam optimizer on 4 ? v100 16gb gpus,0.47209692001342773
translation,24,120,experimental-setup,"200,000 steps",on,4 ? v100 16gb gpus,"200,000 steps on 4 ? v100 16gb gpus",0.5049228668212891
translation,24,120,experimental-setup,experimental setup,trained with,adam optimizer,experimental setup trained with adam optimizer,0.7210889458656311
translation,24,121,experimental-setup,summary generation,use,beam width,summary generation use beam width,0.5944111943244934
translation,24,121,experimental-setup,summary generation,use,trigram blocking,summary generation use trigram blocking,0.6061694025993347
translation,24,121,experimental-setup,summary generation,use,length penalty,summary generation use length penalty,0.6464108228683472
translation,24,121,experimental-setup,summary generation,to generate,at least two sentences,summary generation to generate at least two sentences,0.6380698680877686
translation,24,121,experimental-setup,summary generation,to generate,at least 15 words,summary generation to generate at least 15 words,0.6235839128494263
translation,24,121,experimental-setup,beam width,=,5,beam width = 5,0.6853561997413635
translation,24,121,experimental-setup,length penalty,to generate,at least two sentences,length penalty to generate at least two sentences,0.5927532911300659
translation,24,121,experimental-setup,length penalty,to generate,at least 15 words,length penalty to generate at least 15 words,0.6039854884147644
translation,24,121,experimental-setup,experimental setup,For,summary generation,experimental setup For summary generation,0.5558696389198303
translation,24,5,experiments,articles,from,liputan6.com,articles from liputan6.com,0.5473427772521973
translation,24,5,experiments,articles,obtain,"215,827 documentsummary pairs","articles obtain 215,827 documentsummary pairs",0.5814493298530579
translation,24,94,model,bert,produces,series of contextual representations,bert produces series of contextual representations,0.6168416738510132
translation,24,94,model,series of contextual representations,for,word tokens,series of contextual representations for word tokens,0.588643491268158
translation,24,94,model,series of contextual representations,feed into,( second ) transformer encoder / decoder,series of contextual representations feed into ( second ) transformer encoder / decoder,0.6983561515808105
translation,24,94,model,( second ) transformer encoder / decoder,for,extractive / abstractive summarization model,( second ) transformer encoder / decoder for extractive / abstractive summarization model,0.6096901893615723
translation,24,94,model,model,has,bert,model has bert,0.6085957288742065
translation,24,134,results,substantial gap,between,oracle and lead -2,substantial gap between oracle and lead -2,0.692401111125946
translation,24,134,results,12 - 15 points,for,r1,12 - 15 points for r1,0.6399158835411072
translation,24,134,results,5 - 7 points,for,bertscore,5 - 7 points for bertscore,0.6334263682365417
translation,24,134,results,oracle and lead -2,has,12 - 15 points,oracle and lead -2 has 12 - 15 points,0.5905755162239075
translation,24,136,results,performance,between,canonical and xtreme test sets,performance between canonical and xtreme test sets,0.5860991477966309
translation,24,136,results,performance,see,substantial drop,performance see substantial drop,0.5953092575073242
translation,24,136,results,canonical and xtreme test sets,see,substantial drop,canonical and xtreme test sets see substantial drop,0.619178295135498
translation,24,136,results,substantial drop,in,performance,substantial drop in performance,0.576037585735321
translation,24,136,results,performance,for,lead -n and oracle,performance for lead -n and oracle,0.6486397981643677
translation,24,136,results,results,Comparing,performance,results Comparing performance,0.7179481983184814
translation,24,137,results,pointer - generator models,see,little improvement,pointer - generator models see little improvement,0.5787547826766968
translation,24,137,results,little improvement,when including,coverage mechanism,little improvement when including coverage mechanism,0.6215859055519104
translation,24,137,results,coverage mechanism,has,ptgen + cov vs. ptgen ),coverage mechanism has ptgen + cov vs. ptgen ),0.5647642612457275
translation,24,137,results,results,For,pointer - generator models,results For pointer - generator models,0.5800169110298157
translation,24,141,results,very well,with,mbert and in-dobert models,very well with mbert and in-dobert models,0.6750955581665039
translation,24,141,results,very well,both,mbert and in-dobert models,very well both mbert and in-dobert models,0.7246313691139221
translation,24,141,results,ptgen models,by,comfortable margin,ptgen models by comfortable margin,0.6144068837165833
translation,24,141,results,mbert and in-dobert models,has,outperforming,mbert and in-dobert models has outperforming,0.6173199415206909
translation,24,141,results,outperforming,has,lead -n baselines,outperforming has lead -n baselines,0.5545555949211121
translation,24,141,results,outperforming,has,ptgen models,outperforming has ptgen models,0.5715691447257996
translation,24,142,results,indobert,better than,mbert,indobert better than mbert,0.7127972841262817
translation,24,142,results,mbert,showing,monolingually,mbert showing monolingually,0.6973106861114502
translation,24,142,results,mbert,has,approximately 1 rouge point better,mbert has approximately 1 rouge point better,0.6132062077522278
translation,24,142,results,results,has,indobert,results has indobert,0.5843068957328796
translation,24,143,results,best performance,achieved by,indobert 's bertextabs,best performance achieved by indobert 's bertextabs,0.7202020883560181
translation,24,143,results,results,has,best performance,results has best performance,0.5759831070899963
translation,24,144,results,improvement,over,lead - 2,improvement over lead - 2,0.7098097205162048
translation,24,144,results,improvement,over,+ 3.4 bertscore points,improvement over + 3.4 bertscore points,0.6454421877861023
translation,24,144,results,lead - 2,is,+ 4.4 r1,lead - 2 is + 4.4 r1,0.5913179516792297
translation,24,144,results,lead - 2,is,+2.62 r2,lead - 2 is +2.62 r2,0.5971940755844116
translation,24,144,results,lead - 2,is,+4.3 r3,lead - 2 is +4.3 r3,0.5980122685432434
translation,24,144,results,lead - 2,is,+ 3.4 bertscore points,lead - 2 is + 3.4 bertscore points,0.5672822594642639
translation,24,144,results,canonical test set,has,improvement,canonical test set has improvement,0.5784655809402466
translation,24,144,results,results,In,canonical test set,results In canonical test set,0.5342182517051697
translation,24,145,results,bertextabs,suffers,substantial drop,bertextabs suffers substantial drop,0.7428065538406372
translation,24,145,results,substantial drop,compared to,canonical test set,substantial drop compared to canonical test set,0.6867713928222656
translation,24,145,results,xtreme test set,has,bertextabs,xtreme test set has bertextabs,0.6150355339050293
translation,24,145,results,canonical test set,has,6 - 7 rouge,canonical test set has 6 - 7 rouge,0.6160390377044678
translation,24,145,results,results,In,xtreme test set,results In xtreme test set,0.5478925704956055
translation,24,180,results,bertextabs,occasionally fails to capture,salient information,bertextabs occasionally fails to capture salient information,0.8130288124084473
translation,24,180,results,of the summaries,have,coverage issues,of the summaries have coverage issues,0.5204869508743286
translation,24,180,results,75.0 %,contain,unnecessary ( but valid ) details,75.0 % contain unnecessary ( but valid ) details,0.5923302173614502
translation,24,180,results,average summaries,has,bertextabs,average summaries has bertextabs,0.5948975086212158
translation,24,180,results,salient information,has,100 %,salient information has 100 %,0.5481038093566895
translation,24,180,results,100 %,has,of the summaries,100 % has of the summaries,0.5945216417312622
translation,24,180,results,results,for,average summaries,results for average summaries,0.6397258639335632
translation,25,4,baselines,interactive multi-document summarization system,for,scientific articles,interactive multi-document summarization system for scientific articles,0.556850016117096
translation,25,4,baselines,scisumm,has,interactive multi-document summarization system,scisumm has interactive multi-document summarization system,0.5506964325904846
translation,25,114,baselines,scisumm,is,interactive multi-document summarization system,scisumm is interactive multi-document summarization system,0.5554296374320984
translation,25,114,baselines,interactive multi-document summarization system,for,scientific articles,interactive multi-document summarization system for scientific articles,0.556850016117096
translation,25,102,results,our system,performs,well,our system performs well,0.6882214546203613
translation,25,102,results,well,in comparison to,baseline,well in comparison to baseline,0.6789341568946838
translation,25,102,results,results,see that,our system,results see that our system,0.6429948806762695
translation,26,113,hyperparameters,best choice,for,embdist,best choice for embdist,0.6479549407958984
translation,26,113,hyperparameters,hyperparameters,has,w2v vector,hyperparameters has w2v vector,0.500109851360321
translation,26,4,model,summarization method,using,document level similarity,summarization method using document level similarity,0.6373533606529236
translation,26,4,model,summarization method,using,distributed representations of words,summarization method using distributed representations of words,0.6278843283653259
translation,26,4,model,document level similarity,based on,embeddings,document level similarity based on embeddings,0.618878960609436
translation,26,4,model,model,consider,summarization method,model consider summarization method,0.6781094074249268
translation,26,9,model,meaning of a document,using,embeddings or distributed representations of words in the document,meaning of a document using embeddings or distributed representations of words in the document,0.6455088257789612
translation,26,9,model,embeddings or distributed representations of words in the document,where,embedding of each word,embeddings or distributed representations of words in the document where embedding of each word,0.5757647156715393
translation,26,9,model,embedding of each word,represented as,real valued vector,embedding of each word represented as real valued vector,0.6686835289001465
translation,26,9,model,real valued vector,in,euclidean space,real valued vector in euclidean space,0.4945535957813263
translation,26,9,model,model,characterize,meaning of a document,model characterize meaning of a document,0.649196445941925
translation,26,112,results,exponential scaling,performed,best,exponential scaling performed best,0.2537495195865631
translation,26,112,results,best,for,rouge - 1,best for rouge - 1,0.6531229615211487
translation,26,112,results,results,proposed method EmbDist with,exponential scaling,results proposed method EmbDist with exponential scaling,0.7481490969657898
translation,26,114,results,other proposed method docemb,performed,better,other proposed method docemb performed better,0.24539537727832794
translation,26,114,results,better,than,state - of- the - art methods senemb and tfidf,better than state - of- the - art methods senemb and tfidf,0.5445108413696289
translation,26,114,results,results,has,other proposed method docemb,results has other proposed method docemb,0.5757086873054504
translation,26,116,results,tf - idf vectors,performed,worst,tf - idf vectors performed worst,0.28801071643829346
translation,26,116,results,worst,for,rouge -1,worst for rouge -1,0.641596257686615
translation,26,116,results,results,TfIdf with,tf - idf vectors,results TfIdf with tf - idf vectors,0.7180992960929871
translation,26,119,results,tfidf,performed,best,tfidf performed best,0.26097768545150757
translation,26,119,results,tfidf,for,rouge -4,tfidf for rouge -4,0.6737162470817566
translation,26,119,results,tfidf,for,rouge -4,tfidf for rouge -4,0.6737162470817566
translation,26,119,results,best,for,rouge - 2 and rouge -3,best for rouge - 2 and rouge -3,0.641323983669281
translation,26,119,results,best,for,rouge -4,best for rouge -4,0.6719483137130737
translation,26,119,results,best,for,rouge -4,best for rouge -4,0.6719483137130737
translation,26,119,results,embdist,with,logarithmic scaling,embdist with logarithmic scaling,0.7082895636558533
translation,26,119,results,logarithmic scaling,better than,tfidf,logarithmic scaling better than tfidf,0.7561918497085571
translation,26,119,results,tfidf,for,rouge -4,tfidf for rouge -4,0.6737162470817566
translation,26,119,results,n ? 2,has,tfidf,n ? 2 has tfidf,0.6074870824813843
translation,26,119,results,results,In the case of,n ? 2,results In the case of n ? 2,0.6816727519035339
translation,26,119,results,results,In the case of,tfidf,results In the case of tfidf,0.7240064740180969
translation,26,119,results,results,In the case of,embdist,results In the case of embdist,0.6727051734924316
translation,27,175,baselines,ilpbased method,solved,ilps,ilpbased method solved ilps,0.7144095301628113
translation,27,175,baselines,ilps,with,cplex ver. 12.5.1.0,ilps with cplex ver. 12.5.1.0,0.5798063278198242
translation,27,175,baselines,baselines,has,ilpbased method,baselines has ilpbased method,0.5437944531440735
translation,27,174,experimental-setup,algorithm,implemented in,c ++,algorithm implemented in c ++,0.7363659143447876
translation,27,174,experimental-setup,algorithm,compiled with,gcc version 4.8.5,algorithm compiled with gcc version 4.8.5,0.6813918352127075
translation,27,174,experimental-setup,experimental setup,has,algorithm,experimental setup has algorithm,0.5328841209411621
translation,27,177,experimental-setup,linux machine,CPU,intel xeon e5-2620 v4 2.10 ghz,linux machine CPU intel xeon e5-2620 v4 2.10 ghz,0.7579955458641052
translation,27,177,experimental-setup,linux machine,CPU,32gb ram,linux machine CPU 32gb ram,0.7795116305351257
translation,27,158,experiments,compressive summarization tasks,with,three kinds of objective functions,compressive summarization tasks with three kinds of objective functions,0.5771144032478333
translation,27,7,model,fast greedy method,for,compressive summarization,fast greedy method for compressive summarization,0.6160990595817566
translation,27,7,model,model,propose,fast greedy method,model propose fast greedy method,0.7037006616592407
translation,27,8,model,any monotone submodular objective function,including,many functions,any monotone submodular objective function including many functions,0.6147094964981079
translation,27,8,model,many functions,well -suited for,document summarization,many functions well -suited for document summarization,0.7068039178848267
translation,27,8,model,model,applicable to,any monotone submodular objective function,model applicable to any monotone submodular objective function,0.6976104378700256
translation,27,38,model,submodularity - based greedy method,for,compressive summarization,submodularity - based greedy method for compressive summarization,0.6099076271057129
translation,27,38,model,model,propose,submodularity - based greedy method,model propose submodularity - based greedy method,0.6873860359191895
translation,27,39,model,compressive counterpart,of,greedy method,compressive counterpart of greedy method,0.5945119261741638
translation,27,39,model,greedy method,for,extractive summarization,greedy method for extractive summarization,0.6256038546562195
translation,27,40,results,method,works with,any monotone submodular objective function,method works with any monotone submodular objective function,0.7056056261062622
translation,27,40,results,method,faster than,existing compressive summarization methods,method faster than existing compressive summarization methods,0.7074832916259766
translation,27,45,results,our method,achieved,more that 95 % approximation,our method achieved more that 95 % approximation,0.6689766645431519
translation,27,46,results,our method,attained,rouge 1 scores,our method attained rouge 1 scores,0.6859657168388367
translation,27,46,results,rouge 1 scores,comparable to,ilp - based method,rouge 1 scores comparable to ilp - based method,0.6418635845184326
translation,27,46,results,results,has,our method,results has our method,0.5589964985847473
translation,27,180,results,ilp - based method,are,optimal,ilp - based method are optimal,0.5951999425888062
translation,27,180,results,ilp - based method,always,optimal,ilp - based method always optimal,0.7114190459251404
translation,27,180,results,optimal,in terms of,objective values,optimal in terms of objective values,0.694108247756958
translation,27,180,results,our method,achieved,more than 95 % - approximation,our method achieved more than 95 % - approximation,0.645660400390625
translation,27,180,results,results,has,ilp - based method,results has ilp - based method,0.5424126386642456
translation,27,183,results,rouge 1 scores,of,our method,rouge 1 scores of our method,0.5594157576560974
translation,27,183,results,rouge 1 scores,comparable to,ilp - based method,rouge 1 scores comparable to ilp - based method,0.6418635845184326
translation,27,183,results,results,has,rouge 1 scores,results has rouge 1 scores,0.554366946220398
translation,27,184,results,our method,attained,slightly higher rouge 1 scores,our method attained slightly higher rouge 1 scores,0.6121363639831543
translation,27,184,results,slightly higher rouge 1 scores,than,ilp - based methods,slightly higher rouge 1 scores than ilp - based methods,0.5870065689086914
translation,27,184,results,results,With,coverage function,results With coverage function,0.6060379147529602
translation,27,185,results,approximation ratios and rouge 1 scores,imply,our method,approximation ratios and rouge 1 scores imply our method,0.5516672134399414
translation,27,185,results,compares favorably,with,ilp - based method,compares favorably with ilp - based method,0.6639924645423889
translation,27,185,results,compares favorably,in terms of,empirical performance,compares favorably in terms of empirical performance,0.7146270275115967
translation,27,185,results,our method,has,compares favorably,our method has compares favorably,0.5916560888290405
translation,27,185,results,results,on,approximation ratios and rouge 1 scores,results on approximation ratios and rouge 1 scores,0.5356566309928894
translation,27,186,results,times,has,our method,times has our method,0.6103532910346985
translation,27,186,results,our method,has,substantially outperformed,our method has substantially outperformed,0.5824815630912781
translation,27,186,results,substantially outperformed,has,ilp - based method,substantially outperformed has ilp - based method,0.5852254629135132
translation,27,212,results,1 2 ( 1 ? e ?1 / ? ) - approximation guarantee,of,our method,1 2 ( 1 ? e ?1 / ? ) - approximation guarantee of our method,0.6076366305351257
translation,27,212,results,1 2,has,approximation,1 2 has approximation,0.6055112481117249
translation,27,212,results,results,has,1 2 ( 1 ? e ?1 / ? ) - approximation guarantee,results has 1 2 ( 1 ? e ?1 / ? ) - approximation guarantee,0.58122718334198
translation,28,42,ablation-analysis,our model,can outperform,all existing competitors,our model can outperform all existing competitors,0.7615069150924683
translation,28,42,ablation-analysis,all existing competitors,on,three benchmark datasets,all existing competitors on three benchmark datasets,0.41644373536109924
translation,28,42,ablation-analysis,ablation analysis,has,our model,ablation analysis has our model,0.5855822563171387
translation,28,158,ablation-analysis,ablation analysis,has,ablation,ablation analysis has ablation,0.511965274810791
translation,28,165,ablation-analysis,low tf - idf words,leads to,increases,low tf - idf words leads to increases,0.7080852389335632
translation,28,165,ablation-analysis,low tf - idf words,leads to,drops,low tf - idf words leads to drops,0.7017581462860107
translation,28,165,ablation-analysis,increases,on,r - 1 and r -l,increases on r - 1 and r -l,0.6126181483268738
translation,28,165,ablation-analysis,drops,on,r - 2,drops on r - 2,0.6825599670410156
translation,28,165,ablation-analysis,ablation analysis,removal of,low tf - idf words,ablation analysis removal of low tf - idf words,0.6664660573005676
translation,28,168,ablation-analysis,bilstm initialization,for,sentences,bilstm initialization for sentences,0.5570496916770935
translation,28,168,ablation-analysis,bilstm initialization,show,effectiveness,bilstm initialization show effectiveness,0.5772148370742798
translation,28,168,ablation-analysis,edge features,has,word update,edge features has word update,0.5351582765579224
translation,28,168,ablation-analysis,ablation analysis,introduction of,edge features,ablation analysis introduction of edge features,0.716528594493866
translation,28,193,ablation-analysis,converge,to,baseline,converge to baseline,0.586047887802124
translation,28,193,ablation-analysis,lead baseline,has,raises,lead baseline has raises,0.6229447722434998
translation,28,131,baselines,transformer encoder,learn,pairwise interaction,transformer encoder learn pairwise interaction,0.6757674217224121
translation,28,131,baselines,pairwise interaction,between,sentences,pairwise interaction between sentences,0.6926174759864807
translation,28,131,baselines,sentences,in,purely data-driven way,sentences in purely data-driven way,0.5792486667633057
translation,28,131,baselines,purely data-driven way,with,fully connected priori,purely data-driven way with fully connected priori,0.6583234071731567
translation,28,131,baselines,baselines,has,ext-transformer extractive summarizers,baselines has ext-transformer extractive summarizers,0.5492741465568542
translation,28,132,baselines,transformer - based extractor,contains,same encoder,transformer - based extractor contains same encoder,0.6207770109176636
translation,28,132,baselines,same encoder,for,words,same encoder for words,0.6650379300117493
translation,28,132,baselines,words,followed by,12 transformer encoder layers,words followed by 12 transformer encoder layers,0.6709016561508179
translation,28,132,baselines,12 transformer encoder layers,for,sentences,12 transformer encoder layers for sentences,0.6035769581794739
translation,28,157,experiments,trigram block,shown on,cnn / dailymail,trigram block shown on cnn / dailymail,0.6281201839447021
translation,28,117,hyperparameters,single-document and multi-document summarization,limit,vocabulary,single-document and multi-document summarization limit vocabulary,0.686767041683197
translation,28,117,hyperparameters,single-document and multi-document summarization,initialize,tokens,single-document and multi-document summarization initialize tokens,0.6810660362243652
translation,28,117,hyperparameters,vocabulary,to,"50,000","vocabulary to 50,000",0.6239914298057556
translation,28,117,hyperparameters,tokens,with,300 - dimensional glove embeddings,tokens with 300 - dimensional glove embeddings,0.6120272874832153
translation,28,117,hyperparameters,hyperparameters,For,single-document and multi-document summarization,hyperparameters For single-document and multi-document summarization,0.5874243974685669
translation,28,117,hyperparameters,hyperparameters,limit,vocabulary,hyperparameters limit vocabulary,0.7128279209136963
translation,28,117,hyperparameters,hyperparameters,initialize,tokens,hyperparameters initialize tokens,0.7586647272109985
translation,28,118,hyperparameters,stop words and punctuations,when creating,word nodes,stop words and punctuations when creating word nodes,0.6308692693710327
translation,28,118,hyperparameters,stop words and punctuations,truncate,input document,stop words and punctuations truncate input document,0.6945308446884155
translation,28,118,hyperparameters,input document,to,maximum length,input document to maximum length,0.5289241075515747
translation,28,118,hyperparameters,maximum length,of,50 sentences,maximum length of 50 sentences,0.5935743451118469
translation,28,118,hyperparameters,hyperparameters,filter,stop words and punctuations,hyperparameters filter stop words and punctuations,0.7095330357551575
translation,28,118,hyperparameters,hyperparameters,truncate,input document,hyperparameters truncate input document,0.7207475900650024
translation,28,119,hyperparameters,noisy common words,remove,10 %,noisy common words remove 10 %,0.7287740707397461
translation,28,119,hyperparameters,10 %,of,vocabulary,10 % of vocabulary,0.7014767527580261
translation,28,119,hyperparameters,10 %,with,low tf - idf values,10 % with low tf - idf values,0.7103650569915771
translation,28,119,hyperparameters,low tf - idf values,over,whole dataset,low tf - idf values over whole dataset,0.6650173664093018
translation,28,119,hyperparameters,hyperparameters,To get rid of,noisy common words,hyperparameters To get rid of noisy common words,0.6521798968315125
translation,28,120,hyperparameters,sentence nodes,with,d s = 128,sentence nodes with d s = 128,0.6488485932350159
translation,28,120,hyperparameters,edge features e ij in gat e,with,d e = 50,edge features e ij in gat e with d e = 50,0.6764188408851624
translation,28,120,hyperparameters,hyperparameters,initialize,sentence nodes,hyperparameters initialize sentence nodes,0.6926376223564148
translation,28,120,hyperparameters,hyperparameters,initialize,edge features e ij in gat e,hyperparameters initialize edge features e ij in gat e,0.7265995740890503
translation,28,121,hyperparameters,each gat layer,is,8 heads,each gat layer is 8 heads,0.5906681418418884
translation,28,121,hyperparameters,inner hidden size,of,ffn layers,inner hidden size of ffn layers,0.5547449588775635
translation,28,121,hyperparameters,ffn layers,is,512,ffn layers is 512,0.5909633040428162
translation,28,121,hyperparameters,hyperparameters,has,each gat layer,hyperparameters has each gat layer,0.5024157762527466
translation,28,121,hyperparameters,hyperparameters,has,hidden size,hyperparameters has hidden size,0.535134494304657
translation,28,121,hyperparameters,hyperparameters,has,inner hidden size,hyperparameters has inner hidden size,0.5178404450416565
translation,28,122,hyperparameters,training,use,batch size,training use batch size,0.657907247543335
translation,28,122,hyperparameters,training,apply,adam optimizer,training apply adam optimizer,0.6304361820220947
translation,28,122,hyperparameters,batch size,of,32,batch size of 32,0.6741614937782288
translation,28,122,hyperparameters,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,28,122,hyperparameters,learning rate,has,5e - 4,learning rate has 5e - 4,0.5730709433555603
translation,28,122,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
translation,28,123,hyperparameters,early stop,performed when,valid loss,early stop performed when valid loss,0.7006556987762451
translation,28,123,hyperparameters,does not descent,for,three continuous epochs,does not descent for three continuous epochs,0.665826678276062
translation,28,123,hyperparameters,valid loss,has,does not descent,valid loss has does not descent,0.6285584568977356
translation,28,123,hyperparameters,hyperparameters,has,early stop,hyperparameters has early stop,0.5494171977043152
translation,28,6,model,semantic nodes,of,different granularity levels,semantic nodes of different granularity levels,0.5473554730415344
translation,28,6,model,different granularity levels,apart from,sentences,different granularity levels apart from sentences,0.7019199132919312
translation,28,7,model,additional nodes,act as,intermediary,additional nodes act as intermediary,0.7145991325378418
translation,28,7,model,additional nodes,enrich,cross-sentence relations,additional nodes enrich cross-sentence relations,0.6158645749092102
translation,28,7,model,intermediary,between,sentences,intermediary between sentences,0.679571807384491
translation,28,7,model,model,has,additional nodes,model has additional nodes,0.5844542980194092
translation,28,25,model,heterogeneous graph network,for,extractive summarization,heterogeneous graph network for extractive summarization,0.5664342641830444
translation,28,25,model,model,propose,heterogeneous graph network,model propose heterogeneous graph network,0.6375424861907959
translation,28,26,model,more semantic units,as,additional nodes,more semantic units as additional nodes,0.5074223279953003
translation,28,26,model,additional nodes,in,graph,additional nodes in graph,0.561047375202179
translation,28,26,model,relationships,between,sentences,relationships between sentences,0.6715904474258423
translation,28,26,model,model,introduce,more semantic units,model introduce more semantic units,0.6843437552452087
translation,28,73,model,heterogeneous graph,updates,node representations,heterogeneous graph updates node representations,0.6748955845832825
translation,28,73,model,node representations,by iteratively passing,messages,node representations by iteratively passing messages,0.707598865032196
translation,28,73,model,messages,between,word and sentence nodes,messages between word and sentence nodes,0.6780490279197693
translation,28,73,model,messages,via,graph attention network ( gat ),messages via graph attention network ( gat ),0.7031235098838806
translation,28,73,model,word and sentence nodes,via,graph attention network ( gat ),word and sentence nodes via graph attention network ( gat ),0.6690317392349243
translation,28,73,model,model,has,heterogeneous graph,model has heterogeneous graph,0.5415676236152649
translation,28,136,model,hetersumgraph,directly selects,sentences,hetersumgraph directly selects sentences,0.7488489747047424
translation,28,136,model,hetersumgraph,with,trigram blocking,hetersumgraph with trigram blocking,0.7080588340759277
translation,28,136,model,sentences,for,summary,sentences for summary,0.6314513683319092
translation,28,136,model,summary,by,node classification,summary by node classification,0.5593251585960388
translation,28,136,model,heter - sumgraph,with,trigram blocking,heter - sumgraph with trigram blocking,0.6990709900856018
translation,28,136,model,trigram blocking,utilizes,n-gram blocking,trigram blocking utilizes n-gram blocking,0.6292346715927124
translation,28,136,model,n-gram blocking,to reduce,redundancy,n-gram blocking to reduce redundancy,0.6917715668678284
translation,28,136,model,model,has,hetersumgraph,model has hetersumgraph,0.6015098094940186
translation,28,160,model,filtering mechanism,for,low tf - idf words,filtering mechanism for low tf - idf words,0.6144716143608093
translation,28,160,model,filtering mechanism,for,edge weights,filtering mechanism for edge weights,0.6068642139434814
translation,28,160,model,model,remove,filtering mechanism,model remove filtering mechanism,0.7282124757766724
translation,28,142,results,results,on,cnn / dailymail,results on cnn / dailymail,0.5091644525527954
translation,28,145,results,our heterogeneous graphs,achieve,more than 0.6/0.51/0.7 improvements,our heterogeneous graphs achieve more than 0.6/0.51/0.7 improvements,0.6537929773330688
translation,28,145,results,more than 0.6/0.51/0.7 improvements,on,"r -1 , r - 2 and r -l","more than 0.6/0.51/0.7 improvements on r -1 , r - 2 and r -l",0.5602840185165405
translation,28,145,results,ext-bilstm,has,our heterogeneous graphs,ext-bilstm has our heterogeneous graphs,0.6135751008987427
translation,28,145,results,results,Compared with,ext-bilstm,results Compared with ext-bilstm,0.675927996635437
translation,28,146,results,outperform,based on,fully connected relationships,outperform based on fully connected relationships,0.7105538845062256
translation,28,146,results,ext-transformer,based on,fully connected relationships,ext-transformer based on fully connected relationships,0.6885825991630554
translation,28,146,results,outperform,has,ext-transformer,outperform has ext-transformer,0.683534562587738
translation,28,148,results,trigram blocking,leads to,great improvement,trigram blocking leads to great improvement,0.6905325651168823
translation,28,148,results,great improvement,on,all rouge metrics,great improvement on all rouge metrics,0.4940711557865143
translation,28,148,results,hetersumgraph,has,outperforms,hetersumgraph has outperforms,0.638544499874115
translation,28,148,results,outperforms,has,all previous non-bert - based summarization systems,outperforms has all previous non-bert - based summarization systems,0.5661518573760986
translation,28,151,results,our hetersumgraph,achieve,0.61 improvements,our hetersumgraph achieve 0.61 improvements,0.6224396824836731
translation,28,151,results,0.61 improvements,on,r - 1,0.61 improvements on r - 1,0.5469340682029724
translation,28,151,results,r - 1,over,her,r - 1 over her,0.7297986745834351
translation,28,151,results,r - 1,over,without policy,r - 1 over without policy,0.7145960927009583
translation,28,151,results,her,for,reorganized summaries,her for reorganized summaries,0.6641181111335754
translation,28,151,results,her,for,reorganized summaries,her for reorganized summaries,0.6641181111335754
translation,28,151,results,without policy,for,sentence scoring,without policy for sentence scoring,0.654589831829071
translation,28,151,results,hetersumgraph,with,trigram blocking,hetersumgraph with trigram blocking,0.7080588340759277
translation,28,151,results,outperforms,by,0.65,outperforms by 0.65,0.6214960813522339
translation,28,151,results,0.65,over,her,0.65 over her,0.6781846880912781
translation,28,151,results,her,for,reorganized summaries,her for reorganized summaries,0.6641181111335754
translation,28,151,results,trigram blocking,has,outperforms,trigram blocking has outperforms,0.6521972417831421
translation,28,151,results,results,has,our hetersumgraph,results has our hetersumgraph,0.5994651317596436
translation,28,155,results,our cross-sentence relationship modeling,performs,better,our cross-sentence relationship modeling performs better,0.6114127039909363
translation,28,155,results,better,than,bilstm and transformer,better than bilstm and transformer,0.5801417827606201
translation,28,156,results,our models,have,strong advantages,our models have strong advantages,0.5715736746788025
translation,28,156,results,strong advantages,over,other non-bert - based approaches,strong advantages over other non-bert - based approaches,0.6422395706176758
translation,28,156,results,other non-bert - based approaches,on,nyt50,other non-bert - based approaches on nyt50,0.5945593118667603
translation,28,156,results,results,has,our models,results has our models,0.5733726620674133
translation,28,173,results,heterdoc -sumgraph,achieves,better performance improvements,heterdoc -sumgraph achieves better performance improvements,0.6639442443847656
translation,28,173,results,both of our heter - sumgraph and heterdocsumgraph,has,outperform,both of our heter - sumgraph and heterdocsumgraph has outperform,0.5959032773971558
translation,28,173,results,outperform,has,previous methods,outperform has previous methods,0.5906838774681091
translation,28,173,results,results,observe that,both of our heter - sumgraph and heterdocsumgraph,results observe that both of our heter - sumgraph and heterdocsumgraph,0.6228014826774597
translation,28,175,results,trigram blocking,does not work,multi-,trigram blocking does not work multi-,0.6508491039276123
translation,28,175,results,multi-,has,news dataset,multi- has news dataset,0.5560278296470642
translation,28,175,results,results,has,trigram blocking,results has trigram blocking,0.538478434085846
translation,28,187,results,hetersumgraph,performs,much better,hetersumgraph performs much better,0.6270995736122131
translation,28,187,results,much better,for,documents,much better for documents,0.6554366946220398
translation,30,97,baselines,first one,uses,single mean-pooled feature vector representation,first one uses single mean-pooled feature vector representation,0.5687907338142395
translation,30,97,baselines,single mean-pooled feature vector representation,for,entire video,single mean-pooled feature vector representation for entire video,0.5648070573806763
translation,30,97,baselines,second one,applies,single layer rnn,second one applies single layer rnn,0.5721346735954285
translation,30,97,baselines,single layer rnn,over,vectors in time,single layer rnn over vectors in time,0.6934733390808105
translation,30,97,baselines,two video-only models,has,first one,two video-only models has first one,0.5882919430732727
translation,30,35,experiments,new how2 dataset,contains,human annotated video summaries,new how2 dataset contains human annotated video summaries,0.589499294757843
translation,30,35,experiments,human annotated video summaries,for,varied range of topics,human annotated video summaries for varied range of topics,0.5397396683692932
translation,30,52,experiments,state- of - the - art models,for,distantmicrophone conversational speech recognition,state- of - the - art models for distantmicrophone conversational speech recognition,0.5471612215042114
translation,30,52,experiments,state- of - the - art models,for,as - pire,state- of - the - art models for as - pire,0.6427156925201416
translation,30,52,experiments,state- of - the - art models,for,eesen,state- of - the - art models for eesen,0.607962429523468
translation,30,52,experiments,eesen,has,"miao et al. , 2015 ; le franc et al. , 2018 )","eesen has miao et al. , 2015 ; le franc et al. , 2018 )",0.5763545036315918
translation,30,90,experiments,video action features,to train,various models,video action features to train various models,0.6053070425987244
translation,30,90,experiments,various models,with,different combinations of modalities,various models with different combinations of modalities,0.5993127226829529
translation,30,114,hyperparameters,text encoder,consists of,2 bidirectional layers,text encoder consists of 2 bidirectional layers,0.6772268414497375
translation,30,114,hyperparameters,text encoder,consists of,2 layers,text encoder consists of 2 layers,0.6673661470413208
translation,30,114,hyperparameters,2 bidirectional layers,of,encoder,2 bidirectional layers of encoder,0.5578554272651672
translation,30,114,hyperparameters,encoder,with,256 gated recurrent units ( gru ; ),encoder with 256 gated recurrent units ( gru ; ),0.6399176120758057
translation,30,114,hyperparameters,encoder,with,2 layers,encoder with 2 layers,0.6592667102813721
translation,30,114,hyperparameters,encoder,with,conditional gated recurrent units ( cgru,encoder with conditional gated recurrent units ( cgru,0.6591821312904358
translation,30,114,hyperparameters,of the decoder,with,conditional gated recurrent units ( cgru,of the decoder with conditional gated recurrent units ( cgru,0.6004898548126221
translation,30,114,hyperparameters,2 layers,has,of the decoder,2 layers has of the decoder,0.5491085648536682
translation,30,115,hyperparameters,models,with,"adam optimizer ( kingma and ba , 2014 )","models with adam optimizer ( kingma and ba , 2014 )",0.6100267171859741
translation,30,115,hyperparameters,models,with,learning rate 4 ? 10 ?4,models with learning rate 4 ? 10 ?4,0.6601186990737915
translation,30,115,hyperparameters,"adam optimizer ( kingma and ba , 2014 )",with,learning rate 4 ? 10 ?4,"adam optimizer ( kingma and ba , 2014 ) with learning rate 4 ? 10 ?4",0.6541287899017334
translation,30,115,hyperparameters,learning rate 4 ? 10 ?4,halved after,each epoch,learning rate 4 ? 10 ?4 halved after each epoch,0.7223601341247559
translation,30,115,hyperparameters,each epoch,when,validation performance,each epoch when validation performance,0.6494932174682617
translation,30,115,hyperparameters,does not increase,for,maximum 50 epochs,does not increase for maximum 50 epochs,0.6224852800369263
translation,30,115,hyperparameters,validation performance,has,does not increase,validation performance has does not increase,0.6172052621841431
translation,30,115,hyperparameters,hyperparameters,optimize,models,hyperparameters optimize models,0.7109909653663635
translation,30,63,model,recurrent neural network ( rnn ) sequenceto-sequence ( s2s ) model,consisting of,encoder rnn,recurrent neural network ( rnn ) sequenceto-sequence ( s2s ) model consisting of encoder rnn,0.6991233229637146
translation,30,63,model,recurrent neural network ( rnn ) sequenceto-sequence ( s2s ) model,consisting of,decoder rnn,recurrent neural network ( rnn ) sequenceto-sequence ( s2s ) model consisting of decoder rnn,0.7023106217384338
translation,30,63,model,to encode ( text or video features ),with,attention mechanism,to encode ( text or video features ) with attention mechanism,0.6177843809127808
translation,30,63,model,decoder rnn,to generate,summaries,decoder rnn to generate summaries,0.6790022850036621
translation,30,63,model,encoder rnn,has,to encode ( text or video features ),encoder rnn has to encode ( text or video features ),0.5815067291259766
translation,30,63,model,model,use,recurrent neural network ( rnn ) sequenceto-sequence ( s2s ) model,model use recurrent neural network ( rnn ) sequenceto-sequence ( s2s ) model,0.6439551711082458
translation,30,36,results,natural language descriptions,for,video content,natural language descriptions for video content,0.5044173002243042
translation,30,36,results,natural language descriptions,using,transcriptions,natural language descriptions using transcriptions,0.6311053037643433
translation,30,36,results,visual features,extracted from,video,visual features extracted from video,0.590964674949646
translation,30,91,results,text-only model,performs,best,text-only model performs best,0.5817332863807678
translation,30,91,results,best,when using,complete transcript,best when using complete transcript,0.6992083191871643
translation,30,91,results,complete transcript,in,input ( 650 tokens ),complete transcript in input ( 650 tokens ),0.5004756450653076
translation,30,91,results,results,has,text-only model,results has text-only model,0.5209612846374512
translation,30,93,results,do not perform better,than,s2s models,do not perform better than s2s models,0.59669029712677
translation,30,93,results,pg networks,has,do not perform better,pg networks has do not perform better,0.6249600648880005
translation,30,93,results,results,observe,pg networks,results observe pg networks,0.5781398415565491
translation,30,98,results,action features,in,input,action features in input,0.47447481751441956
translation,30,98,results,action features,reaches,almost competitive rouge and content f1 scores,action features reaches almost competitive rouge and content f1 scores,0.6692327857017517
translation,30,98,results,almost competitive rouge and content f1 scores,compared to,text-only model,almost competitive rouge and content f1 scores compared to text-only model,0.6221960783004761
translation,30,98,results,results,using,action features,results using action features,0.585584282875061
translation,30,99,results,hierarchical attention model,combines,both modalities,hierarchical attention model combines both modalities,0.7232760787010193
translation,30,99,results,both modalities,obtains,highest score,both modalities obtains highest score,0.5557912588119507
translation,30,99,results,results,has,hierarchical attention model,results has hierarchical attention model,0.5210070610046387
translation,30,100,results,human evaluation scores,on,"best text-only , video-only and multimodal models","human evaluation scores on best text-only , video-only and multimodal models",0.46369102597236633
translation,31,124,baselines,transformer - based ncls models,where,input and output,transformer - based ncls models where input and output,0.6221504211425781
translation,31,124,baselines,input and output,are,different granularities combinations of units,input and output are different granularities combinations of units,0.5942339301109314
translation,31,124,baselines,tncls,has,transformer - based ncls models,tncls has transformer - based ncls models,0.5383804440498352
translation,31,125,baselines,cls + ms,refers to,multi-task ncls model,cls + ms refers to multi-task ncls model,0.6939971446990967
translation,31,125,baselines,cls + ms,simultaneously performs,text generation,cls + ms simultaneously performs text generation,0.731227457523346
translation,31,125,baselines,cls + ms,calculates,total losses,cls + ms calculates total losses,0.6122229695320129
translation,31,125,baselines,multi-task ncls model,accepts,input text,multi-task ncls model accepts input text,0.6206741333007812
translation,31,125,baselines,text generation,for,cls and ms tasks,text generation for cls and ms tasks,0.6096919178962708
translation,31,125,baselines,baselines,has,cls + ms,baselines has cls + ms,0.5867966413497925
translation,31,126,baselines,cls + mt,trains,cls and mt tasks,cls + mt trains cls and mt tasks,0.7723945379257202
translation,31,126,baselines,cls and mt tasks,via,alternating training strategy,cls and mt tasks via alternating training strategy,0.5846733450889587
translation,31,126,baselines,baselines,has,cls + mt,baselines has cls + mt,0.5674245953559875
translation,31,96,experimental-setup,english,apply,two different granularities,english apply two different granularities,0.6134295463562012
translation,31,96,experimental-setup,two different granularities,of,segmentation,two different granularities of segmentation,0.61170494556427
translation,31,96,experimental-setup,two different granularities,i.e.,words and subwords,two different granularities i.e. words and subwords,0.6377395987510681
translation,31,96,experimental-setup,segmentation,i.e.,words and subwords,segmentation i.e. words and subwords,0.6513599157333374
translation,31,96,experimental-setup,experimental setup,For,english,experimental setup For english,0.6364397406578064
translation,31,97,experimental-setup,experimental setup,lowercase,all english characters,experimental setup lowercase all english characters,0.7070035934448242
translation,31,103,experimental-setup,our zh2en models,of,target - side english words and subwords,our zh2en models of target - side english words and subwords,0.5565866827964783
translation,31,103,experimental-setup,vocabulary size,of,"sourceside chinese characters , words , and subwords","vocabulary size of sourceside chinese characters , words , and subwords",0.5358294248580933
translation,31,103,experimental-setup,vocabulary size,of,target - side english words and subwords,vocabulary size of target - side english words and subwords,0.5588333606719971
translation,31,103,experimental-setup,"sourceside chinese characters , words , and subwords",are,"10,000 , 100,000 , and 100,000","sourceside chinese characters , words , and subwords are 10,000 , 100,000 , and 100,000",0.5769145488739014
translation,31,103,experimental-setup,target - side english words and subwords,are,"40,000","target - side english words and subwords are 40,000",0.5961363911628723
translation,31,103,experimental-setup,our zh2en models,has,vocabulary size,our zh2en models has vocabulary size,0.5554399490356445
translation,31,103,experimental-setup,experimental setup,In,our zh2en models,experimental setup In our zh2en models,0.5397323966026306
translation,31,104,experimental-setup,all the parameters,via,xavier initialization methods,all the parameters via xavier initialization methods,0.6124990582466125
translation,31,104,experimental-setup,experimental setup,initialize,all the parameters,experimental setup initialize all the parameters,0.7524814009666443
translation,31,105,experimental-setup,our models,using,"configuration transformer base ( vaswani et al. , 2017 )","our models using configuration transformer base ( vaswani et al. , 2017 )",0.6272501349449158
translation,31,105,experimental-setup,"configuration transformer base ( vaswani et al. , 2017 )",contains,6 - layer encoder,"configuration transformer base ( vaswani et al. , 2017 ) contains 6 - layer encoder",0.5887753963470459
translation,31,105,experimental-setup,"configuration transformer base ( vaswani et al. , 2017 )",contains,6 - layer decoder,"configuration transformer base ( vaswani et al. , 2017 ) contains 6 - layer decoder",0.5920209884643555
translation,31,105,experimental-setup,6 - layer decoder,with,512 - dimensional hidden representations,6 - layer decoder with 512 - dimensional hidden representations,0.6220215559005737
translation,31,105,experimental-setup,experimental setup,train,our models,experimental setup train our models,0.6514129638671875
translation,31,106,experimental-setup,training,in,en2zh models,training in en2zh models,0.49346259236335754
translation,31,106,experimental-setup,training,in,zh2en models,training in zh2en models,0.4838674068450928
translation,31,106,experimental-setup,each minibatch,set of,document-summary pairs,each minibatch set of document-summary pairs,0.6702953577041626
translation,31,106,experimental-setup,each minibatch,set of,document-summary pairs,each minibatch set of document-summary pairs,0.6702953577041626
translation,31,106,experimental-setup,document-summary pairs,with,"roughly 2,048 source and 2,048 target tokens","document-summary pairs with roughly 2,048 source and 2,048 target tokens",0.6325533390045166
translation,31,106,experimental-setup,document-summary pairs,with,"roughly 4,096 source and 4,096 target tokens","document-summary pairs with roughly 4,096 source and 4,096 target tokens",0.629478394985199
translation,31,106,experimental-setup,document-summary pairs,with,"roughly 4,096 source and 4,096 target tokens","document-summary pairs with roughly 4,096 source and 4,096 target tokens",0.629478394985199
translation,31,106,experimental-setup,mini-batch,set of,document-summary pairs,mini-batch set of document-summary pairs,0.6412026286125183
translation,31,106,experimental-setup,document-summary pairs,with,"roughly 4,096 source and 4,096 target tokens","document-summary pairs with roughly 4,096 source and 4,096 target tokens",0.629478394985199
translation,31,106,experimental-setup,training,has,each minibatch,training has each minibatch,0.5698633193969727
translation,31,106,experimental-setup,en2zh models,has,each minibatch,en2zh models has each minibatch,0.5892471671104431
translation,31,106,experimental-setup,experimental setup,During,training,experimental setup During training,0.6835477948188782
translation,31,108,experimental-setup,single nvidia titan x,to train,our models,single nvidia titan x to train our models,0.6739097237586975
translation,31,108,experimental-setup,experimental setup,use,single nvidia titan x,experimental setup use single nvidia titan x,0.6014807820320129
translation,31,110,experimental-setup,each task,for,"800,000 iterations","each task for 800,000 iterations",0.5790625810623169
translation,31,110,experimental-setup,"800,000 iterations",in,multi-task ncls models,"800,000 iterations in multi-task ncls models",0.5310751795768738
translation,31,110,experimental-setup,experimental setup,train,each task,experimental setup train each task,0.620620608329773
translation,31,111,experimental-setup,our summaries,produced using,beam search,our summaries produced using beam search,0.7188315987586975
translation,31,111,experimental-setup,beam search,with,beam size 4,beam search with beam size 4,0.6971104741096497
translation,31,111,experimental-setup,experimental setup,At test time,our summaries,experimental setup At test time our summaries,0.6920187473297119
translation,31,111,experimental-setup,experimental setup,has,our summaries,experimental setup has our summaries,0.5569841265678406
translation,31,94,experiments,mt task,employ,2.08 m 6 sentence pairs,mt task employ 2.08 m 6 sentence pairs,0.5093094110488892
translation,31,94,experiments,2.08 m 6 sentence pairs,from,ldc corpora,2.08 m 6 sentence pairs from ldc corpora,0.5155648589134216
translation,31,94,experiments,2.08 m 6 sentence pairs,to train,cls + mt,2.08 m 6 sentence pairs to train cls + mt,0.6402938961982727
translation,31,94,experiments,ldc corpora,with,cls dataset,ldc corpora with cls dataset,0.6473048329353333
translation,31,94,experiments,cls dataset,to train,cls + mt,cls dataset to train cls + mt,0.6813071370124817
translation,31,107,experiments,"adam optimizer ( kingma and ba , 2015 )",with,"? 1 = 0.9 , ? 2 = 0.998 , and = 10 ?9","adam optimizer ( kingma and ba , 2015 ) with ? 1 = 0.9 , ? 2 = 0.998 , and = 10 ?9",0.616439163684845
translation,31,161,experiments,performance,of,model,performance of model,0.6080846190452576
translation,31,161,experiments,model,trained on,non-filter dataset,model trained on non-filter dataset,0.7652626633644104
translation,31,161,experiments,zh2en task,has,performance,zh2en task has performance,0.5715336203575134
translation,31,6,model,end-to- end cls framework,refer to,neural cross -lingual summarization ( ncls ),end-to- end cls framework refer to neural cross -lingual summarization ( ncls ),0.6094266772270203
translation,31,6,model,model,present,end-to- end cls framework,model present end-to- end cls framework,0.622042715549469
translation,31,7,model,ncls,by incorporating,two related tasks,ncls by incorporating two related tasks,0.7221474051475525
translation,31,7,model,two related tasks,into,training process,two related tasks into training process,0.568861186504364
translation,31,7,model,training process,of,cls,training process of cls,0.6331722140312195
translation,31,7,model,cls,under,multi-task learning,cls under multi-task learning,0.5815786719322205
translation,31,7,model,model,further improve,ncls,model further improve ncls,0.777179479598999
translation,31,19,model,novel approach,to directly address,lack of data,novel approach to directly address lack of data,0.6644580364227295
translation,31,19,model,model,introduce,novel approach,model introduce novel approach,0.6833530068397522
translation,31,20,model,simple yet effective round-trip translation strategy,to obtain,cross-lingual documentsummary pairs,simple yet effective round-trip translation strategy to obtain cross-lingual documentsummary pairs,0.5956966876983643
translation,31,20,model,cross-lingual documentsummary pairs,from,existing monolingual summarization datasets,cross-lingual documentsummary pairs from existing monolingual summarization datasets,0.5016975998878479
translation,31,20,model,model,propose,simple yet effective round-trip translation strategy,model propose simple yet effective round-trip translation strategy,0.6893918514251709
translation,31,77,model,self-attention block,in,decoder,self-attention block in decoder,0.5465261340141296
translation,31,77,model,self-attention block,modified with,masking,self-attention block modified with masking,0.6269357800483704
translation,31,77,model,masking,to prevent,present positions from attending to future positions,masking to prevent present positions from attending to future positions,0.6901485919952393
translation,31,77,model,present positions from attending to future positions,during,training,present positions from attending to future positions during training,0.7242186665534973
translation,31,77,model,model,has,self-attention block,model has self-attention block,0.5491565465927124
translation,31,114,model,source document,via,transformer - based machine translation model,source document via transformer - based machine translation model,0.6216376423835754
translation,31,114,model,transformer - based machine translation model,trained on,ldc corpora,transformer - based machine translation model trained on ldc corpora,0.7122482061386108
translation,31,114,model,model,translate,source document,model translate source document,0.704820454120636
translation,31,127,model,cls task,in,mini-batch,cls task in mini-batch,0.5400951504707336
translation,31,127,model,mt task,in,next mini-batch,mt task in next mini-batch,0.5371692180633545
translation,31,127,model,model,optimize,cls task,model optimize cls task,0.7938727736473083
translation,31,127,model,model,optimize,mt task,model optimize mt task,0.744304895401001
translation,31,109,results,convergence,reached within,"1,000,000 iterations","convergence reached within 1,000,000 iterations",0.7373536229133606
translation,31,109,results,"1,000,000 iterations",in,tncls models,"1,000,000 iterations in tncls models",0.533877968788147
translation,31,109,results,"1,000,000 iterations",in,baseline models,"1,000,000 iterations in baseline models",0.4927576780319214
translation,31,109,results,results,has,convergence,results has convergence,0.5442201495170593
translation,31,130,results,tetran,indicates that,pipeline - based methods,tetran indicates that pipeline - based methods,0.6489344239234924
translation,31,130,results,pipeline - based methods,perform,better,pipeline - based methods perform better,0.6090168952941895
translation,31,130,results,better,when using,stronger machine translation system,better when using stronger machine translation system,0.6698235273361206
translation,31,130,results,gltran,has,outperforms,gltran has outperforms,0.6553070545196533
translation,31,130,results,outperforms,has,tltran,outperforms has tltran,0.6094399690628052
translation,31,130,results,getran,has,outperforms,getran has outperforms,0.6563138365745544
translation,31,130,results,outperforms,has,tetran,outperforms has tetran,0.6472795605659485
translation,31,130,results,results,find that,gltran,results find that gltran,0.6589995622634888
translation,31,131,results,our tncls models,achieve,significant improvements,our tncls models achieve significant improvements,0.6370802521705627
translation,31,131,results,gltran or getran,has,our tncls models,gltran or getran has our tncls models,0.6308016777038574
translation,31,131,results,results,Compared with,gltran or getran,results Compared with gltran or getran,0.6716209650039673
translation,31,132,results,results,In,en2zh cls task,results In en2zh cls task,0.48022687435150146
translation,31,134,results,existing mt,for,news reports,existing mt for news reports,0.60821533203125
translation,31,134,results,news reports,has,excellent performance,news reports has excellent performance,0.5503281354904175
translation,31,134,results,results,has,existing mt,results has existing mt,0.591796338558197
translation,31,137,results,tncls ( w-c ),performs,significantly better,tncls ( w-c ) performs significantly better,0.6355658173561096
translation,31,137,results,significantly better,than,tncls ( w-w ),significantly better than tncls ( w-w ),0.5785834789276123
translation,31,137,results,results,has,tncls ( w-c ),results has tncls ( w-c ),0.5373647809028625
translation,31,139,results,zh2en cls task,has,subword - based models,zh2en cls task has subword - based models,0.5539587140083313
translation,31,139,results,subword - based models,has,outperform,subword - based models has outperform,0.6007478833198547
translation,31,139,results,outperform,has,others,outperform has others,0.6135737299919128
translation,31,139,results,results,In,zh2en cls task,results In zh2en cls task,0.490491658449173
translation,31,140,results,tncls,achieve,maximum improvement,tncls achieve maximum improvement,0.6515589356422424
translation,31,140,results,maximum improvement,up to,"+ 4.52 rouge -1 , +6.56 rouge -2 , +5.03 rouge -l","maximum improvement up to + 4.52 rouge -1 , +6.56 rouge -2 , +5.03 rouge -l",0.5766874551773071
translation,31,140,results,maximum improvement,up to,"+ 3.40 rouge -1 , +5.07 rouge - 2 , +3.77 rouge -l","maximum improvement up to + 3.40 rouge -1 , +5.07 rouge - 2 , +3.77 rouge -l",0.5852667093276978
translation,31,140,results,"+ 4.52 rouge -1 , +6.56 rouge -2 , +5.03 rouge -l",on,zh2ensum,"+ 4.52 rouge -1 , +6.56 rouge -2 , +5.03 rouge -l on zh2ensum",0.5738381147384644
translation,31,140,results,"+ 4.52 rouge -1 , +6.56 rouge -2 , +5.03 rouge -l",on,zh2ensum,"+ 4.52 rouge -1 , +6.56 rouge -2 , +5.03 rouge -l on zh2ensum",0.5738381147384644
translation,31,140,results,"+ 4.52 rouge -1 , +6.56 rouge -2 , +5.03 rouge -l",on,zh2ensum,"+ 4.52 rouge -1 , +6.56 rouge -2 , +5.03 rouge -l on zh2ensum",0.5738381147384644
translation,31,140,results,"+ 3.40 rouge -1 , +5.07 rouge - 2 , +3.77 rouge -l",on,zh2ensum,"+ 3.40 rouge -1 , +5.07 rouge - 2 , +3.77 rouge -l on zh2ensum",0.5745496153831482
translation,31,140,results,baselines,has,tncls,baselines has tncls,0.5661027431488037
translation,31,140,results,results,Compared with,baselines,results Compared with baselines,0.6914018392562866
translation,31,141,results,drops obviously,on,human-corrected test set,drops obviously on human-corrected test set,0.5651757717132568
translation,31,141,results,tncls,has,drops obviously,tncls has drops obviously,0.6633151769638062
translation,31,141,results,results,results of,tncls,results results of tncls,0.7956560254096985
translation,31,147,results,models,trained on,filter dataset,models trained on filter dataset,0.7383297681808472
translation,31,147,results,models,trained on,pseudo- filter dataset,models trained on pseudo- filter dataset,0.758726954460144
translation,31,147,results,pseudo- filter dataset,on,en2zh and zh2en tasks,pseudo- filter dataset on en2zh and zh2en tasks,0.5308229923248291
translation,31,147,results,filter dataset,has,significantly outperform,filter dataset has significantly outperform,0.6097056865692139
translation,31,147,results,significantly outperform,has,models,significantly outperform has models,0.6364627480506897
translation,31,147,results,results,has,models,results has models,0.5335168838500977
translation,31,167,results,both cls + ms and cls + mt,improve,performance,both cls + ms and cls + mt improve performance,0.6781834363937378
translation,31,167,results,performance,of,ncls,performance of ncls,0.6332347393035889
translation,31,167,results,results,has,both cls + ms and cls + mt,results has both cls + ms and cls + mt,0.5144312977790833
translation,31,168,results,cls + mt,performs,comparably,cls + mt performs comparably,0.6265217065811157
translation,31,168,results,cls + ms,in,en2zh task,cls + ms in en2zh task,0.5610151886940002
translation,31,168,results,cls + ms,performs,comparably,cls + ms performs comparably,0.6492456197738647
translation,31,168,results,comparably,with,cls + mt,comparably with cls + mt,0.6965324878692627
translation,31,168,results,cls + mt,in,zh2en task,cls + mt in zh2en task,0.5731968283653259
translation,31,168,results,cls + mt,has,significantly outperforms,cls + mt has significantly outperforms,0.6125932931900024
translation,31,168,results,significantly outperforms,has,cls + ms,significantly outperforms has cls + ms,0.609996497631073
translation,31,168,results,results,has,cls + mt,results has cls + mt,0.5594093799591064
translation,31,172,results,ncls with multi-task learning,achieves,more significant improvement,ncls with multi-task learning achieves more significant improvement,0.6563646197319031
translation,31,172,results,more significant improvement,in,en2zh task,more significant improvement in en2zh task,0.5239508152008057
translation,31,172,results,en2zh task,than in,zh2en task,en2zh task than in zh2en task,0.6140938401222229
translation,31,172,results,results,has,ncls with multi-task learning,results has ncls with multi-task learning,0.5285377502441406
translation,31,178,results,tncls,generate,more informative summaries,tncls generate more informative summaries,0.6544492840766907
translation,31,178,results,more informative summaries,compared with,gltran,more informative summaries compared with gltran,0.7136826515197754
translation,31,178,results,more informative summaries,shows,advantage,more informative summaries shows advantage,0.6915178298950195
translation,31,178,results,advantage,of,end-to - end models,advantage of end-to - end models,0.5986682772636414
translation,31,178,results,results,has,tncls,results has tncls,0.5706424713134766
translation,31,179,results,conciseness score and fluency score,of,tncls,conciseness score and fluency score of tncls,0.59355628490448
translation,31,179,results,tncls,comparable to,gltran,tncls comparable to gltran,0.730238139629364
translation,31,179,results,results,has,conciseness score and fluency score,results has conciseness score and fluency score,0.5159030556678772
translation,31,182,results,it investment,in,china 's circulation industry,it investment in china 's circulation industry,0.46557852625846863
translation,31,182,results,china 's circulation industry,increase by,14.1 %,china 's circulation industry increase by 14.1 %,0.6621593832969666
translation,31,182,results,china 's circulation industry,increase by,14.1 %,china 's circulation industry increase by 14.1 %,0.6621593832969666
translation,31,182,results,14.1 %,in,2011,14.1 % in 2011,0.6063782572746277
translation,31,182,results,gltran,has,it investment,gltran has it investment,0.600391685962677
translation,31,182,results,results,has,gltran,results has gltran,0.5596714019775391
translation,31,189,results,conciseness and fluency,of,generated summaries,conciseness and fluency of generated summaries,0.5636399388313293
translation,31,189,results,significantly improve,has,conciseness and fluency,significantly improve has conciseness and fluency,0.5246139764785767
translation,31,189,results,results,has,our cls +ms and cls + mt,results has our cls +ms and cls + mt,0.5358794331550598
translation,31,190,results,tncls,generate,more informative summaries,tncls generate more informative summaries,0.6544492840766907
translation,31,190,results,tncls,difficult to improve,conciseness and fluency,tncls difficult to improve conciseness and fluency,0.7099168300628662
translation,31,190,results,results,has,tncls,results has tncls,0.5706424713134766
translation,31,191,results,conciseness and fluency scores,can be,significantly improved,conciseness and fluency scores can be significantly improved,0.6496394872665405
translation,31,191,results,mt and ms tasks,has,conciseness and fluency scores,mt and ms tasks has conciseness and fluency scores,0.5313768982887268
translation,31,191,results,results,with the help of,mt and ms tasks,results with the help of mt and ms tasks,0.6014870405197144
translation,32,146,ablation-analysis,impact,on,qa accuracies,impact on qa accuracies,0.6010853052139282
translation,32,146,ablation-analysis,enlarged answer space,has,impact,enlarged answer space has impact,0.5915610790252686
translation,32,146,ablation-analysis,ablation analysis,has,enlarged answer space,ablation analysis has enlarged answer space,0.5921679139137268
translation,32,139,baselines,summarization model,attend to,specific regions,summarization model attend to specific regions,0.7447713613510132
translation,32,139,baselines,specific regions,of,input documents,specific regions of input documents,0.5741680264472961
translation,32,139,baselines,attention,to traverse,different content,attention to traverse different content,0.7219980359077454
translation,32,139,baselines,different content,of,source document,different content of source document,0.5865876078605652
translation,32,139,baselines,distract,has,attention,distract has attention,0.5993187427520752
translation,32,131,experiments,entity q,uses,qa pairs,entity q uses qa pairs,0.6479874849319458
translation,32,131,experiments,qa pairs,whose,answers,qa pairs whose answers,0.7101381421089172
translation,32,117,hyperparameters,hidden state size,of,bi-lstm,hidden state size of bi-lstm,0.5796380043029785
translation,32,117,hyperparameters,hidden state size,of,single-direction lstm encoder,hidden state size of single-direction lstm encoder,0.527131199836731
translation,32,117,hyperparameters,hidden state size,of,single-direction lstm encoder,hidden state size of single-direction lstm encoder,0.527131199836731
translation,32,117,hyperparameters,bi-lstm,is,256,bi-lstm is 256,0.5635505318641663
translation,32,117,hyperparameters,hidden state size,of,single-direction lstm encoder,hidden state size of single-direction lstm encoder,0.527131199836731
translation,32,117,hyperparameters,single-direction lstm encoder,is,30,single-direction lstm encoder is 30,0.5393884181976318
translation,32,118,hyperparameters,"dropout rate ( srivastava , 2013 )",used twice in,sampling component,"dropout rate ( srivastava , 2013 ) used twice in sampling component",0.6791637539863586
translation,32,118,hyperparameters,hyperparameters,has,"dropout rate ( srivastava , 2013 )","hyperparameters has dropout rate ( srivastava , 2013 )",0.5110914707183838
translation,32,119,hyperparameters,minibatch size,set to,256,minibatch size set to 256,0.6753116250038147
translation,32,119,hyperparameters,hyperparameters,has,minibatch size,hyperparameters has minibatch size,0.4892244338989258
translation,32,120,hyperparameters,early stopping,on,validation set,early stopping on validation set,0.5784603357315063
translation,32,120,hyperparameters,hyperparameters,apply,early stopping,hyperparameters apply early stopping,0.6104239225387573
translation,32,121,hyperparameters,source vocabulary,contains,150 k words,source vocabulary contains 150 k words,0.6349698305130005
translation,32,121,hyperparameters,hyperparameters,has,source vocabulary,hyperparameters has source vocabulary,0.4696158170700073
translation,32,122,hyperparameters,100 - dimensional,initialized by,glove,100 - dimensional initialized by glove,0.6979328989982605
translation,32,122,hyperparameters,word embeddings,remain,trainable,word embeddings remain trainable,0.5644240975379944
translation,32,122,hyperparameters,100 - dimensional,has,word embeddings,100 - dimensional has word embeddings,0.5435778498649597
translation,32,122,hyperparameters,hyperparameters,use,100 - dimensional,hyperparameters use 100 - dimensional,0.630931556224823
translation,32,124,hyperparameters,maximum length,of,input,maximum length of input,0.6013240218162537
translation,32,124,hyperparameters,input,set to,100 words,input set to 100 words,0.6780381202697754
translation,32,124,hyperparameters,input,set to,0.4 ( ?40 words ),input set to 0.4 ( ?40 words ),0.7282130122184753
translation,32,124,hyperparameters,hyperparameters,has,maximum length,hyperparameters has maximum length,0.5061417818069458
translation,32,125,hyperparameters,adam optimizer,with,initial learning rate,adam optimizer with initial learning rate,0.5838022828102112
translation,32,125,hyperparameters,adam optimizer,halve,learning rate,adam optimizer halve learning rate,0.7313622832298279
translation,32,125,hyperparameters,initial learning rate,of,1e - 4,initial learning rate of 1e - 4,0.6140750050544739
translation,32,125,hyperparameters,learning rate,if,objective,learning rate if objective,0.5674840807914734
translation,32,125,hyperparameters,worsens,beyond,threshold ( > 10 % ),worsens beyond threshold ( > 10 % ),0.6768166422843933
translation,32,125,hyperparameters,objective,has,worsens,objective has worsens,0.6153466701507568
translation,32,125,hyperparameters,hyperparameters,use,adam optimizer,hyperparameters use adam optimizer,0.6090166568756104
translation,32,37,model,attention mechanism,to locate,segments,attention mechanism to locate segments,0.6760973334312439
translation,32,37,model,segments,of,summary,segments of summary,0.6561954617500305
translation,32,37,model,summary,relevant to,given question,summary relevant to given question,0.7053570747375488
translation,32,37,model,answer,has,multiple questions,answer has multiple questions,0.5723053812980652
translation,32,37,model,model,use,attention mechanism,model use attention mechanism,0.6464574933052063
translation,32,140,model,words,from,source text,words from source text,0.5468318462371826
translation,32,140,model,words,via,pointing,words via pointing,0.6787258982658386
translation,32,140,model,novel words,through,generator,novel words through generator,0.6875094175338745
translation,32,140,model,copy,has,words,copy has words,0.5742371678352356
translation,32,147,results,entities,as,answers,entities as answers,0.5271488428115845
translation,32,147,results,training accuracy,is,34.8 % ( q5 ),training accuracy is 34.8 % ( q5 ),0.5752133131027222
translation,32,147,results,validation,is,15.4 % ( q5 ),validation is 15.4 % ( q5 ),0.5691683888435364
translation,32,147,results,entities,has,training accuracy,entities has training accuracy,0.5387943387031555
translation,32,147,results,answers,has,training accuracy,answers has training accuracy,0.5849090218544006
translation,32,147,results,results,using,entities,results using entities,0.591489851474762
translation,33,239,experiments,tf - idf and glove,perform,worst,tf - idf and glove perform worst,0.6539844870567322
translation,33,197,model,glove,is,context independent model,glove is context independent model,0.5797439813613892
translation,33,197,model,context independent model,computes,single embedding,context independent model computes single embedding,0.7119094133377075
translation,33,197,model,single embedding,for,each word,single embedding for each word,0.6261793375015259
translation,33,197,model,model,has,glove,model has glove,0.6085261702537537
translation,33,198,model,bert,is,contextualized embedding model,bert is contextualized embedding model,0.5919185876846313
translation,33,198,model,contextualized embedding model,takes,entire sentence,contextualized embedding model takes entire sentence,0.608249843120575
translation,33,198,model,entire sentence,into,account,entire sentence into account,0.5854260325431824
translation,33,198,model,model,has,bert,model has bert,0.6085957288742065
translation,33,240,results,bert embedding,does,better,bert embedding does better,0.35627976059913635
translation,33,240,results,bert embedding,reaches,relatively low f1 score,bert embedding reaches relatively low f1 score,0.6785742044448853
translation,33,240,results,relatively low f1 score,of,0.4,relatively low f1 score of 0.4,0.5788964033126831
translation,33,240,results,results,has,bert embedding,results has bert embedding,0.5736426711082458
translation,33,241,results,supervised models,shown to perform,well,supervised models shown to perform well,0.6728200316429138
translation,33,242,results,bert,with,fine tuning,bert with fine tuning,0.7213186025619507
translation,33,242,results,fine tuning,leads to,substantial improvement,fine tuning leads to substantial improvement,0.6774308681488037
translation,33,242,results,substantial improvement,reaching,f1 score,substantial improvement reaching f1 score,0.7618216276168823
translation,33,242,results,f1 score,of,0.657,f1 score of 0.657,0.5644539594650269
translation,33,242,results,f1 score,of,0.684,f1 score of 0.684,0.5654435753822327
translation,33,242,results,0.657,with,bert - base model,0.657 with bert - base model,0.5941371917724609
translation,33,242,results,0.684,with,bert - large model,0.684 with bert - large model,0.6116120219230652
translation,33,242,results,results,has,bert,results has bert,0.43097156286239624
translation,33,244,results,models,trained on,nli data,models trained on nli data,0.7740622162818909
translation,33,244,results,nli data,are,considerably better,nli data are considerably better,0.6316432952880859
translation,33,244,results,considerably better,than,unsupervised methods,considerably better than unsupervised methods,0.5947126150131226
translation,33,244,results,unsupervised methods,with,best model,unsupervised methods with best model,0.6027261018753052
translation,33,244,results,best model,reaching F1 of,0.526,best model reaching F1 of 0.526,0.6575635075569153
translation,33,244,results,results,has,models,results has models,0.5335168838500977
translation,33,261,results,dual threshold policy,achieves,best f1 score,dual threshold policy achieves best f1 score,0.6907713413238525
translation,33,261,results,best f1 score,of,0.73,best f1 score of 0.73,0.526598334312439
translation,33,261,results,all the arguments,has,dual threshold policy,all the arguments has dual threshold policy,0.5836905241012573
translation,33,261,results,results,considering,all the arguments,results considering all the arguments,0.6721907258033752
translation,33,266,results,dual threshold method,improves,recall,dual threshold method improves recall,0.7394022941589355
translation,33,266,results,dual threshold method,improves,therefore the f1 score,dual threshold method improves therefore the f1 score,0.6959884762763977
translation,33,266,results,dual threshold method,maintaining,good performance,dual threshold method maintaining good performance,0.7267888784408569
translation,33,266,results,therefore the f1 score,for,multiple matches,therefore the f1 score for multiple matches,0.6478972434997559
translation,33,266,results,good performance,for,arguments,good performance for arguments,0.637916088104248
translation,33,266,results,arguments,with,single or no matches,arguments with single or no matches,0.6044228672981262
translation,33,266,results,results,has,dual threshold method,results has dual threshold method,0.5633689165115356
translation,34,9,model,reconstruction,by,bayesian framework,reconstruction by bayesian framework,0.5482165813446045
translation,34,9,model,sentences,to form,good summary,sentences to form good summary,0.6127641797065735
translation,34,9,model,model,describe,reconstruction,model describe reconstruction,0.6471041440963745
translation,34,33,model,bayesian nonparametric model,of,extractive multi-document summarization,bayesian nonparametric model of extractive multi-document summarization,0.5243634581565857
translation,34,33,model,model,advance,bayesian nonparametric model,model advance bayesian nonparametric model,0.6677654385566711
translation,34,38,model,beta process,as,prior,beta process as prior,0.556097686290741
translation,34,38,model,prior,to generate,binary vectors,prior to generate binary vectors,0.6945782899856567
translation,34,38,model,binary vectors,for selecting,active sentences,binary vectors for selecting active sentences,0.6748412251472473
translation,34,38,model,active sentences,that reconstruct,original documents,active sentences that reconstruct original documents,0.6152899861335754
translation,34,38,model,model,use,beta process,model use beta process,0.6973463296890259
translation,34,39,model,bayesian framework,for,summarization,bayesian framework for summarization,0.6021811962127686
translation,34,39,model,bayesian framework,use,variational approximation,bayesian framework use variational approximation,0.5386323928833008
translation,34,39,model,variational approximation,for,inference,variational approximation for inference,0.5686984062194824
translation,34,39,model,model,construct,bayesian framework,model construct bayesian framework,0.7345426082611084
translation,34,39,model,model,use,variational approximation,model use variational approximation,0.601073145866394
translation,34,207,model,model,has,bayesian nonparametric model,model has bayesian nonparametric model,0.5036465525627136
translation,34,155,results,bnp summarization,gets,second best value,bnp summarization gets second best value,0.5633343458175659
translation,34,155,results,second best value,among,all systems,second best value among all systems,0.578247606754303
translation,34,155,results,results,result of,bnp summarization,results result of bnp summarization,0.7373005747795105
translation,34,162,results,model,gets,best performance ( rouge values ),model gets best performance ( rouge values ),0.5846617817878723
translation,34,162,results,best performance ( rouge values ),at,length,best performance ( rouge values ) at length,0.5259523391723633
translation,34,162,results,length,around,164 words,length around 164 words,0.6161264777183533
translation,34,162,results,results,Comparing,summaries,results Comparing summaries,0.6558952331542969
translation,34,197,results,summaries,not change,predefined length,summaries not change predefined length,0.7118091583251953
translation,34,197,results,summaries,perform,significantly worse,summaries perform significantly worse,0.6075434684753418
translation,34,197,results,significantly worse,than,bnp summarization,significantly worse than bnp summarization,0.6100075244903564
translation,34,197,results,results,show,summaries,results show summaries,0.629460871219635
translation,35,211,ablation-analysis,more likely,to extract,quotes,more likely to extract quotes,0.7076634168624878
translation,35,221,ablation-analysis,pretraining,has,accuracy,pretraining has accuracy,0.5627294778823853
translation,35,221,ablation-analysis,accuracy,has,decreased,accuracy has decreased,0.62474524974823
translation,35,221,ablation-analysis,ablation analysis,Without,pretraining,ablation analysis Without pretraining,0.7334417104721069
translation,35,165,baselines,baselines,has,textrank,baselines has textrank,0.5301661491394043
translation,35,167,baselines,klsum,employs,kullbuck - leibler divergence,klsum employs kullbuck - leibler divergence,0.5616104602813721
translation,35,167,baselines,kullbuck - leibler divergence,to constrain,extracted sentences,kullbuck - leibler divergence to constrain extracted sentences,0.6681438088417053
translation,35,167,baselines,the source text,to have,similar word distribution,the source text to have similar word distribution,0.6179822087287903
translation,35,167,baselines,baselines,has,klsum,baselines has klsum,0.5707942247390747
translation,35,171,baselines,cosine similarities,of,sentence vectors,cosine similarities of sentence vectors,0.5996239185333252
translation,35,171,baselines,cosine similarities,between,sentences,cosine similarities between sentences,0.6872897744178772
translation,35,171,baselines,sentence vectors,of,iqe 's encoder,sentence vectors of iqe 's encoder,0.577842652797699
translation,35,171,baselines,sentence vectors,as,similarities,sentence vectors as similarities,0.5257658958435059
translation,35,171,baselines,similarities,between,sentences,similarities between sentences,0.7269238829612732
translation,35,141,hyperparameters,dimensions,of,embedding layers and hidden layers,dimensions of embedding layers and hidden layers,0.5644383430480957
translation,35,141,hyperparameters,embedding layers and hidden layers,of,lstm,embedding layers and hidden layers of lstm,0.5459739565849304
translation,35,141,hyperparameters,lstm,are,100,lstm are 100,0.6405370831489563
translation,35,141,hyperparameters,hyperparameters,has,dimensions,hyperparameters has dimensions,0.5247315764427185
translation,35,142,hyperparameters,size,of,vocabulary,size of vocabulary,0.5929076671600342
translation,35,142,hyperparameters,vocabulary,set to,"30,000","vocabulary set to 30,000",0.6806634068489075
translation,35,142,hyperparameters,hyperparameters,has,size,hyperparameters has size,0.5313372611999512
translation,35,143,hyperparameters,each email or post,into,sentences,each email or post into sentences,0.6325783133506775
translation,35,143,hyperparameters,each sentence,into,words,each sentence into words,0.5824304819107056
translation,35,143,hyperparameters,words,using,nltk tokenizer,words using nltk tokenizer,0.5936943292617798
translation,35,143,hyperparameters,hyperparameters,tokenize,each email or post,hyperparameters tokenize each email or post,0.7542480826377869
translation,35,143,hyperparameters,hyperparameters,tokenize,each sentence,hyperparameters tokenize each sentence,0.7487660646438599
translation,35,144,hyperparameters,upper limit,of,number of sentences,upper limit of number of sentences,0.5653991103172302
translation,35,144,hyperparameters,upper limit,of,words,upper limit of words,0.6241888403892517
translation,35,144,hyperparameters,number of sentences,set to,30,number of sentences set to 30,0.6930902004241943
translation,35,144,hyperparameters,words,in,each sentence,words in each sentence,0.5155081748962402
translation,35,144,hyperparameters,each sentence,set to,200,each sentence set to 200,0.7540167570114136
translation,35,144,hyperparameters,hyperparameters,has,upper limit,hyperparameters has upper limit,0.5257745981216431
translation,35,145,hyperparameters,epoch size,is,10,epoch size is 10,0.644287109375
translation,35,145,hyperparameters,"adam ( kingma and ba , 2015 )",as,optimizer,"adam ( kingma and ba , 2015 ) as optimizer",0.5069490075111389
translation,35,145,hyperparameters,hyperparameters,use,"adam ( kingma and ba , 2015 )","hyperparameters use adam ( kingma and ba , 2015 )",0.5931493639945984
translation,35,145,hyperparameters,hyperparameters,has,epoch size,hyperparameters has epoch size,0.4962337613105774
translation,35,161,hyperparameters,rouge computation,use,rouge 2.0,rouge computation use rouge 2.0,0.646439790725708
translation,35,161,hyperparameters,rouge 2.0,has,),rouge 2.0 has ),0.6248534917831421
translation,35,161,hyperparameters,hyperparameters,For,rouge computation,hyperparameters For rouge computation,0.5645295977592468
translation,35,4,model,unsupervised extractive neural summarization,for,conversational texts,unsupervised extractive neural summarization for conversational texts,0.5459505915641785
translation,35,4,model,implicit quote extractor,has,endto-end,implicit quote extractor has endto-end,0.5781949758529663
translation,35,4,model,endto-end,has,unsupervised extractive neural summarization,endto-end has unsupervised extractive neural summarization,0.5450754165649414
translation,35,4,model,model,propose,implicit quote extractor,model propose implicit quote extractor,0.66812664270401
translation,35,79,model,model,comprises,encoder,model comprises encoder,0.7547256946563721
translation,35,79,model,model,comprises,extractor,model comprises extractor,0.713591992855072
translation,35,79,model,model,comprises,predictor,model comprises predictor,0.6748126745223999
translation,35,174,results,outperforms,on,mail datasets ( ecs and eps ),outperforms on mail datasets ( ecs and eps ),0.5473761558532715
translation,35,174,results,baseline models,on,mail datasets ( ecs and eps ),baseline models on mail datasets ( ecs and eps ),0.507767379283905
translation,35,174,results,mail datasets ( ecs and eps ),in,most metrics,mail datasets ( ecs and eps ) in most metrics,0.48741385340690613
translation,35,174,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,35,174,results,outperforms,has,baseline models,outperforms has baseline models,0.5940393209457397
translation,35,174,results,results,has,our model,results has our model,0.5871725678443909
translation,35,175,results,most baseline models,except,textrank,most baseline models except textrank,0.5930621027946472
translation,35,175,results,reddit tifu dataset,has,iqe with reranking,reddit tifu dataset has iqe with reranking,0.5848370790481567
translation,35,175,results,iqe with reranking,has,outperforms,iqe with reranking has outperforms,0.6257497072219849
translation,35,175,results,outperforms,has,most baseline models,outperforms has most baseline models,0.5734854340553284
translation,35,175,results,results,On,reddit tifu dataset,results On reddit tifu dataset,0.5411304831504822
translation,35,176,results,reranking,improves,accuracy,reranking improves accuracy,0.6894406080245972
translation,35,176,results,accuracy,on,ecs and tifu,accuracy on ecs and tifu,0.5365622043609619
translation,35,176,results,accuracy,on,eps,accuracy on eps,0.5996953248977661
translation,35,176,results,accuracy,not on,eps,accuracy not on eps,0.6783473491668701
translation,35,176,results,results,has,reranking,results has reranking,0.5736030340194702
translation,35,177,results,textrank,on,"news article dataset ( zheng and lapata , 2019 )","textrank on news article dataset ( zheng and lapata , 2019 )",0.5039364695549011
translation,35,177,results,does not work well,on,our datasets,does not work well on our datasets,0.4784552752971649
translation,35,177,results,our datasets,where,sentence position is not an important factor,our datasets where sentence position is not an important factor,0.6210764050483704
translation,35,177,results,pacsum,has,significantly outperformed,pacsum has significantly outperformed,0.6153272986412048
translation,35,177,results,significantly outperformed,has,textrank,significantly outperformed has textrank,0.6099336743354797
translation,35,177,results,results,has,pacsum,results has pacsum,0.5782414078712463
translation,35,178,results,iqe - textrank,performed,worse,iqe - textrank performed worse,0.2950812578201294
translation,35,178,results,iqe - textrank,with,mail datasets,iqe - textrank with mail datasets,0.618252158164978
translation,35,178,results,worse,than,iqe,worse than iqe,0.6165652275085449
translation,35,178,results,worse,with,mail datasets,worse with mail datasets,0.6554166674613953
translation,35,178,results,iqe,with,mail datasets,iqe with mail datasets,0.684741735458374
translation,35,178,results,results,has,iqe - textrank,results has iqe - textrank,0.5665704011917114
translation,35,180,results,outperforms,more with,eps dataset,outperforms more with eps dataset,0.6542367339134216
translation,35,180,results,baseline models,more with,eps dataset,baseline models more with eps dataset,0.6129965782165527
translation,35,180,results,baseline models,more with,ecs dataset,baseline models more with ecs dataset,0.6089780330657959
translation,35,180,results,eps dataset,than,ecs dataset,eps dataset than ecs dataset,0.5485714077949524
translation,35,180,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,35,180,results,outperforms,has,baseline models,outperforms has baseline models,0.5940393209457397
translation,35,180,results,results,has,our model,results has our model,0.5871725678443909
translation,35,206,results,more likely,to,extract quotes,more likely to extract quotes,0.5577843189239502
translation,35,206,results,extract quotes,than,textrank,extract quotes than textrank,0.6185992956161499
translation,35,206,results,extract quotes,than,lexrank,extract quotes than lexrank,0.6113895177841187
translation,35,206,results,extract quotes,than,random,extract quotes than random,0.625352680683136
translation,35,206,results,results,has,iqe,results has iqe,0.5893509387969971
translation,36,6,model,extractive summary,by selecting,sentences,extractive summary by selecting sentences,0.7265185713768005
translation,36,6,model,sentences,with,closest embeddings,sentences with closest embeddings,0.6098808646202087
translation,36,6,model,closest embeddings,to,document embedding,closest embeddings to document embedding,0.492406964302063
translation,36,31,model,"inexpensive , scalable , cputrainable and efficient method",of,extractive text summarization,"inexpensive , scalable , cputrainable and efficient method of extractive text summarization",0.48282158374786377
translation,36,31,model,model,propose,"inexpensive , scalable , cputrainable and efficient method","model propose inexpensive , scalable , cputrainable and efficient method",0.6843731999397278
translation,37,48,baselines,evaluation library,receives,same input,evaluation library receives same input,0.7040290236473083
translation,37,48,baselines,two software packages,has,evaluation library,two software packages has evaluation library,0.5398487448692322
translation,37,49,baselines,"our pytorch ( paszke et al. , 2017 ) based summarizer",optimizes,apes scores,"our pytorch ( paszke et al. , 2017 ) based summarizer optimizes apes scores",0.7464598417282104
translation,37,49,baselines,apes scores,together with,trained models,apes scores together with trained models,0.5823233127593994
translation,37,49,baselines,baselines,has,"our pytorch ( paszke et al. , 2017 ) based summarizer","baselines has our pytorch ( paszke et al. , 2017 ) based summarizer",0.5323416590690613
translation,37,32,model,new automatic evaluation metric,more suitable for,single reference news article datasets,new automatic evaluation metric more suitable for single reference news article datasets,0.6115393042564392
translation,37,32,model,model,introduce,new automatic evaluation metric,model introduce new automatic evaluation metric,0.6312101483345032
translation,37,199,results,our model,achieves,significantly higher apes scores,our model achieves significantly higher apes scores,0.6489255428314209
translation,37,199,results,our model,improves,all rouge metrics,our model improves all rouge metrics,0.6838064789772034
translation,37,199,results,significantly higher apes scores,has,46.1 vs. 39.8 ),significantly higher apes scores has 46.1 vs. 39.8 ),0.5257541537284851
translation,37,199,results,results,has,our model,results has our model,0.5871725678443909
translation,37,200,results,scores,on,validation set,scores on validation set,0.5589589476585388
translation,37,200,results,validation set,are,"46.6 , 41.2 , 18.4 , 38.1","validation set are 46.6 , 41.2 , 18.4 , 38.1",0.5121650099754333
translation,37,200,results,"46.6 , 41.2 , 18.4 , 38.1",for,"apes , r1 , r2 , rl","46.6 , 41.2 , 18.4 , 38.1 for apes , r1 , r2 , rl",0.6024441123008728
translation,37,200,results,results,has,scores,results has scores,0.5219217538833618
translation,37,201,results,our model,increases,corresponding rouge scores,our model increases corresponding rouge scores,0.6867987513542175
translation,37,201,results,apes score,has,our model,apes score has our model,0.5942608714103699
translation,38,5,model,sentences,based on,timestamps and temporal similarity,sentences based on timestamps and temporal similarity,0.6680736541748047
translation,38,38,model,clustering method,for clustering,timestamped sentences,clustering method for clustering timestamped sentences,0.7788847088813782
translation,38,38,model,model,adopt,clustering method,model adopt clustering method,0.6982222199440002
translation,38,121,results,improvements,in general,textrank system,improvements in general textrank system,0.6796073317527771
translation,38,121,results,improvements,using,textrank system,improvements using textrank system,0.6445723176002502
translation,38,121,results,results,observe,improvements,results observe improvements,0.6329426169395447
translation,38,122,results,rouge - 2 score,increased by,15.72 %,rouge - 2 score increased by 15.72 %,0.6692772507667542
translation,38,122,results,15.72 %,across,all the articles,15.72 % across all the articles,0.5777741074562073
translation,38,122,results,results,has,rouge - 2 score,results has rouge - 2 score,0.5442942976951599
translation,38,132,results,temporal importance weighting,worked,very well,temporal importance weighting worked very well,0.7135682106018066
translation,38,132,results,temporal importance weighting,worked,reasonably well,temporal importance weighting worked reasonably well,0.7055438756942749
translation,38,132,results,very well,with,textrank,very well with textrank,0.6615203022956848
translation,38,132,results,reasonably well,with,random ranking,reasonably well with random ranking,0.6366543173789978
translation,38,132,results,results,has,temporal importance weighting,results has temporal importance weighting,0.522819459438324
translation,39,179,hyperparameters,weights,terms in,objective,weights terms in objective,0.7177870869636536
translation,39,34,model,selecting a good summary chain,using,graph-theoretic approach,selecting a good summary chain using graph-theoretic approach,0.6887444853782654
translation,39,37,model,highly connected scenes,by jointly optimizing,logical progression,highly connected scenes by jointly optimizing logical progression,0.6203939318656921
translation,39,37,model,highly connected scenes,by jointly optimizing,diversity,highly connected scenes by jointly optimizing diversity,0.6902438998222351
translation,39,37,model,highly connected scenes,by jointly optimizing,importance,highly connected scenes by jointly optimizing importance,0.6828283071517944
translation,39,225,model,graph - based model,for,script summarization,graph - based model for script summarization,0.611644446849823
translation,39,225,model,model,developed,graph - based model,model developed graph - based model,0.6517202854156494
translation,39,160,results,mlp,obtains,performance,mlp obtains performance,0.6107963919639587
translation,39,160,results,performance,superior to,any individual measure of graph connectivity,performance superior to any individual measure of graph connectivity,0.7128602862358093
translation,39,160,results,full- feature set,has,mlp,full- feature set has mlp,0.5965763330459595
translation,39,160,results,results,Using,full- feature set,results Using full- feature set,0.6422779560089111
translation,39,204,results,performance,of,scenesum,performance of scenesum,0.6555460691452026
translation,39,204,results,performance,of,our scene extraction model,performance of our scene extraction model,0.5660145282745361
translation,39,204,results,performance,of,three comparison systems,performance of three comparison systems,0.6425750851631165
translation,39,204,results,performance,on,automatic gold standard,performance on automatic gold standard,0.5647351145744324
translation,39,204,results,three comparison systems,on,automatic gold standard,three comparison systems on automatic gold standard,0.5552690625190735
translation,39,204,results,random ),on,automatic gold standard,random ) on automatic gold standard,0.5717852711677551
translation,39,204,results,automatic gold standard,at,five compression rates,automatic gold standard at five compression rates,0.5515468120574951
translation,39,205,results,maxov,performs,best,maxov performs best,0.6255996823310852
translation,39,205,results,best,in terms of,f1,best in terms of f1,0.7015265822410583
translation,39,205,results,best,followed by,scenesum,best followed by scenesum,0.7113364934921265
translation,39,205,results,results,has,maxov,results has maxov,0.5749973654747009
translation,39,214,results,scenesum summaries,are,more informative,scenesum summaries are more informative,0.6194835305213928
translation,39,214,results,scenesum summaries,overall,more informative,scenesum summaries overall more informative,0.720940351486206
translation,39,214,results,results,observe,scenesum summaries,results observe scenesum summaries,0.62442946434021
translation,40,7,model,summarization task,as,combinatorial optimization problem,summarization task as combinatorial optimization problem,0.47876882553100586
translation,40,7,model,combinatorial optimization problem,in which,nested tree,combinatorial optimization problem in which nested tree,0.5828852653503418
translation,40,7,model,model,formulated,summarization task,model formulated summarization task,0.5853393077850342
translation,40,26,model,rooted document subtree and sentence subtree,from,each node,rooted document subtree and sentence subtree from each node,0.585861086845398
translation,40,27,model,relations,between,sentences and relations between words,relations between sentences and relations between words,0.655463695526123
translation,40,27,model,rooted document subtree,from,document tree,rooted document subtree from document tree,0.5785454511642456
translation,40,27,model,arbitrary subtrees,of,sentence tree,arbitrary subtrees of sentence tree,0.5983043909072876
translation,40,28,model,elementary discourse units ( edus ),in,rst,elementary discourse units ( edus ) in rst,0.5441427826881409
translation,40,28,model,elementary discourse units ( edus ),defined as,minimal building blocks of discourse,elementary discourse units ( edus ) defined as minimal building blocks of discourse,0.6014397144317627
translation,40,28,model,rst,defined as,minimal building blocks of discourse,rst defined as minimal building blocks of discourse,0.5614827275276184
translation,40,28,model,model,has,elementary discourse units ( edus ),model has elementary discourse units ( edus ),0.5908777713775635
translation,40,109,results,sentence compression,to,system,sentence compression to system,0.5512648224830627
translation,40,109,results,system,has,greatly improved,system has greatly improved,0.612308144569397
translation,40,109,results,greatly improved,has,rouge score ( 0.354 ),greatly improved has rouge score ( 0.354 ),0.5756685733795166
translation,40,109,results,results,introducing,sentence compression,results introducing sentence compression,0.6523944735527039
translation,40,110,results,score,higher than,edu selection,score higher than edu selection,0.7120367288589478
translation,40,110,results,edu selection,is,state - of - the - art method,edu selection is state - of - the - art method,0.5860001444816589
translation,40,110,results,results,has,score,results has score,0.5091484785079956
translation,40,111,results,multiple test,by using,holm 's method,multiple test by using holm 's method,0.6222907900810242
translation,40,111,results,multiple test,found that,our method,multiple test found that our method,0.6846475601196289
translation,40,111,results,our method,has,significantly outperformed,our method has significantly outperformed,0.5876103639602661
translation,40,111,results,significantly outperformed,has,edu selection,significantly outperformed has edu selection,0.6180510520935059
translation,40,111,results,results,applied,multiple test,results applied multiple test,0.6677002310752869
translation,41,99,baselines,rnn,has,"nallapati et al. , 2017 )","rnn has nallapati et al. , 2017 )",0.5668616890907288
translation,41,20,model,approach,for,aligning,approach for aligning,0.6587679386138916
translation,41,20,model,approach,incorporating,weighting,approach incorporating weighting,0.7828944325447083
translation,41,20,model,aligning,incorporating,weighting,aligning incorporating weighting,0.753126859664917
translation,41,20,model,chapter sentences,with,abstractive summary sentences,chapter sentences with abstractive summary sentences,0.5818070769309998
translation,41,20,model,chapter sentences,incorporating,weighting,chapter sentences incorporating weighting,0.7486263513565063
translation,41,20,model,weighting,to,"rouge ( lin , 2004 )","weighting to rouge ( lin , 2004 )",0.5591751933097839
translation,41,20,model,weighting,to,"meteor ( lavie and denkowski , 2009 ) metrics","weighting to meteor ( lavie and denkowski , 2009 ) metrics",0.5255035161972046
translation,41,20,model,weighting,to enable,alignment,weighting to enable alignment,0.7243720889091492
translation,41,20,model,"meteor ( lavie and denkowski , 2009 ) metrics",to enable,alignment,"meteor ( lavie and denkowski , 2009 ) metrics to enable alignment",0.7024604082107544
translation,41,20,model,alignment,of,salient words,alignment of salient words,0.5636188983917236
translation,41,20,model,aligning,has,chapter sentences,aligning has chapter sentences,0.5203302502632141
translation,41,22,results,stable matching algorithm,to select,best alignments,stable matching algorithm to select best alignments,0.6543415188789368
translation,41,22,results,one - toone alignments,between,reference summary sentences and chapter sentences,one - toone alignments between reference summary sentences and chapter sentences,0.626510500907898
translation,41,22,results,one - toone alignments,is,best alignment method,one - toone alignments is best alignment method,0.5541653037071228
translation,41,22,results,reference summary sentences and chapter sentences,is,best alignment method,reference summary sentences and chapter sentences is best alignment method,0.5405482649803162
translation,41,22,results,results,use,stable matching algorithm,results use stable matching algorithm,0.6385974884033203
translation,41,86,results,crowd -sourced evaluation,show that,sentence - level stable matching,crowd -sourced evaluation show that sentence - level stable matching,0.4638410210609436
translation,41,86,results,sentence - level stable matching,is,significantly better,sentence - level stable matching is significantly better,0.5254600048065186
translation,41,86,results,results,has,crowd -sourced evaluation,results has crowd -sourced evaluation,0.48021620512008667
translation,41,112,results,our proposed alignment method,performs,significantly better,our proposed alignment method performs significantly better,0.6041502356529236
translation,41,112,results,significantly better,for,all mod-model,significantly better for all mod-model,0.6419747471809387
translation,41,112,results,results,see,our proposed alignment method,results see our proposed alignment method,0.5280706286430359
translation,41,116,results,k and n baseline models,perform,better,k and n baseline models perform better,0.5895491242408752
translation,41,116,results,better,than,cb baseline,better than cb baseline,0.6134340167045593
translation,41,116,results,results,at,first glance,results at first glance,0.5233327746391296
translation,42,140,ablation-analysis,relative performance degradations,are,"about 15 % , 34 % and 23 %","relative performance degradations are about 15 % , 34 % and 23 %",0.5867551565170288
translation,42,140,ablation-analysis,"about 15 % , 34 % and 23 %",for,"rouge -1 , rouge2 and rouge -l measures","about 15 % , 34 % and 23 % for rouge -1 , rouge2 and rouge -l measures",0.631315290927887
translation,42,140,ablation-analysis,ablation analysis,has,relative performance degradations,ablation analysis has relative performance degradations,0.5645710825920105
translation,42,158,ablation-analysis,additional consideration,of,sentence -sentence relationship,additional consideration of sentence -sentence relationship,0.5786870121955872
translation,42,158,ablation-analysis,sentence -sentence relationship,appears to be,beneficial,sentence -sentence relationship appears to be beneficial,0.6732819080352783
translation,42,158,ablation-analysis,ablation analysis,see that,additional consideration,ablation analysis see that additional consideration,0.614759087562561
translation,42,4,model,extractive summarization,as,risk minimization problem,extractive summarization as risk minimization problem,0.4941764771938324
translation,42,4,model,extractive summarization,propose,unified probabilistic framework,extractive summarization propose unified probabilistic framework,0.5905428528785706
translation,42,4,model,model,formulate,extractive summarization,model formulate extractive summarization,0.6671257019042969
translation,42,4,model,model,propose,unified probabilistic framework,model propose unified probabilistic framework,0.6547600030899048
translation,42,5,model,various loss functions,provides,summarization framework,various loss functions provides summarization framework,0.5353788137435913
translation,42,5,model,summarization framework,with,flexible but systematic way,summarization framework with flexible but systematic way,0.6350327134132385
translation,42,5,model,summarization framework,to render,redundancy and coherence relationships,summarization framework to render redundancy and coherence relationships,0.5335134863853455
translation,42,5,model,flexible but systematic way,to render,redundancy and coherence relationships,flexible but systematic way to render redundancy and coherence relationships,0.5599691867828369
translation,42,5,model,redundancy and coherence relationships,among,sentences,redundancy and coherence relationships among sentences,0.6331279277801514
translation,42,5,model,redundancy and coherence relationships,between,sentences and the whole document,redundancy and coherence relationships between sentences and the whole document,0.6382932066917419
translation,42,5,model,model,introduction of,various loss functions,model introduction of various loss functions,0.6304931640625
translation,42,29,model,probabilistic summarization framework,stemming from,bayes decision theory,probabilistic summarization framework stemming from bayes decision theory,0.6162115335464478
translation,42,29,model,probabilistic summarization framework,for,speech summarization,probabilistic summarization framework for speech summarization,0.5548822283744812
translation,42,29,model,model,present,probabilistic summarization framework,model present probabilistic summarization framework,0.5780876874923706
translation,42,139,results,significant performance gaps,between,summarization,significant performance gaps between summarization,0.6580463647842407
translation,42,139,results,summarization,using,manual transcripts,summarization using manual transcripts,0.6658831238746643
translation,42,139,results,summarization,using,erroneous speech recognition transcripts,summarization using erroneous speech recognition transcripts,0.6587160229682922
translation,42,142,results,"supervised summarizer ( i.e. , bc )",has,outperforms,"supervised summarizer ( i.e. , bc ) has outperforms",0.5351421236991882
translation,42,142,results,outperforms,has,"unsupervised summarizer ( i.e. , lm )","outperforms has unsupervised summarizer ( i.e. , lm )",0.5555835366249084
translation,42,142,results,results,has,"supervised summarizer ( i.e. , bc )","results has supervised summarizer ( i.e. , bc )",0.5388198494911194
translation,42,149,results,about 4 % to 5 % absolute improvements,compared to,results of bc,about 4 % to 5 % absolute improvements compared to results of bc,0.685855507850647
translation,42,150,results,feasibility,of combining,supervised and unsupervised summarizers,feasibility of combining supervised and unsupervised summarizers,0.7213020920753479
translation,42,150,results,results,confirms,feasibility,results confirms feasibility,0.6084875464439392
translation,42,152,results,results,achieved by,mmr,results achieved by mmr,0.6614269018173218
translation,42,152,results,mmr,with,bc and lm,mmr with bc and lm,0.6728430986404419
translation,42,152,results,mmr,find,significant improvements,mmr find significant improvements,0.5954270958900452
translation,42,152,results,significant improvements,both for,td and sd cases,significant improvements both for td and sd cases,0.709006130695343
translation,42,152,results,results,compare,results,results compare results,0.5562655925750732
translation,42,152,results,results,achieved by,mmr,results achieved by mmr,0.6614269018173218
translation,42,152,results,results,find,significant improvements,results find significant improvements,0.59103924036026
translation,42,153,results,proposed summarization method,offers,relative performance improvements,proposed summarization method offers relative performance improvements,0.6334714889526367
translation,42,153,results,relative performance improvements,of about,"19 % , 23 % and 19 %","relative performance improvements of about 19 % , 23 % and 19 %",0.5675653219223022
translation,42,153,results,relative performance improvements,in,"rouge -1 , rouge - 2 and rouge -l measures","relative performance improvements in rouge -1 , rouge - 2 and rouge -l measures",0.5157739520072937
translation,42,153,results,"rouge -1 , rouge - 2 and rouge -l measures",compared to,bc baseline,"rouge -1 , rouge - 2 and rouge -l measures compared to bc baseline",0.6562215685844421
translation,42,153,results,td case,has,proposed summarization method,td case has proposed summarization method,0.6015608310699463
translation,42,153,results,results,for,td case,results for td case,0.6500804424285889
translation,42,154,results,performance gap,between,td and sd cases,performance gap between td and sd cases,0.6417299509048462
translation,42,154,results,reduced,to,good extent,reduced to good extent,0.592848002910614
translation,42,154,results,reduced,by using,proposed summarization framework,reduced by using proposed summarization framework,0.6550171971321106
translation,42,154,results,good extent,by using,proposed summarization framework,good extent by using proposed summarization framework,0.639209508895874
translation,42,154,results,results,has,performance gap,results has performance gap,0.5742626190185547
translation,42,159,results,competitive results,performance of,bc,competitive results performance of bc,0.7870902419090271
translation,42,159,results,bc,for,sd case,bc for sd case,0.6857942938804626
translation,42,159,results,results,gives,competitive results,results gives competitive results,0.6604114770889282
translation,42,165,results,lexrank,gives,very promising performance,lexrank gives very promising performance,0.6029793620109558
translation,42,165,results,results,has,lexrank,results has lexrank,0.5527518391609192
translation,42,167,results,our proposed methods,has,significantly outperform,our proposed methods has significantly outperform,0.5861532092094421
translation,42,167,results,significantly outperform,has,all the conventional summarization methods,significantly outperform has all the conventional summarization methods,0.5579882264137268
translation,43,226,ablation-analysis,meeting dataset,percentage of,completely grammatical sentences,meeting dataset percentage of completely grammatical sentences,0.6552265882492065
translation,43,226,ablation-analysis,completely grammatical sentences,has,drops dramatically,completely grammatical sentences has drops dramatically,0.6014145016670227
translation,43,226,ablation-analysis,ablation analysis,For,meeting dataset,ablation analysis For meeting dataset,0.6427273154258728
translation,43,168,baselines,cosine-1st,rank,utterances,cosine-1st rank utterances,0.76385498046875
translation,43,168,baselines,utterances,in,chat log,utterances in chat log,0.5406454205513
translation,43,168,baselines,utterances,based on,cosine similarity,utterances based on cosine similarity,0.6880403757095337
translation,43,168,baselines,cosine similarity,between,utterance and query,cosine similarity between utterance and query,0.6641153693199158
translation,43,169,baselines,first uttrance,as,summary,first uttrance as summary,0.5794453620910645
translation,43,169,baselines,cosine-all,rank,utterances,cosine-all rank utterances,0.7248377799987793
translation,43,169,baselines,cosine-all,rank,utterances,cosine-all rank utterances,0.7248377799987793
translation,43,169,baselines,cosine-all,select,utterances,cosine-all select utterances,0.6875510811805725
translation,43,169,baselines,cosine-all,use,similarity,cosine-all use similarity,0.6566599011421204
translation,43,169,baselines,utterances,in,chat log,utterances in chat log,0.5406454205513
translation,43,169,baselines,utterances,based on,cosine similarity,utterances based on cosine similarity,0.6880403757095337
translation,43,169,baselines,utterances,with,cosine similarity,utterances with cosine similarity,0.6604140400886536
translation,43,169,baselines,cosine similarity,between,utterance and query,cosine similarity between utterance and query,0.6641153693199158
translation,43,169,baselines,cosine similarity,greater than,0,cosine similarity greater than 0,0.6912524700164795
translation,43,169,baselines,utterances,with,cosine similarity,utterances with cosine similarity,0.6604140400886536
translation,43,169,baselines,cosine similarity,greater than,0,cosine similarity greater than 0,0.6912524700164795
translation,43,169,baselines,textrank,use,similarity,textrank use similarity,0.6331451535224915
translation,43,169,baselines,similarity,as,edges,similarity as edges,0.5606964230537415
translation,43,169,baselines,similarity,to compute,salience,similarity to compute salience,0.7148889899253845
translation,43,169,baselines,of sentences,in,graph,of sentences in graph,0.5701505541801453
translation,43,169,baselines,salience,has,of sentences,salience has of sentences,0.5929905772209167
translation,43,163,experiments,email,use,"bc3 ( ulrich et al. , 2008 )","email use bc3 ( ulrich et al. , 2008 )",0.6184243559837341
translation,43,163,experiments,"bc3 ( ulrich et al. , 2008 )",contains,40 threads,"bc3 ( ulrich et al. , 2008 ) contains 40 threads",0.6346039772033691
translation,43,163,experiments,40 threads,from,w3c corpus,40 threads from w3c corpus,0.5564179420471191
translation,43,189,experiments,language model,use,tri-gram smoothed language model,language model use tri-gram smoothed language model,0.5371310114860535
translation,43,189,experiments,tri-gram smoothed language model,trained using,newswire text,tri-gram smoothed language model trained using newswire text,0.7065300941467285
translation,43,189,experiments,newswire text,provided in,english gigaword corpus,newswire text provided in english gigaword corpus,0.5483264327049255
translation,43,188,hyperparameters,k parameter,in,our clustering phase,k parameter in our clustering phase,0.5381925106048584
translation,43,188,hyperparameters,our clustering phase,to,10,our clustering phase to 10,0.6251609325408936
translation,43,188,hyperparameters,10,based on,average number of sentences,10 based on average number of sentences,0.6521725654602051
translation,43,188,hyperparameters,average number of sentences,in,human written summaries,average number of sentences in human written summaries,0.4776260256767273
translation,43,188,hyperparameters,hyperparameters,set,k parameter,hyperparameters set k parameter,0.621323823928833
translation,43,5,model,utterances,in,conversation,utterances in conversation,0.5510271191596985
translation,43,5,model,utterances,based on,overall content,utterances based on overall content,0.6444315910339355
translation,43,5,model,utterances,based on,phrasal query information,utterances based on phrasal query information,0.667731523513794
translation,43,5,model,model,rank and extract,utterances,model rank and extract utterances,0.7226848006248474
translation,43,49,model,fully automatic unsupervised abstract generation framework,based on,phrasal queries,fully automatic unsupervised abstract generation framework based on phrasal queries,0.5688777565956116
translation,43,49,model,phrasal queries,for,multimodal conversation summarization,phrasal queries for multimodal conversation summarization,0.5745423436164856
translation,43,49,model,model,propose,fully automatic unsupervised abstract generation framework,model propose fully automatic unsupervised abstract generation framework,0.6495208144187927
translation,43,51,model,extractive summarization model,based on,phrasal queries,extractive summarization model based on phrasal queries,0.6161149740219116
translation,43,51,model,summary - worthy sentences,in,conversation,summary - worthy sentences in conversation,0.535786509513855
translation,43,51,model,summary - worthy sentences,based on,query terms and signature terms,summary - worthy sentences based on query terms and signature terms,0.6107131242752075
translation,43,51,model,model,propose,extractive summarization model,model propose extractive summarization model,0.6300677061080933
translation,43,192,results,results,has,automatic evaluation ( chat dataset ),results has automatic evaluation ( chat dataset ),0.5360031127929688
translation,43,195,results,our full system,produces,highest rouge - 1 precision score,our full system produces highest rouge - 1 precision score,0.6072629690170288
translation,43,195,results,highest rouge - 1 precision score,among,all models,highest rouge - 1 precision score among all models,0.5771223902702332
translation,43,195,results,results,observe,our full system,results observe our full system,0.6025213003158569
translation,43,196,results,absolute improvement,of,10 %,absolute improvement of 10 %,0.5704579949378967
translation,43,196,results,10 %,in,precision,10 % in precision,0.5906538963317871
translation,43,196,results,precision,for,rouge - 1,precision for rouge - 1,0.6684565544128418
translation,43,196,results,rouge - 1,in,our abstractive model,rouge - 1 in our abstractive model,0.5510345101356506
translation,43,196,results,our abstractive model,over,our extractive model,our abstractive model over our extractive model,0.6416301727294922
translation,43,196,results,results,has,absolute improvement,results has absolute improvement,0.588019609451294
translation,43,197,results,extractive query - based method,beats,all other extractive systems,extractive query - based method beats all other extractive systems,0.743020236492157
translation,43,197,results,all other extractive systems,with,higher rouge - 1 and rouge - 2,all other extractive systems with higher rouge - 1 and rouge - 2,0.6562358140945435
translation,43,197,results,results,has,extractive query - based method,results has extractive query - based method,0.5818648934364319
translation,43,199,results,our abstractive model,for,rouge - 2 score,our abstractive model for rouge - 2 score,0.6128901839256287
translation,43,199,results,our extractive model,has,outperforms,our extractive model has outperforms,0.6382582783699036
translation,43,199,results,outperforms,has,our abstractive model,outperforms has our abstractive model,0.6179961562156677
translation,43,199,results,results,observe,our extractive model,results observe our extractive model,0.5928980708122253
translation,43,201,results,query relevance,relying only on,"cosine similarity ( i.e. , cosine - all )","query relevance relying only on cosine similarity ( i.e. , cosine - all )",0.6643044948577881
translation,43,201,results,"cosine similarity ( i.e. , cosine - all )",to measure,query relevance,"cosine similarity ( i.e. , cosine - all ) to measure query relevance",0.640539824962616
translation,43,201,results,query relevance,presents,quite strong baseline,query relevance presents quite strong baseline,0.6209738254547119
translation,43,201,results,results,has,query relevance,results has query relevance,0.5397581458091736
translation,43,208,results,biased lexrank model,has,slightly improves,biased lexrank model has slightly improves,0.5580674409866333
translation,43,208,results,slightly improves,has,generic lexrank system,slightly improves has generic lexrank system,0.5529919266700745
translation,43,208,results,results,has,biased lexrank model,results has biased lexrank model,0.5176647305488586
translation,43,214,results,significantly outperforms,in,overall quality and responsiveness,significantly outperforms in overall quality and responsiveness,0.5547230243682861
translation,43,214,results,significantly outperforms,for,meeting and email datasets,significantly outperforms for meeting and email datasets,0.609075129032135
translation,43,214,results,baselines,in,overall quality and responsiveness,baselines in overall quality and responsiveness,0.5040457844734192
translation,43,214,results,baselines,for,meeting and email datasets,baselines for meeting and email datasets,0.574142575263977
translation,43,214,results,our system,has,significantly outperforms,our system has significantly outperforms,0.619092583656311
translation,43,214,results,significantly outperforms,has,baselines,significantly outperforms has baselines,0.6014122366905212
translation,43,214,results,results,indicate,our system,results indicate our system,0.6009559035301208
translation,43,216,results,absolute improvements,in,overall quality and responsiveness,absolute improvements in overall quality and responsiveness,0.4929184019565582
translation,43,216,results,overall quality and responsiveness,for,emails ( 0.9 and 0.7 ),overall quality and responsiveness for emails ( 0.9 and 0.7 ),0.5892705917358398
translation,43,216,results,overall quality and responsiveness,greater than for,meetings ( 0.4 and 0.6 ),overall quality and responsiveness greater than for meetings ( 0.4 and 0.6 ),0.6920062303543091
translation,43,216,results,emails ( 0.9 and 0.7 ),greater than for,meetings ( 0.4 and 0.6 ),emails ( 0.9 and 0.7 ) greater than for meetings ( 0.4 and 0.6 ),0.7119430899620056
translation,43,216,results,results,observe that,absolute improvements,results observe that absolute improvements,0.5655816197395325
translation,43,221,results,both datasets ( meeting and email ),in,majority of cases ( 70 % and 60 % respectively ),both datasets ( meeting and email ) in majority of cases ( 70 % and 60 % respectively ),0.4711078107357025
translation,43,221,results,users,prefer,query - based abstractive summary,users prefer query - based abstractive summary,0.7554682493209839
translation,43,221,results,query - based abstractive summary,generated by,our system,query - based abstractive summary generated by our system,0.6680464744567871
translation,43,221,results,both datasets ( meeting and email ),has,users,both datasets ( meeting and email ) has users,0.5806018114089966
translation,43,221,results,majority of cases ( 70 % and 60 % respectively ),has,users,majority of cases ( 70 % and 60 % respectively ) has users,0.5660189986228943
translation,43,221,results,results,For,both datasets ( meeting and email ),results For both datasets ( meeting and email ),0.5740044116973877
translation,43,224,results,chat dataset results,demonstrate,highest scores,chat dataset results demonstrate highest scores,0.611518144607544
translation,43,224,results,73 %,of,sentences,73 % of sentences,0.6727170944213867
translation,43,224,results,sentences,generated by,our phrasal query abstraction model,sentences generated by our phrasal query abstraction model,0.652380645275116
translation,43,224,results,generated sentences,are,almost correct,generated sentences are almost correct,0.5780839323997498
translation,43,224,results,highest scores,has,73 %,highest scores has 73 %,0.5566004514694214
translation,43,224,results,results,has,chat dataset results,results has chat dataset results,0.5399190783500671
translation,43,271,results,original sentences,for,all datasets,original sentences for all datasets,0.5660414695739746
translation,43,271,results,our model,reports,slightly lower results,our model reports slightly lower results,0.6500087380409241
translation,43,271,results,slightly lower results,for,grammaticality score,slightly lower results for grammaticality score,0.5745618939399719
translation,43,271,results,original sentences,has,our model,original sentences has our model,0.5569302439689636
translation,43,271,results,all datasets,has,our model,all datasets has our model,0.5856253504753113
translation,43,271,results,results,In comparison with,original sentences,results In comparison with original sentences,0.6440191864967346
translation,43,272,results,grammaticality score and the percentage of fully grammatical sentences,generated by,our system,grammaticality score and the percentage of fully grammatical sentences generated by our system,0.6396955251693726
translation,43,272,results,grammaticality score and the percentage of fully grammatical sentences,with,higher rouge or quality scores,grammaticality score and the percentage of fully grammatical sentences with higher rouge or quality scores,0.6317709684371948
translation,43,272,results,higher rouge or quality scores,in comparison with,other methods,higher rouge or quality scores in comparison with other methods,0.5970470905303955
translation,44,104,baselines,"elmo ( peters et al. , 2018 )",with,two conventional word embedding methods,"elmo ( peters et al. , 2018 ) with two conventional word embedding methods",0.5695423483848572
translation,44,104,baselines,universal sentence encoder for english ( use ),with,two conventional word embedding methods,universal sentence encoder for english ( use ) with two conventional word embedding methods,0.6273621320724487
translation,44,154,baselines,edua,has,two implementations,edua has two implementations,0.601265013217926
translation,44,154,baselines,baselines,has,edua,baselines has edua,0.6337452530860901
translation,44,155,baselines,edua - c,implements,complete solution,edua - c implements complete solution,0.7100227475166321
translation,44,155,baselines,complete solution,based on,depth first search ( dfs ),complete solution based on depth first search ( dfs ),0.6635309457778931
translation,44,155,baselines,depth first search ( dfs ),of,candidate scus,depth first search ( dfs ) of candidate scus,0.6097152829170227
translation,44,155,baselines,baselines,has,edua - c,baselines has edua - c,0.6184896230697632
translation,44,107,experimental-setup,meaning vectors,for,strings of words,meaning vectors for strings of words,0.5853322148323059
translation,44,107,experimental-setup,meaning vectors,use,pretrained elmo vectors,meaning vectors use pretrained elmo vectors,0.5389397740364075
translation,44,107,experimental-setup,3 output layers,as,word embeddings,3 output layers as word embeddings,0.5289167761802673
translation,44,107,experimental-setup,experimental setup,To create,meaning vectors,experimental setup To create meaning vectors,0.6940956115722656
translation,44,109,experimental-setup,meaning vectors,for,word strings,meaning vectors for word strings,0.5724729895591736
translation,44,109,experimental-setup,experimental setup,create,meaning vectors,experimental setup create meaning vectors,0.6035897731781006
translation,44,120,experimental-setup,wtmf matrices,trained on,guo and diab ( 2012 ) corpus,wtmf matrices trained on guo and diab ( 2012 ) corpus,0.7647653222084045
translation,44,120,experimental-setup,guo and diab ( 2012 ) corpus,consists of,"wordnet , wiktionary , and the brown corpus","guo and diab ( 2012 ) corpus consists of wordnet , wiktionary , and the brown corpus",0.6093193888664246
translation,44,120,experimental-setup,guo and diab ( 2012 ) corpus,has,"393 k sentences , 81 k vocabulary size","guo and diab ( 2012 ) corpus has 393 k sentences , 81 k vocabulary size",0.547874927520752
translation,44,120,experimental-setup,experimental setup,use,wtmf matrices,experimental setup use wtmf matrices,0.6301193833351135
translation,44,6,model,automated method,than,previous automated pyramid methods,automated method than previous automated pyramid methods,0.5657966136932373
translation,44,6,model,more complete,than,previous automated pyramid methods,more complete than previous automated pyramid methods,0.5397934913635254
translation,44,6,model,model,present,automated method,model present automated method,0.7194998264312744
translation,44,20,model,automated method,to assess,importance,automated method to assess importance,0.680905282497406
translation,44,20,model,importance,of,summary content,importance of summary content,0.5847606658935547
translation,44,20,model,widely used manual evaluation,called,"pyramid ( nenkova et al. , 2007 )","widely used manual evaluation called pyramid ( nenkova et al. , 2007 )",0.6103357076644897
translation,44,20,model,model,present,automated method,model present automated method,0.7194998264312744
translation,44,30,results,outperforms,has,previous work,outperforms has previous work,0.6127970814704895
translation,44,30,results,results,present,pyreval,results present pyreval,0.6456547379493713
translation,44,170,results,both variants,suffer from,coarse-grained segmentation output,both variants suffer from coarse-grained segmentation output,0.7007629871368408
translation,44,170,results,coarse-grained segmentation output,from,decomposition parser,coarse-grained segmentation output from decomposition parser,0.5256466865539551
translation,44,170,results,zipfian distribution,observed in,most pyramids,zipfian distribution observed in most pyramids,0.7086530327796936
translation,44,170,results,results,has,both variants,results has both variants,0.5083457231521606
translation,44,250,results,outperforms,in,accuracy,outperforms in accuracy,0.5779992341995239
translation,44,250,results,outperforms,in,efficiency,outperforms in efficiency,0.5732713341712952
translation,44,250,results,outperforms,in,score normalization,outperforms in score normalization,0.5373634696006775
translation,44,250,results,outperforms,in,interpretability,outperforms in interpretability,0.5245874524116516
translation,44,250,results,previous automated pyramid methods,in,accuracy,previous automated pyramid methods in accuracy,0.5224539041519165
translation,44,250,results,previous automated pyramid methods,in,efficiency,previous automated pyramid methods in efficiency,0.540947675704956
translation,44,250,results,previous automated pyramid methods,in,score normalization,previous automated pyramid methods in score normalization,0.4529765248298645
translation,44,250,results,previous automated pyramid methods,in,interpretability,previous automated pyramid methods in interpretability,0.4660690128803253
translation,44,250,results,outperforms,has,previous automated pyramid methods,outperforms has previous automated pyramid methods,0.5834226608276367
translation,45,56,baselines,ips,supports,human ? system interaction,ips supports human ? system interaction,0.7297457456588745
translation,45,56,baselines,human ? system interaction,by clicking into,summary sentences,human ? system interaction by clicking into summary sentences,0.6388827562332153
translation,45,56,baselines,human ? system interaction,examining,source contexts,human ? system interaction examining source contexts,0.6893686056137085
translation,45,170,baselines,gmds,selects,important sentences,gmds selects important sentences,0.6931995153427124
translation,45,170,baselines,sentence connectivity graph,based on,cosine similarity,sentence connectivity graph based on cosine similarity,0.6481387615203857
translation,45,170,baselines,important sentences,based on,concept of eigenvector centrality,important sentences based on concept of eigenvector centrality,0.6389627456665039
translation,45,170,baselines,baselines,has,gmds,baselines has gmds,0.5464202165603638
translation,45,199,experiments,optimal ?,directly in,ips,optimal ? directly in ips,0.7237736582756042
translation,45,202,hyperparameters,number of topics,set at,n=50,number of topics set at n=50,0.6166885495185852
translation,45,202,hyperparameters,hyperparameters,has,number of topics,hyperparameters has number of topics,0.5296748876571655
translation,45,7,model,source documents,captures,user interests,source documents captures user interests,0.6883246898651123
translation,45,7,model,ips,captures,user interests,ips captures user interests,0.7713284492492676
translation,45,7,model,ips,incorporates,personalization,ips incorporates personalization,0.735588788986206
translation,45,7,model,user interests,by enabling,interactive clicks,user interests by enabling interactive clicks,0.6801710724830627
translation,45,7,model,personalization,by modeling,captured reader preference,personalization by modeling captured reader preference,0.7448402643203735
translation,45,7,model,source documents,has,ips,source documents has ips,0.5621709823608398
translation,45,7,model,model,Given,source documents,model Given source documents,0.6795185208320618
translation,45,29,model,balanced optimization framework,via,iterative substitution,balanced optimization framework via iterative substitution,0.6679680347442627
translation,45,29,model,iterative substitution,to generate,summaries,iterative substitution to generate summaries,0.6565857529640198
translation,45,29,model,summaries,with,maximum overall utilities,summaries with maximum overall utilities,0.5666890144348145
translation,45,173,model,ips,with,personalization component,ips with personalization component,0.6537617444992065
translation,45,173,model,proposed algorithms,with,personalization component,proposed algorithms with personalization component,0.6393857598304749
translation,45,173,model,personalization component,to capture,interest,personalization component to capture interest,0.6729474067687988
translation,45,173,model,interest,by,user feedbacks,interest by user feedbacks,0.5765814185142517
translation,45,173,model,ips,has,proposed algorithms,ips has proposed algorithms,0.6074583530426025
translation,45,173,model,model,has,ips,model has ips,0.6410464644432068
translation,45,174,model,ips,generates,summaries,ips generates summaries,0.7141667604446411
translation,45,174,model,summaries,via,iterative sentence substitutions,summaries via iterative sentence substitutions,0.652775228023529
translation,45,174,model,iterative sentence substitutions,within,user interactive sessions,iterative sentence substitutions within user interactive sessions,0.622852087020874
translation,45,174,model,model,has,ips,model has ips,0.6410464644432068
translation,45,172,results,initial generated summary,from,ips,initial generated summary from ips,0.5871676206588745
translation,45,172,results,initial generated summary,models,coverage and diversity utility,initial generated summary models coverage and diversity utility,0.7252108454704285
translation,45,172,results,results,has,initial generated summary,results has initial generated summary,0.5633594393730164
translation,45,180,results,worst performance,both in,rouge - 1 scores,worst performance both in rouge - 1 scores,0.5674760937690735
translation,45,180,results,worst performance,both in,human judgements,worst performance both in human judgements,0.6215324401855469
translation,45,180,results,random,has,worst performance,random has worst performance,0.5852498412132263
translation,45,181,results,rouge - 1 and human ratings,of,centroid and gmds,rouge - 1 and human ratings of centroid and gmds,0.6035870909690857
translation,45,181,results,centroid and gmds,better than,random,centroid and gmds better than random,0.739269495010376
translation,45,181,results,results,has,rouge - 1 and human ratings,results has rouge - 1 and human ratings,0.5200375914573669
translation,46,188,ablation-analysis,ablations,have,larger effect,ablations have larger effect,0.5612384080886841
translation,46,188,ablation-analysis,1.89,on,reddit,1.89 on reddit,0.5198150873184204
translation,46,188,ablation-analysis,2.56,on,ami,2.56 on ami,0.57340008020401
translation,46,188,ablation-analysis,1.3,on,pubmed,1.3 on pubmed,0.5557270646095276
translation,46,188,ablation-analysis,non-news datasets,has,ablations,non-news datasets has ablations,0.5995248556137085
translation,46,188,ablation-analysis,ablation analysis,On,non-news datasets,ablation analysis On non-news datasets,0.5511180758476257
translation,46,189,ablation-analysis,nouns,leads to,largest drop,nouns leads to largest drop,0.6528960466384888
translation,46,189,ablation-analysis,largest drop,on,ami and pubmed,largest drop on ami and pubmed,0.5518474578857422
translation,46,189,ablation-analysis,ablation analysis,Removing,nouns,ablation analysis Removing nouns,0.8040667176246643
translation,46,190,ablation-analysis,adjectives and adverbs,leads to,largest drop,adjectives and adverbs leads to largest drop,0.6529353260993958
translation,46,190,ablation-analysis,largest drop,on,reddit,largest drop on reddit,0.5311508774757385
translation,46,190,ablation-analysis,ablation analysis,Removing,adjectives and adverbs,ablation analysis Removing adjectives and adverbs,0.7027782797813416
translation,46,207,ablation-analysis,shuffling,reduces,lead overlap,shuffling reduces lead overlap,0.7284131646156311
translation,46,207,ablation-analysis,lead overlap,to,35.2 %,lead overlap to 35.2 %,0.5460330247879028
translation,46,207,ablation-analysis,overall system performance,has,drops,overall system performance has drops,0.5936403870582581
translation,46,207,ablation-analysis,drops,has,significantly,drops has significantly,0.6735619306564331
translation,46,207,ablation-analysis,ablation analysis,has,shuffling,ablation analysis has shuffling,0.5667745471000671
translation,46,199,experiments,news domains and pubmed,suffer,significant drop,news domains and pubmed suffer significant drop,0.6676981449127197
translation,46,199,experiments,significant drop,in,performance,significant drop in performance,0.5765949487686157
translation,46,199,experiments,performance,when,document order,performance when document order,0.6662882566452026
translation,46,199,experiments,document order,is,shuffled,document order is shuffled,0.605553388595581
translation,46,133,hyperparameters,word embeddings,initialized using,pretrained glove embeddings,word embeddings initialized using pretrained glove embeddings,0.6893852353096008
translation,46,133,hyperparameters,hyperparameters,has,word embeddings,hyperparameters has word embeddings,0.4784160554409027
translation,46,20,results,sentence position bias,dominates,learning signal,sentence position bias dominates learning signal,0.6817896962165833
translation,46,20,results,learning signal,for,news summarization,learning signal for news summarization,0.5720294117927551
translation,46,20,results,results,reveal,sentence position bias,results reveal sentence position bias,0.503205418586731
translation,46,21,results,summary quality,for,news,summary quality for news,0.589774489402771
translation,46,21,results,summary quality,is,only slightly degraded,summary quality is only slightly degraded,0.5622589588165283
translation,46,21,results,only slightly degraded,when,content words,only slightly degraded when content words,0.6486135721206665
translation,46,21,results,content words,omitted from,sentence embeddings,content words omitted from sentence embeddings,0.5766386389732361
translation,46,21,results,results,has,summary quality,results has summary quality,0.4788559079170227
translation,46,22,results,word embedding averaging,is,as good or better,word embedding averaging is as good or better,0.5372141599655151
translation,46,22,results,as good or better,than,rnns or cnns,as good or better than rnns or cnns,0.6200046539306641
translation,46,22,results,as good or better,for,sentence embedding,as good or better for sentence embedding,0.6672948002815247
translation,46,22,results,rnns or cnns,for,sentence embedding,rnns or cnns for sentence embedding,0.5810831785202026
translation,46,22,results,sentence embedding,across,all domains,sentence embedding across all domains,0.7161065340042114
translation,46,22,results,results,has,word embedding averaging,results has word embedding averaging,0.5496339201927185
translation,46,23,results,pre-trained word embeddings,"as good , or better than",learned embeddings,"pre-trained word embeddings as good , or better than learned embeddings",0.5138310790061951
translation,46,23,results,learned embeddings,in,five of six datasets,learned embeddings in five of six datasets,0.47369810938835144
translation,46,23,results,results,has,pre-trained word embeddings,results has pre-trained word embeddings,0.50806725025177
translation,46,24,results,non auto-regressive sentence extraction,performs,as good or better,non auto-regressive sentence extraction performs as good or better,0.60020512342453
translation,46,24,results,as good or better,than,auto-regressive extraction,as good or better than auto-regressive extraction,0.601556658744812
translation,46,24,results,auto-regressive extraction,in,all domains,auto-regressive extraction in all domains,0.5030232667922974
translation,46,24,results,results,has,non auto-regressive sentence extraction,results has non auto-regressive sentence extraction,0.5668139457702637
translation,46,145,results,no major advantage,when using,cnn and rnn sentence encoders,no major advantage when using cnn and rnn sentence encoders,0.6848284006118774
translation,46,145,results,cnn and rnn sentence encoders,over,averaging encoder,cnn and rnn sentence encoders over averaging encoder,0.6610221266746521
translation,46,147,results,seq2seq extractor,is,not statistically distinguishable,seq2seq extractor is not statistically distinguishable,0.5878435373306274
translation,46,147,results,seq2seq extractor,part of,best performing system,seq2seq extractor part of best performing system,0.6490344405174255
translation,46,147,results,not statistically distinguishable,from,best extractor,not statistically distinguishable from best extractor,0.5560139417648315
translation,46,147,results,extractors,has,seq2seq extractor,extractors has seq2seq extractor,0.5426101088523865
translation,46,147,results,best performing system,has,three out of six datasets,best performing system has three out of six datasets,0.5650057792663574
translation,46,147,results,results,looking at,extractors,results looking at extractors,0.6327664256095886
translation,46,187,results,any word class,from,representation,any word class from representation,0.6037023663520813
translation,46,187,results,performance,with,statistical significance,performance with statistical significance,0.6452575922012329
translation,46,187,results,performance,on,news domains,performance on news domains,0.5411171317100525
translation,46,187,results,absolute values,of,differences,absolute values of differences,0.5609550476074219
translation,46,187,results,differences,are,quite small,differences are quite small,0.5620050430297852
translation,46,187,results,.18,on,cnn / dm,.18 on cnn / dm,0.5696966052055359
translation,46,187,results,.18,on,duc,.18 on duc,0.5998506546020508
translation,46,187,results,.18,on,duc,.18 on duc,0.5998506546020508
translation,46,187,results,.41,on,nyt,.41 on nyt,0.6453399062156677
translation,46,187,results,hurts,has,performance,hurts has performance,0.6043882966041565
translation,46,187,results,performance,has,absolute values,performance has absolute values,0.5766329169273376
translation,46,187,results,news domains,has,absolute values,news domains has absolute values,0.5732390880584717
translation,46,187,results,quite small,has,.18,quite small has .18,0.5582172870635986
translation,46,187,results,results,removing,any word class,results removing any word class,0.7271478176116943
translation,46,191,results,function word pos class,yields,significant improvement,function word pos class yields significant improvement,0.7089978456497192
translation,46,191,results,significant improvement,on,duc 2002 and ami,significant improvement on duc 2002 and ami,0.5933178663253784
translation,46,191,results,results,removing,function word pos class,results removing function word pos class,0.6827816367149353
translation,46,200,results,no significant difference,between,shuffled and inorder models,no significant difference between shuffled and inorder models,0.6528647541999817
translation,46,200,results,shuffled and inorder models,on,reddit domain,shuffled and inorder models on reddit domain,0.5842450857162476
translation,46,200,results,performance,on,ami,performance on ami,0.6029216647148132
translation,46,200,results,improves,has,performance,improves has performance,0.5770372748374939
translation,46,204,results,summaries,generated by,systems,summaries generated by systems,0.7238866686820984
translation,46,204,results,results,has,summaries,results has summaries,0.5243220329284668
translation,46,205,results,cheng & lapata and seq2seq extractors,share,87.8 %,cheng & lapata and seq2seq extractors share 87.8 %,0.6881169676780701
translation,46,205,results,87.8 %,of,output sentences,87.8 % of output sentences,0.5517873764038086
translation,46,205,results,output sentences,on average,cnn / dm data,output sentences on average cnn / dm data,0.6762663125991821
translation,46,205,results,output sentences,on,cnn / dm data,output sentences on cnn / dm data,0.5417771935462952
translation,46,205,results,results,has,cheng & lapata and seq2seq extractors,results has cheng & lapata and seq2seq extractors,0.547665536403656
translation,47,7,model,two alternative grouping techniques,based on,locality sensitive hashing,two alternative grouping techniques based on locality sensitive hashing,0.6235678195953369
translation,47,7,model,two alternative grouping techniques,based on,approximate nearest neighbor search,two alternative grouping techniques based on approximate nearest neighbor search,0.6694812178611755
translation,47,7,model,two alternative grouping techniques,based on,fast clustering algorithm,two alternative grouping techniques based on fast clustering algorithm,0.6906278133392334
translation,47,7,model,model,propose,two alternative grouping techniques,model propose two alternative grouping techniques,0.6866540908813477
translation,47,23,model,locality sensitive hashing ( lsh ),to,word embeddings,locality sensitive hashing ( lsh ) to word embeddings,0.5125012993812561
translation,47,23,model,word embeddings,to find,similar mentions,word embeddings to find similar mentions,0.547849178314209
translation,47,23,model,similar mentions,without making,all pairwise comparisons,similar mentions without making all pairwise comparisons,0.7118331789970398
translation,47,23,model,model,apply,locality sensitive hashing ( lsh ),model apply locality sensitive hashing ( lsh ),0.629497766494751
translation,47,25,model,novel grouping technique,combines,hashing approach,novel grouping technique combines hashing approach,0.6950871348381042
translation,47,25,model,hashing approach,with,fast partitioning algorithm,hashing approach with fast partitioning algorithm,0.6131493449211121
translation,47,25,model,fast partitioning algorithm,called,chinese whispers ( cw ),fast partitioning algorithm called chinese whispers ( cw ),0.6749143004417419
translation,47,25,model,model,propose,novel grouping technique,model propose novel grouping technique,0.7119939923286438
translation,47,30,results,orders of mag-nitude faster runtimes,with,only small reductions,orders of mag-nitude faster runtimes with only small reductions,0.5832338929176331
translation,47,30,results,only small reductions,in,summary quality,only small reductions in summary quality,0.476859986782074
translation,47,30,results,results,observe,orders of mag-nitude faster runtimes,results observe orders of mag-nitude faster runtimes,0.5781101584434509
translation,47,109,results,our newly proposed techniques,orders of magnitude,faster,our newly proposed techniques orders of magnitude faster,0.6401143670082092
translation,47,109,results,our newly proposed techniques,show,more moderate runtime growth,our newly proposed techniques show more moderate runtime growth,0.6368811130523682
translation,47,109,results,lsh -only and lsh - cw,orders of magnitude,faster,lsh -only and lsh - cw orders of magnitude faster,0.660561740398407
translation,47,109,results,lsh -only and lsh - cw,show,more moderate runtime growth,lsh -only and lsh - cw show more moderate runtime growth,0.6460776925086975
translation,47,109,results,faster,in,absolute terms,faster in absolute terms,0.5533943772315979
translation,47,109,results,results,has,our newly proposed techniques,results has our newly proposed techniques,0.5558343529701233
translation,48,194,experimental-setup,beam,size,ten,beam size ten,0.7833813428878784
translation,48,194,experimental-setup,experimental setup,use,beam,experimental setup use beam,0.5945382714271545
translation,48,48,experiments,corpus,of,related news articles,corpus of related news articles,0.568759560585022
translation,48,19,model,novel mds paradigm,has,hierarchical summarization,novel mds paradigm has hierarchical summarization,0.5607095956802368
translation,48,19,model,model,present,novel mds paradigm,model present novel mds paradigm,0.6947050094604492
translation,48,193,model,algorithm,is,two level nested search algorithm,algorithm is two level nested search algorithm,0.5540664196014404
translation,48,193,model,algorithm,is,beam search,algorithm is beam search,0.5712631940841675
translation,48,193,model,beam search,in,outer loop,beam search in outer loop,0.5486831068992615
translation,48,193,model,beam search,in,inner loop,beam search in inner loop,0.5546637177467346
translation,48,193,model,beam search,to search through,space,beam search to search through space,0.7812145352363586
translation,48,193,model,beam search,in,inner loop,beam search in inner loop,0.5546637177467346
translation,48,193,model,outer loop,to search through,space,outer loop to search through space,0.7037646770477295
translation,48,193,model,space,of,partial summaries,space of partial summaries,0.5958644151687622
translation,48,193,model,local search,in,inner loop,local search in inner loop,0.5376726984977722
translation,48,193,model,local search,to pick,best sentence,local search to pick best sentence,0.7131227254867554
translation,48,193,model,hill climbing,with,random restarts,hill climbing with random restarts,0.6336166262626648
translation,48,193,model,best sentence,to add to,existing partial summary,best sentence to add to existing partial summary,0.5752784609794617
translation,48,193,model,two level nested search algorithm,has,beam search,two level nested search algorithm has beam search,0.5647438168525696
translation,48,193,model,local search,has,hill climbing,local search has hill climbing,0.5220881700515747
translation,48,193,model,model,has,algorithm,model has algorithm,0.577422559261322
translation,48,6,results,summa,produces,hierarchy,summa produces hierarchy,0.7043638229370117
translation,48,6,results,hierarchy,of,relatively short summaries,hierarchy of relatively short summaries,0.5850175619125366
translation,48,6,results,top level,provides,general overview,top level provides general overview,0.6834771037101746
translation,48,6,results,hierarchy,to drill down,more details,hierarchy to drill down more details,0.7529001832008362
translation,48,6,results,more details,on,topics of interest,more details on topics of interest,0.4879235625267029
translation,48,6,results,results,has,summa,results has summa,0.5666882991790771
translation,49,50,experiments,cnn / dm,see that,low entropy decisions,cnn / dm see that low entropy decisions,0.6581619381904602
translation,49,50,experiments,low entropy decisions,are largely those generating,existing bigrams,low entropy decisions are largely those generating existing bigrams,0.7237773537635803
translation,49,5,model,"entropy , or uncertainty",of,model 's token - level predictions,"entropy , or uncertainty of model 's token - level predictions",0.5666688084602356
translation,49,5,model,model,analyze,summarization decoders,model analyze summarization decoders,0.6431412100791931
translation,49,18,model,n-grams,between,input document and generated summaries,n-grams between input document and generated summaries,0.5994021892547607
translation,49,18,model,n-grams,establish,two coarse types,n-grams establish two coarse types,0.5733166337013245
translation,49,18,model,two coarse types,for,decoded tokens,two coarse types for decoded tokens,0.6549599170684814
translation,49,18,model,model,by,n-grams,model by n-grams,0.5883676409721375
translation,49,18,model,model,comparing,n-grams,model comparing n-grams,0.7066253423690796
translation,49,18,model,model,establish,two coarse types,model establish two coarse types,0.6499777436256409
translation,50,9,model,neural networks,on,user- generated travel reviews,neural networks on user- generated travel reviews,0.5248929262161255
translation,50,9,model,user- generated travel reviews,to generate,summaries,user- generated travel reviews to generate summaries,0.6445214152336121
translation,50,9,model,summaries,take into account,shifting opinions,summaries take into account shifting opinions,0.6620585322380066
translation,50,9,model,shifting opinions,has,over time,shifting opinions has over time,0.5609896183013916
translation,51,120,ablation-analysis,review,raise,performance,review raise performance,0.6907544136047363
translation,51,120,ablation-analysis,performance,of,model,performance of model,0.6080846190452576
translation,51,120,ablation-analysis,truncating,has,review,truncating has review,0.558426558971405
translation,51,136,ablation-analysis,selective mechanism,into,s2satt,selective mechanism into s2satt,0.6142250299453735
translation,51,136,ablation-analysis,performance,of,seass,performance of seass,0.6324639320373535
translation,51,136,ablation-analysis,selective mechanism,has,performance,selective mechanism has performance,0.5904111862182617
translation,51,136,ablation-analysis,s2satt,has,performance,s2satt has performance,0.5871727466583252
translation,51,136,ablation-analysis,seass,has,decreases slightly,seass has decreases slightly,0.6532560586929321
translation,51,136,ablation-analysis,ablation analysis,After considering,selective mechanism,ablation analysis After considering selective mechanism,0.7117198705673218
translation,51,158,ablation-analysis,one kind of attribute information,obtain,"at least 0.41 rouge -1 , 0.18 rouge - 2 and 0.22 rouge -l gains","one kind of attribute information obtain at least 0.41 rouge -1 , 0.18 rouge - 2 and 0.22 rouge -l gains",0.5583918690681458
translation,51,158,ablation-analysis,ablation analysis,Adding,one kind of attribute information,ablation analysis Adding one kind of attribute information,0.7080597877502441
translation,51,159,ablation-analysis,travel status information,is,most important attribute,travel status information is most important attribute,0.5196276903152466
translation,51,159,ablation-analysis,most important attribute,for,review summarization,most important attribute for review summarization,0.5777230262756348
translation,51,159,ablation-analysis,review summarization,in,tripatt,review summarization in tripatt,0.5717918276786804
translation,51,159,ablation-analysis,ablation analysis,has,travel status information,ablation analysis has travel status information,0.510113000869751
translation,51,168,ablation-analysis,models,deletes,one kind of attribute - based strategy,models deletes one kind of attribute - based strategy,0.7352080345153809
translation,51,168,ablation-analysis,one kind of attribute - based strategy,from,asn,one kind of attribute - based strategy from asn,0.6025174260139465
translation,51,168,ablation-analysis,one kind of attribute - based strategy,from,asn,one kind of attribute - based strategy from asn,0.6025174260139465
translation,51,168,ablation-analysis,rouge - 2,compared with,asn,rouge - 2 compared with asn,0.7700045108795166
translation,51,168,ablation-analysis,ablation analysis,deletes,one kind of attribute - based strategy,ablation analysis deletes one kind of attribute - based strategy,0.727239727973938
translation,51,168,ablation-analysis,ablation analysis,has,models,ablation analysis has models,0.5171767473220825
translation,51,114,baselines,lead1,is,extractive approach,lead1 is extractive approach,0.6066004633903503
translation,51,114,baselines,extractive approach,selects,first sentence,extractive approach selects first sentence,0.7039642930030823
translation,51,114,baselines,first sentence,in review,summary,first sentence in review summary,0.733534574508667
translation,51,114,baselines,first sentence,as,summary,first sentence as summary,0.5574838519096375
translation,51,114,baselines,baselines,has,lead1,baselines has lead1,0.5862944722175598
translation,51,116,baselines,baselines,has,"tex - trank ( mihalcea and tarau , 2004 )","baselines has tex - trank ( mihalcea and tarau , 2004 )",0.550379753112793
translation,51,167,baselines,apre and amg,are,two most effective strategies,apre and amg are two most effective strategies,0.578928530216217
translation,51,167,baselines,apre and amg,directly affect,word prediction module,apre and amg directly affect word prediction module,0.7008768320083618
translation,51,167,baselines,two most effective strategies,directly affect,word prediction module,two most effective strategies directly affect word prediction module,0.6976473927497864
translation,51,167,baselines,baselines,has,apre and amg,baselines has apre and amg,0.5788640975952148
translation,51,132,experiments,extractive methods,see that,lead1,extractive methods see that lead1,0.668768584728241
translation,51,132,experiments,lead1,performs,best,lead1 performs best,0.6456983089447021
translation,51,118,hyperparameters,hyperparameters,set,batch size,hyperparameters set batch size,0.6402808427810669
translation,51,119,hyperparameters,review,to,200 tokens,review to 200 tokens,0.6741116046905518
translation,51,119,hyperparameters,expedite,has,training and testing,expedite has training and testing,0.5326650142669678
translation,51,119,hyperparameters,hyperparameters,truncate,review,hyperparameters truncate review,0.7693703770637512
translation,51,122,hyperparameters,"gradient clipping ( pascanu et al. , 2013 )",to make,our model,"gradient clipping ( pascanu et al. , 2013 ) to make our model",0.5797032117843628
translation,51,122,hyperparameters,"adam ( kingma and ba , 2015 )",has,"dropout et al. , 2014","adam ( kingma and ba , 2015 ) has dropout et al. , 2014",0.5340815186500549
translation,51,122,hyperparameters,our model,has,robust,our model has robust,0.6044266819953918
translation,51,122,hyperparameters,hyperparameters,use,"adam ( kingma and ba , 2015 )","hyperparameters use adam ( kingma and ba , 2015 )",0.5931493639945984
translation,51,123,hyperparameters,word embedding size,to,128,word embedding size to 128,0.5780414342880249
translation,51,123,hyperparameters,all lstm hidden state sizes,to,200,all lstm hidden state sizes to 200,0.5707694292068481
translation,51,123,hyperparameters,hyperparameters,set,word embedding size,hyperparameters set word embedding size,0.6111344695091248
translation,51,123,hyperparameters,hyperparameters,set,all lstm hidden state sizes,hyperparameters set all lstm hidden state sizes,0.5796375870704651
translation,51,124,hyperparameters,adam,as,optimizing algorithm,adam as optimizing algorithm,0.5845425724983215
translation,51,124,hyperparameters,hyperparameters,use,adam,hyperparameters use adam,0.6479569673538208
translation,51,125,hyperparameters,two momentum parameters,has,? 1 = 0.9 and ? 2 = 0.999,two momentum parameters has ? 1 = 0.9 and ? 2 = 0.999,0.5566332340240479
translation,51,126,hyperparameters,dropout,with,probability p = 0.2,dropout with probability p = 0.2,0.6400600075721741
translation,51,126,hyperparameters,hyperparameters,use,dropout,hyperparameters use dropout,0.6254391074180603
translation,51,127,hyperparameters,gradient clipping,with,"range [ ? 5 , 5 ]","gradient clipping with range [ ? 5 , 5 ]",0.6277234554290771
translation,51,127,hyperparameters,hyperparameters,apply,gradient clipping,hyperparameters apply gradient clipping,0.6040953397750854
translation,51,129,hyperparameters,test time,use,beam search,test time use beam search,0.6696777939796448
translation,51,129,hyperparameters,beam search,with,beam size,beam search with beam size,0.6320963501930237
translation,51,129,hyperparameters,beam size,of,4,beam size of 4,0.6962505578994751
translation,51,129,hyperparameters,hyperparameters,At,test time,hyperparameters At test time,0.5056650638580322
translation,51,7,model,' characteristics,into,account,' characteristics into account,0.633457601070404
translation,51,7,model,attribute encoder,encodes,attribute preferences,attribute encoder encodes attribute preferences,0.7740994095802307
translation,51,7,model,attribute preferences,over,words,attribute preferences over words,0.6693277359008789
translation,51,7,model,attribute - aware review encoder,adopts,attribute - based selective mechanism,attribute - aware review encoder adopts attribute - based selective mechanism,0.6165019869804382
translation,51,7,model,attribute - based selective mechanism,to select,important information,attribute - based selective mechanism to select important information,0.6826866269111633
translation,51,7,model,important information,of,review,important information of review,0.5691670179367065
translation,51,7,model,attribute - aware summary decoder,incorporates,attribute embedding and attribute -specific word-using habits,attribute - aware summary decoder incorporates attribute embedding and attribute -specific word-using habits,0.6792700290679932
translation,51,7,model,attribute embedding and attribute -specific word-using habits,into,word prediction,attribute embedding and attribute -specific word-using habits into word prediction,0.5371524095535278
translation,51,7,model,three modules,has,attribute encoder,three modules has attribute encoder,0.5680032968521118
translation,51,7,model,model,propose,attribute- aware sequence network ( asn ),model propose attribute- aware sequence network ( asn ),0.6739094853401184
translation,51,28,model,attribute - aware sequence network ( asn ),to consider,attribute information,attribute - aware sequence network ( asn ) to consider attribute information,0.6802546381950378
translation,51,28,model,attribute information,into,review summarization,attribute information into review summarization,0.5380910038948059
translation,51,28,model,model,called,attribute - aware sequence network ( asn ),model called attribute - aware sequence network ( asn ),0.6769047379493713
translation,51,29,model,asn,based on,sequence to sequence models ( s2s ),asn based on sequence to sequence models ( s2s ),0.7096045613288879
translation,51,29,model,model,has,asn,model has asn,0.6419049501419067
translation,51,30,model,attribute encoder,encodes,attribute preference,attribute encoder encodes attribute preference,0.7584784626960754
translation,51,30,model,attribute preference,for using,words,attribute preference for using words,0.6873442530632019
translation,51,30,model,words,into,attribute embedding,words into attribute embedding,0.5518491864204407
translation,51,30,model,model,design,attribute encoder,model design attribute encoder,0.6034526824951172
translation,51,31,model,attribute - aware review encoder,to generate,attribute - aware review representation,attribute - aware review encoder to generate attribute - aware review representation,0.6874646544456482
translation,51,32,model,bidirectional - lstm,to encode,review,bidirectional - lstm to encode review,0.7352827787399292
translation,51,32,model,attribute - based selective mechanism,to select,important information,attribute - based selective mechanism to select important information,0.6826866269111633
translation,51,32,model,important information,of,better review representation,important information of better review representation,0.53596031665802
translation,51,32,model,important information,to obtain,better review representation,important information to obtain better review representation,0.5445547699928284
translation,51,32,model,model,utilizes,bidirectional - lstm,model utilizes bidirectional - lstm,0.6038299202919006
translation,51,33,model,attribute - aware summary decoder,to consider,different writing styles,attribute - aware summary decoder to consider different writing styles,0.6463575959205627
translation,51,33,model,different writing styles,of,users,different writing styles of users,0.51998370885849
translation,51,33,model,different writing styles,with,different attributes,different writing styles with different attributes,0.6175389289855957
translation,51,33,model,model,propose,attribute - aware summary decoder,model propose attribute - aware summary decoder,0.6893434524536133
translation,51,214,model,attribute - aware sequence network ( asn ),to consider,attribute information,attribute - aware sequence network ( asn ) to consider attribute information,0.6802546381950378
translation,51,214,model,attribute information,into,review summarization,attribute information into review summarization,0.5380910038948059
translation,51,214,model,model,propose,attribute - aware sequence network ( asn ),model propose attribute - aware sequence network ( asn ),0.6739094853401184
translation,51,215,model,asn,imports,attribute-specific vocabulary,asn imports attribute-specific vocabulary,0.6945109367370605
translation,51,215,model,asn,utilizes,four attribute - based strategies,asn utilizes four attribute - based strategies,0.6522327065467834
translation,51,215,model,attribute-specific vocabulary,to model,attribute information,attribute-specific vocabulary to model attribute information,0.6796992421150208
translation,51,215,model,four attribute - based strategies,to build,attribute - aware review encoder,four attribute - based strategies to build attribute - aware review encoder,0.7269974946975708
translation,51,215,model,four attribute - based strategies,to build,attribute - aware summary decoder,four attribute - based strategies to build attribute - aware summary decoder,0.7164132595062256
translation,51,215,model,model,has,asn,model has asn,0.6419049501419067
translation,51,135,results,abstractive methods,find that,s2satt,abstractive methods find that s2satt,0.6446681618690491
translation,51,135,results,s2satt,better than,all extractive methods,s2satt better than all extractive methods,0.6918662786483765
translation,51,135,results,results,For,abstractive methods,results For abstractive methods,0.46942442655563354
translation,51,139,results,copy mechanism,into,s2satt,copy mechanism into s2satt,0.6221755743026733
translation,51,139,results,pgn,obtains,better performance,pgn obtains better performance,0.6397676467895508
translation,51,139,results,incoporating,has,copy mechanism,incoporating has copy mechanism,0.5848241448402405
translation,51,139,results,incoporating,has,pgn,incoporating has pgn,0.6045053005218506
translation,51,139,results,copy mechanism,has,pgn,copy mechanism has pgn,0.5905077457427979
translation,51,139,results,s2satt,has,pgn,s2satt has pgn,0.6880151033401489
translation,51,139,results,results,When,incoporating,results When incoporating,0.6878321766853333
translation,51,139,results,results,When,copy mechanism,results When copy mechanism,0.7222861647605896
translation,51,140,results,asn,performs,significantly better,asn performs significantly better,0.6680150628089905
translation,51,140,results,significantly better,than,all previous methods,significantly better than all previous methods,0.5623738169670105
translation,51,140,results,proposed attribute encoder,has,asn,proposed attribute encoder has asn,0.611267626285553
translation,51,140,results,four attribute - based strategies,has,asn,four attribute - based strategies has asn,0.5495579838752747
translation,51,140,results,results,after considering,proposed attribute encoder,results after considering proposed attribute encoder,0.7491819858551025
translation,51,141,results,s2satt,has,our model,s2satt has our model,0.6358316540718079
translation,51,141,results,our model,has,"0.91 rouge -1 , 0.65 rouge - 2 , and 0.79 rouge -l gains","our model has 0.91 rouge -1 , 0.65 rouge - 2 , and 0.79 rouge -l gains",0.5634283423423767
translation,51,141,results,results,Compared to,s2satt,results Compared to s2satt,0.6322070360183716
translation,51,153,results,aspect-level,find that,asn,aspect-level find that asn,0.63163822889328
translation,51,153,results,asn,outperforms,other models,asn outperforms other models,0.7232977151870728
translation,51,153,results,other models,by,large margin,other models by large margin,0.5815244913101196
translation,51,166,results,performance,of,re-,performance of re-,0.6193137764930725
translation,51,166,results,performance,of,view summarization,performance of view summarization,0.588426411151886
translation,51,166,results,re-,has,view summarization,re- has view summarization,0.5545221567153931
translation,51,173,results,asn,obtains,best result,asn obtains best result,0.6817700266838074
translation,51,173,results,best result,when considering,all these strategies,best result when considering all these strategies,0.7257875800132751
translation,51,173,results,results,has,asn,results has asn,0.4522075355052948
translation,52,100,ablation-analysis,ablation analysis,has,study 1 : sentence aggregation,ablation analysis has study 1 : sentence aggregation,0.5229212641716003
translation,52,171,ablation-analysis,difference,between,adding in- domain and outof-domain text,difference between adding in- domain and outof-domain text,0.6744463443756104
translation,52,171,ablation-analysis,adding in- domain and outof-domain text,is,significant p < 10 ?3 ( study 3 ),adding in- domain and outof-domain text is significant p < 10 ?3 ( study 3 ),0.5523965358734131
translation,52,171,ablation-analysis,ablation analysis,has,difference,ablation analysis has difference,0.5341284871101379
translation,52,168,results,more in - domain text,on,caseframe coverage,more in - domain text on caseframe coverage,0.5231483578681946
translation,52,168,results,caseframe coverage,is,substantial,caseframe coverage is substantial,0.6170390844345093
translation,52,168,results,noticeably more,than using,out - of- domain text,noticeably more than using out - of- domain text,0.6748642921447754
translation,53,104,ablation-analysis,pre-training,improve,"rouge -1 , rouge - 2 , and rouge -l","pre-training improve rouge -1 , rouge - 2 , and rouge -l",0.6428311467170715
translation,53,104,ablation-analysis,"rouge -1 , rouge - 2 , and rouge -l",by,2.38,"rouge -1 , rouge - 2 , and rouge -l by 2.38",0.6071739196777344
translation,53,104,ablation-analysis,"rouge -1 , rouge - 2 , and rouge -l",on,chinese-to - english summarization,"rouge -1 , rouge - 2 , and rouge -l on chinese-to - english summarization",0.4769571125507355
translation,53,104,ablation-analysis,2.38,has,", 1.74 , and 1.13 points","2.38 has , 1.74 , and 1.13 points",0.44819414615631104
translation,53,104,ablation-analysis,ablation analysis,has,pre-training,ablation analysis has pre-training,0.5351816415786743
translation,53,105,ablation-analysis,objectives,have,various degrees of contribution,objectives have various degrees of contribution,0.4788764417171478
translation,53,88,baselines,tetran,translates,source document,tetran translates source document,0.7211598753929138
translation,53,88,baselines,tetran,uses,"lexrank ( erkan and radev , 2004 )","tetran uses lexrank ( erkan and radev , 2004 )",0.6109063625335693
translation,53,88,baselines,"lexrank ( erkan and radev , 2004 )",to summarize,translated document,"lexrank ( erkan and radev , 2004 ) to summarize translated document",0.6651111245155334
translation,53,88,baselines,baselines,has,tetran,baselines has tetran,0.625622570514679
translation,53,90,baselines,getran and gltran,replace,translation model,getran and gltran replace translation model,0.6233936548233032
translation,53,90,baselines,translation model,in,tetran and tltran,translation model in tetran and tltran,0.5628161430358887
translation,53,90,baselines,tetran and tltran,with,google translator,tetran and tltran with google translator,0.6769172549247742
translation,53,90,baselines,baselines,has,getran and gltran,baselines has getran and gltran,0.577569305896759
translation,53,91,baselines,three strong baselines,from,zhu et al . ( 2019 b ),three strong baselines from zhu et al . ( 2019 b ),0.5210492014884949
translation,53,91,baselines,three strong baselines,from,ncls,three strong baselines from ncls,0.5977330207824707
translation,53,91,baselines,three strong baselines,from,ncls - ms,three strong baselines from ncls - ms,0.6230819225311279
translation,53,91,baselines,three strong baselines,from,ncls -mt,three strong baselines from ncls -mt,0.575047492980957
translation,53,91,baselines,baselines,include,three strong baselines,baselines include three strong baselines,0.6282264590263367
translation,53,54,experiments,cross-lingual parallel corpus,introduce,cross-lingual masked language model ( cmlm ),cross-lingual parallel corpus introduce cross-lingual masked language model ( cmlm ),0.584217369556427
translation,53,80,experiments,monolingual summarization objective,use,cnn / dailymail dataset,monolingual summarization objective use cnn / dailymail dataset,0.5239384770393372
translation,53,80,experiments,monolingual summarization objective,use,lcsts dataset,monolingual summarization objective use lcsts dataset,0.5480313301086426
translation,53,80,experiments,cnn / dailymail dataset,for,english summarization,cnn / dailymail dataset for english summarization,0.5520052909851074
translation,53,80,experiments,cnn / dailymail dataset,for,chinese summarization,cnn / dailymail dataset for chinese summarization,0.5403980016708374
translation,53,80,experiments,cnn / dailymail dataset,for,chinese summarization,cnn / dailymail dataset for chinese summarization,0.5403980016708374
translation,53,80,experiments,lcsts dataset,),chinese summarization,lcsts dataset ) chinese summarization,0.5084642767906189
translation,53,80,experiments,lcsts dataset,for,chinese summarization,lcsts dataset for chinese summarization,0.5086439847946167
translation,53,83,hyperparameters,d model,for,all transformer blocks,d model for all transformer blocks,0.6265071630477905
translation,53,83,hyperparameters,all transformer blocks,are,512,all transformer blocks are 512,0.6171561479568481
translation,53,83,hyperparameters,inner dimension d f f,is,2048,inner dimension d f f is 2048,0.6406925320625305
translation,53,83,hyperparameters,input and output dimensions,has,d model,input and output dimensions has d model,0.555138111114502
translation,53,83,hyperparameters,hyperparameters,has,input and output dimensions,hyperparameters has input and output dimensions,0.5056123733520508
translation,53,83,hyperparameters,hyperparameters,has,inner dimension d f f,hyperparameters has inner dimension d f f,0.5302199721336365
translation,53,84,hyperparameters,dropout probability,of,0.1,dropout probability of 0.1,0.5893268585205078
translation,53,84,hyperparameters,0.1,on,all layers,0.1 on all layers,0.549395740032196
translation,53,25,model,monolingual tasks,including,masked language model ( mlm ),monolingual tasks including masked language model ( mlm ),0.6516268849372864
translation,53,25,model,monolingual tasks,including,denoising autoencoder ( dae ),monolingual tasks including denoising autoencoder ( dae ),0.647101104259491
translation,53,25,model,monolingual tasks,including,monolingual summarization ( ms ),monolingual tasks including monolingual summarization ( ms ),0.6545087695121765
translation,53,25,model,model,pre-trained on,monolingual tasks,model pre-trained on monolingual tasks,0.7166454792022705
translation,53,25,model,model,pre-trained on,cross-lingual tasks,model pre-trained on cross-lingual tasks,0.7442651391029358
translation,53,82,model,8 heads,in,attention,8 heads in attention,0.5907102823257446
translation,53,82,model,transformer model,has,6 layers,transformer model has 6 layers,0.5797926187515259
translation,53,82,model,transformer model,has,8 heads,transformer model has 8 heads,0.5706098675727844
translation,53,82,model,model,has,transformer model,model has transformer model,0.5662795305252075
translation,53,89,model,tltran,summarizes,source document,tltran summarizes source document,0.6788454055786133
translation,53,89,model,tltran,translates,summary,tltran translates summary,0.7075942158699036
translation,53,89,model,model,has,tltran,model has tltran,0.6095191836357117
translation,53,97,results,pipeline models,achieve,sub-optimal performance,pipeline models achieve sub-optimal performance,0.6302486062049866
translation,53,97,results,sub-optimal performance,in,both directions,sub-optimal performance in both directions,0.5145291686058044
translation,53,97,results,results,has,pipeline models,results has pipeline models,0.535555362701416
translation,53,98,results,all baseline models,in,all metrics,all baseline models in all metrics,0.4532868266105652
translation,53,98,results,rouge -l,in,english - to - chinese,rouge -l in english - to - chinese,0.5378745198249817
translation,53,98,results,outperforms,has,all baseline models,outperforms has all baseline models,0.5771976113319397
translation,53,110,results,gain,from,pre-training,gain from pre-training,0.6049356460571289
translation,53,110,results,gain,is,larger,gain is larger,0.6491493582725525
translation,53,110,results,pre-training,is,larger,pre-training is larger,0.6248359680175781
translation,53,110,results,larger,when,size of training data,larger when size of training data,0.6744018197059631
translation,53,110,results,size of training data,is,relatively small,size of training data is relatively small,0.5339939594268799
translation,53,110,results,results,has,gain,results has gain,0.5428218841552734
translation,54,84,baselines,rnn and rnn - context,are,rnnbased seq2seq models,rnn and rnn - context are rnnbased seq2seq models,0.585141122341156
translation,54,84,baselines,rnnbased seq2seq models,without and with,attention mechanism,rnnbased seq2seq models without and with attention mechanism,0.6675094366073608
translation,54,84,baselines,baselines,has,rnn and rnn - context,baselines has rnn and rnn - context,0.5787656903266907
translation,54,85,baselines,copy - net,is,attention - based seq2seq model,copy - net is attention - based seq2seq model,0.551713228225708
translation,54,85,baselines,attention - based seq2seq model,with,copy mechanism,attention - based seq2seq model with copy mechanism,0.6267285346984863
translation,54,85,baselines,baselines,has,copy - net,baselines has copy - net,0.5883392691612244
translation,54,86,baselines,srb,improves,semantic relevance,srb improves semantic relevance,0.6446844339370728
translation,54,86,baselines,semantic relevance,between,source text and summary,semantic relevance between source text and summary,0.6102339625358582
translation,54,86,baselines,baselines,has,srb,baselines has srb,0.5641846656799316
translation,54,87,baselines,drgd,is,conventional seq2seq,drgd is conventional seq2seq,0.603036105632782
translation,54,87,baselines,conventional seq2seq,with,deep recurrent generative decoder,conventional seq2seq with deep recurrent generative decoder,0.6408311724662781
translation,54,87,baselines,baselines,has,drgd,baselines has drgd,0.5539196729660034
translation,54,88,baselines,abs and abs +,are,models,abs and abs + are models,0.6463883519172668
translation,54,88,baselines,models,with,local attention and handcrafted features,models with local attention and handcrafted features,0.6198472380638123
translation,54,88,baselines,gigaword,has,abs and abs +,gigaword has abs and abs +,0.6345670819282532
translation,54,89,baselines,feats,is,fully rnn seq2seq model,feats is fully rnn seq2seq model,0.5718393325805664
translation,54,89,baselines,fully rnn seq2seq model,with,some specific methods,fully rnn seq2seq model with some specific methods,0.5967108011245728
translation,54,89,baselines,some specific methods,to control,vocabulary size,some specific methods to control vocabulary size,0.6304687261581421
translation,54,89,baselines,baselines,has,feats,baselines has feats,0.5980815291404724
translation,54,90,baselines,ras - lstm and ras - elman,are,seq2seq models,ras - lstm and ras - elman are seq2seq models,0.5981556177139282
translation,54,90,baselines,seq2seq models,with,convolutional encoder,seq2seq models with convolutional encoder,0.5680133700370789
translation,54,90,baselines,seq2seq models,with,lstm decoder,seq2seq models with lstm decoder,0.5927742123603821
translation,54,90,baselines,seq2seq models,with,elman rnn decoder,seq2seq models with elman rnn decoder,0.6054061651229858
translation,54,90,baselines,baselines,has,ras - lstm and ras - elman,baselines has ras - lstm and ras - elman,0.5739426016807556
translation,54,91,baselines,seass,is,seq2seq model,seass is seq2seq model,0.5937669277191162
translation,54,91,baselines,seq2seq model,with,selective gate mechanism,seq2seq model with selective gate mechanism,0.6454686522483826
translation,54,91,baselines,baselines,has,seass,baselines has seass,0.6242193579673767
translation,54,92,baselines,drgd,baseline for,gigaword,drgd baseline for gigaword,0.7912877202033997
translation,54,92,baselines,baselines,has,drgd,baselines has drgd,0.5539196729660034
translation,54,50,experimental-setup,1dimension convolution,to extract,n-gram features,1dimension convolution to extract n-gram features,0.702302098274231
translation,54,50,experimental-setup,experimental setup,use,1dimension convolution,experimental setup use 1dimension convolution,0.5755840539932251
translation,54,72,experimental-setup,our experiments,in,pytorch,our experiments in pytorch,0.6006278991699219
translation,54,72,experimental-setup,pytorch,on,nvidia 1080 ti gpu,pytorch on nvidia 1080 ti gpu,0.5291914939880371
translation,54,72,experimental-setup,experimental setup,implement,our experiments,experimental setup implement our experiments,0.6475666165351868
translation,54,73,experimental-setup,experimental setup,has,word embedding dimension,experimental setup has word embedding dimension,0.49582281708717346
translation,54,73,experimental-setup,experimental setup,has,the number of hidden units,experimental setup has the number of hidden units,0.5077459812164307
translation,54,74,experimental-setup,batch size,set to,64,batch size set to 64,0.7451491355895996
translation,54,74,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,54,75,experimental-setup,"adam optimizer ( kingma and ba , 2014 )",with,"default setting ? = 0.001 , ? 1 = 0.9 , ? 2 = 0.999 and = 1 ? 10 ?8","adam optimizer ( kingma and ba , 2014 ) with default setting ? = 0.001 , ? 1 = 0.9 , ? 2 = 0.999 and = 1 ? 10 ?8",0.618190586566925
translation,54,75,experimental-setup,experimental setup,use,"adam optimizer ( kingma and ba , 2014 )","experimental setup use adam optimizer ( kingma and ba , 2014 )",0.5912119150161743
translation,54,76,experimental-setup,learning rate,halved,every epoch,learning rate halved every epoch,0.7393481135368347
translation,54,76,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,54,77,experimental-setup,gradient clipping,applied with,"range [ - 10 , 10 ]","gradient clipping applied with range [ - 10 , 10 ]",0.6853857636451721
translation,54,77,experimental-setup,experimental setup,has,gradient clipping,experimental setup has gradient clipping,0.4766085147857666
translation,54,5,model,global encoding framework,controls,information flow,global encoding framework controls information flow,0.6834184527397156
translation,54,5,model,information flow,from,encoder,information flow from encoder,0.6011125445365906
translation,54,5,model,encoder,to,decoder,encoder to decoder,0.6084607243537903
translation,54,5,model,decoder,based on,global information,decoder based on global information,0.6610247492790222
translation,54,5,model,global information,of,source context,global information of source context,0.5221431851387024
translation,54,5,model,model,propose,global encoding framework,model propose global encoding framework,0.6928632855415344
translation,54,6,model,convolutional gated unit,to perform,global encoding,convolutional gated unit to perform global encoding,0.6817705035209656
translation,54,6,model,global encoding,to improve,representations,global encoding to improve representations,0.7280024886131287
translation,54,6,model,representations,of,source-side information,representations of source-side information,0.5866155624389648
translation,54,6,model,model,consists of,convolutional gated unit,model consists of convolutional gated unit,0.6361586451530457
translation,54,20,model,global encoding,for,abstractive summarization,global encoding for abstractive summarization,0.6016627550125122
translation,54,20,model,model,of,global encoding,model of global encoding,0.5912600159645081
translation,54,21,model,convolutional gated unit,to perform,global encoding,convolutional gated unit to perform global encoding,0.6817705035209656
translation,54,21,model,global encoding,on,source context,global encoding on source context,0.5246301889419556
translation,54,21,model,model,set,convolutional gated unit,model set convolutional gated unit,0.6250333189964294
translation,54,22,model,gate,based on,convolutional neural network ( cnn ),gate based on convolutional neural network ( cnn ),0.6920121312141418
translation,54,22,model,gate,filters,each encoder output,gate filters each encoder output,0.7925451397895813
translation,54,22,model,each encoder output,based on,global context,each encoder output based on global context,0.6122937202453613
translation,54,22,model,global context,due to,parameter sharing,global context due to parameter sharing,0.6627369523048401
translation,54,22,model,model,has,gate,model has gate,0.6086838841438293
translation,54,28,model,seq2seq model,with,attention,seq2seq model with attention,0.6613655686378479
translation,54,28,model,model,based on,seq2seq model,model based on seq2seq model,0.6690468788146973
translation,54,29,model,encoder,set,convolutional gated unit,encoder set convolutional gated unit,0.6714333891868591
translation,54,29,model,convolutional gated unit,for,global encoding,convolutional gated unit for global encoding,0.6433591246604919
translation,54,29,model,model,For,encoder,model For encoder,0.6582764387130737
translation,54,99,results,our model,achieves,advantages,our model achieves advantages,0.6288143992424011
translation,54,99,results,our model,achieves,advantages,our model achieves advantages,0.6288143992424011
translation,54,99,results,advantages,of,rouge score,advantages of rouge score,0.5502564311027527
translation,54,99,results,rouge score,over,baselines,rouge score over baselines,0.6914411783218384
translation,54,99,results,rouge score,on,lcsts,rouge score on lcsts,0.5414993166923523
translation,54,99,results,lcsts,are,significant,lcsts are significant,0.6566257476806641
translation,54,99,results,two datasets,has,our model,two datasets has our model,0.5825160145759583
translation,54,99,results,results,on,two datasets,results on two datasets,0.4891073405742645
translation,54,100,results,results,on,lc - sts,results on lc - sts,0.539354145526886
translation,54,102,results,our model,owns an advantage,rouge - 2 score 3.7 and 1.5,our model owns an advantage rouge - 2 score 3.7 and 1.5,0.686466634273529
translation,54,102,results,our model,of,rouge - 2 score 3.7 and 1.5,our model of rouge - 2 score 3.7 and 1.5,0.5681496262550354
translation,54,102,results,rouge - 2 score 3.7 and 1.5,on,lcsts and gigaword,rouge - 2 score 3.7 and 1.5 on lcsts and gigaword,0.5334919095039368
translation,54,102,results,conventional seq2seq model,has,our model,conventional seq2seq model has our model,0.5562107563018799
translation,54,102,results,results,Compared with,conventional seq2seq model,results Compared with conventional seq2seq model,0.6553287506103516
translation,54,115,results,gigaword,for,duplicates,gigaword for duplicates,0.6543835401535034
translation,54,115,results,duplicates,of,1 - gram to 4 gram,duplicates of 1 - gram to 4 gram,0.6429746747016907
translation,54,115,results,duplicates,prove,our model,duplicates prove our model,0.6688855886459351
translation,54,115,results,repetition,compared to,conventional seq2seq,repetition compared to conventional seq2seq,0.6879208087921143
translation,54,115,results,our model,has,significantly reduces,our model has significantly reduces,0.580021321773529
translation,54,115,results,significantly reduces,has,repetition,significantly reduces has repetition,0.6246080994606018
translation,55,8,model,hahsum,spotlights,redundancy dependencies,hahsum spotlights redundancy dependencies,0.7351535558700562
translation,55,8,model,different levels of information,including,words and sentences,different levels of information including words and sentences,0.6867676377296448
translation,55,8,model,redundancy dependencies,between,sentences,redundancy dependencies between sentences,0.6557376980781555
translation,55,8,model,model,propose,hahsum,model propose hahsum,0.7041482329368591
translation,55,9,model,sentence representations,with,redundancy - aware graph,sentence representations with redundancy - aware graph,0.5924167633056641
translation,55,9,model,label dependencies,by,message passing,label dependencies by message passing,0.5761284828186035
translation,55,30,model,source article,as,hierarchical heterogeneous graph ( hhg ),source article as hierarchical heterogeneous graph ( hhg ),0.5499683022499084
translation,55,30,model,"graph attention net ( veli?kovi ? et al. , 2018 ) based model ( hahsum )",to extract,sentences,"graph attention net ( veli?kovi ? et al. , 2018 ) based model ( hahsum ) to extract sentences",0.7673304080963135
translation,55,30,model,sentences,by simultaneously balancing,salience and redundancy,sentences by simultaneously balancing salience and redundancy,0.7371832728385925
translation,55,30,model,model,construct,source article,model construct source article,0.75260329246521
translation,55,30,model,model,propose,"graph attention net ( veli?kovi ? et al. , 2018 ) based model ( hahsum )","model propose graph attention net ( veli?kovi ? et al. , 2018 ) based model ( hahsum )",0.6353554129600525
translation,55,31,model,both words and sentences,constructed as,nodes,both words and sentences constructed as nodes,0.6642783284187317
translation,55,31,model,relations,constructed as,different types of edges,relations constructed as different types of edges,0.5485971570014954
translation,55,31,model,hhg,has,both words and sentences,hhg has both words and sentences,0.6065776944160461
translation,55,31,model,model,In,hhg,model In hhg,0.5902158617973328
translation,55,33,model,word- level graph ( word-word ),design,abstract layer,word- level graph ( word-word ) design abstract layer,0.5779469013214111
translation,55,33,model,abstract layer,to learn,semantic representation,abstract layer to learn semantic representation,0.6015681028366089
translation,55,33,model,semantic representation,of,each word,semantic representation of each word,0.5369380712509155
translation,55,33,model,model,For,word- level graph ( word-word ),model For word- level graph ( word-word ),0.6031836867332458
translation,55,34,model,word- level graph,into,sentence - level one,word- level graph into sentence - level one,0.5919767618179321
translation,55,34,model,each word,to,corresponding sentence node,each word to corresponding sentence node,0.5412850379943848
translation,55,34,model,model,transduce,word- level graph,model transduce word- level graph,0.7054027915000916
translation,55,35,model,sentence - level graph ( sentencesentence ),design,redundancy layer,sentence - level graph ( sentencesentence ) design redundancy layer,0.5659358501434326
translation,55,35,model,redundancy layer,firstly pre-labels,each sentence,redundancy layer firstly pre-labels each sentence,0.8374052047729492
translation,55,35,model,redundancy layer,iteratively updates,label dependencies,redundancy layer iteratively updates label dependencies,0.7963542938232422
translation,55,35,model,label dependencies,by propagating,redundancy information,label dependencies by propagating redundancy information,0.7174006700515747
translation,55,35,model,model,For,sentence - level graph ( sentencesentence ),model For sentence - level graph ( sentencesentence ),0.5973196625709534
translation,55,36,model,redundancy layer,restricts,scale of receptive field,redundancy layer restricts scale of receptive field,0.6196689605712891
translation,55,36,model,scale of receptive field,for,redundancy information,scale of receptive field for redundancy information,0.5964632630348206
translation,55,36,model,information passing,guided by,ground - truth labels of sentences,information passing guided by ground - truth labels of sentences,0.6510013341903687
translation,55,36,model,model,has,redundancy layer,model has redundancy layer,0.5985195636749268
translation,56,17,ablation-analysis,hypothesis,states that,goodness ( or effectiveness ),hypothesis states that goodness ( or effectiveness ),0.6655788421630859
translation,56,17,ablation-analysis,goodness ( or effectiveness ),of,any summary candidate,goodness ( or effectiveness ) of any summary candidate,0.5919129252433777
translation,56,17,ablation-analysis,goodness ( or effectiveness ),closely linked with,spectral impact,goodness ( or effectiveness ) closely linked with spectral impact,0.7037240266799927
translation,56,17,ablation-analysis,spectral impact,on,document cluster,spectral impact on document cluster,0.6007289290428162
translation,56,17,ablation-analysis,ablation analysis,has,hypothesis,ablation analysis has hypothesis,0.582574188709259
translation,56,240,ablation-analysis,bert encoder,brings about,one percent improvement,bert encoder brings about one percent improvement,0.7041870951652527
translation,56,240,ablation-analysis,one percent improvement,in,r,one percent improvement in r,0.6204362511634827
translation,56,240,ablation-analysis,r,-,2 score,r - 2 score,0.6523276567459106
translation,56,240,ablation-analysis,r,has,2 score,r has 2 score,0.5851866006851196
translation,56,240,ablation-analysis,ablation analysis,has,bert encoder,ablation analysis has bert encoder,0.5872500538825989
translation,56,92,baselines,simple tf - idf model,with,finer granularity,simple tf - idf model with finer granularity,0.6543437242507935
translation,56,92,baselines,tf -isf,has,simple tf - idf model,tf -isf has simple tf - idf model,0.5646902322769165
translation,56,92,baselines,baselines,has,tf -isf,baselines has tf -isf,0.5387747883796692
translation,56,93,baselines,ese,has,enhanced feature embedding model,ese has enhanced feature embedding model,0.509305477142334
translation,56,198,baselines,baselines,has,"lexrank ( erkan and radev , 2004 )","baselines has lexrank ( erkan and radev , 2004 )",0.518614649772644
translation,56,200,baselines,baselines,has,"classy04 ( conroy et al. , 2004 )","baselines has classy04 ( conroy et al. , 2004 )",0.5318037867546082
translation,56,203,baselines,baselines,has,"c-attention ( li et al. , 2017a )","baselines has c-attention ( li et al. , 2017a )",0.5084033608436584
translation,56,205,baselines,baselines,has,gru -gcn,baselines has gru -gcn,0.5609504580497742
translation,56,212,baselines,baselines,has,"centroid ( rossiello et al. , 2017 )","baselines has centroid ( rossiello et al. , 2017 )",0.5155718326568604
translation,56,217,baselines,baselines,has,pg,baselines has pg,0.6129253506660461
translation,56,219,baselines,baselines,has,"hi-map ( fabbri et al. , 2019 )","baselines has hi-map ( fabbri et al. , 2019 )",0.5112755298614502
translation,56,179,experimental-setup,strategy bert,used,uncased bert - base model,strategy bert used uncased bert - base model,0.5688824653625488
translation,56,179,experimental-setup,uncased bert - base model,pre-trained on,wikipedia,uncased bert - base model pre-trained on wikipedia,0.7347602248191833
translation,56,179,experimental-setup,wikipedia,through,bert - as-service,wikipedia through bert - as-service,0.6666640639305115
translation,56,179,experimental-setup,bert - as-service,to obtain,sentence embedding,bert - as-service to obtain sentence embedding,0.5819090008735657
translation,56,179,experimental-setup,sentence embedding,of,768 dimensions,sentence embedding of 768 dimensions,0.5988790988922119
translation,56,179,experimental-setup,experimental setup,For,strategy bert,experimental setup For strategy bert,0.5994948744773865
translation,56,180,experimental-setup,machine,with,two cpus ( 3.5 ghz ),machine with two cpus ( 3.5 ghz ),0.5488688349723816
translation,56,180,experimental-setup,machine,with,one gpu ( 16g memory ),machine with one gpu ( 16g memory ),0.6093158721923828
translation,56,197,experimental-setup,documents,in,cluster,documents in cluster,0.5409110188484192
translation,56,197,experimental-setup,cluster,are,randomly shuffled,cluster are randomly shuffled,0.6609184741973877
translation,56,197,experimental-setup,first sentence,of,document,first sentence of document,0.5656303763389587
translation,56,197,experimental-setup,document,added to,summary,document added to summary,0.6864140629768372
translation,56,197,experimental-setup,summary,until,length limit,summary until length limit,0.6893319487571716
translation,56,197,experimental-setup,length limit,is,reached,length limit is reached,0.5580995082855225
translation,56,197,experimental-setup,experimental setup,has,documents,experimental setup has documents,0.5115273594856262
translation,56,5,model,spectral - based hypothesis,states that,goodness of summary candidate,spectral - based hypothesis states that goodness of summary candidate,0.6911818385124207
translation,56,5,model,goodness of summary candidate,closely linked to,spectral impact,goodness of summary candidate closely linked to spectral impact,0.6924067735671997
translation,56,5,model,model,propose,spectral - based hypothesis,model propose spectral - based hypothesis,0.7158569693565369
translation,56,16,model,novel spectral - based hypothesis,for,unsupervised mds task,novel spectral - based hypothesis for unsupervised mds task,0.6069858074188232
translation,56,16,model,model,propose,novel spectral - based hypothesis,model propose novel spectral - based hypothesis,0.7138976454734802
translation,56,95,model,sentence encoder,learns,vector representations,sentence encoder learns vector representations,0.7177416086196899
translation,56,95,model,vector representations,by pre-training,deep bi-directional transformer network,vector representations by pre-training deep bi-directional transformer network,0.7122147083282471
translation,56,95,model,bert,has,sentence encoder,bert has sentence encoder,0.5934374928474426
translation,56,95,model,model,has,bert,model has bert,0.6085957288742065
translation,56,199,model,sentence relevancy estimation,by,random walk process,sentence relevancy estimation by random walk process,0.5116125345230103
translation,56,199,model,random walk process,on,sentence graph,random walk process on sentence graph,0.5827066898345947
translation,56,199,model,model,performs,sentence relevancy estimation,model performs sentence relevancy estimation,0.5744324922561646
translation,56,202,model,supervised method,uses,hidden markov model,supervised method uses hidden markov model,0.5618005394935608
translation,56,202,model,supervised method,uses,qr decomposition,supervised method uses qr decomposition,0.5918868184089661
translation,56,202,model,hidden markov model,to rank,sentences,hidden markov model to rank sentences,0.7120085954666138
translation,56,202,model,qr decomposition,to produce,summary,qr decomposition to produce summary,0.6915150880813599
translation,56,202,model,model,As,supervised method,model As supervised method,0.5366320610046387
translation,56,204,model,cascaded attention based auto-encoder,for estimating,relevancy of words and sentences,cascaded attention based auto-encoder for estimating relevancy of words and sentences,0.6718572378158569
translation,56,204,model,model,has,cascaded attention based auto-encoder,model has cascaded attention based auto-encoder,0.5412283539772034
translation,56,218,model,pointergenerator ( pg ) network,motivates,summarizer,pointergenerator ( pg ) network motivates summarizer,0.6515744924545288
translation,56,218,model,summarizer,to copy,original words,summarizer to copy original words,0.6341471672058105
translation,56,218,model,original words,from,input,original words from input,0.577919065952301
translation,56,218,model,input,via,pointing,input via pointing,0.7082462906837463
translation,56,218,model,preserving,has,ability,preserving has ability,0.6131309866905212
translation,56,218,model,ability,has,to generate new words,ability has to generate new words,0.5977731347084045
translation,56,218,model,model,introduces,pointergenerator ( pg ) network,model introduces pointergenerator ( pg ) network,0.6512727737426758
translation,56,230,results,parafuse,by,1.2 percent,parafuse by 1.2 percent,0.5889034271240234
translation,56,230,results,1.2 percent,in,r - 1 score,1.2 percent in r - 1 score,0.5475484132766724
translation,56,230,results,our method,has,spectral - bert,our method has spectral - bert,0.6191797256469727
translation,56,230,results,spectral - bert,has,outperforms,spectral - bert has outperforms,0.6441561579704285
translation,56,230,results,outperforms,has,parafuse,outperforms has parafuse,0.6434566974639893
translation,56,232,results,enormous advantage,say,3.7 % and 2.7 % higher r - 1 and r - 2 score,enormous advantage say 3.7 % and 2.7 % higher r - 1 and r - 2 score,0.6496322154998779
translation,56,232,results,classy04,has,spectral - bert,classy04 has spectral - bert,0.6363949179649353
translation,56,232,results,spectral - bert,has,enormous advantage,spectral - bert has enormous advantage,0.5668472647666931
translation,56,232,results,results,Compared with,classy04,results Compared with classy04,0.6914047002792358
translation,56,233,results,all supervised methods,have,relatively low performance,all supervised methods have relatively low performance,0.5269583463668823
translation,56,233,results,results,Notice,all supervised methods,results Notice all supervised methods,0.6463556885719299
translation,56,236,results,our method,beaten,other unsupervised methods,our method beaten other unsupervised methods,0.5602145195007324
translation,56,236,results,spectral - bert,beaten,other unsupervised methods,spectral - bert beaten other unsupervised methods,0.5943087339401245
translation,56,236,results,other unsupervised methods,by,wide margin,other unsupervised methods by wide margin,0.5454223155975342
translation,56,236,results,r - su4 score,than,c-attention,r - su4 score than c-attention,0.5901864767074585
translation,56,236,results,our method,has,spectral - bert,our method has spectral - bert,0.6191797256469727
translation,56,236,results,2.1 % higher,has,r - su4 score,2.1 % higher has r - su4 score,0.5529758334159851
translation,56,236,results,results,has,our method,results has our method,0.5589964985847473
translation,56,238,results,spectral - bert,beaten,other supervised system ( i.e. pg ),spectral - bert beaten other supervised system ( i.e. pg ),0.5860195159912109
translation,56,238,results,other supervised system ( i.e. pg ),according to,r -,other supervised system ( i.e. pg ) according to r -,0.7265916466712952
translation,56,238,results,r -,has,2 and r - su4 score,r - has 2 and r - su4 score,0.5832175612449646
translation,56,238,results,results,has,spectral - bert,results has spectral - bert,0.5624411106109619
translation,56,239,results,better matrix building strategy,led to,considerable improvement,better matrix building strategy led to considerable improvement,0.6980431079864502
translation,56,239,results,considerable improvement,of,our method,considerable improvement of our method,0.5800655484199524
translation,56,239,results,our method,on,all three datasets,our method on all three datasets,0.46218279004096985
translation,56,239,results,results,observe,better matrix building strategy,results observe better matrix building strategy,0.645362138748169
translation,57,154,baselines,abs +,includes,additional selective gate,abs + includes additional selective gate,0.7101127505302429
translation,57,154,baselines,additional selective gate,to control,information flow,additional selective gate to control information flow,0.7407802939414978
translation,57,154,baselines,information flow,from,encoder,information flow from encoder,0.6011125445365906
translation,57,154,baselines,encoder,to,decoder,encoder to decoder,0.6084607243537903
translation,57,154,baselines,baselines,has,abs +,baselines has abs +,0.6148451566696167
translation,57,155,baselines,pointer-generator,is,integrated pointer network,pointer-generator is integrated pointer network,0.5883228778839111
translation,57,155,baselines,pointer-generator,is,seq2seq model,pointer-generator is seq2seq model,0.5807328224182129
translation,57,155,baselines,baselines,has,pointer-generator,baselines has pointer-generator,0.5389614105224609
translation,57,157,baselines,two pointer - generator based extensions,for,global encoding,two pointer - generator based extensions for global encoding,0.6064887046813965
translation,57,157,baselines,cgu,sets,convolutional gated unit,cgu sets convolutional gated unit,0.6771579384803772
translation,57,157,baselines,cgu,sets,self-attention,cgu sets self-attention,0.6574208736419678
translation,57,157,baselines,cgu,for,global encoding,cgu for global encoding,0.6477320194244385
translation,57,157,baselines,self-attention,for,global encoding,self-attention for global encoding,0.5678425431251526
translation,57,137,experimental-setup,word embeddings,with,128 -d vectors,word embeddings with 128 -d vectors,0.6012167930603027
translation,57,137,experimental-setup,experimental setup,initialize,word embeddings,experimental setup initialize word embeddings,0.7035120129585266
translation,57,139,experimental-setup,vocabulary size,set to,150k,vocabulary size set to 150k,0.7219755053520203
translation,57,139,experimental-setup,150k,for,both the source and target text,150k for both the source and target text,0.6209536790847778
translation,57,139,experimental-setup,experimental setup,has,vocabulary size,experimental setup has vocabulary size,0.5189570188522339
translation,57,140,experimental-setup,hidden state size,set to,256,hidden state size set to 256,0.7133681178092957
translation,57,140,experimental-setup,experimental setup,has,hidden state size,experimental setup has hidden state size,0.5191042423248291
translation,57,141,experimental-setup,vocabulary size,increased,around 602 to 2216 concepts,vocabulary size increased around 602 to 2216 concepts,0.6604979634284973
translation,57,141,experimental-setup,around 602 to 2216 concepts,w.r.t,"different number ( k = 1 , ? ? ? , 5 ) of concept candidates","around 602 to 2216 concepts w.r.t different number ( k = 1 , ? ? ? , 5 ) of concept candidates",0.5979679822921753
translation,57,141,experimental-setup,"different number ( k = 1 , ? ? ? , 5 ) of concept candidates",for,each word,"different number ( k = 1 , ? ? ? , 5 ) of concept candidates for each word",0.5708839297294617
translation,57,141,experimental-setup,experimental setup,has,vocabulary size,experimental setup has vocabulary size,0.5189570188522339
translation,57,143,experimental-setup,https ://,has,github.com / wprojectsn /codes,https :// has github.com / wprojectsn /codes,0.5726005434989929
translation,57,144,experimental-setup,models,on,single gtx ti - tan gpu machine,models on single gtx ti - tan gpu machine,0.5610561370849609
translation,57,144,experimental-setup,experimental setup,trained,models,experimental setup trained models,0.6523525714874268
translation,57,145,experimental-setup,adagrad optimizer,with,batch size,adagrad optimizer with batch size,0.5849330425262451
translation,57,145,experimental-setup,batch size,of,64,batch size of 64,0.6741159558296204
translation,57,145,experimental-setup,64,to minimize,loss,64 to minimize loss,0.6980224251747131
translation,57,145,experimental-setup,experimental setup,used,adagrad optimizer,experimental setup used adagrad optimizer,0.5891650915145874
translation,57,146,experimental-setup,accumulator value,set to,0.15 and 0.1,accumulator value set to 0.15 and 0.1,0.742186963558197
translation,57,146,experimental-setup,experimental setup,set to,0.15 and 0.1,experimental setup set to 0.15 and 0.1,0.7002220749855042
translation,57,146,experimental-setup,experimental setup,has,initial learning rate,experimental setup has initial learning rate,0.49018073081970215
translation,57,146,experimental-setup,experimental setup,has,accumulator value,experimental setup has accumulator value,0.5425283908843994
translation,57,147,experimental-setup,gradient clipping,with,maximum gradient norm,gradient clipping with maximum gradient norm,0.5783113837242126
translation,57,147,experimental-setup,maximum gradient norm,of,2,maximum gradient norm of 2,0.6099976897239685
translation,57,147,experimental-setup,experimental setup,used,gradient clipping,experimental setup used gradient clipping,0.5482161641120911
translation,57,148,experimental-setup,summaries,produced through,beam search,summaries produced through beam search,0.7515744566917419
translation,57,148,experimental-setup,beam search,of size,8,beam search of size 8,0.7339420318603516
translation,57,148,experimental-setup,decoding,has,summaries,decoding has summaries,0.5914456248283386
translation,57,151,experimental-setup,distancesupervised training,at,5 k iterations,distancesupervised training at 5 k iterations,0.5116850137710571
translation,57,151,experimental-setup,distancesupervised training,at,6.5 k iterations,distancesupervised training at 6.5 k iterations,0.5210133790969849
translation,57,151,experimental-setup,5 k iterations,on,duc - 2004,5 k iterations on duc - 2004,0.5750394463539124
translation,57,151,experimental-setup,6.5 k iterations,on,gigaword,6.5 k iterations on gigaword,0.548176646232605
translation,57,151,experimental-setup,experimental setup,took,distancesupervised training,experimental setup took distancesupervised training,0.6198081970214844
translation,57,150,experiments,concept pointer generator,for,450k iterations,concept pointer generator for 450k iterations,0.5861706733703613
translation,57,150,experiments,450k iterations,yielded,best performance,450k iterations yielded best performance,0.6122555732727051
translation,57,150,experiments,optimization using rl rewards,for,rg -l,optimization using rl rewards for rg -l,0.5951733589172363
translation,57,150,experiments,rg -l,at,95 k iterations,rg -l at 95 k iterations,0.592318058013916
translation,57,150,experiments,rg -l,at,50 k iterations,rg -l at 50 k iterations,0.5661352276802063
translation,57,150,experiments,95 k iterations,on,duc - 2004,95 k iterations on duc - 2004,0.5925455689430237
translation,57,150,experiments,50 k iterations,on,gigaword,50 k iterations on gigaword,0.5661215782165527
translation,57,6,model,"knowledge - based , context - aware conceptualizations",to derive,extended set of candidate concepts,"knowledge - based , context - aware conceptualizations to derive extended set of candidate concepts",0.6492677330970764
translation,57,9,model,training model,adapts,different data,training model adapts different data,0.7784674167633057
translation,57,9,model,novel method,of,distantly - supervised learning,novel method of distantly - supervised learning,0.5097399950027466
translation,57,9,model,novel method,guided by,reference summaries and testing set,novel method guided by reference summaries and testing set,0.690635085105896
translation,57,9,model,model,has,training model,model has training model,0.5591098666191101
translation,57,25,model,novel model,based on,concept pointer generator,novel model based on concept pointer generator,0.6803181767463684
translation,57,25,model,model,propose,novel model,model propose novel model,0.6891457438468933
translation,57,26,model,model,alleviates,oov problems,model alleviates oov problems,0.7532491087913513
translation,57,34,model,network,optimized end-to - end,reinforcement learning,network optimized end-to - end reinforcement learning,0.7694233059883118
translation,57,34,model,network,using,reinforcement learning,network using reinforcement learning,0.688444197177887
translation,57,34,model,reinforcement learning,with,distant - supervision strategy,reinforcement learning with distant - supervision strategy,0.6280748248100281
translation,57,34,model,model,has,network,model has network,0.6001628041267395
translation,57,35,model,novel concept pointer generator network,leverages,context - aware conceptualization,novel concept pointer generator network leverages context - aware conceptualization,0.7034826874732971
translation,57,35,model,novel concept pointer generator network,leverages,concept pointer,novel concept pointer generator network leverages concept pointer,0.6648390293121338
translation,57,35,model,novel concept pointer generator network,jointly integrated into,generator,novel concept pointer generator network jointly integrated into generator,0.6939254403114319
translation,57,35,model,concept pointer,jointly integrated into,generator,concept pointer jointly integrated into generator,0.6874897480010986
translation,57,35,model,generator,to deliver,informative and abstract-oriented summaries,generator to deliver informative and abstract-oriented summaries,0.6907915472984314
translation,57,35,model,novel distant supervision training strategy,favors,model adaptation and generalization,novel distant supervision training strategy favors model adaptation and generalization,0.6951637864112854
translation,57,35,model,model adaptation and generalization,results in,performance,model adaptation and generalization results in performance,0.69405597448349
translation,57,35,model,performance,that,outperforms,performance that outperforms,0.6653058528900146
translation,57,35,model,wellaccepted evaluation - based reinforcement learning optimization,on,test-only dataset,wellaccepted evaluation - based reinforcement learning optimization on test-only dataset,0.4823772609233856
translation,57,35,model,test-only dataset,in terms of,rouge metrics,test-only dataset in terms of rouge metrics,0.6158276796340942
translation,57,35,model,statistical analysis,of,quantitative results and human evaluations,statistical analysis of quantitative results and human evaluations,0.550288736820221
translation,57,35,model,outperforms,has,wellaccepted evaluation - based reinforcement learning optimization,outperforms has wellaccepted evaluation - based reinforcement learning optimization,0.5352240800857544
translation,57,10,results,proposed approach,provides,statistically significant improvements,proposed approach provides statistically significant improvements,0.6427999138832092
translation,57,10,results,statistically significant improvements,over,several,statistically significant improvements over several,0.7541900873184204
translation,57,10,results,statistically significant improvements,over,state - of - the - art models,statistically significant improvements over state - of - the - art models,0.6567311882972717
translation,57,10,results,state - of - the - art models,on,duc - 2004 and gigaword datasets,state - of - the - art models on duc - 2004 and gigaword datasets,0.48215410113334656
translation,57,10,results,results,has,proposed approach,results has proposed approach,0.6086713075637817
translation,57,162,results,our model,has,outperformed,our model has outperformed,0.5903543829917908
translation,57,162,results,results,observe,our model,results observe our model,0.6353915333747864
translation,57,163,results,pointer generator performance,improvements made by,our concept pointer,pointer generator performance improvements made by our concept pointer,0.6140866279602051
translation,57,163,results,our concept pointer,are,statistically significant ( p < 0.01 ),our concept pointer are statistically significant ( p < 0.01 ),0.5786355137825012
translation,57,163,results,statistically significant ( p < 0.01 ),across,all metrics,statistically significant ( p < 0.01 ) across all metrics,0.6975923180580139
translation,57,163,results,results,In terms of,pointer generator performance,results In terms of pointer generator performance,0.6843549609184265
translation,57,167,results,our concept pointer generator,achieves,closest performance,our concept pointer generator achieves closest performance,0.6615822911262512
translation,57,167,results,closest performance,with,human-written summaries,closest performance with human-written summaries,0.6300910711288452
translation,57,183,results,ds training,has,noticeable effect,ds training has noticeable effect,0.5787877440452576
translation,57,183,results,ds training,provides,less improvement,ds training provides less improvement,0.6639237999916077
translation,57,183,results,noticeable effect,when,testing set,noticeable effect when testing set,0.661713719367981
translation,57,183,results,testing set,is,substantially semantically different,testing set is substantially semantically different,0.5715501308441162
translation,57,183,results,testing set,provides,less improvement,testing set provides less improvement,0.6784544587135315
translation,57,183,results,substantially semantically different,from,training set,substantially semantically different from training set,0.5683131814002991
translation,57,183,results,less improvement,than,rl,less improvement than rl,0.6534501314163208
translation,57,183,results,ds training,has,noticeable effect,ds training has noticeable effect,0.5787877440452576
translation,57,183,results,results,clear that,ds training,results clear that ds training,0.6806995272636414
translation,58,173,ablation-analysis,summary word generation and attention weights,taught,simultaneously,summary word generation and attention weights taught simultaneously,0.6703060269355774
translation,58,173,ablation-analysis,performance,is,further improved,performance is further improved,0.6026434898376465
translation,58,173,ablation-analysis,performance,surpassing,best baseline,performance surpassing best baseline,0.7434492707252502
translation,58,173,ablation-analysis,best baseline,by,more than two points,best baseline by more than two points,0.5739733576774597
translation,58,173,ablation-analysis,best baseline,by,more than one point,best baseline by more than one point,0.5747462511062622
translation,58,173,ablation-analysis,more than two points,on,gigaword evaluation set,more than two points on gigaword evaluation set,0.522106945514679
translation,58,173,ablation-analysis,more than one point,on,duc2004,more than one point on duc2004,0.599116325378418
translation,58,173,ablation-analysis,summary word generation and attention weights,has,performance,summary word generation and attention weights has performance,0.5244734883308411
translation,58,173,ablation-analysis,ablation analysis,When,summary word generation and attention weights,ablation analysis When summary word generation and attention weights,0.6105082035064697
translation,58,185,ablation-analysis,top - 4 monolingual assum attention weights,achieve,best performance,top - 4 monolingual assum attention weights achieve best performance,0.5864940285682678
translation,58,185,ablation-analysis,best performance,on,validation set,best performance on validation set,0.5691216588020325
translation,58,185,ablation-analysis,ablation analysis,find that,top - 4 monolingual assum attention weights,ablation analysis find that top - 4 monolingual assum attention weights,0.588051438331604
translation,58,167,baselines,transformer,instead of,rnn,transformer instead of rnn,0.6807742714881897
translation,58,158,experiments,transformer bpe,is,monolingual assum system,transformer bpe is monolingual assum system,0.5213234424591064
translation,58,164,experiments,pseudo-,has,summary method,pseudo- has summary method,0.5914162993431091
translation,58,122,hyperparameters,chinese sentences,segmented by,word segmentation tool jieba,chinese sentences segmented by word segmentation tool jieba,0.7740636467933655
translation,58,122,hyperparameters,hyperparameters,has,chinese sentences,hyperparameters has chinese sentences,0.5193877816200256
translation,58,134,hyperparameters,six layers,stacked in,encoder and decoder,six layers stacked in encoder and decoder,0.7448936700820923
translation,58,134,hyperparameters,six layers,stacked in,dimensions,six layers stacked in dimensions,0.786284327507019
translation,58,134,hyperparameters,dimensions,of,embedding vectors,dimensions of embedding vectors,0.5295122861862183
translation,58,134,hyperparameters,dimensions,of,all hidden vectors,dimensions of all hidden vectors,0.5887040495872498
translation,58,134,hyperparameters,all hidden vectors,set,512,all hidden vectors set 512,0.7310564517974854
translation,58,134,hyperparameters,hyperparameters,has,six layers,hyperparameters has six layers,0.5276304483413696
translation,58,135,hyperparameters,eight heads,in,multi-head attention,eight heads in multi-head attention,0.5580825209617615
translation,58,135,hyperparameters,hyperparameters,set,eight heads,hyperparameters set eight heads,0.6557369828224182
translation,58,137,hyperparameters,byte-pair encoding,employed with,vocabulary,byte-pair encoding employed with vocabulary,0.7189146280288696
translation,58,137,hyperparameters,vocabulary,of about,32 k tokens,vocabulary of about 32 k tokens,0.5831005573272705
translation,58,137,hyperparameters,32 k tokens,on,english side and chinese side,32 k tokens on english side and chinese side,0.5254367589950562
translation,58,137,hyperparameters,hyperparameters,has,byte-pair encoding,hyperparameters has byte-pair encoding,0.530939519405365
translation,58,187,model,layers,for,attention relay transformer architecture,layers for attention relay transformer architecture,0.6128843426704407
translation,58,187,model,attention relay transformer architecture,with,six layers,attention relay transformer architecture with six layers,0.6691887974739075
translation,58,187,model,six layers,on,encoder and decoder,six layers on encoder and decoder,0.5564204454421997
translation,58,187,model,six layers,both,encoder and decoder,six layers both encoder and decoder,0.6797495484352112
translation,58,187,model,model,has,layers,model has layers,0.5694299936294556
translation,58,12,results,assum,quick access to,important content,assum quick access to important content,0.05441747605800629
translation,58,12,results,important content,of,source sentences,important content of source sentences,0.5439828634262085
translation,58,12,results,results,has,assum,results has assum,0.426052987575531
translation,58,166,results,pipeline systems,are,pivot- based system,pipeline systems are pivot- based system,0.5921594500541687
translation,58,168,results,pseudo - chinese system,is,best baseline sys-tem,pseudo - chinese system is best baseline sys-tem,0.5447782874107361
translation,58,168,results,best baseline sys-tem,indicating,pseudo source based parallel data,best baseline sys-tem indicating pseudo source based parallel data,0.6518277525901794
translation,58,168,results,results,has,pseudo - chinese system,results has pseudo - chinese system,0.5730987787246704
translation,58,172,results,teaching attention weights,able to,improve,teaching attention weights able to improve,0.6230359673500061
translation,58,172,results,performance,over,baselines,performance over baselines,0.6955903172492981
translation,58,172,results,improve,has,performance,improve has performance,0.5578044652938843
translation,58,172,results,results,manifests,teaching summary word generation,results manifests teaching summary word generation,0.7617833018302917
translation,58,172,results,results,manifests,teaching attention weights,results manifests teaching attention weights,0.6875350475311279
translation,58,191,results,relaying attention,on,last layer,relaying attention on last layer,0.6032490730285645
translation,58,191,results,relaying attention,achieves,best performance,relaying attention achieves best performance,0.7164092659950256
translation,58,191,results,results,show,relaying attention,results show relaying attention,0.5931590795516968
translation,59,26,ablation-analysis,predictors,include,positional information,predictors include positional information,0.6142982840538025
translation,59,26,ablation-analysis,predictors,use of,unique bigram information,predictors use of unique bigram information,0.6618869304656982
translation,59,26,ablation-analysis,predictors,use of,positional information,predictors use of positional information,0.6823244690895081
translation,59,26,ablation-analysis,predictors,avoid,redundancy,predictors avoid redundancy,0.7223668098449707
translation,59,26,ablation-analysis,unique bigram information,to model,content,unique bigram information to model content,0.710493266582489
translation,59,26,ablation-analysis,unique bigram information,avoid,redundancy,unique bigram information avoid redundancy,0.7017355561256409
translation,59,26,ablation-analysis,positional information,to model,important and poor locations,positional information to model important and poor locations,0.7060185074806213
translation,59,26,ablation-analysis,important and poor locations,of,content,important and poor locations of content,0.6419379115104675
translation,59,26,ablation-analysis,language modeling,to capture,stylistic conventions,language modeling to capture stylistic conventions,0.6429580450057983
translation,59,26,ablation-analysis,ablation analysis,has,predictors,ablation analysis has predictors,0.5042789578437805
translation,59,202,ablation-analysis,bigram content indicators,are,important element,bigram content indicators are important element,0.5736315250396729
translation,59,202,ablation-analysis,important element,for,rouge scores,important element for rouge scores,0.6256445050239563
translation,59,202,ablation-analysis,removal,yields,reduction,removal yields reduction,0.7184228301048279
translation,59,202,ablation-analysis,reduction,of,2.46 points,reduction of 2.46 points,0.5594765543937683
translation,59,203,ablation-analysis,model,without,qtsg rules ( ilp w/ o qtsg ),model without qtsg rules ( ilp w/ o qtsg ),0.7251574993133545
translation,59,203,ablation-analysis,model,effectively limited to,sentence extraction,model effectively limited to sentence extraction,0.6396355628967285
translation,59,203,ablation-analysis,model,removing,rewrite rules,model removing rewrite rules,0.701898992061615
translation,59,203,ablation-analysis,qtsg rules ( ilp w/ o qtsg ),effectively limited to,sentence extraction,qtsg rules ( ilp w/ o qtsg ) effectively limited to sentence extraction,0.6752925515174866
translation,59,203,ablation-analysis,rewrite rules,lowers,rouge scores,rewrite rules lowers rouge scores,0.6627461314201355
translation,59,203,ablation-analysis,ablation analysis,has,model,ablation analysis has model,0.4912438988685608
translation,59,6,model,individual aspects,learned separately from,data,individual aspects learned separately from data,0.7500024437904358
translation,59,6,model,optimized jointly,using,integer linear programme,optimized jointly using integer linear programme,0.6648867726325989
translation,59,7,model,ilp framework,combine,decisions,ilp framework combine decisions,0.7156360745429993
translation,59,7,model,ilp framework,select and rewrite,source content,ilp framework select and rewrite source content,0.7112559676170349
translation,59,7,model,decisions,of,expert learners,decisions of expert learners,0.5386319756507874
translation,59,7,model,decisions,select and rewrite,source content,decisions select and rewrite source content,0.7357802391052246
translation,59,7,model,source content,mixture of,"objective setting , soft and hard constraints","source content mixture of objective setting , soft and hard constraints",0.6684753894805908
translation,59,7,model,model,has,ilp framework,model has ilp framework,0.5553948879241943
translation,59,23,model,learned separately,using,specific   expert   predictors,learned separately using specific   expert   predictors,0.6781167984008789
translation,59,23,model,jointly,using,integer linear programming model ( ilp ),jointly using integer linear programming model ( ilp ),0.7305262684822083
translation,59,23,model,integer linear programming model ( ilp ),to generate,output summary,integer linear programming model ( ilp ) to generate output summary,0.7027292847633362
translation,59,25,model,experts,learned from,data,experts learned from data,0.7006365060806274
translation,59,25,model,data,without requiring,additional annotation,data without requiring additional annotation,0.6650416851043701
translation,59,25,model,additional annotation,over and above,summaries,additional annotation over and above summaries,0.6492509841918945
translation,59,25,model,summaries,written for,each document cluster,summaries written for each document cluster,0.7220649719238281
translation,59,25,model,model,has,experts,model has experts,0.5949857234954834
translation,59,29,model,synchronous tree substitution grammar ( stsg ) formalism,can model,non-isomorphic tree structures,synchronous tree substitution grammar ( stsg ) formalism can model non-isomorphic tree structures,0.6635879874229431
translation,59,29,model,model,adopt,synchronous tree substitution grammar ( stsg ) formalism,model adopt synchronous tree substitution grammar ( stsg ) formalism,0.6317644715309143
translation,59,30,model,quasi-synchronous tree substitution grammar ( qtsg ),as,flexible formalism,quasi-synchronous tree substitution grammar ( qtsg ) as flexible formalism,0.5095012784004211
translation,59,30,model,flexible formalism,to learn,general treeedits,flexible formalism to learn general treeedits,0.5976163148880005
translation,59,30,model,general treeedits,from,loosely - aligned phrase structure trees,general treeedits from loosely - aligned phrase structure trees,0.5305494070053101
translation,59,30,model,model,propose,quasi-synchronous tree substitution grammar ( qtsg ),model propose quasi-synchronous tree substitution grammar ( qtsg ),0.6404399275779724
translation,59,51,model,rewrite rules,encoded in,quasi-synchronous tree substitution grammar,rewrite rules encoded in quasi-synchronous tree substitution grammar,0.6667242050170898
translation,59,51,model,rewrite rules,learned automatically from,source documents,rewrite rules learned automatically from source documents,0.7103310823440552
translation,59,51,model,model,has,rewrite rules,model has rewrite rules,0.5592097043991089
translation,59,62,model,approach,where,elements,approach where elements,0.6052652597427368
translation,59,62,model,approach,learned from,training data,approach learned from training data,0.7213638424873352
translation,59,62,model,elements,learned from,training data,elements learned from training data,0.7122635245323181
translation,59,62,model,elements,combined in,integer linear programme,elements combined in integer linear programme,0.683293342590332
translation,59,62,model,training data,by,separate dedicated components,training data by separate dedicated components,0.5759859085083008
translation,59,63,model,content selection,performed partly through,identifying the most salient topics ( bigrams ),content selection performed partly through identifying the most salient topics ( bigrams ),0.616435170173645
translation,59,63,model,summary,based on,positional information,summary based on positional information,0.6512889266014099
translation,59,63,model,model,has,content selection,model has content selection,0.542818546295166
translation,59,27,results,each predictor,gives,better generalization,each predictor gives better generalization,0.616790771484375
translation,59,27,results,separately,gives,better generalization,separately gives better generalization,0.6597403287887573
translation,59,27,results,ilp framework,combine,decisions,ilp framework combine decisions,0.7156360745429993
translation,59,27,results,decisions,of,expert learners,decisions of expert learners,0.5386319756507874
translation,59,27,results,decisions,use of,"objectives , hard and soft constraints","decisions use of objectives , hard and soft constraints",0.6268460750579834
translation,59,27,results,each predictor,has,separately,each predictor has separately,0.5960031747817993
translation,59,27,results,results,Learning,each predictor,results Learning each predictor,0.6843981146812439
translation,59,27,results,results,Learning,ilp framework,results Learning ilp framework,0.6847132444381714
translation,59,164,results,salience svm,obtained,precision,salience svm obtained precision,0.5874438881874084
translation,59,164,results,salience svm,obtained,recall,salience svm obtained recall,0.5847742557525635
translation,59,164,results,precision,of,0.28,precision of 0.28,0.5672427415847778
translation,59,164,results,recall,of,0.43,recall of 0.43,0.5919443964958191
translation,59,164,results,results,has,salience svm,results has salience svm,0.6049140095710754
translation,59,198,results,systems incorporating some form of rewriting,gain,slightly higher rouge scores,systems incorporating some form of rewriting gain slightly higher rouge scores,0.6912527680397034
translation,59,198,results,slightly higher rouge scores,than,icsi -1,slightly higher rouge scores than icsi -1,0.6145609617233276
translation,59,199,results,multiple aspects ilp system ( ma - ilp ),yields,rouge scores,multiple aspects ilp system ( ma - ilp ) yields rouge scores,0.7067826390266418
translation,59,199,results,results,has,multiple aspects ilp system ( ma - ilp ),results has multiple aspects ilp system ( ma - ilp ),0.5691329836845398
translation,59,204,results,rouge scores,allowing,model,rouge scores allowing model,0.7120465040206909
translation,59,204,results,model,to select,  poor quality   sentences ( ilp w/o style ),model to select   poor quality   sentences ( ilp w/o style ),0.6873316764831543
translation,59,204,results,results,has,rouge scores,results has rouge scores,0.541054368019104
translation,59,205,results,non-summary language ( ilp w/o logratio ),affect,rouge scores,non-summary language ( ilp w/o logratio ) affect rouge scores,0.6957910656929016
translation,59,205,results,rouge scores,to,same extent,rouge scores to same extent,0.5761356353759766
translation,59,205,results,results,inclusion of,non-summary language ( ilp w/o logratio ),results inclusion of non-summary language ( ilp w/o logratio ),0.6329586505889893
translation,59,227,results,ma-ilp system,obtains,highest rating,ma-ilp system obtains highest rating,0.6322506070137024
translation,59,227,results,highest rating,with respect to,information content,highest rating with respect to information content,0.5995439291000366
translation,59,227,results,results,has,ma-ilp system,results has ma-ilp system,0.580958366394043
translation,59,230,results,significant gap,from,all systems,significant gap from all systems,0.5990946292877197
translation,59,230,results,significant gap,to,gold -standard human-authored summaries,significant gap to gold -standard human-authored summaries,0.5368265509605408
translation,59,230,results,all systems,to,gold -standard human-authored summaries,all systems to gold -standard human-authored summaries,0.4906098246574402
translation,59,231,results,output summaries,of,full ilp model,output summaries of full ilp model,0.5871213674545288
translation,59,231,results,best results,when considering,contributions,best results when considering contributions,0.7100738286972046
translation,59,231,results,contributions,from,individual model experts collectively,contributions from individual model experts collectively,0.5138453245162964
translation,59,231,results,results,has,output summaries,results has output summaries,0.5401570200920105
translation,60,117,baselines,summaries,from,two state- ofthe - art,summaries from two state- ofthe - art,0.6104546785354614
translation,60,117,baselines,pointer - generator model ( ptgen ),is,rnn - based abstractive systems,pointer - generator model ( ptgen ) is rnn - based abstractive systems,0.565115213394165
translation,60,117,baselines,pointer - generator model ( ptgen ),is,abstractive model,pointer - generator model ( ptgen ) is abstractive model,0.522050142288208
translation,60,117,baselines,topic- aware convolutional sequence to sequence model ( tconvs2s ),is,abstractive model,topic- aware convolutional sequence to sequence model ( tconvs2s ) is abstractive model,0.5525985360145569
translation,60,117,baselines,abstractive model,based entirely on,convolutional neural networks,abstractive model based entirely on convolutional neural networks,0.5681261420249939
translation,60,117,baselines,two state- ofthe - art,has,abstractive summarization systems,two state- ofthe - art has abstractive summarization systems,0.506899356842041
translation,60,117,baselines,baselines,assessed,summaries,baselines assessed summaries,0.6735760569572449
translation,60,6,model,novel approach,for,manual evaluation,novel approach for manual evaluation,0.6105865836143494
translation,60,6,model,highlight - based reference -less evaluation of summarization ( highres ),in which,summaries,highlight - based reference -less evaluation of summarization ( highres ) in which summaries,0.6220353245735168
translation,60,6,model,summaries,assessed by,multiple annotators,summaries assessed by multiple annotators,0.6922756433486938
translation,60,6,model,multiple annotators,against,source document,multiple annotators against source document,0.572752833366394
translation,60,6,model,source document,via,manually highlighted salient content,source document via manually highlighted salient content,0.655040442943573
translation,60,6,model,manual evaluation,has,highlight - based reference -less evaluation of summarization ( highres ),manual evaluation has highlight - based reference -less evaluation of summarization ( highres ),0.5719799399375916
translation,60,6,model,model,propose,novel approach,model propose novel approach,0.7168048620223999
translation,60,142,results,highlight - based and non-highlight based assessment,of,summaries,highlight - based and non-highlight based assessment of summaries,0.6029790639877319
translation,60,142,results,summaries,agree on,ranking,summaries agree on ranking,0.7052921056747437
translation,60,142,results,ranking,among,"tconvs2s , ptgen and reference","ranking among tconvs2s , ptgen and reference",0.6034051179885864
translation,60,142,results,results,Both,highlight - based and non-highlight based assessment,results Both highlight - based and non-highlight based assessment,0.6585491895675659
translation,60,142,results,results,has,highlight - based and non-highlight based assessment,results has highlight - based and non-highlight based assessment,0.5241560339927673
translation,60,143,results,human-authored summaries,considered,best,human-authored summaries considered best,0.704391360282898
translation,60,143,results,tconvs2s,ranked,2nd,tconvs2s ranked 2nd,0.6910285949707031
translation,60,143,results,2nd,followed by,pt - gen,2nd followed by pt - gen,0.7317160367965698
translation,60,143,results,results,Perhaps,human-authored summaries,results Perhaps human-authored summaries,0.692333996295929
translation,60,143,results,results,unsurprisingly,human-authored summaries,results unsurprisingly human-authored summaries,0.6536158919334412
translation,60,144,results,performance difference,in,tconvs2s and ptgen,performance difference in tconvs2s and ptgen,0.5607792139053345
translation,60,144,results,tconvs2s and ptgen,is,greatly amplified,tconvs2s and ptgen is greatly amplified,0.5990419387817383
translation,60,144,results,document,with,highlights,document with highlights,0.6313446164131165
translation,60,144,results,results,has,performance difference,results has performance difference,0.5745248198509216
translation,60,145,results,performance difference,is,lowest,performance difference is lowest,0.6050025224685669
translation,60,145,results,performance difference,evaluated against,reference summary,performance difference evaluated against reference summary,0.7214586138725281
translation,60,145,results,lowest,evaluated against,reference summary,lowest evaluated against reference summary,0.7523995637893677
translation,60,145,results,reference summary,has,2.51 and - 1.79 precision and recall points,reference summary has 2.51 and - 1.79 precision and recall points,0.523993730545044
translation,60,145,results,results,has,performance difference,results has performance difference,0.5745248198509216
translation,60,146,results,superiority,of,tconvs2s,superiority of tconvs2s,0.6915376782417297
translation,60,146,results,superiority,of,tconvs2s,superiority of tconvs2s,0.6915376782417297
translation,60,146,results,tconvs2s,better than,ptgen,tconvs2s better than ptgen,0.7535615563392639
translation,60,146,results,tconvs2s,better than,ptgen,tconvs2s better than ptgen,0.7535615563392639
translation,60,146,results,ptgen,for recognizing,pertinent content,ptgen for recognizing pertinent content,0.7365639805793762
translation,60,146,results,ptgen,generating,informative summaries,ptgen generating informative summaries,0.7424762845039368
translation,60,146,results,results,has,superiority,results has superiority,0.5378279089927673
translation,60,152,results,assessment,of,tconvs2s summaries,assessment of tconvs2s summaries,0.6126964688301086
translation,60,152,results,tconvs2s summaries,achieves,0.67 and 0.80,tconvs2s summaries achieves 0.67 and 0.80,0.6505793929100037
translation,60,152,results,0.67 and 0.80,of,precision and recall cv points,0.67 and 0.80 of precision and recall cv points,0.5885318517684937
translation,60,152,results,precision and recall cv points,are,0.08 and 0.03 points below,precision and recall cv points are 0.08 and 0.03 points below,0.5445587038993835
translation,60,152,results,0.08 and 0.03 points below,assessed against,documents,0.08 and 0.03 points below assessed against documents,0.7063923478126526
translation,60,152,results,documents,has,with no highlights,documents has with no highlights,0.6018929481506348
translation,60,152,results,results,has,assessment,results has assessment,0.5119617581367493
translation,60,157,results,human- authored summaries,considered,best,human- authored summaries considered best,0.704391360282898
translation,60,157,results,ranked 2nd,followed by,ptgen,ranked 2nd followed by ptgen,0.7513514161109924
translation,60,159,results,results,has,highlight - based rouge evaluation,results has highlight - based rouge evaluation,0.5345969200134277
translation,60,163,results,method of copying content,from,original document,method of copying content from original document,0.5532476902008057
translation,60,163,results,ptgen,superior to,tconvs2s,ptgen superior to tconvs2s,0.7744712233543396
translation,60,164,results,ptgen,is,better,ptgen is better,0.6796625256538391
translation,60,164,results,better,in terms of,hrouge,better in terms of hrouge,0.748977541923523
translation,60,164,results,pt - gen,selects,salient content,pt - gen selects salient content,0.7520989775657654
translation,60,165,results,reference summaries,against,original documents,reference summaries against original documents,0.6171842813491821
translation,60,165,results,reference summaries,are,rather abstractive,reference summaries are rather abstractive,0.6043160557746887
translation,60,165,results,reference summaries,are,rather abstractive,reference summaries are rather abstractive,0.6043160557746887
translation,60,165,results,results,comparing,reference summaries,results comparing reference summaries,0.648714542388916
translation,61,240,ablation-analysis,biggest gap,in,date selection f,biggest gap in date selection f,0.5277976989746094
translation,61,240,ablation-analysis,ablation analysis,has,biggest gap,ablation analysis has biggest gap,0.5464926362037659
translation,61,179,baselines,chieu,has,unsupervised approach,chieu has unsupervised approach,0.6279575228691101
translation,61,179,baselines,baselines,has,first baseline,baselines has first baseline,0.595583438873291
translation,61,193,baselines,reg,has,supervised linear regression model,reg has supervised linear regression model,0.5566266179084778
translation,61,6,model,mds optimization models,using,submodular functions,mds optimization models using submodular functions,0.5390129089355469
translation,61,6,model,submodular functions,adapted to yield,wellperforming tls models,submodular functions adapted to yield wellperforming tls models,0.6899834871292114
translation,61,6,model,wellperforming tls models,by designing,objective functions and constraints,wellperforming tls models by designing objective functions and constraints,0.7044731974601746
translation,61,6,model,wellperforming tls models,that model,temporal dimension,wellperforming tls models that model temporal dimension,0.7296212315559387
translation,61,6,model,objective functions and constraints,that model,temporal dimension,objective functions and constraints that model temporal dimension,0.7290059924125671
translation,61,6,model,temporal dimension,inherent in,tls,temporal dimension inherent in tls,0.6963580846786499
translation,61,6,model,model,show that,mds optimization models,model show that mds optimization models,0.49745169281959534
translation,61,28,model,scalable and modular,allowing,  plug-and - play   approach,scalable and modular allowing   plug-and - play   approach,0.7103909254074097
translation,61,28,model,  plug-and - play   approach,for,different submodular functions,  plug-and - play   approach for different submodular functions,0.631101667881012
translation,61,5,results,results,has,tls,results has tls,0.548947811126709
translation,61,214,results,outperforms,except for,date selection,outperforms except for date selection,0.6687095165252686
translation,61,214,results,reg,except for,date selection,reg except for date selection,0.6828328967094421
translation,61,214,results,crisis,has,chieu,crisis has chieu,0.688895583152771
translation,61,214,results,chieu,has,outperforms,chieu has outperforms,0.6664040684700012
translation,61,214,results,outperforms,has,reg,outperforms has reg,0.6728631258010864
translation,61,214,results,results,On,crisis,results On crisis,0.46061691641807556
translation,61,215,results,chieu,for,four out of seven metrics,chieu for four out of seven metrics,0.6233388781547546
translation,61,215,results,time - line17,has,reg,time - line17 has reg,0.6204139590263367
translation,61,215,results,reg,has,outperforms,reg has outperforms,0.6654644012451172
translation,61,215,results,outperforms,has,chieu,outperforms has chieu,0.6601002812385559
translation,61,215,results,results,On,time - line17,results On time - line17,0.5725165605545044
translation,61,220,results,concat and date selection,on,crisis,concat and date selection on crisis,0.618960976600647
translation,61,220,results,advantages,of,modularity,advantages of modularity,0.5275887250900269
translation,61,220,results,advantages,of,non-supervision,advantages of non-supervision,0.5265756249427795
translation,61,220,results,advantages,of,feature / inference separation,advantages of feature / inference separation,0.5480983853340149
translation,61,220,results,outperforms,has,both baselines,outperforms has both baselines,0.5924112200737
translation,61,220,results,results,Except for,concat and date selection,results Except for concat and date selection,0.6581438779830933
translation,61,223,results,improvements,on,all metrics,improvements on all metrics,0.47892019152641296
translation,61,223,results,improvements,on,similar performance,improvements on similar performance,0.5408254861831665
translation,61,223,results,all metrics,on,timeline17,all metrics on timeline17,0.5452713370323181
translation,61,223,results,similar performance,on,crisis,similar performance on crisis,0.5953591465950012
translation,61,223,results,results,Compared to,asmds,results Compared to asmds,0.6468414664268494
translation,61,228,results,similarity,is,not effective,similarity is not effective,0.5844231843948364
translation,61,228,results,roughly the same,according to,most metrics,roughly the same according to most metrics,0.6966360807418823
translation,61,228,results,results drop or stay,has,roughly the same,results drop or stay has roughly the same,0.5526306629180908
translation,61,228,results,results,Modifying,similarity,results Modifying similarity,0.7286848425865173
translation,62,147,ablation-analysis,gradient flow,between,encoder and closed - book decoder,gradient flow between encoder and closed - book decoder,0.6366696953773499
translation,62,147,ablation-analysis,flow,between,closed - book decoder and embedding matrix,flow between closed - book decoder and embedding matrix,0.6000559329986572
translation,62,147,ablation-analysis,flow,observe,non-significant improvements,flow observe non-significant improvements,0.6233985424041748
translation,62,147,ablation-analysis,closed - book decoder and embedding matrix,observe,non-significant improvements,closed - book decoder and embedding matrix observe non-significant improvements,0.5920361280441284
translation,62,147,ablation-analysis,non-significant improvements,in,rouge,non-significant improvements in rouge,0.5607976913452148
translation,62,147,ablation-analysis,non-significant improvements,compared to,baseline,non-significant improvements compared to baseline,0.6715937852859497
translation,62,147,ablation-analysis,rouge,compared to,baseline,rouge compared to baseline,0.6710789799690247
translation,62,147,ablation-analysis,ablation analysis,stop,gradient flow,ablation analysis stop gradient flow,0.6663373708724976
translation,62,148,ablation-analysis,gradient flow,at,2,gradient flow at 2,0.5805670619010925
translation,62,148,ablation-analysis,gradient flow,keep,1,gradient flow keep 1,0.6741964221000671
translation,62,148,ablation-analysis,improvements,are,statistically significant ( p < 0.01 ),improvements are statistically significant ( p < 0.01 ),0.586742639541626
translation,62,148,ablation-analysis,gradient flow,has,improvements,gradient flow has improvements,0.5746947526931763
translation,62,148,ablation-analysis,ablation analysis,stop,gradient flow,ablation analysis stop gradient flow,0.6663373708724976
translation,62,112,baselines,kardashians,strong example of,large celebrity family,kardashians strong example of large celebrity family,0.741598904132843
translation,62,112,baselines,large celebrity family,where,siblings,large celebrity family where siblings,0.5973736643791199
translation,62,112,baselines,siblings,share,very different personality traits,siblings share very different personality traits,0.6426438689231873
translation,62,112,baselines,pointer - gen baseline,has,kardashians,pointer - gen baseline has kardashians,0.631312370300293
translation,62,112,baselines,baselines,has,pointer - gen baseline,baselines has pointer - gen baseline,0.5250119566917419
translation,62,138,baselines,first model,restore,pre-trained encoder,first model restore pre-trained encoder,0.7068511247634888
translation,62,138,baselines,first model,restore,pretrained encoder,first model restore pretrained encoder,0.6905601620674133
translation,62,138,baselines,pre-trained encoder,from,our pointer - generator baseline,pre-trained encoder from our pointer - generator baseline,0.5170955657958984
translation,62,138,baselines,pre-trained encoder,from,our 2 - decoder model,pre-trained encoder from our 2 - decoder model,0.5760285258293152
translation,62,138,baselines,second model,restore,pretrained encoder,second model restore pretrained encoder,0.709325909614563
translation,62,138,baselines,pretrained encoder,from,our 2 - decoder model,pretrained encoder from our 2 - decoder model,0.5635957717895508
translation,62,142,baselines,baselines,has,gradient - flow - cut ablation,baselines has gradient - flow - cut ablation,0.5179121494293213
translation,62,7,experiments,baseline,in terms of,rouge and meteor metrics,baseline in terms of rouge and meteor metrics,0.6028556823730469
translation,62,7,experiments,cnn / daily mail dataset,has,our 2 - decoder model,cnn / daily mail dataset has our 2 - decoder model,0.5496299266815186
translation,62,7,experiments,our 2 - decoder model,has,outperforms,our 2 - decoder model has outperforms,0.5998717546463013
translation,62,7,experiments,outperforms,has,baseline,outperforms has baseline,0.6131853461265564
translation,62,7,experiments,baseline,has,significantly,baseline has significantly,0.5920042991638184
translation,62,5,model,memorization capabilities,of,encoder,memorization capabilities of encoder,0.5979545712471008
translation,62,5,model,encoder,of,pointer - generator model,encoder of pointer - generator model,0.5796356201171875
translation,62,5,model,additional ' closed - book ' decoder,without,attention and pointer mechanisms,additional ' closed - book ' decoder without attention and pointer mechanisms,0.724632740020752
translation,62,5,model,model,improve,memorization capabilities,model improve memorization capabilities,0.6909462809562683
translation,62,28,model,' closed book ' decoder,without,attention layer,' closed book ' decoder without attention layer,0.7068844437599182
translation,62,28,model,attention layer,to,popular pointer - generator baseline,attention layer to popular pointer - generator baseline,0.4644356667995453
translation,62,28,model,model,propose,novel 2 - decoder architecture,model propose novel 2 - decoder architecture,0.6877205967903137
translation,62,50,results,summaries,generated by,our 2 - decoder model,summaries generated by our 2 - decoder model,0.6634519696235657
translation,62,50,results,our 2 - decoder model,are,qualitatively better,our 2 - decoder model are qualitatively better,0.5306429862976074
translation,62,50,results,qualitatively better,than,baseline summaries,qualitatively better than baseline summaries,0.5796304941177368
translation,62,50,results,results,show that,summaries,results show that summaries,0.46289414167404175
translation,62,105,results,results,on,cnn / daily mail dataset,results on cnn / daily mail dataset,0.4994055926799774
translation,62,106,results,2 - decoder model,achieves,statistically significant improvements,2 - decoder model achieves statistically significant improvements,0.6861080527305603
translation,62,106,results,statistically significant improvements,upon,pointergenerator baseline ( pg ),statistically significant improvements upon pointergenerator baseline ( pg ),0.5736232995986938
translation,62,106,results,statistically significant improvements,with,"+ 1.51 , + 0.74 , and + 0.96 points advantage","statistically significant improvements with + 1.51 , + 0.74 , and + 0.96 points advantage",0.5833359956741333
translation,62,106,results,statistically significant improvements,with,+ 1.43 points advantage,statistically significant improvements with + 1.43 points advantage,0.6004346013069153
translation,62,106,results,"+ 1.51 , + 0.74 , and + 0.96 points advantage",in,"rouge -1 , rouge - 2 and rouge -l ( lin , 2004 )","+ 1.51 , + 0.74 , and + 0.96 points advantage in rouge -1 , rouge - 2 and rouge -l ( lin , 2004 )",0.5029730200767517
translation,62,106,results,+ 1.43 points advantage,in,meteor,+ 1.43 points advantage in meteor,0.5636855959892273
translation,62,107,results,our 2 - decoder model,maintains,significant ( p < 0.001 ),our 2 - decoder model maintains significant ( p < 0.001 ),0.5742147564888
translation,62,107,results,reinforced setting,has,our 2 - decoder model,reinforced setting has our 2 - decoder model,0.5168455243110657
translation,62,107,results,results,In,reinforced setting,results In reinforced setting,0.48771944642066956
translation,62,118,results,coverage mechanism,to,baseline and 2 - decoder model,coverage mechanism to baseline and 2 - decoder model,0.5083514451980591
translation,62,118,results,our 2 - decoder model ( pg + cbdec ),receives,significantly higher 2 scores,our 2 - decoder model ( pg + cbdec ) receives significantly higher 2 scores,0.6857223510742188
translation,62,118,results,significantly higher 2 scores,than,original pointer - generator ( pg ),significantly higher 2 scores than original pointer - generator ( pg ),0.5699535608291626
translation,62,118,results,results,add,coverage mechanism,results add coverage mechanism,0.6017008423805237
translation,62,119,results,our 2 - decoder model ( rl + pg + cbdec ),outperforms,strong rl baseline ( rl + pg ),our 2 - decoder model ( rl + pg + cbdec ) outperforms strong rl baseline ( rl + pg ),0.7549598217010498
translation,62,119,results,strong rl baseline ( rl + pg ),by,considerable margin,strong rl baseline ( rl + pg ) by considerable margin,0.5525929927825928
translation,62,125,results,our 2 - decoder model,has,outperforms,our 2 - decoder model has outperforms,0.5998717546463013
translation,62,125,results,outperforms,has,pointer - generator baseline,outperforms has pointer - generator baseline,0.5795822143554688
translation,62,134,results,significantly ( p < 0.001 ) higher,has,article -summary similarity score,significantly ( p < 0.001 ) higher has article -summary similarity score,0.5409646034240723
translation,62,134,results,results,encoder of,our 2 - decoder model,results encoder of our 2 - decoder model,0.7044507265090942
translation,62,140,results,pointer - generator,with,our 2 - decoder model 's encoder,pointer - generator with our 2 - decoder model 's encoder,0.6475948691368103
translation,62,140,results,pointer - generator,with,baseline 's encoder,pointer - generator with baseline 's encoder,0.6470150351524353
translation,62,140,results,pointer - generator,with,baseline 's encoder,pointer - generator with baseline 's encoder,0.6470150351524353
translation,62,140,results,our 2 - decoder model 's encoder,receives,significantly higher ( p < 0.001 ) scores,our 2 - decoder model 's encoder receives significantly higher ( p < 0.001 ) scores,0.6399214267730713
translation,62,140,results,significantly higher ( p < 0.001 ) scores,in,rouge,significantly higher ( p < 0.001 ) scores in rouge,0.5647257566452026
translation,62,140,results,significantly higher ( p < 0.001 ) scores,than,pointer - generator,significantly higher ( p < 0.001 ) scores than pointer - generator,0.5671478509902954
translation,62,140,results,pointer - generator,with,baseline 's encoder,pointer - generator with baseline 's encoder,0.6470150351524353
translation,62,161,results,both saliency tests,demonstrate,our 2 - decoder model 's ability,both saliency tests demonstrate our 2 - decoder model 's ability,0.6247864961624146
translation,62,161,results,our 2 - decoder model 's ability,to memorize,important information,our 2 - decoder model 's ability to memorize important information,0.7917116284370422
translation,62,161,results,important information,address them properly,generated summary,important information address them properly generated summary,0.661055326461792
translation,62,161,results,results,has,both saliency tests,results has both saliency tests,0.5001861453056335
translation,62,167,results,out 2 - decoder model,generate,summaries,out 2 - decoder model generate summaries,0.668091893196106
translation,62,167,results,summaries,that are,less redundant,summaries that are less redundant,0.6710842847824097
translation,62,167,results,summaries,when,both models,summaries when both models,0.7136630415916443
translation,62,167,results,less redundant,compared to,baseline,less redundant compared to baseline,0.7130573987960815
translation,62,167,results,both models,not trained with,coverage mechanism,both models not trained with coverage mechanism,0.754295289516449
translation,62,167,results,results,observe,out 2 - decoder model,results observe out 2 - decoder model,0.6236573457717896
translation,62,177,results,significantly outperforms,as,single - decoder models,significantly outperforms as single - decoder models,0.5020890831947327
translation,62,177,results,single - decoder models,with,more layers / parameters,single - decoder models with more layers / parameters,0.6029679179191589
translation,62,177,results,our 2 - decoder model ( pg + cbdec ),has,significantly outperforms,our 2 - decoder model ( pg + cbdec ) has significantly outperforms,0.5751224756240845
translation,62,177,results,significantly outperforms,has,2 - duplicate - decoder model ( pg + ptrdec ),significantly outperforms has 2 - duplicate - decoder model ( pg + ptrdec ),0.6009003520011902
translation,63,33,experimental-setup,training data,generated from,source documents,training data generated from source documents,0.6401957273483276
translation,63,33,experimental-setup,experimental setup,has,training data,experimental setup has training data,0.5218086242675781
translation,63,133,experimental-setup,models,trained for,10 epochs,models trained for 10 epochs,0.7570341229438782
translation,63,133,experimental-setup,10 epochs,using,batch size,10 epochs using batch size,0.6121646761894226
translation,63,133,experimental-setup,10 epochs,using,learning rate,10 epochs using learning rate,0.6833917498588562
translation,63,133,experimental-setup,batch size,of,12 examples,batch size of 12 examples,0.5667187571525574
translation,63,133,experimental-setup,learning rate,of,2e - 5,learning rate of 2e - 5,0.6410878896713257
translation,63,133,experimental-setup,experimental setup,trained for,10 epochs,experimental setup trained for 10 epochs,0.7368022203445435
translation,63,133,experimental-setup,experimental setup,has,models,experimental setup has models,0.5060054659843445
translation,63,134,experimental-setup,experiments,conducted on,8 nvidia v100 gpus,experiments conducted on 8 nvidia v100 gpus,0.6258215308189392
translation,63,134,experimental-setup,experimental setup,has,experiments,experimental setup has experiments,0.5502888560295105
translation,63,6,model,training data,generated by applying,series of rule- based transformations,training data generated by applying series of rule- based transformations,0.774109423160553
translation,63,6,model,series of rule- based transformations,to,sentences,series of rule- based transformations to sentences,0.5936007499694824
translation,63,6,model,sentences,of,source documents,sentences of source documents,0.5324488282203674
translation,63,6,model,model,has,training data,model has training data,0.5542957186698914
translation,63,7,model,factual consistency model,predict,each summary sentence is factually consistent or not,factual consistency model predict each summary sentence is factually consistent or not,0.6697961091995239
translation,63,7,model,span,in,source document,span in source document,0.5541857481002808
translation,63,7,model,span,to support,consistency prediction,span to support consistency prediction,0.6210219264030457
translation,63,7,model,each summary sentence,extract,inconsistent span,each summary sentence extract inconsistent span,0.6881018877029419
translation,63,7,model,model,has,factual consistency model,model has factual consistency model,0.5613589882850647
translation,63,32,model,"novel , weakly - supervised bertbased ( devlin et al. , 2018 ) model",for verifying,factual consistency,"novel , weakly - supervised bertbased ( devlin et al. , 2018 ) model for verifying factual consistency",0.6925936937332153
translation,63,32,model,"novel , weakly - supervised bertbased ( devlin et al. , 2018 ) model",add,specialized modules,"novel , weakly - supervised bertbased ( devlin et al. , 2018 ) model add specialized modules",0.5941152572631836
translation,63,32,model,specialized modules,explain,portions of both the source document,specialized modules explain portions of both the source document,0.656436562538147
translation,63,32,model,model,propose,"novel , weakly - supervised bertbased ( devlin et al. , 2018 ) model","model propose novel , weakly - supervised bertbased ( devlin et al. , 2018 ) model",0.614061176776886
translation,63,32,model,model,add,specialized modules,model add specialized modules,0.6476561427116394
translation,63,140,results,classifiers,trained on,mnli and fever datasets,classifiers trained on mnli and fever datasets,0.7420778870582581
translation,63,140,results,classifiers,evaluated on,cnn / dailymail test set,classifiers evaluated on cnn / dailymail test set,0.623712420463562
translation,63,140,results,factcc and factccx models,has,substantially outperform,factcc and factccx models has substantially outperform,0.6021037101745605
translation,63,140,results,substantially outperform,has,classifiers,substantially outperform has classifiers,0.6099352240562439
translation,63,140,results,results,has,factcc and factccx models,results has factcc and factccx models,0.5221090316772461
translation,63,141,results,reverse trend,with,mnli model,reverse trend with mnli model,0.6796212792396545
translation,63,141,results,mnli model,achieving,highest performance,mnli model achieving highest performance,0.7036999464035034
translation,63,141,results,results,on,more abstractive xsum data,results on more abstractive xsum data,0.5741636753082275
translation,63,148,results,our model,transfers well to,( sentence - sentence setting,our model transfers well to ( sentence - sentence setting,0.7284411787986755
translation,63,148,results,all other nli models,including,bert finetuned,all other nli models including bert finetuned,0.7363037467002869
translation,63,148,results,bert finetuned,on,mnli dataset,bert finetuned on mnli dataset,0.5495803952217102
translation,63,148,results,outperforms,has,all other nli models,outperforms has all other nli models,0.5873899459838867
translation,63,153,results,synthetic data,train,models,synthetic data train models,0.6436001062393188
translation,63,153,results,models,with,explainable components,models with explainable components,0.6023443341255188
translation,63,153,results,explainable components,such as,factccx,explainable components such as factccx,0.6292192935943604
translation,63,189,results,task,with,highlights,task with highlights,0.618697464466095
translation,63,189,results,annotators,were,21 %,annotators were 21 %,0.5767540335655212
translation,63,189,results,task,has,annotators,task has annotators,0.49730005860328674
translation,63,189,results,highlights,has,annotators,highlights has annotators,0.49939456582069397
translation,63,189,results,21 %,has,faster,21 % has faster,0.6070455312728882
translation,63,189,results,results,When completing,task,results When completing task,0.6948870420455933
translation,64,201,experimental-setup,algorithm,implemented in,c + +,algorithm implemented in c + +,0.7363659143447876
translation,64,201,experimental-setup,linux machine,with,xeon e5-2670 2.60 ghz cpu,linux machine with xeon e5-2670 2.60 ghz cpu,0.539354681968689
translation,64,201,experimental-setup,linux machine,with,192 gb,linux machine with 192 gb,0.5894867777824402
translation,64,201,experimental-setup,experimental setup,has,algorithm,experimental setup has algorithm,0.5328841209411621
translation,64,7,model,tree trimming problems,whose,running time,tree trimming problems whose running time,0.5252006649971008
translation,64,7,model,model,propose,dynamic programming ( dp ) algorithm,model propose dynamic programming ( dp ) algorithm,0.6642892360687256
translation,64,8,model,data structure,represents,family of sets,data structure represents family of sets,0.6074648499488831
translation,64,8,model,family of sets,as,directed acyclic graph,family of sets as directed acyclic graph,0.5287992358207703
translation,64,8,model,set of subtrees,in,compact form,set of subtrees in compact form,0.5471172332763672
translation,64,8,model,zero-suppressed binary decision diagram ( zdd ),has,data structure,zero-suppressed binary decision diagram ( zdd ) has data structure,0.5377886891365051
translation,64,21,model,dynamic programming ( dp ) algorithm,for,tree trimming problems,dynamic programming ( dp ) algorithm for tree trimming problems,0.5397800207138062
translation,64,21,model,tree trimming problems,focus on,text summarization,tree trimming problems focus on text summarization,0.6338457465171814
translation,64,21,model,model,propose,dynamic programming ( dp ) algorithm,model propose dynamic programming ( dp ) algorithm,0.6642892360687256
translation,64,226,model,proposed algorithm,extends,zdd - based optimization algorithm,proposed algorithm extends zdd - based optimization algorithm,0.6523794531822205
translation,64,226,model,zdd - based optimization algorithm,to solve,knapsack problems,zdd - based optimization algorithm to solve knapsack problems,0.6754201054573059
translation,64,226,model,model,has,proposed algorithm,model has proposed algorithm,0.6020276546478271
translation,64,22,results,results,has,algorithm,results has algorithm,0.5900275707244873
translation,65,147,baselines,lexrank,is,unsupervised extractive graph - based algorithm,lexrank is unsupervised extractive graph - based algorithm,0.5588731169700623
translation,65,147,baselines,unsupervised extractive graph - based algorithm,selecting,sentences,unsupervised extractive graph - based algorithm selecting sentences,0.703371524810791
translation,65,147,baselines,sentences,based on,graph centrality,sentences based on graph centrality,0.6844116449356079
translation,65,128,experimental-setup,"transformer architecture ( vaswani et al. , 2017 )",with,trainable length embeddings,"transformer architecture ( vaswani et al. , 2017 ) with trainable length embeddings",0.5729787945747375
translation,65,128,experimental-setup,"transformer architecture ( vaswani et al. , 2017 )",with,shared parameters,"transformer architecture ( vaswani et al. , 2017 ) with shared parameters",0.6069337129592896
translation,65,128,experimental-setup,shared parameters,between,"encoder and generator ( raffel et al. , 2019 )","shared parameters between encoder and generator ( raffel et al. , 2019 )",0.6178978681564331
translation,65,129,experimental-setup,subwords,obtained with,"bpe ( sennrich et al. , 2016 )","subwords obtained with bpe ( sennrich et al. , 2016 )",0.5886237621307373
translation,65,129,experimental-setup,"bpe ( sennrich et al. , 2016 )",using,32000 merges,"bpe ( sennrich et al. , 2016 ) using 32000 merges",0.6301703453063965
translation,65,129,experimental-setup,experimental setup,has,subwords,experimental setup has subwords,0.5182080268859863
translation,65,135,experimental-setup,parameter optimization,performed using,"adam ( kingma and ba , 2014 )","parameter optimization performed using adam ( kingma and ba , 2014 )",0.6111711263656616
translation,65,135,experimental-setup,beam search,with,"n-gram blocking ( paulus et al. , 2017 )","beam search with n-gram blocking ( paulus et al. , 2017 )",0.5881001353263855
translation,65,135,experimental-setup,beam search,with,copycat,beam search with copycat,0.6758108735084534
translation,65,135,experimental-setup,"n-gram blocking ( paulus et al. , 2017 )",applied to,our model,"n-gram blocking ( paulus et al. , 2017 ) applied to our model",0.650254487991333
translation,65,135,experimental-setup,copycat,for,summary generation,copycat for summary generation,0.6009671688079834
translation,65,135,experimental-setup,experimental setup,has,parameter optimization,experimental setup has parameter optimization,0.5086594223976135
translation,65,136,experimental-setup,experiments,conducted on,4 x geforce rtx 2080 ti,experiments conducted on 4 x geforce rtx 2080 ti,0.7257052659988403
translation,65,136,experimental-setup,experimental setup,conducted on,4 x geforce rtx 2080 ti,experimental setup conducted on 4 x geforce rtx 2080 ti,0.6938410997390747
translation,65,136,experimental-setup,experimental setup,has,experiments,experimental setup has experiments,0.5502888560295105
translation,65,138,experimental-setup,parameter - shared encoder-generator model,used,8 - head and 6 - layer transformer stack,parameter - shared encoder-generator model used 8 - head and 6 - layer transformer stack,0.5929732918739319
translation,65,138,experimental-setup,experimental setup,has,parameter - shared encoder-generator model,experimental setup has parameter - shared encoder-generator model,0.5238471627235413
translation,65,139,experimental-setup,dropout,in,sub-layers and subword embeddings dropout,dropout in sub-layers and subword embeddings dropout,0.4561736583709717
translation,65,139,experimental-setup,dropout,used,1000 dimensional,dropout used 1000 dimensional,0.5554312467575073
translation,65,139,experimental-setup,sub-layers and subword embeddings dropout,set to,0.1,sub-layers and subword embeddings dropout set to 0.1,0.596206784248352
translation,65,139,experimental-setup,1000 dimensional,has,position - wise feed -forward neural networks,1000 dimensional has position - wise feed -forward neural networks,0.5813015699386597
translation,65,139,experimental-setup,experimental setup,used,1000 dimensional,experimental setup used 1000 dimensional,0.6123027801513672
translation,65,139,experimental-setup,experimental setup,has,dropout,experimental setup has dropout,0.5067690014839172
translation,65,140,experimental-setup,subword and length embeddings,to,390 and 10,subword and length embeddings to 390 and 10,0.5931220650672913
translation,65,140,experimental-setup,subword and length embeddings,used as,input,subword and length embeddings used as input,0.5510721802711487
translation,65,140,experimental-setup,experimental setup,set,subword and length embeddings,experimental setup set subword and length embeddings,0.6062763333320618
translation,65,141,experimental-setup,plug- in network,set,output dimension,plug- in network set output dimension,0.6566132307052612
translation,65,141,experimental-setup,plug- in network,set,internal feedforward network hidden dimensions,plug- in network set internal feedforward network hidden dimensions,0.6641648411750793
translation,65,141,experimental-setup,output dimension,to,30,output dimension to 30,0.6162145137786865
translation,65,141,experimental-setup,internal feedforward network hidden dimensions,to,20,internal feedforward network hidden dimensions to 20,0.6114152073860168
translation,65,141,experimental-setup,experimental setup,For,plug- in network,experimental setup For plug- in network,0.6055565476417542
translation,65,143,experimental-setup,experimental setup,applied,0.4 internal dropout,experimental setup applied 0.4 internal dropout,0.6670374274253845
translation,65,143,experimental-setup,experimental setup,applied,0.15 attention dropout,experimental setup applied 0.15 attention dropout,0.6460296511650085
translation,65,269,experimental-setup,yelp,trained for,87 epochs,yelp trained for 87 epochs,0.776008129119873
translation,65,269,experimental-setup,yelp,fine- tuned,plugin network,yelp fine- tuned plugin network,0.6832691431045532
translation,65,269,experimental-setup,87 epochs,with,1,87 epochs with 1,0.6706522107124329
translation,65,269,experimental-setup,1,*,10 ?5,1 * 10 ?5,0.693529486656189
translation,65,269,experimental-setup,plugin network,on,human-written summaries,plugin network on human-written summaries,0.5377025604248047
translation,65,269,experimental-setup,human-written summaries,by,output matching,human-written summaries by output matching,0.5382471084594727
translation,65,269,experimental-setup,output matching,with,oracle,output matching with oracle,0.6129030585289001
translation,65,269,experimental-setup,1,has,10 ?5,1 has 10 ?5,0.6002153158187866
translation,65,269,experimental-setup,experimental setup,On,yelp,experimental setup On yelp,0.6036489009857178
translation,65,269,experimental-setup,experimental setup,fine- tuned,plugin network,experimental setup fine- tuned plugin network,0.6800014972686768
translation,65,264,experiments,yelp,trained it for,27 epochs,yelp trained it for 27 epochs,0.700434684753418
translation,65,264,experiments,27 epochs,with,lr,27 epochs with lr,0.6745697259902954
translation,65,264,experiments,lr,set to,7 * 10 ?4,lr set to 7 * 10 ?4,0.6575790047645569
translation,65,11,model,conditional transformer language model,to generate,new product review,conditional transformer language model to generate new product review,0.6647411584854126
translation,65,11,model,new product review,given,other available reviews,new product review given other available reviews,0.6327555775642395
translation,65,11,model,other available reviews,of,product,other available reviews of product,0.617165207862854
translation,65,11,model,model,training,conditional transformer language model,model training conditional transformer language model,0.6111604571342468
translation,65,13,model,second stage,fine - tune,plug- in module,second stage fine - tune plug- in module,0.7281355857849121
translation,65,13,model,plug- in module,learns to predict,property values,plug- in module learns to predict property values,0.7079988121986389
translation,65,13,model,property values,on,handful of summaries,property values on handful of summaries,0.4918439984321594
translation,65,13,model,model,In,second stage,model In second stage,0.5526903867721558
translation,65,39,model,number of annotated instances,is sufficient to bootstrap,generation,number of annotated instances is sufficient to bootstrap generation,0.6814389228820801
translation,65,39,model,generation,of,formal summary text,generation of formal summary text,0.5616190433502197
translation,65,39,model,generation,that is both,informative and fluent,generation that is both informative and fluent,0.6860726475715637
translation,65,39,model,formal summary text,that is both,informative and fluent,formal summary text that is both informative and fluent,0.5987508296966553
translation,65,39,model,model,propose,few-shot learning framework,model propose few-shot learning framework,0.6182550191879272
translation,65,52,model,fine-tuning parts,of,model,fine-tuning parts of model,0.5612145066261292
translation,65,52,model,fine-tuning parts,jointly with,tiny plug-in network,fine-tuning parts jointly with tiny plug-in network,0.6008087396621704
translation,65,52,model,tiny plug-in network,handful of,human-written summaries,tiny plug-in network handful of human-written summaries,0.6307468414306641
translation,65,53,model,plug- in network,trained to,output,plug- in network trained to output,0.7548525333404541
translation,65,53,model,property values,that make,summaries,property values that make summaries,0.6517045497894287
translation,65,53,model,summaries,under,trained clm,summaries under trained clm,0.6745138764381409
translation,65,53,model,output,has,property values,output has property values,0.5782807469367981
translation,65,53,model,summaries,has,likely,summaries has likely,0.6023316383361816
translation,65,53,model,model,has,plug- in network,model has plug- in network,0.580578088760376
translation,65,108,model,plug- in network,employed,multi-layer feed - forward network,plug- in network employed multi-layer feed - forward network,0.5905138254165649
translation,65,108,model,multi-layer feed - forward network,with,multi-head attention modules,multi-layer feed - forward network with multi-head attention modules,0.615008533000946
translation,65,108,model,multi-head attention modules,over,encoded states,multi-head attention modules over encoded states,0.6413601636886597
translation,65,108,model,encoded states,of,source reviews,encoded states of source reviews,0.6122211813926697
translation,65,108,model,encoded states,followed by,linear transformation,encoded states followed by linear transformation,0.6696409583091736
translation,65,108,model,source reviews,at,each layer,source reviews at each layer,0.5588727593421936
translation,65,108,model,linear transformation,predicting,property values,linear transformation predicting property values,0.7011080980300903
translation,65,108,model,model,For,plug- in network,model For plug- in network,0.6309043169021606
translation,65,133,model,plug- in network,employed,multi-layer feed -forward network,plug- in network employed multi-layer feed -forward network,0.5905138254165649
translation,65,133,model,multi-layer feed -forward network,with,multi-head attention modules,multi-layer feed -forward network with multi-head attention modules,0.615008533000946
translation,65,133,model,multi-head attention modules,over,encoded states,multi-head attention modules over encoded states,0.6413601636886597
translation,65,133,model,encoded states,of,source review,encoded states of source review,0.6081185340881348
translation,65,133,model,model,For,plug- in network,model For plug- in network,0.6309043169021606
translation,65,142,model,stack,of,3 layers,stack of 3 layers,0.636662483215332
translation,65,142,model,attention modules,with,3 heads,attention modules with 3 heads,0.6702002882957458
translation,65,142,model,3 heads,at,each layer,3 heads at each layer,0.5455278754234314
translation,65,142,model,model,used,stack,model used stack,0.6786051988601685
translation,65,142,model,model,used,attention modules,model used attention modules,0.6075948476791382
translation,65,250,model,summary related properties,in,unannotated reviews,summary related properties in unannotated reviews,0.4948631227016449
translation,65,250,model,summary related properties,used for,unsupervised training,summary related properties used for unsupervised training,0.6331840753555298
translation,65,250,model,unsupervised training,of,generator,unsupervised training of generator,0.5851709842681885
translation,65,250,model,model,exploit,summary related properties,model exploit summary related properties,0.7416452765464783
translation,65,251,model,tiny plug-in network,learns to switch,generator,tiny plug-in network learns to switch generator,0.7108151316642761
translation,65,251,model,generator,to,summarization regime,generator to summarization regime,0.573714554309845
translation,65,251,model,model,train,tiny plug-in network,model train tiny plug-in network,0.6523084044456482
translation,65,10,results,even a handful of summaries,is sufficient to bootstrap,generation,even a handful of summaries is sufficient to bootstrap generation,0.7813767194747925
translation,65,10,results,generation,of,summary text,generation of summary text,0.6012471914291382
translation,65,10,results,summary text,with,all expected properties,summary text with all expected properties,0.6450772285461426
translation,65,10,results,all expected properties,such as,writing style,all expected properties such as writing style,0.5986409187316895
translation,65,10,results,all expected properties,such as,informativeness,all expected properties such as informativeness,0.5672321915626526
translation,65,10,results,all expected properties,such as,fluency,all expected properties such as fluency,0.6371169686317444
translation,65,10,results,all expected properties,such as,sentiment preservation,all expected properties such as sentiment preservation,0.5894522070884705
translation,65,10,results,results,show that,even a handful of summaries,results show that even a handful of summaries,0.5270577073097229
translation,65,164,results,amazon data,indicate,our model,amazon data indicate our model,0.6111387014389038
translation,65,164,results,our model,preferred across,board,our model preferred across board,0.7366618514060974
translation,65,164,results,board,over,baselines,board over baselines,0.7280598878860474
translation,65,164,results,results,On,amazon data,results On amazon data,0.5028453469276428
translation,65,165,results,copycat,preferred over,lexrank,copycat preferred over lexrank,0.7489235997200012
translation,65,165,results,copycat,shows,worse results,copycat shows worse results,0.756849467754364
translation,65,165,results,lexrank,in terms of,fluency,lexrank in terms of fluency,0.6921141147613525
translation,65,165,results,lexrank,in terms of,non-redundancy,lexrank in terms of non-redundancy,0.6814590692520142
translation,65,165,results,worse results,in terms of,informativeness,worse results in terms of informativeness,0.6332890391349792
translation,65,165,results,worse results,in terms of,overall sentiment preservation,worse results in terms of overall sentiment preservation,0.6576173305511475
translation,65,165,results,results,has,copycat,results has copycat,0.5847207903862
translation,65,173,results,more diverse and detailed summaries,on par with,copycat,more diverse and detailed summaries on par with copycat,0.7661721110343933
translation,65,186,results,model,is,noticeably improves,model is noticeably improves,0.6307060122489929
translation,65,186,results,model,is,substantially worse,model is substantially worse,0.6002184152603149
translation,65,186,results,model,fine-tuned on,gold summaries ( usl + f ),model fine-tuned on gold summaries ( usl + f ),0.748102605342865
translation,65,186,results,substantially worse,than,our proposed few-shot approach,substantially worse than our proposed few-shot approach,0.5803605914115906
translation,65,186,results,noticeably improves,has,results,noticeably improves has results,0.5850417017936707
translation,65,186,results,results,when,model,results when model,0.6575631499290466
translation,65,188,results,mtl,fails to,decouple,mtl fails to decouple,0.8009864687919617
translation,65,188,results,slight improvement,over,usl,slight improvement over usl,0.7360480427742004
translation,65,188,results,decouple,has,tasks,decouple has tasks,0.5962104201316833
translation,65,188,results,results,observed that,mtl,results observed that mtl,0.6992328763008118
translation,65,231,results,models,perform,onpar,models perform onpar,0.6608904600143433
translation,65,231,results,onpar,on,most of the domains,onpar on most of the domains,0.6475767493247986
translation,65,231,results,results,indicate,models,results indicate models,0.5438011288642883
translation,65,232,results,in-domain model,shows,substantially better results,in-domain model shows substantially better results,0.674698531627655
translation,65,232,results,substantially better results,on,health domain,substantially better results on health domain,0.49244219064712524
translation,65,232,results,results,has,in-domain model,results has in-domain model,0.5603916049003601
translation,65,469,results,1:1 review-summary proportion,works,best,1:1 review-summary proportion works best,0.6312634348869324
translation,65,469,results,results,observed,1:1 review-summary proportion,results observed 1:1 review-summary proportion,0.6094905734062195
translation,66,211,ablation-analysis,richer relations,more helpful to,mds,richer relations more helpful to mds,0.6894689798355103
translation,66,211,ablation-analysis,ablation analysis,graphs with,richer relations,ablation analysis graphs with richer relations,0.7403398752212524
translation,66,155,baselines,seq2seq baselines,truncate,n input documents,seq2seq baselines truncate n input documents,0.7116732597351074
translation,66,155,baselines,n input documents,to,l tokens,n input documents to l tokens,0.4799076318740845
translation,66,155,baselines,first l/n tokens,from,each source document,first l/n tokens from each source document,0.5680365562438965
translation,66,172,baselines,pretrained lms,apply,different optimizers,pretrained lms apply different optimizers,0.6254212856292725
translation,66,172,baselines,different optimizers,for,pretrained part and other parts,different optimizers for pretrained part and other parts,0.5543155074119568
translation,66,184,baselines,transformer - based encoderdecoder model,on,flat token sequence,transformer - based encoderdecoder model on flat token sequence,0.5453857183456421
translation,66,184,baselines,ft ( flat transformer ),has,transformer - based encoderdecoder model,ft ( flat transformer ) has transformer - based encoderdecoder model,0.5546826720237732
translation,66,184,baselines,t-dmca,has,best performing model,t-dmca has best performing model,0.5810002088546753
translation,66,191,baselines,encoder of ft,by,base versions,encoder of ft by base versions,0.5792569518089294
translation,66,191,baselines,base versions,of,pre-trained lms,base versions of pre-trained lms,0.557923436164856
translation,66,161,experimental-setup,all models,with,maximum likelihood estimation,all models with maximum likelihood estimation,0.5254611372947693
translation,66,161,experimental-setup,label smoothing,with,smoothing factor,label smoothing with smoothing factor,0.62163907289505
translation,66,161,experimental-setup,smoothing factor,has,0.1,smoothing factor has 0.1,0.514997661113739
translation,66,161,experimental-setup,experimental setup,train,all models,experimental setup train all models,0.6761652231216431
translation,66,163,experimental-setup,learning rate warmup,over,"first 8,000 steps","learning rate warmup over first 8,000 steps",0.6533567905426025
translation,66,163,experimental-setup,experimental setup,apply,learning rate warmup,experimental setup apply learning rate warmup,0.5720770955085754
translation,66,164,experimental-setup,gradient clipping,with,maximum gradient norm 2.0,gradient clipping with maximum gradient norm 2.0,0.6004931926727295
translation,66,164,experimental-setup,maximum gradient norm 2.0,utilized during,training,maximum gradient norm 2.0 utilized during training,0.6684382557868958
translation,66,164,experimental-setup,experimental setup,has,gradient clipping,experimental setup has gradient clipping,0.4766085147857666
translation,66,165,experimental-setup,4 gpus ( tesla v100 ),for,"500,000 steps","4 gpus ( tesla v100 ) for 500,000 steps",0.5707353949546814
translation,66,165,experimental-setup,4 gpus ( tesla v100 ),with,gradient accumulation,4 gpus ( tesla v100 ) with gradient accumulation,0.5879825949668884
translation,66,165,experimental-setup,"500,000 steps",with,gradient accumulation,"500,000 steps with gradient accumulation",0.6087288856506348
translation,66,165,experimental-setup,gradient accumulation,every,four steps,gradient accumulation every four steps,0.6197498440742493
translation,66,165,experimental-setup,experimental setup,trained on,4 gpus ( tesla v100 ),experimental setup trained on 4 gpus ( tesla v100 ),0.6936171054840088
translation,66,166,experimental-setup,dropout,with,probability 0.1,dropout with probability 0.1,0.6266955733299255
translation,66,166,experimental-setup,probability 0.1,before,all linear layers,probability 0.1 before all linear layers,0.6418308615684509
translation,66,166,experimental-setup,experimental setup,apply,dropout,experimental setup apply dropout,0.5288015604019165
translation,66,167,experimental-setup,number of hidden units,in,our models,number of hidden units in our models,0.4497196674346924
translation,66,167,experimental-setup,number of hidden units,set as,256,number of hidden units set as 256,0.6601744890213013
translation,66,167,experimental-setup,feed -forward hidden size,is,"1,024","feed -forward hidden size is 1,024",0.5904868841171265
translation,66,167,experimental-setup,number of heads,is,8,number of heads is 8,0.5963917374610901
translation,66,167,experimental-setup,experimental setup,has,number of hidden units,experimental setup has number of hidden units,0.5046243071556091
translation,66,167,experimental-setup,experimental setup,has,feed -forward hidden size,experimental setup has feed -forward hidden size,0.5573694705963135
translation,66,167,experimental-setup,experimental setup,has,number of heads,experimental setup has number of heads,0.5273870229721069
translation,66,168,experimental-setup,graph encoding layers and graph decoding layers,set as,"6 , 2 and 8","graph encoding layers and graph decoding layers set as 6 , 2 and 8",0.6396976709365845
translation,66,168,experimental-setup,experimental setup,has,number of transformer encoding layers,experimental setup has number of transformer encoding layers,0.5099906325340271
translation,66,169,experimental-setup,parameter,set as,2.0,parameter set as 2.0,0.6196287274360657
translation,66,169,experimental-setup,2.0,after,tuning,2.0 after tuning,0.6255984902381897
translation,66,169,experimental-setup,tuning,on,validation dataset,tuning on validation dataset,0.5594532489776611
translation,66,169,experimental-setup,experimental setup,has,parameter,experimental setup has parameter,0.513629138469696
translation,66,170,experimental-setup,decoding,use,beam search,decoding use beam search,0.6366719603538513
translation,66,170,experimental-setup,beam search,with,beam size 5,beam search with beam size 5,0.7065784335136414
translation,66,170,experimental-setup,beam search,with,length penalty,beam search with length penalty,0.6435361504554749
translation,66,170,experimental-setup,length penalty,with,factor 0.6,length penalty with factor 0.6,0.6250239014625549
translation,66,170,experimental-setup,experimental setup,During,decoding,experimental setup During decoding,0.6438420414924622
translation,66,171,experimental-setup,trigram blocking,reduce,repetitions,trigram blocking reduce repetitions,0.698482871055603
translation,66,171,experimental-setup,experimental setup,has,trigram blocking,experimental setup has trigram blocking,0.5485354065895081
translation,66,173,experimental-setup,two adam optimizers,with,? 1 =0.9 and ? 2 =0.999,two adam optimizers with ? 1 =0.9 and ? 2 =0.999,0.6343840956687927
translation,66,173,experimental-setup,? 1 =0.9 and ? 2 =0.999,used for,pretrained part,? 1 =0.9 and ? 2 =0.999 used for pretrained part,0.7184406518936157
translation,66,173,experimental-setup,? 1 =0.9 and ? 2 =0.999,used for,other parts,? 1 =0.9 and ? 2 =0.999 used for other parts,0.7100562453269958
translation,66,173,experimental-setup,experimental setup,has,two adam optimizers,experimental setup has two adam optimizers,0.5296508073806763
translation,66,174,experimental-setup,learning rate and warmup steps,for,pretrained part,learning rate and warmup steps for pretrained part,0.6016329526901245
translation,66,174,experimental-setup,learning rate and warmup steps,set as,0.002 and 20000,learning rate and warmup steps set as 0.002 and 20000,0.6286417245864868
translation,66,174,experimental-setup,0.2 and 10000,for,other parts,0.2 and 10000 for other parts,0.6522072553634644
translation,66,174,experimental-setup,experimental setup,has,learning rate and warmup steps,experimental setup has learning rate and warmup steps,0.4855553209781647
translation,66,162,experiments,optimizer,is,"adam ( kingma and ba , 2015 )","optimizer is adam ( kingma and ba , 2015 )",0.5079987645149231
translation,66,162,experiments,"adam ( kingma and ba , 2015 )",with,"learning rate 2 , ? 1 =0.9 and ? 2 =0.998","adam ( kingma and ba , 2015 ) with learning rate 2 , ? 1 =0.9 and ? 2 =0.998",0.5972273945808411
translation,66,5,model,model,develop,neural abstractive multidocument summarization ( mds ),model develop neural abstractive multidocument summarization ( mds ),0.6074607968330383
translation,66,6,model,graphs,to encode,documents,graphs to encode documents,0.7294677495956421
translation,66,6,model,graphs,to encode,documents,graphs to encode documents,0.7294677495956421
translation,66,6,model,documents,to capture,cross-document relations,documents to capture cross-document relations,0.6564179062843323
translation,66,6,model,model,utilizes,graphs,model utilizes graphs,0.6569812893867493
translation,66,7,model,graphs,to guide,summary generation process,graphs to guide summary generation process,0.6706708669662476
translation,66,7,model,model,take advantage of,graphs,model take advantage of graphs,0.6680207848548889
translation,66,18,model,neural abstractive mds model,leverage,explicit graph representations,neural abstractive mds model leverage explicit graph representations,0.7096404433250427
translation,66,18,model,neural abstractive mds model,distill,abstractive summaries,neural abstractive mds model distill abstractive summaries,0.6748857498168945
translation,66,18,model,explicit graph representations,of,documents,explicit graph representations of documents,0.6028634309768677
translation,66,18,model,explicit graph representations,to more effectively process,multiple input documents,explicit graph representations to more effectively process multiple input documents,0.6000432372093201
translation,66,18,model,model,develop,neural abstractive mds model,model develop neural abstractive mds model,0.6292160749435425
translation,66,19,model,well - established graphs,into,document representation,well - established graphs into document representation,0.5555745363235474
translation,66,19,model,well - established graphs,into,summary generation processes,well - established graphs into summary generation processes,0.5726401805877686
translation,66,19,model,model,augments,end-toend neural architecture,model augments end-toend neural architecture,0.6752731800079346
translation,66,20,model,graph- informed attention mechanism,developed to incorporate,graphs,graph- informed attention mechanism developed to incorporate graphs,0.6664599180221558
translation,66,20,model,graphs,into,document encoding process,graphs into document encoding process,0.5755655765533447
translation,66,20,model,graphs,enables,our model,graphs enables our model,0.7017405033111572
translation,66,20,model,our model,to capture,richer cross-document relations,our model to capture richer cross-document relations,0.6770113110542297
translation,66,20,model,model,has,graph- informed attention mechanism,model has graph- informed attention mechanism,0.5207233428955078
translation,66,21,model,graphs,to guide,summary generation process,graphs to guide summary generation process,0.6706708669662476
translation,66,21,model,summary generation process,via,hierarchical graph attention mechanism,summary generation process via hierarchical graph attention mechanism,0.6022497415542603
translation,66,21,model,hierarchical graph attention mechanism,takes advantage of,explicit graph structure,hierarchical graph attention mechanism takes advantage of explicit graph structure,0.6401557922363281
translation,66,21,model,explicit graph structure,to help,organize,explicit graph structure to help organize,0.6369937062263489
translation,66,21,model,organize,has,summary content,organize has summary content,0.5286022424697876
translation,66,21,model,model,has,graphs,model has graphs,0.6093022227287292
translation,66,22,model,our model,extract,salient information,our model extract salient information,0.6834123134613037
translation,66,22,model,our model,generate,coherent summaries,our model generate coherent summaries,0.6787598133087158
translation,66,22,model,salient information,from,long documents,salient information from long documents,0.5167175531387329
translation,66,22,model,graph modeling,has,our model,graph modeling has our model,0.5919289588928223
translation,66,22,model,coherent summaries,has,more effectively,coherent summaries has more effectively,0.5401104092597961
translation,66,22,model,model,Benefiting from,graph modeling,model Benefiting from graph modeling,0.672082781791687
translation,66,22,model,model,extract,salient information,model extract salient information,0.7376812100410461
translation,66,24,model,model,complementary to,most pre-trained language models ( lms ),model complementary to most pre-trained language models ( lms ),0.6204673647880554
translation,66,24,model,most pre-trained language models ( lms ),like,"bert ( devlin et al. , 2019 )","most pre-trained language models ( lms ) like bert ( devlin et al. , 2019 )",0.5446469783782959
translation,66,24,model,most pre-trained language models ( lms ),like,"xlnet ( yang et al. , 2019 b )","most pre-trained language models ( lms ) like xlnet ( yang et al. , 2019 b )",0.5602480173110962
translation,66,24,model,model,complementary to,most pre-trained language models ( lms ),model complementary to most pre-trained language models ( lms ),0.6204673647880554
translation,66,24,model,model,has,model,model has model,0.5623406171798706
translation,66,30,model,effective method,to incorporate,explicit graph representations,effective method to incorporate explicit graph representations,0.6938872337341309
translation,66,30,model,explicit graph representations,into,neural architecture,explicit graph representations into neural architecture,0.545515239238739
translation,66,30,model,effective method,to combine,pre-trained lms,effective method to combine pre-trained lms,0.6863301396369934
translation,66,30,model,pre-trained lms,with,our graph model,pre-trained lms with our graph model,0.6566265225410461
translation,66,30,model,our graph model,to process,long inputs,our graph model to process long inputs,0.7103227972984314
translation,66,30,model,long inputs,has,more effectively,long inputs has more effectively,0.5817292928695679
translation,66,30,model,model,propose,effective method,model propose effective method,0.6655846834182739
translation,66,30,model,model,propose,effective method,model propose effective method,0.6655846834182739
translation,66,134,model,similarity graph,built based on,tf-idf cosine similarities,similarity graph built based on tf-idf cosine similarities,0.6948044300079346
translation,66,134,model,tf-idf cosine similarities,between,paragraphs,tf-idf cosine similarities between paragraphs,0.6512529253959656
translation,66,134,model,paragraphs,to capture,lexical relations,paragraphs to capture lexical relations,0.6691768765449524
translation,66,134,model,model,has,similarity graph,model has similarity graph,0.5669412016868591
translation,66,23,results,three types of graph representations,including,similarity graph,three types of graph representations including similarity graph,0.659515380859375
translation,66,23,results,three types of graph representations,including,topic graph,three types of graph representations including topic graph,0.6796568632125854
translation,66,23,results,three types of graph representations,including,discourse graph,three types of graph representations including discourse graph,0.6759223937988281
translation,66,23,results,three types of graph representations,including,significantly improve,three types of graph representations including significantly improve,0.6256935596466064
translation,66,23,results,significantly improve,has,mds performance,significantly improve has mds performance,0.5673434734344482
translation,66,23,results,results,experiment with,three types of graph representations,results experiment with three types of graph representations,0.6208832859992981
translation,66,31,results,our model,brings,substantial improvements,our model brings substantial improvements,0.5985453128814697
translation,66,31,results,substantial improvements,over,several strong baselines,substantial improvements over several strong baselines,0.6738615036010742
translation,66,31,results,several strong baselines,on,wik-isum and multinews dataset,several strong baselines on wik-isum and multinews dataset,0.5226906538009644
translation,66,31,results,results,has,our model,results has our model,0.5871725678443909
translation,66,181,results,results,on,wikisum dataset,results on wikisum dataset,0.5374239683151245
translation,66,187,results,all abstractive models,has,outperform,all abstractive models has outperform,0.616695761680603
translation,66,187,results,outperform,has,extractive ones,outperform has extractive ones,0.5962715744972229
translation,66,187,results,results,show,all abstractive models,results show all abstractive models,0.6120851039886475
translation,66,188,results,our model graphsum,achieves,significant improvements,our model graphsum achieves significant improvements,0.6899463534355164
translation,66,188,results,significant improvements,on,all three metrics,significant improvements on all three metrics,0.4842647612094879
translation,66,188,results,ft,has,t-dmca and ht,ft has t-dmca and ht,0.6626347303390503
translation,66,188,results,ft,has,our model graphsum,ft has our model graphsum,0.6665065884590149
translation,66,188,results,t-dmca and ht,has,our model graphsum,t-dmca and ht has our model graphsum,0.5807576775550842
translation,66,188,results,results,Compared with,ft,results Compared with ft,0.5827099680900574
translation,66,190,results,wikisum table,develop,several strong base - lines,wikisum table develop several strong base - lines,0.6103504300117493
translation,66,190,results,several strong base - lines,which combine,flat transformer,several strong base - lines which combine flat transformer,0.7064134478569031
translation,66,190,results,flat transformer,with,pre-trained lms,flat transformer with pre-trained lms,0.6758283376693726
translation,66,190,results,results,on,wikisum table,results on wikisum table,0.5361601114273071
translation,66,190,results,results,develop,several strong base - lines,results develop several strong base - lines,0.5904170870780945
translation,66,194,results,roberta,boosts,summarization performance,roberta boosts summarization performance,0.677578330039978
translation,66,194,results,roberta,combine it with,our graphsum model,roberta combine it with our graphsum model,0.6968765258789062
translation,66,194,results,our graphsum model,namely,graphsum+ roberta,our graphsum model namely graphsum+ roberta,0.684070885181427
translation,66,194,results,summarization performance,has,most significantly,summarization performance has most significantly,0.5441272854804993
translation,66,194,results,results,combine it with,our graphsum model,results combine it with our graphsum model,0.6782844066619873
translation,66,194,results,results,has,roberta,results has roberta,0.530095100402832
translation,66,195,results,graphsum+ roberta,improves,summarization performance,graphsum+ roberta improves summarization performance,0.7002402544021606
translation,66,195,results,summarization performance,on,all metrics,summarization performance on all metrics,0.484049916267395
translation,66,195,results,results,show that,graphsum+ roberta,results show that graphsum+ roberta,0.5496799349784851
translation,66,196,results,significant improvements,over,roberta + ft,significant improvements over roberta + ft,0.7014065384864807
translation,66,196,results,graph modeling,even with,pre-trained lms,graph modeling even with pre-trained lms,0.6570104360580444
translation,66,196,results,results,has,significant improvements,results has significant improvements,0.5530977845191956
translation,66,197,results,results,on,multinews dataset,results on multinews dataset,0.5189805626869202
translation,66,201,results,our model graphsum,has,consistently outperforms,our model graphsum has consistently outperforms,0.6239349842071533
translation,66,201,results,consistently outperforms,has,all baselines,consistently outperforms has all baselines,0.5969254374504089
translation,66,201,results,results,show that,our model graphsum,results show that our model graphsum,0.5538725852966309
translation,66,202,results,performance,of,roberta +ft and graphsum + roberta,performance of roberta +ft and graphsum + roberta,0.6024002432823181
translation,66,202,results,roberta +ft and graphsum + roberta,show,our model,roberta +ft and graphsum + roberta show our model,0.6573789119720459
translation,66,202,results,our model,has,significantly improves,our model has significantly improves,0.5935119986534119
translation,66,202,results,significantly improves,has,all metrics,significantly improves has all metrics,0.5698973536491394
translation,66,202,results,results,compare,performance,results compare performance,0.7135117650032043
translation,66,203,results,wikisum and multinews dataset,validate,effectiveness of our model,wikisum and multinews dataset validate effectiveness of our model,0.6151673197746277
translation,66,204,results,proposed method,to modeling,graph,proposed method to modeling graph,0.6926304697990417
translation,66,204,results,graph,in,end-to - end neural model,graph in end-to - end neural model,0.5582485795021057
translation,66,204,results,performance,of,mds,performance of mds,0.6480542421340942
translation,66,204,results,results,has,proposed method,results has proposed method,0.5845219492912292
translation,66,210,results,topic graph,achieves,better performance,topic graph achieves better performance,0.6843709349632263
translation,66,210,results,better performance,than,similarity graph,better performance than similarity graph,0.5676657557487488
translation,66,210,results,similarity graph,on,rouge - 1 and rouge - 2,similarity graph on rouge - 1 and rouge - 2,0.5539644956588745
translation,66,210,results,discourse graph,achieves,best performance,discourse graph achieves best performance,0.7034056186676025
translation,66,210,results,best performance,on,rouge - 2 and rouge -l,best performance on rouge - 2 and rouge -l,0.5134167671203613
translation,66,228,results,graphsum and graphsum+ roberta,able to generate,higher quality summaries,graphsum and graphsum+ roberta able to generate higher quality summaries,0.7003782391548157
translation,66,228,results,higher quality summaries,than,other models,higher quality summaries than other models,0.5947375297546387
translation,66,228,results,results,demonstrate,graphsum and graphsum+ roberta,results demonstrate graphsum and graphsum+ roberta,0.6105942130088806
translation,66,229,results,summaries,generated by,graphsum and graphsum+ roberta,summaries generated by graphsum and graphsum+ roberta,0.7254988551139832
translation,66,229,results,graphsum and graphsum+ roberta,contains,more salient information,graphsum and graphsum+ roberta contains more salient information,0.6845446228981018
translation,66,229,results,more fluent and concise,than,other models,more fluent and concise than other models,0.5815516114234924
translation,66,229,results,results,has,summaries,results has summaries,0.5243220329284668
translation,67,5,model,a* search algorithm,to find,best extractive summary,a* search algorithm to find best extractive summary,0.6347399353981018
translation,67,5,model,best extractive summary,up to,given length,best extractive summary up to given length,0.5858755111694336
translation,67,5,model,model,propose,a* search algorithm,model propose a* search algorithm,0.699454665184021
translation,67,6,model,discriminative training algorithm,directly maximises,quality of the best summary,discriminative training algorithm directly maximises quality of the best summary,0.7862399220466614
translation,67,6,model,model,propose,discriminative training algorithm,model propose discriminative training algorithm,0.6601499915122986
translation,67,21,model,full prediction model,directly with,search algorithm intact,full prediction model directly with search algorithm intact,0.7020959854125977
translation,67,22,model,parameters,such that,best scoring whole summary,parameters such that best scoring whole summary,0.5912103652954102
translation,67,22,model,best scoring whole summary,under,model,best scoring whole summary under model,0.670756459236145
translation,67,22,model,best scoring whole summary,has,high score,best scoring whole summary has high score,0.5655573606491089
translation,67,22,model,high score,under,evaluation metric,high score under evaluation metric,0.5915945172309875
translation,67,22,model,model,has,high score,model has high score,0.5868687033653259
translation,67,89,results,aggregated + final heuristic,has,h 3,aggregated + final heuristic has h 3,0.60365891456604
translation,67,89,results,aggregated + final heuristic,has,consistently outperforms,aggregated + final heuristic has consistently outperforms,0.6184874176979065
translation,67,89,results,h 3,has,consistently outperforms,h 3 has consistently outperforms,0.6247532367706299
translation,67,89,results,consistently outperforms,has,other two methods,consistently outperforms has other two methods,0.5646145939826965
translation,67,89,results,results,has,aggregated + final heuristic,results has aggregated + final heuristic,0.6045722365379333
translation,67,184,results,scores,obtained with,r - su4 metric trained weights,scores obtained with r - su4 metric trained weights,0.6020409464836121
translation,67,184,results,scores,obtained using,weights,scores obtained using weights,0.6242038011550903
translation,67,184,results,r - su4 metric trained weights,achieve,higher scores,r - su4 metric trained weights achieve higher scores,0.6329320073127747
translation,67,184,results,higher scores,on,r - 1 and r - 2,higher scores on r - 1 and r - 2,0.5676987171173096
translation,67,184,results,higher scores,compared to,scores,higher scores compared to scores,0.5895517468452454
translation,67,184,results,scores,obtained using,weights,scores obtained using weights,0.6242038011550903
translation,67,184,results,results,has,scores,results has scores,0.5219217538833618
translation,67,191,results,wordlimit summaries,score,highest,wordlimit summaries score highest,0.7567315697669983
translation,67,191,results,highest,compared to,other two types of summaries,highest compared to other two types of summaries,0.6835076212882996
translation,67,201,results,regression type summaries,achieved,worst rouge metric scores,regression type summaries achieved worst rouge metric scores,0.6866751909255981
translation,67,201,results,results,has,regression type summaries,results has regression type summaries,0.5088286995887756
translation,67,204,results,summaries,obtained from,virtualtourist captions ( in domain data ),summaries obtained from virtualtourist captions ( in domain data ),0.6446419358253479
translation,67,204,results,summaries,generated using,web-documents ( out of domain data ),summaries generated using web-documents ( out of domain data ),0.6883979439735413
translation,67,204,results,virtualtourist captions ( in domain data ),score,roughly the same,virtualtourist captions ( in domain data ) score roughly the same,0.7223381400108337
translation,67,204,results,roughly the same,as,summaries,roughly the same as summaries,0.6292771697044373
translation,67,204,results,summaries,generated using,web-documents ( out of domain data ),summaries generated using web-documents ( out of domain data ),0.6883979439735413
translation,67,220,results,results,for,"clarity , coherence and focus criteria","results for clarity , coherence and focus criteria",0.5610185861587524
translation,67,220,results,"clarity , coherence and focus criteria",in,wordlimit summaries,"clarity , coherence and focus criteria in wordlimit summaries",0.5200856924057007
translation,67,220,results,"clarity , coherence and focus criteria",are,significantly better,"clarity , coherence and focus criteria are significantly better",0.5461385846138
translation,67,220,results,wordlimit summaries,are,significantly better,wordlimit summaries are significantly better,0.5752918124198914
translation,67,220,results,significantly better,than,regression ones ( p< 0.001 ),significantly better than regression ones ( p< 0.001 ),0.5601494312286377
translation,67,220,results,results,for,"clarity , coherence and focus criteria","results for clarity , coherence and focus criteria",0.5610185861587524
translation,67,220,results,results,has,results,results has results,0.48582205176353455
translation,67,223,results,scores,for,grammaticality criterion,scores for grammaticality criterion,0.5449163913726807
translation,67,223,results,grammaticality criterion,are,lower,grammaticality criterion are lower,0.5448046922683716
translation,67,223,results,lower,than,wordlimit summaries,lower than wordlimit summaries,0.6152710318565369
translation,67,223,results,regression type summaries,has,scores,regression type summaries has scores,0.5715534090995789
translation,67,223,results,wordlimit summaries,has,difference,wordlimit summaries has difference,0.6092477440834045
translation,67,223,results,results,in,regression type summaries,results in regression type summaries,0.4880119264125824
translation,68,8,experiments,textual entailment,to measure,sentence connectivity,textual entailment to measure sentence connectivity,0.6597975492477417
translation,68,8,experiments,textual entailment,construct,graph,textual entailment construct graph,0.6832234263420105
translation,68,8,experiments,graph,on,wmvc,graph on wmvc,0.6224938035011292
translation,68,7,model,text summarization task,as,optimization problem,text summarization task as optimization problem,0.4473704993724823
translation,68,7,model,weighted minimum vertex cover ( wmvc ),has,graph - based algorithm,weighted minimum vertex cover ( wmvc ) has graph - based algorithm,0.5605992674827576
translation,68,7,model,model,poses,text summarization task,model poses text summarization task,0.7165718078613281
translation,68,151,model,summarization task,as,optimization problem,summarization task as optimization problem,0.4528804421424866
translation,68,151,model,weighted minimum vertex cover algorithm,on,graph,weighted minimum vertex cover algorithm on graph,0.5183901190757751
translation,68,151,model,graph,based on,textual entailment relations,graph based on textual entailment relations,0.6099320650100708
translation,68,151,model,model,formulate,summarization task,model formulate summarization task,0.6489774584770203
translation,68,151,model,model,employ,weighted minimum vertex cover algorithm,model employ weighted minimum vertex cover algorithm,0.551122784614563
translation,69,6,model,new hierarchical encoder,models,discourse structure,new hierarchical encoder models discourse structure,0.7154683470726013
translation,69,6,model,discourse structure,of,document,discourse structure of document,0.5729039311408997
translation,69,6,model,attentive discourse - aware decoder,to generate,summary,attentive discourse - aware decoder to generate summary,0.6888941526412964
translation,69,22,model,abstractive model,for,summarizing scientific papers,abstractive model for summarizing scientific papers,0.5875390768051147
translation,69,23,model,hierarchical encoder,capturing,discourse structure,hierarchical encoder capturing discourse structure,0.7433854937553406
translation,69,23,model,discourse structure,of,document,discourse structure of document,0.5729039311408997
translation,69,23,model,discourse - aware decoder,generates,summary,discourse - aware decoder generates summary,0.6585066914558411
translation,69,23,model,model,includes,hierarchical encoder,model includes hierarchical encoder,0.6530205607414246
translation,69,23,model,model,includes,discourse - aware decoder,model includes discourse - aware decoder,0.6355038285255432
translation,69,24,model,our decoder,attends to,different discourse sections,our decoder attends to different discourse sections,0.6848604679107666
translation,69,24,model,our decoder,allows,model,our decoder allows model,0.6948040127754211
translation,69,24,model,model,to more accurately represent,important information,model to more accurately represent important information,0.7061642408370972
translation,69,24,model,important information,from,source,important information from source,0.5213828086853027
translation,69,24,model,important information,resulting in,better context vector,important information resulting in better context vector,0.6364983916282654
translation,69,24,model,model,has,our decoder,model has our decoder,0.569315493106842
translation,69,132,results,significantly outperforms,showing,effectiveness,significantly outperforms showing effectiveness,0.7401412129402161
translation,69,132,results,significantly outperforms,has,state - of - the - art abstractive methods,significantly outperforms has state - of - the - art abstractive methods,0.5731334686279297
translation,69,133,results,rouge - 1 score,is,4 and 3 points higher,rouge - 1 score is 4 and 3 points higher,0.5721087455749512
translation,69,133,results,rouge - 1 score,about,4 and 3 points higher,rouge - 1 score about 4 and 3 points higher,0.6147768497467041
translation,69,133,results,4 and 3 points higher,than,abstractive model pntr-gen-seq2seq,4 and 3 points higher than abstractive model pntr-gen-seq2seq,0.5928664803504944
translation,69,133,results,abstractive model pntr-gen-seq2seq,for,arxiv and pubmed datasets,abstractive model pntr-gen-seq2seq for arxiv and pubmed datasets,0.5826966762542725
translation,69,133,results,abstractive model pntr-gen-seq2seq,providing,significant improvement,abstractive model pntr-gen-seq2seq providing significant improvement,0.6358414888381958
translation,69,134,results,most of the extractive methods,except for,lexrank,most of the extractive methods except for lexrank,0.6569476127624512
translation,69,134,results,lexrank,in,one of the rouge scores,lexrank in one of the rouge scores,0.5296701192855835
translation,69,134,results,outperforms,has,most of the extractive methods,outperforms has most of the extractive methods,0.6074466705322266
translation,70,4,baselines,opiniondigest,has,abstractive opinion summarization framework,opiniondigest has abstractive opinion summarization framework,0.5161064863204956
translation,70,69,baselines,baselines,has,baselines lexrank,baselines has baselines lexrank,0.5341866612434387
translation,70,78,hyperparameters,opinion merging,used,pre-trained word embeddings ( glove .6b.300d ),opinion merging used pre-trained word embeddings ( glove .6b.300d ),0.5293639898300171
translation,70,78,hyperparameters,pre-trained word embeddings ( glove .6b.300d ),selected,top -k ( k = 15 ) most popular opinion clusters,pre-trained word embeddings ( glove .6b.300d ) selected top -k ( k = 15 ) most popular opinion clusters,0.5711754560470581
translation,70,78,hyperparameters,hyperparameters,For,opinion merging,hyperparameters For opinion merging,0.5773223042488098
translation,70,79,hyperparameters,transformer,with,"original architecture ( vaswani et al. , 2017 )","transformer with original architecture ( vaswani et al. , 2017 )",0.6179513335227966
translation,70,79,hyperparameters,hyperparameters,trained,transformer,hyperparameters trained transformer,0.6320951581001282
translation,70,80,hyperparameters,sgd,with,initial learning rate,sgd with initial learning rate,0.5956356525421143
translation,70,80,hyperparameters,sgd,with,momentum,sgd with momentum,0.7183332443237305
translation,70,80,hyperparameters,sgd,with,decay,sgd with decay,0.652528703212738
translation,70,80,hyperparameters,sgd,with,batch size,sgd with batch size,0.6358768343925476
translation,70,80,hyperparameters,initial learning rate,of,0.1,initial learning rate of 0.1,0.5805032253265381
translation,70,80,hyperparameters,momentum,of,? = 0.1,momentum of ? = 0.1,0.6407715082168579
translation,70,80,hyperparameters,decay,of,? = 0.1,decay of ? = 0.1,0.6398928165435791
translation,70,80,hyperparameters,decay,with,batch size,decay with batch size,0.6632699370384216
translation,70,80,hyperparameters,? = 0.1,for,5 epochs,? = 0.1 for 5 epochs,0.6821706891059875
translation,70,80,hyperparameters,? = 0.1,with,batch size,? = 0.1 with batch size,0.6523872017860413
translation,70,80,hyperparameters,5 epochs,with,batch size,5 epochs with batch size,0.618772029876709
translation,70,80,hyperparameters,batch size,of,8,batch size of 8,0.6920418739318848
translation,70,80,hyperparameters,hyperparameters,used,sgd,hyperparameters used sgd,0.6063317060470581
translation,70,81,hyperparameters,decoding,used,beam search,decoding used beam search,0.5754576921463013
translation,70,81,hyperparameters,beam search,with,beam size,beam search with beam size,0.6320963501930237
translation,70,81,hyperparameters,beam search,with,length penalty,beam search with length penalty,0.6435361504554749
translation,70,81,hyperparameters,beam search,with,3,beam search with 3,0.6815756559371948
translation,70,81,hyperparameters,beam search,with,maximum generation length,beam search with maximum generation length,0.6182403564453125
translation,70,81,hyperparameters,beam size,of,5,beam size of 5,0.7073217034339905
translation,70,81,hyperparameters,beam size,of,3,beam size of 3,0.6917811036109924
translation,70,81,hyperparameters,length penalty,of,0.6,length penalty of 0.6,0.5868892073631287
translation,70,81,hyperparameters,maximum generation length,of,60,maximum generation length of 60,0.6618744730949402
translation,70,81,hyperparameters,beam search,has,length penalty,beam search has length penalty,0.5589801073074341
translation,70,81,hyperparameters,3,has,gram blocking,3 has gram blocking,0.5676985383033752
translation,70,81,hyperparameters,hyperparameters,For,decoding,hyperparameters For decoding,0.5911394953727722
translation,70,5,model,aspect- based sentiment analysis model,to extract,opinion phrases,aspect- based sentiment analysis model to extract opinion phrases,0.6940546631813049
translation,70,5,model,aspect- based sentiment analysis model,trains,transformer model,aspect- based sentiment analysis model trains transformer model,0.7185680270195007
translation,70,5,model,opinion phrases,from,reviews,opinion phrases from reviews,0.5806528329849243
translation,70,5,model,transformer model,to reconstruct,original reviews,transformer model to reconstruct original reviews,0.6950222849845886
translation,70,6,model,summarization time,merge,extractions,summarization time merge extractions,0.7695177793502808
translation,70,6,model,summarization time,select,most popular ones,summarization time select most popular ones,0.6043796539306641
translation,70,6,model,extractions,from,multiple reviews,extractions from multiple reviews,0.533413827419281
translation,70,6,model,model,At,summarization time,model At summarization time,0.5314134359359741
translation,70,25,model,more interpretable and controllable opinion aggregation,replace,end-to - end architectures,more interpretable and controllable opinion aggregation replace end-to - end architectures,0.5715592503547668
translation,70,25,model,end-to - end architectures,with,pipeline framework,end-to - end architectures with pipeline framework,0.6511389017105103
translation,70,26,model,pre-trained opinion extractor,identifies,opinion phrases,pre-trained opinion extractor identifies opinion phrases,0.639421284198761
translation,70,26,model,opinion phrases,in,reviews,opinion phrases in reviews,0.5275474190711975
translation,70,26,model,opinion phrases,in,reviews,opinion phrases in reviews,0.5275474190711975
translation,70,26,model,reviews,from,extracted opinion phrases,reviews from extracted opinion phrases,0.5096780061721802
translation,70,26,model,simple and controllable opinion selector,"merges , ranks , and - optionally - filters",extracted opinions,"simple and controllable opinion selector merges , ranks , and - optionally - filters extracted opinions",0.7324022650718689
translation,70,26,model,generator model,trained to reconstruct,reviews,generator model trained to reconstruct reviews,0.7169113755226135
translation,70,26,model,generator model,generate,opinion summaries,generator model generate opinion summaries,0.7026358246803284
translation,70,26,model,reviews,from,extracted opinion phrases,reviews from extracted opinion phrases,0.5096780061721802
translation,70,26,model,opinion summaries,based on,selected opinions,opinion summaries based on selected opinions,0.6593551635742188
translation,70,70,model,sentences,based on,centrality scores,sentences based on centrality scores,0.5979856848716736
translation,70,70,model,centrality scores,calculated on,graph - based sentence similarity,centrality scores calculated on graph - based sentence similarity,0.6583899259567261
translation,70,70,model,model,selects,sentences,model selects sentences,0.766882061958313
translation,70,87,results,our framework,has,outperforms,our framework has outperforms,0.6379533410072327
translation,70,87,results,outperforms,has,all baseline approaches,outperforms has all baseline approaches,0.5795095562934875
translation,70,87,results,results,has,our framework,results has our framework,0.6097875237464905
translation,70,94,results,opiniondigest,produced,significantly more sentences,opiniondigest produced significantly more sentences,0.658849835395813
translation,70,94,results,opiniondigest,produced,fewer sentences,opiniondigest produced fewer sentences,0.6568509340286255
translation,70,94,results,significantly more sentences,with,full or partial support,significantly more sentences with full or partial support,0.5621799826622009
translation,70,94,results,fewer sentences,without,any support,fewer sentences without any support,0.7153757214546204
translation,70,94,results,results,has,opiniondigest,results has opiniondigest,0.5660804510116577
translation,71,129,ablation-analysis,ablation analysis,effectiveness of,regularization,ablation analysis effectiveness of regularization,0.7280940413475037
translation,71,98,baselines,icsi,is,global linear optimization approach,icsi is global linear optimization approach,0.5977296829223633
translation,71,98,baselines,global linear optimization approach,extracts,summary,global linear optimization approach extracts summary,0.6523862481117249
translation,71,98,baselines,summary,by solving,maximum coverage problem,summary by solving maximum coverage problem,0.6449405550956726
translation,71,98,baselines,maximum coverage problem,considering,most frequent bigrams,maximum coverage problem considering most frequent bigrams,0.6598184704780579
translation,71,98,baselines,most frequent bigrams,in,source documents,most frequent bigrams in source documents,0.4582737982273102
translation,71,98,baselines,baselines,has,icsi,baselines has icsi,0.5849870443344116
translation,71,98,baselines,baselines,has,baselines,baselines has baselines,0.6036415696144104
translation,71,98,baselines,baselines,has,icsi,baselines has icsi,0.5849870443344116
translation,71,99,baselines,sfour,is,supervised structured prediction approach,sfour is supervised structured prediction approach,0.5582137703895569
translation,71,99,baselines,supervised structured prediction approach,trains,end-to -end,supervised structured prediction approach trains end-to -end,0.710587739944458
translation,71,99,baselines,end-to -end,on,convex relaxation,end-to -end on convex relaxation,0.5146728157997131
translation,71,99,baselines,convex relaxation,of,"rouge ( sipos et al. , 2012 )","convex relaxation of rouge ( sipos et al. , 2012 )",0.5129711031913757
translation,71,99,baselines,baselines,has,icsi,baselines has icsi,0.5849870443344116
translation,71,5,model,summarylevel scoring function,including,human judgments,summarylevel scoring function including human judgments,0.6637625694274902
translation,71,5,model,summarylevel scoring function,including,automatically generated data,summarylevel scoring function including automatically generated data,0.6977752447128296
translation,71,5,model,human judgments,as,supervision,human judgments as supervision,0.5330771207809448
translation,71,5,model,automatically generated data,as,regularization,automatically generated data as regularization,0.5085470080375671
translation,71,5,model,model,learn,summarylevel scoring function,model learn summarylevel scoring function,0.6411995887756348
translation,71,6,model,summaries,with,genetic algorithm,summaries with genetic algorithm,0.6518499255180359
translation,71,6,model,summaries,using,?,summaries using ?,0.7361563444137573
translation,71,6,model,summaries,as,fitness function,summaries as fitness function,0.5626311302185059
translation,71,6,model,genetic algorithm,using,?,genetic algorithm using ?,0.6558673977851868
translation,71,6,model,genetic algorithm,using,fitness function,genetic algorithm using fitness function,0.6694364547729492
translation,71,6,model,?,as,fitness function,? as fitness function,0.5481772422790527
translation,71,6,model,model,extract,summaries,model extract summaries,0.7345701456069946
translation,71,19,model,objective function,at,summary - level,objective function at summary - level,0.5296352505683899
translation,71,19,model,summary - level,from,pool of manually annotated system summaries,summary - level from pool of manually annotated system summaries,0.5374804139137268
translation,71,19,model,pool of manually annotated system summaries,to ensure,extraction,pool of manually annotated system summaries to ensure extraction,0.6439437866210938
translation,71,19,model,of summaries,considered good by,humans,of summaries considered good by humans,0.7593903541564941
translation,71,19,model,extraction,has,of summaries,extraction has of summaries,0.5652749538421631
translation,71,19,model,model,propose to learn,objective function,model propose to learn objective function,0.626635730266571
translation,71,130,results,"r2 , gen )",performs,on par,"r2 , gen ) performs on par",0.6041194200515747
translation,71,130,results,"r2 , gen )",is,consistently and often significantly better,"r2 , gen ) is consistently and often significantly better",0.5815011262893677
translation,71,130,results,on par,with,other supervised baseline sfour,on par with other supervised baseline sfour,0.6470188498497009
translation,71,130,results,consistently and often significantly better,than,baselines,consistently and often significantly better than baselines,0.5886288285255432
translation,71,130,results,consistently and often significantly better,across,datasets,consistently and often significantly better across datasets,0.7273368239402771
translation,71,130,results,baselines,across,datasets,baselines across datasets,0.7018778920173645
translation,71,130,results,results,note,"r2 , gen )","results note r2 , gen )",0.6102803945541382
translation,71,132,results,manual inspection,of,summaries,manual inspection of summaries,0.6153389811515808
translation,71,132,results,summaries,reveals,"( ? , gen )","summaries reveals ( ? , gen )",0.7351495623588562
translation,71,132,results,summaries,reveals,lower redundancy,summaries reveals lower redundancy,0.7718271613121033
translation,71,132,results,lower redundancy,than,previous baselines,lower redundancy than previous baselines,0.5829994678497314
translation,71,132,results,"( ? , gen )",has,lower redundancy,"( ? , gen ) has lower redundancy",0.5958887338638306
translation,71,132,results,results,has,manual inspection,results has manual inspection,0.5435076355934143
translation,72,8,model,machine translation metric,to measure,content coverage,machine translation metric to measure content coverage,0.6073023080825806
translation,72,8,model,machine translation metric,apply,enhanced discourse coherence model,machine translation metric apply enhanced discourse coherence model,0.5392881631851196
translation,72,8,model,enhanced discourse coherence model,to evaluate,summary readability,enhanced discourse coherence model to evaluate summary readability,0.6513347625732422
translation,72,8,model,enhanced discourse coherence model,combine both in,trained regression model,enhanced discourse coherence model combine both in trained regression model,0.5960622429847717
translation,72,8,model,trained regression model,to evaluate,overall responsiveness,trained regression model to evaluate overall responsiveness,0.6656529307365417
translation,72,8,model,model,adapt,machine translation metric,model adapt machine translation metric,0.6719782948493958
translation,72,26,results,all aesop 2011 submissions,on,initial and update tasks,all aesop 2011 submissions on initial and update tasks,0.50288987159729
translation,72,26,results,all aesop 2011 submissions,on,initial task,all aesop 2011 submissions on initial task,0.5255931615829468
translation,72,26,results,all aesop 2011 submissions,on,initial task,all aesop 2011 submissions on initial task,0.5255931615829468
translation,72,26,results,all submissions,on,initial task,all submissions on initial task,0.5167216062545776
translation,72,26,results,coherence model,has,significantly outperforms,coherence model has significantly outperforms,0.618694007396698
translation,72,26,results,significantly outperforms,has,all aesop 2011 submissions,significantly outperforms has all aesop 2011 submissions,0.5664258003234863
translation,72,26,results,significantly outperform,has,all submissions,significantly outperform has all submissions,0.5458365082740784
translation,72,101,results,tesla -s,ranks,"second , first , and second","tesla -s ranks second , first , and second",0.6735800504684448
translation,72,101,results,"second , first , and second",on,"pearson 's r , spearman's ? , and kendall's ?","second , first , and second on pearson 's r , spearman's ? , and kendall's ?",0.45186394453048706
translation,72,101,results,update task,has,tesla -s,update task has tesla -s,0.6047715544700623
translation,72,101,results,results,On,update task,results On update task,0.5894156098365784
translation,72,146,results,outperforms,on,all correlations,outperforms on all correlations,0.522955596446991
translation,72,146,results,all metrics,on,all correlations,all metrics on all correlations,0.518880307674408
translation,72,146,results,lin,has,outperforms,lin has outperforms,0.637789785861969
translation,72,146,results,outperforms,has,all metrics,outperforms has all metrics,0.589000403881073
translation,72,146,results,results,has,lin,results has lin,0.5584407448768616
translation,72,147,results,best scores,by,"3.62 % , 16.20 % , and 12.95 %","best scores by 3.62 % , 16.20 % , and 12.95 %",0.5598733425140381
translation,72,147,results,"3.62 % , 16.20 % , and 12.95 %",on,"pearson , spearman , and kendall","3.62 % , 16.20 % , and 12.95 % on pearson , spearman , and kendall",0.4962714910507202
translation,72,147,results,outperforms,has,best scores,outperforms has best scores,0.6284096837043762
translation,72,147,results,results,On,initial task,results On initial task,0.53757244348526
translation,72,149,results,results,are,much better,results are much better,0.5832657217979431
translation,72,149,results,much better,on,spearman and kendall,much better on spearman and kendall,0.5404505729675293
translation,72,149,results,results,are,much better,results are much better,0.5832657217979431
translation,72,149,results,results,has,results,results has results,0.48582205176353455
translation,72,151,results,explicit / non-explicit,giving,more pronounced improvements,explicit / non-explicit giving more pronounced improvements,0.6677982211112976
translation,72,151,results,intra-cell or explicit / non-explicit features,has,improves,intra-cell or explicit / non-explicit features has improves,0.6047877669334412
translation,72,151,results,improves,has,all correlation scores,improves has all correlation scores,0.57464998960495
translation,72,151,results,results,Adding,intra-cell or explicit / non-explicit features,results Adding intra-cell or explicit / non-explicit features,0.6385176777839661
translation,72,152,results,both new feature sources,are,in - corporated,both new feature sources are in - corporated,0.6638374924659729
translation,72,152,results,both new feature sources,obtain,best results,both new feature sources obtain best results,0.5724864602088928
translation,72,152,results,in - corporated,into,metric,in - corporated into metric,0.6464334726333618
translation,72,152,results,best results,for,all correlation scores,best results for all correlation scores,0.5943740606307983
translation,72,152,results,dicomer,outperforms,lin,dicomer outperforms lin,0.7677079439163208
translation,72,152,results,lin,by,"1.10 % , 5.29 % , and 3.95 %","lin by 1.10 % , 5.29 % , and 3.95 %",0.5792444944381714
translation,72,152,results,lin,by,"2.50 % , 4.74 % , and 4.27 %","lin by 2.50 % , 4.74 % , and 4.27 %",0.6068794131278992
translation,72,152,results,"1.10 % , 5.29 % , and 3.95 %",on,initial task,"1.10 % , 5.29 % , and 3.95 % on initial task",0.5332149267196655
translation,72,152,results,"2.50 % , 4.74 % , and 4.27 %",on,update task,"2.50 % , 4.74 % , and 4.27 % on update task",0.5608633756637573
translation,72,152,results,results,When,both new feature sources,results When both new feature sources,0.6258906722068787
translation,72,153,results,summarization evaluation metric,tops,all other aesop metrics,summarization evaluation metric tops all other aesop metrics,0.6275026202201843
translation,72,155,results,koehn 's significance test,demonstrates,all four models,koehn 's significance test demonstrates all four models,0.597410261631012
translation,72,155,results,all four models,has,outperform,all four models has outperform,0.6079422831535339
translation,72,155,results,outperform,has,metric 4,outperform has metric 4,0.6755545139312744
translation,72,155,results,results,results of,koehn 's significance test,results results of koehn 's significance test,0.7695639133453369
translation,72,169,results,pf,performs,better,pf performs better,0.6684013605117798
translation,72,169,results,better,than,lf,better than lf,0.6451597213745117
translation,72,169,results,results,has,pf,results has pf,0.443032443523407
translation,72,170,results,rbf,gives,better performances,rbf gives better performances,0.618471086025238
translation,72,170,results,better performances,than,lf and pf,better performances than lf and pf,0.5835676789283752
translation,72,170,results,results,has,rbf,results has rbf,0.42545264959335327
translation,72,171,results,model,trained with,rbf,model trained with rbf,0.7782759666442871
translation,72,171,results,initial task,has,model,initial task has model,0.5448800325393677
translation,72,171,results,rbf,has,outperforms,rbf has outperforms,0.6364566683769226
translation,72,171,results,outperforms,has,all submitted metrics,outperforms has all submitted metrics,0.5692882537841797
translation,72,171,results,results,On,initial task,results On initial task,0.53757244348526
translation,72,172,results,best correlation scores,by,"1.71 % , 3.86 % , and 4.60 %","best correlation scores by 1.71 % , 3.86 % , and 4.60 %",0.5612626075744629
translation,72,172,results,"1.71 % , 3.86 % , and 4.60 %",on,"pearson , spearman , and kendall","1.71 % , 3.86 % , and 4.60 % on pearson , spearman , and kendall",0.48849257826805115
translation,72,172,results,outperforms,has,best correlation scores,outperforms has best correlation scores,0.5886225700378418
translation,72,172,results,results,has,outperforms,results has outperforms,0.6657275557518005
translation,72,173,results,all three regression models,do not perform,as well,all three regression models do not perform as well,0.6690323352813721
translation,72,173,results,as well,on,update task,as well on update task,0.6342856287956238
translation,72,173,results,results,has,all three regression models,results has all three regression models,0.4675905108451843
translation,72,174,results,rouge - 2 and rouge - su4,on,initial task,rouge - 2 and rouge - su4 on initial task,0.5726372599601746
translation,72,174,results,rouge - 2 and rouge - su4,at,significance level,rouge - 2 and rouge - su4 at significance level,0.5260358452796936
translation,72,174,results,initial task,at,significance level,initial task at significance level,0.48700085282325745
translation,72,174,results,significance level,of,99 %,significance level of 99 %,0.5507299900054932
translation,72,174,results,99 %,for,all three correlation measures,99 % for all three correlation measures,0.5957289934158325
translation,72,174,results,rbf,has,cremer,rbf has cremer,0.6085667610168457
translation,72,174,results,cremer,has,outperforms,cremer has outperforms,0.6582962274551392
translation,72,174,results,outperforms,has,rouge - 2 and rouge - su4,outperforms has rouge - 2 and rouge - su4,0.6156418323516846
translation,72,174,results,results,has,koehn 's significance test,results has koehn 's significance test,0.4973965585231781
translation,72,187,results,correlate better,on,initial task,correlate better on initial task,0.5762717127799988
translation,72,187,results,correlate better,on,update task,correlate better on update task,0.5602033734321594
translation,72,187,results,initial task,than on,update task,initial task than on update task,0.5789886713027954
translation,73,108,baselines,textrank,runs,"modified version of pagerank ( brin and page , 1998 )","textrank runs modified version of pagerank ( brin and page , 1998 )",0.6156502962112427
translation,73,108,baselines,"modified version of pagerank ( brin and page , 1998 )",on,weighted graph,"modified version of pagerank ( brin and page , 1998 ) on weighted graph",0.502609372138977
translation,73,108,baselines,weighted graph,consisting of,nodes,weighted graph consisting of nodes,0.7559241652488708
translation,73,108,baselines,weighted graph,consisting of,edges,weighted graph consisting of edges,0.7418566346168518
translation,73,108,baselines,nodes,as,sentences,nodes as sentences,0.5302060842514038
translation,73,108,baselines,similarities,between,sentences,similarities between sentences,0.7269238829612732
translation,73,108,baselines,baselines,has,textrank,baselines has textrank,0.5301661491394043
translation,73,109,baselines,baselines,has,maximal marginal relevance ( mmr ),baselines has maximal marginal relevance ( mmr ),0.5532195568084717
translation,73,113,baselines,pointer generator-mmr,uses,mmr,pointer generator-mmr uses mmr,0.6105159521102905
translation,73,113,baselines,pg -mmr,uses,mmr,pg -mmr uses mmr,0.5976209044456482
translation,73,113,baselines,mmr,along with,pg,mmr along with pg,0.6740481853485107
translation,73,113,baselines,pg,for,better coverage,pg for better coverage,0.6268646121025085
translation,73,113,baselines,pg,for,redundancy mitigation,pg for redundancy mitigation,0.6237519383430481
translation,73,113,baselines,baselines,has,pointer generator-mmr,baselines has pointer generator-mmr,0.5634817481040955
translation,73,114,baselines,hi-map,has,hierarchical mmr -attention,hi-map has hierarchical mmr -attention,0.6066983342170715
translation,73,114,baselines,hierarchical mmr -attention,has,pg model,hierarchical mmr -attention has pg model,0.5646644234657288
translation,73,114,baselines,baselines,has,hi-map,baselines has hi-map,0.5614745616912842
translation,73,38,experiments,mmr,has,"carbinell and goldstein , 2017 )","mmr has carbinell and goldstein , 2017 )",0.555178165435791
translation,73,38,experiments,pg,has,"see et al. , 2017 )","pg has see et al. , 2017 )",0.6023714542388916
translation,73,38,experiments,copy-transformer,has,"gehrmann et al. , 2018 )","copy-transformer has gehrmann et al. , 2018 )",0.5374853014945984
translation,73,112,model,pointer generator ( pg ) network,is,sequence - to-sequence summarization model,pointer generator ( pg ) network is sequence - to-sequence summarization model,0.5585157871246338
translation,73,112,model,sequence - to-sequence summarization model,copying,words,sequence - to-sequence summarization model copying words,0.716016948223114
translation,73,112,model,sequence - to-sequence summarization model,generating,words,sequence - to-sequence summarization model generating words,0.6592168211936951
translation,73,112,model,words,from,source,words from source,0.6029993891716003
translation,73,112,model,words,from,fixed vocabulary,words from fixed vocabulary,0.6041096448898315
translation,73,112,model,words,by,pointing,words by pointing,0.5681892037391663
translation,73,112,model,words,from,fixed vocabulary,words from fixed vocabulary,0.6041096448898315
translation,73,112,model,source,by,pointing,source by pointing,0.5580665469169617
translation,73,112,model,words,from,fixed vocabulary,words from fixed vocabulary,0.6041096448898315
translation,73,112,model,model,has,pointer generator ( pg ) network,model has pointer generator ( pg ) network,0.5986745953559875
translation,73,131,results,copytransformer,which achieves,state - of - the - art performance,copytransformer which achieves state - of - the - art performance,0.6830371618270874
translation,73,131,results,state - of - the - art performance,on,multinews,state - of - the - art performance on multinews,0.5498304963111877
translation,73,131,results,10 points less,than,best system,10 points less than best system,0.6237975358963013
translation,73,131,results,best system,on,duc,best system on duc,0.5595880746841431
translation,73,131,results,results,see that,copytransformer,results see that copytransformer,0.6855465173721313
translation,73,132,results,lexrank,achieves,almost 12 points less,lexrank achieves almost 12 points less,0.6814922094345093
translation,73,132,results,almost 12 points less,than,best system,almost 12 points less than best system,0.607593297958374
translation,73,132,results,best system,on,tac,best system on tac,0.6053988933563232
translation,73,132,results,results,has,lexrank,results has lexrank,0.5527518391609192
translation,74,191,baselines,ngram + liwc,amazingly performs,as well,ngram + liwc amazingly performs as well,0.728783130645752
translation,74,191,baselines,as well,as,umbc,as well as umbc,0.6592910885810852
translation,74,191,baselines,sentence alignment,model of,negation,sentence alignment model of negation,0.6588559746742249
translation,74,189,results,liwc + ngram,significantly different than,ngram alone ( p < 0.01 ),liwc + ngram significantly different than ngram alone ( p < 0.01 ),0.6621856689453125
translation,74,189,results,liwc + ngram,significantly different than,ngram alone,liwc + ngram significantly different than ngram alone,0.5989271402359009
translation,74,189,results,rouge + ngram,significantly different than,ngram alone,rouge + ngram significantly different than ngram alone,0.5920490622520447
translation,74,189,results,results,has,liwc + ngram,results has liwc + ngram,0.5300952792167664
translation,74,194,results,umbc,improves,correlation,umbc improves correlation,0.6724823117256165
translation,74,194,results,correlation,from,umbc baseline,correlation from umbc baseline,0.5698544979095459
translation,74,194,results,umbc baseline,of,0.46 to 0.54,umbc baseline of 0.46 to 0.54,0.5354025959968567
translation,75,25,baselines,single model,for,learning and inference,single model for learning and inference,0.6158754229545593
translation,75,25,baselines,supmmd,has,single model,supmmd has single model,0.6255859136581421
translation,75,25,baselines,baselines,has,supmmd,baselines has supmmd,0.6081500053405762
translation,75,172,baselines,supervised variants,use,2 regularized log-linear model,supervised variants use 2 regularized log-linear model,0.5961756706237793
translation,75,172,baselines,2 regularized log-linear model,trained using,oracles ( ?5.4 ),2 regularized log-linear model trained using oracles ( ?5.4 ),0.610414445400238
translation,75,172,baselines,oracles ( ?5.4 ),as,ground truth,oracles ( ?5.4 ) as ground truth,0.4731816351413727
translation,75,172,baselines,importance,has,),importance has ),0.6332573294639587
translation,75,188,baselines,baselines duc -2004,select,top performing methods,baselines duc -2004 select top performing methods,0.5788247585296631
translation,75,188,baselines,top performing methods,from,recent benchmark paper,top performing methods from recent benchmark paper,0.5197341442108154
translation,75,188,baselines,top performing methods,to serve,baselines,top performing methods to serve baselines,0.5687036514282227
translation,75,188,baselines,top performing methods,report,rouge scores,top performing methods report rouge scores,0.5552216172218323
translation,75,188,baselines,rouge scores,from,benchmark paper,rouge scores from benchmark paper,0.5271738767623901
translation,75,188,baselines,baselines,has,baselines duc -2004,baselines has baselines duc -2004,0.5794155597686768
translation,75,189,baselines,icsi,maximizes,coverage,icsi maximizes coverage,0.7845771312713623
translation,75,189,baselines,integer linear programming method,maximizes,coverage,integer linear programming method maximizes coverage,0.7529503107070923
translation,75,189,baselines,determinantal point process method,learns,sentence quality,determinantal point process method learns sentence quality,0.6160635948181152
translation,75,189,baselines,submodular,based on,learned mixture of submodular functions,submodular based on learned mixture of submodular functions,0.6646239161491394
translation,75,189,baselines,occams_v,base on,topic modeling,occams_v base on topic modeling,0.7312186360359192
translation,75,189,baselines,regsum,focuses on learning,word importance,regsum focuses on learning word importance,0.7131153345108032
translation,75,189,baselines,icsi,has,integer linear programming method,icsi has integer linear programming method,0.5499720573425293
translation,75,189,baselines,dpp,has,determinantal point process method,dpp has determinantal point process method,0.5050272941589355
translation,75,189,baselines,lexrank,has,popular graph based sentence scoring method,lexrank has popular graph based sentence scoring method,0.5134304761886597
translation,75,190,baselines,dppsim,extension to,dpp model,dppsim extension to dpp model,0.667097270488739
translation,75,190,baselines,dpp model,learns,sentence -sentence similarity,dpp model learns sentence -sentence similarity,0.7338399887084961
translation,75,190,baselines,sentence -sentence similarity,using,capsule network,sentence -sentence similarity using capsule network,0.6425215005874634
translation,75,190,baselines,sentence -sentence similarity,using,himap,sentence -sentence similarity using himap,0.68166583776474
translation,75,190,baselines,recurrent neural model,employs,modified pointer - generator component,recurrent neural model employs modified pointer - generator component,0.5870020389556885
translation,75,190,baselines,graph convolution network,combined with,recurrent neural network,graph convolution network combined with recurrent neural network,0.6638168096542358
translation,75,190,baselines,recurrent neural network,to learn,sentence saliency,recurrent neural network to learn sentence saliency,0.6101821064949036
translation,75,190,baselines,himap,has,recurrent neural model,himap has recurrent neural model,0.5617586970329285
translation,75,194,baselines,sys.34,uses,integer linear programming,sys.34 uses integer linear programming,0.5910236835479736
translation,75,194,baselines,sys.34,uses,sentence compression,sys.34 uses sentence compression,0.6160882711410522
translation,75,194,baselines,integer linear programming,to maximize,coverage,integer linear programming to maximize coverage,0.6967461109161377
translation,75,194,baselines,coverage,of,concepts,coverage of concepts,0.6090643405914307
translation,75,194,baselines,sys.40,uses,sentence compression,sys.40 uses sentence compression,0.6094783544540405
translation,75,194,baselines,sentence compression,to generate,new candidate sentences,sentence compression to generate new candidate sentences,0.6664401292800903
translation,75,194,baselines,iit,uses,support vector regressor,iit uses support vector regressor,0.6023256182670593
translation,75,194,baselines,support vector regressor,to predict,sentence rouge scores,support vector regressor to predict sentence rouge scores,0.6832799315452576
translation,75,194,baselines,ictcas,has,temporal content filtering method,ictcas has temporal content filtering method,0.5546673536300659
translation,75,194,baselines,icl,has,manifold ranking based method,icl has manifold ranking based method,0.5740254521369934
translation,75,194,baselines,baselines,has,sys.34,baselines has sys.34,0.611009955406189
translation,75,227,experiments,oracle extraction method,achieving,good performance,oracle extraction method achieving good performance,0.6617873311042786
translation,75,227,experiments,good performance,in,rouge - 2 and rouge - su4,good performance in rouge - 2 and rouge - su4,0.5434744358062744
translation,75,173,hyperparameters,number of training epochs,using,5 - fold cross validation,number of training epochs using 5 - fold cross validation,0.6488870978355408
translation,75,173,hyperparameters,hyperparameters,selected,number of training epochs,hyperparameters selected number of training epochs,0.5637000799179077
translation,75,6,model,generic and update summarization,based on,maximum mean discrepancy,generic and update summarization based on maximum mean discrepancy,0.5927086472511292
translation,75,6,model,maximum mean discrepancy,from,kernel two -sample testing,maximum mean discrepancy from kernel two -sample testing,0.5582769513130188
translation,75,6,model,model,present,supmmd,model present supmmd,0.6679542064666748
translation,75,7,model,supmmd,combines,supervised learning,supmmd combines supervised learning,0.7378957867622375
translation,75,7,model,supmmd,combines,unsupervised learning,supmmd combines unsupervised learning,0.7285363078117371
translation,75,7,model,supervised learning,for,salience,supervised learning for salience,0.6555556058883667
translation,75,7,model,supervised learning,for,coverage and diversity,supervised learning for coverage and diversity,0.6094788908958435
translation,75,7,model,unsupervised learning,for,coverage and diversity,unsupervised learning for coverage and diversity,0.6030567288398743
translation,75,7,model,model,has,supmmd,model has supmmd,0.6418296694755554
translation,75,8,model,multiple kernel learning,to make use of,similarity,multiple kernel learning to make use of similarity,0.6560388207435608
translation,75,8,model,similarity,across,multiple information sources,similarity across multiple information sources,0.68987637758255
translation,75,8,model,multiple information sources,e.g.,text features and knowledge based concepts ),multiple information sources e.g. text features and knowledge based concepts ),0.7014238834381104
translation,75,8,model,model,adapt,multiple kernel learning,model adapt multiple kernel learning,0.7056617140769958
translation,75,23,model,supmmd,applied to,generic and comparative summarization,supmmd applied to generic and comparative summarization,0.716025710105896
translation,75,23,model,single model,of learning,salience and inference,single model of learning salience and inference,0.6667752861976624
translation,75,23,model,model,propose,supmmd,model propose supmmd,0.6876458525657654
translation,75,24,model,novel technique,for,generic and update summarization,novel technique for generic and update summarization,0.624752402305603
translation,75,24,model,supervised learning,for,salience,supervised learning for salience,0.6555556058883667
translation,75,24,model,supervised learning,for,coverage and diversity,supervised learning for coverage and diversity,0.6094788908958435
translation,75,24,model,unsupervised learning,for,coverage and diversity,unsupervised learning for coverage and diversity,0.6030567288398743
translation,75,24,model,supmmd,has,novel technique,supmmd has novel technique,0.666336715221405
translation,75,24,model,model,present,supmmd,model present supmmd,0.6679542064666748
translation,75,26,model,multiple kernel learning,into,our model,multiple kernel learning into our model,0.6034494042396545
translation,75,26,model,our model,allows,similarity,our model allows similarity,0.6998377442359924
translation,75,26,model,similarity,across,multiple information sources,similarity across multiple information sources,0.68987637758255
translation,75,26,model,model,adapt,multiple kernel learning,model adapt multiple kernel learning,0.7056617140769958
translation,75,65,model,sentence importance,into,mmd,sentence importance into mmd,0.5749632120132446
translation,75,101,model,two stage kernel learning,where,different kernels,two stage kernel learning where different kernels,0.5733866095542908
translation,75,101,model,linearly combined,to maximize,alignment,linearly combined to maximize alignment,0.7165676355361938
translation,75,101,model,alignment,with,target kernel,alignment with target kernel,0.6342095732688904
translation,75,101,model,target kernel,of,classification problem,target kernel of classification problem,0.5470925569534302
translation,75,101,model,model,adapt,two stage kernel learning,model adapt two stage kernel learning,0.7395649552345276
translation,75,132,model,text features,used to compute,sentence -sentence similarity,text features used to compute sentence -sentence similarity,0.6357396841049194
translation,75,132,model,surface features,used in learning,sentence importance model,surface features used in learning sentence importance model,0.6982795596122742
translation,75,27,results,supmmd,meets or exceeds,state - of - the - art,supmmd meets or exceeds state - of - the - art,0.3909272253513336
translation,75,27,results,state - of - the - art,in,generic and update summarization,state - of - the - art in generic and update summarization,0.5070852041244507
translation,75,27,results,generic and update summarization,on,duc - 2004 and tac - 2009 datasets,generic and update summarization on duc - 2004 and tac - 2009 datasets,0.49726882576942444
translation,75,27,results,results,show that,supmmd,results show that supmmd,0.5022352933883667
translation,75,196,results,results,on,"duc -2004 , tac - 2009 -a and tac - 2009","results on duc -2004 , tac - 2009 -a and tac - 2009",0.5336470603942871
translation,75,201,results,supmmd + mkl + compress,presents,result,supmmd + mkl + compress presents result,0.702088475227356
translation,75,201,results,result,of applying sentence compression,our model,result of applying sentence compression our model,0.732704758644104
translation,75,201,results,results,has,supmmd + mkl + compress,results has supmmd + mkl + compress,0.588624894618988
translation,75,207,results,strong correlation,between,score,strong correlation between score,0.6549239754676819
translation,75,207,results,score,given by,supmmd,score given by supmmd,0.6582392454147339
translation,75,207,results,results,observe,strong correlation,results observe strong correlation,0.6323540806770325
translation,75,212,results,our oracle extraction technique,for transforming,abstractive training data,our oracle extraction technique for transforming abstractive training data,0.6994185447692871
translation,75,212,results,abstractive training data,to,extractive training data,abstractive training data to extractive training data,0.5611201524734497
translation,75,212,results,extractive training data,helps,supmmd methods,extractive training data helps supmmd methods,0.5986855030059814
translation,75,212,results,supmmd methods,achieve,higher rouge performance,supmmd methods achieve higher rouge performance,0.6230559349060059
translation,75,212,results,results,has,our oracle extraction technique,results has our oracle extraction technique,0.5693835616111755
translation,75,217,results,multiple kernels,helps,performance,multiple kernels helps performance,0.6776255965232849
translation,75,217,results,performance,of,supmmd models,performance of supmmd models,0.6388188600540161
translation,75,217,results,supmmd models,on,generic summarization task,supmmd models on generic summarization task,0.5298710465431213
translation,75,217,results,results,combining,multiple kernels,results combining multiple kernels,0.6953262090682983
translation,75,218,results,supmmd + mkl,combines,bigram and entity kernels,supmmd + mkl combines bigram and entity kernels,0.7516506910324097
translation,75,218,results,supmmd + mkl,uses,bigrams kernel,supmmd + mkl uses bigrams kernel,0.6085244417190552
translation,75,218,results,supmmd + mkl,scores,10.31,supmmd + mkl scores 10.31,0.6899701952934265
translation,75,218,results,bigram and entity kernels,has,rouge - 2 of 10.54,bigram and entity kernels has rouge - 2 of 10.54,0.5928274393081665
translation,75,218,results,rouge - 2 of 10.54,on,duc - 2004,rouge - 2 of 10.54 on duc - 2004,0.5712668895721436
translation,75,218,results,supmmd,uses,bigrams kernel,supmmd uses bigrams kernel,0.6146095991134644
translation,75,218,results,supmmd,scores,10.31,supmmd scores 10.31,0.6282078623771667
translation,75,218,results,10.31,in,rouge - 2,10.31 in rouge - 2,0.592881977558136
translation,75,218,results,bigram and entity kernels,has,rouge - 2 of 10.54,bigram and entity kernels has rouge - 2 of 10.54,0.5928274393081665
translation,75,218,results,results,has,supmmd + mkl,results has supmmd + mkl,0.5743940472602844
translation,75,219,results,multiple kernels,show,even clearer gains,multiple kernels show even clearer gains,0.6545823216438293
translation,75,219,results,even clearer gains,in,tac - 2009 - a dataset,even clearer gains in tac - 2009 - a dataset,0.5667440891265869
translation,75,219,results,results,has,multiple kernels,results has multiple kernels,0.5417686700820923
translation,75,220,results,sentence compression,incorporated into,post-processing steps,sentence compression incorporated into post-processing steps,0.6656515598297119
translation,75,220,results,post-processing steps,of,supmmd + mkl + compress,post-processing steps of supmmd + mkl + compress,0.5874966382980347
translation,75,220,results,results,over,supmmd + mkl,results over supmmd + mkl,0.6131615042686462
translation,75,220,results,not clearly improve,has,results,not clearly improve has results,0.5851419568061829
translation,75,220,results,results,over,supmmd + mkl,results over supmmd + mkl,0.6131615042686462
translation,75,220,results,results,has,sentence compression,results has sentence compression,0.5531946420669556
translation,75,221,results,compression,reduces,performance,compression reduces performance,0.6476296782493591
translation,75,221,results,compression,on,duc - 2004,compression on duc - 2004,0.5996928215026855
translation,75,221,results,lower rouge - 2 score,than,supmmd + mkl,lower rouge - 2 score than supmmd + mkl,0.5905966758728027
translation,75,221,results,tac - 2009 -a,has,compression,tac - 2009 -a has compression,0.6511278748512268
translation,75,221,results,duc - 2004,has,supmmd + mkl + compress,duc - 2004 has supmmd + mkl + compress,0.6274691224098206
translation,75,221,results,supmmd + mkl + compress,has,higher rouge - 1 score,supmmd + mkl + compress has higher rouge - 1 score,0.584226131439209
translation,75,221,results,results,On,tac - 2009 -a,results On tac - 2009 -a,0.5377381443977356
translation,75,224,results,comparative summarization task,on,tac - 2009 - b dataset,comparative summarization task on tac - 2009 - b dataset,0.5011626482009888
translation,75,224,results,state- of- theart baseline icsi,in,rouge - su4,state- of- theart baseline icsi in rouge - su4,0.547336995601654
translation,75,224,results,fall short,in,rouge -2,fall short in rouge -2,0.6066884994506836
translation,75,224,results,outperform,has,state- of- theart baseline icsi,outperform has state- of- theart baseline icsi,0.6031955480575562
translation,75,224,results,results,for,comparative summarization task,results for comparative summarization task,0.5605728626251221
translation,75,229,results,multiple kernels,as in,supmmd + mkl,multiple kernels as in supmmd + mkl,0.6384974718093872
translation,75,229,results,rouge - 2 score,to,10.24,rouge - 2 score to 10.24,0.5381478667259216
translation,75,229,results,10.24,from,slightly higher 10.28,10.24 from slightly higher 10.28,0.5399773120880127
translation,75,229,results,slightly higher 10.28,achieved by,supmmd,slightly higher 10.28 achieved by supmmd,0.6949002146720886
translation,75,229,results,supmmd + mkl,has,relatively little effect,supmmd + mkl has relatively little effect,0.6009677052497864
translation,75,229,results,results,has,multiple kernels,results has multiple kernels,0.5417686700820923
translation,76,9,model,collective factor graph ( cofg ) model,to incorporate,all these resources of knowledge,collective factor graph ( cofg ) model to incorporate all these resources of knowledge,0.6980137228965759
translation,76,9,model,all these resources of knowledge,to summarize,personal profiles,all these resources of knowledge to summarize personal profiles,0.694325864315033
translation,76,9,model,personal profiles,with,local textual attribute functions,personal profiles with local textual attribute functions,0.5666579604148865
translation,76,9,model,personal profiles,with,social connection factors,personal profiles with social connection factors,0.6139301657676697
translation,76,27,model,collective factor graph model ( cofg ),to summarize,text of personal profile,collective factor graph model ( cofg ) to summarize text of personal profile,0.6589277982711792
translation,76,27,model,text of personal profile,in,social networks,text of personal profile in social networks,0.5381434559822083
translation,76,27,model,social networks,with,social connection information,social networks with social connection information,0.5690116286277771
translation,76,27,model,model,propose,collective factor graph model ( cofg ),model propose collective factor graph model ( cofg ),0.6599018573760986
translation,76,28,model,cofg framework,utilizes,local textual attribute functions,cofg framework utilizes local textual attribute functions,0.5873643755912781
translation,76,28,model,cofg framework,utilizes,social connection factor,cofg framework utilizes social connection factor,0.5790853500366211
translation,76,28,model,local textual attribute functions,of,individual person,local textual attribute functions of individual person,0.5592934489250183
translation,76,28,model,social connection factor,between,different persons,social connection factor between different persons,0.6514700651168823
translation,76,28,model,different persons,to collectively summarize,personal profile,different persons to collectively summarize personal profile,0.5971269011497498
translation,76,28,model,personal profile,on,one person,personal profile on one person,0.5576942563056946
translation,76,28,model,model,has,cofg framework,model has cofg framework,0.5678123831748962
translation,76,29,model,profile summarization,as,supervised learning,profile summarization as supervised learning,0.4502265751361847
translation,76,29,model,model,treat,profile summarization,model treat profile summarization,0.5901792049407959
translation,76,31,model,training phase,use,vectors,training phase use vectors,0.6858189702033997
translation,76,31,model,vectors,with,social connection,vectors with social connection,0.6117047071456909
translation,76,31,model,vectors,in,testing phase,vectors in testing phase,0.5583102107048035
translation,76,31,model,social connection,between,each person,social connection between each person,0.6183700561523438
translation,76,31,model,each person,to build,cofg model,each person to build cofg model,0.7421010732650757
translation,76,31,model,testing phase,perform,collective inference,testing phase perform collective inference,0.6030734181404114
translation,76,31,model,collective inference,for,importance of each sentence,collective inference for importance of each sentence,0.5413992404937744
translation,76,31,model,collective inference,select,subset of sentences,collective inference select subset of sentences,0.672468900680542
translation,76,31,model,subset of sentences,as,summary,subset of sentences as summary,0.5633258819580078
translation,76,31,model,summary,according to,trained model,summary according to trained model,0.6348429918289185
translation,76,31,model,model,In,training phase,model In training phase,0.5174987316131592
translation,76,31,model,model,in,testing phase,model in testing phase,0.5571871399879456
translation,77,134,experimental-setup,one billion word benchmark corpus,to train,word embeddings,one billion word benchmark corpus to train word embeddings,0.6044679284095764
translation,77,134,experimental-setup,word embeddings,with,skip-gram model,word embeddings with skip-gram model,0.590610146522522
translation,77,134,experimental-setup,skip-gram model,using,context window size,skip-gram model using context window size,0.6137345433235168
translation,77,134,experimental-setup,known words,initialized with,pre-trained embeddings,known words initialized with pre-trained embeddings,0.684353232383728
translation,77,134,experimental-setup,pre-trained embeddings,of size,200,pre-trained embeddings of size 200,0.6583625674247742
translation,77,134,experimental-setup,context window size,has,6,context window size has 6,0.626483142375946
translation,77,134,experimental-setup,negative sampling size,has,10,negative sampling size has 10,0.6543075442314148
translation,77,134,experimental-setup,experimental setup,used,one billion word benchmark corpus,experimental setup used one billion word benchmark corpus,0.5634933114051819
translation,77,142,experimental-setup,sentences,padded with,zeros,sentences padded with zeros,0.680399477481842
translation,77,142,experimental-setup,zeros,to,length,zeros to length,0.5721535682678223
translation,77,142,experimental-setup,length,of,100,length of 100,0.6582071781158447
translation,77,142,experimental-setup,experimental setup,has,sentences,experimental setup has sentences,0.5204469561576843
translation,77,143,experimental-setup,sentence encoder,used,list of kernels,sentence encoder used list of kernels,0.6143481731414795
translation,77,143,experimental-setup,list of kernels,with,output channel size,list of kernels with output channel size,0.5916101336479187
translation,77,143,experimental-setup,output channel size,of,50,output channel size of 50,0.668826162815094
translation,77,143,experimental-setup,list of kernels,has,of widths 1 to 7,list of kernels has of widths 1 to 7,0.5737358331680298
translation,77,143,experimental-setup,experimental setup,For,sentence encoder,experimental setup For sentence encoder,0.5530884861946106
translation,77,144,experimental-setup,sentence embedding size,in,our model,sentence embedding size in our model,0.4455239772796631
translation,77,144,experimental-setup,sentence embedding size,was,350,sentence embedding size was 350,0.6300175786018372
translation,77,144,experimental-setup,experimental setup,has,sentence embedding size,experimental setup has sentence embedding size,0.5022934675216675
translation,77,145,experimental-setup,recurrent neural network component,in,document encoder and sentence extractor,recurrent neural network component in document encoder and sentence extractor,0.5112615823745728
translation,77,145,experimental-setup,single - layered lstm network,size,600,single - layered lstm network size 600,0.6897826194763184
translation,77,145,experimental-setup,experimental setup,For,recurrent neural network component,experimental setup For recurrent neural network component,0.5969792604446411
translation,77,146,experimental-setup,input documents,padded with,zeros,input documents padded with zeros,0.6691662669181824
translation,77,146,experimental-setup,zeros,to,maximum document length,zeros to maximum document length,0.5400410294532776
translation,77,146,experimental-setup,maximum document length,of,120,maximum document length of 120,0.6261135339736938
translation,77,146,experimental-setup,experimental setup,has,input documents,experimental setup has input documents,0.5034042596817017
translation,77,147,experimental-setup,minibatch cross-entropy training,with,batch size,minibatch cross-entropy training with batch size,0.5926710963249207
translation,77,147,experimental-setup,batch size,of,20 documents,batch size of 20 documents,0.5936536192893982
translation,77,147,experimental-setup,batch size,for,20 training epochs,batch size for 20 training epochs,0.5482233762741089
translation,77,147,experimental-setup,20 documents,for,20 training epochs,20 documents for 20 training epochs,0.5909768342971802
translation,77,147,experimental-setup,experimental setup,performed,minibatch cross-entropy training,experimental setup performed minibatch cross-entropy training,0.28183937072753906
translation,77,148,experimental-setup,around 12 hrs,on,single gpu,around 12 hrs on single gpu,0.5688286423683167
translation,77,148,experimental-setup,single gpu,to,train,single gpu to train,0.6012864112854004
translation,77,148,experimental-setup,experimental setup,took,around 12 hrs,experimental setup took around 12 hrs,0.6366593241691589
translation,77,150,experimental-setup,training,used,adam optimizer,training used adam optimizer,0.5732697248458862
translation,77,150,experimental-setup,adam optimizer,with,initial learning rate,adam optimizer with initial learning rate,0.5838022828102112
translation,77,150,experimental-setup,initial learning rate,has,0.001,initial learning rate has 0.001,0.5123841762542725
translation,77,150,experimental-setup,experimental setup,During,training,experimental setup During training,0.6835477948188782
translation,77,151,experimental-setup,system,implemented in,"tensorflow ( abadi et al. , 2015 )","system implemented in tensorflow ( abadi et al. , 2015 )",0.6650471687316895
translation,77,151,experimental-setup,experimental setup,has,system,experimental setup has system,0.535061240196228
translation,77,275,experiments,ranked 2nd best,followed by,lead,ranked 2nd best followed by lead,0.6667178869247437
translation,77,5,model,model,conceptualize,extractive summarization,model conceptualize extractive summarization,0.6511373519897461
translation,77,24,model,neural summarization model,consists of,hierarchical docu-ment encoder,neural summarization model consists of hierarchical docu-ment encoder,0.6047188639640808
translation,77,24,model,neural summarization model,consists of,hierarchical sentence extractor,neural summarization model consists of hierarchical sentence extractor,0.611014187335968
translation,77,25,model,training,combines,maximumlikelihood cross-entropy loss,training combines maximumlikelihood cross-entropy loss,0.6513446569442749
translation,77,25,model,maximumlikelihood cross-entropy loss,with,rewards,maximumlikelihood cross-entropy loss with rewards,0.6260964274406433
translation,77,25,model,rewards,from,policy gradient reinforcement learning,rewards from policy gradient reinforcement learning,0.5230317115783691
translation,77,25,model,policy gradient reinforcement learning,to directly optimize,evaluation metric,policy gradient reinforcement learning to directly optimize evaluation metric,0.6707015633583069
translation,77,25,model,evaluation metric,relevant for,summarization task,evaluation metric relevant for summarization task,0.5747746825218201
translation,77,25,model,model,During,training,model During training,0.714866042137146
translation,77,51,model,document encoder,composes,sequence of sentences,document encoder composes sequence of sentences,0.7380755543708801
translation,77,51,model,sequence of sentences,to obtain,document representation,sequence of sentences to obtain document representation,0.5882194638252258
translation,77,51,model,model,has,document encoder,model has document encoder,0.5483657717704773
translation,77,91,model,reinforcement learning,to,our formulation,reinforcement learning to our formulation,0.5131590962409973
translation,77,91,model,reinforcement learning,of,extractive summarization,reinforcement learning of extractive summarization,0.5027763247489929
translation,77,91,model,our formulation,of,extractive summarization,our formulation of extractive summarization,0.573395848274231
translation,77,91,model,extractive summarization,to rank,sentences,extractive summarization to rank sentences,0.6847124099731445
translation,77,91,model,sentences,for,summary generation,sentences for summary generation,0.5916345715522766
translation,77,91,model,model,adapt,reinforcement learning,model adapt reinforcement learning,0.6925066709518433
translation,77,92,model,objective function,combines,maximum-likelihood cross-entropy loss,objective function combines maximum-likelihood cross-entropy loss,0.6594284772872925
translation,77,92,model,maximum-likelihood cross-entropy loss,with,rewards,maximum-likelihood cross-entropy loss with rewards,0.6179834008216858
translation,77,92,model,rewards,from,policy gradient reinforcement learning,rewards from policy gradient reinforcement learning,0.5230317115783691
translation,77,92,model,policy gradient reinforcement learning,to globally optimize,rouge,policy gradient reinforcement learning to globally optimize rouge,0.6962146759033203
translation,77,92,model,model,propose,objective function,model propose objective function,0.6512690782546997
translation,77,93,model,more robust,to,unseen data,more robust to unseen data,0.578468918800354
translation,77,93,model,our model,has,more robust,our model has more robust,0.5500214695930481
translation,77,93,model,model,has,training algorithm,model has training algorithm,0.5673739910125732
translation,77,235,model,algorithm,explores,space,algorithm explores space,0.7286609411239624
translation,77,235,model,space,of,candidate summaries,space of candidate summaries,0.6123630404472351
translation,77,235,model,space,while,learning,space while learning,0.6298643946647644
translation,77,235,model,learning,to optimize,reward function,learning to optimize reward function,0.7557992339134216
translation,77,235,model,reward function,relevant for,task,reward function relevant for task,0.6950130462646484
translation,77,26,results,global optimization framework,renders,extractive models,global optimization framework renders extractive models,0.6377821564674377
translation,77,26,results,extractive models,better at,discriminating,extractive models better at discriminating,0.7149121761322021
translation,77,26,results,discriminating,among,sentences,discriminating among sentences,0.6171591281890869
translation,77,26,results,sentences,for,final summary,sentences for final summary,0.6164807081222534
translation,77,26,results,results,show,global optimization framework,results show global optimization framework,0.6382697820663452
translation,77,28,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,77,28,results,outperforms,has,state - of - the - art extractive and abstractive systems,outperforms has state - of - the - art extractive and abstractive systems,0.5734484791755676
translation,77,207,results,our automatic evaluation results,demonstrate,refresh,our automatic evaluation results demonstrate refresh,0.6970522403717041
translation,77,207,results,refresh,superior to,abstractive systems,refresh superior to abstractive systems,0.6653172969818115
translation,77,207,results,results,has,our automatic evaluation results,results has our automatic evaluation results,0.5135303735733032
translation,77,262,results,refresh,superior to,our lead baseline and extractive systems,refresh superior to our lead baseline and extractive systems,0.6300122141838074
translation,77,262,results,our lead baseline and extractive systems,across,datasets and metrics,our lead baseline and extractive systems across datasets and metrics,0.7046591639518738
translation,77,262,results,results,show,refresh,results show refresh,0.6948121190071106
translation,77,263,results,outperforms,on,cnn and dailymail test sets,outperforms on cnn and dailymail test sets,0.5231571197509766
translation,77,263,results,results,on,cnn and dailymail test sets,results on cnn and dailymail test sets,0.4625018239021301
translation,77,263,results,outperforms,has,results,outperforms has results,0.648358941078186
translation,77,263,results,results,has,outperforms,results has outperforms,0.6657275557518005
translation,77,279,results,results,has,human evaluation : question answering,results has human evaluation : question answering,0.5268533229827881
translation,77,281,results,summaries,generated by,refresh,summaries generated by refresh,0.6181370615959167
translation,77,281,results,answer,has,66.34 %,answer has 66.34 %,0.5614597201347351
translation,77,281,results,answer,has,of questions,answer has of questions,0.6006631255149841
translation,77,281,results,66.34 %,has,of questions,66.34 % has of questions,0.5599629878997803
translation,77,281,results,of questions,has,correctly,of questions has correctly,0.6072853803634644
translation,77,281,results,results,on,summaries,results on summaries,0.5377346873283386
translation,78,225,ablation-analysis,signals,more and more removed in,d04 - 1 - d04 - 10,signals more and more removed in d04 - 1 - d04 - 10,0.7106762528419495
translation,78,225,ablation-analysis,big performance decrease,in,"both , rouge - 1 and rouge - 2","big performance decrease in both , rouge - 1 and rouge - 2",0.5411267280578613
translation,78,225,ablation-analysis,ablation analysis,When,signals,ablation analysis When signals,0.7105033993721008
translation,78,6,model,summarizer,that learns,importance of information objects,summarizer that learns importance of information objects,0.6277161836624146
translation,78,6,model,importance of information objects,from,background source,importance of information objects from background source,0.5106722116470337
translation,78,6,model,cpsum,has,summarizer,cpsum has summarizer,0.6069564819335938
translation,78,6,model,model,propose,cpsum,model propose cpsum,0.7447808980941772
translation,78,26,model,cpsum,learns about,importance,cpsum learns about importance,0.7382529973983765
translation,78,26,model,cpsum,applies,knowledge,cpsum applies knowledge,0.6870835423469543
translation,78,26,model,importance,by analyzing,independent background corpus,importance by analyzing independent background corpus,0.7015881538391113
translation,78,26,model,importance,applies,knowledge,importance applies knowledge,0.6738789081573486
translation,78,26,model,independent background corpus,of,document-summary pairs,independent background corpus of document-summary pairs,0.5383114218711853
translation,78,26,model,knowledge,in,summarization task,knowledge in summarization task,0.4934248626232147
translation,78,26,model,model,has,cpsum,model has cpsum,0.6362602114677429
translation,78,8,results,cpsum,proves to be able to perform,well,cpsum proves to be able to perform well,0.7225068211555481
translation,78,8,results,results,has,cpsum,results has cpsum,0.5835423469543457
translation,78,216,results,evaluation scores,for,both,evaluation scores for both,0.6417670845985413
translation,78,216,results,evaluation scores,for,rouge - 1 and rouge - 2 recall,evaluation scores for rouge - 1 and rouge - 2 recall,0.5669803023338318
translation,78,216,results,rouge - 1 and rouge - 2 recall,stays,nearly constant,rouge - 1 and rouge - 2 recall stays nearly constant,0.741005003452301
translation,78,216,results,nearly constant,for,oracle system optimal,nearly constant for oracle system optimal,0.6019850969314575
translation,78,216,results,both,has,rouge - 1 and rouge - 2 recall,both has rouge - 1 and rouge - 2 recall,0.5969695448875427
translation,78,223,results,work well,on,original duc 2004 corpus,work well on original duc 2004 corpus,0.4674874246120453
translation,78,223,results,centroid,achieves,best results,centroid achieves best results,0.6730548143386841
translation,78,223,results,two state - of - the - art reference systems,has,work well,two state - of - the - art reference systems has work well,0.574832022190094
translation,78,223,results,results,has,two state - of - the - art reference systems,results has two state - of - the - art reference systems,0.5266443490982056
translation,78,227,results,cpsum,performs,only moderately,cpsum performs only moderately,0.6172480583190918
translation,78,227,results,only moderately,at,original duc 2004 dataset,only moderately at original duc 2004 dataset,0.45562100410461426
translation,78,227,results,results,has,cpsum,results has cpsum,0.5835423469543457
translation,78,235,results,best performance,on,four modified corpora,best performance on four modified corpora,0.48846468329429626
translation,78,235,results,rouge - 1 scores,has,cpsum,rouge - 1 scores has cpsum,0.5929046273231506
translation,78,235,results,cpsum,has,best performance,cpsum has best performance,0.5813294053077698
translation,78,235,results,results,In terms of,rouge - 1 scores,results In terms of rouge - 1 scores,0.6464335322380066
translation,78,236,results,rouge - 2,has,original performance,rouge - 2 has original performance,0.5499563813209534
translation,78,236,results,results,In terms of,rouge - 2,results In terms of rouge - 2,0.6911795735359192
translation,78,239,results,outperforms,increase,amount of noise,outperforms increase amount of noise,0.7680478692054749
translation,78,239,results,all systems,increase,amount of noise,all systems increase amount of noise,0.7069844603538513
translation,78,239,results,amount of noise,in,corpora d04 - 5 and d04 - 10,amount of noise in corpora d04 - 5 and d04 - 10,0.5574941039085388
translation,78,239,results,cpsum,has,outperforms,cpsum has outperforms,0.6548078060150146
translation,78,239,results,outperforms,has,all systems,outperforms has all systems,0.586120069026947
translation,78,239,results,results,has,cpsum,results has cpsum,0.5835423469543457
translation,78,248,results,cpsum,cope with,summarization scenarios,cpsum cope with summarization scenarios,0.6269893646240234
translation,78,248,results,summarization scenarios,where,centrality,summarization scenarios where centrality,0.6513588428497314
translation,78,248,results,results,has,cpsum,results has cpsum,0.5835423469543457
translation,79,150,baselines,encoder,is,6 - layer transformer,encoder is 6 - layer transformer,0.5862450003623962
translation,79,150,baselines,encoder,is,"2,048 feed - forward filter size","encoder is 2,048 feed - forward filter size",0.5758311152458191
translation,79,150,baselines,6 - layer transformer,with,768 hidden size,6 - layer transformer with 768 hidden size,0.636229395866394
translation,79,150,baselines,6 - layer transformer,with,"2,048 feed - forward filter size","6 - layer transformer with 2,048 feed - forward filter size",0.6289793252944946
translation,79,116,experimental-setup,tokenized,with,bert 's subwords tokenizer,tokenized with bert 's subwords tokenizer,0.656258761882782
translation,79,116,experimental-setup,experimental setup,has,both source and target texts,experimental setup has both source and target texts,0.5261622071266174
translation,79,118,experimental-setup,extractive models,trained for,"50,000 steps","extractive models trained for 50,000 steps",0.7938551306724548
translation,79,118,experimental-setup,"50,000 steps",on,3 gpus ( gtx 1080 ti ),"50,000 steps on 3 gpus ( gtx 1080 ti )",0.5334984064102173
translation,79,118,experimental-setup,3 gpus ( gtx 1080 ti ),with,gradient accumulation,3 gpus ( gtx 1080 ti ) with gradient accumulation,0.5901980996131897
translation,79,118,experimental-setup,gradient accumulation,every,two steps,gradient accumulation every two steps,0.6030693054199219
translation,79,118,experimental-setup,experimental setup,has,extractive models,experimental setup has extractive models,0.5586837530136108
translation,79,130,experimental-setup,hidden size,for,all feed -forward layers,hidden size for all feed -forward layers,0.5717353820800781
translation,79,130,experimental-setup,hidden size,is,"2,048","hidden size is 2,048",0.6002551317214966
translation,79,130,experimental-setup,all feed -forward layers,is,"2,048","all feed -forward layers is 2,048",0.5702801942825317
translation,79,130,experimental-setup,transformer decoder,has,768 hidden units,transformer decoder has 768 hidden units,0.5569866895675659
translation,79,130,experimental-setup,experimental setup,has,transformer decoder,experimental setup has transformer decoder,0.5693507790565491
translation,79,131,experimental-setup,"200,000 steps",on,4,"200,000 steps on 4",0.5643428564071655
translation,79,131,experimental-setup,"200,000 steps",on,gpus ( gtx 1080 ti ),"200,000 steps on gpus ( gtx 1080 ti )",0.530742883682251
translation,79,131,experimental-setup,gpus ( gtx 1080 ti ),with,gradient accumulation,gpus ( gtx 1080 ti ) with gradient accumulation,0.5907843708992004
translation,79,131,experimental-setup,gradient accumulation,every,five steps,gradient accumulation every five steps,0.6180159449577332
translation,79,131,experimental-setup,experimental setup,trained for,"200,000 steps","experimental setup trained for 200,000 steps",0.7305269837379456
translation,79,134,experimental-setup,decoding,used,beam search,decoding used beam search,0.5754576921463013
translation,79,134,experimental-setup,decoding,tuned,length penalty,decoding tuned length penalty,0.7233566641807556
translation,79,134,experimental-setup,decoding,decode until,end-of-sequence token,decoding decode until end-of-sequence token,0.7466995120048523
translation,79,134,experimental-setup,0.6 and 1,on,validation set,0.6 and 1 on validation set,0.5934702754020691
translation,79,134,experimental-setup,end-of-sequence token,is,emitted,end-of-sequence token is emitted,0.5846871733665466
translation,79,134,experimental-setup,beam search,has,size 5 ),beam search has size 5 ),0.6424779891967773
translation,79,134,experimental-setup,experimental setup,During,decoding,experimental setup During decoding,0.6438420414924622
translation,79,147,experimental-setup,hidden size,is,512,hidden size is 512,0.6110948920249939
translation,79,147,experimental-setup,feed -forward filter size,is,"2,048","feed -forward filter size is 2,048",0.5968612432479858
translation,79,147,experimental-setup,transformerext,has,6 layers,transformerext has 6 layers,0.6471892595291138
translation,79,147,experimental-setup,transformerext,has,hidden size,transformerext has hidden size,0.6078718900680542
translation,79,147,experimental-setup,experimental setup,has,transformerext,experimental setup has transformerext,0.5659902095794678
translation,79,147,experimental-setup,experimental setup,has,feed -forward filter size,experimental setup has feed -forward filter size,0.5485490560531616
translation,79,187,experiments,bertextabs,produces,less novel n-ngrams,bertextabs produces less novel n-ngrams,0.6487434506416321
translation,79,187,experiments,less novel n-ngrams,than,bertabs,less novel n-ngrams than bertabs,0.6675223708152771
translation,79,187,experiments,cnn / dailymail,has,bertextabs,cnn / dailymail has bertextabs,0.6490573883056641
translation,79,188,experiments,bertextabs,more biased towards,selecting sentences,bertextabs more biased towards selecting sentences,0.7364184856414795
translation,79,188,experiments,selecting sentences,from,source document,selecting sentences from source document,0.5462086796760559
translation,79,4,model,model,has,bidirectional encoder representations from transformers ( bert,model has bidirectional encoder representations from transformers ( bert,0.6501827239990234
translation,79,5,model,general framework,for,extractive and abstractive models,general framework for extractive and abstractive models,0.6163709163665771
translation,79,5,model,model,propose,general framework,model propose general framework,0.678580105304718
translation,79,6,model,novel document - level encoder,based on,bert,novel document - level encoder based on bert,0.6056185364723206
translation,79,6,model,novel document - level encoder,obtain,representations,novel document - level encoder obtain representations,0.5323680639266968
translation,79,6,model,semantics,of,document,semantics of document,0.647544264793396
translation,79,6,model,representations,for,sentences,representations for sentences,0.6841933727264404
translation,79,6,model,model,introduce,novel document - level encoder,model introduce novel document - level encoder,0.6211282014846802
translation,79,7,model,extractive model,built on top of,encoder,extractive model built on top of encoder,0.7645214796066284
translation,79,7,model,extractive model,by stacking,several intersentence transformer layers,extractive model by stacking several intersentence transformer layers,0.7706549763679504
translation,79,7,model,encoder,by stacking,several intersentence transformer layers,encoder by stacking several intersentence transformer layers,0.8003370761871338
translation,79,7,model,model,has,extractive model,model has extractive model,0.5769835114479065
translation,79,8,model,abstractive summarization,propose,new fine-tuning schedule,abstractive summarization propose new fine-tuning schedule,0.6441893577575684
translation,79,8,model,new fine-tuning schedule,adopts,different optimizers,new fine-tuning schedule adopts different optimizers,0.617313027381897
translation,79,8,model,different optimizers,for,encoder and the decoder,different optimizers for encoder and the decoder,0.6239629983901978
translation,79,8,model,model,For,abstractive summarization,model For abstractive summarization,0.5606654286384583
translation,79,20,model,novel documentlevel encoder,based on,bert,novel documentlevel encoder based on bert,0.6182735562324524
translation,79,20,model,novel documentlevel encoder,obtain,representations,novel documentlevel encoder obtain representations,0.5201348662376404
translation,79,20,model,bert,able to encode,document,bert able to encode document,0.7092045545578003
translation,79,20,model,representations,for,sentences,representations for sentences,0.6841933727264404
translation,79,20,model,model,propose,novel documentlevel encoder,model propose novel documentlevel encoder,0.6781508922576904
translation,79,21,model,extractive model,built on top of,encoder,extractive model built on top of encoder,0.7645214796066284
translation,79,21,model,extractive model,by stacking,several intersentence transformer layers,extractive model by stacking several intersentence transformer layers,0.7706549763679504
translation,79,21,model,encoder,by stacking,several intersentence transformer layers,encoder by stacking several intersentence transformer layers,0.8003370761871338
translation,79,21,model,several intersentence transformer layers,to capture,documentlevel features,several intersentence transformer layers to capture documentlevel features,0.642911970615387
translation,79,21,model,documentlevel features,for extracting,sentences,documentlevel features for extracting sentences,0.7010563611984253
translation,79,21,model,model,has,extractive model,model has extractive model,0.5769835114479065
translation,79,22,model,abstractive model,adopts,encoder-decoder architecture,abstractive model adopts encoder-decoder architecture,0.6141757965087891
translation,79,22,model,encoder-decoder architecture,combining,same pretrained bert encoder,encoder-decoder architecture combining same pretrained bert encoder,0.6841512322425842
translation,79,22,model,same pretrained bert encoder,with,randomly - initialized transformer de-coder,same pretrained bert encoder with randomly - initialized transformer de-coder,0.6423831582069397
translation,79,22,model,model,has,abstractive model,model has abstractive model,0.5751802921295166
translation,79,23,model,new training schedule,separates,optimizers,new training schedule separates optimizers,0.6664965748786926
translation,79,23,model,optimizers,of,encoder and the decoder,optimizers of encoder and the decoder,0.6011902689933777
translation,79,23,model,optimizers,to accommodate,former,optimizers to accommodate former,0.720360279083252
translation,79,23,model,model,design,new training schedule,model design new training schedule,0.5875521302223206
translation,79,24,model,two -stage approach,where,encoder,two -stage approach where encoder,0.6246787905693054
translation,79,24,model,encoder,is,fine- tuned twice,encoder is fine- tuned twice,0.5926650762557983
translation,79,24,model,fine- tuned twice,first with,extractive objective,fine- tuned twice first with extractive objective,0.6831269264221191
translation,79,24,model,fine- tuned twice,subsequently on,abstractive summarization task,fine- tuned twice subsequently on abstractive summarization task,0.5640923380851746
translation,79,24,model,model,present,two -stage approach,model present two -stage approach,0.6470140218734741
translation,79,142,results,results,on,cnn / dailymail dataset,results on cnn / dailymail dataset,0.48680755496025085
translation,79,152,results,bert - based models,on,cnn / dailymail corpus,bert - based models on cnn / dailymail corpus,0.5237833261489868
translation,79,152,results,lead - 3 baseline,is,strawman,lead - 3 baseline is strawman,0.6227360367774963
translation,79,152,results,lead - 3 baseline,not,strawman,lead - 3 baseline not strawman,0.7609527707099915
translation,79,152,results,superior,to,several extractive,superior to several extractive,0.6056894063949585
translation,79,152,results,bert - based models,has,outperform,bert - based models has outperform,0.5942625999450684
translation,79,152,results,outperform,has,lead - 3 baseline,outperform has lead - 3 baseline,0.6201044917106628
translation,79,152,results,results,has,bert - based models,results has bert - based models,0.5088379383087158
translation,79,153,results,bert models,has,collectively outperform,bert models has collectively outperform,0.6072316765785217
translation,79,153,results,collectively outperform,has,all previously proposed extractive and abstractive systems,collectively outperform has all previously proposed extractive and abstractive systems,0.5935972929000854
translation,79,153,results,falling behind,has,oracle upper bound,falling behind has oracle upper bound,0.5871052145957947
translation,79,153,results,results,has,bert models,results has bert models,0.5150020122528076
translation,79,155,results,interval embeddings,bring,only slight gains,interval embeddings bring only slight gains,0.6151030659675598
translation,79,156,results,results,on,nyt dataset,results on nyt dataset,0.5333811044692993
translation,79,163,results,outperform,has,previously proposed approaches,outperform has previously proposed approaches,0.6083572506904602
translation,79,164,results,abstractive bert models,perform,better,abstractive bert models perform better,0.6469437479972839
translation,79,164,results,better,compared to,bertsumext,better compared to bertsumext,0.7055351138114929
translation,79,164,results,better,approaching,oracle performance,better approaching oracle performance,0.7419540286064148
translation,79,165,results,results,on,xsum dataset,results on xsum dataset,0.569536030292511
translation,79,167,results,extractive models,perform,poorly,extractive models perform poorly,0.6332418322563171
translation,79,167,results,low performance,of,lead baseline,low performance of lead baseline,0.6026297807693481
translation,79,167,results,results,has,extractive models,results has extractive models,0.5364269614219666
translation,79,170,results,results,of,bert summarizers,results of bert summarizers,0.6322768926620483
translation,79,170,results,superior,to,all previously reported models,superior to all previously reported models,0.5861576199531555
translation,79,170,results,superior,by,wide margin,superior by wide margin,0.5077913999557495
translation,79,170,results,all previously reported models,by,wide margin,all previously reported models by wide margin,0.578800618648529
translation,79,170,results,results,of,bert summarizers,results of bert summarizers,0.6322768926620483
translation,79,182,results,bertsumext outputs,more similar to,oracle summaries,bertsumext outputs more similar to oracle summaries,0.59358149766922
translation,79,182,results,bertsumext outputs,with,pretrained encoder,bertsumext outputs with pretrained encoder,0.5966653227806091
translation,79,182,results,results,has,bertsumext outputs,results has bertsumext outputs,0.587082028388977
translation,79,207,results,differences,between,bertsum and comparison models,differences between bertsum and comparison models,0.6567208170890808
translation,79,207,results,bertsum and comparison models,are,statistically significant ( p < 0.05 ),bertsum and comparison models are statistically significant ( p < 0.05 ),0.564314603805542
translation,79,207,results,results,has,differences,results has differences,0.5601350665092468
translation,80,153,baselines,"pythy ( toutanova et al. , 2007 )",Utilizes,human generated summaries,"pythy ( toutanova et al. , 2007 ) Utilizes human generated summaries",0.6174472570419312
translation,80,153,baselines,human generated summaries,to train,sentence ranking system,human generated summaries to train sentence ranking system,0.6406786441802979
translation,80,153,baselines,sentence ranking system,using,classifier model,sentence ranking system using classifier model,0.6456236839294434
translation,80,153,baselines,"hiersum ( haghighi and vanderwende , 2009 )",Based on,hierarchical topic models,"hiersum ( haghighi and vanderwende , 2009 ) Based on hierarchical topic models",0.6264735460281372
translation,80,155,baselines,baselines,has,hybhsum,baselines has hybhsum,0.5605440139770508
translation,80,158,baselines,pam,has,"li and mc- callum , 2006 )","pam has li and mc- callum , 2006 )",0.6095353364944458
translation,80,158,baselines,hpam,has,"mimno et al. , 2007 )","hpam has mimno et al. , 2007 )",0.5707836151123047
translation,80,158,baselines,baselines,has,pam,baselines has pam,0.6304556727409363
translation,80,151,hyperparameters,gibbs samplers,for,2000 iterations,gibbs samplers for 2000 iterations,0.5476246476173401
translation,80,151,hyperparameters,2000 iterations,for,each configuration,2000 iterations for each configuration,0.6040137410163879
translation,80,151,hyperparameters,each configuration,throwing out,first 500 samples,each configuration throwing out first 500 samples,0.6294286847114563
translation,80,151,hyperparameters,first 500 samples,as,burn- in,first 500 samples as burn- in,0.5906893610954285
translation,80,151,hyperparameters,hyperparameters,ran,gibbs samplers,hyperparameters ran gibbs samplers,0.528815507888794
translation,80,12,model,series of new generative models,for,multiple - documents,series of new generative models for multiple - documents,0.5787878632545471
translation,80,12,model,series of new generative models,based on,discovery of hierarchical topics,series of new generative models based on discovery of hierarchical topics,0.6197488903999329
translation,80,12,model,correlations,to extract,topically coherent sentences,correlations to extract topically coherent sentences,0.6694636940956116
translation,80,12,model,model,introduce,series of new generative models,model introduce series of new generative models,0.624286413192749
translation,80,19,model,"novel , fully generative bayesian model",of,document corpus,"novel , fully generative bayesian model of document corpus",0.5389901399612427
translation,80,19,model,"novel , fully generative bayesian model",can discover,topically coherent sentences,"novel , fully generative bayesian model can discover topically coherent sentences",0.6369489431381226
translation,80,19,model,topically coherent sentences,that contain,key shared information,topically coherent sentences that contain key shared information,0.6618759036064148
translation,80,19,model,key shared information,with,as little detail and redundancy as possible,key shared information with as little detail and redundancy as possible,0.6232662200927734
translation,80,19,model,model,present,"novel , fully generative bayesian model","model present novel , fully generative bayesian model",0.6406689882278442
translation,80,20,model,hierarchical latent structure,of,multi-documents,hierarchical latent structure of multi-documents,0.5625133514404297
translation,80,20,model,hierarchical latent structure,in which,some words,hierarchical latent structure in which some words,0.6439116597175598
translation,80,20,model,some words,governed by,low-level topics ( t ),some words governed by low-level topics ( t ),0.6817271113395691
translation,80,20,model,some words,governed by,high - level topics ( h ),some words governed by high - level topics ( h ),0.6780593395233154
translation,80,20,model,model,discover,hierarchical latent structure,model discover hierarchical latent structure,0.6891934275627136
translation,80,154,model,approximation,for,inference,approximation for inference,0.6390120983123779
translation,80,154,model,sentences,greedily added to,summary,sentences greedily added to summary,0.7099930644035339
translation,80,154,model,kl - divergence,of,generated summary concept distributions,kl - divergence of generated summary concept distributions,0.5492785573005676
translation,80,154,model,generated summary concept distributions,from,document word -frequency distributions,generated summary concept distributions from document word -frequency distributions,0.5106728076934814
translation,80,154,model,approximation,has,sentences,approximation has sentences,0.6408022046089172
translation,80,154,model,inference,has,sentences,inference has sentences,0.6057410836219788
translation,80,154,model,model,Using,approximation,model Using approximation,0.7219786047935486
translation,80,7,results,topical coherence,achieve,44.1 rouge,topical coherence achieve 44.1 rouge,0.6285526752471924
translation,80,7,results,44.1 rouge,on,duc - 07 test set,44.1 rouge on duc - 07 test set,0.4721411466598511
translation,80,7,results,44.1 rouge,in the range,state - of - the - art supervised models,44.1 rouge in the range state - of - the - art supervised models,0.6219942569732666
translation,80,138,results,best performance,in ranking,human generated sentences,best performance in ranking human generated sentences,0.7577093839645386
translation,80,138,results,best performance,better than,ttm model,best performance better than ttm model,0.7732335925102234
translation,80,138,results,human generated sentences,at,top,human generated sentences at top,0.5576500296592712
translation,80,138,results,out of 45 models,has,ettm,out of 45 models has ettm,0.5975143313407898
translation,80,138,results,ettm,has,best performance,ettm has best performance,0.5714637041091919
translation,80,160,results,unsupervised ttm and ettm systems,yield,44.1 r - 1 ( w/ stop-words ),unsupervised ttm and ettm systems yield 44.1 r - 1 ( w/ stop-words ),0.6418179869651794
translation,80,160,results,rest of the models,except,hybhsum,rest of the models except hybhsum,0.666917622089386
translation,80,160,results,44.1 r - 1 ( w/ stop-words ),has,outperforming,44.1 r - 1 ( w/ stop-words ) has outperforming,0.5455895662307739
translation,80,160,results,outperforming,has,rest of the models,outperforming has rest of the models,0.5834972858428955
translation,80,161,results,hybhsum,uses,human generated summaries,hybhsum uses human generated summaries,0.6592079997062683
translation,80,161,results,human generated summaries,as,supervision,human generated summaries as supervision,0.49942418932914734
translation,80,161,results,supervision,during,model development,supervision during model development,0.7073488831520081
translation,80,161,results,our performance,is,quite promising,our performance is quite promising,0.5276963710784912
translation,80,161,results,hybhsum,has,our performance,hybhsum has our performance,0.5937061905860901
translation,80,161,results,human generated summaries,has,our performance,human generated summaries has our performance,0.566055953502655
translation,80,161,results,results,has,hybhsum,results has hybhsum,0.5460250973701477
translation,80,162,results,r-2 evaluation,has,w/ stop-words,r-2 evaluation has w/ stop-words,0.5844905972480774
translation,80,162,results,w/ stop-words,has,does not outperform,w/ stop-words has does not outperform,0.6575887203216553
translation,80,162,results,does not outperform,has,other models,does not outperform has other models,0.5911294221878052
translation,80,162,results,results,has,r-2 evaluation,results has r-2 evaluation,0.5450280904769897
translation,80,173,results,more coherent and focused,compared to,pam,more coherent and focused compared to pam,0.6789302825927734
translation,80,173,results,ettm generated summaries,has,more coherent and focused,ettm generated summaries has more coherent and focused,0.5181846618652344
translation,80,174,results,results,of,ettm,results of ettm,0.5962299704551697
translation,80,174,results,ettm,slightly better than,hybhsum,ettm slightly better than hybhsum,0.7621406316757202
translation,80,174,results,results,of,ettm,results of ettm,0.5962299704551697
translation,80,174,results,results,has,results,results has results,0.48582205176353455
translation,81,133,ablation-analysis,pagerank features,enhance,models,pagerank features enhance models,0.6046698093414307
translation,81,133,ablation-analysis,models,that only use,word features,models that only use word features,0.6456719636917114
translation,81,133,ablation-analysis,word features,of,nodes,word features of nodes,0.5750094652175903
translation,81,133,ablation-analysis,noderank and arearank models,has,pagerank features,noderank and arearank models has pagerank features,0.5580952763557434
translation,81,133,ablation-analysis,ablation analysis,In,noderank and arearank models,ablation analysis In noderank and arearank models,0.5389939546585083
translation,81,114,baselines,naive bayes,to cluster,documents,naive bayes to cluster documents,0.7781959772109985
translation,81,114,baselines,documents,for,event detection,documents for event detection,0.610700249671936
translation,81,114,baselines,clusters,based on,combination score,clusters based on combination score,0.6564324498176575
translation,81,114,baselines,combination score,of,topical relevance and the event impact,combination score of topical relevance and the event impact,0.5666267275810242
translation,81,116,baselines,burstvsm representation,used for,event detection,burstvsm representation used for event detection,0.6187744736671448
translation,81,116,baselines,event detection,using,hierarchical agglomerative clustering algorithm,event detection using hierarchical agglomerative clustering algorithm,0.6622945070266724
translation,81,116,baselines,baselines,has,b-hac,baselines has b-hac,0.5637578964233398
translation,81,177,baselines,stateof - the - art event detection model,used for,event detection,stateof - the - art event detection model used for event detection,0.6112703084945679
translation,81,177,baselines,baselines,has,tahbm,baselines has tahbm,0.5491774678230286
translation,81,141,experimental-setup,run time,tested on,workstation,run time tested on workstation,0.7381230592727661
translation,81,141,experimental-setup,workstation,with,intel xeon 3.5 ghz cpu,workstation with intel xeon 3.5 ghz cpu,0.5428856015205383
translation,81,141,experimental-setup,workstation,with,64gb ram,workstation with 64gb ram,0.5678809881210327
translation,81,141,experimental-setup,experimental setup,has,run time,experimental setup has run time,0.5142230987548828
translation,81,5,model,simple yet effective models,to solve,problem,simple yet effective models to solve problem,0.7163723707199097
translation,81,5,model,problem,based on,novel and promising representation of text streams,problem based on novel and promising representation of text streams,0.6681644320487976
translation,81,5,model,novel and promising representation of text streams,has,burst information networks ( binets ),novel and promising representation of text streams has burst information networks ( binets ),0.5780491232872009
translation,81,5,model,model,propose,simple yet effective models,model propose simple yet effective models,0.6526808142662048
translation,81,122,results,binet- based approaches,perform,comparably,binet- based approaches perform comparably,0.5753393173217773
translation,81,122,results,comparably,to,state- ofthe - art model,comparably to state- ofthe - art model,0.536842942237854
translation,81,122,results,state- ofthe - art model,on generating,summaries,state- ofthe - art model on generating summaries,0.6644687652587891
translation,81,122,results,summaries,on,most topics,summaries on most topics,0.4879734516143799
translation,81,122,results,binet- based approaches,has,outperform,binet- based approaches has outperform,0.6224828958511353
translation,81,122,results,outperform,has,baselines,outperform has baselines,0.6363358497619629
translation,81,122,results,results,observed that,binet- based approaches,results observed that binet- based approaches,0.6754614114761353
translation,81,123,results,arearank,achieves,significant improvement,arearank achieves significant improvement,0.6838001012802124
translation,81,123,results,arearank,performs,comparably,arearank performs comparably,0.6473045945167542
translation,81,123,results,significant improvement,over,state - of - the - art model,significant improvement over state - of - the - art model,0.6344265937805176
translation,81,123,results,state - of - the - art model,on,sports and disasters,state - of - the - art model on sports and disasters,0.52378249168396
translation,81,123,results,comparably,on,politics and military,comparably on politics and military,0.58988356590271
translation,81,123,results,performance,achieves,comparable performance,performance achieves comparable performance,0.7223293781280518
translation,81,123,results,comparable performance,to,previous state - of - the - art model,comparable performance to previous state - of - the - art model,0.5388816595077515
translation,81,123,results,results,has,arearank,results has arearank,0.5991485714912415
translation,81,124,results,almost all models,perform,well,almost all models perform well,0.607767641544342
translation,81,124,results,well,on,disaster and military topics,well on disaster and military topics,0.5786288380622864
translation,81,157,results,binet-based approaches,achieve,better results,binet-based approaches achieve better results,0.6160353422164917
translation,81,157,results,better results,than,online version of b-hac model,better results than online version of b-hac model,0.5848560333251953
translation,81,157,results,online version of b-hac model,on,both topics,online version of b-hac model on both topics,0.5593411922454834
translation,81,157,results,results,has,binet-based approaches,results has binet-based approaches,0.5426567792892456
translation,81,158,results,arearank,performs,better,arearank performs better,0.6291630268096924
translation,81,158,results,better,than,noderank,better than noderank,0.5952760577201843
translation,81,159,results,mrr,on,disaster topic,mrr on disaster topic,0.5724552273750305
translation,81,159,results,disaster topic,is,0.2,disaster topic is 0.2,0.5761788487434387
translation,81,159,results,disaster topic,about,0.2,disaster topic about 0.2,0.6329211592674255
translation,81,159,results,arearank,has,mrr,arearank has mrr,0.6154102683067322
translation,81,159,results,results,For,arearank,results For arearank,0.6595044732093811
translation,82,119,baselines,best,of,stochastic gradient descent,best of stochastic gradient descent,0.5631798505783081
translation,82,119,baselines,best,of,adadelta,best of adadelta,0.6014807224273682
translation,82,119,baselines,best,of,momentum,best of momentum,0.6224619150161743
translation,82,119,baselines,best,of,adam,best of adam,0.5963394045829773
translation,82,119,baselines,best,of,rm - sprop,best of rm - sprop,0.578131377696991
translation,82,114,experimental-setup,pointer - generator models,use,vocabulary,pointer - generator models use vocabulary,0.6707664132118225
translation,82,114,experimental-setup,vocabulary,of,50 k words,vocabulary of 50 k words,0.6213000416755676
translation,82,114,experimental-setup,50 k words,for,source and target,50 k words for source and target,0.6240836381912231
translation,82,114,experimental-setup,experimental setup,For,pointer - generator models,experimental setup For pointer - generator models,0.5838651657104492
translation,82,118,experimental-setup,"adagrad ( duchi et al. , 2011 )",with,learning rate,"adagrad ( duchi et al. , 2011 ) with learning rate",0.53758704662323
translation,82,118,experimental-setup,"adagrad ( duchi et al. , 2011 )",with,initial accumulator value,"adagrad ( duchi et al. , 2011 ) with initial accumulator value",0.5564320087432861
translation,82,118,experimental-setup,initial accumulator value,of,0.1,initial accumulator value of 0.1,0.6011174917221069
translation,82,118,experimental-setup,learning rate,has,0.15,learning rate has 0.15,0.5453540682792664
translation,82,118,experimental-setup,experimental setup,train using,"adagrad ( duchi et al. , 2011 )","experimental setup train using adagrad ( duchi et al. , 2011 )",0.7028688788414001
translation,82,120,experimental-setup,gradient clipping,with,maximum gradient norm,gradient clipping with maximum gradient norm,0.5783113837242126
translation,82,120,experimental-setup,maximum gradient norm,of,2,maximum gradient norm of 2,0.6099976897239685
translation,82,120,experimental-setup,experimental setup,use,gradient clipping,experimental setup use gradient clipping,0.5653892755508423
translation,82,121,experimental-setup,loss,on,validation set,loss on validation set,0.5221169590950012
translation,82,121,experimental-setup,loss,to implement,early stopping,loss to implement early stopping,0.702652096748352
translation,82,128,experimental-setup,single tesla k40 m gpu,with,batch size,single tesla k40 m gpu with batch size,0.6339405179023743
translation,82,128,experimental-setup,batch size,of,16,batch size of 16,0.6842944622039795
translation,82,128,experimental-setup,experimental setup,train on,single tesla k40 m gpu,experimental setup train on single tesla k40 m gpu,0.7045232057571411
translation,82,129,experimental-setup,summaries,produced using,beam search,summaries produced using beam search,0.7317162156105042
translation,82,129,experimental-setup,beam search,with,beam size 4,beam search with beam size 4,0.6971104741096497
translation,82,129,experimental-setup,test time,has,summaries,test time has summaries,0.5689306855201721
translation,82,129,experimental-setup,experimental setup,At,test time,experimental setup At test time,0.5272576808929443
translation,82,8,hyperparameters,coverage,to keep track of,what has been summarized,coverage to keep track of what has been summarized,0.6869155168533325
translation,82,8,hyperparameters,what has been summarized,discourages,repetition,what has been summarized discourages repetition,0.6127519607543945
translation,82,6,model,novel architecture,augments,standard sequence - to-sequence attentional model,novel architecture augments standard sequence - to-sequence attentional model,0.6745314002037048
translation,82,6,model,model,propose,novel architecture,model propose novel architecture,0.7315564155578613
translation,82,7,model,hybrid pointer - generator network,can copy,words,hybrid pointer - generator network can copy words,0.683578372001648
translation,82,7,model,hybrid pointer - generator network,retaining,ability,hybrid pointer - generator network retaining ability,0.7815195918083191
translation,82,7,model,words,from,source text,words from source text,0.5468318462371826
translation,82,7,model,words,via,pointing,words via pointing,0.6787258982658386
translation,82,7,model,source text,via,pointing,source text via pointing,0.681252121925354
translation,82,7,model,pointing,aids,accurate reproduction,pointing aids accurate reproduction,0.6905884146690369
translation,82,7,model,accurate reproduction,of,information,accurate reproduction of information,0.5325565934181213
translation,82,7,model,ability,to produce,novel words,ability to produce novel words,0.6428219676017761
translation,82,7,model,novel words,through,generator,novel words through generator,0.6875094175338745
translation,82,7,model,model,use,hybrid pointer - generator network,model use hybrid pointer - generator network,0.6326307058334351
translation,82,7,model,model,retaining,ability,model retaining ability,0.7927285432815552
translation,82,43,model,novel variant,of,"coverage vector ( tu et al. , 2016 )","novel variant of coverage vector ( tu et al. , 2016 )",0.544306218624115
translation,82,43,model,novel variant,to track and control,coverage,novel variant to track and control coverage,0.6849418878555298
translation,82,43,model,"coverage vector ( tu et al. , 2016 )",from,neural machine translation,"coverage vector ( tu et al. , 2016 ) from neural machine translation",0.5294283032417297
translation,82,43,model,coverage,of,source document,coverage of source document,0.5827162861824036
translation,82,43,model,model,propose,novel variant,model propose novel variant,0.7118100523948669
translation,82,155,results,both our baseline models,perform,poorly,both our baseline models perform poorly,0.6185094714164734
translation,82,155,results,poorly,with respect to,rouge and meteor,poorly with respect to rouge and meteor,0.7285593152046204
translation,82,155,results,results,find that,both our baseline models,results find that both our baseline models,0.6277969479560852
translation,82,162,results,pointer - generator model,achieves,much better rouge and meteor scores,pointer - generator model achieves much better rouge and meteor scores,0.6460919380187988
translation,82,162,results,much better rouge and meteor scores,than,baseline,much better rouge and meteor scores than baseline,0.5506502389907837
translation,82,162,results,results,has,pointer - generator model,results has pointer - generator model,0.5361871719360352
translation,82,191,results,all our models,receive,over 1 meteor point boost,all our models receive over 1 meteor point boost,0.6289483308792114
translation,82,191,results,over 1 meteor point boost,inclusion of,"stem , synonym and paraphrase matching","over 1 meteor point boost inclusion of stem , synonym and paraphrase matching",0.6293010115623474
translation,82,191,results,results,observe,all our models,results observe all our models,0.6054394841194153
translation,82,192,results,not surpassed,by,our models,not surpassed by our models,0.6249923706054688
translation,82,192,results,results,observe,lead - 3 baseline,results observe lead - 3 baseline,0.594422459602356
translation,83,102,baselines,baseline,employs,mmr,baseline employs mmr,0.534309983253479
translation,83,102,baselines,baselines,has,baseline,baselines has baseline,0.6124745607376099
translation,83,5,model,summary,by selecting and ordering sentences,multiple review texts,summary by selecting and ordering sentences multiple review texts,0.7393450736999512
translation,83,5,model,summary,according to,two scores,summary according to two scores,0.7044982314109802
translation,83,5,model,two scores,represent,informativeness and readability,two scores represent informativeness and readability,0.5636481642723083
translation,83,5,model,informativeness and readability,of,sentence order,informativeness and readability of sentence order,0.5524364709854126
translation,83,28,model,algorithm,chooses,sentences,algorithm chooses sentences,0.7384379506111145
translation,83,28,model,ordered sentences,scores of,informativeness and readability,ordered sentences scores of informativeness and readability,0.7027771472930908
translation,83,29,model,best sequence of sentences,by using,dynamic programming,best sequence of sentences by using dynamic programming,0.6400146484375
translation,83,29,model,best sequence of sentences,by using,beam search,best sequence of sentences by using beam search,0.64472496509552
translation,83,114,results,significantly outperformed,to,wilcoxon signed - rank test,significantly outperformed to wilcoxon signed - rank test,0.5780145525932312
translation,83,114,results,significantly outperformed,has,baseline ac-,significantly outperformed has baseline ac-,0.6374309062957764
translation,83,116,results,rouge scores,of,latter,rouge scores of latter,0.6282545328140259
translation,83,116,results,higher,for,all criteria,higher for all criteria,0.6520496606826782
translation,83,116,results,method2 to method3,has,rouge scores,method2 to method3 has rouge scores,0.5739248991012573
translation,83,116,results,results,Comparing,method2 to method3,results Comparing method2 to method3,0.6594703793525696
translation,83,117,results,readability criterion,improved,rouge scores,readability criterion improved rouge scores,0.6664036512374878
translation,83,117,results,results,interesting that,readability criterion,results interesting that readability criterion,0.6240549087524414
translation,84,106,ablation-analysis,more abstractive summaries,trained on,xsum,more abstractive summaries trained on xsum,0.7246602773666382
translation,84,106,ablation-analysis,more abstractive summaries,has,grammaticality scores,more abstractive summaries has grammaticality scores,0.5500091314315796
translation,84,106,ablation-analysis,grammaticality scores,has,drop significantly,grammaticality scores has drop significantly,0.5979012250900269
translation,84,106,ablation-analysis,ablation analysis,on,more abstractive summaries,ablation analysis on more abstractive summaries,0.5282434225082397
translation,84,139,experimental-setup,important text spans,in,sentence,important text spans in sentence,0.48378339409828186
translation,84,139,experimental-setup,sentence,including,noun phrases,sentence including noun phrases,0.6827288866043091
translation,84,139,experimental-setup,noun phrases,extracted by,constituency parser,noun phrases extracted by constituency parser,0.6744974255561829
translation,84,139,experimental-setup,named entities,extracted by,stanford corenlp ner model,named entities extracted by stanford corenlp ner model,0.6069128513336182
translation,84,139,experimental-setup,experimental setup,mask,important text spans,experimental setup mask important text spans,0.6928170919418335
translation,84,107,experiments,bertsum,maintains,good performance,bertsum maintains good performance,0.7282863259315491
translation,84,107,experiments,good performance,on,xsum,good performance on xsum,0.5538702011108398
translation,84,107,experiments,highest grammaticality score,on,both datasets,highest grammaticality score on both datasets,0.4523793160915375
translation,84,138,experiments,natural language questions,from,summary sentence,natural language questions from summary sentence,0.5433933138847351
translation,84,138,experiments,summary sentence,has,automatically,summary sentence has automatically,0.619020938873291
translation,84,9,model,model,propose,automatic question answering ( qa ),model propose automatic question answering ( qa ),0.6519424319267273
translation,84,10,model,questionanswer pairs,generated from,summary,questionanswer pairs generated from summary,0.6343546509742737
translation,84,10,model,questionanswer pairs,generated from,summary,questionanswer pairs generated from summary,0.6343546509742737
translation,84,10,model,qa model,extracts,answers,qa model extracts answers,0.6724572777748108
translation,84,10,model,answers,from,document,answers from document,0.6581774353981018
translation,84,10,model,non-matched answers,indicate,unfaithful information,non-matched answers indicate unfaithful information,0.6068860292434692
translation,84,10,model,unfaithful information,in,summary,unfaithful information in summary,0.5204881429672241
translation,84,10,model,questionanswer pairs,has,qa model,questionanswer pairs has qa model,0.5879899859428406
translation,84,10,model,summary,has,qa model,summary has qa model,0.576707124710083
translation,84,10,model,model,Given,questionanswer pairs,model Given questionanswer pairs,0.7187290191650391
translation,84,29,model,automatically generated qa pairs,to represent,information,automatically generated qa pairs to represent information,0.6595363020896912
translation,84,29,model,information,in,summary,information in summary,0.5359929800033569
translation,84,29,model,model,use,automatically generated qa pairs,model use automatically generated qa pairs,0.6754142045974731
translation,84,30,model,set of   groundtruth   qa pairs,from,summary,set of   groundtruth   qa pairs from summary,0.5371434092521667
translation,84,30,model,set of   groundtruth   qa pairs,using,learned model,set of   groundtruth   qa pairs using learned model,0.5899584889411926
translation,84,30,model,learned model,that converts,declarative sentence,learned model that converts declarative sentence,0.5417485237121582
translation,84,30,model,learned model,that converts,answer span,learned model that converts answer span,0.5850293040275574
translation,84,30,model,answer span,to,question,answer span to question,0.6499432325363159
translation,84,30,model,model,generate,set of   groundtruth   qa pairs,model generate set of   groundtruth   qa pairs,0.6387499570846558
translation,84,25,results,number of unfaithful sentences ( annotated by humans ),increases as,summary,number of unfaithful sentences ( annotated by humans ) increases as summary,0.5975374579429626
translation,84,25,results,summary,becomes,more abstractive,summary becomes more abstractive,0.6160489916801453
translation,84,60,results,cnn / dm,is,more extractive,cnn / dm is more extractive,0.5812218189239502
translation,84,60,results,more extractive,than,xsum,more extractive than xsum,0.6034336090087891
translation,84,61,results,extraction scores,of,reference summaries,extraction scores of reference summaries,0.5614861845970154
translation,84,61,results,reference summaries,in,cnn / dm,reference summaries in cnn / dm,0.5491209030151367
translation,84,61,results,sentences,formed by deleting words,one of the source sentences,sentences formed by deleting words one of the source sentences,0.7581583857536316
translation,84,61,results,results,has,extraction scores,results has extraction scores,0.5318186283111572
translation,84,105,results,outputs,from,all models,outputs from all models,0.6049168109893799
translation,84,105,results,all models,scored high on,grammaticality,all models scored high on grammaticality,0.7072252035140991
translation,84,105,results,grammaticality,with,high inter-annotator agreement,grammaticality with high inter-annotator agreement,0.5336016416549683
translation,84,105,results,results,has,outputs,results has outputs,0.5481858849525452
translation,84,109,results,near-extractive summaries,generated from,models,near-extractive summaries generated from models,0.6426162719726562
translation,84,109,results,models,trained on,cnn / dm,models trained on cnn / dm,0.7248637676239014
translation,84,109,results,models,trained on,xsum,models trained on xsum,0.7353853583335876
translation,84,109,results,cnn / dm,have,significantly higher faithfulness scores,cnn / dm have significantly higher faithfulness scores,0.5656601786613464
translation,84,109,results,significantly higher faithfulness scores,than,highly abstractive summaries,significantly higher faithfulness scores than highly abstractive summaries,0.5574021935462952
translation,84,109,results,highly abstractive summaries,models trained on,xsum,highly abstractive summaries models trained on xsum,0.6852445602416992
translation,84,109,results,results,has,near-extractive summaries,results has near-extractive summaries,0.5453447103500366
translation,84,112,results,human agreement,on,faithfulness,human agreement on faithfulness,0.539417028427124
translation,84,112,results,faithfulness,is,lower,faithfulness is lower,0.6452436447143555
translation,84,112,results,lower,for,abstractive summaries,lower for abstractive summaries,0.6199876666069031
translation,84,112,results,abstractive summaries,from,xsum,abstractive summaries from xsum,0.5699841976165771
translation,84,112,results,results,has,human agreement,results has human agreement,0.48787039518356323
translation,84,114,results,conflicting information,is,more common,conflicting information is more common,0.5688655972480774
translation,84,114,results,more common,among,models,more common among models,0.6320539116859436
translation,84,114,results,more common,among,models,more common among models,0.6320539116859436
translation,84,114,results,more common,among,models,more common among models,0.6320539116859436
translation,84,114,results,models,trained on,cnn / dm,models trained on cnn / dm,0.7248637676239014
translation,84,114,results,models,trained on,xsum,models trained on xsum,0.7353853583335876
translation,84,114,results,more common,among,models,more common among models,0.6320539116859436
translation,84,114,results,models,trained on,xsum,models trained on xsum,0.7353853583335876
translation,84,114,results,results,observe,conflicting information,results observe conflicting information,0.6197967529296875
translation,84,114,results,results,observe,hallucination,results observe hallucination,0.6551441550254822
translation,84,177,results,score,of,qa - based evaluation,score of qa - based evaluation,0.5895345211029053
translation,84,177,results,higher correlation,with,faithfulness,higher correlation with faithfulness,0.6495035290718079
translation,84,177,results,faithfulness,than,other metrics,faithfulness than other metrics,0.5505708456039429
translation,84,177,results,cnn / dm and xsum,has,score,cnn / dm and xsum has score,0.6168350577354431
translation,84,177,results,qa - based evaluation,has,higher correlation,qa - based evaluation has higher correlation,0.5594446659088135
translation,84,177,results,results,observe,cnn / dm and xsum,results observe cnn / dm and xsum,0.593112587928772
translation,84,177,results,results,for,cnn / dm and xsum,results for cnn / dm and xsum,0.6131958365440369
translation,84,178,results,word-overlap based metrics,correlated with,faithfulness,word-overlap based metrics correlated with faithfulness,0.7046412229537964
translation,84,178,results,word-overlap based metrics,correlated with,faithfulness,word-overlap based metrics correlated with faithfulness,0.7046412229537964
translation,84,178,results,faithfulness,in,more extractive settings,faithfulness in more extractive settings,0.5507928133010864
translation,84,178,results,faithfulness,more abstractive settings ( i.e. for,xsum ),faithfulness more abstractive settings ( i.e. for xsum ),0.7176762819290161
translation,84,178,results,results,has,word-overlap based metrics,results has word-overlap based metrics,0.5284029245376587
translation,84,179,results,significantly lower correlation,with,human scores,significantly lower correlation with human scores,0.6480318307876587
translation,84,179,results,human scores,for,xsum,human scores for xsum,0.632544219493866
translation,84,179,results,results,notice,all the metrics,results notice all the metrics,0.6984854340553284
translation,85,91,baselines,enhanced version,relies on,range of separate extractive summarization features,enhanced version relies on range of separate extractive summarization features,0.7423866987228394
translation,85,91,baselines,range of separate extractive summarization features,added as,log-linear features,range of separate extractive summarization features added as log-linear features,0.6364112496376038
translation,85,91,baselines,log-linear features,in,secondary learning step,log-linear features in secondary learning step,0.5108348727226257
translation,85,91,baselines,secondary learning step,with,minimum error rate training,secondary learning step with minimum error rate training,0.6445243954658508
translation,85,84,experimental-setup,final elman architecture ( ras - elman ),uses,single layer,final elman architecture ( ras - elman ) uses single layer,0.5843713879585266
translation,85,84,experimental-setup,single layer,with,"h = 512 , ? = 0.5 , ? = 2 , and ? = 10","single layer with h = 512 , ? = 0.5 , ? = 2 , and ? = 10",0.6361618638038635
translation,85,84,experimental-setup,experimental setup,has,final elman architecture ( ras - elman ),experimental setup has final elman architecture ( ras - elman ),0.5751346349716187
translation,85,85,experimental-setup,single layer,with,"h = 512 , ? = 0.1 , ? = 2 , and ? = 10","single layer with h = 512 , ? = 0.1 , ? = 2 , and ? = 10",0.6306379437446594
translation,85,85,experimental-setup,lstm model ( ras - lstm ),has,single layer,lstm model ( ras - lstm ) has single layer,0.5483065843582153
translation,85,85,experimental-setup,experimental setup,has,lstm model ( ras - lstm ),experimental setup has lstm model ( ras - lstm ),0.5562179088592529
translation,85,105,experimental-setup,two - layer lstms,for,encoder-decoder,two - layer lstms for encoder-decoder,0.6035832166671753
translation,85,105,experimental-setup,encoder-decoder,with,500 hidden units,encoder-decoder with 500 hidden units,0.6175428628921509
translation,85,105,experimental-setup,500 hidden units,in,each layer,500 hidden units in each layer,0.473770409822464
translation,85,6,model,conditioning,provided by,novel convolutional attention - based encoder,conditioning provided by novel convolutional attention - based encoder,0.6170819401741028
translation,85,6,model,novel convolutional attention - based encoder,ensures,decoder,novel convolutional attention - based encoder ensures decoder,0.6110138893127441
translation,85,6,model,decoder,focuses on,appropriate input words,decoder focuses on appropriate input words,0.7435775995254517
translation,85,6,model,appropriate input words,at,each step of generation,appropriate input words at each step of generation,0.5396422147750854
translation,85,6,model,model,has,conditioning,model has conditioning,0.5918759107589722
translation,85,7,model,model,relies only on,learned features,model relies only on learned features,0.6956142783164978
translation,85,7,model,easy to train,in,end-to - end fashion,easy to train in end-to - end fashion,0.5144137740135193
translation,85,7,model,end-to - end fashion,on,large data sets,end-to - end fashion on large data sets,0.5638494491577148
translation,85,7,model,model,relies only on,learned features,model relies only on learned features,0.6956142783164978
translation,85,7,model,model,has,model,model has model,0.5623406171798706
translation,85,15,model,conditional recurrent neural network,acts as,decoder,conditional recurrent neural network acts as decoder,0.7083132266998291
translation,85,15,model,decoder,to generate,summary,decoder to generate summary,0.6773025989532471
translation,85,15,model,summary,of,input sentence,summary of input sentence,0.5831270217895508
translation,85,15,model,model,consists of,conditional recurrent neural network,model consists of conditional recurrent neural network,0.6984977722167969
translation,85,16,model,decoder,takes,conditioning input,decoder takes conditioning input,0.6795623302459717
translation,85,16,model,conditioning input,output of,encoder module,conditioning input output of encoder module,0.7830355763435364
translation,85,16,model,every time step,has,decoder,every time step has decoder,0.6033068299293518
translation,85,16,model,model,at,every time step,model at every time step,0.5601000785827637
translation,85,23,model,convolutional network,to encode,input words,convolutional network to encode input words,0.7354938387870789
translation,85,26,results,our model,beats,state - of - the - art systems of rush et al . ( 2015 ),our model beats state - of - the - art systems of rush et al . ( 2015 ),0.65887051820755
translation,85,26,results,state - of - the - art systems of rush et al . ( 2015 ),on,multiple data sets,state - of - the - art systems of rush et al . ( 2015 ) on multiple data sets,0.5049147009849548
translation,85,26,results,results,show,our model,results show our model,0.6888449192047119
translation,85,27,results,our model,manages to,significantly outperform,our model manages to significantly outperform,0.7396254539489746
translation,85,27,results,significantly outperform,on,gigaword data set,significantly outperform on gigaword data set,0.5263587236404419
translation,85,27,results,abs + system,on,gigaword data set,abs + system on gigaword data set,0.5636676549911499
translation,85,27,results,comparable,on,duc - 2004 task,comparable on duc - 2004 task,0.5367845892906189
translation,85,27,results,significantly outperform,has,abs + system,significantly outperform has abs + system,0.6424977779388428
translation,85,92,results,ras - elman and ras - lstm models,achieve,lower perplexity,ras - elman and ras - lstm models achieve lower perplexity,0.6526766419410706
translation,85,92,results,results,shows,ras - elman and ras - lstm models,results shows ras - elman and ras - lstm models,0.6268220543861389
translation,85,93,results,ras - lstm,performs,slightly worse,ras - lstm performs slightly worse,0.603338897228241
translation,85,93,results,slightly worse,than,ras - elman,slightly worse than ras - elman,0.6357099413871765
translation,85,93,results,results,has,ras - lstm,results has ras - lstm,0.5305496454238892
translation,85,95,results,rouge,show,our models,rouge show our models,0.7289294004440308
translation,85,95,results,comfortably outperform,by,wide margin,comfortably outperform by wide margin,0.63189697265625
translation,85,95,results,abs +,by,wide margin,abs + by wide margin,0.5913633704185486
translation,85,95,results,wide margin,on,all metrics,wide margin on all metrics,0.5254324078559875
translation,85,95,results,our models,has,comfortably outperform,our models has comfortably outperform,0.5740536451339722
translation,85,95,results,results,has,rouge,results has rouge,0.624253511428833
translation,85,99,results,our models,better than,abs +,our models better than abs +,0.7560057044029236
translation,85,107,results,proposed ras - elman model,able to match,performance,proposed ras - elman model able to match performance,0.7135056853294373
translation,85,107,results,performance,of,nmt model,performance of nmt model,0.5789443850517273
translation,85,107,results,nmt model,of,luong at al . ( 2015 ),nmt model of luong at al . ( 2015 ),0.4985544979572296
translation,85,113,results,abs encoder,achieves,final validation perplexity,abs encoder achieves final validation perplexity,0.6690741181373596
translation,85,113,results,position features,as well as,context information,position features as well as context information,0.5765983462333679
translation,85,113,results,final validation perplexity,of,38,final validation perplexity of 38,0.588140606880188
translation,85,113,results,38,compared to,29,38 compared to 29,0.526441752910614
translation,85,113,results,29,for,proposed encoder,29 for proposed encoder,0.631085216999054
translation,85,113,results,proposed encoder,uses,position features,proposed encoder uses position features,0.6068693399429321
translation,85,113,results,position features,as well as,context information,position features as well as context information,0.5765983462333679
translation,85,113,results,results,has,abs encoder,results has abs encoder,0.5647791028022766
translation,86,76,baselines,encoderdecoder model ( enc-dec ),where,sequence of album photos,encoderdecoder model ( enc-dec ) where sequence of album photos,0.6338285207748413
translation,86,76,baselines,encoderdecoder model ( enc-dec ),where,last hidden state,encoderdecoder model ( enc-dec ) where last hidden state,0.624626874923706
translation,86,76,baselines,sequence of album photos,is,encoded,sequence of album photos is encoded,0.6027832627296448
translation,86,76,baselines,last hidden state,fed into,decoder,last hidden state fed into decoder,0.7099126577377319
translation,86,76,baselines,decoder,for,story generation,decoder for story generation,0.5875054597854614
translation,86,76,baselines,"encoder-attention - decoder model ( xu et al. , 2015 )",with,weights,"encoder-attention - decoder model ( xu et al. , 2015 ) with weights",0.6031926870346069
translation,86,76,baselines,enc-attn-dec ),with,weights,enc-attn-dec ) with weights,0.6824182271957397
translation,86,76,baselines,weights,computed using,soft-attention mechanism,weights computed using soft-attention mechanism,0.6592053174972534
translation,86,76,baselines,"encoder-attention - decoder model ( xu et al. , 2015 )",has,enc-attn-dec ),"encoder-attention - decoder model ( xu et al. , 2015 ) has enc-attn-dec )",0.5537546277046204
translation,86,5,model,natural language story,for,album,natural language story for album,0.5515698194503784
translation,86,5,model,model,Given,photo album,model Given photo album,0.7352473139762878
translation,86,29,model,model,of,hierarchically - attentive recurrent neural nets,model of hierarchically - attentive recurrent neural nets,0.5647174119949341
translation,86,29,model,hierarchically - attentive recurrent neural nets,consisting of,three rnn stages,hierarchically - attentive recurrent neural nets consisting of three rnn stages,0.7047317028045654
translation,86,29,model,model,propose,model,model propose model,0.6740307211875916
translation,86,29,model,model,of,hierarchically - attentive recurrent neural nets,model of hierarchically - attentive recurrent neural nets,0.5647174119949341
translation,86,77,model,each decoding time step,weighted sum of,hidden states,each decoding time step weighted sum of hidden states,0.6428310871124268
translation,86,77,model,hidden states,from,encoder,hidden states from encoder,0.6017034649848938
translation,86,77,model,model,At,each decoding time step,model At each decoding time step,0.547511875629425
translation,86,80,results,h-attnrank,achieves,best performance,h-attnrank achieves best performance,0.7190427184104919
translation,86,80,results,best performance,for,all metrics,best performance for all metrics,0.5839367508888245
translation,86,80,results,h-attn,has,outperforms,h-attn has outperforms,0.6742234826087952
translation,86,80,results,outperforms,has,both baselines,outperforms has both baselines,0.5924112200737
translation,86,80,results,results,has,h-attn,results has h-attn,0.5787779092788696
translation,86,93,results,our models,have,higher performance,our models have higher performance,0.5864570736885071
translation,86,93,results,higher performance,compared to,baselines,higher performance compared to baselines,0.6878988146781921
translation,86,93,results,results,has,our models,results has our models,0.5733726620674133
translation,86,99,results,our models,has,outperform,our models has outperform,0.6103132367134094
translation,86,99,results,outperform,has,baselines,outperform has baselines,0.6363358497619629
translation,86,99,results,ranking term,has,does not improve,ranking term has does not improve,0.6322385668754578
translation,86,99,results,does not improve,has,performance significantly,does not improve has performance significantly,0.5817248225212097
translation,86,99,results,results,find that,our models,results find that our models,0.6711100339889526
translation,88,88,hyperparameters,competing models,have,8 convolutional layers,competing models have 8 convolutional layers,0.5302094221115112
translation,88,88,hyperparameters,8 convolutional layers,in,encoder and decoder parts,8 convolutional layers in encoder and decoder parts,0.529470682144165
translation,88,88,hyperparameters,8 convolutional layers,both,encoder and decoder parts,8 convolutional layers both encoder and decoder parts,0.6397231221199036
translation,88,88,hyperparameters,kernel width,as,3,kernel width as 3,0.5795313715934753
translation,88,89,hyperparameters,each convolutional layer,set,hidden vector size,each convolutional layer set hidden vector size,0.6464511752128601
translation,88,89,hyperparameters,each convolutional layer,set,embedding size,each convolutional layer set embedding size,0.6254369020462036
translation,88,89,hyperparameters,hidden vector size,as,512,hidden vector size as 512,0.5653213262557983
translation,88,89,hyperparameters,embedding size,as,256,embedding size as 256,0.5829830765724182
translation,88,89,hyperparameters,hyperparameters,For,each convolutional layer,hyperparameters For each convolutional layer,0.4898409843444824
translation,88,90,hyperparameters,overfitting problem,add,dropout ( p = 0.2 ) layer,overfitting problem add dropout ( p = 0.2 ) layer,0.5930748581886292
translation,88,90,hyperparameters,overfitting problem,add,fully connected layers,overfitting problem add fully connected layers,0.5833036303520203
translation,88,90,hyperparameters,dropout ( p = 0.2 ) layer,for,all convolutional layers,dropout ( p = 0.2 ) layer for all convolutional layers,0.5092780590057373
translation,88,90,hyperparameters,dropout ( p = 0.2 ) layer,for,fully connected layers,dropout ( p = 0.2 ) layer for fully connected layers,0.5077263116836548
translation,88,90,hyperparameters,hyperparameters,To alleviate,overfitting problem,hyperparameters To alleviate overfitting problem,0.6375260949134827
translation,88,91,hyperparameters,proposed model,use,nesterov 's accelerated gradient method,proposed model use nesterov 's accelerated gradient method,0.6116406321525574
translation,88,91,hyperparameters,nesterov 's accelerated gradient method,with,gradient clipping,nesterov 's accelerated gradient method with gradient clipping,0.6116945147514343
translation,88,91,hyperparameters,nesterov 's accelerated gradient method,with,momentum,nesterov 's accelerated gradient method with momentum,0.6379576921463013
translation,88,91,hyperparameters,nesterov 's accelerated gradient method,with,learning rate,nesterov 's accelerated gradient method with learning rate,0.6168067455291748
translation,88,91,hyperparameters,gradient clipping,has,0.1,gradient clipping has 0.1,0.5122664570808411
translation,88,91,hyperparameters,momentum,has,0.99,momentum has 0.99,0.5715008974075317
translation,88,91,hyperparameters,learning rate,has,0.2,learning rate has 0.2,0.5293355584144592
translation,88,92,hyperparameters,training process,when,learning rate,training process when learning rate,0.6520711779594421
translation,88,92,hyperparameters,learning rate,drops below,10e - 5,learning rate drops below 10e - 5,0.7210347056388855
translation,88,92,hyperparameters,hyperparameters,terminate,training process,hyperparameters terminate training process,0.7151339054107666
translation,88,93,hyperparameters,beam size,as,5,beam size as 5,0.594243049621582
translation,88,93,hyperparameters,5,for,beam search algorithm,5 for beam search algorithm,0.5702725052833557
translation,88,93,hyperparameters,beam search algorithm,in,testing step,beam search algorithm in testing step,0.5233778357505798
translation,88,93,hyperparameters,hyperparameters,set,beam size,hyperparameters set beam size,0.6436296701431274
translation,88,6,model,summary length,by extending,convolutional sequence to sequence model,summary length by extending convolutional sequence to sequence model,0.7344890832901001
translation,88,31,model,information,propagated,layer by layer,information propagated layer by layer,0.7802780270576477
translation,88,31,model,layer by layer,during,training,layer by layer during training,0.7263900637626648
translation,88,31,model,model,has,information,model has information,0.5741810202598572
translation,88,113,results,other models,on,all of the evaluation metrics,other models on all of the evaluation metrics,0.45628103613853455
translation,88,113,results,proposed model ( lc ),has,outperforms,proposed model ( lc ) has outperforms,0.5993806719779968
translation,88,113,results,outperforms,has,other models,outperforms has other models,0.5875559449195862
translation,88,117,results,lc model,achieves,highest rouge and similarity scores,lc model achieves highest rouge and similarity scores,0.6807193160057068
translation,88,117,results,lc model,achieves,lowest variance,lc model achieves lowest variance,0.6741703748703003
translation,88,117,results,lc model,as,lowest variance,lc model as lowest variance,0.5265799760818481
translation,88,117,results,lc model,for generating,high quality summaries,lc model for generating high quality summaries,0.6871723532676697
translation,88,117,results,highest rouge and similarity scores,as,lowest variance,highest rouge and similarity scores as lowest variance,0.5007120966911316
translation,88,117,results,lowest variance,in,free and exact version,lowest variance in free and exact version,0.5347744226455688
translation,88,117,results,high quality summaries,under,length constraint,high quality summaries under length constraint,0.6080338358879089
translation,88,117,results,results,has,lc model,results has lc model,0.5337212085723877
translation,88,118,results,other comparable models,on,all evaluation metrics,other comparable models on all evaluation metrics,0.4809136688709259
translation,88,118,results,trunc version,has,lc model,trunc version has lc model,0.574296772480011
translation,88,118,results,lc model,has,outperforms,lc model has outperforms,0.6272022128105164
translation,88,118,results,outperforms,has,other comparable models,outperforms has other comparable models,0.5811851620674133
translation,88,118,results,results,In,trunc version,results In trunc version,0.5446739792823792
translation,88,127,results,cnn model,same R score as,lc model,cnn model same R score as lc model,0.6458168029785156
translation,88,127,results,cnn model,same R score as,higher p score,cnn model same R score as higher p score,0.7438569068908691
translation,88,127,results,cnn model,same R score as,lc model,cnn model same R score as lc model,0.6458168029785156
translation,88,127,results,higher p score,than,lc model,higher p score than lc model,0.5841885805130005
translation,88,128,results,cnn model,achieve,higher f score,cnn model achieve higher f score,0.6319003701210022
translation,88,128,results,higher f score,even,generated summary,higher f score even generated summary,0.6728056073188782
translation,88,128,results,generated summary,is,not good,generated summary is not good,0.6162188053131104
translation,88,128,results,higher f score,has,generated summary,higher f score has generated summary,0.5826050639152527
translation,88,128,results,results,see that,cnn model,results see that cnn model,0.6367156505584717
translation,88,168,results,clear advantage,over,other two models,clear advantage over other two models,0.6888782978057861
translation,88,168,results,other two models,in terms of,summary fluency,other two models in terms of summary fluency,0.7353166937828064
translation,88,168,results,lc model,has,clear advantage,lc model has clear advantage,0.5554870367050171
translation,88,168,results,results,shows,lc model,results shows lc model,0.6641848087310791
translation,89,158,baselines,neural abstractive methods,has,pg - original,neural abstractive methods has pg - original,0.5750125050544739
translation,89,158,baselines,neural abstractive methods,has,pg-mmr,neural abstractive methods has pg-mmr,0.6010891795158386
translation,89,158,baselines,baselines,has,neural abstractive methods,baselines has neural abstractive methods,0.5295031070709229
translation,89,161,baselines,pg - brnn model,is,pointergenerator implementation,pg - brnn model is pointergenerator implementation,0.5273175835609436
translation,89,161,baselines,pointergenerator implementation,from,opennmt,pointergenerator implementation from opennmt,0.47022727131843567
translation,89,161,baselines,baselines,has,pg - brnn model,baselines has pg - brnn model,0.5477190017700195
translation,89,7,experiments,multi-news,has,first large-scale mds news dataset,multi-news has first large-scale mds news dataset,0.5330525636672974
translation,89,169,hyperparameters,neural abstractive models,truncate,input articles,neural abstractive models truncate input articles,0.7723552584648132
translation,89,169,hyperparameters,input articles,to,500 tokens,input articles to 500 tokens,0.5786469578742981
translation,89,169,hyperparameters,each example,with,s source input documents,each example with s source input documents,0.5458173155784607
translation,89,169,hyperparameters,each example,take,first 500 / s tokens,each example take first 500 / s tokens,0.6449946165084839
translation,89,169,hyperparameters,first 500 / s tokens,from,each source document,first 500 / s tokens from each source document,0.5687988996505737
translation,89,169,hyperparameters,hyperparameters,For,neural abstractive models,hyperparameters For neural abstractive models,0.5737278461456299
translation,89,176,hyperparameters,hi-map model,applied,1 - layer bidirectional lstm network,hi-map model applied 1 - layer bidirectional lstm network,0.6147348880767822
translation,89,176,hyperparameters,1 - layer bidirectional lstm network,with,hidden state dimension,1 - layer bidirectional lstm network with hidden state dimension,0.5729789137840271
translation,89,176,hyperparameters,256,in,direction,256 in direction,0.6093701124191284
translation,89,176,hyperparameters,hidden state dimension,has,256,hidden state dimension has 256,0.5983082056045532
translation,89,176,hyperparameters,hyperparameters,For,hi-map model,hyperparameters For hi-map model,0.5695528984069824
translation,89,8,model,end-to- end model,incorporates,traditional extractive summarization model,end-to- end model incorporates traditional extractive summarization model,0.6824248433113098
translation,89,8,model,traditional extractive summarization model,with,standard sds model,traditional extractive summarization model with standard sds model,0.595437228679657
translation,89,8,model,model,propose,end-to- end model,model propose end-to- end model,0.6807149648666382
translation,89,36,model,hierarchical model,for,neural abstractive multi-document summarization,hierarchical model for neural abstractive multi-document summarization,0.5641283392906189
translation,89,36,model,module,calculates,sentence ranking scores,module calculates sentence ranking scores,0.6199665665626526
translation,89,36,model,sentence ranking scores,based on,relevancy and redundancy,sentence ranking scores based on relevancy and redundancy,0.6260130405426025
translation,89,36,model,model,propose,hierarchical model,model propose hierarchical model,0.6887688040733337
translation,89,37,model,sentence - level mmr scores,into,pointergenerator model,sentence - level mmr scores into pointergenerator model,0.5632743239402771
translation,89,37,model,sentence - level mmr scores,to adapt,attention weights,sentence - level mmr scores to adapt attention weights,0.6851710677146912
translation,89,37,model,attention weights,on,word-level,attention weights on word-level,0.5205072164535522
translation,89,37,model,model,integrate,sentence - level mmr scores,model integrate sentence - level mmr scores,0.6105648279190063
translation,89,188,results,pg - mmr,trained and tested on,multi-,pg - mmr trained and tested on multi-,0.7368833422660828
translation,89,188,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,89,188,results,outperforms,has,pg - mmr,outperforms has pg - mmr,0.6416040658950806
translation,89,188,results,results,has,our model,results has our model,0.5871725678443909
translation,89,189,results,much-improved model performances,trained and tested on,in- domain multi- news data,much-improved model performances trained and tested on in- domain multi- news data,0.6927065849304199
translation,89,189,results,results,see,much-improved model performances,results see much-improved model performances,0.5831188559532166
translation,89,190,results,transformer,performs,best,transformer performs best,0.6491952538490295
translation,89,190,results,best,in terms of,r - 1,best in terms of r - 1,0.70838862657547
translation,89,190,results,outperforms,on,r - 2 and r-su,outperforms on r - 2 and r-su,0.5806950926780701
translation,89,190,results,hi-map,has,outperforms,hi-map has outperforms,0.6217375993728638
translation,89,190,results,results,has,transformer,results has transformer,0.4226538836956024
translation,89,191,results,drop,in,performance,drop in performance,0.5523719787597656
translation,89,191,results,performance,between,"pgoriginal , and pg - mmr","performance between pgoriginal , and pg - mmr",0.6477681994438171
translation,89,191,results,results,notice,drop,results notice drop,0.6497618556022644
translation,89,204,results,human-written summaries,easily marked as,better,human-written summaries easily marked as better,0.6842077374458313
translation,89,204,results,better,than,other systems,better than other systems,0.6261990666389465
translation,89,204,results,results,has,human-written summaries,results has human-written summaries,0.5040163993835449
translation,89,206,results,statistically significant differences,found,our hi-map model,statistically significant differences found our hi-map model,0.61646568775177
translation,89,206,results,statistically significant differences,between,human summaries score,statistically significant differences between human summaries score,0.6397764086723328
translation,89,206,results,statistically significant differences,between,all other systems,statistically significant differences between all other systems,0.6199336647987366
translation,89,206,results,statistically significant differences,between,copytransformer,statistically significant differences between copytransformer,0.6410993933677673
translation,89,206,results,statistically significant differences,between,our hi-map model,statistically significant differences between our hi-map model,0.6476002335548401
translation,89,206,results,our hi-map model,compared to,pg - mmr,our hi-map model compared to pg - mmr,0.6582720875740051
translation,89,206,results,results,has,statistically significant differences,results has statistically significant differences,0.48790043592453003
translation,89,207,results,our hi-map model,performs,comparably,our hi-map model performs comparably,0.6184099316596985
translation,89,207,results,our hi-map model,performs,much better,our hi-map model performs much better,0.6128450036048889
translation,89,207,results,comparably,to,pg - mmr,comparably to pg - mmr,0.6096739768981934
translation,89,207,results,pg - mmr,on,informativeness and fluency,pg - mmr on informativeness and fluency,0.5486902594566345
translation,89,207,results,much better,in terms of,non-redundancy,much better in terms of non-redundancy,0.6884453892707825
translation,89,207,results,results,has,our hi-map model,results has our hi-map model,0.5378223657608032
translation,90,218,experiments,duc - 07,in terms of,rouge - 2 score,duc - 07 in terms of rouge - 2 score,0.6195324659347534
translation,90,218,experiments,duc - 07,has,our system,duc - 07 has our system,0.6257966160774231
translation,90,218,experiments,rouge - 2 score,has,our system,rouge - 2 score has our system,0.5743221640586853
translation,90,218,experiments,our system,has,outperforms,our system has outperforms,0.6423544883728027
translation,90,218,experiments,outperforms,has,pythy,outperforms has pythy,0.652926504611969
translation,90,218,experiments,outperforms,has,state - of - the - art supervised summarization system,outperforms has state - of - the - art supervised summarization system,0.5355358123779297
translation,90,4,model,class of submodular functions,meant for,document summarization tasks,class of submodular functions meant for document summarization tasks,0.649756908416748
translation,90,4,model,model,design,class of submodular functions,model design class of submodular functions,0.5802398324012756
translation,90,5,model,two terms,encourages,summary,two terms encourages summary,0.7286913990974426
translation,90,5,model,summary,representative of,corpus,summary representative of corpus,0.7300880551338196
translation,90,5,model,other,positively rewards,diversity,other positively rewards diversity,0.6686356663703918
translation,90,184,results,diversity reward function,achieves,comparable performance,diversity reward function achieves comparable performance,0.6589279770851135
translation,90,184,results,comparable performance,to,duc - 04 best system,comparable performance to duc - 04 best system,0.5350285172462463
translation,90,184,results,results,optimizing,diversity reward function,results optimizing diversity reward function,0.7274796962738037
translation,90,213,results,our system,performs,much better,our system performs much better,0.5920381546020508
translation,90,213,results,extractive system,has,our system,extractive system has our system,0.6234565377235413
translation,90,213,results,much better,has,8.38 % v.s. 7.67 % ),much better has 8.38 % v.s. 7.67 % ),0.5679184794425964
translation,90,213,results,results,Comparing to,extractive system,results Comparing to extractive system,0.7050690650939941
translation,90,214,results,best system,in,duc - 05,best system in duc - 05,0.5390563011169434
translation,90,214,results,our approach,has,outperforms,our approach has outperforms,0.6385829448699951
translation,90,214,results,outperforms,has,best system,outperforms has best system,0.6477405428886414
translation,90,214,results,duc - 05,has,significantly,duc - 05 has significantly,0.6819374561309814
translation,90,217,results,best system,in,duc - 06,best system in duc - 06,0.5452730059623718
translation,90,217,results,our system,has,outperforms,our system has outperforms,0.6423544883728027
translation,90,217,results,outperforms,has,best system,outperforms has best system,0.6477405428886414
translation,90,217,results,results,has,our system,results has our system,0.5954442024230957
translation,90,219,results,comparable performance,to,best duc -07 system,comparable performance to best duc -07 system,0.519106388092041
translation,91,101,ablation-analysis,huge impact,on,performance,huge impact on performance,0.5810632109642029
translation,91,101,ablation-analysis,ablation analysis,has,contextual embeddings,ablation analysis has contextual embeddings,0.5383515357971191
translation,91,102,ablation-analysis,word embeddings,in,our contextual matching model p cm,word embeddings in our contextual matching model p cm,0.5182980298995972
translation,91,102,ablation-analysis,drops significantly,to,below simple baselines,drops significantly to below simple baselines,0.6239398121833801
translation,91,102,ablation-analysis,word embeddings,has,summarization performance,word embeddings has summarization performance,0.5056267380714417
translation,91,102,ablation-analysis,our contextual matching model p cm,has,summarization performance,our contextual matching model p cm has summarization performance,0.5430111885070801
translation,91,102,ablation-analysis,summarization performance,has,drops significantly,summarization performance has drops significantly,0.5948440432548523
translation,91,102,ablation-analysis,ablation analysis,using,word embeddings,ablation analysis using word embeddings,0.644026517868042
translation,91,75,experiments,fluency language model component lm,is,task specific,fluency language model component lm is task specific,0.5657214522361755
translation,91,75,experiments,fluency language model component lm,pretrained on,corpus of summarizations,fluency language model component lm pretrained on corpus of summarizations,0.7465266585350037
translation,91,79,experiments,abstractive summarization,use,english gigaword data set,abstractive summarization use english gigaword data set,0.5338570475578308
translation,91,82,experiments,p fm,on,200k compressed sentences,p fm on 200k compressed sentences,0.5515954494476318
translation,91,82,experiments,p fm,test on,first 1000 pairs,p fm test on first 1000 pairs,0.7716880440711975
translation,91,82,experiments,200k compressed sentences,in,training set,200k compressed sentences in training set,0.47456786036491394
translation,91,82,experiments,first 1000 pairs,of,evaluation set,first 1000 pairs of evaluation set,0.5517444610595703
translation,91,85,experiments,abstractive summarization,use,k = 6,abstractive summarization use k = 6,0.623208224773407
translation,91,85,experiments,abstractive summarization,use,fixed embeddings,abstractive summarization use fixed embeddings,0.5401411652565002
translation,91,85,experiments,abstractive summarization,use,fixed embeddings,abstractive summarization use fixed embeddings,0.5401411652565002
translation,91,85,experiments,k = 6,in,candidate list,k = 6 in candidate list,0.5266732573509216
translation,91,85,experiments,fixed embeddings,at,bottom layer,fixed embeddings at bottom layer,0.5425665378570557
translation,91,85,experiments,bottom layer,of,elmo language model,bottom layer of elmo language model,0.5410701036453247
translation,91,85,experiments,elmo language model,for,similarity,elmo language model for similarity,0.5586162805557251
translation,91,97,experiments,extractive sentence summarization,achieves,good compression rate,extractive sentence summarization achieves good compression rate,0.6439989805221558
translation,91,97,experiments,our method,achieves,good compression rate,our method achieves good compression rate,0.6548342704772949
translation,91,97,experiments,extractive sentence summarization,has,our method,extractive sentence summarization has our method,0.5568775534629822
translation,91,97,experiments,significantly raises,has,previous unsupervised baseline,significantly raises has previous unsupervised baseline,0.5950798988342285
translation,91,71,hyperparameters,contextual matching model 's similarity function s,adopt,forward language model,contextual matching model 's similarity function s adopt forward language model,0.6273019313812256
translation,91,71,hyperparameters,forward language model,of,"elmo ( peters et al. , 2018 )","forward language model of elmo ( peters et al. , 2018 )",0.4789583384990692
translation,91,71,hyperparameters,tokens,to,corresponding hidden states,tokens to corresponding hidden states,0.5420293807983398
translation,91,71,hyperparameters,corresponding hidden states,in,sequence,corresponding hidden states in sequence,0.5521914958953857
translation,91,71,hyperparameters,three - layer representation,dimension,512,three - layer representation dimension 512,0.7027302384376526
translation,91,71,hyperparameters,hyperparameters,For,contextual matching model 's similarity function s,hyperparameters For contextual matching model 's similarity function s,0.6003371477127075
translation,91,76,hyperparameters,lstm model,with,2 layers,lstm model with 2 layers,0.5992031097412109
translation,91,76,hyperparameters,hidden size,set to,1024,hidden size set to 1024,0.740225613117218
translation,91,76,hyperparameters,hyperparameters,use,lstm model,hyperparameters use lstm model,0.5943251252174377
translation,91,77,hyperparameters,sgd,combined with,gradient clipping,sgd combined with gradient clipping,0.6714624166488647
translation,91,83,hyperparameters,generation,set,? = 0.11,generation set ? = 0.11,0.6856589317321777
translation,91,83,hyperparameters,generation,set,beam size,generation set beam size,0.6588505506515503
translation,91,83,hyperparameters,beam size,to,10,beam size to 10,0.6524515748023987
translation,91,83,hyperparameters,hyperparameters,For,generation,hyperparameters For generation,0.5751388072967529
translation,91,84,hyperparameters,source sentence,is,tokenized and lowercased,source sentence is tokenized and lowercased,0.5382839441299438
translation,91,84,hyperparameters,tokenized and lowercased,with,periods,tokenized and lowercased with periods,0.6860089898109436
translation,91,84,hyperparameters,hyperparameters,has,source sentence,hyperparameters has source sentence,0.5202075839042664
translation,91,4,model,unsupervised method,for,sentence summarization,unsupervised method for sentence summarization,0.5686126947402954
translation,91,4,model,model,propose,unsupervised method,model propose unsupervised method,0.7210264205932617
translation,91,15,model,generic pretrained language model,to enforce,contextual matching,generic pretrained language model to enforce contextual matching,0.6620897054672241
translation,91,15,model,contextual matching,between,sentence prefixes,contextual matching between sentence prefixes,0.6396883726119995
translation,91,15,model,model,utilize,generic pretrained language model,model utilize generic pretrained language model,0.5174095034599304
translation,91,16,model,smoothed problem specific target language model,to guide,fluency of the generation process,smoothed problem specific target language model to guide fluency of the generation process,0.6720322966575623
translation,91,16,model,model,use,smoothed problem specific target language model,model use smoothed problem specific target language model,0.5906357765197754
translation,91,17,model,model,in,product- of-experts objective,model in product- of-experts objective,0.4890291690826416
translation,91,72,model,lstms,associated with,generic unsupervised language model,lstms associated with generic unsupervised language model,0.6235076785087585
translation,91,72,model,generic unsupervised language model,trained on,large amount of text data,generic unsupervised language model trained on large amount of text data,0.6845180988311768
translation,91,72,model,model,has,bottom layer,model has bottom layer,0.5245927572250366
translation,91,72,model,model,has,above two layers,model has above two layers,0.5783659219741821
translation,91,73,model,elmo hidden states,to allow,our model,elmo hidden states to allow our model,0.6317938566207886
translation,91,73,model,our model,to generate,contextual embeddings,our model to generate contextual embeddings,0.6547375917434692
translation,91,73,model,contextual embeddings,for,efficient beam search,contextual embeddings for efficient beam search,0.551293671131134
translation,91,73,model,sequentially,for,efficient beam search,sequentially for efficient beam search,0.5752527117729187
translation,91,73,model,contextual embeddings,has,sequentially,contextual embeddings has sequentially,0.5626571774482727
translation,91,73,model,model,explicitly manage,elmo hidden states,model explicitly manage elmo hidden states,0.6674684286117554
translation,91,92,results,outperforms,take,first 75 characters or 8 words,outperforms take first 75 characters or 8 words,0.6837182641029358
translation,91,92,results,commonly used prefix baselines,take,first 75 characters or 8 words,commonly used prefix baselines take first 75 characters or 8 words,0.6328592300415039
translation,91,92,results,method,has,outperforms,method has outperforms,0.6569275856018066
translation,91,92,results,outperforms,has,commonly used prefix baselines,outperforms has commonly used prefix baselines,0.6008527278900146
translation,91,92,results,results,has,method,results has method,0.49327942728996277
translation,91,93,results,our system,achieves,comparable results,our system achieves comparable results,0.6677777767181396
translation,91,93,results,comparable results,to,wang and lee ( 2018 ),comparable results to wang and lee ( 2018 ),0.5334448218345642
translation,91,93,results,comparable results,to,system,comparable results to system,0.6174086332321167
translation,91,93,results,system,based on,gans and reinforcement training,system based on gans and reinforcement training,0.6967930197715759
translation,91,93,results,wang and lee ( 2018 ),has,system,wang and lee ( 2018 ) has system,0.6087290048599243
translation,91,93,results,results,has,our system,results has our system,0.5954442024230957
translation,92,241,ablation-analysis,language models,yields,statistically significant drop,language models yields statistically significant drop,0.7017458081245422
translation,92,241,ablation-analysis,statistically significant drop,in,ngram recall and f-measure,statistically significant drop in ngram recall and f-measure,0.5676724910736084
translation,92,241,ablation-analysis,ablation analysis,Removing,language models,ablation analysis Removing language models,0.680586576461792
translation,92,242,ablation-analysis,basic features,leads to,increase,basic features leads to increase,0.7206737399101257
translation,92,242,ablation-analysis,increase,in,unigram and bigram precision,increase in unigram and bigram precision,0.5388667583465576
translation,92,242,ablation-analysis,increase,in,bigram case,increase in bigram case,0.5689182877540588
translation,92,242,ablation-analysis,increase,both,unigram and bigram precision,increase both unigram and bigram precision,0.6532302498817444
translation,92,242,ablation-analysis,increase,in,bigram case,increase in bigram case,0.5689182877540588
translation,92,242,ablation-analysis,bigram case,to cause,statistically significant increase,bigram case to cause statistically significant increase,0.7215570211410522
translation,92,242,ablation-analysis,statistically significant increase,over,full model,statistically significant increase over full model,0.6223248243331909
translation,92,242,ablation-analysis,statistically significant increase,has,in f-measure,statistically significant increase has in f-measure,0.594063401222229
translation,92,242,ablation-analysis,ablation analysis,removing,basic features,ablation analysis removing basic features,0.7648657560348511
translation,92,245,ablation-analysis,language model and geographic relevance features,leads to,statistically significant drop,language model and geographic relevance features leads to statistically significant drop,0.6713652610778809
translation,92,245,ablation-analysis,statistically significant drop,in,rouge -1 f1 scores,statistically significant drop in rouge -1 f1 scores,0.5400364995002747
translation,92,245,ablation-analysis,ablation analysis,Removing,language model and geographic relevance features,ablation analysis Removing language model and geographic relevance features,0.745529055595398
translation,92,206,baselines,baselines,has,affinity propagation only ( ap ),baselines has affinity propagation only ( ap ),0.527141809463501
translation,92,7,model,"novel , disaster -specific features",for,salience prediction,"novel , disaster -specific features for salience prediction",0.5664903521537781
translation,92,7,model,geo-locations and language models,representing,language of disaster,geo-locations and language models representing language of disaster,0.7280335426330566
translation,92,7,model,model,use,"novel , disaster -specific features","model use novel , disaster -specific features",0.7002539038658142
translation,92,19,model,update summarization system,to track,events,update summarization system to track events,0.6763614416122437
translation,92,19,model,events,has,across time,events has across time,0.6060053110122681
translation,92,19,model,model,present,update summarization system,model present update summarization system,0.6721410155296326
translation,92,20,model,sentence salience,in,context,sentence salience in context,0.47136855125427246
translation,92,20,model,context,of,large-scale event,context of large-scale event,0.5342402458190918
translation,92,20,model,large-scale event,such as,disaster,large-scale event such as disaster,0.65345299243927
translation,92,21,results,salience,with,clustering,salience with clustering,0.6584807634353638
translation,92,21,results,salience,produces,more relevant summaries,salience produces more relevant summaries,0.6427972316741943
translation,92,21,results,more relevant summaries,compared to,baselines,more relevant summaries compared to baselines,0.6572028398513794
translation,92,21,results,baselines,using,clustering or relevance alone,baselines using clustering or relevance alone,0.6805648803710938
translation,92,21,results,results,combining,salience,results combining salience,0.5623089671134949
translation,92,223,results,samples,against,full summary of nuggets,samples against full summary of nuggets,0.6922552585601807
translation,92,223,results,full summary of nuggets,using,rouge,full summary of nuggets using rouge,0.6984342336654663
translation,92,225,results,ap +salience,maintains,performance,ap +salience maintains performance,0.6915242671966553
translation,92,225,results,performance,above,baselines,performance above baselines,0.6758511066436768
translation,92,225,results,results,has,ap +salience,results has ap +salience,0.5056923031806946
translation,92,229,results,ap+salience model,better able to find,salient updates,ap+salience model better able to find salient updates,0.6546685695648193
translation,92,229,results,ap+salience model,for,disaster domain,ap+salience model for disaster domain,0.5842065215110779
translation,92,229,results,salient updates,for,disaster domain,salient updates for disaster domain,0.6165948510169983
translation,92,229,results,results,has,ap+salience model,results has ap+salience model,0.5293555855751038
translation,92,230,results,ap+salience 's recall,not diminished by,high precision,ap+salience 's recall not diminished by high precision,0.7106536626815796
translation,92,230,results,ap+salience 's recall,remains,competitive,ap+salience 's recall remains competitive,0.6703523397445679
translation,92,230,results,competitive,with,ap,competitive with ap,0.7379793524742126
translation,92,230,results,results,has,ap+salience 's recall,results has ap+salience 's recall,0.5552189946174622
translation,92,235,results,summary,on,topic,summary on topic,0.592792272567749
translation,92,235,results,results,has,predicting salience,results has predicting salience,0.5351480841636658
translation,92,236,results,comprehensiveness,of,summaries,comprehensiveness of summaries,0.547878623008728
translation,92,236,results,summaries,has,ap,summaries has ap,0.6565032601356506
translation,92,236,results,summaries,has,outperforms,summaries has outperforms,0.6222266554832458
translation,92,236,results,ap,has,outperforms,ap has outperforms,0.6552500128746033
translation,92,236,results,outperforms,has,ap + salience,outperforms has ap + salience,0.6289403438568115
translation,92,238,results,ap +salience,achieves,best balance,ap +salience achieves best balance,0.6689342260360718
translation,92,238,results,best balance,of,two metrics,best balance of two metrics,0.5806683897972107
translation,92,238,results,results,has,ap +salience,results has ap +salience,0.5056923031806946
translation,93,94,baselines,baseline model,equipped with,pointer -mechanism,baseline model equipped with pointer -mechanism,0.5962383151054382
translation,93,94,baselines,baselines,equipped with,pointer -mechanism,baselines equipped with pointer -mechanism,0.6148351430892944
translation,93,108,baselines,baseline + pointer-mechanism,google claims to have,cracked,baseline + pointer-mechanism google claims to have cracked,0.730814516544342
translation,93,108,baselines,flummoxed,anyone who has tried to read,doctor 's note,flummoxed anyone who has tried to read doctor 's note,0.7156016230583191
translation,93,108,baselines,cracked,has,problem,cracked has problem,0.5796892642974854
translation,93,108,baselines,baselines,has,baseline + pointer-mechanism,baselines has baseline + pointer-mechanism,0.5515062808990479
translation,93,75,experimental-setup,"28,7226",has,training pairs,"28,7226 has training pairs",0.5423385500907898
translation,93,75,experimental-setup,"13,368",has,validation pairs,"13,368 has validation pairs",0.5523396730422974
translation,93,75,experimental-setup,experimental setup,use,cnn / daily mail dataset,experimental setup use cnn / daily mail dataset,0.6049708724021912
translation,93,76,experimental-setup,two 256 - dimensional lstms,for,bidirectional encoder,two 256 - dimensional lstms for bidirectional encoder,0.5772176384925842
translation,93,76,experimental-setup,one 256 - dimensional lstm,for,decoder,one 256 - dimensional lstm for decoder,0.594760000705719
translation,93,76,experimental-setup,experimental setup,use,two 256 - dimensional lstms,experimental setup use two 256 - dimensional lstms,0.5587409734725952
translation,93,76,experimental-setup,experimental setup,use,one 256 - dimensional lstm,experimental setup use one 256 - dimensional lstm,0.585636556148529
translation,93,79,experimental-setup,training and testing,truncate,text,training and testing truncate text,0.7591063380241394
translation,93,79,experimental-setup,training and testing,limit,length,training and testing limit length,0.7600962519645691
translation,93,79,experimental-setup,text,to,400 tokens,text to 400 tokens,0.6030781865119934
translation,93,79,experimental-setup,length,of,summary,length of summary,0.6146816611289978
translation,93,79,experimental-setup,summary,to,100 tokens,summary to 100 tokens,0.584796667098999
translation,93,79,experimental-setup,experimental setup,During,training and testing,experimental setup During training and testing,0.671187162399292
translation,93,80,experimental-setup,"adagrad ( duchi et al. , 2011 )",with,learning rate,"adagrad ( duchi et al. , 2011 ) with learning rate",0.53758704662323
translation,93,80,experimental-setup,"adagrad ( duchi et al. , 2011 )",with,initial accumulator value,"adagrad ( duchi et al. , 2011 ) with initial accumulator value",0.5564320087432861
translation,93,80,experimental-setup,initial accumulator value,of,0.1,initial accumulator value of 0.1,0.6011174917221069
translation,93,80,experimental-setup,learning rate,has,0.15,learning rate has 0.15,0.5453540682792664
translation,93,80,experimental-setup,experimental setup,train using,"adagrad ( duchi et al. , 2011 )","experimental setup train using adagrad ( duchi et al. , 2011 )",0.7028688788414001
translation,93,81,experimental-setup,batch size,set as,16,batch size set as 16,0.6806944012641907
translation,93,81,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,93,83,experimental-setup,prediction - guide mechanism,set,single - layer feed forward network,prediction - guide mechanism set single - layer feed forward network,0.6414172053337097
translation,93,83,experimental-setup,single - layer feed forward network,with,800 nodes,single - layer feed forward network with 800 nodes,0.650225818157196
translation,93,83,experimental-setup,experimental setup,for,prediction - guide mechanism,experimental setup for prediction - guide mechanism,0.5766371488571167
translation,93,87,experimental-setup,m,as,8,m as 8,0.675045907497406
translation,93,87,experimental-setup,mini-batch training,with,batch size,mini-batch training with batch size,0.6118678450584412
translation,93,87,experimental-setup,batch size,to be,16,batch size to be 16,0.6109192967414856
translation,93,87,experimental-setup,experimental setup,set,m,experimental setup set m,0.6836333870887756
translation,93,87,experimental-setup,experimental setup,adapt,mini-batch training,experimental setup adapt mini-batch training,0.726801335811615
translation,93,88,experimental-setup,network,trained with,"adadelta ( zeiler , 2012 )","network trained with adadelta ( zeiler , 2012 )",0.7723897695541382
translation,93,88,experimental-setup,experimental setup,has,network,experimental setup has network,0.5697093605995178
translation,93,89,experimental-setup,training,truncate,input tokens,training truncate input tokens,0.7330005764961243
translation,93,89,experimental-setup,training,limit,length,training limit length,0.7331231236457825
translation,93,89,experimental-setup,input tokens,to,400,input tokens to 400,0.6046611070632935
translation,93,89,experimental-setup,length,of,output summary,length of output summary,0.6028198599815369
translation,93,89,experimental-setup,output summary,to,100 tokens,output summary to 100 tokens,0.5618885159492493
translation,93,89,experimental-setup,output summary,to,120 tokens,output summary to 120 tokens,0.5705841183662415
translation,93,89,experimental-setup,100 tokens,for,training,100 tokens for training,0.6195148229598999
translation,93,89,experimental-setup,120 tokens,at,test time,120 tokens at test time,0.5349500179290771
translation,93,89,experimental-setup,experimental setup,During,training,experimental setup During training,0.6835477948188782
translation,93,89,experimental-setup,experimental setup,limit,length,experimental setup limit length,0.6948102712631226
translation,93,8,model,key information guide network ( kign ),encodes,keywords,key information guide network ( kign ) encodes keywords,0.6577143669128418
translation,93,8,model,keywords,to,key information representation,keywords to key information representation,0.45017388463020325
translation,93,8,model,model,introduce,key information guide network ( kign ),model introduce key information guide network ( kign ),0.6351674199104309
translation,93,9,model,prediction - guide mechanism,can obtain,long- term value,prediction - guide mechanism can obtain long- term value,0.5801350474357605
translation,93,9,model,long- term value,for,future decoding,long- term value for future decoding,0.618667721748352
translation,93,9,model,long- term value,to further guide,summary generation,long- term value to further guide summary generation,0.673173189163208
translation,93,9,model,model,use,prediction - guide mechanism,model use prediction - guide mechanism,0.68736732006073
translation,93,26,model,extractive method,to obtain,keywords,extractive method to obtain keywords,0.5843587517738342
translation,93,26,model,keywords,from,text,keywords from text,0.5043212175369263
translation,93,26,model,model,use,extractive method,model use extractive method,0.6806925535202026
translation,93,27,model,key information guide network ( kign ),encodes,keywords,key information guide network ( kign ) encodes keywords,0.6577143669128418
translation,93,27,model,key information guide network ( kign ),integrates it into,abstractive model,key information guide network ( kign ) integrates it into abstractive model,0.6925214529037476
translation,93,27,model,keywords,to,key information representation,keywords to key information representation,0.45017388463020325
translation,93,27,model,abstractive model,to guide,process of generation,abstractive model to guide process of generation,0.7153529524803162
translation,93,27,model,model,introduce,key information guide network ( kign ),model introduce key information guide network ( kign ),0.6351674199104309
translation,93,29,model,novel predictionguide mechanism,based on,he et al . ( 2017 ),novel predictionguide mechanism based on he et al . ( 2017 ),0.6563910245895386
translation,93,29,model,model,propose,novel predictionguide mechanism,model propose novel predictionguide mechanism,0.6927666068077087
translation,93,98,model,key information guide network and the prediction - guide mechanism,achieve,better performance,key information guide network and the prediction - guide mechanism achieve better performance,0.5828626751899719
translation,93,98,model,model,combining,key information guide network and the prediction - guide mechanism,model combining key information guide network and the prediction - guide mechanism,0.5630145072937012
translation,93,95,results,key information guide network scores,exceed,baseline model,key information guide network scores exceed baseline model,0.6288250088691711
translation,93,95,results,baseline model,equipped with,pointer-mechanism,baseline model equipped with pointer-mechanism,0.5962383151054382
translation,93,95,results,pointer-mechanism,by,"+ 1.3 rouge - 1 , +0.9 rouge - 2 , +1.0 rouge -l )","pointer-mechanism by + 1.3 rouge - 1 , +0.9 rouge - 2 , +1.0 rouge -l )",0.6131265163421631
translation,93,95,results,results,shows,key information guide network scores,results shows key information guide network scores,0.6898913979530334
translation,94,162,ablation-analysis,examination,of,system outputs,examination of system outputs,0.5687043070793152
translation,94,162,ablation-analysis,examination,reveal,"important topical words ( e.g. ,   $ 3 million   )","examination reveal important topical words ( e.g. ,   $ 3 million   )",0.6957914233207703
translation,94,162,ablation-analysis,system outputs,reveal,"important topical words ( e.g. ,   $ 3 million   )","system outputs reveal important topical words ( e.g. ,   $ 3 million   )",0.6903527975082397
translation,94,162,ablation-analysis,"important topical words ( e.g. ,   $ 3 million   )",frequently discussed in,document cluster,"important topical words ( e.g. ,   $ 3 million   ) frequently discussed in document cluster",0.636052668094635
translation,94,162,ablation-analysis,"important topical words ( e.g. ,   $ 3 million   )",crucial for determining,sentence redundancy,"important topical words ( e.g. ,   $ 3 million   ) crucial for determining sentence redundancy",0.6288288831710815
translation,94,6,model,novel similarity measure,inspired by,capsule networks,novel similarity measure inspired by capsule networks,0.7244144082069397
translation,94,7,model,redundancy,between,pair of sentences,redundancy between pair of sentences,0.6627935767173767
translation,94,7,model,pair of sentences,based on,surface form and semantic information,pair of sentences based on surface form and semantic information,0.6510360240936279
translation,94,22,model,extractive method,exploiting,determinantal point process ( dpp,extractive method exploiting determinantal point process ( dpp,0.737358808517456
translation,94,22,model,model,focus on,extractive method,model focus on extractive method,0.728560209274292
translation,94,28,model,"capsule net-works ( hinton et al. , 2018 )",to measure,pairwise sentence ( dis ) similarity,"capsule net-works ( hinton et al. , 2018 ) to measure pairwise sentence ( dis ) similarity",0.643887460231781
translation,94,28,model,"capsule net-works ( hinton et al. , 2018 )",leverage,dpp,"capsule net-works ( hinton et al. , 2018 ) leverage dpp",0.6782040596008301
translation,94,28,model,pairwise sentence ( dis ) similarity,leverage,dpp,pairwise sentence ( dis ) similarity leverage dpp,0.7054745554924011
translation,94,28,model,dpp,to obtain,set of diverse summary sentences,dpp to obtain set of diverse summary sentences,0.6051809787750244
translation,94,28,model,model,exploit,"capsule net-works ( hinton et al. , 2018 )","model exploit capsule net-works ( hinton et al. , 2018 )",0.7308587431907654
translation,94,37,model,novel method,to collect,large amount of sentence pairs,novel method to collect large amount of sentence pairs,0.6238371133804321
translation,94,37,model,deemed similar,for,summarization,deemed similar for summarization,0.6550994515419006
translation,94,37,model,model,describe,novel method,model describe novel method,0.6984125971794128
translation,94,41,model,dpp,set of,summary sentences,dpp set of summary sentences,0.69469153881073
translation,94,41,model,dpp,remain,diverse,dpp remain diverse,0.7021185755729675
translation,94,41,model,summary sentences,representative of,document set,summary sentences representative of document set,0.6565489768981934
translation,94,41,model,model,has,dpp,model has dpp,0.6116493940353394
translation,94,48,model,extractive method,for,multi-document summarization,extractive method for multi-document summarization,0.5790072679519653
translation,94,48,model,model,explore,extractive method,model explore extractive method,0.6921172142028809
translation,94,48,model,model,for,multi-document summarization,model for multi-document summarization,0.568421483039856
translation,94,159,model,dpp,optimizes,summary sentence selection,dpp optimizes summary sentence selection,0.7209476232528687
translation,94,159,model,summary sentence selection,to maximize,content coverage and diversity,summary sentence selection to maximize content coverage and diversity,0.6856479644775391
translation,94,159,model,content coverage and diversity,expressed as,squared volume,content coverage and diversity expressed as squared volume,0.4892922043800354
translation,94,159,model,model,has,dpp,model has dpp,0.6116493940353394
translation,94,23,results,dpp,trained on,small data,dpp trained on small data,0.7601816058158875
translation,94,23,results,results,has,dpp,results has dpp,0.41583016514778137
translation,94,43,results,our dpp system,with,improved similarity measure,our dpp system with improved similarity measure,0.6807411313056946
translation,94,43,results,improved similarity measure,performs,competitively,improved similarity measure performs competitively,0.603073000907898
translation,94,43,results,strong summarization baselines,on,benchmark datasets,strong summarization baselines on benchmark datasets,0.4592491388320923
translation,94,43,results,outperforming,has,strong summarization baselines,outperforming has strong summarization baselines,0.5726480484008789
translation,94,43,results,results,has,our dpp system,results has our dpp system,0.5812174677848816
translation,94,158,results,dpp methods,perform,superior,dpp methods perform superior,0.6052693128585815
translation,94,158,results,superior,to,extractive and abstractive baselines,superior to extractive and abstractive baselines,0.5740396976470947
translation,94,158,results,results,has,dpp methods,results has dpp methods,0.4381142258644104
translation,94,160,results,dpp system,with,combined similarity metrics,dpp system with combined similarity metrics,0.6507251262664795
translation,94,160,results,combined similarity metrics,yields,highest performance,combined similarity metrics yields highest performance,0.736560583114624
translation,94,160,results,highest performance,achieving,10.14 % and 10.13 % f-scores,highest performance achieving 10.14 % and 10.13 % f-scores,0.6447161436080933
translation,94,160,results,10.14 % and 10.13 % f-scores,on,duc -04 and tac - 11,10.14 % and 10.13 % f-scores on duc -04 and tac - 11,0.5441828966140747
translation,94,160,results,results,observe,dpp system,results observe dpp system,0.60793137550354
translation,94,165,results,lexrank,tends to extract,long and comprehensive sentences,lexrank tends to extract long and comprehensive sentences,0.7512571215629578
translation,94,165,results,long and comprehensive sentences,that yield,high graph centrality,long and comprehensive sentences that yield high graph centrality,0.5788452625274658
translation,94,165,results,results,observe,lexrank,results observe lexrank,0.5536436438560486
translation,94,182,results,capsnet,achieves,highest prediction accuracy,capsnet achieves highest prediction accuracy,0.6776946783065796
translation,94,182,results,capsnet,yields,similar performance,capsnet yields similar performance,0.7239468097686768
translation,94,182,results,highest prediction accuracy,of,94.8 %,highest prediction accuracy of 94.8 %,0.5249732732772827
translation,94,182,results,94.8 %,on,src-summ dataset,94.8 % on src-summ dataset,0.5212076902389526
translation,94,182,results,similar performance,on,snli,similar performance on snli,0.5663502812385559
translation,94,182,results,results,observe,capsnet,results observe capsnet,0.5613396167755127
translation,94,196,results,similarity scores,generated by,capsnet + srcsumm,similarity scores generated by capsnet + srcsumm,0.6129340529441833
translation,94,196,results,capsnet + srcsumm,are,more moderate,capsnet + srcsumm are more moderate,0.5776171088218689
translation,94,196,results,more moderate,comparing to,capsnet +snli and cosine,more moderate comparing to capsnet +snli and cosine,0.7334421277046204
translation,94,196,results,results,has,similarity scores,results has similarity scores,0.5321218967437744
translation,95,184,ablation-analysis,2 layers,for,encoder or decoder,2 layers for encoder or decoder,0.638599693775177
translation,95,184,ablation-analysis,2 layers,leads to,rapid performance degradation,2 layers leads to rapid performance degradation,0.6608190536499023
translation,95,184,ablation-analysis,ablation analysis,Employing,2 layers,ablation analysis Employing 2 layers,0.6655486822128296
translation,95,195,ablation-analysis,saliency -selection network,contributes to,notable improvement,saliency -selection network contributes to notable improvement,0.6915407180786133
translation,95,195,ablation-analysis,notable improvement,close to,best results,notable improvement close to best results,0.6666319966316223
translation,95,195,ablation-analysis,shallow layers,has,saliency -selection network,shallow layers has saliency -selection network,0.5335090160369873
translation,95,195,ablation-analysis,ablation analysis,In,shallow layers,ablation analysis In shallow layers,0.5346019268035889
translation,95,196,ablation-analysis,improvement,brought by,saliency -selection network,improvement brought by saliency -selection network,0.7155217528343201
translation,95,196,ablation-analysis,saliency -selection network,is,limited,saliency -selection network is limited,0.4878494143486023
translation,95,196,ablation-analysis,deeper layers,has,improvement,deeper layers has improvement,0.5854235291481018
translation,95,196,ablation-analysis,ablation analysis,for,deeper layers,ablation analysis for deeper layers,0.6026176810264587
translation,95,144,baselines,baselines,has,convs2s,baselines has convs2s,0.5836317539215088
translation,95,146,baselines,rougesal + ent ( rl ),address,main difficulties,rougesal + ent ( rl ) address main difficulties,0.6193821430206299
translation,95,146,baselines,main difficulties,via,reinforcement learning approach,main difficulties via reinforcement learning approach,0.6382391452789307
translation,95,146,baselines,reinforcement learning approach,with,two novel reward functions,reinforcement learning approach with two novel reward functions,0.6002077460289001
translation,95,146,baselines,baselines,has,rougesal + ent ( rl ),baselines has rougesal + ent ( rl ),0.6129201054573059
translation,95,158,baselines,explicit-selection,equips,selection layer,explicit-selection equips selection layer,0.65104079246521
translation,95,120,experimental-setup,word dictionary,shared by,documents and summaries,word dictionary shared by documents and summaries,0.7056471109390259
translation,95,120,experimental-setup,documents and summaries,contains,"50,000 most popular tokens","documents and summaries contains 50,000 most popular tokens",0.6144829392433167
translation,95,120,experimental-setup,"50,000 most popular tokens",in,documents,"50,000 most popular tokens in documents",0.49645930528640747
translation,95,120,experimental-setup,experimental setup,has,word dictionary,experimental setup has word dictionary,0.530780553817749
translation,95,121,experimental-setup,experimental setup,set,number of encoder / decoder layers n = 4,experimental setup set number of encoder / decoder layers n = 4,0.6463420391082764
translation,95,121,experimental-setup,experimental setup,set,number of heads h = 8,experimental setup set number of heads h = 8,0.6594496965408325
translation,95,122,experimental-setup,dimensions,of,signal representation d e and output d m,dimensions of signal representation d e and output d m,0.6336724758148193
translation,95,122,experimental-setup,dimensions,of,intermediate output d f,dimensions of intermediate output d f,0.6455422043800354
translation,95,122,experimental-setup,signal representation d e and output d m,set to,512,signal representation d e and output d m set to 512,0.7460377812385559
translation,95,122,experimental-setup,intermediate output d f,set to,2048,intermediate output d f set to 2048,0.703708291053772
translation,95,122,experimental-setup,experimental setup,has,dimensions,experimental setup has dimensions,0.4716714024543762
translation,95,123,experimental-setup,dropout rate,set to,0.8,dropout rate set to 0.8,0.6770637035369873
translation,95,123,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,95,124,experimental-setup,our model,in,py-torch 2 1.0,our model in py-torch 2 1.0,0.5530937910079956
translation,95,124,experimental-setup,experimental setup,implement,our model,experimental setup implement our model,0.6453344821929932
translation,95,125,experimental-setup,batch size,set to,4096,batch size set to 4096,0.7435297966003418
translation,95,125,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,95,126,experimental-setup,adam optimizer,to train,our model,adam optimizer to train our model,0.6696226000785828
translation,95,126,experimental-setup,our model,with,"? 1 = 0.9 , ? 2 = 0.998 and = 10 ?8","our model with ? 1 = 0.9 , ? 2 = 0.998 and = 10 ?8",0.643841564655304
translation,95,126,experimental-setup,experimental setup,use,adam optimizer,experimental setup use adam optimizer,0.5987385511398315
translation,95,127,experimental-setup,learning rate,varies,every step,learning rate varies every step,0.677341639995575
translation,95,127,experimental-setup,every step,with,noam decay strategy,every step with noam decay strategy,0.6825883388519287
translation,95,127,experimental-setup,warmup threshold,is,8000,warmup threshold is 8000,0.6112315058708191
translation,95,127,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,95,128,experimental-setup,maximum norm,of,gradient - clipping,maximum norm of gradient - clipping,0.5645514130592346
translation,95,128,experimental-setup,gradient - clipping,set to,2,gradient - clipping set to 2,0.6099936366081238
translation,95,128,experimental-setup,experimental setup,has,maximum norm,experimental setup has maximum norm,0.5250097513198853
translation,95,129,experimental-setup,our experiment,on,one machine,our experiment on one machine,0.5571319460868835
translation,95,129,experimental-setup,one machine,with,4 nvidia titan xp gpus,one machine with 4 nvidia titan xp gpus,0.5643370747566223
translation,95,129,experimental-setup,training process,lasts,"200,000 steps","training process lasts 200,000 steps",0.6726151704788208
translation,95,129,experimental-setup,experimental setup,conduct,our experiment,experimental setup conduct our experiment,0.6930578351020813
translation,95,130,experimental-setup,beam search algorithm,with,coverage technique,beam search algorithm with coverage technique,0.6299518942832947
translation,95,130,experimental-setup,beam search algorithm,with,coverage weight,beam search algorithm with coverage weight,0.6158782839775085
translation,95,130,experimental-setup,coverage technique,to generate,multiple summary candidates,coverage technique to generate multiple summary candidates,0.7252686023712158
translation,95,130,experimental-setup,multiple summary candidates,in,parallel,multiple summary candidates in parallel,0.5587599277496338
translation,95,130,experimental-setup,multiple summary candidates,to obtain,better results,multiple summary candidates to obtain better results,0.5773542523384094
translation,95,130,experimental-setup,coverage weight,set to,1,coverage weight set to 1,0.7181669473648071
translation,95,130,experimental-setup,beam search algorithm,has,coverage weight,beam search algorithm has coverage weight,0.5169598460197449
translation,95,130,experimental-setup,experimental setup,use,beam search algorithm,experimental setup use beam search algorithm,0.6239713430404663
translation,95,131,experimental-setup,shorter generated summaries,utilize,length penalty,shorter generated summaries utilize length penalty,0.5751463174819946
translation,95,131,experimental-setup,length penalty,during,process of inference,length penalty during process of inference,0.6845566630363464
translation,95,132,experimental-setup,length penalty parameter,to,0.9,length penalty parameter to 0.9,0.5358151793479919
translation,95,132,experimental-setup,length penalty parameter,to,5,length penalty parameter to 5,0.5680570006370544
translation,95,132,experimental-setup,experimental setup,set,beam size,experimental setup set beam size,0.5894079208374023
translation,95,132,experimental-setup,experimental setup,set,length penalty parameter,experimental setup set length penalty parameter,0.6040855050086975
translation,95,133,experimental-setup,generated summary,set to,35,generated summary set to 35,0.6753944158554077
translation,95,133,experimental-setup,batch size,for,inference,batch size for inference,0.6176421642303467
translation,95,133,experimental-setup,inference,set to,1,inference set to 1,0.701897919178009
translation,95,133,experimental-setup,experimental setup,minimum length of,generated summary,experimental setup minimum length of generated summary,0.6629172563552856
translation,95,133,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,95,5,model,transformerbased encoder-decoder framework,with,two novel extensions,transformerbased encoder-decoder framework with two novel extensions,0.6202554702758789
translation,95,5,model,two novel extensions,for,abstractive document summarization,two novel extensions for abstractive document summarization,0.5519561767578125
translation,95,5,model,model,propose,transformerbased encoder-decoder framework,model propose transformerbased encoder-decoder framework,0.6636936664581299
translation,95,6,model,documents,design,focus-attention mechanism,documents design focus-attention mechanism,0.6245971322059631
translation,95,6,model,focus-attention mechanism,incorporate it into,encoder,focus-attention mechanism incorporate it into encoder,0.6876912713050842
translation,95,6,model,documents,has,comprehensively,documents has comprehensively,0.5991742014884949
translation,95,6,model,model,to encode,documents,model to encode documents,0.7917993664741516
translation,95,7,model,gaussian focal bias,on,attention scores,gaussian focal bias on attention scores,0.5562605261802673
translation,95,7,model,attention scores,to enhance,perception of local context,attention scores to enhance perception of local context,0.6839959621429443
translation,95,8,model,salient information,design,independent saliency -selection network,salient information design independent saliency -selection network,0.5493985414505005
translation,95,8,model,independent saliency -selection network,manages,information flow,independent saliency -selection network manages information flow,0.5868458151817322
translation,95,8,model,information flow,from,encoder to decoder,information flow from encoder to decoder,0.626148521900177
translation,95,8,model,model,To distinguish,salient information,model To distinguish salient information,0.7080923914909363
translation,95,33,model,saliency -selection network,equipped in,encoder and decoder,saliency -selection network equipped in encoder and decoder,0.646044909954071
translation,95,33,model,focus-attention mechanism,where,learnable gaussian focal bias,focus-attention mechanism where learnable gaussian focal bias,0.5371644496917725
translation,95,33,model,learnable gaussian focal bias,employed as,regularization term,learnable gaussian focal bias employed as regularization term,0.6044430732727051
translation,95,33,model,regularization term,on,attention scores,regularization term on attention scores,0.5193407535552979
translation,95,33,model,model,design,novel focusattention mechanism,model design novel focusattention mechanism,0.6254633069038391
translation,95,33,model,model,To comprehensively encode,documents,model To comprehensively encode documents,0.7278580069541931
translation,95,34,model,focal bias,implicitly aggregates,attention,focal bias implicitly aggregates attention,0.8135292530059814
translation,95,34,model,attention,on,local continuous scopes,attention on local continuous scopes,0.5684243440628052
translation,95,34,model,local continuous scopes,to emphasize,corresponding part of document,local continuous scopes to emphasize corresponding part of document,0.638822615146637
translation,95,34,model,model,has,focal bias,model has focal bias,0.5284167528152466
translation,95,35,model,salient information,in,documents,salient information in documents,0.5154567360877991
translation,95,35,model,salient information,design,independent saliency -selection network,salient information design independent saliency -selection network,0.5493985414505005
translation,95,35,model,independent saliency -selection network,to manage,information flow,independent saliency -selection network to manage information flow,0.6437488198280334
translation,95,35,model,information flow,from,encoder to decoder explicitly,information flow from encoder to decoder explicitly,0.5918495059013367
translation,95,35,model,model,To distinguish,salient information,model To distinguish salient information,0.7080923914909363
translation,95,36,model,saliency -selection network,employs,gate mechanism,saliency -selection network employs gate mechanism,0.5268505811691284
translation,95,36,model,gate mechanism,to assign,salient score,gate mechanism to assign salient score,0.7237536907196045
translation,95,36,model,salient score,for,each token,salient score for each token,0.615241527557373
translation,95,36,model,each token,in,source documents,each token in source documents,0.5070821642875671
translation,95,36,model,model,has,saliency -selection network,model has saliency -selection network,0.5094941854476929
translation,95,145,model,explicit-selection,extend,basic seq2seq model,explicit-selection extend basic seq2seq model,0.6790158748626709
translation,95,145,model,basic seq2seq model,with,information selection layer,basic seq2seq model with information selection layer,0.5829028487205505
translation,95,145,model,information selection layer,to explicitly control,information flow,information selection layer to explicitly control information flow,0.7187296152114868
translation,95,145,model,model,has,explicit-selection,model has explicit-selection,0.56571364402771
translation,95,154,results,improvements,provided by,basic model,improvements provided by basic model,0.7043814659118652
translation,95,154,results,basic model,with,saliency -selection network,basic model with saliency -selection network,0.5470318794250488
translation,95,154,results,saliency -selection network,particularly lie in,rouge -1 f1 scores,saliency -selection network particularly lie in rouge -1 f1 scores,0.6209970116615295
translation,95,154,results,results,notice,improvements,results notice improvements,0.7752933502197266
translation,95,156,results,our basic model,capable of achieving,equivalent performance,our basic model capable of achieving equivalent performance,0.6944548487663269
translation,95,156,results,results,Comparing with,two classical rnn - based baselines words - 1 vt2k -temp- att,results Comparing with two classical rnn - based baselines words - 1 vt2k -temp- att,0.6371883153915405
translation,95,159,results,saliencyselection network,achieves,better performance,saliencyselection network achieves better performance,0.6404303908348083
translation,95,159,results,better performance,with the help of,stacked architecture,better performance with the help of stacked architecture,0.6334651112556458
translation,95,160,results,performance,of,reinforcement learning based model rougeesal + ent,performance of reinforcement learning based model rougeesal + ent,0.6115887761116028
translation,95,160,results,reinforcement learning based model rougeesal + ent,worse than,our model,reinforcement learning based model rougeesal + ent worse than our model,0.6817157864570618
translation,95,160,results,results,has,performance,results has performance,0.5972660779953003
translation,95,161,results,bottom - up summarization,combines,advantages,bottom - up summarization combines advantages,0.7609695792198181
translation,95,161,results,advantages,of,cnnbased model and rnn - based model,advantages of cnnbased model and rnn - based model,0.5693807601928711
translation,95,161,results,slightly inferior,to,our model,slightly inferior to our model,0.5749999284744263
translation,95,161,results,strongest baseline,has,bottom - up summarization,strongest baseline has bottom - up summarization,0.5382290482521057
translation,95,161,results,results,has,strongest baseline,results has strongest baseline,0.5895093679428101
translation,95,176,results,extensions,in,our model,extensions in our model,0.5636635422706604
translation,95,176,results,improve,capability of,document summarization,improve capability of document summarization,0.6286691427230835
translation,95,176,results,document summarization,from,qualitative perspectives,document summarization from qualitative perspectives,0.5216618776321411
translation,95,176,results,our model,has,improve,our model has improve,0.6233134865760803
translation,95,176,results,results,verify,extensions,results verify extensions,0.6901554465293884
translation,96,181,ablation-analysis,iterative process,involving,multiple structured attention layers,iterative process involving multiple structured attention layers,0.6504685878753662
translation,96,181,ablation-analysis,multiple structured attention layers,helps generate,better discourse trees,multiple structured attention layers helps generate better discourse trees,0.7536231279373169
translation,96,181,ablation-analysis,ablation analysis,observe,iterative process,ablation analysis observe iterative process,0.640690267086029
translation,96,182,baselines,sumo trees,against,left- and right - branching baseline,sumo trees against left- and right - branching baseline,0.6851208209991455
translation,96,182,baselines,left- and right - branching baseline,where,document,left- and right - branching baseline where document,0.6162264943122864
translation,96,182,baselines,document,is,trivially parsed,document is trivially parsed,0.6116610765457153
translation,96,182,baselines,trivially parsed,into,left- and right - branching tree,trivially parsed into left- and right - branching tree,0.5753545761108398
translation,96,182,baselines,left- and right - branching tree,forming,chain - like structure,left- and right - branching tree forming chain - like structure,0.7270007133483887
translation,96,182,baselines,baselines,compare,sumo trees,baselines compare sumo trees,0.6758547425270081
translation,96,111,experimental-setup,vocabulary size,set to,30k,vocabulary size set to 30k,0.7211837768554688
translation,96,111,experimental-setup,experimental setup,has,vocabulary size,experimental setup has vocabulary size,0.5189570188522339
translation,96,112,experimental-setup,initialized randomly,from,"n ( 0 , 0.01 )","initialized randomly from n ( 0 , 0.01 )",0.5664154291152954
translation,96,112,experimental-setup,experimental setup,used,300d word embeddings,experimental setup used 300d word embeddings,0.5385534763336182
translation,96,113,experimental-setup,hidden size,of,ffn,hidden size of ffn,0.6115601062774658
translation,96,113,experimental-setup,ffn,set to,512,ffn set to 512,0.7329817414283752
translation,96,113,experimental-setup,sentence - level transformer,has,6 layers,sentence - level transformer has 6 layers,0.5815714597702026
translation,96,113,experimental-setup,experimental setup,has,sentence - level transformer,experimental setup has sentence - level transformer,0.5339106321334839
translation,96,113,experimental-setup,experimental setup,has,hidden size,experimental setup has hidden size,0.5616794228553772
translation,96,114,experimental-setup,number of heads,in,mhatt,number of heads in mhatt,0.5765185356140137
translation,96,114,experimental-setup,number of heads,set to,4,number of heads set to 4,0.7330282926559448
translation,96,114,experimental-setup,experimental setup,has,number of heads,experimental setup has number of heads,0.5273870229721069
translation,96,115,experimental-setup,adam,used for,"training ( ? 1 = 0.9 , ? 2 = 0.999 )","adam used for training ( ? 1 = 0.9 , ? 2 = 0.999 )",0.6479362845420837
translation,96,115,experimental-setup,experimental setup,has,adam,experimental setup has adam,0.4964992105960846
translation,96,116,experimental-setup,learning rate schedule,with,warming - up,learning rate schedule with warming - up,0.6647449731826782
translation,96,116,experimental-setup,warming - up,on,"first 8,000 steps","warming - up on first 8,000 steps",0.5726715922355652
translation,96,116,experimental-setup,experimental setup,adopted,learning rate schedule,experimental setup adopted learning rate schedule,0.6658779382705688
translation,96,5,model,multi-root dependency tree,while predicting,output summary,multi-root dependency tree while predicting output summary,0.7106465101242065
translation,96,5,model,model,induces,multi-root dependency tree,model induces multi-root dependency tree,0.6892180442810059
translation,96,7,model,new iterative refinement algorithm,induces,trees,new iterative refinement algorithm induces trees,0.6781237125396729
translation,96,7,model,trees,through,repeatedly refining,trees through repeatedly refining,0.709635317325592
translation,96,7,model,structures,predicted by,previous iterations,structures predicted by previous iterations,0.7383081912994385
translation,96,7,model,repeatedly refining,has,structures,repeatedly refining has structures,0.6139981746673584
translation,96,7,model,model,design,new iterative refinement algorithm,model design new iterative refinement algorithm,0.6099743843078613
translation,96,42,model,new framework,uses,"structured attention ( kim et al. , 2017 )","new framework uses structured attention ( kim et al. , 2017 )",0.5315088629722595
translation,96,42,model,"structured attention ( kim et al. , 2017 )",as,objective and attention weights,"structured attention ( kim et al. , 2017 ) as objective and attention weights",0.4640422761440277
translation,96,42,model,"structured attention ( kim et al. , 2017 )",both,objective and attention weights,"structured attention ( kim et al. , 2017 ) both objective and attention weights",0.6019828915596008
translation,96,42,model,objective and attention weights,for,extractive summarization,objective and attention weights for extractive summarization,0.5946049690246582
translation,96,42,model,model,propose,new framework,model propose new framework,0.7318839430809021
translation,96,43,model,end-to-end,induces,document - level dependency trees,end-to-end induces document - level dependency trees,0.6341671347618103
translation,96,43,model,document - level dependency trees,while predicting,output summary,document - level dependency trees while predicting output summary,0.677482545375824
translation,96,43,model,model,trained,end-to-end,model trained end-to-end,0.7512461543083191
translation,96,44,model,new iterative structure refinement algorithm,learns to induce,document - level structures,new iterative structure refinement algorithm learns to induce document - level structures,0.7335166931152344
translation,96,44,model,document - level structures,through repeatedly refining,trees,document - level structures through repeatedly refining trees,0.6995761394500732
translation,96,44,model,trees,predicted by,previous iterations,trees predicted by previous iterations,0.7226955890655518
translation,96,44,model,complex trees,which go beyond,simple parent- child relations,complex trees which go beyond simple parent- child relations,0.7003254294395447
translation,96,44,model,model,design,new iterative structure refinement algorithm,model design new iterative structure refinement algorithm,0.6038241982460022
translation,96,59,model,fundamental constraint,of,sequential computation,fundamental constraint of sequential computation,0.5614616870880127
translation,96,84,model,word representations,updated at,every layer,word representations updated at every layer,0.528455376625061
translation,96,84,model,every layer,based on,output of previous layers,every layer based on output of previous layers,0.647942841053009
translation,96,84,model,output of previous layers,refine,tree structure,output of previous layers refine tree structure,0.6558961868286133
translation,96,84,model,tree structure,during,each iteration,tree structure during each iteration,0.7150992155075073
translation,96,84,model,model,refine,tree structure,model refine tree structure,0.6959791779518127
translation,96,124,model,transformer,without,document- level self-attention,transformer without document- level self-attention,0.718853771686554
translation,96,124,model,two variants,with,document - level self attention,two variants with document - level self attention,0.634145200252533
translation,96,124,model,model,include,transformer,model include transformer,0.6446012854576111
translation,96,124,model,model,include,two variants,model include two variants,0.635924756526947
translation,96,222,model,structured summarization model,induces,multi-root dependency tree,structured summarization model induces multi-root dependency tree,0.6333796381950378
translation,96,222,model,multi-root dependency tree,of,document,multi-root dependency tree of document,0.5571039915084839
translation,96,222,model,multi-root dependency tree,where,roots,multi-root dependency tree where roots,0.6201242804527283
translation,96,222,model,multi-root dependency tree,where,subtrees,multi-root dependency tree where subtrees,0.6151240468025208
translation,96,222,model,roots,are,summary - worthy sentences,roots are summary - worthy sentences,0.5802257657051086
translation,96,222,model,sentences,elaborate or explain,summary content,sentences elaborate or explain summary content,0.6819239258766174
translation,96,222,model,sumo,has,structured summarization model,sumo has structured summarization model,0.5695035457611084
translation,96,222,model,model,present,sumo,model present sumo,0.7064717411994934
translation,96,223,model,sumo,generates,complex trees,sumo generates complex trees,0.6309199333190918
translation,96,223,model,complex trees,following,iterative refinement process,complex trees following iterative refinement process,0.6509524583816528
translation,96,223,model,latent structures,while using,information,latent structures while using information,0.7626547813415527
translation,96,223,model,information,learned in,previous iterations,information learned in previous iterations,0.7013232707977295
translation,96,223,model,model,has,sumo,model has sumo,0.6074984669685364
translation,96,133,results,sumo,observe,outperforms,sumo observe outperforms,0.633990466594696
translation,96,133,results,simple transformer model,without,any document attention,simple transformer model without any document attention,0.7414580583572388
translation,96,133,results,simple transformer model,without,document attention,simple transformer model without document attention,0.7083620429039001
translation,96,133,results,variants,with,document attention,variants with document attention,0.6315893530845642
translation,96,133,results,outperforms,has,simple transformer model,outperforms has simple transformer model,0.589218020439148
translation,96,134,results,sumo,with,three layers of structured attention,sumo with three layers of structured attention,0.6274152994155884
translation,96,134,results,three layers of structured attention,performs,best,three layers of structured attention performs best,0.6034930944442749
translation,96,134,results,results,has,sumo,results has sumo,0.4405442476272583
translation,96,135,results,sumo and all transformer - based models,with,document attention ( doc - att ),sumo and all transformer - based models with document attention ( doc - att ),0.6459482312202454
translation,96,135,results,lead - 3,across,metrics,lead - 3 across metrics,0.7141984105110168
translation,96,135,results,sumo and all transformer - based models,has,outperform,sumo and all transformer - based models has outperform,0.6017013192176819
translation,96,135,results,document attention ( doc - att ),has,outperform,document attention ( doc - att ) has outperform,0.6298055648803711
translation,96,135,results,outperform,has,lead - 3,outperform has lead - 3,0.6765320897102356
translation,96,135,results,results,reveal,sumo and all transformer - based models,results reveal sumo and all transformer - based models,0.5991082787513733
translation,96,136,results,sumo ( 3 - layer ),is,competitive or better,sumo ( 3 - layer ) is competitive or better,0.554908812046051
translation,96,136,results,competitive or better,than,stateof - the - art approaches,competitive or better than stateof - the - art approaches,0.5671460628509521
translation,96,136,results,results,has,sumo ( 3 - layer ),results has sumo ( 3 - layer ),0.5709001421928406
translation,96,153,results,summaries,generated by,sumo,summaries generated by sumo,0.7248100638389587
translation,96,153,results,of questions,on,cnn / dailymail,of questions on cnn / dailymail,0.5298166275024414
translation,96,153,results,correctly,on,cnn / dailymail,correctly on cnn / dailymail,0.5508091449737549
translation,96,153,results,57.2 %,on,nyt,57.2 % on nyt,0.5691571831703186
translation,96,153,results,answer,has,65.3 %,answer has 65.3 %,0.5529038310050964
translation,96,153,results,answer,has,of questions,answer has of questions,0.6006631255149841
translation,96,153,results,65.3 %,has,of questions,65.3 % has of questions,0.5538132190704346
translation,96,153,results,of questions,has,correctly,of questions has correctly,0.6072853803634644
translation,96,153,results,results,on,summaries,results on summaries,0.5377346873283386
translation,96,183,results,sumo,has,outperforms,sumo has outperforms,0.6101767420768738
translation,96,183,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
translation,96,183,results,results,has,sumo,results has sumo,0.4405442476272583
translation,97,5,model,novel and non-redundant community answer summary,segment,complex original multi-sentence question,novel and non-redundant community answer summary segment complex original multi-sentence question,0.655586838722229
translation,97,5,model,novel and non-redundant community answer summary,propose,general conditional random field ( crf ) based answer summary method,novel and non-redundant community answer summary propose general conditional random field ( crf ) based answer summary method,0.6028537750244141
translation,97,5,model,complex original multi-sentence question,into,several sub questions,complex original multi-sentence question into several sub questions,0.548349916934967
translation,97,5,model,general conditional random field ( crf ) based answer summary method,with,group l 1 regularization,general conditional random field ( crf ) based answer summary method with group l 1 regularization,0.5717226266860962
translation,97,5,model,model,to automatically generate,novel and non-redundant community answer summary,model to automatically generate novel and non-redundant community answer summary,0.7064654231071472
translation,97,7,model,four different types of contextual factors,namely,information novelty,four different types of contextual factors namely information novelty,0.675830066204071
translation,97,7,model,four different types of contextual factors,namely,non-redundancy modeling,four different types of contextual factors namely non-redundancy modeling,0.680532693862915
translation,97,7,model,non-redundancy modeling,for,local and non-local sentence interactions,non-redundancy modeling for local and non-local sentence interactions,0.6144505143165588
translation,97,7,model,local and non-local sentence interactions,under,question segmentation,local and non-local sentence interactions under question segmentation,0.6060905456542969
translation,97,7,model,model,explore,four different types of contextual factors,model explore four different types of contextual factors,0.6775774359703064
translation,97,8,model,potential,of,abundant cqa features,potential of abundant cqa features,0.5915433168411255
translation,97,8,model,potential,introduce,group l 1 regularization,potential introduce group l 1 regularization,0.6262749433517456
translation,97,8,model,abundant cqa features,introduce,group l 1 regularization,abundant cqa features introduce group l 1 regularization,0.6329022645950317
translation,97,8,model,group l 1 regularization,for,feature learning,group l 1 regularization for feature learning,0.5764145255088806
translation,97,8,model,model,To further unleash,potential,model To further unleash potential,0.7510063648223877
translation,97,8,model,model,introduce,group l 1 regularization,model introduce group l 1 regularization,0.6286295652389526
translation,97,49,model,answer summary task,as,sequential labeling process,answer summary task as sequential labeling process,0.5021637082099915
translation,97,49,model,sequential labeling process,under,general conditional random fields ( crf ) framework,sequential labeling process under general conditional random fields ( crf ) framework,0.5998361110687256
translation,97,49,model,sentences,with,summary label,sentences with summary label,0.6318958401679993
translation,97,49,model,summary label,to form,final summarized answer,summary label to form final summarized answer,0.7033214569091797
translation,97,49,model,model,tackle,answer summary task,model tackle answer summary task,0.7223578095436096
translation,97,50,model,general crf based framework,incorporate,four different contextual factors,general crf based framework incorporate four different contextual factors,0.6584550142288208
translation,97,50,model,four different contextual factors,based on,question segmentation,four different contextual factors based on question segmentation,0.6454129815101624
translation,97,50,model,four different contextual factors,to model,local and non-local semantic sentence interactions,four different contextual factors to model local and non-local semantic sentence interactions,0.6646714806556702
translation,97,50,model,local and non-local semantic sentence interactions,problem of,redundancy and information novelty,local and non-local semantic sentence interactions problem of redundancy and information novelty,0.7305364012718201
translation,97,50,model,model,present,general crf based framework,model present general crf based framework,0.618446409702301
translation,97,50,model,model,incorporate,four different contextual factors,model incorporate four different contextual factors,0.6650921106338501
translation,97,52,model,group l 1 - regularization approach,in,crf model,group l 1 - regularization approach in crf model,0.5102599859237671
translation,97,52,model,crf model,for,automatic optimal feature learning,crf model for automatic optimal feature learning,0.6243454217910767
translation,97,52,model,automatic optimal feature learning,to unleash,potential,automatic optimal feature learning to unleash potential,0.6673277616500854
translation,97,52,model,automatic optimal feature learning,enhance,performance,automatic optimal feature learning enhance performance,0.6145137548446655
translation,97,52,model,performance,of,answer summarization,performance of answer summarization,0.5872815251350403
translation,97,52,model,model,propose,group l 1 - regularization approach,model propose group l 1 - regularization approach,0.6652643084526062
translation,98,94,results,paired t-test and wilcoxon signed - rank test,offer,good improvement,paired t-test and wilcoxon signed - rank test offer good improvement,0.7131397128105164
translation,98,94,results,good improvement,over,unpaired t-test,good improvement over unpaired t-test,0.6370864510536194
translation,99,158,ablation-analysis,k,becomes,smaller,k becomes smaller,0.7668942213058472
translation,99,158,ablation-analysis,metrics,de-correlate with,humans,metrics de-correlate with humans,0.7102124691009521
translation,99,158,ablation-analysis,metrics,getting,negative correlations,metrics getting negative correlations,0.6291031837463379
translation,99,158,ablation-analysis,humans,on,tac - 2008 and cnndm mix datasets,humans on tac - 2008 and cnndm mix datasets,0.5396774411201477
translation,99,158,ablation-analysis,negative correlations,for,small values of k,negative correlations for small values of k,0.6319345235824585
translation,99,158,ablation-analysis,smaller,has,metrics,smaller has metrics,0.5945099592208862
translation,99,158,ablation-analysis,ablation analysis,As,k,ablation analysis As k,0.6100401282310486
translation,99,159,ablation-analysis,"sms , r -1 , r - 2 and r-l",improve in,performance,"sms , r -1 , r - 2 and r-l improve in performance",0.7138051986694336
translation,99,159,ablation-analysis,performance,as,k,performance as k,0.6085526347160339
translation,99,159,ablation-analysis,becomes smaller,on,cnndm ext.,becomes smaller on cnndm ext.,0.6276401281356812
translation,99,159,ablation-analysis,human judgments,on,tac - 2009,human judgments on tac - 2009,0.5797601938247681
translation,99,159,ablation-analysis,human judgments,for,k < 50,human judgments for k < 50,0.6396744847297668
translation,99,159,ablation-analysis,tac - 2009,for,k < 50,tac - 2009 for k < 50,0.6844600439071655
translation,99,159,ablation-analysis,k,has,becomes smaller,k has becomes smaller,0.6396690011024475
translation,99,159,ablation-analysis,cnndm ext.,has,r - 2,cnndm ext. has r - 2,0.6115647554397583
translation,99,159,ablation-analysis,ablation analysis,has,"sms , r -1 , r - 2 and r-l","ablation analysis has sms , r -1 , r - 2 and r-l",0.5461118221282959
translation,99,57,baselines,cnn-lstm - biclassifier,has,clstm -sl,cnn-lstm - biclassifier has clstm -sl,0.5844939351081848
translation,99,57,baselines,latent,has,"zhang et al. , 2018 )","latent has zhang et al. , 2018 )",0.5640152096748352
translation,99,57,baselines,refresh,has,"narayan et al. , 2018 )","refresh has narayan et al. , 2018 )",0.5560245513916016
translation,99,63,baselines,bertscore ( bscore ),measures,soft overlap,bertscore ( bscore ) measures soft overlap,0.4845775067806244
translation,99,63,baselines,soft overlap,between,contextual bert embeddings,soft overlap between contextual bert embeddings,0.6277748346328735
translation,99,63,baselines,contextual bert embeddings,of,tokens,contextual bert embeddings of tokens,0.5495235323905945
translation,99,63,baselines,contextual bert embeddings,between,two texts,contextual bert embeddings between two texts,0.6239163875579834
translation,99,63,baselines,moverscore ( mscore ),applies,distance measure,moverscore ( mscore ) applies distance measure,0.6221174597740173
translation,99,63,baselines,distance measure,to,contextualized bert and elmo word embeddings,distance measure to contextualized bert and elmo word embeddings,0.5130512714385986
translation,99,63,baselines,sentence mover similarity ( sms ),applies,minimum distance matching,sentence mover similarity ( sms ) applies minimum distance matching,0.5914536118507385
translation,99,63,baselines,minimum distance matching,between,text,minimum distance matching between text,0.6458510756492615
translation,99,63,baselines,minimum distance matching,between,texts,minimum distance matching between texts,0.6069684028625488
translation,99,63,baselines,minimum distance matching,between,texts,minimum distance matching between texts,0.6069684028625488
translation,99,63,baselines,text,based on,sentence embeddings,text based on sentence embeddings,0.6018992066383362
translation,99,63,baselines,word mover similarity ( wms ),measures,similarity,word mover similarity ( wms ) measures similarity,0.5199268460273743
translation,99,63,baselines,similarity,using,minimum distance matching,similarity using minimum distance matching,0.6831327676773071
translation,99,63,baselines,texts,represented as,bag of word embeddings,texts represented as bag of word embeddings,0.5807313323020935
translation,99,63,baselines,baselines,has,bertscore ( bscore ),baselines has bertscore ( bscore ),0.536984920501709
translation,99,25,experiments,rouge - 2,highly correlates with,humans,rouge - 2 highly correlates with humans,0.7358424663543701
translation,99,25,experiments,extractive and abstractive systems,has,rouge - 2,extractive and abstractive systems has rouge - 2,0.6479732394218445
translation,99,59,experiments,fastabsrl,has,"chen and bansal , 2018 )","fastabsrl has chen and bansal , 2018 )",0.6055465340614319
translation,99,59,experiments,fastabsrlrank,has,"chen and bansal , 2018 )","fastabsrlrank has chen and bansal , 2018 )",0.5900728702545166
translation,99,59,experiments,pre-summabs,has,"liu and lapata , 2019 b )","pre-summabs has liu and lapata , 2019 b )",0.6075493693351746
translation,99,21,results,rouge - 2,has,outperforms,rouge - 2 has outperforms,0.6198058724403381
translation,99,21,results,outperforms,has,all other metrics,outperforms has all other metrics,0.5665997862815857
translation,99,21,results,results,has,rouge - 2,results has rouge - 2,0.609738290309906
translation,99,22,results,moverscore and js - 2,performs,worse,moverscore and js - 2 performs worse,0.6909967660903931
translation,99,22,results,worse,both in,extractive,worse both in extractive,0.7081075310707092
translation,99,22,results,worse,both in,abstractive summaries,worse both in abstractive summaries,0.648443341255188
translation,99,22,results,results,has,moverscore and js - 2,results has moverscore and js - 2,0.5451124906539917
translation,99,27,results,exp - iii,has,moverscore and js - 2,exp - iii has moverscore and js - 2,0.6151396036148071
translation,99,27,results,moverscore and js - 2,has,outperform,moverscore and js - 2 has outperform,0.6136937141418457
translation,99,27,results,outperform,has,all other metrics,outperform has all other metrics,0.5645695328712463
translation,99,27,results,results,has,exp - iii,results has exp - iii,0.5395970940589905
translation,99,28,results,rouge - 2,is,most reliable,rouge - 2 is most reliable,0.5791174173355103
translation,99,28,results,rouge - 2,is,most reliable,rouge - 2 is most reliable,0.5791174173355103
translation,99,28,results,most reliable,for,abstractive systems,most reliable for abstractive systems,0.5836732387542725
translation,99,28,results,most reliable,for,extractive systems,most reliable for extractive systems,0.6116257905960083
translation,99,28,results,most reliable,for,extractive systems,most reliable for extractive systems,0.6116257905960083
translation,99,28,results,results,has,rouge - 2,results has rouge - 2,0.609738290309906
translation,99,29,results,exp-iv,has,moverscore and js - 2,exp-iv has moverscore and js - 2,0.6108837723731995
translation,99,29,results,moverscore and js - 2,has,outperform,moverscore and js - 2 has outperform,0.6136937141418457
translation,99,29,results,outperform,has,all other metrics,outperform has all other metrics,0.5645695328712463
translation,99,29,results,results,has,exp-iv,results has exp-iv,0.5363410115242004
translation,99,31,results,rouge metrics,has,outperform,rouge metrics has outperform,0.6169657707214355
translation,99,31,results,outperform,has,all other metrics,outperform has all other metrics,0.5645695328712463
translation,99,31,results,results,has,rouge metrics,results has rouge metrics,0.540330708026886
translation,99,32,results,summaries,than,systems,summaries than systems,0.6549093723297119
translation,99,32,results,extractive summaries,has,most metrics,extractive summaries has most metrics,0.5198462009429932
translation,99,32,results,results,For,extractive summaries,results For extractive summaries,0.5868006348609924
translation,99,33,results,some metrics,better at,summary level,some metrics better at summary level,0.6337906122207642
translation,99,33,results,some metrics,better at,system level,some metrics better at system level,0.6600823998451233
translation,99,33,results,abstractive summaries,has,some metrics,abstractive summaries has some metrics,0.5646914839744568
translation,99,33,results,results,For,abstractive summaries,results For abstractive summaries,0.5401402115821838
translation,99,138,results,results,has,exp-i : evaluating all systems,results has exp-i : evaluating all systems,0.5821250677108765
translation,99,143,results,most metrics,have,high correlations,most metrics have high correlations,0.5569764971733093
translation,99,143,results,high correlations,on,tac - 2008 dataset,high correlations on tac - 2008 dataset,0.5552579164505005
translation,99,143,results,tac - 2009,especially,rouge based metrics,tac - 2009 especially rouge based metrics,0.6215018033981323
translation,99,143,results,results,has,most metrics,results has most metrics,0.49889275431632996
translation,99,144,results,rouge metrics,consistently perform,well,rouge metrics consistently perform well,0.7538970708847046
translation,99,144,results,well,on,collected cnndm datasets,well on collected cnndm datasets,0.5348122119903564
translation,99,144,results,results,has,rouge metrics,results has rouge metrics,0.540330708026886
translation,99,147,results,moverscore and js - 2,are,significantly better,moverscore and js - 2 are significantly better,0.5940232276916504
translation,99,147,results,significantly better,than,other metrics,significantly better than other metrics,0.5308383107185364
translation,99,147,results,other metrics,in correlating with,human judgments,other metrics in correlating with human judgments,0.6149107813835144
translation,99,147,results,human judgments,on,tac datasets,human judgments on tac datasets,0.5115826725959778
translation,99,147,results,results,find that,moverscore and js - 2,results find that moverscore and js - 2,0.6167930960655212
translation,99,148,results,r - 2,on,cnndm,r - 2 on cnndm,0.6101835370063782
translation,99,148,results,cnndm abs and cnndm mix,has,r - 2,cnndm abs and cnndm mix has r - 2,0.6637261509895325
translation,99,148,results,cnndm abs and cnndm mix,has,significantly outperforms,cnndm abs and cnndm mix has significantly outperforms,0.617106020450592
translation,99,148,results,r - 2,has,significantly outperforms,r - 2 has significantly outperforms,0.6124379634857178
translation,99,148,results,significantly outperforms,has,all others,significantly outperforms has all others,0.5907230377197266
translation,99,148,results,results,on,cnndm abs and cnndm mix,results on cnndm abs and cnndm mix,0.5629024505615234
translation,99,149,results,ext,show,significant improvements,ext show significant improvements,0.7192124128341675
translation,99,149,results,none of the metrics,show,significant improvements,none of the metrics show significant improvements,0.5802395939826965
translation,99,149,results,ext,has,none of the metrics,ext has none of the metrics,0.6215441226959229
translation,99,149,results,results,has,ext,results has ext,0.47583630681037903
translation,99,149,results,results,has,none of the metrics,results has none of the metrics,0.5357860326766968
translation,99,171,results,rouge based metrics,perform,moderately well,rouge based metrics perform moderately well,0.5214899778366089
translation,99,171,results,results,find that,rouge based metrics,results find that rouge based metrics,0.5854285955429077
translation,99,172,results,r - 2,performs,best,r - 2 performs best,0.6740782856941223
translation,99,172,results,best,on,cnndm datasets,best on cnndm datasets,0.5306000709533691
translation,99,172,results,results,has,r - 2,results has r - 2,0.5618029832839966
translation,99,173,results,js - 2,achieves,highest f1 score,js - 2 achieves highest f1 score,0.6491140723228455
translation,99,173,results,low,on,cnndm ext,low on cnndm ext,0.6226295828819275
translation,99,173,results,tac 2009 dataset,has,js - 2,tac 2009 dataset has js - 2,0.60150146484375
translation,99,173,results,results,on,tac 2009 dataset,results on tac 2009 dataset,0.559522271156311
translation,99,181,results,"r - 1 , r - 2 and r-l",have,lower correlations,"r - 1 , r - 2 and r-l have lower correlations",0.5617200136184692
translation,99,181,results,"r - 1 , r - 2 and r-l",strong indicators of,good summaries,"r - 1 , r - 2 and r-l strong indicators of good summaries",0.7721537351608276
translation,99,181,results,lower correlations,on,tac datasets,lower correlations on tac datasets,0.5495045185089111
translation,99,181,results,good summaries,especially for,extractive summaries,good summaries especially for extractive summaries,0.6813356280326843
translation,99,181,results,extractive summaries,on,cnndm dataset,extractive summaries on cnndm dataset,0.5516473054885864
translation,99,181,results,semantic matching metrics,has,"r - 1 , r - 2 and r-l","semantic matching metrics has r - 1 , r - 2 and r-l",0.5739222168922424
translation,99,182,results,"wms , r - 1 and r -l",have,negative correlations,"wms , r - 1 and r -l have negative correlations",0.532001256942749
translation,99,182,results,"wms , r - 1 and r -l",perform,moderately well,"wms , r - 1 and r -l perform moderately well",0.58814936876297
translation,99,182,results,negative correlations,on,tac - 2009,negative correlations on tac - 2009,0.5628297328948975
translation,99,182,results,moderately well,on,other datasets,moderately well on other datasets,0.46171459555625916
translation,99,182,results,other datasets,including,cnndm,other datasets including cnndm,0.6974084377288818
translation,99,182,results,bertscore,has,"wms , r - 1 and r -l","bertscore has wms , r - 1 and r -l",0.6101431846618652
translation,99,182,results,results,has,bertscore,results has bertscore,0.5357906818389893
translation,99,183,results,automatic metrics,correlate well with,humans,automatic metrics correlate well with humans,0.6903231143951416
translation,100,168,experiments,auto-hmds,contains,heterogeneous topics,auto-hmds contains heterogeneous topics,0.6569538116455078
translation,100,168,experiments,auto-hmds,contains,automatically collected source documents,auto-hmds contains automatically collected source documents,0.631352961063385
translation,100,168,experiments,heterogeneous topics,retrieved from,wikipedia,heterogeneous topics retrieved from wikipedia,0.5290486216545105
translation,100,168,experiments,automatically collected source documents,retrieved from,web sites,automatically collected source documents retrieved from web sites,0.5682591199874878
translation,100,27,results,learning,to predict,rouge precision scores,learning to predict rouge precision scores,0.659382700920105
translation,100,27,results,learning,to predict,rouge recall scores,learning to predict rouge recall scores,0.6488921046257019
translation,100,27,results,learning,leads to,better results,learning leads to better results,0.6983882784843445
translation,100,27,results,learning,to predict,rouge recall scores,learning to predict rouge recall scores,0.6488921046257019
translation,100,27,results,rouge precision scores,of,sentences,rouge precision scores of sentences,0.579765260219574
translation,100,27,results,rouge precision scores,leads to,better results,rouge precision scores leads to better results,0.6054202914237976
translation,100,27,results,rouge precision scores,if,scores,rouge precision scores if scores,0.5775828957557678
translation,100,27,results,better results,than,learning,better results than learning,0.6252540946006775
translation,100,27,results,learning,to predict,rouge recall scores,learning to predict rouge recall scores,0.6488921046257019
translation,100,27,results,rouge recall scores,if,scores,rouge recall scores if scores,0.5707193613052368
translation,100,27,results,scores,selected with,greedy algorithm,scores selected with greedy algorithm,0.6014876365661621
translation,100,155,results,all corpora,use of,rouge - 1 precision regressands,all corpora use of rouge - 1 precision regressands,0.5828544497489929
translation,100,155,results,rouge - 1 precision regressands,of,sentences,rouge - 1 precision regressands of sentences,0.5883204340934753
translation,100,155,results,rouge - 1 precision regressands,leads to,better results,rouge - 1 precision regressands leads to better results,0.6777490973472595
translation,100,155,results,better results,than using,rouge - 1 recall regressands,better results than using rouge - 1 recall regressands,0.671486496925354
translation,100,155,results,rouge - 1 recall regressands,if,rouge - 1 recall,rouge - 1 recall regressands if rouge - 1 recall,0.6121678948402405
translation,100,155,results,rouge - 1 recall,used as,evaluation metric,rouge - 1 recall used as evaluation metric,0.5664907693862915
translation,100,155,results,evaluation metric,for,final summary,evaluation metric for final summary,0.6050289869308472
translation,100,155,results,results,seen that,all corpora,results seen that all corpora,0.6274124979972839
translation,100,155,results,results,in,all corpora,results in all corpora,0.4888726472854614
translation,100,170,results,rouge - 1 precision,seems to be a bit stronger,dbs,rouge - 1 precision seems to be a bit stronger dbs,0.7100203037261963
translation,100,170,results,dbs,compared to,rouge - 2 precision,dbs compared to rouge - 2 precision,0.6789081692695618
translation,100,170,results,results,observe,rouge - 1 precision,results observe rouge - 1 precision,0.5748226046562195
translation,100,188,results,sentence selection,using,rouge - 1/2 precision scores,sentence selection using rouge - 1/2 precision scores,0.6118660569190979
translation,100,188,results,sentence selection,using,rouge - 1/2 recall scores,sentence selection using rouge - 1/2 recall scores,0.6106816530227661
translation,100,188,results,better results,than,rouge - 1/2 recall scores,better results than rouge - 1/2 recall scores,0.5545538663864136
translation,100,188,results,better results,for,all chosen redundancy thresholds,better results for all chosen redundancy thresholds,0.6166346073150635
translation,100,188,results,results,see that,sentence selection,results see that sentence selection,0.6228550672531128
translation,100,189,results,maximum adw similarity,leads to,consistently better results,maximum adw similarity leads to consistently better results,0.687978208065033
translation,100,189,results,consistently better results,selecting according to,average adw similarity,consistently better results selecting according to average adw similarity,0.6918480396270752
translation,101,161,ablation-analysis,performance,of,transformer + contrastiveatt,performance of transformer + contrastiveatt,0.6390215158462524
translation,101,161,ablation-analysis,dropping,has,p o,dropping has p o,0.5646522641181946
translation,101,161,ablation-analysis,p o,has,significantly harms,p o has significantly harms,0.6362661123275757
translation,101,161,ablation-analysis,significantly harms,has,performance,significantly harms has performance,0.6263677477836609
translation,101,161,ablation-analysis,ablation analysis,shows,dropping,ablation analysis shows dropping,0.6274398565292358
translation,101,161,ablation-analysis,ablation analysis,shows,p o,ablation analysis shows p o,0.5744368433952332
translation,101,178,experiments,nonsynchronous head,performs,worst,nonsynchronous head performs worst,0.6402501463890076
translation,101,178,experiments,even worse,than,transformer baseline,even worse than transformer baseline,0.6328396201133728
translation,101,178,experiments,transformer baseline,on,gigaword,transformer baseline on gigaword,0.569564700126648
translation,101,120,hyperparameters,transformer,as,basis architecture,transformer as basis architecture,0.5848915576934814
translation,101,120,hyperparameters,six layers,stacked in,encoder and decoder,six layers stacked in encoder and decoder,0.7448936700820923
translation,101,120,hyperparameters,dimensions,of,embedding vectors and all hidden vectors,dimensions of embedding vectors and all hidden vectors,0.5634757876396179
translation,101,120,hyperparameters,embedding vectors and all hidden vectors,set,512,embedding vectors and all hidden vectors set 512,0.7173574566841125
translation,101,120,hyperparameters,basis architecture,has,six layers,basis architecture has six layers,0.5983225107192993
translation,101,120,hyperparameters,hyperparameters,employ,transformer,hyperparameters employ transformer,0.5699284076690674
translation,101,121,hyperparameters,inner layer,of,feed -forward sublayer,inner layer of feed -forward sublayer,0.5971990823745728
translation,101,121,hyperparameters,dimensionality,of,2048,dimensionality of 2048,0.6843213438987732
translation,101,121,hyperparameters,feed -forward sublayer,has,dimensionality,feed -forward sublayer has dimensionality,0.5550157427787781
translation,101,121,hyperparameters,hyperparameters,has,inner layer,hyperparameters has inner layer,0.5019038319587708
translation,101,122,hyperparameters,eight heads,in,multihead attention,eight heads in multihead attention,0.554088294506073
translation,101,122,hyperparameters,hyperparameters,set,eight heads,hyperparameters set eight heads,0.6557369828224182
translation,101,127,hyperparameters,dimensions,in,contrastive architecture,dimensions in contrastive architecture,0.5480179190635681
translation,101,127,hyperparameters,hyperparameters,has,dimensions,hyperparameters has dimensions,0.5247315764427185
translation,101,131,hyperparameters,training,use,dropout rate,training use dropout rate,0.623676598072052
translation,101,131,hyperparameters,dropout rate,of,0.3,dropout rate of 0.3,0.5798346996307373
translation,101,131,hyperparameters,0.3,on,all datasets,0.3 on all datasets,0.4967070519924164
translation,101,131,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
translation,101,4,model,contrastive attention mechanism,to extend,sequence - to-sequence framework,contrastive attention mechanism to extend sequence - to-sequence framework,0.6803558468818665
translation,101,4,model,sequence - to-sequence framework,for,abstractive sentence summarization,sequence - to-sequence framework for abstractive sentence summarization,0.56181800365448
translation,101,5,model,contrastive attention mechanism,accommodates,two categories of attention,contrastive attention mechanism accommodates two categories of attention,0.6863152980804443
translation,101,5,model,two categories of attention,one is,conventional attention,two categories of attention one is conventional attention,0.6502498984336853
translation,101,5,model,conventional attention,attends to,relevant parts,conventional attention attends to relevant parts,0.6989568471908569
translation,101,5,model,relevant parts,of,source sentence,relevant parts of source sentence,0.5320519804954529
translation,101,5,model,relevant parts,of,source sentence,relevant parts of source sentence,0.5320519804954529
translation,101,5,model,relevant parts,of,source sentence,relevant parts of source sentence,0.5320519804954529
translation,101,5,model,opponent attention,attends to,irrelevant or less relevant parts,opponent attention attends to irrelevant or less relevant parts,0.6967945098876953
translation,101,5,model,irrelevant or less relevant parts,of,source sentence,irrelevant or less relevant parts of source sentence,0.5491235852241516
translation,101,5,model,two categories of attention,has,other,two categories of attention has other,0.5903441309928894
translation,101,6,model,both attentions,trained,opposite way,both attentions trained opposite way,0.6923370957374573
translation,101,6,model,contribution,from,conventional attention,contribution from conventional attention,0.5635353922843933
translation,101,6,model,contribution,from,opponent attention,contribution from opponent attention,0.5564440488815308
translation,101,6,model,contribution,from,opponent attention,contribution from opponent attention,0.5564440488815308
translation,101,6,model,contribution,from,opponent attention,contribution from opponent attention,0.5564440488815308
translation,101,6,model,discouraged,through,novel softmax and softmin functionality,discouraged through novel softmax and softmin functionality,0.7352443337440491
translation,101,6,model,model,has,both attentions,model has both attentions,0.5937323570251465
translation,101,18,model,"transformer ( vaswani et al. , 2017 )",as,baseline summarization model,"transformer ( vaswani et al. , 2017 ) as baseline summarization model",0.48566287755966187
translation,101,18,model,model,take,"transformer ( vaswani et al. , 2017 )","model take transformer ( vaswani et al. , 2017 )",0.6221528053283691
translation,101,53,model,both the encoder and the decoder of transformer,adopt,attention,both the encoder and the decoder of transformer adopt attention,0.6621360182762146
translation,101,53,model,attention,as,main function,attention as main function,0.5377748012542725
translation,101,53,model,rnn based architecture,has,both the encoder and the decoder of transformer,rnn based architecture has both the encoder and the decoder of transformer,0.6037850975990295
translation,101,53,model,cnn based architecture,has,both the encoder and the decoder of transformer,cnn based architecture has both the encoder and the decoder of transformer,0.605486273765564
translation,101,53,model,model,In comparison to,rnn based architecture,model In comparison to rnn based architecture,0.6064231991767883
translation,101,53,model,model,In comparison to,cnn based architecture,model In comparison to cnn based architecture,0.580288290977478
translation,101,62,model,attentions,in,transformer,attentions in transformer,0.6261054873466492
translation,101,62,model,attentions,adopts,multihead implementation,attentions adopts multihead implementation,0.6928439736366272
translation,101,62,model,transformer,adopts,multihead implementation,transformer adopts multihead implementation,0.6850535869598389
translation,101,62,model,multihead implementation,in which,each head,multihead implementation in which each head,0.6952266097068787
translation,101,62,model,each head,computes,attention,each head computes attention,0.7195599675178528
translation,101,62,model,attention,with,"smaller q , k , v","attention with smaller q , k , v",0.6616434454917908
translation,101,62,model,"smaller q , k , v",whose,dimension,"smaller q , k , v whose dimension",0.6882036924362183
translation,101,62,model,dimension,is,1/h times,dimension is 1/h times,0.596471905708313
translation,101,62,model,model,has,attentions,model has attentions,0.6162422299385071
translation,101,68,model,each sublayer,employs,residual connection,each sublayer employs residual connection,0.6125273704528809
translation,101,68,model,each sublayer,employs,residual connection,each sublayer employs residual connection,0.6125273704528809
translation,101,68,model,residual connection,adds,input,residual connection adds input,0.6260397434234619
translation,101,68,model,input,to,outcome,input to outcome,0.5580890774726868
translation,101,68,model,input,to,outcome,input to outcome,0.5580890774726868
translation,101,68,model,layer normalization,employed on,outcome,layer normalization employed on outcome,0.6925047636032104
translation,101,68,model,outcome,of,residual connection,outcome of residual connection,0.6117708683013916
translation,101,68,model,outcome,has,of sublayer,outcome has of sublayer,0.6115365028381348
translation,101,68,model,model,has,each sublayer,model has each sublayer,0.5644081234931946
translation,101,69,model,each layer,contains,additional sublayer,each layer contains additional sublayer,0.6674137115478516
translation,101,69,model,of the encoder-decoder attention,between,self-attention sublayer and the feed -forward sublayer,of the encoder-decoder attention between self-attention sublayer and the feed -forward sublayer,0.5871220827102661
translation,101,69,model,target summary side,has,each layer,target summary side has each layer,0.5937639474868774
translation,101,69,model,additional sublayer,has,of the encoder-decoder attention,additional sublayer has of the encoder-decoder attention,0.5797431468963623
translation,101,69,model,model,On,target summary side,model On target summary side,0.5848264694213867
translation,101,22,results,our transformer model,achieves,best reported results,our transformer model achieves best reported results,0.662336528301239
translation,101,22,results,best reported results,on,all data,best reported results on all data,0.5104461312294006
translation,101,22,results,proposed contrastive attention mechanism,has,our transformer model,proposed contrastive attention mechanism has our transformer model,0.5755504369735718
translation,101,139,results,strong baseline,using,transformer,strong baseline using transformer,0.7449193596839905
translation,101,139,results,comparable performance,to,state - of- the - art,comparable performance to state - of- the - art,0.5453924536705017
translation,101,139,results,results,build,strong baseline,results build strong baseline,0.7380045652389526
translation,101,140,results,contrastive attention mechanism,into,transformer,contrastive attention mechanism into transformer,0.558489203453064
translation,101,140,results,contrastive attention mechanism,into,transformer,contrastive attention mechanism into transformer,0.558489203453064
translation,101,140,results,performance,of,transformer,performance of transformer,0.6709149479866028
translation,101,140,results,significantly improves,has,performance,significantly improves has performance,0.5962982177734375
translation,101,140,results,results,introduce,contrastive attention mechanism,results introduce contrastive attention mechanism,0.5608671307563782
translation,101,142,results,results,on,lc - sts,results on lc - sts,0.539354145526886
translation,101,149,results,transformer,equipped with,our proposed contrastive attention mechanism,transformer equipped with our proposed contrastive attention mechanism,0.7068206071853638
translation,101,149,results,state - of- the - art,on,lcsts,state - of- the - art on lcsts,0.5462986826896667
translation,101,149,results,transformer,has,performance,transformer has performance,0.5373453497886658
translation,101,149,results,our proposed contrastive attention mechanism,has,performance,our proposed contrastive attention mechanism has performance,0.5609833598136902
translation,101,149,results,drastically advances,has,state - of- the - art,drastically advances has state - of- the - art,0.5628051161766052
translation,102,94,baselines,long short - term memory ( lstm ),as,basic language model,long short - term memory ( lstm ) as basic language model,0.5283593535423279
translation,102,111,baselines,baselines,experiment with,four baseline models,baselines experiment with four baseline models,0.6430933475494385
translation,102,112,baselines,concat model,concatenates,content and context features,concat model concatenates content and context features,0.7037538290023804
translation,102,112,baselines,text model,uses,only content features,text model uses only content features,0.6003904938697815
translation,102,4,experiments,popularity measure,in,social media,popularity measure in social media,0.454378217458725
translation,102,4,experiments,social media,as,distant label,social media as distant label,0.48078659176826477
translation,102,4,experiments,distant label,for,extractive summarization,distant label for extractive summarization,0.5653993487358093
translation,102,4,experiments,extractive summarization,of,online conversations,extractive summarization of online conversations,0.5249179005622864
translation,102,92,hyperparameters,number of layers,set to,3,number of layers set to 3,0.7173470854759216
translation,102,92,hyperparameters,dimensions,of,each layer,dimensions of each layer,0.6206566095352173
translation,102,92,hyperparameters,each layer,are,64,each layer are 64,0.6286047697067261
translation,102,92,hyperparameters,hyperparameters,has,number of layers,hyperparameters has number of layers,0.5200415253639221
translation,102,92,hyperparameters,hyperparameters,has,dimensions,hyperparameters has dimensions,0.5247315764427185
translation,102,105,hyperparameters,meansquared error,as,loss function,meansquared error as loss function,0.5120082497596741
translation,102,108,hyperparameters,maximum length,of,each comment,maximum length of each comment,0.5798433423042297
translation,102,108,hyperparameters,each comment,clipped to,50,each comment clipped to 50,0.675972580909729
translation,102,108,hyperparameters,hyperparameters,has,maximum length,hyperparameters has maximum length,0.5061417818069458
translation,102,109,hyperparameters,mini-batch size,is,64,mini-batch size is 64,0.615536093711853
translation,102,109,hyperparameters,hyperparameters,has,mini-batch size,hyperparameters has mini-batch size,0.4961773455142975
translation,102,95,model,fnn,is,language model,fnn is language model,0.5390297174453735
translation,102,95,model,fnn,sequentially predicts,next words,fnn sequentially predicts next words,0.7448582053184509
translation,102,95,model,fnn,sequentially predicts,reply,fnn sequentially predicts reply,0.7419902086257935
translation,102,95,model,next words,in,comment,next words in comment,0.546600878238678
translation,102,95,model,next words,in,reply,next words in reply,0.5653256773948669
translation,102,95,model,reply,using,attention mechanism,reply using attention mechanism,0.707619845867157
translation,102,95,model,model,has,fnn,model has fnn,0.6461328864097595
translation,102,37,results,outperforms,that directly adopt,karma scores,outperforms that directly adopt karma scores,0.7076146602630615
translation,102,37,results,baseline models,that directly adopt,karma scores,baseline models that directly adopt karma scores,0.6536079049110413
translation,102,37,results,karma scores,as an indicator of,informativeness,karma scores as an indicator of informativeness,0.6535136699676514
translation,102,37,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,102,37,results,outperforms,has,baseline models,outperforms has baseline models,0.5940393209457397
translation,102,37,results,results,show,our model,results show our model,0.6888449192047119
translation,102,124,results,supervised models,has,our disjunctive models,supervised models has our disjunctive models,0.6057820320129395
translation,102,124,results,our disjunctive models,has,outperform,our disjunctive models has outperform,0.5908151865005493
translation,102,124,results,results,Among,supervised models,results Among supervised models,0.5810810923576355
translation,102,125,results,results,of,concat models,results of concat models,0.5585398077964783
translation,102,125,results,concat models,are,poor,concat models are poor,0.6308218240737915
translation,102,125,results,results,of,concat models,results of concat models,0.5585398077964783
translation,102,125,results,results,has,results,results has results,0.48582205176353455
translation,102,126,results,unsupervised models,which use,global feature,unsupervised models which use global feature,0.6655754446983337
translation,102,126,results,global feature,of,posts,global feature of posts,0.6077597737312317
translation,102,126,results,global feature,in,subset,global feature in subset,0.5435606241226196
translation,102,126,results,global feature,perform,well,global feature perform well,0.6317062377929688
translation,102,126,results,results,has,unsupervised models,results has unsupervised models,0.5426442623138428
translation,102,127,results,fnndisjunctive model,combined with,textrank,fnndisjunctive model combined with textrank,0.6534671783447266
translation,102,127,results,outperforms,both,supervised models,outperforms both supervised models,0.6539368033409119
translation,102,127,results,outperforms,both,the unsupervised models,outperforms both the unsupervised models,0.6544315218925476
translation,102,127,results,textrank,has,outperforms,textrank has outperforms,0.6181371808052063
translation,102,127,results,results,has,fnndisjunctive model,results has fnndisjunctive model,0.5375336408615112
translation,103,4,model,integer linear programming ( ilp ) formulation,to obtain,oracle summary,integer linear programming ( ilp ) formulation to obtain oracle summary,0.6167168021202087
translation,103,4,model,oracle summary,of,compressive summarization paradigm,oracle summary of compressive summarization paradigm,0.5003588795661926
translation,103,4,model,compressive summarization paradigm,in terms of,rouge,compressive summarization paradigm in terms of rouge,0.658261239528656
translation,103,4,model,model,derives,integer linear programming ( ilp ) formulation,model derives integer linear programming ( ilp ) formulation,0.5770861506462097
translation,103,96,results,rouge scores,of,compressive oracle summaries,rouge scores of compressive oracle summaries,0.5723353624343872
translation,103,96,results,rouge scores,of,compressive oracle summaries,rouge scores of compressive oracle summaries,0.5723353624343872
translation,103,96,results,rouge scores,of,compressive oracle summaries,rouge scores of compressive oracle summaries,0.5723353624343872
translation,103,96,results,compressive oracle summaries,has,completely outperformed,compressive oracle summaries has completely outperformed,0.6036255359649658
translation,103,96,results,results,compare,rouge scores,results compare rouge scores,0.5860746502876282
translation,103,104,results,extractive oracle summaries,achieved,near perfect scores,extractive oracle summaries achieved near perfect scores,0.657920777797699
translation,103,104,results,results,has,extractive oracle summaries,results has extractive oracle summaries,0.5766744613647461
translation,104,188,ablation-analysis,background information,seems to be,more important,background information seems to be more important,0.6061455607414246
translation,104,188,ablation-analysis,more important,than,context,more important than context,0.5636482238769531
translation,104,188,ablation-analysis,smallest drop,in,average precision,smallest drop in average precision,0.5376529097557068
translation,104,188,ablation-analysis,ablation analysis,has,background information,ablation analysis has background information,0.5065116882324219
translation,104,136,baselines,first two baselines,agnostic to,aggregatable instance,first two baselines agnostic to aggregatable instance,0.7098129987716675
translation,104,136,baselines,baselines,has,first two baselines,baselines has first two baselines,0.5843560099601746
translation,104,137,baselines,random ordering,of,candidate entities,random ordering of candidate entities,0.6094339489936829
translation,104,139,baselines,candidates,according to,frequency,candidates according to frequency,0.6808701157569885
translation,104,139,baselines,frequency,as,correct aggregation,frequency as correct aggregation,0.5712976455688477
translation,104,139,baselines,correct aggregation,in,training set,correct aggregation in training set,0.5278014540672302
translation,104,169,experimental-setup,version of bart,pre-trained on,cnn / dailymail dataset,version of bart pre-trained on cnn / dailymail dataset,0.7642061710357666
translation,104,169,experimental-setup,experimental setup,use,version of bart,experimental setup use version of bart,0.6365883350372314
translation,104,211,experimental-setup,lr,in,"{ 3e - 6 , 5e- 6 , 1e-5 , 2e-5 , 3e-5 }","lr in { 3e - 6 , 5e- 6 , 1e-5 , 2e-5 , 3e-5 }",0.5888603925704956
translation,104,211,experimental-setup,max-tokens,in,"{ 1024 , 2048 }","max-tokens in { 1024 , 2048 }",0.546514093875885
translation,104,211,experimental-setup,experimental setup,performed,grid search,experimental setup performed grid search,0.25866255164146423
translation,104,216,experimental-setup,our experiments,on,single v100 gpu,our experiments on single v100 gpu,0.5253899097442627
translation,104,216,experimental-setup,single v100 gpu,with,32 gb,single v100 gpu with 32 gb,0.6322280764579773
translation,104,216,experimental-setup,32 gb,of,memory,32 gb of memory,0.5396531820297241
translation,104,216,experimental-setup,memory,with,fp16 option,memory with fp16 option,0.6747812628746033
translation,104,216,experimental-setup,experimental setup,ran,our experiments,experimental setup ran our experiments,0.5802465677261353
translation,104,8,experiments,tesa,dataset of,5.3 k crowd-sourced entity aggregations,tesa dataset of 5.3 k crowd-sourced entity aggregations,0.7033079266548157
translation,104,8,experiments,5.3 k crowd-sourced entity aggregations,of,"person , organization , and lo - cation named entities","5.3 k crowd-sourced entity aggregations of person , organization , and lo - cation named entities",0.5194082856178284
translation,104,38,results,simple classifier,achieves,decent results,simple classifier achieves decent results,0.6876969933509827
translation,104,38,results,decent results,on,tesa,decent results on tesa,0.6159389615058899
translation,104,38,results,results,has,simple classifier,results has simple classifier,0.5861323475837708
translation,104,39,results,wide margin,by,bart,wide margin by bart,0.5120688080787659
translation,104,39,results,our task,in,discriminative manner,our task in discriminative manner,0.5333035588264465
translation,104,40,results,fine-tuned,as,generative model,fine-tuned as generative model,0.5457571744918823
translation,104,40,results,bart,yields,similar performance,bart yields similar performance,0.7732786536216736
translation,104,40,results,similar performance,as,simple classifier,similar performance as simple classifier,0.5484598278999329
translation,104,40,results,fine-tuned,has,bart,fine-tuned has bart,0.5740089416503906
translation,104,40,results,generative model,has,bart,generative model has bart,0.5598994493484497
translation,104,179,results,bart,on,tesa,bart on tesa,0.6637378334999084
translation,104,179,results,bart,performance,significantly,bart performance significantly,0.7740247249603271
translation,104,179,results,fine-tuning,has,bart,fine-tuning has bart,0.5176252722740173
translation,104,179,results,results,has,fine-tuning,results has fine-tuning,0.5543118715286255
translation,104,180,results,discriminative bart,achieves,best results,discriminative bart achieves best results,0.6817014813423157
translation,104,180,results,results,has,discriminative bart,results has discriminative bart,0.5588890910148621
translation,104,187,results,models,perform,best,models perform best,0.5993582010269165
translation,104,187,results,best,when,all information is available,best when all information is available,0.5938913822174072
translation,104,187,results,results,has,models,results has models,0.5335168838500977
translation,104,205,results,tesa,directly measures,ability of summarizers,tesa directly measures ability of summarizers,0.7558636665344238
translation,104,205,results,ability of summarizers,to abstract,semantic level,ability of summarizers to abstract semantic level,0.4873836636543274
translation,104,205,results,results,has,tesa,results has tesa,0.538590669631958
translation,105,87,baselines,baselines,has,summarize - then -translate ( sum- trans ),baselines has summarize - then -translate ( sum- trans ),0.5701718926429749
translation,105,90,baselines,baselines,has,translate - then -summarize ( trans - sum ),baselines has translate - then -summarize ( trans - sum ),0.5805817246437073
translation,105,93,baselines,trans- sum-r,performs,round-trip translation,trans- sum-r performs round-trip translation,0.67261803150177
translation,105,93,baselines,round-trip translation,of,articles,round-trip translation of articles,0.5797032713890076
translation,105,93,baselines,round-trip translation,to get,noisy articles,round-trip translation to get noisy articles,0.6536944508552551
translation,105,93,baselines,target language,through,source language,target language through source language,0.6111465096473694
translation,105,93,baselines,source language,to get,noisy articles,source language to get noisy articles,0.6435450315475464
translation,105,93,baselines,noisy articles,in,target language,noisy articles in target language,0.482679158449173
translation,105,93,baselines,baselines,has,trans- sum-r,baselines has trans- sum-r,0.5932500958442688
translation,105,97,baselines,gold translation,of,source language article,gold translation of source language article,0.5361060500144958
translation,105,97,baselines,baselines,has,trans- sum-g,baselines has trans- sum-g,0.6107693910598755
translation,105,119,baselines,performance,of,trans,performance of trans,0.6459147930145264
translation,105,119,baselines,performance,-,sum,performance - sum,0.6480866074562073
translation,105,119,baselines,trans,-,sum,trans - sum,0.6212641000747681
translation,105,119,baselines,trans,has,sum,trans has sum,0.5880634784698486
translation,105,76,experiments,mbart,is,multi-lingual language model,mbart is multi-lingual language model,0.5102734565734863
translation,105,76,experiments,multi-lingual language model,trained on,"large , monolingual corpora","multi-lingual language model trained on large , monolingual corpora",0.6532388925552368
translation,105,76,experiments,"large , monolingual corpora",in,25 languages,"large , monolingual corpora in 25 languages",0.4740384519100189
translation,105,88,experiments,mbart,for,monolingual summarization,mbart for monolingual summarization,0.5404770970344543
translation,105,88,experiments,mbart,summarize,article,mbart summarize article,0.6939865350723267
translation,105,88,experiments,mbart,translate,summary,mbart translate summary,0.7288079261779785
translation,105,88,experiments,monolingual summarization,in,source language,monolingual summarization in source language,0.4469120502471924
translation,105,88,experiments,summary,into,target language,summary into target language,0.5336745977401733
translation,105,96,experiments,translation,used,amazon web services ( aws ) translate service,translation used amazon web services ( aws ) translate service,0.544951856136322
translation,105,129,experiments,direct cross-lingual summarization,find that,performance,direct cross-lingual summarization find that performance,0.5674892067909241
translation,105,129,experiments,performance,of,base model ( dc ),performance of base model ( dc ),0.594298243522644
translation,105,129,experiments,worse,than,translate - then - summarize baselines,worse than translate - then - summarize baselines,0.591738760471344
translation,105,129,experiments,worse,for,all languages,worse for all languages,0.6516302824020386
translation,105,129,experiments,translate - then - summarize baselines,for,all languages,translate - then - summarize baselines for all languages,0.5712778568267822
translation,105,129,experiments,all languages,except,spanish,all languages except spanish,0.6593571305274963
translation,105,77,model,"shared sub-word vocabulary , encoder , and decoder",across,all 25 languages,"shared sub-word vocabulary , encoder , and decoder across all 25 languages",0.6847900748252869
translation,105,77,model,denoising auto-encoder,during,pre-training step,denoising auto-encoder during pre-training step,0.6292533278465271
translation,105,77,model,model,uses,"shared sub-word vocabulary , encoder , and decoder","model uses shared sub-word vocabulary , encoder , and decoder",0.5776459574699402
translation,105,118,results,lead baseline,performs,poorly,lead baseline performs poorly,0.6858537197113037
translation,105,118,results,results,observe,lead baseline,results observe lead baseline,0.6071098446846008
translation,105,120,results,performance,depends on,amount of summarization data,performance depends on amount of summarization data,0.742609441280365
translation,105,120,results,amount of summarization data,available in,source language,amount of summarization data available in source language,0.6103302836418152
translation,105,121,results,tran - sum,works,significantly better,tran - sum works significantly better,0.6264809966087341
translation,105,121,results,significantly better,when,amount of data,significantly better when amount of data,0.6526567935943604
translation,105,121,results,amount of data,in,source language,amount of data in source language,0.4859420657157898
translation,105,121,results,amount of data,is,limited,amount of data is limited,0.5493821501731873
translation,105,121,results,results,find that,tran - sum,results find that tran - sum,0.6772318482398987
translation,105,125,results,trans - sum -r,works,best,trans - sum -r works best,0.6544337272644043
translation,105,125,results,baseline methods,has,trans - sum -r,baseline methods has trans - sum -r,0.5860794186592102
translation,105,125,results,results,Amongst,baseline methods,results Amongst baseline methods,0.5699945092201233
translation,105,126,results,consistently does better,than,trans -,consistently does better than trans -,0.6518188714981079
translation,105,126,results,trans -,has,sum baseline,trans - has sum baseline,0.6218310594558716
translation,105,126,results,results,has,consistently does better,results has consistently does better,0.6470844149589539
translation,105,128,results,round-trip translation method,able to recover,about 22 %,round-trip translation method able to recover about 22 %,0.7460297346115112
translation,105,128,results,about 22 %,of,performance loss,about 22 % of performance loss,0.5813289880752563
translation,105,128,results,performance loss,due to,translation errors,performance loss due to translation errors,0.6866747140884399
translation,105,128,results,results,on average,round-trip translation method,results on average round-trip translation method,0.7036204934120178
translation,105,144,results,some synthetic data ( dc + synth ),see,performance,some synthetic data ( dc + synth ) see performance,0.5930697321891785
translation,105,144,results,improves significantly,especially for,lower resource languages ( tr and vi ),improves significantly especially for lower resource languages ( tr and vi ),0.7241056561470032
translation,105,144,results,lower resource languages ( tr and vi ),on par with,best baseline model,lower resource languages ( tr and vi ) on par with best baseline model,0.6725857257843018
translation,105,144,results,performance,has,improves significantly,performance has improves significantly,0.6086756587028503
translation,105,144,results,results,add,some synthetic data ( dc + synth ),results add some synthetic data ( dc + synth ),0.658299446105957
translation,105,146,results,mbart model,for,document - level machine translation,mbart model for document - level machine translation,0.5300725102424622
translation,105,146,results,performance,for,all languages,performance for all languages,0.5505669713020325
translation,105,146,results,fine-tuning,has,mbart model,fine-tuning has mbart model,0.5406462550163269
translation,105,147,results,dc + synth + mt ),performs,significantly better,dc + synth + mt ) performs significantly better,0.6673972606658936
translation,105,147,results,significantly better,than,all baseline systems,significantly better than all baseline systems,0.549856424331665
translation,105,147,results,all baseline systems,for,"spanish , russian and vietnamese","all baseline systems for spanish , russian and vietnamese",0.5648831725120544
translation,105,148,results,performance,of,dc + synth + mt,performance of dc + synth + mt,0.6047704815864563
translation,105,148,results,performance,is,statistically the same,performance is statistically the same,0.5984085202217102
translation,105,148,results,dc + synth + mt,is,statistically the same,dc + synth + mt is statistically the same,0.6084110736846924
translation,105,148,results,statistically the same,as,trans - sum-r,statistically the same as trans - sum-r,0.6374403834342957
translation,105,148,results,our model,is,significantly better,our model is significantly better,0.5736820101737976
translation,105,148,results,significantly better,than,trans - sum baseline,significantly better than trans - sum baseline,0.5940542817115784
translation,105,148,results,turkish,has,performance,turkish has performance,0.5843871831893921
translation,105,148,results,results,For,turkish,results For turkish,0.6034180521965027
translation,105,157,results,human annotators,find,all three systems,human annotators find all three systems,0.5086404085159302
translation,105,157,results,all three systems,has,relatively fluent,all three systems has relatively fluent,0.6190862059593201
translation,105,157,results,results,shows,human annotators,results shows human annotators,0.5674270391464233
translation,105,159,results,trans - sum-r,note,dc + synth + mt,trans - sum-r note dc + synth + mt,0.6165837645530701
translation,105,159,results,dc + synth + mt,note,dc + synth + mt,dc + synth + mt note dc + synth + mt,0.6192044615745544
translation,105,159,results,dc + synth + mt,scored,significantly higher,dc + synth + mt scored significantly higher,0.7607694864273071
translation,105,159,results,dc + synth + mt,scored,significantly higher,dc + synth + mt scored significantly higher,0.7607694864273071
translation,105,159,results,significantly higher,than,trans - sum,significantly higher than trans - sum,0.6227348446846008
translation,105,159,results,results,note,dc + synth + mt,results note dc + synth + mt,0.6179923415184021
translation,105,160,results,content overlap,with,reference,content overlap with reference,0.6452128887176514
translation,105,160,results,content overlap,find that,dc + synth + mt model,content overlap find that dc + synth + mt model,0.6144905686378479
translation,105,160,results,dc + synth + mt model,scored,significantly better,dc + synth + mt model scored significantly better,0.7680714130401611
translation,105,160,results,significantly better,than,both the baseline systems,significantly better than both the baseline systems,0.5579732060432434
translation,106,104,ablation-analysis,model,made,changes,model made changes,0.6507759094238281
translation,106,104,ablation-analysis,changes,to,26.27 %,changes to 26.27 %,0.5820684432983398
translation,106,104,ablation-analysis,changes,results in,73.73 % correction accuracy,changes results in 73.73 % correction accuracy,0.6492751836776733
translation,106,104,ablation-analysis,26.27 %,results in,73.73 % correction accuracy,26.27 % results in 73.73 % correction accuracy,0.6302604675292969
translation,106,104,ablation-analysis,73.73 % correction accuracy,on,clean summaries,73.73 % correction accuracy on clean summaries,0.49512988328933716
translation,106,104,ablation-analysis,5710 clean summaries,has,model,5710 clean summaries has model,0.6188356280326843
translation,106,104,ablation-analysis,ablation analysis,For,5710 clean summaries,ablation analysis For 5710 clean summaries,0.624192476272583
translation,106,99,experimental-setup,pre-trained bart model,fine-tuned on,our training dataset,pre-trained bart model fine-tuned on our training dataset,0.7472701072692871
translation,106,99,experimental-setup,our training dataset,for,10 epochs,our training dataset for 10 epochs,0.5627398490905762
translation,106,99,experimental-setup,experimental setup,has,pre-trained bart model,experimental setup has pre-trained bart model,0.5644967555999756
translation,106,100,experimental-setup,learning rate,set to,3e - 5,learning rate set to 3e - 5,0.7297655940055847
translation,106,100,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,106,101,experimental-setup,experiments,done on,4 nvidia tesla v100 gpus,experiments done on 4 nvidia tesla v100 gpus,0.6505186557769775
translation,106,6,model,post-editing corrector module,identifying and correcting,factual errors,post-editing corrector module identifying and correcting factual errors,0.6705047488212585
translation,106,6,model,factual errors,in,generated summaries,factual errors in generated summaries,0.5262109637260437
translation,106,7,model,neural corrector model,pre-trained on,artificial examples,neural corrector model pre-trained on artificial examples,0.6971606612205505
translation,106,7,model,artificial examples,created by applying,series of heuristic transformations,artificial examples created by applying series of heuristic transformations,0.7225115299224854
translation,106,7,model,series of heuristic transformations,on,reference summaries,series of heuristic transformations on reference summaries,0.5463380217552185
translation,106,7,model,model,has,neural corrector model,model has neural corrector model,0.5582939982414246
translation,106,27,model,model,to improve,factual consistency,model to improve factual consistency,0.6903349161148071
translation,106,27,model,factual consistency,of,system summaries,factual consistency of system summaries,0.5950371026992798
translation,106,27,model,factual consistency,with,post-editing correction,factual consistency with post-editing correction,0.6126546859741211
translation,106,27,model,model,propose,model,model propose model,0.6740307211875916
translation,106,27,model,model,to improve,factual consistency,model to improve factual consistency,0.6903349161148071
translation,106,28,model,draft summary,generated by,abstractive summarization model,draft summary generated by abstractive summarization model,0.628837525844574
translation,106,28,model,draft summary,produces,corrected final summary,draft summary produces corrected final summary,0.5982678532600403
translation,106,28,model,corrected final summary,conditioned on,source document,corrected final summary conditioned on source document,0.6361356973648071
translation,106,28,model,model,takes,draft summary,model takes draft summary,0.6270573139190674
translation,106,155,model,end-to- end correction model,with,artificial examples,end-to- end correction model with artificial examples,0.632744550704956
translation,106,155,model,artificial examples,created by,corrupting reference summaries,artificial examples created by corrupting reference summaries,0.5957478880882263
translation,106,155,model,model,train,end-to- end correction model,model train end-to- end correction model,0.6993096470832825
translation,106,32,results,downstream setting,apply,corrector,downstream setting apply corrector,0.689190685749054
translation,106,32,results,corrector,to,output,corrector to output,0.5732246041297913
translation,106,32,results,corrector,find that,our corrector,corrector find that our corrector,0.6482576131820679
translation,106,32,results,corrector,able to accurately correct,errors,corrector able to accurately correct errors,0.7389124631881714
translation,106,32,results,output,of,abstractive summarizer,output of abstractive summarizer,0.6041306853294373
translation,106,32,results,our corrector,able to accurately correct,errors,our corrector able to accurately correct errors,0.7077763080596924
translation,106,32,results,errors,in,generated summaries,errors in generated summaries,0.5545461773872375
translation,106,108,results,consistency checking,on,k2019 test set,consistency checking on k2019 test set,0.5468813180923462
translation,106,109,results,our model,improves,performance,our model improves performance,0.7238103747367859
translation,106,109,results,performance,by,more than 10 %,performance by more than 10 %,0.5995782017707825
translation,106,109,results,performance,by,0.11,performance by 0.11,0.5855141282081604
translation,106,109,results,more than 10 %,higher in,accuracy,more than 10 % higher in accuracy,0.6960204839706421
translation,106,109,results,0.11,in,macro f1 - score,0.11 in macro f1 - score,0.5100482702255249
translation,106,109,results,factcc,has,our model,factcc has our model,0.6357393264770508
translation,106,109,results,results,Compared with,factcc,results Compared with factcc,0.7020509839057922
translation,106,156,results,our model,achieved,promising performance,our model achieved promising performance,0.6900779008865356
translation,106,156,results,our model,achieved,outperformed,our model achieved outperformed,0.6667829155921936
translation,106,156,results,promising performance,on,our artificial test set,promising performance on our artificial test set,0.5215018391609192
translation,106,156,results,previous models,on,manually annotated test set,previous models on manually annotated test set,0.5062816739082336
translation,106,156,results,outperformed,has,previous models,outperformed has previous models,0.6209362149238586
translation,106,156,results,results,has,our model,results has our model,0.5871725678443909
translation,107,277,ablation-analysis,"summary level , word level and system identity features",are,useful,"summary level , word level and system identity features are useful",0.5644057393074036
translation,107,277,ablation-analysis,"summary level , word level and system identity features",all,useful,"summary level , word level and system identity features all useful",0.6193846464157104
translation,107,277,ablation-analysis,0.0031 to 0.0041 decrease,on,r - 1,0.0031 to 0.0041 decrease on r - 1,0.5348303914070129
translation,107,277,ablation-analysis,ablation analysis,has,"summary level , word level and system identity features","ablation analysis has summary level , word level and system identity features",0.5661067962646484
translation,107,278,ablation-analysis,summary and word level features,lead to,significant decrease,summary and word level features lead to significant decrease,0.6401814222335815
translation,107,278,ablation-analysis,significant decrease,in,performance,significant decrease in performance,0.5324122905731201
translation,107,278,ablation-analysis,performance,on,some sets,performance on some sets,0.5912441611289978
translation,107,278,ablation-analysis,ablation analysis,Ablating,summary and word level features,ablation analysis Ablating summary and word level features,0.6443027853965759
translation,107,279,ablation-analysis,summary and word level features,turn out to be,more useful,summary and word level features turn out to be more useful,0.6185598969459534
translation,107,279,ablation-analysis,more useful,than,system identity features,more useful than system identity features,0.5876867771148682
translation,107,279,ablation-analysis,ablation analysis,use,single set of features,ablation analysis use single set of features,0.6742622256278992
translation,107,282,ablation-analysis,input-based features,are,most important,input-based features are most important,0.5442368984222412
translation,107,282,ablation-analysis,removing them,leads to,very large,removing them leads to very large,0.6915387511253357
translation,107,282,ablation-analysis,ablation analysis,has,input-based features,ablation analysis has input-based features,0.5500155687332153
translation,107,284,ablation-analysis,features,derived from,basic summaries,features derived from basic summaries,0.6608156561851501
translation,107,284,ablation-analysis,basic summaries,are,effective,basic summaries are effective,0.6045254468917847
translation,107,284,ablation-analysis,decrease,on,all five sets,decrease on all five sets,0.6210262775421143
translation,107,284,ablation-analysis,ablation analysis,has,features,ablation analysis has features,0.5585079789161682
translation,107,285,ablation-analysis,global indicators,leads to,average decrease,global indicators leads to average decrease,0.6272591948509216
translation,107,285,ablation-analysis,average decrease,of about,0.002,average decrease of about 0.002,0.5214840173721313
translation,107,285,ablation-analysis,0.002,on,r - 1 and r - 2,0.002 on r - 1 and r - 2,0.531981885433197
translation,107,285,ablation-analysis,ablation analysis,Ablating,global indicators,ablation analysis Ablating global indicators,0.6687878966331482
translation,107,27,baselines,two methods,uses,entire basic summaries,two methods uses entire basic summaries,0.6434494256973267
translation,107,27,baselines,one,uses,entire basic summaries,one uses entire basic summaries,0.6413859128952026
translation,107,27,baselines,summaries,on,sentence level,summaries on sentence level,0.543224036693573
translation,107,77,baselines,four basic unsupervised systems icsisumm,optimizes,coverage,four basic unsupervised systems icsisumm optimizes coverage,0.7949410676956177
translation,107,77,baselines,coverage,of,bigrams,coverage of bigrams,0.6199437975883484
translation,107,77,baselines,coverage,using,integer linear programming ( ilp ),coverage using integer linear programming ( ilp ),0.7281414866447449
translation,107,77,baselines,bigrams,weighted by,document frequency,bigrams weighted by document frequency,0.7210288643836975
translation,107,77,baselines,document frequency,within,input,document frequency within input,0.6474484205245972
translation,107,77,baselines,input,using,integer linear programming ( ilp ),input using integer linear programming ( ilp ),0.6721954345703125
translation,107,77,baselines,baselines,has,four basic unsupervised systems icsisumm,baselines has four basic unsupervised systems icsisumm,0.5987575054168701
translation,107,79,baselines,greedy - kl,minimize,kullback - leibler ( kl ) divergence,greedy - kl minimize kullback - leibler ( kl ) divergence,0.7863094210624695
translation,107,79,baselines,kullback - leibler ( kl ) divergence,between,word probability distribution,kullback - leibler ( kl ) divergence between word probability distribution,0.5873432755470276
translation,107,79,baselines,word probability distribution,of,summary and that of the input,word probability distribution of summary and that of the input,0.6140154600143433
translation,107,79,baselines,baselines,has,greedy - kl,baselines has greedy - kl,0.5511394143104553
translation,107,84,baselines,llrsum,employs,log-likelihood ratio ( llr ) test,llrsum employs log-likelihood ratio ( llr ) test,0.5993348956108093
translation,107,84,baselines,log-likelihood ratio ( llr ) test,to select,topic words,log-likelihood ratio ( llr ) test to select topic words,0.6687963604927063
translation,107,84,baselines,topic words,of,input,topic words of input,0.5880581140518188
translation,107,84,baselines,baselines,has,llrsum,baselines has llrsum,0.5839448571205139
translation,107,207,baselines,baselines,has,jensen-shannon ( js ) divergence,baselines has jensen-shannon ( js ) divergence,0.5367962121963501
translation,107,248,baselines,top performing systems,find,r2n2 ilp,top performing systems find r2n2 ilp,0.6138128638267517
translation,107,248,baselines,duc 01 and 02 data,has,top performing systems,duc 01 and 02 data has top performing systems,0.5785506963729858
translation,107,257,baselines,sumcombine,to,"ssa ( pei et al. , 2012 )","sumcombine to ssa ( pei et al. , 2012 )",0.5446714758872986
translation,107,257,baselines,sumcombine,to,wcs,sumcombine to wcs,0.6163081526756287
translation,107,257,baselines,wcs,has,"wang and li , 2012 )","wcs has wang and li , 2012 )",0.5934774875640869
translation,107,257,baselines,baselines,compare,sumcombine,baselines compare sumcombine,0.745928168296814
translation,107,7,model,supervised model,to select among,candidates,supervised model to select among candidates,0.7573004961013794
translation,107,7,model,model,present,supervised model,model present supervised model,0.7039233446121216
translation,107,8,model,rich set of features,that capture,content importance,rich set of features that capture content importance,0.7011646628379822
translation,107,8,model,content importance,from,different perspectives,content importance from different perspectives,0.5520362257957458
translation,107,8,model,model,relies on,rich set of features,model relies on rich set of features,0.721398115158081
translation,107,80,model,greedy method,iteratively selects,additional sentence,greedy method iteratively selects additional sentence,0.7662647366523743
translation,107,80,model,additional sentence,minimizes,kl divergence,additional sentence minimizes kl divergence,0.6380735039710999
translation,107,80,model,model,employ,greedy method,model employ greedy method,0.60447758436203
translation,107,119,model,classical indicators,of,"content importance ( e.g. , frequency , locations )","classical indicators of content importance ( e.g. , frequency , locations )",0.5111736059188843
translation,107,119,model,classical indicators,of,novel features,classical indicators of novel features,0.5574110746383667
translation,107,119,model,model,includes,classical indicators,model includes classical indicators,0.6206661462783813
translation,107,9,results,our model,performs,better,our model performs better,0.6546649932861328
translation,107,9,results,better,than,systems,better than systems,0.6452228426933289
translation,107,9,results,better,based on,manual and automatic evaluations,better based on manual and automatic evaluations,0.6646302342414856
translation,107,9,results,combined,based on,manual and automatic evaluations,combined based on manual and automatic evaluations,0.6459625959396362
translation,107,9,results,results,has,our model,results has our model,0.5871725678443909
translation,107,10,results,very competitive performance,on,six duc / tac datasets,very competitive performance on six duc / tac datasets,0.45948606729507446
translation,107,10,results,very competitive performance,comparable to,state,very competitive performance comparable to state,0.7230448126792908
translation,107,10,results,six duc / tac datasets,comparable to,state,six duc / tac datasets comparable to state,0.676911473274231
translation,107,10,results,results,achieve,very competitive performance,results achieve very competitive performance,0.6520894765853882
translation,107,226,results,our experiment,shows that,svr,our experiment shows that svr,0.7922250628471375
translation,107,226,results,svr,has,outperforms,svr has outperforms,0.62066251039505
translation,107,226,results,outperforms,has,svm - rank,outperforms has svm - rank,0.5977433919906616
translation,107,235,results,our model,performs,consistently better,our model performs consistently better,0.6439617276191711
translation,107,235,results,consistently better,than,all basic systems,consistently better than all basic systems,0.5981467962265015
translation,107,235,results,all basic systems,on,r - 1,all basic systems on r - 1,0.5602133274078369
translation,107,241,results,baselines,consider,consensus,baselines consider consensus,0.7315163016319275
translation,107,241,results,consensus,between,different systems,consensus between different systems,0.7068849802017212
translation,107,241,results,consensus,perform,poorly,consensus perform poorly,0.6769464015960693
translation,107,241,results,poorly,has,voting,poorly has voting,0.6366402506828308
translation,107,241,results,results,has,baselines,results has baselines,0.5570208430290222
translation,107,242,results,best rouge - 1,among,baselines,best rouge - 1 among baselines,0.5594085454940796
translation,107,242,results,much inferior,to,our model,much inferior to our model,0.5716201663017273
translation,107,242,results,js - i,has,best rouge - 1,js - i has best rouge - 1,0.6341396570205688
translation,107,242,results,results,has,js - i,results has js - i,0.5759969353675842
translation,107,244,results,results,Comparing with,state- of-,results Comparing with state- of-,0.4860362112522125
translation,107,245,results,duc 03 and 04 data,has,icsisumm,duc 03 and 04 data has icsisumm,0.6555285453796387
translation,107,245,results,results,On,duc 03 and 04 data,results On duc 03 and 04 data,0.5736245512962341
translation,107,246,results,sumcombine,performs,significantly better,sumcombine performs significantly better,0.6802739500999451
translation,107,246,results,sumcombine,on,r - 1,sumcombine on r - 1,0.5972924828529358
translation,107,246,results,significantly better,compared to,r - 1,significantly better compared to r - 1,0.6728183627128601
translation,107,246,results,significantly better,on,r - 1,significantly better on r - 1,0.569205105304718
translation,107,246,results,results,has,sumcombine,results has sumcombine,0.5705575942993164
translation,107,247,results,better performance,compared to,other top performing extractive systems,better performance compared to other top performing extractive systems,0.6280401349067688
translation,107,247,results,other top performing extractive systems,has,"dpp ( kulesza and taskar , 2012 )","other top performing extractive systems has dpp ( kulesza and taskar , 2012 )",0.5661117434501648
translation,107,247,results,results,achieve,better performance,results achieve better performance,0.6580345034599304
translation,107,249,results,sumcombine,achieves,lower performance,sumcombine achieves lower performance,0.7086480259895325
translation,107,249,results,sumcombine,achieves,higher performance,sumcombine achieves higher performance,0.7144664525985718
translation,107,249,results,lower performance,on,duc 01 data,lower performance on duc 01 data,0.5653625130653381
translation,107,249,results,lower performance,on,duc 02 data,lower performance on duc 02 data,0.5742621421813965
translation,107,249,results,lower performance,on,duc 02 data,lower performance on duc 02 data,0.5742621421813965
translation,107,249,results,higher performance,on,duc 02 data,higher performance on duc 02 data,0.5757697224617004
translation,107,256,results,combination model,achieves,very competitive performance,combination model achieves very competitive performance,0.6879912614822388
translation,107,256,results,very competitive performance,comparable to,state - of - the - art,very competitive performance comparable to state - of - the - art,0.6695265173912048
translation,107,256,results,state - of - the - art,on,multiple benchmarks,state - of - the - art on multiple benchmarks,0.5145991444587708
translation,107,256,results,results,has,combination model,results has combination model,0.5633746385574341
translation,107,268,results,ssa and wcs,achieve,larger improvements,ssa and wcs achieve larger improvements,0.6052512526512146
translation,107,268,results,larger improvements,over,basic systems,larger improvements over basic systems,0.6380269527435303
translation,107,268,results,sumcombine,has,ssa and wcs,sumcombine has ssa and wcs,0.606153666973114
translation,107,268,results,results,compared to,sumcombine,results compared to sumcombine,0.6827840209007263
translation,108,161,baselines,mead,uses,centroid - based summarization techniques,mead uses centroid - based summarization techniques,0.6604577302932739
translation,108,161,baselines,"feature - rich , classic multi-document summarization system",uses,centroid - based summarization techniques,"feature - rich , classic multi-document summarization system uses centroid - based summarization techniques",0.5676043033599854
translation,108,161,baselines,mead,has,"feature - rich , classic multi-document summarization system","mead has feature - rich , classic multi-document summarization system",0.5447561144828796
translation,108,161,baselines,baselines,has,mead,baselines has mead,0.5951924324035645
translation,108,162,baselines,multidocument summarization,uses,tfidf scores,multidocument summarization uses tfidf scores,0.5491122007369995
translation,108,163,baselines,baselines,has,ets,baselines has ets,0.6226847767829895
translation,108,165,baselines,baselines,has,regression,baselines has regression,0.593812882900238
translation,108,27,experiments,novel web- based approach,for harvesting,news images,novel web- based approach for harvesting news images,0.7036154866218567
translation,108,166,experiments,stateof - the- art regression implementation,in,vowpal wabbit,stateof - the- art regression implementation in vowpal wabbit,0.5370041131973267
translation,108,199,experiments,each sentence,in,corpus,each sentence in corpus,0.5063289403915405
translation,108,199,experiments,each sentence,compute,similarity,each sentence compute similarity,0.7102016806602478
translation,108,199,experiments,each sentence,extract,"lexical , event , and temporal features","each sentence extract lexical , event , and temporal features",0.6950465440750122
translation,108,199,experiments,similarity,to,humangenerated abstract,similarity to humangenerated abstract,0.5051389336585999
translation,108,152,hyperparameters,number of training iterations,set to,20,number of training iterations set to 20,0.692241907119751
translation,108,152,hyperparameters,k,set to,200,k set to 200,0.7004260420799255
translation,108,152,hyperparameters,k,set to,300,k set to 300,0.733230471611023
translation,108,152,hyperparameters,200,for,text only model,200 for text only model,0.6512669324874878
translation,108,152,hyperparameters,300,for,joint text / image model,300 for joint text / image model,0.6312914490699768
translation,108,152,hyperparameters,vocabulary,is,10 k words,vocabulary is 10 k words,0.6038786172866821
translation,108,152,hyperparameters,hyperparameters,has,number of training iterations,hyperparameters has number of training iterations,0.49485349655151367
translation,108,152,hyperparameters,hyperparameters,has,k,hyperparameters has k,0.523485004901886
translation,108,152,hyperparameters,hyperparameters,has,vocabulary,hyperparameters has vocabulary,0.5417485237121582
translation,108,8,model,text-only corpora,for,each candidate sentence,text-only corpora for each candidate sentence,0.5643117427825928
translation,108,8,model,text-only corpora,take advantage of,top-ranked relevant images,text-only corpora take advantage of top-ranked relevant images,0.6006549000740051
translation,108,8,model,text-only corpora,model,image,text-only corpora model image,0.7606074810028076
translation,108,8,model,each candidate sentence,in,news article,each candidate sentence in news article,0.49267110228538513
translation,108,8,model,top-ranked relevant images,from,web,top-ranked relevant images from web,0.5469210743904114
translation,108,8,model,image,using,convolutional neural network architecture,image using convolutional neural network architecture,0.6417445540428162
translation,108,8,model,model,To augment,text-only corpora,model To augment text-only corpora,0.6781490445137024
translation,108,21,model,more radical approach,to,timeline summarization,more radical approach to timeline summarization,0.5614929795265198
translation,108,21,model,model,take,more radical approach,model take more radical approach,0.6890420913696289
translation,108,60,model,model,make use of,low-rank approximation techniques,model make use of low-rank approximation techniques,0.6978247761726379
translation,108,70,model,novel matrix factorization framework,recommend,key sentences,novel matrix factorization framework recommend key sentences,0.6434656977653503
translation,108,70,model,key sentences,to,timeline,key sentences to timeline,0.5687093138694763
translation,108,70,model,model,propose,novel matrix factorization framework,model propose novel matrix factorization framework,0.6664445400238037
translation,108,197,model,low-rank approximation based approach,for learning,joint embeddings of news stories and images,low-rank approximation based approach for learning joint embeddings of news stories and images,0.7035629153251648
translation,108,197,model,joint embeddings of news stories and images,for,timeline summarization,joint embeddings of news stories and images for timeline summarization,0.5570255517959595
translation,108,197,model,model,introduce,low-rank approximation based approach,model introduce low-rank approximation based approach,0.6398723721504211
translation,108,198,model,multidocument extractive summarization task,as,sentence recommendation problem,multidocument extractive summarization task as sentence recommendation problem,0.4669093191623688
translation,108,198,model,model,cast,multidocument extractive summarization task,model cast multidocument extractive summarization task,0.5951091647148132
translation,108,168,results,random baseline,performs,worse,random baseline performs worse,0.6547899842262268
translation,108,168,results,worse,than,other methods,worse than other methods,0.5715794563293457
translation,108,168,results,results,see that,random baseline,results see that random baseline,0.675676167011261
translation,108,172,results,matrix factorization approach,achieving,best results,matrix factorization approach achieving best results,0.6440660357475281
translation,108,172,results,best results,in,all three rouge metrics,best results in all three rouge metrics,0.46024149656295776
translation,108,172,results,matrix factorization approach,has,outperforms,matrix factorization approach has outperforms,0.623582661151886
translation,108,172,results,outperforms,has,of these methods,outperforms has of these methods,0.582489013671875
translation,108,172,results,results,has,matrix factorization approach,results has matrix factorization approach,0.5693225264549255
translation,108,173,results,extra boost,in,performance,extra boost in performance,0.5412147641181946
translation,108,173,results,performance,when considering,visual features,performance when considering visual features,0.6751930713653564
translation,108,173,results,visual features,for,timeline summarization,visual features for timeline summarization,0.5768052339553833
translation,108,175,results,images,retrieved by using,more important sentences,images retrieved by using more important sentences,0.6660611629486084
translation,108,175,results,more important sentences,include,objects,more important sentences include objects,0.5534753203392029
translation,108,175,results,results,has,images,results has images,0.5471358299255371
translation,108,185,results,our system,is,significantly better,our system is significantly better,0.5833346843719482
translation,108,185,results,significantly better,than,strong supervised regression baseline,significantly better than strong supervised regression baseline,0.521119236946106
translation,108,185,results,results,has,our system,results has our system,0.5954442024230957
translation,108,186,results,joint learning,of,text and vision,joint learning of text and vision,0.6068755984306335
translation,108,186,results,joint learning,see that,further improvement,joint learning see that further improvement,0.6604787707328796
translation,108,186,results,text and vision,see that,further improvement,text and vision see that further improvement,0.6687505841255188
translation,108,186,results,results,considering,joint learning,results considering joint learning,0.5910177826881409
translation,109,5,model,upper-bound frequency,of,each target vocabulary,upper-bound frequency of each target vocabulary,0.5785271525382996
translation,109,5,model,upper-bound frequency,control,output words,upper-bound frequency control output words,0.7377036213874817
translation,109,5,model,each target vocabulary,in,encoder,each target vocabulary in encoder,0.5215237140655518
translation,109,5,model,output words,based on,estimation,output words based on estimation,0.7080906629562378
translation,109,5,model,estimation,has,in the decoder,estimation has in the decoder,0.5540788769721985
translation,109,5,model,model,jointly estimate,upper-bound frequency,model jointly estimate upper-bound frequency,0.7725671529769897
translation,109,16,model,model,proposes,method,model proposes method,0.7227805256843567
translation,109,17,model,upper-bound frequency,of,each target vocabulary,upper-bound frequency of each target vocabulary,0.5785271525382996
translation,109,17,model,upper-bound frequency,exploit,estimation,upper-bound frequency exploit estimation,0.699279248714447
translation,109,17,model,each target vocabulary,that can occur in,summary,each target vocabulary that can occur in summary,0.6835337281227112
translation,109,17,model,summary,during,encoding process,summary during encoding process,0.7119541764259338
translation,109,17,model,estimation,to control,output words,estimation to control output words,0.7281772494316101
translation,109,17,model,output words,in,each decoding step,output words in each decoding step,0.5071138143539429
translation,109,17,model,model,jointly estimate,upper-bound frequency,model jointly estimate upper-bound frequency,0.7725671529769897
translation,109,18,model,additional component,as,wordfrequency estimation ( wfe ) sub-model,additional component as wordfrequency estimation ( wfe ) sub-model,0.49434253573417664
translation,109,18,model,model,refer to,additional component,model refer to additional component,0.6792385578155518
translation,109,91,results,gigaword data,with,b = 5,gigaword data with b = 5,0.6696094274520874
translation,109,91,results,b = 5,were,"33.65 , 16.12 , and 31.37","b = 5 were 33.65 , 16.12 , and 31.37",0.5985047221183777
translation,109,91,results,"33.65 , 16.12 , and 31.37",for,"rouge - 1 ( f ) , rouge -2 ( f ) and rouge - l( f )","33.65 , 16.12 , and 31.37 for rouge - 1 ( f ) , rouge -2 ( f ) and rouge - l( f )",0.6077851057052612
translation,109,91,results,results,on,gigaword data,results on gigaword data,0.4961525797843933
translation,109,93,results,strong encdec baseline,by,wide margin,strong encdec baseline by wide margin,0.5440659523010254
translation,109,93,results,wide margin,on,rouge scores,wide margin on rouge scores,0.5453750491142273
translation,109,93,results,encdec+wfe,has,significantly outperformed,encdec+wfe has significantly outperformed,0.5800106525421143
translation,109,93,results,significantly outperformed,has,strong encdec baseline,significantly outperformed has strong encdec baseline,0.6026890873908997
translation,109,93,results,results,has,encdec+wfe,results has encdec+wfe,0.5310664772987366
translation,109,96,results,our method encdec+ wfe,successfully achieved,current best scores,our method encdec+ wfe successfully achieved current best scores,0.6727056503295898
translation,109,96,results,current best scores,on,most evaluations,current best scores on most evaluations,0.4509810507297516
translation,109,96,results,results,has,our method encdec+ wfe,results has our method encdec+ wfe,0.5502102971076965
translation,110,104,ablation-analysis,contribute significantly,to,performance,contribute significantly to performance,0.6050375699996948
translation,110,104,ablation-analysis,performance,of,qsbp,performance of qsbp,0.5984715223312378
translation,110,104,ablation-analysis,minimum dependency distance,has,query snowball,minimum dependency distance has query snowball,0.5671706199645996
translation,110,104,ablation-analysis,ablation analysis,has,minimum dependency distance,ablation analysis has minimum dependency distance,0.5384554266929626
translation,110,121,experiments,qsbp,is,top performer,qsbp is top performer,0.5632376074790955
translation,110,121,experiments,qsbp,is,top performer,qsbp is top performer,0.5632376074790955
translation,110,121,experiments,qsbp,is,top performer,qsbp is top performer,0.5632376074790955
translation,110,121,experiments,top performer,for,"bio , def and rel questions","top performer for bio , def and rel questions",0.6201964020729065
translation,110,121,experiments,top performer,for,event and why questions,top performer for event and why questions,0.6199890375137329
translation,110,121,experiments,top performer,for,event and why questions,top performer for event and why questions,0.6199890375137329
translation,110,121,experiments,qsbp ( idf ),is,top performer,qsbp ( idf ) is top performer,0.5481076240539551
translation,110,121,experiments,top performer,for,event and why questions,top performer for event and why questions,0.6199890375137329
translation,110,5,model,information need representation,build,co-occurrence graph,information need representation build co-occurrence graph,0.6309981942176819
translation,110,5,model,co-occurrence graph,to obtain,words,co-occurrence graph to obtain words,0.5788739919662476
translation,110,5,model,words,augment,original query terms,words augment original query terms,0.5326933860778809
translation,110,5,model,model,To enrich,information need representation,model To enrich information need representation,0.68525630235672
translation,110,6,model,summarization problem,as,maximum coverage problem,summarization problem as maximum coverage problem,0.4484170079231262
translation,110,6,model,maximum coverage problem,with,knapsack constraints,maximum coverage problem with knapsack constraints,0.5434677600860596
translation,110,6,model,knapsack constraints,based on,word pairs,knapsack constraints based on word pairs,0.6277709007263184
translation,110,6,model,word pairs,rather than,single words,word pairs rather than single words,0.6786264181137085
translation,110,6,model,model,formulate,summarization problem,model formulate summarization problem,0.6572495102882385
translation,110,35,model,indirect relationships,between,query and words,indirect relationships between query and words,0.6596969962120056
translation,110,35,model,indirect relationships,to enrich,information need representation,indirect relationships to enrich information need representation,0.6869610548019409
translation,110,35,model,query and words,in,documents,query and words in documents,0.5467111468315125
translation,110,109,model,summarization problem,as,mckp,summarization problem as mckp,0.4968043267726898
translation,110,109,model,mckp,based on,word pairs,mckp based on word pairs,0.6531344056129456
translation,110,109,model,word pairs,rather than,single words,word pairs rather than single words,0.6786264181137085
translation,110,109,model,model,formulated,summarization problem,model formulated summarization problem,0.6200669407844543
translation,110,103,results,significantly outperforms,in terms of,all evaluation metrics,significantly outperforms in terms of all evaluation metrics,0.6755868196487427
translation,110,103,results,baseline,in terms of,all evaluation metrics,baseline in terms of all evaluation metrics,0.5788062810897827
translation,110,103,results,qsbp and qsbp ( idf ),has,significantly outperforms,qsbp and qsbp ( idf ) has significantly outperforms,0.5985578894615173
translation,110,103,results,significantly outperforms,has,qsbp ( nodist ),significantly outperforms has qsbp ( nodist ),0.6051198840141296
translation,110,103,results,significantly outperforms,has,qsb,significantly outperforms has qsb,0.600073516368866
translation,110,103,results,significantly outperforms,has,wp,significantly outperforms has wp,0.6295053958892822
translation,110,103,results,significantly outperforms,has,baseline,significantly outperforms has baseline,0.6019589304924011
translation,110,103,results,results,observed that,qsbp and qsbp ( idf ),results observed that qsbp and qsbp ( idf ),0.700921893119812
translation,110,106,results,qsbp and qsbp ( idf ),achieve,0.312 and 0.313,qsbp and qsbp ( idf ) achieve 0.312 and 0.313,0.6192709803581238
translation,110,106,results,0.312 and 0.313,in,f3 score,0.312 and 0.313 in f3 score,0.5379848480224609
translation,110,106,results,results,has,qsbp and qsbp ( idf ),results has qsbp and qsbp ( idf ),0.552821934223175
translation,111,18,model,iterative annotation process,for producing,aligned summaries,iterative annotation process for producing aligned summaries,0.7278387546539307
translation,111,18,model,aligned summaries,annotated with,text - to - text generation techniques,aligned summaries annotated with text - to - text generation techniques,0.746139407157898
translation,111,18,model,model,present,iterative annotation process,model present iterative annotation process,0.6290639042854309
translation,112,124,baselines,refresh,is,extractive summarization system,refresh is extractive summarization system,0.5770378112792969
translation,112,124,baselines,extractive summarization system,trained by,globally optimizing,extractive summarization system trained by globally optimizing,0.7182406187057495
translation,112,124,baselines,rouge metric,with,reinforcement learning,rouge metric with reinforcement learning,0.6107711791992188
translation,112,124,baselines,globally optimizing,has,rouge metric,globally optimizing has rouge metric,0.5574514865875244
translation,112,124,baselines,baselines,has,refresh,baselines has refresh,0.5795902609825134
translation,112,134,baselines,skip-thought model,exploits,sentence - level distributional hypothesis,skip-thought model exploits sentence - level distributional hypothesis,0.690558910369873
translation,112,134,baselines,sentence - level distributional hypothesis,to train,encoder-decoder model,sentence - level distributional hypothesis to train encoder-decoder model,0.6591336727142334
translation,112,134,baselines,encoder-decoder model,trying to reconstruct,surrounding sentences,encoder-decoder model trying to reconstruct surrounding sentences,0.6141617298126221
translation,112,134,baselines,surrounding sentences,of,encoded sentence,surrounding sentences of encoded sentence,0.5693925619125366
translation,112,112,experimental-setup,publicly released bert model,to initialize,sentence encoder,publicly released bert model to initialize sentence encoder,0.7183262705802917
translation,112,112,experimental-setup,"( devlin et al. , 2018 )",to initialize,sentence encoder,"( devlin et al. , 2018 ) to initialize sentence encoder",0.6903491616249084
translation,112,112,experimental-setup,publicly released bert model,has,"( devlin et al. , 2018 )","publicly released bert model has ( devlin et al. , 2018 )",0.5163239240646362
translation,112,112,experimental-setup,experimental setup,used,publicly released bert model,experimental setup used publicly released bert model,0.6257802248001099
translation,112,116,experimental-setup,"adam ( kingma and ba , 2014 )",as,our optimizer,"adam ( kingma and ba , 2014 ) as our optimizer",0.49311700463294983
translation,112,116,experimental-setup,our optimizer,with,initial learning rate,our optimizer with initial learning rate,0.5868538618087769
translation,112,116,experimental-setup,initial learning rate,set to,4e - 6,initial learning rate set to 4e - 6,0.7198635339736938
translation,112,116,experimental-setup,experimental setup,used,"adam ( kingma and ba , 2014 )","experimental setup used adam ( kingma and ba , 2014 )",0.6106398105621338
translation,112,135,experimental-setup,publicly released skip-thought model,to obtain,vector representations,publicly released skip-thought model to obtain vector representations,0.628132164478302
translation,112,135,experimental-setup,vector representations,for,our task,vector representations for our task,0.5784565806388855
translation,112,135,experimental-setup,experimental setup,used,publicly released skip-thought model,experimental setup used publicly released skip-thought model,0.6165971159934998
translation,112,76,experiments,bert,to encode,sentences,bert to encode sentences,0.7938413619995117
translation,112,76,experiments,sentences,for,unsupervised summarization,sentences for unsupervised summarization,0.579397439956665
translation,112,113,experiments,english and chinese versions,of,bert,english and chinese versions of bert,0.5577480792999268
translation,112,113,experiments,english and chinese versions,used for,english and chinese corpora,english and chinese versions used for english and chinese corpora,0.6456900238990784
translation,112,113,experiments,bert,used for,english and chinese corpora,bert used for english and chinese corpora,0.6302066445350647
translation,112,5,model,model,develop,unsupervised approach,model develop unsupervised approach,0.663181722164154
translation,112,6,model,popular graph - based ranking algorithm,modify,node ( aka sentence ) centrality,popular graph - based ranking algorithm modify node ( aka sentence ) centrality,0.6633496284484863
translation,112,6,model,state - of - the - art neural representation learning model,to better capture,sentential meaning,state - of - the - art neural representation learning model to better capture sentential meaning,0.6658303737640381
translation,112,6,model,graphs,with,directed edges,graphs with directed edges,0.6424235701560974
translation,112,6,model,bert,has,state - of - the - art neural representation learning model,bert has state - of - the - art neural representation learning model,0.5555722713470459
translation,112,6,model,model,revisit,popular graph - based ranking algorithm,model revisit popular graph - based ranking algorithm,0.6487614512443542
translation,112,6,model,model,modify,node ( aka sentence ) centrality,model modify node ( aka sentence ) centrality,0.6877835988998413
translation,112,6,model,model,employ,bert,model employ bert,0.6676851511001587
translation,112,16,model,model,employ,"bert ( devlin et al. , 2018 )","model employ bert ( devlin et al. , 2018 )",0.5556729435920715
translation,112,67,model,"bert ( devlin et al. , 2018 )",as,our sentence encoder,"bert ( devlin et al. , 2018 ) as our sentence encoder",0.4956086277961731
translation,112,67,model,model,use,"bert ( devlin et al. , 2018 )","model use bert ( devlin et al. , 2018 )",0.6193298101425171
translation,112,70,model,bert,to map,sentences,bert to map sentences,0.7873628735542297
translation,112,70,model,sentences,into,deep continuous representations,sentences into deep continuous representations,0.6042287945747375
translation,112,70,model,model,use,bert,model use bert,0.6908881068229675
translation,112,71,model,bert,adopts,multi-layer bidirectional transformer encoder,bert adopts multi-layer bidirectional transformer encoder,0.5822327733039856
translation,112,71,model,bert,uses,two unsupervised prediction tasks,bert uses two unsupervised prediction tasks,0.557013988494873
translation,112,71,model,two unsupervised prediction tasks,i.e.,masked language modeling,two unsupervised prediction tasks i.e. masked language modeling,0.5774769186973572
translation,112,71,model,two unsupervised prediction tasks,i.e.,next sentence prediction,two unsupervised prediction tasks i.e. next sentence prediction,0.5871214270591736
translation,112,71,model,two unsupervised prediction tasks,to pre-train,encoder,two unsupervised prediction tasks to pre-train encoder,0.7307349443435669
translation,112,71,model,model,has,bert,model has bert,0.6085957288742065
translation,112,78,model,bert encoder,exploit,sentence - level distributional hypothesis,bert encoder exploit sentence - level distributional hypothesis,0.6897563338279724
translation,112,78,model,bert encoder,type of,sentence - level distributional hypothesis,bert encoder type of sentence - level distributional hypothesis,0.6089993119239807
translation,112,78,model,model,To fine- tune,bert encoder,model To fine- tune bert encoder,0.7033279538154602
translation,112,196,model,model,developed,unsupervised summarization,model developed unsupervised summarization,0.6124263405799866
translation,112,197,model,model,revisited,popular graph - based ranking algorithm,model revisited popular graph - based ranking algorithm,0.7342066764831543
translation,112,197,model,model,refined,node ( aka sentence ) centrality,model refined node ( aka sentence ) centrality,0.7332250475883484
translation,112,29,results,position - augmented centrality,has,significantly outperforms,position - augmented centrality has significantly outperforms,0.6129512786865234
translation,112,29,results,significantly outperforms,has,strong baselines,significantly outperforms has strong baselines,0.6021574139595032
translation,112,29,results,results,experimentally show,position - augmented centrality,results experimentally show position - augmented centrality,0.5548887252807617
translation,112,30,results,best system,achieves,performance,best system achieves performance,0.7017354965209961
translation,112,30,results,performance,comparable to,supervised systems,performance comparable to supervised systems,0.6838188171386719
translation,112,30,results,supervised systems,trained on,hundreds of thousands of ex-amples,supervised systems trained on hundreds of thousands of ex-amples,0.7197197079658508
translation,112,30,results,results,has,best system,results has best system,0.5931360125541687
translation,112,32,results,directed centrality,selection of,salient content,directed centrality selection of salient content,0.603069543838501
translation,112,32,results,salient content,has,substantially,salient content has substantially,0.5359845161437988
translation,112,32,results,results,indicate,directed centrality,results indicate directed centrality,0.5178293585777283
translation,112,121,results,results,on,nyt and cnn / daily mail corpora,results on nyt and cnn / daily mail corpora,0.47529739141464233
translation,112,121,results,results,has,nyt and cnn / daily mail,results has nyt and cnn / daily mail,0.5507662892341614
translation,112,142,results,sum ( with bert representations ),achieves,highest rouge f1 score,sum ( with bert representations ) achieves highest rouge f1 score,0.6704373955726624
translation,112,142,results,highest rouge f1 score,compared to,other unsupervised approaches,highest rouge f1 score compared to other unsupervised approaches,0.6061185598373413
translation,112,142,results,results,has,sum ( with bert representations ),results has sum ( with bert representations ),0.5815154314041138
translation,112,144,results,our best system,comparable to,supervised systems,our best system comparable to supervised systems,0.6188462376594543
translation,112,144,results,supervised systems,trained on,hundreds of thousands of examples,supervised systems trained on hundreds of thousands of examples,0.7288894057273865
translation,112,145,results,degree ( tf-idf ),very close to,textrank ( tf-idf ),degree ( tf-idf ) very close to textrank ( tf-idf ),0.7122700810432434
translation,112,148,results,textrank,across,sentence representations,textrank across sentence representations,0.6509206891059875
translation,112,148,results,pacsum,has,substantially outperforms,pacsum has substantially outperforms,0.631989598274231
translation,112,148,results,substantially outperforms,has,textrank,substantially outperforms has textrank,0.59691321849823
translation,112,148,results,results,show,pacsum,results show pacsum,0.7006581425666809
translation,112,153,results,results,observed,pacsum,results observed pacsum,0.6731773018836975
translation,112,155,results,textrank,performs,worse,textrank performs worse,0.6724841594696045
translation,112,155,results,worse,with,bert,worse with bert,0.7485924363136292
translation,112,155,results,results,has,textrank,results has textrank,0.5435177087783813
translation,112,157,results,pacsum,obtains,improvements,pacsum obtains improvements,0.6784487962722778
translation,112,157,results,improvements,over,baselines,improvements over baselines,0.6917092204093933
translation,112,157,results,baselines,on,both datasets,baselines on both datasets,0.48653727769851685
translation,112,157,results,results,has,pacsum,results has pacsum,0.5782414078712463
translation,112,162,results,pointer -generator,superior to,unsupervised methods,pointer -generator superior to unsupervised methods,0.6907224655151367
translation,112,162,results,pointer -generator,comes close to,extractive oracle,pointer -generator comes close to extractive oracle,0.7000800371170044
translation,112,162,results,results,show,pointer -generator,results show pointer -generator,0.6203539967536926
translation,112,163,results,lead and textrank,showing,our approach,lead and textrank showing our approach,0.7020263075828552
translation,112,163,results,generally portable,across,different languages and summary styles,generally portable across different languages and summary styles,0.6813374161720276
translation,112,163,results,pacsum,has,outperforms,pacsum has outperforms,0.6840621829032898
translation,112,163,results,outperforms,has,lead and textrank,outperforms has lead and textrank,0.6073451042175293
translation,112,192,results,oracle 's performance,below,100,oracle 's performance below 100,0.6825791597366333
translation,112,193,results,pac - sum,worse than,oracle,pac - sum worse than oracle,0.6965230703353882
translation,112,193,results,significantly outperforms,worse than,oracle,significantly outperforms worse than oracle,0.7608943581581116
translation,112,193,results,pac - sum,has,significantly outperforms,pac - sum has significantly outperforms,0.6170317530632019
translation,112,193,results,significantly outperforms,has,lead,significantly outperforms has lead,0.6413309574127197
translation,112,193,results,results,has,pac - sum,results has pac - sum,0.5587983727455139
translation,112,194,results,pacsum,performs,on par,pacsum performs on par,0.6025436520576477
translation,112,194,results,on par,with,refresh,on par with refresh,0.7360913157463074
translation,112,194,results,results,has,pacsum,results has pacsum,0.5782414078712463
translation,112,214,results,pacsum ( bert ),performed,best,pacsum ( bert ) performed best,0.2900114059448242
translation,112,214,results,best,on,nyt and cnn / daily mail,best on nyt and cnn / daily mail,0.5882550477981567
translation,113,164,ablation-analysis,adg,from,g - flow,adg from g - flow,0.613925039768219
translation,113,164,ablation-analysis,adg,gives,slighly better performance,adg gives slighly better performance,0.651211678981781
translation,113,164,ablation-analysis,ablation analysis,addition of,adg,ablation analysis addition of adg,0.6449598670005798
translation,113,165,ablation-analysis,our personalized discourse graph ( pdg ),enhances,r - 1 score,our personalized discourse graph ( pdg ) enhances r - 1 score,0.6459928154945374
translation,113,165,ablation-analysis,r - 1 score,more than,1.50,r - 1 score more than 1.50,0.5804118514060974
translation,113,165,ablation-analysis,ablation analysis,has,our personalized discourse graph ( pdg ),ablation analysis has our personalized discourse graph ( pdg ),0.5919553637504578
translation,113,212,ablation-analysis,correlation strength,is,pdg > adg,correlation strength is pdg > adg,0.5699876546859741
translation,113,212,ablation-analysis,pdg > adg,>,cosine similarity graph,pdg > adg > cosine similarity graph,0.6793056130409241
translation,113,212,ablation-analysis,correlation strength,has,cosine similarity graph,correlation strength has cosine similarity graph,0.5510695576667786
translation,113,212,ablation-analysis,pdg > adg,has,cosine similarity graph,pdg > adg has cosine similarity graph,0.5780022144317627
translation,113,162,baselines,simple gru model,as,baseline,simple gru model as baseline,0.5457872152328491
translation,113,162,baselines,baseline,of,rnnbased regression approach,baseline of rnnbased regression approach,0.5571801066398621
translation,113,148,experimental-setup,300 - dimensional pre-trained word2vec embeddings,as input to,gru,300 - dimensional pre-trained word2vec embeddings as input to gru,0.6460891962051392
translation,113,148,experimental-setup,300 - dimensional pre-trained word2vec embeddings,has,", 2013 )","300 - dimensional pre-trained word2vec embeddings has , 2013 )",0.5264441967010498
translation,113,148,experimental-setup,experimental setup,use,300 - dimensional pre-trained word2vec embeddings,experimental setup use 300 - dimensional pre-trained word2vec embeddings,0.5555475354194641
translation,113,153,experimental-setup,hidden states,in,"gru sent , gcn hidden layers","hidden states in gru sent , gcn hidden layers",0.5352017283439636
translation,113,153,experimental-setup,hidden states,in,gru doc,hidden states in gru doc,0.5403115749359131
translation,113,153,experimental-setup,experimental setup,has,hidden states,experimental setup has hidden states,0.5453227758407593
translation,113,154,experimental-setup,rescaling factor,in,objective function,rescaling factor in objective function,0.4794255495071411
translation,113,154,experimental-setup,rescaling factor,chosen as,40,rescaling factor chosen as 40,0.6838446259498596
translation,113,154,experimental-setup,40,from,"{ 10 , 20 , 30 , 40 , 50 , 100 }","40 from { 10 , 20 , 30 , 40 , 50 , 100 }",0.6032692790031433
translation,113,154,experimental-setup,40,based on,validation performance,40 based on validation performance,0.6401873230934143
translation,113,154,experimental-setup,"{ 10 , 20 , 30 , 40 , 50 , 100 }",based on,validation performance,"{ 10 , 20 , 30 , 40 , 50 , 100 } based on validation performance",0.6020351052284241
translation,113,154,experimental-setup,experimental setup,has,rescaling factor,experimental setup has rescaling factor,0.49817100167274475
translation,113,155,experimental-setup,objective function,optimized using,"adam ( kingma and ba , 2015 ) stochastic gradient descent","objective function optimized using adam ( kingma and ba , 2015 ) stochastic gradient descent",0.6922967433929443
translation,113,155,experimental-setup,"adam ( kingma and ba , 2015 ) stochastic gradient descent",with,learning rate,"adam ( kingma and ba , 2015 ) stochastic gradient descent with learning rate",0.5927248001098633
translation,113,155,experimental-setup,"adam ( kingma and ba , 2015 ) stochastic gradient descent",with,batch size,"adam ( kingma and ba , 2015 ) stochastic gradient descent with batch size",0.600424587726593
translation,113,155,experimental-setup,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,113,155,experimental-setup,batch size,of,1,batch size of 1,0.6655300855636597
translation,113,155,experimental-setup,experimental setup,has,objective function,experimental setup has objective function,0.4921659529209137
translation,113,156,experimental-setup,gradient clipping,with,maximum gradient norm,gradient clipping with maximum gradient norm,0.5783113837242126
translation,113,156,experimental-setup,maximum gradient norm,of,1.0,maximum gradient norm of 1.0,0.5818200707435608
translation,113,157,experimental-setup,validated,every,10 iterations,validated every 10 iterations,0.7124399542808533
translation,113,157,experimental-setup,early,if,validation performance,early if validation performance,0.6090754866600037
translation,113,157,experimental-setup,does not improve,for,10 consecutive steps,does not improve for 10 consecutive steps,0.6374742984771729
translation,113,157,experimental-setup,stopped,has,early,stopped has early,0.6404634714126587
translation,113,157,experimental-setup,validation performance,has,does not improve,validation performance has does not improve,0.610643744468689
translation,113,157,experimental-setup,experimental setup,has,model,experimental setup has model,0.5338840484619141
translation,113,157,experimental-setup,experimental setup,has,training,experimental setup has training,0.5312813520431519
translation,113,158,experimental-setup,experimental setup,trained using,single tesla k80 gpu,experimental setup trained using single tesla k80 gpu,0.6728203296661377
translation,113,159,experimental-setup,training,took,approximately 30 minutes,training took approximately 30 minutes,0.6162437200546265
translation,113,159,experimental-setup,approximately 30 minutes,until,stop,approximately 30 minutes until stop,0.7691373229026794
translation,113,159,experimental-setup,experiments,has,training,experiments has training,0.5747399926185608
translation,113,8,experiments,duc 2004,consider,three types of sentence relation graphs,duc 2004 consider three types of sentence relation graphs,0.5763992667198181
translation,113,143,experiments,each document cluster,tokenize,all the documents,each document cluster tokenize all the documents,0.7373039722442627
translation,113,143,experiments,all the documents,into,sentences,all the documents into sentences,0.5580834150314331
translation,113,143,experiments,graph representation,of,relations,graph representation of relations,0.5920540690422058
translation,113,143,experiments,relations,by,three methods,relations by three methods,0.5987746715545654
translation,113,143,experiments,approximate discourse graph ( adg ),from,g-flow,approximate discourse graph ( adg ) from g-flow,0.5535582900047302
translation,113,5,model,graph convolutional network ( gcn ),on,relation graphs,graph convolutional network ( gcn ) on relation graphs,0.5336076617240906
translation,113,5,model,relation graphs,with,sentence embeddings,relation graphs with sentence embeddings,0.5435668230056763
translation,113,5,model,sentence embeddings,obtained from,recurrent neural networks,sentence embeddings obtained from recurrent neural networks,0.5047097206115723
translation,113,5,model,recurrent neural networks,as,input node features,recurrent neural networks as input node features,0.5305701494216919
translation,113,5,model,model,employ,graph convolutional network ( gcn ),model employ graph convolutional network ( gcn ),0.53066486120224
translation,113,6,model,gcn,generates,high - level hidden sentence features,gcn generates high - level hidden sentence features,0.5717740654945374
translation,113,6,model,high - level hidden sentence features,for,salience estimation,high - level hidden sentence features for salience estimation,0.5564525127410889
translation,113,6,model,multiple layer - wise propagation,has,gcn,multiple layer - wise propagation has gcn,0.5565367937088013
translation,113,6,model,model,Through,multiple layer - wise propagation,model Through multiple layer - wise propagation,0.6650187373161316
translation,113,22,model,"graph convolutional networks ( kipf and welling , 2017 )",on,sentence relation graphs,"graph convolutional networks ( kipf and welling , 2017 ) on sentence relation graphs",0.4808114171028137
translation,113,22,model,model,apply,"graph convolutional networks ( kipf and welling , 2017 )","model apply graph convolutional networks ( kipf and welling , 2017 )",0.5919062495231628
translation,113,23,model,three different techniques,to produce,sentence relation graphs,three different techniques to produce sentence relation graphs,0.6309115886688232
translation,113,23,model,sentence relation graphs,where,nodes,sentence relation graphs where nodes,0.6041051745414734
translation,113,23,model,sentence relation graphs,where,edges,sentence relation graphs where edges,0.5925732254981995
translation,113,23,model,nodes,represent,sentences,nodes represent sentences,0.5991057753562927
translation,113,23,model,sentences,in,cluster,sentences in cluster,0.516995370388031
translation,113,23,model,edges,capture,connections,edges capture connections,0.7886171936988831
translation,113,23,model,connections,between,sentences,connections between sentences,0.7104929089546204
translation,113,24,model,summarization model,apples,graph convolutional network ( gcn ),summarization model apples graph convolutional network ( gcn ),0.6704253554344177
translation,113,24,model,sentence embeddings,from,recurrent neural networks,sentence embeddings from recurrent neural networks,0.49545031785964966
translation,113,24,model,recurrent neural networks,as,input node features,recurrent neural networks as input node features,0.5305701494216919
translation,113,24,model,relation graph,has,summarization model,relation graph has summarization model,0.5429167747497559
translation,113,24,model,model,Given,relation graph,model Given relation graph,0.6800495386123657
translation,113,25,model,gcn,generates,high - level hidden features,gcn generates high - level hidden features,0.5934000611305237
translation,113,25,model,high - level hidden features,for,sentences,high - level hidden features for sentences,0.5959040522575378
translation,113,25,model,multiple layer - wise prop-agation,has,gcn,multiple layer - wise prop-agation has gcn,0.5886659026145935
translation,113,25,model,model,Through,multiple layer - wise prop-agation,model Through multiple layer - wise prop-agation,0.6431764364242554
translation,113,163,results,cosine similarity graph,on top of,gru,cosine similarity graph on top of gru,0.6670832633972168
translation,113,163,results,cosine similarity graph,boosts,performance,cosine similarity graph boosts performance,0.7261771559715271
translation,113,163,results,results,addition of,cosine similarity graph,results addition of cosine similarity graph,0.608669638633728
translation,113,178,results,comparable level,to,reg - sum,comparable level to reg - sum,0.6195743083953857
translation,113,178,results,state - of - the - art multi-document summarizer,using,regression,state - of - the - art multi-document summarizer using regression,0.5950214862823486
translation,113,178,results,reg - sum,has,state - of - the - art multi-document summarizer,reg - sum has state - of - the - art multi-document summarizer,0.5374999642372131
translation,113,178,results,results,remain at,comparable level,results remain at comparable level,0.5915466547012329
translation,113,184,results,our graph - based models,has,outperform,our graph - based models has outperform,0.5939619541168213
translation,113,184,results,outperform,has,vanilla gru model,outperform has vanilla gru model,0.609843373298645
translation,113,185,results,pdg,improves,r - 1 score,pdg improves r - 1 score,0.6609823107719421
translation,113,185,results,r - 1 score,by,0.82,r - 1 score by 0.82,0.5374690294265747
translation,113,185,results,0.82,over,adg,0.82 over adg,0.6385213732719421
translation,113,185,results,cosine similar - ity graph,by,0.08,cosine similar - ity graph by 0.08,0.5756188631057739
translation,113,185,results,0.08,on,r- 1 score,0.08 on r- 1 score,0.5574049353599548
translation,113,185,results,adg,has,outperforms,adg has outperforms,0.6529868841171265
translation,113,185,results,outperforms,has,cosine similar - ity graph,outperforms has cosine similar - ity graph,0.582135021686554
translation,113,191,results,model,converges,faster,model converges faster,0.782178521156311
translation,113,191,results,model,achieves,lower training cost,model achieves lower training cost,0.6276480555534363
translation,113,191,results,lower training cost,than,cosine similarity graph and adg,lower training cost than cosine similarity graph and adg,0.5652199387550354
translation,113,191,results,graph,has,model,graph has model,0.5926670432090759
translation,113,191,results,results,Without,graph,results Without graph,0.7069166302680969
translation,113,193,results,adg,converges,faster,adg converges faster,0.7934830784797668
translation,113,193,results,better validation performance,than,cosine similarity graph,better validation performance than cosine similarity graph,0.5610564351081848
translation,113,193,results,three graph methods,has,adg,three graph methods has adg,0.6034571528434753
translation,113,193,results,results,For,three graph methods,results For three graph methods,0.5563632845878601
translation,113,194,results,pdg,achieves,lowest training cost,pdg achieves lowest training cost,0.6530546545982361
translation,113,194,results,pdg,achieves,validation cost,pdg achieves validation cost,0.6318812370300293
translation,113,194,results,even faster,than,no graph,even faster than no graph,0.6360077261924744
translation,113,194,results,validation cost,amongst,all methods,validation cost amongst all methods,0.598253607749939
translation,113,194,results,pdg,has,converges,pdg has converges,0.677776575088501
translation,113,194,results,converges,has,even faster,converges has even faster,0.6165482997894287
translation,113,194,results,results,has,pdg,results has pdg,0.5622873306274414
translation,114,6,baselines,rougesal reward,modifies,rouge metric,rougesal reward modifies rouge metric,0.6481379270553589
translation,114,6,baselines,rouge metric,by up-weighting,salient phrases / words,rouge metric by up-weighting salient phrases / words,0.6972079873085022
translation,114,6,baselines,salient phrases / words,detected via,keyphrase classifier,salient phrases / words detected via keyphrase classifier,0.7182276248931885
translation,114,6,baselines,baselines,has,rougesal reward,baselines has rougesal reward,0.5573214292526245
translation,114,62,hyperparameters,hyperparameters,use,rouge full- length f1 variant,hyperparameters use rouge full- length f1 variant,0.5991358757019043
translation,114,18,model,length normalization constraint,to,our entail reward,length normalization constraint to our entail reward,0.4981830418109894
translation,114,18,model,misleadingly high entailment scores,to,very short sentences,misleadingly high entailment scores to very short sentences,0.510840117931366
translation,114,18,model,model,add,length normalization constraint,model add length normalization constraint,0.64275062084198
translation,114,7,results,entail reward,gives,high ( lengthnormalized ) scores,entail reward gives high ( lengthnormalized ) scores,0.6331157684326172
translation,114,7,results,high ( lengthnormalized ) scores,to,logically - entailed summaries,high ( lengthnormalized ) scores to logically - entailed summaries,0.5250129699707031
translation,114,7,results,logically - entailed summaries,using,entailment classifier,logically - entailed summaries using entailment classifier,0.6370450258255005
translation,114,7,results,results,has,entail reward,results has entail reward,0.6123080849647522
translation,114,8,results,superior performance improvement,via,our novel and effective multi-reward approach,superior performance improvement via our novel and effective multi-reward approach,0.6599434614181519
translation,114,8,results,our novel and effective multi-reward approach,of optimizing,multiple rewards simultaneously,our novel and effective multi-reward approach of optimizing multiple rewards simultaneously,0.6980193853378296
translation,114,8,results,multiple rewards simultaneously,in,alternate mini-batches,multiple rewards simultaneously in alternate mini-batches,0.5211257934570312
translation,114,8,results,results,show,superior performance improvement,results show superior performance improvement,0.6656275391578674
translation,114,15,results,rougesal reward,gives,higher weight,rougesal reward gives higher weight,0.6448186635971069
translation,114,15,results,higher weight,to,"important , salient words","higher weight to important , salient words",0.5475996136665344
translation,114,15,results,"important , salient words",in,summary,"important , salient words in summary",0.4860692322254181
translation,114,15,results,results,has,rougesal reward,results has rougesal reward,0.574147641658783
translation,114,19,results,our new rewards,with,policy gradient approaches,our new rewards with policy gradient approaches,0.6300380229949951
translation,114,19,results,significantly better,than,cross-entropy based state - of - the - art pointer - coverage baseline,significantly better than cross-entropy based state - of - the - art pointer - coverage baseline,0.527289092540741
translation,114,19,results,results,show that,our new rewards,results show that our new rewards,0.4907171428203583
translation,114,20,results,novel multi-reward optimization approach,optimize,multiple rewards simultaneously,novel multi-reward optimization approach optimize multiple rewards simultaneously,0.7200133204460144
translation,114,20,results,multiple rewards simultaneously,in,alternate mini-batches,multiple rewards simultaneously in alternate mini-batches,0.5211257934570312
translation,114,20,results,results,show,further performance improvements,results show further performance improvements,0.6690965890884399
translation,114,21,results,our methods,achieve,new state - of- the - art,our methods achieve new state - of- the - art,0.5559868216514587
translation,114,21,results,new state - of- the - art,on,cnn / daily mail dataset,new state - of- the - art on cnn / daily mail dataset,0.5027002692222595
translation,114,21,results,new state - of- the - art,as,strong improvements,new state - of- the - art as strong improvements,0.5266358256340027
translation,114,21,results,strong improvements,in,testonly transfer setup,strong improvements in testonly transfer setup,0.5033310055732727
translation,114,21,results,testonly transfer setup,on,duc - 2002,testonly transfer setup on duc - 2002,0.561389148235321
translation,114,21,results,results,has,our methods,results has our methods,0.5312396883964539
translation,114,73,results,two rewards rougesal + entail,to incorporate,saliency and entailment knowledge,two rewards rougesal + entail to incorporate saliency and entailment knowledge,0.5933924913406372
translation,114,73,results,two rewards rougesal + entail,gives,best results,two rewards rougesal + entail gives best results,0.6240986585617065
translation,114,73,results,best results,setting,new state - of- the- art,best results setting new state - of- the- art,0.4089497923851013
translation,114,73,results,results,combined,two rewards rougesal + entail,results combined two rewards rougesal + entail,0.6898401379585266
translation,114,77,results,final rougesal + entail multi-reward rl model,is,statistically significantly better,final rougesal + entail multi-reward rl model is statistically significantly better,0.522161602973938
translation,114,77,results,statistically significantly better,than,cross-entropy ( pointer - generator + coverage ) baseline,statistically significantly better than cross-entropy ( pointer - generator + coverage ) baseline,0.5577389597892761
translation,114,77,results,statistically significantly better,than,rouge reward rl model,statistically significantly better than rouge reward rl model,0.5427212119102478
translation,114,77,results,results,has,final rougesal + entail multi-reward rl model,results has final rougesal + entail multi-reward rl model,0.5693740844726562
translation,114,80,results,output summaries,generated,see et al . ( 2017 ),output summaries generated see et al . ( 2017 ),0.6191107034683228
translation,114,80,results,output summaries,generated,"our baseline , rouge - reward and rougesal - reward models","output summaries generated our baseline , rouge - reward and rougesal - reward models",0.6277067065238953
translation,114,80,results,output summaries,using,our saliency prediction model,output summaries using our saliency prediction model,0.6578207612037659
translation,114,80,results,"our baseline , rouge - reward and rougesal - reward models",using,our saliency prediction model,"our baseline , rouge - reward and rougesal - reward models using our saliency prediction model",0.6433250904083252
translation,114,80,results,"our baseline , rouge - reward and rougesal - reward models",using,scores,"our baseline , rouge - reward and rougesal - reward models using scores",0.6743373274803162
translation,114,80,results,scores,are,"27.95 % , 28.00 % , 28.80 % , and 30.86 %","scores are 27.95 % , 28.00 % , 28.80 % , and 30.86 %",0.5452020764350891
translation,114,80,results,results,analyzed,output summaries,results analyzed output summaries,0.48663073778152466
translation,114,85,results,entailment scores,of,generated summaries,entailment scores of generated summaries,0.5495015978813171
translation,114,85,results,generated summaries,from,see et al . ( 2017 ),generated summaries from see et al . ( 2017 ),0.5738892555236816
translation,114,85,results,results,are,"27.33 % , 27.21 % , 28.23 % , and 28.98 %","results are 27.33 % , 27.21 % , 28.23 % , and 28.98 %",0.5025550723075867
translation,114,85,results,results,analyzed,entailment scores,results analyzed entailment scores,0.4229726493358612
translation,114,85,results,results,are,"27.33 % , 27.21 % , 28.23 % , and 28.98 %","results are 27.33 % , 27.21 % , 28.23 % , and 28.98 %",0.5025550723075867
translation,114,86,results,entail- reward model,achieves,stat,entail- reward model achieves stat,0.7199255228042603
translation,114,90,results,all our rewardbased rl models,have,significantly ( p < 0.01 ) more novel n-grams,all our rewardbased rl models have significantly ( p < 0.01 ) more novel n-grams,0.5368556380271912
translation,114,90,results,significantly ( p < 0.01 ) more novel n-grams,than,our cross-entropy baseline,significantly ( p < 0.01 ) more novel n-grams than our cross-entropy baseline,0.5493223667144775
translation,115,30,ablation-analysis,sentence position and lead bias,dominate,learning signal,sentence position and lead bias dominate learning signal,0.6670041680335999
translation,115,30,ablation-analysis,learning signal,for,state - of - the - art neural extractive summarizers,learning signal for state - of - the - art neural extractive summarizers,0.6114814281463623
translation,115,30,ablation-analysis,state - of - the - art neural extractive summarizers,in,news domain,state - of - the - art neural extractive summarizers in news domain,0.4945899248123169
translation,115,30,ablation-analysis,ablation analysis,verify,sentence position and lead bias,ablation analysis verify sentence position and lead bias,0.6788050532341003
translation,115,103,ablation-analysis,auxiliary loss,leads to,4.7 % absolute decrease,auxiliary loss leads to 4.7 % absolute decrease,0.6377621293067932
translation,115,103,ablation-analysis,auxiliary loss,reaching,better rouge score,auxiliary loss reaching better rouge score,0.6031432151794434
translation,115,103,ablation-analysis,ablation analysis,has,auxiliary loss,ablation analysis has auxiliary loss,0.5257234573364258
translation,115,7,baselines,first technique,employs,' unbiased ' data,first technique employs ' unbiased ' data,0.6274046301841736
translation,115,7,baselines,' unbiased ' data,i.e.,randomly shuffled sentences,' unbiased ' data i.e. randomly shuffled sentences,0.6703906655311584
translation,115,7,baselines,randomly shuffled sentences,of,source document,randomly shuffled sentences of source document,0.580171525478363
translation,115,7,baselines,baselines,has,first technique,baselines has first technique,0.5857400894165039
translation,115,89,hyperparameters,models,trained for,4 epochs,models trained for 4 epochs,0.8015409708023071
translation,115,89,hyperparameters,hyperparameters,trained for,4 epochs,hyperparameters trained for 4 epochs,0.7206196784973145
translation,115,89,hyperparameters,hyperparameters,has,models,hyperparameters has models,0.5447477102279663
translation,115,90,hyperparameters,multi-stage training,pretrain for,2 epochs,multi-stage training pretrain for 2 epochs,0.7638019323348999
translation,115,90,hyperparameters,multi-stage training,train on,original articles,multi-stage training train on original articles,0.7217603325843811
translation,115,90,hyperparameters,original articles,for,2 epochs,original articles for 2 epochs,0.5843032598495483
translation,115,90,hyperparameters,hyperparameters,For,multi-stage training,hyperparameters For multi-stage training,0.5740495324134827
translation,115,91,hyperparameters,1e ? 4 and ? = 0.0095,based on,grid search,1e ? 4 and ? = 0.0095 based on grid search,0.6668286323547363
translation,115,91,hyperparameters,grid search,using,tune library,grid search using tune library,0.6897580027580261
translation,115,91,hyperparameters,hyperparameters,set,auxiliary loss hyperparameters,hyperparameters set auxiliary loss hyperparameters,0.527156412601471
translation,115,6,model,two techniques,to make,systems,two techniques to make systems,0.7006741762161255
translation,115,6,model,systems,sensitive to,importance,systems sensitive to importance,0.7621102929115295
translation,115,6,model,importance,of,content,importance of content,0.6120966076850891
translation,115,6,model,content,different parts of,article,content different parts of article,0.694115936756134
translation,115,6,model,model,propose,two techniques,model propose two techniques,0.713970422744751
translation,115,8,model,auxiliary rougebased loss,encourages,model,auxiliary rougebased loss encourages model,0.5814377069473267
translation,115,8,model,model,to distribute,importance scores,model to distribute importance scores,0.48986950516700745
translation,115,8,model,importance scores,throughout,document,importance scores throughout document,0.6222399473190308
translation,115,8,model,importance scores,by mimicking,sentence - level rouge scores,importance scores by mimicking sentence - level rouge scores,0.6287319660186768
translation,115,8,model,sentence - level rouge scores,on,training data,sentence - level rouge scores on training data,0.4674476087093353
translation,115,34,model,auxiliary loss,encourages,model 's scores,auxiliary loss encourages model 's scores,0.6170035004615784
translation,115,34,model,model 's scores,for,sentences,model 's scores for sentences,0.6469739675521851
translation,115,34,model,model 's scores,over,sentences,model 's scores over sentences,0.7050617933273315
translation,115,34,model,estimated score distribution,over,sentences,estimated score distribution over sentences,0.6867718696594238
translation,115,34,model,rouge overlap,with,gold standard,rouge overlap with gold standard,0.6683093309402466
translation,115,9,results,performance,of,competitive reinforcement learning based extractive system,performance of competitive reinforcement learning based extractive system,0.5865557789802551
translation,115,9,results,competitive reinforcement learning based extractive system,with,auxiliary loss,competitive reinforcement learning based extractive system with auxiliary loss,0.6226020455360413
translation,115,9,results,auxiliary loss,being,more powerful,auxiliary loss being more powerful,0.6239531636238098
translation,115,9,results,more powerful,than,pretraining,more powerful than pretraining,0.5856910943984985
translation,115,9,results,significantly improve,has,performance,significantly improve has performance,0.5721634030342102
translation,115,36,results,our auxiliary loss,achieves,significantly better rouge scores,our auxiliary loss achieves significantly better rouge scores,0.6389884948730469
translation,115,36,results,significantly better rouge scores,compared to,base systems,significantly better rouge scores compared to base systems,0.6305913925170898
translation,115,36,results,results,find that,our auxiliary loss,results find that our auxiliary loss,0.6582728028297424
translation,115,37,results,pretraining approach,produces,mixed results,pretraining approach produces mixed results,0.6141840219497681
translation,115,37,results,results,has,pretraining approach,results has pretraining approach,0.5529007315635681
translation,115,60,results,distortion,at,single lead position,distortion at single lead position,0.5891480445861816
translation,115,60,results,single lead position,in,insert-lead and insert-lead3,single lead position in insert-lead and insert-lead3,0.5620803833007812
translation,115,60,results,performance,on,original data,performance on original data,0.5373241305351257
translation,115,60,results,performance,is,significantly lower,performance is significantly lower,0.5719402432441711
translation,115,60,results,original data,trained without,distortion,original data trained without distortion,0.7091489434242249
translation,115,60,results,distortion,has,performance,distortion has performance,0.5229398012161255
translation,115,60,results,results,when,distortion,results when distortion,0.6735437512397766
translation,115,100,results,similar improvement,for,rnes,similar improvement for rnes,0.7036224603652954
translation,115,100,results,simple entropy regularizer,has,small but not significant improvement,simple entropy regularizer has small but not significant improvement,0.5648927688598633
translation,115,100,results,pretraining,has,similar improvement,pretraining has similar improvement,0.5762982368469238
translation,115,100,results,results,has,simple entropy regularizer,results has simple entropy regularizer,0.5714571475982666
translation,115,101,results,significantly ( p < 0.001 ) improves,over,banditsum,significantly ( p < 0.001 ) improves over banditsum,0.6777433753013611
translation,115,101,results,auxiliary rouge loss,has,significantly ( p < 0.001 ) improves,auxiliary rouge loss has significantly ( p < 0.001 ) improves,0.5855352282524109
translation,115,101,results,results,has,auxiliary rouge loss,results has auxiliary rouge loss,0.5736070871353149
translation,117,108,ablation-analysis,compression model,to,output,compression model to output,0.5621579885482788
translation,117,108,ablation-analysis,output,of,our latent model ( latent + compress ),output of our latent model ( latent + compress ),0.583256721496582
translation,117,108,ablation-analysis,compression model,has,performance,compression model has performance,0.5605461597442627
translation,117,108,ablation-analysis,performance,has,drops considerably,performance has drops considerably,0.6071020364761353
translation,117,108,ablation-analysis,ablation analysis,applying,compression model,ablation analysis applying compression model,0.7093324065208435
translation,117,83,experimental-setup,extractive model,on,nvidia k80 gpu card,extractive model on nvidia k80 gpu card,0.5439414381980896
translation,117,83,experimental-setup,nvidia k80 gpu card,with,batch size,nvidia k80 gpu card with batch size,0.6195862293243408
translation,117,83,experimental-setup,batch size,of,32,batch size of 32,0.6741614937782288
translation,117,83,experimental-setup,experimental setup,trained,extractive model,experimental setup trained extractive model,0.7230486273765564
translation,117,84,experimental-setup,model parameters,uniformly initialized to,"[ ? 1 ? c , 1 ? c ]","model parameters uniformly initialized to [ ? 1 ? c , 1 ? c ]",0.7018080949783325
translation,117,84,experimental-setup,experimental setup,has,model parameters,experimental setup has model parameters,0.4974170923233032
translation,117,85,experimental-setup,"adam ( kingma and ba , 2014 )",to optimize,our models,"adam ( kingma and ba , 2014 ) to optimize our models",0.6826144456863403
translation,117,85,experimental-setup,our models,with,learning rate,our models with learning rate,0.6224355101585388
translation,117,85,experimental-setup,learning rate,of,"0.001 , ? 1 = 0.9 , and ? 2 = 0.999","learning rate of 0.001 , ? 1 = 0.9 , and ? 2 = 0.999",0.6015655994415283
translation,117,86,experimental-setup,extractive model,for,10 epochs,extractive model for 10 epochs,0.6216638088226318
translation,117,86,experimental-setup,highest rouge,on,validation set,highest rouge on validation set,0.5352998375892639
translation,117,86,experimental-setup,experimental setup,trained,extractive model,experimental setup trained extractive model,0.7230486273765564
translation,117,90,experimental-setup,"word dropout ( iyyer et al. , 2015 )",at,rate 0.2,"word dropout ( iyyer et al. , 2015 ) at rate 0.2",0.5138828754425049
translation,117,90,experimental-setup,experimental setup,applied,"word dropout ( iyyer et al. , 2015 )","experimental setup applied word dropout ( iyyer et al. , 2015 )",0.5952152609825134
translation,117,91,experimental-setup,hidden unit size,=,300,hidden unit size = 300,0.6781669855117798
translation,117,91,experimental-setup,d,=,300,d = 300,0.7323516607284546
translation,117,91,experimental-setup,300,for,word-level and sentence - level lstms,300 for word-level and sentence - level lstms,0.5956586599349976
translation,117,91,experimental-setup,300,both,word-level and sentence - level lstms,300 both word-level and sentence - level lstms,0.6477497816085815
translation,117,91,experimental-setup,hidden unit size,has,d,hidden unit size has d,0.5668347477912903
translation,117,91,experimental-setup,experimental setup,set,hidden unit size,experimental setup set hidden unit size,0.6564314365386963
translation,117,92,experimental-setup,pre-trained fasttext vectors,to initialize,word embeddings,pre-trained fasttext vectors to initialize word embeddings,0.6640282869338989
translation,117,92,experimental-setup,300 dimensional,has,pre-trained fasttext vectors,300 dimensional has pre-trained fasttext vectors,0.5716617107391357
translation,117,92,experimental-setup,experimental setup,used,300 dimensional,experimental setup used 300 dimensional,0.612653911113739
translation,117,93,experimental-setup,latent model,initialized from,extractive model,latent model initialized from extractive model,0.6756094098091125
translation,117,93,experimental-setup,latent model,set,weight,latent model set weight,0.6472892165184021
translation,117,93,experimental-setup,weight,to,? = 0.5,weight to ? = 0.5,0.6108692288398743
translation,117,93,experimental-setup,experimental setup,set,weight,experimental setup set weight,0.6465502381324768
translation,117,93,experimental-setup,experimental setup,has,latent model,experimental setup has latent model,0.5553001165390015
translation,117,94,experimental-setup,latent model,trained with,sgd,latent model trained with sgd,0.7684802412986755
translation,117,94,experimental-setup,sgd,with,learning rate,sgd with learning rate,0.6203937530517578
translation,117,94,experimental-setup,0.01,for,5 epochs,0.01 for 5 epochs,0.6565847992897034
translation,117,94,experimental-setup,learning rate,has,0.01,learning rate has 0.01,0.5422797799110413
translation,117,94,experimental-setup,experimental setup,has,latent model,experimental setup has latent model,0.5553001165390015
translation,117,19,model,latent variable extractive model,view,labels,latent variable extractive model view labels,0.6588121056556702
translation,117,19,model,labels,of,sentences,labels of sentences,0.6009804010391235
translation,117,19,model,labels,as,"binary latent variables ( i.e. , zeros and ones )","labels as binary latent variables ( i.e. , zeros and ones )",0.5046525001525879
translation,117,19,model,sentences,in,document,sentences in document,0.5417858362197876
translation,117,19,model,model,propose,latent variable extractive model,model propose latent variable extractive model,0.6579268574714661
translation,117,20,model,likelihood of human summaries,given,selected sentences,likelihood of human summaries given selected sentences,0.6938498020172119
translation,117,111,model,latent variable extractive summarization,leverages,human summaries,latent variable extractive summarization leverages human summaries,0.6912893056869507
translation,117,111,model,human summaries,with the help,sentence,human summaries with the help sentence,0.6733072400093079
translation,117,111,model,model,proposed,latent variable extractive summarization,model proposed latent variable extractive summarization,0.7137300372123718
translation,117,101,results,outperforms,by,wide margin,outperforms by wide margin,0.6322848796844482
translation,117,101,results,lead3,by,wide margin,lead3 by wide margin,0.6085100769996643
translation,117,101,results,extractive model,has,outperforms,extractive model has outperforms,0.6366674304008484
translation,117,101,results,outperforms,has,lead3,outperforms has lead3,0.6813421845436096
translation,117,102,results,previously published extractive models,i.e.,summarunner,previously published extractive models i.e. summarunner,0.6383408308029175
translation,117,102,results,previously published extractive models,i.e.,extract - cnn,previously published extractive models i.e. extract - cnn,0.5909243226051331
translation,117,102,results,previously published extractive models,i.e.,refresh,previously published extractive models i.e. refresh,0.5951269865036011
translation,117,102,results,extract,has,outperforms,extract has outperforms,0.6591957807540894
translation,117,102,results,outperforms,has,previously published extractive models,outperforms has previously published extractive models,0.5713942050933838
translation,117,102,results,results,has,extract,results has extract,0.5007702112197876
translation,117,105,results,extract,is,better,extract is better,0.646393358707428
translation,117,105,results,better,evaluated with,rouge - 2 and rouge -l.,better evaluated with rouge - 2 and rouge -l.,0.7384709119796753
translation,117,105,results,all abstractive models,except for,abstract - rl.,all abstractive models except for abstract - rl.,0.6557695865631104
translation,117,105,results,all abstractive models,except for,abstract - rl,all abstractive models except for abstract - rl,0.6594530344009399
translation,117,105,results,lower,for,abstract - rl,lower for abstract - rl,0.681343138217926
translation,117,105,results,lower,is,more competitive,lower is more competitive,0.5903534889221191
translation,117,105,results,more competitive,evaluated against,rouge - 1 and rouge -l,more competitive evaluated against rouge - 1 and rouge -l,0.7328670024871826
translation,117,105,results,rouge - 2 and rouge -l.,has,extract,rouge - 2 and rouge -l. has extract,0.6164942383766174
translation,117,105,results,extract,has,outperforms,extract has outperforms,0.6591957807540894
translation,117,105,results,outperforms,has,all abstractive models,outperforms has all abstractive models,0.5922644138336182
translation,117,105,results,abstract - rl.,has,rouge - 2,abstract - rl. has rouge - 2,0.6128326654434204
translation,117,105,results,results,conclude,extract,results conclude extract,0.5923803448677063
translation,117,105,results,results,conclude,extract,results conclude extract,0.5923803448677063
translation,117,106,results,our latent variable model,has,outperforms,our latent variable model has outperforms,0.6104829907417297
translation,117,106,results,outperforms,has,extract,outperforms has extract,0.6795564889907837
translation,117,106,results,results,has,our latent variable model,results has our latent variable model,0.5080042481422424
translation,118,95,baselines,importance,for,n-grams,importance for n-grams,0.623078465461731
translation,118,96,baselines,numerical statistic,to reflect,important,numerical statistic to reflect important,0.6350687146186829
translation,118,96,baselines,term,to,document,term to document,0.5688328742980957
translation,118,96,baselines,simple theoretic model,for,content importance,simple theoretic model for content importance,0.5922574400901794
translation,118,96,baselines,content importance,based on,statistical information theory,content importance based on statistical information theory,0.6197510361671448
translation,118,96,baselines,tf ?idf,has,numerical statistic,tf ?idf has numerical statistic,0.589863657951355
translation,118,96,baselines,important,has,term,important has term,0.5728409886360168
translation,118,96,baselines,"stm ( peyrard , 2019 )",has,simple theoretic model,"stm ( peyrard , 2019 ) has simple theoretic model",0.5960263013839722
translation,118,28,model,novel and generalpurpose approach,to model,content importance,novel and generalpurpose approach to model content importance,0.7317495346069336
translation,118,28,model,content importance,for,summarization,content importance for summarization,0.6223956942558289
translation,118,28,model,model,propose,novel and generalpurpose approach,model propose novel and generalpurpose approach,0.6909937858581543
translation,118,29,model,information theory,on top of,pre-trained language models,information theory on top of pre-trained language models,0.6596338152885437
translation,118,29,model,model,employ,information theory,model employ information theory,0.5331591963768005
translation,118,101,results,our method,has,imp,our method has imp,0.6074074506759644
translation,118,101,results,imp,has,consistently outperform,imp has consistently outperform,0.639128565788269
translation,118,101,results,consistently outperform,has,prior models,consistently outperform has prior models,0.5698121190071106
translation,118,102,results,theory - based methods,achieve,better results,theory - based methods achieve better results,0.6199632287025452
translation,118,102,results,stm and bayesiansr,achieve,better results,stm and bayesiansr achieve better results,0.6206223368644714
translation,118,102,results,theory - based methods,has,stm and bayesiansr,theory - based methods has stm and bayesiansr,0.5811893343925476
translation,118,104,results,significant advantage,of,our method,significant advantage of our method,0.5770584344863892
translation,118,104,results,significant advantage,verifies,hypothesis,significant advantage verifies hypothesis,0.7264645099639893
translation,118,104,results,our method,verifies,hypothesis,our method verifies hypothesis,0.7353208661079407
translation,118,104,results,hypothesis,that,pre-trained language models,hypothesis that pre-trained language models,0.5615916848182678
translation,118,104,results,pre-trained language models,better characterize,background knowledge,pre-trained language models better characterize background knowledge,0.6549224853515625
translation,118,104,results,importance,of,each semantic unit,importance of each semantic unit,0.5895637273788452
translation,118,104,results,results,has,significant advantage,results has significant advantage,0.584705114364624
translation,118,105,results,our methods,have,more significant improvement,our methods have more significant improvement,0.5369744896888733
translation,118,105,results,more significant improvement,on,bigram-level prediction,more significant improvement on bigram-level prediction,0.5205809473991394
translation,118,105,results,bigram-level prediction,than,unigram-level,bigram-level prediction than unigram-level,0.5763929486274719
translation,118,105,results,results,has,our methods,results has our methods,0.5312396883964539
translation,118,107,results,unsupervised extract - based model,for,summarization,unsupervised extract - based model for summarization,0.5931800603866577
translation,118,108,results,our models,achieve,significantly higher rouge scores,our models achieve significantly higher rouge scores,0.6165217757225037
translation,118,108,results,significantly higher rouge scores,than,previous work,significantly higher rouge scores than previous work,0.5618932843208313
translation,118,108,results,previous work,has,by average 2.02,previous work has by average 2.02,0.5316073298454285
translation,118,108,results,results,has,our models,results has our models,0.5733726620674133
translation,118,111,results,mlms,including,bert and distillbert,mlms including bert and distillbert,0.7073242664337158
translation,118,111,results,results,has,mlms,results has mlms,0.5284315943717957
translation,118,112,results,plm,slightly inferior to,mlms,plm slightly inferior to mlms,0.5413978099822998
translation,118,112,results,results,has,plm,results has plm,0.5493592619895935
translation,119,151,ablation-analysis,importance sub-aspect,leads to,worst performance,importance sub-aspect leads to worst performance,0.6368491053581238
translation,119,151,ablation-analysis,improved,when considering,other sub-aspects,improved when considering other sub-aspects,0.7362390756607056
translation,119,151,ablation-analysis,ablation analysis,Only focusing on,importance sub-aspect,ablation analysis Only focusing on importance sub-aspect,0.7428538203239441
translation,119,7,model,neural framework,flexibly control,summary generation,neural framework flexibly control summary generation,0.7955412864685059
translation,119,7,model,summary generation,by introducing,"set of subaspect functions ( i.e. importance , diversity , position )","summary generation by introducing set of subaspect functions ( i.e. importance , diversity , position )",0.6527358293533325
translation,119,7,model,model,propose,neural framework,model propose neural framework,0.707841157913208
translation,119,8,model,set of control codes,to decide,sub-aspect,set of control codes to decide sub-aspect,0.6528365015983582
translation,119,31,model,model,propose,flexible neural summarization,model propose flexible neural summarization,0.6521743535995483
translation,119,41,model,control codes,to label,training data,control codes to label training data,0.7377713918685913
translation,119,41,model,control codes,implement,conditional generation approach,control codes implement conditional generation approach,0.6890315413475037
translation,119,41,model,conditional generation approach,with,neural selector model,conditional generation approach with neural selector model,0.6410959959030151
translation,119,41,model,model,utilize,control codes,model utilize control codes,0.6756815314292908
translation,119,41,model,model,implement,conditional generation approach,model implement conditional generation approach,0.6875467300415039
translation,119,52,model,first stab,to introduce,sub-aspect functions,first stab to introduce sub-aspect functions,0.6614266633987427
translation,119,52,model,sub-aspect functions,for,conditional extractive summarization,sub-aspect functions for conditional extractive summarization,0.6188265681266785
translation,119,52,model,model,take,first stab,model take first stab,0.6769497990608215
translation,119,53,model,possibility,of,disentangling,possibility of disentangling,0.5860245227813721
translation,119,53,model,position,for choosing,sentences,position for choosing sentences,0.6696355938911438
translation,119,53,model,importance,for choosing,relevant and repeating content,importance for choosing relevant and repeating content,0.6976986527442932
translation,119,53,model,relevant and repeating content,across,document,relevant and repeating content across document,0.5337091684341431
translation,119,53,model,diversity,for ensuring,minimal redundancy,diversity for ensuring minimal redundancy,0.7068870663642883
translation,119,53,model,minimal redundancy,between,summary sentences,minimal redundancy between summary sentences,0.6516572833061218
translation,119,53,model,minimal redundancy,during,summary generation process,minimal redundancy during summary generation process,0.6787362098693848
translation,119,53,model,disentangling,has,three sub-aspects,disentangling has three sub-aspects,0.5060490965843201
translation,119,53,model,disentangling,has,importance,disentangling has importance,0.5510926246643066
translation,119,53,model,disentangling,has,2019 ),disentangling has 2019 ),0.6522291302680969
translation,119,54,model,control codes,for,conditional training,control codes for conditional training,0.6394233703613281
translation,119,88,model,"position , importance and diversity",set of,sub-function features,"position , importance and diversity set of sub-function features",0.6887066960334778
translation,119,88,model,sub-function features,to characterize,extractive news summarization,sub-function features to characterize extractive news summarization,0.6493268013000488
translation,119,88,model,extractive news summarization,has,"et al. , 2019 )","extractive news summarization has et al. , 2019 )",0.589011549949646
translation,119,88,model,model,adopt,"position , importance and diversity","model adopt position , importance and diversity",0.6216248273849487
translation,119,150,results,summary,generated from,"code [ 0,0,1 ]","summary generated from code [ 0,0,1 ]",0.6509149670600891
translation,119,150,results,summary,similar to,lead - 3,summary similar to lead - 3,0.6140475273132324
translation,119,150,results,summary,dynamically learn,positional features,summary dynamically learn positional features,0.6672264337539673
translation,119,150,results,"code [ 0,0,1 ]",similar to,lead - 3,"code [ 0,0,1 ] similar to lead - 3",0.6761412620544434
translation,119,150,results,positional features,not limited to,first 3 sentences,positional features not limited to first 3 sentences,0.65725177526474
translation,119,150,results,positional features,isolating out,diversity and importance features,positional features isolating out diversity and importance features,0.7335831522941589
translation,119,150,results,results,observe,summary,results observe summary,0.6141650676727295
translation,119,152,results,"diversity sub-aspect ( i.e. code [ 0,1,0 ] )",generate,results,"diversity sub-aspect ( i.e. code [ 0,1,0 ] ) generate results",0.6821107268333435
translation,119,152,results,results,comparable to,strong baselines,results comparable to strong baselines,0.707408607006073
translation,119,152,results,results,Focusing on,"diversity sub-aspect ( i.e. code [ 0,1,0 ] )","results Focusing on diversity sub-aspect ( i.e. code [ 0,1,0 ] )",0.7107382416725159
translation,119,162,results,summaries,under,diversity code,summaries under diversity code,0.6502473950386047
translation,119,162,results,diversity code,more favored than,importance,diversity code more favored than importance,0.7278697490692139
translation,119,162,results,combination,produce,better results,combination produce better results,0.6781876683235168
translation,119,162,results,results,observed,summaries,results observed summaries,0.6348565816879272
translation,119,164,results,automatic and human evaluations,show,summarizing,automatic and human evaluations show summarizing,0.6358352303504944
translation,119,164,results,summarizing,with,semantic-related sub-aspect condition codes,summarizing with semantic-related sub-aspect condition codes,0.6317101120948792
translation,119,164,results,summarizing,achieves,reasonable summaries,summarizing achieves reasonable summaries,0.6184201240539551
translation,119,164,results,results,has,automatic and human evaluations,results has automatic and human evaluations,0.49579107761383057
translation,119,170,results,outputs,under,position sub-aspect and bertext,outputs under position sub-aspect and bertext,0.6266356110572815
translation,119,170,results,position sub-aspect and bertext,suffer,significant drop,position sub-aspect and bertext suffer significant drop,0.7132042050361633
translation,119,170,results,significant drop,in,performance,significant drop in performance,0.5765949487686157
translation,119,170,results,significant drop,shuffle,sentence order,significant drop shuffle sentence order,0.7630090713500977
translation,119,170,results,results,has,outputs,results has outputs,0.5481858849525452
translation,119,178,results,summaries,under,importance code,summaries under importance code,0.643123984336853
translation,119,178,results,importance code,obtain,highest rouge - 1 and rouge - 2 scores,importance code obtain highest rouge - 1 and rouge - 2 scores,0.5607073903083801
translation,119,178,results,highest rouge - 1 and rouge - 2 scores,better than,best-reported model,highest rouge - 1 and rouge - 2 scores better than best-reported model,0.7484468817710876
translation,119,178,results,results,shows,summaries,results shows summaries,0.6624705791473389
translation,119,179,results,summaries,under,position code,summaries under position code,0.6441214680671692
translation,119,179,results,position code,do not perform,well,position code do not perform well,0.7235282063484192
translation,119,179,results,results,has,summaries,results has summaries,0.5243220329284668
translation,120,14,baselines,majority vote,selects,most frequent label,majority vote selects most frequent label,0.712466835975647
translation,120,14,baselines,most frequent label,from,predicted labels,most frequent label from predicted labels,0.5079521536827087
translation,120,14,baselines,most frequent label,in,post-processing,most frequent label in post-processing,0.5241413712501526
translation,120,14,baselines,predicted labels,of,multiple classifiers,predicted labels of multiple classifiers,0.5757767558097839
translation,120,14,baselines,multiple classifiers,in,post-processing,multiple classifiers in post-processing,0.5299238562583923
translation,120,14,baselines,baselines,has,majority vote,baselines has majority vote,0.5683335065841675
translation,120,194,baselines,three variations,of,model preparation,three variations of model preparation,0.6172850131988525
translation,120,194,baselines,baselines,address,three variations,baselines address three variations,0.5806630849838257
translation,120,195,baselines,from   checkpoints,saved in each epoch,training,from   checkpoints saved in each epoch training,0.7733379602432251
translation,120,195,baselines,first one,has,self-ensemble,first one has self-ensemble,0.5752469897270203
translation,120,195,baselines,baselines,has,first one,baselines has first one,0.6238251328468323
translation,120,195,baselines,baselines,has,self-ensemble,baselines has self-ensemble,0.5534396767616272
translation,120,197,baselines,method of training models,varying in,model structure,method of training models varying in model structure,0.6869465708732605
translation,120,197,baselines,baselines,has,hetero-ensemble,baselines has hetero-ensemble,0.5547763109207153
translation,120,199,baselines,method of training models,by,bagging of training data,method of training models by bagging of training data,0.599726676940918
translation,120,199,baselines,baselines,has,bagging - ensemble,baselines has bagging - ensemble,0.5598092079162598
translation,120,183,experiments,postcosb and postvmfb,see that,performances,postcosb and postvmfb see that performances,0.6536150574684143
translation,120,183,experiments,performances,are,almost the same,performances are almost the same,0.6028462648391724
translation,120,130,hyperparameters,two layers,with,500 dimensional hidden layers,two layers with 500 dimensional hidden layers,0.6050206422805786
translation,120,130,hyperparameters,500 dimensional hidden layers,whose,dropout rates,500 dimensional hidden layers whose dropout rates,0.5718013644218445
translation,120,130,hyperparameters,dropout rates,were,0.3,dropout rates were 0.3,0.5214911699295044
translation,120,130,hyperparameters,input vectors,created by,500 - dimensional word-embedding layer,input vectors created by 500 - dimensional word-embedding layer,0.5623131990432739
translation,120,131,hyperparameters,stochastic gradient descent method,with,learning rate,stochastic gradient descent method with learning rate,0.5918627977371216
translation,120,131,hyperparameters,learning rate,of,1.0,learning rate of 1.0,0.6061668395996094
translation,120,131,hyperparameters,learning rate,where,mini-batch size,learning rate where mini-batch size,0.6003550291061401
translation,120,131,hyperparameters,mini-batch size,set to,64,mini-batch size set to 64,0.7284036874771118
translation,120,131,hyperparameters,hyperparameters,trained with,stochastic gradient descent method,hyperparameters trained with stochastic gradient descent method,0.7177953124046326
translation,120,132,hyperparameters,learning process,ended in,13 epochs,learning process ended in 13 epochs,0.6914869546890259
translation,120,132,hyperparameters,learning process,decaying,learning rate,learning process decaying learning rate,0.7639164328575134
translation,120,132,hyperparameters,learning rate,with,decay factor,learning rate with decay factor,0.6181644201278687
translation,120,132,hyperparameters,decay factor,of,0.5,decay factor of 0.5,0.6346431970596313
translation,120,132,hyperparameters,hyperparameters,has,learning process,hyperparameters has learning process,0.5142287611961365
translation,120,134,hyperparameters,10 learned models,by,random initialization,10 learned models by random initialization,0.546290397644043
translation,120,134,hyperparameters,random initialization,for,ensemble methods,random initialization for ensemble methods,0.5800184011459351
translation,120,134,hyperparameters,hyperparameters,prepared,10 learned models,hyperparameters prepared 10 learned models,0.49130845069885254
translation,120,136,hyperparameters,input sequences,used,beam-search algorithm,input sequences used beam-search algorithm,0.6450405120849609
translation,120,136,hyperparameters,beam-search algorithm,with,beam width,beam-search algorithm with beam width,0.6466465592384338
translation,120,136,hyperparameters,beam width,of,5,beam width of 5,0.7087391018867493
translation,120,136,hyperparameters,hyperparameters,When decoding,input sequences,hyperparameters When decoding input sequences,0.7109041213989258
translation,120,137,hyperparameters,maximum size,of,decoded sequences,maximum size of decoded sequences,0.6038981080055237
translation,120,137,hyperparameters,decoded sequences,was,100,decoded sequences was 100,0.6696507334709167
translation,120,137,hyperparameters,hyperparameters,has,maximum size,hyperparameters has maximum size,0.5025959610939026
translation,120,198,hyperparameters,10 models,for,hetero-ensemble,10 models for hetero-ensemble,0.5894216895103455
translation,120,198,hyperparameters,10 models,consisting of,8 models,10 models consisting of 8 models,0.7183496356010437
translation,120,198,hyperparameters,8 models,prepared by changing,number of layers,8 models prepared by changing number of layers,0.6907840967178345
translation,120,198,hyperparameters,8 models,prepared by changing,size of word embedding,8 models prepared by changing size of word embedding,0.6394438147544861
translation,120,198,hyperparameters,number of layers,in,lstm encoder / decoder,number of layers in lstm encoder / decoder,0.4901564419269562
translation,120,198,hyperparameters,lstm encoder / decoder,in,"{ 2 , 3 }","lstm encoder / decoder in { 2 , 3 }",0.5234804153442383
translation,120,198,hyperparameters,lstm hidden states,in,"{ 250 , 500 }","lstm hidden states in { 250 , 500 }",0.5127499103546143
translation,120,198,hyperparameters,lstm hidden states,in,"{ 250 , 500 }","lstm hidden states in { 250 , 500 }",0.5127499103546143
translation,120,198,hyperparameters,lstm hidden states,in,"{ 250 , 500 }","lstm hidden states in { 250 , 500 }",0.5127499103546143
translation,120,198,hyperparameters,size of word embedding,in,"{ 250 , 500 }","size of word embedding in { 250 , 500 }",0.5131679773330688
translation,120,198,hyperparameters,two models,replacing,bidirectional encoder,two models replacing bidirectional encoder,0.6654748320579529
translation,120,198,hyperparameters,two models,replacing,bidirectional encoder,two models replacing bidirectional encoder,0.6654748320579529
translation,120,198,hyperparameters,bidirectional encoder,with,unidirectional encoder,bidirectional encoder with unidirectional encoder,0.6474083662033081
translation,120,198,hyperparameters,bidirectional encoder,with,different merge action,bidirectional encoder with different merge action,0.6353809833526611
translation,120,198,hyperparameters,bidirectional encoder,with,different merge action,bidirectional encoder with different merge action,0.6353809833526611
translation,120,198,hyperparameters,bidirectional encoder,with,different merge action,bidirectional encoder with different merge action,0.6353809833526611
translation,120,198,hyperparameters,hyperparameters,prepared,10 models,hyperparameters prepared 10 models,0.5388389825820923
translation,120,200,hyperparameters,80 %,of,training data,80 % of training data,0.6096383333206177
translation,120,200,hyperparameters,10 models,for,baggingensemble,10 models for baggingensemble,0.5965688824653625
translation,120,200,hyperparameters,training data,has,10 times,training data has 10 times,0.5763189792633057
translation,120,200,hyperparameters,hyperparameters,randomly extracted,80 %,hyperparameters randomly extracted 80 %,0.7419142723083496
translation,120,200,hyperparameters,hyperparameters,prepared,10 models,hyperparameters prepared 10 models,0.5388389825820923
translation,120,6,model,"alternative , simple but effective unsupervised ensemble method",combines,multiple models,"alternative , simple but effective unsupervised ensemble method combines multiple models",0.7118158340454102
translation,120,6,model,multiple models,by selecting,majority - like output,multiple models by selecting majority - like output,0.7192264795303345
translation,120,6,model,majority - like output,in,post-processing,majority - like output in post-processing,0.5439203977584839
translation,120,6,model,"alternative , simple but effective unsupervised ensemble method",has,post-ensemble,"alternative , simple but effective unsupervised ensemble method has post-ensemble",0.5472844243049622
translation,120,6,model,model,propose,"alternative , simple but effective unsupervised ensemble method","model propose alternative , simple but effective unsupervised ensemble method",0.6654871106147766
translation,120,13,model,alternative method,for,model ensemble,alternative method for model ensemble,0.616943895816803
translation,120,13,model,model ensemble,inspired by,majority vote,model ensemble inspired by majority vote,0.6660098433494568
translation,120,13,model,model,propose,alternative method,model propose alternative method,0.6553483605384827
translation,120,18,model,unsupervised method,for selecting,majority - like output,unsupervised method for selecting majority - like output,0.7116602659225464
translation,120,18,model,majority - like output,close to,other outputs,majority - like output close to other outputs,0.737248957157135
translation,120,18,model,model,propose,unsupervised method,model propose unsupervised method,0.7210264205932617
translation,120,21,model,output selection,in,classification tasks,output selection in classification tasks,0.527845025062561
translation,120,129,model,bidirectional lstm,for,encoder,bidirectional lstm for encoder,0.5897487998008728
translation,120,129,model,stacked lstm,with,input feeding,stacked lstm with input feeding,0.6642047166824341
translation,120,129,model,input feeding,for,decoder,input feeding for decoder,0.65937340259552
translation,120,129,model,model,consisted of,bidirectional lstm,model consisted of bidirectional lstm,0.5989060997962952
translation,120,129,model,model,consisted of,stacked lstm,model consisted of stacked lstm,0.5866831541061401
translation,120,7,results,our method,closely related to,kernel density estimation,our method closely related to kernel density estimation,0.6420486569404602
translation,120,7,results,kernel density estimation,based on,von mises - fisher kernel,kernel density estimation based on von mises - fisher kernel,0.6376808881759644
translation,120,7,results,results,theoretically prove,our method,results theoretically prove our method,0.6520180106163025
translation,120,159,results,results,for,gigaword dataset,results for gigaword dataset,0.510891318321228
translation,120,160,results,variant of our post-ensemble method,has,postcosb,variant of our post-ensemble method has postcosb,0.6049692630767822
translation,120,160,results,variant of our post-ensemble method,has,clearly outperformed,variant of our post-ensemble method has clearly outperformed,0.5972119569778442
translation,120,160,results,postcosb,has,clearly outperformed,postcosb has clearly outperformed,0.5904655456542969
translation,120,160,results,clearly outperformed,has,runtime- ensemble methods,clearly outperformed has runtime- ensemble methods,0.6032097935676575
translation,120,160,results,runtime- ensemble methods,has,strong baselines,runtime- ensemble methods has strong baselines,0.5135951638221741
translation,120,160,results,results,see that,variant of our post-ensemble method,results see that variant of our post-ensemble method,0.6533114910125732
translation,120,176,results,results,has,effect of number of models,results has effect of number of models,0.5509158372879028
translation,120,181,results,37.48 rouge - 1 score,with,32 models,37.48 rouge - 1 score with 32 models,0.5947193503379822
translation,120,181,results,32 models,better than,state - of - the - art results,32 models better than state - of - the - art results,0.7078776359558105
translation,120,184,results,lexrank,did,work well,lexrank did work well,0.5893504619598389
translation,120,184,results,lexrank,not,work well,lexrank not work well,0.6744500398635864
translation,120,184,results,results,has,lexrank,results has lexrank,0.5527518391609192
translation,120,205,results,all variants,of,our post-ensemble method,all variants of our post-ensemble method,0.554573655128479
translation,120,205,results,performed better,than,current runtime -ensemble methods,performed better than current runtime -ensemble methods,0.5585909485816956
translation,120,205,results,performed better,than,enssum and ensmul,performed better than enssum and ensmul,0.6021835207939148
translation,120,205,results,performed better,for,all variations of model preparation,performed better for all variations of model preparation,0.6230836510658264
translation,120,205,results,all variants,has,performed better,all variants has performed better,0.6273545026779175
translation,120,205,results,our post-ensemble method,has,performed better,our post-ensemble method has performed better,0.6040007472038269
translation,120,206,results,row,for,postcose,row for postcose,0.6874256730079651
translation,120,206,results,random-ensemble,was,most effective,random-ensemble was most effective,0.6330063939094543
translation,120,206,results,self-ensemble,was,worst,self-ensemble was worst,0.6770767569541931
translation,120,206,results,row,has,random-ensemble,row has random-ensemble,0.627388060092926
translation,120,206,results,postcose,has,random-ensemble,postcose has random-ensemble,0.6188186407089233
translation,120,206,results,results,Looking at,row,results Looking at row,0.5916851162910461
translation,120,207,results,relatively effective,for,post-ensemble,relatively effective for post-ensemble,0.6521649956703186
translation,120,207,results,relatively effective,according to,relative improvement,relatively effective according to relative improvement,0.655662477016449
translation,120,207,results,post-ensemble,according to,relative improvement,post-ensemble according to relative improvement,0.6218970417976379
translation,120,207,results,relative improvement,from,single,relative improvement from single,0.6300390958786011
translation,120,207,results,results,has,bagging -ensemble,results has bagging -ensemble,0.5620937347412109
translation,121,171,ablation-analysis,increase,of,document length,increase of document length,0.5930605530738831
translation,121,171,ablation-analysis,improvement,produced by,ta,improvement produced by ta,0.5329290628433228
translation,121,171,ablation-analysis,ta,gets,more evident,ta gets more evident,0.706555187702179
translation,121,171,ablation-analysis,increase,has,improvement,increase has improvement,0.6119239330291748
translation,121,171,ablation-analysis,document length,has,improvement,document length has improvement,0.5336416363716125
translation,121,182,ablation-analysis,summarization performance,in,different degrees,summarization performance in different degrees,0.5246762037277222
translation,121,227,ablation-analysis,unilm,cut,cost,unilm cut cost,0.6757221221923828
translation,121,227,ablation-analysis,india 's bank,cut,cost,india 's bank cut cost,0.6425646543502808
translation,121,227,ablation-analysis,cost,from of borrowing,0.25 % to 2.5 %,cost from of borrowing 0.25 % to 2.5 %,0.7611305713653564
translation,121,227,ablation-analysis,0.25 % to 2.5 %,for improving,economy,0.25 % to 2.5 % for improving economy,0.7181112170219421
translation,121,227,ablation-analysis,unilm,has,india 's bank,unilm has india 's bank,0.6094900965690613
translation,121,227,ablation-analysis,ablation analysis,has,unilm,ablation analysis has unilm,0.5655333995819092
translation,121,228,baselines,mass + ta,cut,cost of borrowing,mass + ta cut cost of borrowing,0.5956947207450867
translation,121,228,baselines,india 's central bank,cut,cost of borrowing,india 's central bank cut cost of borrowing,0.5616552233695984
translation,121,228,baselines,cost of borrowing,to,six -year low,cost of borrowing to six -year low,0.5518674254417419
translation,121,228,baselines,six -year low,to boost,growth,six -year low to boost growth,0.7082192301750183
translation,121,228,baselines,mass + ta,has,india 's central bank,mass + ta has india 's central bank,0.611258864402771
translation,121,228,baselines,baselines,has,mass + ta,baselines has mass + ta,0.5583746433258057
translation,121,6,model,semantics,learned by,topic model,semantics learned by topic model,0.6072694063186646
translation,121,6,model,semantics,propose,topic assistant ( ta ),semantics propose topic assistant ( ta ),0.6517406702041626
translation,121,6,model,topic assistant ( ta ),including,three modules,topic assistant ( ta ) including three modules,0.6948966383934021
translation,121,6,model,model,rearrange and explore,semantics,model rearrange and explore semantics,0.6390950679779053
translation,121,6,model,model,propose,topic assistant ( ta ),model propose topic assistant ( ta ),0.6707099676132202
translation,121,29,model,semantics,of,topic model,semantics of topic model,0.49215927720069885
translation,121,29,model,friendly topic assistant ( ta ),for,transformer - based abstractive summarization models,friendly topic assistant ( ta ) for transformer - based abstractive summarization models,0.5963255167007446
translation,121,29,model,model,rearrange and further explore,semantics,model rearrange and further explore semantics,0.6318724751472473
translation,121,30,model,ta,is,flexible plug-and - play model,ta is flexible plug-and - play model,0.5850279331207275
translation,121,30,model,flexible plug-and - play model,consisting of,three modules,flexible plug-and - play model consisting of three modules,0.7200966477394104
translation,121,30,model,finetuning stage,has,ta,finetuning stage has ta,0.5713291168212891
translation,121,239,model,semantics,of,topic model,semantics of topic model,0.49215927720069885
translation,121,239,model,friendly plug-and - play ta,for,transformer - based abstractive summarization models,friendly plug-and - play ta for transformer - based abstractive summarization models,0.6229680776596069
translation,121,239,model,model,explore and rearrange,semantics,model explore and rearrange semantics,0.6657848954200745
translation,121,156,results,bertsum +ta,achieves,superior performance,bertsum +ta achieves superior performance,0.7057806849479675
translation,121,156,results,superior performance,than,bertsum,superior performance than bertsum,0.6208585500717163
translation,121,156,results,superior performance,with,"mass ( song et al. , 2019 )","superior performance with mass ( song et al. , 2019 )",0.5995647311210632
translation,121,156,results,ta,has,bertsum +ta,ta has bertsum +ta,0.6345080733299255
translation,121,156,results,results,Equipped with,ta,results Equipped with ta,0.591230571269989
translation,121,156,results,results,Equipped with,bertsum +ta,results Equipped with bertsum +ta,0.645141065120697
translation,121,164,results,ta,able to improve,different types of transformer encoder - decoder models,ta able to improve different types of transformer encoder - decoder models,0.7191586494445801
translation,121,164,results,different types of transformer encoder - decoder models,for,abstractive summarization,different types of transformer encoder - decoder models for abstractive summarization,0.58952796459198
translation,121,164,results,abstractive summarization,with,few extra parameters,abstractive summarization with few extra parameters,0.5811504125595093
translation,121,164,results,results,observed,ta,results observed ta,0.5818644165992737
translation,121,170,results,bertsum,ignores,subsequent document-tokens,bertsum ignores subsequent document-tokens,0.7252493500709534
translation,121,170,results,bertsum,able to reserve,information,bertsum able to reserve information,0.7821077108383179
translation,121,170,results,bertsum + ta,able to reserve,information,bertsum + ta able to reserve information,0.7652339339256287
translation,121,170,results,information,in,some degree,information in some degree,0.504801332950592
translation,121,170,results,bertsum,has,bertsum + ta,bertsum has bertsum + ta,0.6046249270439148
translation,121,170,results,subsequent document-tokens,has,bertsum + ta,subsequent document-tokens has bertsum + ta,0.6190720200538635
translation,121,170,results,results,Compared with,bertsum,results Compared with bertsum,0.7089589238166809
translation,121,179,results,generated summaries,closer to,have higher similarities,generated summaries closer to have higher similarities,0.6668261289596558
translation,121,179,results,generated summaries,closer to,ground truth,generated summaries closer to ground truth,0.6438463926315308
translation,121,179,results,have higher similarities,to ),document,have higher similarities to ) document,0.630881130695343
translation,121,179,results,ground truth,in,semantic space,ground truth in semantic space,0.5079248547554016
translation,121,179,results,ta,has,generated summaries,ta has generated summaries,0.6380828619003296
translation,121,179,results,results,with the help of,ta,results with the help of ta,0.6185504198074341
translation,121,185,results,global semantics ( topics and topic proportions ),into,transformer - based models,global semantics ( topics and topic proportions ) into transformer - based models,0.5666982531547546
translation,121,185,results,results,Compared with,sia,results Compared with sia,0.7073484659194946
translation,121,229,results,unilm +ta,cut,cost,unilm +ta cut cost,0.6674637794494629
translation,121,229,results,india 's central bank,cut,cost,india 's central bank cut cost,0.6312888264656067
translation,121,229,results,cost,from of borrowing,six -year low,cost from of borrowing six -year low,0.7800896167755127
translation,121,229,results,six -year low,for improving,economy,six -year low for improving economy,0.6885801553726196
translation,121,229,results,unilm +ta,has,india 's central bank,unilm +ta has india 's central bank,0.6050665974617004
translation,121,229,results,results,has,unilm +ta,results has unilm +ta,0.531988799571991
translation,122,153,ablation-analysis,structural -compression and structural - coverage regularization,has,significantly affect,structural -compression and structural - coverage regularization has significantly affect,0.5753564238548279
translation,122,153,ablation-analysis,significantly affect,has,summarization performance,significantly affect has summarization performance,0.5563437342643738
translation,122,153,ablation-analysis,ablation analysis,both,structural -compression and structural - coverage regularization,ablation analysis both structural -compression and structural - coverage regularization,0.6635605096817017
translation,122,155,ablation-analysis,structural -compression and structural - coverage regularization,based on,our hierarchical model,structural -compression and structural - coverage regularization based on our hierarchical model,0.6506112813949585
translation,122,155,ablation-analysis,our hierarchical model,have,significant contributions,our hierarchical model have significant contributions,0.48121505975723267
translation,122,155,ablation-analysis,significant contributions,increase of,rouge scores,significant contributions increase of rouge scores,0.6322365999221802
translation,122,155,ablation-analysis,ablation analysis,conclude that,structural -compression and structural - coverage regularization,ablation analysis conclude that structural -compression and structural - coverage regularization,0.6166054606437683
translation,122,159,ablation-analysis,most samples ( over 95 % ),fall into,righttop area,most samples ( over 95 % ) fall into righttop area,0.699987530708313
translation,122,159,ablation-analysis,righttop area,in,human-made summaries,righttop area in human-made summaries,0.5290305614471436
translation,122,159,ablation-analysis,righttop area,indicates,high structural - compression,righttop area indicates high structural - compression,0.7092288732528687
translation,122,159,ablation-analysis,righttop area,indicates,structural -,righttop area indicates structural -,0.7181392312049866
translation,122,159,ablation-analysis,ablation analysis,shows that,most samples ( over 95 % ),ablation analysis shows that most samples ( over 95 % ),0.6804256439208984
translation,122,161,ablation-analysis,structural regularization,based on,our hierarchical encoder-decoder,structural regularization based on our hierarchical encoder-decoder,0.6200537085533142
translation,122,161,ablation-analysis,our hierarchical encoder-decoder,with,hybrid attention model,our hierarchical encoder-decoder with hybrid attention model,0.6198185682296753
translation,122,161,ablation-analysis,ablation analysis,has,structural regularization,ablation analysis has structural regularization,0.4944537878036499
translation,122,123,baselines,abstractive models,include,seq2seq - baseline,abstractive models include seq2seq - baseline,0.5852637887001038
translation,122,123,baselines,abstractive models,incorporates with,copy mechanism,abstractive models incorporates with copy mechanism,0.6159095168113708
translation,122,123,baselines,seq2seq - baseline,uses,basic seq2seq encoder-decoder architecture,seq2seq - baseline uses basic seq2seq encoder-decoder architecture,0.5913127660751343
translation,122,123,baselines,seq2seq - baseline,incorporates with,copy mechanism,seq2seq - baseline incorporates with copy mechanism,0.6966759562492371
translation,122,123,baselines,basic seq2seq encoder-decoder architecture,with,attention mechanism,basic seq2seq encoder-decoder architecture with attention mechanism,0.604383647441864
translation,122,123,baselines,copy mechanism,to alleviate,oov problem,copy mechanism to alleviate oov problem,0.7057797908782959
translation,122,123,baselines,baselines,has,abstractive models,baselines has abstractive models,0.5310056209564209
translation,122,124,baselines,abs - temp-attn,uses,temporal attention,abs - temp-attn uses temporal attention,0.5731520056724548
translation,122,124,baselines,temporal attention,on,seq2seq architecture,temporal attention on seq2seq architecture,0.5379968881607056
translation,122,124,baselines,temporal attention,to overcome,repetition problem,temporal attention to overcome repetition problem,0.6573097109794617
translation,122,125,baselines,point-cov,is,extension,point-cov is extension,0.6379739046096802
translation,122,125,baselines,extension,of,seq2seq - baseline model,extension of seq2seq - baseline model,0.5685723423957825
translation,122,125,baselines,extension,by importing,word-coverage mechanism,extension by importing word-coverage mechanism,0.7286916375160217
translation,122,125,baselines,seq2seq - baseline model,by importing,word-coverage mechanism,seq2seq - baseline model by importing word-coverage mechanism,0.6925541162490845
translation,122,125,baselines,word-coverage mechanism,to reduce,repetitions,word-coverage mechanism to reduce repetitions,0.6938777565956116
translation,122,125,baselines,repetitions,in,summary,repetitions in summary,0.5827242732048035
translation,122,126,baselines,graph-attention,uses,graph- ranking based attention mechanism,graph-attention uses graph- ranking based attention mechanism,0.5936482548713684
translation,122,126,baselines,graph- ranking based attention mechanism,based on,hierarchical architecture,graph- ranking based attention mechanism based on hierarchical architecture,0.65113765001297
translation,122,126,baselines,hierarchical architecture,to identify,important sentences,hierarchical architecture to identify important sentences,0.661461591720581
translation,122,126,baselines,baselines,has,graph-attention,baselines has graph-attention,0.562926173210144
translation,122,127,baselines,hierachical - baseline,uses,basic hierarchical encoder-decoder,hierachical - baseline uses basic hierarchical encoder-decoder,0.6035751700401306
translation,122,127,baselines,basic hierarchical encoder-decoder,with,hybrid attention model,basic hierarchical encoder-decoder with hybrid attention model,0.6071078777313232
translation,122,113,experimental-setup,dimension,of,word embeddings,dimension of word embeddings,0.5582399964332581
translation,122,113,experimental-setup,word embeddings,is,128,word embeddings is 128,0.5239951610565186
translation,122,113,experimental-setup,128,learned from,scratch,128 learned from scratch,0.7191184163093567
translation,122,113,experimental-setup,scratch,during,training,scratch during training,0.7423295378684998
translation,122,113,experimental-setup,experimental setup,has,dimension,experimental setup has dimension,0.4825572967529297
translation,122,114,experimental-setup,vocabulary,of,50 k words,vocabulary of 50 k words,0.6213000416755676
translation,122,114,experimental-setup,50 k words,for,both the encoder and decoder,50 k words for both the encoder and decoder,0.600631833076477
translation,122,114,experimental-setup,experimental setup,use,vocabulary,experimental setup use vocabulary,0.6008449792861938
translation,122,115,experimental-setup,our model,on,single tesla k40 m gpu,our model on single tesla k40 m gpu,0.5320550799369812
translation,122,115,experimental-setup,single tesla k40 m gpu,with,batch size,single tesla k40 m gpu with batch size,0.6339405179023743
translation,122,115,experimental-setup,batch size,of,16,batch size of 16,0.6842944622039795
translation,122,115,experimental-setup,experimental setup,trained,our model,experimental setup trained our model,0.7027952075004578
translation,122,116,experimental-setup,convergence,reached within,300 epochs,convergence reached within 300 epochs,0.6922215223312378
translation,122,116,experimental-setup,experimental setup,has,convergence,experimental setup has convergence,0.5458384156227112
translation,122,117,experimental-setup,tuning,on,validation set,tuning on validation set,0.5899093151092529
translation,122,117,experimental-setup,"parameters ? 1 , ? 2 , ? 1 and ? 2",set as,"- 0.5 , - 1.0 , 1.2 and 1.4","parameters ? 1 , ? 2 , ? 1 and ? 2 set as - 0.5 , - 1.0 , 1.2 and 1.4",0.5988965630531311
translation,122,117,experimental-setup,tuning,has,"parameters ? 1 , ? 2 , ? 1 and ? 2","tuning has parameters ? 1 , ? 2 , ? 1 and ? 2",0.5880516171455383
translation,122,117,experimental-setup,validation set,has,"parameters ? 1 , ? 2 , ? 1 and ? 2","validation set has parameters ? 1 , ? 2 , ? 1 and ? 2",0.5756846070289612
translation,122,118,experimental-setup,test time,use,hierarchical decoding algorithm,test time use hierarchical decoding algorithm,0.6632199883460999
translation,122,118,experimental-setup,hierarchical decoding algorithm,with,sentence - level beam size 4,hierarchical decoding algorithm with sentence - level beam size 4,0.6217580437660217
translation,122,118,experimental-setup,hierarchical decoding algorithm,with,word - level beam size 8,hierarchical decoding algorithm with word - level beam size 8,0.6427662372589111
translation,122,118,experimental-setup,experimental setup,At,test time,experimental setup At test time,0.5272576808929443
translation,122,6,model,structural information,of,documents,structural information of documents,0.6201505064964294
translation,122,6,model,structural information,of,multi-sentence summaries,structural information of multi-sentence summaries,0.5439438819885254
translation,122,6,model,structural information,both,documents,structural information both documents,0.6878575682640076
translation,122,6,model,structural information,both,multi-sentence summaries,structural information both multi-sentence summaries,0.6206088662147522
translation,122,6,model,structural information,to improve,document summarization performance,structural information to improve document summarization performance,0.6190669536590576
translation,122,6,model,model,leverage,structural information,model leverage structural information,0.7760136127471924
translation,122,7,model,structural -compression and structuralcoverage regularization,into,summarization process,structural -compression and structuralcoverage regularization into summarization process,0.5655584335327148
translation,122,7,model,summarization process,to capture,information compression and information coverage properties,summarization process to capture information compression and information coverage properties,0.7012364268302917
translation,122,7,model,model,import,structural -compression and structuralcoverage regularization,model import structural -compression and structuralcoverage regularization,0.687197208404541
translation,122,83,model,structural - compression and structural - coverage properties,based on,hierarchical encoder-decoder model,structural - compression and structural - coverage properties based on hierarchical encoder-decoder model,0.6764980554580688
translation,122,83,model,structural regularization,during both,model learning phase,structural regularization during both model learning phase,0.6358702182769775
translation,122,83,model,structural regularization,during both,inference phase,structural regularization during both inference phase,0.6607529520988464
translation,122,83,model,model,model,structural - compression and structural - coverage properties,model model structural - compression and structural - coverage properties,0.7810351252555847
translation,122,128,results,significantly outperforms,has,all the neural abstractive baselines,significantly outperforms has all the neural abstractive baselines,0.5986316204071045
translation,122,129,results,performance,of,hierarchical - baseline model,performance of hierarchical - baseline model,0.584644615650177
translation,122,129,results,hierarchical - baseline model,are,lower,hierarchical - baseline model are lower,0.5878486037254333
translation,122,129,results,lower,than,seq2seq - baseline model,lower than seq2seq - baseline model,0.5744253993034363
translation,122,130,results,hierarchical - baseline model,by,4 rouge points,hierarchical - baseline model by 4 rouge points,0.5447959303855896
translation,122,130,results,hierarchical - baseline model,more than,4 rouge points,hierarchical - baseline model more than 4 rouge points,0.5494236350059509
translation,122,130,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,122,130,results,outperforms,has,hierarchical - baseline model,outperforms has hierarchical - baseline model,0.6010619401931763
translation,122,130,results,results,has,our model,results has our model,0.5871725678443909
translation,122,132,results,our model,better at generating,long summary,our model better at generating long summary,0.7072353363037109
translation,122,132,results,long summary,than,seq2seq model,long summary than seq2seq model,0.5450091361999512
translation,122,132,results,results,demonstrate,our model,results demonstrate our model,0.6473821401596069
translation,122,143,results,our model,has,consistently outperforms,our model has consistently outperforms,0.6076288819313049
translation,122,143,results,consistently outperforms,has,seq2seq - baseline model,consistently outperforms has seq2seq - baseline model,0.5628302097320557
translation,122,143,results,results,show,our model,results show our model,0.6888449192047119
translation,122,146,results,summaries,generated by,our method,summaries generated by our method,0.6796572208404541
translation,122,146,results,summaries,contains,more salient information,summaries contains more salient information,0.6383532285690308
translation,122,146,results,summaries,are,more concise,summaries are more concise,0.5860239863395691
translation,122,146,results,our method,contains,more salient information,our method contains more salient information,0.6022999286651611
translation,122,146,results,more concise,through,sentences compression,more concise through sentences compression,0.6667558550834656
translation,122,146,results,more concise,shows,effectiveness,more concise shows effectiveness,0.6670190095901489
translation,122,146,results,sentences compression,shows,effectiveness,sentences compression shows effectiveness,0.634468674659729
translation,122,146,results,effectiveness,of,structural regularization,effectiveness of structural regularization,0.5690996050834656
translation,122,146,results,results,has,summaries,results has summaries,0.5243220329284668
translation,122,147,results,sentence - level modeling,of,document and summary,sentence - level modeling of document and summary,0.5625544190406799
translation,122,147,results,document and summary,in,our model,document and summary in our model,0.5615319609642029
translation,122,147,results,document and summary,makes,generated summaries,document and summary makes generated summaries,0.69333416223526
translation,122,147,results,our model,makes,generated summaries,our model makes generated summaries,0.6870113611221313
translation,122,147,results,generated summaries,achieve,better inter-sentence coherence,generated summaries achieve better inter-sentence coherence,0.5704634785652161
translation,122,147,results,results,show,sentence - level modeling,results show sentence - level modeling,0.5746984481811523
translation,122,152,results,much outperforms,verifies,effectiveness,much outperforms verifies effectiveness,0.6668220162391663
translation,122,152,results,our method,has,much outperforms,our method has much outperforms,0.6259496808052063
translation,122,152,results,much outperforms,has,all the compared systems,much outperforms has all the compared systems,0.6075527667999268
translation,122,152,results,results,has,our method,results has our method,0.5589964985847473
translation,122,162,results,summary,covers,more salient information,summary covers more salient information,0.7139533758163452
translation,122,162,results,summary,contains,very few repetitions,summary contains very few repetitions,0.6692977547645569
translation,122,162,results,results,has,summary,results has summary,0.5453867316246033
translation,123,82,ablation-analysis,base model,on,original datasets,base model on original datasets,0.5403438806533813
translation,123,82,ablation-analysis,base model,observed,performance,base model observed performance,0.7367082834243774
translation,123,82,ablation-analysis,drops,by,"0.60 , 0.72 bleu","drops by 0.60 , 0.72 bleu",0.6195300221443176
translation,123,82,ablation-analysis,drops,by,"1.66 , 2.09 rouge -l points","drops by 1.66 , 2.09 rouge -l points",0.6041717529296875
translation,123,82,ablation-analysis,"1.66 , 2.09 rouge -l points",for,java and python datasets,"1.66 , 2.09 rouge -l points for java and python datasets",0.5466321110725403
translation,123,82,ablation-analysis,performance,has,drops,performance has drops,0.5993483662605286
translation,123,82,ablation-analysis,ablation analysis,ran,base model,ablation analysis ran base model,0.6364228129386902
translation,123,91,experiments,directional information,while modeling,pairwise relationship,directional information while modeling pairwise relationship,0.7241706848144531
translation,123,91,experiments,clipping distance,has,k,clipping distance has k,0.5786294341087341
translation,123,73,hyperparameters,transformer models,using,"adam optimizer ( kingma and ba , 2015 )","transformer models using adam optimizer ( kingma and ba , 2015 )",0.6587966084480286
translation,123,73,hyperparameters,"adam optimizer ( kingma and ba , 2015 )",with,initial learning rate,"adam optimizer ( kingma and ba , 2015 ) with initial learning rate",0.5927683115005493
translation,123,73,hyperparameters,initial learning rate,of,10 ?4,initial learning rate of 10 ?4,0.6392531991004944
translation,123,73,hyperparameters,hyperparameters,train,transformer models,hyperparameters train transformer models,0.6927379369735718
translation,123,74,hyperparameters,mini-batch size and dropout rate,to,32 and 0.2,mini-batch size and dropout rate to 32 and 0.2,0.5564597845077515
translation,123,74,hyperparameters,hyperparameters,set,mini-batch size and dropout rate,hyperparameters set mini-batch size and dropout rate,0.6019110679626465
translation,123,75,hyperparameters,transformer models,for,maximum,transformer models for maximum,0.626340925693512
translation,123,75,hyperparameters,maximum,of,200 epochs,maximum of 200 epochs,0.587211549282074
translation,123,75,hyperparameters,early stop,if,validation performance,early stop if validation performance,0.6208859086036682
translation,123,75,hyperparameters,does not improve,for,20 consecutive iterations,does not improve for 20 consecutive iterations,0.6248425245285034
translation,123,75,hyperparameters,validation performance,has,does not improve,validation performance has does not improve,0.610643744468689
translation,123,75,hyperparameters,hyperparameters,train,transformer models,hyperparameters train transformer models,0.6927379369735718
translation,123,76,hyperparameters,beam search,during,inference,beam search during inference,0.6603623628616333
translation,123,76,hyperparameters,beam search,set,beam size,beam search set beam size,0.6893483400344849
translation,123,76,hyperparameters,beam size,to,4,beam size to 4,0.6383150219917297
translation,123,76,hyperparameters,hyperparameters,use,beam search,hyperparameters use beam search,0.6655463576316833
translation,123,76,hyperparameters,hyperparameters,set,beam size,hyperparameters set beam size,0.6436296701431274
translation,123,6,model,code representation,explore,transformer model,code representation explore transformer model,0.652454674243927
translation,123,6,model,transformer model,uses,self-attention mechanism,transformer model uses self-attention mechanism,0.5384237766265869
translation,123,6,model,model,To learn,code representation,model To learn code representation,0.5770130157470703
translation,123,23,model,pairwise relationship,between,source code tokens,pairwise relationship between source code tokens,0.6420106887817383
translation,123,23,model,pairwise relationship,achieve,significant improvements,pairwise relationship achieve significant improvements,0.6355451941490173
translation,123,23,model,source code tokens,using,relative position representation,source code tokens using relative position representation,0.6249630451202393
translation,123,23,model,significant improvements,over learning,sequence information,significant improvements over learning sequence information,0.6851058006286621
translation,123,23,model,sequence information,of,code tokens,sequence information of code tokens,0.5729379057884216
translation,123,23,model,sequence information,using,absolute position representation,sequence information using absolute position representation,0.6518321633338928
translation,123,23,model,code tokens,using,absolute position representation,code tokens using absolute position representation,0.6655691862106323
translation,123,23,model,model,modeling,pairwise relationship,model modeling pairwise relationship,0.831407904624939
translation,123,23,model,model,achieve,significant improvements,model achieve significant improvements,0.6491693258285522
translation,123,33,model,each layer,performs,self-attention mechanism,each layer performs self-attention mechanism,0.6081052422523499
translation,123,33,model,multi-head attention,employs,h attention heads,multi-head attention employs h attention heads,0.5646092295646667
translation,123,33,model,multi-head attention,performs,self-attention mechanism,multi-head attention performs self-attention mechanism,0.555252730846405
translation,123,33,model,each layer,has,multi-head attention,each layer has multi-head attention,0.564212441444397
translation,123,33,model,model,At,each layer,model At each layer,0.5384435057640076
translation,123,39,model,additional attention layer,to learn,copy distribution,additional attention layer to learn copy distribution,0.6287062764167786
translation,123,39,model,copy distribution,on top of,decoder stack,copy distribution on top of decoder stack,0.7032915949821472
translation,123,39,model,model,use,additional attention layer,model use additional attention layer,0.6693074703216553
translation,123,80,results,base model,has,outperforms,base model has outperforms,0.6523337364196777
translation,123,80,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
translation,123,80,results,full model,has,improves,full model has improves,0.613627016544342
translation,123,80,results,improves,has,performance,improves has performance,0.5770372748374939
translation,123,80,results,performance,has,further,performance has further,0.6324895024299622
translation,123,80,results,results,shows,base model,results shows base model,0.7049206495285034
translation,123,85,results,copy attention,improves,performance,copy attention improves performance,0.672670304775238
translation,123,85,results,0.44 and 0.88 bleu points,for,java and python datasets,0.44 and 0.88 bleu points for java and python datasets,0.5855094790458679
translation,123,85,results,performance,has,0.44 and 0.88 bleu points,performance has 0.44 and 0.88 bleu points,0.5721708536148071
translation,123,88,results,absolute position,of,code tokens,absolute position of code tokens,0.5847769975662231
translation,123,88,results,absolute position,are,not effective,absolute position are not effective,0.5933716893196106
translation,123,88,results,learning,has,absolute position,learning has absolute position,0.5807147026062012
translation,123,88,results,learning,has,slightly hurts,learning has slightly hurts,0.5678166747093201
translation,123,88,results,slightly hurts,has,performance,slightly hurts has performance,0.5844202637672424
translation,123,88,results,results,demonstrates,learning,results demonstrates learning,0.6956058144569397
translation,123,90,results,pairwise relationship,between,source code tokens,pairwise relationship between source code tokens,0.6420106887817383
translation,123,90,results,pairwise relationship,via,relative position representations,pairwise relationship via relative position representations,0.7033926844596863
translation,123,90,results,source code tokens,via,relative position representations,source code tokens via relative position representations,0.6234752535820007
translation,123,90,results,results,learning,pairwise relationship,results learning pairwise relationship,0.6871828436851501
translation,124,184,baselines,"sum-basic ( vanderwende et al. , 2007 )",extracts,sentences,"sum-basic ( vanderwende et al. , 2007 ) extracts sentences",0.6580830812454224
translation,124,184,baselines,"sum-basic ( vanderwende et al. , 2007 )",extracts,sentences,"sum-basic ( vanderwende et al. , 2007 ) extracts sentences",0.6580830812454224
translation,124,184,baselines,sentences,by assuming,words,sentences by assuming words,0.6298226714134216
translation,124,184,baselines,sentences,to,summary,sentences to summary,0.5822682976722717
translation,124,184,baselines,words,occurring frequently in,document,words occurring frequently in document,0.7208064198493958
translation,124,184,baselines,words,have,higher chances,words have higher chances,0.5771306157112122
translation,124,184,baselines,higher chances,of being included in,summary,higher chances of being included in summary,0.7203943729400635
translation,124,184,baselines,higher chances,of being included in,summary,higher chances of being included in summary,0.7203943729400635
translation,124,184,baselines,"kl - sum ( haghighi and vanderwende , 2009 )",adds,sentences,"kl - sum ( haghighi and vanderwende , 2009 ) adds sentences",0.6362389922142029
translation,124,184,baselines,greedily,adds,sentences,greedily adds sentences,0.6953558921813965
translation,124,184,baselines,sentences,to,summary,sentences to summary,0.5822682976722717
translation,124,184,baselines,sentences,to minimize,kl divergence,sentences to minimize kl divergence,0.6335598230361938
translation,124,184,baselines,summary,to minimize,kl divergence,summary to minimize kl divergence,0.6425237059593201
translation,124,184,baselines,"lexrank ( erkan and radev , 2004 )",estimates,sentence importance,"lexrank ( erkan and radev , 2004 ) estimates sentence importance",0.6021203398704529
translation,124,184,baselines,sentence importance,based on,eigenvector centrality,sentence importance based on eigenvector centrality,0.6053450107574463
translation,124,184,baselines,eigenvector centrality,in,document graph representation,eigenvector centrality in document graph representation,0.47167617082595825
translation,124,184,baselines,"kl - sum ( haghighi and vanderwende , 2009 )",has,greedily,"kl - sum ( haghighi and vanderwende , 2009 ) has greedily",0.5955213308334351
translation,124,185,baselines,lead method,that selects,first n sentences,lead method that selects first n sentences,0.7004653811454773
translation,124,185,baselines,first n sentences,from,each document,first n sentences from each document,0.5854612588882446
translation,124,222,baselines,bert - abs - pg,generates,abstract,bert - abs - pg generates abstract,0.6301447153091431
translation,124,222,baselines,bert - abs - pg,decoding,summary sentences,bert - abs - pg decoding summary sentences,0.8240779042243958
translation,124,222,baselines,abstract,by iteratively encoding,singletons or pairs,abstract by iteratively encoding singletons or pairs,0.7050811052322388
translation,124,222,baselines,summary sentences,using,pointer - generator networks,summary sentences using pointer - generator networks,0.6540895700454712
translation,124,222,baselines,pointer - generator networks,has,),pointer - generator networks has ),0.631358802318573
translation,124,222,baselines,baselines,has,bert - abs - pg,baselines has bert - abs - pg,0.6134327054023743
translation,124,156,experiments,"duc -04 ( over and yen , 2004 )",has,benchmark multi-document summarization dataset,"duc -04 ( over and yen , 2004 ) has benchmark multi-document summarization dataset",0.5282001495361328
translation,124,8,model,sentence singletons and pairs together,in,unified space,sentence singletons and pairs together in unified space,0.5343173146247864
translation,124,24,model,singletons and pairs together,by,likelihoods of producing summary sentences,singletons and pairs together by likelihoods of producing summary sentences,0.5558366775512695
translation,124,24,model,model,ranking,singletons and pairs together,model ranking singletons and pairs together,0.7210422158241272
translation,124,27,model,method,to select,sentence singletons and pairs,method to select sentence singletons and pairs,0.6702866554260254
translation,124,27,model,sentence singletons and pairs,serve as,basis,sentence singletons and pairs serve as basis,0.5784728527069092
translation,124,27,model,basis,for,abstractive summarizer,basis for abstractive summarizer,0.6301761269569397
translation,124,27,model,abstractive summarizer,to compose,summary sentence - by-sentence,abstractive summarizer to compose summary sentence - by-sentence,0.6429646611213684
translation,124,27,model,summary sentence - by-sentence,where,singletons,summary sentence - by-sentence where singletons,0.578060507774353
translation,124,27,model,summary sentence - by-sentence,where,pairs,summary sentence - by-sentence where pairs,0.5936732292175293
translation,124,27,model,singletons,are,"shortened ( i.e. , compressed )","singletons are shortened ( i.e. , compressed )",0.525638997554779
translation,124,27,model,model,propose,method,model propose method,0.6280754208564758
translation,124,85,model,"bert architecture ( devlin et al. , 2018 )",to learn,instance representations,"bert architecture ( devlin et al. , 2018 ) to learn instance representations",0.6091892719268799
translation,124,85,model,model,exploit,"bert architecture ( devlin et al. , 2018 )","model exploit bert architecture ( devlin et al. , 2018 )",0.7135676741600037
translation,124,114,model,bert,learns,instance representations,bert learns instance representations,0.6315304040908813
translation,124,114,model,instance representations,by attending to,important content words,instance representations by attending to important content words,0.6124231815338135
translation,124,114,model,model,has,bert,model has bert,0.6085957288742065
translation,124,195,results,bert - singpairmix method,achieves,strong performance,bert - singpairmix method achieves strong performance,0.6557690501213074
translation,124,195,results,strong performance,to,capability of building effective representations,strong performance to capability of building effective representations,0.5217097401618958
translation,124,195,results,capability of building effective representations,for,singletons and pairs,capability of building effective representations for singletons and pairs,0.5957612991333008
translation,124,195,results,results,has,bert - singpairmix method,results has bert - singpairmix method,0.5602840781211853
translation,124,200,results,cnn / dm and xsum datasets,selecting,mixed set of singletons and pairs,cnn / dm and xsum datasets selecting mixed set of singletons and pairs,0.6571044325828552
translation,124,200,results,mixed set of singletons and pairs,based on,bert representations ( bert + singpairmix ),mixed set of singletons and pairs based on bert representations ( bert + singpairmix ),0.667682945728302
translation,124,200,results,mixed set of singletons and pairs,demonstrates,most competitive results,mixed set of singletons and pairs demonstrates most competitive results,0.6170899271965027
translation,124,200,results,results,On,cnn / dm and xsum datasets,results On cnn / dm and xsum datasets,0.5051889419555664
translation,124,201,results,number of strong baselines,evaluated on,full set of ground -truth sentences,number of strong baselines evaluated on full set of ground -truth sentences,0.6437843441963196
translation,124,201,results,outperforms,has,number of strong baselines,outperforms has number of strong baselines,0.6050106883049011
translation,124,201,results,results,has,outperforms,results has outperforms,0.6657275557518005
translation,124,209,results,bert model,performs,consistently,bert model performs consistently,0.7076157927513123
translation,124,209,results,consistently,on identifying,secondary sentences,consistently on identifying secondary sentences,0.6999620199203491
translation,124,209,results,vsm,yields,considerable performance gain,vsm yields considerable performance gain,0.7255969047546387
translation,124,209,results,considerable performance gain,on selecting,primary sentences,considerable performance gain on selecting primary sentences,0.6861734390258789
translation,124,209,results,results,find that,bert model,results find that bert model,0.6439866423606873
translation,124,223,results,bert summarization systems,achieve,results,bert summarization systems achieve results,0.6367711424827576
translation,124,223,results,results,on par with,prior work,results on par with prior work,0.6439887881278992
translation,124,223,results,results,has,largely,results has largely,0.4759385287761688
translation,124,223,results,results,has,bert summarization systems,results has bert summarization systems,0.5789092779159546
translation,124,224,results,abstractive counterparts,on,duc -04 and cnn / dm datasets,abstractive counterparts on duc -04 and cnn / dm datasets,0.5146726965904236
translation,124,224,results,extractive variant ( bert - extr ),has,outperform,extractive variant ( bert - extr ) has outperform,0.6288353204727173
translation,124,224,results,outperform,has,abstractive counterparts,outperform has abstractive counterparts,0.6046250462532043
translation,124,224,results,results,interesting to observe that,extractive variant ( bert - extr ),results interesting to observe that extractive variant ( bert - extr ),0.6623147130012512
translation,125,31,ablation-analysis,benefits,of,our model,benefits of our model,0.5777701139450073
translation,125,27,baselines,baselines,has,lstm,baselines has lstm,0.5395978093147278
translation,125,149,baselines,lexrank,has,"erkan and radev , 2004 )","lexrank has erkan and radev , 2004 )",0.6137660145759583
translation,125,211,baselines,bsl,is,model,bsl is model,0.6349786520004272
translation,125,211,baselines,model,with,sentence and local topic information,model with sentence and local topic information,0.5600304007530212
translation,125,211,baselines,model,with,sentence and global document information,model with sentence and global document information,0.577357828617096
translation,125,211,baselines,model,with,sentence and global document information,model with sentence and global document information,0.577357828617096
translation,125,211,baselines,full model,with,attentive context decoder,full model with attentive context decoder,0.6400365829467773
translation,125,211,baselines,baselines,has,bsl,baselines has bsl,0.5615102052688599
translation,125,134,experimental-setup,our model,using,adam optimizer,our model using adam optimizer,0.6453019976615906
translation,125,134,experimental-setup,our model,with,drop out rate,our model with drop out rate,0.647579550743103
translation,125,134,experimental-setup,adam optimizer,with,learning rate 0.0001,adam optimizer with learning rate 0.0001,0.6063497066497803
translation,125,134,experimental-setup,adam optimizer,with,drop out rate,adam optimizer with drop out rate,0.618686318397522
translation,125,134,experimental-setup,drop out rate,of,0.3,drop out rate of 0.3,0.596635103225708
translation,125,134,experimental-setup,experimental setup,train,our model,experimental setup train our model,0.6429243087768555
translation,125,135,experimental-setup,mini-batch,with,batch size,mini-batch with batch size,0.6728323698043823
translation,125,135,experimental-setup,mini-batch,with,size,mini-batch with size,0.6890663504600525
translation,125,135,experimental-setup,batch size,of,32 documents,batch size of 32 documents,0.598173975944519
translation,125,135,experimental-setup,size,of,gru hidden states,size of gru hidden states,0.6082197427749634
translation,125,135,experimental-setup,gru hidden states,is,300,gru hidden states is 300,0.642998993396759
translation,125,135,experimental-setup,experimental setup,use,mini-batch,experimental setup use mini-batch,0.5772721171379089
translation,125,136,experimental-setup,word embeddings,use,"glove ( pennington et al. , 2014 )","word embeddings use glove ( pennington et al. , 2014 )",0.5481976270675659
translation,125,136,experimental-setup,"glove ( pennington et al. , 2014 )",with,dimension,"glove ( pennington et al. , 2014 ) with dimension",0.6655246615409851
translation,125,136,experimental-setup,"glove ( pennington et al. , 2014 )",pre-trained on,wikipedia and gigaword,"glove ( pennington et al. , 2014 ) pre-trained on wikipedia and gigaword",0.7765293717384338
translation,125,136,experimental-setup,dimension,has,300,dimension has 300,0.6396656632423401
translation,125,136,experimental-setup,experimental setup,For,word embeddings,experimental setup For word embeddings,0.5087873935699463
translation,125,137,experimental-setup,vocabulary size,of,our model,vocabulary size of our model,0.5473134517669678
translation,125,137,experimental-setup,vocabulary size,is,50000,vocabulary size is 50000,0.6058894991874695
translation,125,137,experimental-setup,our model,is,50000,our model is 50000,0.6176478266716003
translation,125,137,experimental-setup,experimental setup,has,vocabulary size,experimental setup has vocabulary size,0.5189570188522339
translation,125,139,experimental-setup,best model,selected with,early stopping,best model selected with early stopping,0.6222785115242004
translation,125,139,experimental-setup,early stopping,on,validation set,early stopping on validation set,0.5784603357315063
translation,125,139,experimental-setup,validation set,according to,rouge - 2 f-score,validation set according to rouge - 2 f-score,0.5763542652130127
translation,125,139,experimental-setup,experimental setup,train,each model,experimental setup train each model,0.6811989545822144
translation,125,217,experimental-setup,https,:,//github.com,https : //github.com,0.6116153597831726
translation,125,4,model,novel neural singledocument extractive summarization,incorporating,local context,novel neural singledocument extractive summarization incorporating local context,0.6443964838981628
translation,125,4,model,local context,within,current topic,local context within current topic,0.6480318307876587
translation,125,4,model,model,propose,novel neural singledocument extractive summarization,model propose novel neural singledocument extractive summarization,0.6516663432121277
translation,125,188,model,novel extractive summarization model,designed for,long documents,novel extractive summarization model designed for long documents,0.6445704698562622
translation,125,188,model,novel extractive summarization model,by incorporating,local context,novel extractive summarization model by incorporating local context,0.6488902568817139
translation,125,188,model,local context,within,each topic,local context within each topic,0.6409129500389099
translation,125,188,model,local context,along with,global context,local context along with global context,0.6085417866706848
translation,125,188,model,global context,of,whole document,global context of whole document,0.5719507932662964
translation,125,188,model,model,propose,novel extractive summarization model,model propose novel extractive summarization model,0.6370922923088074
translation,125,189,model,neural extractive summarization,in,parameter lean and modular architecture,neural extractive summarization in parameter lean and modular architecture,0.5216153860092163
translation,125,6,results,benefits,of,our method,benefits of our method,0.5761390924453735
translation,125,6,results,benefits,become,stronger,benefits become stronger,0.6331393718719482
translation,125,6,results,our method,become,stronger,our method become stronger,0.6271513104438782
translation,125,6,results,our method,apply it to,longer documents,our method apply it to longer documents,0.6120093464851379
translation,125,6,results,stronger,as,longer documents,stronger as longer documents,0.5389730930328369
translation,125,6,results,stronger,apply it to,longer documents,stronger apply it to longer documents,0.6508116126060486
translation,125,28,results,more traditional methods,for capturing,local context,more traditional methods for capturing local context,0.7032498121261597
translation,125,28,results,local context,rely on,hierarchical structures,local context rely on hierarchical structures,0.7327942252159119
translation,125,28,results,lstm - minus,produces,simpler models,lstm - minus produces simpler models,0.6572577953338623
translation,125,28,results,simpler models,with,less parameters,simpler models with less parameters,0.6264357566833496
translation,125,28,results,simpler models,less prone to,overfitting,simpler models less prone to overfitting,0.6337403655052185
translation,125,28,results,more traditional methods,has,lstm - minus,more traditional methods has lstm - minus,0.5890607237815857
translation,125,28,results,local context,has,lstm - minus,local context has lstm - minus,0.6202629804611206
translation,125,28,results,results,With respect to,more traditional methods,results With respect to more traditional methods,0.6453967094421387
translation,125,29,results,method,on,pubmed and arxiv datasets,method on pubmed and arxiv datasets,0.5112394690513611
translation,125,29,results,effectively summarizing,has,long documents,effectively summarizing has long documents,0.5785662531852722
translation,125,29,results,results,test,method,results test method,0.6896118521690369
translation,125,30,results,baseline and previous approaches,by,narrow margin,baseline and previous approaches by narrow margin,0.5677436590194702
translation,125,30,results,narrow margin,on,both datasets,narrow margin on both datasets,0.4948643743991852
translation,125,30,results,benefit,of,our method,benefit of our method,0.5969009399414062
translation,125,30,results,benefit,become,much stronger,benefit become much stronger,0.5745247006416321
translation,125,30,results,our method,become,much stronger,our method become much stronger,0.5438908338546753
translation,125,30,results,our method,apply it to,longer documents,our method apply it to longer documents,0.6120093464851379
translation,125,30,results,much stronger,apply it to,longer documents,much stronger apply it to longer documents,0.6552637219429016
translation,125,30,results,outperform,has,baseline and previous approaches,outperform has baseline and previous approaches,0.5889123678207397
translation,125,163,results,outperforms,on,"informativeness ( rouge - 1,2 )","outperforms on informativeness ( rouge - 1,2 )",0.5385946035385132
translation,125,163,results,traditional extractive models,on,"informativeness ( rouge - 1,2 )","traditional extractive models on informativeness ( rouge - 1,2 )",0.49447330832481384
translation,125,163,results,traditional extractive models,by,wide margin,traditional extractive models by wide margin,0.567865788936615
translation,125,163,results,"informativeness ( rouge - 1,2 )",by,wide margin,"informativeness ( rouge - 1,2 ) by wide margin",0.5387864708900452
translation,125,163,results,both datasets,has,neural extractive models,both datasets has neural extractive models,0.5911522507667542
translation,125,163,results,neural extractive models,has,outperforms,neural extractive models has outperforms,0.6156789064407349
translation,125,163,results,outperforms,has,traditional extractive models,outperforms has traditional extractive models,0.5816203355789185
translation,125,166,results,neural abstractive models,on,"rouge - 1,2","neural abstractive models on rouge - 1,2",0.5681937336921692
translation,125,166,results,models,tend to have,highest rouge -l scores,models tend to have highest rouge -l scores,0.6817273497581482
translation,125,166,results,neural extractive models,has,dominate,neural extractive models has dominate,0.5695266723632812
translation,125,166,results,dominate,has,neural abstractive models,dominate has neural abstractive models,0.607907772064209
translation,125,166,results,results,has,neural extractive models,results has neural extractive models,0.5609817504882812
translation,125,167,results,our models,with,attentive context and concatenation decoder,our models with attentive context and concatenation decoder,0.6273760795593262
translation,125,167,results,our models,have,better performances,our models have better performances,0.5797327160835266
translation,125,167,results,better performances,on,all three rouge scores,better performances on all three rouge scores,0.47723910212516785
translation,125,167,results,better performances,on,meteor,better performances on meteor,0.5856173634529114
translation,125,167,results,other neural extractive models,has,our models,other neural extractive models has our models,0.5611090064048767
translation,125,167,results,results,Compared with,other neural extractive models,results Compared with other neural extractive models,0.670152485370636
translation,125,169,results,baseline model,achieves,slightly better performance,baseline model achieves slightly better performance,0.6749427318572998
translation,125,169,results,slightly better performance,than,previous works,slightly better performance than previous works,0.5504746437072754
translation,125,169,results,results,has,baseline model,results has baseline model,0.5361924171447754
translation,125,170,results,benefits,of,our method,benefits of our method,0.5761390924453735
translation,125,170,results,our method,explicitly designed to deal with,longer documents,our method explicitly designed to deal with longer documents,0.6616610288619995
translation,125,170,results,do actually become stronger,apply it to,longer docu-ments,do actually become stronger apply it to longer docu-ments,0.6874872446060181
translation,125,170,results,most important result,has,benefits,most important result has benefits,0.5532277226448059
translation,125,170,results,most important result,has,do actually become stronger,most important result has do actually become stronger,0.5837005376815796
translation,125,171,results,performance gain,of,our model,performance gain of our model,0.5619418025016785
translation,125,171,results,our model,with respect to,current state - of - the - art extractive summarizer,our model with respect to current state - of - the - art extractive summarizer,0.5941399335861206
translation,125,171,results,more pronounced,for,documents,more pronounced for documents,0.6181849837303162
translation,125,171,results,documents,with,>= 3000 words,documents with >= 3000 words,0.6259520649909973
translation,125,171,results,>= 3000 words,in,both datasets,>= 3000 words in both datasets,0.4812636375427246
translation,125,181,results,significantly improves,when,local topic information ( i.e. local context ),significantly improves when local topic information ( i.e. local context ),0.6082058548927307
translation,125,181,results,both datasets,has,performance,both datasets has performance,0.5999640226364136
translation,125,181,results,performance,has,significantly improves,performance has significantly improves,0.6344982981681824
translation,125,184,results,representation,of,whole document,representation of whole document,0.5884405970573425
translation,125,184,results,representation,has,never significantly improves,representation has never significantly improves,0.6279717087745667
translation,125,184,results,whole document,has,i.e. global context ),whole document has i.e. global context ),0.5418354272842407
translation,125,184,results,whole document,has,never significantly improves,whole document has never significantly improves,0.6176300644874573
translation,125,184,results,i.e. global context ),has,never significantly improves,i.e. global context ) has never significantly improves,0.5818939805030823
translation,125,184,results,never significantly improves,has,performance,never significantly improves has performance,0.618258535861969
translation,125,184,results,results,Adding,representation,results Adding representation,0.6126043200492859
translation,126,51,baselines,rouge - 2 ( r - 2 ),measures,bigram overlap,rouge - 2 ( r - 2 ) measures bigram overlap,0.524376630783081
translation,126,51,baselines,bigram overlap,between,candidate summary,bigram overlap between candidate summary,0.6372131705284119
translation,126,51,baselines,bigram overlap,between,pool of reference summaries,bigram overlap between pool of reference summaries,0.6348592638969421
translation,126,51,baselines,rouge -l ( r-l ),measures,size of the longest common subsequence,rouge -l ( r-l ) measures size of the longest common subsequence,0.49637991189956665
translation,126,51,baselines,size of the longest common subsequence,between,candidate and reference summaries,size of the longest common subsequence between candidate and reference summaries,0.6001406908035278
translation,126,52,baselines,rouge -we ( r- we ),uses,soft matching,rouge -we ( r- we ) uses soft matching,0.5976788401603699
translation,126,52,baselines,soft matching,based on,cosine similarity,soft matching based on cosine similarity,0.6038590669631958
translation,126,52,baselines,cosine similarity,of,word embeddings,cosine similarity of word embeddings,0.5476437211036682
translation,126,52,baselines,js divergence ( js - 2 ),uses,jensen,js divergence ( js - 2 ) uses jensen,0.6624884009361267
translation,126,52,baselines,divergence,between,bigram distributions,divergence between bigram distributions,0.6574113965034485
translation,126,52,baselines,bigram distributions,of,references and candidate summaries,bigram distributions of references and candidate summaries,0.5766781568527222
translation,126,52,baselines,trained explicitly,to maximize,correlation,trained explicitly to maximize correlation,0.7173544764518738
translation,126,52,baselines,correlation,with,manual pyramid annotations,correlation with manual pyramid annotations,0.6699455380439758
translation,126,52,baselines,rouge -we ( r- we ),has,metric,rouge -we ( r- we ) has metric,0.6004260182380676
translation,126,52,baselines,jensen,has,divergence,jensen has divergence,0.6045093536376953
translation,126,52,baselines,jensen,has,metric,jensen has metric,0.6014204025268555
translation,126,52,baselines,s3,has,metric,s3 has metric,0.6085185408592224
translation,126,52,baselines,metric,has,trained explicitly,metric has trained explicitly,0.6080012917518616
translation,126,52,baselines,baselines,has,rouge -we ( r- we ),baselines has rouge -we ( r- we ),0.5525517463684082
translation,127,175,ablation-analysis,prototype summary,with,original input,prototype summary with original input,0.6313425898551941
translation,127,175,ablation-analysis,prototype summary,does,increase,prototype summary does increase,0.3579968810081482
translation,127,175,ablation-analysis,prototype summary,not,increase,prototype summary not increase,0.7146297097206116
translation,127,175,ablation-analysis,prototype summary,leading to,drops,prototype summary leading to drops,0.7535400390625
translation,127,175,ablation-analysis,original input,does,increase,original input does increase,0.31010571122169495
translation,127,175,ablation-analysis,drops,of,"9 % , 17 % , 8 % and 1 % , 3 % , 2 %","drops of 9 % , 17 % , 8 % and 1 % , 3 % , 2 %",0.6338779330253601
translation,127,175,ablation-analysis,"9 % , 17 % , 8 % and 1 % , 3 % , 2 %",in terms of,"rouge 1,2 , l","9 % , 17 % , 8 % and 1 % , 3 % , 2 % in terms of rouge 1,2 , l",0.7333158254623413
translation,127,175,ablation-analysis,"9 % , 17 % , 8 % and 1 % , 3 % , 2 %",on,s2s and unified models,"9 % , 17 % , 8 % and 1 % , 3 % , 2 % on s2s and unified models",0.5512084364891052
translation,127,175,ablation-analysis,"rouge 1,2 , l",on,s2s and unified models,"rouge 1,2 , l on s2s and unified models",0.5549360513687134
translation,127,175,ablation-analysis,increase,has,performance,increase has performance,0.5566895604133606
translation,127,175,ablation-analysis,ablation analysis,directly concatenating,prototype summary,ablation analysis directly concatenating prototype summary,0.7130478620529175
translation,127,176,ablation-analysis,prototype summary,as,generated summary,prototype summary as generated summary,0.5433609485626221
translation,127,148,baselines,lead - 3,is,commonly used summarization baseline,lead - 3 is commonly used summarization baseline,0.5426099896430969
translation,127,148,baselines,lead - 3,selects,first three sentences of document,lead - 3 selects first three sentences of document,0.7303524017333984
translation,127,148,baselines,first three sentences of document,as,summary,first three sentences of document as summary,0.5224903225898743
translation,127,149,baselines,s2s,is,sequence - to-sequence framework,s2s is sequence - to-sequence framework,0.5935099720954895
translation,127,149,baselines,s2s,extends,seq2seq framework,s2s extends seq2seq framework,0.7324173450469971
translation,127,149,baselines,sequence - to-sequence framework,with,pointer network,sequence - to-sequence framework with pointer network,0.6315799355506897
translation,127,149,baselines,proto,is,context - aware prototype editing dialog response generation model,proto is context - aware prototype editing dialog response generation model,0.5314717888832092
translation,127,149,baselines,re 3 sum,uses,ir platform,re 3 sum uses ir platform,0.6907489895820618
translation,127,149,baselines,re 3 sum,extends,seq2seq framework,re 3 sum extends seq2seq framework,0.7463448643684387
translation,127,149,baselines,ir platform,to retrieve,proper summaries,ir platform to retrieve proper summaries,0.7485564351081848
translation,127,149,baselines,seq2seq framework,to jointly conduct,template - aware summary generation,seq2seq framework to jointly conduct template - aware summary generation,0.6294876933097839
translation,127,149,baselines,baselines,has,s2s,baselines has s2s,0.5941101908683777
translation,127,150,baselines,abstractive summarization approach,on,cnn / dailymail dataset,abstractive summarization approach on cnn / dailymail dataset,0.49468427896499634
translation,127,150,baselines,baselines,has,uni-model,baselines has uni-model,0.5878727436065674
translation,127,159,experimental-setup,our experiments,in,"tensor-flow ( abadi et al. , 2016 )","our experiments in tensor-flow ( abadi et al. , 2016 )",0.47625383734703064
translation,127,159,experimental-setup,"tensor-flow ( abadi et al. , 2016 )",on,nvidia gtx 1080 ti gpu,"tensor-flow ( abadi et al. , 2016 ) on nvidia gtx 1080 ti gpu",0.48283928632736206
translation,127,159,experimental-setup,experimental setup,implement,our experiments,experimental setup implement our experiments,0.6475666165351868
translation,127,160,experimental-setup,word embedding dimension,is,256,word embedding dimension is 256,0.5879477262496948
translation,127,160,experimental-setup,word embedding dimension,is,256,word embedding dimension is 256,0.5879477262496948
translation,127,160,experimental-setup,word embedding dimension,is,256,word embedding dimension is 256,0.5879477262496948
translation,127,160,experimental-setup,number of hidden units,is,256,number of hidden units is 256,0.5547053217887878
translation,127,160,experimental-setup,experimental setup,has,word embedding dimension,experimental setup has word embedding dimension,0.49582281708717346
translation,127,161,experimental-setup,batch size,set to,64,batch size set to 64,0.7451491355895996
translation,127,161,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,127,162,experimental-setup,input document,to contain,exactly 250 words,input document to contain exactly 250 words,0.6441002488136292
translation,127,162,experimental-setup,decoding length,set to,100,decoding length set to 100,0.7383432984352112
translation,127,162,experimental-setup,experimental setup,padded or cut,input document,experimental setup padded or cut input document,0.6008133292198181
translation,127,168,experimental-setup,gradient clipping,with,"range [ ? 5 , 5 ]","gradient clipping with range [ ? 5 , 5 ]",0.6277234554290771
translation,127,168,experimental-setup,gradient clipping,during,training,gradient clipping during training,0.652908980846405
translation,127,168,experimental-setup,"al. , 2013 )",with,"range [ ? 5 , 5 ]","al. , 2013 ) with range [ ? 5 , 5 ]",0.5611739754676819
translation,127,168,experimental-setup,"range [ ? 5 , 5 ]",during,training,"range [ ? 5 , 5 ] during training",0.7188433408737183
translation,127,168,experimental-setup,experimental setup,apply,gradient clipping,experimental setup apply gradient clipping,0.5863145589828491
translation,127,169,experimental-setup,"dropout ( srivastava et al. , 2014 )",as,regularization,"dropout ( srivastava et al. , 2014 ) as regularization",0.5052658915519714
translation,127,169,experimental-setup,regularization,with,keep probability p = 0.7,regularization with keep probability p = 0.7,0.6345096230506897
translation,127,8,model,pesg,learns,summary patterns and prototype facts,pesg learns summary patterns and prototype facts,0.6660661101341248
translation,127,8,model,summary patterns and prototype facts,by analyzing,correlation,summary patterns and prototype facts by analyzing correlation,0.7528505325317383
translation,127,8,model,correlation,between,prototype document,correlation between prototype document,0.6843220591545105
translation,127,8,model,correlation,between,summary,correlation between summary,0.6815494298934937
translation,127,8,model,model,has,pesg,model has pesg,0.6359965205192566
translation,127,35,model,summarization framework,named,prototype editing based summary generator ( pesg ),summarization framework named prototype editing based summary generator ( pesg ),0.678058922290802
translation,127,35,model,summarization framework,incorporates,prototype document-summary pairs,summarization framework incorporates prototype document-summary pairs,0.6671910285949707
translation,127,35,model,prototype document-summary pairs,to improve,summa-rization performance,prototype document-summary pairs to improve summa-rization performance,0.6646090745925903
translation,127,35,model,summa-rization performance,when generating,summaries,summa-rization performance when generating summaries,0.7051705121994019
translation,127,35,model,summaries,with,pattern,summaries with pattern,0.7315461039543152
translation,127,35,model,model,propose,summarization framework,model propose summarization framework,0.6531766057014465
translation,127,173,results,our model,performs,consistently better,our model performs consistently better,0.6439617276191711
translation,127,173,results,consistently better,than,other summarization models,consistently better than other summarization models,0.5245322585105896
translation,127,173,results,other summarization models,including,state - of - the - art model,other summarization models including state - of - the - art model,0.593244731426239
translation,127,173,results,state - of - the - art model,with,improvements,state - of - the - art model with improvements,0.6036038398742676
translation,127,173,results,improvements,of,"6 % , 12 % and 6 %","improvements of 6 % , 12 % and 6 %",0.6334245800971985
translation,127,173,results,"6 % , 12 % and 6 %",in terms of,"rouge -1 , rouge - 2 and rouge -l","6 % , 12 % and 6 % in terms of rouge -1 , rouge - 2 and rouge -l",0.6967736482620239
translation,127,173,results,results,has,our model,results has our model,0.5871725678443909
translation,127,179,results,outperforms,in,fluency and consistency,outperforms in fluency and consistency,0.55389004945755
translation,127,179,results,pesg,has,outperforms,pesg has outperforms,0.6608750224113464
translation,127,179,results,outperforms,has,other baseline models,outperforms has other baseline models,0.570330023765564
translation,127,184,results,consistently good performance,with,"rouge -1 , rouge -2 , rouge -l scores","consistently good performance with rouge -1 , rouge -2 , rouge -l scores",0.6229550242424011
translation,127,184,results,our model,has,consistently good performance,our model has consistently good performance,0.5922372937202454
translation,127,184,results,"rouge -1 , rouge -2 , rouge -l scores",has,"above 39.5 , 27.5 , 39.4","rouge -1 , rouge -2 , rouge -l scores has above 39.5 , 27.5 , 39.4",0.5542657375335693
translation,127,186,results,rouge scores,of,different ablation models,rouge scores of different ablation models,0.5621368288993835
translation,127,186,results,results,has,rouge scores,results has rouge scores,0.541054368019104
translation,127,187,results,all ablation models,perform,worse,all ablation models perform worse,0.6480700373649597
translation,127,187,results,worse,than,pesg,worse than pesg,0.621357262134552
translation,127,187,results,pesg,in terms of,all metrics,pesg in terms of all metrics,0.713973343372345
translation,127,187,results,results,has,all ablation models,results has all ablation models,0.46684911847114563
translation,128,77,baselines,proposed approach,compared against,range of baselines,proposed approach compared against range of baselines,0.7085145711898804
translation,128,77,baselines,baselines,has,proposed approach,baselines has proposed approach,0.6193264126777649
translation,128,78,baselines,centroid- based summarization system,scores,sentences,centroid- based summarization system scores sentences,0.7120875120162964
translation,128,78,baselines,sentences,based on,"length , centroid , and position","sentences based on length , centroid , and position",0.5883792638778687
translation,128,78,baselines,graph- based summarization approach,based on,eigenvector centrality,graph- based summarization approach based on eigenvector centrality,0.6498486995697021
translation,128,78,baselines,mead,has,centroid- based summarization system,mead has centroid- based summarization system,0.5826259255409241
translation,128,78,baselines,"lexrank ( erkan and radev , 2004 )",has,graph- based summarization approach,"lexrank ( erkan and radev , 2004 ) has graph- based summarization approach",0.554103434085846
translation,128,78,baselines,baseline-ilp,has,baseline ilp framework,baseline-ilp has baseline ilp framework,0.565370500087738
translation,128,78,baselines,"- kirkpatrick et al. , 2011 )",has,baseline ilp framework,"- kirkpatrick et al. , 2011 ) has baseline ilp framework",0.5506824851036072
translation,128,78,baselines,baseline ilp framework,has,without data imputation,baseline ilp framework has without data imputation,0.5357545018196106
translation,128,79,hyperparameters,ilp based approaches,use,bigrams,ilp based approaches use bigrams,0.616805374622345
translation,128,79,hyperparameters,bigrams,as,concepts,bigrams as concepts,0.5769437551498413
translation,128,79,hyperparameters,sentence frequency,as,concept weights,sentence frequency as concept weights,0.5321435332298279
translation,128,82,hyperparameters,soft-impute algorithm,perform,grid search,soft-impute algorithm perform grid search,0.6028278470039368
translation,128,82,hyperparameters,grid search,on,"scale of [ 0 , 5 ]","grid search on scale of [ 0 , 5 ]",0.5501188635826111
translation,128,82,hyperparameters,grid search,with,stepsize 0.5,grid search with stepsize 0.5,0.6201191544532776
translation,128,82,hyperparameters,grid search,to tune,hyper-parameter,grid search to tune hyper-parameter,0.7491042613983154
translation,128,82,hyperparameters,"scale of [ 0 , 5 ]",with,stepsize 0.5,"scale of [ 0 , 5 ] with stepsize 0.5",0.6151626706123352
translation,128,82,hyperparameters,hyperparameters,For,soft-impute algorithm,hyperparameters For soft-impute algorithm,0.5993853807449341
translation,128,6,model,new approach,to summarizing,student course feedback,new approach to summarizing student course feedback,0.6715699434280396
translation,128,6,model,student course feedback,based on,integer linear programming ( ilp ) framework,student course feedback based on integer linear programming ( ilp ) framework,0.6708901524543762
translation,128,6,model,model,propose,new approach,model propose new approach,0.7215121984481812
translation,128,7,model,different student responses,to share,co-occurrence statistics,different student responses to share co-occurrence statistics,0.5953768491744995
translation,128,7,model,different student responses,alleviates,sparsity issues,different student responses alleviates sparsity issues,0.778540313243866
translation,128,26,model,new approach,to summarizing,student feedback,new approach to summarizing student feedback,0.6835213303565979
translation,128,26,model,student feedback,extends,standard ilp framework,student feedback extends standard ilp framework,0.7064183354377747
translation,128,26,model,standard ilp framework,by approximating,co-occurrence matrix,standard ilp framework by approximating co-occurrence matrix,0.7508037686347961
translation,128,26,model,co-occurrence matrix,using,low-rank alternative,co-occurrence matrix using low-rank alternative,0.6448996067047119
translation,128,26,model,model,propose,new approach,model propose new approach,0.7215121984481812
translation,128,89,results,all the baselines,based on,three standard rouge metrics,all the baselines based on three standard rouge metrics,0.5799819231033325
translation,128,89,results,proposed approach,has,outperforms,proposed approach has outperforms,0.6428829431533813
translation,128,89,results,outperforms,has,all the baselines,outperforms has all the baselines,0.596747100353241
translation,128,89,results,results,has,proposed approach,results has proposed approach,0.6086713075637817
translation,129,5,model,braille length,of,each sentence,braille length of each sentence,0.5796857476234436
translation,129,5,model,braille length,into,ilpbased summarization method,braille length into ilpbased summarization method,0.5495200753211975
translation,129,5,model,each sentence,in,news articles,each sentence in news articles,0.4994126856327057
translation,129,103,results,braillesum and basicsum,achieve,very similar rouge scores,braillesum and basicsum achieve very similar rouge scores,0.5928301215171814
translation,129,103,results,score differences,are,non-significant,score differences are non-significant,0.5590842962265015
translation,129,103,results,results,see that,braillesum and basicsum,results see that braillesum and basicsum,0.6627554893493652
translation,129,104,results,scores,of,braillesum and basicsum,scores of braillesum and basicsum,0.5906786322593689
translation,129,104,results,braillesum and basicsum,much higher than,nist baseline,braillesum and basicsum much higher than nist baseline,0.5960507392883301
translation,129,104,results,braillesum and basicsum,much higher than,average scores,braillesum and basicsum much higher than average scores,0.622460663318634
translation,129,104,results,results,has,scores,results has scores,0.5219217538833618
translation,129,105,results,braillesum,produce,summaries,braillesum produce summaries,0.6538488268852234
translation,129,105,results,summaries,with,much shorter braille lengths,summaries with much shorter braille lengths,0.6753453612327576
translation,129,105,results,much shorter braille lengths,than,basicsum,much shorter braille lengths than basicsum,0.5735206604003906
translation,129,105,results,braille length reduction,is,significant,braille length reduction is significant,0.5876451134681702
translation,129,105,results,results,has,braillesum,results has braillesum,0.5529987812042236
translation,129,106,results,braillesum,produce,much shorter braille summaries,braillesum produce much shorter braille summaries,0.6820958256721497
translation,129,106,results,results,demonstrate,braillesum,results demonstrate braillesum,0.5883105397224426
translation,129,107,results,braille length factor,into,ilp framework,braille length factor into ilp framework,0.5968983769416809
translation,129,107,results,braille length factor,is,very effective,braille length factor is very effective,0.5527793765068054
translation,129,107,results,very effective,for addressing,new summarization task,very effective for addressing new summarization task,0.6605746746063232
translation,129,107,results,results,incorporation of,braille length factor,results incorporation of braille length factor,0.6341026425361633
translation,130,70,model,statistical machine translation,for,document summarization,statistical machine translation for document summarization,0.4879166781902313
translation,130,39,results,systems,ranked based on,rouge - 1 metric,systems ranked based on rouge - 1 metric,0.7726587653160095
translation,130,39,results,borda and reciprocal rank,perform,better,borda and reciprocal rank perform better,0.5652886033058167
translation,130,39,results,better,than,four of the five systems,better than four of the five systems,0.6373957395553589
translation,130,39,results,rouge - 1 metric,has,borda and reciprocal rank,rouge - 1 metric has borda and reciprocal rank,0.5942739844322205
translation,130,40,results,all five methods,when,systems,all five methods when systems,0.708678126335144
translation,130,40,results,systems,ranked based on,rouge - 2 and rouge -4,systems ranked based on rouge - 2 and rouge -4,0.7510597705841064
translation,130,40,results,both combination techniques,has,outperformed,both combination techniques has outperformed,0.5968955755233765
translation,130,40,results,outperformed,has,all five methods,outperformed has all five methods,0.6238444447517395
translation,130,40,results,results,has,both combination techniques,results has both combination techniques,0.5096806287765503
translation,130,43,results,only noticeable improvement,in,all cases,only noticeable improvement in all cases,0.5485883355140686
translation,130,43,results,only noticeable improvement,in,average - rank,only noticeable improvement in average - rank,0.5271231532096863
translation,130,43,results,results,has,only noticeable improvement,results has only noticeable improvement,0.5731685757637024
translation,130,44,results,combined systems,were,more consistent,combined systems were more consistent,0.6320521831512451
translation,130,44,results,more consistent,than,individual systems,more consistent than individual systems,0.6078157424926758
translation,130,44,results,results,has,combined systems,results has combined systems,0.5660588145256042
translation,131,150,ablation-analysis,distillation objective,helps,both cases,distillation objective helps both cases,0.6025266647338867
translation,131,150,ablation-analysis,ablation analysis,using,distillation objective,ablation analysis using distillation objective,0.6678953170776367
translation,131,22,baselines,reinforcement learning ( rl ),with,bilingual semantic similarity metric,reinforcement learning ( rl ) with bilingual semantic similarity metric,0.6041214466094971
translation,131,22,baselines,bilingual semantic similarity metric,as,reward,bilingual semantic similarity metric as reward,0.5812634229660034
translation,131,94,experimental-setup,byte-pair encoding,with,60 k merge operations,byte-pair encoding with 60 k merge operations,0.6406572461128235
translation,131,94,experimental-setup,english - german dataset,has,byte-pair encoding,english - german dataset has byte-pair encoding,0.5421866178512573
translation,131,94,experimental-setup,experimental setup,For,english - german dataset,experimental setup For english - german dataset,0.6098212599754333
translation,131,7,model,model,propose,end-to - end cross-lingual text summarization,model propose end-to - end cross-lingual text summarization,0.612110435962677
translation,131,8,model,reinforcement learning,to directly optimize,bilingual semantic similarity metric,reinforcement learning to directly optimize bilingual semantic similarity metric,0.6575140357017517
translation,131,8,model,bilingual semantic similarity metric,between,summaries,bilingual semantic similarity metric between summaries,0.5825026631355286
translation,131,8,model,bilingual semantic similarity metric,between,gold summaries,bilingual semantic similarity metric between gold summaries,0.606313943862915
translation,131,8,model,summaries,generated in,target language,summaries generated in target language,0.6805921792984009
translation,131,8,model,gold summaries,in,source language,gold summaries in source language,0.5005965828895569
translation,131,8,model,model,uses,reinforcement learning,model uses reinforcement learning,0.5377182960510254
translation,131,21,model,end-to- end xls model,to directly generate,target language summaries,end-to- end xls model to directly generate target language summaries,0.7093966007232666
translation,131,21,model,target language summaries,given,source articles,target language summaries given source articles,0.6806564331054688
translation,131,21,model,source articles,by matching,semantics,source articles by matching semantics,0.7885839939117432
translation,131,21,model,semantics,of,predictions,semantics of predictions,0.6429093480110168
translation,131,21,model,semantics,of,source language summaries,semantics of source language summaries,0.512657642364502
translation,131,21,model,semantics,semantics of,source language summaries,semantics semantics of source language summaries,0.7008405923843384
translation,131,21,model,model,train,end-to- end xls model,model train end-to- end xls model,0.7037019729614258
translation,131,24,model,new multi-task pretraining objective,based on,machine translation and monolingual summarization,new multi-task pretraining objective based on machine translation and monolingual summarization,0.5871443152427673
translation,131,24,model,machine translation and monolingual summarization,to encode,common information,machine translation and monolingual summarization to encode common information,0.6561593413352966
translation,131,24,model,common information,available from,two tasks,common information available from two tasks,0.5641883611679077
translation,131,24,model,model,propose,new multi-task pretraining objective,model propose new multi-task pretraining objective,0.6652634143829346
translation,131,28,results,without fine-tuning,with,rl,without fine-tuning with rl,0.7402879595756531
translation,131,28,results,bestperforming baseline,by,up to 0.8 rouge -l points,bestperforming baseline by up to 0.8 rouge -l points,0.5440374612808228
translation,131,28,results,proposed pre-training method,has,without fine-tuning,proposed pre-training method has without fine-tuning,0.5332010388374329
translation,131,28,results,proposed pre-training method,has,improves,proposed pre-training method has improves,0.6120079755783081
translation,131,28,results,without fine-tuning,has,improves,without fine-tuning has improves,0.6481047868728638
translation,131,28,results,rl,has,improves,rl has improves,0.6502761840820312
translation,131,28,results,improves,has,bestperforming baseline,improves has bestperforming baseline,0.6043461561203003
translation,131,28,results,results,demonstrate,proposed pre-training method,results demonstrate proposed pre-training method,0.6253223419189453
translation,131,28,results,results,just using,proposed pre-training method,results just using proposed pre-training method,0.700954794883728
translation,131,29,results,reinforcement learning,yields,further improvements,reinforcement learning yields further improvements,0.7056500315666199
translation,131,29,results,further improvements,in,performance,further improvements in performance,0.552592396736145
translation,131,29,results,performance,by,up to 0.5 rouge -l points,performance by up to 0.5 rouge -l points,0.5878329873085022
translation,131,29,results,results,Applying,reinforcement learning,results Applying reinforcement learning,0.621554970741272
translation,131,126,results,improve,has,summarization quality,improve has summarization quality,0.5461786389350891
translation,131,132,results,pipeline approaches,show,weakest performance,pipeline approaches show weakest performance,0.6344758868217468
translation,131,132,results,weakest performance,lagging behind,weakest end-to - end approach,weakest performance lagging behind weakest end-to - end approach,0.6845117807388306
translation,131,132,results,weakest end-to - end approach,by,more than 5 rouge -l points,weakest end-to - end approach by more than 5 rouge -l points,0.5731531977653503
translation,131,132,results,results,has,pipeline approaches,results has pipeline approaches,0.5527309775352478
translation,131,133,results,tran - sum,performs,even worse,tran - sum performs even worse,0.6463304162025452
translation,131,133,results,even worse,than,sum - tran,even worse than sum - tran,0.6635564565658569
translation,131,133,results,results,has,tran - sum,results has tran - sum,0.5915104150772095
translation,131,135,results,outperforms,in,rouge -l,outperforms in rouge -l,0.5964601039886475
translation,131,135,results,strongest baseline ( mle - xls + mt ),in,rouge -l,strongest baseline ( mle - xls + mt ) in rouge -l,0.582099199295044
translation,131,135,results,strongest baseline ( mle - xls + mt ),in,0.8 ) and xsim,strongest baseline ( mle - xls + mt ) in 0.8 ) and xsim,0.5482632517814636
translation,131,135,results,strongest baseline ( mle - xls + mt ),by,0.8 ) and xsim,strongest baseline ( mle - xls + mt ) by 0.8 ) and xsim,0.5793179869651794
translation,131,135,results,rouge -l,by,0.8 ) and xsim,rouge -l by 0.8 ) and xsim,0.5955595374107361
translation,131,135,results,pre-training method,has,proposed model,pre-training method has proposed model,0.5924466252326965
translation,131,135,results,mle - xls + mt + dis ),has,proposed model,mle - xls + mt + dis ) has proposed model,0.5699358582496643
translation,131,135,results,proposed model,has,outperforms,proposed model has outperforms,0.642342746257782
translation,131,135,results,outperforms,has,strongest baseline ( mle - xls + mt ),outperforms has strongest baseline ( mle - xls + mt ),0.5848368406295776
translation,131,135,results,results,Using,pre-training method,results Using pre-training method,0.6679127812385559
translation,131,136,results,reinforcement learning,fine- tune,model,reinforcement learning fine- tune model,0.6636057496070862
translation,131,136,results,model,with,"rouge ( rl - rouge ) , xsim ( rl - xsim )","model with rouge ( rl - rouge ) , xsim ( rl - xsim )",0.6511390209197998
translation,131,136,results,model,with,mean ( rl - rouge + xsim ),model with mean ( rl - rouge + xsim ),0.6446512937545776
translation,131,136,results,model,as,rewards,model as rewards,0.5578901171684265
translation,131,136,results,mean ( rl - rouge + xsim ),as,rewards,mean ( rl - rouge + xsim ) as rewards,0.5799906849861145
translation,131,136,results,rewards,results in,further improvements,rewards results in further improvements,0.6670367121696472
translation,131,136,results,results,Applying,reinforcement learning,results Applying reinforcement learning,0.621554970741272
translation,131,137,results,rl - xsim,performs,best,rl - xsim performs best,0.6291912198066711
translation,131,137,results,proposed method,has,rl - xsim,proposed method has rl - xsim,0.5954487919807434
translation,131,137,results,results,has,proposed method,results has proposed method,0.5845219492912292
translation,131,137,results,results,has,rl - xsim,results has rl - xsim,0.5550026297569275
translation,131,149,results,l dis,With,mle -xls,l dis With mle -xls,0.73639315366745
translation,131,149,results,mle -xls,as,pre-training,mle -xls as pre-training,0.5537874698638916
translation,131,149,results,extract,shows,improvement,extract shows improvement,0.6863469481468201
translation,131,149,results,extract,leads to,decrease,extract leads to decrease,0.7007800340652466
translation,131,149,results,improvement,in,performance,improvement in performance,0.5151869058609009
translation,131,149,results,improvement,in,performance,improvement in performance,0.5151869058609009
translation,131,149,results,performance,of,mle-xls + mt,performance of mle-xls + mt,0.6097168326377869
translation,131,149,results,decrease,in,performance,decrease in performance,0.5227082371711731
translation,131,149,results,performance,of,mle-xls + mt,performance of mle-xls + mt,0.6097168326377869
translation,131,151,results,simple enhancements,helps in,improving,simple enhancements helps in improving,0.8257983922958374
translation,131,151,results,lower - layers,of,decoder,lower - layers of decoder,0.6075347065925598
translation,131,151,results,task - specific tags ( tags ),during,multi-task pre-training,task - specific tags ( tags ) during multi-task pre-training,0.651360809803009
translation,131,151,results,task - specific tags ( tags ),helps in,improving,task - specific tags ( tags ) helps in improving,0.7471855282783508
translation,131,151,results,sharing,has,lower - layers,sharing has lower - layers,0.5890097618103027
translation,131,151,results,improving,has,performance,improving has performance,0.5706568360328674
translation,131,151,results,results,introducing,simple enhancements,results introducing simple enhancements,0.7572104334831238
translation,131,157,results,outputs,of,model,outputs of model,0.6474190354347229
translation,131,157,results,model,trained with,rouge -l rewards,model trained with rouge -l rewards,0.7519447803497314
translation,131,157,results,rouge -l rewards,are,more favored,rouge -l rewards are more favored,0.5969768762588501
translation,131,157,results,only pre-trained model,in terms of,relevance,only pre-trained model in terms of relevance,0.6664578914642334
translation,131,157,results,only pre-trained model,not,fluency,only pre-trained model not fluency,0.7215362191200256
translation,131,157,results,results,observe,outputs,results observe outputs,0.5839269757270813
translation,132,159,ablation-analysis,manual co-reference resolution,on,test set,manual co-reference resolution on test set,0.5686768293380737
translation,132,159,ablation-analysis,manual co-reference resolution,resulted in,further 2 % increase,manual co-reference resolution resulted in further 2 % increase,0.6470022797584534
translation,132,159,ablation-analysis,further 2 % increase,in,scores,further 2 % increase in scores,0.5836658477783203
translation,132,159,ablation-analysis,further 2 % increase,to,62.4 %,further 2 % increase to 62.4 %,0.5831543803215027
translation,132,7,baselines,more comprehensive method,to generate,story amr graph,more comprehensive method to generate story amr graph,0.7322805523872375
translation,132,7,baselines,story amr graph,using,state - ofthe - art co-reference resolution and meta nodes,story amr graph using state - ofthe - art co-reference resolution and meta nodes,0.6423251628875732
translation,132,154,hyperparameters,target summary size,to control,length,target summary size to control length,0.6442230343818665
translation,132,154,hyperparameters,length,of,output summary,length of output summary,0.6028198599815369
translation,132,154,hyperparameters,hyperparameters,use,target summary size,hyperparameters use target summary size,0.6078245043754578
translation,132,6,model,sas,generates,amr graph,sas generates amr graph,0.686033308506012
translation,132,6,model,sas,creates,summary sentences,sas creates summary sentences,0.6714847087860107
translation,132,6,model,amr graph,of,input story,amr graph of input story,0.5366305708885193
translation,132,6,model,summary sentences,from,summary graph,summary sentences from summary graph,0.5609382390975952
translation,132,6,model,model,has,sas,model has sas,0.607878565788269
translation,132,41,model,alternative method,to use,amrs,alternative method to use amrs,0.7216018438339233
translation,132,41,model,amrs,for,abstractive summarization,amrs for abstractive summarization,0.5645074248313904
translation,132,41,model,model,propose,alternative method,model propose alternative method,0.6553483605384827
translation,132,49,model,more comprehensive method,to generate,story amr,more comprehensive method to generate story amr,0.6988368034362793
translation,132,49,model,story amr,from,sentence amrs,story amr from sentence amrs,0.5499460697174072
translation,132,49,model,sentence amrs,based on,event / entity co-reference resolution and meta nodes,sentence amrs based on event / entity co-reference resolution and meta nodes,0.643872082233429
translation,132,49,model,model,develop,more comprehensive method,model develop more comprehensive method,0.6343094110488892
translation,132,71,model,node co-reference resolution,using,text co-reference resolution,node co-reference resolution using text co-reference resolution,0.7117009162902832
translation,132,71,model,text co-reference resolution,followed by,mapping the text,text co-reference resolution followed by mapping the text,0.6444917917251587
translation,132,71,model,to a node,using,alignments,to a node using alignments,0.7186629176139832
translation,132,71,model,mapping the text,has,to a node,mapping the text has to a node,0.6204810738563538
translation,132,71,model,model,solve,node co-reference resolution,model solve node co-reference resolution,0.6176387667655945
translation,132,153,results,state- of- the- art,in,sas,state- of- the- art in sas,0.5493361353874207
translation,132,153,results,state- of- the- art,by,1.7 % f1 scores,state- of- the- art by 1.7 % f1 scores,0.5433686971664429
translation,132,153,results,1.7 % f1 scores,in,node prediction,1.7 % f1 scores in node prediction,0.5138227939605713
translation,132,153,results,outperform,has,state- of- the- art,outperform has state- of- the- art,0.5776163339614868
translation,132,153,results,results,has,outperform,results has outperform,0.642206609249115
translation,132,164,results,previous state - of - the - art methods,for,sas,previous state - of - the - art methods for sas,0.5815815925598145
translation,132,164,results,previous state - of - the - art methods,by,3.7 %,previous state - of - the - art methods by 3.7 %,0.5235233306884766
translation,132,164,results,sas,by,1.7 %,sas by 1.7 %,0.6211798191070557
translation,132,164,results,3.7 %,using,human coreference resolution,3.7 % using human coreference resolution,0.6081485748291016
translation,132,164,results,outperform,has,previous state - of - the - art methods,outperform has previous state - of - the - art methods,0.5405260920524597
translation,132,164,results,results,has,outperform,results has outperform,0.642206609249115
translation,133,6,ablation-analysis,novel multitask architectures,with,high- level ( semantic ) layer -specific sharing,novel multitask architectures with high- level ( semantic ) layer -specific sharing,0.6105353236198425
translation,133,6,ablation-analysis,ablation analysis,propose,novel multitask architectures,ablation analysis propose novel multitask architectures,0.640119731426239
translation,133,175,ablation-analysis,2 - way mtl model,with,entailment generation,2 - way mtl model with entailment generation,0.5686314105987549
translation,133,175,ablation-analysis,entailment generation,reduces,extraneous count,entailment generation reduces extraneous count,0.6515757441520691
translation,133,175,ablation-analysis,extraneous count,by,17.2 %,extraneous count by 17.2 %,0.5325686931610107
translation,133,175,ablation-analysis,17.2 %,w.r.t.,baseline,17.2 % w.r.t. baseline,0.5527365803718567
translation,133,175,ablation-analysis,ablation analysis,found that,2 - way mtl model,ablation analysis found that 2 - way mtl model,0.6408352255821228
translation,133,45,baselines,baselines,has,baseline pointer + coverage model,baselines has baseline pointer + coverage model,0.5264441967010498
translation,133,22,model,novel multi-task learning architectures,based on,multi-layered encoder and decoder models,novel multi-task learning architectures based on multi-layered encoder and decoder models,0.6175107359886169
translation,133,22,model,novel multi-task learning architectures,empirically show,substantially better,novel multi-task learning architectures empirically show substantially better,0.7384781241416931
translation,133,22,model,substantially better,to share,higherlevel semantic layers,substantially better to share higherlevel semantic layers,0.6583468914031982
translation,133,22,model,higherlevel semantic layers,between,three aforementioned tasks,higherlevel semantic layers between three aforementioned tasks,0.6272236704826355
translation,133,22,model,higherlevel semantic layers,keeping,lower - level ( lexico-syntactic ) layers unshared,higherlevel semantic layers keeping lower - level ( lexico-syntactic ) layers unshared,0.6683128476142883
translation,133,22,model,model,present,novel multi-task learning architectures,model present novel multi-task learning architectures,0.6056849360466003
translation,133,7,results,statistically significant improvements,over,state-ofthe- art,statistically significant improvements over state-ofthe- art,0.6395524144172668
translation,133,7,results,state-ofthe- art,on,cnn / dailymail and gigaword datasets,state-ofthe- art on cnn / dailymail and gigaword datasets,0.4564495086669922
translation,133,24,results,"soft , layer -specific sharing model",with,question and entailment generation auxiliary tasks,"soft , layer -specific sharing model with question and entailment generation auxiliary tasks",0.6045814156532288
translation,133,24,results,"soft , layer -specific sharing model",achieves,statistically significant improvements,"soft , layer -specific sharing model achieves statistically significant improvements",0.674355685710907
translation,133,24,results,statistically significant improvements,over,state - of- the - art,statistically significant improvements over state - of- the - art,0.642184853553772
translation,133,24,results,statistically significant improvements,on,cnn / dailymail and gigaword datasets,statistically significant improvements on cnn / dailymail and gigaword datasets,0.48684871196746826
translation,133,24,results,state - of- the - art,on,cnn / dailymail and gigaword datasets,state - of- the - art on cnn / dailymail and gigaword datasets,0.45976904034614563
translation,133,24,results,results,has,"soft , layer -specific sharing model","results has soft , layer -specific sharing model",0.5474333167076111
translation,133,25,results,significantly better,on,duc - 2002 transfer setup,significantly better on duc - 2002 transfer setup,0.516511857509613
translation,133,38,results,our new soft sharing parameter approach,gives,stat,our new soft sharing parameter approach gives stat,0.6439568996429443
translation,133,38,results,results,has,our new soft sharing parameter approach,results has our new soft sharing parameter approach,0.5623038411140442
translation,133,122,results,results,shows,our baseline model,results shows our baseline model,0.6813945174217224
translation,133,123,results,performs better,than,all previous works,performs better than all previous works,0.5429725646972656
translation,133,123,results,gigaword dataset,has,our baseline model,gigaword dataset has our baseline model,0.5414698123931885
translation,133,123,results,our baseline model,has,performs better,our baseline model has performs better,0.5884690880775452
translation,133,123,results,results,On,gigaword dataset,results On gigaword dataset,0.48238715529441833
translation,133,126,results,multi-task setting,better than,our strong baseline models,multi-task setting better than our strong baseline models,0.7063779830932617
translation,133,128,results,statistically significant,for,cnn / dailymail,statistically significant for cnn / dailymail,0.6315647959709167
translation,133,128,results,statistically significant,for,gigaword,statistically significant for gigaword,0.6660758852958679
translation,133,128,results,statistically significant,for,gigaword,statistically significant for gigaword,0.6660758852958679
translation,133,128,results,meteor ( p < 0.01 ),for,cnn / dailymail,meteor ( p < 0.01 ) for cnn / dailymail,0.6385622024536133
translation,133,128,results,meteor ( p < 0.01 ),for,gigaword,meteor ( p < 0.01 ) for gigaword,0.6713833808898926
translation,133,128,results,all metrics ( p < 0.01 ),for,gigaword,all metrics ( p < 0.01 ) for gigaword,0.6237583160400391
translation,133,128,results,multi-task learning with question generation,has,improvements,multi-task learning with question generation has improvements,0.5291669368743896
translation,133,128,results,results,For,multi-task learning with question generation,results For multi-task learning with question generation,0.5358951687812805
translation,133,135,results,2 - way multi-task models,perform,sig- multi-task with entailment,2 - way multi-task models perform sig- multi-task with entailment,0.5819026827812195
translation,133,135,results,2 - way multi-task models,perform,multi-task learning,2 - way multi-task models perform multi-task learning,0.5608952045440674
translation,133,135,results,multi-task learning,with,all three tasks together,multi-task learning with all three tasks together,0.6001851558685303
translation,133,135,results,results,perform,multi-task learning,results perform multi-task learning,0.5503182411193848
translation,133,136,results,our full multi-task model,achieves,best scores,our full multi-task model achieves best scores,0.6629871129989624
translation,133,136,results,best scores,on,cnn / dailymail and gigaword datasets,best scores on cnn / dailymail and gigaword datasets,0.46041393280029297
translation,133,136,results,results,show,our full multi-task model,results show our full multi-task model,0.5881885290145874
translation,133,137,results,3 - way multi-task model,with both,entailment and question generation,3 - way multi-task model with both entailment and question generation,0.6298240423202515
translation,133,137,results,outperforms,with,stat,outperforms with stat,0.7333526015281677
translation,133,137,results,publicly - available pretrained result,of,previous sota,publicly - available pretrained result of previous sota,0.5026894211769104
translation,133,137,results,publicly - available pretrained result,with,stat,publicly - available pretrained result with stat,0.6595594882965088
translation,133,137,results,),of,previous sota,) of previous sota,0.636786162853241
translation,133,137,results,3 - way multi-task model,has,outperforms,3 - way multi-task model has outperforms,0.574936032295227
translation,133,137,results,outperforms,has,publicly - available pretrained result,outperforms has publicly - available pretrained result,0.547378659248352
translation,133,137,results,publicly - available pretrained result,has,),publicly - available pretrained result has ),0.5815847516059875
translation,133,137,results,results,has,3 - way multi-task model,results has 3 - way multi-task model,0.5207887887954712
translation,133,143,results,our mtl model,is,better,our mtl model is better,0.6036057472229004
translation,133,143,results,better,than,our state - of - theart baseline,better than our state - of - theart baseline,0.5425453186035156
translation,133,143,results,better,on,relevance and readability,better on relevance and readability,0.49409928917884827
translation,133,150,results,multitask model,achieves,statistically significant improvements,multitask model achieves statistically significant improvements,0.6532428860664368
translation,133,150,results,statistically significant improvements,over,strong baseline,statistically significant improvements over strong baseline,0.6640254855155945
translation,133,150,results,statistically significant improvements,over,pointercoverage model,statistically significant improvements over pointercoverage model,0.661251962184906
translation,133,173,results,our multi-task model,improves upon,baseline,our multi-task model improves upon baseline,0.74957674741745
translation,133,173,results,baseline,in the aspect of being entailed by,source document,baseline in the aspect of being entailed by source document,0.6783629059791565
translation,134,9,experiments,22 querychains sessions,of,length,22 querychains sessions of length,0.5636329650878906
translation,134,9,experiments,up to 3,with,3 matching human summaries,up to 3 with 3 matching human summaries,0.6595669984817505
translation,134,9,experiments,3 matching human summaries,each in,consumerhealth domain,3 matching human summaries each in consumerhealth domain,0.5585466027259827
translation,134,9,experiments,length,has,up to 3,length has up to 3,0.6117213368415833
translation,134,77,model,pag-erank,spreads,query similarity,pag-erank spreads query similarity,0.6753365397453308
translation,134,77,model,query similarity,of,vertex,query similarity of vertex,0.5871018767356873
translation,134,77,model,vertex,to,close neighbors,vertex to close neighbors,0.5584145784378052
translation,134,77,model,model,has,pag-erank,model has pag-erank,0.6631928086280823
translation,134,197,results,all of the modified versions,of,our algorithm,all of the modified versions of our algorithm,0.5631920695304871
translation,134,197,results,performed better,than,focused klsum,performed better than focused klsum,0.6206918954849243
translation,134,197,results,focused klsum,with,more than 95 % confidence,focused klsum with more than 95 % confidence,0.6582217812538147
translation,134,197,results,our algorithm,has,performed better,our algorithm has performed better,0.6001600623130798
translation,134,197,results,results,has,all of the modified versions,results has all of the modified versions,0.521299421787262
translation,135,192,ablation-analysis,salience,to discriminate between,plausible spans,salience to discriminate between plausible spans,0.7288362979888916
translation,135,192,ablation-analysis,plausible spans,increases,rouge,plausible spans increases rouge,0.7811023592948914
translation,135,192,ablation-analysis,ablation analysis,Using,salience,ablation analysis Using salience,0.6985645890235901
translation,135,210,baselines,plausibility model,optimized for,grammaticality,plausibility model optimized for grammaticality,0.6533602476119995
translation,135,210,baselines,salience model,optimized for,rouge,salience model optimized for rouge,0.7650287747383118
translation,135,200,experiments,three source ? target transfer tasks,guided by,real-world settings,three source ? target transfer tasks guided by real-world settings,0.6321765780448914
translation,135,200,experiments,single to multiple sentence summaries,with,heavy paraphrasing,single to multiple sentence summaries with heavy paraphrasing,0.6470345258712769
translation,135,200,experiments,nyt ? cnn,has,one newswire outlet to another,nyt ? cnn has one newswire outlet to another,0.6318765878677368
translation,135,200,experiments,cnn ? reddit,has,newswire to social media,cnn ? reddit has newswire to social media,0.5977742671966553
translation,135,200,experiments,xsum ? wikihow,has,single to multiple sentence summaries,xsum ? wikihow has single to multiple sentence summaries,0.620882511138916
translation,135,206,experiments,our system,on,xsum ? wikihow,our system on xsum ? wikihow,0.5883986353874207
translation,135,206,experiments,in- domain extraction baselines,trained on,tens of thousands of examples,in- domain extraction baselines trained on tens of thousands of examples,0.7047319412231445
translation,135,206,experiments,xsum ? wikihow,comes within,0.3 in - domain average rouge,xsum ? wikihow comes within 0.3 in - domain average rouge,0.548689067363739
translation,135,206,experiments,nyt ? cnn and cnn ? reddit,has,our system,nyt ? cnn and cnn ? reddit has our system,0.6306527853012085
translation,135,206,experiments,our system,has,outperforms,our system has outperforms,0.6423544883728027
translation,135,206,experiments,outperforms,has,in- domain extraction baselines,outperforms has in- domain extraction baselines,0.6079514622688293
translation,135,17,model,summarization system,compresses,text,summarization system compresses text,0.7632239460945129
translation,135,17,model,text,in,more data-driven way,text in more data-driven way,0.5317022800445557
translation,135,17,model,model,build,summarization system,model build summarization system,0.7372603416442871
translation,135,18,model,high-recall constituency - based compression rules,that cover,space of legal deletions,high-recall constituency - based compression rules that cover space of legal deletions,0.7067925930023193
translation,135,30,results,salience,with,500 in- domain samples,salience with 500 in- domain samples,0.6765425205230713
translation,135,30,results,salience,demonstrate,compressive system,salience demonstrate compressive system,0.6505973935127258
translation,135,30,results,compressive system,can,match or exceed,compressive system can match or exceed,0.6509562134742737
translation,135,30,results,rouge,of,in- domain extractive model,rouge of in- domain extractive model,0.548308253288269
translation,135,30,results,in- domain extractive model,trained on,tens of thousands of documentsummary pairs,in- domain extractive model trained on tens of thousands of documentsummary pairs,0.7101555466651917
translation,135,30,results,match or exceed,has,rouge,match or exceed has rouge,0.6297383308410645
translation,135,30,results,results,By finetuning,salience,results By finetuning salience,0.5594289302825928
translation,135,146,results,compression,consistently improves,rouge,compression consistently improves rouge,0.7752658128738403
translation,135,147,results,improvements,in,rouge,improvements in rouge,0.5621152520179749
translation,135,147,results,improvements,when using,cups,improvements when using cups,0.7372732758522034
translation,135,147,results,rouge,when using,cups,rouge when using cups,0.732149064540863
translation,135,148,results,decreases,over,sentence extractive baselines,decreases over sentence extractive baselines,0.6889984607696533
translation,135,148,results,rouge,over,sentence extractive baselines,rouge over sentence extractive baselines,0.6815342903137207
translation,135,148,results,decreases,has,rouge,decreases has rouge,0.6310204267501831
translation,135,176,results,both cnn and reddit,see evidence,plausibility model,both cnn and reddit see evidence plausibility model,0.6154411435127258
translation,135,176,results,plausibility model 's deletions,are,highly grammatical,plausibility model 's deletions are highly grammatical,0.586098313331604
translation,135,176,results,plausibility model,makes,more semantically - informed deletions,plausibility model makes more semantically - informed deletions,0.6371970176696777
translation,135,176,results,more semantically - informed deletions,to maintain,factuality,more semantically - informed deletions to maintain factuality,0.6422078609466553
translation,135,176,results,factuality,especially on,cnn,factuality especially on cnn,0.681616485118866
translation,135,176,results,both cnn and reddit,has,plausibility model 's deletions,both cnn and reddit has plausibility model 's deletions,0.5911628603935242
translation,135,176,results,results,On,both cnn and reddit,results On both cnn and reddit,0.5222876667976379
translation,135,177,results,factuality performance,incorporating,plausibility model,factuality performance incorporating plausibility model,0.69319748878479
translation,135,177,results,lower,on,reddit,lower on reddit,0.560724139213562
translation,135,177,results,plausibility model,on top of,compression rules,plausibility model on top of compression rules,0.6865422129631042
translation,135,177,results,plausibility model,results in,6 % gain,plausibility model results in 6 % gain,0.6132002472877502
translation,135,177,results,6 % gain,in,precision,6 % gain in precision,0.5679332613945007
translation,135,177,results,results,has,factuality performance,results has factuality performance,0.513500988483429
translation,135,203,results,our system,maintains,strong zero-shot out - of- domain performance,our system maintains strong zero-shot out - of- domain performance,0.6402530670166016
translation,135,203,results,strong zero-shot out - of- domain performance,despite,distribution shifts,strong zero-shot out - of- domain performance despite distribution shifts,0.6624436378479004
translation,135,203,results,compression,adds,roughly + 1 rouge -1,compression adds roughly + 1 rouge -1,0.6312081217765808
translation,135,203,results,extraction,has,outperforms,extraction has outperforms,0.6290209889411926
translation,135,203,results,outperforms,has,lead -k baseline,outperforms has lead -k baseline,0.5961108803749084
translation,135,203,results,results,has,our system,results has our system,0.5954442024230957
translation,135,205,results,performance,via,fine-tuning,performance via fine-tuning,0.7200210094451904
translation,135,205,results,fine-tuning,on,500 samples,fine-tuning on 500 samples,0.5620089173316956
translation,135,205,results,500 samples,matches or exceeds,in - domain extraction rouge,500 samples matches or exceeds in - domain extraction rouge,0.6090315580368042
translation,135,205,results,results,see that,performance,results see that performance,0.6803712844848633
translation,135,214,results,compression module,stack with,state - of - the - art sentence extraction models,compression module stack with state - of - the - art sentence extraction models,0.6752148270606995
translation,135,214,results,results,demonstrate,compression module,results demonstrate compression module,0.6680498719215393
translation,136,215,ablation-analysis,about 44 %,of,summary sentences,about 44 % of summary sentences,0.5798142552375793
translation,136,215,ablation-analysis,about 44 %,generated by,compression,about 44 % generated by compression,0.6881784200668335
translation,136,215,ablation-analysis,ablation analysis,has,about 44 %,ablation analysis has about 44 %,0.5339696407318115
translation,136,4,model,abstraction - based multidocument summarization framework,construct,new sentences,abstraction - based multidocument summarization framework construct new sentences,0.6755475401878357
translation,136,4,model,new sentences,by exploring,more fine- grained syntactic units,new sentences by exploring more fine- grained syntactic units,0.608074426651001
translation,136,4,model,more fine- grained syntactic units,than,sentences,more fine- grained syntactic units than sentences,0.5804247260093689
translation,136,4,model,more fine- grained syntactic units,namely,noun / verb phrases,more fine- grained syntactic units namely noun / verb phrases,0.6019697785377502
translation,136,4,model,model,propose,abstraction - based multidocument summarization framework,model propose abstraction - based multidocument summarization framework,0.631315290927887
translation,136,5,model,pool of concepts and facts,represented by,phrases,pool of concepts and facts represented by phrases,0.7343041896820068
translation,136,5,model,phrases,from,input documents,phrases from input documents,0.5239231586456299
translation,136,6,model,new sentences,selecting and merging,informative phrases,new sentences selecting and merging informative phrases,0.6442413926124573
translation,136,6,model,new sentences,satisfy,sentence construction constraints,new sentences satisfy sentence construction constraints,0.6004100441932678
translation,136,6,model,informative phrases,to maximize,salience of phrases,informative phrases to maximize salience of phrases,0.6278882622718811
translation,136,6,model,informative phrases,satisfy,sentence construction constraints,informative phrases satisfy sentence construction constraints,0.6073273420333862
translation,136,6,model,model,has,new sentences,model has new sentences,0.5923789143562317
translation,136,7,model,integer linear optimization,for conducting,phrase selection and merging simultaneously,integer linear optimization for conducting phrase selection and merging simultaneously,0.6565493941307068
translation,136,7,model,global optimal solution,for,summary,global optimal solution for summary,0.6296284794807434
translation,136,7,model,model,employ,integer linear optimization,model employ integer linear optimization,0.5659329891204834
translation,136,22,model,abstractive mds framework,construct,new sentences,abstractive mds framework construct new sentences,0.7034141421318054
translation,136,22,model,model,propose,abstractive mds framework,model propose abstractive mds framework,0.6863226294517517
translation,136,30,model,salience score,computed for,each phrase,salience score computed for each phrase,0.6920872926712036
translation,136,30,model,each phrase,by exploiting,redundancy,each phrase by exploiting redundancy,0.7054823637008667
translation,136,30,model,redundancy,of,document content,redundancy of document content,0.605713963508606
translation,136,30,model,document content,in,global manner,document content in global manner,0.5362493991851807
translation,136,30,model,model,has,salience score,model has salience score,0.5601779818534851
translation,136,31,model,second component,constructs,new sentences,second component constructs new sentences,0.6673800945281982
translation,136,31,model,second component,ensures,validity,second component ensures validity,0.5436960458755493
translation,136,31,model,new sentences,by selecting and merging,phrases,new sentences by selecting and merging phrases,0.722159206867218
translation,136,31,model,phrases,based on,salience scores,phrases based on salience scores,0.6537235975265503
translation,136,31,model,validity,of,new sentences,validity of new sentences,0.5972251296043396
translation,136,31,model,new sentences,using,integer linear optimization model,new sentences using integer linear optimization model,0.666317343711853
translation,136,31,model,model,has,second component,model has second component,0.5531283020973206
translation,136,36,model,compatibility relations,among,nps and vps,compatibility relations among nps and vps,0.5864062905311584
translation,136,36,model,compatibility relations,among,other optimization constraints,compatibility relations among other optimization constraints,0.5382989048957825
translation,136,36,model,compatibility relations,to ensure,generated sentences,compatibility relations to ensure generated sentences,0.6245920062065125
translation,136,36,model,generated sentences,contain,correct facts,generated sentences contain correct facts,0.5853356122970581
translation,136,37,model,framework,explores,"more fine - grained textual unit ( i.e. , phrases )","framework explores more fine - grained textual unit ( i.e. , phrases )",0.6597065925598145
translation,136,37,model,framework,maximizes,salience,framework maximizes salience,0.7670536637306213
translation,136,37,model,salience,of,selected phrases,salience of selected phrases,0.6111730933189392
translation,136,37,model,selected phrases,in,global manner,selected phrases in global manner,0.5394769906997681
translation,136,39,model,nps and vps,from,constituency trees,nps and vps from constituency trees,0.5620193481445312
translation,136,39,model,nps and vps,subsequently calculate,salience scores,nps and vps subsequently calculate salience scores,0.5930970311164856
translation,136,39,model,model,extract,nps and vps,model extract nps and vps,0.6961672902107239
translation,136,39,model,model,subsequently calculate,salience scores,model subsequently calculate salience scores,0.6333903670310974
translation,136,41,model,model,perform,several post-processing steps,model perform several post-processing steps,0.6932973265647888
translation,136,253,model,designed optimization framework,operates on,summary level,designed optimization framework operates on summary level,0.719473123550415
translation,136,253,model,summary level,so that,more complementary semantic content units,summary level so that more complementary semantic content units,0.5761263966560364
translation,136,253,model,model,has,designed optimization framework,model has designed optimization framework,0.5633828639984131
translation,136,177,results,our method,achieves,best results,our method achieves best results,0.6332839131355286
translation,136,177,results,best results,in,both thresholds,best results in both thresholds,0.5201058983802795
translation,136,177,results,our method,able to find,more semantic content units ( scus ),our method able to find more semantic content units ( scus ),0.6659902334213257
translation,136,177,results,more semantic content units ( scus ),than,state - of- the - art system,more semantic content units ( scus ) than state - of- the - art system,0.5639776587486267
translation,136,177,results,results,has,our method,results has our method,0.5589964985847473
translation,136,178,results,paired t-test,comparing,our model,paired t-test comparing our model,0.7421590089797974
translation,136,178,results,our model,with,best system in tac 2011,our model with best system in tac 2011,0.6829138398170471
translation,136,178,results,our model,shows,performance,our model shows performance,0.7101472020149231
translation,136,178,results,performance,of,our model,performance of our model,0.5847885608673096
translation,136,178,results,performance,is,significantly better,performance is significantly better,0.577980101108551
translation,136,178,results,results,has,paired t-test,results has paired t-test,0.5556200742721558
translation,136,190,results,our performance,not as good as,system 43 and 17,our performance not as good as system 43 and 17,0.7375397682189941
translation,136,190,results,slightly better,than,system 22,slightly better than system 22,0.6174391508102417
translation,136,190,results,results,has,our performance,results has our performance,0.5777909755706787
translation,136,203,results,two systems,perform,very closely,two systems perform very closely,0.5427578091621399
translation,136,203,results,results,On average,two systems,results On average two systems,0.7041690349578857
translation,136,204,results,system 22,is,extraction - based method,system 22 is extraction - based method,0.6116048097610474
translation,136,204,results,system 22,achieves,higher score,system 22 achieves higher score,0.6457433104515076
translation,136,204,results,extraction - based method,picks,original sentences,extraction - based method picks original sentences,0.6596375107765198
translation,136,204,results,higher score,in,q1 grammaticality,higher score in q1 grammaticality,0.4829436242580414
translation,136,204,results,results,has,system 22,results has system 22,0.6173292398452759
translation,136,205,results,score,higher than,system 22,score higher than system 22,0.6805003881454468
translation,136,205,results,our summary sentences,are,relatively more cohesive,our summary sentences are relatively more cohesive,0.5944164991378784
translation,136,205,results,q4 focus,has,score,q4 focus has score,0.5966103672981262
translation,136,205,results,results,For,q4 focus,results For q4 focus,0.5930240154266357
translation,136,206,results,score,of,q3 referential clarity,score of q3 referential clarity,0.5449889898300171
translation,136,206,results,q3 referential clarity,shows that,referential relation,q3 referential clarity shows that referential relation,0.6823973059654236
translation,136,206,results,referential relation,is,basically clear,referential relation is basically clear,0.5132107138633728
translation,136,206,results,basically clear,in,our summaries,basically clear in our summaries,0.5496191382408142
translation,136,206,results,results,has,score,results has score,0.5091484785079956
translation,136,207,results,our system,performs,better,our system performs better,0.6538035869598389
translation,136,207,results,better,than,system 22,better than system 22,0.6233727335929871
translation,136,207,results,grammaticality scores,has,our system,grammaticality scores has our system,0.5679284930229187
translation,137,68,ablation-analysis,aru module,achieved,gain,aru module achieved gain,0.7265368700027466
translation,137,68,ablation-analysis,gain,of,"0.97 rouge -1 , 0.35 rouge - 2 , and 0.64 rouge -l points","gain of 0.97 rouge -1 , 0.35 rouge - 2 , and 0.64 rouge -l points",0.6119148135185242
translation,137,68,ablation-analysis,local variance loss,boosts,model,local variance loss boosts model,0.6991854310035706
translation,137,68,ablation-analysis,model,by,"3.01 rouge -1 , 1.6 rouge - 2 , and 2.58 rouge -l","model by 3.01 rouge -1 , 1.6 rouge - 2 , and 2.58 rouge -l",0.6009638905525208
translation,137,68,ablation-analysis,ablation analysis,has,aru module,ablation analysis has aru module,0.5452507138252258
translation,137,69,ablation-analysis,global variance loss,helps with eliminating,n-gram repetitions,global variance loss helps with eliminating n-gram repetitions,0.7090302109718323
translation,137,85,ablation-analysis,our model,avoid,repetition,our model avoid repetition,0.7103862762451172
translation,137,85,ablation-analysis,our model,generate,summary,our model generate summary,0.6528347134590149
translation,137,85,ablation-analysis,summary,with,salient information,summary with salient information,0.5140738487243652
translation,137,85,ablation-analysis,variance loss,has,our model,variance loss has our model,0.5818328261375427
translation,137,85,ablation-analysis,ablation analysis,optimized by,variance loss,ablation analysis optimized by variance loss,0.753024160861969
translation,137,31,baselines,pointer-generator network ( pgn ),as,baseline model,pointer-generator network ( pgn ) as baseline model,0.5075604319572449
translation,137,31,baselines,baseline model,augments,standard attention - based seq2seq model,baseline model augments standard attention - based seq2seq model,0.6081079244613647
translation,137,31,baselines,standard attention - based seq2seq model,with,hybrid pointer network,standard attention - based seq2seq model with hybrid pointer network,0.6190876364707947
translation,137,5,model,powerful attention,helping with,reproducing,powerful attention helping with reproducing,0.766360342502594
translation,137,5,model,powerful attention,avoiding,repetitions,powerful attention avoiding repetitions,0.7295581102371216
translation,137,5,model,powerful attention,augment,vanilla attention model,powerful attention augment vanilla attention model,0.595776379108429
translation,137,5,model,vanilla attention model,from,local and global aspects,vanilla attention model from local and global aspects,0.5676926970481873
translation,137,5,model,reproducing,has,most salient information,reproducing has most salient information,0.5691056251525879
translation,137,5,model,model,To obtain,powerful attention,model To obtain powerful attention,0.5508390665054321
translation,137,6,model,attention refinement unit,paired with,local variance loss,attention refinement unit paired with local variance loss,0.6381034255027771
translation,137,6,model,attention refinement unit,paired with,global variance loss,attention refinement unit paired with global variance loss,0.6557486057281494
translation,137,6,model,local variance loss,to impose,supervision,local variance loss to impose supervision,0.7017480134963989
translation,137,6,model,supervision,on,attention model,supervision on attention model,0.57224041223526
translation,137,6,model,attention model,at,each decoding step,attention model at each decoding step,0.5138589143753052
translation,137,6,model,global variance loss,to optimize,attention distributions,global variance loss to optimize attention distributions,0.7256653308868408
translation,137,6,model,attention distributions,of,all decoding steps,attention distributions of all decoding steps,0.5387224555015564
translation,137,6,model,attention distributions,from,global perspective,attention distributions from global perspective,0.5769983530044556
translation,137,6,model,model,propose,attention refinement unit,model propose attention refinement unit,0.6625966429710388
translation,137,19,model,good performance,of,hard attention,good performance of hard attention,0.5148727893829346
translation,137,19,model,endto-end trainability,of,soft attention,endto-end trainability of soft attention,0.565390408039093
translation,137,19,model,local variance loss,to encourage,model,local variance loss to encourage model,0.7011187672615051
translation,137,19,model,model,to put,most of the attention,model to put most of the attention,0.6025005578994751
translation,137,19,model,most of the attention,on,few parts,most of the attention on few parts,0.5765325427055359
translation,137,19,model,few parts,of,input states,few parts of input states,0.5837854146957397
translation,137,19,model,input states,at,each decoding step,input states at each decoding step,0.5556639432907104
translation,137,19,model,model,To maintain,good performance,model To maintain good performance,0.6432344913482666
translation,137,19,model,model,to put,most of the attention,model to put most of the attention,0.6025005578994751
translation,137,20,model,global variance loss,to directly optimize,attention,global variance loss to directly optimize attention,0.7308008670806885
translation,137,20,model,attention,from,global perspective,attention from global perspective,0.5828210115432739
translation,137,20,model,high weights,to,same locations,high weights to same locations,0.5686452984809875
translation,137,20,model,model,propose,global variance loss,model propose global variance loss,0.6926820278167725
translation,137,25,results,our model,surpasses,strong pointer - generator baseline ( w/ o coverage ),our model surpasses strong pointer - generator baseline ( w/ o coverage ),0.5883142948150635
translation,137,25,results,strong pointer - generator baseline ( w/ o coverage ),on,all rouge metrics,strong pointer - generator baseline ( w/ o coverage ) on all rouge metrics,0.506736695766449
translation,137,25,results,all rouge metrics,by,large margin,all rouge metrics by large margin,0.5122319459915161
translation,137,25,results,results,has,our model,results has our model,0.5871725678443909
translation,138,46,baselines,supert,introducing,pseudo references,supert introducing pseudo references,0.7018107771873474
translation,138,46,baselines,baselines,has,supert,baselines has supert,0.6091812252998352
translation,138,55,baselines,tf - idf,computes,cosine similarity,tf - idf computes cosine similarity,0.6742845177650452
translation,138,55,baselines,tf - idf,computes,js divergence,tf - idf computes js divergence,0.7158330082893372
translation,138,55,baselines,cosine similarity,of,tf-idf vectors,cosine similarity of tf-idf vectors,0.5756504535675049
translation,138,55,baselines,tf-idf vectors,of,source and summaries,tf-idf vectors of source and summaries,0.5702221393585205
translation,138,55,baselines,js,computes,js divergence,js computes js divergence,0.7040839195251465
translation,138,55,baselines,js divergence,between,words distributions,js divergence between words distributions,0.6349687576293945
translation,138,55,baselines,words distributions,in,source documents and summaries,words distributions in source documents and summaries,0.49715298414230347
translation,138,55,baselines,baselines,consider,tf - idf,baselines consider tf - idf,0.5907918214797974
translation,138,93,baselines,sbert - based lexrank ( slr ),extends,"classic lexrank ( erkan and radev , 2004 ) method","sbert - based lexrank ( slr ) extends classic lexrank ( erkan and radev , 2004 ) method",0.6941186785697937
translation,138,93,baselines,"classic lexrank ( erkan and radev , 2004 ) method",by measuring,similarity,"classic lexrank ( erkan and radev , 2004 ) method by measuring similarity",0.6825538873672485
translation,138,93,baselines,similarity,using,sbert embeddings cosine similarity,similarity using sbert embeddings cosine similarity,0.599452018737793
translation,138,93,baselines,of sentences,using,sbert embeddings cosine similarity,of sentences using sbert embeddings cosine similarity,0.5043224692344666
translation,138,93,baselines,similarity,has,of sentences,similarity has of sentences,0.6056612133979797
translation,138,96,baselines,individual - graph version,builds,graph,individual - graph version builds graph,0.668895959854126
translation,138,96,baselines,individual - graph version,builds,graph,individual - graph version builds graph,0.668895959854126
translation,138,96,baselines,individual - graph version,selects,centers ( sc ),individual - graph version selects centers ( sc ),0.7194364070892334
translation,138,96,baselines,graph,for,each source document,graph for each source document,0.6206992268562317
translation,138,96,baselines,graph,considering,all sentences,graph considering all sentences,0.668873131275177
translation,138,96,baselines,centers ( sc ),from,each graph,centers ( sc ) from each graph,0.5784628391265869
translation,138,96,baselines,global - graph version,builds,graph,global - graph version builds graph,0.6676681637763977
translation,138,96,baselines,graph,considering,all sentences,graph considering all sentences,0.668873131275177
translation,138,96,baselines,all sentences,across,all source documents,all sentences across all source documents,0.667917788028717
translation,138,117,baselines,different rewards,to guide,neural temporal difference ( ntd ),different rewards to guide neural temporal difference ( ntd ),0.6466925740242004
translation,138,117,baselines,different rewards,to guide,rl - based multi-document summarizer,different rewards to guide rl - based multi-document summarizer,0.6254227161407471
translation,138,117,baselines,different rewards,has,rl - based multi-document summarizer,different rewards has rl - based multi-document summarizer,0.5297200679779053
translation,138,117,baselines,neural temporal difference ( ntd ),has,rl - based multi-document summarizer,neural temporal difference ( ntd ) has rl - based multi-document summarizer,0.5381911396980286
translation,138,118,baselines,best version of supert,selects,top 10 ( tac '08 ) or 15 ( tac '09 ) sentences,best version of supert selects top 10 ( tac '08 ) or 15 ( tac '09 ) sentences,0.683967649936676
translation,138,118,baselines,top 10 ( tac '08 ) or 15 ( tac '09 ) sentences,from,each source document,top 10 ( tac '08 ) or 15 ( tac '09 ) sentences from each source document,0.530935525894165
translation,138,118,baselines,top 10 ( tac '08 ) or 15 ( tac '09 ) sentences,to build,pseudo references,top 10 ( tac '08 ) or 15 ( tac '09 ) sentences to build pseudo references,0.6594849228858948
translation,138,118,baselines,sbert,to measure,similarity,sbert to measure similarity,0.7227513194084167
translation,138,118,baselines,similarity,between,summaries and pseudo references,similarity between summaries and pseudo references,0.6584309935569763
translation,138,118,baselines,baselines,consider,three unsupervised reward functions,baselines consider three unsupervised reward functions,0.5975176095962524
translation,138,5,model,supert,quality of,summary,supert quality of summary,0.6168414950370789
translation,138,5,model,semantic similarity,with,pseudo reference summary,semantic similarity with pseudo reference summary,0.6198081970214844
translation,138,5,model,pseudo reference summary,using,contextualized embeddings,pseudo reference summary using contextualized embeddings,0.6324365139007568
translation,138,5,model,pseudo reference summary,using,soft token alignment techniques,pseudo reference summary using soft token alignment techniques,0.6151708364486694
translation,138,5,model,model,propose,supert,model propose supert,0.6890323162078857
translation,138,7,model,supert,as,rewards,supert as rewards,0.5987292528152466
translation,138,7,model,rewards,to guide,neural - based reinforcement learning summarizer,rewards to guide neural - based reinforcement learning summarizer,0.6813102960586548
translation,138,7,model,neural - based reinforcement learning summarizer,yielding,favorable performance,neural - based reinforcement learning summarizer yielding favorable performance,0.6233047842979431
translation,138,7,model,favorable performance,compared to,state - of - the - art unsupervised summarizers,favorable performance compared to state - of - the - art unsupervised summarizers,0.6034529805183411
translation,138,7,model,model,use,supert,model use supert,0.6548197865486145
translation,138,22,model,supert,as,reward functions,supert as reward functions,0.5633699893951416
translation,138,22,model,reward functions,to guide,reinforcement learning ( rl ) based extractive summarizers,reward functions to guide reinforcement learning ( rl ) based extractive summarizers,0.6464831233024597
translation,138,22,model,model,use,supert,model use supert,0.6548197865486145
translation,138,91,model,two families of graph - based methods,to build,pseudo references,two families of graph - based methods to build pseudo references,0.5844222903251648
translation,138,91,model,model,explore,two families of graph - based methods,model explore two families of graph - based methods,0.6562165021896362
translation,138,94,model,sbert - based clustering ( sc ) method,to build,graphs,sbert - based clustering ( sc ) method to build graphs,0.721961498260498
translation,138,94,model,graphs,first measures,similarity,graphs first measures similarity,0.5831431746482849
translation,138,94,model,graphs,clusters,sentences,graphs clusters sentences,0.815539538860321
translation,138,94,model,similarity,of,sentence pairs,similarity of sentence pairs,0.5906773805618286
translation,138,94,model,sentence pairs,using,sbert,sentence pairs using sbert,0.6907463073730469
translation,138,94,model,sentences,by using,"affinity propagation ( frey and dueck , 2007 ) clustering algorithm","sentences by using affinity propagation ( frey and dueck , 2007 ) clustering algorithm",0.6598808765411377
translation,138,94,model,model,propose,sbert - based clustering ( sc ) method,model propose sbert - based clustering ( sc ) method,0.6734338402748108
translation,138,102,model,pacsum,by using,sbert,pacsum by using sbert,0.6034140586853027
translation,138,102,model,sbert,to measure,sentences similarity,sbert to measure sentences similarity,0.6685757637023926
translation,138,102,model,individualand global - graph versions,of,sps,individualand global - graph versions of sps,0.6239343881607056
translation,138,102,model,model,extend,pacsum,model extend pacsum,0.7036224603652954
translation,138,63,results,embedding - based methods ( b?hm19 and c elmo ),perform,worse,embedding - based methods ( b?hm19 and c elmo ) perform worse,0.60517817735672
translation,138,63,results,worse,than,other lexical - based baselines,worse than other lexical - based baselines,0.571521520614624
translation,138,63,results,baselines,has,embedding - based methods ( b?hm19 and c elmo ),baselines has embedding - based methods ( b?hm19 and c elmo ),0.5403566360473633
translation,138,63,results,results,Among,baselines,results Among baselines,0.6001414060592651
translation,138,65,results,embeddings,of,source documents and the summaries,embeddings of source documents and the summaries,0.5765925049781799
translation,138,65,results,embeddings,yields,higher correlation,embeddings yields higher correlation,0.7615137696266174
translation,138,65,results,higher correlation,computing,embeddings,higher correlation computing embeddings,0.7757648229598999
translation,138,65,results,results,Soft aligning,embeddings,results Soft aligning embeddings,0.5898382067680359
translation,138,73,results,outperforms,by,large margin,outperforms by large margin,0.6290680766105652
translation,138,73,results,outperforms,compared to,lexical - based metrics,outperforms compared to lexical - based metrics,0.6659675240516663
translation,138,73,results,other cosine-embedding based metrics,by,large margin,other cosine-embedding based metrics by large margin,0.5291407108306885
translation,138,73,results,outperforms,has,other cosine-embedding based metrics,outperforms has other cosine-embedding based metrics,0.5806679129600525
translation,138,73,results,lexical - based metrics,has,performance,lexical - based metrics has performance,0.5674685835838318
translation,138,73,results,performance,has,falls short,performance has falls short,0.5906819701194763
translation,138,80,results,wmd - based scores,has,substantially outperform,wmd - based scores has substantially outperform,0.5729251503944397
translation,138,80,results,substantially outperform,has,cosine-embedding counterparts,substantially outperform has cosine-embedding counterparts,0.5550926923751831
translation,138,80,results,outperforms,has,all lexical - based baselines,outperforms has all lexical - based baselines,0.5908561944961548
translation,138,80,results,results,has,wmd - based scores,results has wmd - based scores,0.488871693611145
translation,138,87,results,top 10 - 15 sentences,as,pseudo references,top 10 - 15 sentences as pseudo references,0.4860195815563202
translation,138,87,results,outperforms,by,over 16 %,outperforms by over 16 %,0.6206808686256409
translation,138,87,results,lexical - based baselines,by,over 16 %,lexical - based baselines by over 16 %,0.5851548314094543
translation,138,87,results,m sbert,by,over 4 %,m sbert by over 4 %,0.641565203666687
translation,138,87,results,outperforms,has,lexical - based baselines,outperforms has lexical - based baselines,0.59810471534729
translation,138,87,results,results,extracting,top 10 - 15 sentences,results extracting top 10 - 15 sentences,0.6493330001831055
translation,138,113,results,positionagnostic graph - based methods,perform,worse,positionagnostic graph - based methods perform worse,0.6274928450584412
translation,138,113,results,worse,than,position - aware ones,worse than position - aware ones,0.6199003458023071
translation,138,113,results,sc g,has,all other graph - based methods,sc g has all other graph - based methods,0.5560634136199951
translation,138,113,results,all other graph - based methods,has,outperform,all other graph - based methods has outperform,0.563313901424408
translation,138,113,results,outperform,has,baselines,outperform has baselines,0.6363358497619629
translation,138,113,results,results,Except for,sc g,results Except for sc g,0.6775143146514893
translation,138,114,results,position - aware graph - based sentence extraction methods,perform,worse,position - aware graph - based sentence extraction methods perform worse,0.5775576829910278
translation,138,114,results,worse,than,extracting,worse than extracting,0.6519818305969238
translation,138,114,results,extracting,has,top sentences,extracting has top sentences,0.5466805100440979
translation,138,114,results,results,find that,position - aware graph - based sentence extraction methods,results find that position - aware graph - based sentence extraction methods,0.5910676717758179
translation,138,122,results,supert,is,strongest reward,supert is strongest reward,0.5708435773849487
translation,138,122,results,strongest reward,among,considered rewards,strongest reward among considered rewards,0.5535248517990112
translation,138,122,results,ntd,perform,significantly better,ntd perform significantly better,0.6372921466827393
translation,138,122,results,perform,on par with,ysl15,perform on par with ysl15,0.7436783909797668
translation,138,122,results,ysl15,on,tac '08,ysl15 on tac '08,0.6062890887260437
translation,138,122,results,ysl15,on,tac'09,ysl15 on tac'09,0.6112505793571472
translation,138,122,results,ysl15,on,tac'09,ysl15 on tac'09,0.6112505793571472
translation,138,122,results,significantly better,on,tac'09,significantly better on tac'09,0.6106420755386353
translation,138,122,results,ntd,has,perform,ntd has perform,0.6944169998168945
translation,138,122,results,results,find,supert,results find supert,0.6390928626060486
translation,139,163,ablation-analysis,bart,has,extractive models,bart has extractive models,0.5890083312988281
translation,139,163,ablation-analysis,ablation analysis,excluding,bart,ablation analysis excluding bart,0.6733922362327576
translation,139,190,ablation-analysis,point-generator- with -coverage,reduces,duplication errors,point-generator- with -coverage reduces duplication errors,0.661492645740509
translation,139,190,ablation-analysis,point-generator- with -coverage,reduces,omission errors,point-generator- with -coverage reduces omission errors,0.6628632545471191
translation,139,190,ablation-analysis,duplication errors,from,68 to 11,duplication errors from 68 to 11,0.5991169810295105
translation,139,190,ablation-analysis,omission errors,from,286 to 256,omission errors from 286 to 256,0.5446431040763855
translation,139,190,ablation-analysis,point-generator,has,point-generator- with -coverage,point-generator has point-generator- with -coverage,0.6394897699356079
translation,139,190,ablation-analysis,ablation analysis,Compared with,point-generator,ablation analysis Compared with point-generator,0.6846056580543518
translation,139,24,baselines,baselines,has,preneural vs neural,baselines has preneural vs neural,0.5657832026481628
translation,139,76,baselines,textrank textrank,is,unsupervised key text units selection method,textrank textrank is unsupervised key text units selection method,0.525946319103241
translation,139,76,baselines,unsupervised key text units selection method,based on,graph - based ranking models,unsupervised key text units selection method based on graph - based ranking models,0.6232463121414185
translation,139,76,baselines,baselines,has,textrank textrank,baselines has textrank textrank,0.5255703926086426
translation,139,83,baselines,bertsumext,takes,"pre-trained bert ( devlin et al. , 2019 )","bertsumext takes pre-trained bert ( devlin et al. , 2019 )",0.6037284731864929
translation,139,83,baselines,"pre-trained bert ( devlin et al. , 2019 )",as,sentence encoder,"pre-trained bert ( devlin et al. , 2019 ) as sentence encoder",0.45859643816947937
translation,139,83,baselines,additional transformer,as,document encoder,additional transformer as document encoder,0.5806572437286377
translation,139,83,baselines,baselines,has,bertsumext,baselines has bertsumext,0.5867757797241211
translation,139,95,baselines,bottom - up bertsumextabs,adopts,same encoder,bottom - up bertsumextabs adopts same encoder,0.6175690293312073
translation,139,95,baselines,bottom - up bertsumextabs,adopts,6 - layer transformer decoder,bottom - up bertsumextabs adopts 6 - layer transformer decoder,0.6342257857322693
translation,139,95,baselines,same encoder,as,bertsumext,same encoder as bertsumext,0.5708484649658203
translation,139,95,baselines,6 - layer transformer decoder,with,randomly initialized parameters,6 - layer transformer decoder with randomly initialized parameters,0.5917505621910095
translation,139,95,baselines,baselines,has,bottom - up bertsumextabs,baselines has bottom - up bertsumextabs,0.573219358921051
translation,139,170,baselines,neural methods,has,bertsumext,neural methods has bertsumext,0.6108410954475403
translation,139,219,baselines,jointly,with,seq2seq denoising auto-encoder tasks,jointly with seq2seq denoising auto-encoder tasks,0.6076171398162842
translation,139,219,baselines,encoder and decoder,has,jointly,encoder and decoder has jointly,0.5874607563018799
translation,139,156,experiments,polytope,ranks,3rd,polytope ranks 3rd,0.6467856168746948
translation,139,156,experiments,polytope,ranks,4th,polytope ranks 4th,0.6369282007217407
translation,139,156,experiments,3rd,among,extractive models,3rd among extractive models,0.6110000014305115
translation,139,156,experiments,4th,among,all models,4th among all models,0.6743476390838623
translation,139,158,experiments,textrank,ranks,9th and 7th,textrank ranks 9th and 7th,0.7577188611030579
translation,139,158,experiments,9th and 7th,among,all methods,9th and 7th among all methods,0.598407506942749
translation,139,158,experiments,9th and 7th,still,competitive,9th and 7th still competitive,0.6766204833984375
translation,139,158,experiments,all methods,on,rouge and polytope,all methods on rouge and polytope,0.5654306411743164
translation,139,158,experiments,competitive,to,some abstractive neural models,competitive to some abstractive neural models,0.5848962068557739
translation,139,218,experiments,bart,ranks,1st,bart ranks 1st,0.7605063319206238
translation,139,218,experiments,1st,on,rouge and polytope,1st on rouge and polytope,0.5770969986915588
translation,139,9,model,mqm,declaring and describing,human writing quality,mqm declaring and describing human writing quality,0.5497738718986511
translation,139,9,model,framework,declaring and describing,human writing quality,framework declaring and describing human writing quality,0.6082441806793213
translation,139,9,model,human writing quality,stipulates,hierarchical listing,human writing quality stipulates hierarchical listing,0.655834972858429
translation,139,9,model,hierarchical listing,of,error types,hierarchical listing of error types,0.575974702835083
translation,139,9,model,error types,restricted to,human writing and translation,error types restricted to human writing and translation,0.6741490364074707
translation,139,9,model,model,has,mqm,model has mqm,0.634271502494812
translation,139,25,results,results,has,extractive vs abstractive,results has extractive vs abstractive,0.5086410641670227
translation,139,33,results,pre-training,achieves,better content selection capability,pre-training achieves better content selection capability,0.645020604133606
translation,139,33,results,highly effective,for,summarization,highly effective for summarization,0.6138910055160522
translation,139,33,results,better content selection capability,without,copy and coverage mechanisms,better content selection capability without copy and coverage mechanisms,0.7253740429878235
translation,139,33,results,results,has,pre-training,results has pre-training,0.528938889503479
translation,139,34,results,joint pre-training,combining,text understanding and generation,joint pre-training combining text understanding and generation,0.7584165930747986
translation,139,34,results,text understanding and generation,gives,most salient advantage,text understanding and generation gives most salient advantage,0.5761606693267822
translation,139,34,results,most salient advantage,with,bart model,most salient advantage with bart model,0.6554620265960693
translation,139,34,results,bart model,achieving,state - of - the - art results,bart model achieving state - of - the - art results,0.647341251373291
translation,139,34,results,state - of - the - art results,on,automatic and our human evaluations,state - of - the - art results on automatic and our human evaluations,0.4946087598800659
translation,139,34,results,results,has,joint pre-training,results has joint pre-training,0.5291851162910461
translation,139,159,results,largest numbers of addition errors,demonstrates,unsupervised methods,largest numbers of addition errors demonstrates unsupervised methods,0.6130351424217224
translation,139,159,results,unsupervised methods,are,relatively weaker,unsupervised methods are relatively weaker,0.5840532779693604
translation,139,161,results,no strong gap,between,extractive and abstractive methods,no strong gap between extractive and abstractive methods,0.6546592712402344
translation,139,161,results,extractive and abstractive methods,with,bart and bert - sumext,extractive and abstractive methods with bart and bert - sumext,0.6742896437644958
translation,139,161,results,bart and bert - sumext,being,top abstractive and extractive models,bart and bert - sumext being top abstractive and extractive models,0.6124278903007507
translation,139,161,results,results,On,rouge,results On rouge,0.590454638004303
translation,139,162,results,polytope,has,bart,polytope has bart,0.5692430734634399
translation,139,162,results,bart,has,overwhelmingly outperforms,bart has overwhelmingly outperforms,0.6256250739097595
translation,139,162,results,overwhelmingly outperforms,has,others,overwhelmingly outperforms has others,0.5967791676521301
translation,139,162,results,results,On,polytope,results On polytope,0.5651981830596924
translation,139,164,results,extractive methods,are,better,extractive methods are better,0.6264919638633728
translation,139,164,results,better,compared with,abstractive counterparts,better compared with abstractive counterparts,0.703618586063385
translation,139,167,results,extractive methods,do not show,stronger performances,extractive methods do not show stronger performances,0.7197046875953674
translation,139,167,results,stronger performances,in,addition and omission,stronger performances in addition and omission,0.5140653252601624
translation,139,167,results,results,has,extractive methods,results has extractive methods,0.5130205750465393
translation,139,168,results,two approaches,are,generally competitive,two approaches are generally competitive,0.6080644726753235
translation,139,168,results,generally competitive,with,each other,generally competitive with each other,0.6651543378829956
translation,139,168,results,fluency,has,two approaches,fluency has two approaches,0.6022826433181763
translation,139,171,results,bertsumext,gives,better rouge - 1/2,bertsumext gives better rouge - 1/2,0.6842285394668579
translation,139,171,results,better rouge - 1/2,compared to,summarunner,better rouge - 1/2 compared to summarunner,0.6729823350906372
translation,139,171,results,results,has,bertsumext,results has bertsumext,0.5842133164405823
translation,139,172,results,bertsumext,demonstrates advantages only in,duplication,bertsumext demonstrates advantages only in duplication,0.7156722545623779
translation,139,172,results,detailed errors,has,bertsumext,detailed errors has bertsumext,0.5781063437461853
translation,139,172,results,results,Among,detailed errors,results Among detailed errors,0.6104322671890259
translation,139,195,results,hybrid abstractive / extractive model,gives,high rouge scores,hybrid abstractive / extractive model gives high rouge scores,0.6139794588088989
translation,139,195,results,hybrid abstractive / extractive model,ranks,second worst,hybrid abstractive / extractive model ranks second worst,0.7178614735603333
translation,139,195,results,bottom - up,gives,high rouge scores,bottom - up gives high rouge scores,0.6105747222900391
translation,139,195,results,second worst,on,polytope,second worst on polytope,0.5542264580726624
translation,139,195,results,hybrid abstractive / extractive model,has,bottom - up,hybrid abstractive / extractive model has bottom - up,0.6032949090003967
translation,139,195,results,results,has,hybrid abstractive / extractive model,results has hybrid abstractive / extractive model,0.5897497534751892
translation,139,200,results,non-pretraining abstractive models,by,large margin,non-pretraining abstractive models by large margin,0.5423817038536072
translation,139,200,results,bertsumextabs and bart,has,outperform,bertsumextabs and bart has outperform,0.6495214700698853
translation,139,200,results,outperform,has,non-pretraining abstractive models,outperform has non-pretraining abstractive models,0.5807020664215088
translation,139,200,results,results,has,pre-training,results has pre-training,0.528938889503479
translation,139,220,results,large improvements,on,"addition , omission and inacc errors","large improvements on addition , omission and inacc errors",0.5369733572006226
translation,139,220,results,unified pre-training,has,for both understanding and generation,unified pre-training has for both understanding and generation,0.5655979514122009
translation,139,220,results,results,gives,large improvements,results gives large improvements,0.6470450758934021
translation,139,221,results,bart,shows,superior performance,bart shows superior performance,0.7258855104446411
translation,139,221,results,superior performance,in handling,leading bias,superior performance in handling leading bias,0.7573406100273132
translation,139,221,results,leading bias,of,cnn / dm dataset,leading bias of cnn / dm dataset,0.5586687326431274
translation,139,221,results,results,has,bart,results has bart,0.40022552013397217
translation,139,237,results,instance - level comparison,find,weak correlation,instance - level comparison find weak correlation,0.59263676404953
translation,139,237,results,weak correlation,between,rouge,weak correlation between rouge,0.7128523588180542
translation,139,237,results,weak correlation,between,human judgement,weak correlation between human judgement,0.6670074462890625
translation,139,237,results,results,For,instance - level comparison,results For instance - level comparison,0.5862593650817871
translation,139,238,results,accuracy and fluency,to,certain extent,accuracy and fluency to certain extent,0.5601711273193359
translation,139,238,results,rouge,can measure,accuracy,rouge can measure accuracy,0.6867000460624695
translation,139,238,results,accuracy,to,certain extent,accuracy to certain extent,0.5773182511329651
translation,139,238,results,rouge - 2,better than,rouge - 1/ l,rouge - 2 better than rouge - 1/ l,0.7402722239494324
translation,139,238,results,accuracy and fluency,has,rouge,accuracy and fluency has rouge,0.6148747205734253
translation,139,239,results,pearson correlation coefficient,is,"0.78 , 0.73 , 0.52","pearson correlation coefficient is 0.78 , 0.73 , 0.52",0.5094550251960754
translation,139,239,results,"0.78 , 0.73 , 0.52",for,"rouge -1 , rouge - 2 , and rouge -l","0.78 , 0.73 , 0.52 for rouge -1 , rouge - 2 , and rouge -l",0.6072670817375183
translation,139,239,results,"0.78 , 0.73 , 0.52",higher than,"0.40 , 0.32 , 0.32","0.78 , 0.73 , 0.52 higher than 0.40 , 0.32 , 0.32",0.6675861477851868
translation,139,239,results,system-level comparison,has,pearson correlation coefficient,system-level comparison has pearson correlation coefficient,0.5427169799804688
translation,139,239,results,results,For,system-level comparison,results For system-level comparison,0.5775260329246521
translation,139,245,results,pearson correlation coefficients,between,length,pearson correlation coefficients between length,0.6771079301834106
translation,139,245,results,pearson correlation coefficients,between,corresponding scores,pearson correlation coefficients between corresponding scores,0.6124429702758789
translation,139,245,results,length,of,out-puts,length of out-puts,0.6054243445396423
translation,139,245,results,length,of,corresponding scores,length of corresponding scores,0.578988254070282
translation,139,245,results,corresponding scores,is,0.25 and 0.27,corresponding scores is 0.25 and 0.27,0.5899987816810608
translation,139,245,results,bertsumext and bertsumextabs models,has,pearson correlation coefficients,bertsumext and bertsumextabs models has pearson correlation coefficients,0.5398877263069153
translation,139,246,results,reference summaries,of,150 test trials,reference summaries of 150 test trials,0.5928323268890381
translation,139,246,results,150 test trials,by means of,polytope,150 test trials by means of polytope,0.654987096786499
translation,139,246,results,polytope,obtaining,general score,polytope obtaining general score,0.6508467793464661
translation,139,246,results,general score,of,96.41,general score of 96.41,0.4654487073421478
translation,139,246,results,63 errors,in,accuracy aspect,63 errors in accuracy aspect,0.5186750292778015
translation,139,246,results,0 errors,in,fluency aspect,0 errors in fluency aspect,0.5236483216285706
translation,139,246,results,results,evaluate,reference summaries,results evaluate reference summaries,0.6148855686187744
translation,140,164,ablation-analysis,fine-tuned 12 - layer bert model,with,"bert - of-theseus ( xu et al. , 2020 )","fine-tuned 12 - layer bert model with bert - of-theseus ( xu et al. , 2020 )",0.6083709597587585
translation,140,164,ablation-analysis,fine-tuned 12 - layer bert model,obtain,performance,fine-tuned 12 - layer bert model obtain performance,0.5262231230735779
translation,140,164,ablation-analysis,performance,of,6 - layer model,performance of 6 - layer model,0.593063473701477
translation,140,164,ablation-analysis,ablation analysis,compress,fine-tuned 12 - layer bert model,ablation analysis compress fine-tuned 12 - layer bert model,0.6506058573722839
translation,140,46,baselines,baselines,has,multi-task applicable,baselines has multi-task applicable,0.5445358157157898
translation,140,48,baselines,multi-task field - shared sequence to sequence ( mtf - s2s ),achieves,better performance,multi-task field - shared sequence to sequence ( mtf - s2s ) achieves better performance,0.6823172569274902
translation,140,48,baselines,straightforward yet effective model,achieves,better performance,straightforward yet effective model achieves better performance,0.6434603333473206
translation,140,48,baselines,better performance,on,all three tasks,better performance on all three tasks,0.47258245944976807
translation,140,48,baselines,better performance,compared to,singletask counterparts,better performance compared to singletask counterparts,0.7024063467979431
translation,140,48,baselines,multi-task field - shared sequence to sequence ( mtf - s2s ),has,straightforward yet effective model,multi-task field - shared sequence to sequence ( mtf - s2s ) has straightforward yet effective model,0.5341946482658386
translation,140,48,baselines,baselines,present,multi-task field - shared sequence to sequence ( mtf - s2s ),baselines present multi-task field - shared sequence to sequence ( mtf - s2s ),0.6433030366897583
translation,140,152,baselines,mtf - s2s,on,each task,mtf - s2s on each task,0.5758542418479919
translation,140,152,baselines,mtf - s2s,to provide,single - task baseline,mtf - s2s to provide single - task baseline,0.595133364200592
translation,140,153,baselines,both mtf - s2s and seq2seq baselines,are,character - based,both mtf - s2s and seq2seq baselines are character - based,0.5946839451789856
translation,140,153,baselines,embeddings,initialized with,tencent ai lab embedding,embeddings initialized with tencent ai lab embedding,0.706073522567749
translation,140,153,baselines,baselines,has,both mtf - s2s and seq2seq baselines,baselines has both mtf - s2s and seq2seq baselines,0.5561704635620117
translation,140,154,baselines,both mtf - s2s and seq2seq baselines,use,"beam search ( wiseman and rush , 2016 )","both mtf - s2s and seq2seq baselines use beam search ( wiseman and rush , 2016 )",0.6043394207954407
translation,140,154,baselines,"beam search ( wiseman and rush , 2016 )",when,decoding,"beam search ( wiseman and rush , 2016 ) when decoding",0.656682550907135
translation,140,154,baselines,baselines,For,both mtf - s2s and seq2seq baselines,baselines For both mtf - s2s and seq2seq baselines,0.5834733843803406
translation,140,157,baselines,statistical baselines,extract,character - based unigram and bigram features,statistical baselines extract character - based unigram and bigram features,0.6719874739646912
translation,140,157,baselines,statistical baselines,use,logistic classifier,statistical baselines use logistic classifier,0.6009821891784668
translation,140,157,baselines,logistic classifier,to predict,classes,logistic classifier to predict classes,0.7660362720489502
translation,140,169,baselines,generation - based baselines,use,character - based seq2seq,generation - based baselines use character - based seq2seq,0.6299313902854919
translation,140,169,baselines,generation - based baselines,use,seq2seq,generation - based baselines use seq2seq,0.6223224997520447
translation,140,169,baselines,seq2seq,with,attention,seq2seq with attention,0.6781299710273743
translation,140,169,baselines,baselines,For,generation - based baselines,baselines For generation - based baselines,0.6334996223449707
translation,140,173,baselines,extractive methods,choose,two widely used classical methods,extractive methods choose two widely used classical methods,0.6497295498847961
translation,140,173,baselines,extractive methods,choose,lexrank,extractive methods choose lexrank,0.6590303182601929
translation,140,173,baselines,extractive methods,choose,"dpcnn ( johnson and zhang , 2017 ) radev","extractive methods choose dpcnn ( johnson and zhang , 2017 ) radev",0.7087169289588928
translation,140,173,baselines,lexrank,has,"erkan and text cnn ( kim , 2014 )","lexrank has erkan and text cnn ( kim , 2014 )",0.5920969843864441
translation,140,173,baselines,baselines,For,extractive methods,baselines For extractive methods,0.581946849822998
translation,140,174,baselines,abstractive methods,use,wean and global encoding,abstractive methods use wean and global encoding,0.6233871579170227
translation,140,174,baselines,wean and global encoding,along with,seq2seq,wean and global encoding along with seq2seq,0.6509782671928406
translation,140,174,baselines,wean and global encoding,as,baselines,wean and global encoding as baselines,0.5323567986488342
translation,140,174,baselines,baselines,For,abstractive methods,baselines For abstractive methods,0.5074514746665955
translation,140,175,baselines,"bertabs ( liu and lapata , 2019 )",has,bert - based summarization model,"bertabs ( liu and lapata , 2019 ) has bert - based summarization model",0.5657497644424438
translation,140,7,experiments,mat -inf,contains,1.07 million,mat -inf contains 1.07 million,0.6772519946098328
translation,140,7,experiments,question - answer pairs,with,human-labeled categories,question - answer pairs with human-labeled categories,0.580299437046051
translation,140,7,experiments,question - answer pairs,with,usergenerated question descriptions,question - answer pairs with usergenerated question descriptions,0.581619381904602
translation,140,7,experiments,1.07 million,has,question - answer pairs,1.07 million has question - answer pairs,0.5664623379707336
translation,140,23,experiments,matinf,consists of,question answering data,matinf consists of question answering data,0.6508010029792786
translation,140,23,experiments,question answering data,crawled from,large chinese maternity and baby caring qa site,question answering data crawled from large chinese maternity and baby caring qa site,0.7551038265228271
translation,140,47,experiments,mat -inf,is,first dataset,mat -inf is first dataset,0.5763067007064819
translation,140,47,experiments,first dataset,simultaneously contains,ground truths,first dataset simultaneously contains ground truths,0.6362221240997314
translation,140,47,experiments,ground truths,for,three major nlp tasks,ground truths for three major nlp tasks,0.5299996733665466
translation,140,47,experiments,ground truths,could facilitate,new multi-task learning methods,ground truths could facilitate new multi-task learning methods,0.633611798286438
translation,140,148,experiments,mtf - s2s,set,all ? i = 0.25,mtf - s2s set all ? i = 0.25,0.6686503291130066
translation,140,148,experiments,mtf - s2s,use,"adam ( kingma and ba , 2015 ) optimizer","mtf - s2s use adam ( kingma and ba , 2015 ) optimizer",0.6136380434036255
translation,140,148,experiments,"adam ( kingma and ba , 2015 ) optimizer",to co-train,model,"adam ( kingma and ba , 2015 ) optimizer to co-train model",0.7069347500801086
translation,140,148,experiments,model,for,one epoch,model for one epoch,0.6177431344985962
translation,140,148,experiments,one epoch,with,batch sizes,one epoch with batch sizes,0.6623514294624329
translation,140,148,experiments,one epoch,with,learning rate,one epoch with learning rate,0.6672982573509216
translation,140,148,experiments,one epoch,with,learning rate,one epoch with learning rate,0.6672982573509216
translation,140,148,experiments,batch sizes,of,"64 , 64 , 12 and 52","batch sizes of 64 , 64 , 12 and 52",0.6398622989654541
translation,140,148,experiments,"64 , 64 , 12 and 52",for,"bs summ , bs qa , bs ct opic , and bs cage","64 , 64 , 12 and 52 for bs summ , bs qa , bs ct opic , and bs cage",0.6558094620704651
translation,140,148,experiments,"bs summ , bs qa , bs ct opic , and bs cage",with,learning rate,"bs summ , bs qa , bs ct opic , and bs cage with learning rate",0.6371941566467285
translation,140,148,experiments,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,140,158,experiments,neural networks,choose,"fasttext ( grave et al. , 2017 )","neural networks choose fasttext ( grave et al. , 2017 )",0.6799436807632446
translation,140,158,experiments,neural networks,choose,"text cnn ( kim , 2014 )","neural networks choose text cnn ( kim , 2014 )",0.6953911185264587
translation,140,158,experiments,neural networks,choose,"dcnn ( kalchbrenner et al. , 2014 )","neural networks choose dcnn ( kalchbrenner et al. , 2014 )",0.612150251865387
translation,140,158,experiments,neural networks,choose,"rcnn ( lai et al. , 2015 )","neural networks choose rcnn ( lai et al. , 2015 )",0.6215212941169739
translation,140,158,experiments,neural networks,choose,"dpcnn ( johnson and zhang , 2017 )","neural networks choose dpcnn ( johnson and zhang , 2017 )",0.6705675721168518
translation,140,162,experiments,language models,fine- tune,"bert ( devlin et al. , 2019 )","language models fine- tune bert ( devlin et al. , 2019 )",0.6694621443748474
translation,140,162,experiments,language models,fine- tune,ernie,language models fine- tune ernie,0.6489564180374146
translation,140,180,experiments,tougher matinf -c-topic,has,language models,tougher matinf -c-topic has language models,0.5768499970436096
translation,140,180,experiments,language models,has,prominently outperform,language models has prominently outperform,0.5881555080413818
translation,140,180,experiments,prominently outperform,has,other baselines,prominently outperform has other baselines,0.5699324607849121
translation,140,183,experiments,language models,with,accuracy,language models with accuracy,0.6087610721588135
translation,140,183,experiments,accuracy,of,91.02,accuracy of 91.02,0.5568457245826721
translation,140,199,experiments,single - task counterpart,by,4.73,single - task counterpart by 4.73,0.5340232253074646
translation,140,199,experiments,4.73,on,rouge -l.,4.73 on rouge -l.,0.6146253943443298
translation,140,199,experiments,outperforms,has,single - task counterpart,outperforms has single - task counterpart,0.5856255888938904
translation,140,149,hyperparameters,model,for,each task,model for each task,0.6081339716911316
translation,140,149,hyperparameters,model,with,learning rate,model with learning rate,0.6086345314979553
translation,140,149,hyperparameters,learning rate,of,5 ? 10 ?5,learning rate of 5 ? 10 ?5,0.6658074855804443
translation,140,149,hyperparameters,hyperparameters,fine- tune,model,hyperparameters fine- tune model,0.7390584349632263
translation,140,151,hyperparameters,hidden size,of,all lstm encoders / decoders and attentions,hidden size of all lstm encoders / decoders and attentions,0.5590137243270874
translation,140,151,hyperparameters,all lstm encoders / decoders and attentions,is,200,all lstm encoders / decoders and attentions is 200,0.5634708404541016
translation,140,151,hyperparameters,hyperparameters,has,hidden size,hyperparameters has hidden size,0.535134494304657
translation,140,159,hyperparameters,sentences,into,words,sentences into words,0.5743101239204407
translation,140,159,hyperparameters,words,with,jieba,words with jieba,0.6950194835662842
translation,140,160,hyperparameters,word embedding,with,pretrained tencent ai lab embedding,word embedding with pretrained tencent ai lab embedding,0.5702677369117737
translation,140,160,hyperparameters,pretrained tencent ai lab embedding,except for,fasttext,pretrained tencent ai lab embedding except for fasttext,0.5860962271690369
translation,140,160,hyperparameters,hyperparameters,initialize,word embedding,hyperparameters initialize word embedding,0.7091864347457886
translation,140,161,hyperparameters,cross-entropy,with,"adam ( kingma and ba , 2015 ) optimizer","cross-entropy with adam ( kingma and ba , 2015 ) optimizer",0.6072732210159302
translation,140,161,hyperparameters,"adam ( kingma and ba , 2015 ) optimizer",with,learning rate,"adam ( kingma and ba , 2015 ) optimizer with learning rate",0.6114457249641418
translation,140,161,hyperparameters,"adam ( kingma and ba , 2015 ) optimizer",apply,early stopping,"adam ( kingma and ba , 2015 ) optimizer apply early stopping",0.6030715107917786
translation,140,161,hyperparameters,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
translation,140,161,hyperparameters,hyperparameters,minimize,cross-entropy,hyperparameters minimize cross-entropy,0.7335095405578613
translation,140,163,hyperparameters,learning rate,for,fine- tuning,learning rate for fine- tuning,0.6222808957099915
translation,140,163,hyperparameters,learning rate,apply,early stopping,learning rate apply early stopping,0.6021116971969604
translation,140,163,hyperparameters,fine- tuning,to,5 ? 10 ?5,fine- tuning to 5 ? 10 ?5,0.6243557333946228
translation,140,163,hyperparameters,hyperparameters,set,learning rate,hyperparameters set learning rate,0.5994082689285278
translation,140,163,hyperparameters,hyperparameters,apply,early stopping,hyperparameters apply early stopping,0.6104239225387573
translation,140,6,model,first jointly labeled large-scale dataset,for,classification,first jointly labeled large-scale dataset for classification,0.5444111227989197
translation,140,6,model,first jointly labeled large-scale dataset,for,question answering and summarization,first jointly labeled large-scale dataset for question answering and summarization,0.5656916499137878
translation,140,6,model,matinf,has,first jointly labeled large-scale dataset,matinf has first jointly labeled large-scale dataset,0.5018057823181152
translation,140,6,model,model,propose,matinf,model propose matinf,0.6745692491531372
translation,140,181,results,other baselines,with,considerable margin,other baselines with considerable margin,0.6373078227043152
translation,140,181,results,-lm neural networks,has,"dpcnn ( johnson and zhang , 2017 )","-lm neural networks has dpcnn ( johnson and zhang , 2017 )",0.5615208148956299
translation,140,181,results,"dpcnn ( johnson and zhang , 2017 )",has,outperforms,"dpcnn ( johnson and zhang , 2017 ) has outperforms",0.5717357397079468
translation,140,181,results,outperforms,has,other baselines,outperforms has other baselines,0.5879674553871155
translation,140,181,results,results,Among,-lm neural networks,results Among -lm neural networks,0.5844607353210449
translation,140,181,results,results,non,-lm neural networks,results non -lm neural networks,0.5944481492042542
translation,140,182,results,all other baselines,including,cnn / dm,all other baselines including cnn / dm,0.678593099117279
translation,140,182,results,all other baselines,including,lcsts,all other baselines including lcsts,0.7175990343093872
translation,140,182,results,all other baselines,including,matinf - summ,all other baselines including matinf - summ,0.6595508456230164
translation,140,182,results,matinf -c-age,has,dpcnn,matinf -c-age has dpcnn,0.6855394244194031
translation,140,182,results,dpcnn,has,outperforms,dpcnn has outperforms,0.6520873308181763
translation,140,182,results,outperforms,has,all other baselines,outperforms has all other baselines,0.5747320652008057
translation,140,182,results,cnn / dm,has,lcsts,cnn / dm has lcsts,0.6396871209144592
translation,140,182,results,lcsts,has,matinf -summ,lcsts has matinf -summ,0.6652388572692871
translation,140,182,results,results,On,matinf -c-age,results On matinf -c-age,0.5291755795478821
translation,140,186,results,mtf - s2s,shows,satisfying performance,mtf - s2s shows satisfying performance,0.6210175156593323
translation,140,186,results,satisfying performance,on,matinf -c-age and matinf - c-topic,satisfying performance on matinf -c-age and matinf - c-topic,0.550715982913971
translation,140,186,results,satisfying performance,on,outperforming,satisfying performance on outperforming,0.5908790230751038
translation,140,186,results,single task,by,0.14 and 0.19,single task by 0.14 and 0.19,0.5741496086120605
translation,140,186,results,single task,in terms of,accuracy,single task in terms of accuracy,0.6466346383094788
translation,140,186,results,0.14 and 0.19,in terms of,accuracy,0.14 and 0.19 in terms of accuracy,0.7329379320144653
translation,140,186,results,multi-task baseline,has,mtf - s2s,multi-task baseline has mtf - s2s,0.5579202771186829
translation,140,186,results,outperforming,has,same model,outperforming has same model,0.6406975388526917
translation,140,186,results,results,For,multi-task baseline,results For multi-task baseline,0.5656383037567139
translation,140,191,results,outperforms,by,margin,outperforms by margin,0.6504319310188293
translation,140,191,results,retrieval - based baseline,by,margin,retrieval - based baseline by margin,0.573697566986084
translation,140,191,results,margin,of,2.56,margin of 2.56,0.6066352725028992
translation,140,191,results,seq2seq with attention,has,outperforms,seq2seq with attention has outperforms,0.6124613285064697
translation,140,191,results,outperforms,has,retrieval - based baseline,outperforms has retrieval - based baseline,0.5845874547958374
translation,140,191,results,results,has,seq2seq with attention,results has seq2seq with attention,0.5524299740791321
translation,140,195,results,mtf - s2s,is,effective,mtf - s2s is effective,0.6827504634857178
translation,140,195,results,effective,on,qa task,effective on qa task,0.5876505374908447
translation,140,195,results,outperforms,by,0.74,outperforms by 0.74,0.6156151294708252
translation,140,195,results,single - task version,by,0.74,single - task version by 0.74,0.5316451191902161
translation,140,195,results,0.74,on,rouge - l.,0.74 on rouge - l.,0.5962496995925903
translation,140,195,results,outperforms,has,single - task version,outperforms has single - task version,0.5825417637825012
translation,140,195,results,results,has,mtf - s2s,results has mtf - s2s,0.5368064641952515
translation,140,197,results,performance,of,two basic baselines,performance of two basic baselines,0.5813236832618713
translation,140,197,results,performance,see,obvious difference,performance see obvious difference,0.5827008485794067
translation,140,197,results,two basic baselines,see,obvious difference,two basic baselines see obvious difference,0.6069635152816772
translation,140,197,results,"seq2seq + att ( luong et al. , 2015 )",see,obvious difference,"seq2seq + att ( luong et al. , 2015 ) see obvious difference",0.5564640760421753
translation,140,197,results,obvious difference,in,performance,obvious difference in performance,0.49978312849998474
translation,140,197,results,performance,between,extractive and abstractive methods,performance between extractive and abstractive methods,0.622796356678009
translation,140,197,results,extractive and abstractive methods,on,datasets,extractive and abstractive methods on datasets,0.5151613354682922
translation,140,197,results,results,comparing,performance,results comparing performance,0.7179481983184814
translation,140,198,results,all other baselines,on,matinf - summ,all other baselines on matinf - summ,0.5400347113609314
translation,140,198,results,"bertabs ( liu and lapata , 2019 )",has,significantly outperforms,"bertabs ( liu and lapata , 2019 ) has significantly outperforms",0.5960744023323059
translation,140,198,results,significantly outperforms,has,all other baselines,significantly outperforms has all other baselines,0.5790193676948547
translation,140,198,results,results,has,"bertabs ( liu and lapata , 2019 )","results has bertabs ( liu and lapata , 2019 )",0.5673883557319641
translation,141,181,ablation-analysis,outputs of re 3 sum,longer and more flu-ent than,outputs of opennmt,outputs of re 3 sum longer and more flu-ent than outputs of opennmt,0.7596619129180908
translation,141,35,baselines,rerank,measure,informativeness,rerank measure informativeness,0.6592722535133362
translation,141,35,baselines,informativeness,of,candidate template,informativeness of candidate template,0.5738450288772583
translation,141,35,baselines,candidate template,according to,hidden state relevance,candidate template according to hidden state relevance,0.5895860195159912
translation,141,35,baselines,hidden state relevance,to,input sentence,hidden state relevance to input sentence,0.523419201374054
translation,141,129,experimental-setup,popular seq2seq framework,has,open-nmt,popular seq2seq framework has open-nmt,0.5559473037719727
translation,141,129,experimental-setup,experimental setup,use,popular seq2seq framework,experimental setup use popular seq2seq framework,0.58658367395401
translation,141,130,experimental-setup,default settings,of,opennmt,default settings of opennmt,0.5763955116271973
translation,141,130,experimental-setup,opennmt,to build,network architecture,opennmt to build network architecture,0.6018284559249878
translation,141,130,experimental-setup,experimental setup,retain,default settings,experimental setup retain default settings,0.5881302356719971
translation,141,135,experimental-setup,beam search,of size,5,beam search of size 5,0.7482876181602478
translation,141,135,experimental-setup,5,to generate,summaries,5 to generate summaries,0.6165750622749329
translation,141,135,experimental-setup,experimental setup,During test,beam search,experimental setup During test beam search,0.7873499989509583
translation,141,137,experimental-setup,alpha 1,to encourage,longer generation,alpha 1 to encourage longer generation,0.7626453042030334
translation,141,137,experimental-setup,experimental setup,introduce,additional length penalty argument,experimental setup introduce additional length penalty argument,0.6067095994949341
translation,141,142,experimental-setup,standard attentional seq2seq model,with,opennmt,standard attentional seq2seq model with opennmt,0.6310296058654785
translation,141,142,experimental-setup,experimental setup,implement,standard attentional seq2seq model,experimental setup implement standard attentional seq2seq model,0.6109575033187866
translation,141,6,experiments,popular ir platform,to Retrieve,proper summaries,popular ir platform to Retrieve proper summaries,0.716064453125
translation,141,6,experiments,proper summaries,as,candidate templates,proper summaries as candidate templates,0.535599946975708
translation,141,32,experiments,widely - used information retrieval ( ir ) platform,to find out,candidate soft templates,widely - used information retrieval ( ir ) platform to find out candidate soft templates,0.6577786803245544
translation,141,32,experiments,candidate soft templates,from,training corpus,candidate soft templates from training corpus,0.5295697450637817
translation,141,134,experiments,our computer,GPU,gtx 1080,our computer GPU gtx 1080,0.7742193937301636
translation,141,134,experiments,our computer,GPU,training spends about 2 days,our computer GPU training spends about 2 days,0.6688138246536255
translation,141,134,experiments,our computer,Memory,16g,our computer Memory 16g,0.715916097164154
translation,141,134,experiments,our computer,CPU,i7-7700k,our computer CPU i7-7700k,0.7056857347488403
translation,141,134,experiments,our computer,has,training spends about 2 days,our computer has training spends about 2 days,0.5906743407249451
translation,141,214,experiments,popular ir platform lucene,to retrieve,proper existing summaries,popular ir platform lucene to retrieve proper existing summaries,0.7219407558441162
translation,141,214,experiments,proper existing summaries,as,candidate soft templates,proper existing summaries as candidate soft templates,0.5054868459701538
translation,141,95,hyperparameters,batch size,is,64,batch size is 64,0.6388692259788513
translation,141,95,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,141,96,hyperparameters,generalization,introduce,"dropout ( srivastava et al. , 2014 )","generalization introduce dropout ( srivastava et al. , 2014 )",0.5814127922058105
translation,141,96,hyperparameters,"dropout ( srivastava et al. , 2014 )",with,probability p = 0.3,"dropout ( srivastava et al. , 2014 ) with probability p = 0.3",0.6094828844070435
translation,141,96,hyperparameters,probability p = 0.3,for,rnn layers,probability p = 0.3 for rnn layers,0.5863299369812012
translation,141,96,hyperparameters,hyperparameters,To enhance,generalization,hyperparameters To enhance generalization,0.7079787850379944
translation,141,97,hyperparameters,initial learning rate,is,1,initial learning rate is 1,0.555188775062561
translation,141,97,hyperparameters,decay,by,50 %,decay by 50 %,0.6341553926467896
translation,141,97,hyperparameters,50 %,if,generation loss,50 % if generation loss,0.6478408575057983
translation,141,97,hyperparameters,generation loss,not,decrease,generation loss not decrease,0.6827065348625183
translation,141,97,hyperparameters,decrease,on,validation set,decrease on validation set,0.5466793775558472
translation,141,97,hyperparameters,hyperparameters,has,initial learning rate,hyperparameters has initial learning rate,0.4469831883907318
translation,141,5,model,existing summaries,as,soft templates,existing summaries as soft templates,0.5325955748558044
translation,141,5,model,soft templates,to guide,seq2seq model,soft templates to guide seq2seq model,0.7102941274642944
translation,141,5,model,model,proposes to use,existing summaries,model proposes to use existing summaries,0.8046182990074158
translation,141,7,model,seq2seq framework,to jointly conduct,template reranking,seq2seq framework to jointly conduct template reranking,0.619407057762146
translation,141,7,model,seq2seq framework,to jointly conduct,templateaware summary generation ( rewriting ),seq2seq framework to jointly conduct templateaware summary generation ( rewriting ),0.6410863399505615
translation,141,7,model,model,extend,seq2seq framework,model extend seq2seq framework,0.711383581161499
translation,141,30,model,model,combine,seq2seq and template based summarization approaches,model combine seq2seq and template based summarization approaches,0.6939200758934021
translation,141,31,model,re 3 sum,consists of,three modules,re 3 sum consists of three modules,0.7439109683036804
translation,141,31,model,summarization system,has,re 3 sum,summarization system has re 3 sum,0.6016032099723816
translation,141,31,model,model,call,summarization system,model call summarization system,0.6498425006866455
translation,141,33,model,seq2seq model,to jointly learn,template saliency measurement ( rerank ),seq2seq model to jointly learn template saliency measurement ( rerank ),0.7388999462127686
translation,141,33,model,seq2seq model,to jointly learn,final summary generation ( rewrite ),seq2seq model to jointly learn final summary generation ( rewrite ),0.7270859479904175
translation,141,33,model,model,extend,seq2seq model,model extend seq2seq model,0.6921594738960266
translation,141,34,model,recurrent neural network ( rnn ) encoder,convert,input sentence,recurrent neural network ( rnn ) encoder convert input sentence,0.654411256313324
translation,141,34,model,input sentence,into,hidden states,input sentence into hidden states,0.5687952041625977
translation,141,34,model,each candidate template,into,hidden states,each candidate template into hidden states,0.6241956949234009
translation,141,34,model,model,has,recurrent neural network ( rnn ) encoder,model has recurrent neural network ( rnn ) encoder,0.5631759762763977
translation,141,136,model,replace unk,to replace,generated unknown words,replace unk to replace generated unknown words,0.761179506778717
translation,141,136,model,generated unknown words,with,source word,generated unknown words with source word,0.5936879515647888
translation,141,136,model,source word,that holds,highest attention weight,source word that holds highest attention weight,0.5377892255783081
translation,141,136,model,model,add,argument,model add argument,0.6668785214424133
translation,141,215,model,seq2seq framework,to jointly conduct,template reranking,seq2seq framework to jointly conduct template reranking,0.619407057762146
translation,141,215,model,seq2seq framework,to jointly conduct,template - aware summary generation,seq2seq framework to jointly conduct template - aware summary generation,0.6294876933097839
translation,141,215,model,model,extend,seq2seq framework,model extend seq2seq framework,0.711383581161499
translation,141,39,results,soft templates,demonstrate,high competitiveness,soft templates demonstrate high competitiveness,0.6078768968582153
translation,141,39,results,informativeness,has,re 3 sum,informativeness has re 3 sum,0.5858563780784607
translation,141,39,results,re 3 sum,has,significantly outperforms,re 3 sum has significantly outperforms,0.645144522190094
translation,141,39,results,significantly outperforms,has,state- ofthe - art seq2seq models,significantly outperforms has state- ofthe - art seq2seq models,0.5377285480499268
translation,141,162,results,performance,of,random,performance of random,0.6605991125106812
translation,141,162,results,random,is,terrible,random is terrible,0.6320270895957947
translation,141,163,results,rerank,has,largely outperforms,rerank has largely outperforms,0.6075616478919983
translation,141,163,results,largely outperforms,has,first,largely outperforms has first,0.6331311464309692
translation,141,163,results,results,has,rerank,results has rerank,0.6142305135726929
translation,141,164,results,max and rerank,find,rerank performance,max and rerank find rerank performance,0.5895735621452332
translation,141,164,results,rerank performance,of,re 3 sum,rerank performance of re 3 sum,0.6052705645561218
translation,141,164,results,re 3 sum,far from,perfect,re 3 sum far from perfect,0.741998016834259
translation,141,165,results,max and first,observe that,improving capacity,max and first observe that improving capacity,0.6495354175567627
translation,141,165,results,improving capacity,of,retrieve module,improving capacity of retrieve module,0.6176176071166992
translation,141,165,results,retrieve module,is,high,retrieve module is high,0.6158398389816284
translation,141,166,results,optimal,has,greatly exceeds,optimal has greatly exceeds,0.6346983909606934
translation,141,166,results,greatly exceeds,has,all the state - of - the - art approaches,greatly exceeds has all the state - of - the - art approaches,0.5852892994880676
translation,141,166,results,results,Notice,optimal,results Notice optimal,0.7104356288909912
translation,141,171,results,linguistic quality,of,generated summaries,linguistic quality of generated summaries,0.5626705884933472
translation,141,171,results,generated summaries,from,various aspects,generated summaries from various aspects,0.5848315954208374
translation,141,171,results,results,measure,linguistic quality,results measure linguistic quality,0.7114694118499756
translation,141,172,results,re 3 sum,almost the same as,soft templates,re 3 sum almost the same as soft templates,0.7188856601715088
translation,141,174,results,soft templates,affect,our model,soft templates affect our model,0.6725901365280151
translation,141,174,results,results,investigate,soft templates,results investigate soft templates,0.5839440822601318
translation,141,176,results,more high-quality templates,provided,higher rouge scores,more high-quality templates provided higher rouge scores,0.6349825859069824
translation,141,177,results,our model,generate,acceptable summaries,our model generate acceptable summaries,0.6695721745491028
translation,141,177,results,acceptable summaries,with,random templates,acceptable summaries with random templates,0.6563389897346497
translation,142,25,ablation-analysis,models with autoregressive decoder,prone to achieving,better performance,models with autoregressive decoder prone to achieving better performance,0.7385373115539551
translation,142,25,ablation-analysis,better performance,against,non auto-regressive decoder,better performance against non auto-regressive decoder,0.6952332258224487
translation,142,30,ablation-analysis,reinforcement learning,has,42.69 r -1 score,reinforcement learning has 42.69 r -1 score,0.5346303582191467
translation,142,26,results,lstm,more likely to suffer from,architecture overfitting problem,lstm more likely to suffer from architecture overfitting problem,0.6116229891777039
translation,142,26,results,results,has,lstm,results has lstm,0.5593706965446472
translation,142,28,results,unsupervised transferable knowledge,more useful than,supervised transferable knowl- edge,unsupervised transferable knowledge more useful than supervised transferable knowl- edge,0.7188088893890381
translation,142,28,results,results,has,unsupervised transferable knowledge,results has unsupervised transferable knowledge,0.5736182332038879
translation,142,29,results,effective way,to improve,current system,effective way to improve current system,0.7445094585418701
translation,142,29,results,state - of - the - art result,on,cnn / dailymail,state - of - the - art result on cnn / dailymail,0.5192040205001831
translation,142,29,results,cnn / dailymail,by,large margin,cnn / dailymail by large margin,0.5683649778366089
translation,142,29,results,large margin,with the help of,unsupervised transferable knowledge ( 42.39 r - 1 score ),large margin with the help of unsupervised transferable knowledge ( 42.39 r - 1 score ),0.6886560320854187
translation,142,29,results,results,find,effective way,results find effective way,0.6591020822525024
translation,143,9,results,sub-summaries,automatically identified by,proposed stream-based and semantic - based approaches,sub-summaries automatically identified by proposed stream-based and semantic - based approaches,0.8076300621032715
translation,143,102,results,advantages,leads to,best overall results,advantages leads to best overall results,0.6467913389205933
translation,143,102,results,two approaches,leads to,best overall results,two approaches leads to best overall results,0.670636773109436
translation,143,102,results,advantages,has,two approaches,advantages has two approaches,0.547798752784729
translation,143,102,results,results,Combining,advantages,results Combining advantages,0.7052311301231384
translation,144,225,ablation-analysis,initial p,is,substantial,initial p is substantial,0.6895444393157959
translation,144,201,baselines,max-flow / mincut,implemented,improved shortest augmented path ( sap ) method,max-flow / mincut implemented improved shortest augmented path ( sap ) method,0.623412013053894
translation,144,201,baselines,baselines,For,max-flow / mincut,baselines For max-flow / mincut,0.6013656854629517
translation,144,8,model,length constraint,using,lagrangian relaxation,length constraint using lagrangian relaxation,0.6394758820533752
translation,144,9,model,relaxed objective function,by,supermodular binary quadratic programming problem,relaxed objective function by supermodular binary quadratic programming problem,0.5008056163787842
translation,144,9,model,efficiently,using,graph max-flow / min-cut,efficiently using graph max-flow / min-cut,0.6247676014900208
translation,144,9,model,model,bound,relaxed objective function,model bound relaxed objective function,0.7708308696746826
translation,144,10,model,convex relaxation,for,initialization,convex relaxation for initialization,0.5686304569244385
translation,144,10,model,model,use,convex relaxation,model use convex relaxation,0.6201200485229492
translation,144,27,model,model,propose,efficient decoding algorithm,model propose efficient decoding algorithm,0.6528778076171875
translation,144,35,model,grammar constraints,can be,eliminated,grammar constraints can be eliminated,0.6577501893043518
translation,144,35,model,eliminated,by adding,sbqfs,eliminated by adding sbqfs,0.7890667915344238
translation,144,35,model,sbqfs,to,objective function,sbqfs to objective function,0.5234924554824829
translation,144,35,model,model,show that,subtree deletion model,model show that subtree deletion model,0.5231928825378418
translation,144,36,model,summary length constraint,using,lagrangian relaxation,summary length constraint using lagrangian relaxation,0.5713805556297302
translation,144,36,model,model,relax,summary length constraint,model relax summary length constraint,0.7358987927436829
translation,144,37,model,lower bounds,of,ilp objective function,lower bounds of ilp objective function,0.5354800820350647
translation,144,37,model,model,propose,family of sbqfs,model propose family of sbqfs,0.6835348010063171
translation,144,226,results,convex relaxation,helps,our method,convex relaxation helps our method,0.6419501900672913
translation,144,226,results,our method,to survive,local optimality,our method to survive local optimality,0.7131136059761047
translation,144,226,results,results,Using,convex relaxation,results Using convex relaxation,0.6522216796875
translation,144,227,results,non-negativity assumption,has,very little effect,non-negativity assumption has very little effect,0.5709862112998962
translation,144,227,results,very little effect,on,standard compressive summarization,very little effect on standard compressive summarization,0.5276816487312317
translation,144,227,results,non-negativity assumption,has,very little effect,non-negativity assumption has very little effect,0.5709862112998962
translation,144,227,results,results,has,non-negativity assumption,results has non-negativity assumption,0.5691518187522888
translation,144,238,results,our proposed method,balanced,speed and quality,our proposed method balanced speed and quality,0.6756612062454224
translation,144,238,results,results,see that,our proposed method,results see that our proposed method,0.6374303698539734
translation,144,239,results,ilp,achieved,competitive rouge scores,ilp achieved competitive rouge scores,0.687105655670166
translation,144,239,results,ilp,with,about 100x speedup,ilp with about 100x speedup,0.6505991816520691
translation,144,239,results,results,Compared with,ilp,results Compared with ilp,0.6800093650817871
translation,144,242,results,constraint c0,improves,language quality,constraint c0 improves language quality,0.6675512790679932
translation,144,242,results,constraint c0,has,significantly decreases,constraint c0 has significantly decreases,0.6026178002357483
translation,144,242,results,significantly decreases,has,r - 2 score,significantly decreases has r - 2 score,0.5836703777313232
translation,144,242,results,results,Regarding,grammar constraints,results Regarding grammar constraints,0.5853139162063599
translation,145,136,baselines,kl inp,does not use,background information,kl inp does not use background information,0.6721149682998657
translation,145,136,baselines,baselines,has,kl inp,baselines has kl inp,0.6006104946136475
translation,145,7,model,systems,for,generic and update summarization,systems for generic and update summarization,0.6310847997665405
translation,145,7,model,model,develop,systems,model develop systems,0.6815473437309265
translation,145,17,model,bayesian model,for assessing,novelty,bayesian model for assessing novelty,0.7273655533790588
translation,145,17,model,novelty,taken from,summarization input,novelty taken from summarization input,0.5473065376281738
translation,145,17,model,of a sentence,taken from,summarization input,of a sentence taken from summarization input,0.5774284601211548
translation,145,17,model,summarization input,with respect to,background corpus of documents,summarization input with respect to background corpus of documents,0.6387972235679626
translation,145,17,model,novelty,has,of a sentence,novelty has of a sentence,0.5860468745231628
translation,145,17,model,model,present,bayesian model,model present bayesian model,0.6990283727645874
translation,145,28,results,our method,performs,competitively,our method performs competitively,0.5989846587181091
translation,145,28,results,competitively,with,previous log-likelihood ratio approach,competitively with previous log-likelihood ratio approach,0.6722297072410583
translation,145,28,results,previous log-likelihood ratio approach,which identifies,words,previous log-likelihood ratio approach which identifies words,0.7348822355270386
translation,145,28,results,words,with,significantly higher probability,words with significantly higher probability,0.6166730523109436
translation,145,28,results,significantly higher probability,in,input,significantly higher probability in input,0.5494224429130554
translation,145,28,results,significantly higher probability,compared to,background,significantly higher probability compared to background,0.6927319765090942
translation,145,28,results,results,find that,our method,results find that our method,0.6377817392349243
translation,145,154,results,kl back baseline,performs,significantly worse,kl back baseline performs significantly worse,0.668363094329834
translation,145,154,results,significantly worse,than,topic words and surprise summaries,significantly worse than topic words and surprise summaries,0.568001925945282
translation,145,154,results,results,has,kl back baseline,results has kl back baseline,0.605756938457489
translation,145,155,results,tops,according to,rouge -2,tops according to rouge -2,0.6843609809875488
translation,145,155,results,sr sum,has,highest rouge - 1 score,sr sum has highest rouge - 1 score,0.5598735213279724
translation,145,155,results,ts sum,has,tops,ts sum has tops,0.6611341238021851
translation,145,155,results,results,has,sr sum,results has sr sum,0.5785173177719116
translation,145,160,results,either ts or sr scores,to,kl inp,either ts or sr scores to kl inp,0.6107610464096069
translation,145,160,results,either ts or sr scores,to,kl inp + ts avg,either ts or sr scores to kl inp + ts avg,0.6001366376876831
translation,145,160,results,either ts or sr scores,leads to,better results,either ts or sr scores leads to better results,0.6716516017913818
translation,145,160,results,better results,with,kl inp + ts avg,better results with kl inp + ts avg,0.676093578338623
translation,145,160,results,kl inp + ts avg,giving,best score,kl inp + ts avg giving best score,0.6733483076095581
translation,145,160,results,results,adding,either ts or sr scores,results adding either ts or sr scores,0.6480824947357178
translation,145,161,results,surprise- based methods,have,an advantage,surprise- based methods have an advantage,0.545802116394043
translation,145,161,results,an advantage,over,topic word ones,an advantage over topic word ones,0.7239403128623962
translation,145,161,results,update summarization,has,surprise- based methods,update summarization has surprise- based methods,0.5535421967506409
translation,145,161,results,results,In,update summarization,results In update summarization,0.5761926770210266
translation,145,162,results,sr avg,significantly better than,ts avg,sr avg significantly better than ts avg,0.6728914976119995
translation,145,162,results,sr avg,significantly better than,ts sum,sr avg significantly better than ts sum,0.6905455589294434
translation,145,162,results,sr avg,better than,ts sum,sr avg better than ts sum,0.7249859571456909
translation,145,162,results,ts avg,for,rouge - 1 and rouge - 2 scores,ts avg for rouge - 1 and rouge - 2 scores,0.653458297252655
translation,145,162,results,ts sum,according to,rouge -1,ts sum according to rouge -1,0.6705427765846252
translation,145,162,results,results,has,sr avg,results has sr avg,0.5555137395858765
translation,145,164,results,surprise methods,provide,improved results,surprise methods provide improved results,0.5925552248954773
translation,145,164,results,significantly better,in terms of,rouge - 1 scores,significantly better in terms of rouge - 1 scores,0.6897348165512085
translation,145,164,results,kl inp,has,surprise methods,kl inp has surprise methods,0.605040967464447
translation,145,164,results,improved results,has,significantly better,improved results has significantly better,0.5508238673210144
translation,145,164,results,results,combined with,kl inp,results combined with kl inp,0.7548013925552368
translation,145,165,results,ts methods,lead to,any improvement,ts methods lead to any improvement,0.7540700435638428
translation,145,165,results,significantly worse,than,kl inp,significantly worse than kl inp,0.5967646241188049
translation,145,165,results,results,has,ts methods,results has ts methods,0.46939194202423096
translation,146,202,ablation-analysis,hmnet,on,test set,hmnet on test set,0.5352871417999268
translation,146,202,ablation-analysis,test set,of,ami and icsi,test set of ami and icsi,0.5829346179962158
translation,146,202,ablation-analysis,ablation analysis,of,hmnet,ablation analysis of hmnet,0.5871578454971313
translation,146,203,ablation-analysis,pretraining,on,news summarization data,pretraining on news summarization data,0.5036740303039551
translation,146,203,ablation-analysis,news summarization data,help increase,rouge - 1,news summarization data help increase rouge - 1,0.6427555680274963
translation,146,203,ablation-analysis,rouge - 1,on,ami,rouge - 1 on ami,0.6548256874084473
translation,146,203,ablation-analysis,rouge - 1,on,icsi,rouge - 1 on icsi,0.6225318908691406
translation,146,203,ablation-analysis,rouge - 1,on,icsi,rouge - 1 on icsi,0.6225318908691406
translation,146,203,ablation-analysis,ami,by,4.3 points,ami by 4.3 points,0.6210141181945801
translation,146,203,ablation-analysis,icsi,by,4.0 points,icsi by 4.0 points,0.6234129667282104
translation,146,203,ablation-analysis,ablation analysis,has,pretraining,ablation analysis has pretraining,0.5372554659843445
translation,146,204,ablation-analysis,role vector,is,removed,role vector is removed,0.6124297380447388
translation,146,204,ablation-analysis,rouge - 1 score,drops,5.2 points,rouge - 1 score drops 5.2 points,0.693178653717041
translation,146,204,ablation-analysis,rouge - 1 score,drops,2.3 points,rouge - 1 score drops 2.3 points,0.6956518888473511
translation,146,204,ablation-analysis,5.2 points,on,ami,5.2 points on ami,0.5262467265129089
translation,146,204,ablation-analysis,2.3 points,on,icsi,2.3 points on icsi,0.5485506653785706
translation,146,204,ablation-analysis,role vector,has,rouge - 1 score,role vector has rouge - 1 score,0.5462203025817871
translation,146,204,ablation-analysis,removed,has,rouge - 1 score,removed has rouge - 1 score,0.5786253213882446
translation,146,204,ablation-analysis,ablation analysis,When,role vector,ablation analysis When role vector,0.6587188243865967
translation,146,205,ablation-analysis,hm - net,without,hierarchy structure,hm - net without hierarchy structure,0.7016897201538086
translation,146,205,ablation-analysis,drops,as much as,7.9 points,drops as much as 7.9 points,0.5816470980644226
translation,146,205,ablation-analysis,drops,as much as,5.3 points,drops as much as 5.3 points,0.5249038338661194
translation,146,205,ablation-analysis,7.9 points,on,ami,7.9 points on ami,0.5575318932533264
translation,146,205,ablation-analysis,5.3 points,on,icsi,5.3 points on icsi,0.5476778745651245
translation,146,205,ablation-analysis,hm - net,has,rouge - 1 score,hm - net has rouge - 1 score,0.5804076194763184
translation,146,205,ablation-analysis,rouge - 1 score,has,drops,rouge - 1 score has drops,0.5978357791900635
translation,146,205,ablation-analysis,ablation analysis,When,hm - net,ablation analysis When hm - net,0.6568931937217712
translation,146,165,baselines,baseline model extractive oracle,concatenates,top sentences,baseline model extractive oracle concatenates top sentences,0.6975113749504089
translation,146,165,baselines,top sentences,with,highest rouge - 1 scores,top sentences with highest rouge - 1 scores,0.5961747169494629
translation,146,165,baselines,highest rouge - 1 scores,with,golden summary,highest rouge - 1 scores with golden summary,0.6336608529090881
translation,146,175,experimental-setup,"spacy ( honnibal and johnson , 2015 )",as,word tokenizer,"spacy ( honnibal and johnson , 2015 ) as word tokenizer",0.48704278469085693
translation,146,175,experimental-setup,"spacy ( honnibal and johnson , 2015 )",embed,pos and ner tags,"spacy ( honnibal and johnson , 2015 ) embed pos and ner tags",0.71484375
translation,146,175,experimental-setup,pos and ner tags,into,16 - dim vectors,pos and ner tags into 16 - dim vectors,0.5892776250839233
translation,146,175,experimental-setup,experimental setup,employ,"spacy ( honnibal and johnson , 2015 )","experimental setup employ spacy ( honnibal and johnson , 2015 )",0.5518624782562256
translation,146,176,experimental-setup,dimension,of,role vector,dimension of role vector,0.5767624378204346
translation,146,176,experimental-setup,role vector,is,32,role vector is 32,0.5975709557533264
translation,146,176,experimental-setup,experimental setup,has,dimension,experimental setup has dimension,0.4825572967529297
translation,146,178,experimental-setup,dimension,for,each word,dimension for each word,0.6295526027679443
translation,146,178,experimental-setup,each word,is,512,each word is 512,0.6177500486373901
translation,146,178,experimental-setup,each word,is,512,each word is 512,0.6177500486373901
translation,146,178,experimental-setup,model,are,512,model are 512,0.6690332293510437
translation,146,178,experimental-setup,512,for,decoder,512 for decoder,0.6898866295814514
translation,146,178,experimental-setup,512 + 16 + 16 = 544,for,word-level transformer,512 + 16 + 16 = 544 for word-level transformer,0.62236487865448
translation,146,178,experimental-setup,512 + 16 + 16 + 32 = 576,for,turn level transformer,512 + 16 + 16 + 32 = 576 for turn level transformer,0.60200434923172
translation,146,178,experimental-setup,experimental setup,has,dimension,experimental setup has dimension,0.4825572967529297
translation,146,180,experimental-setup,hmnet,has,204 m parameters,hmnet has 204 m parameters,0.5713958144187927
translation,146,180,experimental-setup,experimental setup,has,hmnet,experimental setup has hmnet,0.5773383975028992
translation,146,181,experimental-setup,dropout probability,of,0.1,dropout probability of 0.1,0.5893268585205078
translation,146,181,experimental-setup,0.1,on,all layers,0.1 on all layers,0.549395740032196
translation,146,183,experimental-setup,initial learning rate,set to,1e 9,initial learning rate set to 1e 9,0.7308791875839233
translation,146,183,experimental-setup,linearly increased,to,0.001,linearly increased to 0.001,0.5504793524742126
translation,146,183,experimental-setup,0.001,with,16000 warmup steps,0.001 with 16000 warmup steps,0.6106319427490234
translation,146,183,experimental-setup,experimental setup,has,initial learning rate,experimental setup has initial learning rate,0.49018073081970215
translation,146,184,experimental-setup,finetuning,on,meeting data,finetuning on meeting data,0.5702001452445984
translation,146,184,experimental-setup,same,except,initial learning rate,same except initial learning rate,0.7233566045761108
translation,146,184,experimental-setup,initial learning rate,set to,0.0001,initial learning rate set to 0.0001,0.6790791153907776
translation,146,184,experimental-setup,finetuning,has,optimization setup,finetuning has optimization setup,0.5649070143699646
translation,146,184,experimental-setup,meeting data,has,optimization setup,meeting data has optimization setup,0.5665733814239502
translation,146,184,experimental-setup,experimental setup,For,finetuning,experimental setup For finetuning,0.6077455878257751
translation,146,185,experimental-setup,gradient clipping,with,maximum norm,gradient clipping with maximum norm,0.6052015423774719
translation,146,185,experimental-setup,gradient clipping,with,gradient accumulation steps,gradient clipping with gradient accumulation steps,0.6140876412391663
translation,146,185,experimental-setup,maximum norm,of,2,maximum norm of 2,0.6142091155052185
translation,146,185,experimental-setup,gradient accumulation steps,as,16,gradient accumulation steps as 16,0.52684485912323
translation,146,67,experiments,hmnet model,on,news task,hmnet model on news task,0.5253823399543762
translation,146,67,experiments,hmnet model,before finetuning,meeting summarization,hmnet model before finetuning meeting summarization,0.7449661493301392
translation,146,67,experiments,news task,before finetuning,meeting summarization,news task before finetuning meeting summarization,0.730143129825592
translation,146,182,experiments,hmnet,on,news summarization data,hmnet on news summarization data,0.5212299227714539
translation,146,182,experiments,news summarization data,using,radam optimizer,news summarization data using radam optimizer,0.6345099806785583
translation,146,182,experiments,radam optimizer,with,"1 = 0.9 , 2 = 0.999","radam optimizer with 1 = 0.9 , 2 = 0.999",0.6463190317153931
translation,146,8,model,novel abstractive summary network,adapts to,meeting scenario,novel abstractive summary network adapts to meeting scenario,0.7494651675224304
translation,146,8,model,model,propose,novel abstractive summary network,model propose novel abstractive summary network,0.689449667930603
translation,146,9,model,hierarchical structure,to accommodate,long meeting transcripts,hierarchical structure to accommodate long meeting transcripts,0.6834772229194641
translation,146,9,model,role vector,to depict,difference among speakers,role vector to depict difference among speakers,0.6888123154640198
translation,146,9,model,model,design,hierarchical structure,model design hierarchical structure,0.6525158286094666
translation,146,9,model,model,design,role vector,model design role vector,0.60186767578125
translation,146,51,model,end-to - end deep learning framework,has,hierarchical meeting summarization network ( hmnet ),end-to - end deep learning framework has hierarchical meeting summarization network ( hmnet ),0.5476849675178528
translation,146,51,model,model,propose,end-to - end deep learning framework,model propose end-to - end deep learning framework,0.6496652364730835
translation,146,52,model,encoder-decoder transformer architecture,to produce,abstractive summaries,encoder-decoder transformer architecture to produce abstractive summaries,0.6810230612754822
translation,146,52,model,abstractive summaries,based on,meeting transcripts,abstractive summaries based on meeting transcripts,0.6705637574195862
translation,146,53,model,structure,to,meeting summarization,structure to meeting summarization,0.5579739809036255
translation,146,177,model,transformers,have,6 layers,transformers have 6 layers,0.6173477172851562
translation,146,177,model,transformers,have,8 heads,transformers have 8 heads,0.6151759028434753
translation,146,177,model,8 heads,in,attention,8 heads in attention,0.5907102823257446
translation,146,177,model,model,has,transformers,model has transformers,0.5978372097015381
translation,146,188,results,all baseline models,in,all metrics,all baseline models in all metrics,0.4532868266105652
translation,146,188,results,rouge - 1,has,in ami,rouge - 1 has in ami,0.6617621183395386
translation,146,188,results,rouge - 1,has,hmnet,rouge - 1 has hmnet,0.6098296046257019
translation,146,188,results,in ami,has,hmnet,in ami has hmnet,0.65378737449646
translation,146,188,results,hmnet,has,outperforms,hmnet has outperforms,0.6530845165252686
translation,146,188,results,outperforms,has,all baseline models,outperforms has all baseline models,0.5771976113319397
translation,146,189,results,hm - net,achieves,"11.62 , 2.60 and 6.66","hm - net achieves 11.62 , 2.60 and 6.66",0.6128345727920532
translation,146,189,results,higher rouge points,than,previously best results,higher rouge points than previously best results,0.5703481435775757
translation,146,189,results,icsi dataset,has,hm - net,icsi dataset has hm - net,0.5674194693565369
translation,146,189,results,"11.62 , 2.60 and 6.66",has,higher rouge points,"11.62 , 2.60 and 6.66 has higher rouge points",0.5770430564880371
translation,146,189,results,results,On,icsi dataset,results On icsi dataset,0.5393016934394836
translation,146,192,results,hmnet,higher than,mm ( topicseg + vfoa ),hmnet higher than mm ( topicseg + vfoa ),0.704431414604187
translation,146,192,results,outperforms,by,1.49 points,outperforms by 1.49 points,0.631294310092926
translation,146,192,results,outperforms,by,6.34 points,outperforms by 6.34 points,0.6225422024726868
translation,146,192,results,outperforms,higher than,mm ( topicseg + vfoa ),outperforms higher than mm ( topicseg + vfoa ),0.7467576265335083
translation,146,192,results,mm ( topicseg ),by,1.49 points,mm ( topicseg ) by 1.49 points,0.5766780376434326
translation,146,192,results,mm ( topicseg ),by,6.34 points,mm ( topicseg ) by 6.34 points,0.5853989720344543
translation,146,192,results,mm ( topicseg ),by,5.06 points,mm ( topicseg ) by 5.06 points,0.5874582529067993
translation,146,192,results,mm ( topicseg ),higher than,mm ( topicseg + vfoa ),mm ( topicseg ) higher than mm ( topicseg + vfoa ),0.7178812026977539
translation,146,192,results,mm ( topicseg ),by,5.06 points,mm ( topicseg ) by 5.06 points,0.5874582529067993
translation,146,192,results,1.49 points,in,rouge - 1,1.49 points in rouge - 1,0.5490686297416687
translation,146,192,results,6.34 points,in,rouge - 2,6.34 points in rouge - 2,0.5656386613845825
translation,146,192,results,mm ( topicseg + vfoa ),by,5.06 points,mm ( topicseg + vfoa ) by 5.06 points,0.5890388488769531
translation,146,192,results,5.06 points,in,rouge - 2,5.06 points in rouge - 2,0.579250156879425
translation,146,192,results,ami dataset,has,hmnet,ami dataset has hmnet,0.5648631453514099
translation,146,192,results,hmnet,has,outperforms,hmnet has outperforms,0.6530845165252686
translation,146,192,results,outperforms,has,mm ( topicseg ),outperforms has mm ( topicseg ),0.6398952007293701
translation,146,192,results,results,on,ami dataset,results on ami dataset,0.5442414879798889
translation,146,193,results,significantly outperforms,has,document summarization model pgnet,significantly outperforms has document summarization model pgnet,0.5971819758415222
translation,146,194,results,compares favorably,to,extractive oracle,compares favorably to extractive oracle,0.5641193389892578
translation,146,194,results,hmnet,has,compares favorably,hmnet has compares favorably,0.6197724342346191
translation,146,194,results,results,has,hmnet,results has hmnet,0.575340986251831
translation,146,195,results,copy from train,obtains,surprisingly good result,copy from train obtains surprisingly good result,0.634611964225769
translation,146,195,results,surprisingly good result,in,ami and icsi,surprisingly good result in ami and icsi,0.5697762966156006
translation,146,195,results,surprisingly good result,higher than,most baselines,surprisingly good result higher than most baselines,0.6639991402626038
translation,146,195,results,most baselines,including,pgnet,most baselines including pgnet,0.7421294450759888
translation,146,195,results,results,worth noting,copy from train,results worth noting copy from train,0.6689000725746155
translation,146,218,results,hmnet,achieves,much higher scores,hmnet achieves much higher scores,0.6627417802810669
translation,146,218,results,much higher scores,in,readability and relevance,much higher scores in readability and relevance,0.4754585325717926
translation,146,218,results,readability and relevance,than,uns,readability and relevance than uns,0.552411675453186
translation,146,218,results,results,shows,hmnet,results shows hmnet,0.6880788207054138
translation,146,219,results,scores,for,hmnet,scores for hmnet,0.6622745394706726
translation,146,219,results,scores,above,4.0,scores above 4.0,0.7079489827156067
translation,146,219,results,hmnet,above,4.0,hmnet above 4.0,0.7004022598266602
translation,146,219,results,results,has,scores,results has scores,0.5219217538833618
translation,147,156,baselines,peer65,is,best performing system,peer65 is best performing system,0.6046803593635559
translation,147,156,baselines,best performing system,in,task 2,best performing system in task 2,0.5397137403488159
translation,147,156,baselines,best performing system,in terms of,rouge - 1,best performing system in terms of rouge - 1,0.6536465287208557
translation,147,156,baselines,task 2,of,duc2004 competition,task 2 of duc2004 competition,0.4959867596626282
translation,147,156,baselines,baselines,has,peer65,baselines has peer65,0.6236052513122559
translation,147,132,hyperparameters,trace decay parameter,fixed to,1,trace decay parameter fixed to 1,0.7523819208145142
translation,147,132,hyperparameters,1,for,episodic tasks,1 for episodic tasks,0.4977492690086365
translation,147,132,hyperparameters,hyperparameters,has,discount rate ?,hyperparameters has discount rate ?,0.54886794090271
translation,147,132,hyperparameters,hyperparameters,has,trace decay parameter,hyperparameters has trace decay parameter,0.4663928151130676
translation,147,5,model,method of reinforcement learning,can be adapted to,automatic summarization,method of reinforcement learning can be adapted to automatic summarization,0.6837847232818604
translation,147,163,results,asrl,superior to,peer65,asrl superior to peer65,0.7192280888557434
translation,147,163,results,asrl,superior to,ilp,asrl superior to ilp,0.7519199252128601
translation,147,163,results,asrl,superior to,greedy,asrl superior to greedy,0.7488322257995605
translation,147,163,results,asrl,comparable to,mckp,asrl comparable to mckp,0.7024882435798645
translation,147,163,results,results,imply,asrl,results imply asrl,0.5337461233139038
translation,147,166,results,asrl,achieved,higher rouge scores,asrl achieved higher rouge scores,0.7385655641555786
translation,147,166,results,higher rouge scores,than,ilp,higher rouge scores than ilp,0.6378440856933594
translation,148,8,experiments,visual information,learn,joint representations,visual information learn joint representations,0.6365329027175903
translation,148,8,experiments,joint representations,of,text and images,joint representations of text and images,0.6064408421516418
translation,148,8,experiments,joint representations,using,neural network,joint representations using neural network,0.6381791830062866
translation,148,8,experiments,text and images,using,neural network,text and images using neural network,0.6804838180541992
translation,148,7,model,audio information,design,approach,audio information design approach,0.6044079661369324
translation,148,7,model,audio information,to selectively use,transcription,audio information to selectively use transcription,0.7010241150856018
translation,148,7,model,approach,to selectively use,transcription,approach to selectively use transcription,0.7834766507148743
translation,148,7,model,model,For,audio information,model For audio information,0.6156378984451294
translation,149,153,baselines,majority baselines,assign,same aspect,majority baselines assign same aspect,0.6932927370071411
translation,149,153,baselines,same aspect,to,all words ( sentences ),same aspect to all words ( sentences ),0.5611620545387268
translation,149,153,baselines,all words ( sentences ),in,doc,all words ( sentences ) in doc,0.5399610996246338
translation,149,153,baselines,baselines,has,majority baselines,baselines has majority baselines,0.5700070261955261
translation,149,159,baselines,each word,to,document 's most prevalent aspect,each word to document 's most prevalent aspect,0.567873477935791
translation,149,159,baselines,document 's most prevalent aspect,on,word -( wordmax ),document 's most prevalent aspect on word -( wordmax ),0.5516098737716675
translation,149,159,baselines,document 's most prevalent aspect,on,sentence level ( sent-max ),document 's most prevalent aspect on sentence level ( sent-max ),0.5185926556587219
translation,149,160,baselines,unsupervised topic model baseline ( lda ),trained on,training portion,unsupervised topic model baseline ( lda ) trained on training portion,0.7307072281837463
translation,149,160,baselines,training portion,of,synthetic data set,training portion of synthetic data set,0.6040744185447693
translation,149,160,baselines,baselines,has,unsupervised topic model baseline ( lda ),baselines has unsupervised topic model baseline ( lda ),0.5355056524276733
translation,149,122,hyperparameters,development set,for,early stopping,development set for early stopping,0.6377935409545898
translation,149,122,hyperparameters,hyperparameters,use,development set,hyperparameters use development set,0.662821888923645
translation,149,7,model,latent document structure,jointly with,abstractive summarization objective,latent document structure jointly with abstractive summarization objective,0.5664553642272949
translation,149,7,model,our models,in,scalable synthetic setup,our models in scalable synthetic setup,0.49243977665901184
translation,149,7,model,model,induce,latent document structure,model induce latent document structure,0.6544997692108154
translation,149,36,model,summaries,with respect to,abstract input aspects,summaries with respect to abstract input aspects,0.6433299779891968
translation,149,37,model,neural encoder-decoder architectures,with,attention,neural encoder-decoder architectures with attention,0.6245285868644714
translation,149,37,model,neural encoder-decoder architectures,extend,pointer- generator architecture,neural encoder-decoder architectures extend pointer- generator architecture,0.6629996299743652
translation,149,37,model,model,build on,neural encoder-decoder architectures,model build on neural encoder-decoder architectures,0.7061775326728821
translation,149,37,model,model,extend,pointer- generator architecture,model extend pointer- generator architecture,0.7015513181686401
translation,149,211,model,document structure,as,latent,document structure as latent,0.5307368636131287
translation,149,211,model,efficient training,with no need for,subdocument level topic annotations,efficient training with no need for subdocument level topic annotations,0.6843084096908569
translation,149,211,model,model,Treating,document structure,model Treating document structure,0.6522894501686096
translation,149,212,model,latent document structure,induced jointly with,summarization objective,latent document structure induced jointly with summarization objective,0.6299763917922974
translation,149,212,model,model,has,latent document structure,model has latent document structure,0.51111900806427
translation,149,8,results,benefit,of,learnt document structure,benefit of learnt document structure,0.5399461984634399
translation,149,8,results,documents,by,aspect,documents by aspect,0.5891839265823364
translation,149,8,results,structure,to produce,abstractive and extractive aspectbased summaries,structure to produce abstractive and extractive aspectbased summaries,0.6846111416816711
translation,149,8,results,results,demonstrate,benefit,results demonstrate benefit,0.5929981470108032
translation,149,136,results,aspect-aware models,beat,both baselines,aspect-aware models beat both baselines,0.6483261585235596
translation,149,136,results,both baselines,by,large margin,both baselines by large margin,0.5835109353065491
translation,149,136,results,results,has,aspect-aware models,results has aspect-aware models,0.5355996489524841
translation,149,137,results,classical summarization,has,lead - 3 baseline,classical summarization has lead - 3 baseline,0.5641031861305237
translation,149,138,results,abstractive counterparts,in terms of,rouge,abstractive counterparts in terms of rouge,0.6821342706680298
translation,149,138,results,extractive aspect-aware models,has,outperform,extractive aspect-aware models has outperform,0.581900417804718
translation,149,138,results,outperform,has,abstractive counterparts,outperform has abstractive counterparts,0.6046250462532043
translation,149,139,results,models,enable,abstractive and extractive aspectaware summarization,models enable abstractive and extractive aspectaware summarization,0.6888434290885925
translation,149,141,results,performance,of,our own models,performance of our own models,0.6042152643203735
translation,149,141,results,performance,degrades,more gracefully,performance degrades more gracefully,0.808466911315918
translation,149,141,results,results,has,performance,results has performance,0.5972660779953003
translation,149,142,results,pg - net,on,natural documents,pg - net on natural documents,0.589257001876831
translation,149,142,results,some of our aspect-aware methods,has,outperform,some of our aspect-aware methods has outperform,0.5619817972183228
translation,149,142,results,outperform,has,pg - net,outperform has pg - net,0.6481828093528748
translation,149,142,results,results,Note,some of our aspect-aware methods,results Note some of our aspect-aware methods,0.5677658319473267
translation,149,167,results,attention - aware summarization models,has,outperform,attention - aware summarization models has outperform,0.5671116709709167
translation,149,167,results,outperform,has,all baselines,outperform has all baselines,0.594290554523468
translation,149,167,results,results,has,attention - aware summarization models,results has attention - aware summarization models,0.5232958793640137
translation,149,168,results,lda,not,more informed per-document majority baselines,lda not more informed per-document majority baselines,0.6805804967880249
translation,149,168,results,lda,has,outperforms,lda has outperforms,0.6188812851905823
translation,149,168,results,outperforms,has,most basic global - max baseline,outperforms has most basic global - max baseline,0.5659582018852234
translation,149,168,results,results,has,lda,results has lda,0.4954433739185333
translation,149,169,results,mnb,as,supervised model,mnb as supervised model,0.5249460935592651
translation,149,169,results,supervised model,trained specifically to classify,sentences,supervised model trained specifically to classify sentences,0.773409903049469
translation,149,169,results,sentences,performs,competitively,sentences performs competitively,0.6567595601081848
translation,149,169,results,unsurprisingly,has,mnb,unsurprisingly has mnb,0.6674039363861084
translation,149,169,results,results,has,unsurprisingly,results has unsurprisingly,0.5609158277511597
translation,149,169,results,results,has,mnb,results has mnb,0.5745794177055359
translation,149,180,results,outperform,by,large margin,outperform by large margin,0.6385749578475952
translation,149,180,results,pg - baseline,by,large margin,pg - baseline by large margin,0.6128427386283875
translation,149,180,results,pg - baseline,on,long and average documents,pg - baseline on long and average documents,0.5597109198570251
translation,149,180,results,our models,has,outperform,our models has outperform,0.6103132367134094
translation,149,180,results,outperform,has,pg - baseline,outperform has pg - baseline,0.6272966265678406
translation,149,192,results,global majority baseline,shows that,gold aspect distribution,global majority baseline shows that gold aspect distribution,0.6576717495918274
translation,149,192,results,gold aspect distribution,in,rcv1 corpus,gold aspect distribution in rcv1 corpus,0.48774537444114685
translation,149,192,results,majority class assignment,leads to,strong baseline,majority class assignment leads to strong baseline,0.6476673483848572
translation,149,192,results,results,has,global majority baseline,results has global majority baseline,0.5452545285224915
translation,149,204,results,extractive models,score,higher,extractive models score higher,0.6868427395820618
translation,149,204,results,extractive models,score,aspect-agnostic informativeness,extractive models score aspect-agnostic informativeness,0.706062912940979
translation,149,204,results,extractive models,on,aspect-agnostic informativeness,extractive models on aspect-agnostic informativeness,0.5044310688972473
translation,149,204,results,higher,on,fluency,higher on fluency,0.5246376395225525
translation,149,204,results,results,has,extractive models,results has extractive models,0.5364269614219666
translation,149,205,results,outperform,in terms of,aspect-labeling accuracy ( acc ),outperform in terms of aspect-labeling accuracy ( acc ),0.6900871396064758
translation,149,205,results,all other systems,in terms of,aspect-labeling accuracy ( acc ),all other systems in terms of aspect-labeling accuracy ( acc ),0.6188549399375916
translation,149,205,results,distinct aspects,to,two summaries,distinct aspects to two summaries,0.600882351398468
translation,149,205,results,abstractive models,has,outperform,abstractive models has outperform,0.6073485016822815
translation,149,205,results,outperform,has,all other systems,outperform has all other systems,0.6043971180915833
translation,149,205,results,results,has,abstractive models,results has abstractive models,0.5407604575157166
translation,149,242,results,encoder attention model,has,outperforms,encoder attention model has outperforms,0.6215093731880188
translation,149,242,results,outperforms,has,all other systems,outperforms has all other systems,0.5778034329414368
translation,149,242,results,outperforms,has,both baselines,outperforms has both baselines,0.5924112200737
translation,149,242,results,results,shows,encoder attention model,results shows encoder attention model,0.6630620360374451
translation,150,144,ablation-analysis,unified - sum ( e ),covers,most facets ( high far ),unified - sum ( e ) covers most facets ( high far ),0.7697951793670654
translation,150,144,ablation-analysis,decent lexical matches,has,high rr ),decent lexical matches has high rr ),0.6089186072349548
translation,150,144,ablation-analysis,ablation analysis,has,unified - sum ( e ),ablation analysis has unified - sum ( e ),0.5503577589988708
translation,150,130,baselines,fastrl ( e +a ),has,ex-tractive + abstractive ),fastrl ( e +a ) has ex-tractive + abstractive ),0.6090378165245056
translation,150,159,experiments,sentence regression,to automatically create,fams,sentence regression to automatically create fams,0.6375346183776855
translation,150,5,model,facetaware evaluation setup,for better assessment,information coverage,facetaware evaluation setup for better assessment information coverage,0.625845193862915
translation,150,5,model,information coverage,in,extracted summaries,information coverage in extracted summaries,0.4687563478946686
translation,150,5,model,model,present,facetaware evaluation setup,model present facetaware evaluation setup,0.6412369012832642
translation,150,6,model,each sentence,in,reference summary,each sentence in reference summary,0.4981461465358734
translation,150,6,model,reference summary,as,facet,reference summary as facet,0.5666099786758423
translation,150,6,model,sentences,in,document,sentences in document,0.5417858362197876
translation,150,6,model,sentences,express,semantics,sentences express semantics,0.6727557182312012
translation,150,6,model,sentences,of,facet,sentences of facet,0.6003595590591431
translation,150,6,model,sentences,of,facet,sentences of facet,0.6003595590591431
translation,150,6,model,semantics,of,each facet,semantics of each facet,0.590700626373291
translation,150,6,model,each facet,as,support sentences,each facet as support sentences,0.5258941650390625
translation,150,6,model,support sentences,of,facet,support sentences of facet,0.5972056984901428
translation,150,6,model,model,treat,each sentence,model treat each sentence,0.5749233365058899
translation,150,120,results,almost no discrimination,among,last four methods,almost no discrimination among last four methods,0.6056007742881775
translation,150,120,results,last four methods,under,rouge - 1 f1,last four methods under rouge - 1 f1,0.6281394362449646
translation,150,120,results,rankings,under,rouge - 1/2/l,rankings under rouge - 1/2/l,0.6878404021263123
translation,150,123,results,upper bound of far,by extracting,3 sentences,upper bound of far by extracting 3 sentences,0.7477890849113464
translation,150,123,results,facet coverage,has,upper bound of far,facet coverage has upper bound of far,0.5774329304695129
translation,150,123,results,results,For,facet coverage,results For facet coverage,0.5655805468559265
translation,150,131,results,extractive methods,perform,poorly,extractive methods perform poorly,0.6413651704788208
translation,150,131,results,poorly,on,high abstraction samples,poorly on high abstraction samples,0.5799494981765747
translation,150,131,results,results,has,extractive methods,results has extractive methods,0.5130205750465393
translation,150,132,results,abstractive methods,exhibit,huge performance gap,abstractive methods exhibit huge performance gap,0.6020558476448059
translation,150,132,results,huge performance gap,between,low and high abstraction samples,huge performance gap between low and high abstraction samples,0.6155982613563538
translation,150,132,results,results,has,abstractive methods,results has abstractive methods,0.44091805815696716
translation,150,133,results,all the compared methods,perform,much worse,all the compared methods perform much worse,0.5475642085075378
translation,150,133,results,much worse,on,documents,much worse on documents,0.5762378573417664
translation,150,133,results,much worse,with,  noisy   reference summaries,much worse with   noisy   reference summaries,0.6474488973617554
translation,150,133,results,results,found that,all the compared methods,results found that all the compared methods,0.58089679479599
translation,150,185,results,three support groups,shows,highest correlation,three support groups shows highest correlation,0.6771889328956604
translation,150,185,results,highest correlation,for,same sentence regression approach,highest correlation for same sentence regression approach,0.6251756548881531
translation,150,185,results,results,observe,three support groups,results observe three support groups,0.5526487231254578
translation,150,185,results,results,creating,three support groups,results creating three support groups,0.5852258801460266
translation,150,186,results,fams,created by,rouge -1 f1 and rouge - avg f1,fams created by rouge -1 f1 and rouge - avg f1,0.627634584903717
translation,150,186,results,fams,very high correlation with,human annotation,fams very high correlation with human annotation,0.692969024181366
translation,150,186,results,rouge -1 f1 and rouge - avg f1,very high correlation with,human annotation,rouge -1 f1 and rouge - avg f1 very high correlation with human annotation,0.7187113165855408
translation,150,186,results,results,has,fams,results has fams,0.5645242929458618
translation,151,28,baselines,baselines,investigate,"unsupervised graph , term frequency and probability based single document summarization","baselines investigate unsupervised graph , term frequency and probability based single document summarization",0.5725146532058716
translation,151,50,baselines,klsum,utilizes,probabilistic approach,klsum utilizes probabilistic approach,0.628460705280304
translation,151,50,baselines,baselines,has,klsum,baselines has klsum,0.5707942247390747
translation,151,122,experiments,luhn summarization,performs,poorly,luhn summarization performs poorly,0.6457960605621338
translation,151,122,experiments,poorly,on,question answering task,poorly on question answering task,0.5519158244132996
translation,151,122,experiments,question answering task,scoring,only 38.8 %,question answering task scoring only 38.8 %,0.6686261892318726
translation,151,7,results,luhn and textrank summaries,have,highest rouge scores,luhn and textrank summaries have highest rouge scores,0.5496019124984741
translation,151,7,results,intrinsically,has,luhn and textrank summaries,intrinsically has luhn and textrank summaries,0.5864413380622864
translation,151,7,results,results,evaluated,intrinsically,results evaluated intrinsically,0.508211612701416
translation,151,8,results,textrank summaries,performs,best,textrank summaries performs best,0.605276882648468
translation,151,8,results,extrinsically,has,textrank summaries,extrinsically has textrank summaries,0.5808554291725159
translation,151,8,results,results,evaluated,extrinsically,results evaluated extrinsically,0.7129507064819336
translation,151,107,results,reference summaries,scored,67.3 %,reference summaries scored 67.3 %,0.7018850445747375
translation,151,107,results,summaries,of,tex -trank,summaries of tex -trank,0.6765869855880737
translation,151,107,results,lexrank,scoring,40.8 %,lexrank scoring 40.8 %,0.6621679067611694
translation,151,107,results,luhn,scoring,38.8 %,luhn scoring 38.8 %,0.6662822961807251
translation,151,107,results,klsum,scoring,46.9 %,klsum scoring 46.9 %,0.6723061203956604
translation,151,107,results,67.3 %,has,outperforming,67.3 % has outperforming,0.6011568903923035
translation,151,107,results,outperforming,has,summaries,outperforming has summaries,0.6003885269165039
translation,151,107,results,results,has,reference summaries,results has reference summaries,0.49279356002807617
translation,151,109,results,question answering ability,see that,human generated reference summaries,question answering ability see that human generated reference summaries,0.5697618126869202
translation,151,109,results,human generated reference summaries,are able to score,better,human generated reference summaries are able to score better,0.67311030626297
translation,151,109,results,better,on,exam questions,better on exam questions,0.4964563846588135
translation,151,109,results,better,compared to,machine generated summaries,better compared to machine generated summaries,0.6563867330551147
translation,151,110,results,tex -trank,scores,most,tex -trank scores most,0.6879467368125916
translation,151,110,results,most,on,exam questions,most on exam questions,0.46684712171554565
translation,151,110,results,unsupervised approaches,has,tex -trank,unsupervised approaches has tex -trank,0.6462558507919312
translation,151,110,results,results,Among,unsupervised approaches,results Among unsupervised approaches,0.5615434050559998
translation,152,174,ablation-analysis,primarily improve,has,rouge - 1,primarily improve has rouge - 1,0.6349493861198425
translation,152,174,ablation-analysis,ablation analysis,expect,better content selection,ablation analysis expect better content selection,0.6730012893676758
translation,152,5,model,data-efficient content selector,to overdetermine,phrases,data-efficient content selector to overdetermine phrases,0.7252936959266663
translation,152,5,model,phrases,in,source document,phrases in source document,0.4803101122379303
translation,152,5,model,phrases,part of,summary,phrases part of summary,0.6641557812690735
translation,152,5,model,model,use,data-efficient content selector,model use data-efficient content selector,0.6557855606079102
translation,152,6,model,selector,as,bottom - up attention step,selector as bottom - up attention step,0.5464102029800415
translation,152,6,model,bottom - up attention step,to constrain,model,bottom - up attention step to constrain model,0.7270203828811646
translation,152,6,model,model,to,likely phrases,model to likely phrases,0.575040876865387
translation,152,6,model,model,use,selector,model use selector,0.7036148309707642
translation,152,28,model,bottom - up attention,for,neural abstractive summarization,bottom - up attention for neural abstractive summarization,0.5541103482246399
translation,152,28,model,model,consider,bottom - up attention,model consider bottom - up attention,0.682485044002533
translation,152,29,model,selection mask,for,source document,selection mask for source document,0.5916861891746521
translation,152,29,model,selection mask,constrains,standard neural model,selection mask constrains standard neural model,0.8021421432495117
translation,152,29,model,standard neural model,by,mask,standard neural model by mask,0.6093002557754517
translation,152,32,model,separate content selection system,to decide on,relevant aspects,separate content selection system to decide on relevant aspects,0.7188646197319031
translation,152,32,model,relevant aspects,of,source document,relevant aspects of source document,0.5545853972434998
translation,152,32,model,model,incorporates,separate content selection system,model incorporates separate content selection system,0.6905490756034851
translation,152,33,model,selection task,as,sequence - tagging problem,selection task as sequence - tagging problem,0.5016401410102844
translation,152,33,model,sequence - tagging problem,with the objective of,identifying tokens,sequence - tagging problem with the objective of identifying tokens,0.6362449526786804
translation,152,33,model,identifying tokens,from,document,identifying tokens from document,0.5800398588180542
translation,152,33,model,identifying tokens,part of,summary,identifying tokens part of summary,0.6898579001426697
translation,152,33,model,model,frame,selection task,model frame selection task,0.7332189083099365
translation,152,35,model,masking,to constrain,copying words,masking to constrain copying words,0.71556556224823
translation,152,35,model,masking,produces,grammatical outputs,masking produces grammatical outputs,0.7031037211418152
translation,152,35,model,copying words,to,selected parts,copying words to selected parts,0.6009759306907654
translation,152,35,model,selected parts,of,text,selected parts of text,0.633080005645752
translation,152,35,model,model,employ,masking,model employ masking,0.6260147094726562
translation,152,38,results,bottom - up attention,leads to,improvement,bottom - up attention leads to improvement,0.6739158630371094
translation,152,38,results,improvement,in,rouge -l score,improvement in rouge -l score,0.5210318565368652
translation,152,38,results,improvement,on,cnn - daily mail ( cnn - dm ) corpus,improvement on cnn - daily mail ( cnn - dm ) corpus,0.5296998620033264
translation,152,38,results,rouge -l score,on,cnn - daily mail ( cnn - dm ) corpus,rouge -l score on cnn - daily mail ( cnn - dm ) corpus,0.5261080265045166
translation,152,38,results,cnn - daily mail ( cnn - dm ) corpus,from,36.4 to 38.3,cnn - daily mail ( cnn - dm ) corpus from 36.4 to 38.3,0.5261396765708923
translation,152,38,results,baseline models,has,bottom - up attention,baseline models has bottom - up attention,0.5305778384208679
translation,152,38,results,results,Compared to,baseline models,results Compared to baseline models,0.6755432486534119
translation,152,39,results,comparable or better results,than,recent reinforcement - learning based methods,comparable or better results than recent reinforcement - learning based methods,0.5874711871147156
translation,152,39,results,recent reinforcement - learning based methods,with,our mle trained system,recent reinforcement - learning based methods with our mle trained system,0.6405209898948669
translation,152,39,results,results,see,comparable or better results,results see comparable or better results,0.5207095742225647
translation,152,40,results,content selection model,is,very data-efficient,content selection model is very data-efficient,0.5208150148391724
translation,152,40,results,content selection model,can be trained with,less than 1 %,content selection model can be trained with less than 1 %,0.7522590756416321
translation,152,40,results,less than 1 %,of,original training data,less than 1 % of original training data,0.5957332849502563
translation,152,40,results,results,find that,content selection model,results find that content selection model,0.632112979888916
translation,152,42,results,summarization model,trained on,cnn - dm,summarization model trained on cnn - dm,0.6880732178688049
translation,152,42,results,summarization model,evaluated on,nyt corpus,summarization model evaluated on nyt corpus,0.6399216055870056
translation,152,42,results,summarization model,improved,over 5 points,summarization model improved over 5 points,0.6875706911087036
translation,152,42,results,nyt corpus,improved,over 5 points,nyt corpus improved over 5 points,0.6671174764633179
translation,152,42,results,over 5 points,in,rouge -l,over 5 points in rouge -l,0.6056649684906006
translation,152,42,results,over 5 points,with,content selector,over 5 points with content selector,0.6691588759422302
translation,152,42,results,content selector,trained on,"only 1,000 in- domain sentences","content selector trained on only 1,000 in- domain sentences",0.7248032689094543
translation,152,42,results,results,show,summarization model,results show summarization model,0.6433671712875366
translation,152,169,results,results,on,cnn - dm corpus,results on cnn - dm corpus,0.5040934085845947
translation,152,170,results,coverage inference penalty,scores,the same,coverage inference penalty scores the same,0.6840020418167114
translation,152,170,results,the same,as a,full coverage mechanism,the same as a full coverage mechanism,0.5698162317276001
translation,152,170,results,full coverage mechanism,without requiring,additional model parameters,full coverage mechanism without requiring additional model parameters,0.7167470455169678
translation,152,171,results,end-to - end models,lead to,improvements,end-to - end models lead to improvements,0.7183060050010681
translation,152,171,results,results,found that,end-to - end models,results found that end-to - end models,0.6427470445632935
translation,152,171,results,results,none of,end-to - end models,results none of end-to - end models,0.5229172110557556
translation,152,172,results,mask only model,with,increased supervision,mask only model with increased supervision,0.6350009441375732
translation,152,172,results,mask only model,performs,very similar,mask only model performs very similar,0.6550209522247314
translation,152,172,results,increased supervision,on,copy mechanism,increased supervision on copy mechanism,0.5622325539588928
translation,152,172,results,increased supervision,performs,very similar,increased supervision performs very similar,0.5663427114486694
translation,152,172,results,very similar,to,multi-task model,very similar to multi-task model,0.5756454467773438
translation,152,172,results,results,has,mask only model,results has mask only model,0.5781034231185913
translation,152,173,results,bottom - up attention,leads to,major improvement,bottom - up attention leads to major improvement,0.64466392993927
translation,152,173,results,major improvement,across,all three scores,major improvement across all three scores,0.5813156962394714
translation,152,173,results,results,has,bottom - up attention,results has bottom - up attention,0.5159711241722107
translation,152,175,results,outperforms,in,rouge - 1 and 2,outperforms in rouge - 1 and 2,0.5637883543968201
translation,152,175,results,all of the reinforcementlearning based approaches,in,rouge - 1 and 2,all of the reinforcementlearning based approaches in rouge - 1 and 2,0.5549191832542419
translation,152,175,results,cross-entropy trained approach,has,outperforms,cross-entropy trained approach has outperforms,0.6129999756813049
translation,152,175,results,outperforms,has,all of the reinforcementlearning based approaches,outperforms has all of the reinforcementlearning based approaches,0.591751754283905
translation,152,175,results,results,has,cross-entropy trained approach,results has cross-entropy trained approach,0.5641940236091614
translation,152,177,results,2 point improvement,compared to,baseline pointer - generator maximum -likelihood approach,2 point improvement compared to baseline pointer - generator maximum -likelihood approach,0.6355060935020447
translation,152,177,results,results,see that,2 point improvement,results see that 2 point improvement,0.6831379532814026
translation,152,182,results,main benefit,of,bottom - up summarization,main benefit of bottom - up summarization,0.5483872294425964
translation,152,182,results,bottom - up summarization,seems to be from,reduction of mistakenly copied words,bottom - up summarization seems to be from reduction of mistakenly copied words,0.6032674312591553
translation,152,182,results,results,has,main benefit,results has main benefit,0.4998520016670227
translation,152,183,results,precision,of,copied words,precision of copied words,0.603057324886322
translation,152,183,results,copied words,is,50.0 %,copied words is 50.0 %,0.555444061756134
translation,152,183,results,50.0 %,compared to,reference,50.0 % compared to reference,0.7016239762306213
translation,152,183,results,best pointer - generator models,has,precision,best pointer - generator models has precision,0.504144012928009
translation,152,183,results,results,With,best pointer - generator models,results With best pointer - generator models,0.5980119109153748
translation,152,186,results,average sentence length,of,summaries,average sentence length of summaries,0.5756163597106934
translation,152,186,results,summaries,from,13 to 12 words,summaries from 13 to 12 words,0.6158397793769836
translation,152,186,results,13 to 12 words,when adding,content selection,13 to 12 words when adding content selection,0.7136687636375427
translation,152,186,results,content selection,compared to,pointer - generator,content selection compared to pointer - generator,0.6509481072425842
translation,152,186,results,other inference parameters,has,constant,other inference parameters has constant,0.49050337076187134
translation,152,195,results,model,trained on,smallest subset,model trained on smallest subset,0.7470699548721313
translation,152,195,results,model,leads to,improvement,model leads to improvement,0.6852579116821289
translation,152,195,results,improvement,of,almost 5 points,improvement of almost 5 points,0.5303534269332886
translation,152,195,results,almost 5 points,over,model,almost 5 points over model,0.7698603272438049
translation,152,195,results,model,without,bottom - up attention,model without bottom - up attention,0.6853699684143066
translation,152,195,results,results,demonstrates,model,results demonstrates model,0.6817872524261475
translation,152,195,results,results,even,model,results even model,0.6514250636100769
translation,152,223,results,part- of-speech -tags,of,novel generated words,part- of-speech -tags of novel generated words,0.5685392618179321
translation,152,223,results,part- of-speech -tags,observe,interesting,part- of-speech -tags observe interesting,0.6228346228599548
translation,152,223,results,results,shows,part- of-speech -tags,results shows part- of-speech -tags,0.640284538269043
translation,152,234,results,all three penalties,has,improve,all three penalties has improve,0.5755765438079834
translation,152,234,results,improve,has,all three scores,improve has all three scores,0.5769301652908325
translation,152,234,results,results,observe,all three penalties,results observe all three penalties,0.5497816205024719
translation,153,70,baselines,dsr,without,teacher forcing,dsr without teacher forcing,0.771062433719635
translation,153,70,baselines,pure f bert objective function,without,teacher forcing,pure f bert objective function without teacher forcing,0.7561655044555664
translation,153,70,baselines,dsr,has,pure f bert objective function,dsr has pure f bert objective function,0.585860550403595
translation,153,70,baselines,baselines,has,dsr,baselines has dsr,0.5560033321380615
translation,153,56,hyperparameters,pointer mechanism,for handling,out of vocabulary words,pointer mechanism for handling out of vocabulary words,0.720199704170227
translation,153,56,hyperparameters,hyperparameters,incorporates,pointer mechanism,hyperparameters incorporates pointer mechanism,0.6946564316749573
translation,153,17,model,distributional semantic reward,to boost,rl - based abstractive summarization system,distributional semantic reward to boost rl - based abstractive summarization system,0.6244925856590271
translation,153,17,model,model,propose to use,distributional semantic reward,model propose to use distributional semantic reward,0.726310670375824
translation,153,18,model,model,design,several novel objective functions,model design several novel objective functions,0.6023374795913696
translation,153,55,model,sequence - to-sequence model,with,attention,sequence - to-sequence model with attention,0.6383501887321472
translation,153,55,model,sequence - to-sequence model,select,best parameters,sequence - to-sequence model select best parameters,0.647795557975769
translation,153,55,model,attention,using,xent,attention using xent,0.7561768889427185
translation,153,55,model,best parameters,to initialize,models,best parameters to initialize models,0.7250836491584778
translation,153,55,model,models,for,rl,models for rl,0.6424643397331238
translation,153,55,model,model,pretrain,sequence - to-sequence model,model pretrain sequence - to-sequence model,0.7278622984886169
translation,154,6,model,obligatory case constraints,as,must-link dependency constraints,obligatory case constraints as must-link dependency constraints,0.44148075580596924
translation,154,6,model,readability,of,generated summary,readability of generated summary,0.6114720702171326
translation,154,6,model,model,encode,obligatory case constraints,model encode obligatory case constraints,0.6854600310325623
translation,154,7,model,model,to handle,subtree extraction problem,model to handle subtree extraction problem,0.7384192943572998
translation,154,29,model,new greedy algorithm,solves,new class of maximization problem,new greedy algorithm solves new class of maximization problem,0.6618152856826782
translation,154,29,model,new class of maximization problem,with,performance guarantee 1 2 ( 1 ? e ?1 ),new class of maximization problem with performance guarantee 1 2 ( 1 ? e ?1 ),0.6207672953605652
translation,154,29,model,model,propose,new greedy algorithm,model propose new greedy algorithm,0.7171448469161987
translation,154,233,results,f1measure and f3 - measure,of,our method,f1measure and f3 - measure of our method,0.5805556178092957
translation,154,233,results,our method,are,0.159 and 0.190,our method are 0.159 and 0.190,0.5566772222518921
translation,154,233,results,state - of - the - art baseline,are,0.135 and 0.174,state - of - the - art baseline are 0.135 and 0.174,0.5528526306152344
translation,154,233,results,results,has,f1measure and f3 - measure,results has f1measure and f3 - measure,0.54610675573349
translation,154,235,results,method,with,one without compression,method with one without compression,0.6768385767936707
translation,154,235,results,method,see that,improvements,method see that improvements,0.6835095882415771
translation,154,235,results,improvements,in,f1 and f3 scores,improvements in f1 and f3 scores,0.5676870346069336
translation,154,235,results,improvements,in,pourpre score,improvements in pourpre score,0.5559948086738586
translation,154,235,results,f1 and f3 scores,of,human evaluation,f1 and f3 scores of human evaluation,0.582485556602478
translation,154,235,results,pourpre score,of,version of our method without compression,pourpre score of version of our method without compression,0.5883892774581909
translation,154,235,results,pourpre score,higher than,our method with compression,pourpre score higher than our method with compression,0.6981236338615417
translation,154,235,results,results,Comparing,method,results Comparing method,0.5884876847267151
translation,154,236,results,compression,improved,precision,compression improved precision,0.7687950730323792
translation,154,236,results,precision,of,our method,precision of our method,0.5687888860702515
translation,154,236,results,slightly decreased,has,recall,slightly decreased has recall,0.6070524454116821
translation,154,236,results,results,has,compression,results has compression,0.5383331179618835
translation,154,240,results,results,of,sbe and rc,results of sbe and rc,0.5328037738800049
translation,154,240,results,sbe and rc,see that,sentence compression,sbe and rc see that sentence compression,0.6402726173400879
translation,154,240,results,sentence compression,caused,recall,sentence compression caused recall,0.6671331524848938
translation,154,240,results,recall,of,sbe,recall of sbe,0.6421882510185242
translation,154,240,results,7 % lower,than,rc,7 % lower than rc,0.5881026983261108
translation,154,240,results,results,Comparing,results,results Comparing results,0.5670907497406006
translation,154,240,results,results,of,sbe and rc,results of sbe and rc,0.5328037738800049
translation,155,6,model,participant - based event summarization approach,zooms - in,twitter event streams,participant - based event summarization approach zooms - in twitter event streams,0.7089436650276184
translation,155,6,model,twitter event streams,to,participant level,twitter event streams to participant level,0.5345351696014404
translation,155,6,model,important sub-events,associated with,each participant,important sub-events associated with each participant,0.6814680099487305
translation,155,6,model,important sub-events,using,novel mixture model,important sub-events using novel mixture model,0.6103535890579224
translation,155,6,model,novel mixture model,combines,  burstiness   and   cohesiveness   properties,novel mixture model combines   burstiness   and   cohesiveness   properties,0.6961988210678101
translation,155,6,model,novel mixture model,generates,event summaries,novel mixture model generates event summaries,0.6212858557701111
translation,155,6,model,  burstiness   and   cohesiveness   properties,of,event tweets,  burstiness   and   cohesiveness   properties of event tweets,0.6074056029319763
translation,155,6,model,model,propose,participant - based event summarization approach,model propose participant - based event summarization approach,0.6850237250328064
translation,155,29,model,novel participantbased event summarization approach,dynamically identifies,participants,novel participantbased event summarization approach dynamically identifies participants,0.7333849668502808
translation,155,29,model,novel participantbased event summarization approach,detects,important sub-events,novel participantbased event summarization approach detects important sub-events,0.6949085593223572
translation,155,29,model,novel participantbased event summarization approach,generates,event summary,novel participantbased event summarization approach generates event summary,0.6183139085769653
translation,155,29,model,participants,from,data streams,participants from data streams,0.5071470737457275
translation,155,29,model,participants,zooms - in,event stream,participants zooms - in event stream,0.7038520574569702
translation,155,29,model,data streams,to,participant level,data streams to participant level,0.5162623524665833
translation,155,29,model,event stream,to,participant level,event stream to participant level,0.5468931198120117
translation,155,29,model,important sub-events,related to,each participant,important sub-events related to each participant,0.6748461723327637
translation,155,29,model,important sub-events,using,novel time - content mixture model,important sub-events using novel time - content mixture model,0.5773653388023376
translation,155,29,model,event summary,by concatenating,descriptions,event summary by concatenating descriptions,0.6680955290794373
translation,155,29,model,descriptions,of,important sub-events,descriptions of important sub-events,0.5823625326156616
translation,155,29,model,model,propose,novel participantbased event summarization approach,model propose novel participantbased event summarization approach,0.6785416007041931
translation,155,193,model,event summaries,using,twitter data streams,event summaries using twitter data streams,0.6220747828483582
translation,155,193,model,model,generate,event summaries,model generate event summaries,0.707462728023529
translation,155,158,results,clustering performance,especially on,mention - level,clustering performance especially on mention - level,0.6290143132209778
translation,155,158,results,lexical similarity,with,local context,lexical similarity with local context,0.5766879916191101
translation,155,158,results,local context,is,even more helpful,local context is even more helpful,0.5419920086860657
translation,155,158,results,even more helpful,for,some events,even more helpful for some events,0.636769711971283
translation,155,158,results,lexical similarity measure,has,greatly boosted,lexical similarity measure has greatly boosted,0.5963138341903687
translation,155,158,results,greatly boosted,has,clustering performance,greatly boosted has clustering performance,0.5789061188697815
translation,155,158,results,results,adding,lexical similarity measure,results adding lexical similarity measure,0.6367548704147339
translation,155,158,results,results,combining,lexical similarity,results combining lexical similarity,0.640154242515564
translation,155,159,results,two events ( celtics vs 76ers,yield,relatively low precision,two events ( celtics vs 76ers yield relatively low precision,0.7026703953742981
translation,155,159,results,relatively low precision,on,participant - and mention - level,relatively low precision on participant - and mention - level,0.4837915897369385
translation,155,159,results,results,notice,two events ( celtics vs 76ers,results notice two events ( celtics vs 76ers,0.639165461063385
translation,155,182,results,two participantbased methods,yield,similar recall,two participantbased methods yield similar recall,0.7030141949653625
translation,155,182,results,participant + spike,yields,slightly worse precision,participant + spike yields slightly worse precision,0.7162780165672302
translation,155,182,results,results,has,two participantbased methods,results has two participantbased methods,0.3981548845767975
translation,155,183,results,  participant + mm   approach,much better in,precision,  participant + mm   approach much better in precision,0.6788319945335388
translation,155,183,results,results,has,  participant + mm   approach,results has   participant + mm   approach,0.4953196942806244
translation,156,222,ablation-analysis,attention ( w/ o r i ),results in,more generic summaries,attention ( w/ o r i ) results in more generic summaries,0.6066884398460388
translation,156,222,ablation-analysis,dropping,has,attention ( w/ o r i ),dropping has attention ( w/ o r i ),0.593075156211853
translation,156,222,ablation-analysis,ablation analysis,has,dropping,ablation analysis has dropping,0.5323478579521179
translation,156,324,ablation-analysis,amount of generated repetitions,both in,reconstructed reviews and summaries,amount of generated repetitions both in reconstructed reviews and summaries,0.6101119518280029
translation,156,324,ablation-analysis,amount of generated repetitions,when,z-related kl term,amount of generated repetitions when z-related kl term,0.5840995907783508
translation,156,324,ablation-analysis,z-related kl term,is,low,z-related kl term is low,0.547869861125946
translation,156,161,baselines,opinosis,is,graph- based abstractive summarizer,opinosis is graph- based abstractive summarizer,0.600408136844635
translation,156,161,baselines,graph- based abstractive summarizer,designed to generate,short opinions,graph- based abstractive summarizer designed to generate short opinions,0.6882544755935669
translation,156,161,baselines,short opinions,based on,highly redundant texts,short opinions based on highly redundant texts,0.6062396168708801
translation,156,591,baselines,ffnns,with,tanh non-linearity,ffnns with tanh non-linearity,0.6625379920005798
translation,156,591,baselines,tanh non-linearity,in,several model components,tanh non-linearity in several model components,0.5308843851089478
translation,156,158,experimental-setup,word embeddings,shared across,model,word embeddings shared across model,0.6176047325134277
translation,156,158,experimental-setup,word embeddings,as a form of,regularization,word embeddings as a form of regularization,0.6552442908287048
translation,156,158,experimental-setup,model,as a form of,regularization,model as a form of regularization,0.7297232747077942
translation,156,158,experimental-setup,experimental setup,randomly initialized,word embeddings,experimental setup randomly initialized word embeddings,0.7045426368713379
translation,156,159,experimental-setup,optimization,performed using,"adam ( kingma and ba , 2014 )","optimization performed using adam ( kingma and ba , 2014 )",0.6306155920028687
translation,156,159,experimental-setup,experimental setup,has,optimization,experimental setup has optimization,0.5174741744995117
translation,156,160,experimental-setup,posterior collapse,applied,"cyclical annealing ( fu et al. , 2019 )","posterior collapse applied cyclical annealing ( fu et al. , 2019 )",0.7523643374443054
translation,156,160,experimental-setup,experimental setup,to overcome,posterior collapse,experimental setup to overcome posterior collapse,0.6698061227798462
translation,156,160,experimental-setup,experimental setup,applied,"cyclical annealing ( fu et al. , 2019 )","experimental setup applied cyclical annealing ( fu et al. , 2019 )",0.691234827041626
translation,156,269,experimental-setup,sequential encoding and decoding,used,"grus ( cho et al. , 2014 )","sequential encoding and decoding used grus ( cho et al. , 2014 )",0.571972668170929
translation,156,269,experimental-setup,"grus ( cho et al. , 2014 )",with,600 - dimensional hidden states,"grus ( cho et al. , 2014 ) with 600 - dimensional hidden states",0.622385561466217
translation,156,269,experimental-setup,experimental setup,For,sequential encoding and decoding,experimental setup For sequential encoding and decoding,0.5980494618415833
translation,156,270,experimental-setup,word embeddings dimension,set to,200,word embeddings dimension set to 200,0.6653305292129517
translation,156,270,experimental-setup,experimental setup,has,word embeddings dimension,experimental setup has word embeddings dimension,0.4782761335372925
translation,156,271,experimental-setup,vocabulary size,set to,"50,000 most frequent words","vocabulary size set to 50,000 most frequent words",0.6560633778572083
translation,156,271,experimental-setup,"extra 30,000",allowed in,extended vocabulary,"extra 30,000 allowed in extended vocabulary",0.7186627984046936
translation,156,271,experimental-setup,experimental setup,has,vocabulary size,experimental setup has vocabulary size,0.5189570188522339
translation,156,272,experimental-setup,experimental setup,used,"moses ' ( koehn et al. , 2007 ) reversible tokenizer and truecaser","experimental setup used moses ' ( koehn et al. , 2007 ) reversible tokenizer and truecaser",0.5604457855224609
translation,156,273,experimental-setup,"xavier uniform initialization ( glorot and bengio , 2010 )",of,2d weights,"xavier uniform initialization ( glorot and bengio , 2010 ) of 2d weights",0.5311800241470337
translation,156,273,experimental-setup,"xavier uniform initialization ( glorot and bengio , 2010 )",of,1d weights,"xavier uniform initialization ( glorot and bengio , 2010 ) of 1d weights",0.5331406593322754
translation,156,273,experimental-setup,1d weights,initialized with,scaled normal noise ( ? = 0.1 ),1d weights initialized with scaled normal noise ( ? = 0.1 ),0.7983548045158386
translation,156,273,experimental-setup,experimental setup,has,"xavier uniform initialization ( glorot and bengio , 2010 )","experimental setup has xavier uniform initialization ( glorot and bengio , 2010 )",0.5162045359611511
translation,156,274,experimental-setup,adam optimizer,set,learning rate,adam optimizer set learning rate,0.6309201717376709
translation,156,274,experimental-setup,learning rate,to,0.0008 and 0.0001,learning rate to 0.0008 and 0.0001,0.5757357478141785
translation,156,274,experimental-setup,0.0008 and 0.0001,on,yelp and amazon,0.0008 and 0.0001 on yelp and amazon,0.5835597515106201
translation,156,274,experimental-setup,experimental setup,used,adam optimizer,experimental setup used adam optimizer,0.5961911082267761
translation,156,274,experimental-setup,experimental setup,set,learning rate,experimental setup set learning rate,0.6158276796340942
translation,156,275,experimental-setup,summary decoding,used,lengthnormalized beam search,summary decoding used lengthnormalized beam search,0.5661385655403137
translation,156,275,experimental-setup,summary decoding,relied on,latent code means,summary decoding relied on latent code means,0.6942606568336487
translation,156,275,experimental-setup,lengthnormalized beam search,of size,5,lengthnormalized beam search of size 5,0.7015488743782043
translation,156,275,experimental-setup,experimental setup,For,summary decoding,experimental setup For summary decoding,0.601460874080658
translation,156,276,experimental-setup,posterior collapse,applied,"cycling annealing ( fu et al. , 2019 )","posterior collapse applied cycling annealing ( fu et al. , 2019 )",0.7385433912277222
translation,156,276,experimental-setup,"cycling annealing ( fu et al. , 2019 )",with,r = 0.8,"cycling annealing ( fu et al. , 2019 ) with r = 0.8",0.6083335876464844
translation,156,276,experimental-setup,"cycling annealing ( fu et al. , 2019 )",with,new cycle,"cycling annealing ( fu et al. , 2019 ) with new cycle",0.6240090727806091
translation,156,276,experimental-setup,r = 0.8,for,z and c related kl terms,r = 0.8 for z and c related kl terms,0.6344530582427979
translation,156,276,experimental-setup,new cycle,over,every 2 epochs,new cycle over every 2 epochs,0.7262895107269287
translation,156,276,experimental-setup,every 2 epochs,over,training set,every 2 epochs over training set,0.6692942976951599
translation,156,276,experimental-setup,experimental setup,to overcome,posterior collapse,experimental setup to overcome posterior collapse,0.6698061227798462
translation,156,276,experimental-setup,experimental setup,applied,"cycling annealing ( fu et al. , 2019 )","experimental setup applied cycling annealing ( fu et al. , 2019 )",0.6862982511520386
translation,156,199,experiments,lexrank,according to,opinion consensus criterion,lexrank according to opinion consensus criterion,0.6769294142723083
translation,156,245,model,abstractive summarizer,of,opinions,abstractive summarizer of opinions,0.5947866439819336
translation,156,245,model,abstractive summarizer,trained,end -to - end,abstractive summarizer trained end -to - end,0.7073490619659424
translation,156,245,model,end -to - end,on,large collection of reviews,end -to - end on large collection of reviews,0.5433322191238403
translation,156,245,model,model,presented,abstractive summarizer,model presented abstractive summarizer,0.6709927320480347
translation,156,280,model,decoder 's attention mechanism,used,single layer neural network,decoder 's attention mechanism used single layer neural network,0.5620919466018677
translation,156,280,model,decoder 's attention mechanism,used,tanh non-linearity,decoder 's attention mechanism used tanh non-linearity,0.5820837616920471
translation,156,280,model,single layer neural network,with,200 - dimensional hidden layer,single layer neural network with 200 - dimensional hidden layer,0.6308562755584717
translation,156,280,model,model,has,decoder 's attention mechanism,model has decoder 's attention mechanism,0.5384751558303833
translation,156,176,results,our model,yields,highest scores,our model yields highest scores,0.716133177280426
translation,156,176,results,copycat,yields,highest scores,copycat yields highest scores,0.7205201387405396
translation,156,176,results,highest scores,on,yelp and amazon datasets,highest scores on yelp and amazon datasets,0.47486045956611633
translation,156,176,results,our model,has,copycat,our model has copycat,0.6386781334877014
translation,156,177,results,large gains,over,vanila vae,large gains over vanila vae,0.7030346989631653
translation,156,177,results,results,observe,large gains,results observe large gains,0.6368597149848938
translation,156,182,results,our model,has,substantially outperforms,our model has substantially outperforms,0.6102578043937683
translation,156,182,results,substantially outperforms,has,mean-sum,substantially outperforms has mean-sum,0.5916589498519897
translation,156,182,results,results,has,our model,results has our model,0.5871725678443909
translation,156,194,results,our model,scores higher than,other models,our model scores higher than other models,0.6628995537757874
translation,156,194,results,other models,according to,most criteria,other models according to most criteria,0.6328433752059937
translation,156,194,results,other models,including,overall quality,other models including overall quality,0.6795099377632141
translation,156,194,results,yelp,has,our model,yelp has our model,0.6165018081665039
translation,156,194,results,results,On,yelp,results On yelp,0.5617244839668274
translation,156,196,results,fluency,between,our system and gold summaries,fluency between our system and gold summaries,0.6669463515281677
translation,156,196,results,fluency,is,not statistically significant,fluency is not statistically significant,0.5838871598243713
translation,156,196,results,our system and gold summaries,is,not statistically significant,our system and gold summaries is not statistically significant,0.5820567011833191
translation,156,196,results,results,difference in,fluency,results difference in fluency,0.5902794003486633
translation,156,198,results,other methods,in terms of,fluency,other methods in terms of fluency,0.6912456154823303
translation,156,198,results,other methods,in terms of,coherence,other methods in terms of coherence,0.741335391998291
translation,156,198,results,other methods,in terms of,non-redundancy,other methods in terms of non-redundancy,0.6498830318450928
translation,156,198,results,our system,has,outperforms,our system has outperforms,0.6423544883728027
translation,156,198,results,outperforms,has,other methods,outperforms has other methods,0.5689877271652222
translation,156,198,results,results,has,our system,results has our system,0.5954442024230957
translation,156,200,results,results,has,lexrank,results has lexrank,0.5527518391609192
translation,157,155,ablation-analysis,transformer -decoder,demonstrate,absolute superiority,transformer -decoder demonstrate absolute superiority,0.6204562187194824
translation,157,155,ablation-analysis,t-dmca and transformer - xl,demonstrate,absolute superiority,t-dmca and transformer - xl demonstrate absolute superiority,0.6371971964836121
translation,157,155,ablation-analysis,absolute superiority,in reducing,parameter size,absolute superiority in reducing parameter size,0.6301434636116028
translation,157,155,ablation-analysis,transformer -decoder,has,t-dmca and transformer - xl,transformer -decoder has t-dmca and transformer - xl,0.5915572643280029
translation,157,155,ablation-analysis,ablation analysis,has,transformer -decoder,ablation analysis has transformer -decoder,0.5940840840339661
translation,157,156,ablation-analysis,speed,of,forward - propagating,speed of forward - propagating,0.5587254166603088
translation,157,156,ablation-analysis,transformer - xl,dominates,recurrent mechanism,transformer - xl dominates recurrent mechanism,0.7108597159385681
translation,157,156,ablation-analysis,vht,performs,worst,vht performs worst,0.6865406036376953
translation,157,156,ablation-analysis,speed,has,transformer - xl,speed has transformer - xl,0.6005620360374451
translation,157,156,ablation-analysis,forward - propagating,has,transformer - xl,forward - propagating has transformer - xl,0.6106243133544922
translation,157,156,ablation-analysis,ablation analysis,For,speed,ablation analysis For speed,0.6278747916221619
translation,157,28,baselines,several strong baselines,covering,abstractive models,several strong baselines covering abstractive models,0.7394447922706604
translation,157,28,baselines,abstractive models,of,flat strucuture,abstractive models of flat strucuture,0.5547679662704468
translation,157,28,baselines,abstractive models,of,hierarchical structure,abstractive models of hierarchical structure,0.5886317491531372
translation,157,28,baselines,flat strucuture,has,t- dmca,flat strucuture has t- dmca,0.6052599549293518
translation,157,101,baselines,t-dmca,is,transformerdecoder model,t-dmca is transformerdecoder model,0.5381459593772888
translation,157,101,baselines,t-dmca,uses,memory compressed attention,t-dmca uses memory compressed attention,0.5594832897186279
translation,157,101,baselines,transformerdecoder model,splits,concatenated sequence,transformerdecoder model splits concatenated sequence,0.6728813648223877
translation,157,101,baselines,concatenated sequence,into,segments,concatenated sequence into segments,0.6032139658927917
translation,157,101,baselines,memory compressed attention,to exchange,information,memory compressed attention to exchange information,0.6379067897796631
translation,157,101,baselines,baselines,has,t-dmca,baselines has t-dmca,0.5723312497138977
translation,157,87,experimental-setup,of encoder-decoder,in both of our hierarchical Transformers,256 hidden units,of encoder-decoder in both of our hierarchical Transformers 256 hidden units,0.7130511999130249
translation,157,87,experimental-setup,of encoder-decoder,in both of our hierarchical Transformers,1024 units,of encoder-decoder in both of our hierarchical Transformers 1024 units,0.705035388469696
translation,157,87,experimental-setup,1024 units,in,feed -forward network,1024 units in feed -forward network,0.5459943413734436
translation,157,87,experimental-setup,1024 units,in,4,1024 units in 4,0.6215382218360901
translation,157,87,experimental-setup,3 - layers,has,of encoder-decoder,3 - layers has of encoder-decoder,0.5618559718132019
translation,157,87,experimental-setup,4,has,headers,4 has headers,0.5935463309288025
translation,157,87,experimental-setup,experimental setup,stack,3 - layers,experimental setup stack 3 - layers,0.7764575481414795
translation,157,89,experimental-setup,parameters,are,randomly initialized,parameters are randomly initialized,0.5731635093688965
translation,157,89,experimental-setup,randomly initialized,including,token embeddings,randomly initialized including token embeddings,0.6252813935279846
translation,157,89,experimental-setup,experimental setup,has,parameters,experimental setup has parameters,0.4818422794342041
translation,157,90,experimental-setup,multi-layer models,trained for,"approximately 600,000 steps","multi-layer models trained for approximately 600,000 steps",0.7448919415473938
translation,157,90,experimental-setup,multi-layer models,while,single - layer models,multi-layer models while single - layer models,0.5984039306640625
translation,157,90,experimental-setup,single - layer models,for,"approximately 300,000 steps","single - layer models for approximately 300,000 steps",0.5990504622459412
translation,157,90,experimental-setup,experimental setup,has,multi-layer models,experimental setup has multi-layer models,0.5372653603553772
translation,157,92,experimental-setup,beam size,set as,5,beam size set as 5,0.7022161483764648
translation,157,92,experimental-setup,inference,has,beam size,inference has beam size,0.5730389356613159
translation,157,92,experimental-setup,inference,has,average length normalization,inference has average length normalization,0.5584864616394043
translation,157,92,experimental-setup,experimental setup,During,inference,experimental setup During inference,0.6632732152938843
translation,157,149,experimental-setup,1600,has,input tokens,1600 has input tokens,0.5909476280212402
translation,157,149,experimental-setup,experimental setup,uniformly hire,3 - layers architecture,experimental setup uniformly hire 3 - layers architecture,0.6996332406997681
translation,157,150,experimental-setup,batch size,until out of memory,2080 ti gpu,batch size until out of memory 2080 ti gpu,0.7906664609909058
translation,157,150,experimental-setup,model,with,maximum batch size,model with maximum batch size,0.6354097127914429
translation,157,150,experimental-setup,maximum batch size,occupies,lowest memory space,maximum batch size occupies lowest memory space,0.6390054821968079
translation,157,4,model,word - and paragraph - level multihead attentions,in,decoder,word - and paragraph - level multihead attentions in decoder,0.5274631977081299
translation,157,4,model,word - and paragraph - level multihead attentions,proposed,parallel and vertical hierarchical transformers ( pht & vht ),word - and paragraph - level multihead attentions proposed parallel and vertical hierarchical transformers ( pht & vht ),0.6742942333221436
translation,157,4,model,decoder,based on,parallel and vertical architectures,decoder based on parallel and vertical architectures,0.6804777383804321
translation,157,4,model,parallel and vertical hierarchical transformers ( pht & vht ),generate,summaries,parallel and vertical hierarchical transformers ( pht & vht ) generate summaries,0.6402000188827515
translation,157,4,model,summaries,utilizing,context - aware word embeddings,summaries utilizing context - aware word embeddings,0.6298627853393555
translation,157,4,model,context - aware word embeddings,together with,static and dynamics paragraph embeddings,context - aware word embeddings together with static and dynamics paragraph embeddings,0.5821648836135864
translation,157,4,model,model,incorporating,word - and paragraph - level multihead attentions,model incorporating word - and paragraph - level multihead attentions,0.6692885160446167
translation,157,4,model,model,proposed,parallel and vertical hierarchical transformers ( pht & vht ),model proposed parallel and vertical hierarchical transformers ( pht & vht ),0.6995152235031128
translation,157,20,model,two novel hierarchical transformers,to address,text-length and cross-document linkage problems,two novel hierarchical transformers to address text-length and cross-document linkage problems,0.5970784425735474
translation,157,20,model,two novel hierarchical transformers,both,text-length and cross-document linkage problems,two novel hierarchical transformers both text-length and cross-document linkage problems,0.6188681721687317
translation,157,20,model,text-length and cross-document linkage problems,in,mds,text-length and cross-document linkage problems in mds,0.4856998324394226
translation,157,20,model,model,develop,two novel hierarchical transformers,model develop two novel hierarchical transformers,0.6306211948394775
translation,157,21,model,our models,designed to learn,cross-token and cross - document relationships,our models designed to learn cross-token and cross - document relationships,0.6653454899787903
translation,157,21,model,word-level and paragraph - level multi-head attention mechanisms,has,our models,word-level and paragraph - level multi-head attention mechanisms has our models,0.5296578407287598
translation,157,21,model,model,introducing,word-level and paragraph - level multi-head attention mechanisms,model introducing word-level and paragraph - level multi-head attention mechanisms,0.6161947846412659
translation,157,22,model,jointly used,to generate,target sequences,jointly used to generate target sequences,0.6637970209121704
translation,157,22,model,target sequences,to abandon,flat structure,target sequences to abandon flat structure,0.6744977235794067
translation,157,22,model,flat structure,to mitigate,long-dependency problem,flat structure to mitigate long-dependency problem,0.6508464813232422
translation,157,22,model,model,has,word - and paragraphlevel context vectors,model has word - and paragraphlevel context vectors,0.48980361223220825
translation,157,23,model,architectures,based on,transformer encoder- decoder model,architectures based on transformer encoder- decoder model,0.69927579164505
translation,157,23,model,transformer encoder- decoder model,with,context - aware word embeddings,transformer encoder- decoder model with context - aware word embeddings,0.6050640940666199
translation,157,23,model,context - aware word embeddings,obtained from,shared encoder,context - aware word embeddings obtained from shared encoder,0.5493246912956238
translation,157,23,model,context - aware word embeddings,obtained from,cross-token linkages,context - aware word embeddings obtained from cross-token linkages,0.592968225479126
translation,157,23,model,cross-token linkages,described by,word-level multi-head attention mechanism,cross-token linkages described by word-level multi-head attention mechanism,0.6744727492332458
translation,157,23,model,word-level multi-head attention mechanism,in,decoder,word-level multi-head attention mechanism in decoder,0.4972781240940094
translation,157,27,model,vertical hierarchical transformer ( vht ),stacks,paragraph - level attention layer,vertical hierarchical transformer ( vht ) stacks paragraph - level attention layer,0.7107527852058411
translation,157,27,model,paragraph - level attention layer,on top of,word- level attention layer,paragraph - level attention layer on top of word- level attention layer,0.6364825367927551
translation,157,27,model,paragraph - level attention layer,to learn,latent relationship,paragraph - level attention layer to learn latent relationship,0.5920330882072449
translation,157,27,model,latent relationship,between,paragraphs,latent relationship between paragraphs,0.6468384861946106
translation,157,27,model,latent relationship,with,dynamic 3 paragraph embeddings,latent relationship with dynamic 3 paragraph embeddings,0.5950703620910645
translation,157,27,model,dynamic 3 paragraph embeddings,from,previous layer,dynamic 3 paragraph embeddings from previous layer,0.5019184350967407
translation,157,27,model,model,has,vertical hierarchical transformer ( vht ),model has vertical hierarchical transformer ( vht ),0.5848775506019592
translation,157,66,model,model,has,paragraph - level multi-head attention,model has paragraph - level multi-head attention,0.5127185583114624
translation,157,97,model,extractive model lead,is,extractive,extractive model lead is extractive,0.6022005677223206
translation,157,97,model,extractive,extracts,top k tokens,extractive extracts top k tokens,0.6537162065505981
translation,157,97,model,top k tokens,from,concatenated sequence,top k tokens from concatenated sequence,0.5367802381515503
translation,157,97,model,model,has,extractive model lead,model has extractive model lead,0.5712307691574097
translation,157,99,model,abstractive model with flat structure,is,vanilla transformer encoder-decoder model,abstractive model with flat structure is vanilla transformer encoder-decoder model,0.552042543888092
translation,157,99,model,abstractive model with flat structure,has,transformer ( ft ),abstractive model with flat structure has transformer ( ft ),0.6085498332977295
translation,157,99,model,model,has,abstractive model with flat structure,model has abstractive model with flat structure,0.5956892371177673
translation,157,120,results,summaries,of,greatest similarity,summaries of greatest similarity,0.5751726031303406
translation,157,120,results,91.42 %,with,gold summaries,91.42 % with gold summaries,0.6218845844268799
translation,157,120,results,greatest similarity,has,91.42 %,greatest similarity has 91.42 %,0.5255631804466248
translation,157,125,results,extractive model lead,exhibits,overall inferior performance,extractive model lead exhibits overall inferior performance,0.7086824774742126
translation,157,125,results,overall inferior performance,in comparison to,abstractive models,overall inferior performance in comparison to abstractive models,0.6517484784126282
translation,157,125,results,0.11 - higher rouge -l,than,flat transformer,0.11 - higher rouge -l than flat transformer,0.5766735672950745
translation,157,126,results,ft,with,hierarchical structure,ft with hierarchical structure,0.7012555599212646
translation,157,126,results,two extended flat models,i.e.,transformer - xl,two extended flat models i.e. transformer - xl,0.7118503451347351
translation,157,126,results,fails to outperform,has,two extended flat models,fails to outperform has two extended flat models,0.6118486523628235
translation,157,127,results,two flat models,report,comparable results,two flat models report comparable results,0.6188865900039673
translation,157,127,results,comparable results,in terms of,informativeness ( rouge - 1 & - 2 ),comparable results in terms of informativeness ( rouge - 1 & - 2 ),0.6673454642295837
translation,157,127,results,outperforms,by,0.41,outperforms by 0.41,0.610474705696106
translation,157,127,results,former,by,0.41,former by 0.41,0.5916606783866882
translation,157,127,results,0.41,in terms of,fluency ( rouge - l ),0.41 in terms of fluency ( rouge - l ),0.6903330087661743
translation,157,127,results,t-dmca and transformer - xl,has,two flat models,t-dmca and transformer - xl has two flat models,0.5565628409385681
translation,157,127,results,latter,has,outperforms,latter has outperforms,0.6338182687759399
translation,157,127,results,outperforms,has,former,outperforms has former,0.6476501822471619
translation,157,127,results,results,has,t-dmca and transformer - xl,results has t-dmca and transformer - xl,0.5165865421295166
translation,157,128,results,proposed hierarchical transformers,show,promising rouge results,proposed hierarchical transformers show promising rouge results,0.6262467503547668
translation,157,128,results,results,has,proposed hierarchical transformers,results has proposed hierarchical transformers,0.6178439259529114
translation,157,129,results,pure hierarchical structure,that enlarges,input length of tokens,pure hierarchical structure that enlarges input length of tokens,0.649294912815094
translation,157,129,results,liu 's ht,all domains of,rouge test,liu 's ht all domains of rouge test,0.7041921615600586
translation,157,129,results,pure hierarchical structure,has,pht & vht,pure hierarchical structure has pht & vht,0.6331942677497864
translation,157,129,results,pure hierarchical structure,has,outperform,pure hierarchical structure has outperform,0.6328451037406921
translation,157,129,results,pht & vht,has,outperform,pht & vht has outperform,0.6766234040260315
translation,157,129,results,outperform,has,liu 's ht,outperform has liu 's ht,0.6443643569946289
translation,157,129,results,results,Profited from,pure hierarchical structure,results Profited from pure hierarchical structure,0.508169949054718
translation,157,131,results,ultimate 3 - layer pht & vht,surpass,t-dmca,ultimate 3 - layer pht & vht surpass t-dmca,0.6998794078826904
translation,157,131,results,ultimate 3 - layer pht & vht,surpass,transformer - xl,ultimate 3 - layer pht & vht surpass transformer - xl,0.7103755474090576
translation,157,131,results,two flat models,handle,long input sequences,two flat models handle long input sequences,0.5940597057342529
translation,157,131,results,long input sequences,has,"of 3,000 tokens","long input sequences has of 3,000 tokens",0.5798890590667725
translation,157,131,results,results,has,ultimate 3 - layer pht & vht,results has ultimate 3 - layer pht & vht,0.5710653066635132
translation,157,132,results,pht,appears to be,more informative,pht appears to be more informative,0.6685868501663208
translation,157,132,results,pht,produces,highest rouge - 1 & - 2,pht produces highest rouge - 1 & - 2,0.6816669702529907
translation,157,132,results,more informative,in,summaries,more informative in summaries,0.5054065585136414
translation,157,132,results,highest rouge - 1 & - 2,among,all models,highest rouge - 1 & - 2 among all models,0.6280258893966675
translation,157,132,results,vht,is,more fluent,vht is more fluent,0.6114479303359985
translation,157,132,results,more fluent,with,highest rouge -l.,more fluent with highest rouge -l.,0.6993594765663147
translation,157,132,results,parallel and vertical architectures,has,pht,parallel and vertical architectures has pht,0.6271514296531677
translation,157,132,results,results,Between,parallel and vertical architectures,results Between parallel and vertical architectures,0.5703428983688354
translation,157,157,results,pht,proven to outperform,vht,pht proven to outperform vht,0.7158037424087524
translation,157,157,results,vht,in both,memory usage,vht in both memory usage,0.5851474404335022
translation,157,157,results,vht,in both,inference speed,vht in both inference speed,0.6352245807647705
translation,157,157,results,vht,due to,parallel,vht due to parallel,0.7330400347709656
translation,158,4,model,extractive summarization,called,swap - net,extractive summarization called swap - net,0.6826168894767761
translation,158,6,model,swap - net,models,interaction of key words and salient sentences,swap - net models interaction of key words and salient sentences,0.6753954887390137
translation,158,6,model,interaction of key words and salient sentences,using,new twolevel pointer network based architecture,interaction of key words and salient sentences using new twolevel pointer network based architecture,0.6789417862892151
translation,158,6,model,model,design,swap - net,model design swap - net,0.5964769721031189
translation,158,7,model,swap - net,identifies,salient sentences and key words,swap - net identifies salient sentences and key words,0.665972113609314
translation,158,7,model,swap - net,combines them,extractive summary,swap - net combines them extractive summary,0.7112366557121277
translation,158,7,model,salient sentences and key words,in,input document,salient sentences and key words in input document,0.46861645579338074
translation,158,7,model,model,has,swap - net,model has swap - net,0.6366660594940186
translation,158,30,model,new deep learning model,for,extractive summarization,new deep learning model for extractive summarization,0.5633708834648132
translation,158,30,model,model,design SWAP - NET,new deep learning model,model design SWAP - NET new deep learning model,0.6826372742652893
translation,158,31,model,encoderdecoder architecture,with,attention mechanism,encoderdecoder architecture with attention mechanism,0.6005984544754028
translation,158,31,model,attention mechanism,to select,important sentences,attention mechanism to select important sentences,0.6453145146369934
translation,158,31,model,model,use,encoderdecoder architecture,model use encoderdecoder architecture,0.6101259589195251
translation,158,32,model,architecture,utilizes,key words,architecture utilizes key words,0.5405329465866089
translation,158,32,model,key words,in,selection process,key words in selection process,0.4564230442047119
translation,158,32,model,model,design,architecture,model design architecture,0.6584272980690002
translation,158,34,model,interaction,through,two -level encoder and decoder,interaction through two -level encoder and decoder,0.7183685302734375
translation,158,34,model,two -level encoder and decoder,one for,words,two -level encoder and decoder one for words,0.7149227261543274
translation,158,34,model,two -level encoder and decoder,other for,sentences,two -level encoder and decoder other for sentences,0.6118452548980713
translation,158,34,model,model,model,interaction,model model interaction,0.8852783441543579
translation,158,35,model,attention - based mechanism,to learn,important words and sentences,attention - based mechanism to learn important words and sentences,0.639261782169342
translation,158,35,model,important words and sentences,from,labeled data,important words and sentences from labeled data,0.5095659494400024
translation,158,35,model,model,has,attention - based mechanism,model has attention - based mechanism,0.53761887550354
translation,159,185,ablation-analysis,seneca with all rewards r coh +r ref +r app,reports,highest coherence score,seneca with all rewards r coh +r ref +r app reports highest coherence score,0.5987172722816467
translation,159,185,ablation-analysis,seneca with all rewards r coh +r ref +r app,reports,drop,seneca with all rewards r coh +r ref +r app reports drop,0.6162490844726562
translation,159,185,ablation-analysis,highest coherence score,of,0.76,highest coherence score of 0.76,0.513932466506958
translation,159,185,ablation-analysis,drop,in,rouge -l ( 44.01 ),drop in rouge -l ( 44.01 ),0.5564593076705933
translation,159,185,ablation-analysis,ablation analysis,has,seneca with all rewards r coh +r ref +r app,ablation analysis has seneca with all rewards r coh +r ref +r app,0.5724408626556396
translation,159,174,baselines,echain,labels,pair of sentences,echain labels pair of sentences,0.7117098569869995
translation,159,174,baselines,echain,labels,pair of sentences,echain labels pair of sentences,0.7117098569869995
translation,159,174,baselines,pair of sentences,as,more coherent,pair of sentences as more coherent,0.5162894129753113
translation,159,174,baselines,pair of sentences,with,higher cosine similarity,pair of sentences with higher cosine similarity,0.6413427591323853
translation,159,174,baselines,more coherent,if,one or more entity mentions coreferred,more coherent if one or more entity mentions coreferred,0.5899813175201416
translation,159,174,baselines,cossim,labels,pair of sentences,cossim labels pair of sentences,0.7321336269378662
translation,159,174,baselines,pair of sentences,with,higher cosine similarity,pair of sentences with higher cosine similarity,0.6413427591323853
translation,159,174,baselines,higher cosine similarity,as,more coherent,higher cosine similarity as more coherent,0.5131837725639343
translation,159,60,experimental-setup,off - the-shelf coreference resolution system,from,stanford corenlp,off - the-shelf coreference resolution system from stanford corenlp,0.5663263201713562
translation,159,60,experimental-setup,off - the-shelf coreference resolution system,),input articles,off - the-shelf coreference resolution system ) input articles,0.582964301109314
translation,159,60,experimental-setup,off - the-shelf coreference resolution system,on,input articles,off - the-shelf coreference resolution system on input articles,0.5153796076774597
translation,159,60,experimental-setup,input articles,to extract,entities,input articles to extract entities,0.6786512732505798
translation,159,60,experimental-setup,entities,represented as,cluster of mentions,entities represented as cluster of mentions,0.5853274464607239
translation,159,60,experimental-setup,experimental setup,run,off - the-shelf coreference resolution system,experimental setup run off - the-shelf coreference resolution system,0.7007226347923279
translation,159,149,experimental-setup,vocabulary,of,50 k most common words,vocabulary of 50 k most common words,0.587597131729126
translation,159,149,experimental-setup,vocabulary,with,128 - dimensional word embeddings,vocabulary with 128 - dimensional word embeddings,0.5995007157325745
translation,159,149,experimental-setup,50 k most common words,in,training set,50 k most common words in training set,0.4770541787147522
translation,159,149,experimental-setup,experimental setup,used,vocabulary,experimental setup used vocabulary,0.6548969745635986
translation,159,150,experimental-setup,content selection component,for,both entity and sentence encoders,content selection component for both entity and sentence encoders,0.5686188340187073
translation,159,150,experimental-setup,content selection component,implemented,one - layer convolutional network,content selection component implemented one - layer convolutional network,0.6310220956802368
translation,159,150,experimental-setup,both entity and sentence encoders,implemented,one - layer convolutional network,both entity and sentence encoders implemented one - layer convolutional network,0.6444405913352966
translation,159,150,experimental-setup,one - layer convolutional network,with,100 dimensions,one - layer convolutional network with 100 dimensions,0.6297479867935181
translation,159,150,experimental-setup,one - layer convolutional network,used,shared embedding matrix,one - layer convolutional network used shared embedding matrix,0.5701947808265686
translation,159,150,experimental-setup,shared embedding matrix,between,two,shared embedding matrix between two,0.7234995365142822
translation,159,150,experimental-setup,experimental setup,In,content selection component,experimental setup In content selection component,0.4928220510482788
translation,159,151,experimental-setup,lstm models,with,256 - dimensional hidden states,lstm models with 256 - dimensional hidden states,0.6170610785484314
translation,159,151,experimental-setup,256 - dimensional hidden states,for,input article encoder ( per direction ),256 - dimensional hidden states for input article encoder ( per direction ),0.5795764327049255
translation,159,151,experimental-setup,256 - dimensional hidden states,for,content selection decoder,256 - dimensional hidden states for content selection decoder,0.5682679414749146
translation,159,151,experimental-setup,experimental setup,employed,lstm models,experimental setup employed lstm models,0.6222761273384094
translation,159,153,experimental-setup,ml training,of,both components,ml training of both components,0.5827152729034424
translation,159,153,experimental-setup,"adam ( kingma and ba , 2015 )",applied with,learning rate,"adam ( kingma and ba , 2015 ) applied with learning rate",0.6597578525543213
translation,159,153,experimental-setup,"adam ( kingma and ba , 2015 )",applied with,gradient clipping 2.0,"adam ( kingma and ba , 2015 ) applied with gradient clipping 2.0",0.6206486225128174
translation,159,153,experimental-setup,"adam ( kingma and ba , 2015 )",applied with,batch size,"adam ( kingma and ba , 2015 ) applied with batch size",0.6786870956420898
translation,159,153,experimental-setup,ml training,has,"adam ( kingma and ba , 2015 )","ml training has adam ( kingma and ba , 2015 )",0.5368737578392029
translation,159,153,experimental-setup,both components,has,"adam ( kingma and ba , 2015 )","both components has adam ( kingma and ba , 2015 )",0.5903570055961609
translation,159,153,experimental-setup,learning rate,has,0.001,learning rate has 0.001,0.5318801999092102
translation,159,153,experimental-setup,batch size,has,32,batch size has 32,0.630264163017273
translation,159,153,experimental-setup,experimental setup,During,ml training,experimental setup During ml training,0.6704784035682678
translation,159,154,experimental-setup,rl stage,reduced,learning rate,rl stage reduced learning rate,0.7063397765159607
translation,159,154,experimental-setup,rl stage,reduced,set batch size,rl stage reduced set batch size,0.6725660562515259
translation,159,154,experimental-setup,learning rate,to,0.0001,learning rate to 0.0001,0.5543551445007324
translation,159,154,experimental-setup,set batch size,to,50,set batch size to 50,0.611602783203125
translation,159,154,experimental-setup,experimental setup,During,rl stage,experimental setup During rl stage,0.6908159852027893
translation,159,155,experimental-setup,abstract generator,sampled,5 sequences,abstract generator sampled 5 sequences,0.752511203289032
translation,159,155,experimental-setup,5 sequences,took,average,5 sequences took average,0.7278531193733215
translation,159,155,experimental-setup,average,over,samples,average over samples,0.7347831130027771
translation,159,155,experimental-setup,samples,using,batch size 10,samples using batch size 10,0.7049911618232727
translation,159,155,experimental-setup,experimental setup,For,abstract generator,experimental setup For abstract generator,0.5461685061454773
translation,159,157,experimental-setup,inference,adopted,trigram repetition avoidance strategy,inference adopted trigram repetition avoidance strategy,0.6567308902740479
translation,159,157,experimental-setup,trigram repetition avoidance strategy,with,additional length normalization,trigram repetition avoidance strategy with additional length normalization,0.6436087489128113
translation,159,157,experimental-setup,additional length normalization,to encourage,generation,additional length normalization to encourage generation,0.6991930603981018
translation,159,157,experimental-setup,experimental setup,During,inference,experimental setup During inference,0.6632732152938843
translation,159,184,experiments,seneca model,trained with,apposition reward ( r app ),seneca model trained with apposition reward ( r app ),0.7325620055198669
translation,159,184,experiments,seneca model,shows,drop,seneca model shows drop,0.6915256977081299
translation,159,184,experiments,apposition reward ( r app ),reports,highest rouge -l ( 44.60 ),apposition reward ( r app ) reports highest rouge -l ( 44.60 ),0.5836097598075867
translation,159,184,experiments,apposition reward ( r app ),shows,drop,apposition reward ( r app ) shows drop,0.6768443584442139
translation,159,184,experiments,drop,in,coherence score,drop in coherence score,0.5361058115959167
translation,159,184,experiments,coherence score,to,0.69,coherence score to 0.69,0.49281224608421326
translation,159,6,model,entity - driven coherent abstractive summarization,leverages,entity information,entity - driven coherent abstractive summarization leverages entity information,0.6616880297660828
translation,159,6,model,entity information,to generate,informative and coherent abstracts,entity information to generate informative and coherent abstracts,0.6227864623069763
translation,159,6,model,model,introduce,seneca,model introduce seneca,0.6294666528701782
translation,159,7,model,entity - aware content selection module,first identifies,salient sentences,entity - aware content selection module first identifies salient sentences,0.6257686614990234
translation,159,7,model,salient sentences,from,input,salient sentences from input,0.5659542679786682
translation,159,7,model,abstract generation module,conducts,cross-sentence information compression and abstraction,abstract generation module conducts cross-sentence information compression and abstraction,0.6359521150588989
translation,159,7,model,cross-sentence information compression and abstraction,to generate,final summary,cross-sentence information compression and abstraction to generate final summary,0.6667397022247314
translation,159,7,model,final summary,trained with,rewards,final summary trained with rewards,0.7028497457504272
translation,159,7,model,rewards,to promote,"coherence , conciseness , and clarity","rewards to promote coherence , conciseness , and clarity",0.6917049884796143
translation,159,7,model,two-step approach,has,entity - aware content selection module,two-step approach has entity - aware content selection module,0.525079607963562
translation,159,56,model,salient content,by aligning,entity mentions and their contexts,salient content by aligning entity mentions and their contexts,0.6508206129074097
translation,159,56,model,entity mentions and their contexts,with,human summaries,entity mentions and their contexts with human summaries,0.6217461824417114
translation,159,56,model,model,learns to identify,salient content,model learns to identify salient content,0.7780625820159912
translation,159,57,model,two encoders,one learns,entity representations,two encoders one learns entity representations,0.6349963545799255
translation,159,57,model,two encoders,other learns,sentence representations,two encoders other learns sentence representations,0.6457969546318054
translation,159,57,model,entity representations,by encoding,mention clusters,entity representations by encoding mention clusters,0.7137768268585205
translation,159,57,model,model,employ,two encoders,model employ two encoders,0.6040608882904053
translation,159,146,results,our coherence model,for,cnn / dm,our coherence model for cnn / dm,0.5839371681213379
translation,159,146,results,our coherence model,achieves,86 % accuracy,our coherence model achieves 86 % accuracy,0.6851661801338196
translation,159,146,results,our coherence model,achieves,84 %,our coherence model achieves 84 %,0.7048487663269043
translation,159,146,results,our coherence model,for,nyt,our coherence model for nyt,0.6823925375938416
translation,159,146,results,cnn / dm,achieves,86 % accuracy,cnn / dm achieves 86 % accuracy,0.716107964515686
translation,159,146,results,results,has,our coherence model,results has our coherence model,0.5620085000991821
translation,159,175,results,our model,yields,significantly higher accuracy,our model yields significantly higher accuracy,0.7007126212120056
translation,159,175,results,significantly higher accuracy,on,both tasks,significantly higher accuracy on both tasks,0.4864439368247986
translation,159,175,results,significantly higher accuracy,than,comparisons,significantly higher accuracy than comparisons,0.5977294445037842
translation,159,175,results,greater than 80 % ),on,both tasks,greater than 80 % ) on both tasks,0.5068562030792236
translation,159,175,results,both tasks,than,comparisons,both tasks than comparisons,0.5909942388534546
translation,159,175,results,significantly higher accuracy,has,greater than 80 % ),significantly higher accuracy has greater than 80 % ),0.5471864938735962
translation,159,175,results,results,has,our model,results has our model,0.5871725678443909
translation,159,180,results,all comparisons,on,coherence score,all comparisons on coherence score,0.49516552686691284
translation,159,180,results,our seneca models,has,outperform,our seneca models has outperform,0.6028243899345398
translation,159,180,results,outperform,has,all comparisons,outperform has all comparisons,0.5781707763671875
translation,159,180,results,results,has,our seneca models,results has our seneca models,0.5470547080039978
translation,159,182,results,base seneca model,reports,higher rouge and coherence score ( 0.73 ),base seneca model reports higher rouge and coherence score ( 0.73 ),0.5869942903518677
translation,159,182,results,higher rouge and coherence score ( 0.73 ),compared to,version without entity as input,higher rouge and coherence score ( 0.73 ) compared to version without entity as input,0.6409839391708374
translation,159,186,results,results,on,cnn / dm,results on cnn / dm,0.548747181892395
translation,159,187,results,all seneca models,produce,higher rouge - 1 scores,all seneca models produce higher rouge - 1 scores,0.6463151574134827
translation,159,187,results,seneca base model,achieving,highest,seneca base model achieving highest,0.7345640659332275
translation,159,187,results,outperforming,has,previous best performing models,outperforming has previous best performing models,0.5793373584747314
translation,159,188,results,even higher,than,human summaries,even higher than human summaries,0.625365674495697
translation,159,188,results,results,report,highest coherence score ( 0.63 ),results report highest coherence score ( 0.63 ),0.6010582447052002
translation,159,196,results,base seneca model,reports,higher rouge and coherence score,base seneca model reports higher rouge and coherence score,0.5904518365859985
translation,159,196,results,base seneca model,without,entity input,base seneca model without entity input,0.6941680312156677
translation,159,196,results,higher rouge and coherence score,compared to,seneca,higher rouge and coherence score compared to seneca,0.700813889503479
translation,159,196,results,seneca,without,entity input,seneca without entity input,0.6886953711509705
translation,159,196,results,results,has,base seneca model,results has base seneca model,0.5672187209129333
translation,159,197,results,seneca model,with,all rewards,seneca model with all rewards,0.6092043519020081
translation,159,197,results,seneca model,with,coherence reward,seneca model with coherence reward,0.6152881383895874
translation,159,197,results,seneca model,SENECA model with,coherence reward,seneca model SENECA model with coherence reward,0.6675385236740112
translation,159,197,results,all rewards,yields,highest coherence score,all rewards yields highest coherence score,0.7213996648788452
translation,159,197,results,highest coherence score,of,0.63,highest coherence score of 0.63,0.518524706363678
translation,159,197,results,coherence reward,performs,considerably better,coherence reward performs considerably better,0.6026762127876282
translation,159,197,results,considerably better,on,rouge -l,considerably better on rouge -l,0.6023734211921692
translation,159,197,results,considerably better,drop in,coherence score,considerably better drop in coherence score,0.7114498615264893
translation,159,197,results,results,has,seneca model,results has seneca model,0.5504769682884216
translation,159,213,results,our model seneca + r coh,ranks,significantly higher,our model seneca + r coh ranks significantly higher,0.7523823380470276
translation,159,213,results,significantly higher,on,informativeness,significantly higher on informativeness,0.5020663142204285
translation,159,213,results,significantly higher,on,coherence,significantly higher on coherence,0.5291815996170044
translation,159,213,results,informativeness,as well as,coherence,informativeness as well as coherence,0.597694993019104
translation,159,213,results,results,shows,our model seneca + r coh,results shows our model seneca + r coh,0.6686895489692688
translation,159,214,results,seneca + r coh,ranks,higher,seneca + r coh ranks higher,0.7592980861663818
translation,159,214,results,higher,on,informativeness,higher on informativeness,0.5066260099411011
translation,159,214,results,results,has,seneca + r coh,results has seneca + r coh,0.5288202166557312
translation,159,216,results,pointgen + cov,ranks,higher,pointgen + cov ranks higher,0.7886825799942017
translation,159,216,results,higher,on,grammaticality,higher on grammaticality,0.5158799290657043
translation,159,216,results,grammaticality,than,seneca + r coh,grammaticality than seneca + r coh,0.5449100732803345
translation,159,216,results,results,has,pointgen + cov,results has pointgen + cov,0.5397006273269653
translation,160,82,baselines,baselines,include,two commonly used unsupervised extractive summarization models,baselines include two commonly used unsupervised extractive summarization models,0.5328001379966736
translation,160,85,baselines,pointer- generator model,with,maximal marginal relevance ( mmr ),pointer- generator model with maximal marginal relevance ( mmr ),0.6283535361289978
translation,160,85,baselines,weights,over,multi-document inputs,weights over multi-document inputs,0.6866469979286194
translation,160,86,baselines,hiersumm,uses,passage ranker,hiersumm uses passage ranker,0.5544630289077759
translation,160,86,baselines,passage ranker,selects,most important document,passage ranker selects most important document,0.6649627089500427
translation,160,86,baselines,most important document,as,input,most important document as input,0.5166378021240234
translation,160,86,baselines,input,to,hierarchical transformer - based generation model,input to hierarchical transformer - based generation model,0.5576825737953186
translation,160,86,baselines,baselines,has,hiersumm,baselines has hiersumm,0.5865527391433716
translation,160,90,baselines,bertabs,uses,"pretrained bert ( devlin et al. , 2019 )","bertabs uses pretrained bert ( devlin et al. , 2019 )",0.5436726212501526
translation,160,90,baselines,bertabs,trains,randomly initialized transformer decoder,bertabs trains randomly initialized transformer decoder,0.7731934785842896
translation,160,90,baselines,"pretrained bert ( devlin et al. , 2019 )",as,encoder,"pretrained bert ( devlin et al. , 2019 ) as encoder",0.493673712015152
translation,160,90,baselines,randomly initialized transformer decoder,for,abstractive summarization,randomly initialized transformer decoder for abstractive summarization,0.5950961709022522
translation,160,90,baselines,baselines,has,bertabs,baselines has bertabs,0.6398734450340271
translation,160,5,experiments,multi-xscience,has,large-scale multi-document summarization,multi-xscience has large-scale multi-document summarization,0.5108605623245239
translation,160,83,experiments,supervised abstractive models,test,stateof - the- art multi-document summarization models,supervised abstractive models test stateof - the- art multi-document summarization models,0.6519017219543457
translation,160,83,experiments,stateof - the- art multi-document summarization models,has,himap and hiersumm,stateof - the- art multi-document summarization models has himap and hiersumm,0.5470660328865051
translation,160,95,hyperparameters,decoding process,use,beam search,decoding process use beam search,0.6372861266136169
translation,160,95,hyperparameters,decoding process,use,tri-gram blocking,decoding process use tri-gram blocking,0.6438044309616089
translation,160,95,hyperparameters,hyperparameters,During,decoding process,hyperparameters During decoding process,0.6414378881454468
translation,160,96,hyperparameters,minimal generation length,to,110 tokens,minimal generation length to 110 tokens,0.566266655921936
translation,160,96,hyperparameters,110 tokens,given,dataset statistics,110 tokens given dataset statistics,0.6658153533935547
translation,160,96,hyperparameters,hyperparameters,set,minimal generation length,hyperparameters set minimal generation length,0.6503779292106628
translation,160,87,model,single - document summarization models,including,"pointer - generator ( see et al. , 2017 )","single - document summarization models including pointer - generator ( see et al. , 2017 )",0.6453012228012085
translation,160,87,model,single - document summarization models,including,"bart ( lewis et al. , 2019 )","single - document summarization models including bart ( lewis et al. , 2019 )",0.6372124552726746
translation,160,87,model,single - document summarization models,including,"bertabs ( liu and lapata , 2019 b )","single - document summarization models including bertabs ( liu and lapata , 2019 b )",0.6843422055244446
translation,160,87,model,multi-document summarization,concatenating,input references,multi-document summarization concatenating input references,0.6861814260482788
translation,160,88,model,pointer - generator,incorporates,attention,pointer - generator incorporates attention,0.7269308567047119
translation,160,88,model,attention,over,source texts,attention over source texts,0.6769955158233643
translation,160,88,model,attention,as,copy mechanism,attention as copy mechanism,0.5481852889060974
translation,160,88,model,copy mechanism,to aid,generation,copy mechanism to aid generation,0.6036233901977539
translation,160,88,model,model,has,pointer - generator,model has pointer - generator,0.586395800113678
translation,160,89,model,bart,is,sequence - to-sequence model,bart is sequence - to-sequence model,0.6353853344917297
translation,160,89,model,sequence - to-sequence model,with,encoder,sequence - to-sequence model with encoder,0.6595244407653809
translation,160,89,model,encoder,pre-trained with,denosing auto-encoder objective,encoder pre-trained with denosing auto-encoder objective,0.7549670338630676
translation,160,89,model,model,has,bart,model has bart,0.5764365196228027
translation,160,62,results,multi-xscience,one of,highest novel n-grams scores,multi-xscience one of highest novel n-grams scores,0.6269479990005493
translation,160,62,results,highest novel n-grams scores,among,existing largescale datasets,highest novel n-grams scores among existing largescale datasets,0.5756176710128784
translation,160,62,results,results,has,multi-xscience,results has multi-xscience,0.5479597449302673
translation,160,69,results,multi-xscience,imposes,much less position bias,multi-xscience imposes much less position bias,0.6462571024894714
translation,160,69,results,multi-xscience,requires,higher level of abstractiveness,multi-xscience requires higher level of abstractiveness,0.6917889714241028
translation,160,69,results,higher level of abstractiveness,from,models,higher level of abstractiveness from models,0.5924036502838135
translation,160,103,results,abstractive models,observe,almost all,abstractive models observe almost all,0.6506102681159973
translation,160,103,results,outperform,by,wide margins,outperform by wide margins,0.6592182517051697
translation,160,103,results,outperform,has,unsupervised extractive models,outperform has unsupervised extractive models,0.5667073130607605
translation,160,103,results,results,comparing,abstractive models,results comparing abstractive models,0.6593770384788513
translation,160,104,results,extractive oracle,in terms of,r-l,extractive oracle in terms of r-l,0.6728329062461853
translation,160,104,results,abstractive models,has,significantly outperform,abstractive models has significantly outperform,0.5934272408485413
translation,160,104,results,significantly outperform,has,extractive oracle,significantly outperform has extractive oracle,0.6024752259254456
translation,160,106,results,self-pretrained abstractive summarization models,such as,bart,self-pretrained abstractive summarization models such as bart,0.5985090136528015
translation,160,106,results,outperforms,has,self-pretrained abstractive summarization models,outperforms has self-pretrained abstractive summarization models,0.5361034870147705
translation,160,108,results,highly extractive,with,lowest novel n-gram,highly extractive with lowest novel n-gram,0.6760309934616089
translation,160,108,results,lowest novel n-gram,among,all approaches,lowest novel n-gram among all approaches,0.6084903478622437
translation,160,108,results,results,has,bart,results has bart,0.40022552013397217
translation,160,114,results,average score,is,"1.54 , 2.28 and 2.18","average score is 1.54 , 2.28 and 2.18",0.5462689995765686
translation,160,114,results,"1.54 , 2.28 and 2.18",for,"ext-oracle , himap , and pointer - generator","1.54 , 2.28 and 2.18 for ext-oracle , himap , and pointer - generator",0.6047860980033875
translation,160,114,results,results,has,average score,results has average score,0.5574581027030945
translation,160,115,results,overall writing style,of,abstractive models,overall writing style of abstractive models,0.5596853494644165
translation,160,115,results,abstractive models,are,much better,abstractive models are much better,0.609584391117096
translation,160,115,results,much better,than,extractive models,much better than extractive models,0.6066534519195557
translation,161,185,baselines,lead3,is,baseline,lead3 is baseline,0.6410661935806274
translation,161,185,baselines,lead3,selects,first three sentences,lead3 selects first three sentences,0.7212614417076111
translation,161,185,baselines,baselines,has,lead3,baselines has lead3,0.6291682720184326
translation,161,190,baselines,hierarchical transformer summarization model,without,pretraining,hierarchical transformer summarization model without pretraining,0.6572328209877014
translation,161,193,baselines,"pre-trained bert ( devlin et al. , 2018 )",finetuned on,cnndm dataset,"pre-trained bert ( devlin et al. , 2018 ) finetuned on cnndm dataset",0.7050190567970276
translation,161,149,experimental-setup,vocabulary size,applied,byte pair encoding ( bpe,vocabulary size applied byte pair encoding ( bpe,0.701405942440033
translation,161,149,experimental-setup,byte pair encoding ( bpe,),datasets,byte pair encoding ( bpe ) datasets,0.628912627696991
translation,161,149,experimental-setup,byte pair encoding ( bpe,to,datasets,byte pair encoding ( bpe to datasets,0.5593954920768738
translation,161,149,experimental-setup,experimental setup,To reduce,vocabulary size,experimental setup To reduce vocabulary size,0.6090720891952515
translation,161,159,experimental-setup,experimental setup,trained,two model sizes,experimental setup trained two model sizes,0.6995320320129395
translation,161,160,experimental-setup,hibert s and hibert m,on,single machine,hibert s and hibert m on single machine,0.5768659710884094
translation,161,160,experimental-setup,single machine,with,8 nvidia tesla v100 gpus,single machine with 8 nvidia tesla v100 gpus,0.5578872561454773
translation,161,160,experimental-setup,8 nvidia tesla v100 gpus,with,batch size,8 nvidia tesla v100 gpus with batch size,0.6141195297241211
translation,161,160,experimental-setup,batch size,of,256 documents,batch size of 256 documents,0.590327799320221
translation,161,160,experimental-setup,experimental setup,trained,hibert s and hibert m,experimental setup trained hibert s and hibert m,0.6275529861450195
translation,161,161,experimental-setup,our models,using,adam,our models using adam,0.7394709587097168
translation,161,161,experimental-setup,adam,with,learning rate,adam with learning rate,0.6478732824325562
translation,161,161,experimental-setup,adam,with,l2 norm,adam with l2 norm,0.6526591777801514
translation,161,161,experimental-setup,adam,with,learning rate warmup,adam with learning rate warmup,0.641314685344696
translation,161,161,experimental-setup,adam,with,learning rate decay,adam with learning rate decay,0.6362478137016296
translation,161,161,experimental-setup,learning rate,of,"1e - 4 , ? 1 = 0.9 , ? 2 = 0.999","learning rate of 1e - 4 , ? 1 = 0.9 , ? 2 = 0.999",0.5961929559707642
translation,161,161,experimental-setup,l2 norm,of,0.01,l2 norm of 0.01,0.5415064692497253
translation,161,161,experimental-setup,learning rate warmup,has,"10,000 steps","learning rate warmup has 10,000 steps",0.5547572374343872
translation,161,161,experimental-setup,experimental setup,optimized,our models,experimental setup optimized our models,0.7094513773918152
translation,161,162,experimental-setup,dropout rate,in,all layers,dropout rate in all layers,0.4733750522136688
translation,161,162,experimental-setup,all layers,are,0.1,all layers are 0.1,0.5723021626472473
translation,161,162,experimental-setup,experimental setup,has,dropout rate,experimental setup has dropout rate,0.505321204662323
translation,161,180,experiments,sequence to sequence learning based models,with,copy and coverage modeling,sequence to sequence learning based models with copy and coverage modeling,0.6443792581558228
translation,161,180,experiments,sequence to sequence learning based models,with,reinforcement learning,sequence to sequence learning based models with reinforcement learning,0.6047653555870056
translation,161,180,experiments,sequence to sequence learning based models,with,deep communicating agents extensions,sequence to sequence learning based models with deep communicating agents extensions,0.6406292915344238
translation,161,6,model,hibert,for,document encoding,hibert for document encoding,0.6285587549209595
translation,161,6,model,model,propose,hibert,model propose hibert,0.661041259765625
translation,161,6,model,model,propose,method,model propose method,0.6280754208564758
translation,161,32,model,hibert,stands for,hierachical bidirectional encoder representations,hibert stands for hierachical bidirectional encoder representations,0.6953389048576355
translation,161,32,model,model,propose,hibert,model propose hibert,0.661041259765625
translation,161,33,model,unsupervised method,to pre-train,hibert,unsupervised method to pre-train hibert,0.6901360750198364
translation,161,33,model,hibert,for,document modeling,hibert for document modeling,0.6189387440681458
translation,161,33,model,model,design,unsupervised method,model design unsupervised method,0.6282100081443787
translation,161,152,model,three stages,includes,two pre-training stages,three stages includes two pre-training stages,0.6451967358589172
translation,161,152,model,three stages,includes,one finetuning stage,three stages includes one finetuning stage,0.6807732582092285
translation,161,152,model,model,trained in,three stages,model trained in three stages,0.7665232419967651
translation,161,166,model,our models,during,fine-tuning stage,our models during fine-tuning stage,0.706746518611908
translation,161,166,model,fine-tuning stage,trained on,single gpu,fine-tuning stage trained on single gpu,0.7215290665626526
translation,161,166,model,model,during,fine-tuning stage,model during fine-tuning stage,0.6755735874176025
translation,161,166,model,model,has,our models,model has our models,0.5889175534248352
translation,161,7,results,pre-trained hibert,to,our summarization model,pre-trained hibert to our summarization model,0.528428316116333
translation,161,7,results,outperforms,by,2.0 rouge,outperforms by 2.0 rouge,0.5749791264533997
translation,161,7,results,randomly initialized counterpart,by,1.25 rouge,randomly initialized counterpart by 1.25 rouge,0.5959039926528931
translation,161,7,results,randomly initialized counterpart,by,2.0 rouge,randomly initialized counterpart by 2.0 rouge,0.5875418782234192
translation,161,7,results,1.25 rouge,on,cnn / dailymail dataset,1.25 rouge on cnn / dailymail dataset,0.4827824532985687
translation,161,7,results,2.0 rouge,on,version of new york times dataset,2.0 rouge on version of new york times dataset,0.48166829347610474
translation,161,7,results,outperforms,has,randomly initialized counterpart,outperforms has randomly initialized counterpart,0.6127644777297974
translation,161,7,results,results,apply,pre-trained hibert,results apply pre-trained hibert,0.5832125544548035
translation,161,186,results,stage,on,in- domain cnndm training set,stage on in- domain cnndm training set,0.5358051657676697
translation,161,186,results,our model,has,hibert s ( in-domain ),our model has hibert s ( in-domain ),0.6076899766921997
translation,161,186,results,results,has,our model,results has our model,0.5871725678443909
translation,161,188,results,our models,with,two pre-training stages ( hibert s ) or larger size ( hibert m ),our models with two pre-training stages ( hibert s ) or larger size ( hibert m ),0.6315863728523254
translation,161,188,results,hibert m,outperforms,bert,hibert m outperforms bert,0.6267854571342468
translation,161,188,results,bert,by,0.5 rouge 5,bert by 0.5 rouge 5,0.6065938472747803
translation,161,188,results,results,has,our models,results has our models,0.5733726620674133
translation,161,192,results,pre-training,leads to,+ 1.25 rouge improvement,pre-training leads to + 1.25 rouge improvement,0.6006942987442017
translation,161,192,results,results,see that,pre-training,results see that pre-training,0.6290469765663147
translation,161,198,results,bert,according to,rouge script,bert according to rouge script,0.7455288767814636
translation,161,198,results,significantly,according to,rouge script,significantly according to rouge script,0.6874306201934814
translation,161,198,results,hibert m,has,outperform,hibert m has outperform,0.5256856679916382
translation,161,198,results,outperform,has,bert,outperform has bert,0.6977319717407227
translation,161,198,results,outperform,has,significantly,outperform has significantly,0.6728229522705078
translation,161,198,results,bert,has,significantly,bert has significantly,0.7157989144325256
translation,161,198,results,results,has,hibert s ( in-domain ),results has hibert s ( in-domain ),0.5289487838745117
translation,161,204,results,hibert m,indicates,our model,hibert m indicates our model,0.6914397478103638
translation,161,204,results,significantly different,from,all systems,significantly different from all systems,0.6169909834861755
translation,161,204,results,our model,lags behind,human,our model lags behind human,0.7319200038909912
translation,161,204,results,our model,better than,all other systems,our model better than all other systems,0.7336874604225159
translation,161,204,results,all systems,has,in comparison ( p < 0.05 ),all systems has in comparison ( p < 0.05 ),0.5706632733345032
translation,161,204,results,results,has,hibert m,results has hibert m,0.566179633140564
translation,161,210,results,large open-domain dataset,to,pre-training,large open-domain dataset to pre-training,0.5278091430664062
translation,161,210,results,large open-domain dataset,leads to,even better performance,large open-domain dataset leads to even better performance,0.6112107038497925
translation,161,210,results,results,Adding,large open-domain dataset,results Adding large open-domain dataset,0.6458374857902527
translation,162,63,experimental-setup,source length,at,400 tokens,source length at 400 tokens,0.5272923111915588
translation,162,63,experimental-setup,source length,at,100 tokens,source length at 100 tokens,0.5271443128585815
translation,162,63,experimental-setup,target length,at,100 tokens,target length at 100 tokens,0.545236349105835
translation,162,63,experimental-setup,100 tokens,to expedite,training,100 tokens to expedite training,0.6333841681480408
translation,162,63,experimental-setup,experimental setup,truncate,source length,experimental setup truncate source length,0.687523603439331
translation,162,63,experimental-setup,experimental setup,truncate,target length,experimental setup truncate target length,0.6862228512763977
translation,162,65,experimental-setup,transformer model,implemented in,opennmt -py,transformer model implemented in opennmt -py,0.731512188911438
translation,162,65,experimental-setup,experimental setup,has,transformer model,experimental setup has transformer model,0.5146511793136597
translation,162,66,experimental-setup,model,use,single gpu,model use single gpu,0.6763530373573303
translation,162,67,experimental-setup,model,to,gpu cluster,model to gpu cluster,0.5594468116760254
translation,162,67,experimental-setup,batch size,equal to,"4,096","batch size equal to 4,096",0.7124521732330322
translation,162,67,experimental-setup,"4,096",selected for,training,"4,096 selected for training",0.5961391925811768
translation,162,67,experimental-setup,model,has,batch size,model has batch size,0.5527312755584717
translation,162,67,experimental-setup,gpu cluster,has,batch size,gpu cluster has batch size,0.5511400699615479
translation,162,67,experimental-setup,experimental setup,To fit,model,experimental setup To fit model,0.7038907408714294
translation,162,68,experimental-setup,validation batch size,set to,8,validation batch size set to 8,0.7670554518699646
translation,162,68,experimental-setup,experimental setup,has,validation batch size,experimental setup has validation batch size,0.5391285419464111
translation,162,69,experimental-setup,initial learning rate,of,2,initial learning rate of 2,0.6204834580421448
translation,162,69,experimental-setup,drop out,of,"0.2 and 8,000 warm - up steps","drop out of 0.2 and 8,000 warm - up steps",0.576666533946991
translation,162,70,experimental-setup,decoding,uses,beam size,decoding uses beam size,0.6223880052566528
translation,162,70,experimental-setup,beam size,of,10,beam size of 10,0.7020634412765503
translation,162,70,experimental-setup,experimental setup,has,decoding,experimental setup has decoding,0.5178583860397339
translation,162,85,experimental-setup,systems,trained for,300 k iterations,systems trained for 300 k iterations,0.7950881123542786
translation,162,85,experimental-setup,experimental setup,has,systems,experimental setup has systems,0.525396466255188
translation,162,7,model,iterative data augmentation approach,uses,synthetic data,iterative data augmentation approach uses synthetic data,0.5762754678726196
translation,162,7,model,synthetic data,along with,real summarization data,synthetic data along with real summarization data,0.6150180101394653
translation,162,7,model,real summarization data,for,german language,real summarization data for german language,0.5550827383995056
translation,162,7,model,model,propose,iterative data augmentation approach,model propose iterative data augmentation approach,0.6890955567359924
translation,163,79,baselines,basic encoderdecoder rnn,used in,neural machine translation,basic encoderdecoder rnn used in neural machine translation,0.6254305839538574
translation,163,79,baselines,basic encoderdecoder rnn,used in,text summarization,basic encoderdecoder rnn used in text summarization,0.6111341714859009
translation,163,183,experiments,gigaword dataset,where,texts are short,gigaword dataset where texts are short,0.5735277533531189
translation,163,183,experiments,our best model,achieves,comparable performance,our best model achieves comparable performance,0.6653156280517578
translation,163,183,experiments,comparable performance,with,current state - of - the- art,comparable performance with current state - of - the- art,0.6543008685112
translation,163,183,experiments,gigaword dataset,has,our best model,gigaword dataset has our best model,0.546048641204834
translation,163,183,experiments,texts are short,has,our best model,texts are short has our best model,0.5680481195449829
translation,163,7,model,linked entities,to guide,decoder of a neural text summarizer,linked entities to guide decoder of a neural text summarizer,0.6293991804122925
translation,163,7,model,decoder of a neural text summarizer,to generate,concise and better summaries,decoder of a neural text summarizer to generate concise and better summaries,0.6587967276573181
translation,163,8,model,off-the-shelf entity linking system ( els ),to extract,linked entities,off-the-shelf entity linking system ( els ) to extract linked entities,0.6739334464073181
translation,163,8,model,off-the-shelf entity linking system ( els ),propose,entity2topic ( e2t ),off-the-shelf entity linking system ( els ) propose entity2topic ( e2t ),0.6296231746673584
translation,163,8,model,model,leverage on,off-the-shelf entity linking system ( els ),model leverage on off-the-shelf entity linking system ( els ),0.7207218408584595
translation,163,8,model,model,propose,entity2topic ( e2t ),model propose entity2topic ( e2t ),0.6641325354576111
translation,163,10,model,imperfections,of,els,imperfections of els,0.6242663860321045
translation,163,10,model,els,by,encoding entities,els by encoding entities,0.5817848443984985
translation,163,10,model,encoding entities,with,selective disambiguation,encoding entities with selective disambiguation,0.604322075843811
translation,163,10,model,pooling entity vectors,using,firm attention,pooling entity vectors using firm attention,0.6352977156639099
translation,163,10,model,model,resolve,imperfections,model resolve imperfections,0.634794294834137
translation,163,26,model,entities,found in,original text,entities found in original text,0.6930737495422363
translation,163,26,model,entities,to infer,summary topic,entities to infer summary topic,0.6805342435836792
translation,163,26,model,model,use,entities,model use entities,0.6796106100082397
translation,163,32,model,method,to effectively apply,linked entities,method to effectively apply linked entities,0.6467423439025879
translation,163,32,model,linked entities,in,sequence - tosequence models,linked entities in sequence - tosequence models,0.5469972491264343
translation,163,32,model,sequence - tosequence models,called,entity2topic ( e2t ),sequence - tosequence models called entity2topic ( e2t ),0.6903886198997498
translation,163,33,model,model,has,e2t,model has e2t,0.6266078948974609
translation,163,71,model,entity2topic ( e2t ),has,module,entity2topic ( e2t ) has module,0.6131646633148193
translation,163,71,model,model,present,entity2topic ( e2t ),model present entity2topic ( e2t ),0.6599587798118591
translation,163,72,model,e2t,encodes,linked entities,e2t encodes linked entities,0.731455385684967
translation,163,72,model,e2t,transforms them into,single topic vector,e2t transforms them into single topic vector,0.6517922878265381
translation,163,72,model,linked entities,extracted from,text,linked entities extracted from text,0.5930098295211792
translation,163,72,model,linked entities,transforms them into,single topic vector,linked entities transforms them into single topic vector,0.6834635138511658
translation,163,72,model,model,has,e2t,model has e2t,0.6266078948974609
translation,163,11,results,e2t,to,simple sequenceto-sequence model,e2t to simple sequenceto-sequence model,0.5775023102760315
translation,163,11,results,e2t,see,significant improvements,e2t see significant improvements,0.6672456860542297
translation,163,11,results,simple sequenceto-sequence model,with,attention mechanism,simple sequenceto-sequence model with attention mechanism,0.6021081209182739
translation,163,11,results,attention mechanism,as,base model,attention mechanism as base model,0.5352070927619934
translation,163,11,results,significant improvements,of,performance,significant improvements of performance,0.6412339210510254
translation,163,11,results,performance,in,gigaword ( sentence to title ) and cnn ( long document to multi-sentence highlights ) summarization datasets,performance in gigaword ( sentence to title ) and cnn ( long document to multi-sentence highlights ) summarization datasets,0.520022988319397
translation,163,11,results,gigaword ( sentence to title ) and cnn ( long document to multi-sentence highlights ) summarization datasets,by,at least 2 rouge points,gigaword ( sentence to title ) and cnn ( long document to multi-sentence highlights ) summarization datasets by at least 2 rouge points,0.5527298450469971
translation,163,11,results,results,applying,e2t,results applying e2t,0.642937183380127
translation,163,38,results,our module,to,sequence - to-sequence model,our module to sequence - to-sequence model,0.5771241784095764
translation,163,38,results,sequence - to-sequence model,with,attention mechanism,sequence - to-sequence model with attention mechanism,0.6175161600112915
translation,163,38,results,performance,on,both datasets,performance on both datasets,0.4974806010723114
translation,163,38,results,attention mechanism,has,significantly increases,attention mechanism has significantly increases,0.5969133377075195
translation,163,38,results,significantly increases,has,performance,significantly increases has performance,0.5964246988296509
translation,163,38,results,results,show,our module,results show our module,0.6599667072296143
translation,163,38,results,results,applying,our module,results applying our module,0.6951378583908081
translation,163,39,results,state - of - the - art models,for,each dataset,state - of - the - art models for each dataset,0.5486652255058289
translation,163,39,results,model,obtains,comparable performance,model obtains comparable performance,0.6423370838165283
translation,163,39,results,comparable performance,on,gigaword dataset,comparable performance on gigaword dataset,0.49990949034690857
translation,163,39,results,comparable performance,on,cnn dataset,comparable performance on cnn dataset,0.5291147232055664
translation,163,39,results,comparable performance,on,cnn dataset,comparable performance on cnn dataset,0.5291147232055664
translation,163,39,results,gigaword dataset,where,texts are short,gigaword dataset where texts are short,0.5735277533531189
translation,163,39,results,gigaword dataset,where,texts are longer,gigaword dataset where texts are longer,0.586100161075592
translation,163,39,results,all competing models,on,cnn dataset,all competing models on cnn dataset,0.49118366837501526
translation,163,39,results,cnn dataset,where,texts are longer,cnn dataset where texts are longer,0.5964936017990112
translation,163,39,results,state - of - the - art models,has,model,state - of - the - art models has model,0.5235479474067688
translation,163,39,results,outperforms,has,all competing models,outperforms has all competing models,0.5733827352523804
translation,163,39,results,results,compared with,state - of - the - art models,results compared with state - of - the - art models,0.6394352912902832
translation,163,111,results,cnn - based encoder,is,good,cnn - based encoder is good,0.5503414273262024
translation,163,111,results,cnn - based encoder,minimizes,noise,cnn - based encoder minimizes noise,0.6864575743675232
translation,163,111,results,good,minimizes,noise,good minimizes noise,0.7124505639076233
translation,163,111,results,noise,by totally ignoring,far entities,noise by totally ignoring far entities,0.7520015835762024
translation,163,111,results,far entities,when,disambiguating,far entities when disambiguating,0.6277155876159668
translation,163,111,results,results,has,cnn - based encoder,results has cnn - based encoder,0.5542176961898804
translation,163,112,results,input text,is,short ( e.g. a sentence ),input text is short ( e.g. a sentence ),0.49102717638015747
translation,163,112,results,input text,is,is long ( e.g. a document ),input text is is long ( e.g. a document ),0.5418343544006348
translation,163,112,results,input text,when,is long ( e.g. a document ),input text when is long ( e.g. a document ),0.6360242962837219
translation,163,112,results,both encoders,perform,comparably,both encoders perform comparably,0.5745154619216919
translation,163,112,results,comparably,when,input text,comparably when input text,0.7032183408737183
translation,163,112,results,cnn - based encoder,performs,better,cnn - based encoder performs better,0.6194582581520081
translation,163,112,results,input text,has,both encoders,input text has both encoders,0.5864717960357666
translation,163,112,results,input text,has,is long ( e.g. a document ),input text has is long ( e.g. a document ),0.5609169006347656
translation,163,112,results,results,when,input text,results when input text,0.6498337984085083
translation,163,184,results,cnn dataset,where,texts are longer,cnn dataset where texts are longer,0.5964936017990112
translation,163,184,results,our best model,outperforms,all the previous models,our best model outperforms all the previous models,0.7572806477546692
translation,163,184,results,cnn dataset,has,our best model,cnn dataset has our best model,0.519860565662384
translation,163,184,results,texts are longer,has,our best model,texts are longer has our best model,0.5761492848396301
translation,163,184,results,results,In,cnn dataset,results In cnn dataset,0.5206027626991272
translation,163,186,results,e2t,achieves,significant improvement,e2t achieves significant improvement,0.6987444162368774
translation,163,186,results,significant improvement,over,baseline model base,significant improvement over baseline model base,0.6544985771179199
translation,163,186,results,baseline model base,with,6 rouge - 1 points increase,baseline model base with 6 rouge - 1 points increase,0.6440577507019043
translation,163,186,results,at least 2 rouge - 1 points increase,in,gigaword dataset,at least 2 rouge - 1 points increase in gigaword dataset,0.5489874482154846
translation,163,186,results,at least 2 rouge - 1 points increase,in,cnn dataset,at least 2 rouge - 1 points increase in cnn dataset,0.5464078187942505
translation,163,186,results,at least 2 rouge - 1 points increase,in,cnn dataset,at least 2 rouge - 1 points increase in cnn dataset,0.5464078187942505
translation,163,186,results,6 rouge - 1 points increase,in,cnn dataset,6 rouge - 1 points increase in cnn dataset,0.4956459701061249
translation,163,186,results,results,has,e2t,results has e2t,0.5353831648826599
translation,163,187,results,all variants of e2t,gain,improvements,all variants of e2t gain improvements,0.7168599367141724
translation,163,187,results,all variants of e2t,leveraging on,linked entities,all variants of e2t leveraging on linked entities,0.7515848875045776
translation,163,187,results,improvements,over,baseline,improvements over baseline,0.7402786016464233
translation,163,187,results,performance,of,summarizer,performance of summarizer,0.6232566833496094
translation,163,187,results,linked entities,has,improves,linked entities has improves,0.5969856977462769
translation,163,187,results,improves,has,performance,improves has performance,0.5770372748374939
translation,163,187,results,results,has,all variants of e2t,results has all variants of e2t,0.4913419783115387
translation,163,188,results,cnn - based encoder,with,selective disambiguation and firm attention,cnn - based encoder with selective disambiguation and firm attention,0.648661196231842
translation,163,188,results,cnn - based encoder,performs,best,cnn - based encoder performs best,0.6112924814224243
translation,163,188,results,model variants,has,cnn - based encoder,model variants has cnn - based encoder,0.5324621796607971
translation,163,188,results,results,Among,model variants,results Among model variants,0.5911716222763062
translation,164,192,baselines,lead,is,strong baseline,lead is strong baseline,0.6013855338096619
translation,164,192,baselines,strong baseline,using,leading 3 sentences,strong baseline using leading 3 sentences,0.6365765333175659
translation,164,192,baselines,leading 3 sentences,as,summary,leading 3 sentences as summary,0.5634216666221619
translation,164,192,baselines,"summarunner-abs ( nallapati et al. , 2017 )",trained on,abstractive summaries,"summarunner-abs ( nallapati et al. , 2017 ) trained on abstractive summaries",0.702164351940155
translation,164,193,baselines,abstractive models,include,nn - abs,abstractive models include nn - abs,0.6074816584587097
translation,164,193,baselines,abstractive models,include,nn - we,abstractive models include nn - we,0.6371753215789795
translation,164,193,baselines,abstractive models,include,lreg,abstractive models include lreg,0.5899667143821716
translation,164,193,baselines,baselines,has,abstractive models,baselines has abstractive models,0.5310056209564209
translation,164,195,baselines,nn - abs,is,simple hierarchical extension,nn - abs is simple hierarchical extension,0.5904796719551086
translation,164,195,baselines,nn - abs,is,abstractive model,nn - abs is abstractive model,0.6117009520530701
translation,164,195,baselines,simple hierarchical extension,"of ( Rush et al. , 2015 )",nn - we,"simple hierarchical extension of ( Rush et al. , 2015 ) nn - we",0.6987465023994446
translation,164,195,baselines,nn - we,is,abstractive model,nn - we is abstractive model,0.6204427480697632
translation,164,195,baselines,abstractive model,restricting,generation of words,abstractive model restricting generation of words,0.7216464877128601
translation,164,195,baselines,generation of words,from,original document,generation of words from original document,0.5535113215446472
translation,164,195,baselines,baselines,has,nn - abs,baselines has nn - abs,0.5658594965934753
translation,164,165,experimental-setup,google 's open-source seq2seq -master project,written with,tensorflow,google 's open-source seq2seq -master project written with tensorflow,0.701824426651001
translation,164,167,experimental-setup,dimension,of,hidden state,dimension of hidden state,0.5578262209892273
translation,164,167,experimental-setup,hidden state,of,rnn decoder,hidden state of rnn decoder,0.5704066753387451
translation,164,167,experimental-setup,rnn decoder,is,512,rnn decoder is 512,0.5891936421394348
translation,164,167,experimental-setup,experimental setup,has,dimension,experimental setup has dimension,0.4825572967529297
translation,164,168,experimental-setup,dimension,of,word embedding vector,dimension of word embedding vector,0.5306831002235413
translation,164,168,experimental-setup,word embedding vector,is,128,word embedding vector is 128,0.5479312539100647
translation,164,168,experimental-setup,experimental setup,has,dimension,experimental setup has dimension,0.4825572967529297
translation,164,169,experimental-setup,dimension,of,hidden state,dimension of hidden state,0.5578262209892273
translation,164,169,experimental-setup,hidden state,of,bi-directional rnn encoder,hidden state of bi-directional rnn encoder,0.5305859446525574
translation,164,169,experimental-setup,bi-directional rnn encoder,is,256,bi-directional rnn encoder is 256,0.5317639708518982
translation,164,169,experimental-setup,experimental setup,has,dimension,experimental setup has dimension,0.4825572967529297
translation,164,170,experimental-setup,word embeddings,with,google 's word2vec tools,word embeddings with google 's word2vec tools,0.6147648692131042
translation,164,170,experimental-setup,word embeddings,trained in,whole text of dailymail / cnn corpora,word embeddings trained in whole text of dailymail / cnn corpora,0.665705144405365
translation,164,170,experimental-setup,experimental setup,initialize,word embeddings,experimental setup initialize word embeddings,0.7035120129585266
translation,164,171,experimental-setup,4096 - dimension full-connected layer,of,19 - layer vggnet,4096 - dimension full-connected layer of 19 - layer vggnet,0.5833220481872559
translation,164,171,experimental-setup,vector representation,of,images,vector representation of images,0.6260372400283813
translation,164,171,experimental-setup,experimental setup,extract,4096 - dimension full-connected layer,experimental setup extract 4096 - dimension full-connected layer,0.7052239179611206
translation,164,172,experimental-setup,adam,to,"kingma and ba , 2014 )","adam to kingma and ba , 2014 )",0.6060695052146912
translation,164,172,experimental-setup,experimental setup,parameters of,adam,experimental setup parameters of adam,0.7265005707740784
translation,164,173,experimental-setup,batch size,set to,5,batch size set to 5,0.7845883965492249
translation,164,173,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,164,174,experimental-setup,convergence,reached within,800k training steps,convergence reached within 800k training steps,0.6716435551643372
translation,164,174,experimental-setup,experimental setup,has,convergence,experimental setup has convergence,0.5458384156227112
translation,164,175,experimental-setup,experimental setup,training,40 k ~ 50 k steps,experimental setup training 40 k ~ 50 k steps,0.7131912708282471
translation,164,176,experimental-setup,sentence beam width and the word beam width,set as,2 and 5,sentence beam width and the word beam width set as 2 and 5,0.6249780654907227
translation,164,176,experimental-setup,experimental setup,has,sentence beam width and the word beam width,experimental setup has sentence beam width and the word beam width,0.5419541597366333
translation,164,177,experimental-setup,m,set as,3,m set as 3,0.6634991765022278
translation,164,177,experimental-setup,experimental setup,has,m,experimental setup has m,0.5818241834640503
translation,164,73,hyperparameters,gru,used as,rnn cell,gru used as rnn cell,0.7144026756286621
translation,164,73,hyperparameters,hyperparameters,has,gru,hyperparameters has gru,0.5805169343948364
translation,164,87,hyperparameters,cnn model,adopted,19 - layer vggnet,cnn model adopted 19 - layer vggnet,0.6018098592758179
translation,164,87,hyperparameters,cnn model,is,19 - layer vggnet,cnn model is 19 - layer vggnet,0.5196940302848816
translation,164,87,hyperparameters,hyperparameters,has,cnn model,hyperparameters has cnn model,0.4958081543445587
translation,164,88,hyperparameters,last full-connected layer,as,image 's vector representation,last full-connected layer as image 's vector representation,0.5563469529151917
translation,164,88,hyperparameters,last full-connected layer,has,dimension,last full-connected layer has dimension,0.5655688047409058
translation,164,88,hyperparameters,hyperparameters,drop,last dropout layer,hyperparameters drop last dropout layer,0.6413235664367676
translation,164,129,hyperparameters,"adam ( kingma and ba , 2014 ) gradient - based optimization method",to optimize,model parameters,"adam ( kingma and ba , 2014 ) gradient - based optimization method to optimize model parameters",0.6814646124839783
translation,164,129,hyperparameters,hyperparameters,use,"adam ( kingma and ba , 2014 ) gradient - based optimization method","hyperparameters use adam ( kingma and ba , 2014 ) gradient - based optimization method",0.630156934261322
translation,164,7,model,multi-modal attentional mechanism,proposed to attend,"original sentences , images , and captions","multi-modal attentional mechanism proposed to attend original sentences , images , and captions",0.6725476384162903
translation,164,7,model,"original sentences , images , and captions",when,decoding,"original sentences , images , and captions when decoding",0.6248589754104614
translation,164,7,model,model,has,multi-modal attentional mechanism,model has multi-modal attentional mechanism,0.5389530062675476
translation,164,32,model,inference stage,adopt,multi-modal beam search algorithm,inference stage adopt multi-modal beam search algorithm,0.6395756602287292
translation,164,32,model,multi-modal beam search algorithm,scores,beams,multi-modal beam search algorithm scores beams,0.6320480704307556
translation,164,32,model,beams,based on,bigram overlaps,beams based on bigram overlaps,0.6813498139381409
translation,164,32,model,bigram overlaps,of,generated sentences,bigram overlaps of generated sentences,0.5698770880699158
translation,164,32,model,bigram overlaps,of,attended captions,bigram overlaps of attended captions,0.5927913784980774
translation,164,32,model,model,In,inference stage,model In inference stage,0.547758936882019
translation,164,66,model,hierarchical rnn,to encode,original sentences and the captions,hierarchical rnn to encode original sentences and the captions,0.6806219220161438
translation,164,66,model,cnn + rnn encoder,to encode,image set,cnn + rnn encoder to encode image set,0.6992154717445374
translation,164,66,model,model,consists of,three parts,model consists of three parts,0.7203089594841003
translation,164,139,model,algorithm,by adding,caption - level and image-level beam search,algorithm by adding caption - level and image-level beam search,0.7306935787200928
translation,164,139,model,model,extend,algorithm,model extend algorithm,0.7502309083938599
translation,164,140,model,multi-modal hierarchical beam search algorithm,comprises,k-best word - level beam search,multi-modal hierarchical beam search algorithm comprises k-best word - level beam search,0.590190052986145
translation,164,140,model,multi-modal hierarchical beam search algorithm,comprises,nbest sentence - caption - level beam search,multi-modal hierarchical beam search algorithm comprises nbest sentence - caption - level beam search,0.5927329063415527
translation,164,140,model,model,has,multi-modal hierarchical beam search algorithm,model has multi-modal hierarchical beam search algorithm,0.5420732498168945
translation,164,166,model,one layer,of,gru cell,one layer of gru cell,0.6463063359260559
translation,164,166,model,model,use,one layer,model use one layer,0.7093313932418823
translation,164,224,model,multi-modal attentional mechanism,attends,"original sentences , images , captions simultaneously","multi-modal attentional mechanism attends original sentences , images , captions simultaneously",0.6788820624351501
translation,164,224,model,"original sentences , images , captions simultaneously",in,hierarchical encoder- decoder model,"original sentences , images , captions simultaneously in hierarchical encoder- decoder model",0.5063688158988953
translation,164,224,model,rnn model,to encode,ordered image set,rnn model to encode ordered image set,0.721115231513977
translation,164,224,model,ordered image set,as,initial state,ordered image set as initial state,0.5383380651473999
translation,164,224,model,initial state,of,decoder,initial state of decoder,0.5983436107635498
translation,164,224,model,multimodal beam search algorithm,scores,beams,multimodal beam search algorithm scores beams,0.6747183799743652
translation,164,224,model,beams,using,bigram overlaps,beams using bigram overlaps,0.7221644520759583
translation,164,224,model,bigram overlaps,of,generated sentences and the captions,bigram overlaps of generated sentences and the captions,0.5432608723640442
translation,164,224,model,model,propose,multi-modal attentional mechanism,model propose multi-modal attentional mechanism,0.66229647397995
translation,164,224,model,model,propose,multimodal beam search algorithm,model propose multimodal beam search algorithm,0.7038449048995972
translation,164,197,results,our method hnnattti,has,outperforms,our method hnnattti has outperforms,0.6613134145736694
translation,164,197,results,outperforms,has,three extractive models,outperforms has three extractive models,0.5817530751228333
translation,164,197,results,results,has,our method hnnattti,results has our method hnnattti,0.6334208846092224
translation,164,201,results,hnnattti,performs,better,hnnattti performs better,0.7346016764640808
translation,164,201,results,better,than,hnnattt,better than hnnattt,0.6742657423019409
translation,164,201,results,better,than,hnnatttc,better than hnnatttc,0.6394912600517273
translation,164,201,results,better,than,hnnatttic,better than hnnatttic,0.6637091636657715
translation,164,201,results,results,show,hnnattti,results show hnnattti,0.652309775352478
translation,164,204,results,oov replacement mechanism,improves,summarization models,oov replacement mechanism improves summarization models,0.6501668691635132
translation,164,204,results,results,has,oov replacement mechanism,results has oov replacement mechanism,0.5646416544914246
translation,164,205,results,combining and attending images,in,neural summarization model,combining and attending images in neural summarization model,0.509200394153595
translation,164,205,results,neural summarization model,improves,document summarization,neural summarization model improves document summarization,0.619733989238739
translation,164,210,results,hnnattti,perform,worse,hnnattti perform worse,0.70613694190979
translation,164,210,results,hnnatttc and hnnatttic,perform,worse,hnnatttc and hnnatttic perform worse,0.6900261640548706
translation,164,210,results,hnnattti,has,outperforms,hnnattti has outperforms,0.6905945539474487
translation,164,210,results,outperforms,has,random baseline,outperforms has random baseline,0.5755338072776794
translation,164,210,results,results,show,hnnattti,results show hnnattti,0.652309775352478
translation,164,210,results,results,show,hnnatttc and hnnatttic,results show hnnatttc and hnnatttic,0.6248640418052673
translation,164,236,results,interesting observation,is that,hnnatttc and hnnatttic,interesting observation is that hnnatttc and hnnatttic,0.7074682712554932
translation,164,236,results,hnnatttc and hnnatttic,not better than,hnnattt,hnnatttc and hnnatttic not better than hnnattt,0.7010501623153687
translation,164,236,results,results,has,interesting observation,results has interesting observation,0.5519416928291321
translation,165,42,baselines,baseline,is,standard ( unguided ) seq2seq model,baseline is standard ( unguided ) seq2seq model,0.5377725958824158
translation,165,42,baselines,standard ( unguided ) seq2seq model,with,"attention ( luong et al. , 2015 )","standard ( unguided ) seq2seq model with attention ( luong et al. , 2015 )",0.5618448257446289
translation,165,42,baselines,"attention ( luong et al. , 2015 )",consists of,encoder,"attention ( luong et al. , 2015 ) consists of encoder",0.644701361656189
translation,165,42,baselines,"attention ( luong et al. , 2015 )",consists of,decoder,"attention ( luong et al. , 2015 ) consists of decoder",0.648768961429596
translation,165,42,baselines,baselines,has,baseline,baselines has baseline,0.6124745607376099
translation,165,19,model,information,missing from,amr,information missing from amr,0.7867496013641357
translation,165,19,model,information,needed for,nlg,information needed for nlg,0.7215351462364197
translation,165,19,model,model,to retrieve,information,model to retrieve information,0.7707304954528809
translation,165,22,results,proposed guided amr - to- text nlg,improve,summarization results,proposed guided amr - to- text nlg improve summarization results,0.600645899772644
translation,165,22,results,summarization results,using,gold standard amr parses and parses,summarization results using gold standard amr parses and parses,0.5996310710906982
translation,165,22,results,gold standard amr parses and parses,obtained using,"riga ( barzdins and gosko , 2016 ) parser","gold standard amr parses and parses obtained using riga ( barzdins and gosko , 2016 ) parser",0.6206454038619995
translation,165,22,results,"riga ( barzdins and gosko , 2016 ) parser",by,7.4 and 10.5 rouge - 2 points,"riga ( barzdins and gosko , 2016 ) parser by 7.4 and 10.5 rouge - 2 points",0.5841758251190186
translation,165,22,results,results,Using,proposed guided amr - to- text nlg,results Using proposed guided amr - to- text nlg,0.6688292622566223
translation,165,23,results,strong baseline seq2seq model,for,summarization,strong baseline seq2seq model for summarization,0.5864128470420837
translation,165,23,results,summarization,by,2 rouge - 2 points,summarization by 2 rouge - 2 points,0.5622668266296387
translation,165,23,results,outperforms,has,strong baseline seq2seq model,outperforms has strong baseline seq2seq model,0.5576534867286682
translation,165,72,results,difference,between,guided and the unguided model,difference between guided and the unguided model,0.6497213840484619
translation,165,72,results,guided and the unguided model,is,16.2 points,guided and the unguided model is 16.2 points,0.5557608604431152
translation,165,72,results,guided and the unguided model,is,9.9 points,guided and the unguided model is 9.9 points,0.561556875705719
translation,165,72,results,16.2 points,in,bleu,16.2 points in bleu,0.5099472403526306
translation,165,72,results,9.9 points,in,rouge - 2,9.9 points in rouge - 2,0.5799168348312378
translation,165,72,results,results,has,difference,results has difference,0.5636705756187439
translation,165,79,results,riga parses,result in,higher rouge scores,riga parses result in higher rouge scores,0.6358013153076172
translation,165,79,results,higher rouge scores,than,gold parses,higher rouge scores than gold parses,0.5972563624382019
translation,165,79,results,gold parses,for,guided model,gold parses for guided model,0.6137285828590393
translation,165,79,results,results,using,riga parses,results using riga parses,0.6191392540931702
translation,166,149,experimental-setup,"bert 's wordpiece ( schuster and nakajima , 2012 ) tokenizer",for,source documents,"bert 's wordpiece ( schuster and nakajima , 2012 ) tokenizer for source documents",0.5746952295303345
translation,166,149,experimental-setup,experimental setup,use,"bert 's wordpiece ( schuster and nakajima , 2012 ) tokenizer","experimental setup use bert 's wordpiece ( schuster and nakajima , 2012 ) tokenizer",0.5751425623893738
translation,166,150,experimental-setup,rel and ext,trained for,"30,000 steps","rel and ext trained for 30,000 steps",0.808559238910675
translation,166,150,experimental-setup,"30,000 steps",with,batch size,"30,000 steps with batch size",0.6407588124275208
translation,166,150,experimental-setup,batch size,of,12,batch size of 12,0.6833499670028687
translation,166,150,experimental-setup,experimental setup,has,rel and ext,experimental setup has rel and ext,0.5345141887664795
translation,166,156,experimental-setup,reduced,when,metric,reduced when metric,0.7166340351104736
translation,166,156,experimental-setup,improving,by using,reducelronplateau scheduler,improving by using reducelronplateau scheduler,0.6598894000053406
translation,166,156,experimental-setup,reducelronplateau scheduler,in,pytorch,reducelronplateau scheduler in pytorch,0.523914098739624
translation,166,156,experimental-setup,metric,has,stopped,metric has stopped,0.6499477028846741
translation,166,156,experimental-setup,stopped,has,improving,stopped has improving,0.6641426086425781
translation,166,156,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,166,157,experimental-setup,multi-head attention module,from,opennmt,multi-head attention module from opennmt,0.5742300748825073
translation,166,157,experimental-setup,abs,has,multi-head attention module,abs has multi-head attention module,0.5762314200401306
translation,166,157,experimental-setup,experimental setup,decoder of,abs,experimental setup decoder of abs,0.6612236499786377
translation,166,159,experimental-setup,vocabulary size,is,"120,000","vocabulary size is 120,000",0.5893678069114685
translation,166,159,experimental-setup,"120,000",includes,"29,351 mesh codes","120,000 includes 29,351 mesh codes",0.6392724514007568
translation,166,159,experimental-setup,experimental setup,has,vocabulary size,experimental setup has vocabulary size,0.5189570188522339
translation,166,161,experimental-setup,model dimension,is,768,model dimension is 768,0.6136478781700134
translation,166,161,experimental-setup,feed-forward layer size,is,2048,feed-forward layer size is 2048,0.5783006548881531
translation,166,161,experimental-setup,experimental setup,has,model dimension,experimental setup has model dimension,0.5250306129455566
translation,166,161,experimental-setup,experimental setup,has,feed-forward layer size,experimental setup has feed-forward layer size,0.5334299802780151
translation,166,162,experimental-setup,different initial learning rates,for,encoder and decoder,different initial learning rates for encoder and decoder,0.5981407761573792
translation,166,162,experimental-setup,experimental setup,use,different initial learning rates,experimental setup use different initial learning rates,0.6125137805938721
translation,166,164,experimental-setup,beam search,in,abs,beam search in abs,0.6010971665382385
translation,166,164,experimental-setup,beam size,set to,4,beam size set to 4,0.7776850461959839
translation,166,164,experimental-setup,beam search,has,beam size,beam search has beam size,0.5626558065414429
translation,166,164,experimental-setup,abs,has,beam size,abs has beam size,0.5656988024711609
translation,166,164,experimental-setup,experimental setup,For,beam search,experimental setup For beam search,0.5473336577415466
translation,166,166,experimental-setup,max length,of,target sentence,max length of target sentence,0.5764726400375366
translation,166,166,experimental-setup,target sentence,limited to,50,target sentence limited to 50,0.6684626340866089
translation,166,166,experimental-setup,incrementally generated,until,abs,incrementally generated until abs,0.5875159502029419
translation,166,166,experimental-setup,abs,outputs,corresponding eos token,abs outputs corresponding eos token,0.7246299386024475
translation,166,166,experimental-setup,corresponding eos token,for,each facet,corresponding eos token for each facet,0.6279630661010742
translation,166,166,experimental-setup,experimental setup,has,max length,experimental setup has max length,0.52577805519104
translation,166,166,experimental-setup,experimental setup,has,sequence,experimental setup has sequence,0.5219777822494507
translation,166,197,experimental-setup,training and testing,done on,single nvidia titan x gpu,training and testing done on single nvidia titan x gpu,0.6552423238754272
translation,166,197,experimental-setup,single nvidia titan x gpu,in,desktop,single nvidia titan x gpu in desktop,0.5118125081062317
translation,166,197,experimental-setup,desktop,with,64gb ram,desktop with 64gb ram,0.5594038963317871
translation,166,197,experimental-setup,experimental setup,has,machine configuration,experimental setup has machine configuration,0.5194721221923828
translation,166,151,experiments,maximum number of tokens,for,source texts,maximum number of tokens for source texts,0.5296639800071716
translation,166,151,experiments,maximum number of tokens,limited to,384,maximum number of tokens limited to 384,0.6539576053619385
translation,166,8,model,document reranking approach,combines,neural query - document matching,document reranking approach combines neural query - document matching,0.6572757959365845
translation,166,8,model,document reranking approach,combines,text summarization,document reranking approach combines text summarization,0.6296027302742004
translation,166,8,model,model,present,document reranking approach,model present document reranking approach,0.608279824256897
translation,166,14,model,full architecture,benefits from,complementary potential,full architecture benefits from complementary potential,0.671234667301178
translation,166,14,model,full architecture,benefits from,novel document transformation approach,full architecture benefits from novel document transformation approach,0.628623902797699
translation,166,14,model,complementary potential,of,document- query matching,complementary potential of document- query matching,0.601337730884552
translation,166,14,model,novel document transformation approach,based on,summarization,novel document transformation approach based on summarization,0.6208290457725525
translation,166,14,model,summarization,along,pm facets,summarization along pm facets,0.6696284413337708
translation,166,14,model,model,has,full architecture,model has full architecture,0.565035879611969
translation,166,47,model,decoder,output,concept codes,decoder output concept codes,0.7816980481147766
translation,166,47,model,concept codes,from,biomedical terminologies,concept codes from biomedical terminologies,0.4764210879802704
translation,166,47,model,biomedical terminologies,that capture,disease and gene names,biomedical terminologies that capture disease and gene names,0.6167677640914917
translation,166,160,model,six transformer layers,in,decoder,six transformer layers in decoder,0.5990058183670044
translation,166,160,model,model,use,six transformer layers,model use six transformer layers,0.6453195810317993
translation,166,50,results,absolute 4 % improvement,in,p@10,absolute 4 % improvement in p@10,0.6106892824172974
translation,166,50,results,absolute 4 % improvement,compared to,prior best approaches,absolute 4 % improvement compared to prior best approaches,0.6638699173927307
translation,166,50,results,prior best approaches,while obtaining,small ? 1 % gain,prior best approaches while obtaining small ? 1 % gain,0.6234942674636841
translation,166,50,results,small ? 1 % gain,in,r - prec,small ? 1 % gain in r - prec,0.5793431997299194
translation,166,50,results,results,show,absolute 4 % improvement,results show absolute 4 % improvement,0.6941438913345337
translation,166,173,results,rel,recover,92 %,rel recover 92 %,0.7557893395423889
translation,166,173,results,92 %,of,relevant documents,92 % of relevant documents,0.5652197003364563
translation,166,173,results,ext,identify,88 %,ext identify 88 %,0.6679936051368713
translation,166,173,results,88 %,of,tokens,88 % of tokens,0.6679952144622803
translation,166,173,results,tokens,occur in,patient case information,tokens occur in patient case information,0.7088873386383057
translation,166,184,results,models,present,stable baseline scores,models present stable baseline scores,0.6887685656547546
translation,166,184,results,models,present,combined method ( + rel + abs ),models present combined method ( + rel + abs ),0.6961110234260559
translation,166,184,results,stable baseline scores,in,p@10,stable baseline scores in p@10,0.5538029670715332
translation,166,184,results,combined method ( + rel + abs ),tops,list,combined method ( + rel + abs ) tops list,0.7507172226905823
translation,166,184,results,4 % improvement,over,"prior best model ( faessler et al. , 2020 )","4 % improvement over prior best model ( faessler et al. , 2020 )",0.6258116960525513
translation,166,184,results,results,has,models,results has models,0.5335168838500977
translation,166,185,results,baseline with rel,does,best,baseline with rel does best,0.32625240087509155
translation,166,185,results,best,in terms of,r,best in terms of r,0.7218157649040222
translation,166,185,results,r,-,prec,r - prec,0.7263633608818054
translation,166,185,results,r,has,prec,r has prec,0.699984610080719
translation,166,185,results,results,has,baseline with rel,results has baseline with rel,0.5624196529388428
translation,167,187,ablation-analysis,ff-ext based model,out -performs,rnn-ext model,ff-ext based model out -performs rnn-ext model,0.7273149490356445
translation,167,187,ablation-analysis,abstractor,has,ff-ext based model,abstractor has ff-ext based model,0.5474538207054138
translation,167,187,ablation-analysis,ablation analysis,After applying,abstractor,ablation analysis After applying abstractor,0.6964896321296692
translation,167,73,experimental-setup,experimental setup,use,standard encoder-aligner - decoder,experimental setup use standard encoder-aligner - decoder,0.5959305167198181
translation,167,156,experiments,cnn / daily mail dataset,modified for,summarization,cnn / daily mail dataset modified for summarization,0.6528539061546326
translation,167,4,model,accurate and fast summarization,first selects,salient sentences,accurate and fast summarization first selects salient sentences,0.6571852564811707
translation,167,4,model,accurate and fast summarization,rewrites,abstractively,accurate and fast summarization rewrites abstractively,0.656927764415741
translation,167,5,model,novel sentence - level policy gradient method,to bridge,nondifferentiable computation,novel sentence - level policy gradient method to bridge nondifferentiable computation,0.5317361950874329
translation,167,5,model,nondifferentiable computation,between,two neural networks,nondifferentiable computation between two neural networks,0.597537636756897
translation,167,5,model,nondifferentiable computation,while maintaining,language fluency,nondifferentiable computation while maintaining language fluency,0.6572526693344116
translation,167,5,model,two neural networks,in,hierarchical way,two neural networks in hierarchical way,0.5531675815582275
translation,167,5,model,model,use,novel sentence - level policy gradient method,model use novel sentence - level policy gradient method,0.5424105525016785
translation,167,7,model,parallel decoding,of,our neural generative model,parallel decoding of our neural generative model,0.5451996922492981
translation,167,7,model,parallel decoding,results in,substantially faster ( 10 - 20x ) inference speed,parallel decoding results in substantially faster ( 10 - 20x ) inference speed,0.629021406173706
translation,167,7,model,parallel decoding,results in,4x faster training convergence,parallel decoding results in 4x faster training convergence,0.5803493857383728
translation,167,7,model,4x faster training convergence,than,previous long- paragraph encoder-decoder models,4x faster training convergence than previous long- paragraph encoder-decoder models,0.5655924081802368
translation,167,7,model,model,first operating at,sentence - level,model first operating at sentence - level,0.6889224052429199
translation,167,7,model,model,enable,parallel decoding,model enable parallel decoding,0.7117906212806702
translation,167,15,model,hybrid extractive - abstractive architecture,with,policy - based reinforcement learning ( rl ),hybrid extractive - abstractive architecture with policy - based reinforcement learning ( rl ),0.632655143737793
translation,167,15,model,policy - based reinforcement learning ( rl ),to bridge together,two networks,policy - based reinforcement learning ( rl ) to bridge together two networks,0.6140090823173523
translation,167,15,model,model,propose,hybrid extractive - abstractive architecture,model propose hybrid extractive - abstractive architecture,0.7085209488868713
translation,167,16,model,extractor agent,to select,salient sentences or highlights,extractor agent to select salient sentences or highlights,0.6675649881362915
translation,167,16,model,abstractor network,"to rewrite ( i.e. , compress and paraphrase )",extracted sentences,"abstractor network to rewrite ( i.e. , compress and paraphrase ) extracted sentences",0.7231840491294861
translation,167,16,model,model,first uses,extractor agent,model first uses extractor agent,0.6860076189041138
translation,167,16,model,model,employs,abstractor network,model employs abstractor network,0.5704454779624939
translation,167,17,model,available document -summary pairs,without,saliency label,available document -summary pairs without saliency label,0.6605878472328186
translation,167,17,model,available document -summary pairs,use,actorcritic policy gradient,available document -summary pairs use actorcritic policy gradient,0.6723431944847107
translation,167,17,model,actorcritic policy gradient,with,sentence - level metric rewards,actorcritic policy gradient with sentence - level metric rewards,0.5586153864860535
translation,167,17,model,actorcritic policy gradient,to learn,sentence saliency,actorcritic policy gradient to learn sentence saliency,0.5880988836288452
translation,167,17,model,sentence - level metric rewards,to connect,two neural networks,sentence - level metric rewards to connect two neural networks,0.6027171015739441
translation,167,17,model,model,train on,available document -summary pairs,model train on available document -summary pairs,0.7269665002822876
translation,167,17,model,model,use,actorcritic policy gradient,model use actorcritic policy gradient,0.6565573215484619
translation,167,19,model,sentence - level reinforcement learning,takes into account,word-sentence hierarchy,sentence - level reinforcement learning takes into account word-sentence hierarchy,0.6111506819725037
translation,167,19,model,word-sentence hierarchy,better models,language structure,word-sentence hierarchy better models language structure,0.7400755882263184
translation,167,19,model,model,has,sentence - level reinforcement learning,model has sentence - level reinforcement learning,0.5016865730285645
translation,167,20,model,extractor,combines,reinforcement learning,extractor combines reinforcement learning,0.7141276597976685
translation,167,20,model,extractor,combines,pointer networks,extractor combines pointer networks,0.6859211921691895
translation,167,20,model,model,has,extractor,model has extractor,0.5773076415061951
translation,167,21,model,abstractor,is,simple encoder-aligner - decoder model,abstractor is simple encoder-aligner - decoder model,0.502326250076294
translation,167,21,model,abstractor,trained on,pseudo document - summary sentence pairs,abstractor trained on pseudo document - summary sentence pairs,0.7123181819915771
translation,167,21,model,pseudo document - summary sentence pairs,obtained via,simple automatic matching criteria,pseudo document - summary sentence pairs obtained via simple automatic matching criteria,0.606094479560852
translation,167,21,model,model,has,abstractor,model has abstractor,0.5381795167922974
translation,167,23,model,human-inspired coarse- to -fine approach,first extracts,all the salient sentences,human-inspired coarse- to -fine approach first extracts all the salient sentences,0.6826137900352478
translation,167,23,model,model,adopts,human-inspired coarse- to -fine approach,model adopts human-inspired coarse- to -fine approach,0.6397331357002258
translation,167,34,model,model,propose,novel sentence - level rl technique,model propose novel sentence - level rl technique,0.6515732407569885
translation,167,74,model,copy mechanism,to help,directly copy,copy mechanism to help directly copy,0.6793111562728882
translation,167,74,model,directly copy,has,some outof-vocabulary ( oov ) words,directly copy has some outof-vocabulary ( oov ) words,0.5860649943351746
translation,167,74,model,model,add,copy mechanism,model add copy mechanism,0.6568287014961243
translation,167,214,model,full model,composed of,extremely fast extractor,full model composed of extremely fast extractor,0.6768375039100647
translation,167,214,model,full model,composed of,parallelizable abstractor,full model composed of parallelizable abstractor,0.6343348622322083
translation,167,214,model,parallelizable abstractor,where,computation bottleneck,parallelizable abstractor where computation bottleneck,0.5997146964073181
translation,167,214,model,model,has,full model,model has full model,0.5830336213111877
translation,167,228,model,convolutional encoder,at,word- level,convolutional encoder at word- level,0.5239585638046265
translation,167,228,model,convolutional encoder,made parallelizable by,hierarchical rnn-ext,convolutional encoder made parallelizable by hierarchical rnn-ext,0.7221578359603882
translation,167,228,model,our model,scalable for,very long documents,our model scalable for very long documents,0.7305265665054321
translation,167,228,model,convolutional encoder,has,our model,convolutional encoder has our model,0.5596310496330261
translation,167,228,model,hierarchical rnn-ext,has,our model,hierarchical rnn-ext has our model,0.601394772529602
translation,167,228,model,model,with,convolutional encoder,model with convolutional encoder,0.6138977408409119
translation,167,25,results,our approach,is,new state- ofthe - art,our approach is new state- ofthe - art,0.5389842987060547
translation,167,25,results,our approach,achieving,statistically significant improvements,our approach achieving statistically significant improvements,0.6359091997146606
translation,167,25,results,new state- ofthe - art,on,all rouge metrics,new state- ofthe - art on all rouge metrics,0.5197112560272217
translation,167,25,results,new state- ofthe - art,on,"meteor ( denkowski and lavie , 2014 )","new state- ofthe - art on meteor ( denkowski and lavie , 2014 )",0.5538349747657776
translation,167,25,results,new state- ofthe - art,of,cnn / daily mail dataset,new state- ofthe - art of cnn / daily mail dataset,0.5281670093536377
translation,167,25,results,new state- ofthe - art,achieving,statistically significant improvements,new state- ofthe - art achieving statistically significant improvements,0.6543214917182922
translation,167,25,results,"meteor ( denkowski and lavie , 2014 )",of,cnn / daily mail dataset,"meteor ( denkowski and lavie , 2014 ) of cnn / daily mail dataset",0.5332529544830322
translation,167,25,results,statistically significant improvements,over,previous models,statistically significant improvements over previous models,0.5938616991043091
translation,167,25,results,results,has,our approach,results has our approach,0.6050099730491638
translation,167,26,results,test-only duc - 2002 improvement,shows,our model 's better generalization,test-only duc - 2002 improvement shows our model 's better generalization,0.6368995904922485
translation,167,26,results,our model 's better generalization,than,strong abstractive system,our model 's better generalization than strong abstractive system,0.5911089777946472
translation,167,26,results,results,has,test-only duc - 2002 improvement,results has test-only duc - 2002 improvement,0.5644177198410034
translation,167,27,results,popular lead - 3 baseline,on,all rouge scores,popular lead - 3 baseline on all rouge scores,0.46930915117263794
translation,167,27,results,popular lead - 3 baseline,with,abstractive model,popular lead - 3 baseline with abstractive model,0.6189987659454346
translation,167,27,results,results,surpass,popular lead - 3 baseline,results surpass popular lead - 3 baseline,0.6791849732398987
translation,167,28,results,sentence - level abstractive rewriting module,produces,substantially more ( 3x ) novel n - grams,sentence - level abstractive rewriting module produces substantially more ( 3x ) novel n - grams,0.6233644485473633
translation,167,28,results,substantially more ( 3x ) novel n - grams,not seen in,input document,substantially more ( 3x ) novel n - grams not seen in input document,0.6720265746116638
translation,167,28,results,substantially more ( 3x ) novel n - grams,compared to,strong flat-structured model,substantially more ( 3x ) novel n - grams compared to strong flat-structured model,0.6614022254943848
translation,167,28,results,results,has,sentence - level abstractive rewriting module,results has sentence - level abstractive rewriting module,0.5263923406600952
translation,167,30,results,our model,maintains,same level of fluency,our model maintains same level of fluency,0.6545965671539307
translation,167,30,results,same level of fluency,as,conventional rnn - based model,same level of fluency as conventional rnn - based model,0.5488178133964539
translation,167,30,results,results,show,our model,results show our model,0.6888449192047119
translation,167,31,results,our model 's training,is,4x,our model 's training is 4x,0.6246888637542725
translation,167,31,results,more than 20x faster,than,previous state - of - the- art,more than 20x faster than previous state - of - the- art,0.5191975235939026
translation,167,31,results,results,has,our model 's training,results has our model 's training,0.5338820815086365
translation,167,32,results,optional final reranker,gives,further improvements,optional final reranker gives further improvements,0.6239070296287537
translation,167,32,results,further improvements,while maintaining,7x speedup,further improvements while maintaining 7x speedup,0.6474198698997498
translation,167,32,results,results,has,optional final reranker,results has optional final reranker,0.5803478360176086
translation,167,36,results,our parallel decoding,results in,significant 10 - 20x speed - up,our parallel decoding results in significant 10 - 20x speed - up,0.657935380935669
translation,167,36,results,significant 10 - 20x speed - up,over,previous best neural abstractive summarization system,significant 10 - 20x speed - up over previous best neural abstractive summarization system,0.6782293319702148
translation,167,36,results,previous best neural abstractive summarization system,with,even better accuracy,previous best neural abstractive summarization system with even better accuracy,0.6003498435020447
translation,167,36,results,results,has,our parallel decoding,results has our parallel decoding,0.5324423313140869
translation,167,174,results,our model,achieves,strong improvements,our model achieves strong improvements,0.6387211084365845
translation,167,174,results,our model,achieves,new state - of - the - art,our model achieves new state - of - the - art,0.6142838597297668
translation,167,174,results,new state - of - the - art,on,extractive and abstractive settings,new state - of - the - art on extractive and abstractive settings,0.5042561292648315
translation,167,174,results,extractive and abstractive settings,for,both versions of the cnn / dm dataset,extractive and abstractive settings for both versions of the cnn / dm dataset,0.5753787755966187
translation,167,174,results,results,has,our model,results has our model,0.5871725678443909
translation,167,179,results,our feed -forward extractor,out -performs,lead - 3 baseline,our feed -forward extractor out -performs lead - 3 baseline,0.7504369616508484
translation,167,179,results,our hierarchical sentence encoding model,capable of extracting,salient sentences,our hierarchical sentence encoding model capable of extracting salient sentences,0.681764543056488
translation,167,179,results,results,see that,our feed -forward extractor,results see that our feed -forward extractor,0.6285830140113831
translation,167,181,results,reinforced extractor,performs,best,reinforced extractor performs best,0.6587149500846863
translation,167,181,results,results,has,reinforced extractor,results has reinforced extractor,0.5568338632583618
translation,167,183,results,outperforms,has,previous best neural extractive model,outperforms has previous best neural extractive model,0.5874456763267517
translation,167,188,results,combined models,exceed,pointer- generator model,combined models exceed pointer- generator model,0.6103011965751648
translation,167,188,results,pointer- generator model,without coverage,large margin,pointer- generator model without coverage large margin,0.8156930208206177
translation,167,188,results,large margin,for,all metrics,large margin for all metrics,0.5919229388237
translation,167,188,results,results,has,combined models,results has combined models,0.5037420988082886
translation,167,189,results,our model,performs better,best model of see et al . ( 2017 ),our model performs better best model of see et al . ( 2017 ),0.7058475613594055
translation,167,189,results,our model,performs better,best ml trained model,our model performs better best ml trained model,0.7492427229881287
translation,167,189,results,reinforcement learning,has,our model,reinforcement learning has our model,0.5724539756774902
translation,167,189,results,results,after applying,reinforcement learning,results after applying reinforcement learning,0.6915825009346008
translation,167,190,results,ml trained rnn-ext + abs baseline,with,statistical significance,ml trained rnn-ext + abs baseline with statistical significance,0.620455801486969
translation,167,190,results,statistical significance,of,p < 0.01,statistical significance of p < 0.01,0.530430018901825
translation,167,190,results,p < 0.01,for,both version,p < 0.01 for both version,0.6234944462776184
translation,167,190,results,all metrics,for,both version,all metrics for both version,0.5625286102294922
translation,167,190,results,reinforced model,has,outperforms,reinforced model has outperforms,0.6347556114196777
translation,167,190,results,outperforms,has,ml trained rnn-ext + abs baseline,outperforms has ml trained rnn-ext + abs baseline,0.5433745980262756
translation,167,190,results,results,has,reinforced model,results has reinforced model,0.5475556254386902
translation,167,191,results,statistically significant better,than,see et al . ( 2017 ),statistically significant better than see et al . ( 2017 ),0.5228812098503113
translation,167,191,results,statistically significant better,than,all metrics,statistically significant better than all metrics,0.5576459169387817
translation,167,191,results,statistically significant better,for,all metrics,statistically significant better for all metrics,0.6030849814414978
translation,167,191,results,see et al . ( 2017 ),for,all metrics,see et al . ( 2017 ) for all metrics,0.6058099269866943
translation,167,191,results,results,has,rnn-ext + abs + rl,results has rnn-ext + abs + rl,0.5543820858001709
translation,167,199,results,trivial lead - 3 + abs baseline,ROUGE of,"( 37.37 , 15.59 , 34.82 )","trivial lead - 3 + abs baseline ROUGE of ( 37.37 , 15.59 , 34.82 )",0.6380845308303833
translation,167,199,results,best abstractive,has,trivial lead - 3 + abs baseline,best abstractive has trivial lead - 3 + abs baseline,0.5648608803749084
translation,167,199,results,results,has,best abstractive,results has best abstractive,0.569783627986908
translation,167,202,results,comparable,on,r - 1 and r - 2,comparable on r - 1 and r - 2,0.6356213688850403
translation,167,202,results,0.4 point improvement,on,r -l,0.4 point improvement on r -l,0.5652720332145691
translation,167,203,results,outperform,on,original and anonymized dataset versions,outperform on original and anonymized dataset versions,0.5369296073913574
translation,167,203,results,results,on,original and anonymized dataset versions,results on original and anonymized dataset versions,0.5428292155265808
translation,167,203,results,outperform,has,results,outperform has results,0.6540238857269287
translation,167,203,results,results,has,outperform,results has outperform,0.642206609249115
translation,167,205,results,best model,is,first abstractive models,best model is first abstractive models,0.5886749029159546
translation,167,205,results,best model,one of,first abstractive models,best model one of first abstractive models,0.6341896653175354
translation,167,205,results,first abstractive models,to outperform,lead - 3 baseline,first abstractive models to outperform lead - 3 baseline,0.7818812727928162
translation,167,205,results,lead - 3 baseline,on,originaltext cnn / dm dataset,lead - 3 baseline on originaltext cnn / dm dataset,0.4951413571834564
translation,167,205,results,results,Note,best model,results Note best model,0.6434658169746399
translation,167,222,results,speed up,of,18x,speed up of 18x,0.6470287442207336
translation,167,222,results,speed up,of,24x,speed up of 24x,0.6301968693733215
translation,167,222,results,18x,in,time,18x in time,0.6227998733520508
translation,167,222,results,24x,in,word generation rate,24x in word generation rate,0.5301113128662109
translation,167,238,results,our model,rewrites,substantially more abstractive summaries,our model rewrites substantially more abstractive summaries,0.8168644905090332
translation,167,238,results,substantially more abstractive summaries,than,previous work,substantially more abstractive summaries than previous work,0.5567248463630676
translation,168,120,experimental-setup,experimental setup,use,single - layer bi-lstm encoders,experimental setup use single - layer bi-lstm encoders,0.5984350442886353
translation,168,122,experimental-setup,256,for,gigaword,256 for gigaword,0.6642251014709473
translation,168,122,experimental-setup,32,for,other two,32 for other two,0.6330441832542419
translation,168,122,experimental-setup,batch size,has,256,batch size has 256,0.61165452003479
translation,168,122,experimental-setup,experimental setup,train with,batch size,experimental setup train with batch size,0.699195384979248
translation,168,122,experimental-setup,experimental setup,train with,32,experimental setup train with 32,0.5953400135040283
translation,168,123,experimental-setup,vocabulary size,set to,30 k,vocabulary size set to 30 k,0.7263486385345459
translation,168,123,experimental-setup,30 k,for,all dataset,30 k for all dataset,0.6033205986022949
translation,168,123,experimental-setup,experimental setup,has,vocabulary size,experimental setup has vocabulary size,0.5189570188522339
translation,168,125,experimental-setup,words,with,wordpiece segmentation,words with wordpiece segmentation,0.6452236175537109
translation,168,125,experimental-setup,words,to eliminate,oov problem,words to eliminate oov problem,0.722926676273346
translation,168,125,experimental-setup,experimental setup,tokenize,words,experimental setup tokenize words,0.6780206561088562
translation,168,115,experiments,xsum corpus,provides,single-sentence summary,xsum corpus provides single-sentence summary,0.614402711391449
translation,168,115,experiments,single-sentence summary,for,each bbc long story,single-sentence summary for each bbc long story,0.5986230969429016
translation,168,9,model,editing,transforming,pointed word vector,editing transforming pointed word vector,0.7726348042488098
translation,168,9,model,pointed word vector,into,target space,pointed word vector into target space,0.5953413248062134
translation,168,9,model,target space,with,learned relation embedding,target space with learned relation embedding,0.5859349370002747
translation,168,9,model,model,has,editing,model has editing,0.5107150673866272
translation,168,13,model,sentinel,to sample,words,sentinel to sample words,0.6931768655776978
translation,168,13,model,sentinel,directly copy from,aligned source context,sentinel directly copy from aligned source context,0.5999392867088318
translation,168,13,model,words,based on,neural attention,words based on neural attention,0.5104551911354065
translation,168,13,model,neural attention,has,generation mode ),neural attention has generation mode ),0.6142674684524536
translation,168,13,model,model,At,each decoding step,model At each decoding step,0.5567165613174438
translation,168,13,model,model,generates,sentinel,model generates sentinel,0.6763906478881836
translation,168,24,model,generalized pointer generator ( gpg ),replaces,hard copy component,generalized pointer generator ( gpg ) replaces hard copy component,0.7150623202323914
translation,168,24,model,hard copy component,with,more general soft   editing   function,hard copy component with more general soft   editing   function,0.6278653144836426
translation,168,24,model,model,propose,generalized pointer generator ( gpg ),model propose generalized pointer generator ( gpg ),0.6736272573471069
translation,168,31,model,oov problem,utilize,byte-pairencoding ( bpe ) segmentation,oov problem utilize byte-pairencoding ( bpe ) segmentation,0.589616060256958
translation,168,31,model,byte-pairencoding ( bpe ) segmentation,to split,rare words,byte-pairencoding ( bpe ) segmentation to split rare words,0.6249106526374817
translation,168,31,model,rare words,into,sub-units,rare words into sub-units,0.5807787775993347
translation,168,31,model,model,To eliminate,oov problem,model To eliminate oov problem,0.7204756736755371
translation,168,31,model,model,utilize,byte-pairencoding ( bpe ) segmentation,model utilize byte-pairencoding ( bpe ) segmentation,0.6337305903434753
translation,168,124,model,word representations,shared between,encoder and decoder,word representations shared between encoder and decoder,0.7092975378036499
translation,168,124,model,model,has,word representations,model has word representations,0.5698586106300354
translation,168,34,results,benefits,capture,richer latent alignment,benefits capture richer latent alignment,0.7704280614852905
translation,168,35,results,generated summaries,are,more faithful,generated summaries are more faithful,0.6043288707733154
translation,168,35,results,more faithful,to,source context,more faithful to source context,0.5644997954368591
translation,168,35,results,more faithful,because of,explicit alignment grounding,more faithful because of explicit alignment grounding,0.6678294539451599
translation,168,35,results,results,has,generated summaries,results has generated summaries,0.5842030048370361
translation,168,147,results,pointer generators,bring,only slight improvements,pointer generators bring only slight improvements,0.5472466945648193
translation,168,147,results,only slight improvements,over,seq2seq baseline,only slight improvements over seq2seq baseline,0.6344727277755737
translation,168,147,results,results,has,pointer generators,results has pointer generators,0.5482014417648315
translation,168,149,results,outperform,on,all dataset,outperform on all dataset,0.5421688556671143
translation,168,149,results,pointer generators,on,all dataset,pointer generators on all dataset,0.46492698788642883
translation,168,149,results,gpg models,has,outperform,gpg models has outperform,0.617654025554657
translation,168,149,results,outperform,has,seq2seq,outperform has seq2seq,0.5977397561073303
translation,168,149,results,outperform,has,pointer generators,outperform has pointer generators,0.5627706050872803
translation,168,149,results,results,has,gpg models,results has gpg models,0.5052469968795776
translation,168,150,results,improvement,is,more significant,improvement is more significant,0.5719013810157776
translation,168,150,results,more significant,for,more abstractive corpus gigaword and xsum,more significant for more abstractive corpus gigaword and xsum,0.6235799789428711
translation,168,150,results,more significant,indicating,our model,more significant indicating our model,0.7041879296302795
translation,168,150,results,our model,effective at identifying,more latent alignment relations,our model effective at identifying more latent alignment relations,0.6391337513923645
translation,168,150,results,results,has,improvement,results has improvement,0.6248279809951782
translation,168,154,results,standard pointer generators,in,cnn / dm and gigaword,standard pointer generators in cnn / dm and gigaword,0.5196005702018738
translation,168,154,results,generation mode,has,outperforms,generation mode has outperforms,0.6539007425308228
translation,168,154,results,outperforms,has,standard pointer generators,outperforms has standard pointer generators,0.5588198304176331
translation,168,154,results,results,without,generation mode,results without generation mode,0.7222349643707275
translation,168,180,results,gpg model,enables,point mode,gpg model enables point mode,0.7373563051223755
translation,168,180,results,point mode,than,standord pointer generators,point mode than standord pointer generators,0.5498329401016235
translation,168,180,results,point mode,especially on,gigaword dataset,point mode especially on gigaword dataset,0.5767567753791809
translation,168,180,results,more frequently,than,standord pointer generators,more frequently than standord pointer generators,0.56855708360672
translation,168,180,results,point mode,has,more frequently,point mode has more frequently,0.5923341512680054
translation,168,180,results,results,find,gpg model,results find gpg model,0.5807855129241943
translation,168,199,results,gpg,generates,over half,gpg generates over half,0.695058286190033
translation,168,199,results,over half,of,target words,over half of target words,0.6223495006561279
translation,168,199,results,results,has,gpg,results has gpg,0.5624361634254456
translation,168,216,results,gpg,improves,alignment precision,gpg improves alignment precision,0.7262397408485413
translation,168,216,results,alignment precision,by,0.1,alignment precision by 0.1,0.5420598387718201
translation,168,216,results,alignment precision,compared with,standard pointer generator,alignment precision compared with standard pointer generator,0.607210099697113
translation,168,216,results,0.1,compared with,standard pointer generator,0.1 compared with standard pointer generator,0.6190782785415649
translation,168,216,results,results,show,gpg,results show gpg,0.6412053108215332
translation,168,217,results,posterior alignment,is,more accurate,posterior alignment is more accurate,0.5560636520385742
translation,168,217,results,posterior alignment,enabling,better human interpretation,posterior alignment enabling better human interpretation,0.7056071758270264
translation,168,217,results,more accurate,than,prior one,more accurate than prior one,0.6281994581222534
translation,168,217,results,more accurate,enabling,better human interpretation,more accurate enabling better human interpretation,0.7360649108886719
translation,168,217,results,results,has,posterior alignment,results has posterior alignment,0.48447734117507935
translation,169,129,baselines,textrank,where,sentence embeddings,textrank where sentence embeddings,0.5610606670379639
translation,169,129,baselines,sentence embeddings,used instead of,bag-ofwords representations,sentence embeddings used instead of bag-ofwords representations,0.5584044456481934
translation,169,129,baselines,graph- based unsupervised sentence extraction method,has,textrank,graph- based unsupervised sentence extraction method has textrank,0.5331324338912964
translation,169,133,baselines,novel unsupervised baseline model meansum-single,is,unsupervised neural multi-document summarization model,novel unsupervised baseline model meansum-single is unsupervised neural multi-document summarization model,0.5253333449363708
translation,169,133,baselines,novel unsupervised baseline model meansum-single,extended version of,unsupervised neural multi-document summarization model,novel unsupervised baseline model meansum-single extended version of unsupervised neural multi-document summarization model,0.5656999945640564
translation,169,137,baselines,vanilla neural sequence - to-sequence models,for,abstractive summarization,vanilla neural sequence - to-sequence models for abstractive summarization,0.5175514817237854
translation,169,138,baselines,model,as,seq-seq,model as seq-seq,0.5561907291412354
translation,169,138,baselines,model,with,attention mechanism,model with attention mechanism,0.594793438911438
translation,169,138,baselines,attention mechanism,as,seq-seq-att,attention mechanism as seq-seq-att,0.5535328388214111
translation,169,170,baselines,stratt,induces,single-root trees,stratt induces single-root trees,0.7138972282409668
translation,169,170,baselines,single-root trees,via,document classification task,single-root trees via document classification task,0.626740574836731
translation,169,170,baselines,document classification task,based on,review ratings,document classification task based on review ratings,0.5952199697494507
translation,169,170,baselines,baselines,has,stratt,baselines has stratt,0.6796496510505676
translation,169,143,experiments,our model,is,competitive,our model is competitive,0.6213095188140869
translation,169,143,experiments,competitive,with,other unsupervised extractive approaches,competitive with other unsupervised extractive approaches,0.6500754952430725
translation,169,143,experiments,competitive,with,textrank,competitive with textrank,0.6781507134437561
translation,169,143,experiments,competitive,with,opinosis,competitive with opinosis,0.6789627075195312
translation,169,143,experiments,movies & tv,has,our model,movies & tv has our model,0.5875738859176636
translation,169,119,hyperparameters,300 - dimensional word embeddings,initialize them with,pre-trained fast - text vectors,300 - dimensional word embeddings initialize them with pre-trained fast - text vectors,0.574499785900116
translation,169,119,hyperparameters,hyperparameters,set,300 - dimensional word embeddings,hyperparameters set 300 - dimensional word embeddings,0.5543478727340698
translation,169,121,hyperparameters,damping factor,of,discourserank,damping factor of discourserank,0.5749669075012207
translation,169,121,hyperparameters,discourserank,is,0.9,discourserank is 0.9,0.5833432674407959
translation,169,121,hyperparameters,hyperparameters,has,damping factor,hyperparameters has damping factor,0.4970376789569855
translation,169,122,hyperparameters,model,using,ada-grad,model using ada-grad,0.690617561340332
translation,169,122,hyperparameters,ada-grad,with,learning rate,ada-grad with learning rate,0.6621796488761902
translation,169,122,hyperparameters,ada-grad,with,initial accumulator value,ada-grad with initial accumulator value,0.624774694442749
translation,169,122,hyperparameters,ada-grad,with,batch size,ada-grad with batch size,0.6870545744895935
translation,169,122,hyperparameters,learning rate,of,10 ?1,learning rate of 10 ?1,0.6380672454833984
translation,169,122,hyperparameters,learning rate,of,10 ?1,learning rate of 10 ?1,0.6380672454833984
translation,169,122,hyperparameters,initial accumulator value,of,10 ?1,initial accumulator value of 10 ?1,0.6323288679122925
translation,169,122,hyperparameters,batch size,of,16,batch size of 16,0.6842944622039795
translation,169,122,hyperparameters,hyperparameters,train,model,hyperparameters train model,0.7064553499221802
translation,169,123,hyperparameters,beam search,with,beam size,beam search with beam size,0.6320963501930237
translation,169,123,hyperparameters,beam size,of,10,beam size of 10,0.7020634412765503
translation,169,123,hyperparameters,evaluation time,has,beam search,evaluation time has beam search,0.5436726808547974
translation,169,123,hyperparameters,hyperparameters,At,evaluation time,hyperparameters At evaluation time,0.48084113001823425
translation,169,5,model,review,described as,discourse tree,review described as discourse tree,0.5563884377479553
translation,169,5,model,discourse tree,in which,summary,discourse tree in which summary,0.6562731266021729
translation,169,5,model,summary,is,root,summary is root,0.5619710087776184
translation,169,5,model,child sentences,explain,parent,child sentences explain parent,0.6843078136444092
translation,169,5,model,model,assume,review,model assume review,0.6866596937179565
translation,169,6,model,parent,from,children,parent from children,0.5616443157196045
translation,169,6,model,our model,learns,latent discourse tree,our model learns latent discourse tree,0.6677581071853638
translation,169,6,model,our model,generates,concise summary,our model generates concise summary,0.6496965885162354
translation,169,6,model,latent discourse tree,without,external parser,latent discourse tree without external parser,0.7083273530006409
translation,169,6,model,parent,has,our model,parent has our model,0.6286166906356812
translation,169,6,model,children,has,our model,children has our model,0.6298871636390686
translation,169,6,model,model,By recursively estimating,parent,model By recursively estimating parent,0.7021320462226868
translation,169,7,model,architecture,ranks,importance,architecture ranks importance,0.7351957559585571
translation,169,7,model,importance,of,each sentence,importance of each sentence,0.5860390067100525
translation,169,7,model,each sentence,on,tree,each sentence on tree,0.5522312521934509
translation,169,7,model,tree,to support,summary generation,tree to support summary generation,0.6473362445831299
translation,169,7,model,summary generation,focusing on,main review point,summary generation focusing on main review point,0.7484056353569031
translation,169,39,model,summary,propose,novel architecture,summary propose novel architecture,0.7077327966690063
translation,169,39,model,novel architecture,has,strsum,novel architecture has strsum,0.6477643847465515
translation,169,40,model,parent,from,children,parent from children,0.5616443157196045
translation,169,40,model,latent discourse tree,without,parser,latent discourse tree without parser,0.6990715265274048
translation,169,40,model,children,has,recursively,children has recursively,0.6916633248329163
translation,169,40,model,model,reconstructs,parent,model reconstructs parent,0.7771244645118713
translation,169,41,model,our model,generates,summary,our model generates summary,0.6559346914291382
translation,169,41,model,summary,from,surrounding sentences,summary from surrounding sentences,0.5937793254852295
translation,169,41,model,surrounding sentences,of,root,surrounding sentences of root,0.5929407477378845
translation,169,41,model,root,while learning,language model,root while learning language model,0.6795364022254944
translation,169,41,model,language model,through,reconstruction,language model through reconstruction,0.6317412257194519
translation,169,41,model,reconstruction,in,endto-end manner,reconstruction in endto-end manner,0.5496575832366943
translation,169,41,model,model,generates,summary,model generates summary,0.6325527429580688
translation,169,41,model,model,has,our model,model has our model,0.5983551740646362
translation,169,42,model,discourserank,ranks,importance,discourserank ranks importance,0.7279292345046997
translation,169,42,model,importance,of,each sentence,importance of each sentence,0.5860390067100525
translation,169,42,model,each sentence,in terms of,number of descendants,each sentence in terms of number of descendants,0.6681450605392456
translation,169,43,model,strsum,to generate,summary,strsum to generate summary,0.6888163685798645
translation,169,43,model,summary,focuses on,main review point,summary focuses on main review point,0.753581702709198
translation,169,43,model,model,supports,strsum,model supports strsum,0.6732485890388489
translation,169,120,model,encoder,is,single- layer bi-gru,encoder is single- layer bi-gru,0.5372961163520813
translation,169,120,model,encoder,is,uni-directional gru,encoder is uni-directional gru,0.578101634979248
translation,169,120,model,single- layer bi-gru,with,256 - dimensional hidden states,single- layer bi-gru with 256 - dimensional hidden states,0.6088971495628357
translation,169,120,model,256 - dimensional hidden states,for,each direction,256 - dimensional hidden states for each direction,0.5984322428703308
translation,169,120,model,uni-directional gru,with,256 - dimensional hidden states,uni-directional gru with 256 - dimensional hidden states,0.6077965497970581
translation,169,120,model,model,has,encoder,model has encoder,0.5940273404121399
translation,169,130,model,unsupervised word- level extractive approach,employ,opinosis,unsupervised word- level extractive approach employ opinosis,0.6115612387657166
translation,169,130,model,opinosis,detects,salient phrases,opinosis detects salient phrases,0.7478739023208618
translation,169,130,model,salient phrases,in terms of,redundancy,salient phrases in terms of redundancy,0.6890140771865845
translation,169,130,model,model,As,unsupervised word- level extractive approach,model As unsupervised word- level extractive approach,0.5158572793006897
translation,169,134,model,multiple document embeddings,to generate,summary,multiple document embeddings to generate summary,0.6329871416091919
translation,169,134,model,meansum-single,generates,single- document summary,meansum-single generates single- document summary,0.6868131756782532
translation,169,134,model,sentence embeddings,has,in a document,sentence embeddings has in a document,0.5705029964447021
translation,169,134,model,model,decodes,meansum-single,model decodes meansum-single,0.7986261248588562
translation,169,141,results,abstractive approach,generates,concise summary,abstractive approach generates concise summary,0.6350399255752563
translation,169,141,results,concise summary,by omitting,trivial phrases,concise summary by omitting trivial phrases,0.5101360082626343
translation,169,141,results,concise summary,lead to,better performance,concise summary lead to better performance,0.6650901436805725
translation,169,141,results,better performance,than,extractive ones,better performance than extractive ones,0.6039502620697021
translation,169,145,results,all datasets,has,our full model,all datasets has our full model,0.5759081840515137
translation,169,145,results,our full model,has,outperforms,our full model has outperforms,0.633263111114502
translation,169,145,results,outperforms,has,one using only strsum,outperforms has one using only strsum,0.6048112511634827
translation,169,145,results,results,For,all datasets,results For all datasets,0.5338320732116699
translation,169,146,results,our models,has,significantly outperform,our models has significantly outperform,0.5940237045288086
translation,169,146,results,significantly outperform,has,meansum-single,significantly outperform has meansum-single,0.6195910573005676
translation,169,146,results,results,has,our models,results has our models,0.5733726620674133
translation,169,147,results,rouge -l f1 scores,of,our models,rouge -l f1 scores of our models,0.5512660145759583
translation,169,147,results,our models,on,evaluation sets,our models on evaluation sets,0.5461882948875427
translation,169,147,results,evaluation sets,with,various numbers of sentences,evaluation sets with various numbers of sentences,0.5612474679946899
translation,169,147,results,various numbers of sentences,compared to,supervised baseline model ( seq - seq-att ),various numbers of sentences compared to supervised baseline model ( seq - seq-att ),0.6022233963012695
translation,169,147,results,results,shows,rouge -l f1 scores,results shows rouge -l f1 scores,0.6159038543701172
translation,169,148,results,dataset,with,less than 30 sentences,dataset with less than 30 sentences,0.6105289459228516
translation,169,148,results,performance,of,our models,performance of our models,0.5822860598564148
translation,169,148,results,performance,inferior to,supervised baseline model,performance inferior to supervised baseline model,0.662569522857666
translation,169,148,results,our models,inferior to,supervised baseline model,our models inferior to supervised baseline model,0.6052284240722656
translation,169,148,results,dataset,has,performance,dataset has performance,0.5893756151199341
translation,169,148,results,less than 30 sentences,has,performance,less than 30 sentences has performance,0.6055367588996887
translation,169,148,results,results,For the case of,dataset,results For the case of dataset,0.6651798486709595
translation,169,150,results,datasets,with,number of sentences,datasets with number of sentences,0.5548726916313171
translation,169,150,results,number of sentences,exceeding,30,number of sentences exceeding 30,0.6579127311706543
translation,169,150,results,our model,achieves,competitive or better performance,our model achieves competitive or better performance,0.660574197769165
translation,169,150,results,competitive or better performance,than,supervised model,competitive or better performance than supervised model,0.547964870929718
translation,169,150,results,datasets,has,our model,datasets has our model,0.5864376425743103
translation,169,150,results,number of sentences,has,our model,number of sentences has our model,0.5329243540763855
translation,169,150,results,results,for,datasets,results for datasets,0.5882673859596252
translation,170,207,ablation-analysis,supervised cross-lingual summarization,except for,mappers,supervised cross-lingual summarization except for mappers,0.5930178761482239
translation,170,207,ablation-analysis,all components,contribute to,improvement,all components contribute to improvement,0.7379674315452576
translation,170,207,ablation-analysis,improvement,of,performance,improvement of performance,0.6004069447517395
translation,170,207,ablation-analysis,supervised cross-lingual summarization,has,all components,supervised cross-lingual summarization has all components,0.5506304502487183
translation,170,207,ablation-analysis,mappers,has,all components,mappers has all components,0.5943925976753235
translation,170,207,ablation-analysis,ablation analysis,For,supervised cross-lingual summarization,ablation analysis For supervised cross-lingual summarization,0.5286657810211182
translation,170,208,ablation-analysis,performance,removing,any of the components,performance removing any of the components,0.6949225068092346
translation,170,208,ablation-analysis,decreases,removing,any of the components,decreases removing any of the components,0.7485913634300232
translation,170,208,ablation-analysis,performance,has,decreases,performance has decreases,0.599746823310852
translation,170,208,ablation-analysis,ablation analysis,has,performance,ablation analysis has performance,0.5053174495697021
translation,170,140,baselines,baselines,For,unsupervised cross-lingual summarization,baselines For unsupervised cross-lingual summarization,0.5578712224960327
translation,170,148,baselines,pipe -ts baseline,uses,transformer - based translation model,pipe -ts baseline uses transformer - based translation model,0.5831739902496338
translation,170,148,baselines,pipe -ts baseline,uses,monolingual summarization model,pipe -ts baseline uses monolingual summarization model,0.5265195369720459
translation,170,148,baselines,pipe -ts baseline,uses,monolingual summarization model,pipe -ts baseline uses monolingual summarization model,0.5265195369720459
translation,170,148,baselines,transformer - based translation model,to translate,source text,transformer - based translation model to translate source text,0.6655769348144531
translation,170,148,baselines,source text,to,other language,source text to other language,0.5492234230041504
translation,170,148,baselines,monolingual summarization model,to generate,summaries,monolingual summarization model to generate summaries,0.6388081908226013
translation,170,148,baselines,baselines,has,pipe -ts baseline,baselines has pipe -ts baseline,0.6293913722038269
translation,170,152,baselines,pseudo baseline,directly trains,cross-lingual summarization model,pseudo baseline directly trains cross-lingual summarization model,0.690325915813446
translation,170,152,baselines,cross-lingual summarization model,by using,pseudo parallel cross-lingual summarization data,cross-lingual summarization model by using pseudo parallel cross-lingual summarization data,0.6299206018447876
translation,170,152,baselines,baselines,has,pseudo baseline,baselines has pseudo baseline,0.6200549602508545
translation,170,144,experimental-setup,cross-lingual word embeddings,obtained via,projecting embeddings,cross-lingual word embeddings obtained via projecting embeddings,0.5796483159065247
translation,170,144,experimental-setup,projecting embeddings,from,source language,projecting embeddings from source language,0.5727958679199219
translation,170,144,experimental-setup,experimental setup,has,cross-lingual word embeddings,experimental setup has cross-lingual word embeddings,0.5166294574737549
translation,170,145,experimental-setup,vecmap,to learn,cross-lingual word embeddings,vecmap to learn cross-lingual word embeddings,0.6227112412452698
translation,170,145,experimental-setup,experimental setup,use,vecmap,experimental setup use vecmap,0.5987799763679504
translation,170,159,experimental-setup,mapper,is,linear layer,mapper is linear layer,0.5689897537231445
translation,170,159,experimental-setup,mapper,is,two -layer linear layer,mapper is two -layer linear layer,0.5475456118583679
translation,170,159,experimental-setup,linear layer,with,hidden size,linear layer with hidden size,0.6463255882263184
translation,170,159,experimental-setup,linear layer,with,hidden size,linear layer with hidden size,0.6463255882263184
translation,170,159,experimental-setup,linear layer,with,hidden size,linear layer with hidden size,0.6463255882263184
translation,170,159,experimental-setup,hidden size,of,512,hidden size of 512,0.640839695930481
translation,170,159,experimental-setup,hidden size,of,2048,hidden size of 2048,0.6565379500389099
translation,170,159,experimental-setup,hidden size,of,2048,hidden size of 2048,0.6565379500389099
translation,170,159,experimental-setup,discriminator,is,two -layer linear layer,discriminator is two -layer linear layer,0.5048348307609558
translation,170,159,experimental-setup,two -layer linear layer,with,hidden size,two -layer linear layer with hidden size,0.6402673125267029
translation,170,159,experimental-setup,hidden size,of,2048,hidden size of 2048,0.6565379500389099
translation,170,159,experimental-setup,experimental setup,has,mapper,experimental setup has mapper,0.5707632899284363
translation,170,159,experimental-setup,experimental setup,has,discriminator,experimental setup has discriminator,0.5227228403091431
translation,170,163,experimental-setup,experimental setup,set,dis iters,experimental setup set dis iters,0.6672784090042114
translation,170,165,experimental-setup,warm - up learning rate,for,"first 2,000 steps","warm - up learning rate for first 2,000 steps",0.6083410382270813
translation,170,165,experimental-setup,initial warm - up learning,set to,1e ? 7,initial warm - up learning set to 1e ? 7,0.7421757578849792
translation,170,165,experimental-setup,experimental setup,set,learning rate,experimental setup set learning rate,0.6158276796340942
translation,170,165,experimental-setup,experimental setup,adopt,warm - up learning rate,experimental setup adopt warm - up learning rate,0.6360999941825867
translation,170,166,experimental-setup,dropout technique,set,dropout rate,dropout technique set dropout rate,0.6334943771362305
translation,170,166,experimental-setup,dropout rate,to,0.2,dropout rate to 0.2,0.5248656868934631
translation,170,166,experimental-setup,experimental setup,adopt,dropout technique,experimental setup adopt dropout technique,0.6496989130973816
translation,170,133,experiments,lcsts lcsts,is,chinese summarization corpus,lcsts lcsts is chinese summarization corpus,0.49494487047195435
translation,170,133,experiments,chinese summarization corpus,contains,2.40 m training pairs,chinese summarization corpus contains 2.40 m training pairs,0.5451231002807617
translation,170,133,experiments,chinese summarization corpus,contains,"10,666 validation pairs","chinese summarization corpus contains 10,666 validation pairs",0.5603033304214478
translation,170,133,experiments,chinese summarization corpus,contains,725 test pairs,chinese summarization corpus contains 725 test pairs,0.5648956894874573
translation,170,160,experiments,tool,to process,english texts,tool to process english texts,0.6939967274665833
translation,170,160,experiments,tool,to process,chinese texts,tool to process chinese texts,0.7049919366836548
translation,170,160,experiments,tool,to process,chinese texts,tool to process chinese texts,0.7049919366836548
translation,170,160,experiments,nltk,has,tool,nltk has tool,0.6450887322425842
translation,170,160,experiments,jieba,has,tool,jieba has tool,0.6275922060012817
translation,170,164,experiments,adam optimizer,with,"? = ( 0.9 , 0.98 )","adam optimizer with ? = ( 0.9 , 0.98 )",0.6057172417640686
translation,170,164,experiments,"? = ( 0.9 , 0.98 )",for,optimization,"? = ( 0.9 , 0.98 ) for optimization",0.6217607855796814
translation,170,7,model,cross-lingual summarization training,by jointly learning,align and summarize,cross-lingual summarization training by jointly learning align and summarize,0.7380453944206238
translation,170,7,model,model,ease,cross-lingual summarization training,model ease cross-lingual summarization training,0.6423372030258179
translation,170,24,model,model,show,learning of cross-lingual representations,model show learning of cross-lingual representations,0.5570164322853088
translation,170,25,model,multi-task framework,jointly learns to,summarize and align,multi-task framework jointly learns to summarize and align,0.7550644874572754
translation,170,25,model,summarize and align,has,context- level representations,summarize and align has context- level representations,0.5250352025032043
translation,170,25,model,model,propose,multi-task framework,model propose multi-task framework,0.6705433130264282
translation,170,26,model,cross-lingual summarization models,into,one unified model,cross-lingual summarization models into one unified model,0.5616089701652527
translation,170,26,model,two linear mappings,to project,context representation,two linear mappings to project context representation,0.6710666418075562
translation,170,26,model,context representation,from,one language,context representation from one language,0.5664150714874268
translation,170,26,model,model,integrate,monolingual summarization models,model integrate monolingual summarization models,0.6136178970336914
translation,170,26,model,model,build,two linear mappings,model build two linear mappings,0.6977032423019409
translation,170,67,model,encoders and decoders,in,our model,encoders and decoders in our model,0.5234062075614929
translation,170,67,model,model,share,encoders and decoders,model share encoders and decoders,0.7310887575149536
translation,170,141,model,english and chinese monolingual summarizations,in,unified model,english and chinese monolingual summarizations in unified model,0.4973643124103546
translation,170,141,model,first token,of,decoder,first token of decoder,0.6277037262916565
translation,170,141,model,model,jointly trains,english and chinese monolingual summarizations,model jointly trains english and chinese monolingual summarizations,0.7611355185508728
translation,170,141,model,model,uses,first token,model uses first token,0.650032103061676
translation,170,143,model,model,builds,unified model,model builds unified model,0.640487015247345
translation,170,224,model,jointly learns to align and summarize,for,neural crosslingual summarization,jointly learns to align and summarize for neural crosslingual summarization,0.571643054485321
translation,170,224,model,model,propose,framework,model propose framework,0.666053295135498
translation,170,225,model,training objectives,for,supervised and unsupervised cross-lingual summarizations,training objectives for supervised and unsupervised cross-lingual summarizations,0.5804424285888672
translation,170,225,model,model,design,training objectives,model design training objectives,0.6603854894638062
translation,170,169,results,significantly outperforms,by,large margin,significantly outperforms by large margin,0.6154928207397461
translation,170,169,results,all baselines,by,large margin,all baselines by large margin,0.5779300332069397
translation,170,169,results,significantly outperforms,has,all baselines,significantly outperforms has all baselines,0.5895546674728394
translation,170,170,results,unified model,of,all languages,unified model of all languages,0.5549322962760925
translation,170,170,results,model 's crosslingual transferability,still,poor,model 's crosslingual transferability still poor,0.6876998543739319
translation,170,170,results,poor,especially for,gigaword dataset,poor especially for gigaword dataset,0.6001249551773071
translation,170,170,results,unified model,has,model 's crosslingual transferability,unified model has model 's crosslingual transferability,0.5610564351081848
translation,170,170,results,all languages,has,model 's crosslingual transferability,all languages has model 's crosslingual transferability,0.6000143885612488
translation,170,170,results,results,By training,unified model,results By training unified model,0.7274199724197388
translation,170,171,results,cross-lingual word embeddings,into,unified model,cross-lingual word embeddings into unified model,0.5389235615730286
translation,170,171,results,improve,has,performance,improve has performance,0.5578044652938843
translation,170,171,results,results,Incorporating,cross-lingual word embeddings,results Incorporating cross-lingual word embeddings,0.5998001098632812
translation,170,176,results,summaries,generated by,our model,summaries generated by our model,0.6668679714202881
translation,170,176,results,summaries,are,more relevant,summaries are more relevant,0.5798313617706299
translation,170,176,results,summaries,obviously,more relevant,summaries obviously more relevant,0.6259931325912476
translation,170,176,results,our model,are,more relevant,our model are more relevant,0.5628487467765808
translation,170,176,results,results,find that,summaries,results find that summaries,0.6159741878509521
translation,170,177,results,gap,be - tween,our unsupervised results,gap be - tween our unsupervised results,0.6256101131439209
translation,170,177,results,gap,be - tween,supervised results,gap be - tween supervised results,0.6115158796310425
translation,170,180,results,pipeline - based or pseudo baselines,find that,our model,pipeline - based or pseudo baselines find that our model,0.5788935422897339
translation,170,180,results,outperforms,in,all cases,outperforms in all cases,0.5688782334327698
translation,170,180,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,170,180,results,outperforms,has,all these baselines,outperforms has all these baselines,0.5892565250396729
translation,170,180,results,results,with,pipeline - based or pseudo baselines,results with pipeline - based or pseudo baselines,0.6048136353492737
translation,170,181,results,our model,achieves,improvement,our model achieves improvement,0.6709721684455872
translation,170,181,results,improvement,of,0?3,improvement of 0?3,0.635613739490509
translation,170,181,results,results,has,our model,results has our model,0.5871725678443909
translation,170,183,results,models,using,google translation system,models using google translation system,0.6561181545257568
translation,170,183,results,models,using,transformer - based translation system,models using transformer - based translation system,0.6697477102279663
translation,170,183,results,models,using,transformer - based translation system,models using transformer - based translation system,0.6697477102279663
translation,170,183,results,perform better,than,models,perform better than models,0.6039217710494995
translation,170,183,results,models,using,transformer - based translation system,models using transformer - based translation system,0.6697477102279663
translation,170,183,results,results,observe,models,results observe models,0.5709750056266785
translation,170,185,results,perform better,than,pipe -ts models,perform better than pipe -ts models,0.6006866097450256
translation,170,185,results,pipe -st models,has,perform better,pipe -st models has perform better,0.5826675295829773
translation,170,185,results,results,has,pipe -st models,results has pipe -st models,0.486725777387619
translation,170,187,results,pseudo model,performs,better,pseudo model performs better,0.6594468355178833
translation,170,187,results,pseudo model,performs,similarly,pseudo model performs similarly,0.7086586952209473
translation,170,187,results,better,than,pipe -ts models,better than pipe -ts models,0.5777490139007568
translation,170,187,results,similarly,as,pipe -st models,similarly as pipe -st models,0.6170229911804199
translation,170,187,results,results,has,pseudo model,results has pseudo model,0.5502287745475769
translation,170,188,results,outperforms,on,gigaword and duc2004 test sets,outperforms on gigaword and duc2004 test sets,0.46595674753189087
translation,170,188,results,outperforms,on,lcsts dataset,outperforms on lcsts dataset,0.5241865515708923
translation,170,188,results,outperforms,on,lcsts dataset,outperforms on lcsts dataset,0.5241865515708923
translation,170,188,results,outperforms,on,lcsts dataset,outperforms on lcsts dataset,0.5241865515708923
translation,170,188,results,zhu et al . ( 2019 ),on,lcsts dataset,zhu et al . ( 2019 ) on lcsts dataset,0.535186767578125
translation,170,188,results,model,has,outperforms,model has outperforms,0.675282895565033
translation,170,188,results,outperforms,has,shen et al . ( 2018 ),outperforms has shen et al . ( 2018 ),0.5373047590255737
translation,170,188,results,outperforms,has,duan et al . ( 2019 ),outperforms has duan et al . ( 2019 ),0.5520563125610352
translation,170,188,results,outperforms,has,zhu et al . ( 2019 ),outperforms has zhu et al . ( 2019 ),0.5358496308326721
translation,170,188,results,results,find that,model,results find that model,0.6360844373703003
translation,170,197,results,our model,exceeds,all baselines,our model exceeds all baselines,0.6068196892738342
translation,170,197,results,our model,get,slightly lower fluency score,our model get slightly lower fluency score,0.5906055569648743
translation,170,197,results,all baselines,in,informative and conciseness scores,all baselines in informative and conciseness scores,0.4721129834651947
translation,170,197,results,slightly lower fluency score,than,pipe -st *,slightly lower fluency score than pipe -st *,0.5863658785820007
translation,170,197,results,results,has,our model,results has our model,0.5871725678443909
translation,170,237,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,170,237,results,outperforms,has,all baselines significantly ( p < 0.01 ),outperforms has all baselines significantly ( p < 0.01 ),0.5866714715957642
translation,170,237,results,results,has,our model,results has our model,0.5871725678443909
translation,171,93,baselines,two models,adding,similarity component,two models adding similarity component,0.7130282521247864
translation,171,93,baselines,strong baseline,based on,two nearest neighbors,strong baseline based on two nearest neighbors,0.6448455452919006
translation,171,93,baselines,strong baseline,create,link,strong baseline create link,0.6585313677787781
translation,171,93,baselines,link,between,images and texts,link between images and texts,0.6897372603416443
translation,171,93,baselines,similarity component,to,our model zest vanilla,similarity component to our model zest vanilla,0.5593003630638123
translation,171,93,baselines,two models,has,strong baseline,two models has strong baseline,0.575668454170227
translation,171,93,baselines,baselines,propose,two models,baselines propose two models,0.6905028820037842
translation,171,151,baselines,density - based spatial clustering of applications with noise,has,dbscan ),density - based spatial clustering of applications with noise has dbscan ),0.6111359596252441
translation,171,215,baselines,zest vanilla + vrs and zest similarity + vrs,use,captions,zest vanilla + vrs and zest similarity + vrs use captions,0.6322550177574158
translation,171,215,baselines,captions,from,training images,captions from training images,0.4950888752937317
translation,171,215,baselines,training images,in,cub,training images in cub,0.5295506715774536
translation,171,215,baselines,visually relevant extractive summaries,of,original wikipedia documents,visually relevant extractive summaries of original wikipedia documents,0.533410370349884
translation,171,215,baselines,baselines,has,zest vanilla + vrs and zest similarity + vrs,baselines has zest vanilla + vrs and zest similarity + vrs,0.5767197608947754
translation,171,77,experiments,image encoder,for,text - based zsl,image encoder for text - based zsl,0.617343008518219
translation,171,158,hyperparameters,weights w,initialized with,normalized initialization,weights w initialized with normalized initialization,0.7904903888702393
translation,171,158,hyperparameters,hyperparameters,has,weights w,hyperparameters has weights w,0.5098131895065308
translation,171,159,hyperparameters,crossentropy loss function,optimized with,adam optimizer,crossentropy loss function optimized with adam optimizer,0.6995081305503845
translation,171,159,hyperparameters,hyperparameters,has,crossentropy loss function,hyperparameters has crossentropy loss function,0.49128174781799316
translation,171,5,model,birds ' images,with,free-text descriptions,birds ' images with free-text descriptions,0.5938554406166077
translation,171,5,model,birds ' images,classify,images,birds ' images classify images,0.6732863783836365
translation,171,5,model,free-text descriptions,of,their species,free-text descriptions of their species,0.49750247597694397
translation,171,5,model,images,of,previously - unseen species,images of previously - unseen species,0.5947664976119995
translation,171,5,model,previously - unseen species,based on,specie descriptions,previously - unseen species based on specie descriptions,0.5861849188804626
translation,171,5,model,model,given,birds ' images,model given birds ' images,0.6833410263061523
translation,171,9,model,visual summaries,of,texts,visual summaries of texts,0.6095961928367615
translation,171,9,model,visual summaries,i.e.,extractive summaries,visual summaries i.e. extractive summaries,0.6158179044723511
translation,171,9,model,extractive summaries,focus on,visual features,extractive summaries focus on visual features,0.747175931930542
translation,171,9,model,visual features,reflected in,images,visual features reflected in images,0.7026402950286865
translation,171,9,model,model,derive,visual summaries,model derive visual summaries,0.6530101299285889
translation,171,10,model,simple attention - based model,augmented with,similarity and visual summaries components,simple attention - based model augmented with similarity and visual summaries components,0.7106493711471558
translation,171,10,model,model,propose,simple attention - based model,model propose simple attention - based model,0.6680131554603577
translation,171,31,model,similarity feature,enhances,text descriptions ' separability,similarity feature enhances text descriptions ' separability,0.6119582653045654
translation,171,31,model,model,encode,similarity feature,model encode similarity feature,0.7659128308296204
translation,171,35,results,ratio improvement,of,up to 48.16 %,ratio improvement of up to 48.16 %,0.574823260307312
translation,171,35,results,up to 48.16 %,over,state - of - the- art,up to 48.16 % over state - of - the- art,0.6627466082572937
translation,171,36,results,visual -summarization method,generalizes,"cub dataset ( wah et al. , 2011 )","visual -summarization method generalizes cub dataset ( wah et al. , 2011 )",0.7050137519836426
translation,171,36,results,visual -summarization method,demonstrate,contribution,visual -summarization method demonstrate contribution,0.5988762974739075
translation,171,36,results,"cub dataset ( wah et al. , 2011 )",to,nab dataset,"cub dataset ( wah et al. , 2011 ) to nab dataset",0.5258335471153259
translation,171,36,results,contribution,to,additional models,contribution to additional models,0.5494398474693298
translation,171,36,results,additional models,by,ratio improvement,additional models by ratio improvement,0.5931096076965332
translation,171,36,results,ratio improvement,of,up to 59.62 %,ratio improvement of up to 59.62 %,0.5738641023635864
translation,171,36,results,results,show,visual -summarization method,results show visual -summarization method,0.5941643714904785
translation,171,189,results,all previous methods,on,sce - split,all previous methods on sce - split,0.542052686214447
translation,171,189,results,vanilla crossentropy based approach,has,outperforms,vanilla crossentropy based approach has outperforms,0.606330394744873
translation,171,189,results,outperforms,has,all previous methods,outperforms has all previous methods,0.573925256729126
translation,171,189,results,cub,has,+ 14.27 % ratio of improvement,cub has + 14.27 % ratio of improvement,0.5883055925369263
translation,171,189,results,nab,has,+ 18.37 % ratio of improvement,nab has + 18.37 % ratio of improvement,0.5757692456245422
translation,171,207,results,dbscan,achieved,88 % and 84.5 % accuracy,dbscan achieved 88 % and 84.5 % accuracy,0.6846180558204651
translation,171,207,results,dbscan,achieved,93.07 % and 95.05 %,dbscan achieved 93.07 % and 95.05 %,0.6874047517776489
translation,171,207,results,88 % and 84.5 % accuracy,on,cub,88 % and 84.5 % accuracy on cub,0.4989140033721924
translation,171,207,results,93.07 % and 95.05 %,on,nab,93.07 % and 95.05 % on nab,0.5658785104751587
translation,171,207,results,results,has,hdb - scan,results has hdb - scan,0.523543119430542
translation,171,207,results,results,has,dbscan,results has dbscan,0.5679013133049011
translation,171,211,results,two clusters,that capture,different similarities,two clusters that capture different similarities,0.6853284239768982
translation,171,211,results,two clusters,performs,better,two clusters performs better,0.6162402629852295
translation,171,211,results,better,than,embedding,better than embedding,0.6173689961433411
translation,171,211,results,better,by,ratio improvement,better by ratio improvement,0.5891135334968567
translation,171,211,results,embedding,by,ratio improvement,embedding by ratio improvement,0.5678049325942993
translation,171,211,results,bird category,in,text representation,bird category in text representation,0.482866495847702
translation,171,211,results,ratio improvement,of,up to 8.63 %,ratio improvement of up to 8.63 %,0.5831148028373718
translation,171,211,results,embedding,has,bird category,embedding has bird category,0.5602860450744629
translation,171,211,results,results,use of,two clusters,results use of two clusters,0.661589503288269
translation,171,213,results,zest similarity,in,gzsl setup,zest similarity in gzsl setup,0.5780688524246216
translation,171,213,results,results,of,zest similarity,results of zest similarity,0.5531954169273376
translation,171,214,results,zest similarity,achieves,state - of - the - art results,zest similarity achieves state - of - the - art results,0.6652499437332153
translation,171,214,results,state - of - the - art results,with,up to 30.88 % ratio improvement,state - of - the - art results with up to 30.88 % ratio improvement,0.6444835066795349
translation,171,214,results,both datasets and splits,has,zest similarity,both datasets and splits has zest similarity,0.6100918054580688
translation,171,214,results,results,On,both datasets and splits,results On both datasets and splits,0.5117354989051819
translation,171,219,results,improvement,in,accuracy,improvement in accuracy,0.49491560459136963
translation,171,219,results,improvement,in,both models,improvement in both models,0.5501161217689514
translation,171,219,results,accuracy,in,both models,accuracy in both models,0.4923437833786011
translation,171,219,results,accuracy,on,both splits,accuracy on both splits,0.5257662534713745
translation,171,219,results,both models,on,both datasets,both models on both datasets,0.5031692385673523
translation,171,219,results,results,see,improvement,results see improvement,0.6593043208122253
translation,171,227,results,49.4 %,of,sentences,49.4 % of sentences,0.5876705050468445
translation,171,227,results,49.4 %,with,96.23 % recall,49.4 % with 96.23 % recall,0.6542861461639404
translation,171,227,results,49.4 %,with,22.59 % precision,49.4 % with 22.59 % precision,0.6176100969314575
translation,171,227,results,sentences,in,cub dataset,sentences in cub dataset,0.5141221880912781
translation,171,227,results,results,has,vrs method,results has vrs method,0.5371004939079285
translation,171,228,results,49.4 %,of,sentences,49.4 % of sentences,0.5876705050468445
translation,171,228,results,49.4 %,produces,recall,49.4 % produces recall,0.6732653975486755
translation,171,228,results,recall,of,50.6 %,recall of 50.6 %,0.5785727500915527
translation,171,228,results,precision,of,11.9 %,precision of 11.9 %,0.5504490733146667
translation,171,228,results,sentences,has,randomly,sentences has randomly,0.6251732707023621
translation,171,228,results,results,removing,49.4 %,results removing 49.4 %,0.6382492780685425
translation,171,234,results,zest similarity and the gazsl,to the use of,human summarization,zest similarity and the gazsl to the use of human summarization,0.693801999092102
translation,171,234,results,human summarization,in,cub dataset,human summarization in cub dataset,0.4744536876678467
translation,171,234,results,results,compare,zest similarity and the gazsl,results compare zest similarity and the gazsl,0.5986539125442505
translation,171,238,results,vrs,with,five arbitrary captions,vrs with five arbitrary captions,0.6512719392776489
translation,171,238,results,five arbitrary captions,from,cub dataset,five arbitrary captions from cub dataset,0.5380573272705078
translation,171,238,results,five arbitrary captions,on,nab dataset,five arbitrary captions on nab dataset,0.5253588557243347
translation,171,238,results,five arbitrary captions,achieved,39.28 % accuracy,five arbitrary captions achieved 39.28 % accuracy,0.6583099365234375
translation,171,238,results,cub dataset,on,nab dataset,cub dataset on nab dataset,0.4865424633026123
translation,171,238,results,results,Testing,vrs,results Testing vrs,0.740941047668457
translation,172,149,ablation-analysis,jamr parses,large source of,degradation,jamr parses large source of degradation,0.7605342268943787
translation,172,149,ablation-analysis,jamr parses,smaller but still significant source of,degradation,jamr parses smaller but still significant source of degradation,0.7018983960151672
translation,172,149,ablation-analysis,degradation,of,edge prediction performance,degradation of edge prediction performance,0.5592451691627502
translation,172,149,ablation-analysis,degradation,for,concept prediction,degradation for concept prediction,0.6387189030647278
translation,172,149,ablation-analysis,degradation,for,concept prediction,degradation for concept prediction,0.6387189030647278
translation,172,149,ablation-analysis,ablation analysis,find that,jamr parses,ablation analysis find that jamr parses,0.640428900718689
translation,172,5,model,parsed,to,set of amr graphs,parsed to set of amr graphs,0.5664147734642029
translation,172,5,model,graphs,transformed into,summary graph,graphs transformed into summary graph,0.6502264142036438
translation,172,5,model,graphs,transformed into,summary graph,graphs transformed into summary graph,0.6502264142036438
translation,172,5,model,text,generated from,summary graph,text generated from summary graph,0.6754274368286133
translation,172,6,model,graph-tograph transformation,reduces,source semantic graph,graph-tograph transformation reduces source semantic graph,0.5889708399772644
translation,172,6,model,source semantic graph,into,summary graph,source semantic graph into summary graph,0.5270683169364929
translation,172,6,model,model,focus on,graph-tograph transformation,model focus on graph-tograph transformation,0.6812266111373901
translation,172,140,results,oracle performance,on,node prediction,oracle performance on node prediction,0.566211998462677
translation,172,140,results,node prediction,in the range of,80 %,node prediction in the range of 80 %,0.6372923851013184
translation,172,140,results,node prediction,in the range of,70 %,node prediction in the range of 70 %,0.6425186991691589
translation,172,140,results,80 %,when using,gold - standard amr annotations,80 % when using gold - standard amr annotations,0.7120326161384583
translation,172,140,results,70 %,when using,jamr output,70 % when using jamr output,0.7186205983161926
translation,172,140,results,results,has,oracle performance,results has oracle performance,0.5770592093467712
translation,172,141,results,lower performance,yielding,31.1 %,lower performance yielding 31.1 %,0.6547215580940247
translation,172,141,results,52.2 %,for,gold - standard,52.2 % for gold - standard,0.5659654140472412
translation,172,141,results,31.1 %,for,jamr parses,31.1 % for jamr parses,0.6146308779716492
translation,172,141,results,edge prediction,has,lower performance,edge prediction has lower performance,0.5506584048271179
translation,172,141,results,results,has,edge prediction,results has edge prediction,0.5372370481491089
translation,172,150,results,jamr parses,leads to,slightly improved rouge - 1 scores,jamr parses leads to slightly improved rouge - 1 scores,0.6551165580749512
translation,172,150,results,results,using,jamr parses,results using jamr parses,0.620388388633728
translation,172,152,results,65.8 % and 57.8 % f 1 scores,for,gold -standard and jamr parses,65.8 % and 57.8 % f 1 scores for gold -standard and jamr parses,0.5970754623413086
translation,172,152,results,oracle summarization results,has,65.8 % and 57.8 % f 1 scores,oracle summarization results has 65.8 % and 57.8 % f 1 scores,0.5650226473808289
translation,172,152,results,results,has,oracle summarization results,results has oracle summarization results,0.5697163343429565
translation,172,153,results,cost-aware loss function ( hinge vs. perceptron ),has,little effect,cost-aware loss function ( hinge vs. perceptron ) has little effect,0.5320854783058167
translation,172,153,results,ramp loss,leads to,substantial gains,ramp loss leads to substantial gains,0.6700465083122253
translation,172,153,results,cost-aware loss function ( hinge vs. perceptron ),has,little effect,cost-aware loss function ( hinge vs. perceptron ) has little effect,0.5320854783058167
translation,172,155,results,graph expansion,marginally affects,system performance,graph expansion marginally affects system performance,0.7691565155982971
translation,172,155,results,results,find that,graph expansion,results find that graph expansion,0.624098539352417
translation,173,25,baselines,compression - based method,uses,robust sentence compressor,compression - based method uses robust sentence compressor,0.5719618797302246
translation,173,25,baselines,robust sentence compressor,with,aggressive compression rate,robust sentence compressor with aggressive compression rate,0.6319074034690857
translation,173,25,baselines,aggressive compression rate,to get,core,aggressive compression rate to get core,0.6202341318130493
translation,173,25,baselines,baselines,has,compression - based method,baselines has compression - based method,0.5585168600082397
translation,173,154,experimental-setup,parameters,of,network,parameters of network,0.6219789981842041
translation,173,154,experimental-setup,parameters,predefined as,"20,000 nodes","parameters predefined as 20,000 nodes",0.7268962860107422
translation,173,154,experimental-setup,"20,000 nodes",in,hidden layer,"20,000 nodes in hidden layer",0.5114350914955139
translation,173,155,experimental-setup,training,distributed across,20 machines,training distributed across 20 machines,0.7276061773300171
translation,173,155,experimental-setup,20 machines,with,10 gb,20 machines with 10 gb,0.6747915148735046
translation,173,155,experimental-setup,10 gb,of,memory,10 gb of memory,0.5689586997032166
translation,173,155,experimental-setup,experimental setup,has,training,experimental setup has training,0.5312813520431519
translation,173,8,model,every sentence,has,parse sub-trees,every sentence has parse sub-trees,0.6030605435371399
translation,173,8,model,model,introduce,data structure,model introduce data structure,0.6724855303764343
translation,173,8,model,model,introduce,search method,model introduce search method,0.6569533944129944
translation,173,24,model,"two novel , data-driven methods",for,event pattern extraction,"two novel , data-driven methods for event pattern extraction",0.5961074829101562
translation,173,24,model,model,explore,"two novel , data-driven methods","model explore two novel , data-driven methods",0.6834878325462341
translation,173,196,model,memory - based approach,in which,corpus,memory - based approach in which corpus,0.7037380933761597
translation,173,196,model,memory - based approach,use,corpus,memory - based approach use corpus,0.664260745048523
translation,173,196,model,corpus,of,past news,corpus of past news,0.5936499238014221
translation,173,196,model,corpus,to learn,valid syntactic sentence structures,corpus to learn valid syntactic sentence structures,0.5619043707847595
translation,173,196,model,model,describe,memory - based approach,model describe memory - based approach,0.687390923500061
translation,173,168,results,positive outcome,restricting,syntactic structure,positive outcome restricting syntactic structure,0.6590832471847534
translation,173,168,results,syntactic structure,of,extracted patterns,syntactic structure of extracted patterns,0.5149164795875549
translation,173,168,results,end-to - end coverage,when generating,abstractive summaries,end-to - end coverage when generating abstractive summaries,0.686427652835846
translation,173,168,results,extracted patterns,has,to what has been observed in past news,extracted patterns has to what has been observed in past news,0.6081445217132568
translation,173,168,results,negatively affect,has,end-to - end coverage,negatively affect has end-to - end coverage,0.5864993929862976
translation,173,168,results,results,has,positive outcome,results has positive outcome,0.545322597026825
translation,173,177,results,top ranked abstraction,using,three different models,top ranked abstraction using three different models,0.6920863389968872
translation,173,177,results,three different models,for,pattern extraction,three different models for pattern extraction,0.6211099028587341
translation,173,177,results,rating,has,top ranked abstraction,rating has top ranked abstraction,0.571848452091217
translation,173,178,results,abstractions,produced with,memory - based method,abstractions produced with memory - based method,0.7069833874702454
translation,173,178,results,memory - based method,are,more readable,memory - based method are more readable,0.5559005737304688
translation,173,178,results,results,has,abstractions,results has abstractions,0.49885937571525574
translation,174,151,ablation-analysis,model,in,tem2tem way,model in tem2tem way,0.5952767133712769
translation,174,151,ablation-analysis,model,get,better nms and ems,model get better nms and ems,0.617755651473999
translation,174,151,ablation-analysis,ablation analysis,when training,model,ablation analysis when training model,0.6916517615318298
translation,174,137,baselines,bidirectional lstm,with,attention mechanism,bidirectional lstm with attention mechanism,0.6217097043991089
translation,174,137,baselines,lstm,has,bidirectional lstm,lstm has bidirectional lstm,0.5176682472229004
translation,174,139,baselines,encoder-decoder model,with,copy mechanism,encoder-decoder model with copy mechanism,0.6568924188613892
translation,174,139,baselines,pgnet,has,pointergenerator network,pgnet has pointergenerator network,0.5683307647705078
translation,174,139,baselines,pgnet,has,encoder-decoder model,pgnet has encoder-decoder model,0.5682049989700317
translation,174,139,baselines,pointergenerator network,has,encoder-decoder model,pointergenerator network has encoder-decoder model,0.5316532850265503
translation,174,139,baselines,baselines,has,pgnet,baselines has pgnet,0.6229699850082397
translation,174,140,baselines,rawsent,has,raw sentences,rawsent has raw sentences,0.6067858338356018
translation,174,141,baselines,ltr,has,learning -,ltr has learning -,0.6542097926139832
translation,174,141,baselines,baselines,has,ltr,baselines has ltr,0.5987511277198792
translation,174,142,baselines,bidirectional lstm,with,attention mechanism ( abs - lstm ),bidirectional lstm with attention mechanism ( abs - lstm ),0.6400960087776184
translation,174,142,baselines,pointergenerator network ( abs - pgnet ),on,paired commentaries and news articles,pointergenerator network ( abs - pgnet ) on paired commentaries and news articles,0.5644223690032959
translation,174,142,baselines,paired commentaries and news articles,as,abstractive summarization baselines,paired commentaries and news articles as abstractive summarization baselines,0.5218236446380615
translation,174,37,experiments,chinese dataset,for studying,sports game summarization,chinese dataset for studying sports game summarization,0.6683626174926758
translation,174,37,experiments,sportssum,has,chinese dataset,sportssum has chinese dataset,0.5706658363342285
translation,174,40,model,two-step summarization model,for,sportssum,two-step summarization model for sportssum,0.6278402805328369
translation,174,40,model,two-step summarization model,learns,selector,two-step summarization model learns selector,0.6531853079795837
translation,174,40,model,two-step summarization model,trains,rewriter,two-step summarization model trains rewriter,0.7215019464492798
translation,174,40,model,selector,to extract,important commentary sentences,selector to extract important commentary sentences,0.7191835045814514
translation,174,40,model,rewriter,to convert,selected sentences,rewriter to convert selected sentences,0.6224198341369629
translation,174,40,model,selected sentences,to,news article,selected sentences to news article,0.5513564348220825
translation,174,40,model,model,propose,two-step summarization model,model propose two-step summarization model,0.6284815073013306
translation,174,41,model,all the player names,in,training sentences,all the player names in training sentences,0.4614154100418091
translation,174,41,model,training sentences,with,special token,training sentences with special token,0.6058996319770813
translation,174,41,model,proposed model,on,modified template - like sentences,proposed model on modified template - like sentences,0.5640584230422974
translation,174,41,model,model,replace,all the player names,model replace all the player names,0.6300915479660034
translation,174,78,model,twostep model,for,sportssum,twostep model for sportssum,0.665681004524231
translation,174,78,model,model,propose,twostep model,model propose twostep model,0.6300698518753052
translation,174,79,model,selector,to extract,important commentary sentences,selector to extract important commentary sentences,0.7191835045814514
translation,174,79,model,rewriter,to convert,selected sentences,rewriter to convert selected sentences,0.6224198341369629
translation,174,79,model,selected sentences,to,news article,selected sentences to news article,0.5513564348220825
translation,174,79,model,model,first learns,selector,model first learns selector,0.6938406229019165
translation,174,138,model,model,has,transformer,model has transformer,0.5715572834014893
translation,174,145,results,extractive models ( rawsent and ltr ),get,low rouge scores,extractive models ( rawsent and ltr ) get low rouge scores,0.5608486533164978
translation,174,145,results,extractive models ( rawsent and ltr ),get,high nms,extractive models ( rawsent and ltr ) get high nms,0.5811834931373596
translation,174,145,results,extractive models ( rawsent and ltr ),get,ems,extractive models ( rawsent and ltr ) get ems,0.550136148929596
translation,174,145,results,results,observe,extractive models ( rawsent and ltr ),results observe extractive models ( rawsent and ltr ),0.5772145390510559
translation,174,147,results,abstractive models,get,higher rouge scores,abstractive models get higher rouge scores,0.5642768144607544
translation,174,147,results,abstractive models,get,lower nms and ems,abstractive models get lower nms and ems,0.5454607009887695
translation,174,147,results,results,has,abstractive models,results has abstractive models,0.5407604575157166
translation,174,149,results,our proposed two -step model,performs,better,our proposed two -step model performs better,0.6557112336158752
translation,174,149,results,better,than,extractive models and the abstractive models,better than extractive models and the abstractive models,0.5722929835319519
translation,174,149,results,extractive models and the abstractive models,on,"rouge scores , nms , and ems","extractive models and the abstractive models on rouge scores , nms , and ems",0.5354905128479004
translation,174,149,results,results,has,our proposed two -step model,results has our proposed two -step model,0.5394062399864197
translation,175,17,baselines,towgs,is,abstractive summarization algorithm,towgs is abstractive summarization algorithm,0.5264211297035217
translation,175,17,baselines,baselines,has,towgs,baselines has towgs,0.6108667850494385
translation,175,143,experimental-setup,ram,used by,algorithm,ram used by algorithm,0.7183515429496765
translation,175,143,experimental-setup,ram,between,1.5 - 2 gb,ram between 1.5 - 2 gb,0.665619969367981
translation,175,148,experiments,towgs,tested on,seven day 0.1 % sample,towgs tested on seven day 0.1 % sample,0.7403459548950195
translation,175,148,experiments,seven day 0.1 % sample,of,entire twitter stream,seven day 0.1 % sample of entire twitter stream,0.5862548351287842
translation,175,8,model,processing,done in,single pass,processing done in single pass,0.7840373516082764
translation,175,8,model,processing,improving,running time,processing improving running time,0.6885854005813599
translation,175,8,model,model,has,processing,model has processing,0.6099295616149902
translation,175,9,model,online approach,able to generate,summaries,online approach able to generate summaries,0.6935861110687256
translation,175,9,model,summaries,in,real time,summaries in real time,0.6047376990318298
translation,175,9,model,summaries,using,latest information,summaries using latest information,0.5699362754821777
translation,175,9,model,model,has,online approach,model has online approach,0.5890678763389587
translation,175,10,model,word graph,along with,optimization techniques,word graph along with optimization techniques,0.6298262476921082
translation,175,10,model,optimization techniques,such as,decaying windows,optimization techniques such as decaying windows,0.6504897475242615
translation,175,10,model,optimization techniques,such as,pruning,optimization techniques such as pruning,0.6558253765106201
translation,175,10,model,model,uses,word graph,model uses word graph,0.5694501996040344
translation,175,147,model,highly efficient algorithm,capable of,online abstractive microblog summarization,highly efficient algorithm capable of online abstractive microblog summarization,0.6030533909797668
translation,175,147,model,towgs,has,highly efficient algorithm,towgs has highly efficient algorithm,0.5501272082328796
translation,175,147,model,model,introduces,towgs,model introduces towgs,0.6888751983642578
translation,175,25,results,our solution,capable of,online summarization,our solution capable of online summarization,0.6486985087394714
translation,175,25,results,outperforms,in terms of,result quality,outperforms in terms of result quality,0.6703542470932007
translation,175,25,results,batch - based event-filtered baseline,in terms of,result quality,batch - based event-filtered baseline in terms of result quality,0.6818186044692993
translation,175,25,results,outperforms,has,batch - based event-filtered baseline,outperforms has batch - based event-filtered baseline,0.5874837040901184
translation,175,25,results,results,has,our solution,results has our solution,0.6060497164726257
translation,176,180,baselines,attention based seq2seq model,denoted as,s2s- att,attention based seq2seq model denoted as s2s- att,0.702590823173523
translation,176,189,baselines,t-convs2s,is,topic-aware convolutional seq2seq model,t-convs2s is topic-aware convolutional seq2seq model,0.5325161814689636
translation,176,189,baselines,baselines,has,t-convs2s,baselines has t-convs2s,0.5700132250785828
translation,176,5,experiments,reddit tifu dataset,consisting of,120k posts,reddit tifu dataset consisting of 120k posts,0.6711915731430054
translation,176,5,experiments,120k posts,from,online discussion forum reddit,120k posts from online discussion forum reddit,0.5678324699401855
translation,176,21,experiments,usergenerated posts,from,online discussion forum reddit,usergenerated posts from online discussion forum reddit,0.5324915051460266
translation,176,21,experiments,online discussion forum reddit,especially,tifu subreddit,online discussion forum reddit especially tifu subreddit,0.6854663491249084
translation,176,179,experiments,pg,as,state - of - the - art methods,pg as state - of - the - art methods,0.5627802610397339
translation,176,179,experiments,drgd,as,state - of - the - art methods,drgd as state - of - the - art methods,0.5461387634277344
translation,176,179,experiments,state - of - the - art methods,of,abstractive summarization,state - of - the - art methods of abstractive summarization,0.5203830599784851
translation,176,8,model,abstractive summarization model,named,multilevel memory networks ( mmn ),abstractive summarization model named multilevel memory networks ( mmn ),0.6792599558830261
translation,176,8,model,abstractive summarization model,equipped with,multi-level memory,abstractive summarization model equipped with multi-level memory,0.6207000017166138
translation,176,8,model,multi-level memory,to store,information,multi-level memory to store information,0.7087951302528381
translation,176,8,model,information,of,text,information of text,0.5671967267990112
translation,176,8,model,information,from,different levels of abstraction,information from different levels of abstraction,0.562229335308075
translation,176,8,model,model,propose,abstractive summarization model,model propose abstractive summarization model,0.6467617750167847
translation,176,8,model,model,novel,abstractive summarization model,model novel abstractive summarization model,0.6607252955436707
translation,176,20,model,bias issue,by changing,source of summarization dataset,bias issue by changing source of summarization dataset,0.6985401511192322
translation,176,20,model,model,alleviate,bias issue,model alleviate bias issue,0.71158766746521
translation,176,26,model,novel memory network model,named,multilevel memory networks ( mmn ),novel memory network model named multilevel memory networks ( mmn ),0.6846698522567749
translation,176,26,model,model,propose,novel memory network model,model propose novel memory network model,0.6214883327484131
translation,176,27,model,multi-level memory networks,storing,information,multi-level memory networks storing information,0.7443265318870544
translation,176,27,model,information,of,source text,information of source text,0.5047129988670349
translation,176,27,model,source text,from,different levels of abstraction,source text from different levels of abstraction,0.549023449420929
translation,176,27,model,different levels of abstraction,i.e.,word- level,different levels of abstraction i.e. word- level,0.6406455636024475
translation,176,27,model,different levels of abstraction,i.e.,sentencelevel,different levels of abstraction i.e. sentencelevel,0.6105594038963318
translation,176,27,model,different levels of abstraction,i.e.,paragraph - level and document- level,different levels of abstraction i.e. paragraph - level and document- level,0.6344995498657227
translation,176,27,model,model,equipped with,multi-level memory networks,model equipped with multi-level memory networks,0.6770045757293701
translation,176,38,results,proposed mmn model,improves,abstractive summarization performance,proposed mmn model improves abstractive summarization performance,0.6414636373519897
translation,176,38,results,abstractive summarization performance,on,our new reddit tifu and existing newsroom - abs,abstractive summarization performance on our new reddit tifu and existing newsroom - abs,0.5503545999526978
translation,176,38,results,results,show,proposed mmn model,results show proposed mmn model,0.6694384813308716
translation,176,193,results,state- of- theart abstractive methods,in,rouge and perplexity scores,state- of- theart abstractive methods in rouge and perplexity scores,0.5188237428665161
translation,176,193,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,176,193,results,outperforms,has,state- of- theart abstractive methods,outperforms has state- of- theart abstractive methods,0.5838853120803833
translation,176,193,results,results,has,our model,results has our model,0.5871725678443909
translation,176,203,results,convolutional - based methods,in,all rouge scores,convolutional - based methods in all rouge scores,0.47832199931144714
translation,176,203,results,outperforms,has,not only,outperforms has not only,0.6648327112197876
translation,176,203,results,not only,has,rnn - based abstractive methods,not only has rnn - based abstractive methods,0.6099885106086731
translation,176,204,results,single end-to - end training procedure,has,our model,single end-to - end training procedure has our model,0.5301714539527893
translation,176,204,results,our model,has,outperforms,our model has outperforms,0.6437026262283325
translation,176,204,results,outperforms,has,t-convs2s,outperforms has t-convs2s,0.6002391576766968
translation,176,204,results,results,trained on,single end-to - end training procedure,results trained on single end-to - end training procedure,0.7006837129592896
translation,176,266,results,abstractive methods,is,higher novel n-gram ratio,abstractive methods is higher novel n-gram ratio,0.5663295388221741
translation,176,267,results,novel n-gram ratios,of,newsroom - abs and xsum,novel n-gram ratios of newsroom - abs and xsum,0.5931318998336792
translation,176,267,results,newsroom - abs and xsum,higher than,our dataset,newsroom - abs and xsum higher than our dataset,0.6465928554534912
translation,176,267,results,results,has,novel n-gram ratios,results has novel n-gram ratios,0.5439044833183289
translation,177,164,ablation-analysis,other chosen internal features,beyond,document frequency ),other chosen internal features beyond document frequency ),0.5732560157775879
translation,177,164,ablation-analysis,other chosen internal features,are,useful,other chosen internal features are useful,0.6064989566802979
translation,177,164,ablation-analysis,ablation analysis,shows that,other chosen internal features,ablation analysis shows that other chosen internal features,0.6679643988609314
translation,177,173,ablation-analysis,features,from,lm scores and wiki,features from lm scores and wiki,0.5523375868797302
translation,177,173,ablation-analysis,lm scores and wiki,are,quite useful,lm scores and wiki are quite useful,0.5498894453048706
translation,177,173,ablation-analysis,ablation analysis,has,features,ablation analysis has features,0.5585079789161682
translation,177,205,ablation-analysis,external resources,contributes to,improved performance,external resources contributes to improved performance,0.6826134920120239
translation,177,205,ablation-analysis,improved performance,of,our method,improved performance of our method,0.5723820924758911
translation,177,205,ablation-analysis,our method,compared to,others that only use internal features,our method compared to others that only use internal features,0.690285325050354
translation,177,205,ablation-analysis,ablation analysis,using,external resources,ablation analysis using external resources,0.585899829864502
translation,177,223,ablation-analysis,external features,like,word embedding model,external features like word embedding model,0.5586752891540527
translation,177,223,ablation-analysis,external features,are,very useful,external features are very useful,0.5258643627166748
translation,177,223,ablation-analysis,word embedding model,based on,large corpus and wiki resource,word embedding model based on large corpus and wiki resource,0.5759981870651245
translation,177,29,experimental-setup,bigrams ' weights,utilize,variety of external resources,bigrams ' weights utilize variety of external resources,0.5720599889755249
translation,177,29,experimental-setup,variety of external resources,including,corpus,variety of external resources including corpus,0.7066293954849243
translation,177,29,experimental-setup,corpus,of,news articles,corpus of news articles,0.579131007194519
translation,177,29,experimental-setup,corpus,with,description of name entities,corpus with description of name entities,0.608108639717102
translation,177,29,experimental-setup,news articles,with,human generated summaries,news articles with human generated summaries,0.6108548045158386
translation,177,29,experimental-setup,news articles,with,wiki documents,news articles with wiki documents,0.5875181555747986
translation,177,29,experimental-setup,description of name entities,from,dbpedia,description of name entities from dbpedia,0.501987099647522
translation,177,29,experimental-setup,description of name entities,from,sentiwordnet,description of name entities from sentiwordnet,0.5059371590614319
translation,177,29,experimental-setup,experimental setup,to estimate,bigrams ' weights,experimental setup to estimate bigrams ' weights,0.6802789568901062
translation,177,29,experimental-setup,experimental setup,utilize,variety of external resources,experimental setup utilize variety of external resources,0.5570569634437561
translation,177,7,model,syntactic information,to select,more important bigrams,syntactic information to select more important bigrams,0.6538372039794922
translation,177,7,model,model,use,syntactic information,model use syntactic information,0.645980715751648
translation,177,8,model,importance,of,bigrams,importance of bigrams,0.6131610870361328
translation,177,8,model,features,by leveraging,multiple external resources,features by leveraging multiple external resources,0.7348984479904175
translation,177,8,model,model,to estimate,importance,model to estimate importance,0.7565407156944275
translation,177,9,model,bigram weights,trained,discriminatively,bigram weights trained discriminatively,0.6957069635391235
translation,177,9,model,bigram weights,selects,summary sentences,bigram weights selects summary sentences,0.6862867474555969
translation,177,9,model,discriminatively,in,joint learning model,discriminatively in joint learning model,0.4545958340167999
translation,177,9,model,joint learning model,that predicts,bigram weights,joint learning model that predicts bigram weights,0.6632828712463379
translation,177,9,model,summary sentences,in,ilp framework,summary sentences in ilp framework,0.5276370048522949
translation,177,9,model,model,has,bigram weights,model has bigram weights,0.5718012452125549
translation,177,31,model,joint bigram weighting and sentence selection process,to train,feature weights,joint bigram weighting and sentence selection process to train feature weights,0.6578583717346191
translation,177,31,model,model,propose to use,joint bigram weighting and sentence selection process,model propose to use joint bigram weighting and sentence selection process,0.6967021822929382
translation,177,58,model,many external resources,to extract,features,many external resources to extract features,0.6765497922897339
translation,177,58,model,many external resources,propose to estimate,feature weights,many external resources propose to estimate feature weights,0.6878277659416199
translation,177,58,model,features,for,bigram candidates,features for bigram candidates,0.6106836199760437
translation,177,58,model,feature weights,in,joint process,feature weights in joint process,0.5151035189628601
translation,177,58,model,joint process,via,structured perceptron learning,joint process via structured perceptron learning,0.6509383916854858
translation,177,58,model,structured perceptron learning,that optimizes,summary sentence selection,structured perceptron learning that optimizes summary sentence selection,0.6472703218460083
translation,177,58,model,model,explore,many external resources,model explore many external resources,0.7126760482788086
translation,177,58,model,model,propose to estimate,feature weights,model propose to estimate feature weights,0.7377910017967224
translation,177,60,model,ilp - based summarization framework,tries to maximize,weights,ilp - based summarization framework tries to maximize weights,0.7628691792488098
translation,177,60,model,weights,of,selected concepts ( bigrams ),weights of selected concepts ( bigrams ),0.5619969964027405
translation,177,60,model,selected concepts ( bigrams ),under,summary length constraint,selected concepts ( bigrams ) under summary length constraint,0.5994452834129333
translation,177,60,model,model,use,ilp - based summarization framework,model use ilp - based summarization framework,0.6117690801620483
translation,177,199,model,methods,to improve,bigram concept selection and weighting,methods to improve bigram concept selection and weighting,0.6256625652313232
translation,177,199,model,model,adopt,ilp based summarization framework,model adopt ilp based summarization framework,0.6633884310722351
translation,177,199,model,model,propose,methods,model propose methods,0.5709900259971619
translation,177,200,model,syntactic information,to filter and select,bigrams,syntactic information to filter and select bigrams,0.6641547083854675
translation,177,200,model,syntactic information,to filter and select,joint learning process,syntactic information to filter and select joint learning process,0.6696124076843262
translation,177,200,model,joint learning process,for,weight training,joint learning process for weight training,0.6071021556854248
translation,177,200,model,model,use,syntactic information,model use syntactic information,0.645980715751648
translation,177,147,results,result,of,our proposed method,result of our proposed method,0.6085896492004395
translation,177,147,results,our proposed method,is,statistically significantly better,our proposed method is statistically significantly better,0.5728170871734619
translation,177,147,results,statistically significantly better,than,icsi ilp,statistically significantly better than icsi ilp,0.5848132967948914
translation,177,147,results,results,of,our proposed method,results of our proposed method,0.5945964455604553
translation,177,147,results,results,has,result,results has result,0.5303357243537903
translation,177,158,results,input bigrams,has,our method,input bigrams has our method,0.5718787908554077
translation,177,158,results,our method,has,outperformed,our method has outperformed,0.6023563742637634
translation,177,158,results,outperformed,has,icsi system,outperformed has icsi system,0.6197941899299622
translation,177,158,results,results,by changing,input bigrams,results by changing input bigrams,0.7164795994758606
translation,177,163,results,system,with,internal features,system with internal features,0.672224223613739
translation,177,163,results,baseline,which used,document frequency,baseline which used document frequency,0.7137265801429749
translation,177,163,results,document frequency,as,weight,document frequency as weight,0.5329006910324097
translation,177,163,results,internal features,has,already outperformed,internal features has already outperformed,0.5995598435401917
translation,177,163,results,already outperformed,has,baseline,already outperformed has baseline,0.6205951571464539
translation,177,163,results,results,see that,system,results see that system,0.6715076565742493
translation,177,165,results,features,from,only one external resource,features from only one external resource,0.5622121095657349
translation,177,165,results,results,from,some resources,results from some resources,0.5716369152069092
translation,177,165,results,some resources,are,competitive,some resources are competitive,0.5850082039833069
translation,177,165,results,competitive,compared to,system,competitive compared to system,0.7518619298934937
translation,177,165,results,system,using,internal features,system using internal features,0.6991725564002991
translation,177,165,results,features,has,results,features has results,0.617753803730011
translation,177,165,results,only one external resource,has,results,only one external resource has results,0.5912525057792664
translation,177,165,results,results,use,features,results use features,0.6547607779502869
translation,177,165,results,results,from,some resources,results from some resources,0.5716369152069092
translation,177,167,results,worse results,than,internal features,worse results than internal features,0.5842844843864441
translation,177,167,results,dbpedia or sentiwordnet,has,worse results,dbpedia or sentiwordnet has worse results,0.581649899482727
translation,177,167,results,results,Using,dbpedia or sentiwordnet,results Using dbpedia or sentiwordnet,0.648465096950531
translation,177,172,results,features,from,word embedding model,features from word embedding model,0.4877065420150757
translation,177,172,results,word embedding model,has,outperform,word embedding model has outperform,0.6192269325256348
translation,177,172,results,outperform,has,others,outperform has others,0.6135737299919128
translation,177,172,results,results,see that,features,results see that features,0.6307434439659119
translation,177,177,results,features,from,dbpedia and sentiwordnet,features from dbpedia and sentiwordnet,0.528781533241272
translation,177,177,results,features,do not perform,well,features do not perform well,0.777960479259491
translation,177,177,results,well,after,combination,well after combination,0.7706117033958435
translation,177,177,results,combination,with,internal features,combination with internal features,0.6588671803474426
translation,177,181,results,"wiki , lm and dbpedia features",give,more improvement,"wiki , lm and dbpedia features give more improvement",0.6070764660835266
translation,177,181,results,more improvement,than,wordnet and sentiword - net features,more improvement than wordnet and sentiword - net features,0.5321540236473083
translation,177,181,results,results,show,"wiki , lm and dbpedia features","results show wiki , lm and dbpedia features",0.5606092214584351
translation,177,197,results,pipeline system,based on,rouge - 2 measurement,pipeline system based on rouge - 2 measurement,0.6944829821586609
translation,177,197,results,joint method,has,outperforms,joint method has outperforms,0.6365148425102234
translation,177,197,results,outperforms,has,pipeline system,outperforms has pipeline system,0.6152527928352356
translation,177,197,results,results,see that,joint method,results see that joint method,0.6371062994003296
translation,177,224,results,system,has,biggest performance degradation,system has biggest performance degradation,0.5665184259414673
translation,177,224,results,biggest performance degradation,compared to,best result,biggest performance degradation compared to best result,0.6465513110160828
translation,177,224,results,system,has,biggest performance degradation,system has biggest performance degradation,0.5665184259414673
translation,178,99,baselines,r2n2,applies,recursive neural networks,r2n2 applies recursive neural networks,0.5971956253051758
translation,178,99,baselines,recursive neural networks,to learn,feature combination,recursive neural networks to learn feature combination,0.6024581789970398
translation,178,99,baselines,baselines,has,r2n2,baselines has r2n2,0.5437486171722412
translation,178,101,baselines,regsum,is,word regression approach,regsum is word regression approach,0.5982720255851746
translation,178,101,baselines,word regression approach,based on,some advanced features,word regression approach based on some advanced features,0.6484541893005371
translation,178,101,baselines,some advanced features,such as,word polarities,some advanced features such as word polarities,0.6298629641532898
translation,178,101,baselines,baselines,has,regsum,baselines has regsum,0.5764636397361755
translation,178,89,hyperparameters,look - up table,of,25 dimensional word embeddings,look - up table of 25 dimensional word embeddings,0.5626578330993652
translation,178,89,hyperparameters,25 dimensional word embeddings,trained by,model of collobert et al . ( 2011 ),25 dimensional word embeddings trained by model of collobert et al . ( 2011 ),0.6832668781280518
translation,178,89,hyperparameters,hyperparameters,directly use,look - up table,hyperparameters directly use look - up table,0.6884781122207642
translation,178,91,hyperparameters,dimension l,of,hidden documentindependent features,dimension l of hidden documentindependent features,0.5199823975563049
translation,178,91,hyperparameters,dimension l,experimented in,range,dimension l experimented in range,0.7154615521430969
translation,178,91,hyperparameters,hidden documentindependent features,experimented in,range,hidden documentindependent features experimented in range,0.6839509606361389
translation,178,91,hyperparameters,range,of,"[ 1 , 40 ]","range of [ 1 , 40 ]",0.5692360997200012
translation,178,91,hyperparameters,range,of,window sizes,range of window sizes,0.6170673370361328
translation,178,91,hyperparameters,range,of,1 and 5,range of 1 and 5,0.6917098164558411
translation,178,91,hyperparameters,window sizes,experimented between,1 and 5,window sizes experimented between 1 and 5,0.728380024433136
translation,178,91,hyperparameters,hyperparameters,has,dimension l,hyperparameters has dimension l,0.49939990043640137
translation,178,92,hyperparameters,l = 20 and m = 3,for,priorsum,l = 20 and m = 3 for priorsum,0.6684391498565674
translation,178,93,hyperparameters,weights w h t and w r,apply,diagonal variant,weights w h t and w r apply diagonal variant,0.6510093212127686
translation,178,93,hyperparameters,of adagrad,with,minibatches,of adagrad with minibatches,0.6013014912605286
translation,178,93,hyperparameters,diagonal variant,has,of adagrad,diagonal variant has of adagrad,0.5472671389579773
translation,178,93,hyperparameters,hyperparameters,update,weights w h t and w r,hyperparameters update weights w h t and w r,0.7593632340431213
translation,178,4,model,sentence,appropriate to be selected into,summary,sentence appropriate to be selected into summary,0.7189058661460876
translation,178,4,model,model,propose,concept of summary,model propose concept of summary,0.7198251485824585
translation,178,5,model,system,called,priorsum,system called priorsum,0.7565850019454956
translation,178,5,model,enhanced convolutional neural networks,to capture,summary prior features,enhanced convolutional neural networks to capture summary prior features,0.6515704989433289
translation,178,5,model,summary prior features,derived from,length - variable phrases,summary prior features derived from length - variable phrases,0.6081331968307495
translation,178,6,model,learned prior features,concatenated with,document - dependent features,learned prior features concatenated with document - dependent features,0.6589378118515015
translation,178,6,model,document - dependent features,for,sentence ranking,document - dependent features for sentence ranking,0.5853332281112671
translation,178,6,model,regression framework,has,learned prior features,regression framework has learned prior features,0.505121648311615
translation,178,6,model,model,Under,regression framework,model Under regression framework,0.6583360433578491
translation,178,34,model,novel summarization system,called,priorsum,novel summarization system called priorsum,0.6838605403900146
translation,178,34,model,all possible semantic aspects,latent in,summary prior nature,all possible semantic aspects latent in summary prior nature,0.6869456171989441
translation,178,34,model,model,develop,novel summarization system,model develop novel summarization system,0.6250122785568237
translation,178,35,model,multiple filters,to capture,comprehensive set of document - independent features derived from length - variable phrases,multiple filters to capture comprehensive set of document - independent features derived from length - variable phrases,0.6514338254928589
translation,178,37,model,priorsum,generates,document- independent features,priorsum generates document- independent features,0.6271907091140747
translation,178,37,model,priorsum,concatenates them with,document - dependent ones,priorsum concatenates them with document - dependent ones,0.7507668137550354
translation,178,37,model,document - dependent ones,to work for,sentence regression,document - dependent ones to work for sentence regression,0.6994718313217163
translation,178,37,model,model,has,priorsum,model has priorsum,0.5842533111572266
translation,178,45,model,enhanced version of convolutional neural networks,to automatically generate,document - independent features,enhanced version of convolutional neural networks to automatically generate document - independent features,0.7221713066101074
translation,178,45,model,document - independent features,according to,summary prior nature,document - independent features according to summary prior nature,0.6075711250305176
translation,178,45,model,model,apply,enhanced version of convolutional neural networks,model apply enhanced version of convolutional neural networks,0.6080328226089478
translation,178,100,model,clustercmrw,incorporates,cluster-level information,clustercmrw incorporates cluster-level information,0.7140522003173828
translation,178,100,model,cluster-level information,into,graph - based ranking algorithm,cluster-level information into graph - based ranking algorithm,0.5409128665924072
translation,178,100,model,model,has,clustercmrw,model has clustercmrw,0.5924984216690063
translation,178,48,results,sentence ranking priorsum,improves,standard convolutional neural networks,sentence ranking priorsum improves standard convolutional neural networks,0.6435278654098511
translation,178,48,results,standard convolutional neural networks,to learn,summary prior,standard convolutional neural networks to learn summary prior,0.616228461265564
translation,178,48,results,cnns ),to learn,summary prior,cnns ) to learn summary prior,0.6125844120979309
translation,178,48,results,standard convolutional neural networks,has,cnns ),standard convolutional neural networks has cnns ),0.57631516456604
translation,178,48,results,results,has,sentence ranking priorsum,results has sentence ranking priorsum,0.5822795033454895
translation,178,108,results,priorsum,achieve,comparable performance,priorsum achieve comparable performance,0.659257709980011
translation,178,108,results,comparable performance,to,stateof - the - art summarization systems r2n2,comparable performance to stateof - the - art summarization systems r2n2,0.5498863458633423
translation,178,108,results,comparable performance,to,cluster - cmrw,comparable performance to cluster - cmrw,0.5879873037338257
translation,178,108,results,results,see that,priorsum,results see that priorsum,0.6877011656761169
translation,178,109,results,significantly 4 outperforms,uses,manually compiled features,significantly 4 outperforms uses manually compiled features,0.6419286131858826
translation,178,109,results,significantly 4 outperforms,uses,graph- based summarization system lexrank,significantly 4 outperforms uses graph- based summarization system lexrank,0.5868540406227112
translation,178,109,results,reg manual,uses,manually compiled features,reg manual uses manually compiled features,0.6584435105323792
translation,178,109,results,reg manual,uses,graph- based summarization system lexrank,reg manual uses graph- based summarization system lexrank,0.5929293632507324
translation,178,109,results,baselines,has,priorsum,baselines has priorsum,0.582577109336853
translation,178,109,results,priorsum,has,significantly 4 outperforms,priorsum has significantly 4 outperforms,0.6588735580444336
translation,178,109,results,significantly 4 outperforms,has,reg manual,significantly 4 outperforms has reg manual,0.6657294631004333
translation,178,109,results,results,With respect to,baselines,results With respect to baselines,0.6985979080200195
translation,178,110,results,priorsum,enjoys,reasonable increase,priorsum enjoys reasonable increase,0.6433718800544739
translation,178,110,results,reasonable increase,over,standardcnn,reasonable increase over standardcnn,0.6901467442512512
translation,178,110,results,results,has,priorsum,results has priorsum,0.579334020614624
translation,178,111,results,stan-dardcnn,achieve,state - of - the - art performance,stan-dardcnn achieve state - of - the - art performance,0.6149049401283264
translation,179,88,baselines,rnn and rnn - cont,are,two sequence - tosequence baseline,rnn and rnn - cont are two sequence - tosequence baseline,0.5885533690452576
translation,179,88,baselines,two sequence - tosequence baseline,with,gru encoder and decoder,two sequence - tosequence baseline with gru encoder and decoder,0.6417696475982666
translation,179,88,baselines,baselines,has,rnn and rnn - cont,baselines has rnn and rnn - cont,0.5965849757194519
translation,179,92,baselines,"srb ( ma et al. , 2017 )",is,sequence - tosequence based neural model,"srb ( ma et al. , 2017 ) is sequence - tosequence based neural model",0.5596762895584106
translation,179,92,baselines,sequence - tosequence based neural model,with improving,semantic relevance,sequence - tosequence based neural model with improving semantic relevance,0.7021318674087524
translation,179,92,baselines,semantic relevance,between,input text,semantic relevance between input text,0.5903497338294983
translation,179,92,baselines,semantic relevance,between,output summary,semantic relevance between output summary,0.6002234220504761
translation,179,92,baselines,baselines,has,"srb ( ma et al. , 2017 )","baselines has srb ( ma et al. , 2017 )",0.5068575739860535
translation,179,93,baselines,drgd,is,deep recurrent generative decoder model,drgd is deep recurrent generative decoder model,0.5319104790687561
translation,179,93,baselines,drgd,combining,decoder,drgd combining decoder,0.7752471566200256
translation,179,93,baselines,decoder,with,variational autoencoder,decoder with variational autoencoder,0.6272417902946472
translation,179,93,baselines,baselines,has,drgd,baselines has drgd,0.5539196729660034
translation,179,60,hyperparameters,"adam ( kingma and ba , 2014 ) optimization method",to train,model,"adam ( kingma and ba , 2014 ) optimization method to train model",0.7065752148628235
translation,179,60,hyperparameters,hyperparameters,use,"adam ( kingma and ba , 2014 ) optimization method","hyperparameters use adam ( kingma and ba , 2014 ) optimization method",0.6393225193023682
translation,179,61,hyperparameters,two momentum parameters,has,? 1 = 0.9 and ? 2 = 0.999,two momentum parameters has ? 1 = 0.9 and ? 2 = 0.999,0.5566332340240479
translation,179,62,hyperparameters,"gradients ( pascanu et al. , 2013 )",to,maximum norm,"gradients ( pascanu et al. , 2013 ) to maximum norm",0.4956899583339691
translation,179,62,hyperparameters,maximum norm,of,10.0,maximum norm of 10.0,0.5928901433944702
translation,179,62,hyperparameters,hyperparameters,clip,"gradients ( pascanu et al. , 2013 )","hyperparameters clip gradients ( pascanu et al. , 2013 )",0.6870273351669312
translation,179,81,hyperparameters,vocabulary size,to,"4,000","vocabulary size to 4,000",0.5865013003349304
translation,179,81,hyperparameters,"4,000",covers,most of the common characters,"4,000 covers most of the common characters",0.7463158965110779
translation,179,81,hyperparameters,hyperparameters,prune,vocabulary size,hyperparameters prune vocabulary size,0.6896604895591736
translation,179,82,hyperparameters,hyper-parameters,based on,rouge scores,hyper-parameters based on rouge scores,0.6305689215660095
translation,179,82,hyperparameters,rouge scores,on,validation sets,rouge scores on validation sets,0.5185956358909607
translation,179,82,hyperparameters,hyperparameters,tune,hyper-parameters,hyperparameters tune hyper-parameters,0.7161423563957214
translation,179,83,hyperparameters,word embedding size and the hidden size,to,512,word embedding size and the hidden size to 512,0.5920020937919617
translation,179,83,hyperparameters,number of lstm layers,is,2,number of lstm layers is 2,0.5495396852493286
translation,179,83,hyperparameters,hyperparameters,set,word embedding size and the hidden size,hyperparameters set word embedding size and the hidden size,0.6403974294662476
translation,179,83,hyperparameters,hyperparameters,set,number of lstm layers,hyperparameters set number of lstm layers,0.5872710347175598
translation,179,84,hyperparameters,batch size,is,64,batch size is 64,0.6388692259788513
translation,179,84,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
translation,179,85,hyperparameters,beam size,to,10,beam size to 10,0.6524515748023987
translation,179,85,hyperparameters,hyperparameters,implement,beam search,hyperparameters implement beam search,0.6573067903518677
translation,179,85,hyperparameters,hyperparameters,set,beam size,hyperparameters set beam size,0.6436296701431274
translation,179,22,model,summary autoencoder,as,assistant supervisor,summary autoencoder as assistant supervisor,0.4704993963241577
translation,179,22,model,assistant supervisor,of,seq2seq,assistant supervisor of seq2seq,0.6148879528045654
translation,179,22,model,model,regard,summary autoencoder,model regard summary autoencoder,0.6073431968688965
translation,179,23,model,autoencoder,inputs and reconstructs,summaries,autoencoder inputs and reconstructs summaries,0.7766826152801514
translation,179,23,model,autoencoder,inputs and reconstructs,summaries,autoencoder inputs and reconstructs summaries,0.7766826152801514
translation,179,23,model,better representation,to generate,summaries,better representation to generate summaries,0.6813662052154541
translation,179,23,model,model,train,autoencoder,model train autoencoder,0.6967796087265015
translation,179,24,model,internal representation,of,seq2seq,internal representation of seq2seq,0.5858491063117981
translation,179,24,model,internal representation,by minimizing,distance,internal representation by minimizing distance,0.721342921257019
translation,179,24,model,seq2seq,with,autoencoder,seq2seq with autoencoder,0.6354156732559204
translation,179,24,model,distance,between,two representations,distance between two representations,0.6765323877334595
translation,179,24,model,model,supervise,internal representation,model supervise internal representation,0.729507565498352
translation,179,25,model,adversarial learning,to enhance,supervision,adversarial learning to enhance supervision,0.666694700717926
translation,179,101,results,large improvement,over,seq2seq baseline,large improvement over seq2seq baseline,0.6621918678283691
translation,179,101,results,seq2seq baseline,by,"7.1 rouge -1 , 6.1 rouge - 2 , and 7.0 rouge -l","seq2seq baseline by 7.1 rouge -1 , 6.1 rouge - 2 , and 7.0 rouge -l",0.531589925289154
translation,179,101,results,superae model,has,large improvement,superae model has large improvement,0.5990588665008545
translation,179,101,results,results,has,superae model,results has superae model,0.537243664264679
translation,179,104,results,outperforms,with,relative gain,outperforms with relative gain,0.7116236686706543
translation,179,104,results,all of these models,with,relative gain,all of these models with relative gain,0.6567307114601135
translation,179,104,results,relative gain,of,"2.2 rouge -1 , 1.8 rouge - 2 , and 2.0 rouge -l","relative gain of 2.2 rouge -1 , 1.8 rouge - 2 , and 2.0 rouge -l",0.6276143193244934
translation,179,104,results,superae,has,outperforms,superae has outperforms,0.6526538133621216
translation,179,104,results,outperforms,has,all of these models,outperforms has all of these models,0.571760356426239
translation,179,106,results,adversarial learning,improves,performance,adversarial learning improves performance,0.6950660943984985
translation,179,106,results,performance,of,"1.5 rouge -1 , 0.7 rouge - 2 , and 1.0 rouge -l","performance of 1.5 rouge -1 , 0.7 rouge - 2 , and 1.0 rouge -l",0.6262074708938599
translation,179,106,results,results,shows that,adversarial learning,results shows that adversarial learning,0.642277181148529
translation,179,125,results,seq2seq model,achieves,80.7 % and 65.1 % accuracy,seq2seq model achieves 80.7 % and 65.1 % accuracy,0.6524124145507812
translation,179,125,results,80.7 % and 65.1 % accuracy,of,2 - class and 5 - class,80.7 % and 65.1 % accuracy of 2 - class and 5 - class,0.5664085745811462
translation,179,126,results,outperforms,with,large margin,outperforms with large margin,0.7049083113670349
translation,179,126,results,baselines,with,large margin,baselines with large margin,0.6904941201210022
translation,179,126,results,large margin,of,8.1 % and 6.6 %,large margin of 8.1 % and 6.6 %,0.5478559732437134
translation,179,126,results,superae model,has,outperforms,superae model has outperforms,0.6261265873908997
translation,179,126,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
translation,179,126,results,results,has,superae model,results has superae model,0.537243664264679
translation,180,76,baselines,baselines,has,stand - hdp ( 1 ),baselines has stand - hdp ( 1 ),0.5854548811912537
translation,180,78,baselines,baselines,has,stand - hdp ( 2 ),baselines has stand - hdp ( 2 ),0.585548460483551
translation,180,80,baselines,several baselines,used in,traditional summarization,several baselines used in traditional summarization,0.5874021053314209
translation,180,80,baselines,several baselines,used in,timeline summarization,several baselines used in timeline summarization,0.6223613619804382
translation,180,80,baselines,baselines,implement,several baselines,baselines implement several baselines,0.6642009615898132
translation,180,81,baselines,centroid,applies,mead algorithm,centroid applies mead algorithm,0.6010806560516357
translation,180,81,baselines,mead algorithm,according to,features,mead algorithm according to features,0.6992174983024597
translation,180,81,baselines,features,including,centroid value,features including centroid value,0.6379445791244507
translation,180,81,baselines,features,including,position and first-sentence overlap,features including position and first-sentence overlap,0.6475366950035095
translation,180,81,baselines,m anif old,is,graph based unsupervised method,m anif old is graph based unsupervised method,0.6128811240196228
translation,180,81,baselines,graph based unsupervised method,for,summarization,graph based unsupervised method for summarization,0.6153796315193176
translation,180,81,baselines,score,of,each sentence,score of each sentence,0.5898162722587585
translation,180,81,baselines,each sentence,got from,propagation,each sentence got from propagation,0.5930353999137878
translation,180,81,baselines,et s,is,timeline summarization approach,et s is timeline summarization approach,0.5974566340446472
translation,180,81,baselines,timeline summarization approach,is,graph based approach,timeline summarization approach is graph based approach,0.5533424019813538
translation,180,81,baselines,graph based approach,with,optimized global and local biased summarization,graph based approach with optimized global and local biased summarization,0.6334986090660095
translation,180,81,baselines,baselines,has,centroid,baselines has centroid,0.5995363593101501
translation,180,82,baselines,chieu,is,timeline system,chieu is timeline system,0.6291030049324036
translation,180,82,baselines,timeline system,utilizing,interest and bursty ranking,timeline system utilizing interest and bursty ranking,0.6983534097671509
translation,180,82,baselines,timeline system,neglecting,trans-temporal news evolution,timeline system neglecting trans-temporal news evolution,0.6408436298370361
translation,180,82,baselines,interest and bursty ranking,neglecting,trans-temporal news evolution,interest and bursty ranking neglecting trans-temporal news evolution,0.65049809217453
translation,180,82,baselines,baselines,has,chieu,baselines has chieu,0.6105822324752808
translation,180,7,model,novel model,called,evolutionary hierarchical dirichlet process ( ehdp ),novel model called evolutionary hierarchical dirichlet process ( ehdp ),0.6464284658432007
translation,180,7,model,novel model,to capture,topic evolution pattern,novel model to capture topic evolution pattern,0.6666032671928406
translation,180,7,model,topic evolution pattern,in,timeline summarization,topic evolution pattern in timeline summarization,0.46780768036842346
translation,180,7,model,model,develop,novel model,model develop novel model,0.6538716554641724
translation,180,8,model,time varying information,formulated as,series of hdps,time varying information formulated as series of hdps,0.59761643409729
translation,180,8,model,series of hdps,by considering,time -dependent information,series of hdps by considering time -dependent information,0.7294189929962158
translation,180,8,model,ehdp,has,time varying information,ehdp has time varying information,0.4753405749797821
translation,180,8,model,model,In,ehdp,model In ehdp,0.5635653734207153
translation,180,24,model,evolutionary hierarchical dirichlet process ( hdp ) model,for,timeline summarization,evolutionary hierarchical dirichlet process ( hdp ) model for timeline summarization,0.5570423007011414
translation,180,24,model,ehdp,has,evolutionary hierarchical dirichlet process ( hdp ) model,ehdp has evolutionary hierarchical dirichlet process ( hdp ) model,0.5229678750038147
translation,180,24,model,model,propose,ehdp,model propose ehdp,0.7052445411682129
translation,180,25,model,each hdp,built for,multiple corpora,each hdp built for multiple corpora,0.7125016450881958
translation,180,25,model,multiple corpora,at,each time epoch,multiple corpora at each time epoch,0.5156834721565247
translation,180,25,model,time dependencies,incorporated into,epochs,time dependencies incorporated into epochs,0.6952647566795349
translation,180,25,model,epochs,under,markovian assumptions,epochs under markovian assumptions,0.6692197918891907
translation,180,25,model,ehdp,has,each hdp,ehdp has each hdp,0.6122695207595825
translation,180,25,model,model,In,ehdp,model In ehdp,0.5635653734207153
translation,180,83,results,centroid and m anif old,get,worst results,centroid and m anif old get worst results,0.5619504451751709
translation,180,86,results,ehdp,under,our proposed framework,ehdp under our proposed framework,0.6602082848548889
translation,180,86,results,ehdp,outputs,existing timeline summarization approaches ets and chieu,ehdp outputs existing timeline summarization approaches ets and chieu,0.6915113925933838
translation,180,86,results,results,see that,ehdp,results see that ehdp,0.6741101145744324
translation,180,87,results,"yan et al. , ( 2011a ) s model",by,6.9 % and 9.3 %,"yan et al. , ( 2011a ) s model by 6.9 % and 9.3 %",0.5650481581687927
translation,181,19,model,modular construction,of,interactive summaries,modular construction of interactive summaries,0.5602939128875732
translation,181,19,model,model,To facilitate,modular construction,model To facilitate modular construction,0.715583860874176
translation,181,84,model,frontend,implemented with,an-gularjs library,frontend implemented with an-gularjs library,0.7373806238174438
translation,181,84,model,model,has,frontend,model has frontend,0.5853614211082458
translation,181,167,model,"open knowledge representation ( wities et al. , 2017 )",for consolidating,information,"open knowledge representation ( wities et al. , 2017 ) for consolidating information",0.7813341021537781
translation,181,167,model,information,of,multiple texts,information of multiple texts,0.5602878928184509
translation,181,167,model,summary,fully captures,information,summary fully captures information,0.615405261516571
translation,181,149,results,our system,consistently received,highest ranks,our system consistently received highest ranks,0.7147696614265442
translation,181,149,results,somewhat more complex to use,than,baselines,somewhat more complex to use than baselines,0.6043938994407654
translation,181,149,results,somewhat more complex to use,consistently received,highest ranks,somewhat more complex to use consistently received highest ranks,0.7380045056343079
translation,181,149,results,reading,consistently received,highest ranks,reading consistently received highest ranks,0.7934273481369019
translation,181,149,results,highest ranks,in,dimensions,highest ranks in dimensions,0.528978168964386
translation,181,149,results,dimensions,of,usefulness,dimensions of usefulness,0.5643635988235474
translation,181,149,results,dimensions,of,satisfaction,dimensions of satisfaction,0.5573213696479797
translation,181,149,results,dimensions,of,knowledge exploration,dimensions of knowledge exploration,0.5229795575141907
translation,182,149,ablation-analysis,small decoder,with,large drop out ratio,small decoder with large drop out ratio,0.6499158143997192
translation,182,149,ablation-analysis,large drop out ratio,performs,best,large drop out ratio performs best,0.6410380601882935
translation,182,149,ablation-analysis,sentence coverage,has,small decoder,sentence coverage has small decoder,0.5758109092712402
translation,182,175,ablation-analysis,macro dpps,have,more diverse embedding presentation,macro dpps have more diverse embedding presentation,0.5599144697189331
translation,182,175,ablation-analysis,more diverse embedding presentation,compared to,vanilla model,more diverse embedding presentation compared to vanilla model,0.6596633791923523
translation,182,175,ablation-analysis,ablation analysis,has,macro dpps,ablation analysis has macro dpps,0.5456485152244568
translation,182,23,baselines,dpps,propose,two kinds of methods,dpps propose two kinds of methods,0.6527647972106934
translation,182,112,experiments,micro dpps,for,each summary,micro dpps for each summary,0.6938201189041138
translation,182,112,experiments,each summary,sample,20 points,each summary sample 20 points,0.6709323525428772
translation,182,112,experiments,20 points,to generate,gaussian mixture distributions,20 points to generate gaussian mixture distributions,0.6300140619277954
translation,182,31,hyperparameters,residual connection,has,"he et al. , 2016 )","residual connection has he et al. , 2016 )",0.536626935005188
translation,182,31,hyperparameters,batch normalization,has,"ioffe and szegedy , 2015 )","batch normalization has ioffe and szegedy , 2015 )",0.5399115681648254
translation,182,31,hyperparameters,hyperparameters,has,residual connection,hyperparameters has residual connection,0.5008456707000732
translation,182,103,hyperparameters,20000 words summary dictionary,with,byte pair encoding ( bpe ),20000 words summary dictionary with byte pair encoding ( bpe ),0.62863689661026
translation,182,104,hyperparameters,word embeddings,pretrained on,training corpus,word embeddings pretrained on training corpus,0.7393761873245239
translation,182,104,hyperparameters,training corpus,using,fasttext,training corpus using fasttext,0.6243706345558167
translation,182,104,hyperparameters,hyperparameters,has,word embeddings,hyperparameters has word embeddings,0.4784160554409027
translation,182,107,hyperparameters,cnn channels,to,256,cnn channels to 256,0.5962822437286377
translation,182,108,hyperparameters,20 blocks,with,kernel size 5,20 blocks with kernel size 5,0.6548394560813904
translation,182,108,hyperparameters,20 blocks,with,kernel size 3,20 blocks with kernel size 3,0.6480233669281006
translation,182,108,hyperparameters,4 blocks,with,kernel size 3,4 blocks with kernel size 3,0.6597726345062256
translation,182,108,hyperparameters,encoder,has,20 blocks,encoder has 20 blocks,0.6091830134391785
translation,182,108,hyperparameters,decoder,has,4 blocks,decoder has 4 blocks,0.6230151057243347
translation,182,108,hyperparameters,hyperparameters,has,encoder,hyperparameters has encoder,0.5368890762329102
translation,182,108,hyperparameters,hyperparameters,has,decoder,hyperparameters has decoder,0.5540591478347778
translation,182,111,hyperparameters,macro dpps,choose,top 30 points,macro dpps choose top 30 points,0.6788967251777649
translation,182,111,hyperparameters,top 30 points,when optimizing,diversity,top 30 points when optimizing diversity,0.7219815850257874
translation,182,111,hyperparameters,stride,of,20,stride of 20,0.6561552286148071
translation,182,111,hyperparameters,20,for,equidistant,20 for equidistant,0.652225136756897
translation,182,111,hyperparameters,hyperparameters,In,macro dpps,hyperparameters In macro dpps,0.5070834755897522
translation,182,113,hyperparameters,model,with,nesterov 's accelerated gradient method,model with nesterov 's accelerated gradient method,0.6446482539176941
translation,182,113,hyperparameters,nesterov 's accelerated gradient method,using,momentum of 0.99,nesterov 's accelerated gradient method using momentum of 0.99,0.6749335527420044
translation,182,113,hyperparameters,nesterov 's accelerated gradient method,using,renormalized gradients,nesterov 's accelerated gradient method using renormalized gradients,0.6695379018783569
translation,182,113,hyperparameters,renormalized gradients,when,norm,renormalized gradients when norm,0.6545451283454895
translation,182,113,hyperparameters,norm,exceeded,0.1,norm exceeded 0.1,0.6886825561523438
translation,182,113,hyperparameters,hyperparameters,train,model,hyperparameters train model,0.7064553499221802
translation,182,114,hyperparameters,beam search size,is,5,beam search size is 5,0.6676753759384155
translation,182,114,hyperparameters,beam search size,apply,dropout,beam search size apply dropout,0.6702647805213928
translation,182,114,hyperparameters,dropout,of,0.1,dropout of 0.1,0.6162222623825073
translation,182,114,hyperparameters,0.1,to,embeddings and linear transform layers,0.1 to embeddings and linear transform layers,0.5224214196205139
translation,182,114,hyperparameters,hyperparameters,has,beam search size,hyperparameters has beam search size,0.5214741826057434
translation,182,6,model,diverse convolutional seq2seq model ( divcnn seq2seq ),using,determinantal point processes methods ( micro dpps and macro dpps ),diverse convolutional seq2seq model ( divcnn seq2seq ) using determinantal point processes methods ( micro dpps and macro dpps ),0.6682870388031006
translation,182,6,model,determinantal point processes methods ( micro dpps and macro dpps ),to produce,attention distribution,determinantal point processes methods ( micro dpps and macro dpps ) to produce attention distribution,0.6876917481422424
translation,182,6,model,attention distribution,considering,quality and diversity,attention distribution considering quality and diversity,0.6620019674301147
translation,182,6,model,model,introduce,diverse convolutional seq2seq model ( divcnn seq2seq ),model introduce diverse convolutional seq2seq model ( divcnn seq2seq ),0.6108251810073853
translation,182,29,model,both encoder and decoder,in,cnn seq2seq,both encoder and decoder in cnn seq2seq,0.5198519825935364
translation,182,29,model,cnn seq2seq,consist of,convolutional blocks,cnn seq2seq consist of convolutional blocks,0.619723916053772
translation,182,29,model,model,has,both encoder and decoder,model has both encoder and decoder,0.5757463574409485
translation,182,30,model,several fully connected layers,for,dimension transformation,several fully connected layers for dimension transformation,0.5766869187355042
translation,182,7,results,di-vcnn seq2seq,achieves,higher level of comprehensiveness,di-vcnn seq2seq achieves higher level of comprehensiveness,0.6795913577079773
translation,182,7,results,higher level of comprehensiveness,compared to,vanilla models,higher level of comprehensiveness compared to vanilla models,0.6901553273200989
translation,182,7,results,higher level of comprehensiveness,compared to,strong baselines,higher level of comprehensiveness compared to strong baselines,0.669033944606781
translation,182,7,results,end to end architecture,has,di-vcnn seq2seq,end to end architecture has di-vcnn seq2seq,0.5550144910812378
translation,182,7,results,results,Without breaking,end to end architecture,results Without breaking end to end architecture,0.6858579516410828
translation,182,148,results,scaled attention,has,lowest jaccard similarity upper bound,scaled attention has lowest jaccard similarity upper bound,0.5729817152023315
translation,182,148,results,results,has,scaled attention,results has scaled attention,0.5325178503990173
translation,182,156,results,four models,in,strong baselines,four models in strong baselines,0.5364668965339661
translation,182,156,results,four models,reach,nearly 40 points,four models reach nearly 40 points,0.7485338449478149
translation,182,156,results,four models,perform,poorly,four models perform poorly,0.6415774822235107
translation,182,156,results,nearly 40 points,in,rouge - 1,nearly 40 points in rouge - 1,0.5799680948257446
translation,182,156,results,poorly,on,article-related metrics,poorly on article-related metrics,0.5423969030380249
translation,182,156,results,large model parameters and dictionaries,has,four models,large model parameters and dictionaries has four models,0.5403791666030884
translation,182,156,results,results,With,large model parameters and dictionaries,results With large model parameters and dictionaries,0.5948435068130493
translation,182,158,results,three models,with,multiple modules,three models with multiple modules,0.6613246202468872
translation,182,158,results,three models,with,post-processing,three models with post-processing,0.6728419065475464
translation,182,158,results,three models,has,bottomup model,three models has bottomup model,0.5854178667068481
translation,182,158,results,multiple modules,has,bottomup model,multiple modules has bottomup model,0.5826117396354675
translation,182,158,results,post-processing,has,bottomup model,post-processing has bottomup model,0.5677847862243652
translation,182,158,results,bottomup model,has,relatively good jaccard similarity upper bound,bottomup model has relatively good jaccard similarity upper bound,0.5636751651763916
translation,182,159,results,better score,on,js,better score on js,0.559788703918457
translation,182,159,results,rl rerank model,has,better score,rl rerank model has better score,0.5239644050598145
translation,182,159,results,sumgan,has,better novel score,sumgan has better novel score,0.5930333733558655
translation,182,159,results,results,has,rl rerank model,results has rl rerank model,0.544592022895813
translation,182,160,results,divcnn seq2seq,improves,js and novel points,divcnn seq2seq improves js and novel points,0.6742329001426697
translation,182,160,results,divcnn seq2seq,raises,rouge score,divcnn seq2seq raises rouge score,0.5814439058303833
translation,182,160,results,vanilla cnn seq2seq,has,divcnn seq2seq,vanilla cnn seq2seq has divcnn seq2seq,0.553551971912384
translation,182,160,results,results,Compared to,vanilla cnn seq2seq,results Compared to vanilla cnn seq2seq,0.5947425961494446
translation,182,161,results,divcnn seq2seq,achieves,best,divcnn seq2seq achieves best,0.6370207667350769
translation,182,161,results,best,in,novel,best in novel,0.6199029088020325
translation,182,161,results,second and third,in,js and sc,second and third in js and sc,0.5712685585021973
translation,182,161,results,strong baselines,has,divcnn seq2seq,strong baselines has divcnn seq2seq,0.5361729264259338
translation,182,161,results,results,Compared with,strong baselines,results Compared with strong baselines,0.7150382995605469
translation,182,169,results,divcnn,performs,better,divcnn performs better,0.7161553502082825
translation,182,169,results,divcnn,reaches,incredible rouge scores,divcnn reaches incredible rouge scores,0.6893051862716675
translation,182,169,results,better,than,best baselines,better than best baselines,0.6059743165969849
translation,182,169,results,best baselines,on,newsroom,best baselines on newsroom,0.5502648949623108
translation,182,169,results,incredible rouge scores,has,more than 60,incredible rouge scores has more than 60,0.5605687499046326
translation,182,169,results,results,shows,divcnn,results shows divcnn,0.644519567489624
translation,182,170,results,little impact,on,performance,little impact on performance,0.5643328428268433
translation,182,170,results,performance,of,divcnn,performance of divcnn,0.6437061429023743
translation,182,170,results,results,has,compression ratio,results has compression ratio,0.5637606382369995
translation,182,170,results,results,has,article length,results has article length,0.4972985088825226
translation,182,171,results,di-vcnn,prefers,short summaries,di-vcnn prefers short summaries,0.7129057049751282
translation,182,171,results,results,show,di-vcnn,results show di-vcnn,0.601993978023529
translation,182,181,results,other five open datasets,show,divcnn seq2seq,other five open datasets show divcnn seq2seq,0.5975720882415771
translation,182,181,results,divcnn seq2seq,improve,comprehensiveness,divcnn seq2seq improve comprehensiveness,0.6780520677566528
translation,182,181,results,comprehensiveness,of,summaries,comprehensiveness of summaries,0.547878623008728
translation,182,181,results,results,on,cnn - dm,results on cnn - dm,0.5473266839981079
translation,182,230,results,extractive summarization,reaches,better rouge values,extractive summarization reaches better rouge values,0.730134904384613
translation,183,22,baselines,encoder ( enc - 2 ),based on,"deep averaging net ( iyyer et al. , 2015 )","encoder ( enc - 2 ) based on deep averaging net ( iyyer et al. , 2015 )",0.5855725407600403
translation,183,22,baselines,encoder ( enc - 3 ),based on,"transformer ( vaswani et al. , 2017 )","encoder ( enc - 3 ) based on transformer ( vaswani et al. , 2017 )",0.6448118090629578
translation,183,24,baselines,average ( elmo -a ) and max ( elm o - m ),over,each dimension,average ( elmo -a ) and max ( elm o - m ) over each dimension,0.7256530523300171
translation,183,24,baselines,each dimension,of,"all elmo ( peters et al. , 2018 ) word embeddings","each dimension of all elmo ( peters et al. , 2018 ) word embeddings",0.5202925205230713
translation,183,24,baselines,"all elmo ( peters et al. , 2018 ) word embeddings",of,input text,"all elmo ( peters et al. , 2018 ) word embeddings of input text",0.4913846552371979
translation,183,27,baselines,bilstm encoder,producing,representation,bilstm encoder producing representation,0.6776102781295776
translation,183,27,baselines,representation,of,"4,096 dimensions","representation of 4,096 dimensions",0.6090222001075745
translation,183,27,baselines,infersent,has,bilstm encoder,infersent has bilstm encoder,0.5499365329742432
translation,183,27,baselines,baselines,has,infersent,baselines has infersent,0.6182729005813599
translation,183,67,baselines,7 systems,are,lead3 sentences,7 systems are lead3 sentences,0.6077322363853455
translation,183,67,baselines,7 systems,are,textrank,7 systems are textrank,0.5699343085289001
translation,183,67,baselines,7 systems,are,extractive oracle ' fragments ' system,7 systems are extractive oracle ' fragments ' system,0.5986835956573486
translation,183,67,baselines,7 systems,are,abstractive model,7 systems are abstractive model,0.5701305270195007
translation,183,67,baselines,lead3 sentences,of,article,lead3 sentences of article,0.6208621263504028
translation,183,67,baselines,textrank,with,word limit,textrank with word limit,0.6419239044189453
translation,183,67,baselines,word limit,of,50,word limit of 50,0.6697085499763489
translation,183,67,baselines,abstractive model,trained on,newsroom training data,abstractive model trained on newsroom training data,0.7090237736701965
translation,183,67,baselines,pointer - generator,trained on,cnn / dailymail data set,pointer - generator trained on cnn / dailymail data set,0.7200192213058472
translation,183,67,baselines,cnn / dailymail data set,on,complete and a subset of newsroom training set,cnn / dailymail data set on complete and a subset of newsroom training set,0.5045293569564819
translation,183,67,baselines,baselines,has,7 systems,baselines has 7 systems,0.5891025066375732
translation,183,21,experiments,dense low-dimenional representation,of,texts,dense low-dimenional representation of texts,0.5480258464813232
translation,183,21,experiments,dense low-dimenional representation,test,seven representations,dense low-dimenional representation test seven representations,0.6870954036712646
translation,183,21,experiments,seven representations,covering,sentence embedding,seven representations covering sentence embedding,0.7527371048927307
translation,183,21,experiments,seven representations,variants of,un-contextualized word embedding,seven representations variants of un-contextualized word embedding,0.7417625784873962
translation,183,21,experiments,seven representations,variants of,contextualized word embedding,seven representations variants of contextualized word embedding,0.7413355112075806
translation,183,21,experiments,seven representations,variants of,contextualized word embedding,seven representations variants of contextualized word embedding,0.7413355112075806
translation,183,23,model,input text,to,512 - dimensional vector,input text to 512 - dimensional vector,0.5352760553359985
translation,183,7,results,our experimental results,show,max value,our experimental results show max value,0.6439089179039001
translation,183,7,results,max value,over,each dimension,max value over each dimension,0.6960561871528625
translation,183,7,results,each dimension,of,summary elmo word embeddings,each dimension of summary elmo word embeddings,0.5371089577674866
translation,183,7,results,summary elmo word embeddings,is,good representation,summary elmo word embeddings is good representation,0.4285701811313629
translation,183,7,results,good representation,results in,high correlation,good representation results in high correlation,0.5834806561470032
translation,183,7,results,high correlation,with,human ratings,high correlation with human ratings,0.6541269421577454
translation,183,7,results,results,show,max value,results show max value,0.6421717405319214
translation,183,7,results,results,has,our experimental results,results has our experimental results,0.561657190322876
translation,183,8,results,cosine similarity,of,all encoders,cosine similarity of all encoders,0.5859078168869019
translation,183,8,results,cosine similarity,yields,high correlation,cosine similarity yields high correlation,0.6701906323432922
translation,183,8,results,high correlation,with,manual scores,high correlation with manual scores,0.6142670512199402
translation,183,8,results,manual scores,in,reference -free setting,manual scores in reference -free setting,0.5214647054672241
translation,183,8,results,results,Averaging,cosine similarity,results Averaging cosine similarity,0.7134217023849487
translation,183,33,results,r1 - f,correlates better,human ratings,r1 - f correlates better human ratings,0.7499603629112244
translation,183,33,results,human ratings,on,duc '01,human ratings on duc '01,0.561564564704895
translation,183,33,results,human ratings,on,duc '02,human ratings on duc '02,0.5553787350654602
translation,183,33,results,r2 - r,works,extremely well,r2 - r works extremely well,0.6260931491851807
translation,183,33,results,extremely well,on,duc '02,extremely well on duc '02,0.598136305809021
translation,183,35,results,f-measure,correlate well with,human,f-measure correlate well with human,0.7203381061553955
translation,183,35,results,results,has,f-measure,results has f-measure,0.5699124336242676
translation,183,36,results,single optimal representation,gives,best correlation,single optimal representation gives best correlation,0.6384246349334717
translation,183,36,results,best correlation,on,both data sets,best correlation on both data sets,0.5242327451705933
translation,183,37,results,clear increase,of,performance,clear increase of performance,0.6302359104156494
translation,183,37,results,performance,from,duc '01 to duc '02,performance from duc '01 to duc '02,0.5596575736999512
translation,183,58,results,noticeable difference,between,duc '01 and duc '02,noticeable difference between duc '01 and duc '02,0.6852102875709534
translation,183,58,results,duc '01 and duc '02,is,performance,duc '01 and duc '02 is performance,0.5974453687667847
translation,183,58,results,performance,of,elmo - a word embeddings,performance of elmo - a word embeddings,0.5326766967773438
translation,183,58,results,results,has,noticeable difference,results has noticeable difference,0.5840412974357605
translation,183,60,results,better spearman correlation,averaging,elmo embeddings,better spearman correlation averaging elmo embeddings,0.686702311038971
translation,183,60,results,elmo -m,has,better spearman correlation,elmo -m has better spearman correlation,0.5927169322967529
translation,183,61,results,elmo -a,leads to,lower correlations,elmo -a leads to lower correlations,0.700575590133667
translation,183,61,results,lower correlations,than,other encoders,lower correlations than other encoders,0.5808978080749512
translation,183,61,results,duc '02,has,elmo -a,duc '02 has elmo -a,0.6126646399497986
translation,183,61,results,results,On,duc '02,results On duc '02,0.5411984920501709
translation,183,83,results,max value,over,each dimension,max value over each dimension,0.6960561871528625
translation,183,83,results,each dimension,of,elmo word embedding,each dimension of elmo word embedding,0.5397942066192627
translation,183,83,results,elmo word embedding,of,input text,elmo word embedding of input text,0.5182670950889587
translation,183,83,results,best,when,reference summaries,best when reference summaries,0.6700063943862915
translation,183,83,results,results,has,max value,results has max value,0.5523287057876587
translation,183,84,results,embedding,between,system summary and document,embedding between system summary and document,0.665690004825592
translation,183,84,results,averaged cosine similarities,of,all seven representation,averaged cosine similarities of all seven representation,0.5893215537071228
translation,183,84,results,averaged cosine similarities,gives,good results,averaged cosine similarities gives good results,0.6258453726768494
translation,183,84,results,embedding,has,averaged cosine similarities,embedding has averaged cosine similarities,0.5717479586601257
translation,183,84,results,system summary and document,has,averaged cosine similarities,system summary and document has averaged cosine similarities,0.5668862462043762
translation,183,84,results,results,compare,embedding,results compare embedding,0.5606774687767029
translation,183,85,results,elmo-m evaluation,ranks,consistently better,elmo-m evaluation ranks consistently better,0.6940414309501648
translation,183,85,results,consistently better,than,rouge - f,consistently better than rouge - f,0.6097886562347412
translation,183,85,results,consistently better,for,all evaluation settings,consistently better for all evaluation settings,0.5816487669944763
translation,183,85,results,rouge - f,for,all evaluation settings,rouge - f for all evaluation settings,0.584113359451294
translation,183,85,results,results,has,elmo-m evaluation,results has elmo-m evaluation,0.551050066947937
translation,184,49,ablation-analysis,lead bias,introduced by,journalistic convention,lead bias introduced by journalistic convention,0.7183019518852234
translation,184,49,ablation-analysis,journalistic convention,of writing,inverted pyramid structure,journalistic convention of writing inverted pyramid structure,0.47112661600112915
translation,184,49,ablation-analysis,inverted pyramid structure,placing,most important information,inverted pyramid structure placing most important information,0.6796525716781616
translation,184,49,ablation-analysis,most important information,in,beginning,most important information in beginning,0.49726250767707825
translation,184,49,ablation-analysis,beginning,of,article,beginning of article,0.6241363286972046
translation,184,49,ablation-analysis,ablation analysis,has,lead bias,ablation analysis has lead bias,0.43412646651268005
translation,184,173,ablation-analysis,unsupervised fine-tuning,of,ted,unsupervised fine-tuning of ted,0.562082827091217
translation,184,173,ablation-analysis,ted,improves upon,pretrained model,ted improves upon pretrained model,0.7263737916946411
translation,184,173,ablation-analysis,pretrained model,by,2.75%/1.06%/2.37 %,pretrained model by 2.75%/1.06%/2.37 %,0.5391822457313538
translation,184,173,ablation-analysis,2.75%/1.06%/2.37 %,on,rouge-1/rouge-2/rouge -l,2.75%/1.06%/2.37 % on rouge-1/rouge-2/rouge -l,0.5494572520256042
translation,184,173,ablation-analysis,nyt,has,unsupervised fine-tuning,nyt has unsupervised fine-tuning,0.6118532419204712
translation,184,173,ablation-analysis,ablation analysis,In,nyt,ablation analysis In nyt,0.539185106754303
translation,184,58,baselines,sentencepiece tokenization,to alleviates,long-standing out -ofvocabulary ( oov ) problem,sentencepiece tokenization to alleviates long-standing out -ofvocabulary ( oov ) problem,0.6713878512382507
translation,184,58,baselines,long-standing out -ofvocabulary ( oov ) problem,in,language generation,long-standing out -ofvocabulary ( oov ) problem in language generation,0.47439414262771606
translation,184,77,baselines,open vocabulary problem,adopt,"sentence - piece ( kudo and richardson , 2018 )","open vocabulary problem adopt sentence - piece ( kudo and richardson , 2018 )",0.6253228187561035
translation,184,77,baselines,open vocabulary problem,adopt,data-driven method,open vocabulary problem adopt data-driven method,0.6576604843139648
translation,184,77,baselines,data-driven method,that trains,tokenization models,data-driven method that trains tokenization models,0.6585986018180847
translation,184,77,baselines,tokenization models,from,sentences,tokenization models from sentences,0.5594267249107361
translation,184,77,baselines,sentences,in,large-scale corpora,sentences in large-scale corpora,0.5221712589263916
translation,184,77,baselines,"sentence - piece ( kudo and richardson , 2018 )",has,data-driven method,"sentence - piece ( kudo and richardson , 2018 ) has data-driven method",0.5490286946296692
translation,184,164,baselines,without supervised finetuning,with,ground- truths summaries,without supervised finetuning with ground- truths summaries,0.6872601509094238
translation,184,164,baselines,unsupervised abstractive systems,has,without supervised finetuning,unsupervised abstractive systems has without supervised finetuning,0.574931800365448
translation,184,164,baselines,gpt - 2,has,radford et al. ( 2019 ),gpt - 2 has radford et al. ( 2019 ),0.5507380962371826
translation,184,164,baselines,baselines,has,unsupervised abstractive systems,baselines has unsupervised abstractive systems,0.5547484159469604
translation,184,165,baselines,baselines,has,unsupervised extractive systems,baselines has unsupervised extractive systems,0.5726574063301086
translation,184,166,baselines,pgnet,has,"see et al. , 2017 )","pgnet has see et al. , 2017 )",0.6141421794891357
translation,184,166,baselines,refresh,has,"narayan et al. , 2018 )","refresh has narayan et al. , 2018 )",0.5560245513916016
translation,184,166,baselines,sumo,has,"liu et al. , 2019 b )","sumo has liu et al. , 2019 b )",0.5812009572982788
translation,184,166,baselines,baselines,has,supervised abstractive and abstractive,baselines has supervised abstractive and abstractive,0.5890598893165588
translation,184,7,model,transformerbased unsupervised abstractive summarization,with pretraining on,large-scale data,transformerbased unsupervised abstractive summarization with pretraining on large-scale data,0.6399285793304443
translation,184,7,model,ted,has,transformerbased unsupervised abstractive summarization,ted has transformerbased unsupervised abstractive summarization,0.5126869678497314
translation,184,7,model,model,propose,ted,model propose ted,0.7441897392272949
translation,184,8,model,lead bias,in,news articles,lead bias in news articles,0.5045384764671326
translation,184,8,model,lead bias,to pretrain,model,lead bias to pretrain model,0.7766086459159851
translation,184,8,model,model,on,millions of unlabeled corpora,model on millions of unlabeled corpora,0.5042349696159363
translation,184,8,model,model,leverage,lead bias,model leverage lead bias,0.7839990258216858
translation,184,9,model,target domains,through,theme modeling,target domains through theme modeling,0.7014574408531189
translation,184,9,model,denoising autoencoder,to enhance,quality of generated summaries,denoising autoencoder to enhance quality of generated summaries,0.664302408695221
translation,184,45,model,finetuned,with,theme modeling,finetuned with theme modeling,0.6211744546890259
translation,184,45,model,finetuned,with,denoising,finetuned with denoising,0.6509778499603271
translation,184,45,model,denoising,on,in- domain data,denoising on in- domain data,0.5605427622795105
translation,184,46,model,ted,utilizes,transformer - based encoder-decoder structure,ted utilizes transformer - based encoder-decoder structure,0.6582337021827698
translation,184,46,model,ted,utilizes,pretraining,ted utilizes pretraining,0.6696117520332336
translation,184,46,model,pretraining,leverages,large-scale corpora,pretraining leverages large-scale corpora,0.7268871665000916
translation,184,46,model,large-scale corpora,containing,millions of unlabeled articles,large-scale corpora containing millions of unlabeled articles,0.5908538699150085
translation,184,46,model,model,has,ted,model has ted,0.6839973330497742
translation,184,48,model,lead bias,in,news articles,lead bias in news articles,0.5045384764671326
translation,184,48,model,news articles,to pretrain,ted,news articles to pretrain ted,0.7484747171401978
translation,184,48,model,model,leverage,lead bias,model leverage lead bias,0.7839990258216858
translation,184,51,model,summarization model,on,large-scale corpus,summarization model on large-scale corpus,0.46622350811958313
translation,184,51,model,large-scale corpus,with,21.4 m news articles,large-scale corpus with 21.4 m news articles,0.5891541242599487
translation,184,51,model,model,pretrain,summarization model,model pretrain summarization model,0.6990113854408264
translation,184,53,model,ted,trained with,theme modeling loss,ted trained with theme modeling loss,0.7714146971702576
translation,184,53,model,ted,trained with,denoising autoencoder,ted trained with denoising autoencoder,0.7105205059051514
translation,184,53,model,specific datasets,has,ted,specific datasets has ted,0.5921948552131653
translation,184,56,model,generated summary tokens,adopt,"gumbel -softmax ( jang et al. , 2016 ) estimator","generated summary tokens adopt gumbel -softmax ( jang et al. , 2016 ) estimator",0.6030232310295105
translation,184,56,model,"gumbel -softmax ( jang et al. , 2016 ) estimator",to replace,non-differentiable arg max,"gumbel -softmax ( jang et al. , 2016 ) estimator to replace non-differentiable arg max",0.6572957038879395
translation,184,56,model,model,optimize on,generated summary tokens,model optimize on generated summary tokens,0.7275654673576355
translation,184,116,model,model,has,theme modeling,model has theme modeling,0.577987015247345
translation,184,10,results,all unsupervised abstractive baselines,on,"nyt , cnn / dm and english gigaword datasets","all unsupervised abstractive baselines on nyt , cnn / dm and english gigaword datasets",0.48160356283187866
translation,184,10,results,"nyt , cnn / dm and english gigaword datasets",with,various document styles,"nyt , cnn / dm and english gigaword datasets with various document styles",0.5819898843765259
translation,184,10,results,ted,has,outperforms,ted has outperforms,0.6919299960136414
translation,184,10,results,outperforms,has,all unsupervised abstractive baselines,outperforms has all unsupervised abstractive baselines,0.587285578250885
translation,184,10,results,results,has,ted,results has ted,0.5523617267608643
translation,184,60,results,all unsupervised abstractive baselines,on,all datasets,all unsupervised abstractive baselines on all datasets,0.4760516583919525
translation,184,60,results,ted,has,outperforms,ted has outperforms,0.6919299960136414
translation,184,60,results,outperforms,has,all unsupervised abstractive baselines,outperforms has all unsupervised abstractive baselines,0.587285578250885
translation,184,60,results,results,show,ted,results show ted,0.6301622986793518
translation,184,60,results,results,show,outperforms,results show outperforms,0.7454270124435425
translation,184,171,results,ted,has,outperforms,ted has outperforms,0.6919299960136414
translation,184,171,results,outperforms,has,all unsupervised baselines,outperforms has all unsupervised baselines,0.5897190570831299
translation,184,171,results,results,on,english gigaword dataset,results on english gigaword dataset,0.4925307333469391
translation,184,172,results,results,on,nyt and cnn / dm datasets,results on nyt and cnn / dm datasets,0.5204682350158691
translation,184,176,results,cnn / dm,TED with,larger model size ( 10l8h ),cnn / dm TED with larger model size ( 10l8h ),0.6355135440826416
translation,184,176,results,compares favorably,with,unsupervised extractive baselines,compares favorably with unsupervised extractive baselines,0.6696349382400513
translation,184,176,results,cnn / dm,has,outperforms,cnn / dm has outperforms,0.6107370853424072
translation,184,176,results,larger model size ( 10l8h ),has,outperforms,larger model size ( 10l8h ) has outperforms,0.5861984491348267
translation,184,176,results,outperforms,has,all unsupervised abstractive methods,outperforms has all unsupervised abstractive methods,0.5658096075057983
translation,184,176,results,results,In,cnn / dm,results In cnn / dm,0.5469038486480713
translation,184,177,results,large scale webpage textual data,by,significant margins,large scale webpage textual data by significant margins,0.5504167675971985
translation,184,177,results,ted,has,outperforms,ted has outperforms,0.6919299960136414
translation,184,177,results,outperforms,has,gpt - 2,outperforms has gpt - 2,0.6368796229362488
translation,184,177,results,results,Note,ted,results Note ted,0.5964523553848267
translation,185,48,baselines,basic sequence - to-sequence model,with,attention mechanism,basic sequence - to-sequence model with attention mechanism,0.5902361273765564
translation,185,48,baselines,basic sequence - to-sequence model,with,coverage mechanism,basic sequence - to-sequence model with coverage mechanism,0.6154921650886536
translation,185,48,baselines,pointer - generator system,plus,coverage mechanism,pointer - generator system plus coverage mechanism,0.6949628591537476
translation,185,49,baselines,distract,has,attention,distract has attention,0.5993187427520752
translation,185,83,results,pure copy system,could produce,equally or slightly better results,pure copy system could produce equally or slightly better results,0.6711797714233398
translation,185,83,results,equally or slightly better results,in terms of,word-matching metrics,equally or slightly better results in terms of word-matching metrics,0.6598979830741882
translation,185,83,results,results,see that,pure copy system,results see that pure copy system,0.6914342045783997
translation,185,89,results,pure copy system,performs,similarly,pure copy system performs similarly,0.652249813079834
translation,185,89,results,results,find,pure copy system,results find pure copy system,0.5986184477806091
translation,186,6,model,review,with,optimized rubrics,review with optimized rubrics,0.6635445356369019
translation,186,6,model,coursemirror,has,mobile insitu reflections,coursemirror has mobile insitu reflections,0.5956679582595825
translation,186,7,model,coursemirror,uses,mobile interface,coursemirror uses mobile interface,0.6141155362129211
translation,186,7,model,coursemirror,collect,reflective responses,coursemirror collect reflective responses,0.6258522272109985
translation,186,7,model,mobile interface,to administer,prompts,mobile interface to administer prompts,0.6896332502365112
translation,186,7,model,reflective responses,for,set of instructorassigned course lectures,reflective responses for set of instructorassigned course lectures,0.6139593124389648
translation,186,7,model,model,has,coursemirror,model has coursemirror,0.5888168215751648
translation,186,15,model,mobile application,for collecting and sharing,learners ' in - situ reflections,mobile application for collecting and sharing learners ' in - situ reflections,0.462268203496933
translation,186,15,model,learners ' in - situ reflections,in,large classrooms,learners ' in - situ reflections in large classrooms,0.5533394813537598
translation,186,15,model,coursemirror,has,mobile application,coursemirror has mobile application,0.5476866960525513
translation,186,15,model,model,developed,coursemirror,model developed coursemirror,0.6657953262329102
translation,187,141,baselines,pointer - generator,uses,encoderdecoder architecture,pointer - generator uses encoderdecoder architecture,0.5871566534042358
translation,187,141,baselines,encoderdecoder architecture,with,attention and copy mechanism,encoderdecoder architecture with attention and copy mechanism,0.6169171333312988
translation,187,141,baselines,transformer,adopts,decoder-only transformer architecture,transformer adopts decoder-only transformer architecture,0.6508122086524963
translation,187,129,experiments,stanford corenlp,achieves,highest f1 -score ( 35.3 % ),stanford corenlp achieves highest f1 -score ( 35.3 % ),0.6398555636405945
translation,187,129,experiments,spacy resolver,has,highest precision ( 59.2 % ),spacy resolver has highest precision ( 59.2 % ),0.5468355417251587
translation,187,71,model,model,construct,sizeable sentence fusion dataset,model construct sizeable sentence fusion dataset,0.7066258788108826
translation,187,128,results,three resolvers,exhibit,similar performance,three resolvers exhibit similar performance,0.6640345454216003
translation,187,128,results,scores,on,identifying points of correspondence,scores on identifying points of correspondence,0.5658888816833496
translation,187,128,results,identifying points of correspondence,are,satisfying,identifying points of correspondence are satisfying,0.5717938542366028
translation,187,128,results,identifying points of correspondence,less than,satisfying,identifying points of correspondence less than satisfying,0.6363779306411743
translation,187,128,results,results,has,three resolvers,results has three resolvers,0.5142579078674316
translation,187,151,results,transformer,has,significantly outperforms,transformer has significantly outperforms,0.6344193816184998
translation,187,151,results,significantly outperforms,has,other methods,significantly outperforms has other methods,0.5518229007720947
translation,187,151,results,results,has,transformer,results has transformer,0.4226538836956024
translation,188,100,baselines,mrw model,to,sentences,mrw model to sentences,0.5435046553611755
translation,188,100,baselines,sentences,consisted of,noun words,sentences consisted of noun words,0.5919350385665894
translation,188,124,hyperparameters,tf * idf and kl - distance,to,80 and 0.9,tf * idf and kl - distance to 80 and 0.9,0.5873478055000305
translation,188,124,hyperparameters,hyperparameters,set,tf * idf and kl - distance,hyperparameters set tf * idf and kl - distance,0.6344570517539978
translation,188,125,hyperparameters,minimum entropy value,was,0.050,minimum entropy value was 0.050,0.604694128036499
translation,188,125,hyperparameters,number of topics and documents,were,500 and 600,number of topics and documents were 500 and 600,0.620437741279602
translation,188,125,hyperparameters,hyperparameters,has,minimum entropy value,hyperparameters has minimum entropy value,0.5406230688095093
translation,188,125,hyperparameters,hyperparameters,has,number of topics and documents,hyperparameters has number of topics and documents,0.5270131826400757
translation,188,119,results,our approach,has,outperforms,our approach has outperforms,0.6385829448699951
translation,188,119,results,event & topic,has,outperforms,event & topic has outperforms,0.6633442640304565
translation,188,119,results,outperforms,has,other baselines,outperforms has other baselines,0.5879674553871155
translation,188,119,results,results,shows,our approach,results shows our approach,0.7114341259002686
translation,188,131,results,rouge - 1,obtained by,our approach,rouge - 1 obtained by our approach,0.6102277040481567
translation,188,131,results,rouge - 1,was,best,rouge - 1 was best,0.6587841510772705
translation,188,131,results,best,compared to,baselines,best compared to baselines,0.6979286074638367
translation,189,6,model,transformer - based model,to enhance,copy mechanism,transformer - based model to enhance copy mechanism,0.7018178701400757
translation,189,6,model,model,propose,transformer - based model,model propose transformer - based model,0.6238863468170166
translation,189,7,model,importance,of,each source word,importance of each source word,0.5774872303009033
translation,189,7,model,each source word,based on,degree centrality,each source word based on degree centrality,0.598080039024353
translation,189,7,model,degree centrality,with,directed graph,degree centrality with directed graph,0.6208838820457458
translation,189,7,model,directed graph,built by,self-attention layer,directed graph built by self-attention layer,0.7410392761230469
translation,189,7,model,self-attention layer,in,transformer,self-attention layer in transformer,0.5530276894569397
translation,189,7,model,model,identify,importance,model identify importance,0.6316007971763611
translation,189,8,model,centrality,of,each source word,centrality of each source word,0.621321439743042
translation,189,8,model,each source word,to guide,copy process,each source word to guide copy process,0.7070855498313904
translation,189,8,model,model,use,centrality,model use centrality,0.7163779735565186
translation,189,25,model,self-attention guided copy mechanism ( sagcopy ),aims to encourage,summarizer,self-attention guided copy mechanism ( sagcopy ) aims to encourage summarizer,0.6500228047370911
translation,189,25,model,summarizer,to copy,important source words,summarizer to copy important source words,0.5904564261436462
translation,189,25,model,model,propose,self-attention guided copy mechanism ( sagcopy ),model propose self-attention guided copy mechanism ( sagcopy ),0.6909303069114685
translation,189,26,model,selfattention layer,in,transformer,selfattention layer in transformer,0.5782650709152222
translation,189,26,model,selfattention layer,builds,directed graph,selfattention layer builds directed graph,0.6234243512153625
translation,189,26,model,directed graph,whose,vertices,directed graph whose vertices,0.6783057451248169
translation,189,26,model,directed graph,whose,edges,directed graph whose edges,0.6817029714584351
translation,189,26,model,vertices,represent,source words,vertices represent source words,0.5662447214126587
translation,189,26,model,edges,defined in terms of,relevance score,edges defined in terms of relevance score,0.70111483335495
translation,189,26,model,relevance score,between,each pair of source words,relevance score between each pair of source words,0.5950155854225159
translation,189,26,model,each pair of source words,by,dot-product attention,each pair of source words by dot-product attention,0.5379185080528259
translation,189,26,model,dot-product attention,between,query q,dot-product attention between query q,0.6163852214813232
translation,189,26,model,dot-product attention,between,key k,dot-product attention between key k,0.669940710067749
translation,189,26,model,model,has,selfattention layer,model has selfattention layer,0.5475683212280273
translation,189,31,model,centrality score,as,guidance,centrality score as guidance,0.5767269730567932
translation,189,31,model,centrality score,for,copy distribution,centrality score for copy distribution,0.5796776413917542
translation,189,31,model,guidance,for,copy distribution,guidance for copy distribution,0.6515268683433533
translation,189,31,model,model,utilize,centrality score,model utilize centrality score,0.5946620106697083
translation,189,32,model,dotproduct attention,to,centrality - aware function,dotproduct attention to centrality - aware function,0.5669379234313965
translation,189,32,model,model,extend,dotproduct attention,model extend dotproduct attention,0.6719843149185181
translation,189,33,model,auxiliary loss,computed by,divergence,auxiliary loss computed by divergence,0.5974763035774231
translation,189,33,model,divergence,between,copy distribution and the centrality distribution,divergence between copy distribution and the centrality distribution,0.6092087626457214
translation,189,33,model,model,to focus on,important words,model to focus on important words,0.757030725479126
translation,189,33,model,model,introduce,auxiliary loss,model introduce auxiliary loss,0.6877086758613586
translation,189,40,model,copy module,upon,transformer - based summarization model,copy module upon transformer - based summarization model,0.598304808139801
translation,189,40,model,model,explore,copy module,model explore copy module,0.7487765550613403
translation,189,10,results,significantly outperform,on,cnn / daily mail dataset,significantly outperform on cnn / daily mail dataset,0.536423921585083
translation,189,10,results,significantly outperform,on,the gigaword dataset,significantly outperform on the gigaword dataset,0.5139139890670776
translation,189,10,results,baseline methods,on,cnn / daily mail dataset,baseline methods on cnn / daily mail dataset,0.4572029411792755
translation,189,10,results,baseline methods,on,the gigaword dataset,baseline methods on the gigaword dataset,0.5112111568450928
translation,189,10,results,our proposed models,has,significantly outperform,our proposed models has significantly outperform,0.5903535485267639
translation,189,10,results,significantly outperform,has,baseline methods,significantly outperform has baseline methods,0.568272054195404
translation,189,10,results,results,has,our proposed models,results has our proposed models,0.5751316547393799
translation,190,155,experiments,sentence compressions,evaluated by,5 - gram language model,sentence compressions evaluated by 5 - gram language model,0.694011390209198
translation,190,155,experiments,5 - gram language model,trained on,"gigaword ( graff , 2003 )","5 - gram language model trained on gigaword ( graff , 2003 )",0.6863933801651001
translation,190,155,experiments,"gigaword ( graff , 2003 )",by,"srilm ( stolcke , 2002 )","gigaword ( graff , 2003 ) by srilm ( stolcke , 2002 )",0.5765938758850098
translation,190,153,hyperparameters,maxent classifiers,used for,tree - based compression,maxent classifiers used for tree - based compression,0.6463143825531006
translation,190,153,hyperparameters,hyperparameters,has,maxent classifiers,hyperparameters has maxent classifiers,0.5257952809333801
translation,190,154,hyperparameters,beam size,fixed at,2000,beam size fixed at 2000,0.7113165855407715
translation,190,154,hyperparameters,hyperparameters,has,beam size,hyperparameters has beam size,0.516274631023407
translation,190,7,model,various indicative metrics,such as,linguistic motivation,various indicative metrics such as linguistic motivation,0.5964393615722656
translation,190,7,model,various indicative metrics,such as,query relevance,various indicative metrics such as query relevance,0.6151805520057678
translation,190,7,model,various indicative metrics,into,compression process,various indicative metrics into compression process,0.6127976179122925
translation,190,7,model,compression process,by deriving,novel formulation,compression process by deriving novel formulation,0.6772399544715881
translation,190,7,model,novel formulation,of,compression scoring function,novel formulation of compression scoring function,0.5433789491653442
translation,190,7,model,model,show how to integrate,various indicative metrics,model show how to integrate various indicative metrics,0.7499730587005615
translation,190,30,model,three types of approaches,to,sentence - compressionrule - based,three types of approaches to sentence - compressionrule - based,0.4948102533817291
translation,190,30,model,three types of approaches,to,sequence - based,three types of approaches to sequence - based,0.5156145095825195
translation,190,30,model,three types of approaches,to,tree - based,three types of approaches to tree - based,0.5184096693992615
translation,190,30,model,framework,for,query -specific mds,framework for query -specific mds,0.632449746131897
translation,190,30,model,model,design,three types of approaches,model design three types of approaches,0.6010231971740723
translation,190,33,results,standard document understanding conference ( duc ) 2006 and 2007 corpora,for,queryfocused mds,standard document understanding conference ( duc ) 2006 and 2007 corpora for queryfocused mds,0.6291661858558655
translation,190,33,results,all of our compressionbased summarization models,achieve,statistically significantly better performance,all of our compressionbased summarization models achieve statistically significantly better performance,0.621893584728241
translation,190,33,results,statistically significantly better performance,than,best duc 2006 systems,statistically significantly better performance than best duc 2006 systems,0.5542802214622498
translation,190,159,results,our sentence -compression - based systems,show,statistically significant improvements,our sentence -compression - based systems show statistically significant improvements,0.6088335514068604
translation,190,159,results,statistically significant improvements,over,pure extractive summarization,statistically significant improvements over pure extractive summarization,0.694229781627655
translation,190,159,results,pure extractive summarization,for,r - 2 and r - su4,pure extractive summarization for r - 2 and r - su4,0.6596130132675171
translation,190,159,results,results,has,our sentence -compression - based systems,results has our sentence -compression - based systems,0.5422623753547668
translation,190,161,results,head - driven beam search method,with,multi,head - driven beam search method with multi,0.6604891419410706
translation,190,161,results,multi,-,scorer,multi - scorer,0.7304069995880127
translation,190,161,results,scorer,beats,all systems,scorer beats all systems,0.7462623119354248
translation,190,161,results,scorer,beats,all systems,scorer beats all systems,0.7462623119354248
translation,190,161,results,all systems,on,duc 2006,all systems on duc 2006,0.5475049614906311
translation,190,161,results,all systems,on,duc 2007,all systems on duc 2007,0.5532327890396118
translation,190,161,results,all systems,on,duc 2007,all systems on duc 2007,0.5532327890396118
translation,190,161,results,all systems,on,duc 2007,all systems on duc 2007,0.5532327890396118
translation,190,161,results,duc 2007,except,best system,duc 2007 except best system,0.6070069074630737
translation,190,161,results,best system,in terms of,r - 2 ( p < 0.01 ),best system in terms of r - 2 ( p < 0.01 ),0.7256902456283569
translation,190,161,results,multi,has,scorer,multi has scorer,0.6429223418235779
translation,190,161,results,results,has,head - driven beam search method,results has head - driven beam search method,0.5166283845901489
translation,190,162,results,significantly ( p < 0.01 ) better,than,extractive methods,significantly ( p < 0.01 ) better than extractive methods,0.5798759460449219
translation,190,162,results,significantly ( p < 0.01 ) better,than,rule-based and sequence - based compression methods,significantly ( p < 0.01 ) better than rule-based and sequence - based compression methods,0.5478258728981018
translation,190,162,results,rule-based and sequence - based compression methods,on,duc 2006 and 2007,rule-based and sequence - based compression methods on duc 2006 and 2007,0.5395870804786682
translation,190,162,results,results,has,r - su4 score,results has r - su4 score,0.5281310677528381
translation,190,163,results,our systems,with,learning - based compression,our systems with learning - based compression,0.6603965759277344
translation,190,163,results,learning - based compression,have,considerable compression rates,learning - based compression have considerable compression rates,0.5616042613983154
translation,190,163,results,results,has,our systems,results has our systems,0.5982377529144287
translation,190,175,results,our system,achieves,higher pyramid score,our system achieves higher pyramid score,0.6756440997123718
translation,190,175,results,results,has,our system,results has our system,0.5954442024230957
translation,190,176,results,better non-redundancy,than,jagarlamudi et al . ( 2006 ),better non-redundancy than jagarlamudi et al . ( 2006 ),0.5503129363059998
translation,190,176,results,results,attain,better non-redundancy,results attain better non-redundancy,0.6791654229164124
translation,190,189,results,context- aware and head- driven tree- based compression systems,show,statistically significantly ( p < 0.01 ) higher precisions,context- aware and head- driven tree- based compression systems show statistically significantly ( p < 0.01 ) higher precisions,0.6247718334197998
translation,190,192,results,our head - driven tree - based system,obtains,statistically significantly ( p < 0.01 ) better f1 score,our head - driven tree - based system obtains statistically significantly ( p < 0.01 ) better f1 score,0.5827619433403015
translation,190,192,results,rel - f1,than,all the other systems,rel - f1 than all the other systems,0.6138989329338074
translation,190,192,results,all the other systems,except,rule- based system,all the other systems except rule- based system,0.6492497324943542
translation,190,192,results,grammatical relation evaluation,has,our head - driven tree - based system,grammatical relation evaluation has our head - driven tree - based system,0.5361838340759277
translation,190,192,results,statistically significantly ( p < 0.01 ) better f1 score,has,rel - f1,statistically significantly ( p < 0.01 ) better f1 score has rel - f1,0.5867558717727661
translation,190,192,results,results,For,grammatical relation evaluation,results For grammatical relation evaluation,0.5388291478157043
translation,191,96,experiments,copynet,has,highest socres,copynet has highest socres,0.6175006628036499
translation,191,70,hyperparameters,model vocabulary size,to,4000,model vocabulary size to 4000,0.6017010807991028
translation,191,70,hyperparameters,4000,covers,most of the common characters,4000 covers most of the common characters,0.7937312126159668
translation,191,70,hyperparameters,hyperparameters,limit,model vocabulary size,hyperparameters limit model vocabulary size,0.6298510432243347
translation,191,71,hyperparameters,each character,represented by,random initialized word embedding,each character represented by random initialized word embedding,0.6768345236778259
translation,191,71,hyperparameters,hyperparameters,has,each character,hyperparameters has each character,0.5174843072891235
translation,191,73,hyperparameters,embedding size,is,400,embedding size is 400,0.6189626455307007
translation,191,73,hyperparameters,hidden state size,of,encoderdecoder,hidden state size of encoderdecoder,0.5760331749916077
translation,191,73,hyperparameters,encoderdecoder,is,500,encoderdecoder is 500,0.5995431542396545
translation,191,73,hyperparameters,size,of,gated attention network,size of gated attention network,0.5592377185821533
translation,191,73,hyperparameters,gated attention network,is,1000,gated attention network is 1000,0.5921783447265625
translation,191,74,hyperparameters,adam optimizer,to learn,model parameters,adam optimizer to learn model parameters,0.5990433692932129
translation,191,74,hyperparameters,batch size,set as,32,batch size set as 32,0.6738126873970032
translation,191,74,hyperparameters,hyperparameters,use,adam optimizer,hyperparameters use adam optimizer,0.6090166568756104
translation,191,75,hyperparameters,hyperparameters,has,parameter,hyperparameters has parameter,0.4950053095817566
translation,191,7,model,semantic relevance based neural model,to encourage,high semantic similarity,semantic relevance based neural model to encourage high semantic similarity,0.6679753065109253
translation,191,7,model,high semantic similarity,between,texts and summaries,high semantic similarity between texts and summaries,0.6370698809623718
translation,191,7,model,model,introduce,semantic relevance based neural model,model introduce semantic relevance based neural model,0.6033725738525391
translation,191,8,model,source text,represented by,gated attention encoder,source text represented by gated attention encoder,0.6828600764274597
translation,191,8,model,summary representation,produced by,decoder,summary representation produced by decoder,0.6561325192451477
translation,191,25,model,model,propose,semantic relevance based neural model,model propose semantic relevance based neural model,0.6653788685798645
translation,191,28,model,source texts,produced by,encoder,source texts produced by encoder,0.6581274271011353
translation,191,28,model,summaries,computed by,decoder,summaries computed by decoder,0.7159626483917236
translation,191,29,model,model,introduce,gated attention encoder,model introduce gated attention encoder,0.6355400681495667
translation,191,76,model,both the encoder and decoder,based on,lstm unit,both the encoder and decoder based on lstm unit,0.6827059984207153
translation,191,76,model,model,has,both the encoder and decoder,model has both the encoder and decoder,0.5658589005470276
translation,192,127,baselines,extractive summarization,as,contextual bandit problem,extractive summarization as contextual bandit problem,0.5124821662902832
translation,192,127,baselines,baselines,has,extractive models,baselines has extractive models,0.5605137944221497
translation,192,128,baselines,neusum,is,extractive model,neusum is extractive model,0.6002310514450073
translation,192,128,baselines,neusum,emits,index,neusum emits index,0.7632985711097717
translation,192,128,baselines,extractive model,with,seq2seq architecture,extractive model with seq2seq architecture,0.6252530813217163
translation,192,128,baselines,seq2seq architecture,where,attention mechanism,seq2seq architecture where attention mechanism,0.5882228016853333
translation,192,128,baselines,attention mechanism,scores,document,attention mechanism scores document,0.7173218727111816
translation,192,128,baselines,attention mechanism,emits,index,attention mechanism emits index,0.6862030625343323
translation,192,128,baselines,index,as,selection,index as selection,0.5444706678390503
translation,192,128,baselines,baselines,has,neusum,baselines has neusum,0.6260150074958801
translation,192,129,baselines,jecs,is,neural textcompression - based summarization model,jecs is neural textcompression - based summarization model,0.5317112803459167
translation,192,129,baselines,neural textcompression - based summarization model,using,blstm,neural textcompression - based summarization model using blstm,0.6252152323722839
translation,192,129,baselines,blstm,as,encoder,blstm as encoder,0.5360795855522156
translation,192,129,baselines,compressive models,has,jecs,compressive models has jecs,0.5780118107795715
translation,192,129,baselines,baselines,has,compressive models,baselines has compressive models,0.5703877806663513
translation,192,139,experimental-setup,experiments,conducted on,single nvidia p100 card,experiments conducted on single nvidia p100 card,0.6522963047027588
translation,192,139,experimental-setup,mini-batch size,set to,6,mini-batch size set to 6,0.7500121593475342
translation,192,139,experimental-setup,6,due to,gpu memory capacity,6 due to gpu memory capacity,0.6139838695526123
translation,192,139,experimental-setup,experimental setup,has,experiments,experimental setup has experiments,0.5502888560295105
translation,192,140,experimental-setup,length,of,each document,length of each document,0.5776292681694031
translation,192,140,experimental-setup,each document,truncated to,768 bpes,each document truncated to 768 bpes,0.6511644721031189
translation,192,140,experimental-setup,experimental setup,has,length,experimental setup has length,0.4883878827095032
translation,192,141,experimental-setup,pre-trained ' bert - base-uncased ' model,fine tune,experiments,pre-trained ' bert - base-uncased ' model fine tune experiments,0.7553885579109192
translation,192,141,experimental-setup,experimental setup,use,pre-trained ' bert - base-uncased ' model,experimental setup use pre-trained ' bert - base-uncased ' model,0.5952069163322449
translation,192,142,experimental-setup,our models,for,"up to 80,000 steps","our models for up to 80,000 steps",0.6643966436386108
translation,192,142,experimental-setup,experimental setup,train,our models,experimental setup train our models,0.6514129638671875
translation,192,7,model,discobert,extracts,sub-sentential discourse units,discobert extracts sub-sentential discourse units,0.61989426612854
translation,192,7,model,sub-sentential discourse units,instead of,sentences,sub-sentential discourse units instead of sentences,0.5906434655189514
translation,192,7,model,sub-sentential discourse units,as,candidates,sub-sentential discourse units as candidates,0.5445773005485535
translation,192,7,model,candidates,for,extractive selection,candidates for extractive selection,0.6294419765472412
translation,192,7,model,extractive selection,on,finer granularity,extractive selection on finer granularity,0.5600595474243164
translation,192,7,model,model,present,discourse- aware neural summarization,model present discourse- aware neural summarization,0.6225059628486633
translation,192,8,model,rst trees and coreference mentions,encoded with,graph convolutional networks,rst trees and coreference mentions encoded with graph convolutional networks,0.669731855392456
translation,192,20,model,discourse- aware neural extractive summarization,built upon,bert,discourse- aware neural extractive summarization built upon bert,0.5815843939781189
translation,192,20,model,discobert,has,discourse- aware neural extractive summarization,discobert has discourse- aware neural extractive summarization,0.5455382466316223
translation,192,20,model,model,present,discobert,model present discobert,0.7234522700309753
translation,192,21,model,compression,take,elementary discourse unit ( edu ),compression take elementary discourse unit ( edu ),0.6302553415298462
translation,192,21,model,minimal selection unit,for,extractive summarization,minimal selection unit for extractive summarization,0.5826716423034668
translation,192,21,model,model,perform,compression,model perform compression,0.6617233753204346
translation,192,24,model,representations,of,discourse units,representations of discourse units,0.5376596450805664
translation,192,24,model,prior knowledge,to leverage,intra-sentence discourse relations,prior knowledge to leverage intra-sentence discourse relations,0.6368158459663391
translation,192,24,model,model,finetune,representations,model finetune representations,0.7013990879058838
translation,192,30,model,discourse - aware extractive summarization model,operates on,subsentential discourse unit level,discourse - aware extractive summarization model operates on subsentential discourse unit level,0.7100702524185181
translation,192,30,model,subsentential discourse unit level,to generate,concise and informative summary,subsentential discourse unit level to generate concise and informative summary,0.6376461982727051
translation,192,30,model,concise and informative summary,with,low redundancy,concise and informative summary with low redundancy,0.6056163907051086
translation,192,30,model,model,propose,discourse - aware extractive summarization model,model propose discourse - aware extractive summarization model,0.6249291300773621
translation,192,31,model,structurally model,adopt,rst,structurally model adopt rst,0.7513294816017151
translation,192,31,model,rst,as,discourse framework,rst as discourse framework,0.5570026636123657
translation,192,31,model,model,adopt,rst,model adopt rst,0.6992746591567993
translation,192,98,model,architecture design,present,single discourse graph encoder ( dge ) layer,architecture design present single discourse graph encoder ( dge ) layer,0.6299747228622437
translation,192,98,model,model,modularize,architecture design,model modularize architecture design,0.7324069738388062
translation,192,131,results,bert - based models,achieved,significant improvement,bert - based models achieved significant improvement,0.70474773645401
translation,192,131,results,bert - based models,achieved,significant improvement,bert - based models achieved significant improvement,0.70474773645401
translation,192,131,results,significant improvement,on,cnndm and nyt,significant improvement on cnndm and nyt,0.5713164210319519
translation,192,131,results,significant improvement,compared with,lstm counterparts,significant improvement compared with lstm counterparts,0.6665518283843994
translation,192,131,results,bert - based models,has,bert - based models,bert - based models has bert - based models,0.6102187633514404
translation,192,131,results,results,has,bert - based models,results has bert - based models,0.5088379383087158
translation,192,162,results,results,on,cnndm,results on cnndm,0.5700629353523254
translation,192,164,results,proposed discobert,beats,sentencebased counterpart,proposed discobert beats sentencebased counterpart,0.6929289102554321
translation,192,164,results,proposed discobert,beats,all the competitor models,proposed discobert beats all the competitor models,0.7373528480529785
translation,192,164,results,results,has,proposed discobert,results has proposed discobert,0.6653691530227661
translation,192,165,results,graph- based discobert,beats,stateof - the- art bert model,graph- based discobert beats stateof - the- art bert model,0.6747065186500549
translation,192,165,results,stateof - the- art bert model,by,significant margin,stateof - the- art bert model by significant margin,0.5664269328117371
translation,192,165,results,discourse graph encoder,has,graph- based discobert,discourse graph encoder has graph- based discobert,0.5944429636001587
translation,192,165,results,significant margin,has,0.52/0.61/1.04 on r-1/-2/-l on f 1,significant margin has 0.52/0.61/1.04 on r-1/-2/-l on f 1,0.5641618371009827
translation,192,165,results,results,With the help of,discourse graph encoder,results With the help of discourse graph encoder,0.692589521408081
translation,192,172,results,results,on,nyt,results on nyt,0.5398449301719666
translation,192,173,results,proposed model,surpasses,previous state - of- the- art bert - based model,proposed model surpasses previous state - of- the- art bert - based model,0.6695324182510376
translation,192,173,results,previous state - of- the- art bert - based model,by,significant margin,previous state - of- the- art bert - based model by significant margin,0.5551647543907166
translation,192,173,results,results,has,proposed model,results has proposed model,0.5938616394996643
translation,192,175,results,most of the improvement,comes from,use of edus,most of the improvement comes from use of edus,0.7144097089767456
translation,192,175,results,use of edus,as,minimal selection units,use of edus as minimal selection units,0.5644201040267944
translation,192,175,results,nyt dataset,has,most of the improvement,nyt dataset has most of the improvement,0.5940915942192078
translation,192,175,results,results,notice,nyt dataset,results notice nyt dataset,0.6902087330818176
translation,192,175,results,results,in,nyt dataset,results in nyt dataset,0.5172497630119324
translation,192,176,results,dis -cobert,provides,1.30/1.29/1.82 gain,dis -cobert provides 1.30/1.29/1.82 gain,0.6670846939086914
translation,192,176,results,1.30/1.29/1.82 gain,on,r-1/-2 / -l,1.30/1.29/1.82 gain on r-1/-2 / -l,0.6048124432563782
translation,192,176,results,r-1/-2 / -l,over,bert baseline,r-1/-2 / -l over bert baseline,0.7326012849807739
translation,192,176,results,results,has,dis -cobert,results has dis -cobert,0.6336683630943298
translation,193,4,model,bigram based supervised method,for,extractive document summarization,bigram based supervised method for extractive document summarization,0.597529947757721
translation,193,4,model,extractive document summarization,in,integer linear programming ( ilp ) framework,extractive document summarization in integer linear programming ( ilp ) framework,0.48580989241600037
translation,193,4,model,model,propose,bigram based supervised method,model propose bigram based supervised method,0.6744469404220581
translation,193,5,model,regression model,to estimate,frequency,regression model to estimate frequency,0.7828887104988098
translation,193,5,model,frequency,in,reference summary,frequency in reference summary,0.49953001737594604
translation,193,5,model,each bigram,has,regression model,each bigram has regression model,0.587925374507904
translation,193,5,model,model,For,each bigram,model For each bigram,0.6001308560371399
translation,193,27,model,bigram frequency,in,summary,bigram frequency in summary,0.5336758494377136
translation,193,27,model,bigram frequency,propose to use,supervised regression model,bigram frequency propose to use supervised regression model,0.6932364702224731
translation,193,27,model,supervised regression model,discriminatively trained using,variety of features,supervised regression model discriminatively trained using variety of features,0.7695363163948059
translation,193,27,model,model,To estimate,bigram frequency,model To estimate bigram frequency,0.6708487272262573
translation,193,90,results,our proposed system,has,consistently outperforms,our proposed system has consistently outperforms,0.6153808832168579
translation,193,90,results,consistently outperforms,has,icsi ilp system,consistently outperforms has icsi ilp system,0.6048176884651184
translation,193,90,results,results,see that,our proposed system,results see that our proposed system,0.6535804271697998
translation,193,91,results,better performance,on,three data sets,better performance on three data sets,0.5388689637184143
translation,193,91,results,three data sets,except,2011 data,three data sets except 2011 data,0.6833238005638123
translation,193,91,results,best reported tac result,has,our method,best reported tac result has our method,0.5410573482513428
translation,193,91,results,our method,has,better performance,our method has better performance,0.549237072467804
translation,193,91,results,results,Compared to,best reported tac result,results Compared to best reported tac result,0.6121053695678711
translation,193,94,results,performs better,than,full icsi system,performs better than full icsi system,0.5954351425170898
translation,193,94,results,compression,has,our approach,compression has our approach,0.633160412311554
translation,193,94,results,our approach,has,performs better,our approach has performs better,0.6098462343215942
translation,193,94,results,results,without using,compression,results without using compression,0.6420161724090576
translation,193,111,results,both ilp systems,has,our estimated bigram weights,both ilp systems has our estimated bigram weights,0.6037479043006897
translation,193,111,results,our estimated bigram weights,has,outperform,our estimated bigram weights has outperform,0.6079689860343933
translation,193,111,results,outperform,has,other frequency - based weights,outperform has other frequency - based weights,0.5858989357948303
translation,193,111,results,results,for,both ilp systems,results for both ilp systems,0.6051635146141052
translation,193,115,results,weight,is,document frequency,weight is document frequency,0.5097756385803223
translation,193,115,results,weight,using,term frequency,weight using term frequency,0.6778919100761414
translation,193,115,results,icsi 's result,is,better,icsi 's result is better,0.6056809425354004
translation,193,115,results,better,than,our proposed ilp,better than our proposed ilp,0.5816963911056519
translation,193,115,results,term frequency,as,weights,term frequency as weights,0.5531080961227417
translation,193,115,results,weight,has,icsi 's result,weight has icsi 's result,0.5856423377990723
translation,193,115,results,document frequency,has,icsi 's result,document frequency has icsi 's result,0.585106372833252
translation,193,115,results,weights,has,our ilp,weights has our ilp,0.6186143159866333
translation,193,115,results,our ilp,has,better results,our ilp has better results,0.6094714403152466
translation,193,117,results,icsi 's ilp,performs,slightly better,icsi 's ilp performs slightly better,0.6190199851989746
translation,193,117,results,icsi 's ilp,equipped with,initial bigram,icsi 's ilp equipped with initial bigram,0.697157084941864
translation,193,117,results,slightly better,than,ours,slightly better than ours,0.5834624171257019
translation,193,117,results,slightly better,equipped with,initial bigram,slightly better equipped with initial bigram,0.6545854806900024
translation,193,117,results,ours,equipped with,initial bigram,ours equipped with initial bigram,0.6569401025772095
translation,193,117,results,our proposed ilp,has,much better results,our proposed ilp has much better results,0.6025956869125366
translation,193,117,results,much better results,using,our selected top100 bigrams,much better results using our selected top100 bigrams,0.6229571104049683
translation,193,117,results,our proposed ilp,has,much better results,our proposed ilp has much better results,0.6025956869125366
translation,193,117,results,results,has,icsi 's ilp,results has icsi 's ilp,0.5577865242958069
translation,193,128,results,systems,have,similar trends,systems have similar trends,0.56669020652771
translation,193,128,results,increases,at,beginning,increases at beginning,0.6350800395011902
translation,193,128,results,beginning,when using,more bigrams,beginning when using more bigrams,0.7290508151054382
translation,193,128,results,degrading,with,too many bigrams,degrading with too many bigrams,0.6807112693786621
translation,193,128,results,similar trends,has,performance,similar trends has performance,0.5976837277412415
translation,193,128,results,performance,has,increases,performance has increases,0.5947421193122864
translation,193,139,results,significantly better,than,automatic systems,significantly better than automatic systems,0.6129815578460693
translation,193,140,results,icsi 's ilp,performs,marginally better,icsi 's ilp performs marginally better,0.6238886117935181
translation,193,140,results,marginally better,than,our proposed ilp,marginally better than our proposed ilp,0.5665184259414673
translation,193,140,results,results,notice that,icsi 's ilp,results notice that icsi 's ilp,0.6034289002418518
translation,193,151,results,our system,hits,31,our system hits 31,0.6827914714813232
translation,193,151,results,188 sentences,has,our system,188 sentences has our system,0.6037516593933105
translation,193,151,results,icsi,has,23,icsi has 23,0.6289694905281067
translation,193,159,results,results,worse than,previous oracle experiments,results worse than previous oracle experiments,0.6242480278015137
translation,193,159,results,results,better than using,automatically generated bigrams,results better than using automatically generated bigrams,0.680149495601654
translation,193,159,results,results,see that,results,results see that results,0.5610302686691284
translation,193,177,results,our bigrams,together with,weights,our bigrams together with weights,0.6769822239875793
translation,194,7,model,underlying structure of narratives,into,general unsupervised and supervised extractive summarization models,underlying structure of narratives into general unsupervised and supervised extractive summarization models,0.5757139325141907
translation,194,7,model,model,explicitly incorporate,underlying structure of narratives,model explicitly incorporate underlying structure of narratives,0.7650834918022156
translation,194,8,model,narrative structure,in terms of,key narrative events ( turning points ),narrative structure in terms of key narrative events ( turning points ),0.7205570936203003
translation,194,8,model,narrative structure,treat it as,latent,narrative structure treat it as latent,0.6408665180206299
translation,194,8,model,latent,to summarize,screenplays,latent to summarize screenplays,0.6940370798110962
translation,194,8,model,model,formalize,narrative structure,model formalize narrative structure,0.7183831930160522
translation,194,45,model,general - purpose extractive summarization,to identify,informative scenes,general - purpose extractive summarization to identify informative scenes,0.6019155979156494
translation,194,45,model,informative scenes,in,screenplays,informative scenes in screenplays,0.5314105749130249
translation,194,45,model,knowledge,about,narrative film structure,knowledge about narrative film structure,0.5920700430870056
translation,194,45,model,model,adapt,general - purpose extractive summarization,model adapt general - purpose extractive summarization,0.7220621705055237
translation,196,21,baselines,manually creating,has,multidocument summarization corpora,manually creating has multidocument summarization corpora,0.5186000466346741
translation,196,5,model,complex summarization task,into,multiple steps,complex summarization task into multiple steps,0.5620924234390259
translation,196,5,model,annotators,to store,intermediate results,annotators to store intermediate results,0.6463620662689209
translation,197,4,model,gowvis,leverages,graph degeneracy and community detection,gowvis leverages graph degeneracy and community detection,0.7262112498283386
translation,197,4,model,graph degeneracy and community detection,to generate,extractive summary ( keyphrases and sentences ),graph degeneracy and community detection to generate extractive summary ( keyphrases and sentences ),0.6795992851257324
translation,197,4,model,extractive summary ( keyphrases and sentences ),of,inputted text,extractive summary ( keyphrases and sentences ) of inputted text,0.555293619632721
translation,197,4,model,extractive summary ( keyphrases and sentences ),in,unsupervised fashion,extractive summary ( keyphrases and sentences ) in unsupervised fashion,0.4879598617553711
translation,197,4,model,inputted text,in,unsupervised fashion,inputted text in unsupervised fashion,0.5300542116165161
translation,197,4,model,gowvis,has,interactive web application,gowvis has interactive web application,0.576231062412262
translation,197,4,model,model,introduce,gowvis,model introduce gowvis,0.6669808030128479
translation,198,143,experimental-setup,optimal hyper-parameters,of,deep learning model,optimal hyper-parameters of deep learning model,0.5351460576057434
translation,198,143,experimental-setup,encoder ( bi-directional lstm ),consists of,two layers,encoder ( bi-directional lstm ) consists of two layers,0.6013431549072266
translation,198,143,experimental-setup,experimental setup,has,optimal hyper-parameters,experimental setup has optimal hyper-parameters,0.5281089544296265
translation,198,144,experimental-setup,batch size,set to,64,batch size set to 64,0.7451491355895996
translation,198,144,experimental-setup,learning rate,to,0.001,learning rate to 0.001,0.5506307482719421
translation,198,144,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
translation,198,144,experimental-setup,experimental setup,has,learning rate,experimental setup has learning rate,0.4954811632633209
translation,198,144,experimental-setup,experimental setup,has,training data,experimental setup has training data,0.5218086242675781
translation,198,145,experimental-setup,adam algorithm,with,gradient norm clipping,adam algorithm with gradient norm clipping,0.5850253701210022
translation,198,145,experimental-setup,adam algorithm,with,crossentropy,adam algorithm with crossentropy,0.6286876201629639
translation,198,145,experimental-setup,crossentropy,as,loss function,crossentropy as loss function,0.5194394588470459
translation,198,146,experimental-setup,all words,of,vocabulary,all words of vocabulary,0.627366304397583
translation,198,146,experimental-setup,all words,considered in,training phase,all words considered in training phase,0.6353066563606262
translation,198,146,experimental-setup,beam search,of,width,beam search of width,0.6198558211326599
translation,198,146,experimental-setup,width,equal to,4,width equal to 4,0.7522347569465637
translation,198,146,experimental-setup,experimental setup,has,all words,experimental setup has all words,0.5709360241889954
translation,198,163,experimental-setup,model configurations,trained on,single titan xp gpu,model configurations trained on single titan xp gpu,0.7217862606048584
translation,198,163,experimental-setup,experimental setup,trained on,single titan xp gpu,experimental setup trained on single titan xp gpu,0.726242184638977
translation,198,163,experimental-setup,experimental setup,has,model configurations,experimental setup has model configurations,0.508564829826355
translation,198,164,experimental-setup,training epoch,took,approximately 3.5 hours,training epoch took approximately 3.5 hours,0.6364085674285889
translation,198,164,experimental-setup,experimental setup,has,training epoch,experimental setup has training epoch,0.5109440088272095
translation,198,5,model,theoretical model,for,semantic - based text generalization,theoretical model for semantic - based text generalization,0.5681168437004089
translation,198,5,model,semantic - based text generalization,in conjunction with,deep encoder-decoder architecture,semantic - based text generalization in conjunction with deep encoder-decoder architecture,0.6738255023956299
translation,198,5,model,deep encoder-decoder architecture,to produce,summary,deep encoder-decoder architecture to produce summary,0.6741361618041992
translation,198,5,model,summary,in,generalized form,summary in generalized form,0.5237289667129517
translation,198,15,model,abstractive ts technique,combines,deep learning models,abstractive ts technique combines deep learning models,0.6656892895698547
translation,198,15,model,deep learning models,of,encoder-decoder architecture,deep learning models of encoder-decoder architecture,0.5293586850166321
translation,198,15,model,deep learning models,of,semantic - based data transformations,deep learning models of semantic - based data transformations,0.5027452111244202
translation,198,16,model,framework,combines,potential,framework combines potential,0.7532539367675781
translation,198,16,model,model,introducing,framework,model introducing framework,0.7098987698554993
translation,198,18,model,model,has,proposed framework,model has proposed framework,0.5661647915840149
translation,198,142,results,best performance,windows around,candidate and the generalized concepts,best performance windows around candidate and the generalized concepts,0.7456697225570679
translation,198,142,results,results,has,best performance,results has best performance,0.5759831070899963
translation,198,172,results,outperform,on,both datasets,outperform on both datasets,0.5134523510932922
translation,198,172,results,baseline approach,on,both datasets,baseline approach on both datasets,0.4790436625480652
translation,198,172,results,other configurations,has,of our model,other configurations has of our model,0.574194073677063
translation,198,172,results,of our model,has,outperform,of our model has outperform,0.6250489950180054
translation,198,172,results,outperform,has,baseline approach,outperform has baseline approach,0.6383320689201355
translation,198,173,results,improved results,expected,generalization of low-frequency words,improved results expected generalization of low-frequency words,0.6820692420005798
translation,198,173,results,improved results,especially in,generalization of low-frequency words,improved results especially in generalization of low-frequency words,0.6210784912109375
translation,198,173,results,results,has,improved results,results has improved results,0.5928851366043091
translation,198,178,results,positive effect,on,system performance,positive effect on system performance,0.5674550533294678
translation,198,178,results,neg strategy,has,positive effect,neg strategy has positive effect,0.5784155130386353
translation,198,178,results,results,has,neg strategy,results has neg strategy,0.5717143416404724
translation,198,180,results,configurations,generalize,all concepts,configurations generalize all concepts,0.74997878074646
translation,198,180,results,all concepts,regardless of,frequency,all concepts regardless of frequency,0.7172829508781433
translation,198,180,results,all concepts,exhibit,worst performance,all concepts exhibit worst performance,0.6529005169868469
translation,198,180,results,both strategies,has,configurations,both strategies has configurations,0.6371060609817505
translation,198,184,results,best neg and lg configurations,demonstrate,near-optimal performance,best neg and lg configurations demonstrate near-optimal performance,0.6173414587974548
translation,198,184,results,near-optimal performance,when,rouge - 1 score,near-optimal performance when rouge - 1 score,0.6102858185768127
translation,198,184,results,best neg and lg configurations,has,outperform,best neg and lg configurations has outperform,0.6013475060462952
translation,198,184,results,outperform,has,other systems,outperform has other systems,0.6242401599884033
translation,198,184,results,results,exhibit,best neg and lg configurations,results exhibit best neg and lg configurations,0.6278772950172424
translation,198,185,results,further preprocessing,led to,significant performance improvements,further preprocessing led to significant performance improvements,0.681535542011261
translation,198,185,results,gigaword dataset,has,further preprocessing,gigaword dataset has further preprocessing,0.5097341537475586
translation,198,185,results,further preprocessing,has,of data,further preprocessing has of data,0.570419430732727
translation,198,185,results,results,In case of,gigaword dataset,results In case of gigaword dataset,0.598886251449585
translation,199,165,ablation-analysis,redundancy,in,nonstepwise models,redundancy in nonstepwise models,0.5744775533676147
translation,199,165,ablation-analysis,redundancy,has,in generated summaries,redundancy has in generated summaries,0.5951539278030396
translation,199,165,ablation-analysis,ablation analysis,has,trigram filtering,ablation analysis has trigram filtering,0.5708358883857727
translation,199,214,ablation-analysis,roundtrip,of,realization,roundtrip of realization,0.609691321849823
translation,199,214,ablation-analysis,cs quality slightly,for,both models,cs quality slightly for both models,0.654853105545044
translation,199,214,ablation-analysis,1.68 %,for,stepwise hibert,1.68 % for stepwise hibert,0.6897908449172974
translation,199,214,ablation-analysis,1.74 %,for,stepwise etcsum,1.74 % for stepwise etcsum,0.6592618823051453
translation,199,214,ablation-analysis,subsequent information extraction,has,decreases,subsequent information extraction has decreases,0.594470739364624
translation,199,214,ablation-analysis,decreases,has,cs quality slightly,decreases has cs quality slightly,0.5794228911399841
translation,199,214,ablation-analysis,ablation analysis,see,roundtrip,ablation analysis see roundtrip,0.6114470362663269
translation,199,133,baselines,lead,selects,first 3 sentences,lead selects first 3 sentences,0.7619016170501709
translation,199,133,baselines,first 3 sentences,to form,summary,first 3 sentences to form summary,0.6454741358757019
translation,199,133,baselines,first 3 sentences,to form,summary,first 3 sentences to form summary,0.6454741358757019
translation,199,133,baselines,oracle baselines,creates,summary,oracle baselines creates summary,0.6277416944503784
translation,199,133,baselines,"highest average of rouge -1 , rouge - 2 and rouge -l f1 scores",with respect to,human written summary,"highest average of rouge -1 , rouge - 2 and rouge -l f1 scores with respect to human written summary",0.642551839351654
translation,199,133,baselines,baselines,has,lead,baselines has lead,0.5889924764633179
translation,199,134,experimental-setup,oracle ( 512 ),truncates,input document,oracle ( 512 ) truncates input document,0.771591067314148
translation,199,134,experimental-setup,input document,to,512 tokens,input document to 512 tokens,0.5335979461669922
translation,199,134,experimental-setup,experimental setup,has,oracle ( 512 ),experimental setup has oracle ( 512 ),0.5780657529830933
translation,199,146,experimental-setup,layers,in,document and sentence encoders,layers in document and sentence encoders,0.4971681833267212
translation,199,146,experimental-setup,experimental setup,has,layers,experimental setup has layers,0.4684261083602905
translation,199,147,experimental-setup,etcsum models,initialized with,"uncased version of etc pretrained checkpoints ( ainslie et al. , 2020 )","etcsum models initialized with uncased version of etc pretrained checkpoints ( ainslie et al. , 2020 )",0.7524988055229187
translation,199,147,experimental-setup,"uncased version of etc pretrained checkpoints ( ainslie et al. , 2020 )",pretrained using,standard masked language model task,"uncased version of etc pretrained checkpoints ( ainslie et al. , 2020 ) pretrained using standard masked language model task",0.7416155934333801
translation,199,147,experimental-setup,"uncased version of etc pretrained checkpoints ( ainslie et al. , 2020 )",pretrained using,contrastive predictive coding,"uncased version of etc pretrained checkpoints ( ainslie et al. , 2020 ) pretrained using contrastive predictive coding",0.7360481023788452
translation,199,147,experimental-setup,contrastive predictive coding,has,"van den oord et al. , 2018 )","contrastive predictive coding has van den oord et al. , 2018 )",0.5554009079933167
translation,199,147,experimental-setup,experimental setup,has,etcsum models,experimental setup has etcsum models,0.5237399339675903
translation,199,7,experiments,stepwise models,achieve,state - of - the - art performance,stepwise models achieve state - of - the - art performance,0.6022269129753113
translation,199,7,experiments,state - of - the - art performance,in terms of,rouge,state - of - the - art performance in terms of rouge,0.660275936126709
translation,199,7,experiments,cnn / dailymail extractive summarization,has,stepwise models,cnn / dailymail extractive summarization has stepwise models,0.5535069108009338
translation,199,36,experiments,intricate extractive content plan,for,rotowire table - to - text generation task,intricate extractive content plan for rotowire table - to - text generation task,0.6142463684082031
translation,199,36,experiments,learning,has,intricate extractive content plan,learning has intricate extractive content plan,0.6245962381362915
translation,199,5,model,stepwise summarization,by injecting,previously generated summary,stepwise summarization by injecting previously generated summary,0.6501750349998474
translation,199,5,model,previously generated summary,into,structured transformer,previously generated summary into structured transformer,0.6121091246604919
translation,199,5,model,structured transformer,as,auxiliary sub-structure,structured transformer as auxiliary sub-structure,0.5495955944061279
translation,199,5,model,model,enable,stepwise summarization,model enable stepwise summarization,0.731950044631958
translation,199,26,model,structured transformers,are,transformer - based architectures,structured transformers are transformer - based architectures,0.6148242354393005
translation,199,26,model,model,has,structured transformers,model has structured transformers,0.6196944713592529
translation,199,29,model,stepwise summarization,by injecting,previously planned summary content,stepwise summarization by injecting previously planned summary content,0.6534972786903381
translation,199,29,model,previously planned summary content,into,structured transformer,previously planned summary content into structured transformer,0.5823103785514832
translation,199,29,model,structured transformer,as,auxiliary sub-structure,structured transformer as auxiliary sub-structure,0.5495955944061279
translation,199,29,model,model,enable,stepwise summarization,model enable stepwise summarization,0.731950044631958
translation,199,35,model,flexible,in terms of,"content type ( e.g. , text or tables )","flexible in terms of content type ( e.g. , text or tables )",0.6311180591583252
translation,199,35,model,model,has,structured transformers,model has structured transformers,0.6196944713592529
translation,199,8,results,models,surpass,previously reported metrics,models surpass previously reported metrics,0.6470974683761597
translation,199,8,results,previously reported metrics,for,content selection,previously reported metrics for content selection,0.5341692566871643
translation,199,8,results,previously reported metrics,for,ordering,previously reported metrics for ordering,0.6436532735824585
translation,199,9,results,stepwise extended transformers,provides,best performance,stepwise extended transformers provides best performance,0.6821198463439941
translation,199,9,results,best performance,across,both datasets,best performance across both datasets,0.674172043800354
translation,199,9,results,two structured transformers,has,stepwise extended transformers,two structured transformers has stepwise extended transformers,0.6203473210334778
translation,199,9,results,results,Amongst,two structured transformers,results Amongst two structured transformers,0.565446674823761
translation,199,38,results,stepwise framework,achieves,higher content selection,stepwise framework achieves higher content selection,0.6579369306564331
translation,199,38,results,ordering scores,relative to,prior work,ordering scores relative to prior work,0.6917120814323425
translation,199,38,results,prior work,with,task -specific planning mechanisms,prior work with task -specific planning mechanisms,0.5323988795280457
translation,199,38,results,higher content selection,has,planning,higher content selection has planning,0.5932390093803406
translation,199,38,results,results,show,stepwise framework,results show stepwise framework,0.6036220192909241
translation,199,159,results,appears to be far more superior,than,hib - ert,appears to be far more superior than hib - ert,0.6387447714805603
translation,199,159,results,hib - ert,when modeling,long documents,hib - ert when modeling long documents,0.7174635529518127
translation,199,159,results,long documents,for,extractive summarization,long documents for extractive summarization,0.5627206563949585
translation,199,159,results,hibert,in,all cases,hibert in all cases,0.6286001205444336
translation,199,159,results,hibert,with or without,trigram blocking,hibert with or without trigram blocking,0.6793783903121948
translation,199,159,results,all cases,including,stepwise or non-stepwise predictions,all cases including stepwise or non-stepwise predictions,0.6989997029304504
translation,199,159,results,etcsum,has,appears to be far more superior,etcsum has appears to be far more superior,0.5949968695640564
translation,199,159,results,etcsum,has,outperformed,etcsum has outperformed,0.6607933640480042
translation,199,159,results,outperformed,has,hibert,outperformed has hibert,0.6661284565925598
translation,199,161,results,etcsum and etcsum + triblk,performed,better,etcsum and etcsum + triblk performed better,0.2810245156288147
translation,199,161,results,better,than,bertsum and bertsum + triblk,better than bertsum and bertsum + triblk,0.6467307806015015
translation,199,161,results,results,has,etcsum and etcsum + triblk,results has etcsum and etcsum + triblk,0.5581928491592407
translation,199,166,results,almost all models,including,our hibert and etcsum,almost all models including our hibert and etcsum,0.6900705695152283
translation,199,166,results,results,helps,almost all models,results helps almost all models,0.6188892126083374
translation,199,169,results,stepwise,applied with,triblk,stepwise applied with triblk,0.7702040672302246
translation,199,169,results,triblk,n't always see,improvements,triblk n't always see improvements,0.6067785024642944
translation,199,172,results,stepwise etcsum + triblk,n't see,significant improvement,stepwise etcsum + triblk n't see significant improvement,0.6482719779014587
translation,199,172,results,significant improvement,over,stepwise etc - sum,significant improvement over stepwise etc - sum,0.7111389636993408
translation,199,172,results,results,With,stepwise etcsum + triblk,results With stepwise etcsum + triblk,0.6311848759651184
translation,199,177,results,our stepwise etcsum model,achieved,comparable performance,our stepwise etcsum model achieved comparable performance,0.6759719848632812
translation,199,177,results,comparable performance,to,state of the art,comparable performance to state of the art,0.545836865901947
translation,199,177,results,state of the art,on,cnn / dailymail,state of the art on cnn / dailymail,0.5367562770843506
translation,199,177,results,our stepwise etcsum model,has,without any explicit redundancy or sentence selection mechanisms,our stepwise etcsum model has without any explicit redundancy or sentence selection mechanisms,0.567972719669342
translation,199,177,results,results,has,our stepwise etcsum model,results has our stepwise etcsum model,0.5274172425270081
translation,199,182,results,stepwise etcsum,performs,significantly better,stepwise etcsum performs significantly better,0.6559324264526367
translation,199,182,results,significantly better,than,robertasum ( + triblk ),significantly better than robertasum ( + triblk ),0.5823205709457397
translation,199,182,results,significantly better,than,etc - sum + triblk,significantly better than etc - sum + triblk,0.6371889114379883
translation,199,182,results,significantly better,than,stepwise etcsum + triblk,significantly better than stepwise etcsum + triblk,0.6288405656814575
translation,199,182,results,significantly better,of,rouge scores,significantly better of rouge scores,0.5808644890785217
translation,199,182,results,stepwise etcsum + triblk,of,rouge scores,stepwise etcsum + triblk of rouge scores,0.6055005788803101
translation,199,182,results,our best model,has,stepwise etcsum,our best model has stepwise etcsum,0.5886687636375427
translation,199,182,results,results,has,our best model,results has our best model,0.5419765710830688
translation,199,220,results,stepwise etc summaries,ranked,most informative,stepwise etc summaries ranked most informative,0.7383493185043335
translation,199,220,results,stepwise etc summaries,performed,worst,stepwise etc summaries performed worst,0.2939642369747162
translation,199,220,results,worst,on,readability,worst on readability,0.5456398129463196
translation,199,221,results,off - the-shelf sentence - level realizer,favors,statistics -,off - the-shelf sentence - level realizer favors statistics -,0.7488977313041687
translation,199,221,results,dense sentences,of,baseline summaries,dense sentences of baseline summaries,0.5616602301597595
translation,199,221,results,statistics -,has,dense sentences,statistics - has dense sentences,0.6129120588302612
translation,199,221,results,results,has,off - the-shelf sentence - level realizer,results has off - the-shelf sentence - level realizer,0.5354709029197693
translation,199,223,results,stepwise etc summaries,are,significantly better,stepwise etc summaries are significantly better,0.5794231295585632
translation,199,223,results,significantly better,than,gold,significantly better than gold,0.5932024121284485
translation,199,223,results,significantly better,than,stepwise etc truncated,significantly better than stepwise etc truncated,0.5941623449325562
translation,199,223,results,significantly better,than,stepwise hibert truncated summaries,significantly better than stepwise hibert truncated summaries,0.6023423075675964
translation,199,223,results,informativeness,has,stepwise etc summaries,informativeness has stepwise etc summaries,0.5587493181228638
translation,199,223,results,results,For,informativeness,results For informativeness,0.6021713614463806
translation,199,224,results,stepwise hibert summaries,are,significantly better,stepwise hibert summaries are significantly better,0.5882010459899902
translation,199,224,results,significantly better,than,truncated variants,significantly better than truncated variants,0.6023286581039429
translation,199,224,results,results,has,stepwise hibert summaries,results has stepwise hibert summaries,0.5914286375045776
translation,199,226,results,baseline summaries,are,significantly better,baseline summaries are significantly better,0.5694408416748047
translation,199,226,results,significantly better,than,etc variants,significantly better than etc variants,0.6288748979568481
translation,199,226,results,significantly better,than,stepwise hib-ert,significantly better than stepwise hib-ert,0.5721547603607178
translation,199,226,results,readability,has,baseline summaries,readability has baseline summaries,0.55697101354599
translation,199,226,results,results,For,readability,results For readability,0.5498998165130615
translation,200,147,ablation-analysis,model,shows,great decline,model shows great decline,0.6765480637550354
translation,200,147,ablation-analysis,great decline,in,performance,great decline in performance,0.5454487800598145
translation,200,147,ablation-analysis,ablation analysis,removed,template-to-article ( t2a ) gate,ablation analysis removed template-to-article ( t2a ) gate,0.7020421028137207
translation,200,148,ablation-analysis,role,to control,weight of t2a,role to control weight of t2a,0.77156662940979
translation,200,148,ablation-analysis,weight of t2a,in,article representations,weight of t2a in article representations,0.5032371282577515
translation,200,148,ablation-analysis,ablation analysis,removed,article-to- template ( a2t ) gate,ablation analysis removed article-to- template ( a2t ) gate,0.7147932648658752
translation,200,25,experiments,template - equipped summarization model,has,biset,template - equipped summarization model has biset,0.5665833353996277
translation,200,25,experiments,template - equipped summarization model,has,outperforms,template - equipped summarization model has outperforms,0.5904627442359924
translation,200,25,experiments,biset,has,outperforms,biset has outperforms,0.6747777462005615
translation,200,25,experiments,outperforms,has,all the state- ofthe - art models,outperforms has all the state- ofthe - art models,0.5599398016929626
translation,200,25,experiments,all the state- ofthe - art models,has,significantly,all the state- ofthe - art models has significantly,0.5564952492713928
translation,200,131,experiments,dcn attention,works,impressively,dcn attention works impressively,0.6041168570518494
translation,200,131,experiments,dcn attention,performs,even worse,dcn attention performs even worse,0.6076885461807251
translation,200,131,experiments,impressively,in,machine reading comprehension,impressively in machine reading comprehension,0.49081847071647644
translation,200,131,experiments,even worse,than,simple concatenation,even worse than simple concatenation,0.6118419766426086
translation,200,95,hyperparameters,both the fast rerank and biset modules,have,batch size,both the fast rerank and biset modules have batch size,0.5952223539352417
translation,200,95,hyperparameters,batch size,of,64,batch size of 64,0.6741159558296204
translation,200,95,hyperparameters,64,with,adam optimizer,64 with adam optimizer,0.6210550665855408
translation,200,95,hyperparameters,training,has,both the fast rerank and biset modules,training has both the fast rerank and biset modules,0.49155157804489136
translation,200,95,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
translation,200,96,hyperparameters,grad clipping,with,range,grad clipping with range,0.6519075632095337
translation,200,96,hyperparameters,range,of,"[ - 5,5 ]","range of [ - 5,5 ]",0.6093835830688477
translation,200,96,hyperparameters,hyperparameters,apply,grad clipping,hyperparameters apply grad clipping,0.6368666887283325
translation,200,99,hyperparameters,word embeddings,to,300,word embeddings to 300,0.5788252949714661
translation,200,99,hyperparameters,convolution encoder block number,to,1,convolution encoder block number to 1,0.5661211013793945
translation,200,99,hyperparameters,kernel size,of,cnn,kernel size of cnn,0.5865076780319214
translation,200,99,hyperparameters,kernel size,to,3,kernel size to 3,0.6128697991371155
translation,200,99,hyperparameters,cnn,to,3,cnn to 3,0.6219725012779236
translation,200,99,hyperparameters,hyperparameters,set,convolution encoder block number,hyperparameters set convolution encoder block number,0.6238756775856018
translation,200,99,hyperparameters,hyperparameters,set,kernel size,hyperparameters set kernel size,0.634819507598877
translation,200,101,hyperparameters,k-max pooling,set to,10,k-max pooling set to 10,0.654169499874115
translation,200,101,hyperparameters,hyperparameters,k of,k-max pooling,hyperparameters k of k-max pooling,0.570984959602356
translation,200,102,hyperparameters,l2 weight decay,with,? = 3 ? 10 ?6,l2 weight decay with ? = 3 ? 10 ?6,0.6463454961776733
translation,200,102,hyperparameters,l2 weight decay,performed over,all trainable variables,l2 weight decay performed over all trainable variables,0.7261220216751099
translation,200,102,hyperparameters,? = 3 ? 10 ?6,performed over,all trainable variables,? = 3 ? 10 ?6 performed over all trainable variables,0.766488790512085
translation,200,102,hyperparameters,hyperparameters,has,l2 weight decay,hyperparameters has l2 weight decay,0.46854475140571594
translation,200,103,hyperparameters,initial learning rate,is,0.001,initial learning rate is 0.001,0.5461736917495728
translation,200,103,hyperparameters,initial learning rate,multiplied by,0.1,initial learning rate multiplied by 0.1,0.6316030025482178
translation,200,103,hyperparameters,0.1,every,10 k steps,0.1 every 10 k steps,0.6432440876960754
translation,200,103,hyperparameters,hyperparameters,has,initial learning rate,hyperparameters has initial learning rate,0.4469831883907318
translation,200,104,hyperparameters,dropout,between,layers,dropout between layers,0.6490000486373901
translation,200,104,hyperparameters,hyperparameters,has,dropout,hyperparameters has dropout,0.5324090719223022
translation,200,107,hyperparameters,sizes,of,word embeddings and lstm hidden states,sizes of word embeddings and lstm hidden states,0.561598002910614
translation,200,107,hyperparameters,word embeddings and lstm hidden states,set to,500,word embeddings and lstm hidden states set to 500,0.629532516002655
translation,200,107,hyperparameters,hyperparameters,has,sizes,hyperparameters has sizes,0.5322444438934326
translation,200,108,hyperparameters,dropout,in,lstm stack,dropout in lstm stack,0.5033516883850098
translation,200,108,hyperparameters,dropout,with,rate,dropout with rate,0.5557408928871155
translation,200,108,hyperparameters,rate,of,0.3,rate of 0.3,0.6309453845024109
translation,200,108,hyperparameters,hyperparameters,apply,dropout,hyperparameters apply dropout,0.6019902229309082
translation,200,109,hyperparameters,learning rate,set to,0.001,learning rate set to 0.001,0.6954665780067444
translation,200,109,hyperparameters,learning rate,halved,every 10 k steps,learning rate halved every 10 k steps,0.721977710723877
translation,200,109,hyperparameters,0.001,for,first 50 k steps,0.001 for first 50 k steps,0.6090341210365295
translation,200,109,hyperparameters,hyperparameters,has,learning rate,hyperparameters has learning rate,0.46666839718818665
translation,200,6,model,novel bi-directional selective encoding with template ( biset ) model,leverages,template,novel bi-directional selective encoding with template ( biset ) model leverages template,0.6911025047302246
translation,200,6,model,template,discovered from,training data,template discovered from training data,0.7216880321502686
translation,200,6,model,key information,from,each source article,key information from each source article,0.5464636087417603
translation,200,6,model,key information,to guide,summarization process,key information to guide summarization process,0.6694274544715881
translation,200,6,model,model,propose,novel bi-directional selective encoding with template ( biset ) model,model propose novel bi-directional selective encoding with template ( biset ) model,0.6363274455070496
translation,200,20,model,novel bi-directional selective layer,with,two gates,novel bi-directional selective layer with two gates,0.5869522094726562
translation,200,20,model,two gates,to mutually select,key information,two gates to mutually select key information,0.7118467688560486
translation,200,20,model,key information,from,article,key information from article,0.5606324672698975
translation,200,20,model,template,to assist with,summary generation,template to assist with summary generation,0.6791587471961975
translation,200,20,model,model,involves,novel bi-directional selective layer,model involves novel bi-directional selective layer,0.5753176808357239
translation,200,21,model,multi-stage process,for,automatic retrieval,multi-stage process for automatic retrieval,0.6314713954925537
translation,200,21,model,automatic retrieval,of,high-quality templates,automatic retrieval of high-quality templates,0.571329653263092
translation,200,21,model,high-quality templates,from,training corpus,high-quality templates from training corpus,0.5194215178489685
translation,200,21,model,model,propose,multi-stage process,model propose multi-stage process,0.6757947206497192
translation,200,100,model,weights,shared between,article and template encoders,weights shared between article and template encoders,0.7124139666557312
translation,200,100,model,model,has,weights,model has weights,0.5368953943252563
translation,200,106,model,two - layer bilstm,used as,encoder,two - layer bilstm used as encoder,0.5688497424125671
translation,200,106,model,another two - layer lstm,as,decoder,another two - layer lstm as decoder,0.5612867474555969
translation,200,106,model,model,has,two - layer bilstm,model has two - layer bilstm,0.5050566792488098
translation,200,23,results,templates,selected by,our approach,templates selected by our approach,0.6451549530029297
translation,200,23,results,our approach,as,final summaries,our approach as final summaries,0.5782551169395447
translation,200,23,results,our model,achieve,superior performance,our model achieve superior performance,0.6495733261108398
translation,200,23,results,superior performance,to,some baseline models,superior performance to some baseline models,0.5240775346755981
translation,200,23,results,templates,has,our model,templates has our model,0.6119320392608643
translation,200,23,results,our approach,has,our model,our approach has our model,0.5680160522460938
translation,200,23,results,final summaries,has,our model,final summaries has our model,0.5647172331809998
translation,200,23,results,results,using,templates,results using templates,0.54085373878479
translation,200,120,results,n-optimal templates,used,additional improvements,n-optimal templates used additional improvements,0.5925149917602539
translation,200,120,results,additional improvements,as,n,additional improvements as n,0.577686607837677
translation,200,120,results,n-optimal templates,has,additional improvements,n-optimal templates has additional improvements,0.549541711807251
translation,200,120,results,n,has,grows,n has grows,0.6681299209594727
translation,200,120,results,results,when,n-optimal templates,results when n-optimal templates,0.6218643188476562
translation,200,134,results,overall performance,of,all the studied models,overall performance of all the studied models,0.5404670834541321
translation,200,134,results,results,has,overall performance,results has overall performance,0.5951287150382996
translation,200,135,results,new state of the art,for,abstractive sentence summarization,new state of the art for abstractive sentence summarization,0.5663247108459473
translation,200,135,results,significantly outperforms,has,all the baseline models,significantly outperforms has all the baseline models,0.5726946592330933
translation,200,135,results,results,sets,new state of the art,results sets new state of the art,0.6797065734863281
translation,200,137,results,performance,of,our model,performance of our model,0.5847885608673096
translation,200,137,results,improves constantly,improvement of,template quality,improves constantly improvement of template quality,0.7313227653503418
translation,200,137,results,our model,has,improves constantly,our model has improves constantly,0.6154431700706482
translation,200,138,results,our model,works with,stable performance,our model works with stable performance,0.7011326551437378
translation,200,138,results,stable performance,demonstrating,robustness,stable performance demonstrating robustness,0.7011843919754028
translation,200,138,results,randomly -selected templates,has,our model,randomly -selected templates has our model,0.5810825228691101
translation,200,138,results,results,Even with,randomly -selected templates,results Even with randomly -selected templates,0.6558428406715393
translation,201,128,experimental-setup,wordpiece embeddings,with,"30,000 vocabulary","wordpiece embeddings with 30,000 vocabulary",0.6250435709953308
translation,201,128,experimental-setup,experimental setup,use,wordpiece embeddings,experimental setup use wordpiece embeddings,0.559541642665863
translation,201,129,experimental-setup,layer of transformer decoders,to,12 ( 8 on nyt50 ),layer of transformer decoders to 12 ( 8 on nyt50 ),0.5874105095863342
translation,201,129,experimental-setup,layer of transformer decoders,to,12 ( 8 on nyt50 ),layer of transformer decoders to 12 ( 8 on nyt50 ),0.5874105095863342
translation,201,129,experimental-setup,layer of transformer decoders,to,12 ( 8 on nyt50 ),layer of transformer decoders to 12 ( 8 on nyt50 ),0.5874105095863342
translation,201,129,experimental-setup,attention heads number,to,12 ( 8 on nyt50 ),attention heads number to 12 ( 8 on nyt50 ),0.6274490356445312
translation,201,129,experimental-setup,fully - connected sub-layer hidden size,to,3072,fully - connected sub-layer hidden size to 3072,0.521523654460907
translation,201,129,experimental-setup,experimental setup,set,layer of transformer decoders,experimental setup set layer of transformer decoders,0.6370592713356018
translation,201,129,experimental-setup,experimental setup,set,attention heads number,experimental setup set attention heads number,0.6298050880432129
translation,201,129,experimental-setup,experimental setup,set,attention heads number,experimental setup set attention heads number,0.6298050880432129
translation,201,130,experimental-setup,model,using,adam optimizer,model using adam optimizer,0.6418686509132385
translation,201,130,experimental-setup,adam optimizer,with,learning rate,adam optimizer with learning rate,0.6042178273200989
translation,201,130,experimental-setup,adam optimizer,use,dynamic learning rate,adam optimizer use dynamic learning rate,0.6246058344841003
translation,201,130,experimental-setup,learning rate,of,"3e ? 4 , ? 1 = 0.9 , ? 2 = 0.999 and = 10 ?9","learning rate of 3e ? 4 , ? 1 = 0.9 , ? 2 = 0.999 and = 10 ?9",0.6165170073509216
translation,201,130,experimental-setup,dynamic learning rate,during,training process,dynamic learning rate during training process,0.6481055617332458
translation,201,130,experimental-setup,experimental setup,train,model,experimental setup train model,0.6514950394630432
translation,201,131,experimental-setup,regularization,use,dropout,regularization use dropout,0.6589788198471069
translation,201,131,experimental-setup,regularization,set,dropout rate,regularization set dropout rate,0.5686860680580139
translation,201,131,experimental-setup,regularization,set,label smoothing value,regularization set label smoothing value,0.6149822473526001
translation,201,131,experimental-setup,dropout rate,to,0.15,dropout rate to 0.15,0.5387094616889954
translation,201,131,experimental-setup,label smoothing value,to,0.1,label smoothing value to 0.1,0.5451316237449646
translation,201,131,experimental-setup,dropout,has,"srivastava et al. , 2014 )","dropout has srivastava et al. , 2014 )",0.5718802213668823
translation,201,131,experimental-setup,label smoothing,has,"szegedy et al. , 2016 )","label smoothing has szegedy et al. , 2016 )",0.532588005065918
translation,201,131,experimental-setup,experimental setup,For,regularization,experimental setup For regularization,0.5391446948051453
translation,201,131,experimental-setup,experimental setup,set,dropout rate,experimental setup set dropout rate,0.62116938829422
translation,201,131,experimental-setup,experimental setup,set,label smoothing value,experimental setup set label smoothing value,0.6333585381507874
translation,201,132,experimental-setup,rl objective factor,to,0.99,rl objective factor to 0.99,0.540907084941864
translation,201,132,experimental-setup,experimental setup,set,rl objective factor,experimental setup set rl objective factor,0.6082264184951782
translation,201,133,experimental-setup,training,set,batch size,training set batch size,0.6989448666572571
translation,201,133,experimental-setup,training,train for,4 epochs,training train for 4 epochs,0.7542321085929871
translation,201,133,experimental-setup,batch size,to,36,batch size to 36,0.6316425800323486
translation,201,133,experimental-setup,best model,selected from,last 10 models,best model selected from last 10 models,0.5863773822784424
translation,201,133,experimental-setup,last 10 models,based on,development set performance,last 10 models based on development set performance,0.6306144595146179
translation,201,133,experimental-setup,experimental setup,During,training,experimental setup During training,0.6835477948188782
translation,201,134,experimental-setup,gpu memory limit,use,gradient accumulation,gpu memory limit use gradient accumulation,0.6215066909790039
translation,201,134,experimental-setup,gradient accumulation,set,accumulate step,gradient accumulation set accumulate step,0.6926673054695129
translation,201,134,experimental-setup,gradient accumulation,feed,3 samples,gradient accumulation feed 3 samples,0.7294020652770996
translation,201,134,experimental-setup,accumulate step,to,12,accumulate step to 12,0.6229053735733032
translation,201,134,experimental-setup,3 samples,at,each step,3 samples at each step,0.5540160536766052
translation,201,134,experimental-setup,experimental setup,Due to,gpu memory limit,experimental setup Due to gpu memory limit,0.6513442397117615
translation,201,135,experimental-setup,length penalty,of,1.0,length penalty of 1.0,0.572574257850647
translation,201,135,experimental-setup,1.0,to generate,logical form sequences,1.0 to generate logical form sequences,0.6533970832824707
translation,201,135,experimental-setup,beam size,has,4,beam size has 4,0.6405593156814575
translation,201,4,model,novel pretrainingbased encoder-decoder framework,can generate,output sequence,novel pretrainingbased encoder-decoder framework can generate output sequence,0.7066316604614258
translation,201,4,model,output sequence,based on,input sequence,output sequence based on input sequence,0.6365900039672852
translation,201,4,model,input sequence,in,two -stage manner,input sequence in two -stage manner,0.5739167928695679
translation,201,4,model,model,propose,novel pretrainingbased encoder-decoder framework,model propose novel pretrainingbased encoder-decoder framework,0.6526836156845093
translation,201,5,model,encoder,encode,input sequence,encoder encode input sequence,0.7610542178153992
translation,201,5,model,input sequence,into,context representations,input sequence into context representations,0.5860968828201294
translation,201,5,model,context representations,using,bert,context representations using bert,0.7007447481155396
translation,201,5,model,model,For,encoder,model For encoder,0.6582764387130737
translation,201,6,model,two stages,in,first stage,two stages in first stage,0.5738741755485535
translation,201,6,model,two stages,in,first stage,two stages in first stage,0.5738741755485535
translation,201,6,model,first stage,use,transformer - based decoder,first stage use transformer - based decoder,0.6148790121078491
translation,201,6,model,transformer - based decoder,to generate,draft output sequence,transformer - based decoder to generate draft output sequence,0.7063045501708984
translation,201,6,model,model,For,decoder,model For decoder,0.6749407649040222
translation,201,7,model,second stage,mask,each word,second stage mask each word,0.7647114992141724
translation,201,7,model,second stage,feed it to,bert,second stage feed it to bert,0.6447058916091919
translation,201,7,model,second stage,feed it to,bert,second stage feed it to bert,0.6447058916091919
translation,201,7,model,second stage,use,transformer - based decoder,second stage use transformer - based decoder,0.6085568070411682
translation,201,7,model,each word,of,draft sequence,each word of draft sequence,0.6248928904533386
translation,201,7,model,bert,combining,input sequence,bert combining input sequence,0.6979557275772095
translation,201,7,model,bert,combining,draft representation,bert combining draft representation,0.7449856996536255
translation,201,7,model,bert,use,transformer - based decoder,bert use transformer - based decoder,0.6156154274940491
translation,201,7,model,draft representation,generated by,bert,draft representation generated by bert,0.7319950461387634
translation,201,7,model,draft representation,use,transformer - based decoder,draft representation use transformer - based decoder,0.6157411336898804
translation,201,7,model,transformer - based decoder,to predict,refined word,transformer - based decoder to predict refined word,0.705467164516449
translation,201,7,model,refined word,for,each masked position,refined word for each masked position,0.6451709866523743
translation,201,23,model,novel natural language generation model,based on,pre-trained language models,novel natural language generation model based on pre-trained language models,0.5737715363502502
translation,201,23,model,model,present,novel natural language generation model,model present novel natural language generation model,0.5956054329872131
translation,201,24,model,first work,extend,bert,first work extend bert,0.6753066778182983
translation,201,24,model,model,is,first work,model is first work,0.6094924211502075
translation,201,24,model,model,extend,bert,model extend bert,0.6953198909759521
translation,201,26,model,first stage,generate,summary,first stage generate summary,0.7000285983085632
translation,201,26,model,summary,using,leftcontext-only - decoder,summary using leftcontext-only - decoder,0.6506099104881287
translation,201,26,model,model,On,first stage,model On first stage,0.5841516852378845
translation,201,27,model,second stage,mask,each word,second stage mask each word,0.7647114992141724
translation,201,27,model,second stage,predict,refined word,second stage predict refined word,0.7420635223388672
translation,201,27,model,each word,of,summary,each word of summary,0.5907084941864014
translation,201,27,model,refined word,using,refine decoder,refined word using refine decoder,0.7089829444885254
translation,201,27,model,one - by- one,using,refine decoder,one - by- one using refine decoder,0.6938632726669312
translation,201,27,model,refined word,has,one - by- one,refined word has one - by- one,0.615049421787262
translation,201,28,model,reinforcement objective,with,refine decoder,reinforcement objective with refine decoder,0.6673822402954102
translation,201,33,results,our model,achieves,33.48 average,our model achieves 33.48 average,0.6505536437034607
translation,201,33,results,33.48 average,of,"rouge -1 , rouge - 2 and rouge -l","33.48 average of rouge -1 , rouge - 2 and rouge -l",0.585010290145874
translation,201,33,results,33.48 average,on,cnn / daily mail,33.48 average on cnn / daily mail,0.49975550174713135
translation,201,33,results,results,has,our model,results has our model,0.5871725678443909
translation,201,34,results,our model,achieves,about 5.6 % relative improvement,our model achieves about 5.6 % relative improvement,0.6363859176635742
translation,201,34,results,about 5.6 % relative improvement,over,rouge -1,about 5.6 % relative improvement over rouge -1,0.6729756593704224
translation,201,34,results,new york times dataset,has,our model,new york times dataset has our model,0.5727517008781433
translation,201,34,results,results,On,new york times dataset,results On new york times dataset,0.5169159173965454
translation,201,156,results,results,on,cnn / daily mail dataset,results on cnn / daily mail dataset,0.4994055926799774
translation,201,158,results,rouge - 1 and rouge - 2 score,of,our full model,rouge - 1 and rouge - 2 score of our full model,0.5897676348686218
translation,201,158,results,our full model,comparable with,dca,our full model comparable with dca,0.7083190679550171
translation,201,158,results,our full model,outperforms on,rouge -l,our full model outperforms on rouge -l,0.8168855309486389
translation,201,159,results,extractive models neusum and mask - lm global,achieve,slight higher rouge -1,extractive models neusum and mask - lm global achieve slight higher rouge -1,0.6057689189910889
translation,201,167,results,pointer - generator,with,coverage,pointer - generator with coverage,0.6533694863319397
translation,201,167,results,improvements,of,our model,improvements of our model,0.5960047841072083
translation,201,167,results,improvements,higher than,shorter samples,improvements higher than shorter samples,0.72299724817276
translation,201,167,results,our model,higher than,shorter samples,our model higher than shorter samples,0.7266338467597961
translation,201,167,results,pointer - generator,has,improvements,pointer - generator has improvements,0.5828354954719543
translation,201,167,results,length interval,has,40 - 80,length interval has 40 - 80,0.5475324392318726
translation,201,167,results,length interval,has,improvements,length interval has improvements,0.6028213500976562
translation,201,174,results,2.39 rouge - 1 improvements,compared to,ml +rl,2.39 rouge - 1 improvements compared to ml +rl,0.6653112769126892
translation,201,174,results,ml +rl,with,intraattn approach,ml +rl with intraattn approach,0.6763952374458313
translation,201,175,results,rouge - 2,get,improvement,rouge - 2 get improvement,0.6288033127784729
translation,201,175,results,our model,get,improvement,our model get improvement,0.6077530384063721
translation,201,175,results,improvement,of,0.51,improvement of 0.51,0.575422465801239
translation,201,175,results,rouge - 2,has,our model,rouge - 2 has our model,0.6204230785369873
translation,201,175,results,results,On,rouge - 2,results On rouge - 2,0.5640044212341309
translation,202,106,ablation-analysis,topic segmentation,contributes to,better coverage,topic segmentation contributes to better coverage,0.6810752153396606
translation,202,106,ablation-analysis,better coverage,of,salient words,better coverage of salient words,0.5752736330032349
translation,202,106,ablation-analysis,better coverage,demonstrated by,improvement,better coverage demonstrated by improvement,0.6808343529701233
translation,202,106,ablation-analysis,improvement,on,rouge metrics,improvement on rouge metrics,0.4941054880619049
translation,202,106,ablation-analysis,rouge metrics,of,model,rouge metrics of model,0.5514974594116211
translation,202,106,ablation-analysis,model,without,vfoa features,model without vfoa features,0.6948028802871704
translation,202,106,ablation-analysis,ablation analysis,has,topic segmentation,ablation analysis has topic segmentation,0.5462769865989685
translation,202,6,model,multi-modal hierarchical attention mechanism,across,three levels,multi-modal hierarchical attention mechanism across three levels,0.6692229509353638
translation,202,6,model,model,propose,multi-modal hierarchical attention mechanism,model propose multi-modal hierarchical attention mechanism,0.6477615833282471
translation,202,7,model,focus,into,topically - relevant segments,focus into topically - relevant segments,0.5942259430885315
translation,202,7,model,topically - relevant segments,jointly model,topic segmentation and summarization,topically - relevant segments jointly model topic segmentation and summarization,0.7653710246086121
translation,202,7,model,model,To narrow down,focus,model To narrow down focus,0.79268479347229
translation,202,8,model,new multi-modal features,derived from,visual focus of attention,new multi-modal features derived from visual focus of attention,0.6008557081222534
translation,202,8,model,model,introduce,new multi-modal features,model introduce new multi-modal features,0.6794113516807556
translation,202,22,model,topic segmentation,as,auxiliary task,topic segmentation as auxiliary task,0.48368144035339355
translation,202,22,model,topic segmentation,limit,attention,topic segmentation limit attention,0.6312199234962463
translation,202,22,model,attention,within,each segment,attention within each segment,0.6886458396911621
translation,202,22,model,model,learn,topic segmentation,model learn topic segmentation,0.6597802042961121
translation,202,121,model,hierarchical attention,based on,topic segmentation,hierarchical attention based on topic segmentation,0.5937864184379578
translation,202,121,model,model,propose,hierarchical attention,model propose hierarchical attention,0.6719768643379211
translation,202,95,results,vfoa estimation model,trained separately on,vfoa annotation,vfoa estimation model trained separately on vfoa annotation,0.6844953894615173
translation,202,95,results,vfoa estimation model,achieve,64.5 % prediction accuracy,vfoa estimation model achieve 64.5 % prediction accuracy,0.6040633916854858
translation,202,95,results,vfoa annotation,of,14 meetings,vfoa annotation of 14 meetings,0.6231358647346497
translation,202,95,results,14 meetings,in,dataset,14 meetings in dataset,0.49365565180778503
translation,202,95,results,results,has,vfoa estimation model,results has vfoa estimation model,0.4979264736175537
translation,202,100,results,multimodal summarizer,achieves,larger improvement,multimodal summarizer achieves larger improvement,0.6706297993659973
translation,202,100,results,larger improvement,on,rouge,larger improvement on rouge,0.5451657772064209
translation,202,100,results,larger improvement,than,bleu,larger improvement than bleu,0.5729604959487915
translation,202,100,results,abstractive method pgn,has,multimodal summarizer,abstractive method pgn has multimodal summarizer,0.5176597237586975
translation,202,100,results,results,Compared to,abstractive method pgn,results Compared to abstractive method pgn,0.6930673122406006
translation,202,113,results,topic segmentation accuracy,is,57.74 %,topic segmentation accuracy is 57.74 %,0.5328432321548462
translation,202,113,results,topic segmentation accuracy,is,60.11 %,topic segmentation accuracy is 60.11 %,0.542094349861145
translation,202,113,results,57.74 %,without,vfoa feature,57.74 % without vfoa feature,0.7073456645011902
translation,202,113,results,57.74 %,without,vfoa feature,57.74 % without vfoa feature,0.7073456645011902
translation,202,113,results,vfoa feature,in,segmentation attention,vfoa feature in segmentation attention,0.4869868755340576
translation,202,113,results,60.11 %,with,vfoa feature,60.11 % with vfoa feature,0.6278666853904724
translation,202,113,results,vfoa feature,in,segmentation attention,vfoa feature in segmentation attention,0.4869868755340576
translation,202,113,results,results,has,topic segmentation accuracy,results has topic segmentation accuracy,0.5542189478874207
translation,202,114,results,extractive method corerank,demonstrate that,abstractive summaries,extractive method corerank demonstrate that abstractive summaries,0.6625991463661194
translation,202,114,results,our bleu scores,are,doubled,our bleu scores are doubled,0.5978323817253113
translation,202,114,results,doubled,demonstrate that,abstractive summaries,doubled demonstrate that abstractive summaries,0.6763671040534973
translation,202,114,results,abstractive summaries,are,more coherent,abstractive summaries are more coherent,0.581313967704773
translation,202,114,results,extractive method corerank,has,our bleu scores,extractive method corerank has our bleu scores,0.5620890259742737
translation,202,114,results,results,Compared to,extractive method corerank,results Compared to extractive method corerank,0.6552411317825317
translation,203,109,experimental-setup,svd,takes,approximately 2.5 minutes ( 150 seconds ),svd takes approximately 2.5 minutes ( 150 seconds ),0.6881932616233826
translation,203,109,experimental-setup,approximately 2.5 minutes ( 150 seconds ),using,matlab,approximately 2.5 minutes ( 150 seconds ) using matlab,0.6394526362419128
translation,203,109,experimental-setup,matlab,on,our 12 - core machine,matlab on our 12 - core machine,0.5330706834793091
translation,203,109,experimental-setup,our 12 - core machine,with,24gb ram,our 12 - core machine with 24gb ram,0.5865240097045898
translation,203,109,experimental-setup,experimental setup,has,svd,experimental setup has svd,0.5345869660377502
translation,203,6,model,each sentence,in,semantic space,each sentence in semantic space,0.5095775127410889
translation,203,6,model,summary,by choosing,subset of sentences,summary by choosing subset of sentences,0.6692138910293579
translation,203,6,model,convex hull,maximizes,volume,convex hull maximizes volume,0.7045114040374756
translation,203,7,model,greedy algorithm,based on,gram-schmidt process,greedy algorithm based on gram-schmidt process,0.6832253932952881
translation,203,7,model,greedy algorithm,to efficiently perform,volume maximization,greedy algorithm to efficiently perform volume maximization,0.7001704573631287
translation,203,7,model,model,provide,greedy algorithm,model provide greedy algorithm,0.6545225977897644
translation,203,18,model,new objective function,for,summarization,new objective function for summarization,0.6146880388259888
translation,203,18,model,new objective function,provide,fast greedy algorithm,new objective function provide fast greedy algorithm,0.6014750599861145
translation,203,18,model,summarization,based on,semantic volume,summarization based on semantic volume,0.6060971617698669
translation,203,18,model,model,formalize,new objective function,model formalize new objective function,0.700254499912262
translation,203,18,model,model,provide,fast greedy algorithm,model provide fast greedy algorithm,0.6266834139823914
translation,203,43,model,new scoring function,for,summarization,new scoring function for summarization,0.628684937953949
translation,203,43,model,model,introduce,new scoring function,model introduce new scoring function,0.6639699935913086
translation,203,98,results,cbs,generally better than,mmr,cbs generally better than mmr,0.6664707660675049
translation,203,99,results,our method,has,outperforms,our method has outperforms,0.6322360634803772
translation,203,99,results,outperforms,has,other competing methods,outperforms has other competing methods,0.5592252016067505
translation,203,99,results,results,has,our method,results has our method,0.5589964985847473
translation,204,107,results,boost,in,bleu score,boost in bleu score,0.496162086725235
translation,204,107,results,bleu score,when not removing,automatically generated code,bleu score when not removing automatically generated code,0.6580215692520142
translation,204,107,results,results,found,boost,results found boost,0.5594690442085266
translation,205,137,ablation-analysis,transformer encoder,with,seq2seq encoder,transformer encoder with seq2seq encoder,0.6012994647026062
translation,205,137,ablation-analysis,drop,of,2 rouge points,drop of 2 rouge points,0.6014474630355835
translation,205,137,ablation-analysis,drop,about,2 rouge points,drop about 2 rouge points,0.6393174529075623
translation,205,137,ablation-analysis,ablation analysis,Switching,transformer encoder,ablation analysis Switching transformer encoder,0.7839990854263306
translation,205,108,hyperparameters,hyperparameters,used,pre-trained 300 - dimensional glov e 2 word-embeddings,hyperparameters used pre-trained 300 - dimensional glov e 2 word-embeddings,0.5369145274162292
translation,205,109,hyperparameters,transformer encoder,setup with,transf ormer base hyperparameter setting,transformer encoder setup with transf ormer base hyperparameter setting,0.7586060166358948
translation,205,109,hyperparameters,transf ormer base hyperparameter setting,from,tensor2tensor library,transf ormer base hyperparameter setting from tensor2tensor library,0.5343090295791626
translation,205,109,hyperparameters,hidden size and dropout,reset to,300 and 0.0,hidden size and dropout reset to 300 and 0.0,0.7371039390563965
translation,205,109,hyperparameters,hyperparameters,has,transformer encoder,hyperparameters has transformer encoder,0.5264022350311279
translation,205,109,hyperparameters,hyperparameters,has,hidden size and dropout,hyperparameters has hidden size and dropout,0.5237963795661926
translation,205,110,hyperparameters,300 hidden units,for,gru - rnn encoder,300 hidden units for gru - rnn encoder,0.5536166429519653
translation,205,116,hyperparameters,early stopping,when,validation loss,early stopping when validation loss,0.6184698343276978
translation,205,116,hyperparameters,does not decrease,after,5 epochs,does not decrease after 5 epochs,0.6962795853614807
translation,205,116,hyperparameters,validation loss,has,does not decrease,validation loss has does not decrease,0.6212654709815979
translation,205,116,hyperparameters,hyperparameters,employ,early stopping,hyperparameters employ early stopping,0.5583352446556091
translation,205,117,hyperparameters,gradient clipping,at,5.0,gradient clipping at 5.0,0.5251092314720154
translation,205,117,hyperparameters,hyperparameters,apply,gradient clipping,hyperparameters apply gradient clipping,0.6040953397750854
translation,205,118,hyperparameters,greedy - decoding,during,training and validation,greedy - decoding during training and validation,0.6798409223556519
translation,205,118,hyperparameters,greedy - decoding,set,maximum number of iterations,greedy - decoding set maximum number of iterations,0.6419907212257385
translation,205,118,hyperparameters,maximum number of iterations,to,5 times,maximum number of iterations to 5 times,0.5600403547286987
translation,205,118,hyperparameters,5 times,has,target sentence length,5 times has target sentence length,0.5595189929008484
translation,205,118,hyperparameters,hyperparameters,use,greedy - decoding,hyperparameters use greedy - decoding,0.6462412476539612
translation,205,118,hyperparameters,hyperparameters,set,maximum number of iterations,hyperparameters set maximum number of iterations,0.6202464699745178
translation,205,23,model,neural extractor,uses,"recent bidirectional transformer encoder ( vaswani et al. , 2017 )","neural extractor uses recent bidirectional transformer encoder ( vaswani et al. , 2017 )",0.5099483728408813
translation,205,23,model,model,has,neural extractor,model has neural extractor,0.5638430118560791
translation,205,93,model,two efficient models,has,"transformer ( vaswani et al. , 2017 )","two efficient models has transformer ( vaswani et al. , 2017 )",0.5487570762634277
translation,205,93,model,model,interleave,two efficient models,model interleave two efficient models,0.7914707064628601
translation,205,119,model,beam-search decoding,used during,inference,beam-search decoding used during inference,0.6559163331985474
translation,205,119,model,model,has,beam-search decoding,model has beam-search decoding,0.5453792214393616
translation,205,163,model,model,jointly learn to,paraphrase and compress,model jointly learn to paraphrase and compress,0.7806099057197571
translation,205,4,results,single document summarization,using,cnn / dailymail and newsroom datasets,single document summarization using cnn / dailymail and newsroom datasets,0.5949949026107788
translation,205,9,results,transformer and seq2seq model,complement,adequately,transformer and seq2seq model complement adequately,0.7168874144554138
translation,205,9,results,transformer and seq2seq model,making for,richer encoded vector representation,transformer and seq2seq model making for richer encoded vector representation,0.716054379940033
translation,205,9,results,results,find that,transformer and seq2seq model,results find that transformer and seq2seq model,0.6213327050209045
translation,205,10,results,more attention,to,vocabulary,more attention to vocabulary,0.5825815200805664
translation,205,10,results,more attention,improves,performance,more attention improves performance,0.6907064914703369
translation,205,10,results,vocabulary,of,target words,vocabulary of target words,0.5808107256889343
translation,205,10,results,vocabulary,improves,performance,vocabulary improves performance,0.6493275761604309
translation,205,10,results,abstraction,improves,performance,abstraction improves performance,0.7405301928520203
translation,205,10,results,results,find that,more attention,results find that more attention,0.6588196754455566
translation,205,10,results,results,paying,more attention,results paying more attention,0.7618318200111389
translation,205,24,results,abstractive summaries,correlate better with,summaries,abstractive summaries correlate better with summaries,0.6531804800033569
translation,205,31,results,improvements,using,framework,improvements using framework,0.7534710764884949
translation,205,31,results,improvements,in,abstractive setting,improvements in abstractive setting,0.5023874640464783
translation,205,31,results,framework,in,abstractive setting,framework in abstractive setting,0.4737507104873657
translation,205,31,results,results,notice,improvements,results notice improvements,0.7752933502197266
translation,205,131,results,our baseline non-filtered extractive ( transext ) model,is,highly competitive,our baseline non-filtered extractive ( transext ) model is highly competitive,0.5110418200492859
translation,205,131,results,highly competitive,with,top models,highly competitive with top models,0.6855272650718689
translation,205,131,results,results,has,our baseline non-filtered extractive ( transext ) model,results has our baseline non-filtered extractive ( transext ) model,0.5410621166229248
translation,205,132,results,trans - ext + filter,produces,average of about + 1 and + 9 points,trans - ext + filter produces average of about + 1 and + 9 points,0.6197469234466553
translation,205,132,results,average of about + 1 and + 9 points,across,reported rouge variants,average of about + 1 and + 9 points across reported rouge variants,0.7160139679908752
translation,205,132,results,reported rouge variants,on,cnn / dm and newsroom datasets,reported rouge variants on cnn / dm and newsroom datasets,0.49279284477233887
translation,205,132,results,results,has,trans - ext + filter,results has trans - ext + filter,0.5834348201751709
translation,205,136,results,extractive model,trained with,balanced labels,extractive model trained with balanced labels,0.758089542388916
translation,205,136,results,+ 20 % f score increase,trained with,balanced labels,+ 20 % f score increase trained with balanced labels,0.7191798686981201
translation,205,136,results,extractive model,has,+ 20 % f score increase,extractive model has + 20 % f score increase,0.5922452807426453
translation,205,136,results,results,has,extractive model,results has extractive model,0.5668983459472656
translation,205,138,results,highly competitive,with,top models,highly competitive with top models,0.6855272650718689
translation,205,138,results,highly competitive,with,drop,highly competitive with drop,0.7304883599281311
translation,205,138,results,drop,of,- 0.81 rouge - 2 points,drop of - 0.81 rouge - 2 points,0.5941414833068848
translation,205,138,results,results,has,baseline non-filtered abstractive ( transext + abs ) model,results has baseline non-filtered abstractive ( transext + abs ) model,0.5556959509849548
translation,205,139,results,trans - ext + filter + abs,average of about,+ 0.5 and + 7 points,trans - ext + filter + abs average of about + 0.5 and + 7 points,0.7215388417243958
translation,205,139,results,+ 0.5 and + 7 points,across,reported rouge variants,+ 0.5 and + 7 points across reported rouge variants,0.7216736078262329
translation,205,139,results,reported rouge variants,on,cnn / dm and newsroom datasets,reported rouge variants on cnn / dm and newsroom datasets,0.49279284477233887
translation,205,139,results,results,has,trans - ext + filter + abs,results has trans - ext + filter + abs,0.5890744924545288
translation,205,164,results,gru - rnn decoder,with,two -level stack of hybrid encoders ( transformer and gru - rnn ),gru - rnn decoder with two -level stack of hybrid encoders ( transformer and gru - rnn ),0.634075403213501
translation,205,164,results,two -level stack of hybrid encoders ( transformer and gru - rnn ),gives,better performance,two -level stack of hybrid encoders ( transformer and gru - rnn ) gives better performance,0.5826091766357422
translation,206,164,baselines,lead baseline,extracts,first 75 characters as the summary,lead baseline extracts first 75 characters as the summary,0.6357578039169312
translation,206,164,baselines,baselines,has,duc2004,baselines has duc2004,0.5307953953742981
translation,206,201,baselines,cycle-consistency,to reconstruct,source sentences,cycle-consistency to reconstruct source sentences,0.6615465879440308
translation,206,201,baselines,source sentences,from,"headline generation corpus ( rush et al. , 2015 )","source sentences from headline generation corpus ( rush et al. , 2015 )",0.4828222095966339
translation,206,194,hyperparameters,forward and backward language models,use,long short term memory units,forward and backward language models use long short term memory units,0.5677629113197327
translation,206,194,hyperparameters,forward and backward language models,optimized for,50 epochs,forward and backward language models optimized for 50 epochs,0.7030971646308899
translation,206,194,hyperparameters,50 epochs,by,stochastic gradient descent,50 epochs by stochastic gradient descent,0.5301805734634399
translation,206,194,hyperparameters,hyperparameters,has,forward and backward language models,hyperparameters has forward and backward language models,0.5302854180335999
translation,206,195,hyperparameters,embeddings and hidden sizes,set to,1024 dimensions,embeddings and hidden sizes set to 1024 dimensions,0.6306886076927185
translation,206,195,hyperparameters,hyperparameters,has,embeddings and hidden sizes,hyperparameters has embeddings and hidden sizes,0.5302416682243347
translation,206,196,hyperparameters,hyperparameters,on,development data,hyperparameters on development data,0.5105860829353333
translation,206,196,hyperparameters,development data,of,headline corpus,development data of headline corpus,0.5265918970108032
translation,206,196,hyperparameters,weighting parameter,for,all models,weighting parameter for all models,0.5550682544708252
translation,206,196,hyperparameters,12,for,all models,12 for all models,0.6505037546157837
translation,206,196,hyperparameters,hyperparameters,tune,hyperparameters,hyperparameters tune hyperparameters,0.7059558629989624
translation,206,196,hyperparameters,hyperparameters,on,development data,hyperparameters on development data,0.5105860829353333
translation,206,196,hyperparameters,hyperparameters,set,weighting parameter,hyperparameters set weighting parameter,0.6093893051147461
translation,206,197,hyperparameters,search steps and restarts,set to,? t = 0.1 and ? r = 0.035,search steps and restarts set to ? t = 0.1 and ? r = 0.035,0.6757614612579346
translation,206,197,hyperparameters,hyperparameters,has,search steps and restarts,hyperparameters has search steps and restarts,0.5192739367485046
translation,206,207,hyperparameters,summary length,set to,50 %,summary length set to 50 %,0.6263920068740845
translation,206,207,hyperparameters,summary length,controlled by,length embeddings,summary length controlled by length embeddings,0.6721108555793762
translation,206,207,hyperparameters,50 %,of,input length,50 % of input length,0.6343013644218445
translation,206,207,hyperparameters,length embeddings,in,decoder,length embeddings in decoder,0.5214774012565613
translation,206,207,hyperparameters,hyperparameters,has,summary length,hyperparameters has summary length,0.4892083406448364
translation,206,6,model,unsupervised objective function,consisting of,language modeling,unsupervised objective function consisting of language modeling,0.6480278968811035
translation,206,6,model,unsupervised objective function,consisting of,semantic similarity metrics,unsupervised objective function consisting of semantic similarity metrics,0.6655319333076477
translation,206,7,model,high-scoring summary,by,discrete optimization,high-scoring summary by discrete optimization,0.5554136633872986
translation,206,7,model,model,search for,high-scoring summary,model search for high-scoring summary,0.6825530529022217
translation,206,28,results,humanwritten reference summaries,exhibit,high word overlap,humanwritten reference summaries exhibit high word overlap,0.5980844497680664
translation,206,28,results,high word overlap,with,source sentence,high word overlap with source sentence,0.6271856427192688
translation,206,221,results,competing methods,in,all rouge f1 scores,competing methods in all rouge f1 scores,0.4549718499183655
translation,206,221,results,title side only,has,hc title 10 model,title side only has hc title 10 model,0.6112618446350098
translation,206,221,results,hc title 10 model,has,outperforms,hc title 10 model has outperforms,0.651808500289917
translation,206,221,results,outperforms,has,competing methods,outperforms has competing methods,0.5765846371650696
translation,206,221,results,results,Trained on,title side only,results Trained on title side only,0.7132920622825623
translation,206,223,results,same extra corpus,to pretrain,sent2vec embeddings,same extra corpus to pretrain sent2vec embeddings,0.6757838129997253
translation,206,223,results,hc title + billion 10 variant,achieves,even better performance,hc title + billion 10 variant achieves even better performance,0.6723559498786926
translation,206,223,results,outperforming,by,2.32 rouge - 1 and 1.41 rouge -l points,outperforming by 2.32 rouge - 1 and 1.41 rouge -l points,0.6052120923995972
translation,206,223,results,zhou and rush ( 2019 ),by,2.32 rouge - 1 and 1.41 rouge -l points,zhou and rush ( 2019 ) by 2.32 rouge - 1 and 1.41 rouge -l points,0.5734978318214417
translation,206,223,results,same extra corpus,has,hc title + billion 10 variant,same extra corpus has hc title + billion 10 variant,0.6208357810974121
translation,206,223,results,sent2vec embeddings,has,hc title + billion 10 variant,sent2vec embeddings has hc title + billion 10 variant,0.567625105381012
translation,206,223,results,outperforming,has,zhou and rush ( 2019 ),outperforming has zhou and rush ( 2019 ),0.6087770462036133
translation,206,223,results,results,With,same extra corpus,results With same extra corpus,0.6711153984069824
translation,206,227,results,hc title+ twitter 10,performs,better,hc title+ twitter 10 performs better,0.6824916005134583
translation,206,227,results,better,than,hc title 10,better than hc title 10,0.6326988935470581
translation,206,227,results,better,than,other competitors,better than other competitors,0.646047830581665
translation,206,227,results,results,has,hc title+ twitter 10,results has hc title+ twitter 10,0.6075939536094666
translation,206,231,results,our model,is,only unsupervised summarization system,our model is only unsupervised summarization system,0.5257667899131775
translation,206,231,results,outperforms,has,lead - p-50 baseline,outperforms has lead - p-50 baseline,0.5897855162620544
translation,206,231,results,results,has,our model,results has our model,0.5871725678443909
translation,206,232,results,our models,trained on,title side ( hc title ),our models trained on title side ( hc title ),0.7452499866485596
translation,206,232,results,consistently outperform,trained on,article side ( hc article ),consistently outperform trained on article side ( hc article ),0.7573041915893555
translation,206,232,results,our models,has,consistently outperform,our models has consistently outperform,0.5840821266174316
translation,206,232,results,results,noted that,our models,results noted that our models,0.6685352325439453
translation,206,236,results,results,on,duc2004 data,results on duc2004 data,0.5198886394500732
translation,206,238,results,outperform,has,all previous methods,outperform has all previous methods,0.5781576037406921
translation,206,251,results,our model,on par with,zhou and rush ( 2019 ),our model on par with zhou and rush ( 2019 ),0.6991477012634277
translation,206,251,results,results,has,our model,results has our model,0.5871725678443909
translation,207,8,ablation-analysis,generation of in- length summaries,for,post-editing,generation of in- length summaries for post-editing,0.6041955947875977
translation,207,8,ablation-analysis,post-editing,with,dataset mainich,post-editing with dataset mainich,0.6151663064956665
translation,207,8,ablation-analysis,dataset mainich,created with,strict length constraints,dataset mainich created with strict length constraints,0.6428399085998535
translation,207,8,ablation-analysis,ablation analysis,importance of,generation of in- length summaries,ablation analysis importance of generation of in- length summaries,0.6909195780754089
translation,207,34,ablation-analysis,importance of the generation of in-length summaries,for,post-editing,importance of the generation of in-length summaries for post-editing,0.5978077054023743
translation,207,34,ablation-analysis,ablation analysis,demonstrate,importance of the generation of in-length summaries,ablation analysis demonstrate importance of the generation of in-length summaries,0.6059266328811646
translation,207,6,experiments,golc,on,japanese single document summarization data set,golc on japanese single document summarization data set,0.47292548418045044
translation,207,6,experiments,minimum risk training,on,cnn / daily mail,minimum risk training on cnn / daily mail,0.5532981157302856
translation,207,6,experiments,japanese single document summarization data set,of,mainichi shimbun newspapers,japanese single document summarization data set of mainichi shimbun newspapers,0.48862123489379883
translation,207,6,experiments,two optimization methods,has,maximum log-likelihood,two optimization methods has maximum log-likelihood,0.5125715732574463
translation,207,4,model,length constraint ( golc ),for,neural text summarization models,length constraint ( golc ) for neural text summarization models,0.5922161936759949
translation,207,24,model,global training,based on,minimum risk training ( mrt ),global training based on minimum risk training ( mrt ),0.6850824952125549
translation,207,24,model,minimum risk training ( mrt ),under,length constraint,minimum risk training ( mrt ) under length constraint,0.6384962201118469
translation,207,24,model,model,incorporating,global training,model incorporating global training,0.7588808536529541
translation,207,5,results,golc,increases,probabilities of generating summaries,golc increases probabilities of generating summaries,0.7469183802604675
translation,207,5,results,probabilities of generating summaries,that have,high evaluation scores,probabilities of generating summaries that have high evaluation scores,0.6018979549407959
translation,207,5,results,high evaluation scores,within,desired length,high evaluation scores within desired length,0.623426079750061
translation,207,5,results,results,has,golc,results has golc,0.5606849789619446
translation,207,7,results,neural summarization model,optimized with,golc,neural summarization model optimized with golc,0.6860427260398865
translation,207,7,results,fewer overlength summaries,while maintaining,fastest processing speed,fewer overlength summaries while maintaining fastest processing speed,0.7115209102630615
translation,207,7,results,summaries,on,cnn / daily,summaries on cnn / daily,0.5514227151870728
translation,207,7,results,summaries,on,cnn,summaries on cnn,0.6139718294143677
translation,207,7,results,7.8 %,on,long summary,7.8 % on long summary,0.5056039094924927
translation,207,7,results,long summary,of,mainichi,long summary of mainichi,0.598746120929718
translation,207,7,results,long summary,of,mainichi,long summary of mainichi,0.598746120929718
translation,207,7,results,approximately 20 % to 50 %,on,cnn,approximately 20 % to 50 % on cnn,0.5925685167312622
translation,207,7,results,10 % to 30 %,on,mainichi,10 % to 30 % on mainichi,0.5866780281066895
translation,207,7,results,mainichi,with,other optimization methods,mainichi with other optimization methods,0.5742589235305786
translation,207,7,results,6.70 %,has,overlength,6.70 % has overlength,0.6027371287345886
translation,207,7,results,overlength,has,summaries,overlength has summaries,0.6019664406776428
translation,207,7,results,results,show,state-,results show state-,0.6017173528671265
translation,207,28,results,neural summarization models,trained with,golc,neural summarization models trained with golc,0.7386141419410706
translation,207,28,results,better,than,existing methods,better than existing methods,0.5996989607810974
translation,207,28,results,output length,has,better,output length has better,0.602658748626709
translation,207,28,results,results,show,neural summarization models,results show neural summarization models,0.5766400694847107
translation,207,32,results,models,trained with,golc,models trained with golc,0.787423849105835
translation,207,32,results,golc,showed,better rouge scores,golc showed better rouge scores,0.69658362865448
translation,207,32,results,better rouge scores,than,maximum loglikelihood based methods,better rouge scores than maximum loglikelihood based methods,0.5676454901695251
translation,207,32,results,better rouge scores,of,maximum loglikelihood based methods,better rouge scores of maximum loglikelihood based methods,0.5601148009300232
translation,207,32,results,maximum loglikelihood based methods,while generating,summaries,maximum loglikelihood based methods while generating summaries,0.6977640986442566
translation,207,32,results,summaries,satisfying,length constraint,summaries satisfying length constraint,0.7695223689079285
translation,207,32,results,results,has,models,results has models,0.5335168838500977
translation,208,91,baselines,different submodular functions,i.e.,facility location function,different submodular functions i.e. facility location function,0.6361570954322815
translation,208,91,baselines,different submodular functions,i.e.,saturated coverage function,different submodular functions i.e. saturated coverage function,0.6546794772148132
translation,208,4,model,methods,from,submodular function optimization,methods from submodular function optimization,0.4948384463787079
translation,208,4,model,submodular function optimization,developed for,document summarization,submodular function optimization developed for document summarization,0.5769920349121094
translation,208,4,model,model,leverage,methods,model leverage methods,0.6369438767433167
translation,208,101,model,acoustic data subset selection,based on,submodular function optimization,acoustic data subset selection based on submodular function optimization,0.5874524712562561
translation,208,79,results,facility location,superior to,saturated cover function,facility location superior to saturated cover function,0.7311681509017944
translation,208,79,results,saturated cover function,across,board,saturated cover function across board,0.7069411873817444
translation,208,79,results,results,found that,facility location,results found that facility location,0.6631691455841064
translation,208,80,results,performance,of,random and entropy - based baselines,performance of random and entropy - based baselines,0.5911004543304443
translation,208,80,results,performance,of,facility location function,performance of facility location function,0.6098518371582031
translation,208,80,results,performance,of,facility location function,performance of facility location function,0.6098518371582031
translation,208,80,results,performance,of,facility location function,performance of facility location function,0.6098518371582031
translation,208,80,results,facility location function,with,similarity measures,facility location function with similarity measures,0.6354076862335205
translation,208,81,results,entropy - based baseline,beats,random baseline,entropy - based baseline beats random baseline,0.6981889605522156
translation,208,81,results,random baseline,for,most percentage cases,random baseline for most percentage cases,0.6026692390441895
translation,208,81,results,results,has,entropy - based baseline,results has entropy - based baseline,0.573445737361908
translation,208,89,results,best performance,obtained with,string kernel,best performance obtained with string kernel,0.5983157753944397
translation,208,89,results,string kernel,when using,small training data sets ( 2.5 % - 10 % ),string kernel when using small training data sets ( 2.5 % - 10 % ),0.7100467681884766
translation,208,89,results,results,has,best performance,results has best performance,0.5759831070899963
translation,208,90,results,submodular selection methods,yield,significant improvements ( p < 0.05 ),submodular selection methods yield significant improvements ( p < 0.05 ),0.6933326125144958
translation,208,90,results,significant improvements ( p < 0.05 ),over,random baseline,significant improvements ( p < 0.05 ) over random baseline,0.6506333351135254
translation,208,90,results,significant improvements ( p < 0.05 ),over,entropybased method,significant improvements ( p < 0.05 ) over entropybased method,0.6619912981987
translation,208,90,results,results,has,submodular selection methods,results has submodular selection methods,0.479089617729187
translation,209,198,baselines,"bertext ( liu and lapata , 2019 )",is,extractive model,"bertext ( liu and lapata , 2019 ) is extractive model",0.5794743895530701
translation,209,198,baselines,extractive model,fine-tuned on,"bert ( devlin et al. , 2019 )","extractive model fine-tuned on bert ( devlin et al. , 2019 )",0.7210453152656555
translation,209,198,baselines,baselines,has,"bertext ( liu and lapata , 2019 )","baselines has bertext ( liu and lapata , 2019 )",0.569980263710022
translation,209,165,experimental-setup,shallower,with,6 layers,shallower with 6 layers,0.6903751492500305
translation,209,165,experimental-setup,experimental setup,has,decoder,experimental setup has decoder,0.5172298550605774
translation,209,166,experimental-setup,number,of,total trainable model parameters,number of total trainable model parameters,0.593446671962738
translation,209,166,experimental-setup,total trainable model parameters,is,585m,total trainable model parameters is 585m,0.5692057013511658
translation,209,166,experimental-setup,experimental setup,has,number,experimental setup has number,0.5432479977607727
translation,209,167,experimental-setup,hidden size and number of attention head,of,decoder,hidden size and number of attention head of decoder,0.6086721420288086
translation,209,167,experimental-setup,hidden size and number of attention head,of,encoder,hidden size and number of attention head of encoder,0.6000874638557434
translation,209,167,experimental-setup,hidden size and number of attention head,identical to,encoder,hidden size and number of attention head identical to encoder,0.6411275267601013
translation,209,167,experimental-setup,feed -forward filter size,is,"2,048","feed -forward filter size is 2,048",0.5968612432479858
translation,209,167,experimental-setup,experimental setup,has,hidden size and number of attention head,experimental setup has hidden size and number of attention head,0.5329119563102722
translation,209,168,experimental-setup,smaller filter size,in,decoder,smaller filter size in decoder,0.53742915391922
translation,209,168,experimental-setup,smaller filter size,to re-duce,computational and memory cost,smaller filter size to re-duce computational and memory cost,0.7172027230262756
translation,209,168,experimental-setup,experimental setup,use,smaller filter size,experimental setup use smaller filter size,0.5966386795043945
translation,209,169,experimental-setup,dropout rates,of,all layers,dropout rates of all layers,0.5479505062103271
translation,209,169,experimental-setup,all layers,in,encoder,all layers in encoder,0.5206490755081177
translation,209,169,experimental-setup,all layers,in,decoder,all layers in decoder,0.5444942712783813
translation,209,169,experimental-setup,all layers,set to,0.1,all layers set to 0.1,0.6847108006477356
translation,209,169,experimental-setup,all layers,in,decoder,all layers in decoder,0.5444942712783813
translation,209,169,experimental-setup,encoder,set to,0.1,encoder set to 0.1,0.644302487373352
translation,209,169,experimental-setup,dropout rates,in,decoder,dropout rates in decoder,0.5297099351882935
translation,209,169,experimental-setup,dropout rates,set to,0.3,dropout rates set to 0.3,0.6571010947227478
translation,209,169,experimental-setup,experimental setup,has,dropout rates,experimental setup has dropout rates,0.5134919285774231
translation,209,169,experimental-setup,experimental setup,has,dropout rates,experimental setup has dropout rates,0.5134919285774231
translation,209,170,experimental-setup,"adam ( kingma and ba , 2015 )",with,"? 1 = 0.9 , ? 2 = 0.98","adam ( kingma and ba , 2015 ) with ? 1 = 0.9 , ? 2 = 0.98",0.6447789072990417
translation,209,170,experimental-setup,experimental setup,optimized using,"adam ( kingma and ba , 2015 )","experimental setup optimized using adam ( kingma and ba , 2015 )",0.6984754204750061
translation,209,174,experimental-setup,peak learning rates,of,encoder and decoder,peak learning rates of encoder and decoder,0.6029037237167358
translation,209,174,experimental-setup,peak learning rates,set to,2e ? 5 and 1e ? 4,peak learning rates set to 2e ? 5 and 1e ? 4,0.7244967222213745
translation,209,174,experimental-setup,encoder and decoder,set to,2e ? 5 and 1e ? 4,encoder and decoder set to 2e ? 5 and 1e ? 4,0.7187512516975403
translation,209,174,experimental-setup,2e ? 5 and 1e ? 4,with,"10,000 warmup steps","2e ? 5 and 1e ? 4 with 10,000 warmup steps",0.6518601179122925
translation,209,174,experimental-setup,experimental setup,has,peak learning rates,experimental setup has peak learning rates,0.5025298595428467
translation,209,175,experimental-setup,experimental setup,adopted,same learning rate schedule strategies,experimental setup adopted same learning rate schedule strategies,0.6841580271720886
translation,209,176,experimental-setup,smaller batch sizes,for,datasets,smaller batch sizes for datasets,0.5902427434921265
translation,209,176,experimental-setup,datasets,with,less examples,datasets with less examples,0.6221332550048828
translation,209,176,experimental-setup,less examples,i.e.,"1,024","less examples i.e. 1,024",0.6787881255149841
translation,209,176,experimental-setup,"1,024",for,giga - cm,"1,024 for giga - cm",0.6703268885612488
translation,209,176,experimental-setup,256,for,cnndm,256 for cnndm,0.6627607941627502
translation,209,176,experimental-setup,128,for,nyt,128 for nyt,0.7354234457015991
translation,209,176,experimental-setup,each epoch,has,sufficient number of model updates,each epoch has sufficient number of model updates,0.5637502074241638
translation,209,176,experimental-setup,experimental setup,used,smaller batch sizes,experimental setup used smaller batch sizes,0.644214928150177
translation,209,177,experimental-setup,our models,until,convergence,our models until convergence,0.7803941965103149
translation,209,177,experimental-setup,convergence,of,validation perplexities,convergence of validation perplexities,0.5734893679618835
translation,209,177,experimental-setup,validation perplexities,around,30 epochs,validation perplexities around 30 epochs,0.6509934663772583
translation,209,177,experimental-setup,validation perplexities,around,60 epochs,validation perplexities around 60 epochs,0.6495038270950317
translation,209,177,experimental-setup,validation perplexities,around,40 epochs,validation perplexities around 40 epochs,0.6286112070083618
translation,209,177,experimental-setup,30 epochs,on,giga - cm,30 epochs on giga - cm,0.583380937576294
translation,209,177,experimental-setup,60 epochs,on,cnndm,60 epochs on cnndm,0.5720710158348083
translation,209,177,experimental-setup,40 epochs,on,nyt,40 epochs on nyt,0.6099846363067627
translation,209,177,experimental-setup,experimental setup,trained,our models,experimental setup trained our models,0.7180708050727844
translation,209,182,experimental-setup,learning rates,for,both the encoder and decoder,learning rates for both the encoder and decoder,0.5968345999717712
translation,209,182,experimental-setup,learning rates,set to,2e - 5,learning rates set to 2e - 5,0.7367083430290222
translation,209,182,experimental-setup,both the encoder and decoder,set to,2e - 5,both the encoder and decoder set to 2e - 5,0.7075206637382507
translation,209,182,experimental-setup,2e - 5,with,"4,000 warmup steps","2e - 5 with 4,000 warmup steps",0.6302993893623352
translation,209,182,experimental-setup,experimental setup,has,learning rates,experimental setup has learning rates,0.5043967366218567
translation,209,183,experimental-setup,our models,for,30 epochs,our models for 30 epochs,0.5807260274887085
translation,209,183,experimental-setup,our models,for,50 epochs,our models for 50 epochs,0.5818191170692444
translation,209,183,experimental-setup,30 epochs,on,cnndm,30 epochs on cnndm,0.5707211494445801
translation,209,183,experimental-setup,50 epochs,on,nyt,50 epochs on nyt,0.6196214556694031
translation,209,183,experimental-setup,experimental setup,trained,our models,experimental setup trained our models,0.7180708050727844
translation,209,190,experimental-setup,step size,of,5,step size of 5,0.6724815368652344
translation,209,190,experimental-setup,step size,to get,quick feedback,step size to get quick feedback,0.7034035325050354
translation,209,190,experimental-setup,experimental setup,used,step size,experimental setup used step size,0.6159926056861877
translation,209,191,experimental-setup,datasets,with,less instances,datasets with less instances,0.6012422442436218
translation,209,191,experimental-setup,fine-tuned,with,smaller batch sizes,fine-tuned with smaller batch sizes,0.6362064480781555
translation,209,191,experimental-setup,64,for,nyt,64 for nyt,0.731107771396637
translation,209,191,experimental-setup,768,for,cnndm,768 for cnndm,0.6520016193389893
translation,209,178,experiments,one epoch,on,giga - cm,one epoch on giga - cm,0.55179363489151
translation,209,6,model,seq2seq based abstractive summarization model,on,unlabeled text,seq2seq based abstractive summarization model on unlabeled text,0.5106861591339111
translation,209,6,model,model,pre-train,seq2seq based abstractive summarization model,model pre-train seq2seq based abstractive summarization model,0.7196949124336243
translation,209,7,model,input text,artificially constructed from,document,input text artificially constructed from document,0.687415599822998
translation,209,7,model,pre-trained,to reinstate,original document,pre-trained to reinstate original document,0.6839821934700012
translation,209,7,model,input text,has,model,input text has model,0.6026861667633057
translation,209,7,model,model,given,input text,model given input text,0.7274294495582581
translation,209,25,model,model,adopt,seq2seq transformer,model adopt seq2seq transformer,0.710460901260376
translation,209,26,model,seq2seq model,on,unlabeled text,seq2seq model on unlabeled text,0.491455614566803
translation,209,26,model,seq2seq model,namely,sentence reordering ( sr ),seq2seq model namely sentence reordering ( sr ),0.6958795189857483
translation,209,26,model,seq2seq model,namely,next sentence generation ( nsg ),seq2seq model namely next sentence generation ( nsg ),0.7117743492126465
translation,209,26,model,seq2seq model,namely,masked document generation ( mdg ),seq2seq model namely masked document generation ( mdg ),0.7299365401268005
translation,209,26,model,model,proposes,three sequence - to-sequence pre-training,model proposes three sequence - to-sequence pre-training,0.6849444508552551
translation,209,26,model,model,pretrain,seq2seq model,model pretrain seq2seq model,0.7579503059387207
translation,209,158,model,each piece,to,predict,each piece to predict,0.6419692039489746
translation,209,158,model,nsg,has,each piece,nsg has each piece,0.6294354796409607
translation,209,158,model,predict,has,next 256 tokens,predict has next 256 tokens,0.6251788139343262
translation,209,158,model,model,In,nsg,model In nsg,0.5782181620597839
translation,209,164,model,feed -forward filter size,are,"1,024 and 4,096","feed -forward filter size are 1,024 and 4,096",0.5964245200157166
translation,209,164,model,each layer,has,16 attention heads,each layer has 16 attention heads,0.5805294513702393
translation,209,164,model,model,has,each layer,model has each layer,0.5642764568328857
translation,209,203,model,transformer -s2s,is,12 - layer seq2seq,transformer -s2s is 12 - layer seq2seq,0.592014729976654
translation,209,203,model,model,has,transformer -s2s,model has transformer -s2s,0.5761575102806091
translation,209,10,results,models,pre-trained on,large-scale data ( ? 160gb ),models pre-trained on large-scale data ( ? 160gb ),0.7714395523071289
translation,209,10,results,models,achieves,comparable results,models achieves comparable results,0.671861469745636
translation,209,10,results,our method,with,only 19gb text,our method with only 19gb text,0.6032541394233704
translation,209,10,results,our method,achieves,comparable results,our method achieves comparable results,0.6143419146537781
translation,209,10,results,only 19gb text,for,pre-training,only 19gb text for pre-training,0.5657927393913269
translation,209,10,results,models,has,our method,models has our method,0.5946323275566101
translation,209,10,results,large-scale data ( ? 160gb ),has,our method,large-scale data ( ? 160gb ) has our method,0.5533718466758728
translation,209,10,results,results,Compared to,models,results Compared to models,0.6620222926139832
translation,209,31,results,model,finetune it on,supervised summarization datasets,model finetune it on supervised summarization datasets,0.6930857300758362
translation,209,31,results,supervised summarization datasets,"i.e. ,",cnn / dailymail and new york times,"supervised summarization datasets i.e. , cnn / dailymail and new york times",0.5437761545181274
translation,209,31,results,results,pre-training,model,results pre-training model,0.6919490694999695
translation,209,35,results,models,pre-trained with,much more data ( ? 160gb ),models pre-trained with much more data ( ? 160gb ),0.755157470703125
translation,209,35,results,models,achieve,comparable or even higher rouge scores,models achieve comparable or even higher rouge scores,0.5888900756835938
translation,209,35,results,results,Compared to,models,results Compared to models,0.6620222926139832
translation,209,217,results,our method,leads to,significant performance gains,our method leads to significant performance gains,0.6465311646461487
translation,209,217,results,significant performance gains,has,with p < 0.05 ),significant performance gains has with p < 0.05 ),0.5665286183357239
translation,209,220,results,all three objectives,when employing,unlabeled documents,all three objectives when employing unlabeled documents,0.627142608165741
translation,209,220,results,unlabeled documents,of,training splits,unlabeled documents of training splits,0.530399739742279
translation,209,220,results,training splits,of,cnndm or nyt,training splits of cnndm or nyt,0.5943241119384766
translation,209,220,results,outperforms,has,all three objectives,outperforms has all three objectives,0.5596572160720825
translation,209,221,results,"more data ( i.e. , giag - cm )",for,pre-training,"more data ( i.e. , giag - cm ) for pre-training",0.5773902535438538
translation,209,221,results,sr,consistently achieves,highest rouge - 2,sr consistently achieves highest rouge - 2,0.7301324009895325
translation,209,221,results,highest rouge - 2,on,cnndm and nyt,highest rouge - 2 on cnndm and nyt,0.5756167769432068
translation,209,221,results,"more data ( i.e. , giag - cm )",has,sr,"more data ( i.e. , giag - cm ) has sr",0.5181577205657959
translation,209,232,results,t5 and pegasus ( hugenews ),achieve,significantly higher rouge - 2 scores,t5 and pegasus ( hugenews ) achieve significantly higher rouge - 2 scores,0.6450265645980835
translation,209,232,results,significantly higher rouge - 2 scores,than,our model,significantly higher rouge - 2 scores than our model,0.5874431729316711
translation,209,232,results,results,has,t5 and pegasus ( hugenews ),results has t5 and pegasus ( hugenews ),0.5787068009376526
translation,210,134,baselines,neusum,uses,seq2seq model,neusum uses seq2seq model,0.6163091063499451
translation,210,134,baselines,seq2seq model,to predict,sequence of sentences,seq2seq model to predict sequence of sentences,0.7225725054740906
translation,210,134,baselines,baselines,has,neusum,baselines has neusum,0.6260150074958801
translation,210,137,baselines,abstractive models,including,pointgen - cov,abstractive models including pointgen - cov,0.7106415629386902
translation,210,137,baselines,abstractive models,including,"fars ( chen and bansal , 2018 )","abstractive models including fars ( chen and bansal , 2018 )",0.6824877858161926
translation,210,137,baselines,abstractive models,including,cbdec,abstractive models including cbdec,0.7155249118804932
translation,210,137,baselines,cbdec,has,"jiang and bansal , 2018 )","cbdec has jiang and bansal , 2018 )",0.6073138117790222
translation,210,138,baselines,joint model,with,pipeline model,joint model with pipeline model,0.647712230682373
translation,210,138,baselines,pipeline model,with,off-the-shelf compression module,pipeline model with off-the-shelf compression module,0.6131075620651245
translation,210,138,baselines,baselines,compare,joint model,baselines compare joint model,0.7107105851173401
translation,210,139,baselines,deletion - based bilstm model,for,sentence compression,deletion - based bilstm model for sentence compression,0.5778671503067017
translation,210,170,baselines,extractdropout,randomly drops,words,extractdropout randomly drops words,0.7891101241111755
translation,210,170,baselines,words,in,sentence,words in sentence,0.5547311902046204
translation,210,170,baselines,words,to match,compression ratio,words to match compression ratio,0.6249703764915466
translation,210,5,model,neural model,for,single -document summarization,neural model for single -document summarization,0.5607971549034119
translation,210,5,model,single -document summarization,based on,joint extraction,single -document summarization based on joint extraction,0.6026636958122253
translation,210,5,model,single -document summarization,based on,syntactic compression,single -document summarization based on syntactic compression,0.6003783941268921
translation,210,5,model,model,present,neural model,model present neural model,0.667643129825592
translation,210,6,model,sentences,from,document,sentences from document,0.5663099884986877
translation,210,6,model,possible compressions,based on,constituency parses,possible compressions based on constituency parses,0.6737326979637146
translation,210,6,model,compressions,with,neural model,compressions with neural model,0.6398528814315796
translation,210,6,model,neural model,to produce,final summary,neural model to produce final summary,0.6826896667480469
translation,210,6,model,model,chooses,sentences,model chooses sentences,0.7481139302253723
translation,210,6,model,model,scores,compressions,model scores compressions,0.7363216876983643
translation,210,7,model,learning,construct,oracle extractive - compressive summaries,learning construct oracle extractive - compressive summaries,0.6903145909309387
translation,210,7,model,learning,learn,both of our components,learning learn both of our components,0.6571958661079407
translation,210,7,model,oracle extractive - compressive summaries,learn,both of our components,oracle extractive - compressive summaries learn both of our components,0.6368698477745056
translation,210,7,model,jointly,with,supervision,jointly with supervision,0.5758785605430603
translation,210,7,model,both of our components,has,jointly,both of our components has jointly,0.587664783000946
translation,210,7,model,model,For,learning,model For learning,0.6567170023918152
translation,210,13,model,high performance,of,neural extractive systems,high performance of neural extractive systems,0.6043444275856018
translation,210,13,model,additional flexibility,from,compression,additional flexibility from compression,0.5470361709594727
translation,210,13,model,interpretability,given by,discrete compression options,interpretability given by discrete compression options,0.6536816954612732
translation,210,13,model,model,combine,high performance,model combine high performance,0.7473326325416565
translation,210,14,model,model,first encodes,source document,model first encodes source document,0.7371218800544739
translation,210,15,model,set of compression options,derived from,syntactic constituency parses,set of compression options derived from syntactic constituency parses,0.6185615658760071
translation,210,15,model,set of compression options,represent,expanded set of discrete options,set of compression options represent expanded set of discrete options,0.6063170433044434
translation,210,15,model,selected,to preserve,meaning and grammaticality,selected to preserve meaning and grammaticality,0.7073042392730713
translation,210,15,model,sentence,has,set of compression options,sentence has set of compression options,0.5968179702758789
translation,210,15,model,model,has,sentence,model has sentence,0.6082519292831421
translation,210,19,model,our model 's training objective,combines,extractive and compressive components,our model 's training objective combines extractive and compressive components,0.7156352400779724
translation,210,19,model,our model 's training objective,learns them,jointly,our model 's training objective learns them jointly,0.6314789652824402
translation,210,19,model,model,has,our model 's training objective,model has our model 's training objective,0.5304916501045227
translation,210,22,results,our model,achieves,largest improvement,our model achieves largest improvement,0.6368074417114258
translation,210,22,results,largest improvement,on,cnn,largest improvement on cnn,0.5482620596885681
translation,210,22,results,largest improvement,due to,more compressed nature of cnn summaries,largest improvement due to more compressed nature of cnn summaries,0.6926177740097046
translation,210,22,results,+ 2.4 rouge - f 1,over,our extractive baseline,+ 2.4 rouge - f 1 over our extractive baseline,0.6054495573043823
translation,210,22,results,our model,has,matches or exceeds,our model has matches or exceeds,0.5824629068374634
translation,210,22,results,matches or exceeds,has,state - of - the - art,matches or exceeds has state - of - the - art,0.574615478515625
translation,210,22,results,cnn,has,+ 2.4 rouge - f 1,cnn has + 2.4 rouge - f 1,0.6051042079925537
translation,210,22,results,results,has,our model,results has our model,0.5871725678443909
translation,210,145,results,our model,achieves,substantially higher performance,our model achieves substantially higher performance,0.6847285032272339
translation,210,145,results,substantially higher performance,than,all baselines and past systems,substantially higher performance than all baselines and past systems,0.5708004236221313
translation,210,145,results,all baselines and past systems,has,+ 2 rouge f1,all baselines and past systems has + 2 rouge f1,0.5804367065429688
translation,210,145,results,results,has,our model,results has our model,0.5871725678443909
translation,210,146,results,compression,is,substantially useful,compression is substantially useful,0.5772607922554016
translation,210,147,results,somewhat effective,in,isolation,somewhat effective in isolation,0.5419386625289917
translation,210,147,results,results,has,compression,results has compression,0.5383331179618835
translation,210,148,results,compression,in,isolation,compression in isolation,0.6081253290176392
translation,210,148,results,compression,gives,less benefit,compression gives less benefit,0.6332991123199463
translation,210,148,results,less benefit,on top of,lead,less benefit on top of lead,0.6535086035728455
translation,210,148,results,less benefit,combined with,extractive model ( jecs ),less benefit combined with extractive model ( jecs ),0.7452853918075562
translation,210,148,results,extractive model ( jecs ),in,joint framework,extractive model ( jecs ) in joint framework,0.5362952351570129
translation,210,148,results,results,has,compression,results has compression,0.5383331179618835
translation,210,149,results,our model,beats,pipeline model extlstmdel,our model beats pipeline model extlstmdel,0.7100099921226501
translation,210,149,results,results,has,our model,results has our model,0.5871725678443909
translation,210,153,results,our models,yield,strong performance,our models yield strong performance,0.7207310795783997
translation,210,153,results,strong performance,compared to,baselines,strong performance compared to baselines,0.7034717798233032
translation,210,153,results,results,has,our models,results has our models,0.5733726620674133
translation,210,154,results,extraction model,achieves,comparable results,extraction model achieves comparable results,0.6652317047119141
translation,210,154,results,comparable results,to,past successful extractive approaches,comparable results to past successful extractive approaches,0.5478807687759399
translation,210,154,results,past successful extractive approaches,on,cnndm,past successful extractive approaches on cnndm,0.541049599647522
translation,210,154,results,results,has,extraction model,results has extraction model,0.5839176177978516
translation,210,157,results,compressive approach,has,substantially outperforms,compressive approach has substantially outperforms,0.6067584753036499
translation,210,157,results,substantially outperforms,has,compression - augmented latsum model,substantially outperforms has compression - augmented latsum model,0.5947092175483704
translation,210,157,results,results,note,compressive approach,results note compressive approach,0.6075684428215027
translation,210,159,results,nyt,see again that,inclusion of compression,nyt see again that inclusion of compression,0.6938628554344177
translation,210,159,results,inclusion of compression,leads to,improvements,inclusion of compression leads to improvements,0.7136496901512146
translation,210,159,results,improvements,in,both the lead setting,improvements in both the lead setting,0.5832720398902893
translation,210,159,results,improvements,in,our full jecs model,improvements in our full jecs model,0.5345475673675537
translation,210,159,results,improvements,for,our full jecs model,improvements for our full jecs model,0.5988861918449402
translation,210,159,results,results,On,nyt,results On nyt,0.5398449301719666
translation,210,172,results,turkers,give,roughly equal preference,turkers give roughly equal preference,0.6263436675071716
translation,210,172,results,turkers,give,extlstmdel model,turkers give extlstmdel model,0.613853394985199
translation,210,172,results,roughly equal preference,to,our model,roughly equal preference to our model,0.5472314357757568
translation,210,172,results,extlstmdel model,learned from,supervised compression data,extlstmdel model learned from supervised compression data,0.7008360028266907
translation,210,172,results,results,has,turkers,results has turkers,0.518525242805481
translation,210,173,results,jecs model,achieves,substantially higher rouge score,jecs model achieves substantially higher rouge score,0.6678588390350342
translation,210,173,results,substantially higher rouge score,represents,more effective compression approach,substantially higher rouge score represents more effective compression approach,0.6777174472808838
translation,210,173,results,results,has,jecs model,results has jecs model,0.5299664735794067
translation,211,55,hyperparameters,note text,as,features,note text as features,0.5243038535118103
translation,211,55,hyperparameters,bag of n-grams,for,svm and the crf,bag of n-grams for svm and the crf,0.6163092851638794
translation,211,55,hyperparameters,randomly initialized word embeddings,updated during,training,randomly initialized word embeddings updated during training,0.5856567025184631
translation,211,55,hyperparameters,randomly initialized word embeddings,for,cnn - rand,randomly initialized word embeddings for cnn - rand,0.5380594730377197
translation,211,55,hyperparameters,features,has,bag of n-grams,features has bag of n-grams,0.5549769997596741
translation,211,5,model,clinical note processing pipeline,extends beyond,basic medical natural language processing ( nlp ),clinical note processing pipeline extends beyond basic medical natural language processing ( nlp ),0.7165332436561584
translation,211,5,model,basic medical natural language processing ( nlp ),with,concept recognition and relation detection,basic medical natural language processing ( nlp ) with concept recognition and relation detection,0.6040732264518738
translation,211,5,model,structured data,associated with,sentencelevel clinical aspects,structured data associated with sentencelevel clinical aspects,0.6314706206321716
translation,211,5,model,structured data,associated with,structures of the clinical notes,structured data associated with structures of the clinical notes,0.6305646896362305
translation,211,20,model,individual components,trained on,separate labeled datasets,individual components trained on separate labeled datasets,0.7191585302352905
translation,211,20,model,separate labeled datasets,target,lower level clinical nlp,separate labeled datasets target lower level clinical nlp,0.745669960975647
translation,211,20,model,lower level clinical nlp,chain,individual components together,lower level clinical nlp chain individual components together,0.7624046206474304
translation,211,20,model,individual components together,into,pipeline,individual components together into pipeline,0.6834092140197754
translation,211,20,model,model,leverage,individual components,model leverage individual components,0.7439922094345093
translation,211,147,model,clinical note processing pipeline,includes,basic nlp processing layer,clinical note processing pipeline includes basic nlp processing layer,0.6497601270675659
translation,211,147,model,clinical note processing pipeline,includes,additional ehr - specific components,clinical note processing pipeline includes additional ehr - specific components,0.6524181962013245
translation,211,86,results,document level information ( crf & cnn ),is,important,document level information ( crf & cnn ) is important,0.555608868598938
translation,211,86,results,cnn,performing,reasonably well,cnn performing reasonably well,0.5862362384796143
translation,211,86,results,reasonably well,even with,limited labeled data,reasonably well even with limited labeled data,0.6952410340309143
translation,211,86,results,results,considering,document level information ( crf & cnn ),results considering document level information ( crf & cnn ),0.654339611530304
translation,211,148,results,incremental improvement,in,overall system performance,incremental improvement in overall system performance,0.5155786871910095
translation,211,148,results,incremental improvement,from,f-scores,incremental improvement from f-scores,0.521207869052887
translation,211,148,results,f-scores,of,0.555 and 0.581,f-scores of 0.555 and 0.581,0.5847479104995728
translation,211,148,results,0.555 and 0.581,for,hypertension and diabetes mellitus,0.555 and 0.581 for hypertension and diabetes mellitus,0.5506782531738281
translation,211,148,results,0.555 and 0.581,when using,only unigrams,0.555 and 0.581 when using only unigrams,0.6997553110122681
translation,211,148,results,0.657 and 0.679,all components are included in,pipeline,0.657 and 0.679 all components are included in pipeline,0.7198812961578369
translation,211,148,results,results,show,incremental improvement,results show incremental improvement,0.6664872765541077
translation,212,2,experiments,mlsum,has,multilingual summarization corpus,mlsum has multilingual summarization corpus,0.5157606601715088
translation,212,26,experiments,automatic summarization,proposing,largescale,automatic summarization proposing largescale,0.6556270122528076
translation,212,26,experiments,largescale,has,summarization ( mlsum ) dataset,largescale has summarization ( mlsum ) dataset,0.5530325174331665
translation,213,142,experimental-setup,embedding size,set at,128,embedding size set at 128,0.5849176049232483
translation,213,142,experimental-setup,experimental setup,has,embedding size,experimental setup has embedding size,0.5223031640052795
translation,213,143,experimental-setup,adagrad,with,initial learning rate,adagrad with initial learning rate,0.5808988213539124
translation,213,143,experimental-setup,adagrad,with,initial accumulator value,adagrad with initial accumulator value,0.5819831490516663
translation,213,143,experimental-setup,initial learning rate,of,0.15,initial learning rate of 0.15,0.5833383202552795
translation,213,143,experimental-setup,initial accumulator value,of,0.1,initial accumulator value of 0.1,0.6011174917221069
translation,213,143,experimental-setup,experimental setup,trained with,adagrad,experimental setup trained with adagrad,0.7153127789497375
translation,213,144,experimental-setup,training,until,convergence,training until convergence,0.7276614308357239
translation,213,144,experimental-setup,training,when,validation perplexity,training when validation perplexity,0.5934478044509888
translation,213,144,experimental-setup,does not decrease,after,epoch,does not decrease after epoch,0.6947311758995056
translation,213,144,experimental-setup,validation perplexity,has,does not decrease,validation perplexity has does not decrease,0.584938108921051
translation,213,144,experimental-setup,validation perplexity,has,learning rate,validation perplexity has learning rate,0.4938289225101471
translation,213,144,experimental-setup,experimental setup,continue,training,experimental setup continue training,0.6315174102783203
translation,213,145,experimental-setup,gradient -clipping,with,maximum norm,gradient -clipping with maximum norm,0.6023236513137817
translation,213,145,experimental-setup,maximum norm,of,2,maximum norm of 2,0.6142091155052185
translation,213,145,experimental-setup,experimental setup,use,gradient -clipping,experimental setup use gradient -clipping,0.5803185701370239
translation,213,141,model,one- layer bi-lstm,with,512 hidden states,one- layer bi-lstm with 512 hidden states,0.6108137369155884
translation,213,141,model,one- layer bi-lstm,with,512 hidden states,one- layer bi-lstm with 512 hidden states,0.6108137369155884
translation,213,141,model,512 hidden states,for,one-layer decoder,512 hidden states for one-layer decoder,0.5953065752983093
translation,213,21,results,human judgments,from,carefully chosen measures,human judgments from carefully chosen measures,0.546283483505249
translation,213,21,results,human judgments,allows one to,successfully train,human judgments allows one to successfully train,0.7301725149154663
translation,213,21,results,improving,over,state - of- theart,improving over state - of- theart,0.6269974708557129
translation,213,21,results,state - of- theart,in terms of,rouge and human assessments,state - of- theart in terms of rouge and human assessments,0.733954668045044
translation,213,21,results,successfully train,has,reinforcement learningbased model,successfully train has reinforcement learningbased model,0.5165565609931946
translation,213,21,results,results,fitting,human judgments,results fitting human judgments,0.670527994632721
translation,213,83,results,language model metric,captures,readability,language model metric captures readability,0.7102321982383728
translation,213,83,results,language model metric,captures,falling short,language model metric captures falling short,0.7029677033424377
translation,213,83,results,readability,better than,rouge,readability better than rouge,0.7522137761116028
translation,213,83,results,falling short,on,relevance,falling short on relevance,0.6126529574394226
translation,213,84,results,proposed qa - based metrics,indicate,potential benefits,proposed qa - based metrics indicate potential benefits,0.5933851003646851
translation,213,84,results,potential benefits,especially under,unsupervised setting,potential benefits especially under unsupervised setting,0.6878781318664551
translation,213,84,results,unsupervised setting,with,qa conf and qa f score,unsupervised setting with qa conf and qa f score,0.6523236632347107
translation,213,84,results,qa conf and qa f score,capturing,readability and relevance,qa conf and qa f score capturing readability and relevance,0.7319751977920532
translation,213,84,results,better,than,all the others reported metrics,better than all the others reported metrics,0.5739855766296387
translation,213,84,results,all the others reported metrics,including,rouge,all the others reported metrics including rouge,0.6799890398979187
translation,213,84,results,readability and relevance,has,better,readability and relevance has better,0.5800064206123352
translation,213,156,results,in -domain vs out-of-domain,experiment with,qa conf and qa f score metrics,in -domain vs out-of-domain experiment with qa conf and qa f score metrics,0.6949393153190613
translation,213,156,results,qa conf and qa f score metrics,in,unsupervised fashion,qa conf and qa f score metrics in unsupervised fashion,0.5440443754196167
translation,213,156,results,results,has,in -domain vs out-of-domain,results has in -domain vs out-of-domain,0.5376707315444946
translation,213,167,results,reinforcing,on,rouge -l,reinforcing on rouge -l,0.6859687566757202
translation,213,167,results,reinforcing,on,rouge,reinforcing on rouge,0.611592710018158
translation,213,167,results,reinforcing,allows to obtain,significant improvements,reinforcing allows to obtain significant improvements,0.6714569926261902
translation,213,167,results,rouge -l,allows to obtain,significant improvements,rouge -l allows to obtain significant improvements,0.6664723753929138
translation,213,167,results,significant improvements,over,state - of - the -art,significant improvements over state - of - the -art,0.631626546382904
translation,213,167,results,significant improvements,in terms of,rouge,significant improvements in terms of rouge,0.7047067284584045
translation,213,167,results,significant improvements,at,cost,significant improvements at cost,0.5886659026145935
translation,213,167,results,cost,of,lower qabased metrics,cost of lower qabased metrics,0.527973473072052
translation,213,167,results,results,observe,reinforcing,results observe reinforcing,0.4771649241447449
translation,213,168,results,consistently,has,all its components,consistently has all its components,0.620084285736084
translation,213,168,results,all its components,has,rouge -l,all its components has rouge -l,0.6518374085426331
translation,213,168,results,all its components,has,qa conf,all its components has qa conf,0.6287564039230347
translation,213,176,results,proposed metric,includes,qa based metrics,proposed metric includes qa based metrics,0.6222564578056335
translation,213,176,results,proposed metric,leads to,comparable performance,proposed metric leads to comparable performance,0.6452134847640991
translation,213,176,results,comparable performance,in terms of,rouge w.r.t.,comparable performance in terms of rouge w.r.t.,0.7286633849143982
translation,213,176,results,significant improvement,in terms of,relevance,significant improvement in terms of relevance,0.6654322147369385
translation,213,177,results,improvement,for,our model,improvement for our model,0.6427575349807739
translation,213,177,results,improvement,reinforced through,learned metric,improvement reinforced through learned metric,0.6724551916122437
translation,213,177,results,learned metric,compared to,one equally weighted,learned metric compared to one equally weighted,0.6430525779724121
translation,213,177,results,results,observe,improvement,results observe improvement,0.6390148997306824
translation,213,180,results,proposed qabased metrics,not depend on,reference summaries,proposed qabased metrics not depend on reference summaries,0.6760603785514832
translation,213,180,results,reference summaries,allows to leverage,raw text data,reference summaries allows to leverage raw text data,0.6522965431213379
translation,213,180,results,fine-tuning ( without supervision ),on,documents,fine-tuning ( without supervision ) on documents,0.5488747358322144
translation,213,180,results,fine-tuning ( without supervision ),is,beneficial,fine-tuning ( without supervision ) is beneficial,0.5694564580917358
translation,213,180,results,documents,to be,summarized,documents to be summarized,0.6332199573516846
translation,213,180,results,documents,is,beneficial,documents is beneficial,0.6188544631004333
translation,213,180,results,summarized,is,beneficial,summarized is beneficial,0.6341167092323303
translation,213,180,results,results,show,proposed qabased metrics,results show proposed qabased metrics,0.6543781757354736
translation,213,180,results,results,using,proposed qabased metrics,results using proposed qabased metrics,0.6753934025764465
translation,213,181,results,results,obtained,models,results obtained models,0.6859749555587769
translation,213,181,results,results,by,models,results by models,0.5729843378067017
translation,213,181,results,models,reinforced on,qa learned and qa equally,models reinforced on qa learned and qa equally,0.6275905966758728
translation,213,181,results,models,obtain,very similar scores,models obtain very similar scores,0.5730592608451843
translation,213,181,results,results,applying,learned coefficients,results applying learned coefficients,0.7313286066055298
translation,213,184,results,qa learned,performs,significantly better,qa learned performs significantly better,0.6577259302139282
translation,213,184,results,significantly better,in term of,readability,significantly better in term of readability,0.6075388789176941
translation,213,184,results,readability,than,qa learned + cnn - dm ( val ),readability than qa learned + cnn - dm ( val ),0.5577839016914368
translation,213,195,results,partially self-supervised training,has,of summarization models,partially self-supervised training has of summarization models,0.5351702570915222
translation,215,102,ablation-analysis,summary,during,fusion,summary during fusion,0.681769073009491
translation,215,18,experiments,constrained summarization task,where,abstract,constrained summarization task where abstract,0.5823932886123657
translation,215,90,hyperparameters,hyperparameter,for weighing,per-target - word loss,hyperparameter for weighing per-target - word loss,0.653828501701355
translation,215,90,hyperparameters,per-target - word loss,set to,0.2,per-target - word loss set to 0.2,0.6335800886154175
translation,215,90,hyperparameters,threshold value,is,0.15,threshold value is 0.15,0.5768905282020569
translation,215,90,hyperparameters,hyperparameters,for weighing,per-target - word loss,hyperparameters for weighing per-target - word loss,0.6376199722290039
translation,215,90,hyperparameters,hyperparameters,has,hyperparameter,hyperparameters has hyperparameter,0.48939356207847595
translation,215,45,model,tokens,fed to,series of transformer block layers,tokens fed to series of transformer block layers,0.7161146998405457
translation,215,45,model,series of transformer block layers,consisting of,multi-head self-attention modules,series of transformer block layers consisting of multi-head self-attention modules,0.7606980800628662
translation,215,45,model,model,has,tokens,model has tokens,0.6422943472862244
translation,215,62,model,sentence singleton or pair,to,encoder,sentence singleton or pair to encoder,0.5471292734146118
translation,215,62,model,encoder,along with,highlights,encoder along with highlights,0.7098438739776611
translation,215,62,model,highlights,derived by,fine- grained content selector,highlights derived by fine- grained content selector,0.6975730061531067
translation,215,62,model,fine- grained content selector,has,latter,fine- grained content selector has latter,0.5603040456771851
translation,215,62,model,model,feed,sentence singleton or pair,model feed sentence singleton or pair,0.7058150172233582
translation,215,7,results,empirical results,performance of,cascaded pipeline,empirical results performance of cascaded pipeline,0.768961489200592
translation,215,7,results,cascaded pipeline,separately identifies,important content pieces,cascaded pipeline separately identifies important content pieces,0.7682561874389648
translation,215,7,results,stitches them together,into,coherent text,stitches them together into coherent text,0.6163738965988159
translation,215,7,results,coherent text,is,outranks,coherent text is outranks,0.5777187347412109
translation,215,7,results,coherent text,comparable to,outranks,coherent text comparable to outranks,0.6195777654647827
translation,215,84,results,our cascade approach,performs,comparable,our cascade approach performs comparable,0.6542279124259949
translation,215,84,results,comparable,to,strong extractive and abstractive baselines,comparable to strong extractive and abstractive baselines,0.5453612208366394
translation,215,84,results,oracle models,using,ground -truth sentences and segment highlights,oracle models using ground -truth sentences and segment highlights,0.5998000502586365
translation,215,84,results,ground -truth sentences and segment highlights,perform,best,ground -truth sentences and segment highlights perform best,0.5932256579399109
translation,215,84,results,results,has,our cascade approach,results has our cascade approach,0.5881771445274353
translation,215,94,results,performance,of,our cascade approaches,performance of our cascade approaches,0.5685864090919495
translation,215,94,results,performance,comparable to,outranks,performance comparable to outranks,0.6536582112312317
translation,215,94,results,our cascade approaches,comparable to,outranks,our cascade approaches comparable to outranks,0.7028579711914062
translation,215,94,results,outranks,has,number of extractive and abstractive baselines,outranks has number of extractive and abstractive baselines,0.5696609616279602
translation,215,94,results,results,has,performance,results has performance,0.5972660779953003
translation,215,98,results,only a moderate impact,on,rouge scores,only a moderate impact on rouge scores,0.5626807808876038
translation,215,98,results,fusion process,reorder,text segments,fusion process reorder text segments,0.7179608345031738
translation,215,98,results,text segments,to create,true and grammatical sentences,text segments to create true and grammatical sentences,0.6756475567817688
translation,215,98,results,fusion model,has,only a moderate impact,fusion model has only a moderate impact,0.5923006534576416
translation,215,98,results,results,addition of,fusion model,results addition of fusion model,0.6382101774215698
translation,215,100,results,ground - truth sentences,as,input,ground - truth sentences as input,0.5080145597457886
translation,215,100,results,our cascade models,achieve,?10 points of improvement,our cascade models achieve ?10 points of improvement,0.6333515644073486
translation,215,100,results,?10 points of improvement,in,all rouge metrics,?10 points of improvement in all rouge metrics,0.5135629773139954
translation,215,100,results,ground - truth sentences,has,our cascade models,ground - truth sentences has our cascade models,0.5268805027008057
translation,215,100,results,input,has,our cascade models,input has our cascade models,0.5806046724319458
translation,215,100,results,results,given,ground - truth sentences,results given ground - truth sentences,0.6182169914245605
translation,215,101,results,models,given,ground - truth highlights,models given ground - truth highlights,0.7142121195793152
translation,215,101,results,ground - truth highlights,achieve,additional 20 points of improvement,ground - truth highlights achieve additional 20 points of improvement,0.6347390413284302
translation,215,112,results,probability thresholding,performs,best,probability thresholding performs best,0.6610726714134216
translation,215,112,results,three methods,has,probability thresholding,three methods has probability thresholding,0.5709351897239685
translation,215,112,results,results,Among,three methods,results Among three methods,0.5625443458557129
translation,216,81,model,additional positional embeddings,to,hidden state,additional positional embeddings to hidden state,0.552198052406311
translation,216,81,model,hidden state,of,sentence - level rnn,hidden state of sentence - level rnn,0.5478228330612183
translation,216,81,model,positional importance,of,sentences,positional importance of sentences,0.5933995842933655
translation,216,81,model,sentences,in,document,sentences in document,0.5417858362197876
translation,216,81,model,model,concatenate,additional positional embeddings,model concatenate additional positional embeddings,0.7273751497268677
translation,217,5,baselines,supervised summarizer,based on,genetic algorithm ( ga ),supervised summarizer based on genetic algorithm ( ga ),0.6680917143821716
translation,217,5,baselines,supervised summarizer,based on,linear programming ( lp ),supervised summarizer based on linear programming ( lp ),0.6447782516479492
translation,217,5,baselines,supervised summarizer,ranks,document sentences,supervised summarizer ranks document sentences,0.6966418027877808
translation,217,5,baselines,supervised summarizer,extracts,top-ranking sentences,supervised summarizer extracts top-ranking sentences,0.606636643409729
translation,217,5,baselines,genetic algorithm ( ga ),ranks,document sentences,genetic algorithm ( ga ) ranks document sentences,0.7345777153968811
translation,217,5,baselines,top-ranking sentences,into,summary,top-ranking sentences into summary,0.5588551759719849
translation,217,5,baselines,unsupervised summarizer,selects,best extract of document sentences,unsupervised summarizer selects best extract of document sentences,0.6639615893363953
translation,217,5,baselines,unsupervised extension of poly,compiles,document summary,unsupervised extension of poly compiles document summary,0.6507658958435059
translation,217,5,baselines,document summary,from,compressed sentences,document summary from compressed sentences,0.5500960350036621
translation,217,5,baselines,muse,has,supervised summarizer,muse has supervised summarizer,0.5883857607841492
translation,217,5,baselines,poly,has,unsupervised summarizer,poly has unsupervised summarizer,0.6010501384735107
translation,217,5,baselines,wecom,has,unsupervised extension of poly,wecom has unsupervised extension of poly,0.6375292539596558
translation,217,18,baselines,museec,implements,three single-document summarization algorithms,museec implements three single-document summarization algorithms,0.658228874206543
translation,217,18,baselines,museec,has,multilingual text summarization platform,museec has multilingual text summarization platform,0.5563821196556091
translation,217,18,baselines,baselines,present,museec,baselines present museec,0.7017844915390015
translation,217,129,baselines,several variations,of,three single-document summarization methods,several variations of three single-document summarization methods,0.544760525226593
translation,217,49,experiments,multilingual sentence extractor ( muse ),implements,supervised learning approach,multilingual sentence extractor ( muse ) implements supervised learning approach,0.682822585105896
translation,217,49,experiments,supervised learning approach,to,extractive summarization,supervised learning approach to extractive summarization,0.5383456945419312
translation,217,49,experiments,extractive summarization,where,best set of weights,extractive summarization where best set of weights,0.6147063374519348
translation,217,17,model,wecom,extends,poly,wecom extends poly,0.7347291111946106
translation,217,17,model,poly,by utilizing,choice of poly 's objective functions,poly by utilizing choice of poly 's objective functions,0.647876501083374
translation,217,17,model,choice of poly 's objective functions,for,term-weighting model,choice of poly 's objective functions for term-weighting model,0.6171216368675232
translation,217,17,model,model,has,wecom,model has wecom,0.6512401103973389
translation,217,75,model,weighted compression ( wecom ),shorten,sentences,weighted compression ( wecom ) shorten sentences,0.6407526731491089
translation,217,75,model,sentences,by iteratively removing,elementary discourse units ( edus ),sentences by iteratively removing elementary discourse units ( edus ),0.7618315815925598
translation,217,75,model,elementary discourse units ( edus ),defined as,grammatically independent parts,elementary discourse units ( edus ) defined as grammatically independent parts,0.5806614756584167
translation,217,75,model,model,has,weighted compression ( wecom ),model has weighted compression ( wecom ),0.6059322357177734
translation,217,128,model,model,present,museec,model present museec,0.719354510307312
translation,217,100,results,all other participating systems,except for,ccs,all other participating systems except for ccs,0.719448983669281
translation,217,100,results,ccs,in,hebrew,ccs in hebrew,0.5216378569602966
translation,217,100,results,muse,has,outperformed,muse has outperformed,0.6536640524864197
translation,217,100,results,outperformed,has,all other participating systems,outperformed has all other participating systems,0.6196191906929016
translation,217,100,results,results,has,muse,results has muse,0.40223199129104614
translation,217,110,results,compression,has,significantly improves,compression has significantly improves,0.623389720916748
translation,217,110,results,significantly improves,has,quality of generated summaries,significantly improves has quality of generated summaries,0.5781780481338501
translation,217,111,results,poly and wecom summarizers,on,duc 2002 dataset,poly and wecom summarizers on duc 2002 dataset,0.4904869794845581
translation,218,94,baselines,lead - 3,is,strong extractive baseline,lead - 3 is strong extractive baseline,0.5891339182853699
translation,218,94,baselines,strong extractive baseline,uses,first 3 sentences,strong extractive baseline uses first 3 sentences,0.5755047798156738
translation,218,94,baselines,first 3 sentences,of the document,summary,first 3 sentences of the document summary,0.7273425459861755
translation,218,94,baselines,first 3 sentences,as,summary,first 3 sentences as summary,0.535348653793335
translation,218,94,baselines,baselines,has,lead - 3,baselines has lead - 3,0.6145645976066589
translation,218,95,baselines,abstractive models,include,seq2seq - baseline,abstractive models include seq2seq - baseline,0.5852637887001038
translation,218,95,baselines,abstractive models,include,abs - temp-attn seq2seq architecture,abstractive models include abs - temp-attn seq2seq architecture,0.5745100975036621
translation,218,95,baselines,abstractive models,incorporates with,copy mechanism,abstractive models incorporates with copy mechanism,0.6159095168113708
translation,218,95,baselines,seq2seq - baseline,uses,basic seq2seq encoder-decoder structure,seq2seq - baseline uses basic seq2seq encoder-decoder structure,0.6024783253669739
translation,218,95,baselines,seq2seq - baseline,incorporates with,copy mechanism,seq2seq - baseline incorporates with copy mechanism,0.6966759562492371
translation,218,95,baselines,basic seq2seq encoder-decoder structure,with,attention mechanism,basic seq2seq encoder-decoder structure with attention mechanism,0.6105746626853943
translation,218,95,baselines,abs - temp-attn seq2seq architecture,to overcome,repetition problem,abs - temp-attn seq2seq architecture to overcome repetition problem,0.6850464344024658
translation,218,95,baselines,baselines,has,abstractive models,baselines has abstractive models,0.5310056209564209
translation,218,96,baselines,graph-attention,uses,graph- ranking based attention mechanism,graph-attention uses graph- ranking based attention mechanism,0.5936482548713684
translation,218,96,baselines,graph- ranking based attention mechanism,based on,hierarchical architecture,graph- ranking based attention mechanism based on hierarchical architecture,0.65113765001297
translation,218,96,baselines,hierarchical architecture,to identify,important source sentences,hierarchical architecture to identify important source sentences,0.6565104126930237
translation,218,97,baselines,"deep-reinforced ( paulus et al. , 2017 )",trains,seq2seq encoder-decoder model,"deep-reinforced ( paulus et al. , 2017 ) trains seq2seq encoder-decoder model",0.6951062083244324
translation,218,97,baselines,seq2seq encoder-decoder model,with,reinforcement learning techniques,seq2seq encoder-decoder model with reinforcement learning techniques,0.6076457500457764
translation,218,98,baselines,coverage,is,extension,coverage is extension,0.6142451167106628
translation,218,98,baselines,extension,of,seq2seq - baseline model,extension of seq2seq - baseline model,0.5685723423957825
translation,218,98,baselines,extension,by importing,coverage mechanism,extension by importing coverage mechanism,0.687453031539917
translation,218,98,baselines,seq2seq - baseline model,by importing,coverage mechanism,seq2seq - baseline model by importing coverage mechanism,0.7108661532402039
translation,218,98,baselines,coverage mechanism,to control,repetitions,coverage mechanism to control repetitions,0.7249947190284729
translation,218,98,baselines,baselines,has,coverage,baselines has coverage,0.590352475643158
translation,218,84,hyperparameters,word- level encoder and summary decoder,use,256 - dimensional hidden states,word- level encoder and summary decoder use 256 - dimensional hidden states,0.5838028192520142
translation,218,84,hyperparameters,sentence - level encoder and sentence selection network,use,512 - dimensional hidden states,sentence - level encoder and sentence selection network use 512 - dimensional hidden states,0.5830640196800232
translation,218,85,hyperparameters,) vector,for,initialization,) vector for initialization,0.6390432119369507
translation,218,85,hyperparameters,initialization,of,word embeddings,initialization of word embeddings,0.5096065998077393
translation,218,85,hyperparameters,pre-trained glove,has,) vector,pre-trained glove has ) vector,0.5497711300849915
translation,218,85,hyperparameters,hyperparameters,use,pre-trained glove,hyperparameters use pre-trained glove,0.6045145392417908
translation,218,86,hyperparameters,dimension,of,word embeddings,dimension of word embeddings,0.5582399964332581
translation,218,86,hyperparameters,word embeddings,is,100,word embeddings is 100,0.5938218832015991
translation,218,86,hyperparameters,hyperparameters,has,dimension,hyperparameters has dimension,0.514893651008606
translation,218,87,hyperparameters,vocabulary,of,50 k words,vocabulary of 50 k words,0.6213000416755676
translation,218,87,hyperparameters,vocabulary,for,both encoder and decoder,vocabulary for both encoder and decoder,0.6875026822090149
translation,218,87,hyperparameters,hyperparameters,use,vocabulary,hyperparameters use vocabulary,0.6648092865943909
translation,218,90,hyperparameters,beam sizes,for,word decoder and sentence decoder,beam sizes for word decoder and sentence decoder,0.6317825317382812
translation,218,90,hyperparameters,word decoder and sentence decoder,are,15 and 2,word decoder and sentence decoder are 15 and 2,0.5943868160247803
translation,218,5,model,basic neural encoding - decoding framework,with,information selection layer,basic neural encoding - decoding framework with information selection layer,0.5665006041526794
translation,218,5,model,information selection layer,to explicitly model and optimize,information selection process,information selection layer to explicitly model and optimize information selection process,0.7015260457992554
translation,218,5,model,information selection process,in,abstractive document summarization,information selection process in abstractive document summarization,0.44209566712379456
translation,218,5,model,model,extend,basic neural encoding - decoding framework,model extend basic neural encoding - decoding framework,0.6584852933883667
translation,218,6,model,information selection layer,consists of,two parts,information selection layer consists of two parts,0.690325140953064
translation,218,6,model,model,has,information selection layer,model has information selection layer,0.5369638204574585
translation,218,7,model,unnecessary information,in,original document,unnecessary information in original document,0.4661462903022766
translation,218,7,model,unnecessary information,first,globally filtered,unnecessary information first globally filtered,0.7472068667411804
translation,218,7,model,salient sentences,selected,locally,salient sentences selected locally,0.6476879119873047
translation,218,7,model,locally,while generating,each summary sentence sequentially,locally while generating each summary sentence sequentially,0.7337801456451416
translation,218,7,model,model,has,unnecessary information,model has unnecessary information,0.5747349262237549
translation,218,21,model,encoding - decoding framework,to model,information selection process explicitly,encoding - decoding framework to model information selection process explicitly,0.7365591526031494
translation,218,21,model,model,extend,encoding - decoding framework,model extend encoding - decoding framework,0.6859256625175476
translation,218,22,model,document summarization,as,three - phase task,document summarization as three - phase task,0.5162073969841003
translation,218,22,model,model,treat,document summarization,model treat document summarization,0.5485493540763855
translation,218,43,model,basic encoderdecoder framework,by adding,information selection layer,basic encoderdecoder framework by adding information selection layer,0.6780686378479004
translation,218,43,model,information selection layer,to model,information selection process explicitly,information selection layer to model information selection process explicitly,0.7302035093307495
translation,218,43,model,model,extend,basic encoderdecoder framework,model extend basic encoderdecoder framework,0.6588030457496643
translation,218,89,model,hierarchical beam search algorithm,with,reference mechanism,hierarchical beam search algorithm with reference mechanism,0.6136091351509094
translation,218,89,model,reference mechanism,to generate,multi-sentence summaries,reference mechanism to generate multi-sentence summaries,0.678808331489563
translation,218,89,model,model,To improve,information correctness,model To improve information correctness,0.6429331302642822
translation,218,89,model,model,use,hierarchical beam search algorithm,model use hierarchical beam search algorithm,0.6330413222312927
translation,218,144,model,global gated information filtering network,removes,unnecessary information,global gated information filtering network removes unnecessary information,0.6860834360122681
translation,218,144,model,global gated information filtering network,helps generate,more informative summary,global gated information filtering network helps generate more informative summary,0.719473123550415
translation,218,144,model,unnecessary information,from,original document,unnecessary information from original document,0.5347638726234436
translation,218,144,model,model,has,global gated information filtering network,model has global gated information filtering network,0.548446774482727
translation,218,145,model,distantly - supervised training,for,sentence selection decisions,distantly - supervised training for sentence selection decisions,0.5661475658416748
translation,218,145,model,sentence selection decisions,helps,model,sentence selection decisions helps model,0.6068407297134399
translation,218,145,model,model,to detect,important and relevant source sentences,model to detect important and relevant source sentences,0.6200312972068787
translation,218,145,model,important and relevant source sentences,for,each summary sentence,important and relevant source sentences for each summary sentence,0.591454803943634
translation,218,145,model,model,has,distantly - supervised training,model has distantly - supervised training,0.5482217073440552
translation,218,177,model,information selection process,in,document summarization,information selection process in document summarization,0.4697331488132477
translation,218,177,model,encoder-decoder framework,with,information selection layer,encoder-decoder framework with information selection layer,0.5881835222244263
translation,218,177,model,model,explicitly models,information selection process,model explicitly models information selection process,0.6951404213905334
translation,218,102,results,our method,has,significant improvement,our method has significant improvement,0.5297049283981323
translation,218,102,results,significant improvement,over,state - of - the - art neural abstractive baselines,significant improvement over state - of - the - art neural abstractive baselines,0.6757416725158691
translation,218,102,results,our method,has,significant improvement,our method has significant improvement,0.5297049283981323
translation,218,102,results,results,show,our method,results show our method,0.6411243081092834
translation,218,103,results,deep-reinforced model,achieves,best rouge -l performance,deep-reinforced model achieves best rouge -l performance,0.6594703793525696
translation,218,104,results,our model,achieves,significant better performance,our model achieves significant better performance,0.6944459676742554
translation,218,104,results,our model,achieves,comparable performance,our model achieves comparable performance,0.6824068427085876
translation,218,104,results,significant better performance,on,rouge - 1 and rouge - 2 metrics,significant better performance on rouge - 1 and rouge - 2 metrics,0.5278493165969849
translation,218,104,results,comparable performance,on,rouge -l metric,comparable performance on rouge -l metric,0.5170477628707886
translation,218,104,results,current state - of- the - art model coverage,has,our model,current state - of- the - art model coverage has our model,0.5530216693878174
translation,218,104,results,results,Comparing with,current state - of- the - art model coverage,results Comparing with current state - of- the - art model coverage,0.6598058938980103
translation,218,119,results,our model,has,consistently outperforms,our model has consistently outperforms,0.6076288819313049
translation,218,119,results,consistently outperforms,has,seq2seq - baseline model,consistently outperforms has seq2seq - baseline model,0.5628302097320557
translation,218,119,results,results,show,our model,results show our model,0.6888449192047119
translation,218,126,results,sentence - level modeling,of,document and summary,sentence - level modeling of document and summary,0.5625544190406799
translation,218,126,results,sentence - level modeling,makes,generated summaries,sentence - level modeling makes generated summaries,0.6031836867332458
translation,218,126,results,document and summary,in,our model,document and summary in our model,0.5615319609642029
translation,218,126,results,generated summaries,achieve,better inter-sentence coherence,generated summaries achieve better inter-sentence coherence,0.5704634785652161
translation,218,127,results,our model,able to generate,more informative and concise summaries,our model able to generate more informative and concise summaries,0.703495979309082
translation,218,127,results,more informative and concise summaries,shows,advantage,more informative and concise summaries shows advantage,0.6830061674118042
translation,218,127,results,advantage,of,abstractive methods,advantage of abstractive methods,0.5924050211906433
translation,218,127,results,strong extractive baseline lead - 3,has,our model,strong extractive baseline lead - 3 has our model,0.5775428414344788
translation,218,127,results,results,Compared with,strong extractive baseline lead - 3,results Compared with strong extractive baseline lead - 3,0.6789854168891907
translation,218,128,results,fluency scores,show,good ability,fluency scores show good ability,0.62890625
translation,218,128,results,good ability,of,our model,good ability of our model,0.5609300136566162
translation,218,128,results,good ability,to generate,fluent and grammatical sentences,good ability to generate fluent and grammatical sentences,0.5969564318656921
translation,218,128,results,our model,to generate,fluent and grammatical sentences,our model to generate fluent and grammatical sentences,0.6367262005805969
translation,218,128,results,results,has,fluency scores,results has fluency scores,0.4856947362422943
translation,218,129,results,human evaluation results,demonstrate,our model,human evaluation results demonstrate our model,0.585276186466217
translation,218,129,results,our model,able to generate,"more informative , concise and coherent summaries","our model able to generate more informative , concise and coherent summaries",0.7167963981628418
translation,218,129,results,"more informative , concise and coherent summaries",than,baselines,"more informative , concise and coherent summaries than baselines",0.5650852918624878
translation,218,129,results,results,has,human evaluation results,results has human evaluation results,0.5168871283531189
translation,218,132,results,coverage,learns to reduce,repetitions,coverage learns to reduce repetitions,0.7363008856773376
translation,218,132,results,coverage,fails to detect,all the salient information,coverage fails to detect all the salient information,0.7976487278938293
translation,218,132,results,results,has,coverage,results has coverage,0.5053455829620361
translation,218,134,results,different sets,of,source sentences,different sets of source sentences,0.5614319443702698
translation,218,143,results,our method,removing,each component,our method removing each component,0.700304388999939
translation,218,143,results,each component,of,our model,each component of our model,0.5908254981040955
translation,218,143,results,each component,leads to,sustained significant performance declining,each component leads to sustained significant performance declining,0.6684800386428833
translation,218,143,results,our method,has,much outperforms,our method has much outperforms,0.6259496808052063
translation,218,143,results,much outperforms,has,all the comparison systems,much outperforms has all the comparison systems,0.5931620001792908
translation,218,143,results,each component,has,one by one,each component has one by one,0.6171807646751404
translation,218,143,results,results,has,our method,results has our method,0.5589964985847473
translation,218,146,results,information selection process,has,significantly improves,information selection process has significantly improves,0.5808936953544617
translation,218,146,results,significantly improves,has,document summarization performance,significantly improves has document summarization performance,0.5419061779975891
translation,218,146,results,results,explicitly modeling,information selection process,results explicitly modeling information selection process,0.7241436243057251
translation,218,151,results,our simple extractive method ourextractive,has,significantly outperforms,our simple extractive method ourextractive has significantly outperforms,0.6133347749710083
translation,218,151,results,significantly outperforms,has,state - of - the - art neural extractive baselines,significantly outperforms has state - of - the - art neural extractive baselines,0.581300675868988
translation,218,151,results,results,show,our simple extractive method ourextractive,results show our simple extractive method ourextractive,0.6501323580741882
translation,218,152,results,significantly outperforms,which remove,different components,significantly outperforms which remove different components,0.728361964225769
translation,218,152,results,two comparison systems,which remove,different components,two comparison systems which remove different components,0.7634150981903076
translation,218,152,results,different components,of,our model one by one,different components of our model one by one,0.597540557384491
translation,218,152,results,ourextractive,has,significantly outperforms,ourextractive has significantly outperforms,0.6146572232246399
translation,218,152,results,significantly outperforms,has,two comparison systems,significantly outperforms has two comparison systems,0.5922303795814514
translation,218,152,results,results,has,ourextractive,results has ourextractive,0.5622110962867737
translation,218,157,results,our method,better at generating,long summary,our method better at generating long summary,0.7106409072875977
translation,218,157,results,long summary,for,long document,long summary for long document,0.6377115845680237
translation,218,157,results,results,demonstrate,our method,results demonstrate our method,0.6011868715286255
translation,219,156,baselines,aed,uses,attentional encoder-decoder framework,aed uses attentional encoder-decoder framework,0.592315673828125
translation,219,156,baselines,aed,adds,some linguistic features,aed adds some linguistic features,0.6215146780014038
translation,219,156,baselines,some linguistic features,such as,pos,some linguistic features such as pos,0.697437047958374
translation,219,156,baselines,some linguistic features,such as,named -entities,some linguistic features such as named -entities,0.5947164297103882
translation,219,156,baselines,some linguistic features,such as,tf - idf,some linguistic features such as tf - idf,0.6179603338241577
translation,219,156,baselines,tf - idf,into,encoder,tf - idf into encoder,0.5834829807281494
translation,219,156,baselines,baselines,has,aed,baselines has aed,0.5234296917915344
translation,219,157,baselines,seq2seq model,with,attention ( s2s +attn ),seq2seq model with attention ( s2s +attn ),0.6577339768409729
translation,219,158,baselines,extractive method,based on,guiderank ( gr ),extractive method based on guiderank ( gr ),0.6694830656051636
translation,219,158,baselines,extractive method,based on,guiderank,extractive method based on guiderank,0.7041116952896118
translation,219,158,baselines,guiderank ( gr ),applies,lexrank,guiderank ( gr ) applies lexrank,0.5493713021278381
translation,219,158,baselines,guiderank,applies,lexrank,guiderank applies lexrank,0.5603317022323608
translation,219,158,baselines,lexrank,with,guidance strategy,lexrank with guidance strategy,0.6659037470817566
translation,219,195,experiments,max sim,performs,best,max sim performs best,0.6280101537704468
translation,219,195,experiments,max sim,comparable to,several rouge metrics,max sim comparable to several rouge metrics,0.6404568552970886
translation,219,195,experiments,image-text relevance,has,max sim,image-text relevance has max sim,0.5672612190246582
translation,219,62,model,images,with,visual attention,images with visual attention,0.5837095379829407
translation,219,62,model,visual attention,to learn,text-image alignment,visual attention to learn text-image alignment,0.6108885407447815
translation,219,62,model,model,explore,images,model explore images,0.7232186198234558
translation,219,63,model,attention mechanism,to,visual attention mechanisms,attention mechanism to visual attention mechanisms,0.5346798300743103
translation,219,63,model,visual attention mechanisms,which attend,vi-sual signals,visual attention mechanisms which attend vi-sual signals,0.665540337562561
translation,219,63,model,model,extend,attention mechanism,model extend attention mechanism,0.6971702575683594
translation,219,18,results,multimodal output ( text+image ),increases,users ' satisfaction,multimodal output ( text+image ) increases users ' satisfaction,0.673581600189209
translation,219,18,results,users ' satisfaction,by,12.4 %,users ' satisfaction by 12.4 %,0.562759280204773
translation,219,18,results,users ' satisfaction,compared to,single-modality output ( text ),users ' satisfaction compared to single-modality output ( text ),0.6541344523429871
translation,219,18,results,12.4 %,compared to,single-modality output ( text ),12.4 % compared to single-modality output ( text ),0.646706223487854
translation,219,19,results,images,help,users,images help users,0.7469350099563599
translation,219,19,results,users,to grasp,events,users to grasp events,0.6571524143218994
translation,219,19,results,users,to grasp,events,users to grasp events,0.6571524143218994
translation,219,19,results,events,while,texts,events while texts,0.6778305172920227
translation,219,19,results,texts,provide,more details,texts provide more details,0.6040881872177124
translation,219,19,results,more details,related to,events,more details related to events,0.6924086213111877
translation,219,19,results,results,has,images,results has images,0.5471358299255371
translation,219,148,results,user ratings,of,pictorial summaries,user ratings of pictorial summaries,0.5807918310165405
translation,219,148,results,pictorial summaries,are,12.4 % higher,pictorial summaries are 12.4 % higher,0.5589329600334167
translation,219,148,results,12.4 % higher,than,text summaries,12.4 % higher than text summaries,0.5566950440406799
translation,219,148,results,results,has,user ratings,results has user ratings,0.5111224055290222
translation,219,164,results,all multimodal models,decrease in,rouge scores,all multimodal models decrease in rouge scores,0.5896492004394531
translation,219,164,results,results,has,rouge,results has rouge,0.624253511428833
translation,219,168,results,multimodal models,better than,text model,multimodal models better than text model,0.6952974200248718
translation,219,168,results,text model,in,human judgments,text model in human judgments,0.49099335074424744
translation,219,168,results,results,has,multimodal models,results has multimodal models,0.520115852355957
translation,219,188,results,ip ( image precision ),correlates best with,human assessments,ip ( image precision ) correlates best with human assessments,0.6992102861404419
translation,219,188,results,human assessments,according to,three correlation coefficients,human assessments according to three correlation coefficients,0.6517504453659058
translation,219,188,results,results,has,ip ( image precision ),results has ip ( image precision ),0.5549981594085693
translation,219,203,results,i -i ( max ) and i -i ( avg ),correlate with,human assessments,i -i ( max ) and i -i ( avg ) correlate with human assessments,0.7211281061172485
translation,219,203,results,results,find,i -i ( max ) and i -i ( avg ),results find i -i ( max ) and i -i ( avg ),0.595025897026062
translation,219,215,results,model atg,achieves,highest mmae score,model atg achieves highest mmae score,0.6553463339805603
translation,219,215,results,results,has,model atg,results has model atg,0.5141972303390503
translation,219,216,results,max sim score,of,atg,max sim score of atg,0.649839460849762
translation,219,216,results,atg,much higher than,atl,atg much higher than atl,0.649878740310669
translation,219,216,results,atg,much higher than,han,atg much higher than han,0.6215295195579529
translation,219,216,results,results,has,max sim score,results has max sim score,0.5590096116065979
translation,219,219,results,our proposed multimodal attention models,achieves,higher performance,our proposed multimodal attention models achieves higher performance,0.6890371441841125
translation,219,219,results,higher performance,than,extractive baseline gr,higher performance than extractive baseline gr,0.5836646556854248
translation,219,219,results,results,has,our proposed multimodal attention models,results has our proposed multimodal attention models,0.5374855995178223
translation,220,5,model,mrc model,trained on,squad1.1 dataset,mrc model trained on squad1.1 dataset,0.7318279147148132
translation,220,5,model,mrc model,build,extractive query - based summarizer,mrc model build extractive query - based summarizer,0.6932927966117859
translation,220,5,model,squad1.1 dataset,as,core system component,squad1.1 dataset as core system component,0.5154419541358948
translation,220,5,model,model,Using,mrc model,model Using mrc model,0.6937679052352905
translation,220,5,model,model,build,extractive query - based summarizer,model build extractive query - based summarizer,0.6702643036842346
translation,220,6,model,output,of,mrc model,output of mrc model,0.6363171935081482
translation,220,33,model,three modules,for,extractive summarization,three modules for extractive summarization,0.6209278702735901
translation,220,33,model,extractive summarization,retrieval of,candidate answer phrases,extractive summarization retrieval of candidate answer phrases,0.7033081650733948
translation,220,33,model,candidate answer phrases,using,reading comprehension system,candidate answer phrases using reading comprehension system,0.6168811321258545
translation,220,34,model,to paraphrase,for,abstractive summarization,to paraphrase for abstractive summarization,0.6377666592597961
translation,220,34,model,two mt modules ( english to spanish and back ),has,to paraphrase,two mt modules ( english to spanish and back ) has to paraphrase,0.5752588510513306
translation,220,34,model,model,utilize,two mt modules ( english to spanish and back ),model utilize two mt modules ( english to spanish and back ),0.6191191673278809
translation,220,73,results,our models,both,extractive and abstractive,our models both extractive and abstractive,0.7053185105323792
translation,220,73,results,published results,on,both test sets,published results on both test sets,0.5000531077384949
translation,220,73,results,our models,has,outperform,our models has outperform,0.6103132367134094
translation,220,73,results,extractive and abstractive,has,outperform,extractive and abstractive has outperform,0.6152127981185913
translation,220,73,results,outperform,has,published results,outperform has published results,0.6087759137153625
translation,220,73,results,results,has,our models,results has our models,0.5733726620674133
translation,220,76,results,proposed approach,yields,best system,proposed approach yields best system,0.7180147171020508
translation,220,76,results,best system,for,extractive and abstractive summarization,best system for extractive and abstractive summarization,0.6140747666358948
translation,220,76,results,results,has,proposed approach,results has proposed approach,0.6086713075637817
translation,220,78,results,abstractive performances,show,effectiveness,abstractive performances show effectiveness,0.6052125096321106
translation,220,78,results,effectiveness,of,machine translation,effectiveness of machine translation,0.575094997882843
translation,220,78,results,results,has,abstractive performances,results has abstractive performances,0.5293005108833313
translation,220,79,results,improvement,over,baseline,improvement over baseline,0.7266895771026611
translation,220,79,results,abstractive,than in,extractive case,abstractive than in extractive case,0.6368371844291687
translation,220,79,results,cnn / dm test set,has,improvement,cnn / dm test set has improvement,0.572693943977356
translation,220,79,results,results,in,cnn / dm test set,results in cnn / dm test set,0.5031188726425171
Blogs,0,76,baselines,dual embedding space model,is,recent embedding based retrieval model,dual embedding space model is recent embedding based retrieval model,0.5335233211517334
Blogs,0,76,baselines,lmd - lda,has,language modeling,lmd - lda has language modeling,0.5357854962348938
Blogs,0,104,baselines,best baseline performance,is,query reformulation ( qr ) method,best baseline performance is query reformulation ( qr ) method,0.5156142711639404
Blogs,0,104,baselines,query reformulation ( qr ) method,improves over,other baselines,query reformulation ( qr ) method improves over other baselines,0.7149540781974792
Blogs,0,104,baselines,baselines,has,best baseline performance,baselines has best baseline performance,0.5445016026496887
Blogs,0,129,baselines,-known summarization algorithms,on,set of citation texts,-known summarization algorithms on set of citation texts,0.5117993354797363
Blogs,0,129,baselines,-known summarization algorithms,on,retrieved citation -contexts,-known summarization algorithms on retrieved citation -contexts,0.5261237025260925
Blogs,0,129,baselines,-known summarization algorithms,-,divergence,-known summarization algorithms - divergence,0.6552198529243469
Blogs,0,129,baselines,kl,-,divergence,kl - divergence,0.6015083193778992
Blogs,0,5,model,unsupervised model,uses,distributed representation of words,unsupervised model uses distributed representation of words,0.563175618648529
Blogs,0,5,model,distributed representation of words,as well as,domain knowledge,distributed representation of words as well as domain knowledge,0.6139692664146423
Blogs,0,5,model,domain knowledge,to extract,appropriate context,domain knowledge to extract appropriate context,0.6425549387931824
Blogs,0,5,model,appropriate context,from,reference paper,appropriate context from reference paper,0.6058260798454285
Blogs,0,5,model,model,propose,unsupervised model,model propose unsupervised model,0.7041364312171936
Blogs,0,28,model,model,utilizes,word embeddings and domain speci c knowledge,model utilizes word embeddings and domain speci c knowledge,0.5053159594535828
Blogs,0,28,model,model,utilizes,word embeddings and domain speci c knowledge,model utilizes word embeddings and domain speci c knowledge,0.5053159594535828
Blogs,0,102,results,our models,achieve,signi cant improvements,our models achieve signi cant improvements,0.6131786704063416
Blogs,0,102,results,signi cant improvements,over,baselines,signi cant improvements over baselines,0.666839063167572
Blogs,0,102,results,results,has,our models,results has our models,0.5733726620674133
Blogs,0,105,results,general domain embeddings,does not provide,much advantage,general domain embeddings does not provide much advantage,0.7050001621246338
Blogs,0,105,results,much advantage,in comparison with,best baseline,much advantage in comparison with best baseline,0.6147056818008423
Blogs,0,105,results,results,observe,general domain embeddings,results observe general domain embeddings,0.5782988667488098
Blogs,0,105,results,results,using,general domain embeddings,results using general domain embeddings,0.6257097721099854
Blogs,0,106,results,domain speci c embeddings ( we bio ),results in,10 % c-f improvement,domain speci c embeddings ( we bio ) results in 10 % c-f improvement,0.5710052251815796
Blogs,0,106,results,10 % c-f improvement,over,best baseline,10 % c-f improvement over best baseline,0.663796067237854
Blogs,0,106,results,results,using,domain speci c embeddings ( we bio ),results using domain speci c embeddings ( we bio ),0.6491755843162537
Blogs,0,114,results,domain knowledge,in,model,domain knowledge in model,0.49072396755218506
Blogs,0,114,results,domain knowledge,results in,signi cant improvement,domain knowledge results in signi cant improvement,0.5426628589630127
Blogs,0,114,results,signi cant improvement,over,best baseline,signi cant improvement over best baseline,0.6524360775947571
Blogs,0,114,results,best baseline,in terms of,most metrics,best baseline in terms of most metrics,0.6098764538764954
Blogs,0,114,results,most metrics,has,e.g. 14 % and 16 % c-f improvements,most metrics has e.g. 14 % and 16 % c-f improvements,0.5669447183609009
Blogs,0,114,results,results,incorporating,domain knowledge,results incorporating domain knowledge,0.6507285237312317
Blogs,0,116,results,results,interpolating,domain knowledge directly,results interpolating domain knowledge directly,0.6736886501312256
Blogs,0,124,results,our best method,comparable with,average human precision score,our best method comparable with average human precision score,0.5932931900024414
Blogs,0,124,results,our best method,has,c - p@1 of 56.1 %,our best method has c - p@1 of 56.1 %,0.5831528902053833
Blogs,0,124,results,average human precision score,has,of 56.7 %,average human precision score has of 56.7 %,0.5721965432167053
Blogs,0,124,results,results,observe that,our best method,results observe that our best method,0.5434888005256653
Blogs,0,135,results,e ective contextualization,positively impacts,generated summaries,e ective contextualization positively impacts generated summaries,0.6864849328994751
Blogs,0,135,results,results,has,e ective contextualization,results has e ective contextualization,0.5417191982269287
Blogs,1,6,model,new hierarchical encoder,models,discourse structure,new hierarchical encoder models discourse structure,0.7154683470726013
Blogs,1,6,model,discourse structure,of,document,discourse structure of document,0.5729039311408997
Blogs,1,6,model,attentive discourse - aware decoder,to generate,summary,attentive discourse - aware decoder to generate summary,0.6888941526412964
Blogs,1,22,model,abstractive model,for,summarizing scientific papers,abstractive model for summarizing scientific papers,0.5875390768051147
Blogs,1,23,model,hierarchical encoder,capturing,discourse structure,hierarchical encoder capturing discourse structure,0.7433854937553406
Blogs,1,23,model,discourse structure,of,document,discourse structure of document,0.5729039311408997
Blogs,1,23,model,discourse - aware decoder,generates,summary,discourse - aware decoder generates summary,0.6585066914558411
Blogs,1,23,model,model,includes,hierarchical encoder,model includes hierarchical encoder,0.6530205607414246
Blogs,1,23,model,model,includes,discourse - aware decoder,model includes discourse - aware decoder,0.6355038285255432
Blogs,1,24,model,our decoder,attends to,different discourse sections,our decoder attends to different discourse sections,0.6848604679107666
Blogs,1,24,model,our decoder,allows,model,our decoder allows model,0.6948040127754211
Blogs,1,24,model,model,to more accurately represent,important information,model to more accurately represent important information,0.7061642408370972
Blogs,1,24,model,important information,from,source,important information from source,0.5213828086853027
Blogs,1,24,model,important information,resulting in,better context vector,important information resulting in better context vector,0.6364983916282654
Blogs,1,24,model,model,has,our decoder,model has our decoder,0.569315493106842
Blogs,1,132,results,significantly outperforms,showing,effectiveness,significantly outperforms showing effectiveness,0.7401412129402161
Blogs,1,132,results,significantly outperforms,has,state - of - the - art abstractive methods,significantly outperforms has state - of - the - art abstractive methods,0.5731334686279297
Blogs,1,133,results,rouge - 1 score,is,4 and 3 points higher,rouge - 1 score is 4 and 3 points higher,0.5721087455749512
Blogs,1,133,results,rouge - 1 score,about,4 and 3 points higher,rouge - 1 score about 4 and 3 points higher,0.6147768497467041
Blogs,1,133,results,4 and 3 points higher,than,abstractive model pntr-gen-seq2seq,4 and 3 points higher than abstractive model pntr-gen-seq2seq,0.5928664803504944
Blogs,1,133,results,abstractive model pntr-gen-seq2seq,for,arxiv and pubmed datasets,abstractive model pntr-gen-seq2seq for arxiv and pubmed datasets,0.5826966762542725
Blogs,1,133,results,abstractive model pntr-gen-seq2seq,providing,significant improvement,abstractive model pntr-gen-seq2seq providing significant improvement,0.6358414888381958
Blogs,1,134,results,most of the extractive methods,except for,lexrank,most of the extractive methods except for lexrank,0.6569476127624512
Blogs,1,134,results,lexrank,in,one of the rouge scores,lexrank in one of the rouge scores,0.5296701192855835
Blogs,1,134,results,outperforms,has,most of the extractive methods,outperforms has most of the extractive methods,0.6074466705322266
Blogs,2,120,baselines,best,of,stochastic gradient descent,best of stochastic gradient descent,0.5631798505783081
Blogs,2,120,baselines,best,of,adadelta,best of adadelta,0.6014807224273682
Blogs,2,120,baselines,best,of,momentum,best of momentum,0.6224619150161743
Blogs,2,120,baselines,best,of,adam,best of adam,0.5963394045829773
Blogs,2,120,baselines,best,of,rm - sprop,best of rm - sprop,0.578131377696991
Blogs,2,119,experimental-setup,"adagrad ( duchi et al. , 2011 )",with,learning rate,"adagrad ( duchi et al. , 2011 ) with learning rate",0.53758704662323
Blogs,2,119,experimental-setup,"adagrad ( duchi et al. , 2011 )",with,initial accumulator value,"adagrad ( duchi et al. , 2011 ) with initial accumulator value",0.5564320087432861
Blogs,2,119,experimental-setup,initial accumulator value,of,0.1,initial accumulator value of 0.1,0.6011174917221069
Blogs,2,119,experimental-setup,learning rate,has,0.15,learning rate has 0.15,0.5453540682792664
Blogs,2,119,experimental-setup,experimental setup,train using,"adagrad ( duchi et al. , 2011 )","experimental setup train using adagrad ( duchi et al. , 2011 )",0.7028688788414001
Blogs,2,121,experimental-setup,gradient clipping,with,maximum gradient norm,gradient clipping with maximum gradient norm,0.5783113837242126
Blogs,2,121,experimental-setup,maximum gradient norm,of,2,maximum gradient norm of 2,0.6099976897239685
Blogs,2,121,experimental-setup,experimental setup,use,gradient clipping,experimental setup use gradient clipping,0.5653892755508423
Blogs,2,122,experimental-setup,loss,on,validation set,loss on validation set,0.5221169590950012
Blogs,2,122,experimental-setup,loss,to implement,early stopping,loss to implement early stopping,0.702652096748352
Blogs,2,127,experimental-setup,single tesla k40 m gpu,with,batch size,single tesla k40 m gpu with batch size,0.6339405179023743
Blogs,2,127,experimental-setup,batch size,of,16,batch size of 16,0.6842944622039795
Blogs,2,127,experimental-setup,experimental setup,train on,single tesla k40 m gpu,experimental setup train on single tesla k40 m gpu,0.7045232057571411
Blogs,2,128,experimental-setup,summaries,produced using,beam search,summaries produced using beam search,0.7317162156105042
Blogs,2,128,experimental-setup,beam search,with,beam size 4,beam search with beam size 4,0.6971104741096497
Blogs,2,128,experimental-setup,test time,has,summaries,test time has summaries,0.5689306855201721
Blogs,2,128,experimental-setup,experimental setup,At,test time,experimental setup At test time,0.5272576808929443
Blogs,2,8,hyperparameters,coverage,to keep track of,what has been summarized,coverage to keep track of what has been summarized,0.6869155168533325
Blogs,2,8,hyperparameters,what has been summarized,discourages,repetition,what has been summarized discourages repetition,0.6127519607543945
Blogs,2,6,model,novel architecture,augments,standard sequence - to-sequence attentional model,novel architecture augments standard sequence - to-sequence attentional model,0.6745314002037048
Blogs,2,6,model,model,propose,novel architecture,model propose novel architecture,0.7315564155578613
Blogs,2,7,model,hybrid pointer - generator network,can copy,words,hybrid pointer - generator network can copy words,0.683578372001648
Blogs,2,7,model,hybrid pointer - generator network,retaining,ability,hybrid pointer - generator network retaining ability,0.7815195918083191
Blogs,2,7,model,words,from,source text,words from source text,0.5468318462371826
Blogs,2,7,model,words,via,pointing,words via pointing,0.6787258982658386
Blogs,2,7,model,source text,via,pointing,source text via pointing,0.681252121925354
Blogs,2,7,model,pointing,aids,accurate reproduction,pointing aids accurate reproduction,0.6905884146690369
Blogs,2,7,model,accurate reproduction,of,information,accurate reproduction of information,0.5325565934181213
Blogs,2,7,model,ability,to produce,novel words,ability to produce novel words,0.6428219676017761
Blogs,2,7,model,novel words,through,generator,novel words through generator,0.6875094175338745
Blogs,2,7,model,model,use,hybrid pointer - generator network,model use hybrid pointer - generator network,0.6326307058334351
Blogs,2,7,model,model,retaining,ability,model retaining ability,0.7927285432815552
Blogs,2,45,model,novel variant,of,"coverage vector ( tu et al. , 2016 )","novel variant of coverage vector ( tu et al. , 2016 )",0.544306218624115
Blogs,2,45,model,novel variant,to track and control,coverage,novel variant to track and control coverage,0.6849418878555298
Blogs,2,45,model,"coverage vector ( tu et al. , 2016 )",from,neural machine translation,"coverage vector ( tu et al. , 2016 ) from neural machine translation",0.5294283032417297
Blogs,2,45,model,coverage,of,source document,coverage of source document,0.5827162861824036
Blogs,2,45,model,model,propose,novel variant,model propose novel variant,0.7118100523948669
Blogs,2,154,results,both our baseline models,perform,poorly,both our baseline models perform poorly,0.6185094714164734
Blogs,2,154,results,poorly,with respect to,rouge and meteor,poorly with respect to rouge and meteor,0.7285593152046204
Blogs,2,154,results,results,find that,both our baseline models,results find that both our baseline models,0.6277969479560852
Blogs,2,161,results,pointer - generator model,achieves,much better rouge and meteor scores,pointer - generator model achieves much better rouge and meteor scores,0.6460919380187988
Blogs,2,161,results,much better rouge and meteor scores,than,baseline,much better rouge and meteor scores than baseline,0.5506502389907837
Blogs,2,161,results,results,has,pointer - generator model,results has pointer - generator model,0.5361871719360352
Blogs,2,190,results,all our models,receive,over 1 meteor point boost,all our models receive over 1 meteor point boost,0.6289483308792114
Blogs,2,190,results,over 1 meteor point boost,inclusion of,"stem , synonym and paraphrase matching","over 1 meteor point boost inclusion of stem , synonym and paraphrase matching",0.6293010115623474
Blogs,2,190,results,results,observe,all our models,results observe all our models,0.6054394841194153
Blogs,2,191,results,not surpassed,by,our models,not surpassed by our models,0.6249923706054688
Blogs,2,191,results,results,observe,lead - 3 baseline,results observe lead - 3 baseline,0.594422459602356
Blogs,3,84,ablation-analysis,base model,on,original datasets,base model on original datasets,0.5403438806533813
Blogs,3,84,ablation-analysis,base model,observed,performance,base model observed performance,0.7367082834243774
Blogs,3,84,ablation-analysis,drops,by,"0.60 , 0.72 bleu","drops by 0.60 , 0.72 bleu",0.6195300221443176
Blogs,3,84,ablation-analysis,drops,by,"1.66 , 2.09 rouge -l points","drops by 1.66 , 2.09 rouge -l points",0.6041717529296875
Blogs,3,84,ablation-analysis,"1.66 , 2.09 rouge -l points",for,java and python datasets,"1.66 , 2.09 rouge -l points for java and python datasets",0.5466321110725403
Blogs,3,84,ablation-analysis,performance,has,drops,performance has drops,0.5993483662605286
Blogs,3,84,ablation-analysis,ablation analysis,ran,base model,ablation analysis ran base model,0.6364228129386902
Blogs,3,94,ablation-analysis,empirical results,suggest,directional information,empirical results suggest directional information,0.5669543147087097
Blogs,3,94,ablation-analysis,directional information,is,important,directional information is important,0.582560658454895
Blogs,3,94,ablation-analysis,directional information,indeed,important,directional information indeed important,0.6510790586471558
Blogs,3,94,ablation-analysis,"16 , 32 , and 2 i relative distances",result in,similar performance,"16 , 32 , and 2 i relative distances result in similar performance",0.6660940051078796
Blogs,3,94,ablation-analysis,ablation analysis,has,empirical results,ablation analysis has empirical results,0.5157642364501953
Blogs,3,108,ablation-analysis,frequent tokens,in,code snippet,frequent tokens in code snippet,0.5055884122848511
Blogs,3,108,ablation-analysis,frequent tokens,get,higher copy probability,frequent tokens get higher copy probability,0.5873277187347412
Blogs,3,108,ablation-analysis,code snippet,get,higher copy probability,code snippet get higher copy probability,0.5946073532104492
Blogs,3,108,ablation-analysis,higher copy probability,when,relative position representations,higher copy probability when relative position representations,0.6473787426948547
Blogs,3,108,ablation-analysis,relative position representations,in comparison to,absolute position representations,relative position representations in comparison to absolute position representations,0.6280242800712585
Blogs,3,108,ablation-analysis,copy enabled model,has,frequent tokens,copy enabled model has frequent tokens,0.5883552432060242
Blogs,3,108,ablation-analysis,ablation analysis,observe,copy enabled model,ablation analysis observe copy enabled model,0.6132128238677979
Blogs,3,93,experiments,directional information,while modeling,pairwise relationship,directional information while modeling pairwise relationship,0.7241706848144531
Blogs,3,93,experiments,clipping distance,has,k,clipping distance has k,0.5786294341087341
Blogs,3,75,hyperparameters,transformer models,using,"adam optimizer ( kingma and ba , 2015 )","transformer models using adam optimizer ( kingma and ba , 2015 )",0.6587966084480286
Blogs,3,75,hyperparameters,"adam optimizer ( kingma and ba , 2015 )",with,initial learning rate,"adam optimizer ( kingma and ba , 2015 ) with initial learning rate",0.5927683115005493
Blogs,3,75,hyperparameters,initial learning rate,of,10 ?4,initial learning rate of 10 ?4,0.6392531991004944
Blogs,3,75,hyperparameters,hyperparameters,train,transformer models,hyperparameters train transformer models,0.6927379369735718
Blogs,3,76,hyperparameters,mini-batch size and dropout rate,to,32 and 0.2,mini-batch size and dropout rate to 32 and 0.2,0.5564597845077515
Blogs,3,76,hyperparameters,hyperparameters,set,mini-batch size and dropout rate,hyperparameters set mini-batch size and dropout rate,0.6019110679626465
Blogs,3,77,hyperparameters,transformer models,for,maximum,transformer models for maximum,0.626340925693512
Blogs,3,77,hyperparameters,maximum,of,200 epochs,maximum of 200 epochs,0.587211549282074
Blogs,3,77,hyperparameters,early stop,if,validation performance,early stop if validation performance,0.6208859086036682
Blogs,3,77,hyperparameters,does not improve,for,20 consecutive iterations,does not improve for 20 consecutive iterations,0.6248425245285034
Blogs,3,77,hyperparameters,validation performance,has,does not improve,validation performance has does not improve,0.610643744468689
Blogs,3,77,hyperparameters,hyperparameters,train,transformer models,hyperparameters train transformer models,0.6927379369735718
Blogs,3,78,hyperparameters,beam search,during,inference,beam search during inference,0.6603623628616333
Blogs,3,78,hyperparameters,beam search,set,beam size,beam search set beam size,0.6893483400344849
Blogs,3,78,hyperparameters,beam size,to,4,beam size to 4,0.6383150219917297
Blogs,3,78,hyperparameters,hyperparameters,use,beam search,hyperparameters use beam search,0.6655463576316833
Blogs,3,78,hyperparameters,hyperparameters,set,beam size,hyperparameters set beam size,0.6436296701431274
Blogs,3,6,model,code representation,explore,transformer model,code representation explore transformer model,0.652454674243927
Blogs,3,6,model,transformer model,uses,self-attention mechanism,transformer model uses self-attention mechanism,0.5384237766265869
Blogs,3,6,model,model,To learn,code representation,model To learn code representation,0.5770130157470703
Blogs,3,23,model,pairwise relationship,between,source code tokens,pairwise relationship between source code tokens,0.6420106887817383
Blogs,3,23,model,pairwise relationship,achieve,significant improvements,pairwise relationship achieve significant improvements,0.6355451941490173
Blogs,3,23,model,source code tokens,using,relative position representation,source code tokens using relative position representation,0.6249630451202393
Blogs,3,23,model,significant improvements,over learning,sequence information,significant improvements over learning sequence information,0.6851058006286621
Blogs,3,23,model,sequence information,of,code tokens,sequence information of code tokens,0.5729379057884216
Blogs,3,23,model,sequence information,using,absolute position representation,sequence information using absolute position representation,0.6518321633338928
Blogs,3,23,model,code tokens,using,absolute position representation,code tokens using absolute position representation,0.6655691862106323
Blogs,3,23,model,model,modeling,pairwise relationship,model modeling pairwise relationship,0.831407904624939
Blogs,3,23,model,model,achieve,significant improvements,model achieve significant improvements,0.6491693258285522
Blogs,3,33,model,each layer,performs,self-attention mechanism,each layer performs self-attention mechanism,0.6081052422523499
Blogs,3,33,model,multi-head attention,employs,h attention heads,multi-head attention employs h attention heads,0.5646092295646667
Blogs,3,33,model,multi-head attention,performs,self-attention mechanism,multi-head attention performs self-attention mechanism,0.555252730846405
Blogs,3,33,model,each layer,has,multi-head attention,each layer has multi-head attention,0.564212441444397
Blogs,3,33,model,model,At,each layer,model At each layer,0.5384435057640076
Blogs,3,39,model,additional attention layer,to learn,copy distribution,additional attention layer to learn copy distribution,0.6287062764167786
Blogs,3,39,model,copy distribution,on top of,decoder stack,copy distribution on top of decoder stack,0.7032915949821472
Blogs,3,39,model,model,use,additional attention layer,model use additional attention layer,0.6693074703216553
Blogs,3,82,results,base model,has,outperforms,base model has outperforms,0.6523337364196777
Blogs,3,82,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
Blogs,3,82,results,full model,has,improves,full model has improves,0.613627016544342
Blogs,3,82,results,improves,has,performance,improves has performance,0.5770372748374939
Blogs,3,82,results,performance,has,further,performance has further,0.6324895024299622
Blogs,3,82,results,results,shows,base model,results shows base model,0.7049206495285034
Blogs,3,87,results,copy attention,improves,performance,copy attention improves performance,0.672670304775238
Blogs,3,87,results,0.44 and 0.88 bleu points,for,java and python datasets,0.44 and 0.88 bleu points for java and python datasets,0.5855094790458679
Blogs,3,87,results,performance,has,0.44 and 0.88 bleu points,performance has 0.44 and 0.88 bleu points,0.5721708536148071
Blogs,3,90,results,absolute position,of,code tokens,absolute position of code tokens,0.5847769975662231
Blogs,3,90,results,absolute position,are,not effective,absolute position are not effective,0.5933716893196106
Blogs,3,90,results,learning,has,absolute position,learning has absolute position,0.5807147026062012
Blogs,3,90,results,learning,has,slightly hurts,learning has slightly hurts,0.5678166747093201
Blogs,3,90,results,slightly hurts,has,performance,slightly hurts has performance,0.5844202637672424
Blogs,3,90,results,results,demonstrates,learning,results demonstrates learning,0.6956058144569397
Blogs,3,92,results,pairwise relationship,between,source code tokens,pairwise relationship between source code tokens,0.6420106887817383
Blogs,3,92,results,pairwise relationship,via,relative position representations,pairwise relationship via relative position representations,0.7033926844596863
Blogs,3,92,results,source code tokens,via,relative position representations,source code tokens via relative position representations,0.6234752535820007
Blogs,3,92,results,results,learning,pairwise relationship,results learning pairwise relationship,0.6871828436851501
Blogs,3,107,results,qualitative analysis,reveals,copy enabled model,qualitative analysis reveals copy enabled model,0.6584791541099548
Blogs,3,107,results,qualitative analysis,in comparison to,vanilla transformer model,qualitative analysis in comparison to vanilla transformer model,0.6606775522232056
Blogs,3,107,results,copy enabled model,generates,shorter summaries,copy enabled model generates shorter summaries,0.7048214077949524
Blogs,3,107,results,shorter summaries,with,more accurate keywords,shorter summaries with more accurate keywords,0.5815410017967224
Blogs,3,107,results,qualitative analysis,has,copy enabled model,qualitative analysis has copy enabled model,0.5888272523880005
Blogs,3,107,results,vanilla transformer model,has,copy enabled model,vanilla transformer model has copy enabled model,0.6122305989265442
Blogs,3,107,results,results,has,qualitative analysis,results has qualitative analysis,0.47837314009666443
Blogs,4,26,experiments,pre-training objectives,specifically for,abstractive text summarization,pre-training objectives specifically for abstractive text summarization,0.5924472808837891
Blogs,4,26,experiments,pre-training objectives,evaluate on,12 downstream datasets,pre-training objectives evaluate on 12 downstream datasets,0.6948260068893433
Blogs,4,26,experiments,12 downstream datasets,spanning,news,12 downstream datasets spanning news,0.662669837474823
Blogs,4,26,experiments,emails,has,"zhang & tetreault , 2019 )","emails has zhang & tetreault , 2019 )",0.586651086807251
Blogs,4,26,experiments,patents,has,"sharma et al. , 2019 )","patents has sharma et al. , 2019 )",0.60317462682724
Blogs,4,172,experiments,unigram 96 k,achieved,highest rouge scores,unigram 96 k achieved highest rouge scores,0.6611340641975403
Blogs,4,172,experiments,xsum and cnn / dailymail,has,unigram 96 k,xsum and cnn / dailymail has unigram 96 k,0.6204124689102173
Blogs,4,126,hyperparameters,l,denotes,number of layers,l denotes number of layers,0.6756424307823181
Blogs,4,126,hyperparameters,number of layers,for,encoder and decoder ( i.e. transformer blocks ),number of layers for encoder and decoder ( i.e. transformer blocks ),0.5598995089530945
Blogs,4,126,hyperparameters,h,for,hidden size,h for hidden size,0.6429546475410461
Blogs,4,126,hyperparameters,h,for,number of self-attention heads,h for number of self-attention heads,0.5789253115653992
Blogs,4,126,hyperparameters,h,for,number of self-attention heads,h for number of self-attention heads,0.5789253115653992
Blogs,4,126,hyperparameters,f,for,feed -forward layer size,f for feed -forward layer size,0.6342305541038513
Blogs,4,126,hyperparameters,f,for,number of self-attention heads,f for number of self-attention heads,0.5881437063217163
Blogs,4,126,hyperparameters,pegasus base,has,"l = 12 , h = 768","pegasus base has l = 12 , h = 768",0.5725604891777039
Blogs,4,126,hyperparameters,pegasus large,has,"l = 16 , h = 1024 , f = 4096 , a = 16","pegasus large has l = 16 , h = 1024 , f = 4096 , a = 16",0.5634862184524536
Blogs,4,126,hyperparameters,hyperparameters,has,pegasus base,hyperparameters has pegasus base,0.5270460844039917
Blogs,4,129,hyperparameters,sinusoidal positional encoding,following,vaswani et al .,sinusoidal positional encoding following vaswani et al .,0.5982441902160645
Blogs,4,129,hyperparameters,hyperparameters,used,sinusoidal positional encoding,hyperparameters used sinusoidal positional encoding,0.6304168105125427
Blogs,4,130,hyperparameters,pre-training and finetuning,used,"adafactor ( shazeer & stern , 2018 )","pre-training and finetuning used adafactor ( shazeer & stern , 2018 )",0.5534564852714539
Blogs,4,130,hyperparameters,"adafactor ( shazeer & stern , 2018 )",with,square root learning rate decay,"adafactor ( shazeer & stern , 2018 ) with square root learning rate decay",0.6055557727813721
Blogs,4,130,hyperparameters,"adafactor ( shazeer & stern , 2018 )",with,dropout rate,"adafactor ( shazeer & stern , 2018 ) with dropout rate",0.5934425592422485
Blogs,4,130,hyperparameters,dropout rate,of,0.1,dropout rate of 0.1,0.5755324363708496
Blogs,4,130,hyperparameters,optimization,has,pre-training and finetuning,optimization has pre-training and finetuning,0.5549750328063965
Blogs,4,130,hyperparameters,hyperparameters,For,optimization,hyperparameters For optimization,0.5834490656852722
Blogs,4,131,hyperparameters,greedy - decoding,used,beam-search,greedy - decoding used beam-search,0.5376335978507996
Blogs,4,131,hyperparameters,beam-search,with,length- penalty,beam-search with length- penalty,0.6693060398101807
Blogs,4,131,hyperparameters,length- penalty,for,final large model,length- penalty for final large model,0.608795702457428
Blogs,4,131,hyperparameters,hyperparameters,used,greedy - decoding,hyperparameters used greedy - decoding,0.6032806634902954
Blogs,4,131,hyperparameters,hyperparameters,used,beam-search,hyperparameters used beam-search,0.6136123538017273
Blogs,4,131,hyperparameters,hyperparameters,used,beam-search,hyperparameters used beam-search,0.6136123538017273
Blogs,4,7,model,pre-training large transformer - based encoder- decoder models,on,massive text corpora,pre-training large transformer - based encoder- decoder models on massive text corpora,0.4989036023616791
Blogs,4,7,model,massive text corpora,with,new selfsupervised objective,massive text corpora with new selfsupervised objective,0.5666360855102539
Blogs,4,7,model,model,propose,pre-training large transformer - based encoder- decoder models,model propose pre-training large transformer - based encoder- decoder models,0.6344372034072876
Blogs,4,8,model,important sentences,generated together,one output sequence,important sentences generated together one output sequence,0.6759485602378845
Blogs,4,8,model,removed / masked,from,input document,removed / masked from input document,0.5698804259300232
Blogs,4,8,model,one output sequence,from,remaining sentences,one output sequence from remaining sentences,0.5711057186126709
Blogs,4,8,model,pegasus,has,important sentences,pegasus has important sentences,0.6074421405792236
Blogs,4,8,model,model,In,pegasus,model In pegasus,0.5644243955612183
Blogs,4,91,model,mlm,to train,transformer encoder,mlm to train transformer encoder,0.6352366805076599
Blogs,4,91,model,mlm,along with,gsg,mlm along with gsg,0.6683098077774048
Blogs,4,91,model,transformer encoder,along with,gsg,transformer encoder along with gsg,0.6334934830665588
Blogs,4,91,model,model,apply,mlm,model apply mlm,0.670459508895874
Blogs,4,27,results,whole sentences,from,document,whole sentences from document,0.5596014857292175
Blogs,4,27,results,gap-sentences,from,rest of the document,gap-sentences from rest of the document,0.5974696278572083
Blogs,4,27,results,gap-sentences,works well as,pre-training objective,gap-sentences works well as pre-training objective,0.6789945960044861
Blogs,4,27,results,pre-training objective,for,downstream summarization tasks,pre-training objective for downstream summarization tasks,0.5885570049285889
Blogs,4,27,results,results,masking,whole sentences,results masking whole sentences,0.6968591213226318
Blogs,4,32,results,best 568 m parameter model,trained on,"recently introduced c4 ( raffel et al. , 2019 ) corpus","best 568 m parameter model trained on recently introduced c4 ( raffel et al. , 2019 ) corpus",0.7334458827972412
Blogs,4,32,results,"recently introduced c4 ( raffel et al. , 2019 ) corpus",equal or exceed,state - of - the - art,"recently introduced c4 ( raffel et al. , 2019 ) corpus equal or exceed state - of - the - art",0.6204750537872314
Blogs,4,32,results,state - of - the - art,on,12 summarization tasks,state - of - the - art on 12 summarization tasks,0.4875926673412323
Blogs,4,32,results,results,With,best 568 m parameter model,results With best 568 m parameter model,0.6203030943870544
Blogs,4,142,results,pre-training,on,hugenews,pre-training on hugenews,0.5604629516601562
Blogs,4,142,results,pre-training,was,more effective,pre-training was more effective,0.5965940952301025
Blogs,4,142,results,hugenews,was,more effective,hugenews was more effective,0.6291956305503845
Blogs,4,142,results,more effective,than,c4,more effective than c4,0.6402853727340698
Blogs,4,142,results,more effective,than,c4,more effective than c4,0.6402853727340698
Blogs,4,142,results,c4,on,two news downstream datasets,c4 on two news downstream datasets,0.5203590989112854
Blogs,4,142,results,non-news informal datasets ( wikihow and reddit tifu ),prefer,pre-training,non-news informal datasets ( wikihow and reddit tifu ) prefer pre-training,0.7131407856941223
Blogs,4,142,results,pre-training,on,c4,pre-training on c4,0.5992346405982971
Blogs,4,142,results,results,shows,pre-training,results shows pre-training,0.6213840246200562
Blogs,4,147,results,ind-orig,achieved,best performance,ind-orig achieved best performance,0.7348774671554565
Blogs,4,147,results,best performance,followed by,seq- uniq,best performance followed by seq- uniq,0.6449753046035767
Blogs,4,148,results,consistently better ( or similar ),than,random and lead,consistently better ( or similar ) than random and lead,0.6020869612693787
Blogs,4,148,results,random and lead,across,four downstream datasets,random and lead across four downstream datasets,0.7539196610450745
Blogs,4,148,results,results,has,ind-orig and seq- uniq,results has ind-orig and seq- uniq,0.5398924350738525
Blogs,4,149,results,decent performance,on,two news datasets,decent performance on two news datasets,0.4852887690067291
Blogs,4,149,results,significantly worse,on,two non-news datasets,significantly worse on two non-news datasets,0.51763916015625
Blogs,4,149,results,lead,has,decent performance,lead has decent performance,0.6054425239562988
Blogs,4,149,results,results,has,lead,results has lead,0.40229085087776184
Blogs,4,160,results,model,with,15 % gap sentences,model with 15 % gap sentences,0.6706567406654358
Blogs,4,160,results,15 % gap sentences,achieved,highest rouge scores,15 % gap sentences achieved highest rouge scores,0.7022108435630798
Blogs,4,160,results,highest rouge scores,on,cnn / dailymail,highest rouge scores on cnn / dailymail,0.5031620860099792
Blogs,4,160,results,xsum / reddit tifu and wikihow,did,better,xsum / reddit tifu and wikihow did better,0.6320295333862305
Blogs,4,160,results,better,with,30 % and 45 %,better with 30 % and 45 %,0.711030900478363
Blogs,4,160,results,results,has,model,results has model,0.5339115858078003
Blogs,4,186,results,pegasus base,exceeded,current state- ofthe - art,pegasus base exceeded current state- ofthe - art,0.6695548892021179
Blogs,4,186,results,current state- ofthe - art,on,many datasets,current state- ofthe - art on many datasets,0.500353991985321
Blogs,4,186,results,pegasus large,achieved,better,pegasus large achieved better,0.7023784518241882
Blogs,4,186,results,better,than,state - of - the - art results,better than state - of - the - art results,0.5490322113037109
Blogs,4,186,results,better,on,all downstream datasets,better on all downstream datasets,0.5025564432144165
Blogs,4,186,results,state - of - the - art results,on,all downstream datasets,state - of - the - art results on all downstream datasets,0.5164549946784973
Blogs,4,186,results,all downstream datasets,using,hugenews,all downstream datasets using hugenews,0.6733646392822266
Blogs,4,186,results,results,has,pegasus base,results has pegasus base,0.5430145859718323
Blogs,4,201,results,significance level,of,p < 0.01,significance level of p < 0.01,0.5539028644561768
Blogs,4,201,results,pegasus large ( hugenews ) and pegasus large ( c4 ) outputs,at least as good as,reference summaries,pegasus large ( hugenews ) and pegasus large ( c4 ) outputs at least as good as reference summaries,0.6016983985900879
Blogs,4,201,results,reference summaries,in,all cases,reference summaries in all cases,0.5245869755744934
Blogs,5,69,baselines,baselines,has,lsa [ 16 ],baselines has lsa [ 16 ],0.5249087810516357
Blogs,5,71,baselines,baselines,has,lexrank [ 5 ],baselines has lexrank [ 5 ],0.4778149127960205
Blogs,5,72,baselines,extractive method,employs,graph - based centrality ranking,extractive method employs graph - based centrality ranking,0.5739324688911438
Blogs,5,72,baselines,graph - based centrality ranking,of,sentence,graph - based centrality ranking of sentence,0.5708984136581421
Blogs,5,73,baselines,4,-,pointer-generator ( pg ),4 - pointer-generator ( pg ),0.641194224357605
Blogs,5,73,baselines,4,has,pointer-generator ( pg ),4 has pointer-generator ( pg ),0.5659388303756714
Blogs,5,73,baselines,baselines,has,4,baselines has 4,0.633842408657074
Blogs,5,73,baselines,baselines,has,pointer-generator ( pg ),baselines has pointer-generator ( pg ),0.5477702617645264
Blogs,5,75,baselines,baselines,has,background - aware pointer-generator ( back . pg ),baselines has background - aware pointer-generator ( back . pg ),0.5276293158531189
Blogs,5,83,experiments,extractive summarization methods,perform,particularly poorly,extractive summarization methods perform particularly poorly,0.5598924160003662
Blogs,5,83,experiments,lexrank and lsa ),perform,particularly poorly,lexrank and lsa ) perform particularly poorly,0.6292114853858948
Blogs,5,83,experiments,extractive summarization methods,has,lexrank and lsa ),extractive summarization methods has lexrank and lsa ),0.5701656937599182
Blogs,5,78,hyperparameters,100 - dimensional glove embeddings,pre-trained over,large corpus,100 - dimensional glove embeddings pre-trained over large corpus,0.7751966714859009
Blogs,5,78,hyperparameters,large corpus,of,4.5 million radiology reports,large corpus of 4.5 million radiology reports,0.559132993221283
Blogs,5,78,hyperparameters,2 - layer bilstm encoder,with,hidden size,2 - layer bilstm encoder with hidden size,0.5721961259841919
Blogs,5,78,hyperparameters,2 - layer bilstm encoder,with,hidden size,2 - layer bilstm encoder with hidden size,0.5721961259841919
Blogs,5,78,hyperparameters,hidden size,of,100,hidden size of 100,0.6616772413253784
Blogs,5,78,hyperparameters,1 - layer lstm decoder,with,hidden size,1 - layer lstm decoder with hidden size,0.588563859462738
Blogs,5,78,hyperparameters,hidden size,of,200,hidden size of 200,0.6639924049377441
Blogs,5,78,hyperparameters,hyperparameters,use,100 - dimensional glove embeddings,hyperparameters use 100 - dimensional glove embeddings,0.5542042851448059
Blogs,5,79,hyperparameters,inference time,use,beam search,inference time use beam search,0.6249086260795593
Blogs,5,79,hyperparameters,beam search,beam size of,5,beam search beam size of 5,0.7566386461257935
Blogs,5,79,hyperparameters,hyperparameters,At,inference time,hyperparameters At inference time,0.5029842853546143
Blogs,5,80,hyperparameters,dropout,of,0.5,dropout of 0.5,0.6125851273536682
Blogs,5,80,hyperparameters,negative loglikelihood loss,using,adam optimizer,negative loglikelihood loss using adam optimizer,0.520921528339386
Blogs,5,80,hyperparameters,negative loglikelihood loss,using,learning rate,negative loglikelihood loss using learning rate,0.5786098837852478
Blogs,5,80,hyperparameters,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
Blogs,5,80,hyperparameters,adam optimizer,has,learning rate,adam optimizer has learning rate,0.5073457360267639
Blogs,5,80,hyperparameters,hyperparameters,train to optimize,negative loglikelihood loss,hyperparameters train to optimize negative loglikelihood loss,0.7117505669593811
Blogs,5,19,model,entities,in,clinical text,entities in clinical text,0.5471178889274597
Blogs,5,19,model,entities,with,domain-specific medical ontology,entities with domain-specific medical ontology,0.5352447628974915
Blogs,5,19,model,entities,encode them into,separate context vector,entities encode them into separate context vector,0.6601539850234985
Blogs,5,19,model,clinical text,with,domain-specific medical ontology,clinical text with domain-specific medical ontology,0.5560712218284607
Blogs,5,19,model,separate context vector,to aid,generation process,separate context vector to aid generation process,0.5985634922981262
Blogs,5,19,model,model,link,entities,model link entities,0.7123862504959106
Blogs,5,74,model,abstractive seq2seq attention summarization,incorporates,copy mechanism,abstractive seq2seq attention summarization incorporates copy mechanism,0.6619808673858643
Blogs,5,74,model,copy mechanism,to directly copy,text,copy mechanism to directly copy text,0.7269923686981201
Blogs,5,74,model,text,from,input,text from input,0.5922976732254028
Blogs,5,88,results,our ontology - aware models ( umls pg and radlex pg ),has,significantly outperform,our ontology - aware models ( umls pg and radlex pg ) has significantly outperform,0.5523611903190613
Blogs,5,88,results,significantly outperform,has,all other approaches,significantly outperform has all other approaches,0.5610361695289612
Blogs,5,88,results,results,observe,our ontology - aware models ( umls pg and radlex pg ),results observe our ontology - aware models ( umls pg and radlex pg ),0.5915117263793945
Blogs,5,89,results,slightly outperforms,suggesting,radiology -specific ontology,slightly outperforms suggesting radiology -specific ontology,0.674100935459137
Blogs,5,89,results,umls model,suggesting,radiology -specific ontology,umls model suggesting radiology -specific ontology,0.6447247862815857
Blogs,5,89,results,radiology -specific ontology,is,beneficial,radiology -specific ontology is beneficial,0.580033540725708
Blogs,5,89,results,radlex model,has,slightly outperforms,radlex model has slightly outperforms,0.6186287999153137
Blogs,5,89,results,slightly outperforms,has,umls model,slightly outperforms has umls model,0.5794870257377625
Blogs,5,89,results,results,has,radlex model,results has radlex model,0.5354623794555664
Blogs,5,90,results,both ontologies,in,model simultaneously,both ontologies in model simultaneously,0.5420409440994263
Blogs,5,90,results,1.26 % lower,than,best model,1.26 % lower than best model,0.5335507392883301
Blogs,5,90,results,best model,on,rouge - 1,best model on rouge - 1,0.5398115515708923
Blogs,5,90,results,slightly lower performance,has,1.26 % lower,slightly lower performance has 1.26 % lower,0.5492271184921265
Blogs,5,90,results,results,experimented incorporating,both ontologies,results experimented incorporating both ontologies,0.708670437335968
Blogs,6,8,baselines,coverage loss,to reduce,repetition,coverage loss to reduce repetition,0.6968106627464294
Blogs,6,108,hyperparameters,frozen pre-trained uncased 300 - dimensional glove embeddings,trained on,6 billion tokens,frozen pre-trained uncased 300 - dimensional glove embeddings trained on 6 billion tokens,0.6981853246688843
Blogs,6,108,hyperparameters,6 billion tokens,with,400k vocabulary,6 billion tokens with 400k vocabulary,0.6189208030700684
Blogs,6,108,hyperparameters,hyperparameters,used,frozen pre-trained uncased 300 - dimensional glove embeddings,hyperparameters used frozen pre-trained uncased 300 - dimensional glove embeddings,0.5614606142044067
Blogs,6,112,hyperparameters,dropout rate,of,0.1,dropout rate of 0.1,0.5755324363708496
Blogs,6,112,hyperparameters,batch size,of,4,batch size of 4,0.6922571659088135
Blogs,6,112,hyperparameters,maximum source sequence length,of,400,maximum source sequence length of 400,0.6304855942726135
Blogs,6,112,hyperparameters,maximum target sequence length,of,128,maximum target sequence length of 128,0.6113051176071167
Blogs,6,114,hyperparameters,baseline model,for,450 epochs,baseline model for 450 epochs,0.584513247013092
Blogs,6,114,hyperparameters,baseline model,took,approximately 24 hours,baseline model took approximately 24 hours,0.6674873232841492
Blogs,6,114,hyperparameters,450 epochs,took,approximately 24 hours,450 epochs took approximately 24 hours,0.6511315107345581
Blogs,6,114,hyperparameters,hyperparameters,trained,baseline model,hyperparameters trained baseline model,0.7305794954299927
Blogs,6,9,model,pointer - generator network,to allow,model,pointer - generator network to allow model,0.70651775598526
Blogs,6,9,model,model,to copy,words,model to copy words,0.6283966898918152
Blogs,6,9,model,words,from,source texts,words from source texts,0.5265707969665527
Blogs,6,9,model,model,add,pointer - generator network,model add pointer - generator network,0.6382426619529724
Blogs,6,26,results,pointer- generator network,improves,rouge scores,pointer- generator network improves rouge scores,0.7036877274513245
Blogs,6,26,results,rouge scores,for,transformer models,rouge scores for transformer models,0.6047062277793884
Blogs,6,26,results,results,find that,pointer- generator network,results find that pointer- generator network,0.6485458612442017
Blogs,6,29,results,coverage loss,against,n-gram blocking,coverage loss against n-gram blocking,0.6893965005874634
Blogs,6,29,results,repetition,in,summaries,repetition in summaries,0.5705008506774902
Blogs,6,29,results,results,compare,coverage loss,results compare coverage loss,0.6540303826332092
Blogs,6,30,results,coverage,in terms of both,rouge scores,coverage in terms of both rouge scores,0.6178192496299744
Blogs,6,30,results,coverage,in terms of both,fluidity of summary,coverage in terms of both fluidity of summary,0.724578857421875
Blogs,6,30,results,n-gram blocking,has,significantly outperforms,n-gram blocking has significantly outperforms,0.6230136156082153
Blogs,6,30,results,significantly outperforms,has,coverage,significantly outperforms has coverage,0.6265489459037781
Blogs,6,30,results,results,find that,n-gram blocking,results find that n-gram blocking,0.6250631213188171
Blogs,6,130,results,pointer - generator,improved,our rouge scores,pointer - generator improved our rouge scores,0.7206021547317505
Blogs,6,130,results,results,saw that,pointer - generator,results saw that pointer - generator,0.6908326745033264
Blogs,6,133,results,model,to learn,transformer and pointer - generator parameters,model to learn transformer and pointer - generator parameters,0.5500473976135254
Blogs,6,133,results,transformer and pointer - generator parameters,produced,significantly higher results,transformer and pointer - generator parameters produced significantly higher results,0.6851271390914917
Blogs,6,133,results,significantly higher results,than,freezing,significantly higher results than freezing,0.5912092328071594
Blogs,6,133,results,freezing,learned during,baseline training,freezing learned during baseline training,0.7026790380477905
Blogs,6,133,results,transformer parameters,learned during,baseline training,transformer parameters learned during baseline training,0.7261015772819519
Blogs,6,133,results,transformer parameters,learning,pointer - generator parameters,transformer parameters learning pointer - generator parameters,0.7026558518409729
Blogs,6,133,results,freezing,has,transformer parameters,freezing has transformer parameters,0.5568103194236755
Blogs,6,133,results,results,allowing,model,results allowing model,0.6204224228858948
Blogs,6,134,results,our scores,by adding,coverage loss,our scores by adding coverage loss,0.7183631658554077
Blogs,6,134,results,coverage loss,to,our transformer -pointergenerator model,coverage loss to our transformer -pointergenerator model,0.5008801221847534
Blogs,6,134,results,results,improved,our scores,results improved our scores,0.7254393696784973
Blogs,6,136,results,n-gram blocking,has,significantly improved,n-gram blocking has significantly improved,0.5845414996147156
Blogs,6,136,results,significantly improved,has,our scores,significantly improved has our scores,0.5853357315063477
Blogs,6,136,results,results,saw that,n-gram blocking,results saw that n-gram blocking,0.6743702292442322
Blogs,6,137,results,rouge scores,function of,n,rouge scores function of n,0.6703426241874695
Blogs,6,137,results,n,for,transformer + pointer - generator model,n for transformer + pointer - generator model,0.6310042142868042
Blogs,6,160,results,less repetitive summaries,than,coverage loss,less repetitive summaries than coverage loss,0.5698970556259155
Blogs,6,160,results,n-gram blocking,much hackier way of producing,summaries,n-gram blocking much hackier way of producing summaries,0.7278593182563782
Blogs,7,257,ablation-analysis,root - type,can achieve,high qa accuracy,root - type can achieve high qa accuracy,0.7105847001075745
Blogs,7,257,ablation-analysis,high qa accuracy,when using,notext input,high qa accuracy when using notext input,0.6998982429504395
Blogs,7,257,ablation-analysis,ablation analysis,Regarding,different types of qa pairs,ablation analysis Regarding different types of qa pairs,0.6186235547065735
Blogs,7,203,baselines,non-neural approaches,extract,sentences,non-neural approaches extract sentences,0.6668779850006104
Blogs,7,203,baselines,sentences,from,source article,sentences from source article,0.5841383934020996
Blogs,7,203,baselines,sentences,form,summary,sentences form summary,0.7175372838973999
Blogs,7,211,baselines,summarunner,presents,autoregressive sequence labeling method,summarunner presents autoregressive sequence labeling method,0.6640098690986633
Blogs,7,211,baselines,autoregressive sequence labeling method,based on,recurrent neural networks,autoregressive sequence labeling method based on recurrent neural networks,0.6413489580154419
Blogs,7,211,baselines,baselines,has,summarunner,baselines has summarunner,0.5826749801635742
Blogs,7,214,baselines,distraction -m3,trains,summarization system,distraction -m3 trains summarization system,0.7421886920928955
Blogs,7,214,baselines,summarization system,to distract,attention,summarization system to distract attention,0.6688007116317749
Blogs,7,214,baselines,attention,to traverse,different regions,attention to traverse different regions,0.7874721884727478
Blogs,7,214,baselines,different regions,of,source article,different regions of source article,0.5691860318183899
Blogs,7,214,baselines,baselines,has,distraction -m3,baselines has distraction -m3,0.5976544618606567
Blogs,7,267,baselines,baselines,has,cnn,baselines has cnn,0.5907226204872131
Blogs,7,187,hyperparameters,hidden state dimension,of,lstm,hidden state dimension of lstm,0.5725870728492737
Blogs,7,187,hyperparameters,hidden state dimension,to be,256,hidden state dimension to be 256,0.5605120658874512
Blogs,7,187,hyperparameters,lstm,to be,256,lstm to be 256,0.5123810172080994
Blogs,7,187,hyperparameters,hyperparameters,set,hidden state dimension,hyperparameters set hidden state dimension,0.6264743208885193
Blogs,7,193,hyperparameters,sinusoidal positional encodings,of,30 dimensions,sinusoidal positional encodings of 30 dimensions,0.586737871170044
Blogs,7,193,hyperparameters,"et al. , 2017 )",of,30 dimensions,"et al. , 2017 ) of 30 dimensions",0.5759252905845642
Blogs,7,193,hyperparameters,hyperparameters,use,100 - dimensional word embeddings,hyperparameters use 100 - dimensional word embeddings,0.5437304973602295
Blogs,7,194,hyperparameters,maximum article length,set to,400 words,maximum article length set to 400 words,0.5853229761123657
Blogs,7,194,hyperparameters,hyperparameters,has,maximum article length,hyperparameters has maximum article length,0.47672945261001587
Blogs,7,196,hyperparameters,at most 10 qa pairs ( k=10 ),to guide,extraction,at most 10 qa pairs ( k=10 ) to guide extraction,0.6398481726646423
Blogs,7,196,hyperparameters,extraction,of,summary segments,extraction of summary segments,0.6170009970664978
Blogs,7,197,hyperparameters,mini-batch training,with,"adam optimizer ( kingma and ba , 2014 )","mini-batch training with adam optimizer ( kingma and ba , 2014 )",0.6231120228767395
Blogs,7,197,hyperparameters,"adam optimizer ( kingma and ba , 2014 )",where,mini-batch,"adam optimizer ( kingma and ba , 2014 ) where mini-batch",0.5925194025039673
Blogs,7,197,hyperparameters,mini-batch,contains,128 articles,mini-batch contains 128 articles,0.6938420534133911
Blogs,7,197,hyperparameters,mini-batch,contains,qa pairs,mini-batch contains qa pairs,0.654100775718689
Blogs,7,197,hyperparameters,hyperparameters,apply,mini-batch training,hyperparameters apply mini-batch training,0.5783742666244507
Blogs,7,198,hyperparameters,summary ratio,set to,0.15,summary ratio set to 0.15,0.6933051943778992
Blogs,7,198,hyperparameters,0.15,yielding,extractive summaries,0.15 yielding extractive summaries,0.6322953701019287
Blogs,7,198,hyperparameters,extractive summaries,has,of about 60 words,extractive summaries has of about 60 words,0.5868194699287415
Blogs,7,198,hyperparameters,hyperparameters,has,summary ratio,hyperparameters has summary ratio,0.5305749773979187
Blogs,7,35,model,human abstracts,to guide,extraction of summary text units,human abstracts to guide extraction of summary text units,0.6433182954788208
Blogs,7,41,model,novel reinforcement learning framework,to explore,space of possible extractive summaries,novel reinforcement learning framework to explore space of possible extractive summaries,0.6464291214942932
Blogs,7,41,model,novel reinforcement learning framework,assess,each summary,novel reinforcement learning framework assess each summary,0.6407633423805237
Blogs,7,41,model,each summary,using,novel reward function,each summary using novel reward function,0.6255669593811035
Blogs,7,41,model,novel reward function,judging,summary 's,novel reward function judging summary 's,0.6644484400749207
Blogs,7,41,model,novel reward function,judging,"fluency , length","novel reward function judging fluency , length",0.58107990026474
Blogs,7,41,model,novel reward function,judging,competency,novel reward function judging competency,0.6696186065673828
Blogs,7,41,model,competency,to answer,important questions,competency to answer important questions,0.6990929841995239
Blogs,7,41,model,summary 's,has,adequacy,summary 's has adequacy,0.5762854814529419
Blogs,7,41,model,model,utilize,novel reinforcement learning framework,model utilize novel reinforcement learning framework,0.5321887731552124
Blogs,7,71,model,novel supervised framework,encouraging,selection of consecutive sequences of words,novel supervised framework encouraging selection of consecutive sequences of words,0.6365816593170166
Blogs,7,71,model,selection of consecutive sequences of words,to form,extractive summary,selection of consecutive sequences of words to form extractive summary,0.6400794982910156
Blogs,7,71,model,model,present,novel supervised framework,model present novel supervised framework,0.6468057036399841
Blogs,7,72,model,reinforcement learning,to explore,space,reinforcement learning to explore space,0.6723940372467041
Blogs,7,72,model,reinforcement learning,promote,"fluent , adequate , and competent","reinforcement learning promote fluent , adequate , and competent",0.597131609916687
Blogs,7,72,model,space,of,possible extractive summaries,space of possible extractive summaries,0.560110867023468
Blogs,7,72,model,"fluent , adequate , and competent",in,question answering,"fluent , adequate , and competent in question answering",0.45087096095085144
Blogs,7,72,model,model,leverage,reinforcement learning,model leverage reinforcement learning,0.7140412926673889
Blogs,7,215,model,graph attention,introduces,graph- based attention mechanism,graph attention introduces graph- based attention mechanism,0.609276533126831
Blogs,7,215,model,graph- based attention mechanism,to enhance,encoderdecoder framework,graph- based attention mechanism to enhance encoderdecoder framework,0.672297477722168
Blogs,7,215,model,model,has,graph attention,model has graph attention,0.5476001501083374
Blogs,7,42,results,extractive summaries,yielding,highest expected rewards,extractive summaries yielding highest expected rewards,0.6134320497512817
Blogs,7,228,results,our qasumm methods,focusing on,chunk extraction,our qasumm methods focusing on chunk extraction,0.7338860630989075
Blogs,7,228,results,chunk extraction,perform,on par,chunk extraction perform on par,0.5600650310516357
Blogs,7,228,results,on par,with,competitive systems,on par with competitive systems,0.7219261527061462
Blogs,7,228,results,results,has,our qasumm methods,results has our qasumm methods,0.5411455631256104
Blogs,7,237,results,variants,of,qasumm method,variants of qasumm method,0.6211110353469849
Blogs,7,237,results,variants,find that,qasumm + root,variants find that qasumm + root,0.6538283824920654
Blogs,7,237,results,qasumm method,find that,qasumm + root,qasumm method find that qasumm + root,0.6512143015861511
Blogs,7,237,results,qasumm + root,achieves,highest scores,qasumm + root achieves highest scores,0.6712303161621094
Blogs,7,237,results,highest scores,on,dm dataset,highest scores on dm dataset,0.5106605887413025
Blogs,7,237,results,results,Among,variants,results Among variants,0.5687075853347778
Blogs,7,238,results,qasumm +ner,performs,consistently well,qasumm +ner performs consistently well,0.6261714100837708
Blogs,7,238,results,consistently well,on,cnn and dm datasets,consistently well on cnn and dm datasets,0.5295956134796143
Blogs,7,238,results,results,has,qasumm +ner,results has qasumm +ner,0.5592830777168274
Blogs,7,252,results,question - answering with goldsumm,performs,best,question - answering with goldsumm performs best,0.6088256239891052
Blogs,7,252,results,best,for,all qa types,best for all qa types,0.6185498237609863
Blogs,7,252,results,results,observe,question - answering with goldsumm,results observe question - answering with goldsumm,0.560562789440155
Blogs,7,253,results,outperforms,using,full - text,outperforms using full - text,0.6408496499061584
Blogs,7,253,results,scenarios,using,full - text,scenarios using full - text,0.6195931434631348
Blogs,7,253,results,full - text,as,source input,full - text as source input,0.5124569535255432
Blogs,7,253,results,outperforms,has,scenarios,outperforms has scenarios,0.6612445116043091
Blogs,7,253,results,results,has,outperforms,results has outperforms,0.6657275557518005
Blogs,7,255,results,performance,of,qa - summ + noq,performance of qa - summ + noq,0.6138985753059387
Blogs,7,255,results,qa - summ + noq,in between,notext and goldsumm,qa - summ + noq in between notext and goldsumm,0.7383760213851929
Blogs,7,255,results,notext and goldsumm,for,all answer types,notext and goldsumm for all answer types,0.6152620315551758
Blogs,7,255,results,results,observe that,performance,results observe that performance,0.6149371862411499
Blogs,7,259,results,ner - type qa pairs,work,best,ner - type qa pairs work best,0.6408628821372986
Blogs,7,259,results,best,for,goldsumm and full - text,best for goldsumm and full - text,0.576815128326416
Blogs,7,259,results,results,has,ner - type qa pairs,results has ner - type qa pairs,0.543272852897644
Blogs,7,260,results,subj / obj - type qa pairs,have,smallest gap,subj / obj - type qa pairs have smallest gap,0.5550492405891418
Blogs,7,260,results,smallest gap,between,train / dev accuracies,smallest gap between train / dev accuracies,0.6113104224205017
Blogs,7,260,results,results,find,subj / obj - type qa pairs,results find subj / obj - type qa pairs,0.5708459615707397
Blogs,7,270,results,extracting chunks,performs,superior,extracting chunks performs superior,0.6177071928977966
Blogs,7,270,results,extracting chunks,combining,chunks,extracting chunks combining chunks,0.768095850944519
Blogs,7,270,results,chunks,with,lstm representations,chunks with lstm representations,0.6277991533279419
Blogs,7,270,results,lstm representations,yield,highest scores,lstm representations yield highest scores,0.7207468748092651
Blogs,7,270,results,results,find that,extracting chunks,results find that extracting chunks,0.6608311533927917
Blogs,7,270,results,results,combining,chunks,results combining chunks,0.6124908328056335
Blogs,7,283,results,all systems,resulted in,similar performance times,all systems resulted in similar performance times,0.6270067095756531
Blogs,7,283,results,human abstracts,has,all systems,human abstracts has all systems,0.5943697094917297
Blogs,7,284,results,large margin,in,qa accuracy,large margin in qa accuracy,0.563387930393219
Blogs,7,284,results,large margin,compared to,abstractive and our supervised approach,large margin compared to abstractive and our supervised approach,0.6266524195671082
Blogs,7,284,results,qa accuracy,in,our full system,qa accuracy in our full system,0.5048866868019104
Blogs,7,284,results,results,observe,large margin,results observe large margin,0.6539188027381897
Blogs,7,285,results,informativeness,of,summaries,informativeness of summaries,0.6002655625343323
Blogs,7,285,results,informativeness,yielded,higher performance,informativeness yielded higher performance,0.6097134947776794
Blogs,7,285,results,summaries,to be,same,summaries to be same,0.6581137180328369
Blogs,7,285,results,summaries,to be,our systems,summaries to be our systems,0.5651859641075134
Blogs,7,285,results,same,has,our systems,same has our systems,0.6465820074081421
Blogs,8,113,baselines,baselines,has,word vector models,baselines has word vector models,0.525046706199646
Blogs,8,117,baselines,baselines,has,lstm -rnn method,baselines has lstm -rnn method,0.533993124961853
Blogs,8,123,baselines,baselines,has,ensemble methods,baselines has ensemble methods,0.54196697473526
Blogs,8,17,experiments,more than 10k documents,extended,automatically,more than 10k documents extended automatically,0.6583275198936462
Blogs,8,17,experiments,automatically,to,additional 26 domains,automatically to additional 26 domains,0.588474452495575
Blogs,8,115,hyperparameters,sentence,represented as,averaged word vector,sentence represented as averaged word vector,0.6057383418083191
Blogs,8,115,hyperparameters,averaged word vector,of,100 numbers,averaged word vector of 100 numbers,0.6085280776023865
Blogs,8,115,hyperparameters,hyperparameters,has,word2,hyperparameters has word2,0.5673266053199768
Blogs,8,116,hyperparameters,word2vecaf,takes,sentence average vector,word2vecaf takes sentence average vector,0.6059154868125916
Blogs,8,116,hyperparameters,word2vecaf,takes,abstract average vector,word2vecaf takes abstract average vector,0.5843127965927124
Blogs,8,116,hyperparameters,word2vecaf,takes,handcrafted features,word2vecaf takes handcrafted features,0.6012842059135437
Blogs,8,116,hyperparameters,sentence average vector,giving,208 - dimensional vector,sentence average vector giving 208 - dimensional vector,0.6631380319595337
Blogs,8,116,hyperparameters,handcrafted features,giving,208 - dimensional vector,handcrafted features giving 208 - dimensional vector,0.6294569969177246
Blogs,8,116,hyperparameters,208 - dimensional vector,for,classification,208 - dimensional vector for classification,0.6085847616195679
Blogs,8,116,hyperparameters,hyperparameters,has,word2vecaf,hyperparameters has word2vecaf,0.5096527934074402
Blogs,8,119,hyperparameters,dropout probability,set to,0.5,dropout probability set to 0.5,0.7214090824127197
Blogs,8,119,hyperparameters,hyperparameters,has,dropout probability,hyperparameters has dropout probability,0.5095431804656982
Blogs,8,23,model,framework,called,service - flow map ( sfmap ),framework called service - flow map ( sfmap ),0.6703598499298096
Blogs,8,23,model,framework,works on top of,dng.sfmap,framework works on top of dng.sfmap,0.7053266763687134
Blogs,8,23,model,framework,estimates,hostname,framework estimates hostname,0.6028051376342773
Blogs,8,23,model,service - flow map ( sfmap ),works on top of,dng.sfmap,service - flow map ( sfmap ) works on top of dng.sfmap,0.6526085138320923
Blogs,8,23,model,service - flow map ( sfmap ),estimates,hostname,service - flow map ( sfmap ) estimates hostname,0.5919535160064697
Blogs,8,23,model,hostname,of,https server,hostname of https server,0.5309473276138306
Blogs,8,23,model,hostname,when given,pair,hostname when given pair,0.7096908688545227
Blogs,8,23,model,pair,of,client and server ip addresses,pair of client and server ip addresses,0.5907585620880127
Blogs,8,23,model,model,develop,framework,model develop framework,0.6104331016540527
Blogs,8,32,model,framework,called,service - flow map ( sfmap ),framework called service - flow map ( sfmap ),0.6703598499298096
Blogs,8,32,model,framework,works on top of,dng,framework works on top of dng,0.7185283899307251
Blogs,8,32,model,service - flow map ( sfmap ),works on top of,dng,service - flow map ( sfmap ) works on top of dng,0.7207603454589844
Blogs,8,32,model,model,develop,framework,model develop framework,0.6104331016540527
Blogs,8,33,model,sfmap,estimates,hostname,sfmap estimates hostname,0.6092487573623657
Blogs,8,33,model,hostname,of,https server,hostname of https server,0.5309473276138306
Blogs,8,33,model,hostname,when given,pair,hostname when given pair,0.7096908688545227
Blogs,8,33,model,pair,of,client and server ip addresses,pair of client and server ip addresses,0.5907585620880127
Blogs,8,33,model,model,has,sfmap,model has sfmap,0.5895856618881226
Blogs,8,8,results,models,on,dataset,models on dataset,0.5683951377868652
Blogs,8,8,results,models,making use of,neural sentence encoding,models making use of neural sentence encoding,0.6165814995765686
Blogs,8,8,results,models,making use of,traditionally used summarisation features,models making use of traditionally used summarisation features,0.6287223100662231
Blogs,8,8,results,models,show,models,models show models,0.6700940728187561
Blogs,8,8,results,models,which encode,sentences,models which encode sentences,0.7526852488517761
Blogs,8,8,results,dataset,making use of,neural sentence encoding,dataset making use of neural sentence encoding,0.5977469682693481
Blogs,8,8,results,dataset,making use of,traditionally used summarisation features,dataset making use of traditionally used summarisation features,0.6127681136131287
Blogs,8,8,results,models,which encode,sentences,models which encode sentences,0.7526852488517761
Blogs,8,8,results,sentences,as well as,local and global context,sentences as well as local and global context,0.666797399520874
Blogs,8,8,results,local and global context,perform,best,local and global context perform best,0.6014956831932068
Blogs,8,8,results,significantly outperforming,has,well - established baseline methods,significantly outperforming has well - established baseline methods,0.5440510511398315
Blogs,8,8,results,results,develop,models,results develop models,0.5674806237220764
Blogs,8,147,results,significantly outperform,including,graph - based methods,significantly outperform including graph - based methods,0.696224570274353
Blogs,8,147,results,graph - based methods,take account of,global context,graph - based methods take account of global context,0.6666537523269653
Blogs,8,147,results,probabilistic methods,in,kl - sum,probabilistic methods in kl - sum,0.5369314551353455
Blogs,8,147,results,singular value decomposition,with,lsa,singular value decomposition with lsa,0.6436495184898376
Blogs,8,147,results,simple methods,based on,counting,simple methods based on counting,0.7129054665565491
Blogs,8,147,results,counting,in,sumbasic,counting in sumbasic,0.6125112771987915
Blogs,8,147,results,textrank,has,"mihalcea and tarau , 2004 )","textrank has mihalcea and tarau , 2004 )",0.5993709564208984
Blogs,8,147,results,sumbasic,has,"vanderwende et al. , 2007 )","sumbasic has vanderwende et al. , 2007 )",0.5749207139015198
Blogs,8,148,results,encouraging result,showing that,our methods,encouraging result showing that our methods,0.6214663982391357
Blogs,8,148,results,our methods,combine,neural sentence encoding,our methods combine neural sentence encoding,0.6475439071655273
Blogs,8,148,results,our methods,combine,simple features,our methods combine simple features,0.6788330674171448
Blogs,8,148,results,simple features,for representing,global context and positional information,simple features for representing global context and positional information,0.6696380972862244
Blogs,8,148,results,global context and positional information,are,very effective,global context and positional information are very effective,0.5455042719841003
Blogs,8,148,results,very effective,for modelling,extractive summarisation problem,very effective for modelling extractive summarisation problem,0.7371692657470703
Blogs,8,149,results,performance,measured in terms of,accuracy and rouge -l,performance measured in terms of accuracy and rouge -l,0.7187674045562744
Blogs,8,149,results,accuracy and rouge -l,on,cspubsumext test and cspubsum test,accuracy and rouge -l on cspubsumext test and cspubsum test,0.5619845986366272
Blogs,8,150,results,best,by,both mea- sures,best by both mea- sures,0.6313070058822632
Blogs,8,151,results,outperforms,by,6.7 % accuracy,outperforms by 6.7 % accuracy,0.5882009267807007
Blogs,8,151,results,models,based on,averaged word embeddings,models based on averaged word embeddings,0.5784644484519958
Blogs,8,151,results,averaged word embeddings,by,6.7 % accuracy,averaged word embeddings by 6.7 % accuracy,0.501246452331543
Blogs,8,151,results,averaged word embeddings,by,2.1 rouge points,averaged word embeddings by 2.1 rouge points,0.45095086097717285
Blogs,8,151,results,outperforms,has,models,outperforms has models,0.6708464026451111
Blogs,8,151,results,results,has,lstm encoding,results has lstm encoding,0.530105471611023
Blogs,8,154,results,highest accuracy,on,cspubsumext,highest accuracy on cspubsumext,0.548890233039856
Blogs,8,154,results,results,has,interesting result,results has interesting result,0.5762223601341248
Blogs,8,156,results,safnet,achieved,highest accuracy,safnet achieved highest accuracy,0.7435475587844849
Blogs,8,156,results,highest accuracy,on,cspubsumext test,highest accuracy on cspubsumext test,0.5586619973182678
Blogs,8,156,results,worse,than,abstractrouge summariser,worse than abstractrouge summariser,0.5884047746658325
Blogs,8,156,results,abstractrouge summariser,on,cspubsum test,abstractrouge summariser on cspubsum test,0.5515349507331848
Blogs,8,156,results,results,has,safnet,results has safnet,0.5534816384315491
Blogs,8,182,results,ensemblers,ensure,good performance,ensemblers ensure good performance,0.7343922853469849
Blogs,8,182,results,good performance,on,both test sets,good performance on both test sets,0.4818648397922516
Blogs,8,182,results,results,has,ensemblers,results has ensemblers,0.5937036275863647
Blogs,8,183,results,sentence,superior to,averaging,sentence superior to averaging,0.7115266919136047
Blogs,8,183,results,simple features,that model,global context and positional information,simple features that model global context and positional information,0.7042762041091919
Blogs,8,183,results,global context and positional information,are,very effective,global context and positional information are very effective,0.5455042719841003
Blogs,8,183,results,automatically generated test set,not guarantee,high rouge - l score,automatically generated test set not guarantee high rouge - l score,0.7244046330451965
Blogs,8,183,results,reading,has,sentence,reading has sentence,0.5682310461997986
Blogs,8,183,results,reading,has,sequentially,reading has sequentially,0.6503971815109253
Blogs,8,183,results,sentence,has,sequentially,sentence has sequentially,0.6508120894432068
Blogs,8,183,results,averaging,has,its word vectors,averaging has its word vectors,0.6016177535057068
Blogs,8,188,results,performance,from using,unexpanded dataset,performance from using unexpanded dataset,0.6674370765686035
Blogs,8,188,results,results,has,fnet summariser and sfnet,results has fnet summariser and sfnet,0.5884485840797424
Blogs,8,193,results,ab-stractrouge metric,does,improve,ab-stractrouge metric does improve,0.32530659437179565
Blogs,8,193,results,ab-stractrouge metric,learning,representation,ab-stractrouge metric learning representation,0.7228425741195679
Blogs,8,193,results,performance,for,summarisation techniques,performance for summarisation techniques,0.62624591588974
Blogs,8,193,results,summarisation techniques,based only on,feature engineering,summarisation techniques based only on feature engineering,0.5885000824928284
Blogs,8,193,results,improve,has,performance,improve has performance,0.5578044652938843
Blogs,8,193,results,features,has,results,features has results,0.617753803730011
Blogs,9,158,ablation-analysis,encoder,to,roberta - base,encoder to roberta - base,0.5668755769729614
Blogs,9,158,ablation-analysis,encoder,has,performance,encoder has performance,0.5466384291648865
Blogs,9,158,ablation-analysis,roberta - base,has,performance,roberta - base has performance,0.6190093755722046
Blogs,9,158,ablation-analysis,ablation analysis,change,encoder,ablation analysis change encoder,0.6512295007705688
Blogs,9,141,experimental-setup,base version of bert,to implement,our models,base version of bert to implement our models,0.7360849976539612
Blogs,9,141,experimental-setup,experimental setup,use,base version of bert,experimental setup use base version of bert,0.6363224983215332
Blogs,9,142,experimental-setup,adam optimizer,with,warming - up,adam optimizer with warming - up,0.6577191352844238
Blogs,9,142,experimental-setup,batch size,of,32,batch size of 32,0.6741614937782288
Blogs,9,142,experimental-setup,experimental setup,has,adam optimizer,experimental setup has adam optimizer,0.5293667316436768
Blogs,9,147,experimental-setup,siamese - bert model,on,cnn / dm,siamese - bert model on cnn / dm,0.5491845607757568
Blogs,9,147,experimental-setup,siamese - bert model,use,8 tesla- v100 - 16g gpus,siamese - bert model use 8 tesla- v100 - 16g gpus,0.6017186641693115
Blogs,9,147,experimental-setup,8 tesla- v100 - 16g gpus,for about 30 hours,training,8 tesla- v100 - 16g gpus for about 30 hours training,0.7521969676017761
Blogs,9,147,experimental-setup,experimental setup,To obtain,siamese - bert model,experimental setup To obtain siamese - bert model,0.582886278629303
Blogs,9,30,model,extractive summarization,as,semantic text matching problem,extractive summarization as semantic text matching problem,0.47854694724082947
Blogs,9,30,model,model,conceptualize,extractive summarization,model conceptualize extractive summarization,0.6511373519897461
Blogs,9,34,model,extractive summarization,propose,siamese - bert architecture,extractive summarization propose siamese - bert architecture,0.6283749938011169
Blogs,9,34,model,siamese - bert architecture,to compute,similarity,siamese - bert architecture to compute similarity,0.7369126677513123
Blogs,9,34,model,similarity,between,source document,similarity between source document,0.5861660838127136
Blogs,9,34,model,similarity,between,candidate summary,similarity between candidate summary,0.6703184843063354
Blogs,9,34,model,model,Specific to,extractive summarization,model Specific to extractive summarization,0.6726552248001099
Blogs,9,34,model,model,propose,siamese - bert architecture,model propose siamese - bert architecture,0.6838381290435791
Blogs,9,35,model,siamese bert,leverages,"pre-trained bert ( devlin et al. , 2019 )","siamese bert leverages pre-trained bert ( devlin et al. , 2019 )",0.663781464099884
Blogs,9,35,model,"pre-trained bert ( devlin et al. , 2019 )",in,siamese network structure,"pre-trained bert ( devlin et al. , 2019 ) in siamese network structure",0.48874184489250183
Blogs,9,35,model,semantically meaningful text embeddings,compared using,cosine-similarity,semantically meaningful text embeddings compared using cosine-similarity,0.6181989908218384
Blogs,9,35,model,model,has,siamese bert,model has siamese bert,0.5858060121536255
Blogs,9,45,results,state - of- the - art extractive result,on,cnn / dailymail ( 44.41 in rouge - 1 ),state - of- the - art extractive result on cnn / dailymail ( 44.41 in rouge - 1 ),0.4579637944698334
Blogs,9,45,results,cnn / dailymail ( 44.41 in rouge - 1 ),by only using,base version of bert,cnn / dailymail ( 44.41 in rouge - 1 ) by only using base version of bert,0.7676202058792114
Blogs,9,45,results,results,obtain,state - of- the - art extractive result,results obtain state - of- the - art extractive result,0.5579274892807007
Blogs,9,152,results,trigram blocking,is,simple yet effective heuristic,trigram blocking is simple yet effective heuristic,0.5714223980903625
Blogs,9,152,results,simple yet effective heuristic,on,cnn / dm,simple yet effective heuristic on cnn / dm,0.5570305585861206
Blogs,9,152,results,all redundancy removal methods,based on,neural models,all redundancy removal methods based on neural models,0.6426535248756409
Blogs,9,152,results,results,has,trigram blocking,results has trigram blocking,0.538478434085846
Blogs,9,154,results,results,on,cnn / dm,results on cnn / dm,0.548747181892395
Blogs,9,155,results,outperformed,by,large margin,outperformed by large margin,0.6321641206741333
Blogs,9,155,results,all competitors,by,large margin,all competitors by large margin,0.6121708154678345
Blogs,9,155,results,proposed matchsum,has,outperformed,proposed matchsum has outperformed,0.6342592835426331
Blogs,9,155,results,outperformed,has,all competitors,outperformed has all competitors,0.6222853660583496
Blogs,9,156,results,bertext,by,1.51 rouge - 1 score,bertext by 1.51 rouge - 1 score,0.5448479056358337
Blogs,9,156,results,bertext,when using,bert - base as the encoder,bertext when using bert - base as the encoder,0.7306499481201172
Blogs,9,160,results,superior performance,effectiveness of,proposed matching framework,superior performance effectiveness of proposed matching framework,0.6829817295074463
Blogs,9,160,results,results,has,superior performance,results has superior performance,0.5873436331748962
Blogs,9,166,results,number of sentences,increases to,two,number of sentences increases to two,0.7633793950080872
Blogs,9,166,results,summary - level semantics,taken into,account,summary - level semantics taken into account,0.5913366079330444
Blogs,9,166,results,matchsum,obtain,more remarkable improvement,matchsum obtain more remarkable improvement,0.5753932595252991
Blogs,9,166,results,more remarkable improvement,compared to,bertext ( num = 2 ),more remarkable improvement compared to bertext ( num = 2 ),0.6877470016479492
Blogs,9,166,results,1.04 ?r - 1,on,reddit,1.04 ?r - 1 on reddit,0.5636592507362366
Blogs,9,166,results,1.62 ?r - 1,on,xsum,1.62 ?r - 1 on xsum,0.6204040050506592
Blogs,9,166,results,results,when,number of sentences,results when number of sentences,0.6075831651687622
Blogs,9,172,results,trigram blocking,works,well,trigram blocking works well,0.6134951114654541
Blogs,9,172,results,trigram blocking,not always maintain,stable improvement,trigram blocking not always maintain stable improvement,0.7018967866897583
Blogs,9,172,results,well,on,cnn / dm,well on cnn / dm,0.580892026424408
Blogs,9,172,results,cnn / dm,not always maintain,stable improvement,cnn / dm not always maintain stable improvement,0.7070764303207397
Blogs,9,172,results,results,presents,trigram blocking,results presents trigram blocking,0.6357791423797607
Blogs,9,173,results,ngram blocking,causes,large performance drop,ngram blocking causes large performance drop,0.6545063257217407
Blogs,9,173,results,little effect,on,wikihow and multi-news,little effect on wikihow and multi-news,0.5616491436958313
Blogs,9,173,results,large performance drop,on,pubmed,large performance drop on pubmed,0.6132296323776245
Blogs,9,173,results,ngram blocking,has,little effect,ngram blocking has little effect,0.6361730694770813
Blogs,9,173,results,results,has,ngram blocking,results has ngram blocking,0.5447913408279419
Blogs,10,76,ablation-analysis,high impact,on,performance,high impact on performance,0.5840471982955933
Blogs,10,76,ablation-analysis,redundancy,has,high impact,redundancy has high impact,0.6119723916053772
Blogs,10,76,ablation-analysis,ablation analysis,observe,redundancy,ablation analysis observe redundancy,0.6261307597160339
Blogs,10,62,baselines,most-entity ( me ),greedily picks,sentences,most-entity ( me ) greedily picks sentences,0.5513473153114319
Blogs,10,62,baselines,sentences,with,most medical entities,sentences with most medical entities,0.5521931648254395
Blogs,10,63,baselines,tf - idf,weights,sentences,tf - idf weights sentences,0.7125120162963867
Blogs,10,63,baselines,sentences,using,sum of words ' tf - idf scores,sentences using sum of words ' tf - idf scores,0.6388083696365356
Blogs,10,64,baselines,duplicate information,incorporated,maximal marginal relevance ( mmr ),duplicate information incorporated maximal marginal relevance ( mmr ),0.6526304483413696
Blogs,10,64,baselines,maximal marginal relevance ( mmr ),with,tf - idf,maximal marginal relevance ( mmr ) with tf - idf,0.61802077293396
Blogs,10,70,hyperparameters,l 1 and l 2,set as,| d i |/5.5 and | d i |/4,l 1 and l 2 set as | d i |/5.5 and | d i |/4,0.6444197297096252
Blogs,10,70,hyperparameters,ilp,has,l 1 and l 2,ilp has l 1 and l 2,0.6049425601959229
Blogs,10,70,hyperparameters,hyperparameters,For,ilp,hyperparameters For ilp,0.6043009161949158
Blogs,10,71,hyperparameters,word embedding and bi-gru,have,200 dimensions,word embedding and bi-gru have 200 dimensions,0.5708019733428955
Blogs,10,71,hyperparameters,hyperparameters,For,neural model,hyperparameters For neural model,0.5670974850654602
Blogs,10,72,hyperparameters,batch size,is,16,batch size is 16,0.643535315990448
Blogs,10,72,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
Blogs,10,73,hyperparameters,adam,as,optimizer,adam as optimizer,0.5250696539878845
Blogs,10,73,hyperparameters,optimizer,with,learning rate,optimizer with learning rate,0.6228271722793579
Blogs,10,73,hyperparameters,learning rate,has,10 ?3,learning rate has 10 ?3,0.6000961661338806
Blogs,10,73,hyperparameters,hyperparameters,used,adam,hyperparameters used adam,0.6175551414489746
Blogs,10,74,results,results,for,most-entity,results for most-entity,0.6146423816680908
Blogs,10,75,results,our method,achieved,best performance,our method achieved best performance,0.6870955228805542
Blogs,10,75,results,best performance,over,all three metrics,best performance over all three metrics,0.6320486068725586
Blogs,10,75,results,results,has,our method,results has our method,0.5589964985847473
Blogs,10,77,results,mmr and novelty,both,tf - idf and ours,mmr and novelty both tf - idf and ours,0.6746402382850647
Blogs,10,77,results,tf - idf and ours,degraded,significantly,tf - idf and ours degraded significantly,0.6904686093330383
Blogs,10,77,results,significantly,in,performance,significantly in performance,0.570052981376648
Blogs,10,77,results,results,Without,mmr and novelty,results Without mmr and novelty,0.5961970686912537
Blogs,10,78,results,position term,improves,performance,position term improves performance,0.7272930145263672
Blogs,10,78,results,performance,of,ours,performance of ours,0.6501802802085876
Blogs,10,78,results,results,notice,position term,results notice position term,0.7577414512634277
Blogs,11,76,ablation-analysis,high impact,on,performance,high impact on performance,0.5840471982955933
Blogs,11,76,ablation-analysis,redundancy,has,high impact,redundancy has high impact,0.6119723916053772
Blogs,11,76,ablation-analysis,ablation analysis,observe,redundancy,ablation analysis observe redundancy,0.6261307597160339
Blogs,11,62,baselines,most-entity ( me ),greedily picks,sentences,most-entity ( me ) greedily picks sentences,0.5513473153114319
Blogs,11,62,baselines,sentences,with,most medical entities,sentences with most medical entities,0.5521931648254395
Blogs,11,63,baselines,tf - idf,weights,sentences,tf - idf weights sentences,0.7125120162963867
Blogs,11,63,baselines,sentences,using,sum of words ' tf - idf scores,sentences using sum of words ' tf - idf scores,0.6388083696365356
Blogs,11,64,baselines,duplicate information,incorporated,maximal marginal relevance ( mmr ),duplicate information incorporated maximal marginal relevance ( mmr ),0.6526304483413696
Blogs,11,64,baselines,maximal marginal relevance ( mmr ),with,tf - idf,maximal marginal relevance ( mmr ) with tf - idf,0.61802077293396
Blogs,11,70,hyperparameters,l 1 and l 2,set as,| d i |/5.5 and | d i |/4,l 1 and l 2 set as | d i |/5.5 and | d i |/4,0.6444197297096252
Blogs,11,70,hyperparameters,ilp,has,l 1 and l 2,ilp has l 1 and l 2,0.6049425601959229
Blogs,11,70,hyperparameters,hyperparameters,For,ilp,hyperparameters For ilp,0.6043009161949158
Blogs,11,71,hyperparameters,word embedding and bi-gru,have,200 dimensions,word embedding and bi-gru have 200 dimensions,0.5708019733428955
Blogs,11,71,hyperparameters,hyperparameters,For,neural model,hyperparameters For neural model,0.5670974850654602
Blogs,11,72,hyperparameters,batch size,is,16,batch size is 16,0.643535315990448
Blogs,11,72,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
Blogs,11,73,hyperparameters,adam,as,optimizer,adam as optimizer,0.5250696539878845
Blogs,11,73,hyperparameters,optimizer,with,learning rate,optimizer with learning rate,0.6228271722793579
Blogs,11,73,hyperparameters,learning rate,has,10 ?3,learning rate has 10 ?3,0.6000961661338806
Blogs,11,73,hyperparameters,hyperparameters,used,adam,hyperparameters used adam,0.6175551414489746
Blogs,11,74,results,results,for,most-entity,results for most-entity,0.6146423816680908
Blogs,11,75,results,our method,achieved,best performance,our method achieved best performance,0.6870955228805542
Blogs,11,75,results,best performance,over,all three metrics,best performance over all three metrics,0.6320486068725586
Blogs,11,75,results,results,has,our method,results has our method,0.5589964985847473
Blogs,11,77,results,mmr and novelty,both,tf - idf and ours,mmr and novelty both tf - idf and ours,0.6746402382850647
Blogs,11,77,results,tf - idf and ours,degraded,significantly,tf - idf and ours degraded significantly,0.6904686093330383
Blogs,11,77,results,significantly,in,performance,significantly in performance,0.570052981376648
Blogs,11,77,results,results,Without,mmr and novelty,results Without mmr and novelty,0.5961970686912537
Blogs,11,78,results,position term,improves,performance,position term improves performance,0.7272930145263672
Blogs,11,78,results,performance,of,ours,performance of ours,0.6501802802085876
Blogs,11,78,results,results,notice,position term,results notice position term,0.7577414512634277
Blogs,12,140,results,quality,of,section - based summaries,quality of section - based summaries,0.5722014307975769
Blogs,12,140,results,section - agnostic summaries,on,all 3 tasks,section - agnostic summaries on all 3 tasks,0.5103369951248169
Blogs,12,140,results,section - based summaries,has,significantly outperforms,section - based summaries has significantly outperforms,0.6136730313301086
Blogs,12,140,results,significantly outperforms,has,section - agnostic summaries,significantly outperforms has section - agnostic summaries,0.6087973117828369
Blogs,12,140,results,results,has,quality,results has quality,0.38800710439682007
Blogs,13,84,ablation-analysis,base model,on,original datasets,base model on original datasets,0.5403438806533813
Blogs,13,84,ablation-analysis,base model,observed,performance,base model observed performance,0.7367082834243774
Blogs,13,84,ablation-analysis,drops,by,"0.60 , 0.72 bleu","drops by 0.60 , 0.72 bleu",0.6195300221443176
Blogs,13,84,ablation-analysis,drops,by,"1.66 , 2.09 rouge -l points","drops by 1.66 , 2.09 rouge -l points",0.6041717529296875
Blogs,13,84,ablation-analysis,"1.66 , 2.09 rouge -l points",for,java and python datasets,"1.66 , 2.09 rouge -l points for java and python datasets",0.5466321110725403
Blogs,13,84,ablation-analysis,performance,has,drops,performance has drops,0.5993483662605286
Blogs,13,84,ablation-analysis,ablation analysis,ran,base model,ablation analysis ran base model,0.6364228129386902
Blogs,13,94,ablation-analysis,empirical results,suggest,directional information,empirical results suggest directional information,0.5669543147087097
Blogs,13,94,ablation-analysis,directional information,is,important,directional information is important,0.582560658454895
Blogs,13,94,ablation-analysis,directional information,indeed,important,directional information indeed important,0.6510790586471558
Blogs,13,94,ablation-analysis,"16 , 32 , and 2 i relative distances",result in,similar performance,"16 , 32 , and 2 i relative distances result in similar performance",0.6660940051078796
Blogs,13,94,ablation-analysis,ablation analysis,has,empirical results,ablation analysis has empirical results,0.5157642364501953
Blogs,13,108,ablation-analysis,frequent tokens,in,code snippet,frequent tokens in code snippet,0.5055884122848511
Blogs,13,108,ablation-analysis,frequent tokens,get,higher copy probability,frequent tokens get higher copy probability,0.5873277187347412
Blogs,13,108,ablation-analysis,code snippet,get,higher copy probability,code snippet get higher copy probability,0.5946073532104492
Blogs,13,108,ablation-analysis,higher copy probability,when,relative position representations,higher copy probability when relative position representations,0.6473787426948547
Blogs,13,108,ablation-analysis,relative position representations,in comparison to,absolute position representations,relative position representations in comparison to absolute position representations,0.6280242800712585
Blogs,13,108,ablation-analysis,copy enabled model,has,frequent tokens,copy enabled model has frequent tokens,0.5883552432060242
Blogs,13,108,ablation-analysis,ablation analysis,observe,copy enabled model,ablation analysis observe copy enabled model,0.6132128238677979
Blogs,13,93,experiments,directional information,while modeling,pairwise relationship,directional information while modeling pairwise relationship,0.7241706848144531
Blogs,13,93,experiments,clipping distance,has,k,clipping distance has k,0.5786294341087341
Blogs,13,75,hyperparameters,transformer models,using,"adam optimizer ( kingma and ba , 2015 )","transformer models using adam optimizer ( kingma and ba , 2015 )",0.6587966084480286
Blogs,13,75,hyperparameters,"adam optimizer ( kingma and ba , 2015 )",with,initial learning rate,"adam optimizer ( kingma and ba , 2015 ) with initial learning rate",0.5927683115005493
Blogs,13,75,hyperparameters,initial learning rate,of,10 ?4,initial learning rate of 10 ?4,0.6392531991004944
Blogs,13,75,hyperparameters,hyperparameters,train,transformer models,hyperparameters train transformer models,0.6927379369735718
Blogs,13,76,hyperparameters,mini-batch size and dropout rate,to,32 and 0.2,mini-batch size and dropout rate to 32 and 0.2,0.5564597845077515
Blogs,13,76,hyperparameters,hyperparameters,set,mini-batch size and dropout rate,hyperparameters set mini-batch size and dropout rate,0.6019110679626465
Blogs,13,77,hyperparameters,transformer models,for,maximum,transformer models for maximum,0.626340925693512
Blogs,13,77,hyperparameters,maximum,of,200 epochs,maximum of 200 epochs,0.587211549282074
Blogs,13,77,hyperparameters,early stop,if,validation performance,early stop if validation performance,0.6208859086036682
Blogs,13,77,hyperparameters,does not improve,for,20 consecutive iterations,does not improve for 20 consecutive iterations,0.6248425245285034
Blogs,13,77,hyperparameters,validation performance,has,does not improve,validation performance has does not improve,0.610643744468689
Blogs,13,77,hyperparameters,hyperparameters,train,transformer models,hyperparameters train transformer models,0.6927379369735718
Blogs,13,78,hyperparameters,beam search,during,inference,beam search during inference,0.6603623628616333
Blogs,13,78,hyperparameters,beam search,set,beam size,beam search set beam size,0.6893483400344849
Blogs,13,78,hyperparameters,beam size,to,4,beam size to 4,0.6383150219917297
Blogs,13,78,hyperparameters,hyperparameters,use,beam search,hyperparameters use beam search,0.6655463576316833
Blogs,13,78,hyperparameters,hyperparameters,set,beam size,hyperparameters set beam size,0.6436296701431274
Blogs,13,6,model,code representation,explore,transformer model,code representation explore transformer model,0.652454674243927
Blogs,13,6,model,transformer model,uses,self-attention mechanism,transformer model uses self-attention mechanism,0.5384237766265869
Blogs,13,6,model,model,To learn,code representation,model To learn code representation,0.5770130157470703
Blogs,13,23,model,pairwise relationship,between,source code tokens,pairwise relationship between source code tokens,0.6420106887817383
Blogs,13,23,model,pairwise relationship,achieve,significant improvements,pairwise relationship achieve significant improvements,0.6355451941490173
Blogs,13,23,model,source code tokens,using,relative position representation,source code tokens using relative position representation,0.6249630451202393
Blogs,13,23,model,significant improvements,over learning,sequence information,significant improvements over learning sequence information,0.6851058006286621
Blogs,13,23,model,sequence information,of,code tokens,sequence information of code tokens,0.5729379057884216
Blogs,13,23,model,sequence information,using,absolute position representation,sequence information using absolute position representation,0.6518321633338928
Blogs,13,23,model,code tokens,using,absolute position representation,code tokens using absolute position representation,0.6655691862106323
Blogs,13,23,model,model,modeling,pairwise relationship,model modeling pairwise relationship,0.831407904624939
Blogs,13,23,model,model,achieve,significant improvements,model achieve significant improvements,0.6491693258285522
Blogs,13,33,model,each layer,performs,self-attention mechanism,each layer performs self-attention mechanism,0.6081052422523499
Blogs,13,33,model,multi-head attention,employs,h attention heads,multi-head attention employs h attention heads,0.5646092295646667
Blogs,13,33,model,multi-head attention,performs,self-attention mechanism,multi-head attention performs self-attention mechanism,0.555252730846405
Blogs,13,33,model,each layer,has,multi-head attention,each layer has multi-head attention,0.564212441444397
Blogs,13,33,model,model,At,each layer,model At each layer,0.5384435057640076
Blogs,13,39,model,additional attention layer,to learn,copy distribution,additional attention layer to learn copy distribution,0.6287062764167786
Blogs,13,39,model,copy distribution,on top of,decoder stack,copy distribution on top of decoder stack,0.7032915949821472
Blogs,13,39,model,model,use,additional attention layer,model use additional attention layer,0.6693074703216553
Blogs,13,82,results,base model,has,outperforms,base model has outperforms,0.6523337364196777
Blogs,13,82,results,outperforms,has,baselines,outperforms has baselines,0.6144351959228516
Blogs,13,82,results,full model,has,improves,full model has improves,0.613627016544342
Blogs,13,82,results,improves,has,performance,improves has performance,0.5770372748374939
Blogs,13,82,results,performance,has,further,performance has further,0.6324895024299622
Blogs,13,82,results,results,shows,base model,results shows base model,0.7049206495285034
Blogs,13,87,results,copy attention,improves,performance,copy attention improves performance,0.672670304775238
Blogs,13,87,results,0.44 and 0.88 bleu points,for,java and python datasets,0.44 and 0.88 bleu points for java and python datasets,0.5855094790458679
Blogs,13,87,results,performance,has,0.44 and 0.88 bleu points,performance has 0.44 and 0.88 bleu points,0.5721708536148071
Blogs,13,90,results,absolute position,of,code tokens,absolute position of code tokens,0.5847769975662231
Blogs,13,90,results,absolute position,are,not effective,absolute position are not effective,0.5933716893196106
Blogs,13,90,results,learning,has,absolute position,learning has absolute position,0.5807147026062012
Blogs,13,90,results,learning,has,slightly hurts,learning has slightly hurts,0.5678166747093201
Blogs,13,90,results,slightly hurts,has,performance,slightly hurts has performance,0.5844202637672424
Blogs,13,90,results,results,demonstrates,learning,results demonstrates learning,0.6956058144569397
Blogs,13,92,results,pairwise relationship,between,source code tokens,pairwise relationship between source code tokens,0.6420106887817383
Blogs,13,92,results,pairwise relationship,via,relative position representations,pairwise relationship via relative position representations,0.7033926844596863
Blogs,13,92,results,source code tokens,via,relative position representations,source code tokens via relative position representations,0.6234752535820007
Blogs,13,92,results,results,learning,pairwise relationship,results learning pairwise relationship,0.6871828436851501
Blogs,13,107,results,qualitative analysis,reveals,copy enabled model,qualitative analysis reveals copy enabled model,0.6584791541099548
Blogs,13,107,results,qualitative analysis,in comparison to,vanilla transformer model,qualitative analysis in comparison to vanilla transformer model,0.6606775522232056
Blogs,13,107,results,copy enabled model,generates,shorter summaries,copy enabled model generates shorter summaries,0.7048214077949524
Blogs,13,107,results,shorter summaries,with,more accurate keywords,shorter summaries with more accurate keywords,0.5815410017967224
Blogs,13,107,results,qualitative analysis,has,copy enabled model,qualitative analysis has copy enabled model,0.5888272523880005
Blogs,13,107,results,vanilla transformer model,has,copy enabled model,vanilla transformer model has copy enabled model,0.6122305989265442
Blogs,13,107,results,results,has,qualitative analysis,results has qualitative analysis,0.47837314009666443
Blogs,14,46,baselines,supert,introducing,pseudo references,supert introducing pseudo references,0.7018107771873474
Blogs,14,46,baselines,baselines,has,supert,baselines has supert,0.6091812252998352
Blogs,14,55,baselines,tf - idf,computes,cosine similarity,tf - idf computes cosine similarity,0.6742845177650452
Blogs,14,55,baselines,tf - idf,computes,js divergence,tf - idf computes js divergence,0.7158330082893372
Blogs,14,55,baselines,cosine similarity,of,tf-idf vectors,cosine similarity of tf-idf vectors,0.5756504535675049
Blogs,14,55,baselines,tf-idf vectors,of,source and summaries,tf-idf vectors of source and summaries,0.5702221393585205
Blogs,14,55,baselines,js,computes,js divergence,js computes js divergence,0.7040839195251465
Blogs,14,55,baselines,js divergence,between,words distributions,js divergence between words distributions,0.6349687576293945
Blogs,14,55,baselines,words distributions,in,source documents and summaries,words distributions in source documents and summaries,0.49715298414230347
Blogs,14,55,baselines,baselines,consider,tf - idf,baselines consider tf - idf,0.5907918214797974
Blogs,14,93,baselines,sbert - based lexrank ( slr ),extends,"classic lexrank ( erkan and radev , 2004 ) method","sbert - based lexrank ( slr ) extends classic lexrank ( erkan and radev , 2004 ) method",0.6941186785697937
Blogs,14,93,baselines,"classic lexrank ( erkan and radev , 2004 ) method",by measuring,similarity,"classic lexrank ( erkan and radev , 2004 ) method by measuring similarity",0.6825538873672485
Blogs,14,93,baselines,similarity,using,sbert embeddings cosine similarity,similarity using sbert embeddings cosine similarity,0.599452018737793
Blogs,14,93,baselines,of sentences,using,sbert embeddings cosine similarity,of sentences using sbert embeddings cosine similarity,0.5043224692344666
Blogs,14,93,baselines,similarity,has,of sentences,similarity has of sentences,0.6056612133979797
Blogs,14,96,baselines,individual - graph version,builds,graph,individual - graph version builds graph,0.668895959854126
Blogs,14,96,baselines,individual - graph version,builds,graph,individual - graph version builds graph,0.668895959854126
Blogs,14,96,baselines,individual - graph version,selects,centers ( sc ),individual - graph version selects centers ( sc ),0.7194364070892334
Blogs,14,96,baselines,graph,for,each source document,graph for each source document,0.6206992268562317
Blogs,14,96,baselines,graph,considering,all sentences,graph considering all sentences,0.668873131275177
Blogs,14,96,baselines,centers ( sc ),from,each graph,centers ( sc ) from each graph,0.5784628391265869
Blogs,14,96,baselines,global - graph version,builds,graph,global - graph version builds graph,0.6676681637763977
Blogs,14,96,baselines,graph,considering,all sentences,graph considering all sentences,0.668873131275177
Blogs,14,96,baselines,all sentences,across,all source documents,all sentences across all source documents,0.667917788028717
Blogs,14,117,baselines,different rewards,to guide,neural temporal difference ( ntd ),different rewards to guide neural temporal difference ( ntd ),0.6466925740242004
Blogs,14,117,baselines,different rewards,to guide,rl - based multi-document summarizer,different rewards to guide rl - based multi-document summarizer,0.6254227161407471
Blogs,14,117,baselines,different rewards,has,rl - based multi-document summarizer,different rewards has rl - based multi-document summarizer,0.5297200679779053
Blogs,14,117,baselines,neural temporal difference ( ntd ),has,rl - based multi-document summarizer,neural temporal difference ( ntd ) has rl - based multi-document summarizer,0.5381911396980286
Blogs,14,118,baselines,best version of supert,selects,top 10 ( tac '08 ) or 15 ( tac '09 ) sentences,best version of supert selects top 10 ( tac '08 ) or 15 ( tac '09 ) sentences,0.683967649936676
Blogs,14,118,baselines,top 10 ( tac '08 ) or 15 ( tac '09 ) sentences,from,each source document,top 10 ( tac '08 ) or 15 ( tac '09 ) sentences from each source document,0.530935525894165
Blogs,14,118,baselines,top 10 ( tac '08 ) or 15 ( tac '09 ) sentences,to build,pseudo references,top 10 ( tac '08 ) or 15 ( tac '09 ) sentences to build pseudo references,0.6594849228858948
Blogs,14,118,baselines,sbert,to measure,similarity,sbert to measure similarity,0.7227513194084167
Blogs,14,118,baselines,similarity,between,summaries and pseudo references,similarity between summaries and pseudo references,0.6584309935569763
Blogs,14,118,baselines,baselines,consider,three unsupervised reward functions,baselines consider three unsupervised reward functions,0.5975176095962524
Blogs,14,5,model,supert,quality of,summary,supert quality of summary,0.6168414950370789
Blogs,14,5,model,semantic similarity,with,pseudo reference summary,semantic similarity with pseudo reference summary,0.6198081970214844
Blogs,14,5,model,pseudo reference summary,using,contextualized embeddings,pseudo reference summary using contextualized embeddings,0.6324365139007568
Blogs,14,5,model,pseudo reference summary,using,soft token alignment techniques,pseudo reference summary using soft token alignment techniques,0.6151708364486694
Blogs,14,5,model,model,propose,supert,model propose supert,0.6890323162078857
Blogs,14,7,model,supert,as,rewards,supert as rewards,0.5987292528152466
Blogs,14,7,model,rewards,to guide,neural - based reinforcement learning summarizer,rewards to guide neural - based reinforcement learning summarizer,0.6813102960586548
Blogs,14,7,model,neural - based reinforcement learning summarizer,yielding,favorable performance,neural - based reinforcement learning summarizer yielding favorable performance,0.6233047842979431
Blogs,14,7,model,favorable performance,compared to,state - of - the - art unsupervised summarizers,favorable performance compared to state - of - the - art unsupervised summarizers,0.6034529805183411
Blogs,14,7,model,model,use,supert,model use supert,0.6548197865486145
Blogs,14,22,model,supert,as,reward functions,supert as reward functions,0.5633699893951416
Blogs,14,22,model,reward functions,to guide,reinforcement learning ( rl ) based extractive summarizers,reward functions to guide reinforcement learning ( rl ) based extractive summarizers,0.6464831233024597
Blogs,14,22,model,model,use,supert,model use supert,0.6548197865486145
Blogs,14,91,model,two families of graph - based methods,to build,pseudo references,two families of graph - based methods to build pseudo references,0.5844222903251648
Blogs,14,91,model,model,explore,two families of graph - based methods,model explore two families of graph - based methods,0.6562165021896362
Blogs,14,94,model,sbert - based clustering ( sc ) method,to build,graphs,sbert - based clustering ( sc ) method to build graphs,0.721961498260498
Blogs,14,94,model,graphs,first measures,similarity,graphs first measures similarity,0.5831431746482849
Blogs,14,94,model,graphs,clusters,sentences,graphs clusters sentences,0.815539538860321
Blogs,14,94,model,similarity,of,sentence pairs,similarity of sentence pairs,0.5906773805618286
Blogs,14,94,model,sentence pairs,using,sbert,sentence pairs using sbert,0.6907463073730469
Blogs,14,94,model,sentences,by using,"affinity propagation ( frey and dueck , 2007 ) clustering algorithm","sentences by using affinity propagation ( frey and dueck , 2007 ) clustering algorithm",0.6598808765411377
Blogs,14,94,model,model,propose,sbert - based clustering ( sc ) method,model propose sbert - based clustering ( sc ) method,0.6734338402748108
Blogs,14,102,model,pacsum,by using,sbert,pacsum by using sbert,0.6034140586853027
Blogs,14,102,model,sbert,to measure,sentences similarity,sbert to measure sentences similarity,0.6685757637023926
Blogs,14,102,model,individualand global - graph versions,of,sps,individualand global - graph versions of sps,0.6239343881607056
Blogs,14,102,model,model,extend,pacsum,model extend pacsum,0.7036224603652954
Blogs,14,63,results,embedding - based methods ( b?hm19 and c elmo ),perform,worse,embedding - based methods ( b?hm19 and c elmo ) perform worse,0.60517817735672
Blogs,14,63,results,worse,than,other lexical - based baselines,worse than other lexical - based baselines,0.571521520614624
Blogs,14,63,results,baselines,has,embedding - based methods ( b?hm19 and c elmo ),baselines has embedding - based methods ( b?hm19 and c elmo ),0.5403566360473633
Blogs,14,63,results,results,Among,baselines,results Among baselines,0.6001414060592651
Blogs,14,65,results,embeddings,of,source documents and the summaries,embeddings of source documents and the summaries,0.5765925049781799
Blogs,14,65,results,embeddings,yields,higher correlation,embeddings yields higher correlation,0.7615137696266174
Blogs,14,65,results,higher correlation,computing,embeddings,higher correlation computing embeddings,0.7757648229598999
Blogs,14,65,results,results,Soft aligning,embeddings,results Soft aligning embeddings,0.5898382067680359
Blogs,14,73,results,outperforms,by,large margin,outperforms by large margin,0.6290680766105652
Blogs,14,73,results,outperforms,compared to,lexical - based metrics,outperforms compared to lexical - based metrics,0.6659675240516663
Blogs,14,73,results,other cosine-embedding based metrics,by,large margin,other cosine-embedding based metrics by large margin,0.5291407108306885
Blogs,14,73,results,outperforms,has,other cosine-embedding based metrics,outperforms has other cosine-embedding based metrics,0.5806679129600525
Blogs,14,73,results,lexical - based metrics,has,performance,lexical - based metrics has performance,0.5674685835838318
Blogs,14,73,results,performance,has,falls short,performance has falls short,0.5906819701194763
Blogs,14,80,results,wmd - based scores,has,substantially outperform,wmd - based scores has substantially outperform,0.5729251503944397
Blogs,14,80,results,substantially outperform,has,cosine-embedding counterparts,substantially outperform has cosine-embedding counterparts,0.5550926923751831
Blogs,14,80,results,outperforms,has,all lexical - based baselines,outperforms has all lexical - based baselines,0.5908561944961548
Blogs,14,80,results,results,has,wmd - based scores,results has wmd - based scores,0.488871693611145
Blogs,14,87,results,top 10 - 15 sentences,as,pseudo references,top 10 - 15 sentences as pseudo references,0.4860195815563202
Blogs,14,87,results,outperforms,by,over 16 %,outperforms by over 16 %,0.6206808686256409
Blogs,14,87,results,lexical - based baselines,by,over 16 %,lexical - based baselines by over 16 %,0.5851548314094543
Blogs,14,87,results,m sbert,by,over 4 %,m sbert by over 4 %,0.641565203666687
Blogs,14,87,results,outperforms,has,lexical - based baselines,outperforms has lexical - based baselines,0.59810471534729
Blogs,14,87,results,results,extracting,top 10 - 15 sentences,results extracting top 10 - 15 sentences,0.6493330001831055
Blogs,14,113,results,positionagnostic graph - based methods,perform,worse,positionagnostic graph - based methods perform worse,0.6274928450584412
Blogs,14,113,results,worse,than,position - aware ones,worse than position - aware ones,0.6199003458023071
Blogs,14,113,results,sc g,has,all other graph - based methods,sc g has all other graph - based methods,0.5560634136199951
Blogs,14,113,results,all other graph - based methods,has,outperform,all other graph - based methods has outperform,0.563313901424408
Blogs,14,113,results,outperform,has,baselines,outperform has baselines,0.6363358497619629
Blogs,14,113,results,results,Except for,sc g,results Except for sc g,0.6775143146514893
Blogs,14,114,results,position - aware graph - based sentence extraction methods,perform,worse,position - aware graph - based sentence extraction methods perform worse,0.5775576829910278
Blogs,14,114,results,worse,than,extracting,worse than extracting,0.6519818305969238
Blogs,14,114,results,extracting,has,top sentences,extracting has top sentences,0.5466805100440979
Blogs,14,114,results,results,find that,position - aware graph - based sentence extraction methods,results find that position - aware graph - based sentence extraction methods,0.5910676717758179
Blogs,14,122,results,supert,is,strongest reward,supert is strongest reward,0.5708435773849487
Blogs,14,122,results,strongest reward,among,considered rewards,strongest reward among considered rewards,0.5535248517990112
Blogs,14,122,results,ntd,perform,significantly better,ntd perform significantly better,0.6372921466827393
Blogs,14,122,results,perform,on par with,ysl15,perform on par with ysl15,0.7436783909797668
Blogs,14,122,results,ysl15,on,tac '08,ysl15 on tac '08,0.6062890887260437
Blogs,14,122,results,ysl15,on,tac'09,ysl15 on tac'09,0.6112505793571472
Blogs,14,122,results,ysl15,on,tac'09,ysl15 on tac'09,0.6112505793571472
Blogs,14,122,results,significantly better,on,tac'09,significantly better on tac'09,0.6106420755386353
Blogs,14,122,results,ntd,has,perform,ntd has perform,0.6944169998168945
Blogs,14,122,results,results,find,supert,results find supert,0.6390928626060486
Blogs,15,84,baselines,baselines,has,lsa,baselines has lsa,0.5501673817634583
Blogs,15,86,baselines,state- of - the - art extractive model,integrates,process,state- of - the - art extractive model integrates process,0.6188179850578308
Blogs,15,86,baselines,process,of,source sentence scoring and selection,process of source sentence scoring and selection,0.561164140701294
Blogs,15,86,baselines,"neusum ( zhou et al. , 2018 )",has,state- of - the - art extractive model,"neusum ( zhou et al. , 2018 ) has state- of - the - art extractive model",0.5183697938919067
Blogs,15,86,baselines,baselines,has,"neusum ( zhou et al. , 2018 )","baselines has neusum ( zhou et al. , 2018 )",0.5035542845726013
Blogs,15,87,baselines,4,-,pointer-generator ( pg ),4 - pointer-generator ( pg ),0.641194224357605
Blogs,15,87,baselines,4,has,pointer-generator ( pg ),4 has pointer-generator ( pg ),0.5659388303756714
Blogs,15,87,baselines,baselines,has,4,baselines has 4,0.633842408657074
Blogs,15,87,baselines,baselines,has,pointer-generator ( pg ),baselines has pointer-generator ( pg ),0.5477702617645264
Blogs,15,90,baselines,baselines,has,bottom - up summarization ( bus ),baselines has bottom - up summarization ( bus ),0.5614982843399048
Blogs,15,93,hyperparameters,scibert model,pre-trained over,biomedical text,scibert model pre-trained over biomedical text,0.6939518451690674
Blogs,15,93,hyperparameters,hyperparameters,has,parameters,hyperparameters has parameters,0.4783959984779358
Blogs,15,94,hyperparameters,2 - layer bi-lstm encoder,with,hidden size,2 - layer bi-lstm encoder with hidden size,0.5937467813491821
Blogs,15,94,hyperparameters,2 - layer bi-lstm encoder,upon,bert model,2 - layer bi-lstm encoder upon bert model,0.5415715575218201
Blogs,15,94,hyperparameters,hidden size,of,256,hidden size of 256,0.6364261507987976
Blogs,15,94,hyperparameters,256,upon,bert model,256 upon bert model,0.6576036810874939
Blogs,15,94,hyperparameters,hyperparameters,employ,2 - layer bi-lstm encoder,hyperparameters employ 2 - layer bi-lstm encoder,0.5029570460319519
Blogs,15,95,hyperparameters,dropout,set to,0.2,dropout set to 0.2,0.6009831428527832
Blogs,15,95,hyperparameters,hyperparameters,has,dropout,hyperparameters has dropout,0.5324090719223022
Blogs,15,96,hyperparameters,network,to minimize,cross entropy loss function,network to minimize cross entropy loss function,0.6879969239234924
Blogs,15,96,hyperparameters,"adam optimizer ( kingma and ba , 2015 )",with,learning rate,"adam optimizer ( kingma and ba , 2015 ) with learning rate",0.5968794822692871
Blogs,15,96,hyperparameters,learning rate,of,2e ?5,learning rate of 2e ?5,0.6425188183784485
Blogs,15,96,hyperparameters,hyperparameters,train,network,hyperparameters train network,0.7148590087890625
Blogs,15,97,hyperparameters,summarization model,extended on,open base code,summarization model extended on open base code,0.688733696937561
Blogs,15,99,hyperparameters,1 - layer lstm,as,"findings encoder , ontology encoder , and decoder","1 - layer lstm as findings encoder , ontology encoder , and decoder",0.5122153162956238
Blogs,15,99,hyperparameters,"findings encoder , ontology encoder , and decoder",with,hidden sizes,"findings encoder , ontology encoder , and decoder with hidden sizes",0.6019487380981445
Blogs,15,99,hyperparameters,hidden sizes,of,200 and 100,hidden sizes of 200 and 100,0.6715147495269775
Blogs,15,99,hyperparameters,2 - layer bi-lstm,has,1 - layer lstm,2 - layer bi-lstm has 1 - layer lstm,0.5467823147773743
Blogs,15,99,hyperparameters,hyperparameters,use,2 - layer bi-lstm,hyperparameters use 2 - layer bi-lstm,0.5805023312568665
Blogs,15,99,hyperparameters,hyperparameters,use,1 - layer lstm,hyperparameters use 1 - layer lstm,0.5840073823928833
Blogs,15,100,hyperparameters,100d glove embeddings,pretrained on,large collection of 4.5 million radiology reports,100d glove embeddings pretrained on large collection of 4.5 million radiology reports,0.7191155552864075
Blogs,15,100,hyperparameters,hyperparameters,exploit,100d glove embeddings,hyperparameters exploit 100d glove embeddings,0.6306446194648743
Blogs,15,101,hyperparameters,network,to optimize,negative log likelihood,network to optimize negative log likelihood,0.7026439905166626
Blogs,15,101,hyperparameters,network,with,learning rate,network with learning rate,0.6546292901039124
Blogs,15,101,hyperparameters,negative log likelihood,with,adam optimizer,negative log likelihood with adam optimizer,0.592313826084137
Blogs,15,101,hyperparameters,learning rate,of,0.001,learning rate of 0.001,0.5913695693016052
Blogs,15,101,hyperparameters,hyperparameters,train,network,hyperparameters train network,0.7148590087890625
Blogs,15,20,model,copying likelihood,of,word,copying likelihood of word,0.619118869304657
Blogs,15,20,model,saliency,in terms of,forming impression,saliency in terms of forming impression,0.7145130634307861
Blogs,15,20,model,model,pose,copying likelihood,model pose copying likelihood,0.7705417275428772
Blogs,15,37,model,summarizer,refines,findings word representation,summarizer refines findings word representation,0.7167765498161316
Blogs,15,37,model,findings word representation,based on,salient ontology word representation,findings word representation based on salient ontology word representation,0.6458185315132141
Blogs,15,37,model,model,has,summarizer,model has summarizer,0.5916265845298767
Blogs,15,47,model,content selection,train,bi-lstm network,content selection train bi-lstm network,0.6765831708908081
Blogs,15,47,model,bi-lstm network,on top of,bert embeddings,bi-lstm network on top of bert embeddings,0.6723162531852722
Blogs,15,47,model,bert embeddings,with,softmax activation function,bert embeddings with softmax activation function,0.6438261270523071
Blogs,15,85,model,extractive vector-based model,employs,sigular value decomposition ( svd ) concept,extractive vector-based model employs sigular value decomposition ( svd ) concept,0.580182671546936
Blogs,15,88,model,abstractive summarizer,extends,ses2seq networks,abstractive summarizer extends ses2seq networks,0.6455652713775635
Blogs,15,88,model,ses2seq networks,by adding,copy mechanism,ses2seq networks by adding copy mechanism,0.7430791854858398
Blogs,15,88,model,copy mechanism,allows for,directly copying tokens,copy mechanism allows for directly copying tokens,0.7576670050621033
Blogs,15,88,model,directly copying tokens,from,source,directly copying tokens from source,0.6097937226295471
Blogs,15,89,model,pg model,first encodes,entire ontological concepts,pg model first encodes entire ontological concepts,0.6915304064750671
Blogs,15,89,model,pg model,then,encoded vector,pg model then encoded vector,0.5816071629524231
Blogs,15,89,model,pg model,uses,encoded vector,pg model uses encoded vector,0.5978206992149353
Blogs,15,89,model,entire ontological concepts,within,findings,entire ontological concepts within findings,0.6823723316192627
Blogs,15,89,model,encoded vector,to guide,decoder,encoded vector to guide decoder,0.6860427260398865
Blogs,15,89,model,decoder,in,summary decoding process,decoder in summary decoding process,0.5198724865913391
Blogs,15,89,model,model,has,pg model,model has pg model,0.6038079261779785
Blogs,15,21,results,statistically significantly improves,over,competitive baselines,statistically significantly improves over competitive baselines,0.7056632041931152
Blogs,15,21,results,competitive baselines,on,mimic - cxr publicly available clinical dataset,competitive baselines on mimic - cxr publicly available clinical dataset,0.4974953532218933
Blogs,15,21,results,our model,has,statistically significantly improves,our model has statistically significantly improves,0.5861653089523315
Blogs,15,21,results,results,has,our model,results has our model,0.5871725678443909
Blogs,15,102,results,all rouge metrics,with,"2.9 % , 2.5 % , and 1.9 % improvements","all rouge metrics with 2.9 % , 2.5 % , and 1.9 % improvements",0.6224114894866943
Blogs,15,102,results,"2.9 % , 2.5 % , and 1.9 % improvements",for,"rg - 1 , rg - 2 , and rg -l","2.9 % , 2.5 % , and 1.9 % improvements for rg - 1 , rg - 2 , and rg -l",0.6186513900756836
Blogs,15,102,results,results,on,all rouge metrics,results on all rouge metrics,0.4859713613986969
Blogs,15,103,results,non-neural lsa,in,extractive setting,non-neural lsa in extractive setting,0.55256187915802
Blogs,15,103,results,neusum,has,outperforms,neusum has outperforms,0.647962212562561
Blogs,15,103,results,outperforms,has,non-neural lsa,outperforms has non-neural lsa,0.5984530448913574
Blogs,15,103,results,models,has,lag behind,models has lag behind,0.5789695978164673
Blogs,15,103,results,lag behind,has,abstractive methods,lag behind has abstractive methods,0.5508223176002502
Blogs,15,103,results,results,has,neusum,results has neusum,0.5647803544998169
Blogs,15,109,results,bus approach,achieves,best results,bus approach achieves best results,0.6887763738632202
Blogs,15,109,results,best results,among,baseline models,best results among baseline models,0.608468234539032
Blogs,15,109,results,decoder 's attention,over,odds - on- copied terms,decoder 's attention over odds - on- copied terms,0.7125102877616882
Blogs,15,109,results,still underperforms,has,our model,still underperforms has our model,0.6042563319206238
Blogs,15,109,results,results,has,bus approach,results has bus approach,0.6046703457832336
Blogs,15,114,results,results,on,rg - 1 and rg - 2,results on rg - 1 and rg - 2,0.5422335267066956
Blogs,15,114,results,our model,has,statistically significantly improves,our model has statistically significantly improves,0.5861653089523315
Blogs,15,114,results,statistically significantly improves,has,results,statistically significantly improves has results,0.5606109499931335
Blogs,15,114,results,results,on,rg - 1 and rg - 2,results on rg - 1 and rg - 2,0.5422335267066956
Blogs,15,114,results,results,has,our model,results has our model,0.5871725678443909
Blogs,15,116,results,significantly outperforms,suggesting,promising cross-organizational transferability,significantly outperforms suggesting promising cross-organizational transferability,0.6739333271980286
Blogs,15,116,results,top-performing abstractive baseline model,suggesting,promising cross-organizational transferability,top-performing abstractive baseline model suggesting promising cross-organizational transferability,0.634764552116394
Blogs,15,116,results,significantly outperforms,has,top-performing abstractive baseline model,significantly outperforms has top-performing abstractive baseline model,0.5580049157142639
Blogs,15,117,results,"81 % ( a ) , 82 % ( b ) , and 80 % ( c )",of,our system- generated impressions,"81 % ( a ) , 82 % ( b ) , and 80 % ( c ) of our system- generated impressions",0.5861369967460632
Blogs,15,117,results,"81 % ( a ) , 82 % ( b ) , and 80 % ( c )",as good as,human-written impressions,"81 % ( a ) , 82 % ( b ) , and 80 % ( c ) as good as human-written impressions",0.7002776265144348
Blogs,15,117,results,our system- generated impressions,as good as,human-written impressions,our system- generated impressions as good as human-written impressions,0.6621027588844299
Blogs,15,117,results,human-written impressions,across,different metrics,human-written impressions across different metrics,0.6772171854972839
Blogs,16,172,ablation-analysis,oov percentages,dropped from,4.02 % to 1.12 %,oov percentages dropped from 4.02 % to 1.12 %,0.7011258006095886
Blogs,16,172,ablation-analysis,oov percentages,dropped from,2.08 % to 0.23 %,oov percentages dropped from 2.08 % to 0.23 %,0.7053946852684021
Blogs,16,172,ablation-analysis,4.02 % to 1.12 %,on,gigaword,4.02 % to 1.12 % on gigaword,0.5757367014884949
Blogs,16,172,ablation-analysis,2.08 % to 0.23 %,on,duc - 2004,2.08 % to 0.23 % on duc - 2004,0.5970423817634583
Blogs,16,172,ablation-analysis,ablation analysis,has,oov percentages,ablation analysis has oov percentages,0.48899200558662415
Blogs,16,151,baselines,abs +,is,tuned abs model,abs + is tuned abs model,0.6086687445640564
Blogs,16,151,baselines,baselines,has,abs +,baselines has abs +,0.6148451566696167
Blogs,16,152,baselines,luong-nmt,is,two -layer lstm encoder-decoder,luong-nmt is two -layer lstm encoder-decoder,0.537695050239563
Blogs,16,152,baselines,baselines,has,luong-nmt,baselines has luong-nmt,0.5944314002990723
Blogs,16,153,baselines,ras - elman,is,convolution encoder,ras - elman is convolution encoder,0.5674704313278198
Blogs,16,153,baselines,ras - elman,is,elman rnn decoder,ras - elman is elman rnn decoder,0.5879787802696228
Blogs,16,153,baselines,elman rnn decoder,with,attention,elman rnn decoder with attention,0.6222808957099915
Blogs,16,153,baselines,baselines,has,ras - elman,baselines has ras - elman,0.5979651212692261
Blogs,16,154,baselines,seq2seq +att,is,two - layer bilstm encoder,seq2seq +att is two - layer bilstm encoder,0.5115085244178772
Blogs,16,154,baselines,seq2seq +att,is,one- layer lstm decoder,seq2seq +att is one- layer lstm decoder,0.5318101048469543
Blogs,16,154,baselines,one- layer lstm decoder,equipped with,attention,one- layer lstm decoder equipped with attention,0.6080521941184998
Blogs,16,154,baselines,baselines,has,seq2seq +att,baselines has seq2seq +att,0.5843769311904907
Blogs,16,155,baselines,lvt5k-lsent,uses,temporal attention,lvt5k-lsent uses temporal attention,0.571273922920227
Blogs,16,155,baselines,temporal attention,to keep track of,past attentive weights,temporal attention to keep track of past attentive weights,0.6728925704956055
Blogs,16,155,baselines,past attentive weights,of,decoder,past attentive weights of decoder,0.6057776212692261
Blogs,16,155,baselines,repetition,in,later sequences,repetition in later sequences,0.5539501309394836
Blogs,16,155,baselines,baselines,has,lvt5k-lsent,baselines has lvt5k-lsent,0.6096360087394714
Blogs,16,156,baselines,seass,includes,additional selective gate,seass includes additional selective gate,0.7301373481750488
Blogs,16,156,baselines,additional selective gate,to control,information flow,additional selective gate to control information flow,0.7407802939414978
Blogs,16,156,baselines,information flow,from,encoder,information flow from encoder,0.6011125445365906
Blogs,16,156,baselines,encoder,to,decoder,encoder to decoder,0.6084607243537903
Blogs,16,156,baselines,baselines,has,seass,baselines has seass,0.6242193579673767
Blogs,16,157,baselines,pointer-generator,is,integrated pointer network,pointer-generator is integrated pointer network,0.5883228778839111
Blogs,16,157,baselines,pointer-generator,is,seq2seq model,pointer-generator is seq2seq model,0.5807328224182129
Blogs,16,157,baselines,baselines,has,pointer-generator,baselines has pointer-generator,0.5389614105224609
Blogs,16,159,baselines,two pointer - generator based extensions,for,global encoding,two pointer - generator based extensions for global encoding,0.6064887046813965
Blogs,16,159,baselines,cgu,sets,convolutional gated unit,cgu sets convolutional gated unit,0.6771579384803772
Blogs,16,159,baselines,cgu,sets,self-attention,cgu sets self-attention,0.6574208736419678
Blogs,16,159,baselines,cgu,for,global encoding,cgu for global encoding,0.6477320194244385
Blogs,16,159,baselines,self-attention,for,global encoding,self-attention for global encoding,0.5678425431251526
Blogs,16,134,experimental-setup,word embeddings,with,128 -d vectors,word embeddings with 128 -d vectors,0.6012167930603027
Blogs,16,134,experimental-setup,experimental setup,initialize,word embeddings,experimental setup initialize word embeddings,0.7035120129585266
Blogs,16,136,experimental-setup,vocabulary size,set to,150k,vocabulary size set to 150k,0.7219755053520203
Blogs,16,136,experimental-setup,150k,for,both the source and target text,150k for both the source and target text,0.6209536790847778
Blogs,16,136,experimental-setup,experimental setup,has,vocabulary size,experimental setup has vocabulary size,0.5189570188522339
Blogs,16,137,experimental-setup,hidden state size,set to,256,hidden state size set to 256,0.7133681178092957
Blogs,16,137,experimental-setup,experimental setup,has,hidden state size,experimental setup has hidden state size,0.5191042423248291
Blogs,16,138,experimental-setup,vocabulary size,increased,around 602 to 2216 concepts,vocabulary size increased around 602 to 2216 concepts,0.6604979634284973
Blogs,16,138,experimental-setup,around 602 to 2216 concepts,w.r.t,"different number ( k = 1 , ? ? ? , 5 ) of concept candidates","around 602 to 2216 concepts w.r.t different number ( k = 1 , ? ? ? , 5 ) of concept candidates",0.5979679822921753
Blogs,16,138,experimental-setup,"different number ( k = 1 , ? ? ? , 5 ) of concept candidates",for,each word,"different number ( k = 1 , ? ? ? , 5 ) of concept candidates for each word",0.5708839297294617
Blogs,16,138,experimental-setup,experimental setup,has,vocabulary size,experimental setup has vocabulary size,0.5189570188522339
Blogs,16,140,experimental-setup,https ://,has,github.com / wprojectsn /codes,https :// has github.com / wprojectsn /codes,0.5726005434989929
Blogs,16,141,experimental-setup,models,on,single gtx ti - tan gpu machine,models on single gtx ti - tan gpu machine,0.5610561370849609
Blogs,16,141,experimental-setup,experimental setup,trained,models,experimental setup trained models,0.6523525714874268
Blogs,16,142,experimental-setup,adagrad optimizer,with,batch size,adagrad optimizer with batch size,0.5849330425262451
Blogs,16,142,experimental-setup,batch size,of,64,batch size of 64,0.6741159558296204
Blogs,16,142,experimental-setup,64,to minimize,loss,64 to minimize loss,0.6980224251747131
Blogs,16,142,experimental-setup,experimental setup,used,adagrad optimizer,experimental setup used adagrad optimizer,0.5891650915145874
Blogs,16,143,experimental-setup,accumulator value,set to,0.15 and 0.1,accumulator value set to 0.15 and 0.1,0.742186963558197
Blogs,16,143,experimental-setup,experimental setup,set to,0.15 and 0.1,experimental setup set to 0.15 and 0.1,0.7002220749855042
Blogs,16,143,experimental-setup,experimental setup,has,initial learning rate,experimental setup has initial learning rate,0.49018073081970215
Blogs,16,143,experimental-setup,experimental setup,has,accumulator value,experimental setup has accumulator value,0.5425283908843994
Blogs,16,144,experimental-setup,gradient clipping,with,maximum gradient norm,gradient clipping with maximum gradient norm,0.5783113837242126
Blogs,16,144,experimental-setup,maximum gradient norm,of,2,maximum gradient norm of 2,0.6099976897239685
Blogs,16,144,experimental-setup,experimental setup,used,gradient clipping,experimental setup used gradient clipping,0.5482161641120911
Blogs,16,145,experimental-setup,summaries,produced through,beam search,summaries produced through beam search,0.7515744566917419
Blogs,16,145,experimental-setup,beam search,of size,8,beam search of size 8,0.7339420318603516
Blogs,16,145,experimental-setup,decoding,has,summaries,decoding has summaries,0.5914456248283386
Blogs,16,148,experimental-setup,distancesupervised training,at,5 k iterations,distancesupervised training at 5 k iterations,0.5116850137710571
Blogs,16,148,experimental-setup,distancesupervised training,at,6.5 k iterations,distancesupervised training at 6.5 k iterations,0.5210133790969849
Blogs,16,148,experimental-setup,5 k iterations,on,duc - 2004,5 k iterations on duc - 2004,0.5750394463539124
Blogs,16,148,experimental-setup,6.5 k iterations,on,gigaword,6.5 k iterations on gigaword,0.548176646232605
Blogs,16,148,experimental-setup,experimental setup,took,distancesupervised training,experimental setup took distancesupervised training,0.6198081970214844
Blogs,16,147,experiments,concept pointer generator,for,450k iterations,concept pointer generator for 450k iterations,0.5861706733703613
Blogs,16,147,experiments,450k iterations,yielded,best performance,450k iterations yielded best performance,0.6122555732727051
Blogs,16,147,experiments,optimization using rl rewards,for,rg -l,optimization using rl rewards for rg -l,0.5951733589172363
Blogs,16,147,experiments,rg -l,at,95 k iterations,rg -l at 95 k iterations,0.592318058013916
Blogs,16,147,experiments,rg -l,at,50 k iterations,rg -l at 50 k iterations,0.5661352276802063
Blogs,16,147,experiments,95 k iterations,on,duc - 2004,95 k iterations on duc - 2004,0.5925455689430237
Blogs,16,147,experiments,50 k iterations,on,gigaword,50 k iterations on gigaword,0.5661215782165527
Blogs,16,6,model,"knowledge - based , context - aware conceptualizations",to derive,extended set of candidate concepts,"knowledge - based , context - aware conceptualizations to derive extended set of candidate concepts",0.6492677330970764
Blogs,16,9,model,training model,adapts,different data,training model adapts different data,0.7784674167633057
Blogs,16,9,model,novel method,of,distantly - supervised learning,novel method of distantly - supervised learning,0.5097399950027466
Blogs,16,9,model,novel method,guided by,reference summaries and testing set,novel method guided by reference summaries and testing set,0.690635085105896
Blogs,16,9,model,model,has,training model,model has training model,0.5591098666191101
Blogs,16,25,model,novel model,based on,concept pointer generator,novel model based on concept pointer generator,0.6803181767463684
Blogs,16,25,model,model,propose,novel model,model propose novel model,0.6891457438468933
Blogs,16,26,model,model,alleviates,oov problems,model alleviates oov problems,0.7532491087913513
Blogs,16,34,model,network,optimized end-to - end,reinforcement learning,network optimized end-to - end reinforcement learning,0.7694233059883118
Blogs,16,34,model,network,using,reinforcement learning,network using reinforcement learning,0.688444197177887
Blogs,16,34,model,reinforcement learning,with,distant - supervision strategy,reinforcement learning with distant - supervision strategy,0.6280748248100281
Blogs,16,34,model,model,has,network,model has network,0.6001628041267395
Blogs,16,35,model,novel concept pointer generator network,leverages,context - aware conceptualization,novel concept pointer generator network leverages context - aware conceptualization,0.7034826874732971
Blogs,16,35,model,novel concept pointer generator network,leverages,concept pointer,novel concept pointer generator network leverages concept pointer,0.6648390293121338
Blogs,16,35,model,novel concept pointer generator network,jointly integrated into,generator,novel concept pointer generator network jointly integrated into generator,0.6939254403114319
Blogs,16,35,model,concept pointer,jointly integrated into,generator,concept pointer jointly integrated into generator,0.6874897480010986
Blogs,16,35,model,generator,to deliver,informative and abstract-oriented summaries,generator to deliver informative and abstract-oriented summaries,0.6907915472984314
Blogs,16,35,model,novel distant supervision training strategy,favors,model adaptation and generalization,novel distant supervision training strategy favors model adaptation and generalization,0.6951637864112854
Blogs,16,35,model,model adaptation and generalization,results in,performance,model adaptation and generalization results in performance,0.69405597448349
Blogs,16,35,model,performance,that,outperforms,performance that outperforms,0.6653058528900146
Blogs,16,35,model,wellaccepted evaluation - based reinforcement learning optimization,on,test-only dataset,wellaccepted evaluation - based reinforcement learning optimization on test-only dataset,0.4823772609233856
Blogs,16,35,model,test-only dataset,in terms of,rouge metrics,test-only dataset in terms of rouge metrics,0.6158276796340942
Blogs,16,35,model,statistical analysis,of,quantitative results and human evaluations,statistical analysis of quantitative results and human evaluations,0.550288736820221
Blogs,16,35,model,outperforms,has,wellaccepted evaluation - based reinforcement learning optimization,outperforms has wellaccepted evaluation - based reinforcement learning optimization,0.5352240800857544
Blogs,16,10,results,proposed approach,provides,statistically significant improvements,proposed approach provides statistically significant improvements,0.6427999138832092
Blogs,16,10,results,statistically significant improvements,over,several,statistically significant improvements over several,0.7541900873184204
Blogs,16,10,results,statistically significant improvements,over,state - of - the - art models,statistically significant improvements over state - of - the - art models,0.6567311882972717
Blogs,16,10,results,state - of - the - art models,on,duc - 2004 and gigaword datasets,state - of - the - art models on duc - 2004 and gigaword datasets,0.48215410113334656
Blogs,16,10,results,results,has,proposed approach,results has proposed approach,0.6086713075637817
Blogs,16,164,results,our model,has,outperformed,our model has outperformed,0.5903543829917908
Blogs,16,164,results,results,observe,our model,results observe our model,0.6353915333747864
Blogs,16,165,results,pointer generator performance,improvements made by,our concept pointer,pointer generator performance improvements made by our concept pointer,0.6140866279602051
Blogs,16,165,results,our concept pointer,are,statistically significant ( p < 0.01 ),our concept pointer are statistically significant ( p < 0.01 ),0.5786355137825012
Blogs,16,165,results,statistically significant ( p < 0.01 ),across,all metrics,statistically significant ( p < 0.01 ) across all metrics,0.6975923180580139
Blogs,16,165,results,results,In terms of,pointer generator performance,results In terms of pointer generator performance,0.6843549609184265
Blogs,16,166,results,oov and summary length,has,oov,oov and summary length has oov,0.5862582325935364
Blogs,16,166,results,results,has,oov and summary length,results has oov and summary length,0.5363882780075073
Blogs,16,177,results,our concept pointer generator,achieves,closest performance,our concept pointer generator achieves closest performance,0.6615822911262512
Blogs,16,177,results,closest performance,with,human-written summaries,closest performance with human-written summaries,0.6300910711288452
Blogs,17,51,baselines,rouge - 2 ( r - 2 ),measures,bigram overlap,rouge - 2 ( r - 2 ) measures bigram overlap,0.524376630783081
Blogs,17,51,baselines,bigram overlap,between,candidate summary,bigram overlap between candidate summary,0.6372131705284119
Blogs,17,51,baselines,bigram overlap,between,pool of reference summaries,bigram overlap between pool of reference summaries,0.6348592638969421
Blogs,17,51,baselines,rouge -l ( r-l ),measures,size of the longest common subsequence,rouge -l ( r-l ) measures size of the longest common subsequence,0.49637991189956665
Blogs,17,51,baselines,size of the longest common subsequence,between,candidate and reference summaries,size of the longest common subsequence between candidate and reference summaries,0.6001406908035278
Blogs,17,52,baselines,rouge -we ( r- we ),uses,soft matching,rouge -we ( r- we ) uses soft matching,0.5976788401603699
Blogs,17,52,baselines,soft matching,based on,cosine similarity,soft matching based on cosine similarity,0.6038590669631958
Blogs,17,52,baselines,cosine similarity,of,word embeddings,cosine similarity of word embeddings,0.5476437211036682
Blogs,17,52,baselines,js divergence ( js - 2 ),uses,jensen,js divergence ( js - 2 ) uses jensen,0.6624884009361267
Blogs,17,52,baselines,divergence,between,bigram distributions,divergence between bigram distributions,0.6574113965034485
Blogs,17,52,baselines,bigram distributions,of,references and candidate summaries,bigram distributions of references and candidate summaries,0.5766781568527222
Blogs,17,52,baselines,trained explicitly,to maximize,correlation,trained explicitly to maximize correlation,0.7173544764518738
Blogs,17,52,baselines,correlation,with,manual pyramid annotations,correlation with manual pyramid annotations,0.6699455380439758
Blogs,17,52,baselines,rouge -we ( r- we ),has,metric,rouge -we ( r- we ) has metric,0.6004260182380676
Blogs,17,52,baselines,jensen,has,divergence,jensen has divergence,0.6045093536376953
Blogs,17,52,baselines,jensen,has,metric,jensen has metric,0.6014204025268555
Blogs,17,52,baselines,s3,has,metric,s3 has metric,0.6085185408592224
Blogs,17,52,baselines,metric,has,trained explicitly,metric has trained explicitly,0.6080012917518616
Blogs,17,52,baselines,baselines,has,rouge -we ( r- we ),baselines has rouge -we ( r- we ),0.5525517463684082
Blogs,18,65,baselines,summae,projects,articles and sentences,summae projects articles and sentences,0.7855133414268494
Blogs,18,65,baselines,articles and sentences,into,common space,articles and sentences into common space,0.6118488311767578
Blogs,18,65,baselines,baselines,has,summae,baselines has summae,0.5907942056655884
Blogs,18,120,baselines,seq2seq architecture,based on,transformer,seq2seq architecture based on transformer,0.6865866184234619
Blogs,18,163,baselines,supervised models,include,abs,supervised models include abs,0.6258896589279175
Blogs,18,163,baselines,supervised models,include,bertsum,supervised models include bertsum,0.6016448736190796
Blogs,18,163,baselines,supervised models,include,fine-tuned version of pegasus,supervised models include fine-tuned version of pegasus,0.5756413340568542
Blogs,18,163,baselines,supervised models,based on,bert,supervised models based on bert,0.6271752715110779
Blogs,18,163,baselines,abs,based on,neural attention,abs based on neural attention,0.6433536410331726
Blogs,18,163,baselines,abs,based on,bert,abs based on bert,0.5911170244216919
Blogs,18,163,baselines,bertsum,based on,bert,bertsum based on bert,0.5753868818283081
Blogs,18,163,baselines,abs,has,bertsum,abs has bertsum,0.596681535243988
Blogs,18,163,baselines,abs,has,],abs has ],0.6536208391189575
Blogs,18,163,baselines,fine-tuned version of pegasus,has,],fine-tuned version of pegasus has ],0.614804208278656
Blogs,18,163,baselines,baselines,has,supervised models,baselines has supervised models,0.5437198281288147
Blogs,18,164,baselines,unsupervised models,include,seq 3 [ 2 ],unsupervised models include seq 3 [ 2 ],0.6085076332092285
Blogs,18,164,baselines,unsupervised models,include,brief [ 27 ],unsupervised models include brief [ 27 ],0.6260876059532166
Blogs,18,164,baselines,unsupervised models,include,ted [ 28 ],unsupervised models include ted [ 28 ],0.6096216440200806
Blogs,18,164,baselines,seq 3 [ 2 ],based on,article reconstruction,seq 3 [ 2 ] based on article reconstruction,0.7379562258720398
Blogs,18,164,baselines,brief [ 27 ],based on,adversarial training and reinforcement learning,brief [ 27 ] based on adversarial training and reinforcement learning,0.6454545855522156
Blogs,18,164,baselines,ted [ 28 ],based on,lead bias,ted [ 28 ] based on lead bias,0.6917769312858582
Blogs,18,164,baselines,baselines,has,unsupervised models,baselines has unsupervised models,0.5513634085655212
Blogs,18,165,baselines,large-scale pre-trained nlg models,including,pegasus,large-scale pre-trained nlg models including pegasus,0.672637403011322
Blogs,18,165,baselines,large-scale pre-trained nlg models,including,gpt - 2,large-scale pre-trained nlg models including gpt - 2,0.6716139316558838
Blogs,18,165,baselines,large-scale pre-trained nlg models,including,t5 large,large-scale pre-trained nlg models including t5 large,0.6657146215438843
Blogs,18,165,baselines,gpt - 2,has,],gpt - 2 has ],0.6601308584213257
Blogs,18,165,baselines,t5 large,has,],t5 large has ],0.6684163212776184
Blogs,18,118,experimental-setup,two versions,of,our model,two versions of our model,0.6042904257774353
Blogs,18,118,experimental-setup,our model,with,pre-trained language model bart - large [ 13 ],our model with pre-trained language model bart - large [ 13 ],0.5952885746955872
Blogs,18,118,experimental-setup,experimental setup,initialize,two versions,experimental setup initialize two versions,0.7348529696464539
Blogs,18,136,experimental-setup,filtering procedure,to remove,documents,filtering procedure to remove documents,0.72319096326828
Blogs,18,136,experimental-setup,documents,with,summaries,documents with summaries,0.6924825310707092
Blogs,18,136,experimental-setup,summaries,of,fewer than 50 words,summaries of fewer than 50 words,0.5916784405708313
Blogs,18,136,experimental-setup,experimental setup,follow,filtering procedure,experimental setup follow filtering procedure,0.5894083380699158
Blogs,18,149,experimental-setup,models,start from,bart large and t5 large checkpoints,models start from bart large and t5 large checkpoints,0.6461641192436218
Blogs,18,149,experimental-setup,models,further pre-trained for,1 epoch,models further pre-trained for 1 epoch,0.711450457572937
Blogs,18,149,experimental-setup,1 epoch,on,our data,1 epoch on our data,0.5277218222618103
Blogs,18,149,experimental-setup,1 epoch,takes,47 hours,1 epoch takes 47 hours,0.6923815011978149
Blogs,18,149,experimental-setup,47 hours,on,32 v-100 gpus,47 hours on 32 v-100 gpus,0.49902018904685974
Blogs,18,149,experimental-setup,experimental setup,has,models,experimental setup has models,0.5060054659843445
Blogs,18,150,experimental-setup,batch size,is,"1,024","batch size is 1,024",0.623492956161499
Blogs,18,150,experimental-setup,experimental setup,has,batch size,experimental setup has batch size,0.5429967045783997
Blogs,18,151,experimental-setup,radam,as,optimizer,radam as optimizer,0.580005407333374
Blogs,18,151,experimental-setup,radam,with,learning rate,radam with learning rate,0.6685057878494263
Blogs,18,151,experimental-setup,optimizer,with,learning rate,optimizer with learning rate,0.6228271722793579
Blogs,18,151,experimental-setup,learning rate,of,3 ? 10 ?4,learning rate of 3 ? 10 ?4,0.6455434560775757
Blogs,18,151,experimental-setup,experimental setup,use,radam,experimental setup use radam,0.6145078539848328
Blogs,18,172,experiments,zero-shot version,of,pegasus,zero-shot version of pegasus,0.597813606262207
Blogs,18,172,experiments,pegasus,by,"7.6 % , 6.9 % and 1.8 %","pegasus by 7.6 % , 6.9 % and 1.8 %",0.5595124363899231
Blogs,18,172,experiments,"7.6 % , 6.9 % and 1.8 %",in,rouge - 1,"7.6 % , 6.9 % and 1.8 % in rouge - 1",0.5668252110481262
Blogs,18,172,experiments,"7.6 % , 6.9 % and 1.8 %",on,"cnn / dailymail , xsum and gigawords","7.6 % , 6.9 % and 1.8 % on cnn / dailymail , xsum and gigawords",0.5199205279350281
Blogs,18,172,experiments,rouge - 1,on,"cnn / dailymail , xsum and gigawords","rouge - 1 on cnn / dailymail , xsum and gigawords",0.5515207052230835
Blogs,18,172,experiments,outperforms,has,zero-shot version,outperforms has zero-shot version,0.5892781615257263
Blogs,18,280,experiments,large-scale news corpus,conduct,cleaning and filtering,large-scale news corpus conduct cleaning and filtering,0.5767012238502502
Blogs,18,280,experiments,cleaning and filtering,based on,statistical analysis,cleaning and filtering based on statistical analysis,0.6028335690498352
Blogs,18,280,experiments,leading sentences,as,delegate summary,leading sentences as delegate summary,0.5301767587661743
Blogs,18,281,hyperparameters,generation model,initialized with,bart / t5,generation model initialized with bart / t5,0.7704285979270935
Blogs,18,281,hyperparameters,bart / t5,using,lead- bias data,bart / t5 using lead- bias data,0.7146477103233337
Blogs,18,281,hyperparameters,hyperparameters,pre-train,generation model,hyperparameters pre-train generation model,0.7047867178916931
Blogs,18,6,model,leveraged,in,simple and effective way,leveraged in simple and effective way,0.5529000163078308
Blogs,18,6,model,simple and effective way,pre-train,abstractive news summarization models,simple and effective way pre-train abstractive news summarization models,0.6919487714767456
Blogs,18,6,model,abstractive news summarization models,on,large-scale unlabeled news corpora,abstractive news summarization models on large-scale unlabeled news corpora,0.4370062053203583
Blogs,18,28,model,novel method,to leverage,lead bias,novel method to leverage lead bias,0.73907870054245
Blogs,18,28,model,lead bias,of,news articles,lead bias of news articles,0.5554222464561462
Blogs,18,28,model,lead bias,in,favor,lead bias in favor,0.5201646685600281
Blogs,18,28,model,lead bias,to conduct,large-scale pre-training,lead bias to conduct large-scale pre-training,0.6740908622741699
Blogs,18,28,model,favor,to conduct,large-scale pre-training,favor to conduct large-scale pre-training,0.6692765355110168
Blogs,18,28,model,large-scale pre-training,has,of summarization models,large-scale pre-training has of summarization models,0.485561341047287
Blogs,18,28,model,model,put forward,novel method,model put forward novel method,0.7062421441078186
Blogs,18,29,model,leading sentences,of,news article,leading sentences of news article,0.5609798431396484
Blogs,18,29,model,leading sentences,given,rest of the content,leading sentences given rest of the content,0.6574912071228027
Blogs,18,29,model,model,predict,leading sentences,model predict leading sentences,0.6941865086555481
Blogs,18,121,model,second phase,of,pre-training,second phase of pre-training,0.5922977328300476
Blogs,18,121,model,pre-training,using,lead bias,pre-training using lead bias,0.6837095022201538
Blogs,18,121,model,model,conduct,second phase,model conduct second phase,0.7694289088249207
Blogs,18,155,model,16 attention heads,in,both the encoder and decoder,16 attention heads in both the encoder and decoder,0.5440860986709595
Blogs,18,155,model,16 attention heads,with,770m parameters,16 attention heads with 770m parameters,0.6650751233100891
Blogs,18,155,model,both the encoder and decoder,with,770m parameters,both the encoder and decoder with 770m parameters,0.6640846133232117
Blogs,18,155,model,t5 - lb,has,24 transformer layers,t5 - lb has 24 transformer layers,0.573261022567749
Blogs,18,155,model,model,has,t5 - lb,model has t5 - lb,0.624212920665741
Blogs,18,51,results,our pre-training,mostly improve,quality,our pre-training mostly improve quality,0.7358516454696655
Blogs,18,51,results,quality,of,summaries,quality of summaries,0.46418988704681396
Blogs,18,51,results,results,has,our pre-training,results has our pre-training,0.5570191144943237
Blogs,18,52,results,bart - lb and t5 - lb,not sensitive to,decoding hyper-parameters,bart - lb and t5 - lb not sensitive to decoding hyper-parameters,0.7645082473754883
Blogs,18,52,results,decoding hyper-parameters,like,maximum summary length,decoding hyper-parameters like maximum summary length,0.5653306841850281
Blogs,18,52,results,decoding hyper-parameters,like,beam width,decoding hyper-parameters like beam width,0.6129586696624756
Blogs,18,52,results,our models,has,bart - lb and t5 - lb,our models has bart - lb and t5 - lb,0.6243500113487244
Blogs,18,52,results,results,has,our models,results has our models,0.5733726620674133
Blogs,18,53,results,human evaluation,show,lead- bias pretraining,human evaluation show lead- bias pretraining,0.6623923778533936
Blogs,18,53,results,lead- bias pretraining,improve,both readability and relevance of generated summaries,lead- bias pretraining improve both readability and relevance of generated summaries,0.6631454229354858
Blogs,18,171,results,bart -lb and t5 - lb,achieve,best results,bart -lb and t5 - lb achieve best results,0.6366363167762756
Blogs,18,171,results,all zero-shot baselines,in,each dataset,all zero-shot baselines in each dataset,0.4544122517108917
Blogs,18,171,results,best results,among,all non-supervised models,best results among all non-supervised models,0.5575065612792969
Blogs,18,171,results,all non-supervised models,in,xsum,all non-supervised models in xsum,0.518392026424408
Blogs,18,171,results,all non-supervised models,in,cnn / dailymail,all non-supervised models in cnn / dailymail,0.4894660711288452
Blogs,18,171,results,all non-supervised models,in,nyt,all non-supervised models in nyt,0.5412415862083435
Blogs,18,171,results,bart -lb and t5 - lb,has,outperform,bart -lb and t5 - lb has outperform,0.6324943900108337
Blogs,18,171,results,outperform,has,all zero-shot baselines,outperform has all zero-shot baselines,0.577936053276062
Blogs,18,171,results,results,has,bart -lb and t5 - lb,results has bart -lb and t5 - lb,0.5352621674537659
Blogs,18,175,results,t5 large,by,"9.9 % , 7.6 % , 6.4 %","t5 large by 9.9 % , 7.6 % , 6.4 %",0.5361548662185669
Blogs,18,175,results,"9.9 % , 7.6 % , 6.4 %",in,rouge - 1,"9.9 % , 7.6 % , 6.4 % in rouge - 1",0.5576355457305908
Blogs,18,175,results,"9.9 % , 7.6 % , 6.4 %",on,"duc2003 , duc2004 and xsum","9.9 % , 7.6 % , 6.4 % on duc2003 , duc2004 and xsum",0.5130942463874817
Blogs,18,175,results,rouge - 1,on,"duc2003 , duc2004 and xsum","rouge - 1 on duc2003 , duc2004 and xsum",0.5349933505058289
Blogs,18,175,results,t5 - lb,has,outperforms,t5 - lb has outperforms,0.6603426337242126
Blogs,18,175,results,outperforms,has,t5 large,outperforms has t5 large,0.6428841948509216
Blogs,18,175,results,results,has,t5 - lb,results has t5 - lb,0.5568682551383972
Blogs,18,176,results,bart -lb and t5 - lb,achieve,very close results,bart -lb and t5 - lb achieve very close results,0.6358577609062195
Blogs,18,176,results,all unsupervised baselines,in,cnn / dm and nyt,all unsupervised baselines in cnn / dm and nyt,0.5321804881095886
Blogs,18,176,results,very close results,on,other datasets,very close results on other datasets,0.48845013976097107
Blogs,18,176,results,bart -lb and t5 - lb,has,significantly outperform,bart -lb and t5 - lb has significantly outperform,0.580018937587738
Blogs,18,176,results,significantly outperform,has,all unsupervised baselines,significantly outperform has all unsupervised baselines,0.579190731048584
Blogs,18,176,results,results,has,bart -lb and t5 - lb,results has bart -lb and t5 - lb,0.5352621674537659
Blogs,18,180,results,lead baseline,in,5 out of 6 datasets,lead baseline in 5 out of 6 datasets,0.5023460984230042
Blogs,18,180,results,lead baseline,ranging from,0.18 % to 9.9 %,lead baseline ranging from 0.18 % to 9.9 %,0.5975251197814941
Blogs,18,180,results,bart - lb and t5 - lb,has,outperform,bart - lb and t5 - lb has outperform,0.6324943900108337
Blogs,18,180,results,outperform,has,lead baseline,outperform has lead baseline,0.636239230632782
Blogs,18,180,results,results,has,bart - lb and t5 - lb,results has bart - lb and t5 - lb,0.5352621674537659
Blogs,18,195,results,results,indicate,our pretraining scheme,results indicate our pretraining scheme,0.5427034497261047
Blogs,18,195,results,our pretraining scheme,help more with producing,better summaries,our pretraining scheme help more with producing better summaries,0.720661997795105
Blogs,18,195,results,better summaries,with,medium and medium-long lengths,better summaries with medium and medium-long lengths,0.6506186127662659
Blogs,18,195,results,results,indicate,our pretraining scheme,results indicate our pretraining scheme,0.5427034497261047
Blogs,18,214,results,outperform,in,relevance,outperform in relevance,0.5374124050140381
Blogs,18,214,results,bart - lb and t5 - lb,has,outperform,bart - lb and t5 - lb has outperform,0.6324943900108337
Blogs,18,282,results,resulting models,achieve,comparable results,resulting models achieve comparable results,0.5770385265350342
Blogs,18,282,results,bart -lb and t5 - lb,achieve,comparable results,bart -lb and t5 - lb achieve comparable results,0.6290457844734192
Blogs,18,282,results,comparable results,with,unsupervised methods,comparable results with unsupervised methods,0.6316658854484558
Blogs,18,282,results,unsupervised methods,on,6 benchmark datasets,unsupervised methods on 6 benchmark datasets,0.40915626287460327
Blogs,18,282,results,resulting models,has,bart -lb and t5 - lb,resulting models has bart -lb and t5 - lb,0.610555112361908
Blogs,18,282,results,resulting models,has,outperform,resulting models has outperform,0.5973414182662964
Blogs,18,282,results,bart -lb and t5 - lb,has,outperform,bart -lb and t5 - lb has outperform,0.6324943900108337
Blogs,18,282,results,outperform,has,all zeroshot baselines,outperform has all zeroshot baselines,0.5903702974319458
Blogs,18,282,results,results,has,resulting models,results has resulting models,0.5172345042228699
Blogs,19,105,baselines,greedy oracle,adds,sentences,greedy oracle adds sentences,0.621851921081543
Blogs,19,105,baselines,sentences,from,cluster,sentences from cluster,0.5437523722648621
Blogs,19,105,baselines,sentences,that optimize,r1 - f,sentences that optimize r1 - f,0.7165804505348206
Blogs,19,105,baselines,r1 - f,of,constructed summary,r1 - f of constructed summary,0.6567504405975342
Blogs,19,105,baselines,r1 - f,until,decreases,r1 - f until decreases,0.7634444832801819
Blogs,19,105,baselines,constructed summary,until,r1 - f,constructed summary until r1 - f,0.6968460083007812
Blogs,19,105,baselines,r1 - f,has,decreases,r1 - f has decreases,0.6185330748558044
Blogs,19,105,baselines,baselines,has,greedy oracle,baselines has greedy oracle,0.5388511419296265
Blogs,19,106,baselines,baselines,has,oracle ( single ),baselines has oracle ( single ),0.5783430933952332
Blogs,19,108,baselines,lead oracle,of,individual article,lead oracle of individual article,0.6045329570770264
Blogs,19,108,baselines,lead ( first sentences up to 40 words ),of,individual article,lead ( first sentences up to 40 words ) of individual article,0.5694775581359863
Blogs,19,108,baselines,individual article,with,best r1 - f score,individual article with best r1 - f score,0.6019721031188965
Blogs,19,108,baselines,best r1 - f score,within,cluster,best r1 - f score within cluster,0.6569898724555969
Blogs,19,108,baselines,lead oracle,has,lead ( first sentences up to 40 words ),lead oracle has lead ( first sentences up to 40 words ),0.6022903323173523
Blogs,19,108,baselines,baselines,has,lead oracle,baselines has lead oracle,0.5967024564743042
Blogs,19,109,baselines,random lead,lead of,randomly selected article,random lead lead of randomly selected article,0.7532951235771179
Blogs,19,109,baselines,lead baseline,used in,single -document summarization,lead baseline used in single -document summarization,0.6186045408248901
Blogs,19,109,baselines,baselines,has,random lead,baselines has random lead,0.5748983025550842
Blogs,19,111,baselines,regression - based sentence ranking,using,statistical features,regression - based sentence ranking using statistical features,0.6459581851959229
Blogs,19,111,baselines,bertreg,with,sentence embeddings,bertreg with sentence embeddings,0.6485952734947205
Blogs,19,111,baselines,sentence embeddings,computed by,pretrained bert model,sentence embeddings computed by pretrained bert model,0.5091171860694885
Blogs,19,113,baselines,submodular + abs,create,extractive multi-document summary,submodular + abs create extractive multi-document summary,0.5942307710647583
Blogs,19,113,baselines,extractive multi-document summary,with,maximum of 100 words,extractive multi-document summary with maximum of 100 words,0.6235008239746094
Blogs,19,113,baselines,extractive multi-document summary,using,submodular,extractive multi-document summary using submodular,0.6514594554901123
Blogs,19,113,baselines,maximum of 100 words,using,submodular,maximum of 100 words using submodular,0.6351661682128906
Blogs,19,7,experiments,wikipedia current events portal ( wcep ),provides,concise and neutral human-written summaries,wikipedia current events portal ( wcep ) provides concise and neutral human-written summaries,0.6186604499816895
Blogs,19,7,experiments,concise and neutral human-written summaries,of,news events,concise and neutral human-written summaries of news events,0.5398687720298767
Blogs,19,7,experiments,concise and neutral human-written summaries,with links to,external source articles,concise and neutral human-written summaries with links to external source articles,0.6968170404434204
Blogs,19,22,experiments,wikipedia current events portal ( wcep ) dataset,to address,real- world mds use cases,wikipedia current events portal ( wcep ) dataset to address real- world mds use cases,0.552807629108429
Blogs,19,110,experiments,unsupervised methods,has,"textrank ( mihalcea and tarau , 2004 )","unsupervised methods has textrank ( mihalcea and tarau , 2004 )",0.5357486605644226
Blogs,19,6,results,mds,large both in,total number of document clusters,mds large both in total number of document clusters,0.6285616755485535
Blogs,19,6,results,mds,large both in,size of individual clusters,mds large both in size of individual clusters,0.6821779012680054
Blogs,19,119,results,supervised methods tsr and bertreg,show,advantages,supervised methods tsr and bertreg show advantages,0.6200103759765625
Blogs,19,119,results,supervised methods tsr and bertreg,not by,large margin,supervised methods tsr and bertreg not by large margin,0.6552532315254211
Blogs,19,119,results,advantages,over,unsupervised methods,advantages over unsupervised methods,0.609341025352478
Blogs,19,119,results,results,has,supervised methods tsr and bertreg,results has supervised methods tsr and bertreg,0.5489199757575989
Blogs,19,122,results,submod - ular + abs heuristic,for applying,pre-trained abstractive model,submod - ular + abs heuristic for applying pre-trained abstractive model,0.7322106957435608
Blogs,19,122,results,pre-trained abstractive model,does not perform,well,pre-trained abstractive model does not perform well,0.7414926886558533
Blogs,19,122,results,results,has,submod - ular + abs heuristic,results has submod - ular + abs heuristic,0.5641815066337585
Blogs,19,123,results,performance,of,several methods,performance of several methods,0.6009435057640076
Blogs,19,123,results,several methods,on,test set,several methods on test set,0.5270808339118958
Blogs,19,123,results,increases,with,different amounts of additional articles,increases with different amounts of additional articles,0.6416507363319397
Blogs,19,123,results,different amounts of additional articles,from,common crawl,different amounts of additional articles from common crawl,0.54795902967453
Blogs,19,123,results,performance,has,increases,performance has increases,0.5947421193122864
Blogs,19,123,results,results,has,effect of additional articles,results has effect of additional articles,0.5468789935112
Blogs,19,124,results,10 additional articles,causes,steep improvement,10 additional articles causes steep improvement,0.6974167227745056
Blogs,19,124,results,steep improvement,compared to,original source articles,steep improvement compared to original source articles,0.6751582622528076
Blogs,19,124,results,steep improvement,only using,original source articles,steep improvement only using original source articles,0.7179366946220398
Blogs,19,124,results,original source articles,from,wcep,original source articles from wcep,0.5950633883476257
Blogs,19,124,results,results,Using,10 additional articles,results Using 10 additional articles,0.6289753317832947
Blogs,19,125,results,more than 100 articles,leads to,minimal gains,more than 100 articles leads to minimal gains,0.6616527438163757
Blogs,19,125,results,results,using,more than 100 articles,results using more than 100 articles,0.5107341408729553
Blogs,20,21,model,fist step,towards,summarization,fist step towards summarization,0.7373359203338623
Blogs,20,21,model,fist step,by composing,two novel large datasets,fist step by composing two novel large datasets,0.7317406535148621
Blogs,20,21,model,summarization,of,whole scientific articles,summarization of whole scientific articles,0.5732670426368713
Blogs,20,21,model,summarization,by composing,two novel large datasets,summarization by composing two novel large datasets,0.7066237330436707
Blogs,20,21,model,two novel large datasets,for,scientific summarization,two novel large datasets for scientific summarization,0.5656909942626953
Blogs,20,21,model,title - abstract pairs ( title-gen ),composed of,5 million papers,title - abstract pairs ( title-gen ) composed of 5 million papers,0.6621289849281311
Blogs,20,21,model,5 million papers,in,biomedical domain,5 million papers in biomedical domain,0.4947546422481537
Blogs,20,21,model,abstractbody pairs ( abstract- gen ),composed of,900k papers,abstractbody pairs ( abstract- gen ) composed of 900k papers,0.6625428199768066
Blogs,21,31,ablation-analysis,benefits,of,our model,benefits of our model,0.5777701139450073
Blogs,21,27,baselines,baselines,has,lstm,baselines has lstm,0.5395978093147278
Blogs,21,149,baselines,lexrank,has,"erkan and radev , 2004 )","lexrank has erkan and radev , 2004 )",0.6137660145759583
Blogs,21,211,baselines,bsl,is,model,bsl is model,0.6349786520004272
Blogs,21,211,baselines,model,with,sentence and local topic information,model with sentence and local topic information,0.5600304007530212
Blogs,21,211,baselines,model,with,sentence and global document information,model with sentence and global document information,0.577357828617096
Blogs,21,211,baselines,model,with,sentence and global document information,model with sentence and global document information,0.577357828617096
Blogs,21,211,baselines,full model,with,attentive context decoder,full model with attentive context decoder,0.6400365829467773
Blogs,21,211,baselines,baselines,has,bsl,baselines has bsl,0.5615102052688599
Blogs,21,134,experimental-setup,our model,using,adam optimizer,our model using adam optimizer,0.6453019976615906
Blogs,21,134,experimental-setup,our model,with,drop out rate,our model with drop out rate,0.647579550743103
Blogs,21,134,experimental-setup,adam optimizer,with,learning rate 0.0001,adam optimizer with learning rate 0.0001,0.6063497066497803
Blogs,21,134,experimental-setup,adam optimizer,with,drop out rate,adam optimizer with drop out rate,0.618686318397522
Blogs,21,134,experimental-setup,drop out rate,of,0.3,drop out rate of 0.3,0.596635103225708
Blogs,21,134,experimental-setup,experimental setup,train,our model,experimental setup train our model,0.6429243087768555
Blogs,21,135,experimental-setup,mini-batch,with,batch size,mini-batch with batch size,0.6728323698043823
Blogs,21,135,experimental-setup,mini-batch,with,size,mini-batch with size,0.6890663504600525
Blogs,21,135,experimental-setup,batch size,of,32 documents,batch size of 32 documents,0.598173975944519
Blogs,21,135,experimental-setup,size,of,gru hidden states,size of gru hidden states,0.6082197427749634
Blogs,21,135,experimental-setup,gru hidden states,is,300,gru hidden states is 300,0.642998993396759
Blogs,21,135,experimental-setup,experimental setup,use,mini-batch,experimental setup use mini-batch,0.5772721171379089
Blogs,21,136,experimental-setup,word embeddings,use,"glove ( pennington et al. , 2014 )","word embeddings use glove ( pennington et al. , 2014 )",0.5481976270675659
Blogs,21,136,experimental-setup,"glove ( pennington et al. , 2014 )",with,dimension,"glove ( pennington et al. , 2014 ) with dimension",0.6655246615409851
Blogs,21,136,experimental-setup,"glove ( pennington et al. , 2014 )",pre-trained on,wikipedia and gigaword,"glove ( pennington et al. , 2014 ) pre-trained on wikipedia and gigaword",0.7765293717384338
Blogs,21,136,experimental-setup,dimension,has,300,dimension has 300,0.6396656632423401
Blogs,21,136,experimental-setup,experimental setup,For,word embeddings,experimental setup For word embeddings,0.5087873935699463
Blogs,21,137,experimental-setup,vocabulary size,of,our model,vocabulary size of our model,0.5473134517669678
Blogs,21,137,experimental-setup,vocabulary size,is,50000,vocabulary size is 50000,0.6058894991874695
Blogs,21,137,experimental-setup,our model,is,50000,our model is 50000,0.6176478266716003
Blogs,21,137,experimental-setup,experimental setup,has,vocabulary size,experimental setup has vocabulary size,0.5189570188522339
Blogs,21,139,experimental-setup,best model,selected with,early stopping,best model selected with early stopping,0.6222785115242004
Blogs,21,139,experimental-setup,early stopping,on,validation set,early stopping on validation set,0.5784603357315063
Blogs,21,139,experimental-setup,validation set,according to,rouge - 2 f-score,validation set according to rouge - 2 f-score,0.5763542652130127
Blogs,21,139,experimental-setup,experimental setup,train,each model,experimental setup train each model,0.6811989545822144
Blogs,21,217,experimental-setup,https,:,//github.com,https : //github.com,0.6116153597831726
Blogs,21,4,model,novel neural singledocument extractive summarization,incorporating,local context,novel neural singledocument extractive summarization incorporating local context,0.6443964838981628
Blogs,21,4,model,local context,within,current topic,local context within current topic,0.6480318307876587
Blogs,21,4,model,model,propose,novel neural singledocument extractive summarization,model propose novel neural singledocument extractive summarization,0.6516663432121277
Blogs,21,188,model,novel extractive summarization model,designed for,long documents,novel extractive summarization model designed for long documents,0.6445704698562622
Blogs,21,188,model,novel extractive summarization model,by incorporating,local context,novel extractive summarization model by incorporating local context,0.6488902568817139
Blogs,21,188,model,local context,within,each topic,local context within each topic,0.6409129500389099
Blogs,21,188,model,local context,along with,global context,local context along with global context,0.6085417866706848
Blogs,21,188,model,global context,of,whole document,global context of whole document,0.5719507932662964
Blogs,21,188,model,model,propose,novel extractive summarization model,model propose novel extractive summarization model,0.6370922923088074
Blogs,21,189,model,neural extractive summarization,in,parameter lean and modular architecture,neural extractive summarization in parameter lean and modular architecture,0.5216153860092163
Blogs,21,6,results,benefits,of,our method,benefits of our method,0.5761390924453735
Blogs,21,6,results,benefits,become,stronger,benefits become stronger,0.6331393718719482
Blogs,21,6,results,our method,become,stronger,our method become stronger,0.6271513104438782
Blogs,21,6,results,our method,apply it to,longer documents,our method apply it to longer documents,0.6120093464851379
Blogs,21,6,results,stronger,as,longer documents,stronger as longer documents,0.5389730930328369
Blogs,21,6,results,stronger,apply it to,longer documents,stronger apply it to longer documents,0.6508116126060486
Blogs,21,28,results,more traditional methods,for capturing,local context,more traditional methods for capturing local context,0.7032498121261597
Blogs,21,28,results,local context,rely on,hierarchical structures,local context rely on hierarchical structures,0.7327942252159119
Blogs,21,28,results,lstm - minus,produces,simpler models,lstm - minus produces simpler models,0.6572577953338623
Blogs,21,28,results,simpler models,with,less parameters,simpler models with less parameters,0.6264357566833496
Blogs,21,28,results,simpler models,less prone to,overfitting,simpler models less prone to overfitting,0.6337403655052185
Blogs,21,28,results,more traditional methods,has,lstm - minus,more traditional methods has lstm - minus,0.5890607237815857
Blogs,21,28,results,local context,has,lstm - minus,local context has lstm - minus,0.6202629804611206
Blogs,21,28,results,results,With respect to,more traditional methods,results With respect to more traditional methods,0.6453967094421387
Blogs,21,29,results,method,on,pubmed and arxiv datasets,method on pubmed and arxiv datasets,0.5112394690513611
Blogs,21,29,results,effectively summarizing,has,long documents,effectively summarizing has long documents,0.5785662531852722
Blogs,21,29,results,results,test,method,results test method,0.6896118521690369
Blogs,21,30,results,baseline and previous approaches,by,narrow margin,baseline and previous approaches by narrow margin,0.5677436590194702
Blogs,21,30,results,narrow margin,on,both datasets,narrow margin on both datasets,0.4948643743991852
Blogs,21,30,results,benefit,of,our method,benefit of our method,0.5969009399414062
Blogs,21,30,results,benefit,become,much stronger,benefit become much stronger,0.5745247006416321
Blogs,21,30,results,our method,become,much stronger,our method become much stronger,0.5438908338546753
Blogs,21,30,results,our method,apply it to,longer documents,our method apply it to longer documents,0.6120093464851379
Blogs,21,30,results,much stronger,apply it to,longer documents,much stronger apply it to longer documents,0.6552637219429016
Blogs,21,30,results,outperform,has,baseline and previous approaches,outperform has baseline and previous approaches,0.5889123678207397
Blogs,21,163,results,outperforms,on,"informativeness ( rouge - 1,2 )","outperforms on informativeness ( rouge - 1,2 )",0.5385946035385132
Blogs,21,163,results,traditional extractive models,on,"informativeness ( rouge - 1,2 )","traditional extractive models on informativeness ( rouge - 1,2 )",0.49447330832481384
Blogs,21,163,results,traditional extractive models,by,wide margin,traditional extractive models by wide margin,0.567865788936615
Blogs,21,163,results,"informativeness ( rouge - 1,2 )",by,wide margin,"informativeness ( rouge - 1,2 ) by wide margin",0.5387864708900452
Blogs,21,163,results,both datasets,has,neural extractive models,both datasets has neural extractive models,0.5911522507667542
Blogs,21,163,results,neural extractive models,has,outperforms,neural extractive models has outperforms,0.6156789064407349
Blogs,21,163,results,outperforms,has,traditional extractive models,outperforms has traditional extractive models,0.5816203355789185
Blogs,21,166,results,neural abstractive models,on,"rouge - 1,2","neural abstractive models on rouge - 1,2",0.5681937336921692
Blogs,21,166,results,models,tend to have,highest rouge -l scores,models tend to have highest rouge -l scores,0.6817273497581482
Blogs,21,166,results,neural extractive models,has,dominate,neural extractive models has dominate,0.5695266723632812
Blogs,21,166,results,dominate,has,neural abstractive models,dominate has neural abstractive models,0.607907772064209
Blogs,21,166,results,results,has,neural extractive models,results has neural extractive models,0.5609817504882812
Blogs,21,167,results,our models,with,attentive context and concatenation decoder,our models with attentive context and concatenation decoder,0.6273760795593262
Blogs,21,167,results,our models,have,better performances,our models have better performances,0.5797327160835266
Blogs,21,167,results,better performances,on,all three rouge scores,better performances on all three rouge scores,0.47723910212516785
Blogs,21,167,results,better performances,on,meteor,better performances on meteor,0.5856173634529114
Blogs,21,167,results,other neural extractive models,has,our models,other neural extractive models has our models,0.5611090064048767
Blogs,21,167,results,results,Compared with,other neural extractive models,results Compared with other neural extractive models,0.670152485370636
Blogs,21,169,results,baseline model,achieves,slightly better performance,baseline model achieves slightly better performance,0.6749427318572998
Blogs,21,169,results,slightly better performance,than,previous works,slightly better performance than previous works,0.5504746437072754
Blogs,21,169,results,results,has,baseline model,results has baseline model,0.5361924171447754
Blogs,21,170,results,benefits,of,our method,benefits of our method,0.5761390924453735
Blogs,21,170,results,our method,explicitly designed to deal with,longer documents,our method explicitly designed to deal with longer documents,0.6616610288619995
Blogs,21,170,results,do actually become stronger,apply it to,longer docu-ments,do actually become stronger apply it to longer docu-ments,0.6874872446060181
Blogs,21,170,results,most important result,has,benefits,most important result has benefits,0.5532277226448059
Blogs,21,170,results,most important result,has,do actually become stronger,most important result has do actually become stronger,0.5837005376815796
Blogs,21,171,results,performance gain,of,our model,performance gain of our model,0.5619418025016785
Blogs,21,171,results,our model,with respect to,current state - of - the - art extractive summarizer,our model with respect to current state - of - the - art extractive summarizer,0.5941399335861206
Blogs,21,171,results,more pronounced,for,documents,more pronounced for documents,0.6181849837303162
Blogs,21,171,results,documents,with,>= 3000 words,documents with >= 3000 words,0.6259520649909973
Blogs,21,171,results,>= 3000 words,in,both datasets,>= 3000 words in both datasets,0.4812636375427246
Blogs,21,181,results,significantly improves,when,local topic information ( i.e. local context ),significantly improves when local topic information ( i.e. local context ),0.6082058548927307
Blogs,21,181,results,both datasets,has,performance,both datasets has performance,0.5999640226364136
Blogs,21,181,results,performance,has,significantly improves,performance has significantly improves,0.6344982981681824
Blogs,21,184,results,representation,of,whole document,representation of whole document,0.5884405970573425
Blogs,21,184,results,representation,has,never significantly improves,representation has never significantly improves,0.6279717087745667
Blogs,21,184,results,whole document,has,i.e. global context ),whole document has i.e. global context ),0.5418354272842407
Blogs,21,184,results,whole document,has,never significantly improves,whole document has never significantly improves,0.6176300644874573
Blogs,21,184,results,i.e. global context ),has,never significantly improves,i.e. global context ) has never significantly improves,0.5818939805030823
Blogs,21,184,results,never significantly improves,has,performance,never significantly improves has performance,0.618258535861969
Blogs,21,184,results,results,Adding,representation,results Adding representation,0.6126043200492859
Blogs,22,119,ablation-analysis,bidirectional encoders,crucial for,squad,bidirectional encoders crucial for squad,0.7549182176589966
Blogs,22,119,ablation-analysis,ablation analysis,has,bidirectional encoders,ablation analysis has bidirectional encoders,0.5625687837600708
Blogs,22,8,baselines,number of noising approaches,finding,best performance,number of noising approaches finding best performance,0.7019438147544861
Blogs,22,8,baselines,best performance,randomly shuffling the order of,original sentences,best performance randomly shuffling the order of original sentences,0.7326474785804749
Blogs,22,8,baselines,best performance,using,novel in - filling scheme,best performance using novel in - filling scheme,0.664218008518219
Blogs,22,8,baselines,novel in - filling scheme,where,spans of text,novel in - filling scheme where spans of text,0.6074098944664001
Blogs,22,8,baselines,spans of text,replaced with,single mask token,spans of text replaced with single mask token,0.7194414734840393
Blogs,22,21,baselines,number of noising approaches,finding,best performance,number of noising approaches finding best performance,0.7019438147544861
Blogs,22,21,baselines,best performance,randomly shuffling the order of,original sentences,best performance randomly shuffling the order of original sentences,0.7326474785804749
Blogs,22,21,baselines,best performance,using,novel in - filling scheme,best performance using novel in - filling scheme,0.664218008518219
Blogs,22,21,baselines,novel in - filling scheme,where,arbitrary length spans of text ( including zero length ),novel in - filling scheme where arbitrary length spans of text ( including zero length ),0.616757869720459
Blogs,22,21,baselines,arbitrary length spans of text ( including zero length ),replaced with,single mask token,arbitrary length spans of text ( including zero length ) replaced with single mask token,0.7506417036056519
Blogs,22,136,baselines,combination,of,text infilling,combination of text infilling,0.5970268845558167
Blogs,22,136,baselines,combination,of,sentence permutation,combination of sentence permutation,0.5932932496070862
Blogs,22,145,baselines,roberta,pre-trained with,same resources,roberta pre-trained with same resources,0.7070217728614807
Blogs,22,145,baselines,roberta,pre-trained with,different objective,roberta pre-trained with different objective,0.6504747271537781
Blogs,22,105,experiments,news summarization,with,highly abstractive summaries,news summarization with highly abstractive summaries,0.5714005827903748
Blogs,22,105,experiments,xsum,has,news summarization,xsum has news summarization,0.568280816078186
Blogs,22,105,experiments,"al. , 2018 )",has,news summarization,"al. , 2018 ) has news summarization",0.5114474892616272
Blogs,22,106,experiments,convai2,conditioned on,context,convai2 conditioned on context,0.7317456007003784
Blogs,22,106,experiments,convai2,conditioned on,persona,convai2 conditioned on persona,0.7422927618026733
Blogs,22,106,experiments,dialogue response generation task,conditioned on,context,dialogue response generation task conditioned on context,0.6779828071594238
Blogs,22,106,experiments,dialogue response generation task,conditioned on,persona,dialogue response generation task conditioned on persona,0.7038552761077881
Blogs,22,106,experiments,convai2,has,dialogue response generation task,convai2 has dialogue response generation task,0.549723744392395
Blogs,22,106,experiments,"et al. , 2019 )",has,dialogue response generation task,"et al. , 2019 ) has dialogue response generation task",0.5434173345565796
Blogs,22,124,experiments,pure language models,perform,best,pure language models perform best,0.5881858468055725
Blogs,22,124,experiments,best,on,eli5,best on eli5,0.6807287931442261
Blogs,22,154,experiments,summaries,in,cnn / dailymail,summaries in cnn / dailymail,0.5553261041641235
Blogs,22,154,experiments,cnn / dailymail,tend to resemble,source sentences,cnn / dailymail tend to resemble source sentences,0.6729696989059448
Blogs,22,221,experiments,machine translation,learn,small additional encoder,machine translation learn small additional encoder,0.6454035639762878
Blogs,22,221,experiments,small additional encoder,replaces,word embeddings,small additional encoder replaces word embeddings,0.7107499241828918
Blogs,22,221,experiments,word embeddings,in,bart,word embeddings in bart,0.5652106404304504
Blogs,22,243,experiments,eli5 r1 r2 rl,has,best extractive,eli5 r1 r2 rl has best extractive,0.6001203656196594
Blogs,22,133,hyperparameters,large model,with,12 layers,large model with 12 layers,0.6757651567459106
Blogs,22,133,hyperparameters,large model,with,hidden size,large model with hidden size,0.6584338545799255
Blogs,22,133,hyperparameters,12 layers,in,encoder and decoder,12 layers in encoder and decoder,0.5382566452026367
Blogs,22,133,hyperparameters,12 layers,in,hidden size,12 layers in hidden size,0.512074887752533
Blogs,22,133,hyperparameters,12 layers,of,encoder and decoder,12 layers of encoder and decoder,0.6057292819023132
Blogs,22,133,hyperparameters,hidden size,of,1024,hidden size of 1024,0.6436027884483337
Blogs,22,133,hyperparameters,hyperparameters,pre-train,large model,hyperparameters pre-train large model,0.7144048810005188
Blogs,22,134,hyperparameters,roberta,use,batch size,roberta use batch size,0.6966485977172852
Blogs,22,134,hyperparameters,roberta,train,model,roberta train model,0.7087476253509521
Blogs,22,134,hyperparameters,batch size,of,8000,batch size of 8000,0.6684168577194214
Blogs,22,134,hyperparameters,model,for,500000 steps,model for 500000 steps,0.6665827035903931
Blogs,22,134,hyperparameters,hyperparameters,Following,roberta,hyperparameters Following roberta,0.6170389652252197
Blogs,22,134,hyperparameters,hyperparameters,train,model,hyperparameters train model,0.7064553499221802
Blogs,22,135,hyperparameters,documents,tokenized with,same byte-pair encoding,documents tokenized with same byte-pair encoding,0.8181062340736389
Blogs,22,135,hyperparameters,same byte-pair encoding,as,gpt - 2,same byte-pair encoding as gpt - 2,0.5959793329238892
Blogs,22,135,hyperparameters,hyperparameters,has,documents,hyperparameters has documents,0.5504506230354309
Blogs,22,169,hyperparameters,beam width,of,5,beam width of 5,0.7087391018867493
Blogs,22,169,hyperparameters,length penalty,of,? = 1,length penalty of ? = 1,0.5922157764434814
Blogs,22,169,hyperparameters,preliminary results,suggested,our approach,preliminary results suggested our approach,0.6553356647491455
Blogs,22,169,hyperparameters,our approach,was,less effective,our approach was less effective,0.6112502813339233
Blogs,22,169,hyperparameters,our approach,prone to,overfitting,our approach prone to overfitting,0.7134122252464294
Blogs,22,169,hyperparameters,less effective,without,back - translation data,less effective without back - translation data,0.7228637337684631
Blogs,22,169,hyperparameters,hyperparameters,prone to,overfitting,hyperparameters prone to overfitting,0.6632512807846069
Blogs,22,6,model,bart,trained by,corrupting text,bart trained by corrupting text,0.7223074436187744
Blogs,22,6,model,bart,learning,model,bart learning model,0.7571066617965698
Blogs,22,6,model,corrupting text,with,arbitrary noising function,corrupting text with arbitrary noising function,0.6354628205299377
Blogs,22,6,model,model,to reconstruct,original text,model to reconstruct original text,0.7042755484580994
Blogs,22,6,model,model,has,bart,model has bart,0.5764365196228027
Blogs,22,16,model,bart,pre-trains,model,bart pre-trains model,0.773481011390686
Blogs,22,16,model,model,combining,bidirectional and auto-regressive transformers,model combining bidirectional and auto-regressive transformers,0.7509109973907471
Blogs,22,16,model,model,present,bart,model present bart,0.7012985348701477
Blogs,22,17,model,bart,is,denoising autoencoder,bart is denoising autoencoder,0.5802873373031616
Blogs,22,17,model,denoising autoencoder,built with,sequence - to- sequence model,denoising autoencoder built with sequence - to- sequence model,0.6898842453956604
Blogs,22,17,model,denoising autoencoder,applicable to,very wide range of end tasks,denoising autoencoder applicable to very wide range of end tasks,0.6790157556533813
Blogs,22,17,model,model,has,bart,model has bart,0.5764365196228027
Blogs,22,18,model,text,corrupted with,arbitrary noising function,text corrupted with arbitrary noising function,0.7862749695777893
Blogs,22,18,model,pretraining,has,two stages,pretraining has two stages,0.5867835283279419
Blogs,22,18,model,pretraining,has,sequence - to- sequence model,pretraining has sequence - to- sequence model,0.5517060160636902
Blogs,22,18,model,two stages,has,text,two stages has text,0.6083877682685852
Blogs,22,18,model,two stages,has,sequence - to- sequence model,two stages has sequence - to- sequence model,0.5694195628166199
Blogs,22,18,model,model,has,pretraining,model has pretraining,0.5657321214675903
Blogs,22,27,model,machine translation,where,bart model,machine translation where bart model,0.5652459859848022
Blogs,22,27,model,bart model,stacked above,few additional transformer layers,bart model stacked above few additional transformer layers,0.7330049872398376
Blogs,22,28,model,foreign language,to,noised,foreign language to noised,0.5991845726966858
Blogs,22,28,model,noised,For,fine-tuning,noised For fine-tuning,0.6588501930236816
Blogs,22,28,model,uncorrupted document,input to,encoder and decoder,uncorrupted document input to encoder and decoder,0.6986901760101318
Blogs,22,28,model,representations,from,final hidden state,representations from final hidden state,0.5299796462059021
Blogs,22,28,model,final hidden state,of,decoder,final hidden state of decoder,0.5828995704650879
Blogs,22,28,model,fine-tuning,has,uncorrupted document,fine-tuning has uncorrupted document,0.5470516681671143
Blogs,22,35,model,model bart,is,denoising autoencoder,model bart is denoising autoencoder,0.5591944456100464
Blogs,22,35,model,denoising autoencoder,maps,corrupted document,denoising autoencoder maps corrupted document,0.6424239873886108
Blogs,22,35,model,corrupted document,to,original document,corrupted document to original document,0.5418218374252319
Blogs,22,35,model,model,has,model bart,model has model bart,0.5857391357421875
Blogs,22,42,model,pre-training bart,trained by,corrupting documents,pre-training bart trained by corrupting documents,0.7520799040794373
Blogs,22,42,model,pre-training bart,optimizing,reconstruction loss,pre-training bart optimizing reconstruction loss,0.7518611550331116
Blogs,22,42,model,model,has,pre-training bart,model has pre-training bart,0.570613443851471
Blogs,22,76,model,model,trained,end-to-end,model trained end-to-end,0.7512461543083191
Blogs,22,76,model,model,trains,new encoder,model trains new encoder,0.7939486503601074
Blogs,22,76,model,new encoder,to map,foreign words,new encoder to map foreign words,0.7467511892318726
Blogs,22,76,model,foreign words,into,input,foreign words into input,0.6429773569107056
Blogs,22,76,model,model,trained,end-to-end,model trained end-to-end,0.7512461543083191
Blogs,22,76,model,model,has,model,model has model,0.5623406171798706
Blogs,22,78,model,source encoder,in,two steps,source encoder in two steps,0.5080752372741699
Blogs,22,78,model,source encoder,backpropagating,cross-entropy loss,source encoder backpropagating cross-entropy loss,0.6632736325263977
Blogs,22,78,model,cross-entropy loss,from,output,cross-entropy loss from output,0.4988362193107605
Blogs,22,78,model,output,of,bart model,output of bart model,0.6264301538467407
Blogs,22,78,model,model,train,source encoder,model train source encoder,0.7031787037849426
Blogs,22,79,model,first step,update,randomly initialized source encoder,first step update randomly initialized source encoder,0.7384843230247498
Blogs,22,79,model,first step,update,self-attention input projection matrix,first step update self-attention input projection matrix,0.7334649562835693
Blogs,22,79,model,self-attention input projection matrix,of,bart 's encoder first layer,self-attention input projection matrix of bart 's encoder first layer,0.5311914682388306
Blogs,22,79,model,model,In,first step,model In first step,0.5735508799552917
Blogs,22,150,model,fine-tuned,as,standard sequence - to-sequence model,fine-tuned as standard sequence - to-sequence model,0.5491059422492981
Blogs,22,150,model,standard sequence - to-sequence model,from,input,standard sequence - to-sequence model from input,0.5800955295562744
Blogs,22,150,model,input,to,output text,input to output text,0.5739200711250305
Blogs,22,150,model,model,has,bart,model has bart,0.5764365196228027
Blogs,22,9,results,bart,works well for,comprehension tasks,bart works well for comprehension tasks,0.6378973722457886
Blogs,22,9,results,effective,fine tuned for,text generation,effective fine tuned for text generation,0.7100208401679993
Blogs,22,9,results,results,has,bart,results has bart,0.40022552013397217
Blogs,22,11,results,1.1 bleu increase,over,back -translation system,1.1 bleu increase over back -translation system,0.5336071252822876
Blogs,22,11,results,1.1 bleu increase,with,only target language pretraining,1.1 bleu increase with only target language pretraining,0.5228607058525085
Blogs,22,11,results,back -translation system,for,machine translation,back -translation system for machine translation,0.6085546612739563
Blogs,22,23,results,bart,works well for,comprehension tasks,bart works well for comprehension tasks,0.6378973722457886
Blogs,22,23,results,effective,fine tuned for,text generation,effective fine tuned for text generation,0.7100208401679993
Blogs,22,23,results,results,has,bart,results has bart,0.40022552013397217
Blogs,22,24,results,performance,of,roberta,performance of roberta,0.6133401989936829
Blogs,22,24,results,roberta,with,comparable training resources,roberta with comparable training resources,0.6443483233451843
Blogs,22,24,results,comparable training resources,on,"glue ( wang et al. , 2018 )","comparable training resources on glue ( wang et al. , 2018 )",0.5142390727996826
Blogs,22,24,results,results,matches,performance,results matches performance,0.799586296081543
Blogs,22,34,results,bart,exhibits,most consistently strong performance,bart exhibits most consistently strong performance,0.711316704750061
Blogs,22,34,results,most consistently strong performance,across,full range of tasks,most consistently strong performance across full range of tasks,0.6082459688186646
Blogs,22,34,results,results,find,bart,results find bart,0.45314696431159973
Blogs,22,126,results,pure language model,performs,best,pure language model performs best,0.6100091934204102
Blogs,22,126,results,pure language model,suggesting,bart,pure language model suggesting bart,0.7008958458900452
Blogs,22,126,results,best,suggesting,bart,best suggesting bart,0.6964291334152222
Blogs,22,126,results,bart,is,less effective,bart is less effective,0.6008737087249756
Blogs,22,126,results,less effective,when,output,less effective when output,0.7050036191940308
Blogs,22,126,results,output,loosely constrained by,input,output loosely constrained by input,0.7141830921173096
Blogs,22,126,results,results,has,pure language model,results has pure language model,0.5356515049934387
Blogs,22,127,results,bart,achieves,most consistently strong performance,bart achieves most consistently strong performance,0.6898685693740845
Blogs,22,127,results,results,has,bart,results has bart,0.40022552013397217
Blogs,22,128,results,bart models,using,text - infilling,bart models using text - infilling,0.6909031867980957
Blogs,22,128,results,bart models,perform,well,bart models perform well,0.609268307685852
Blogs,22,128,results,well,on,all tasks,well on all tasks,0.499264121055603
Blogs,22,128,results,eli5,has,bart models,eli5 has bart models,0.6653962135314941
Blogs,22,128,results,results,With the exception of,eli5,results With the exception of eli5,0.7119482159614563
Blogs,22,139,results,outperforms,on,summarization,outperforms on summarization,0.5698851943016052
Blogs,22,139,results,previous work,on,summarization,previous work on summarization,0.5498442649841309
Blogs,22,139,results,summarization,on,two tasks and all metrics,summarization on two tasks and all metrics,0.491539865732193
Blogs,22,139,results,gains,of,roughly 6 points,gains of roughly 6 points,0.5834210515022278
Blogs,22,139,results,roughly 6 points,on,more abstractive dataset,roughly 6 points on more abstractive dataset,0.4955298602581024
Blogs,22,139,results,bart,has,outperforms,bart has outperforms,0.700005054473877
Blogs,22,139,results,outperforms,has,previous work,outperforms has previous work,0.6127970814704895
Blogs,22,139,results,results,has,bart,results has bart,0.40022552013397217
Blogs,22,146,results,bart,performs,similarly,bart performs similarly,0.7600840330123901
Blogs,22,146,results,results,has,bart,results has bart,0.40022552013397217
Blogs,22,156,results,bart,has,outperforms,bart has outperforms,0.700005054473877
Blogs,22,156,results,outperforms,has,existing work,outperforms has existing work,0.6018839478492737
Blogs,22,156,results,results,has,bart,results has bart,0.40022552013397217
Blogs,22,158,results,outperforms,leverages,bert,outperforms leverages bert,0.7470747828483582
Blogs,22,158,results,outperforms,by,roughly 6.0 points,outperforms by roughly 6.0 points,0.6437905430793762
Blogs,22,158,results,best previous work,leverages,bert,best previous work leverages bert,0.6824066042900085
Blogs,22,158,results,best previous work,by,roughly 6.0 points,best previous work by roughly 6.0 points,0.5380978584289551
Blogs,22,158,results,roughly 6.0 points,on,all rouge metrics,roughly 6.0 points on all rouge metrics,0.5112667083740234
Blogs,22,158,results,roughly 6.0 points,representing,significant advance,roughly 6.0 points representing significant advance,0.6700903177261353
Blogs,22,158,results,significant advance,in,performance,significant advance in performance,0.5307000279426575
Blogs,22,158,results,bart,has,outperforms,bart has outperforms,0.700005054473877
Blogs,22,158,results,outperforms,has,best previous work,outperforms has best previous work,0.5794478058815002
Blogs,22,158,results,results,has,bart,results has bart,0.40022552013397217
Blogs,22,170,results,large improvements,on,summarization metrics,large improvements on summarization metrics,0.5031178593635559
Blogs,22,170,results,up to 6 points,over,prior state - of - the- art,up to 6 points over prior state - of - the- art,0.6713375449180603
Blogs,22,170,results,results,has,qualitative analysis,results has qualitative analysis,0.47837314009666443
Blogs,22,234,results,varies considerably,across,tasks,varies considerably across tasks,0.7343723773956299
Blogs,22,234,results,bart models,with,text infilling,bart models with text infilling,0.6705793738365173
Blogs,22,234,results,text infilling,demonstrate,most consistently strong performance,text infilling demonstrate most consistently strong performance,0.5850110650062561
Blogs,22,234,results,performance,has,varies considerably,performance has varies considerably,0.5925930142402649
Blogs,22,234,results,results,has,performance,results has performance,0.5972660779953003
Blogs,22,236,results,bart,performs,comparably,bart performs comparably,0.7053034901618958
Blogs,22,236,results,comparably,to,roberta and xlnet,comparably to roberta and xlnet,0.6113002300262451
Blogs,22,236,results,results,has,bart,results has bart,0.40022552013397217
Blogs,22,239,results,outperforms,on,conver,outperforms on conver,0.665879487991333
Blogs,22,239,results,conver,-,sational response generation,conver - sational response generation,0.6730262041091919
Blogs,22,239,results,outperforms,has,previous work,outperforms has previous work,0.6127970814704895
Blogs,22,239,results,conver,has,sational response generation,conver has sational response generation,0.578620970249176
Blogs,22,242,results,outperforms,on,two automated metrics,outperforms on two automated metrics,0.5543408393859863
Blogs,22,242,results,bart,has,outperforms,bart has outperforms,0.700005054473877
Blogs,22,242,results,outperforms,has,previous work,outperforms has previous work,0.6127970814704895
Blogs,22,242,results,results,has,bart,results has bart,0.40022552013397217
Blogs,22,244,results,state - of- the - art results,on,challenging eli5 abstractive question answering dataset,state - of- the - art results on challenging eli5 abstractive question answering dataset,0.4782967269420624
Blogs,22,248,results,bart,improves over,strong back - translation ( bt ) baseline,bart improves over strong back - translation ( bt ) baseline,0.7355067133903503
Blogs,22,248,results,strong back - translation ( bt ) baseline,by using,monolingual english pre-training,strong back - translation ( bt ) baseline by using monolingual english pre-training,0.6268602013587952
Blogs,22,248,results,results,has,bart,results has bart,0.40022552013397217
Blogs,22,251,results,outperforms,by,1.2 rouge -l,outperforms by 1.2 rouge -l,0.6081876754760742
Blogs,22,251,results,bart,has,outperforms,bart has outperforms,0.700005054473877
Blogs,22,251,results,outperforms,has,best pre- vious work,outperforms has best pre- vious work,0.6045366525650024
Blogs,22,251,results,results,find,bart,results find bart,0.45314696431159973
Blogs,23,37,ablation-analysis,qags,robust to,number of factors,qags robust to number of factors,0.6837616562843323
Blogs,23,37,ablation-analysis,ablation analysis,show via ablations,qags,ablation analysis show via ablations qags,0.7728191614151001
Blogs,23,6,baselines,automatic evaluation protocol,called,qags,automatic evaluation protocol called qags,0.6921597123146057
Blogs,23,6,baselines,factual inconsistencies,in,generated summary,factual inconsistencies in generated summary,0.5123876929283142
Blogs,23,126,baselines,large-uncased bert variant,via,transformers library,large-uncased bert variant via transformers library,0.6316552758216858
Blogs,23,126,baselines,transformers library,has,"wolf et al. , 2019 )","transformers library has wolf et al. , 2019 )",0.5871671438217163
Blogs,23,121,experimental-setup,questions,using,beam search,questions using beam search,0.6771076321601868
Blogs,23,121,experimental-setup,beam search,with,width 10,beam search with width 10,0.7050411701202393
Blogs,23,121,experimental-setup,width 10,for,100 question candidates,width 10 for 100 question candidates,0.6130236387252808
Blogs,23,121,experimental-setup,width 10,total of,100 question candidates,width 10 total of 100 question candidates,0.6792522072792053
Blogs,23,121,experimental-setup,10,has,answer candidates,10 has answer candidates,0.6057050228118896
Blogs,23,125,experimental-setup,qa models,by fine-tuning,"bert ( devlin et al. , 2019 )","qa models by fine-tuning bert ( devlin et al. , 2019 )",0.7729860544204712
Blogs,23,125,experimental-setup,"bert ( devlin et al. , 2019 )",on,squad2.0,"bert ( devlin et al. , 2019 ) on squad2.0",0.536635160446167
Blogs,23,125,experimental-setup,experimental setup,train,qa models,experimental setup train qa models,0.6723635196685791
Blogs,23,134,experiments,qags,obtains,nearly twice the correlation,qags obtains nearly twice the correlation,0.618685781955719
Blogs,23,134,experiments,nearly twice the correlation,of,next best automatic metric ( bleu - 1 ),nearly twice the correlation of next best automatic metric ( bleu - 1 ),0.5542792677879333
Blogs,23,134,experiments,cnn / dm,has,qags,cnn / dm has qags,0.6524752974510193
Blogs,23,148,experiments,cnn / dm,has,bert - large - wwm,cnn / dm has bert - large - wwm,0.6286556720733643
Blogs,23,148,experiments,bert - large - wwm,has,slightly underperforms,bert - large - wwm has slightly underperforms,0.611997127532959
Blogs,23,148,experiments,slightly underperforms,has,bert- base,slightly underperforms has bert- base,0.6318711638450623
Blogs,23,22,model,three steps,Given,generated text,three steps Given generated text,0.7203885912895203
Blogs,23,22,model,question generation ( qg ) model,generates,set of questions,question generation ( qg ) model generates set of questions,0.6767186522483826
Blogs,23,22,model,set of questions,about,text,set of questions about text,0.6502655148506165
Blogs,23,22,model,three steps,has,question generation ( qg ) model,three steps has question generation ( qg ) model,0.5529792308807373
Blogs,23,22,model,generated text,has,question generation ( qg ) model,generated text has question generation ( qg ) model,0.5787320137023926
Blogs,23,29,results,factuality,of,summaries,factuality of summaries,0.6036443710327148
Blogs,23,29,results,results,has,qags,results has qags,0.5670158863067627
Blogs,23,32,results,qags,shows,robustness,qags shows robustness,0.7269092202186584
Blogs,23,32,results,robustness,to,quality,robustness to quality,0.5585073232650757
Blogs,23,32,results,robustness,to,domain of the models,robustness to domain of the models,0.5673090815544128
Blogs,23,32,results,quality,of,underlying qg and qa models,quality of underlying qg and qa models,0.6060231328010559
Blogs,23,32,results,results,has,qags,results has qags,0.5670158863067627
Blogs,23,132,results,bleu and rouge,perform,comparably,bleu and rouge perform comparably,0.5601980090141296
Blogs,23,132,results,lower order n-gram metrics,work,better,lower order n-gram metrics work better,0.6408320069313049
Blogs,23,132,results,results,has,bleu and rouge,results has bleu and rouge,0.555298388004303
Blogs,23,133,results,bertscore,matches,best n-gram metrics,bertscore matches best n-gram metrics,0.7634906768798828
Blogs,23,133,results,bertscore,matches,worst,bertscore matches worst,0.7645615339279175
Blogs,23,133,results,best n-gram metrics,on,cnn / dm,best n-gram metrics on cnn / dm,0.5063626170158386
Blogs,23,133,results,worst,on,xsum,worst on xsum,0.575076699256897
Blogs,23,133,results,results,has,bertscore,results has bertscore,0.5357906818389893
Blogs,23,137,results,all metrics,correlate worse with,human judgments,all metrics correlate worse with human judgments,0.6813086271286011
Blogs,23,137,results,human judgments,than,cnn / dm,human judgments than cnn / dm,0.6098073124885559
Blogs,23,137,results,xsum,has,all metrics,xsum has all metrics,0.5988572835922241
Blogs,23,138,results,outperforms,has,next best automatic metric,outperforms has next best automatic metric,0.6191501021385193
Blogs,23,138,results,results,has,qags,results has qags,0.5670158863067627
Blogs,23,146,results,qa models,perform,similarly,qa models perform similarly,0.6691685318946838
Blogs,23,146,results,results,has,qa models,results has qa models,0.5290708541870117
Blogs,23,147,results,best qa model ( bert-large - wwm ),lead to,best correlations,best qa model ( bert-large - wwm ) lead to best correlations,0.6783502697944641
Blogs,23,147,results,best correlations,with,human judgments,best correlations with human judgments,0.6014511585235596
Blogs,23,147,results,results,using,best qa model ( bert-large - wwm ),results using best qa model ( bert-large - wwm ),0.6795933842658997
Blogs,23,149,results,xsum,has,bert - base,xsum has bert - base,0.6566473841667175
Blogs,23,149,results,bert - base,has,slightly outperforms,bert - base has slightly outperforms,0.623326301574707
Blogs,23,149,results,slightly outperforms,has,other two bert variants,slightly outperforms has other two bert variants,0.5884121060371399
Blogs,23,149,results,results,On,xsum,results On xsum,0.6111196279525757
Blogs,23,157,results,qg model,get,correlations,qg model get correlations,0.5852376818656921
Blogs,23,157,results,correlations,of,51.53 and 15.28,correlations of 51.53 and 15.28,0.5615522861480713
Blogs,23,157,results,correlations,of,54.53 and 17.49,correlations of 54.53 and 17.49,0.5498625040054321
Blogs,23,157,results,correlations,versus,54.53 and 17.49,correlations versus 54.53 and 17.49,0.6060569286346436
Blogs,23,157,results,51.53 and 15.28,with,human judgments,51.53 and 15.28 with human judgments,0.619259238243103
Blogs,23,157,results,human judgments,on,cnn / dm and xsum,human judgments on cnn / dm and xsum,0.5688110589981079
Blogs,23,157,results,54.53 and 17.49,when using,newsqa - tuned qg model,54.53 and 17.49 when using newsqa - tuned qg model,0.7084400057792664
Blogs,23,161,results,increasing,has,number of questions used,increasing has number of questions used,0.5575838088989258
Blogs,23,161,results,results,show,increasing,results show increasing,0.622439980506897
Blogs,23,173,results,qags,has,outperforms,qags has outperforms,0.6682443022727966
Blogs,23,173,results,results,has,qags,results has qags,0.5670158863067627
Blogs,24,205,baselines,sentences,based on,centrality,sentences based on centrality,0.6579540371894836
Blogs,24,205,baselines,centrality,in,sentence similarity graph,centrality in sentence similarity graph,0.50107342004776
Blogs,24,205,baselines,baselines,has,lexrank,baselines has lexrank,0.5459157228469849
Blogs,24,207,baselines,"kl and js ( haghighi and vanderwende , 2009 )",measure,divergence,"kl and js ( haghighi and vanderwende , 2009 ) measure divergence",0.6568248867988586
Blogs,24,207,baselines,divergence,between,distribution of words,divergence between distribution of words,0.6685817837715149
Blogs,24,207,baselines,distribution of words,in,summary and in the sources,distribution of words in summary and in the sources,0.5673806667327881
Blogs,24,207,baselines,baselines,has,"kl and js ( haghighi and vanderwende , 2009 )","baselines has kl and js ( haghighi and vanderwende , 2009 )",0.542438268661499
Blogs,24,208,baselines,two baselines,account for,background knowledge,two baselines account for background knowledge,0.6762086749076843
Blogs,24,208,baselines,two baselines,measure,divergence,two baselines measure divergence,0.6814175844192505
Blogs,24,211,results,correlate better,with,human judgments,correlate better with human judgments,0.6654916405677795
Blogs,24,211,results,human judgments,than,other quantities,human judgments than other quantities,0.6305435299873352
Blogs,24,211,results,relevance,has,correlate better,relevance has correlate better,0.5744490027427673
Blogs,24,211,results,results,modelizations of,relevance,results modelizations of relevance,0.6001978516578674
Blogs,24,212,results,metrics,accounting for,background knowledge,metrics accounting for background knowledge,0.5810236930847168
Blogs,24,212,results,background knowledge,work,better,background knowledge work better,0.6308783292770386
Blogs,24,212,results,better,in,update scenario,better in update scenario,0.5352869033813477
Blogs,24,212,results,results,has,metrics,results has metrics,0.5256302952766418
Blogs,24,214,results,js divergence,gives,slightly better results,js divergence gives slightly better results,0.6171642541885376
Blogs,24,214,results,slightly better results,than,kl,slightly better results than kl,0.6050073504447937
Blogs,24,214,results,results,observe,js divergence,results observe js divergence,0.5953718423843384
Blogs,24,216,results,all baselines,in both,generic and the update case,all baselines in both generic and the update case,0.6559567451477051
Blogs,24,216,results,significantly,has,outperforms,significantly has outperforms,0.6342213749885559
Blogs,24,216,results,outperforms,has,all baselines,outperforms has all baselines,0.5888515114784241
Blogs,24,216,results,results,has,significantly,results has significantly,0.5334484577178955
Blogs,25,109,baselines,rnn,has,"nallapati et al. , 2017 )","rnn has nallapati et al. , 2017 )",0.5668616890907288
Blogs,25,18,model,approach,for,aligning,approach for aligning,0.6587679386138916
Blogs,25,18,model,approach,incorporating,weighting,approach incorporating weighting,0.7828944325447083
Blogs,25,18,model,aligning,incorporating,weighting,aligning incorporating weighting,0.753126859664917
Blogs,25,18,model,chapter sentences,with,abstractive summary sentences,chapter sentences with abstractive summary sentences,0.5818070769309998
Blogs,25,18,model,chapter sentences,incorporating,weighting,chapter sentences incorporating weighting,0.7486263513565063
Blogs,25,18,model,weighting,to,"rouge ( lin , 2004 )","weighting to rouge ( lin , 2004 )",0.5591751933097839
Blogs,25,18,model,weighting,to,"meteor ( lavie and denkowski , 2009 ) metrics","weighting to meteor ( lavie and denkowski , 2009 ) metrics",0.5255035161972046
Blogs,25,18,model,weighting,to enable,alignment,weighting to enable alignment,0.7243720889091492
Blogs,25,18,model,"meteor ( lavie and denkowski , 2009 ) metrics",to enable,alignment,"meteor ( lavie and denkowski , 2009 ) metrics to enable alignment",0.7024604082107544
Blogs,25,18,model,alignment,of,salient words,alignment of salient words,0.5636188983917236
Blogs,25,18,model,aligning,has,chapter sentences,aligning has chapter sentences,0.5203302502632141
Blogs,25,20,results,stable matching algorithm,to select,best alignments,stable matching algorithm to select best alignments,0.6543415188789368
Blogs,25,20,results,one - toone alignments,between,reference summary sentences and chapter sentences,one - toone alignments between reference summary sentences and chapter sentences,0.626510500907898
Blogs,25,20,results,one - toone alignments,is,best alignment method,one - toone alignments is best alignment method,0.5541653037071228
Blogs,25,20,results,reference summary sentences and chapter sentences,is,best alignment method,reference summary sentences and chapter sentences is best alignment method,0.5405482649803162
Blogs,25,20,results,results,use,stable matching algorithm,results use stable matching algorithm,0.6385974884033203
Blogs,25,96,results,crowd -sourced evaluation,show that,sentence - level stable matching,crowd -sourced evaluation show that sentence - level stable matching,0.4638410210609436
Blogs,25,96,results,sentence - level stable matching,is,significantly better,sentence - level stable matching is significantly better,0.5254600048065186
Blogs,25,96,results,results,has,crowd -sourced evaluation,results has crowd -sourced evaluation,0.48021620512008667
Blogs,25,122,results,our proposed alignment method,performs,significantly better,our proposed alignment method performs significantly better,0.6041502356529236
Blogs,25,122,results,significantly better,for,all mod-model,significantly better for all mod-model,0.6419747471809387
Blogs,25,122,results,results,see,our proposed alignment method,results see our proposed alignment method,0.5280706286430359
Blogs,25,126,results,k and n baseline models,perform,better,k and n baseline models perform better,0.5895491242408752
Blogs,25,126,results,better,than,cb baseline,better than cb baseline,0.6134340167045593
Blogs,25,126,results,results,at,first glance,results at first glance,0.5233327746391296
Blogs,26,4,baselines,opiniondigest,has,abstractive opinion summarization framework,opiniondigest has abstractive opinion summarization framework,0.5161064863204956
Blogs,26,69,baselines,baselines,has,baselines lexrank,baselines has baselines lexrank,0.5341866612434387
Blogs,26,78,hyperparameters,opinion merging,used,pre-trained word embeddings ( glove .6b.300d ),opinion merging used pre-trained word embeddings ( glove .6b.300d ),0.5293639898300171
Blogs,26,78,hyperparameters,pre-trained word embeddings ( glove .6b.300d ),selected,top -k ( k = 15 ) most popular opinion clusters,pre-trained word embeddings ( glove .6b.300d ) selected top -k ( k = 15 ) most popular opinion clusters,0.5711754560470581
Blogs,26,78,hyperparameters,hyperparameters,For,opinion merging,hyperparameters For opinion merging,0.5773223042488098
Blogs,26,79,hyperparameters,transformer,with,"original architecture ( vaswani et al. , 2017 )","transformer with original architecture ( vaswani et al. , 2017 )",0.6179513335227966
Blogs,26,79,hyperparameters,hyperparameters,trained,transformer,hyperparameters trained transformer,0.6320951581001282
Blogs,26,80,hyperparameters,sgd,with,initial learning rate,sgd with initial learning rate,0.5956356525421143
Blogs,26,80,hyperparameters,sgd,with,momentum,sgd with momentum,0.7183332443237305
Blogs,26,80,hyperparameters,sgd,with,decay,sgd with decay,0.652528703212738
Blogs,26,80,hyperparameters,sgd,with,batch size,sgd with batch size,0.6358768343925476
Blogs,26,80,hyperparameters,initial learning rate,of,0.1,initial learning rate of 0.1,0.5805032253265381
Blogs,26,80,hyperparameters,momentum,of,? = 0.1,momentum of ? = 0.1,0.6407715082168579
Blogs,26,80,hyperparameters,decay,of,? = 0.1,decay of ? = 0.1,0.6398928165435791
Blogs,26,80,hyperparameters,decay,with,batch size,decay with batch size,0.6632699370384216
Blogs,26,80,hyperparameters,? = 0.1,for,5 epochs,? = 0.1 for 5 epochs,0.6821706891059875
Blogs,26,80,hyperparameters,? = 0.1,with,batch size,? = 0.1 with batch size,0.6523872017860413
Blogs,26,80,hyperparameters,5 epochs,with,batch size,5 epochs with batch size,0.618772029876709
Blogs,26,80,hyperparameters,batch size,of,8,batch size of 8,0.6920418739318848
Blogs,26,80,hyperparameters,hyperparameters,used,sgd,hyperparameters used sgd,0.6063317060470581
Blogs,26,81,hyperparameters,decoding,used,beam search,decoding used beam search,0.5754576921463013
Blogs,26,81,hyperparameters,beam search,with,beam size,beam search with beam size,0.6320963501930237
Blogs,26,81,hyperparameters,beam search,with,length penalty,beam search with length penalty,0.6435361504554749
Blogs,26,81,hyperparameters,beam search,with,3,beam search with 3,0.6815756559371948
Blogs,26,81,hyperparameters,beam search,with,maximum generation length,beam search with maximum generation length,0.6182403564453125
Blogs,26,81,hyperparameters,beam size,of,5,beam size of 5,0.7073217034339905
Blogs,26,81,hyperparameters,beam size,of,3,beam size of 3,0.6917811036109924
Blogs,26,81,hyperparameters,length penalty,of,0.6,length penalty of 0.6,0.5868892073631287
Blogs,26,81,hyperparameters,maximum generation length,of,60,maximum generation length of 60,0.6618744730949402
Blogs,26,81,hyperparameters,beam search,has,length penalty,beam search has length penalty,0.5589801073074341
Blogs,26,81,hyperparameters,3,has,gram blocking,3 has gram blocking,0.5676985383033752
Blogs,26,81,hyperparameters,hyperparameters,For,decoding,hyperparameters For decoding,0.5911394953727722
Blogs,26,5,model,aspect- based sentiment analysis model,to extract,opinion phrases,aspect- based sentiment analysis model to extract opinion phrases,0.6940546631813049
Blogs,26,5,model,aspect- based sentiment analysis model,trains,transformer model,aspect- based sentiment analysis model trains transformer model,0.7185680270195007
Blogs,26,5,model,opinion phrases,from,reviews,opinion phrases from reviews,0.5806528329849243
Blogs,26,5,model,transformer model,to reconstruct,original reviews,transformer model to reconstruct original reviews,0.6950222849845886
Blogs,26,6,model,summarization time,merge,extractions,summarization time merge extractions,0.7695177793502808
Blogs,26,6,model,summarization time,select,most popular ones,summarization time select most popular ones,0.6043796539306641
Blogs,26,6,model,extractions,from,multiple reviews,extractions from multiple reviews,0.533413827419281
Blogs,26,6,model,model,At,summarization time,model At summarization time,0.5314134359359741
Blogs,26,25,model,more interpretable and controllable opinion aggregation,replace,end-to - end architectures,more interpretable and controllable opinion aggregation replace end-to - end architectures,0.5715592503547668
Blogs,26,25,model,end-to - end architectures,with,pipeline framework,end-to - end architectures with pipeline framework,0.6511389017105103
Blogs,26,26,model,pre-trained opinion extractor,identifies,opinion phrases,pre-trained opinion extractor identifies opinion phrases,0.639421284198761
Blogs,26,26,model,opinion phrases,in,reviews,opinion phrases in reviews,0.5275474190711975
Blogs,26,26,model,opinion phrases,in,reviews,opinion phrases in reviews,0.5275474190711975
Blogs,26,26,model,reviews,from,extracted opinion phrases,reviews from extracted opinion phrases,0.5096780061721802
Blogs,26,26,model,simple and controllable opinion selector,"merges , ranks , and - optionally - filters",extracted opinions,"simple and controllable opinion selector merges , ranks , and - optionally - filters extracted opinions",0.7324022650718689
Blogs,26,26,model,generator model,trained to reconstruct,reviews,generator model trained to reconstruct reviews,0.7169113755226135
Blogs,26,26,model,generator model,generate,opinion summaries,generator model generate opinion summaries,0.7026358246803284
Blogs,26,26,model,reviews,from,extracted opinion phrases,reviews from extracted opinion phrases,0.5096780061721802
Blogs,26,26,model,opinion summaries,based on,selected opinions,opinion summaries based on selected opinions,0.6593551635742188
Blogs,26,70,model,sentences,based on,centrality scores,sentences based on centrality scores,0.5979856848716736
Blogs,26,70,model,centrality scores,calculated on,graph - based sentence similarity,centrality scores calculated on graph - based sentence similarity,0.6583899259567261
Blogs,26,70,model,model,selects,sentences,model selects sentences,0.766882061958313
Blogs,26,87,results,our framework,has,outperforms,our framework has outperforms,0.6379533410072327
Blogs,26,87,results,outperforms,has,all baseline approaches,outperforms has all baseline approaches,0.5795095562934875
Blogs,26,87,results,results,has,our framework,results has our framework,0.6097875237464905
Blogs,26,94,results,opiniondigest,produced,significantly more sentences,opiniondigest produced significantly more sentences,0.658849835395813
Blogs,26,94,results,opiniondigest,produced,fewer sentences,opiniondigest produced fewer sentences,0.6568509340286255
Blogs,26,94,results,significantly more sentences,with,full or partial support,significantly more sentences with full or partial support,0.5621799826622009
Blogs,26,94,results,fewer sentences,without,any support,fewer sentences without any support,0.7153757214546204
Blogs,26,94,results,results,has,opiniondigest,results has opiniondigest,0.5660804510116577
Blogs,27,54,baselines,extremely long documents that exceed several thousand words,perform,sentence extraction,extremely long documents that exceed several thousand words perform sentence extraction,0.5905143618583679
Blogs,27,54,baselines,sentence extraction,using,two different hierarchical document models,sentence extraction using two different hierarchical document models,0.5762581825256348
Blogs,27,54,baselines,two different hierarchical document models,based on,pointer networks,two different hierarchical document models based on pointer networks,0.6068564057350159
Blogs,27,54,baselines,one,based on,pointer networks,one based on pointer networks,0.6217013001441956
Blogs,27,54,baselines,other,based on,sentence classifier,other based on sentence classifier,0.605367124080658
Blogs,27,54,baselines,two different hierarchical document models,has,one,two different hierarchical document models has one,0.5668711066246033
Blogs,27,110,experimental-setup,word embeddings,of size,300,word embeddings of size 300,0.692033052444458
Blogs,27,111,experimental-setup,"token - level lstm ( sentence encoder ) , sentencelevel lstm ( document encoder ) and decoder",2 layers of,512 units,"token - level lstm ( sentence encoder ) , sentencelevel lstm ( document encoder ) and decoder 2 layers of 512 units",0.8056116104125977
Blogs,27,111,experimental-setup,dropout,of,0.5,dropout of 0.5,0.6125851273536682
Blogs,27,111,experimental-setup,0.5,applied at,output,0.5 applied at output,0.6885941028594971
Blogs,27,111,experimental-setup,output,of,each intermediate layer,output of each intermediate layer,0.5980161428451538
Blogs,27,111,experimental-setup,experimental setup,has,"token - level lstm ( sentence encoder ) , sentencelevel lstm ( document encoder ) and decoder","experimental setup has token - level lstm ( sentence encoder ) , sentencelevel lstm ( document encoder ) and decoder",0.5452873706817627
Blogs,27,112,experimental-setup,weight decay,of,10 ?5,weight decay of 10 ?5,0.634735643863678
Blogs,27,112,experimental-setup,batch sizes,of,32,batch sizes of 32,0.6218165159225464
Blogs,27,112,experimental-setup,adam,has,learning rate,adam has learning rate,0.5355743169784546
Blogs,27,112,experimental-setup,adam,has,weight decay,adam has weight decay,0.573876678943634
Blogs,27,112,experimental-setup,learning rate,has,0.001,learning rate has 0.001,0.5318801999092102
Blogs,27,112,experimental-setup,experimental setup,trained it with,adam,experimental setup trained it with adam,0.5596476197242737
Blogs,27,113,experimental-setup,model,every,200 updates,model every 200 updates,0.767342209815979
Blogs,27,113,experimental-setup,model,using,patience,model using patience,0.7123204469680786
Blogs,27,113,experimental-setup,patience,of,50,patience of 50,0.6740272045135498
Blogs,27,113,experimental-setup,experimental setup,evaluate,model,experimental setup evaluate model,0.6536856889724731
Blogs,27,117,experimental-setup,220m parameters,with,20 layers,220m parameters with 20 layers,0.6767699718475342
Blogs,27,117,experimental-setup,model,has,220m parameters,model has 220m parameters,0.5482109189033508
Blogs,27,117,experimental-setup,experimental setup,has,model,experimental setup has model,0.5338840484619141
Blogs,27,119,experimental-setup,language model,for,5 days,language model for 5 days,0.6387166380882263
Blogs,27,119,experimental-setup,language model,on,16 v100 gpus,language model on 16 v100 gpus,0.487140417098999
Blogs,27,119,experimental-setup,5 days,on,16 v100 gpus,5 days on 16 v100 gpus,0.5183734893798828
Blogs,27,119,experimental-setup,16 v100 gpus,on,single nvidia dgx - 2 box,16 v100 gpus on single nvidia dgx - 2 box,0.5489036440849304
Blogs,27,119,experimental-setup,experimental setup,trained,language model,experimental setup trained language model,0.6622421145439148
Blogs,27,120,experimental-setup,linear ramp - up learning rate schedule,for,"first 40 , 000 updates","linear ramp - up learning rate schedule for first 40 , 000 updates",0.6255262494087219
Blogs,27,120,experimental-setup,maximum learning rate,of,2.5 ? e ?4,maximum learning rate of 2.5 ? e ?4,0.6289032697677612
Blogs,27,120,experimental-setup,maximum learning rate,followed by,cosine annealing schedule,maximum learning rate followed by cosine annealing schedule,0.6636931896209717
Blogs,27,120,experimental-setup,cosine annealing schedule,to,0,cosine annealing schedule to 0,0.6148082613945007
Blogs,27,120,experimental-setup,cosine annealing schedule,over,"next 200 , 000 steps","cosine annealing schedule over next 200 , 000 steps",0.6578680276870728
Blogs,27,120,experimental-setup,0,over,"next 200 , 000 steps","0 over next 200 , 000 steps",0.6826342344284058
Blogs,27,120,experimental-setup,"next 200 , 000 steps",with,adam optimizer,"next 200 , 000 steps with adam optimizer",0.6261475086212158
Blogs,27,120,experimental-setup,experimental setup,used,linear ramp - up learning rate schedule,experimental setup used linear ramp - up learning rate schedule,0.5852352976799011
Blogs,27,121,experimental-setup,mixed - precision training,with,batch size,mixed - precision training with batch size,0.6287617683410645
Blogs,27,121,experimental-setup,batch size,of,256 sequences,batch size of 256 sequences,0.6127116680145264
Blogs,27,121,experimental-setup,256 sequences,of,1024 tokens each,256 sequences of 1024 tokens each,0.59809809923172
Blogs,27,142,results,previous extractive baselines,on,arxiv and pubmed datasets,previous extractive baselines on arxiv and pubmed datasets,0.49551668763160706
Blogs,27,142,results,able to outperform,has,previous extractive baselines,able to outperform has previous extractive baselines,0.583097517490387
Blogs,27,142,results,results,has,our extractive models,results has our extractive models,0.5504970550537109
Blogs,27,143,results,our tlm,conditioned on,extractive summary,our tlm conditioned on extractive summary,0.6992188096046448
Blogs,27,143,results,extractive summary,produced by,"our best extractive model ( tlm - i+e ( g , m )","extractive summary produced by our best extractive model ( tlm - i+e ( g , m )",0.610707700252533
Blogs,27,143,results,prior abstractive / mixed results,on,"arxiv , pubmed and bigpatent datasets","prior abstractive / mixed results on arxiv , pubmed and bigpatent datasets",0.5351158380508423
Blogs,27,143,results,our tlm,has,outperforms,our tlm has outperforms,0.6397028565406799
Blogs,27,143,results,extractive summary,has,outperforms,extractive summary has outperforms,0.6309934854507446
Blogs,27,143,results,"our best extractive model ( tlm - i+e ( g , m )",has,outperforms,"our best extractive model ( tlm - i+e ( g , m ) has outperforms",0.586020290851593
Blogs,27,143,results,outperforms,has,prior abstractive / mixed results,outperforms has prior abstractive / mixed results,0.5948206782341003
Blogs,27,143,results,results,has,our tlm,results has our tlm,0.5886947512626648
Blogs,27,173,results,proposed model,produces,more   abstractive   summaries,proposed model produces more   abstractive   summaries,0.6573891043663025
Blogs,27,173,results,results,has,proposed model,results has proposed model,0.5938616394996643
Blogs,27,174,results,our model,copy,longer sequences,our model copy longer sequences,0.6764669418334961
Blogs,27,174,results,longer sequences,conditioned on,introduction and the sentences,longer sequences conditioned on introduction and the sentences,0.7440994381904602
Blogs,27,174,results,introduction and the sentences,from,extractor,introduction and the sentences from extractor,0.5475273132324219
Blogs,27,174,results,results,has,our model,results has our model,0.5871725678443909
