topic,paper_ID,sentence_ID,info-unit,sub,pred,obj
summarization,0,40,ablation-analysis,intrinsic quality,of,citation contextualisation
summarization,0,40,ablation-analysis,direct impact,on,end
summarization,0,40,ablation-analysis,citation contextualisation,has,direct impact
summarization,0,40,ablation-analysis,Ablation analysis,showcase,intrinsic quality
summarization,0,4,baselines,Baselines,has,Citation text
summarization,0,31,experiments,performance,of,biomedical embeddings
summarization,0,31,experiments,biomedical embeddings,with,retro tting
summarization,0,31,experiments,biomedical embeddings,with,interpolating domain knowledge
summarization,0,10,hyperparameters,embeddings,to score,documents
summarization,0,10,hyperparameters,semantically similar,to,chosen query
summarization,0,10,hyperparameters,Hyperparameters,used,embeddings
summarization,0,11,hyperparameters,Hyperparameters,used,domain ontologies
summarization,0,28,hyperparameters,MESH and Protein Ontologies ( PO ),to modify,word embeddings
summarization,0,28,hyperparameters,MESH and Protein Ontologies ( PO ),pull,synonymous words closer together
summarization,0,28,hyperparameters,Hyperparameters,used,MESH and Protein Ontologies ( PO )
summarization,0,30,hyperparameters,performance,of,training
summarization,0,30,hyperparameters,training,using,Wikipedia embeddings and biomedical embeddings
summarization,0,30,hyperparameters,Hyperparameters,directly modi ed,language model
summarization,0,30,hyperparameters,Hyperparameters,incorporate,performance
summarization,0,8,model,language models,by incorporating,word embeddings
summarization,0,8,model,language models,by incorporating,domain ontology
summarization,0,12,model,domain ontologies,extend,embedding based language models
summarization,0,12,model,Model,has,domain ontologies
summarization,0,16,results,biomedical embeddings,with,domain knowledge interpolation
summarization,0,16,results,domain knowledge interpolation,achieved,best performance
summarization,0,16,results,best performance,in,most of the evaluation metrics
summarization,0,16,results,Results,our model of,biomedical embeddings
summarization,0,17,results,our models,are,effective
summarization,0,17,results,effective,in,many different aspects
summarization,0,17,results,Results,indicates,our models
summarization,0,18,results,general embedding,does not provide,much advantage
summarization,0,18,results,much advantage,compared to,best performing baseline model
summarization,0,18,results,Results,found that,general embedding
summarization,0,19,results,domain-speci c embeddings,observed,10 % c-F improvement
summarization,0,19,results,10 % c-F improvement,over,best performing baseline model
summarization,0,19,results,Results,used,domain-speci c embeddings
summarization,0,20,results,biomedical embeddings,better at capturing,word relations
summarization,0,20,results,word relations,in,biomedical context
summarization,0,21,results,top relevant words,for,word
summarization,0,21,results,expression,provided by,two types of embeddings
summarization,0,21,results,Results,showcase,top relevant words
summarization,0,22,results,bene t,of including,domain ontologies
summarization,0,22,results,interpolation,yielded,stronger improvement
summarization,0,22,results,stronger improvement,than,retro tting
summarization,0,22,results,Results,showcase,bene t
summarization,0,24,results,Our models,correlate well with,human annotations
summarization,0,24,results,Results,has,Our models
summarization,0,25,results,human precision,is,high ( upper quartiles )
summarization,0,25,results,our system,performs,better
summarization,0,25,results,better,in,c-F metric
summarization,0,25,results,better,with,higher con dence ( lower standard deviation )
summarization,0,25,results,c-F metric,with,higher con dence ( lower standard deviation )
summarization,0,25,results,human precision,has,our system
summarization,0,33,results,nDCG scores,treat,any partial overlaps
summarization,0,33,results,any partial overlaps,with,goal standard
summarization,0,33,results,goal standard,as,correct context
summarization,0,33,results,ROUGE -N scores,to measure,content similarity
summarization,0,33,results,content similarity,between,retrieved contexts and gold standard
summarization,0,33,results,Results,has,nDCG scores
summarization,0,35,results,summarisation quality,between,no contextualisation
summarization,0,35,results,summarisation quality,between,our proposed contextualisation approaches
summarization,0,35,results,Results,compare,summarisation quality
summarization,0,36,results,evaluation metric,is,ROUGE scores
summarization,0,37,results,No context,showcase,performance
summarization,0,37,results,performance,of,all summarisation approach
summarization,0,37,results,all summarisation approach,on,citations
summarization,0,37,results,all summarisation approach,without,context
summarization,0,37,results,Results,has,No context
summarization,1,6,ablation-analysis,domain-speci c ontology,to improve,content selection
summarization,1,6,ablation-analysis,SOTA results,in terms of,ROUGE scores
summarization,1,6,ablation-analysis,Ablation analysis,use,domain-speci c ontology
summarization,1,35,ablation-analysis,3/8,showcase,attention weights
summarization,1,35,ablation-analysis,attention weights,of,two samples
summarization,1,35,ablation-analysis,Ablation analysis,showcase,attention weights
summarization,1,35,ablation-analysis,Ablation analysis,has,3/8
summarization,1,24,baselines,baseline models,are,LSA
summarization,1,24,baselines,baseline models,are,LexRank
summarization,1,24,baselines,baseline models,are,vanilla pointer - generator model
summarization,1,24,baselines,baseline models,are,background - aware PG
summarization,1,24,baselines,vanilla pointer - generator model,with,copy mechanism
summarization,1,24,baselines,Baselines,has,baseline models
summarization,1,19,experimental-setup,QuickUMLS,to extract,UMLS concepts
summarization,1,19,experimental-setup,UMLS concepts,from,FINDINGS section
summarization,1,19,experimental-setup,Experimental setup,use,QuickUMLS
summarization,1,21,experiments,exact n-gram matching,to identify,important radiology entities
summarization,1,11,model,original pointer - generator network,with,domain-speci c ontology
summarization,1,11,model,Model,extend,original pointer - generator network
summarization,1,12,model,ontologies ( UMLS or RadLex ),to,input clinical text
summarization,1,12,model,input clinical text,to from,new encoded sequence
summarization,1,12,model,Model,link,ontologies ( UMLS or RadLex )
summarization,1,13,model,mapping function,to output,concepts
summarization,1,13,model,concepts,if,input token
summarization,1,13,model,input token,appears in,ontology
summarization,1,13,model,Model,by using,mapping function
summarization,1,14,model,second biLSTM,to encode,additional ontology terms
summarization,1,14,model,second biLSTM,to encode,additional context vector
summarization,1,14,model,additional context vector,uses,domain-ontology sequence
summarization,1,15,model,additional context vector,acts as,additional global information
summarization,1,15,model,additional global information,to help,decoding process
summarization,1,15,model,decoding process,modi,decoder
summarization,1,15,model,decoder,to accept,new ontological - aware context vector
summarization,1,15,model,Model,has,additional context vector
summarization,1,18,model,UMLS,is,medical ontology
summarization,1,18,model,medical ontology,includes,"different procedures , conditions , symptoms"
summarization,1,18,model,Model,has,UMLS
summarization,1,20,model,RadLex,contains,widely - used ontology
summarization,1,20,model,RadLex,organised in,hierarchical structure
summarization,1,20,model,widely - used ontology,of,radiology terms
summarization,1,20,model,Model,has,RadLex
summarization,1,5,results,Radiology report,consists of,two main sections
summarization,1,5,results,Results,has,Radiology report
summarization,1,7,results,human evaluation,on,how good
summarization,1,7,results,how good,in retaining,salient information
summarization,1,7,results,generated summaries,in retaining,salient information
summarization,1,7,results,how good,has,generated summaries
summarization,1,22,results,41066 radiology reports,from,MedStar Georgetown University Hospital
summarization,1,23,results,evaluation metric,is,ROUGE scores
summarization,1,23,results,Results,has,evaluation metric
summarization,1,28,results,our RadLex - generated summary,improves,completeness and accuracy
summarization,1,28,results,completeness and accuracy,while maintaining,readability
summarization,1,28,results,Results,show,our RadLex - generated summary
summarization,1,29,results,net gain,of,10 reports
summarization,1,29,results,10 reports,shown,improvement
summarization,1,29,results,improvement,in,completeness
summarization,1,29,results,completeness,between,score 3 and 4
summarization,1,30,results,signi cantly less critical errors,only 5 % of,generated summaries
summarization,1,31,results,RadLex - generated summaries,to identify,ontology - aware models
summarization,1,31,results,outperformed,on both,dev and test set
summarization,1,31,results,other baseline models,on both,dev and test set
summarization,1,31,results,RadLex ontology model,achieving,highest performance
summarization,1,31,results,ontology - aware models,has,outperformed
summarization,1,31,results,outperformed,has,other baseline models
summarization,1,32,results,LexRank and LSA,has,signi cantly underperformed
summarization,1,32,results,Results,has,LexRank and LSA
summarization,1,33,results,our ontology - aware decoder,able to attend,more radiology terms
summarization,1,33,results,Results,examine,attention weights
summarization,1,36,results,our model,attend,more radiological terms
summarization,1,36,results,more radiological terms,within,FINDINGS section
summarization,1,36,results,more radiological terms,improving,summary generation process
summarization,1,36,results,vanilla PG,has,our model
summarization,1,36,results,Results,Relative to,vanilla PG
summarization,1,37,results,100 reports,where,each report
summarization,1,37,results,each report,has,radiology FINDINGS
summarization,1,37,results,Results,has,HUMAN EVALUATIONWe
summarization,1,38,results,radiologist,score,each IMPRESSION
summarization,1,38,results,Results,has,radiologist
summarization,1,39,results,summaries,could,improve further
summarization,1,39,results,Ryan Data Scientist,has,summaries
summarization,1,39,results,Results,has,Ryan Data Scientist
summarization,1,40,results,RadLex - generated summaries,able to select,important points
summarization,1,40,results,RadLex - generated summaries,able to identify,important points
summarization,1,40,results,important points,related to,RadLex terms
summarization,1,40,results,important points,able to identify,important points
summarization,1,40,results,radiologist,has,missed
summarization,1,40,results,Results,has,RadLex - generated summaries
summarization,1,41,results,our approach,generates,repetitive sentences
summarization,1,41,results,repetitive sentences,by doing,further pre- processing
summarization,1,41,results,mitigate,by doing,further pre- processing
summarization,1,41,results,further pre- processing,removing,repetitive n-grams
summarization,1,42,results,our approach,occasionally mix up,details
summarization,1,42,results,details,leading to,factual inconsistent
summarization,2,14,experimental-setup,data collection process,removed,any documents
summarization,2,14,experimental-setup,any documents,do not have,abstract or discourse structure
summarization,2,14,experimental-setup,Experimental setup,During,data collection process
summarization,2,15,experimental-setup,any math formulas and citation markers,with,special tokens
summarization,2,15,experimental-setup,Experimental setup,remove,any gures and tables
summarization,2,5,model,architecture,consists of,hierarchical encoder
summarization,2,5,model,architecture,consists of,attentive discourse - aware decoder
summarization,2,5,model,hierarchical encoder,captures,discourse structure
summarization,2,5,model,hierarchical encoder,captures,attentive discourse - aware decoder
summarization,2,5,model,discourse structure,of,research paper
summarization,2,5,model,attentive discourse - aware decoder,to generate,summary
summarization,2,5,model,Model,has,architecture
summarization,2,7,model,additional binary variable,to,decoder
summarization,2,7,model,additional binary variable,to determine,decoder
summarization,2,7,model,decoder,to determine,decoder
summarization,2,7,model,decoder,generate,word
summarization,2,7,model,Model,added,additional binary variable
summarization,2,8,model,learned and optimised,during,training
summarization,2,8,model,Model,has,copy probability
summarization,2,11,model,coverage vector,includes,information
summarization,2,11,model,coverage vector,incorporated into,attention function
summarization,2,11,model,information,about,attended document discourse sections
summarization,2,11,model,information,incorporated into,attention function
summarization,2,11,model,Model,has,coverage vector
summarization,2,16,model,abstract,is,ground-truth
summarization,2,16,model,Model,has,abstract
summarization,2,22,model,hierarchical RNN,captures,document discourse structure
summarization,2,23,model,encoder,rst encode,each discourse section
summarization,2,23,model,each discourse section,by parsing in,all the words
summarization,2,23,model,all the words,into,respective section RNN
summarization,2,23,model,Model,has,encoder
summarization,2,24,model,outputs,of,all section RNNs
summarization,2,24,model,outputs,feed,hidden states
summarization,2,24,model,hidden states,into,another RNN
summarization,2,24,model,another RNN,to encode,whole document
summarization,2,24,model,Model,takes,outputs
summarization,2,26,model,our decoder,words of,document
summarization,2,26,model,our decoder,attend to,relevant discourse section
summarization,2,26,model,each decoding step,has,our decoder
summarization,2,26,model,Model,At,each decoding step
summarization,2,27,model,discourse-related information,to modify,wordlevel attention function
summarization,2,27,model,Model,use,discourse-related information
summarization,2,28,model,decoder,use,decoder state and context vector
summarization,2,28,model,decoder state and context vector,to predict,next word
summarization,2,28,model,next word,in,summary
summarization,2,28,model,each decoding step,has,decoder
summarization,2,28,model,Model,At,each decoding step
summarization,2,17,results,average document length,being,3000 - 5000 words
summarization,2,31,results,Results,on,arXiv and PubMed dataset
summarization,2,32,results,able to outperformed,has,all the baseline models
summarization,2,32,results,all the baseline models,has,extractive and abstractive
summarization,2,32,results,Results,show,discourse- aware model
summarization,2,33,results,our model,able to generate,summaries
summarization,2,33,results,summaries,able to capture,methodology
summarization,3,18,ablation-analysis,MLM,n't improve,performance
summarization,3,18,ablation-analysis,MLM,n't improve,ne-tuning performance
summarization,3,18,ablation-analysis,performance,of,Ind -Orig variant
summarization,3,18,ablation-analysis,ne-tuning performance,with,long training steps
summarization,3,18,ablation-analysis,Ablation analysis,shows,MLM
summarization,3,18,ablation-analysis,Ablation analysis,exclude,MLM
summarization,3,23,ablation-analysis,study,set,best choices
summarization,3,23,ablation-analysis,best choices,to scale,PEGASUS model
summarization,3,23,ablation-analysis,Ablation analysis,set,best choices
summarization,3,26,ablation-analysis,our PEGASUS,to,copy
summarization,3,26,ablation-analysis,20 %,of,selected sentences
summarization,3,26,ablation-analysis,left unchanged,instead of,mask
summarization,3,26,ablation-analysis,mask,in,original input
summarization,3,26,ablation-analysis,our PEGASUS,has,20 %
summarization,3,26,ablation-analysis,copy,has,20 %
summarization,3,26,ablation-analysis,Ablation analysis,to encourage,our PEGASUS
summarization,3,27,ablation-analysis,increasing GSR,to,45 %
summarization,3,27,ablation-analysis,increasing GSR,to achieve,optimal GSR ratio
summarization,3,27,ablation-analysis,Ablation analysis,led us to,increasing GSR
summarization,3,8,baselines,pre-training objective GSG,select and mask,whole sentences
summarization,3,8,baselines,whole sentences,from,documents
summarization,3,11,baselines,three strategies,to select,gap sentences
summarization,3,16,baselines,ind-orig variant,to train,our PEGASUS - LARGE
summarization,3,37,baselines,four variants,to,principal strategy
summarization,3,37,baselines,ABLATIONS,ON,PEGASUS - BASE
summarization,3,10,experiments,gap sentences ratio,computes,number of selected gap sentences
summarization,3,10,experiments,number of selected gap sentences,total number of sentences in,document
summarization,3,25,experiments,PEGASUS -LARGE,uses,GSG ( Ind - Orig )
summarization,3,25,experiments,PEGASUS -LARGE,uses,SentencePiece Unigram vocabulary size
summarization,3,25,experiments,PEGASUS -LARGE,pre-train on,C4 and HugeNews corpus
summarization,3,25,experiments,GSG ( Ind - Orig ),without,MLM
summarization,3,25,experiments,MLM,as,pre-training objective
summarization,3,25,experiments,SentencePiece Unigram vocabulary size,of,96K
summarization,3,32,experiments,Human,rate,summaries
summarization,3,32,experiments,summaries,on,1 - 5
summarization,3,19,hyperparameters,important hyperparameter,of,GSG
summarization,3,19,hyperparameters,GSG,is,GSR
summarization,3,19,hyperparameters,GSR,determines,how many sentences to mask
summarization,3,19,hyperparameters,Hyperparameters,has,important hyperparameter
summarization,3,22,hyperparameters,GSR,of,30 %
summarization,3,22,hyperparameters,Hyperparameters,choose,GSR
summarization,3,38,hyperparameters,PEGASUS - BASE and normalised ROUGE scores,to determine,optimal choices
summarization,3,38,hyperparameters,optimal choices,of,"pre-training corpus , objective , and vocabulary size"
summarization,3,38,hyperparameters,Hyperparameters,use,PEGASUS - BASE and normalised ROUGE scores
summarization,3,9,model,concatenated,to form,pseudo summaries
summarization,3,9,model,Model,has,masked sentences
summarization,3,35,model,top-m scored sentences,based on,importance
summarization,3,35,model,Model,selecting,top-m scored sentences
summarization,3,7,results,pre-training objective,that 's,more inline
summarization,3,7,results,pre-training objective,lead to,better and faster ne-tuning performance
summarization,3,7,results,more inline,with,downstream task
summarization,3,7,results,Results,using,pre-training objective
summarization,3,12,results,independently scoring sentences,using,original method ( ind-orig )
summarization,3,12,results,independently scoring sentences,achieved,best performance
summarization,3,12,results,Results,suggests,independently scoring sentences
summarization,3,13,results,Seq-uniq,achieved,second best performance
summarization,3,13,results,Results,has,Seq-uniq
summarization,3,14,results,underperform,against,all the variants of principal method
summarization,3,14,results,Random and lead method,has,underperform
summarization,3,14,results,underperform,has,consistently
summarization,3,14,results,Results,has,Random and lead method
summarization,3,15,results,lead method,performed,well
summarization,3,15,results,lead method,performed,badly
summarization,3,15,results,well,on,two news datasets
summarization,3,15,results,badly,in,two non-news datasets
summarization,3,15,results,Results,has,lead method
summarization,3,17,results,train MLM,with or without,GSG
summarization,3,17,results,Results,show,train MLM
summarization,3,20,results,performance,of,downstream datasets
summarization,3,20,results,downstream datasets,across,different level of GSR
summarization,3,20,results,Results,compared,performance
summarization,3,21,results,best performance GSR,under,50 %
summarization,3,21,results,Results,in terms of,optimal GSR
summarization,3,29,results,performance improvements,between,PEGASUS - BASE and PEGASUS
summarization,3,29,results,Results,showcase,performance improvements
summarization,3,31,results,human evaluation,comparing,generated and human-written summaries
summarization,3,33,results,summaries,as good as,reference summaries
summarization,3,33,results,Results,both our PEGASUS - LARGE pretrained on,C4 and HugeNews
summarization,3,34,results,PEGASUS - LARGE,able to perform,relatively well
summarization,3,34,results,relatively well,compared to,human summaries
summarization,3,34,results,low ne-tuning samples,has,PEGASUS - LARGE
summarization,3,34,results,Results,even with,low ne-tuning samples
summarization,3,41,results,pre-training,with,HugeNews corpus
summarization,3,41,results,pre-training,led to,better performance
summarization,3,41,results,HugeNews corpus,led to,better performance
summarization,3,41,results,better performance,in,downstream datasets
summarization,3,41,results,downstream datasets,of,XSum and CNN / DM
summarization,4,18,ablation-analysis,performance,of,our PG - transformer
summarization,4,18,ablation-analysis,performance,by adding,coverage and n-gram blocking mechanism
summarization,4,18,ablation-analysis,our PG - transformer,by adding,coverage and n-gram blocking mechanism
summarization,4,18,ablation-analysis,Ablation analysis,further improved,performance
summarization,4,23,ablation-analysis,n-gram blocking,eliminates,all repetitions
summarization,4,23,ablation-analysis,Ablation analysis,has,n-gram blocking
summarization,4,3,model,COVERAGE LOSS,has,coverage loss
summarization,4,4,model,coverage loss,compute,minimising
summarization,4,4,model,coverage loss,by,minimising
summarization,4,4,model,minimising,between,attention vector and the coverage vector
summarization,4,4,model,coverage vector,has,coverage loss
summarization,4,4,model,Model,With,coverage vector
summarization,4,6,model,our transformer,ability to copy or generate,words
summarization,4,6,model,words,at,each given time step
summarization,4,6,model,PG Network,has,our transformer
summarization,4,6,model,Model,With,PG Network
summarization,4,7,model,generation probability,computed using,"hidden state , context vector , and decoder input"
summarization,4,7,model,Model,has,generation probability
summarization,4,13,model,Decoder,uses,beam search
summarization,4,13,model,beam search,to construct,summaries
summarization,4,13,model,n-gram blocking,eliminates,those words
summarization,4,13,model,those words,lead to,n-gram
summarization,4,13,model,Model,has,Decoder
summarization,4,11,results,ROUGE scores,has,improved
summarization,4,11,results,Results,show,ROUGE scores
summarization,4,16,results,results,generated from,24 hours
summarization,4,16,results,results,is,signi cant less
summarization,4,16,results,signi cant less,than,PG network
summarization,4,16,results,PG network,took,4 + days
summarization,4,16,results,PG network,took,Sanjabi 's transformer
summarization,4,16,results,PG network,took,2 + days
summarization,4,16,results,Results,generated from,24 hours
summarization,4,17,results,transformer baseline,with,PG - transformer
summarization,4,17,results,transformer baseline,see that,PG network
summarization,4,17,results,PG network,able to improve,performance
summarization,4,17,results,performance,of,our model
summarization,4,17,results,Results,Comparing,transformer baseline
summarization,4,19,results,n-gram block,yielded,much larger performance increase
summarization,4,19,results,much larger performance increase,than,coverage loss
summarization,4,19,results,Results,found,n-gram block
summarization,4,20,results,Baseline transformer,generated,summaries
summarization,4,20,results,summaries,with,lots of repetition
summarization,4,20,results,summaries,unable to handle,OOV words
summarization,4,20,results,Results,has,Baseline transformer
summarization,4,21,results,PG - transformer,able to reduce,OOV mishandling
summarization,4,21,results,PG - transformer,suffer from,factual inconsistency
summarization,4,21,results,Results,has,PG - transformer
summarization,4,22,results,summaries,do n't have,repetition problem
summarization,4,22,results,repetition,starts to appear in,ideas and phrases level
summarization,5,21,ablation-analysis,summaries,generated by,concept pointer
summarization,5,21,ablation-analysis,concept pointer,lowest percentage of,UNK words
summarization,5,21,ablation-analysis,Ablation analysis,show,summaries
summarization,5,42,baselines,concept distribution,on top of,usual text distribution
summarization,5,42,baselines,usual text distribution,over,original source document
summarization,5,54,baselines,selective gate,to control,information
summarization,5,54,baselines,information,owing from,encoder to decoder
summarization,5,57,baselines,CNN,for,encoder and RNN
summarization,5,57,baselines,encoder and RNN,with,attention
summarization,5,57,baselines,attention,for,decoder
summarization,5,57,baselines,Baselines,has,CNN
summarization,5,24,experiments,impact,of,concept update strategy
summarization,5,51,experiments,relevant or irrelevant,for,model adaption
summarization,5,47,hyperparameters,KL divergence,between,each training reference summary
summarization,5,47,hyperparameters,KL divergence,between,set of documents
summarization,5,47,hyperparameters,set of documents,from,test set
summarization,5,50,hyperparameters,KL divergence loss function,included in,training process
summarization,5,50,hyperparameters,KL divergence loss function,measures,overall distance
summarization,5,50,hyperparameters,overall distance,between,test set
summarization,5,50,hyperparameters,overall distance,between,each of our reference summary - document pairs
summarization,5,50,hyperparameters,Hyperparameters,has,KL divergence loss function
summarization,5,5,model,concept set and the original source text,when generating,abstractive summaries
summarization,5,5,model,Model,choose between,concept set and the original source text
summarization,5,7,model,context vector,feed to,our decoder
summarization,5,7,model,context vector,to determine,probability to generating new words ( p_gen )
summarization,5,7,model,our decoder,use,context vector
summarization,5,7,model,context vector,to determine,probability to generating new words ( p_gen )
summarization,5,7,model,probability to generating new words ( p_gen ),from,our vocabulary distribution
summarization,5,7,model,Model,has,context vector
summarization,5,9,model,Microsoft Concept Graph,to map,word
summarization,5,9,model,word,to,related concepts
summarization,5,9,model,Model,use,Microsoft Concept Graph
summarization,5,10,model,knowledge base,covers,huge concept space
summarization,5,10,model,knowledge base,covers,relationships
summarization,5,10,model,relationships,between,concepts and entities
summarization,5,10,model,relationships,are,probabilistic
summarization,5,10,model,Model,has,knowledge base
summarization,5,11,model,concept graph,take in,word
summarization,5,11,model,concept graph,take in,word
summarization,5,11,model,concept graph,estimates,probability
summarization,5,11,model,word,belongs to,particular concept
summarization,5,11,model,probability,that,word
summarization,5,11,model,word,belongs to,particular concept
summarization,5,11,model,Model,has,concept graph
summarization,5,12,model,probabilities,given,each word
summarization,5,12,model,concept graph,set of,concept candidates
summarization,5,12,model,concept candidates,believes,word belongs to
summarization,5,12,model,probabilities,has,concept graph
summarization,5,12,model,each word,has,concept graph
summarization,5,12,model,Model,With,probabilities
summarization,5,13,model,context vector,from,encoder-decoder framework
summarization,5,13,model,Model,to select,right concept candidate
summarization,5,29,model,Abstraction,abstract concepts in,summary
summarization,5,29,model,How appropriate,abstract concepts in,summary
summarization,5,29,model,Abstraction,has,How appropriate
summarization,5,29,model,Model,has,Abstraction
summarization,5,30,model,novel model architecture,proposed,distant supervised learning technique
summarization,5,30,model,distant supervised learning technique,to allow,our model
summarization,5,30,model,our model,to adapt to,different datasets
summarization,5,30,model,Model,On top of,novel model architecture
summarization,5,30,model,Model,proposed,distant supervised learning technique
summarization,5,33,model,encoder-decoder framework,consists of,two -layer bidirectional LSTM - RNN encoder
summarization,5,33,model,encoder-decoder framework,consists of,one- layer LSTM - RNN decoder
summarization,5,33,model,one- layer LSTM - RNN decoder,with,attention mechanism
summarization,5,33,model,Model,has,encoder-decoder framework
summarization,5,34,model,Each word,in,input sequence
summarization,5,34,model,Model,has,Each word
summarization,5,35,model,context vector,computed,attention mechanism
summarization,5,35,model,context vector,applying,attention mechanism
summarization,5,35,model,attention mechanism,over,hidden state representations
summarization,5,35,model,Model,has,context vector
summarization,5,37,model,context vector,to update,concept distribution
summarization,5,37,model,Model,use,context vector
summarization,5,38,model,updated weights,by feeding,current hidden state
summarization,5,38,model,updated weights,by feeding,context vector
summarization,5,38,model,updated weights,by feeding,current concept candidate
summarization,5,38,model,current concept candidate,into,softmax classi er
summarization,5,38,model,Model,compute,updated weights
summarization,5,39,model,updated weight,added to,existing concept probability
summarization,5,39,model,existing concept probability,to factor in,context
summarization,5,39,model,context,of,input sequence
summarization,5,39,model,Model,has,updated weight
summarization,5,40,model,concept pointer network,consists of,normal pointer
summarization,5,40,model,concept pointer network,consists of,concept pointer
summarization,5,40,model,concept pointer network,as,concept pointer
summarization,5,40,model,normal pointer,to,source document
summarization,5,40,model,normal pointer,to,source document
summarization,5,40,model,concept pointer,to,relevant concepts
summarization,5,40,model,relevant concepts,given,source document
summarization,5,40,model,Model,has,concept pointer network
summarization,5,41,model,concept pointer,added to,normal pointer ( attention distribution )
summarization,5,41,model,scaled element -wise,by,attention distribution
summarization,5,41,model,Model,has,concept pointer
summarization,5,45,model,our model,to lower,dissimilarity
summarization,5,45,model,dissimilarity,in,our nal loss
summarization,5,45,model,Model,retrain,our model
summarization,5,46,model,labels,to indicate how close,training set
summarization,5,46,model,training set,to,our test set
summarization,5,46,model,Model,need,labels
summarization,5,48,model,training pairs,are,distantly - labelled
summarization,5,48,model,Model,has,training pairs
summarization,5,49,model,representations,has,of both reference summaries and documents
summarization,5,49,model,Model,has,representations
summarization,5,53,model,temporal attention,on,decoder
summarization,5,53,model,temporal attention,to reduce,repetition
summarization,5,53,model,decoder,to reduce,repetition
summarization,5,53,model,Model,has,temporal attention
summarization,5,67,model,novel model architecture,proposed,distant supervised learning technique
summarization,5,67,model,distant supervised learning technique,to allow,our model
summarization,5,67,model,our model,to adapt to,different datasets
summarization,5,67,model,Model,On top of,novel model architecture
summarization,5,67,model,Model,proposed,distant supervised learning technique
summarization,5,16,results,evaluation metric,is,ROUGE score
summarization,5,16,results,Results,has,evaluation metric
summarization,5,20,results,all the baseline models,on,all metrics
summarization,5,20,results,all metrics,except,RG - 2
summarization,5,20,results,RG - 2,on,Gigaword
summarization,5,20,results,concept pointer,has,outperformed
summarization,5,20,results,outperformed,has,all the baseline models
summarization,5,20,results,Results,has,concept pointer
summarization,5,22,results,abstractiveness,of,our generated summaries
summarization,5,22,results,Results,showcase,abstractiveness
summarization,5,26,results,small variation,in,ROUGE scores
summarization,5,26,results,small variation,between,different number of concept candidates
summarization,5,26,results,ROUGE scores,between,different number of concept candidates
summarization,5,28,results,human evaluations,where,each volunteer
summarization,5,28,results,Results,conducted,human evaluations
summarization,5,31,results,automatic and human evaluation,shown,strong improvement
summarization,5,31,results,strong improvement,over,SOTA baselines
summarization,5,31,results,Results,has,automatic and human evaluation
summarization,5,44,results,summary - document pairs,of,our training set
summarization,5,44,results,summary - document pairs,are,different
summarization,5,44,results,different,than to,testing set
summarization,5,44,results,our model,would perform,poorly
summarization,5,44,results,summary - document pairs,has,our model
summarization,5,44,results,Results,If,summary - document pairs
summarization,5,60,results,Results,has,Normal PG 8 . CGU
summarization,5,63,results,"How readable , relevant , and informative",is,summary
summarization,5,63,results,"How readable , relevant , and informative",randomly selected,20 examples
summarization,5,63,results,summary,randomly selected,20 examples
summarization,5,63,results,20 examples,with,three different summaries
summarization,5,63,results,20 examples,score,often
summarization,5,63,results,Results,has,"How readable , relevant , and informative"
summarization,5,64,results,outperformed,both,seq2seq model
summarization,5,64,results,outperformed,both,pointer generator
summarization,5,64,results,concept pointer network,has,outperformed
summarization,5,64,results,Results,showcase,concept pointer network
summarization,5,65,results,generated summaries,seems to be,uent and informative
summarization,5,65,results,Results,has,generated summaries
summarization,5,68,results,automatic and human evaluation,shown,strong improvement
summarization,5,68,results,strong improvement,over,SOTA baselines
summarization,5,68,results,Results,has,automatic and human evaluation
summarization,6,7,baselines,RNN ( N ),experiment with,alignment methods
summarization,6,7,baselines,alignment methods,applied at both,word and constituent level
summarization,6,7,baselines,Baselines,has,RNN ( N )
summarization,6,8,baselines,evaluation metrics,is,"ROUGE -1 , 2 , L ,"
summarization,6,8,baselines,evaluation metrics,is,METEOR
summarization,6,8,baselines,Baselines,has,evaluation metrics
summarization,6,9,experiments,2 - 5 reference summaries,evaluate,generated summaries
summarization,6,15,model,crowd workers,to identify,generated summary
summarization,6,15,model,generated summary,best convey,sampled reference summary content
summarization,6,15,model,Model,has,crowd workers
summarization,6,21,model,constituents,to make,uent sentences
summarization,6,21,model,uent sentences,without including,irrelevant context
summarization,6,21,model,Model,combine,constituents
summarization,6,11,results,performance,of,three different extractive models
summarization,6,11,results,Results,compared,performance
summarization,6,12,results,outperformed,in,all three extractive models
summarization,6,12,results,baseline method,in,all three extractive models
summarization,6,12,results,proposed alignment method,has,outperformed
summarization,6,12,results,outperformed,has,baseline method
summarization,6,12,results,Results,see that,proposed alignment method
summarization,6,13,results,All three models,seem to perform,similarly
summarization,6,13,results,similarly,using,extractive targets
summarization,6,13,results,Results,has,All three models
summarization,6,14,results,pyramid score,of,each alignment
summarization,6,18,results,"sentence - level , stable -matched alignment method",with,R-wtd similarity metric
summarization,6,18,results,performed better,than,previous method
summarization,6,18,results,previous method,of computing,gold extractive summaries
summarization,6,18,results,R-wtd similarity metric,has,performed better
summarization,6,18,results,Results,shown that,"sentence - level , stable -matched alignment method"
summarization,6,24,results,truncation,of,our documents
summarization,6,24,results,truncation,might,hurt
summarization,6,24,results,our documents,might,hurt
summarization,6,24,results,performance,of,our novel chapter summarisation model
summarization,6,24,results,hurt,has,performance
summarization,6,24,results,Results,has,truncation
summarization,7,23,ablation-analysis,rapidly,as,average score
summarization,7,23,ablation-analysis,average score,of,summaries
summarization,7,23,ablation-analysis,proportion of consistent improvement ( F/ N ratio ),has,decrease
summarization,7,23,ablation-analysis,decrease,has,rapidly
summarization,7,23,ablation-analysis,Ablation analysis,show that,proportion of consistent improvement ( F/ N ratio )
summarization,7,40,ablation-analysis,correlations,between,metrics
summarization,7,40,ablation-analysis,correlations,some are,even negative
summarization,7,40,ablation-analysis,metrics,are,low
summarization,7,40,ablation-analysis,high-scoring summaries ( T ),has,correlations
summarization,7,40,ablation-analysis,Ablation analysis,in,high-scoring summaries ( T )
summarization,7,19,baselines,consistent improvements,across,metrics
summarization,7,28,baselines,Size of longest common subsequence,between,generated summaries and ground- truth
summarization,7,28,baselines,Baselines,has,Size of longest common subsequence
summarization,7,29,baselines,Soft matching,based on,cosine similarity and word embeddings
summarization,7,29,baselines,Baselines,has,Soft matching
summarization,7,30,baselines,Jensen-Shannon divergence,to measure,difference
summarization,7,30,baselines,difference,between,bigram distributions
summarization,7,8,experiments,methodology,to study,evaluation metrics
summarization,7,8,experiments,evaluation metrics,in,high-scoring range
summarization,7,33,experiments,generated dataset,consists of,"160,523 summaries"
summarization,7,34,experiments,high-scoring summaries,use,LexRank
summarization,7,34,experiments,LexRank,to lter out,summaries
summarization,7,34,experiments,underperform,has,benchmark
summarization,7,35,experiments,nal dataset ( T ),of,102 summaries per topic
summarization,7,35,experiments,nal dataset ( T ),around,102 summaries per topic
summarization,7,35,experiments,102 summaries per topic,after removing,duplicates and ltering
summarization,7,36,experiments,Correlation Analysis,has,PARADOX
summarization,7,14,model,correlation,between,pairs of metrics
summarization,7,14,model,correlation,in,different scoring ranges
summarization,7,14,model,pairs of metrics,in,different scoring ranges
summarization,7,14,model,Model,computes,correlation
summarization,7,32,model,genetic algorithm,for,summarisation
summarization,7,32,model,summarisation,to generate,summaries
summarization,7,32,model,summaries,optimise,each metrics
summarization,7,32,model,Model,use,genetic algorithm
summarization,7,6,results,strong disagreement,between,evaluation metrics
summarization,7,6,results,evaluation metrics,behave,similarly
summarization,7,9,results,more human annotations,in,appropriate scoring range
summarization,7,12,results,score distribution,of,ground - truth summaries
summarization,7,12,results,score distribution,of,generated summaries
summarization,7,12,results,generated summaries,by,modern summarisation systems
summarization,7,13,results,evaluation metrics,behave similar to,human evaluation
summarization,7,13,results,human evaluation,in,red distribution ( high -scoring range )
summarization,7,17,results,Results,has,Bigram overlap between generated summaries and ground- truth
summarization,7,18,results,Results,has,MEASURING CONSISTENT IMPROVEMENTS ACROSS METRICS
summarization,7,21,results,summaries,better than,s
summarization,7,21,results,s,for,one metric
summarization,7,21,results,Results,Among,summaries
summarization,7,22,results,summaries,better than,s
summarization,7,22,results,s,for,all metrics
summarization,7,22,results,Results,Among,summaries
summarization,7,25,results,F,divide by,N
summarization,7,25,results,N,to obtain,ratio
summarization,7,25,results,Results,has,F
summarization,7,37,results,high correlation,between,evaluation metrics
summarization,7,37,results,R - 2 and JS - 2,having,strongest correlation
summarization,7,37,results,Results,for,dataset A and W
summarization,7,41,results,no global agreement,between,metrics
summarization,7,41,results,metrics,to measure,improvements
summarization,7,41,results,improvements,examine,summaries
summarization,7,41,results,summaries,better than,LexRank
summarization,7,42,results,increases,with,higher - scoring summaries
summarization,7,42,results,disagreement,has,increases
summarization,7,44,results,our current evaluation metrics,good at distinguishing,very bad summaries
summarization,7,44,results,very bad summaries,from,very good summaries
summarization,7,44,results,Results,tell us that,our current evaluation metrics
summarization,8,39,ablation-analysis,Exconsumm model,dominates,extractive and mixed results
summarization,8,39,ablation-analysis,Ablation analysis,has,Exconsumm model
summarization,8,26,baselines,sentence classi er,uses,hierarchical LSTM
summarization,8,26,baselines,hierarchical LSTM,to encode,document
summarization,8,26,baselines,hierarchical LSTM,produce,sequence of sentence embeddings
summarization,8,10,experiments,single transformer language model,using,""" formatted "" data"
summarization,8,10,experiments,from scratch,using,""" formatted "" data"
summarization,8,10,experiments,single transformer language model,has,from scratch
summarization,8,14,experiments,language model,to understand,domain language
summarization,8,18,experiments,document,to condition,transformer language model EXTRACTIVE SUMMARISATION
summarization,8,29,experiments,transformer language model,is,GPT - 2
summarization,8,15,hyperparameters,introduction section,would be,entire document
summarization,8,15,hyperparameters,some datasets,has,introduction section
summarization,8,15,hyperparameters,Hyperparameters,For,some datasets
summarization,8,6,model,output,of,extractive step
summarization,8,6,model,extractive step,to train,abstractive transformer language model
summarization,8,6,model,Model,has,output
summarization,8,19,model,extractive,involves,sentence extraction
summarization,8,19,model,sentence extraction,using,two different hierarchical document models
summarization,8,19,model,Model,has,extractive
summarization,8,20,model,goal,lter out,noisy sentences
summarization,8,20,model,goal,extract,important sentences
summarization,8,20,model,important sentences,to better train,transformer language model
summarization,8,20,model,Model,lter out,noisy sentences
summarization,8,20,model,Model,has,goal
summarization,8,21,model,encoder,is,bidirectional LSTM
summarization,8,21,model,encoder,is,autoregressive LSTM
summarization,8,21,model,decoder,is,autoregressive LSTM
summarization,8,21,model,hierarchical seq2seq sentence pointer,has,encoder-decoder architecture
summarization,8,21,model,Model,has,hierarchical seq2seq sentence pointer
summarization,8,22,model,hierarchical encoder,combines,word and sentence - level directional LSTM
summarization,8,22,model,Model,has,hierarchical encoder
summarization,8,23,model,token - level biLSTM,encodes,each sentence
summarization,8,23,model,each sentence,in,document
summarization,8,23,model,each sentence,to obtain,sentence embeddings
summarization,8,23,model,Model,has,token - level biLSTM
summarization,8,24,model,sentence - level biLSTM,encodes,sentence embeddings
summarization,8,24,model,sentence embeddings,to obtain,document representations
summarization,8,24,model,Model,has,sentence - level biLSTM
summarization,8,25,model,decoder,is,autoregressive LSTM
summarization,8,25,model,autoregressive LSTM,takes in,hidden state
summarization,8,25,model,autoregressive LSTM,predict,next sentence to be extract
summarization,8,25,model,hidden state,of,previously extracted sentence
summarization,8,25,model,previously extracted sentence,as,input
summarization,8,25,model,Model,has,decoder
summarization,8,27,model,nal document representation,average of,sentence embeddings
summarization,8,27,model,Model,has,nal document representation
summarization,8,28,model,nal document representation,concatenated to,each sentence embedding
summarization,8,28,model,nal document representation,feed into,neural network
summarization,8,28,model,neural network,with,sigmoid function
summarization,8,28,model,sigmoid function,to obtain,probability
summarization,8,28,model,probability,of,each sentence
summarization,8,28,model,each sentence,included in,extractive summary
summarization,8,28,model,Model,has,nal document representation
summarization,8,30,model,Language models,trained by,factorising joint distribution of words autoregressively
summarization,8,30,model,Model,has,Language models
summarization,8,31,model,training data,in,certain format
summarization,8,31,model,training data,put,ground - truth summary
summarization,8,31,model,ground - truth summary,after,information
summarization,8,31,model,Model,organise,training data
summarization,8,32,model,joint distribution,of,document and summary
summarization,8,32,model,document and summary,during,training
summarization,8,32,model,document and summary,at,inference
summarization,8,32,model,conditional distribution ( given the document ),to generate,summary
summarization,8,32,model,summary,at,inference
summarization,8,32,model,Model,model,joint distribution
summarization,8,32,model,Model,use,conditional distribution ( given the document )
summarization,8,34,model,Assumption,that,introduction
summarization,8,34,model,Model,has,Assumption
summarization,8,8,results,generated abstractive summaries,are,more abstractive
summarization,8,8,results,generated abstractive summaries,yielded,higher ROUGE scores
summarization,8,8,results,more abstractive,than,previous work
summarization,8,8,results,more abstractive,employed,copy mechanism
summarization,8,8,results,Results,has,generated abstractive summaries
summarization,8,37,results,all previous extractive baselines,on,arXiv and PubMed datasets
summarization,8,37,results,our extractive models,has,outperformed
summarization,8,37,results,outperformed,has,all previous extractive baselines
summarization,8,37,results,Results,has,showcase
summarization,8,38,results,other abstractive model,by,massive margin
summarization,8,38,results,Newsroom dataset,has,our TLM
summarization,8,38,results,our TLM,has,outperformed
summarization,8,38,results,outperformed,has,other abstractive model
summarization,8,38,results,outperformed,has,pointer - generator network
summarization,8,38,results,Results,On,Newsroom dataset
summarization,8,41,results,previous abstractive results,on,most ROUGE scores metrics
summarization,8,41,results,most ROUGE scores metrics,except on,ROUGE -L
summarization,8,41,results,"best performing TLM ( TLM - I+E ( G , M ) )",has,outperformed
summarization,8,41,results,outperformed,has,previous abstractive results
summarization,8,41,results,Results,has,"best performing TLM ( TLM - I+E ( G , M ) )"
summarization,8,43,results,copy mechanism,of,discourse - aware model
summarization,8,43,results,copy mechanism,copy,up to 25 - grams
summarization,8,43,results,up to 25 - grams,from,source document
summarization,8,45,results,upper bound performance,of,"our TLM ( TLM - I+E ( G, G ) )"
summarization,8,45,results,"our TLM ( TLM - I+E ( G, G ) )",by including,ground -truth extracted sentences
summarization,8,45,results,ground -truth extracted sentences,in,training and testing
summarization,8,45,results,Results,measure,upper bound performance
summarization,8,46,results,qualitative results,of,summaries
summarization,8,46,results,summaries,generated by,our TLM
summarization,8,48,results,uency and coherency,of,generated summaries
summarization,8,48,results,uency and coherency,of,strong level
summarization,8,48,results,generated summaries,are,strong level
summarization,8,48,results,generated summaries,of,strong level
summarization,8,48,results,Results,has,uency and coherency
summarization,9,11,ablation-analysis,position bias,is,very strong
summarization,9,11,ablation-analysis,position bias,remains,most effective approach
summarization,9,11,ablation-analysis,very strong,in,news
summarization,9,11,ablation-analysis,most effective approach,in selecting,positive information
summarization,9,11,ablation-analysis,Ablation analysis,shows us that,position bias
summarization,9,14,baselines,position - agnostic graphs,extended,LexRank
summarization,9,14,baselines,LexRank,using,SBERT ( SLR )
summarization,9,14,baselines,SBERT ( SLR ),to measure,cosine similarity
summarization,9,15,baselines,af nity propagation clustering algorithm ( SC ),which,center
summarization,9,15,baselines,af nity propagation clustering algorithm ( SC ),clusters,sentences
summarization,9,15,baselines,cluster,selected to build,pseudo reference
summarization,9,17,baselines,SLR and SC,have,two variations
summarization,9,23,baselines,TC,Label,top N sentences
summarization,9,23,baselines,top N sentences,from,each document
summarization,9,23,baselines,top N sentences,as,salient
summarization,9,25,baselines,new unsupervised evaluation metric,to guide,training
summarization,9,25,baselines,training,of,RL - based multidocument summariser
summarization,9,27,baselines,SUPERT,selects,top 10 - 15 sentences
summarization,9,27,baselines,SUPERT,uses,SBERT
summarization,9,27,baselines,top 10 - 15 sentences,from,each source document
summarization,9,27,baselines,each source document,as,pseudo references
summarization,9,27,baselines,SBERT,to measure,semantic similarity
summarization,9,27,baselines,semantic similarity,between,summaries and pseudo references
summarization,9,27,baselines,Baselines,has,SUPERT
summarization,9,21,experiments,PacSum,selects,sentences
summarization,9,21,experiments,sentences,that are,semantically central
summarization,9,21,experiments,high average similarity,with,succeeding sentences
summarization,9,21,experiments,low average similarity,with,preceding sentences
summarization,9,21,experiments,semantically central,has,meaning
summarization,9,21,experiments,meaning,has,high average similarity
summarization,9,5,model,pseudo reference summary,generated by selecting,salient sentences
summarization,9,5,model,salient sentences,from,source documents
summarization,9,5,model,salient sentences,using,contextualised embeddings
summarization,9,5,model,salient sentences,using,soft token alignment
summarization,9,5,model,Model,has,pseudo reference summary
summarization,9,13,model,two graph- based approach,to building,pseudo references
summarization,9,13,model,Model,explored,two graph- based approach
summarization,9,16,model,clustering algorithm,preset,number of clusters
summarization,9,16,model,Model,has,clustering algorithm
summarization,9,18,model,individual graph,builds,graph
summarization,9,18,model,individual graph,selects,top K sentences
summarization,9,18,model,graph,for,each source document
summarization,9,18,model,Model,has,individual graph
summarization,9,19,model,global graph,builds,graph
summarization,9,19,model,global graph,selects,top M sentences
summarization,9,19,model,graph,using,all the sentences
summarization,9,19,model,all the sentences,from,all the source documents
summarization,9,19,model,all the source documents,of,same topic
summarization,9,19,model,Model,has,global graph
summarization,9,20,model,position - aware graphs,extended,PacSum
summarization,9,20,model,PacSum,using,SBERT ( SPS )
summarization,9,20,model,PacSum,consider,individual and global - graph versions
summarization,9,20,model,Model,For,position - aware graphs
summarization,9,22,model,Top + Clique ( TC ),selects,top N sentences
summarization,9,22,model,Top + Clique ( TC ),selects,semantically central sentences
summarization,9,22,model,semantically central sentences,to build,pseudo references
summarization,9,22,model,Model,proposed,Top + Clique ( TC )
summarization,9,6,results,SUPERT,achieve,better correlation
summarization,9,6,results,better correlation,with,human evaluation
summarization,9,6,results,human evaluation,of,18 - 39 %
summarization,9,6,results,Results,has,SUPERT
summarization,9,7,results,SUPERT,with,reinforcement learning summariser
summarization,9,7,results,SUPERT,yielded,strong performance
summarization,9,8,results,outperformed,has,baseline models
summarization,9,9,results,positionagnostic graphs,has,underperformed
summarization,9,9,results,underperformed,has,position - aware graphs
summarization,9,9,results,Results,has,positionagnostic graphs
summarization,9,10,results,position - aware graphs,has,underperformed
summarization,9,10,results,underperformed,has,simple sentence extraction method
summarization,9,10,results,Results,has,position - aware graphs
summarization,9,28,results,NTD with SUPERT,yielded,strongest results
summarization,10,27,ablation-analysis,physicians,summarise,clinical notes
summarization,10,27,ablation-analysis,clinical notes,to cover,more relevant entities
summarization,10,30,ablation-analysis,information,in,earlier health record
summarization,10,30,ablation-analysis,information,persist through to,later records
summarization,10,30,ablation-analysis,earlier health record,persist through to,later records
summarization,10,30,ablation-analysis,later records,reminding,physicians
summarization,10,30,ablation-analysis,physicians,to pay,attention
summarization,10,30,ablation-analysis,attention,for,future treatments
summarization,10,10,baselines,approach,is,unsupervised
summarization,10,10,baselines,Baselines,has,approach
summarization,10,11,baselines,sentences,with,most medical entities
summarization,10,41,baselines,Extension of TFIDF,reduce,duplication of information
summarization,10,41,baselines,weight,of,sentence
summarization,10,41,baselines,weight,of,sentences
summarization,10,41,baselines,sentence,that has,high similarity
summarization,10,41,baselines,high similarity,to,sentences
summarization,10,41,baselines,Baselines,has,Extension of TFIDF
summarization,10,42,baselines,our model,without,novelty feature
summarization,10,42,baselines,our model,without,position feature
summarization,10,42,baselines,our model,without,our full model
summarization,10,42,baselines,our model,without,position feature
summarization,10,5,experimental-setup,5875 admissions,contain,at least one diagnostic ICD code
summarization,10,5,experimental-setup,at least one diagnostic ICD code,related to,heart disease
summarization,10,6,experimental-setup,clinical notes,from,NOTEEVENTS table
summarization,10,6,experimental-setup,Experimental setup,utilise,clinical notes
summarization,10,34,experimental-setup,integer linear programming,with,length constraints
summarization,10,34,experimental-setup,integer linear programming,to generate,binary pseudo-labels
summarization,10,34,experimental-setup,length constraints,to generate,binary pseudo-labels
summarization,10,34,experimental-setup,binary pseudo-labels,for,EHR
summarization,10,34,experimental-setup,EHR,using,later records
summarization,10,34,experimental-setup,Experimental setup,used,integer linear programming
summarization,10,7,experiments,experienced physicians,has,to manually annotated 25 clinical notes
summarization,10,32,experiments,inverse document frequency,to measure,importance
summarization,10,32,experiments,inverse document frequency,measures,semantic similarity
summarization,10,32,experiments,of an entity,in,entire corpus
summarization,10,32,experiments,semantic similarity,between,entity and sentences
summarization,10,32,experiments,entity and sentences,in,EHR
summarization,10,32,experiments,word embeddings,trained on,PubMed
summarization,10,32,experiments,importance,has,of an entity
summarization,10,44,experiments,optimisation target,used,ILP
summarization,10,44,experiments,optimisation target,Used,generated pseudo-labels
summarization,10,44,experiments,ILP,to generate,pseudo-labels
summarization,10,44,experiments,ILP,Used,generated pseudo-labels
summarization,10,44,experiments,generated pseudo-labels,to train,our supervised extractive summarisation model
summarization,10,25,model,objective,nd,subset of these EHRs
summarization,10,25,model,subset of these EHRs,best summarises,patient 's information
summarization,10,25,model,patient 's information,for,speci c disease
summarization,10,25,model,Model,nd,subset of these EHRs
summarization,10,25,model,Model,has,objective
summarization,10,28,model,entity set,contains,all the medical entities
summarization,10,31,model,coverage score,to evaluate,quality
summarization,10,31,model,quality,of,EHR
summarization,10,31,model,quality,one of,later records
summarization,10,31,model,Model,use,coverage score
summarization,10,36,model,generated pseudo-labels,to train,supervised extractive summarisation model
summarization,10,36,model,supervised extractive summarisation model,to summarise,medical records
summarization,10,36,model,Model,use,generated pseudo-labels
summarization,10,37,model,two -layer biGRU,generate,sentence embedding
summarization,10,37,model,two -layer biGRU,compute,nal representation
summarization,10,37,model,rst one,focuses on,word level
summarization,10,37,model,second one,focuses on,sentence level
summarization,10,37,model,second one,compute,nal representation
summarization,10,37,model,nal representation,for,each sentence
summarization,10,37,model,Model,consists of,two -layer biGRU
summarization,10,38,model,output layer,have,logistic function
summarization,10,38,model,logistic function,with,several features
summarization,10,38,model,several features,including,content
summarization,10,38,model,several features,including,salience
summarization,10,38,model,several features,including,novelty
summarization,10,38,model,several features,including,position
summarization,10,38,model,Model,For,output layer
summarization,10,39,model,salience feature,helps us identify,how important
summarization,10,39,model,current sentence,to,entire note
summarization,10,39,model,novelty feature,reducing,redundancy
summarization,10,39,model,how important,has,current sentence
summarization,10,39,model,Model,has,salience feature
summarization,10,4,results,Medical Information Mart,for,Intensive Care III ( MIMIC - III )
summarization,10,8,results,evaluation metric,is,standard summarisation metric
summarization,10,8,results,standard summarisation metric,of,ROUGE scores
summarization,10,8,results,Results,has,evaluation metric
summarization,10,13,results,our model,achieved,highest ROUGE scores
summarization,10,13,results,highest ROUGE scores,against,all our baseline models
summarization,10,14,results,redundancy,heavily affects,our summarisation models
summarization,10,14,results,Results,observed,redundancy
summarization,10,15,results,MMR and novelty feature,improves,performance
summarization,10,15,results,performance,of,TF - IDF
summarization,10,15,results,our model,has,signi cantly
summarization,10,15,results,Results,has,MMR and novelty feature
summarization,10,16,results,position feature,proven to,improve
summarization,10,16,results,improve,has,performance
summarization,10,16,results,Results,has,position feature
summarization,10,17,results,some examples,of,extracted sentences
summarization,10,17,results,extracted sentences,by,our models
summarization,10,18,results,strong tendency,to select,long sentences
summarization,10,18,results,long sentences,with,more entities
summarization,10,19,results,important short sentences,like,sentence 1 and 2
summarization,10,19,results,Results,failed to select,important short sentences
summarization,10,20,results,Results,has,Our model
summarization,10,21,results,weakness,of,TM
summarization,10,21,results,TM,by using,TFIDF
summarization,10,21,results,TFIDF,when,sentences
summarization,10,21,results,TFIDF,mislead,model
summarization,10,21,results,sentences,contain,infrequent terms
summarization,10,21,results,infrequent terms,mislead,model
summarization,10,21,results,Results,has,weakness
summarization,10,22,results,sentences,very similar to,our physician 's annotation
summarization,10,24,results,many EHRs,has,recorded over time
summarization,10,24,results,Results,For,most patients
summarization,11,4,baselines,Baselines,has,SAF + F and S+F Ensemblers
summarization,11,5,baselines,ensemble methods,use,weighted average
summarization,11,5,baselines,weighted average,of,output
summarization,11,5,baselines,output,of,two different models
summarization,11,5,baselines,Baselines,has,ensemble methods
summarization,11,14,baselines,two different version,of,dataset
summarization,11,19,baselines,CSPubSumExt,used,HighlightROUGE
summarization,11,19,baselines,HighlightROUGE,to nd,sentences
summarization,11,19,baselines,sentences,similar to,highlights
summarization,11,19,baselines,Baselines,has,CSPubSumExt
summarization,11,29,baselines,8 different summariser features,to help encode,local and global context
summarization,11,29,baselines,local and global context,of,each sentence
summarization,11,38,baselines,idea,is that,important sentences
summarization,11,38,baselines,important sentences,will contain,more keywords
summarization,11,38,baselines,more keywords,has,TFIDF
summarization,11,38,baselines,Baselines,has,idea
summarization,11,41,baselines,count of words,in,sentence
summarization,11,41,baselines,count of words,in,sentence
summarization,11,41,baselines,sentence,is,TF
summarization,11,41,baselines,sentence,relative to,rest of the document
summarization,11,41,baselines,sentence,relative to,rest of the document
summarization,11,48,baselines,single layer NN,uses,all 8 features
summarization,11,48,baselines,all 8 features,to classify,each sentence
summarization,11,49,baselines,single layer networks,where,Word2 Vec
summarization,11,49,baselines,single layer networks,where,Word2VecAF
summarization,11,49,baselines,Word2 Vec,takes in,sentence average vector
summarization,11,49,baselines,Word2 Vec,takes in,sentence average vector
summarization,11,49,baselines,Word2 Vec,takes in,sentence average vector
summarization,11,49,baselines,Word2 Vec,takes in,abstract average vector
summarization,11,49,baselines,Word2VecAF,takes in,sentence average vector
summarization,11,49,baselines,Word2VecAF,takes in,abstract average vector
summarization,11,49,baselines,Word2VecAF,takes in,handcrafted features
summarization,11,50,baselines,sentence vectors,into,bidirectional RNN
summarization,11,50,baselines,bidirectional RNN,with,LSTM
summarization,11,11,experimental-setup,Experimental setup,has,Each paper
summarization,11,16,experimental-setup,CSPubSum,consists of,positive and negative examples
summarization,11,16,experimental-setup,positive and negative examples,for,each paper
summarization,11,16,experimental-setup,Experimental setup,has,CSPubSum
summarization,11,20,experimental-setup,dataset,to,263 K instances
summarization,11,20,experimental-setup,dataset,to,131 K instances
summarization,11,20,experimental-setup,263 K instances,for,training set
summarization,11,20,experimental-setup,131 K instances,for,test set
summarization,11,20,experimental-setup,Experimental setup,extend,dataset
summarization,11,21,experimental-setup,HighlightROUGE,to generate,more training data
summarization,11,21,experimental-setup,Experimental setup,has,HighlightROUGE
summarization,11,23,experimental-setup,top 20 sentences,as,positive instances
summarization,11,23,experimental-setup,top 20 sentences,as,negative instances
summarization,11,23,experimental-setup,top 20 sentences,as,negative instances
summarization,11,23,experimental-setup,bottom 20 sentences,as,negative instances
summarization,11,23,experimental-setup,Experimental setup,selected,top 20 sentences
summarization,11,23,experimental-setup,Experimental setup,selected,bottom 20 sentences
summarization,11,24,experimental-setup,extracting sentences,from,abstracts
summarization,11,24,experimental-setup,Experimental setup,excluded,extracting sentences
summarization,11,39,experimental-setup,TFIDF,calculated for,each word
summarization,11,39,experimental-setup,TFIDF,averaged over,sentence
summarization,11,39,experimental-setup,Experimental setup,has,TFIDF
summarization,11,45,experimental-setup,Average non-stopword word embeddings,in,sentence ( Word2 Vec )
summarization,11,6,experiments,SAF + F,is,ensemble of SAFNet and FNet
summarization,11,6,experiments,SAF + F,is,ensemble of SNet and FNet
summarization,11,10,experiments,publications,collected from,ScienceDirect
summarization,11,10,experiments,publications,grouped into,27 domains
summarization,11,10,experiments,ScienceDirect,grouped into,27 domains
summarization,11,18,experiments,test set,consists of,150 full papers
summarization,11,22,experiments,gold summary and the text,from,research papers
summarization,11,22,experiments,nds sentences,that yield,best ROUGE - L score
summarization,11,22,experiments,best ROUGE - L score,to,highlights
summarization,11,35,experiments,overlap,between,non-stopwords
summarization,11,35,experiments,non-stopwords,of,each sentence
summarization,11,35,experiments,non-stopwords,of,title of the paper
summarization,11,40,experiments,stopwords,has,Document TFIDF
summarization,11,26,model,sentences,which are,good summaries
summarization,11,26,model,sentences,which are,good summaries
summarization,11,26,model,sentences,also likely to be,good summaries
summarization,11,26,model,good summaries,of,abstract
summarization,11,26,model,good summaries,also likely to be,good summaries
summarization,11,26,model,good summaries,of,highlights
summarization,11,43,model,short sentences,are,very unlikely
summarization,11,43,model,very unlikely,to be,good summaries
summarization,11,43,model,sentence,encoded with,RNN ( S )
summarization,11,44,model,Model,has,Vector representation of the abstract
summarization,11,51,model,sentence,with,LSTM
summarization,11,51,model,output,to,fully connected layer
summarization,11,51,model,fully connected layer,with,dropout
summarization,11,51,model,Model,Processes,sentence
summarization,11,53,model,outputs,of,LSTM and features hidden layer
summarization,11,53,model,outputs,output,binary prediction
summarization,11,53,model,Model,has,outputs
summarization,11,54,model,SFNet,encoding,abstract
summarization,11,54,model,Model,Extend,SFNet
summarization,11,12,results,highlight statements,are,gold summary statements
summarization,11,12,results,Results,has,highlight statements
summarization,11,15,results,summary statistics,of,two datasets
summarization,11,15,results,Results,has,summary statistics
summarization,11,33,results,Results,Measure,number of numbers in a sentence
summarization,11,34,results,heavy maths,unlikely to be,good summaries
summarization,11,37,results,author-de ned keywords,appear in,sentence
summarization,11,37,results,how many,has,author-de ned keywords
summarization,11,37,results,Results,Measure,how many
summarization,11,56,results,performance,of,4 models
summarization,11,56,results,4 models,trained with and without,AbstractROUGE
summarization,11,57,results,performance,of,summarisation techniques
summarization,11,57,results,features engineering,lead to,more stable model
summarization,11,57,results,improve,has,performance
summarization,11,57,results,Results,observed,AbstractROUGE
summarization,12,29,baselines,Opinion merging,used,word embeddings
summarization,12,29,baselines,word embeddings,to merge,similar opinions
summarization,12,29,baselines,similar opinions,into,different clusters
summarization,12,29,baselines,Baselines,has,Opinion merging
summarization,12,31,baselines,opinion ltering,control,selection process
summarization,12,31,baselines,selection process,by ltering,opinions
summarization,12,31,baselines,opinions,based on,aspect category or sentiment
summarization,12,31,baselines,Baselines,has,last operation
summarization,12,34,baselines,evaluation metrics,are,"ROUGE - 1 , - 2 , and - L"
summarization,12,34,baselines,Best / Worst Review,has,highest / lowest average word overlap
summarization,12,34,baselines,highest / lowest average word overlap,has,with input reviews
summarization,12,34,baselines,Baselines,has,evaluation metrics
summarization,12,6,experiments,OPINIONDIGEST,generate,summaries
summarization,12,6,experiments,summaries,based on,different user needs
summarization,12,6,experiments,summaries,by ltering,selected opinion
summarization,12,6,experiments,selected opinion,based on,user 's aspect and / or sentiment
summarization,12,7,experiments,model,produces,informative customisable summaries
summarization,12,7,experiments,able to outperformed,has,other baseline models
summarization,12,26,experiments,content support study,where,judges
summarization,12,26,experiments,judges,given,8 input reviews
summarization,12,26,experiments,8 input reviews,from,YELP
summarization,12,33,experiments,YELP dataset,has,624 K training reviews
summarization,12,33,experiments,HOTEL,has,688K reviews
summarization,12,42,experiments,summaries,includes,speci ed aspects
summarization,12,42,experiments,speci ed aspects,has,"exclusively , partially"
summarization,12,49,experiments,generated summaries,using,aspects and sentiments
summarization,12,50,experiments,generated summaries,using,sentiments
summarization,12,14,hyperparameters,pre-trained tagging model,to obtain,our opinion set
summarization,12,14,hyperparameters,our opinion set,contains,extracted opinion phrases
summarization,12,14,hyperparameters,our opinion set,contains,respective polarity and aspect categories
summarization,12,14,hyperparameters,Hyperparameters,used,pre-trained tagging model
summarization,12,5,model,opinion phrases,from,multiple reviews
summarization,12,5,model,most popular ones,at,summarisation time
summarization,12,5,model,Model,merge,opinion phrases
summarization,12,9,model,each entity,to generate,abstractive summaries
summarization,12,9,model,abstractive summaries,of,most salient opinions
summarization,12,9,model,most salient opinions,in,relevant reviews
summarization,12,9,model,Model,For,each entity
summarization,12,10,model,OPINIONDIGEST,has,three main components
summarization,12,10,model,Model,has,OPINIONDIGEST
summarization,12,16,model,each review,extract,entity 's opinion set
summarization,12,16,model,Model,For,each review
summarization,12,19,model,reviews,from,extracted opinion phrases
summarization,12,19,model,reviews,generate,opinion summaries
summarization,12,19,model,opinion summaries,from,selected opinions
summarization,12,19,model,Model,reconstruct,reviews
summarization,12,20,model,transformer model,by encoding,extracted opinion phrases
summarization,12,20,model,transformer model,by encoding,learning
summarization,12,20,model,extracted opinion phrases,of,single review
summarization,12,20,model,learning,to reconstruct,review 's full text
summarization,12,20,model,Model,train,transformer model
summarization,12,21,model,trained transformer,to generate,summaries
summarization,12,21,model,Model,used,trained transformer
summarization,12,22,model,summarisation,use,selected opinions
summarization,12,22,model,selected opinions,as,input
summarization,12,22,model,input,to,our trained transformer model
summarization,12,22,model,our trained transformer model,to generate,respective summary
summarization,12,22,model,selected opinions,has,concatenated together )
summarization,12,22,model,Model,During,summarisation
summarization,12,27,model,Model,has,selecting the most salient opinions
summarization,12,36,model,framework,is,unsupervised model
summarization,12,36,model,framework,not an,unsupervised model
summarization,12,36,model,framework,required,labelled data
summarization,12,36,model,labelled data,for,our opinion extractor model
summarization,12,36,model,Model,has,framework
summarization,12,25,results,OPINIONDIGEST 's summaries,scored,highest
summarization,12,25,results,highest,in,informativeness and coherence
summarization,12,35,results,YELP dataset,shows,our framework
summarization,12,35,results,our framework,has,outperformed
summarization,12,35,results,outperformed,has,all the baseline models
summarization,12,35,results,Results,on,YELP dataset
summarization,12,39,results,proportion of sentences,in,summary
summarization,12,39,results,proportion of sentences,"fully , partially and not supported by",input reviews
summarization,12,39,results,summary,"fully , partially and not supported by",input reviews
summarization,12,39,results,Results,showcase,proportion of sentences
summarization,12,40,results,OPINIONDIGEST,able to generate,summaries
summarization,12,40,results,summaries,higher proportion of,fully and partially supported sentences
summarization,12,40,results,Results,show,OPINIONDIGEST
summarization,12,41,results,how well,generate,summaries
summarization,12,41,results,our framework,generate,summaries
summarization,12,41,results,summaries,based on,different aspects
summarization,12,41,results,how well,has,our framework
summarization,12,41,results,Results,measure,how well
summarization,12,41,results,Results,measure,our framework
summarization,12,44,results,OPINIONDIGEST,can generate,summaries
summarization,12,44,results,summaries,exclusively or partially related to,speci ed aspects
summarization,12,44,results,Results,show,OPINIONDIGEST
summarization,12,46,results,example summaries,showcase,OPINIONDIGEST 's ability
summarization,12,46,results,summaries,using,opinion ltering
summarization,12,46,results,opinion ltering,of,aspects and sentiments
summarization,12,46,results,OPINIONDIGEST 's ability,has,to summaries more 100 + reviews
summarization,12,47,results,model,to generate,summaries
summarization,12,47,results,summaries,based on,8 and 128 reviews
summarization,12,48,results,Our framework,n't aggregate,review representations
summarization,12,48,results,Results,has,Our framework
summarization,13,6,baselines,research papers,in,free-text query
summarization,13,6,baselines,research papers,by choosing,categorised values
summarization,13,6,baselines,categorised values,such as,scienti c tasks
summarization,13,6,baselines,categorised values,such as,datasets
summarization,13,4,experiments,summarising,has,computer science research papers
summarization,13,7,experiments,proposed system,ingested,"270,000 papers"
summarization,13,8,experiments,IBM Science Summariser,produces,summaries
summarization,13,8,experiments,IBM Science Summariser,produces,queryfocused summarisation
summarization,13,8,experiments,summaries,focuses on,user 's queries
summarization,13,8,experiments,user 's queries,has,queryfocused summarisation
summarization,13,12,experiments,12 authors,evaluate,summaries
summarization,13,12,experiments,summaries,of,two papers
summarization,13,16,experiments,48 summaries,to be,evaluated
summarization,13,17,experiments,each sections of the paper,covered in,summary
summarization,13,17,experiments,How well,has,each sections of the paper
summarization,13,24,experiments,support,for,more entities
summarization,13,24,experiments,support,ingest,more papers
summarization,13,27,experiments,queries,use,lters
summarization,13,27,experiments,lters,on,metadata elds
summarization,13,9,model,various sections,of the paper,independently
summarization,13,9,model,Model,summarises,various sections
summarization,13,10,model,interaction,between,user 's queries
summarization,13,10,model,interaction,between,various entities
summarization,13,10,model,Model,allows for,interaction
summarization,13,5,results,different scenarios,such as,discovery
summarization,13,5,results,different scenarios,such as,exploration
summarization,13,5,results,different scenarios,such as,understanding of scienti c documents
summarization,13,21,results,section - based summary,scored,higher
summarization,13,21,results,higher,in,68 %
summarization,13,21,results,task 2,has,section - based summary
summarization,13,21,results,68 %,has,of the papers
summarization,13,21,results,Results,For,task 2
summarization,13,22,results,average score,for,section - based summaries
summarization,13,22,results,section - based summaries,is,3.32
summarization,13,22,results,3.32,highlights,quality of section - based summaries
summarization,13,22,results,Results,has,average score
summarization,13,28,results,Relevant papers,returned with,summarisation results
summarization,13,28,results,Results,has,Relevant papers
summarization,13,29,results,entities,has,accurately highlighted
summarization,14,18,ablation-analysis,whole chapters,of,action
summarization,14,18,ablation-analysis,action,down to,sentence
summarization,14,18,ablation-analysis,sentence,like,Pip
summarization,14,18,ablation-analysis,Ablation analysis,condensing,whole chapters
summarization,14,41,ablation-analysis,rare or out-ofvocabulary words,such as,2 - 0
summarization,14,41,ablation-analysis,Ablation analysis,common for,rare or out-ofvocabulary words
summarization,14,56,ablation-analysis,Repetition,caused by,decoder ? s overreliance
summarization,14,56,ablation-analysis,decoder ? s overreliance,on,decoder input
summarization,14,57,ablation-analysis,single repeated word,commonly triggers,endless repetitive cycle
summarization,14,58,ablation-analysis,single substitution error Germany beat Germany,leads to,catastrophic Germany beat Germany
summarization,14,88,ablation-analysis,""" looking """,produced,word
summarization,14,88,ablation-analysis,Ablation analysis,tells you,network
summarization,14,99,ablation-analysis,network,attending directly to,word backline
summarization,14,99,ablation-analysis,network,not,copied
summarization,14,99,ablation-analysis,copied,has,correctly
summarization,14,102,ablation-analysis,green shading,shows that,generation probability
summarization,14,102,ablation-analysis,generation probability,tends to be,high
summarization,14,102,ablation-analysis,high,whenever,network
summarization,14,102,ablation-analysis,network,is editing,source text
summarization,14,102,ablation-analysis,Ablation analysis,has,green shading
summarization,14,103,ablation-analysis,p,is,high
summarization,14,103,ablation-analysis,high,when,network
summarization,14,103,ablation-analysis,network,produces,period
summarization,14,103,ablation-analysis,period,to shorten,sentence
summarization,14,103,ablation-analysis,Ablation analysis,has,p
summarization,14,105,ablation-analysis,Errors,tend to occur when,attention
summarization,14,105,ablation-analysis,attention,is,more scattered
summarization,14,105,ablation-analysis,Ablation analysis,has,Errors
summarization,14,117,ablation-analysis,most important direction,for,future research
summarization,14,9,baselines,Extractive approaches,select,passages
summarization,14,9,baselines,passages,from,source text
summarization,14,9,baselines,passages,arrange them to form,summary
summarization,14,9,baselines,Baselines,has,Extractive approaches
summarization,14,60,baselines,Problem 1 ( inaccurate copying ),is,pointer - generator network
summarization,14,68,baselines,Baselines,has,P ( w )
summarization,14,106,baselines,Munster and Francis,when producing,first word
summarization,14,106,baselines,first word,of,summary
summarization,14,48,experimental-setup,skip ahead,to,solutions
summarization,14,48,experimental-setup,not interested,has,skip ahead
summarization,14,65,experimental-setup,generation probability p,is,scalar value
summarization,14,65,experimental-setup,scalar value,between,0 and 1
summarization,14,65,experimental-setup,Experimental setup,calculate,generation probability p
summarization,14,89,experimental-setup,value,of,generation probability
summarization,14,89,experimental-setup,generation probability,visualized in,green
summarization,14,89,experimental-setup,pointer - generator models,has,value
summarization,14,89,experimental-setup,Experimental setup,For,pointer - generator models
summarization,14,91,experimental-setup,zoom out,using,your browser window
summarization,14,91,experimental-setup,zoom out,to view,demo
summarization,14,91,experimental-setup,demo,on,one screen
summarization,14,91,experimental-setup,Experimental setup,to,zoom out
summarization,14,21,experiments,excellent tutorials,by,WildML
summarization,14,26,experiments,source text,is,news article
summarization,14,26,experiments,Argentina,on,Saturday
summarization,14,85,experiments,networks,on,CNN / Daily Mail dataset
summarization,14,85,experiments,CNN / Daily Mail dataset,contains,news articles
summarization,14,85,experiments,news articles,paired with,multi-sentence summaries
summarization,14,86,experiments,source text,alongside,reference summary
summarization,14,86,experiments,source text,alongside,three automatic summaries
summarization,14,92,experiments,Does not work,for,mobile
summarization,14,27,model,encoder RNN,reads in,source text word - by -word
summarization,14,27,model,Model,has,encoder RNN
summarization,14,28,model,arrows,in,both directions
summarization,14,29,model,decoder RNN,begins to output,sequence of words
summarization,14,29,model,sequence of words,should form,summary
summarization,14,29,model,entire source text,has,decoder RNN
summarization,14,30,model,decoder,receives as input,previous word
summarization,14,30,model,previous word,of,summary
summarization,14,30,model,each step,has,decoder
summarization,14,30,model,Model,On,each step
summarization,14,31,model,attention distribution,is,probability distribution
summarization,14,31,model,probability distribution,over,words
summarization,14,31,model,words,in,source text
summarization,14,31,model,Model,to calculate,attention distribution
summarization,14,32,model,attention distribution,tells,network
summarization,14,32,model,Model,has,attention distribution
summarization,14,33,model,decoder,produced,first word Germany
summarization,14,33,model,decoder,concentrating on,source words
summarization,14,33,model,source words,has,win and victorious
summarization,14,34,model,attention distribution,to produce,weighted sum
summarization,14,34,model,weighted sum,of,encoder hidden states
summarization,14,34,model,encoder hidden states,known as,context vector
summarization,14,34,model,Model,has,attention distribution
summarization,14,35,model,context vector,what has been read from,source text
summarization,14,35,model,source text,on,step
summarization,14,35,model,step,of,decoder
summarization,14,35,model,Model,has,context vector
summarization,14,36,model,context vector and the decoder hidden state,to calculate,vocabulary distribution
summarization,14,36,model,vocabulary distribution,is,probability distribution
summarization,14,36,model,probability distribution,over,all the words
summarization,14,36,model,all the words,in,large fixed vocabulary
summarization,14,36,model,Model,has,context vector and the decoder hidden state
summarization,14,37,model,word,with,largest probability
summarization,14,37,model,word,chosen as,output
summarization,14,37,model,decoder,moves on,next step
summarization,14,37,model,Model,has,word
summarization,14,38,model,words,in,any order
summarization,14,51,model,w,is,rare word
summarization,14,51,model,rare word,infrequently during,training
summarization,14,51,model,many,has,other words
summarization,14,61,model,hybrid network,choose to copy,words
summarization,14,61,model,hybrid network,choose to copy,words
summarization,14,61,model,hybrid network,retaining,ability
summarization,14,61,model,words,from,source
summarization,14,61,model,words,from,fixed vocabulary
summarization,14,61,model,source,via,pointing
summarization,14,61,model,ability,to generate,words
summarization,14,61,model,words,from,fixed vocabulary
summarization,14,61,model,Model,is,hybrid network
summarization,14,61,model,Model,retaining,ability
summarization,14,63,model,third step,of,decoder
summarization,14,63,model,third step,generated,partial summary Germany beat
summarization,14,63,model,decoder,generated,partial summary Germany beat
summarization,14,64,model,Model,calculate,attention distribution
summarization,14,64,model,Model,calculate,vocabulary distribution
summarization,14,66,model,probability,of,generating
summarization,14,66,model,probability,of,copying a word
summarization,14,66,model,word,from,vocabulary
summarization,14,66,model,word,from,source
summarization,14,66,model,copying a word,from,source
summarization,14,66,model,generating,has,word
summarization,14,66,model,Model,represents,probability
summarization,14,67,model,generation probability p,to,weight
summarization,14,67,model,generation probability p,combine,vocabulary distribution P
summarization,14,67,model,generation probability p,combine,attention distribution a
summarization,14,67,model,attention distribution a,into,final distribution P
summarization,14,67,model,Model,has,generation probability p
summarization,14,69,model,probability of producing word w,equal to,probability
summarization,14,69,model,probability,generating it from,vocabulary
summarization,14,71,model,network,put,su iciently large attention
summarization,14,71,model,su iciently large attention,on,relevant word
summarization,14,71,model,Model,has,network
summarization,14,75,model,pointer - generator network,combining,extraction
summarization,14,75,model,pointer - generator network,combining,abstraction ( generating )
summarization,14,75,model,Model,has,pointer - generator network
summarization,14,77,model,attention distribution,to keep track of,what ?s been covered so far
summarization,14,77,model,attention distribution,penalize,network
summarization,14,77,model,network,attending to,same parts
summarization,14,77,model,Model,use,attention distribution
summarization,14,77,model,Model,penalize,network
summarization,14,78,model,each timestep t,of,decoder
summarization,14,78,model,coverage vector c,sum of,all the attention distributions a so far
summarization,14,78,model,each timestep t,has,coverage vector c
summarization,14,78,model,decoder,has,coverage vector c
summarization,14,78,model,Model,On,each timestep t
summarization,14,82,model,extra loss term,to penalize,any overlap
summarization,14,82,model,any overlap,between,coverage vector c and the new attention distribution a
summarization,14,82,model,Model,introduce,extra loss term
summarization,14,83,model,network,from attending to,summarizing
summarization,14,83,model,network,from attending to,already been covered
summarization,14,83,model,Model,discourages,network
summarization,14,98,model,nonsensical phrase,great addition to,respective prospects
summarization,14,98,model,respective prospects,by,basic sequence - to-sequence system
summarization,14,111,model,Model,has,Higher - level abstraction
summarization,14,118,model,attention mechanism,shines,some precious light
summarization,14,118,model,some precious light,into,black box
summarization,14,118,model,some precious light,helping us to,debug
summarization,14,118,model,black box,of,neural networks
summarization,14,118,model,debug,like,copying
summarization,14,118,model,problems,like,repetition
summarization,14,118,model,problems,like,copying
summarization,14,118,model,debug,has,problems
summarization,14,118,model,Model,has,attention mechanism
summarization,14,12,results,whole sentences,from,source text
summarization,14,12,results,whole sentences,from,source text
summarization,14,12,results,whole sentences,guaranteed to produce,summary
summarization,14,12,results,summary,that is,grammatical
summarization,14,12,results,summary,that is,fairly readable
summarization,14,12,results,summary,related to,source text
summarization,14,12,results,selecting and rearranging,has,whole sentences
summarization,14,14,results,Results,has,extractive approach
summarization,14,17,results,Great Expectations,written in,first person
summarization,14,17,results,synopsis,should be in,third person
summarization,14,43,results,2,has,summaries sometimes repeat
summarization,14,43,results,Results,has,2
summarization,14,43,results,Results,has,summaries sometimes repeat
summarization,14,44,results,Results,has,Germany beat Germany beat
summarization,14,47,results,Results,For,those who are interested
summarization,14,52,results,network,may still have,di iculty reproducing
summarization,14,52,results,good word embedding,has,network
summarization,14,52,results,di iculty reproducing,has,word
summarization,14,59,results,Results,has,Easier Copying
summarization,14,70,results,pointer - generator network,makes it easy to copy,words
summarization,14,70,results,words,from,source text
summarization,14,70,results,sequence-to-sequence - with -attention system,has,pointergenerator network
summarization,14,70,results,pointergenerator network,has,several advantages
summarization,14,70,results,Results,Compared to,sequence-to-sequence - with -attention system
summarization,14,72,results,pointer - generator model,able to copy,out - of- vocabulary words
summarization,14,72,results,out - of- vocabulary words,from,source text
summarization,14,72,results,Results,has,pointer - generator model
summarization,14,74,results,pointer- generator model,faster to,train
summarization,14,74,results,fewer training iterations,to achieve,same performance
summarization,14,74,results,same performance,as,sequence - to-sequence attention system
summarization,14,74,results,Results,has,pointer- generator model
summarization,14,80,results,coverage,of,particular source word
summarization,14,80,results,particular source word,equal to,amount of attention
summarization,14,80,results,Results,has,coverage
summarization,14,84,results,systems,on,real data
summarization,14,87,results,your cursor,over,word
summarization,14,87,results,your cursor,view,attention distribution
summarization,14,87,results,word,from,automatic summaries
summarization,14,87,results,word,one of,automatic summaries
summarization,14,87,results,attention distribution,projected in,yellow
summarization,14,87,results,yellow,on,source text
summarization,14,87,results,Results,By hovering,your cursor
summarization,14,90,results,cursor,over,word
summarization,14,90,results,word,one of,summaries
summarization,14,90,results,generation probability p,for,word
summarization,14,90,results,Hovering,has,cursor
summarization,14,90,results,Results,has,Hovering
summarization,14,94,results,unable to copy out - of- vocabulary words,like,Saili
summarization,14,94,results,unable to copy out - of- vocabulary words,outputting,unknown token
summarization,14,94,results,Results,has,basic sequence - to-sequence system
summarization,14,95,results,pointer- generator systems,have no trouble,copying
summarization,14,95,results,copying,has,word
summarization,14,96,results,basic sequence - to-sequence system,player is,team Irish
summarization,14,97,results,network,mostly attending to,names Munster and Francis
summarization,14,97,results,words,has,network
summarization,14,100,results,basic pointer - generator summary,attending,same parts
summarization,14,100,results,same parts,of,source text
summarization,14,100,results,Results,has,basic pointer - generator summary
summarization,14,101,results,pointergenerator + coverage model,contains,no repetition
summarization,14,104,results,attention distribution,is,fairly focused
summarization,14,104,results,all three systems,has,attention distribution
summarization,14,104,results,Results,For,all three systems
summarization,14,107,results,networks,seek out,names
summarization,14,107,results,names,to begin,summaries
summarization,14,107,results,Results,has,networks
summarization,14,113,results,Argentina,has,beat Germany 2 - 0
summarization,15,25,ablation-analysis,subjectiveness,of,important
summarization,15,25,ablation-analysis,subjectiveness,deem,important
summarization,15,25,ablation-analysis,subjectiveness,to be,important
summarization,15,25,ablation-analysis,important,in,ndings
summarization,15,25,ablation-analysis,Ablation analysis,due to,subjectiveness
summarization,15,28,ablation-analysis,FINDINGS,consists of,detailed observations and interpretation
summarization,15,28,ablation-analysis,detailed observations and interpretation,of,imaging study
summarization,15,28,ablation-analysis,IMPRESSION,summarises,most critical ndings
summarization,15,28,ablation-analysis,Ablation analysis,has,FINDINGS
summarization,15,30,ablation-analysis,improvement,of generation of IMPRESSION,signi cantly improve
summarization,15,30,ablation-analysis,signi cantly improve,to select,most important ontological concepts
summarization,15,30,ablation-analysis,most important ontological concepts,within,report
summarization,15,30,ablation-analysis,Ablation analysis,has,automation
summarization,15,8,experiments,our approach,generated,good quality summaries
summarization,15,8,experiments,good quality summaries,comparison to,ground -truth
summarization,15,12,experiments,MIMIC - CXR,has,107372
summarization,15,12,experiments,MIMIC - CXR,has,radiology reports
summarization,15,12,experiments,107372,has,radiology reports
summarization,15,12,experiments,OpenI,has,3366 reports
summarization,15,13,experiments,radiology lexicon,used,RadLex
summarization,15,13,experiments,RadLex,consists of,68534 radiological terms
summarization,15,20,hyperparameters,100 generated IMPRESSIONs,with,gold summaries
summarization,15,20,hyperparameters,100 generated IMPRESSIONs,associated,gold summaries
summarization,15,20,hyperparameters,Hyperparameters,randomly sampled,100 generated IMPRESSIONs
summarization,15,32,hyperparameters,each word,tag with,1
summarization,15,32,hyperparameters,word,is,ontology term
summarization,15,32,hyperparameters,two criteria,has,word
summarization,15,32,hyperparameters,Hyperparameters,has,each word
summarization,15,33,hyperparameters,word,directly copied into,IMPRESSION
summarization,15,33,hyperparameters,Hyperparameters,has,word
summarization,15,6,model,Content selection,treated as,word-level sequence - tagging problem
summarization,15,6,model,Model,has,Content selection
summarization,15,16,model,BOTTOMSUM,most relevant to,architecture
summarization,15,16,model,BOTTOMSUM,utilises,separate content selector
summarization,15,16,model,separate content selector,for,abstractive text summarisation
summarization,15,16,model,Model,has,BOTTOMSUM
summarization,15,34,model,copy likelihood,of,each word
summarization,15,34,model,copy likelihood,to measure,importance of the word
summarization,15,34,model,Model,capture,copy likelihood
summarization,15,35,model,overall architecture,is,biLSTM
summarization,15,35,model,biLSTM,on top of,BERT embeddings layer
summarization,15,35,model,biLSTM,during,inference time
summarization,15,35,model,our content selector,output,selection probability
summarization,15,35,model,selection probability,of,each token
summarization,15,35,model,each token,in,our source sequence
summarization,15,35,model,inference time,has,our content selector
summarization,15,35,model,Model,has,overall architecture
summarization,15,38,model,biLSTM,takes in,word embeddings
summarization,15,38,model,biLSTM,generates,encoded hidden representation
summarization,15,38,model,Model,is,biLSTM
summarization,15,39,model,LSTM,takes in,identi ed ontology terms
summarization,15,39,model,LSTM,generates,x context vector
summarization,15,39,model,Model,is,LSTM
summarization,15,40,model,LSTM,generates,IMPRESSION
summarization,15,40,model,ltering gate,re ne,FINDINGS word representations
summarization,15,40,model,FINDINGS word representations,using,ontology vector
summarization,15,40,model,ontology vector,to produce,ontology - aware word representations
summarization,15,40,model,Model,is,LSTM
summarization,15,41,model,ltering gate,concatenates,current hidden state
summarization,15,41,model,current hidden state,has,of word x and the x ontology vector
summarization,15,41,model,Model,has,ltering gate
summarization,15,42,model,ontology - aware word representations,take,output
summarization,15,42,model,ontology - aware word representations,perform,element - wise multiplication
summarization,15,42,model,output,of,ltering gate
summarization,15,42,model,element - wise multiplication,with,current hidden state of word x
summarization,15,42,model,Model,To compute,ontology - aware word representations
summarization,15,43,model,decoder,is,LSTM
summarization,15,43,model,LSTM,generates,IMPRESSION
summarization,15,43,model,Model,has,decoder
summarization,15,44,model,decoder,compute,current decoding state
summarization,15,44,model,current decoding state,using,previous hidden state and previous generated tokens
summarization,15,44,model,Model,has,decoder
summarization,15,46,model,attention distribution,to compute,context vector
summarization,15,46,model,Model,has,attention distribution
summarization,15,47,model,context vector and the current decoding state,feed into,feed-forward neural network
summarization,15,47,model,feed-forward neural network,to either generate,next token
summarization,15,47,model,feed-forward neural network,to either generate,copy
summarization,15,47,model,Model,has,context vector and the current decoding state
summarization,15,7,results,improve,on,SOTA results
summarization,15,7,results,SOTA results,based on,MIMIC - CXR and OpenI datasets
summarization,15,7,results,Results,proven to,improve
summarization,15,18,results,our model,has,signi cantly
summarization,15,21,results,three experts,score,IMPRESSIONs
summarization,15,21,results,IMPRESSIONs,on,scale
summarization,15,21,results,IMPRESSIONs,on,"readability , accuracy , and completeness"
summarization,15,21,results,scale,of,1 - 3 ( 3
summarization,15,21,results,Results,asked,three experts
summarization,15,23,results,scored as good,as,associated human-written IMPRESSIONS
summarization,15,24,results,73 % and 71 %,of,our IMPRESSIONS
summarization,15,24,results,73 % and 71 %,of,IMPRESSIONS
summarization,15,24,results,73 % and 71 %,scored,3
summarization,15,24,results,73 % and 71 %,scored,ties
summarization,15,24,results,our IMPRESSIONS,scored,ties
summarization,15,24,results,3,on,readability and accuracy
summarization,15,24,results,ties,with,human-written IMPRESSIONS
summarization,15,24,results,Results,has,73 % and 71 %
summarization,15,26,results,our generated IMPRESSIONS,are,high quality
summarization,15,26,results,our generated IMPRESSIONS,of,high quality
summarization,15,27,results,Results,has,Radiology reports
summarization,15,48,results,outperformed,has,all
summarization,15,48,results,outperformed,has,extractive and abstractive baseline models
summarization,15,48,results,all,has,extractive and abstractive baseline models
summarization,15,48,results,Results,has,outperformed
summarization,15,49,results,Abstractive models,has,signi cantly outperformed
summarization,15,49,results,signi cantly outperformed,has,extractive one
summarization,15,49,results,Results,has,Abstractive models
summarization,15,50,results,ROUGE performance,between,PG
summarization,15,50,results,ROUGE performance,between,Ontologyaware PG
summarization,15,50,results,salient ontological terms,in,summarisation
summarization,15,50,results,Results,difference in,ROUGE performance
summarization,15,51,results,BOTTOMSUM,achieve,best results
summarization,15,51,results,best results,among,baseline models
summarization,15,51,results,Results,has,BOTTOMSUM
summarization,15,53,results,bene t,of incorporating,content selection
summarization,15,53,results,content selection,to,summarisation model
summarization,15,53,results,Results,showcase,bene t
summarization,15,54,results,OpenI,against,BOTTOMSUM
summarization,15,55,results,our model,able to outperformed,BOTTOMSUM
summarization,15,55,results,BOTTOMSUM,in,Ope nI
summarization,15,55,results,Results,has,our model
summarization,16,12,ablation-analysis,Bidirectional encoders,crucial for,QA
summarization,16,12,ablation-analysis,QA,Ignoring,future context
summarization,16,12,ablation-analysis,QA,hinders,performance
summarization,16,12,ablation-analysis,performance,of,left-to- right models
summarization,16,12,ablation-analysis,Ablation analysis,has,Bidirectional encoders
summarization,16,13,ablation-analysis,LM objectives,are,important
summarization,16,6,baselines,span,containing,50 %
summarization,16,6,baselines,50 %,of,tokens
summarization,16,6,baselines,Masked Seq2Seq ( MASS ) masking,has,span
summarization,16,6,baselines,Baselines,has,Masked Seq2Seq ( MASS ) masking
summarization,16,21,baselines,techniques,to incorporate,best of both worlds
summarization,16,21,baselines,LM ( BERT ),replace,15 %
summarization,16,21,baselines,15 %,of,token
summarization,16,21,baselines,token,with,corresponding words
summarization,16,21,baselines,token,predict,corresponding words
summarization,16,22,baselines,autoregressive LM training,with,order
summarization,16,22,baselines,order,of,words
summarization,16,22,baselines,order,to predict,chosen at random
summarization,16,22,baselines,Permuted LM ( XLNet ),has,left to right
summarization,16,22,baselines,Permuted LM ( XLNet ),has,autoregressive LM training
summarization,16,22,baselines,Baselines,has,Permuted LM ( XLNet )
summarization,16,26,experiments,token masking,at,30 %
summarization,16,26,experiments,sentence permutation,as,pre-training textnoising techniques
summarization,16,26,experiments,model,on,160 GB
summarization,16,26,experiments,160 GB,of,"news , books , stories , and web text"
summarization,16,5,hyperparameters,token,chosen,randomly
summarization,16,5,hyperparameters,token,start of,document
summarization,16,5,hyperparameters,Document Rotation,has,token
summarization,16,5,hyperparameters,Hyperparameters,has,Document Rotation
summarization,16,4,model,Sentence Permutation random shuffling,of,document 's sentences
summarization,16,4,model,Model,has,Sentence Permutation random shuffling
summarization,16,7,results,Results,has,Results of the first experiment
summarization,16,9,results,configurations,with,token masking or its variations
summarization,16,9,results,configurations,achieve,consistently great performance
summarization,16,9,results,consistently great performance,on,different tasks
summarization,16,9,results,Results,has,configurations
summarization,16,10,results,Left-to- right pre-training,improves,NLG
summarization,16,10,results,Results,has,Left-to- right pre-training
summarization,16,14,results,permuted language model,performs,much worse
summarization,16,14,results,much worse,than,XLNet
summarization,16,14,results,Results,report that,permuted language model
summarization,16,15,results,"12 layered , transformer - based architecture",for,autoencoding
summarization,16,15,results,Google Colab notebook,using,Hugging Face library
summarization,16,17,results,text-noising techniques,has,corrupt
summarization,16,17,results,corrupt,has,text
summarization,16,29,results,Results,has,5 Figure 5
summarization,17,23,ablation-analysis,best performance,achieved,change
summarization,17,23,ablation-analysis,encoder,to,RoBERTa - base
summarization,17,23,ablation-analysis,change,has,encoder
summarization,17,23,ablation-analysis,Ablation analysis,observed,best performance
summarization,17,13,baselines,siamese - BERT,consists of,two BERTs
summarization,17,13,baselines,siamese - BERT,consists of,cosine similarity layer
summarization,17,13,baselines,two BERTs,with,same weight
summarization,17,11,experiments,evaluation metrics,are,"ROUGE -1 , ROUGE - 2 , and ROUGE -L."
summarization,17,24,experiments,RoBERTa,pre-trained using,63 million news articles
summarization,17,26,experiments,different forms of matching models,to further explore,performance
summarization,17,26,experiments,performance,of,proposed framework
summarization,17,5,model,novel summary - level framework,uses,Siamese - BERT
summarization,17,5,model,Siamese - BERT,to match,source document
summarization,17,5,model,Siamese - BERT,to match,candidate summaries
summarization,17,5,model,candidate summaries,in,semantic space
summarization,17,5,model,MATCHSUM,has,novel summary - level framework
summarization,17,5,model,Model,proposed,MATCHSUM
summarization,17,6,model,good summary,should be,semantically similar
summarization,17,6,model,semantically similar,to,source document
summarization,17,14,model,BERT,to encode,both the document and candidate summaries
summarization,17,14,model,BERT,compute,similarity
summarization,17,14,model,similarity,between,two embeddings
summarization,17,14,model,Model,use,BERT
summarization,17,15,model,gold summary,has,highest matching score
summarization,17,15,model,gold summary,has,good candidate summary
summarization,17,15,model,highest matching score,to,source document
summarization,17,15,model,good candidate summary,should obtain,high score
summarization,17,15,model,gold summary,has,highest matching score
summarization,17,17,model,possible candidates,use,simple candidate pruning strategy
summarization,17,8,results,similar results,in,other evaluation datasets
summarization,17,8,results,Results,achieved,similar results
summarization,17,9,results,performance difference,between,sentence - level and summary - level extractive models
summarization,17,9,results,Results,analyse,performance difference
summarization,17,21,results,Results,for,CNN / DM
summarization,17,22,results,MATCHSUM,has,outperformed
summarization,17,22,results,outperformed,has,all the other baseline models
summarization,17,22,results,Results,has,MATCHSUM
summarization,17,27,results,better understanding,characteristics of,datasets
summarization,17,27,results,advantage,over,which types of models
summarization,17,27,results,which types of models,to,use
summarization,18,5,ablation-analysis,absolute encoding,of,source code tokens ' position
summarization,18,5,ablation-analysis,relative encoding,has,signi cantly improves
summarization,18,5,ablation-analysis,signi cantly improves,has,performance
summarization,18,18,ablation-analysis,mutual interactions,between,tokens
summarization,18,18,ablation-analysis,mutual interactions,in uence,meaning of the source code
summarization,18,27,ablation-analysis,decrease,in,performance
summarization,18,27,ablation-analysis,performance,when include,absolute position encoding
summarization,18,27,ablation-analysis,Ablation analysis,showcase,decrease
summarization,18,24,baselines,Our baseline models,n't incorporate,copy attention mechanism
summarization,18,24,baselines,Our baseline models,n't incorporate,copy attention mechanism
summarization,18,24,baselines,Our baseline models,shown that,copy attention mechanism
summarization,18,24,baselines,copy attention mechanism,does improve,performance
summarization,18,24,baselines,performance,of,our full model
summarization,18,24,baselines,Baselines,has,Our baseline models
summarization,18,8,experiments,Java and Python dataset,from,GitHub
summarization,18,4,model,simple transformer - based model,with,relative position representations
summarization,18,4,model,simple transformer - based model,with,copy attention mechanism
summarization,18,4,model,copy attention mechanism,to generate,SOTA results
summarization,18,4,model,SOTA results,for,source code summarisation
summarization,18,6,model,objective,encode,source code
summarization,18,6,model,objective,generate,readable summary
summarization,18,6,model,readable summary,describes,functionality of the program
summarization,18,6,model,Model,encode,source code
summarization,18,6,model,Model,has,objective
summarization,18,12,model,code and summary,as,sequence of embeddings
summarization,18,12,model,Model,encoded,code and summary
summarization,18,13,model,vanilla Transformer,stacked of,multi-head attention and linear transformation layers
summarization,18,13,model,multi-head attention and linear transformation layers,in,encoder and decoder
summarization,18,13,model,Model,has,vanilla Transformer
summarization,18,14,model,copy attention,in,Transformer
summarization,18,14,model,copy attention,to allow,model
summarization,18,14,model,rare tokens,from,source code
summarization,18,14,model,Model,included,copy attention
summarization,18,16,model,absolute position encoding,on,sequential order
summarization,18,16,model,absolute position encoding,on,pairwise relationship encoding
summarization,18,16,model,sequential order,of,source code tokens
summarization,18,16,model,pairwise relationship encoding,in,Transformer
summarization,18,16,model,Model,explored,absolute position encoding
summarization,18,17,model,absolute position encoding,aims to capture,order information
summarization,18,17,model,absolute position encoding,aims to capture,order information
summarization,18,17,model,absolute position encoding,show,order information
summarization,18,17,model,order information,of,source tokens
summarization,18,17,model,order information,leads to,bad summarisation
summarization,18,17,model,not helpful,in learning,source code representations
summarization,18,17,model,not helpful,leads to,bad summarisation
summarization,18,17,model,Model,has,absolute position encoding
summarization,18,19,model,pairwise relationships,between,input tokens
summarization,18,19,model,pairwise relationships,capture,relative positional representations
summarization,18,19,model,relative positional representations,of,two position i and j
summarization,18,19,model,two position i and j,for,each token
summarization,18,9,results,evaluation metrics,are,BLEU
summarization,18,9,results,evaluation metrics,are,METEOR
summarization,18,9,results,evaluation metrics,are,ROUGE -L.
summarization,18,9,results,Results,has,evaluation metrics
summarization,18,22,results,our full model,has,outperformed
summarization,18,22,results,outperformed,has,all the baseline models
summarization,18,22,results,Results,has,our full model
summarization,18,23,results,base model,trained on,dataset
summarization,18,23,results,all baseline models,except on,ROUGE -L metric
summarization,18,23,results,base model,has,outperformed
summarization,18,23,results,outperformed,has,all baseline models
summarization,18,23,results,Results,has,base model
summarization,18,26,results,performance,of performing,absolute position encoding
summarization,18,26,results,absolute position encoding,on,source and targets
summarization,18,28,results,bene t,of learning,pairwise relationship
summarization,18,28,results,pairwise relationship,between,source code tokens
summarization,18,28,results,Results,showcase,bene t
summarization,18,31,results,deeper model ( more layers ),performs,better
summarization,18,31,results,better,than,wider model
summarization,18,31,results,wider model,has,more neurons per layer
summarization,18,31,results,Results,showcase,deeper model ( more layers )
summarization,18,32,results,deeper model,is,more bene cial
summarization,18,32,results,more bene cial,in,source code summarisation
summarization,18,32,results,more bene cial,depends more on,semantic information
summarization,18,32,results,semantic information,than,syntactic
summarization,18,32,results,Results,suspect,deeper model
summarization,19,6,ablation-analysis,scienti c papers,suitable for,data-driven summarisation
summarization,19,6,ablation-analysis,Ablation analysis,show,scienti c papers
summarization,19,44,ablation-analysis,Large variation,of,sentence locations
summarization,19,44,ablation-analysis,sentence locations,selected by,extractive models
summarization,19,44,ablation-analysis,extractive models,on,title-gen
summarization,19,4,baselines,gen,applied,wide range of extractive and abstractive models
summarization,19,4,baselines,Baselines,has,gen
summarization,19,21,baselines,TFIDF - emb,creates,sentence representation
summarization,19,21,baselines,Baselines,has,TFIDF - emb
summarization,19,22,baselines,Rwmd-rank,ranks,sentences
summarization,19,22,baselines,sentences,by how similar,sentence
summarization,19,22,baselines,sentence,compared to,all the other sentences
summarization,19,22,baselines,all the other sentences,in,document
summarization,19,22,baselines,Baselines,has,Rwmd-rank
summarization,19,23,baselines,Rwmd,stands for,Relaxed Word Mover 's Distance
summarization,19,23,baselines,LexRank,to rank,sentences
summarization,19,23,baselines,Baselines,has,Rwmd
summarization,19,25,baselines,Baselines,has,Three baselines
summarization,19,26,baselines,Lstm,is,common LSTM encoder-decoder model
summarization,19,26,baselines,Lstm,with,attention mechanism
summarization,19,26,baselines,attention mechanism,at,word-level
summarization,19,26,baselines,Baselines,has,Lstm
summarization,19,27,baselines,Fconv,is,CNN encoder-decoder
summarization,19,27,baselines,CNN encoder-decoder,on,subword - level
summarization,19,27,baselines,CNN encoder-decoder,separating,words
summarization,19,27,baselines,words,into,smaller units
summarization,19,27,baselines,smaller units,using,byte-pair encoding ( BPE )
summarization,19,27,baselines,Baselines,has,Fconv
summarization,19,11,experimental-setup,Title-gen,constructed using,MEDLINE
summarization,19,11,experimental-setup,abstract-gen,conducted using,PubMed
summarization,19,11,experimental-setup,Experimental setup,has,Title-gen
summarization,19,5,experiments,title- gen dataset,consists of,5 million biomedical papers
summarization,19,5,experiments,abstract - gen dataset,consists of,900K papers
summarization,19,12,experiments,title- gen,pairs,abstract
summarization,19,12,experiments,abstract,to,title of the paper
summarization,19,12,experiments,full body,to,abstract summary
summarization,19,15,experiments,Repeat score,measures,average overlap
summarization,19,15,experiments,average overlap,of,each sentence
summarization,19,15,experiments,each sentence,in,text
summarization,19,15,experiments,text,with,remainder of the text
summarization,19,16,experiments,repetitive content,body text of,paper
summarization,19,34,experiments,METEOR score,used for,machine translation
summarization,19,13,model,Repeat score,for,each data pairs
summarization,19,13,model,Model,has,text processing pipeline
summarization,19,14,model,Overlap score,measures,overlapping tokens
summarization,19,14,model,overlapping tokens,between,summary ( title or abstract ) and the input text ( abstract or full body )
summarization,19,14,model,Model,has,Overlap score
summarization,19,29,model,C2 c,is,character - level encoder-decoder model
summarization,19,29,model,Model,has,C2 c
summarization,19,30,model,character representations,from,input
summarization,19,30,model,character representations,feed it into,LSTM encoderdecoder model
summarization,19,30,model,input,using,CNN
summarization,19,30,model,Model,builds,character representations
summarization,19,17,results,summary statistics,of,both datasets
summarization,19,28,results,Results,has,Character
summarization,19,32,results,evaluation metrics,are,ROUGE scores
summarization,19,32,results,evaluation metrics,are,METEOR score
summarization,19,32,results,evaluation metrics,are,Overlap score
summarization,19,32,results,evaluation metrics,are,Repeat score
summarization,19,32,results,Results,has,evaluation metrics
summarization,19,37,results,rwmd-rank,is,best extractive model
summarization,19,37,results,all extractive models,by,large margin
summarization,19,37,results,large margin,including,oracle
summarization,19,37,results,title- gen results,has,rwmd-rank
summarization,19,37,results,c2,has,model )
summarization,19,37,results,c2,has,outperformed
summarization,19,37,results,model ),has,outperformed
summarization,19,37,results,outperformed,has,all extractive models
summarization,19,37,results,Results,For,title- gen results
summarization,19,38,results,Both c2 c and fconv,achieved,similar results
summarization,19,38,results,similar results,with,similar high overlap scores
summarization,19,38,results,Results,has,Both c2 c and fconv
summarization,19,39,results,lead - 10,was,strong baseline
summarization,19,39,results,abstract - gen results,has,lead - 10
summarization,19,39,results,Results,For,abstract - gen results
summarization,19,40,results,All extractive models,achieved,similar ROUGE scores
summarization,19,40,results,similar ROUGE scores,with,similar Repeat score
summarization,19,40,results,Results,has,All extractive models
summarization,19,41,results,Abstractive models,performed,poorly
summarization,19,41,results,poorly,based on,ROUGE scores
summarization,19,41,results,all models,in terms of,METEOR score
summarization,19,41,results,outperformed,has,all models
summarization,19,41,results,Results,has,Abstractive models
summarization,19,42,results,Results,has,Qualitative evaluation
summarization,19,45,results,abstractive generated titles,tend to be,high quality
summarization,19,45,results,abstractive generated titles,of,high quality
summarization,19,45,results,Lstm,generate,more novel words
summarization,19,45,results,Results,has,abstractive generated titles
summarization,19,46,results,generated titles,occasionally make mistakes,incorrect words
summarization,19,46,results,Results,has,generated titles
summarization,19,48,results,important content,spread across,sections
summarization,19,48,results,Results,has,important content
summarization,19,49,results,Output,of,fconv abstractive model
summarization,19,49,results,fconv abstractive model,is,bad quality
summarization,19,49,results,fconv abstractive model,of,bad quality
summarization,19,49,results,bad quality,lacks,coherent and content ow
summarization,19,49,results,Results,has,Output
summarization,20,16,ablation-analysis,Entropy,measures,coverage level
summarization,20,16,ablation-analysis,maximised,when,every semantic unit
summarization,20,16,ablation-analysis,every semantic unit,in,summary
summarization,20,16,ablation-analysis,Ablation analysis,has,Entropy
summarization,20,20,baselines,relevancy,compare,probability distributions
summarization,20,20,baselines,probability distributions,of,source document and summary
summarization,20,4,model,simple theoretical model,to capture,information importance
summarization,20,4,model,information importance,in,summarisation
summarization,20,4,model,Model,Proposed,simple theoretical model
summarization,20,5,model,information importance,in,summarisation
summarization,20,5,model,redundancy,has,"relevance , and informativeness"
summarization,20,5,model,Model,captures,redundancy
summarization,20,7,model,three key concepts,in,summarisation
summarization,20,8,model,Importance concept,using,three key concepts
summarization,20,8,model,Importance concept,interpret,results
summarization,20,8,model,three key concepts,in,summarisation
summarization,20,8,model,Model,Formulate,Importance concept
summarization,20,10,model,Framework Semantic unit,small piece of,information
summarization,20,10,model,Model,Overall,Framework Semantic unit
summarization,20,10,model,Model,has,Framework Semantic unit
summarization,20,11,model,Model,represents,possible semantic units
summarization,20,12,model,text input X,made up of,many semantic units
summarization,20,12,model,text input X,represented by,probability distribution
summarization,20,12,model,probability distribution,means,frequency distribution
summarization,20,12,model,of semantic units,in,overall text
summarization,20,12,model,frequency distribution,has,of semantic units
summarization,20,12,model,Model,has,text input X
summarization,20,13,model,probability,that,semantic unit
summarization,20,13,model,semantic unit,appears in,text X
summarization,20,13,model,semantic unit,could be,interepreted
summarization,20,13,model,Model,interpreted as,probability
summarization,20,15,model,level of information,presented in,summary
summarization,20,15,model,summary,measured by,entropy
summarization,20,15,model,Model,has,level of information
summarization,20,19,model,relevant summary,should have,minimum loss of information
summarization,20,31,model,background knowledge,customisethe,model
summarization,20,31,model,Model,customisethe,model
summarization,20,36,model,summary,should be,generated
summarization,20,36,model,generated,with,objective
summarization,20,36,model,objective,to bring,most new information
summarization,20,36,model,most new information,to,user
summarization,20,36,model,knowledge K,has,summary
summarization,20,37,model,each semantic unit,need,function
summarization,20,37,model,function,takes in,probability
summarization,20,37,model,probability,of,semantic unit
summarization,20,37,model,semantic unit,in,source document D
summarization,20,37,model,Model,for,each semantic unit
summarization,20,9,results,of importance,for,summarisation
summarization,20,9,results,of importance,good correlation with,human summarisation
summarization,20,9,results,summarisation,good correlation with,human summarisation
summarization,20,9,results,our theoretical model,has,of importance
summarization,20,9,results,Results,Showed,our theoretical model
summarization,20,18,results,relevant summary,closely approximates,original text
summarization,20,18,results,Results,has,relevant summary
summarization,20,21,results,average surprise,producing,S summary
summarization,20,21,results,S summary,expecting,D source
summarization,20,22,results,INFORMATIVENESS P K I nf,has,K )
summarization,20,22,results,Results,has,INFORMATIVENESS P K I nf
summarization,20,24,results,quantities,correlate with,human judgements
summarization,20,24,results,well,has,quantities
summarization,20,24,results,Results,assess,well
summarization,20,25,results,Each quantity,of,our framework
summarization,20,25,results,Each quantity,to score,sentences
summarization,20,25,results,sentences,for,summary
summarization,20,25,results,Results,has,Each quantity
summarization,20,27,results,generated summaries ( using ),similar to,human reference summaries ( )
summarization,20,28,results,both summaries,found that,human reference summaries
summarization,20,28,results,human reference summaries,scored,signi cantly higher
summarization,20,28,results,signi cantly higher,than,our generated summaries
summarization,20,29,results,S,measured by,cross entropy
summarization,20,29,results,cross entropy,between,summary and background knowledge
summarization,20,30,results,cross entropy,for,relevance
summarization,20,30,results,cross entropy,for,informativeness
summarization,20,30,results,cross entropy,for,informativeness
summarization,20,30,results,relevance,should be,low
summarization,20,30,results,relevance,should be,high
summarization,20,30,results,relevance,want,summary
summarization,20,30,results,low,want,summary
summarization,20,30,results,summary,to be,similar and relevant
summarization,20,30,results,cross entropy,for,informativeness
summarization,20,30,results,informativeness,should be,high
summarization,20,30,results,Results,has,cross entropy
summarization,20,35,results,Results,has,Importance
summarization,21,31,baselines,CNN and biLSTM,to encode,extraction units
summarization,21,9,experiments,QA pairs,extracted from,human abstract
summarization,21,9,experiments,questions,using,source document
summarization,21,16,experiments,better,as,extraction units
summarization,21,27,experiments,words or chunks ( phrases ),as,extraction units
summarization,21,6,hyperparameters,question answer ( QA ) pairs,limit,answer token
summarization,21,6,hyperparameters,answer token,to,salient word
summarization,21,6,hyperparameters,answer token,to,named entity
summarization,21,6,hyperparameters,Hyperparameters,To create,question answer ( QA ) pairs
summarization,21,8,hyperparameters,at least one QA pair,should be extracted from,each sentence
summarization,21,8,hyperparameters,each sentence,of,abstract
summarization,21,20,hyperparameters,answer tokens,chosen,randomly
summarization,21,20,hyperparameters,answer tokens,can be,root word
summarization,21,20,hyperparameters,answer tokens,can be,subj / obj word
summarization,21,20,hyperparameters,answer tokens,can be,NER word
summarization,21,20,hyperparameters,Hyperparameters,has,answer tokens
summarization,21,21,hyperparameters,participants,rate,informativeness
summarization,21,21,hyperparameters,informativeness,of,summary
summarization,21,21,hyperparameters,summary,from,1 - 5
summarization,21,21,hyperparameters,summary,being,most informative
summarization,21,21,hyperparameters,Hyperparameters,asked,participants
summarization,21,21,hyperparameters,Hyperparameters,rate,informativeness
summarization,21,28,hyperparameters,text chunks,using,sentence constituent parse tree
summarization,21,28,hyperparameters,each chunk,has,at most 5 words
summarization,21,28,hyperparameters,Hyperparameters,obtain,text chunks
summarization,21,30,model,Model,focused on,ner-grained extraction units
summarization,21,34,model,framework,whereby,importance
summarization,21,34,model,importance,of,t-th source extraction unit
summarization,21,34,model,importance,determined by,informativeness
summarization,21,34,model,importance,determined by,position
summarization,21,34,model,importance,determined by,relationship with the previously selected extraction units
summarization,21,34,model,t-th source extraction unit,determined by,informativeness
summarization,21,34,model,position,in,document
summarization,21,34,model,Model,use,framework
summarization,21,35,model,positional embeddings,to encode,position
summarization,21,35,model,position,has,of the extraction unit
summarization,21,35,model,Model,have,positional embeddings
summarization,21,36,model,each time step,build,vector representation
summarization,21,36,model,each time step,used it along with,positional embeddings
summarization,21,36,model,vector representation,of,our summary
summarization,21,36,model,vector representation,used it along with,positional embeddings
summarization,21,36,model,vector representation,used it along with,our encoded hidden states
summarization,21,36,model,our summary,has,up to time t - 1
summarization,21,37,model,architecture,is,unidirectional LSTM
summarization,21,37,model,Model,is,unidirectional LSTM
summarization,21,37,model,Model,has,architecture
summarization,21,5,results,Our generated summaries,yielded,competitive results
summarization,21,5,results,competitive results,measured by,automatic metrics
summarization,21,5,results,competitive results,measured by,human assessors
summarization,21,5,results,Results,has,Our generated summaries
summarization,21,7,results,salient word or named entity,in,all the sentences
summarization,21,7,results,all the sentences,in,human abstract
summarization,21,7,results,answer token,with,blank
summarization,21,7,results,blank,to create,Cloze-style QA pair
summarization,21,7,results,Results,identify,salient word or named entity
summarization,21,7,results,Results,replace,answer token
summarization,21,11,results,ROOT - type QA pairs,have,least number of unique answers
summarization,21,11,results,Results,observed,ROOT - type QA pairs
summarization,21,12,results,QASumm + ROOT,performed,best
summarization,21,12,results,QASumm + ROOT,performed,best
summarization,21,12,results,best,amongst,variant
summarization,21,12,results,variant,in,daily mail dataset
summarization,21,12,results,QASumm + NER,performed,best
summarization,21,12,results,best,in,CNN dataset
summarization,21,12,results,Results,has,QASumm + ROOT
summarization,21,13,results,maximise,has,performance
summarization,21,13,results,Results,maintaining,good number of unique answers
summarization,21,17,results,performance,of,LSTM and CNN encoder
summarization,21,17,results,performance,found that,chunks
summarization,21,17,results,performance,found that,chunks
summarization,21,17,results,LSTM and CNN encoder,found that,chunks
summarization,21,17,results,LSTM and CNN encoder,found that,chunks
summarization,21,17,results,chunks,with,LSTM
summarization,21,17,results,chunks,with,CNN
summarization,21,17,results,chunks,with,LSTM
summarization,21,17,results,chunks,performed,best
summarization,21,17,results,LSTM,performed,best
summarization,21,17,results,CNN,with,words
summarization,21,17,results,CNN,has,outperformed
summarization,21,17,results,outperformed,has,LSTM
summarization,21,17,results,outperformed,has,CNN
summarization,21,17,results,Results,compared,performance
summarization,21,22,results,Results,evaluated,summaries
summarization,21,23,results,average time it takes,to complete,single question
summarization,21,23,results,average time it takes,to complete,overall accuracy
summarization,21,23,results,average time it takes,to complete,informativeness score
summarization,21,23,results,Results,showcase,average time it takes
summarization,21,24,results,our QASumm,with,NER - type QA pairs
summarization,21,24,results,NER - type QA pairs,able to achieved,highest accuracy and informativeness
summarization,21,24,results,human performance,has,our QASumm
summarization,21,24,results,Results,Excluding,human performance
summarization,21,25,results,wide margin,in,QA accuracy
summarization,21,25,results,wide margin,despite,similar level
summarization,21,25,results,best performing model,has,wide margin
summarization,21,25,results,Results,found that,best performing model
summarization,22,28,ablation-analysis,article,focuses on,event
summarization,22,28,ablation-analysis,event,described in,summary
summarization,22,28,ablation-analysis,summary,has,Related
summarization,22,15,baselines,dataset construction,has,three steps
summarization,22,15,baselines,Baselines,has,dataset construction
summarization,22,26,baselines,article title,with,rst three sentences
summarization,22,26,baselines,rst three sentences,of,assigned summary
summarization,22,38,baselines,Baselines,considered,different oracles and baseline models
summarization,22,4,experiments,multi-document summarisation,built from,Wikipedia Current Events Portal ( WCEP )
summarization,22,4,experiments,MDS ),built from,Wikipedia Current Events Portal ( WCEP )
summarization,22,4,experiments,multi-document summarisation,has,MDS )
summarization,22,5,experiments,concise and neutral human-written summaries,of,news events
summarization,22,5,experiments,concise and neutral human-written summaries,with links to,external source articles
summarization,22,6,experiments,number of source articles,by looking at,related articles
summarization,22,6,experiments,related articles,of,source articles
summarization,22,6,experiments,source articles,in,Common Crawl archive
summarization,22,9,experiments,100 articles,per cluster,WCEP - 100 )
summarization,22,9,experiments,100 articles,due to,scalability
summarization,22,9,experiments,WCEP - 100 ),due to,scalability
summarization,22,16,experiments,WCEP,lists out,daily news events
summarization,22,16,experiments,daily news events,whereby,each news event
summarization,22,16,experiments,human summary,with,at least one external news articles
summarization,22,16,experiments,each news event,has,human summary
summarization,22,25,experiments,our additional articles,from,Common Crawl
summarization,22,25,experiments,our additional articles,related to,our source articles
summarization,22,32,experiments,summaries,using,coverage and density metrics
summarization,22,33,experiments,Coverage,measures,number of words
summarization,22,33,experiments,number of words,from,summary
summarization,22,33,experiments,summary,extracted from,all the articles
summarization,22,33,experiments,summary,be described as,series of extractions
summarization,22,33,experiments,density,measures,how well
summarization,22,33,experiments,summary,be described as,series of extractions
summarization,22,33,experiments,how well,has,summary
summarization,22,17,hyperparameters,Each individual events,contain,list of URLs
summarization,22,17,hyperparameters,list of URLs,has,to external source articles
summarization,22,17,hyperparameters,Hyperparameters,has,Each individual events
summarization,22,19,model,input articles,for,ground -truth summaries
summarization,22,19,model,input articles,each of,ground -truth summaries
summarization,22,19,model,ground -truth summaries,by searching for,similar articles
summarization,22,19,model,similar articles,in,Common Crawl News dataset
summarization,22,19,model,Model,extended,input articles
summarization,22,20,model,Model,by,simple logistic regression classi er
summarization,22,20,model,Model,training,simple logistic regression classi er
summarization,22,7,results,Results,has,MEASURING EXTRACTIVENESS OF OUR SUMMARIES
summarization,22,10,results,evaluation metrics,are,F1 score
summarization,22,10,results,Results,has,evaluation metrics
summarization,22,22,results,nal dataset,consists of,ground -truth summary
summarization,22,22,results,nal dataset,consists of,cluster of original source articles and related articles
summarization,22,22,results,Results,has,nal dataset
summarization,22,23,results,summary statistics,of,WCEP dataset
summarization,22,23,results,Results,showcase,summary statistics
summarization,22,31,results,related additional articles,from,350 articles
summarization,22,31,results,52 %,has,on-topic
summarization,22,31,results,30 %,has,related additional articles
summarization,22,31,results,Results,have,52 %
summarization,22,31,results,Results,have,30 %
summarization,22,35,results,WCEP dataset,shows,high coverage
summarization,22,35,results,high coverage,if,articles
summarization,22,35,results,articles,included from,Common Crawl
summarization,22,35,results,Results,has,WCEP dataset
summarization,22,36,results,copy mechanisms,useful for,generating summaries
summarization,22,36,results,Results,means,copy mechanisms
summarization,22,40,results,wide margin,between,oracle results
summarization,22,40,results,wide margin,between,best performaing model
summarization,22,41,results,supervised methods,by,small margin
summarization,22,41,results,supervised methods,has,seem to outperformed
summarization,22,41,results,seem to outperformed,has,unsupervised methods
summarization,22,41,results,Results,has,supervised methods
summarization,22,42,results,high single document oracle result,important to select,relevant articles
summarization,22,42,results,relevant articles,before,summarisation
summarization,22,42,results,Results,has,high single document oracle result
summarization,22,43,results,low performance,of,RANDOM LEAD
summarization,23,29,ablation-analysis,out - of- domain data,to train,our models
summarization,23,29,ablation-analysis,our models,may,negatively impact
summarization,23,29,ablation-analysis,our QAGS quality,due to,domain shift
summarization,23,29,ablation-analysis,negatively impact,has,our QAGS quality
summarization,23,34,ablation-analysis,number of questions,affect,correlation with human judgements
summarization,23,34,ablation-analysis,Ablation analysis,explore,number of questions
summarization,23,38,ablation-analysis,8.75 %,of,generated questions
summarization,23,38,ablation-analysis,generated questions,are,nonsense
summarization,23,38,ablation-analysis,3 %,are,well - formed
summarization,23,38,ablation-analysis,3 %,answer by,generated summary
summarization,23,38,ablation-analysis,Ablation analysis,found that,8.75 %
summarization,23,27,baselines,QAGS framework,requires,labelled data
summarization,23,27,baselines,labelled data,to train,QG and QA models
summarization,23,27,baselines,Baselines,has,QAGS framework
summarization,23,18,experiments,each summary,collected,3 annotations
summarization,23,18,experiments,each summary,obtain,single correctness score per summary
summarization,23,18,experiments,single correctness score per summary,by taking,majority vote
summarization,23,18,experiments,single correctness score per summary,averaging,binary scores
summarization,23,18,experiments,majority vote,for,each sentence
summarization,23,18,experiments,binary scores,across,summary sentences
summarization,23,37,experiments,"generated questions , article and summary answers",manually annotated,400 triplets
summarization,23,37,experiments,400 triplets,on,XSUM summaries
summarization,23,37,experiments,400 triplets,label them by,quality
summarization,23,4,hyperparameters,duplicates and questions,with,three tokens or less
summarization,23,4,hyperparameters,Hyperparameters,removing,duplicates and questions
summarization,23,5,hyperparameters,questions,into,QA model
summarization,23,5,hyperparameters,questions,predicted with,answer
summarization,23,5,hyperparameters,Hyperparameters,feed,questions
summarization,23,5,hyperparameters,Hyperparameters,remove,questions
summarization,23,44,hyperparameters,seq2seq model,to generate,questions
summarization,23,44,hyperparameters,questions,based on,answer and source article
summarization,23,44,hyperparameters,Hyperparameters,train,seq2seq model
summarization,23,6,model,K questions,based on,summary
summarization,23,6,model,Model,generated,K questions
summarization,23,9,model,extractive QA models,to extract,answers
summarization,23,9,model,answers,as,text spans
summarization,23,9,model,text spans,from,source document and summary
summarization,23,9,model,Model,have,extractive QA models
summarization,23,11,model,generated questions,using,source and summary
summarization,23,11,model,generated questions,both,source and summary
summarization,23,11,model,source and summary,to obtain,two sets of answers
summarization,23,11,model,Model,answer,generated questions
summarization,23,13,model,simple token - level F1 score,to compare,answers
summarization,23,13,model,simple token - level F1 score,measure,answer similarity
summarization,23,13,model,Model,have,simple token - level F1 score
summarization,23,14,model,answers,using,similarity metric
summarization,23,14,model,answer similarity score,over,all questions
summarization,23,14,model,Model,compare,answers
summarization,23,17,results,correlations,between,QAGS
summarization,23,17,results,correlations,between,human judgements
summarization,23,17,results,human judgements,of,factual consistency
summarization,23,17,results,Results,measured,correlations
summarization,23,21,results,human judgements,of,factual consistency
summarization,23,22,results,QAGS,achieved,highest correlation
summarization,23,22,results,highest correlation,by,substantial margin
summarization,23,22,results,Results,show,QAGS
summarization,23,23,results,QAGS,performed,2x better
summarization,23,23,results,2x better,than,next best performing metric
summarization,23,23,results,Results,has,QAGS
summarization,23,24,results,QAGS,scored,signi cantly lower
summarization,23,24,results,signi cantly lower,in,XSUM
summarization,23,24,results,other metrics,by,wide margin
summarization,23,24,results,still outperformed,has,other metrics
summarization,23,24,results,Results,has,QAGS
summarization,23,25,results,XSUM dataset,is,more abstractive
summarization,23,25,results,Results,showcase,XSUM dataset
summarization,23,28,results,effective,in,data rich domain
summarization,23,28,results,effective,in,niche domains
summarization,23,28,results,effective,in,niche domains
summarization,23,28,results,data rich domain,in,niche domains
summarization,23,28,results,Results,might be,effective
summarization,23,30,results,our QG model,using,SQUAD
summarization,23,30,results,collection,of,wikipedia articles
summarization,23,30,results,wikipedia articles,rather than,CNN articles
summarization,23,31,results,new correlations score,with,SQUAD - QG model
summarization,23,31,results,SQUAD - QG model,is,51.53 and 15.28
summarization,23,31,results,51.53 and 15.28,on,CNN / DM and XSUM dataset
summarization,23,31,results,Results,has,new correlations score
summarization,23,32,results,correlation scores,when using,NewsQA - QG model
summarization,23,32,results,signi cantly outperformed,has,other evaluation metrics
summarization,23,35,results,number of questions,see,consistent increase
summarization,23,35,results,consistent increase,in,correlation scores
summarization,23,35,results,correlation scores,in,both evaluation datasets
summarization,23,35,results,number of questions,has,increase
summarization,23,36,results,only 5 questions,able to achieve,correlations
summarization,23,36,results,higher,than,other evaluation metrics
summarization,23,36,results,only a small increase,in,correlation scores
summarization,23,36,results,only a small increase,when,increasing
summarization,23,36,results,correlation scores,when,increasing
summarization,23,36,results,number of questions,from,20 to 50
summarization,23,36,results,correlations,has,higher
summarization,23,36,results,increasing,has,number of questions
summarization,23,36,results,Results,with,only 5 questions
summarization,23,39,results,generated questions,are,easy to understanding
summarization,23,39,results,generated questions,are,meaningful
summarization,23,40,results,8.25 %,of,questions
summarization,23,40,results,8.25 %,answer by,source document
summarization,23,40,results,questions,are,wellformed
summarization,23,40,results,questions,answer by,source document
summarization,23,40,results,Results,has,8.25 %
summarization,23,41,results,large 32.50 %,of,incorrectly answers
summarization,23,41,results,incorrectly answers,using,source article
summarization,23,41,results,our QA model,is,weak
summarization,23,42,results,8 %,of,questions
summarization,23,42,results,answered correctly,using,source article and summary
summarization,23,42,results,Results,has,8 %
summarization,23,43,results,similarity,based on,how similar
summarization,23,43,results,how similar,has,answers
summarization,24,14,ablation-analysis,effect,of including,global and local context
summarization,24,14,ablation-analysis,global and local context,into,our summarisation approach
summarization,24,14,ablation-analysis,Ablation analysis,study,effect
summarization,24,15,ablation-analysis,local topic context,drives,strong improvement
summarization,24,15,ablation-analysis,strong improvement,in,overall performance
summarization,24,15,ablation-analysis,overall performance,of,model
summarization,24,15,ablation-analysis,Ablation analysis,seems that,local topic context
summarization,24,20,ablation-analysis,better structure,in representing,document
summarization,24,20,ablation-analysis,document,like,discourse tree
summarization,24,20,ablation-analysis,Ablation analysis,investigate,better structure
summarization,24,34,baselines,models,split into,different categories
summarization,24,34,baselines,Baselines,has,models
summarization,24,35,baselines,LSTM - minus,for,text summarisation
summarization,24,39,baselines,topic segment representation,used,LSTM - Minus
summarization,24,53,baselines,SumBasic,has,"LSA , and LexRank"
summarization,24,53,baselines,Baselines,has,SumBasic
summarization,24,32,experiments,ROUGE and METEOR,as,automatic evaluation metrics
summarization,24,40,experiments,LSTM - Minus,used for,learning
summarization,24,40,experiments,learning,has,text span embeddings
summarization,24,44,hyperparameters,embeddings,for,each segment ( from word i to word j )
summarization,24,44,hyperparameters,embeddings,compute,difference
summarization,24,44,hyperparameters,difference,between,hidden state i and hidden state j
summarization,24,44,hyperparameters,Hyperparameters,To create,embeddings
summarization,24,7,model,local context ( section ),to determine,sentence
summarization,24,7,model,sentence,is,informative enough
summarization,24,7,model,informative enough,to be,summary
summarization,24,7,model,informative enough,in,summary
summarization,24,7,model,Model,utilise,global context ( whole document )
summarization,24,7,model,Model,utilise,local context ( section )
summarization,24,19,model,traditional methods,such as,"feature engineering ( e.g. sentence position , saliency )"
summarization,24,19,model,traditional methods,into,our neural models
summarization,24,19,model,Model,integrate,traditional methods
summarization,24,24,model,sentence embedding,use,simple method
summarization,24,24,model,simple method,of averaging,word embeddings
summarization,24,24,model,Model,To compute,sentence embedding
summarization,24,28,model,document encoder,is,biGRU
summarization,24,28,model,biGRU,means,each sentence
summarization,24,28,model,two hidden states,has,forward ( yellow )
summarization,24,28,model,Model,has,document encoder
summarization,24,37,model,sentence representation,be,concatenation
summarization,24,37,model,concatenation,of,forward and backward hidden states
summarization,24,37,model,concatenation,both,forward and backward hidden states
summarization,24,37,model,Model,has,sentence representation
summarization,24,38,model,document representation,would be,concatenation
summarization,24,38,model,concatenation,of,nal end state
summarization,24,38,model,nal end state,of,forward and backward hidden states
summarization,24,38,model,nal end state,both,forward and backward hidden states
summarization,24,38,model,Model,For,document representation
summarization,24,41,model,LSTM - Minus,to create,embeddings
summarization,24,43,model,LSTM,to,whole sentence
summarization,24,43,model,LSTM,to obtain,hidden states
summarization,24,43,model,whole sentence,to obtain,hidden states
summarization,24,43,model,hidden states,of,each word
summarization,24,43,model,each word,in,sentence
summarization,24,43,model,Model,Apply,LSTM
summarization,24,45,model,each hidden state,contains,information
summarization,24,45,model,information,from,previous hidden states plus current word
summarization,24,45,model,information,outside and inside,segments
summarization,24,45,model,segment embeddings,using,information
summarization,24,45,model,information,outside and inside,segments
summarization,24,45,model,nal topic representation,concatenation of,forward and backward segment embeddings
summarization,24,45,model,forward and backward segment embeddings,has,SENTENCE CLASSIFIER
summarization,24,45,model,Model,to create,segment embeddings
summarization,24,46,model,sentence and document encoder,have,"sentence representations , document representation"
summarization,24,46,model,sentence and document encoder,have,topic segment representations
summarization,24,49,model,all three representations together,to form,nal representation
summarization,24,49,model,nal representation,has,Attentive Context
summarization,24,50,model,weighted context vector,for,each sentence
summarization,24,50,model,Model,compute,weighted context vector
summarization,24,51,model,weighted context vector,concatenated with,respective sentence representation
summarization,24,51,model,weighted context vector,feed into,MLP
summarization,24,51,model,MLP,with,sigmoid activation function
summarization,24,51,model,sigmoid activation function,to determine,sentence
summarization,24,51,model,sentence,should be included in,summary
summarization,24,51,model,Model,has,weighted context vector
summarization,24,6,results,topics,to guide,summarisation
summarization,24,6,results,summarisation,could,improvement
summarization,24,6,results,summarisation,see,improvement
summarization,24,6,results,Results,using,topics
summarization,24,9,results,all the other extractive models,in,all evaluation metrics
summarization,24,9,results,outperformed,has,all the other extractive models
summarization,24,10,results,performance,in,our models
summarization,24,10,results,performance,from,baseline models
summarization,24,10,results,performance,demonstrate,bene t
summarization,24,10,results,our models,from,baseline models
summarization,24,10,results,bene t,of including,local and global context
summarization,24,10,results,Results,increase in,performance
summarization,24,12,results,our models,have,good performance gain
summarization,24,12,results,good performance gain,in comparison to,current SOTA models
summarization,24,12,results,document length,has,increase
summarization,24,12,results,document length,has,our models
summarization,24,12,results,increase,has,our models
summarization,24,12,results,Results,as,document length
summarization,24,16,results,no signi cant bene t,in,global context
summarization,24,16,results,no signi cant bene t,including,global context
summarization,24,16,results,Results,seems to be,no signi cant bene t
summarization,24,25,results,RNN and CNN,when computing,sentence embeddings
summarization,24,26,results,BERT sentence embedding,performed,poorly
summarization,24,26,results,Results,has,BERT sentence embedding
summarization,24,58,results,Results,on,ROUGE -L
summarization,24,58,results,ROUGE -L,are,mixed
summarization,24,58,results,Results,on,ROUGE -L
summarization,24,58,results,Results,has,Results
summarization,24,59,results,all the extractive models,in,ROUGE -L
summarization,24,59,results,Discourse- aware abstractive model,has,outperformed
summarization,24,59,results,outperformed,has,all the extractive models
summarization,24,59,results,Results,has,Discourse- aware abstractive model
