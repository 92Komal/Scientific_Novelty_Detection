topic,paper_ID,sentence_ID,info-unit,sub,pred,obj
summarization,0,20,ablation-analysis,ATSA,extracted,aspect terms
summarization,0,20,ablation-analysis,ATSA,map them with,appropriate sentiment
summarization,0,20,ablation-analysis,ATSA,remove,any sentences
summarization,0,20,ablation-analysis,aspect terms,in,sentences
summarization,0,20,ablation-analysis,aspect terms,map them with,appropriate sentiment
summarization,0,20,ablation-analysis,sentences,with,one aspect
summarization,0,20,ablation-analysis,any sentences,with,one aspect
summarization,0,20,ablation-analysis,any sentences,with,multiple aspects
summarization,0,20,ablation-analysis,multiple aspects,with,same sentiment
summarization,0,20,ablation-analysis,Ablation analysis,For,ATSA
summarization,0,6,baselines,Baselines,proposed,simple baseline model
summarization,0,22,experiments,ACSA,pre-de,eight aspect categories
summarization,0,22,experiments,ACSA,ned,eight aspect categories
summarization,0,24,experiments,sentences,with,at least two unique aspect categories
summarization,0,24,experiments,at least two unique aspect categories,with,different sentiment
summarization,0,21,hyperparameters,start and end positions,for,each aspect term
summarization,0,28,hyperparameters,aspect term embedding,average of,aspect word embeddings
summarization,0,28,hyperparameters,Hyperparameters,For,aspect term embedding
summarization,0,29,hyperparameters,aspect category embedding,initialised,embedding
summarization,0,29,hyperparameters,aspect category embedding,learn during,training
summarization,0,29,hyperparameters,embedding,learn during,training
summarization,0,29,hyperparameters,embedding,has,randomly
summarization,0,29,hyperparameters,Hyperparameters,For,aspect category embedding
summarization,0,33,hyperparameters,sentiment matrix,feed into,squash activation
summarization,0,33,hyperparameters,squash activation,to obtain,sentiment capsules
summarization,0,33,hyperparameters,training,has,sentiment matrix
summarization,0,33,hyperparameters,Hyperparameters,During,training
summarization,0,8,model,aspect-aware sentence embedding,feed it into,Bi-directional GRU
summarization,0,8,model,Bi-directional GRU,with,residual connection
summarization,0,8,model,residual connection,to get,contextualised representation
summarization,0,8,model,Model,take,aspect-aware sentence embedding
summarization,0,10,model,squashing activation,get,primary capsules P
summarization,0,10,model,primary capsules P,using,contextualised representation
summarization,0,10,model,primary capsules P,using,aspect capsule
summarization,0,10,model,primary capsules P,using,aspect embedding
summarization,0,10,model,primary capsules P,using,aspect embedding
summarization,0,10,model,aspect capsule,using,aspect embedding
summarization,0,10,model,aspect embedding,from,embedding layer
summarization,0,10,model,Model,Using,linear transformation
summarization,0,12,model,length,cause,training
summarization,0,12,model,aspect capsule,to normalise,primary capsule weights
summarization,0,12,model,primary capsule weights,to select,important primary capsules
summarization,0,12,model,training,has,to be unstable
summarization,0,12,model,Model,use,aspect capsule
summarization,0,15,model,capsule- guided routing weights,compute,nal category capsules
summarization,0,15,model,primary capsules,has,aspect-aware normalised weights
summarization,0,15,model,Model,Using,primary capsules
summarization,0,16,model,embedding and encoding layer,replaced with,pre-trained BERT
summarization,0,16,model,CapsNet - BERT,has,embedding and encoding layer
summarization,0,16,model,Model,for,CapsNet - BERT
summarization,0,23,model,Each sentence,map to,aspect category
summarization,0,23,model,Each sentence,map to,aspect category
summarization,0,23,model,aspect category,along with,appropriate sentiment
summarization,0,23,model,appropriate sentiment,towards,aspect category
summarization,0,23,model,Model,has,Each sentence
summarization,0,26,model,sentence,with respect to,aspects
summarization,0,26,model,model,to predict,sentiment
summarization,0,26,model,sentiment,of,sentence
summarization,0,26,model,sentence,with respect to,aspects
summarization,0,26,model,Model,Given,sentence
summarization,0,26,model,Model,to predict,sentiment
summarization,0,30,model,output,of,embedding layer
summarization,0,30,model,embedding layer,is,aspect- aware sentence embedding
summarization,0,30,model,aspect- aware sentence embedding,concatenate,aspect embedding
summarization,0,30,model,aspect embedding,with,each word embedding
summarization,0,30,model,each word embedding,in,sentence
summarization,0,30,model,Model,has,output
summarization,0,32,model,prior knowledge,of,sentiment categories
summarization,0,32,model,sentiment categories,to improve,routing process
summarization,0,32,model,Model,leverages,prior knowledge
summarization,0,34,model,routing weights,by measuring,similarity
summarization,0,34,model,similarity,between,primary capsules and sentiment capsules
summarization,0,34,model,Model,has,routing weights
summarization,0,5,results,proposed MAMS dataset,could solve,common issue
summarization,0,5,results,ABSA dataset,to,sentence - level sentiment analysis
summarization,0,5,results,ABSA,to,sentence - level sentiment analysis
summarization,0,5,results,Results,has,proposed MAMS dataset
summarization,1,15,ablation-analysis,emotion label,of,happy
summarization,1,43,ablation-analysis,Ablation analysis,effectiveness of,pair filtering phase
summarization,1,8,experiments,fourth clause,considered,emotion clause
summarization,1,8,experiments,emotion clause,conveying,emotion of happiness
summarization,1,9,experiments,clauses,that contain,causes ( cause clause )
summarization,1,9,experiments,causes ( cause clause ),are,second clause and the third clause
summarization,1,14,experiments,pairs,of,emotion - cause clauses
summarization,1,35,experiments,benchmark emotion - cause dataset,by,"Gui et al. , 2016"
summarization,1,33,hyperparameters,pairs,that,output 1
summarization,1,33,hyperparameters,pairs,used for,evaluation
summarization,1,33,hyperparameters,output 1,final set of,emotion - cause pairs
summarization,1,33,hyperparameters,Hyperparameters,has,pairs
summarization,1,4,model,deep learning method,to extract,causes behind emotions in a document
summarization,1,4,model,Model,develop,deep learning method
summarization,1,6,model,emotion and causes,from,text
summarization,1,6,model,emotion and causes,using,multi-task learning
summarization,1,6,model,emotion and causes,conducts,emotion - cause pairing and filtering
summarization,1,6,model,emotion - cause pairing and filtering,using,improved version of the multi-task learning model
summarization,1,6,model,Model,separately extract,emotion and causes
summarization,1,18,model,two parts,conduct,emotion - cause pairing and filtering
summarization,1,18,model,extract sets,of,emotion and cause clauses
summarization,1,18,model,emotion and cause clauses,from,each document
summarization,1,18,model,each document,via,two kinds of multi-task learning networks
summarization,1,19,model,pairing,done via,Cartesian product
summarization,1,19,model,Cartesian product,applied to,set of emotion and cause clauses
summarization,1,19,model,Model,has,pairing
summarization,1,20,model,candidate emotion - cause pairs,on which,filter
summarization,1,20,model,filter,applied to remove,clause pairs
summarization,1,20,model,clause pairs,do n't contain,causal relationship
summarization,1,21,model,emotion and cause clauses,are,mutually independent
summarization,1,21,model,emotion and cause clauses,not,mutually independent
summarization,1,21,model,Model,propose,interactive multi
summarization,1,23,model,prediction,for,each type of clause
summarization,1,23,model,prediction,performed via,Bi-LSTM hidden representations
summarization,1,23,model,Bi-LSTM hidden representations,consist of,context - aware representations
summarization,1,23,model,context - aware representations,of,each clause
summarization,1,23,model,Model,shows how,prediction
summarization,1,25,model,Model,has,task learning network
summarization,1,27,model,improved model,performs,few extra steps
summarization,1,27,model,few extra steps,in,upper layers
summarization,1,27,model,few extra steps,allows,prediction
summarization,1,27,model,prediction,based on,interaction
summarization,1,27,model,interaction,of,two types of representations
summarization,1,28,model,enhancing emotion extraction,based on,cause extraction
summarization,1,28,model,enhancing cause extraction,based on,emotion extraction
summarization,1,28,model,cause extraction,has,Inter CE )
summarization,1,28,model,emotion extraction,has,Inter EC )
summarization,1,29,model,Model,has,output
summarization,1,30,model,objective,pair,two sets
summarization,1,30,model,two sets,to form,emotion - cause pairs
summarization,1,30,model,emotion - cause pairs,that form,causal relationships
summarization,1,30,model,Model,pair,two sets
summarization,1,30,model,Model,has,objective
summarization,1,32,model,cause relationship,determined via,logistic regression model
summarization,1,32,model,cause relationship,outputs,1 or 0
summarization,1,32,model,logistic regression model,takes as input,emotion - cause candidate pair features
summarization,1,32,model,emotion - cause candidate pair features,applied,Sigmoid operation
summarization,1,32,model,1 or 0,to indicate,relationship
summarization,1,32,model,relationship,exists for,each pair
summarization,1,32,model,Model,has,cause relationship
summarization,1,46,model,both sub-tasks,to improve,ECPE
summarization,1,46,model,Model,has,both sub-tasks
summarization,1,16,results,Results,has,identifying causes
summarization,1,36,results,"Precision , recall , and F1 scores",used as,evaluation metrics
summarization,1,36,results,Results,has,"Precision , recall , and F1 scores"
summarization,1,37,results,all the separate tasks,using,three proposed models
summarization,1,37,results,three proposed models,based on,multi-task learning
summarization,1,37,results,Results,for,all the separate tasks
summarization,1,38,results,Inter -EC,produced,better results
summarization,1,38,results,Results,has,Inter -EC
summarization,1,39,results,each other,in,individual tasks
summarization,1,39,results,cause extraction,has,improve
summarization,1,39,results,improve,has,each other
summarization,1,39,results,Results,observe,emotion extraction
summarization,1,40,results,results,consistent with,intuition
summarization,1,40,results,intuition,that,emotion and cause
summarization,1,40,results,emotion and cause,are,mutually indicative
summarization,1,40,results,Results,consistent with,intuition
summarization,1,40,results,Results,has,results
summarization,1,41,results,Other variations,of,models
summarization,1,41,results,models,leverage,dataset available annotations
summarization,1,41,results,improvements,use,additional information
summarization,1,41,results,additional information,to improve,predictions
summarization,1,41,results,predictions,of,different tasks
summarization,1,41,results,Results,has,Other variations
summarization,1,44,results,Results,using,rule-based and machine learning methods
summarization,1,45,results,emotion extraction,helps to improve,cause extraction
summarization,1,45,results,emotion extraction,helps to enhance,emotion extraction
summarization,1,45,results,cause extraction,helps to enhance,emotion extraction
summarization,1,45,results,Results,indicate,emotion extraction
summarization,2,33,ablation-analysis,listener component,does n't improve,model 's performance
summarization,2,38,ablation-analysis,important ablation study,to observe,importance
summarization,2,38,ablation-analysis,importance,of,Emotion GRU
summarization,2,38,ablation-analysis,importance,of,Party State components
summarization,2,38,ablation-analysis,Ablation analysis,to observe,importance
summarization,2,38,ablation-analysis,Ablation analysis,has,important ablation study
summarization,2,39,ablation-analysis,absence of part state,has,decreases
summarization,2,39,ablation-analysis,decreases,has,performance
summarization,2,40,ablation-analysis,party state,seems to be,more important
summarization,2,40,ablation-analysis,more important,than,Emotion GRU
summarization,2,40,ablation-analysis,Ablation analysis,observed,party state
summarization,2,26,baselines,DialogueRNN_l,considers,extra listener state
summarization,2,26,baselines,extra listener state,while,speaker utters
summarization,2,27,baselines,bidirectional RNN architecture,used instead,DialogueRNN + Att
summarization,2,27,baselines,attention,applied over,all surrounding emotion representations
summarization,2,27,baselines,BiDialogueRNN,has,bidirectional RNN architecture
summarization,2,27,baselines,BiDialogueRNN,has,attention
summarization,2,27,baselines,DialogueRNN + Att,has,attention
summarization,2,27,baselines,Baselines,has,BiDialogueRNN
summarization,2,2,experiments,DialogueRNN,has,Emotion Classification in Conversation
summarization,2,11,experiments,utterances,come in,multimodal setting
summarization,2,11,experiments,audio and visual features,extracted using,3D - CNN and openSMILE
summarization,2,11,experiments,utterances,has,audio and visual features
summarization,2,11,experiments,multimodal setting,has,audio and visual features
summarization,2,30,experiments,interactions,has,between multiple parties
summarization,2,36,experiments,DialogueRNN,correctly anticipates,emotion of frustration
summarization,2,36,experiments,emotion of frustration,using,preceding context ( 41 and 42 )
summarization,2,8,model,emotion detection model,considers,individual speakers
summarization,2,8,model,individual speakers,by focusing on,three different aspects
summarization,2,9,model,accurately predict,has,emotion of the utterance
summarization,2,10,model,Model Utterances,for,"party ( i.e. , individual )"
summarization,2,10,model,Model Utterances,represented through,textual features
summarization,2,10,model,"party ( i.e. , individual )",represented through,textual features
summarization,2,10,model,textual features,obtained from,convolutional neural network
summarization,2,10,model,Model,for,"party ( i.e. , individual )"
summarization,2,10,model,Model,has,Model Utterances
summarization,2,12,model,network,trained at,utterance level
summarization,2,12,model,utterance level,with,target emotion labels
summarization,2,12,model,Model,has,network
summarization,2,13,model,final emotion,of,utterance
summarization,2,13,model,Party state,models,parties ' emotion dynamics
summarization,2,13,model,parties ' emotion dynamics,through,conversations
summarization,2,13,model,Model,called,DialogueRNN
summarization,2,13,model,Model,determines,final emotion
summarization,2,14,model,party state,to ensure,model
summarization,2,14,model,model,is aware of,speaker
summarization,2,14,model,speaker,of,each utterance
summarization,2,14,model,each utterance,in,conversation
summarization,2,14,model,Model,behind,party state
summarization,2,15,model,Global state,jointly encoding,preceding utterances and the party state
summarization,2,15,model,Model,has,Global state
summarization,2,16,model,attention mechanism,applied to,global state
summarization,2,16,model,global state,to provide,improved context representation
summarization,2,16,model,Model,Note,attention mechanism
summarization,2,18,model,Emotion representation,inferred through,party state
summarization,2,18,model,Emotion representation,inferred through,preceding speaker 's states
summarization,2,18,model,preceding speaker 's states,as,context ( global state )
summarization,2,18,model,Model,has,Emotion representation
summarization,2,19,model,final emotion classification,via,softmax layer
summarization,2,20,model,Each component,of,architecture
summarization,2,20,model,Model,has,Each component
summarization,2,21,model,speaker state,updated using,current utterance
summarization,2,21,model,current utterance,along with,context
summarization,2,21,model,training,has,speaker state
summarization,2,21,model,Model,during,training
summarization,2,22,model,role,of,attention mechanism
summarization,2,22,model,role,assigns,higher attention scores
summarization,2,22,model,attention mechanism,assigns,higher attention scores
summarization,2,22,model,higher attention scores,to,utterances
summarization,2,22,model,emotionally relevant,to,current utterance
summarization,2,22,model,Model,has,role
summarization,2,23,model,information,on,current utterance
summarization,2,23,model,current utterance,along with,context
summarization,2,23,model,context,from,Global GRU
summarization,2,23,model,speaker update,has,information
summarization,2,23,model,Party GRU,has,information
summarization,2,23,model,Model,has,speaker update
summarization,2,24,model,information,important for performing,final emotion classification
summarization,2,24,model,final emotion classification,performed by,emotion GRU
summarization,2,24,model,Model,has,information
summarization,2,25,model,current emotion classification,relies on,previous emotion - relevant information
summarization,2,41,model,state of the listener,based on,current speaker utterance
summarization,2,41,model,Listener update,has,changes
summarization,2,41,model,changes,has,state of the listener
summarization,2,41,model,Model,has,Listener update
summarization,2,42,model,Visual cues,represent,information
summarization,2,42,model,Model,has,Visual cues
summarization,2,4,results,Results,Welcome to,another paper review
summarization,2,31,results,DialogueRNN,has,outperforms
summarization,2,31,results,outperforms,has,all baselines
summarization,2,31,results,Results,observe that,DialogueRNN
summarization,2,32,results,Results,using,text modality
summarization,2,34,results,other variants,found to perform,well
summarization,2,34,results,well,especially,BiDialogueRNN + Att
summarization,2,34,results,well,produced,better results
summarization,2,34,results,BiDialogueRNN + Att,produced,better results
summarization,2,34,results,Results,has,other variants
summarization,2,35,results,other models,in,multimodal setting
summarization,2,35,results,proposed model,has,DialogueRNN
summarization,2,35,results,proposed model,has,significantly outperforms
summarization,2,35,results,significantly outperforms,has,other models
summarization,2,37,results,Results,For,CMN model
summarization,3,14,ablation-analysis,answers,predicted by,MRC model
summarization,3,14,ablation-analysis,answers,considered to be,key entities
summarization,3,14,ablation-analysis,MRC model,considered to be,key entities
summarization,3,14,ablation-analysis,Ablation analysis,has,answers
summarization,3,7,experiments,key entity detection,in,uni ed approach
summarization,3,7,experiments,uni ed approach,based on,RoBERTa
summarization,3,8,experiments,RoBERTa,using,different methods
summarization,3,8,experiments,different methods,to implement,sentiment analysis
summarization,3,8,experiments,different methods,to implement,key entity detection
summarization,3,9,experiments,sentiment analysis,focus speci cally on,negative emotion information
summarization,3,12,experiments,Tags,can be,corruption
summarization,3,12,experiments,Tags,can be,fraud
summarization,3,20,experiments,sentiment,of,text
summarization,3,20,experiments,text,using,RoBERTa ( classic )
summarization,3,20,experiments,RoBERTa ( classic ),Get,nancial entity list
summarization,3,20,experiments,RoBERTa ( classic ),select,key entities
summarization,3,20,experiments,nancial entity list,select,key entities
summarization,3,20,experiments,key entities,with,tags
summarization,3,20,experiments,key entity,with,tags
summarization,3,24,experiments,number of models,determined by,number of entities
summarization,3,27,experiments,two datasets,for,Financial Field
summarization,3,27,experiments,Event Subject Extraction,for,Financial Field
summarization,3,21,hyperparameters,each piece,of,nancial data
summarization,3,21,hyperparameters,each piece,use,NER
summarization,3,21,hyperparameters,NER,to get,list of entities
summarization,3,21,hyperparameters,Hyperparameters,for,each piece
summarization,3,6,model,outperformed,for,two nancial sentiment analysis and key entity detection datasets
summarization,3,6,model,BERT,for,two nancial sentiment analysis and key entity detection datasets
summarization,3,6,model,outperformed,has,SVM
summarization,3,6,model,outperformed,has,LR
summarization,3,6,model,outperformed,has,NBM
summarization,3,6,model,outperformed,has,BERT
summarization,3,6,model,Model,proposed,ensemble learning
summarization,3,11,model,most relevant key entities,tags of,nancial text
summarization,3,19,model,key entity detection approach,for,online nancial texts
summarization,3,19,model,Model,propose,RoBERTa based sentiment analysis
summarization,3,22,model,coarse- grained task,detected,some key entities
summarization,3,22,model,some key entities,related to,nancial text
summarization,3,22,model,nancial text,from,entity list
summarization,3,22,model,Model,In,coarse- grained task
summarization,3,23,model,each entity and the nancial text,into,RoBERTa
summarization,3,23,model,each entity and the nancial text,into,RoBERTa
summarization,3,23,model,each entity and the nancial text,use,RoBERTa
summarization,3,23,model,RoBERTa,as,sentence matching model
summarization,3,23,model,RoBERTa,as,sentence matching model
summarization,3,23,model,sentence matching model,to determine,each entity
summarization,3,23,model,Model,feed,each entity and the nancial text
summarization,3,25,model,key entities,predicted as,key
summarization,3,25,model,key entities,by,RoBERTa
summarization,3,25,model,key,by,RoBERTa
summarization,3,10,results,key entity detection,able to,match
summarization,3,10,results,each nancial entity,with,text
summarization,3,10,results,each nancial entity,to detect,key entities
summarization,3,10,results,text,to detect,key entities
summarization,3,10,results,key entities,of,negative information
summarization,3,10,results,match,has,each nancial entity
summarization,3,10,results,Results,With,key entity detection
summarization,3,28,results,descriptive statistics,of,dataset
summarization,3,28,results,Results,showcase,descriptive statistics
summarization,3,29,results,evaluation metrics,are,accuracy rate
summarization,3,29,results,evaluation metrics,are,F1 score
summarization,3,29,results,accuracy rate,for,sentiment analysis
summarization,3,29,results,F1 score,for,key entity detection
summarization,3,29,results,Results,has,evaluation metrics
summarization,4,13,ablation-analysis,four most frequent aspects,from,dataset
summarization,4,13,ablation-analysis,Ablation analysis,Selected,four most frequent aspects
summarization,4,35,ablation-analysis,underperforming,of,LSTM
summarization,4,35,ablation-analysis,underperforming,due to,lack of training data
summarization,4,35,ablation-analysis,Ablation analysis,has,underperforming
summarization,4,17,baselines,Different variation of logistic regression,with,linguistic features LSTM
summarization,4,17,baselines,linguistic features LSTM,with,nal and location
summarization,4,17,baselines,nal and location,output,state SentiHood dataset
summarization,4,17,baselines,Baselines,has,Different variation of logistic regression
summarization,4,18,baselines,few strong baseline models,using,logistic regression
summarization,4,18,baselines,few strong baseline models,using,LSTM
summarization,4,18,baselines,few strong baseline models,for,future benchmarking
summarization,4,18,baselines,LSTM,for,future benchmarking
summarization,4,18,baselines,Baselines,Develop,few strong baseline models
summarization,4,34,baselines,best performing model,is,logistic regression
summarization,4,34,baselines,logistic regression,with,location masking
summarization,4,34,baselines,logistic regression,with,POS information
summarization,4,34,baselines,Baselines,has,best performing model
summarization,4,6,experiments,design of the space,is,good
summarization,4,6,experiments,service,is,horrid
summarization,4,10,experiments,"polarity class "" None """,where,sentence
summarization,4,10,experiments,sentence,does not contain,opinion
summarization,4,10,experiments,opinion,for,aspect a of location l
summarization,4,30,experiments,SentiHood Dataset,Contains,annotated sentences
summarization,4,30,experiments,annotated sentences,containing,one or two location entity mentions
summarization,4,31,experiments,sentences,with,single location entity
summarization,4,31,experiments,sentences,with,two location entities
summarization,4,31,experiments,5215 sentences,has,sentences
summarization,4,5,results,Results,has,TABSA vs ABSA
summarization,4,14,results,Results,broken down to,single location sentences
summarization,4,14,results,Results,broken down to,two locations sentences
summarization,4,14,results,Results,broken down to,overall test set
summarization,4,14,results,Results,broken down to,single location sentences
summarization,4,14,results,Results,broken down to,overall test set
summarization,4,14,results,Results,has,Results
summarization,4,15,results,results,on,single location sentences
summarization,4,15,results,results,on,two locations sentences
summarization,4,15,results,results,on,two locations sentences
summarization,4,15,results,single location sentences,showcase,model 's ability
summarization,4,15,results,single location sentences,showcase,model 's ability
summarization,4,15,results,model 's ability,to perform,correct sentiment analysis
summarization,4,15,results,model 's ability,to detect,relevant sentiment
summarization,4,15,results,two locations sentences,show,model 's ability
summarization,4,15,results,model 's ability,to detect,relevant sentiment
summarization,4,15,results,model 's ability,recognise,target entity
summarization,4,15,results,relevant sentiment,of,aspect
summarization,4,15,results,relevant sentiment,recognise,target entity
summarization,4,15,results,target entity,of,opinion
summarization,4,15,results,Results,on,single location sentences
summarization,4,15,results,Results,on,two locations sentences
summarization,4,15,results,Results,on,two locations sentences
summarization,4,26,results,space,is,great
summarization,4,26,results,great,in,McDonalds
summarization,4,26,results,service,is,horrid
summarization,4,26,results,staff,are,very friendly
summarization,4,26,results,Results,design of,space
summarization,4,36,results,consistent outperformance,of,logistic regression
summarization,4,36,results,consistent outperformance,highlights,advantage
summarization,4,36,results,logistic regression,over,LSTM
summarization,4,36,results,advantage,of,features engineering
summarization,4,36,results,advantage,when,volume of data
summarization,4,36,results,features engineering,when,volume of data
summarization,4,36,results,volume of data,is,low
summarization,4,36,results,Results,has,consistent outperformance
summarization,5,23,ablation-analysis,sparse coef cient vector,showcase,importance of different words
summarization,5,23,ablation-analysis,importance of different words,in,context
summarization,5,23,ablation-analysis,importance of different words,using,step function
summarization,5,23,ablation-analysis,context,using,step function
summarization,5,23,ablation-analysis,Ablation analysis,has,sparse coef cient vector
summarization,5,25,baselines,Baselines,has,LSTM - Final
summarization,5,26,baselines,BiLSTM,uses,nal hidden states
summarization,5,26,baselines,LSTM - Loc.,has,BiLSTM
summarization,5,26,baselines,Baselines,has,BiLSTM
summarization,5,28,baselines,Delayed memory mechanism,has,RE
summarization,5,28,baselines,RE,has,+ SenticLSTM
summarization,5,28,baselines,Baselines,has,Delayed memory mechanism
summarization,5,29,baselines,Baselines,has,re ned embeddings + SenticLSTM
summarization,5,32,experiments,SemEval 2015,showcase,similar results
summarization,5,32,experiments,similar results,with,our proposed methods
summarization,5,32,experiments,similar results,has,outperforming
summarization,5,32,experiments,our proposed methods,has,outperforming
summarization,5,32,experiments,outperforming,has,original models
summarization,5,8,hyperparameters,our aspect embeddings,to move closer to,our highly correlated target embeddings
summarization,5,8,hyperparameters,our aspect embeddings,move away from,irrelevant ones
summarization,5,8,hyperparameters,Hyperparameters,ne-tuned,our aspect embeddings
summarization,5,21,hyperparameters,model,to minimise,distance
summarization,5,21,hyperparameters,distance,to obtain,nal re ned embeddings
summarization,5,21,hyperparameters,nal re ned embeddings,for,target and aspect TARGET REPRESENTATION
summarization,5,21,hyperparameters,Hyperparameters,Compute,squared Euclidean function
summarization,5,5,model,aspect embeddings,by using,sparse coef cient vectors
summarization,5,5,model,sparse coef cient vectors,of,highly correlated words
summarization,5,7,model,each aspect,compute,context - aware aspect embeddings
summarization,5,7,model,context - aware aspect embeddings,by minimising,squared Euclidean distance
summarization,5,7,model,squared Euclidean distance,between,aspect embeddings
summarization,5,7,model,squared Euclidean distance,between,context - aware target embeddings
summarization,5,7,model,squared Euclidean distance,between,irrelevant embeddings
summarization,5,7,model,Model,for,each aspect
summarization,5,15,model,context - aware embeddings,for,targets
summarization,5,15,model,context - aware embeddings,by using,sparse coef cient vectors
summarization,5,15,model,sparse coef cient vectors,to identify,words
summarization,5,15,model,words,highly correlated to,targets
summarization,5,15,model,words,ning,target embeddings
summarization,5,15,model,Model,Construct,context - aware embeddings
summarization,5,17,model,Model,has,model framework
summarization,5,18,model,Model,has,Sentence embedding matrix
summarization,5,19,model,X,feed into,fully connected layer
summarization,5,19,model,X,feed into,step function
summarization,5,19,model,step function,to create,sparse coef cient vector u'
summarization,5,19,model,Model,has,X
summarization,5,20,model,hidden output,of,u '
summarization,5,20,model,u ',used to re ne,target and aspect embeddings
summarization,5,20,model,Model,has,hidden output
summarization,5,22,model,re ned target embeddings,multiplying,sentence word embeddings X
summarization,5,22,model,sentence word embeddings X,with,sparse coef cient vector u'
summarization,5,22,model,Model,has,re ned target embeddings
summarization,5,24,model,each target,compute,context - aware target embedding
summarization,5,24,model,context - aware target embedding,by iteratively minimising,squared Euclidean distance
summarization,5,24,model,squared Euclidean distance,between,target and the highly correlative words
summarization,5,24,model,target and the highly correlative words,in,sentence
summarization,5,24,model,Model,For,each target
summarization,5,36,model,highly correlated words,to re ne,targets and aspects embeddings
summarization,5,36,model,highly correlated words,extract,interconnection
summarization,5,36,model,interconnection,between,"speci c target , its aspect"
summarization,5,36,model,interconnection,between,context
summarization,5,36,model,interconnection,to generate,better meaningful embedding
summarization,5,36,model,Model,selecting and using,highly correlated words
summarization,5,30,results,re ned embeddings + Delayed - Memory RESULTS,For,SentiHood
summarization,5,30,results,our proposed methods,added on top of,SenticLSTM
summarization,5,30,results,Delayed - Memory,achieved,better performance
summarization,5,30,results,better performance,than,original models
summarization,5,30,results,original models,in,aspect detection
summarization,5,30,results,original models,in,sentiment classi cation
summarization,5,30,results,Results,has,re ned embeddings + Delayed - Memory RESULTS
summarization,5,31,results,allowed models,to better capture,aspects and sentiment information
summarization,5,31,results,interconnection,between,target
summarization,5,31,results,Our context - aware embeddings,has,allowed models
summarization,5,31,results,better model,has,interconnection
summarization,5,31,results,Results,has,Our context - aware embeddings
summarization,5,33,results,proposed context - aware embeddings,vs,original aspect embeddings
summarization,5,33,results,original aspect embeddings,using,TSNE
summarization,5,34,results,more separation,between,different aspects
summarization,5,34,results,different aspects,using,our context - aware embeddings
summarization,5,34,results,common traits,of,speci c aspects
summarization,6,10,ablation-analysis,temporal information,useful for,emoji prediction
summarization,6,10,ablation-analysis,emoji prediction,even for,emojis
summarization,6,10,ablation-analysis,emojis,associated with,time
summarization,6,10,ablation-analysis,Ablation analysis,demonstrates,temporal information
summarization,6,16,ablation-analysis,top 10 emojis,associated to,each emoji
summarization,6,16,ablation-analysis,top 10 emojis,discovered that,emojis
summarization,6,16,ablation-analysis,each emoji,in,embedding space
summarization,6,16,ablation-analysis,emojis,related to,music
summarization,6,16,ablation-analysis,emojis,related to,animal
summarization,6,16,ablation-analysis,emojis,related to,sweets
summarization,6,16,ablation-analysis,emojis,related to,emotions
summarization,6,16,ablation-analysis,emojis,influenced by,seasonality
summarization,6,16,ablation-analysis,emotions,influenced by,seasonality
summarization,6,16,ablation-analysis,Ablation analysis,comparing,top 10 emojis
summarization,6,27,ablation-analysis,emojis,that had,higher gains
summarization,6,27,ablation-analysis,higher gains,in,F1 score
summarization,6,27,ablation-analysis,F1 score,has,without date
summarization,6,27,ablation-analysis,Ablation analysis,has,emojis
summarization,6,28,ablation-analysis,many emojis,are,"season-specific ( e.g. , ? , ? )"
summarization,6,28,ablation-analysis,many emojis,benefit from,date embeddings
summarization,6,28,ablation-analysis,Ablation analysis,observe,many emojis
summarization,6,29,ablation-analysis,emojis,not associated to,time
summarization,6,29,ablation-analysis,emojis,benefit from,temporal information
summarization,6,29,ablation-analysis,time,benefit from,temporal information
summarization,6,29,ablation-analysis,Ablation analysis,Even,emojis
summarization,6,29,ablation-analysis,Ablation analysis,has,emojis
summarization,6,30,ablation-analysis,More analysis,on,emoji semantics and usage
summarization,6,30,ablation-analysis,emoji semantics and usage,over,specific time of the day or week
summarization,6,30,ablation-analysis,Ablation analysis,on,emoji semantics and usage
summarization,6,30,ablation-analysis,Ablation analysis,has,More analysis
summarization,6,33,ablation-analysis,sport-related emojis,varied in,meaning
summarization,6,33,ablation-analysis,meaning,across,seasons
summarization,6,33,ablation-analysis,Ablation analysis,has,sport-related emojis
summarization,6,34,ablation-analysis,interesting emoji,related to,school
summarization,6,34,ablation-analysis,changed meaning,across,seasons
summarization,6,34,ablation-analysis,during Autumn,associated with,school-related emojis
summarization,6,34,ablation-analysis,interesting emoji,has,changed meaning
summarization,6,34,ablation-analysis,school,has,changed meaning
summarization,6,34,ablation-analysis,Ablation analysis,has,interesting emoji
summarization,6,8,experiments,leaf clover emoji,associated with,good luck wishes
summarization,6,8,experiments,leaf clover emoji,has,)
summarization,6,13,experiments,Twitter,to collect,100 million US tweets corpus
summarization,6,13,experiments,four subsets,by,seasons
summarization,6,13,experiments,four subsets,of,seasonal datasets
summarization,6,20,experiments,"900,000 tweets total",used for,emoji prediction
summarization,6,5,model,emoji prediction model,based on,time information
summarization,6,5,model,Model,develop,emoji prediction model
summarization,6,7,model,Temporal correlation,between,emojis and seasonal events
summarization,6,7,model,Temporal correlation,to disambiguate,emoji meanings
summarization,6,7,model,Model,has,Temporal correlation
summarization,6,21,model,architecture,of,emoji prediction model
summarization,6,21,model,data embeddings,combined through,early fusion approach
summarization,6,21,model,data embeddings,combined through,late fusion approach
summarization,6,21,model,Model,has,architecture
summarization,6,23,model,trained ( W/O ),completely ignores,date embeddings
summarization,6,23,model,Model,has,third model
summarization,6,37,model,multimodal architecture,to conduct,emoji prediction
summarization,6,37,model,emoji prediction,based on,deep neural networks
summarization,6,37,model,Model,has,multimodal architecture
summarization,6,17,results,meaning,across,seasons
summarization,6,25,results,Results,has,"Precision , Recall , and F1 scores"
summarization,6,26,results,time information,using,early fusion
summarization,6,26,results,Early model,outperforms,other models
summarization,6,26,results,time information,has,Early model
summarization,6,26,results,early fusion,has,Early model
summarization,6,35,results,top 10 associated emojis per season,for,pine emoji
summarization,6,35,results,Results,Check out,top 10 associated emojis per season
summarization,7,16,ablation-analysis,linear model,with,L1 regularization
summarization,7,16,ablation-analysis,Ablation analysis,training,linear model
summarization,7,17,ablation-analysis,"single "" sentiment neuron """,highly predictive of,sentiment value
summarization,7,17,ablation-analysis,Ablation analysis,realized there actually existed,"single "" sentiment neuron """
summarization,7,36,ablation-analysis,negative update,after,lost
summarization,7,36,ablation-analysis,negative update,after,larger update
summarization,7,36,ablation-analysis,larger update,at,sentence 's end
summarization,7,97,ablation-analysis,Ablation analysis,has,Lesson learned to avoid
summarization,7,101,ablation-analysis,strongly indicative words,like,""" best "" or "" horrendous """
summarization,7,101,ablation-analysis,""" best "" or "" horrendous """,cause,particularly big shifts
summarization,7,101,ablation-analysis,particularly big shifts,in,color
summarization,7,101,ablation-analysis,Ablation analysis,Note,strongly indicative words
summarization,7,11,experimental-setup,multiplicative LSTM,with,"4,096 units"
summarization,7,11,experimental-setup,multiplicative LSTM,to predict,next character
summarization,7,11,experimental-setup,"4,096 units",on,corpus
summarization,7,11,experimental-setup,corpus,of,82 million Amazon reviews
summarization,7,11,experimental-setup,next character,in,chunk of text
summarization,7,11,experimental-setup,Experimental setup,trained,multiplicative LSTM
summarization,7,12,experimental-setup,Training,took,one month
summarization,7,12,experimental-setup,one month,across,four NVIDIA Pascal GPUs
summarization,7,12,experimental-setup,our model,processing,"12,500 characters per second"
summarization,7,12,experimental-setup,Experimental setup,has,Training
summarization,7,24,experimental-setup,package,received,blank
summarization,7,24,experimental-setup,Experimental setup,has,package
summarization,7,55,experimental-setup,random samples,from,model
summarization,7,55,experimental-setup,random samples,after fixing,sentiment unit 's value
summarization,7,55,experimental-setup,sentiment unit 's value,to determine,sentiment
summarization,7,55,experimental-setup,sentiment,of,review
summarization,7,55,experimental-setup,Experimental setup,select,random samples
summarization,7,56,experimental-setup,prefix,through,model
summarization,7,56,experimental-setup,prefix,select,high - likelihood samples
summarization,7,56,experimental-setup,I could n't figure out,through,model
summarization,7,56,experimental-setup,Experimental setup,pass,prefix
summarization,7,74,experimental-setup,Scrap off,has,everytime
summarization,7,74,experimental-setup,Experimental setup,has,Scrap off
summarization,7,78,experimental-setup,warning,on,box
summarization,7,6,experiments,number of labeled examples,takes,two variants
summarization,7,6,experiments,two variants,of,our model
summarization,7,6,experiments,fully supervised approaches,trained with,"6,920 examples"
summarization,7,39,experiments,labels,for,important problems
summarization,7,39,experiments,important problems,where,reward
summarization,7,39,experiments,reward,is worth the effort,self-driving
summarization,7,54,experiments,synthetic text,generated by,trained model
summarization,7,57,experiments,Unsupervised Sentiment Neuron SENTIMENT,FIXED TO,POSITIVE SENTIMENT
summarization,7,57,experiments,POSITIVE SENTIMENT,FIXED TO,NEGATIVE
summarization,7,4,model,"distinct "" sentiment neuron """,contains,sentiment signal
summarization,7,4,model,"distinct "" sentiment neuron """,almost all of,sentiment signal
summarization,7,13,model,"4,096 units",regarded as,feature vector
summarization,7,13,model,feature vector,string read by,model
summarization,7,13,model,Model,has,"4,096 units"
summarization,7,14,model,mLSTM,turned,model
summarization,7,14,model,model,into,sentiment classifier
summarization,7,14,model,sentiment classifier,by taking,linear combination
summarization,7,14,model,linear combination,learning,weights
summarization,7,14,model,weights,of,combination
summarization,7,14,model,combination,via,available supervised data
summarization,7,14,model,linear combination,has,of these units
summarization,7,14,model,Model,training,mLSTM
summarization,7,20,model,direct dial,to control,sentiment
summarization,7,20,model,direct dial,overwrite,value
summarization,7,20,model,sentiment,of,resulting text
summarization,7,20,model,value,of,sentiment neuron
summarization,7,32,model,Stays,in,place
summarization,7,32,model,Stays,holds,shape
summarization,7,32,model,Model,has,Stays
summarization,7,34,model,sentiment neuron,adjusting,value
summarization,7,34,model,value,on,character - by- character basis
summarization,7,34,model,Model,has,sentiment neuron
summarization,7,48,model,hierarchical models,adapt,timescales
summarization,7,84,model,Model,through away,junk
summarization,7,95,model,sticks,at,end
summarization,7,95,model,Straight high,has,sticks
summarization,7,95,model,Model,has,Straight high
summarization,7,100,model,character - by- character value,of,sentiment neuron
summarization,7,100,model,character - by- character value,displaying,negative values
summarization,7,100,model,character - by- character value,displaying,positive values
summarization,7,100,model,negative values,as,red
summarization,7,100,model,positive values,as,green
summarization,7,3,results,linear model,achieves,state - of - the - art sentiment analysis accuracy
summarization,7,3,results,state - of - the - art sentiment analysis accuracy,on,small but extensively - studied dataset
summarization,7,3,results,performance,of,previous supervised systems
summarization,7,3,results,previous supervised systems,using,30 - 100x fewer labeled examples
summarization,7,3,results,Results,has,linear model
summarization,7,5,results,Our system,beats,other approaches
summarization,7,5,results,other approaches,on,Stanford Sentiment Treebank
summarization,7,5,results,Results,has,Our system
summarization,7,8,results,our model,learned,interpretable feature
summarization,7,8,results,our model,predicting,next character
summarization,7,8,results,next character,in,Amazon reviews
summarization,7,8,results,next character,resulted in,discovering
summarization,7,8,results,discovering,concept of,sentiment
summarization,7,9,results,general property,of,certain large neural networks
summarization,7,9,results,certain large neural networks,trained to predict,next step or dimension
summarization,7,9,results,next step or dimension,in,inputs
summarization,7,9,results,Results,believe,phenomenon
summarization,7,18,results,sentiment neuron,within,our model
summarization,7,18,results,sentiment neuron,classify,reviews
summarization,7,18,results,our model,classify,reviews
summarization,7,18,results,reviews,as,negative or positive
summarization,7,18,results,Results,has,sentiment neuron
summarization,7,21,results,SENTIMENT,FIXED TO,POSITIVE SENTIMENT
summarization,7,21,results,SENTIMENT,FIXED TO,NEGATIVE
summarization,7,21,results,POSITIVE SENTIMENT,FIXED TO,NEGATIVE
summarization,7,21,results,Results,has,SENTIMENT
summarization,7,22,results,exactly matched seam,to,color contrast
summarization,7,22,results,color contrast,with,other pants
summarization,7,22,results,Nice fitted pants,has,exactly matched seam
summarization,7,22,results,Results,has,Nice fitted pants
summarization,7,23,results,Results,has,Highly recommended
summarization,7,25,results,Results,has,waste of time and money
summarization,7,29,results,crib,without,some kind of embellishment
summarization,7,29,results,Results,Hard to put on,crib
summarization,7,30,results,Results,has,guess
summarization,7,33,results,Comfy,has,looks so cute
summarization,7,33,results,Results,has,Comfy
summarization,7,35,results,system,makes,large updates
summarization,7,35,results,large updates,after,completion
summarization,7,35,results,completion,of,sentences and phrases
summarization,7,35,results,Results,interesting to note,system
summarization,7,38,results,Collecting data,is,easy
summarization,7,38,results,Results,has,Collecting data
summarization,7,41,results,large unsupervised next-step-prediction models,on,large amounts of data
summarization,7,41,results,large unsupervised next-step-prediction models,may,good approach
summarization,7,41,results,Results,training,large unsupervised next-step-prediction models
summarization,7,43,results,promising step,towards,general unsupervised representation learning
summarization,7,43,results,Results,has,Our results
summarization,7,44,results,good quality representations,scaled up,existing model
summarization,7,44,results,existing model,on,carefullychosen dataset
summarization,7,44,results,Results,learn,good quality representations
summarization,7,44,results,Results,scaled up,existing model
summarization,7,46,results,not as strong,for,datasets
summarization,7,46,results,datasets,of,long documents
summarization,7,46,results,Results,has,results
summarization,7,47,results,struggles,to remember,information
summarization,7,47,results,information,over,hundreds to thousands of timesteps
summarization,7,47,results,our character - level model,has,struggles
summarization,7,47,results,Results,suspect,our character - level model
summarization,7,49,results,representation fidelity and performance,on,sentiment analysis and similar tasks
summarization,7,50,results,diverges,from,review data
summarization,7,50,results,input text,has,diverges
summarization,7,50,results,Results,has,model
summarization,7,51,results,broadening,results in,equally informative representation
summarization,7,51,results,corpus,of,text samples
summarization,7,51,results,equally informative representation,applies to,broader domains
summarization,7,51,results,broadening,has,corpus
summarization,7,51,results,Results,worth verifying,broadening
summarization,7,52,results,very large next-step-prediction models,learn,excellent unsupervised representations
summarization,7,52,results,Results,has,Our results
summarization,7,53,results,neural network,to predict,next frame
summarization,7,53,results,neural network,result in,unsupervised representations
summarization,7,53,results,Results,Training,neural network
summarization,7,58,results,her Doolittle newsletter,see,great product
summarization,7,59,results,Results,could n't ascertain,another new one coming out next year
summarization,7,62,results,Results,am,prolific stuff
summarization,7,63,results,contents - information consumer,of,company
summarization,7,63,results,company,has,all the time
summarization,7,65,results,weapons,has,look
summarization,7,66,results,beautiful,Fits,good
summarization,7,70,results,Results,suggest,annoying rear piece
summarization,7,72,results,any man,with,huge pull
summarization,7,72,results,huge pull,down,my back
summarization,7,72,results,huge pull,down,black
summarization,7,72,results,Results,must watch for,any man
summarization,7,76,results,SENTIMENT,FIXED TO,POSITIVE SENTIMENT
summarization,7,76,results,SENTIMENT,FIXED TO,NEGATIVE
summarization,7,76,results,POSITIVE SENTIMENT,FIXED TO,NEGATIVE
summarization,7,76,results,Results,has,SENTIMENT
summarization,7,79,results,So glad,to have,found it again
summarization,7,79,results,Results,n't,So glad
summarization,7,82,results,Results,has,book
summarization,7,83,results,Results,Might,fantastic book
summarization,7,85,results,stop,has,drivel
summarization,7,87,results,Results,use it,all the time
summarization,7,88,results,Results,has,Good worst
summarization,7,90,results,Results,skim-read,entire book
summarization,7,91,results,Results,Do n't waste,your time
summarization,7,96,results,Results,On par with,other buds
summarization,7,103,results,shape,at,first
summarization,7,103,results,Results,could n't figure out,shape
summarization,7,107,results,Results,given it,zero stars
summarization,8,10,ablation-analysis,contradiction,between,objective polarity ( usually negative )
summarization,8,10,ablation-analysis,contradiction,between,sarcastic characteristics
summarization,8,10,ablation-analysis,sarcastic characteristics,conveyed by,author
summarization,8,10,ablation-analysis,Ablation analysis,detection of,contradiction
summarization,8,15,ablation-analysis,Generalizability capabilities,of,models
summarization,8,15,ablation-analysis,differed in nature,significantly impacted,results
summarization,8,15,ablation-analysis,datasets,has,differed in nature
summarization,8,15,ablation-analysis,Ablation analysis,has,Generalizability capabilities
summarization,8,17,ablation-analysis,training,done on,Dataset 1
summarization,8,17,ablation-analysis,training,tested on,Dataset 2
summarization,8,17,ablation-analysis,F1 - score,of,model
summarization,8,17,ablation-analysis,model,was,33.05 %
summarization,8,17,ablation-analysis,Ablation analysis,has,training
summarization,8,18,ablation-analysis,sarcasm,from,text
summarization,8,18,ablation-analysis,sentiment and other contextual clues,detect,sarcasm
summarization,8,18,ablation-analysis,sarcasm,from,text
summarization,8,25,ablation-analysis,sarcastic expressions,are,user-specific
summarization,8,25,ablation-analysis,post more sarcasm,than,others
summarization,8,25,ablation-analysis,user-specific,has,some users
summarization,8,25,ablation-analysis,some users,has,post more sarcasm
summarization,8,25,ablation-analysis,Ablation analysis,observe,sarcastic expressions
summarization,8,38,baselines,CNN - extracted features,fed to,SVM classifier ( CNN - SVM )
summarization,8,39,baselines,separate baseline classifier,consisting of,only the CNN model
summarization,8,39,baselines,only the CNN model,incorporation of,"other models ( e.g. , emotion and sentiment )"
summarization,8,39,baselines,Baselines,has,separate baseline classifier
summarization,8,6,experiments,statements,such as,Is it time for your medication or mine ?
summarization,8,6,experiments,statements,such as,I work 40 hours a week
summarization,8,12,experiments,I love the pain,provides,knowledge
summarization,8,12,experiments,knowledge,sentiment expressed by,author
summarization,8,12,experiments,breakup,describes,contradicting sentiment
summarization,8,21,hyperparameters,word embeddings,used as,input features
summarization,8,21,hyperparameters,Hyperparameters,has,word embeddings
summarization,8,29,hyperparameters,Sentences,represented using,word vectors ( embeddings )
summarization,8,29,hyperparameters,Sentences,provided as,input
summarization,8,29,hyperparameters,Hyperparameters,has,Sentences
summarization,8,30,hyperparameters,Google 's word2vec vectors,employed as,input
summarization,8,30,hyperparameters,Hyperparameters,has,Google 's word2vec vectors
summarization,8,35,hyperparameters,features,from,sarcasm datasets
summarization,8,35,hyperparameters,other features,has,CNN models
summarization,8,35,hyperparameters,sentiment ( S ),has,CNN models
summarization,8,35,hyperparameters,personality ( P ),has,CNN models
summarization,8,35,hyperparameters,Hyperparameters,To obtain,other features
summarization,8,36,hyperparameters,Different training datasets,to train,each model
summarization,8,36,hyperparameters,Hyperparameters,has,Different training datasets
summarization,8,41,hyperparameters,NLTK Twitter Tokenizer,used for,tokenization
summarization,8,41,hyperparameters,Hyperparameters,has,"Usernames , URLs , and hashtags"
summarization,8,41,hyperparameters,Hyperparameters,has,NLTK Twitter Tokenizer
summarization,8,14,model,CNNs,to automatically learn,features
summarization,8,14,model,features,from,sarcasm dataset
summarization,8,14,model,Model,avoid,automatic feature extraction
summarization,8,14,model,Model,rely on,CNNs
summarization,8,19,model,"Pre-trained sentiment , emotion , and personality models",to capture,contextualized information
summarization,8,19,model,contextualized information,from,text
summarization,8,19,model,Model,has,"Pre-trained sentiment , emotion , and personality models"
summarization,8,24,model,local features,in,lower layers
summarization,8,24,model,local features,converted into,global features
summarization,8,24,model,local features,in,higher layers
summarization,8,24,model,global features,in,higher layers
summarization,8,24,model,Model,learns,local features
summarization,8,26,model,personality - based features,incorporated into,sarcasm detection framework
summarization,8,26,model,emotion - based features,incorporated into,sarcasm detection framework
summarization,8,27,model,pre-trained models,to extract,sarcasm-related features
summarization,8,27,model,sarcasm-related features,from,dataset
summarization,8,28,model,CNN Framework CNNs,effective at modeling,hierarchy
summarization,8,28,model,hierarchy,of,local features
summarization,8,28,model,Model,has,CNN Framework CNNs
summarization,8,31,model,parameters,for,word vectors
summarization,8,31,model,word vectors,learned during,training phase
summarization,8,31,model,Model,has,Non-static representations
summarization,8,32,model,Max pooling,applied to,feature maps
summarization,8,32,model,feature maps,to generate,features
summarization,8,32,model,Model,has,Max pooling
summarization,8,33,model,fully connected layer,followed by,softmax layer
summarization,8,33,model,softmax layer,for outputting,final prediction
summarization,8,33,model,Model,has,fully connected layer
summarization,8,20,results,"Hand-crafted features ( e.g. , n-grams )",somewhat useful for,sarcasm detection
summarization,8,20,results,"Hand-crafted features ( e.g. , n-grams )",will,very sparse feature vector representations
summarization,8,20,results,"Hand-crafted features ( e.g. , n-grams )",produce,very sparse feature vector representations
summarization,8,20,results,Results,has,"Hand-crafted features ( e.g. , n-grams )"
summarization,8,43,results,performances,of,CNN and CNN - SVM classifier
summarization,8,43,results,CNN and CNN - SVM classifier,applied to,all datasets
summarization,8,43,results,Results,has,performances
summarization,8,44,results,models ( specifically CNN - SVM ),combine,sarcasm features
summarization,8,44,results,models ( specifically CNN - SVM ),combine,emotion features
summarization,8,44,results,models ( specifically CNN - SVM ),combine,sentiment features
summarization,8,44,results,models ( specifically CNN - SVM ),combine,personality traits features
summarization,8,44,results,all the other models,with the exception of,baseline model
summarization,8,44,results,outperforms,has,all the other models
summarization,8,45,results,Results,of,state - of - the - art model
summarization,8,46,results,proposed model,has,consistently outperforms
summarization,8,46,results,consistently outperforms,has,all the other models
summarization,8,46,results,Results,has,proposed model
summarization,9,17,ablation-analysis,nearby utterances,engages,audience
summarization,9,17,ablation-analysis,Ablation analysis,included,nearby utterances
summarization,9,22,baselines,two frameworks,for,fusing modalities
summarization,9,22,baselines,unimodal features,concatenated and fed into,various contextual LSTM networks
summarization,9,22,baselines,Non-hierarchical Framework,has,unimodal features
summarization,9,39,baselines,F1,has,Context-Independent Features
summarization,9,50,baselines,Different variants,of,LSTM model
summarization,9,50,baselines,unimodal features,used directly with,SVM
summarization,9,50,baselines,SVM,for,classification
summarization,9,50,baselines,sc-LSTM,has,unidirectional LSTM cells
summarization,9,50,baselines,uni-SVM,has,unimodal features
summarization,9,50,baselines,Baselines,has,Different variants
summarization,9,41,experimental-setup,Audio feature extraction,performed using,openSMILE open-source software
summarization,9,41,experimental-setup,openSMILE open-source software,where,low-level features
summarization,9,41,experimental-setup,low-level features,such as,voice intensity and pitch
summarization,9,57,experimental-setup,train / test splits,are,disjoint
summarization,9,57,experimental-setup,disjoint,with respect to,speakers
summarization,9,57,experimental-setup,Experimental setup,has,train / test splits
summarization,9,30,experiments,MOUD dataset,involved,some translation
summarization,9,6,model,long short - term memory ( LSTM ) model,enables,utterances ( units of speech bound by breathes or pauses )
summarization,9,6,model,utterances ( units of speech bound by breathes or pauses ),to capture,contextual information
summarization,9,6,model,Model,based on,long short - term memory ( LSTM ) model
summarization,9,9,model,Multimodal information,By combining,vocal modulations and facial expressions
summarization,9,9,model,vocal modulations and facial expressions,with,textual information
summarization,9,9,model,vocal modulations and facial expressions,enrich,feature learning process
summarization,9,9,model,feature learning process,to better understand,affective states
summarization,9,9,model,affective states,of,opinion holders
summarization,9,9,model,Model,Why,Multimodal information
summarization,9,10,model,other behavioral cues,in,vocal and visual modalities
summarization,9,10,model,Model,could be,other behavioral cues
summarization,9,12,model,"order , inter-dependencies , and relations",among,utterances
summarization,9,12,model,utterances,in,video
summarization,9,13,model,surrounding context,help to,better classify
summarization,9,13,model,better classify,sentiment conveyed by,utterances
summarization,9,13,model,Model,has,surrounding context
summarization,9,14,model,"audio , visual , and textual information",to tackle,sentiment and emotion recognition tasks
summarization,9,14,model,combined,to tackle,sentiment and emotion recognition tasks
summarization,9,14,model,Model,has,"audio , visual , and textual information"
summarization,9,24,model,unimodal features,feed,each unimodal feature
summarization,9,24,model,each unimodal feature,has,LSTM network
summarization,9,25,model,framework,as having,some hierarchy
summarization,9,25,model,Model,Think of,framework
summarization,9,34,model,visual modality,caries,more generalized information
summarization,9,34,model,Model,has,visual modality
summarization,9,38,model,few ideas,try to improve,current work
summarization,9,40,model,Textual feature extraction,performed using,convolutional neural network ( CNN )
summarization,9,40,model,convolutional neural network ( CNN ),where,input
summarization,9,40,model,transcription of each utterance,represented by,concatenation of corresponding word2vec word vectors
summarization,9,40,model,Model,has,Textual feature extraction
summarization,9,43,model,Visual feature extraction,performed using,3D - CNN
summarization,9,43,model,3D - CNN,where,frame- level features
summarization,9,43,model,Model,has,Visual feature extraction
summarization,9,45,model,LSTM - based network,to perform,context-dependent feature extraction
summarization,9,45,model,context-dependent feature extraction,by modeling,relations
summarization,9,45,model,relations,among,utterances
summarization,9,45,model,Model,has,LSTM - based network
summarization,9,46,model,unimodal features,fed as,input
summarization,9,46,model,input,to,LSTM layer
summarization,9,46,model,LSTM layer,produces,contextualized features
summarization,9,46,model,Model,has,unimodal features
summarization,9,52,model,output,of,first level
summarization,9,52,model,first level,concatenated and fed into,"another LSTM network ( i.e. , second level )"
summarization,9,52,model,Model,has,output
summarization,9,66,model,more advanced idea,fusion part of,framework
summarization,9,16,results,Results,Without,any context
summarization,9,19,results,highly subjective,train,machine
summarization,9,19,results,machine,to detect,correlations
summarization,9,19,results,correlations,has,automatically
summarization,9,26,results,sc - LSTM and bc - LSTM models,perform,best
summarization,9,26,results,best,out of,LSTM variants
summarization,9,26,results,best,including,uni-SVM model
summarization,9,26,results,first level,has,unimodal features LSTM variants
summarization,9,26,results,Results,In,first level
summarization,9,27,results,contextual information,when classifying,utterances
summarization,9,27,results,Results,importance of considering,contextual information
summarization,9,29,results,unimodal classifiers,trained on,textual information
summarization,9,29,results,unimodal classifiers,perform,best
summarization,9,29,results,best,compared to,other individual modalities
summarization,9,29,results,Results,has,unimodal classifiers
summarization,9,31,results,modalities,tend to,boost
summarization,9,31,results,boost,has,performance
summarization,9,31,results,Results,combining,modalities
summarization,9,35,results,fusing,improved,model
summarization,9,35,results,modalities,improved,model
summarization,9,35,results,fusing,has,modalities
summarization,9,35,results,Results,has,fusing
summarization,9,58,results,MOSI,contains,video- based topic reviews
summarization,9,58,results,video- based topic reviews,annotated by,sentiment polarity
summarization,9,58,results,video- based topic reviews,annotated by,sentiment polarity
summarization,9,58,results,product review videos,annotated by,sentiment polarity
summarization,9,58,results,scripted affect-related utterances,annotated by,emotion categories
summarization,9,59,results,hierarchical model,has,significantly outperform
summarization,9,59,results,significantly outperform,has,non-hierarchical frameworks
summarization,9,59,results,Results,observe,hierarchical model
summarization,9,63,results,own datasets,label them,automatically
summarization,9,63,results,own datasets,rendering,large-scale datasets
summarization,9,64,results,Results,keep in mind,domain
summarization,9,65,results,more cases,where,contextualized information
summarization,9,65,results,contextualized information,helps with,sentiment classification
summarization,9,65,results,Results,interesting to see,more cases
summarization,10,36,ablation-analysis,RNTN,could capture,effect
summarization,10,36,ablation-analysis,effect,of,negative words
summarization,10,36,ablation-analysis,negative words,in,positive and negative sentiment sentences
summarization,10,36,ablation-analysis,Ablation analysis,suggests,RNTN
summarization,10,6,baselines,"215,154 phrases",with,fine- grained sentiment labels
summarization,10,6,baselines,fine- grained sentiment labels,has,5 classes )
summarization,10,20,baselines,Baselines,has,RNN : Recursive Neural Network
summarization,10,23,baselines,MV - RNN,has,Matrix-Vector RNN
summarization,10,23,baselines,Baselines,has,MV - RNN
summarization,10,28,baselines,Baselines,has,RNTN
summarization,10,9,experiments,Sentiment Sentiment Treebank Corpus,of,"11,855 sentences"
summarization,10,9,experiments,"11,855 sentences",with,fully labelled parse trees
summarization,10,11,experiments,label,using,crowdsourcing
summarization,10,11,experiments,crowdsourcing,on,Amazon Mechanical Turk
summarization,10,11,experiments,movie reviews dataset,has,normalise
summarization,10,17,hyperparameters,initialized randomly,from,uniform distribution
summarization,10,17,hyperparameters,Hyperparameters,has,word vectors
summarization,10,18,hyperparameters,classification task,use,compositions word vectors
summarization,10,18,hyperparameters,compositions word vectors,as input for,softmax
summarization,10,18,hyperparameters,Hyperparameters,For,classification task
summarization,10,21,hyperparameters,weight matrix,to be,learnt
summarization,10,25,hyperparameters,Matrix,for,each word
summarization,10,25,hyperparameters,each word,initialized as,identity matrix
summarization,10,25,hyperparameters,identity matrix,plus,small Gaussian noise
summarization,10,25,hyperparameters,Hyperparameters,has,Matrix
summarization,10,26,hyperparameters,Hyperparameters,For,parse tree
summarization,10,27,hyperparameters,Number of parameters,depend on,size of vocabulary
summarization,10,27,hyperparameters,Number of parameters,could be,very large
summarization,10,27,hyperparameters,Hyperparameters,Con,Number of parameters
summarization,10,27,hyperparameters,Hyperparameters,has,Number of parameters
summarization,10,31,hyperparameters,optimal word vector size,between,25 and 35
summarization,10,31,hyperparameters,Hyperparameters,has,optimal word vector size
summarization,10,7,model,Recursive Neural Tensor Network - Model,to learn,fine- grained sentiment labels
summarization,10,7,model,Model,has,Recursive Neural Tensor Network - Model
summarization,10,15,model,given n-gram,into,binary tree
summarization,10,15,model,each word,using,d-dimensional vector
summarization,10,15,model,Model,Parse,given n-gram
summarization,10,16,model,Recursive Neural Models,Compute,parent vectors
summarization,10,16,model,parent vectors,using,bottom - up approach
summarization,10,16,model,parent vectors,using,different composition functions
summarization,10,16,model,Model,has,Recursive Neural Models
summarization,10,22,model,Model,has,Con Input vectors
summarization,10,24,model,every word and phrase,as,vector and a matrix
summarization,10,24,model,Model,Represent,every word and phrase
summarization,10,29,model,V,is,tensor
summarization,10,29,model,tensor,defines,multiple bilinear forms
summarization,10,29,model,Model,has,V
summarization,10,33,model,Each slice,of,tensor V
summarization,10,33,model,Model,has,Each slice
summarization,10,13,results,shorter phrases,are,neutral
summarization,10,13,results,longer phrases,have,stronger sentiments
summarization,10,14,results,5 - class classification,to capture,variability
summarization,10,14,results,variability,has,of sentiments
summarization,10,14,results,Results,has,5 - class classification
summarization,10,32,results,affected,by,word vectors
summarization,10,32,results,affected,using,word vectors
summarization,10,32,results,word vectors,from,say Glove
summarization,10,32,results,results,has,affected
summarization,10,32,results,Results,interesting to see how are,results
summarization,10,32,results,Results,interesting to see how are,affected
summarization,10,35,results,Full Sentence Binary Sentiment RNTN,pushes,state of the art
summarization,10,35,results,state of the art,on,short phrases
summarization,10,35,results,short phrases,to,85.4 %
summarization,10,35,results,outperforms,has,other models
summarization,10,35,results,Results,has,Full Sentence Binary Sentiment RNTN
summarization,11,16,ablation-analysis,table,showcase,which word
summarization,11,16,ablation-analysis,which word,contributes,most
summarization,11,16,ablation-analysis,most,to,aspect sentiment polarity
summarization,11,16,ablation-analysis,aspect sentiment polarity,by visualising,sentence attention vectors
summarization,11,18,ablation-analysis,model,n't handle,ef ciently
summarization,11,18,ablation-analysis,Ablation analysis,In,error analysis
summarization,11,36,baselines,column - wise softmax,to get,target- to-sentence attention
summarization,11,37,baselines,row-wise softmax,to get,sentence - to- target attention
summarization,11,10,experiments,sentiment class,with,highest probability
summarization,11,10,experiments,highest probability,is,predicted label
summarization,11,10,experiments,predicted label,for,sentence
summarization,11,10,experiments,sentence,given,aspect target
summarization,11,23,experiments,two domain-speci c datasets,from,SemEval 2014 Task 4
summarization,11,5,model,AOA module,jointly learns,representations
summarization,11,5,model,AOA module,explicitly captures,interaction
summarization,11,5,model,representations,for,aspects and sentences
summarization,11,5,model,interaction,between,aspects and context sentences
summarization,11,5,model,Model,has,AOA module
summarization,11,8,model,nal sentence representation,weighted sum of,sentence hidden states
summarization,11,8,model,sentence hidden states,using,sentence attention
summarization,11,8,model,sentence attention,from,AOA module
summarization,11,8,model,Model,has,nal sentence representation
summarization,11,9,model,nal sentence representation,feed into,linear layer
summarization,11,9,model,linear layer,with,softmax function
summarization,11,9,model,softmax function,to output,probabilities
summarization,11,9,model,probabilities,of,sentiment classes
summarization,11,9,model,Model,has,nal sentence representation
summarization,11,21,model,prior language knowledge,to,AOA neural network
summarization,11,21,model,Model,incorporate,sentences ' grammar structures
summarization,11,27,model,4 main components,in,architecture
summarization,11,29,model,Word embedding,is,standardised step
summarization,11,29,model,standardised step,convert,text sentence and aspect target
summarization,11,29,model,text sentence and aspect target,into,numerical representations
summarization,11,29,model,Model,has,Word embedding
summarization,11,31,model,word vectors,feed them into,two bi-LSTM
summarization,11,31,model,two bi-LSTM,to learn,hidden semantics
summarization,11,31,model,hidden semantics,of,words
summarization,11,31,model,words,in,sentence and aspect target
summarization,11,31,model,Model,get,word vectors
summarization,11,33,model,attention weights,for,text
summarization,11,33,model,text,using,AOA module
summarization,11,33,model,Model,calculate,attention weights
summarization,11,35,model,pair-wise interaction matrix,between,two hidden states
summarization,11,35,model,correlation,of,word pair
summarization,11,35,model,Model,Calculate,pair-wise interaction matrix
summarization,11,38,model,column- wise average,to get,target - level attention
summarization,11,38,model,target - level attention,tells us,important parts
summarization,11,38,model,important parts,in,aspect target
summarization,11,38,model,Model,Calculate,column- wise average
summarization,11,39,model,nal sentence - level attention,weighted sum of,each individual target - tosentence attention
summarization,11,39,model,Model,has,nal sentence - level attention
summarization,11,6,results,laptop and restaurant datasets,has,outperforms
summarization,11,6,results,outperforms,has,previous LSTM - based architectures
summarization,11,6,results,Results,on,laptop and restaurant datasets
summarization,11,15,results,AOA - LSTM,performed,best
summarization,11,15,results,best,in comparisons to,other baseline methods
summarization,11,15,results,Results,has,AOA - LSTM
summarization,11,24,results,Accuracy,is,evaluation metric
summarization,11,24,results,Results,has,Accuracy
summarization,12,4,baselines,skip-gram,in,word embeddings
summarization,12,4,baselines,skip-though,in,sentence embeddings
summarization,12,7,baselines,textual entailment data,to train,sentence embeddings layer
summarization,12,7,baselines,sentence embeddings layer,calls,InferSent
summarization,12,21,baselines,best approach,is,Bi-directional LSTM
summarization,12,21,baselines,Bi-directional LSTM,with,max polling
summarization,12,23,baselines,embeddings,by,your self
summarization,12,32,baselines,InferSent,uses,supervised learning
summarization,12,32,baselines,supervised learning,to compute,word vectors
summarization,12,33,baselines,InferSent,leverages,word embeddings ( GloVe / fastText )
summarization,12,33,baselines,word embeddings ( GloVe / fastText ),to build,sentence embeddings
summarization,12,33,baselines,Baselines,has,InferSent
summarization,12,36,baselines,7 different architectures,of,last hidden states
summarization,12,36,baselines,last hidden states,of,forward and backward GRU
summarization,12,37,baselines,Baselines,has,Bi-directional LSTM with mean polling
summarization,12,38,baselines,Bi-directional LSTM,with,max polling
summarization,12,38,baselines,Self-attentive Network,has,Attention with BiLSTM
summarization,12,38,baselines,Baselines,has,Bi-directional LSTM
summarization,12,43,baselines,building InferSent,by,your self
summarization,12,47,baselines,Build Vocab,for,"InferSent model model.build_vocab(sentences , tokenize =True )"
summarization,12,47,baselines,5/6,has,Build Vocab
summarization,12,47,baselines,Baselines,has,5/6
summarization,12,51,baselines,Baselines,has,Downloading InferSent pre-trained model
summarization,12,52,baselines,Version 1,trained by,GloVe
summarization,12,52,baselines,Version 2,has,leveraged fastText
summarization,12,52,baselines,Baselines,has,Version 1
summarization,12,29,experimental-setup,following shell script,in,current folder
summarization,12,29,experimental-setup,Experimental setup,execute,following shell script
summarization,12,44,experimental-setup,Load pre-trained Embeddings,provide,2 pre-trained models
summarization,12,44,experimental-setup,version 1,based on,GloVe
summarization,12,44,experimental-setup,version 2,based on,fastText
summarization,12,44,experimental-setup,Experimental setup,has,Load pre-trained Embeddings
summarization,12,48,experimental-setup,Pretrained model,supports,GloVe
summarization,12,48,experimental-setup,Pretrained model,supports,fasttext
summarization,12,48,experimental-setup,Experimental setup,has,Pretrained model
summarization,12,59,experimental-setup,command,to train,embeddings layers.python train_nli.py
summarization,12,59,experimental-setup,Experimental setup,execute,command
summarization,12,9,experiments,two apples and walnuts,on,white towel
summarization,12,12,experiments,relationship,is,entailment
summarization,12,19,experiments,BiLSTM,with,mean polling
summarization,12,19,experiments,mean polling,perform,not very good
summarization,12,25,experiments,supervised learning approach,to generate,sentence embeddings
summarization,12,25,experiments,sentence embeddings,to have,annotated ( labeled ) data
summarization,12,28,experiments,get_data.bash,in,console
summarization,12,28,experiments,console,such that,SNLI ( Stanford Natural Language Inference )
summarization,12,28,experiments,MultiNLI ) MultiGenre NLI ) corpus,downloaded and,processed
summarization,12,6,model,Some features,transferred to,downstream
summarization,12,6,model,Model,has,Some features
summarization,12,15,model,two sentences ( premise input and hypothesis input ),transformed,sentence encoder
summarization,12,15,model,sentence encoder,has,same weights )
summarization,12,16,model,3 matching methods,to recognize,relations
summarization,12,16,model,relations,between,premise input and hypothesis input
summarization,12,16,model,Model,leveraging,3 matching methods
summarization,12,27,model,Clone InferSent original repo,to,local
summarization,12,27,model,Model,has,Clone InferSent original repo
summarization,12,42,model,pre-trained embeddings layer,in,your NLP problems
summarization,12,17,results,best approach,believe that,Attention
summarization,12,17,results,Attention,with,BiLSTM
summarization,12,17,results,Attention,should be,best approach
summarization,12,18,results,may harm,when using it in,transfer learning
summarization,12,45,results,InferSent pre-trained model and GloVe ( or fastText ) model,encode,sentence
summarization,12,45,results,sentence,to,vectors
summarization,12,45,results,Results,Loading,InferSent pre-trained model and GloVe ( or fastText ) model
summarization,13,18,ablation-analysis,emotion label,of,happy
summarization,13,50,ablation-analysis,Jekyll,using,Minimal Mistakes theme
summarization,13,50,ablation-analysis,Ablation analysis,Powered by,Jekyll
summarization,13,49,baselines,Baselines,has,2022 DAIR
summarization,13,11,experiments,fourth clause,considered,emotion clause
summarization,13,11,experiments,emotion clause,conveying,emotion of happiness
summarization,13,12,experiments,clauses,that contain,causes ( cause clause )
summarization,13,12,experiments,causes ( cause clause ),are,second clause and the third clause
summarization,13,17,experiments,pairs,of,emotion - cause clauses
summarization,13,31,experiments,benchmark emotion - cause dataset,by,"Gui et al. , 2016"
summarization,13,29,hyperparameters,pairs,that,output 1
summarization,13,29,hyperparameters,pairs,used for,evaluation
summarization,13,29,hyperparameters,output 1,final set of,emotion - cause pairs
summarization,13,29,hyperparameters,Hyperparameters,has,pairs
summarization,13,4,model,deep learning method,to extract,causes behind
summarization,13,4,model,Model,develop,deep learning method
summarization,13,5,model,deep learning method,to extract,causes behind emotions in a document
summarization,13,5,model,Model,develop,deep learning method
summarization,13,9,model,emotion and causes,from,text
summarization,13,9,model,emotion and causes,using,multi-task learning
summarization,13,9,model,emotion and causes,conducts,emotion - cause pairing and filtering
summarization,13,9,model,emotion - cause pairing and filtering,using,improved version of the multi-task learning model
summarization,13,9,model,Model,separately extract,emotion and causes
summarization,13,22,model,two parts,conduct,emotion - cause pairing and filtering
summarization,13,22,model,extract sets,of,emotion and cause clauses
summarization,13,22,model,emotion and cause clauses,from,each document
summarization,13,22,model,each document,via,two kinds of multi-task learning networks
summarization,13,23,model,pairing,done via,Cartesian product
summarization,13,23,model,Cartesian product,applied to,set of emotion and cause clauses
summarization,13,23,model,Model,has,pairing
summarization,13,24,model,candidate emotion - cause pairs,on which,filter
summarization,13,24,model,filter,applied to remove,clause pairs
summarization,13,24,model,clause pairs,do n't contain,causal relationship
summarization,13,25,model,Model,has,output
summarization,13,26,model,objective,pair,two sets
summarization,13,26,model,two sets,to form,emotion - cause pairs
summarization,13,26,model,emotion - cause pairs,that form,causal relationships
summarization,13,26,model,Model,pair,two sets
summarization,13,26,model,Model,has,objective
summarization,13,27,model,Cartesian product,renders,set of all possible pairs
summarization,13,27,model,set of all possible pairs,represented by,feature vector
summarization,13,27,model,feature vector,consisting of,three components
summarization,13,27,model,distance,between,two clauses
summarization,13,27,model,Model,has,Cartesian product
summarization,13,28,model,cause relationship,determined via,logistic regression model
summarization,13,28,model,cause relationship,outputs,1 or 0
summarization,13,28,model,logistic regression model,takes as input,emotion - cause candidate pair features
summarization,13,28,model,emotion - cause candidate pair features,applied,Sigmoid operation
summarization,13,28,model,1 or 0,to indicate,relationship
summarization,13,28,model,relationship,exists for,each pair
summarization,13,28,model,Model,has,cause relationship
summarization,13,41,model,prediction,for,each type of clause
summarization,13,41,model,prediction,performed via,Bi-LSTM hidden representations
summarization,13,41,model,Bi-LSTM hidden representations,consist of,context - aware representations
summarization,13,41,model,context - aware representations,of,each clause
summarization,13,41,model,Model,shows how,prediction
summarization,13,42,model,assumption,is,emotion and cause clauses
summarization,13,42,model,emotion and cause clauses,are,mutually independent
summarization,13,42,model,emotion and cause clauses,not,mutually independent
summarization,13,42,model,interactive multi-task learning network,to capture,correlation
summarization,13,42,model,Model,propose,interactive multi-task learning network
summarization,13,42,model,Model,has,assumption
summarization,13,44,model,improved model,performs,few extra steps
summarization,13,44,model,few extra steps,in,upper layers
summarization,13,44,model,few extra steps,allows,prediction
summarization,13,44,model,prediction,based on,interaction
summarization,13,44,model,interaction,of,two types of representations
summarization,13,45,model,enhancing emotion extraction,based on,cause extraction
summarization,13,45,model,enhancing cause extraction,based on,emotion extraction
summarization,13,45,model,cause extraction,has,Inter CE )
summarization,13,45,model,emotion extraction,has,Inter EC )
summarization,13,8,results,feasibility,using,multi-task learning approach
summarization,13,8,results,Results,show,feasibility
summarization,13,19,results,Results,has,identifying causes
summarization,13,32,results,"Precision , recall , and F1 scores",used as,evaluation metrics
summarization,13,32,results,Results,has,"Precision , recall , and F1 scores"
summarization,13,33,results,all the separate tasks,using,three proposed models
summarization,13,33,results,three proposed models,based on,multi-task learning
summarization,13,33,results,Results,for,all the separate tasks
summarization,13,34,results,Inter -EC,produced,better results
summarization,13,34,results,Results,has,Inter -EC
summarization,13,35,results,each other,in,individual tasks
summarization,13,35,results,cause extraction,has,improve
summarization,13,35,results,improve,has,each other
summarization,13,35,results,Results,observe,emotion extraction
summarization,13,36,results,results,consistent with,intuition
summarization,13,36,results,intuition,that,emotion and cause
summarization,13,36,results,emotion and cause,are,mutually indicative
summarization,13,36,results,Results,consistent with,intuition
summarization,13,36,results,Results,has,results
summarization,13,37,results,Other variations,of,models
summarization,13,37,results,models,leverage,dataset available annotations
summarization,13,37,results,improvements,use,additional information
summarization,13,37,results,additional information,to improve,predictions
summarization,13,37,results,predictions,of,different tasks
summarization,13,37,results,Results,has,Other variations
summarization,14,11,baselines,4 methods,to construct,auxiliary sentences
summarization,14,11,baselines,auxiliary sentences,to convert,TABSA
summarization,14,7,experiments,Each sentence,list of,target -aspect pairs
summarization,14,7,experiments,target -aspect pairs,with,sentiment polarity
summarization,14,8,experiments,ABSA,not,TABSA
summarization,14,13,experiments,binary classi cation problem,of,yes or no
summarization,14,13,experiments,binary classi cation problem,to obtain,probability distribution
summarization,14,13,experiments,Each target - aspect pair,generate,three sequences
summarization,14,13,experiments,three sequences,such as,polarity
summarization,14,13,experiments,polarity,is,positive
summarization,14,13,experiments,aspect safety,of,location - 1
summarization,14,13,experiments,aspect safety,of,location - 1
summarization,14,13,experiments,aspect safety,of,safety of location - 1
summarization,14,13,experiments,negative,use,probability value
summarization,14,13,experiments,none,use,probability value
summarization,14,13,experiments,none,take,class of sequence
summarization,14,13,experiments,probability value,of,yes
summarization,14,13,experiments,yes,as,matching score
summarization,14,13,experiments,class of sequence,with,highest matching score
summarization,14,13,experiments,highest matching score,for,predicted
summarization,14,13,experiments,probability distribution,has,Each target - aspect pair
summarization,14,9,model,Model,to tackle,subtask 3 ( Aspect Detection )
summarization,14,9,model,Model,to tackle,subtask 4 ( Aspect Polarity )
summarization,14,15,results,evaluation,of,SentiHood
summarization,14,15,results,SentiHood,consider,4 most frequently seen aspects
summarization,14,15,results,4 most frequently seen aspects,in,dataset
summarization,14,15,results,Results,has,evaluation
summarization,15,14,ablation-analysis,writing TLDR,based on,reviewer comments
summarization,15,16,ablation-analysis,3/10 knowledge,to follow,general research areas
summarization,15,16,ablation-analysis,Ablation analysis,has,3/10 knowledge
summarization,15,21,ablation-analysis,SCITLDR,is,much smaller dataset
summarization,15,21,ablation-analysis,K papers,due to,manual data collection and annotations
summarization,15,21,ablation-analysis,Ablation analysis,has,DATASET ANALYSIS
summarization,15,32,ablation-analysis,BART model,to generate,TLDR
summarization,15,32,ablation-analysis,Ablation analysis,netuned,BART model
summarization,15,41,ablation-analysis,limitation,on,input length
summarization,15,41,ablation-analysis,BART,has,limitation
summarization,15,23,experiments,average document length,is,5009
summarization,15,23,experiments,average document length,compressed into,average summary length
summarization,15,23,experiments,5009,compressed into,average summary length
summarization,15,23,experiments,average summary length,of,19
summarization,15,26,experiments,SCITLDR,investigate,ROUGE score difference
summarization,15,26,experiments,at least two ground - truth TLDRs,for,each paper
summarization,15,26,experiments,at least two ground - truth TLDRs,in,test set
summarization,15,26,experiments,each paper,in,test set
summarization,15,26,experiments,ROUGE score difference,between,different ground - truth TLDRs
summarization,15,26,experiments,SCITLDR,has,at least two ground - truth TLDRs
summarization,15,29,experiments,ROUGE -1,of,24.7
summarization,15,35,experiments,small dataset,for training,neural networks
summarization,15,36,experiments,additional 20 K paper-title pairs,from,arXiv
summarization,15,36,experiments,additional 20 K paper-title pairs,up sampling,our SCITLDR
summarization,15,39,experiments,BART - large model,on,XSUM dataset
summarization,15,39,experiments,extreme summarisation dataset,on,general news domain
summarization,15,34,hyperparameters,size,of,training data
summarization,15,45,hyperparameters,ROUGE score,for,each ground - truth TLDRs
summarization,15,45,hyperparameters,ROUGE score,select,maximum
summarization,15,45,hyperparameters,Hyperparameters,compute,ROUGE score
summarization,15,8,model,Model,to have,background knowledge
summarization,15,38,model,new information,ready to train,our model
summarization,15,38,model,Model,With,new information
summarization,15,2,results,TLDR,has,Extreme Summarization of Scientific Documents
summarization,15,5,results,shown to outperform,has,extractive and abstractive baselines
summarization,15,12,results,extractive oracle,provides,upper bound performance
summarization,15,12,results,Results,has,extractive oracle
summarization,15,17,results,reviewer comments,are,high quality summaries
summarization,15,17,results,Results,has,reviewer comments
summarization,15,19,results,uniqueness,of,SCITLDR
summarization,15,19,results,SCITLDR,is,each paper
summarization,15,19,results,each paper,in,test set
summarization,15,19,results,test set,map to,multiple groundtruth TLDRs
summarization,15,20,results,multiple ground - truth summaries,to compute,ROUGE scores
summarization,15,20,results,author and reader 's TLDR,capture,variation
summarization,15,20,results,variation,in,summaries
summarization,15,20,results,summaries,based on,reader 's perspective
summarization,15,22,results,extremely high compression ratio,compared to,other datasets
summarization,15,22,results,SCITLDR,has,extremely high compression ratio
summarization,15,22,results,Results,has,SCITLDR
summarization,15,27,results,low ROUGEE - 1 overlap ( 27.40 ),between,author-generated TLDRs
summarization,15,27,results,low ROUGEE - 1 overlap ( 27.40 ),between,PR - generated TLDRs
summarization,15,28,results,Author-generated TLDRs,ROUGE - 1 of,34.1
summarization,15,28,results,Results,has,Author-generated TLDRs
summarization,15,40,results,our BART model,on,SCITLDR and title dataset
summarization,15,40,results,Results,netune,our BART model
