topic,paper_ID,sentence_ID,info-unit,sub,pred,obj,triplets,pred_weights
Blogs,0,16,ablation-analysis,cons,Misses out on,words,cons Misses out on words,0.785237193107605
Blogs,0,16,ablation-analysis,words,not labelled as,oov,words not labelled as oov,0.7164342403411865
Blogs,0,16,ablation-analysis,oov,in,source language,oov in source language,0.4992927610874176
Blogs,0,16,ablation-analysis,ablation analysis,has,cons,ablation analysis has cons,0.5800865292549133
Blogs,0,25,baselines,pros,Faster than,posall model,pros Faster than posall model,0.7905068397521973
Blogs,0,25,baselines,baselines,has,pros,baselines has pros,0.6120114922523499
Blogs,0,13,experiments,oov word,in,target language,oov word in target language,0.5129311680793762
Blogs,0,13,experiments,oov word,aligned with,known word,oov word aligned with known word,0.7188357710838318
Blogs,0,13,experiments,known word,in,source language,known word in source language,0.47539958357810974
Blogs,0,23,experiments,posunk - positional unknown model oov words,in,source language,posunk - positional unknown model oov words in source language,0.4685186743736267
Blogs,0,23,experiments,posunk - positional unknown model oov words,assigned,single unk token,posunk - positional unknown model oov words assigned single unk token,0.6027828454971313
Blogs,0,29,experiments,dataset subset of wmt '14 datasetalignment,computed using,berkeley aligner,dataset subset of wmt '14 datasetalignment computed using berkeley aligner,0.659676730632782
Blogs,0,29,experiments,architecture,from,sequence to sequence learning,architecture from sequence to sequence learning,0.5927600860595703
Blogs,0,29,experiments,sequence to sequence learning,with,neural networks paper,sequence to sequence learning with neural networks paper,0.6546239256858826
Blogs,0,7,hyperparameters,training corpus,with,information,training corpus with information,0.5985103845596313
Blogs,0,7,hyperparameters,information,about,what do,information about what do,0.7111123204231262
Blogs,0,7,hyperparameters,information,about,different oov words ( in the target sentence ),information about different oov words ( in the target sentence ),0.6199257969856262
Blogs,0,7,hyperparameters,different oov words ( in the target sentence ),correspond to in,source sentence,different oov words ( in the target sentence ) correspond to in source sentence,0.6098233461380005
Blogs,0,7,hyperparameters,what do,has,different oov words ( in the target sentence ),what do has different oov words ( in the target sentence ),0.5655842423439026
Blogs,0,7,hyperparameters,hyperparameters,Annotate,training corpus,hyperparameters Annotate training corpus,0.6721385717391968
Blogs,0,17,hyperparameters,oov words,in,source language,oov words in source language,0.46132686734199524
Blogs,0,17,hyperparameters,oov words,assigned,single unk token,oov words assigned single unk token,0.6453840732574463
Blogs,0,17,hyperparameters,hyperparameters,has,posall - positional all model,hyperparameters has posall - positional all model,0.5199056267738342
Blogs,0,18,hyperparameters,all words,in,target sentences,all words in target sentences,0.48736539483070374
Blogs,0,18,hyperparameters,all words,assigned,positional tokens,all words assigned positional tokens,0.6142099499702454
Blogs,0,18,hyperparameters,positional tokens,denote that,jth word,positional tokens denote that jth word,0.15222948789596558
Blogs,0,18,hyperparameters,jth word,in,target sentence,jth word in target sentence,0.5146692395210266
Blogs,0,18,hyperparameters,ith word,in,source sentence,ith word in source sentence,0.5247073173522949
Blogs,0,18,hyperparameters,hyperparameters,has,all words,hyperparameters has all words,0.5464056730270386
Blogs,0,19,hyperparameters,aligned words,that are too far apart,unaligned,aligned words that are too far apart unaligned,0.6433088779449463
Blogs,0,19,hyperparameters,aligned words,assigned,null token,aligned words assigned null token,0.6738707423210144
Blogs,0,19,hyperparameters,unaligned,assigned,null token,unaligned assigned null token,0.6638457179069519
Blogs,0,19,hyperparameters,hyperparameters,has,aligned words,hyperparameters has aligned words,0.5375375151634216
Blogs,0,24,hyperparameters,oov words,in,target sentences,oov words in target sentences,0.4697098433971405
Blogs,0,24,hyperparameters,oov words,assigned,unk token,oov words assigned unk token,0.6498337984085083
Blogs,0,24,hyperparameters,unk token,with,position,unk token with position,0.6361627578735352
Blogs,0,24,hyperparameters,word,in,target language,word in target language,0.5008992552757263
Blogs,0,24,hyperparameters,word,with respect to,aligned source word,word with respect to aligned source word,0.6406921744346619
Blogs,0,24,hyperparameters,hyperparameters,has,oov words,hyperparameters has oov words,0.5491726994514465
Blogs,0,8,model,nmt,track,alignment,nmt track alignment,0.8002227544784546
Blogs,0,8,model,nmt,emits,alignments,nmt emits alignments,0.7653658390045166
Blogs,0,8,model,alignment,of,rare words,alignment of rare words,0.5357768535614014
Blogs,0,8,model,rare words,across,source and target sentences,rare words across source and target sentences,0.6162741184234619
Blogs,0,8,model,alignments,for,test sentences,alignments for test sentences,0.5593752861022949
Blogs,0,8,model,model,has,nmt,model has nmt,0.5953153967857361
Blogs,0,9,model,dictionary,to map,rare words,dictionary to map rare words,0.6500953435897827
Blogs,0,9,model,rare words,from,source language to target language,rare words from source language to target language,0.5486018061637878
Blogs,0,11,model,oov words,in,source sentence,oov words in source sentence,0.47988036274909973
Blogs,0,12,model,each oov word,aligned to,some oov word,each oov word aligned to some oov word,0.7066936492919922
Blogs,0,12,model,some oov word,in,source language,some oov word in source language,0.4760696589946747
Blogs,0,12,model,some oov word,in,source language,some oov word in source language,0.4760696589946747
Blogs,0,12,model,same token,as,word,same token as word,0.5967795252799988
Blogs,0,12,model,word,in,source language,word in source language,0.4902328550815582
Blogs,0,12,model,target language,has,each oov word,target language has each oov word,0.5529950857162476
Blogs,0,12,model,model,In,target language,model In target language,0.49390336871147156
Blogs,0,20,results,pros,Captures,complete alignment,pros Captures complete alignment,0.756661593914032
Blogs,0,20,results,complete alignment,between,source and target sentences,complete alignment between source and target sentences,0.591005802154541
Blogs,0,20,results,results,has,pros,results has pros,0.4505263864994049
Blogs,0,22,results,length,of,target sentences,length of target sentences,0.5532519221305847
Blogs,0,22,results,results,doubles,length,results doubles length,0.6197922825813293
Blogs,0,26,results,cons,Does not capture,alignment,cons Does not capture alignment,0.7574475407600403
Blogs,0,26,results,alignment,for,all words,alignment for all words,0.5921840071678162
Blogs,0,26,results,results,has,cons,results has cons,0.5004299283027649
Blogs,1,3,ablation-analysis,relevant artifacts,in,paper,relevant artifacts in paper,0.5203087329864502
Blogs,1,3,ablation-analysis,ablation analysis,describe and demystify,relevant artifacts,ablation analysis describe and demystify relevant artifacts,0.7147921919822693
Blogs,1,12,ablation-analysis,decoder,accesses,last hidden state,decoder accesses last hidden state,0.7258039116859436
Blogs,1,12,ablation-analysis,last hidden state,lose,relevant information,last hidden state lose relevant information,0.6866310238838196
Blogs,1,12,ablation-analysis,relevant information,about,first elements of the sequence,relevant information about first elements of the sequence,0.6030790209770203
Blogs,1,12,ablation-analysis,last hidden state,has,of the decoder,last hidden state has of the decoder,0.5771781802177429
Blogs,1,12,ablation-analysis,ablation analysis,if,decoder,ablation analysis if decoder,0.6263576149940491
Blogs,1,52,baselines,baselines,has,scaled dot-product attention formula,baselines has scaled dot-product attention formula,0.5481886863708496
Blogs,1,71,baselines,position embedding,coding,every known position,position embedding coding every known position,0.779130756855011
Blogs,1,71,baselines,every known position,with,vector,every known position with vector,0.6599734425544739
Blogs,1,101,baselines,same residual dropout,executed in,encoder,same residual dropout executed in encoder,0.7301998734474182
Blogs,1,33,hyperparameters,hyperparameters,has,vectors,hyperparameters has vectors,0.5348069071769714
Blogs,1,42,hyperparameters,input,consists of,queries and keys,input consists of queries and keys,0.6342929005622864
Blogs,1,42,hyperparameters,input,consists of,values,input consists of values,0.640492856502533
Blogs,1,42,hyperparameters,values,of,dimension dv,values of dimension dv,0.568855345249176
Blogs,1,42,hyperparameters,queries and keys,has,of dimension dk,queries and keys has of dimension dk,0.6150220632553101
Blogs,1,42,hyperparameters,hyperparameters,has,input,hyperparameters has input,0.4997154176235199
Blogs,1,50,hyperparameters,softmaxing,multiply by,value matrix,softmaxing multiply by value matrix,0.5977227687835693
Blogs,1,50,hyperparameters,value matrix,to keep,values,value matrix to keep values,0.5827018022537231
Blogs,1,50,hyperparameters,values,of,words,values of words,0.6139428615570068
Blogs,1,50,hyperparameters,values,for,irrelevant words,values for irrelevant words,0.6326054334640503
Blogs,1,50,hyperparameters,values,for,irrelevant words,values for irrelevant words,0.6326054334640503
Blogs,1,50,hyperparameters,hyperparameters,After,softmaxing,hyperparameters After softmaxing,0.6513674259185791
Blogs,1,66,hyperparameters,positional encodings,have,same dimension,positional encodings have same dimension,0.5689814686775208
Blogs,1,66,hyperparameters,same dimension,as,embeddings,same dimension as embeddings,0.59543776512146
Blogs,1,66,hyperparameters,hyperparameters,has,positional encodings,hyperparameters has positional encodings,0.5430561304092407
Blogs,1,78,hyperparameters,embedding layers,multiply,weights,embedding layers multiply weights,0.7195728421211243
Blogs,1,78,hyperparameters,weights,by,square root,weights by square root,0.6189128756523132
Blogs,1,78,hyperparameters,square root,of,model dimension,square root of model dimension,0.6297951936721802
Blogs,1,78,hyperparameters,hyperparameters,In,embedding layers,hyperparameters In embedding layers,0.44566336274147034
Blogs,1,83,hyperparameters,dropout,to,sums of the embeddings and the positional encodings,dropout to sums of the embeddings and the positional encodings,0.5629639029502869
Blogs,1,83,hyperparameters,dropout,in,both the encoder and decoder stacks,dropout in both the encoder and decoder stacks,0.5811875462532043
Blogs,1,83,hyperparameters,sums of the embeddings and the positional encodings,in,both the encoder and decoder stacks,sums of the embeddings and the positional encodings in both the encoder and decoder stacks,0.5373272895812988
Blogs,1,83,hyperparameters,both the encoder and decoder stacks,with,dropout rate,both the encoder and decoder stacks with dropout rate,0.6616740226745605
Blogs,1,83,hyperparameters,both the encoder and decoder stacks,of,0.1,both the encoder and decoder stacks of 0.1,0.6376064419746399
Blogs,1,83,hyperparameters,dropout rate,of,0.1,dropout rate of 0.1,0.5755324363708496
Blogs,1,83,hyperparameters,hyperparameters,apply,dropout,hyperparameters apply dropout,0.6019902229309082
Blogs,1,84,hyperparameters,normalization and residual connections,are,standard tricks,normalization and residual connections are standard tricks,0.5391916036605835
Blogs,1,84,hyperparameters,standard tricks,to help,deep neural networks,standard tricks to help deep neural networks,0.608792245388031
Blogs,1,84,hyperparameters,deep neural networks,train,faster and more accurately,deep neural networks train faster and more accurately,0.6377730965614319
Blogs,1,84,hyperparameters,hyperparameters,has,normalization and residual connections,hyperparameters has normalization and residual connections,0.5318103432655334
Blogs,1,85,hyperparameters,layer normalization,applied over,embedding dimension,layer normalization applied over embedding dimension,0.6426995992660522
Blogs,1,85,hyperparameters,hyperparameters,has,layer normalization,hyperparameters has layer normalization,0.494924396276474
Blogs,1,4,model,great advance,in the use of,attention mechanism,great advance in the use of attention mechanism,0.5982799530029297
Blogs,1,6,model,simple model,apply to,small- scale nmt problem,simple model apply to small- scale nmt problem,0.6288808584213257
Blogs,1,6,model,model,build,simple model,model build simple model,0.722750723361969
Blogs,1,6,model,model,apply to,small- scale nmt problem,model apply to small- scale nmt problem,0.6360272765159607
Blogs,1,10,model,great limitation,when working with,long sequences,great limitation when working with long sequences,0.6488352417945862
Blogs,1,10,model,ability,to retain,information,ability to retain information,0.66121506690979
Blogs,1,10,model,information,from,first elements,information from first elements,0.5529778003692627
Blogs,1,10,model,new elements,incorporated into,sequence,new elements incorporated into sequence,0.7306273579597473
Blogs,1,11,model,hidden state,in,every step,hidden state in every step,0.5433423519134521
Blogs,1,11,model,hidden state,associated with,certain word,hidden state associated with certain word,0.6735291481018066
Blogs,1,11,model,certain word,in,input sentence,certain word in input sentence,0.4953155815601349
Blogs,1,11,model,encoder,has,hidden state,encoder has hidden state,0.5545893311500549
Blogs,1,11,model,model,In,encoder,model In encoder,0.5603265166282654
Blogs,1,13,model,new concept,introduced,attention mechanism,new concept introduced attention mechanism,0.6980972290039062
Blogs,1,13,model,limitation,has,new concept,limitation has new concept,0.6180323958396912
Blogs,1,14,model,rnns,in,each step,rnns in each step,0.5711585283279419
Blogs,1,14,model,each step,of,decoder,each step of decoder,0.6398374438285828
Blogs,1,14,model,decoder,look at,all the states of the encoder,decoder look at all the states of the encoder,0.5957489609718323
Blogs,1,15,model,attention,extracts,information,attention extracts information,0.6513959765434265
Blogs,1,15,model,information,from,whole sequence,information from whole sequence,0.5097026228904724
Blogs,1,15,model,weighted sum,of,all the past encoder states,weighted sum of all the past encoder states,0.5542597770690918
Blogs,1,15,model,information,has,weighted sum,information has weighted sum,0.573258101940155
Blogs,1,15,model,whole sequence,has,weighted sum,whole sequence has weighted sum,0.6008045673370361
Blogs,1,15,model,model,is,attention,model is attention,0.6092095375061035
Blogs,1,15,model,model,extracts,information,model extracts information,0.6948923468589783
Blogs,1,16,model,decoder,assign,greater weight or importance,decoder assign greater weight or importance,0.7060692310333252
Blogs,1,16,model,greater weight or importance,to,certain element,greater weight or importance to certain element,0.5182420611381531
Blogs,1,16,model,certain element,of,input,certain element of input,0.6233253479003906
Blogs,1,16,model,certain element,of,output,certain element of output,0.6388818621635437
Blogs,1,16,model,certain element,of,output,certain element of output,0.6388818621635437
Blogs,1,16,model,input,for,each element,input for each element,0.6294252276420593
Blogs,1,16,model,each element,of,output,each element of output,0.6356538534164429
Blogs,1,17,model,learning,in,every step,learning in every step,0.5688589811325073
Blogs,1,17,model,every step,to,focus,every step to focus,0.5948854088783264
Blogs,1,17,model,focus,in,right element,focus in right element,0.5438668727874756
Blogs,1,17,model,right element,of,input,right element of input,0.6070719361305237
Blogs,1,17,model,model,has,learning,model has learning,0.5897330045700073
Blogs,1,19,model,wait,till,completion of t-1 steps,wait till completion of t-1 steps,0.7071058750152588
Blogs,1,19,model,completion of t-1 steps,to process,t-th step,completion of t-1 steps to process t-th step,0.7057844400405884
Blogs,1,19,model,model,has,both the encoder and the decoder,model has both the encoder and the decoder,0.564752459526062
Blogs,1,21,model,attention mechanism,to draw global dependencies,between input and output,attention mechanism to draw global dependencies between input and output,0.7901217937469482
Blogs,1,22,model,first transduction model,relying entirely on,self-attention,first transduction model relying entirely on self-attention,0.6952005624771118
Blogs,1,22,model,first transduction model,to compute,representations,first transduction model to compute representations,0.7641937136650085
Blogs,1,22,model,representations,of,input and output,representations of input and output,0.6151825189590454
Blogs,1,22,model,representations,without using,sequence - aligned rnns or convolution,representations without using sequence - aligned rnns or convolution,0.7236257791519165
Blogs,1,24,model,features,for,each word,features for each word,0.5766362547874451
Blogs,1,24,model,each word,using,self-attention mechanism,each word using self-attention mechanism,0.6929341554641724
Blogs,1,24,model,self-attention mechanism,to figure out,important,self-attention mechanism to figure out important,0.7147937417030334
Blogs,1,24,model,important,has,other words in the sentence,important has other words in the sentence,0.5692484974861145
Blogs,1,25,model,recurrent units,to obtain,features,recurrent units to obtain features,0.5880506634712219
Blogs,1,25,model,recurrent units,are,weighted sums and activations,recurrent units are weighted sums and activations,0.5796571373939514
Blogs,1,25,model,model,has,recurrent units,model has recurrent units,0.5742875337600708
Blogs,1,28,model,encoder model,on,left side,encoder model on left side,0.594417154788971
Blogs,1,28,model,decoder,on,right one,decoder on right one,0.6114698052406311
Blogs,1,29,model,core block,of,an attention and a feed-forward network,core block of an attention and a feed-forward network,0.5870696306228638
Blogs,1,29,model,an attention and a feed-forward network,has,repeated n times,an attention and a feed-forward network has repeated n times,0.5824795365333557
Blogs,1,29,model,model,contains,core block,model contains core block,0.661020815372467
Blogs,1,30,model,core concept,in,depth,core concept in depth,0.5610247850418091
Blogs,1,30,model,depth,has,self-attention mechanism,depth has self-attention mechanism,0.5388299226760864
Blogs,1,30,model,model,explore,core concept,model explore core concept,0.6824681162834167
Blogs,1,31,model,self- attention,has,fundamental operation,self- attention has fundamental operation,0.5728828310966492
Blogs,1,31,model,sequence - to-sequence operation,has,sequence of vectors,sequence - to-sequence operation has sequence of vectors,0.5921849608421326
Blogs,1,31,model,model,has,self- attention,model has self- attention,0.5249809622764587
Blogs,1,32,model,model,call,"corresponding output vectors y1 , y2 , ? , yt","model call corresponding output vectors y1 , y2 , ? , yt",0.6083109378814697
Blogs,1,34,model,self attention operation,takes,weighted average,self attention operation takes weighted average,0.628158450126648
Blogs,1,34,model,weighted average,over,all the input vectors,weighted average over all the input vectors,0.6852783560752869
Blogs,1,34,model,output vector yi,has,self attention operation,output vector yi has self attention operation,0.4934084117412567
Blogs,1,34,model,model,To produce,output vector yi,model To produce output vector yi,0.6967646479606628
Blogs,1,35,model,model,has,transformers from scratch,model has transformers from scratch,0.5889252424240112
Blogs,1,36,model,self-attention mechanism,introduce,three elements,self-attention mechanism introduce three elements,0.6061133146286011
Blogs,1,36,model,three different ways,in,self-attention mechanism,three different ways in self-attention mechanism,0.49588996171951294
Blogs,1,36,model,keys,has,query,keys has query,0.6069030165672302
Blogs,1,36,model,model,In,self-attention mechanism,model In self-attention mechanism,0.5129305720329285
Blogs,1,37,model,every role,compared to,other vectors,every role compared to other vectors,0.6822799444198608
Blogs,1,37,model,every role,to compute,each output vector,every role to compute each output vector,0.745304524898529
Blogs,1,37,model,other vectors,to get,own output yi ( query ),other vectors to get own output yi ( query ),0.6076803207397461
Blogs,1,37,model,each output vector,once,weights,each output vector once weights,0.68015056848526
Blogs,1,37,model,model,In,every role,model In every role,0.5425392985343933
Blogs,1,38,model,three weight matrices,dimensions,k x k,three weight matrices dimensions k x k,0.6547674536705017
Blogs,1,38,model,three linear transformation,for,each xi,three linear transformation for each xi,0.647100031375885
Blogs,1,38,model,model,compute,three linear transformation,model compute three linear transformation,0.7464491128921509
Blogs,1,39,model,three learnable weight layers,applied to,same encoded input,three learnable weight layers applied to same encoded input,0.6475744843482971
Blogs,1,40,model,attention mechanism,of,input vector,attention mechanism of input vector,0.5488440990447998
Blogs,1,40,model,model,apply,attention mechanism,model apply attention mechanism,0.6385172605514526
Blogs,1,43,model,dot product,of,query,dot product of query,0.6080477237701416
Blogs,1,43,model,dot product,divide,square root of dk,dot product divide square root of dk,0.73588627576828
Blogs,1,43,model,dot product,apply,softmax function,dot product apply softmax function,0.6255101561546326
Blogs,1,43,model,query,with,all keys,query with all keys,0.6194281578063965
Blogs,1,43,model,softmax function,to obtain,weights,softmax function to obtain weights,0.591539204120636
Blogs,1,43,model,weights,on,values,weights on values,0.5490699410438538
Blogs,1,43,model,model,compute,dot product,model compute dot product,0.7326132655143738
Blogs,1,45,model,"q , k and v matrices",to calculate,attention scores,"q , k and v matrices to calculate attention scores",0.6846718192100525
Blogs,1,45,model,model,use,"q , k and v matrices","model use q , k and v matrices",0.6609025001525879
Blogs,1,47,model,dot product,of,query vector,dot product of query vector,0.5873607397079468
Blogs,1,47,model,query vector,with,key vector,query vector with key vector,0.6135010123252869
Blogs,1,47,model,key vector,of,respective word,key vector of respective word,0.6183144450187683
Blogs,1,47,model,model,has,dot product,model has dot product,0.5952513813972473
Blogs,1,48,model,position 1,calculate,dot product ( . ),position 1 calculate dot product ( . ),0.6459376811981201
Blogs,1,48,model,dot product ( . ),of,q1 and k1,dot product ( . ) of q1 and k1,0.6245903968811035
Blogs,1,48,model,dot product ( . ),of,q1,dot product ( . ) of q1,0.605009913444519
Blogs,1,48,model,dot product ( . ),apply,scaled   factor,dot product ( . ) apply scaled   factor,0.6462244391441345
Blogs,1,48,model,dot product ( . ),"""",scaled   factor,"dot product ( . ) "" scaled   factor",0.6075125336647034
Blogs,1,48,model,scaled   factor,to have,more stable gradients,scaled   factor to have more stable gradients,0.6287774443626404
Blogs,1,48,model,model,for,position 1,model for position 1,0.6494653224945068
Blogs,1,48,model,model,apply,scaled   factor,model apply scaled   factor,0.6844860911369324
Blogs,1,49,model,softmax function,can not work properly,large values,softmax function can not work properly large values,0.6891902685165405
Blogs,1,49,model,large values,resulting in,vanishing,large values resulting in vanishing,0.7589928507804871
Blogs,1,49,model,vanishing,has,gradients,vanishing has gradients,0.613991379737854
Blogs,1,49,model,slowing down,has,learning,slowing down has learning,0.5932949185371399
Blogs,1,49,model,model,has,softmax function,model has softmax function,0.552506685256958
Blogs,1,51,model,model,has,formula,model has formula,0.6037648916244507
Blogs,1,54,model,attention scores,focused on,whole sentence,attention scores focused on whole sentence,0.6589928269386292
Blogs,1,54,model,whole sentence,would produce,same results,whole sentence would produce same results,0.7021477818489075
Blogs,1,55,model,different segments,of,words,different segments of words,0.5961324572563171
Blogs,1,55,model,model,attend to,different segments,model attend to different segments,0.7094828486442566
Blogs,1,56,model,greater power,of,discrimination,greater power of discrimination,0.6092051863670349
Blogs,1,56,model,several self attention heads,dividing,words vectors,several self attention heads dividing words vectors,0.7184904217720032
Blogs,1,56,model,words vectors,into,"fixed number ( h , number of heads ) of chunks","words vectors into fixed number ( h , number of heads ) of chunks",0.5559269189834595
Blogs,1,56,model,self-attention,applied on,corresponding chunks,self-attention applied on corresponding chunks,0.6947275996208191
Blogs,1,56,model,self-attention,using,"q , k and v submatrices","self-attention using q , k and v submatrices",0.6612957715988159
Blogs,1,56,model,corresponding chunks,using,"q , k and v submatrices","corresponding chunks using q , k and v submatrices",0.6877437829971313
Blogs,1,56,model,self attention,has,greater power,self attention has greater power,0.5526158213615417
Blogs,1,56,model,model,give,self attention,model give self attention,0.621736466884613
Blogs,1,58,model,h different output matrices,of,scores,h different output matrices of scores,0.5840661525726318
Blogs,1,58,model,model,produce,h different output matrices,model produce h different output matrices,0.6661529541015625
Blogs,1,60,model,next layer,expecting,additional weights matrix wo,next layer expecting additional weights matrix wo,0.6839848756790161
Blogs,1,60,model,next layer,of,additional weights matrix wo,next layer of additional weights matrix wo,0.5576430559158325
Blogs,1,60,model,next layer,multiply them by,additional weights matrix wo,next layer multiply them by additional weights matrix wo,0.5941608548164368
Blogs,1,60,model,layer ),expecting,additional weights matrix wo,layer ) expecting additional weights matrix wo,0.6720535159111023
Blogs,1,60,model,layer ),multiply them by,additional weights matrix wo,layer ) multiply them by additional weights matrix wo,0.6014419794082642
Blogs,1,60,model,matrix,multiply them by,additional weights matrix wo,matrix multiply them by additional weights matrix wo,0.5603040456771851
Blogs,1,60,model,output matrices,multiply them by,additional weights matrix wo,output matrices multiply them by additional weights matrix wo,0.5915336608886719
Blogs,1,60,model,model,has,next layer,model has next layer,0.5867395997047424
Blogs,1,61,model,final matrix,captures,information,final matrix captures information,0.7277604937553406
Blogs,1,61,model,information,from,all the attention heads,information from all the attention heads,0.6002374291419983
Blogs,1,61,model,model,has,final matrix,model has final matrix,0.5546296834945679
Blogs,1,64,model,representation,position of the word in,sentence,representation position of the word in sentence,0.7388935089111328
Blogs,1,64,model,representation,add it to,word embedding,representation add it to word embedding,0.6182911396026611
Blogs,1,64,model,model,create,representation,model create representation,0.670674741268158
Blogs,1,65,model,positional encodings,to,input embeddings,positional encodings to input embeddings,0.51185142993927
Blogs,1,65,model,input embeddings,bottoms of,encoder and decoder stacks,input embeddings bottoms of encoder and decoder stacks,0.7641772031784058
Blogs,1,65,model,model,add,positional encodings,model add positional encodings,0.6493856310844421
Blogs,1,69,model,model,apply,function,model apply function,0.6396524906158447
Blogs,1,70,model,network,learn how to use,information,network learn how to use information,0.637003481388092
Blogs,1,70,model,model,has,network,model has network,0.6001628041267395
Blogs,1,72,model,sentences,of,all accepted positions,sentences of all accepted positions,0.5934504270553589
Blogs,1,72,model,all accepted positions,during,training loop,all accepted positions during training loop,0.6851685643196106
Blogs,1,72,model,positional encoding,allow,model,positional encoding allow model,0.645630419254303
Blogs,1,72,model,model,to extrapolate to,sequence lengths,model to extrapolate to sequence lengths,0.6641787886619568
Blogs,1,72,model,sequence lengths,ones encountered during,training,sequence lengths ones encountered during training,0.7591978907585144
Blogs,1,72,model,model,requiere,sentences,model requiere sentences,0.7013053894042969
Blogs,1,76,model,position encoding,to,input embedding,position encoding to input embedding,0.5265244841575623
Blogs,1,76,model,model,Add,position encoding,model Add position encoding,0.6590470671653748
Blogs,1,77,model,same weight matrix,shared between,two embedding layers ( encoder and decoder ),same weight matrix shared between two embedding layers ( encoder and decoder ),0.6816318035125732
Blogs,1,77,model,same weight matrix,shared between,pre-softmax linear transformation,same weight matrix shared between pre-softmax linear transformation,0.7292187213897705
Blogs,1,77,model,model,has,same weight matrix,model has same weight matrix,0.57950359582901
Blogs,1,79,model,n=6 identical layers,containing,two sub-layers,n=6 identical layers containing two sub-layers,0.6914842128753662
Blogs,1,79,model,two linear transformations,with,relu activation,two linear transformations with relu activation,0.6331637501716614
Blogs,1,79,model,two sub-layers,has,multi-head self-attention mechanism,two sub-layers has multi-head self-attention mechanism,0.5130113959312439
Blogs,1,79,model,fully connected feed -forward network,has,two linear transformations,fully connected feed -forward network has two linear transformations,0.5801492929458618
Blogs,1,79,model,model,has,n=6 identical layers,model has n=6 identical layers,0.5498502850532532
Blogs,1,80,model,position - wise,to,input,position - wise to input,0.5869842171669006
Blogs,1,80,model,vector,belonging to,sentence sequence,vector belonging to sentence sequence,0.665307879447937
Blogs,1,81,model,residual connection,around,each sub-layer ( attention and fc network ),residual connection around each sub-layer ( attention and fc network ),0.6445577144622803
Blogs,1,81,model,residual connection,summing up,output,residual connection summing up output,0.5992543697357178
Blogs,1,81,model,output,of,layer,output of layer,0.6212378740310669
Blogs,1,81,model,output,followed by,layer normalization,output followed by layer normalization,0.6972091197967529
Blogs,1,81,model,layer,with,input,layer with input,0.6465345621109009
Blogs,1,81,model,input,followed by,layer normalization,input followed by layer normalization,0.6923039555549622
Blogs,1,82,model,regularization,applied,dropout,regularization applied dropout,0.7276173830032349
Blogs,1,82,model,regularization,apply,dropout,regularization apply dropout,0.657685399055481
Blogs,1,82,model,dropout,to,output,dropout to output,0.5553109645843506
Blogs,1,82,model,output,of,each sub-layer,output of each sub-layer,0.6057036519050598
Blogs,1,82,model,every residual connection,has,regularization,every residual connection has regularization,0.5573859810829163
Blogs,1,82,model,model,Before,every residual connection,model Before every residual connection,0.6843749284744263
Blogs,1,87,model,model,implement,encoder layer,model implement encoder layer,0.6719666123390198
Blogs,1,89,model,model,has,encoder code,model has encoder code,0.560050904750824
Blogs,1,91,model,decoder,share,some components,decoder share some components,0.7563251852989197
Blogs,1,91,model,decoder,used in,different way,decoder used in different way,0.7216737270355225
Blogs,1,91,model,some components,with,encoder,some components with encoder,0.6737387180328369
Blogs,1,91,model,different way,to take into account,encoder output,different way to take into account encoder output,0.6394014358520508
Blogs,1,91,model,model,has,decoder,model has decoder,0.6226420402526855
Blogs,1,95,model,predictions,for,position i,predictions for position i,0.6735017895698547
Blogs,1,95,model,position i,depend only on,known outputs,position i depend only on known outputs,0.7523177862167358
Blogs,1,95,model,known outputs,at,positions less than i,known outputs at positions less than i,0.5572943091392517
Blogs,1,96,model,values,corresponding to,forbidden states,values corresponding to forbidden states,0.6183386445045471
Blogs,1,96,model,forbidden states,in,softmax layer,forbidden states in softmax layer,0.5013512969017029
Blogs,1,96,model,softmax layer,of,dot-product attention modules,softmax layer of dot-product attention modules,0.5081239342689514
Blogs,1,97,model,encoder- decoder attention,performs,multi-head attention,encoder- decoder attention performs multi-head attention,0.5317725539207458
Blogs,1,97,model,multi-head attention,output of,decoder,multi-head attention output of decoder,0.7240061163902283
Blogs,1,97,model,key and value vectors,come from,output of the encoder,key and value vectors come from output of the encoder,0.6065662503242493
Blogs,1,97,model,queries,come from,previous decoder layer,queries come from previous decoder layer,0.6755337119102478
Blogs,1,98,model,every position,in,decoder,every position in decoder,0.5869318842887878
Blogs,1,98,model,every position,to attend,over all positions,every position to attend over all positions,0.6834655404090881
Blogs,1,98,model,decoder,in,input sequence,decoder in input sequence,0.5207729339599609
Blogs,1,98,model,over all positions,in,input sequence,over all positions in input sequence,0.5224571228027344
Blogs,1,98,model,model,allows,every position,model allows every position,0.696484386920929
Blogs,1,100,model,residual connection and layer normalization,around,each sub-layer,residual connection and layer normalization around each sub-layer,0.6382412910461426
Blogs,1,100,model,model,has,residual connection and layer normalization,model has residual connection and layer normalization,0.511116623878479
Blogs,1,7,results,guide,on,encoder-decoder model,guide on encoder-decoder model,0.5736496448516846
Blogs,1,7,results,guide,on,attention mechanism,guide on attention mechanism,0.5254599452018738
Blogs,1,23,results,attention,all you need,paper,attention all you need paper,0.5631428956985474
Blogs,1,23,results,results,has,attention,results has attention,0.4885852336883545
Blogs,1,44,results,attention,all you need,paper,attention all you need paper,0.5631428956985474
Blogs,1,44,results,results,has,attention,results has attention,0.4885852336883545
Blogs,1,46,results,scores,measure,how much focus,scores measure how much focus,0.7128035426139832
Blogs,1,46,results,how much focus,to place,other places or words,how much focus to place other places or words,0.7052729725837708
Blogs,1,46,results,other places or words,of,input sequence,other places or words of input sequence,0.5737308263778687
Blogs,1,46,results,input sequence,w.r.t,word,input sequence w.r.t word,0.5699489712715149
Blogs,1,46,results,word,at,certain position,word at certain position,0.5740185976028442
Blogs,1,46,results,results,has,scores,results has scores,0.5219217538833618
Blogs,1,63,results,words,in,input sentence,words in input sentence,0.5217873454093933
Blogs,1,63,results,words,get,same solutions,words get same solutions,0.6441340446472168
Blogs,1,68,results,attention,all you need,paper,attention all you need paper,0.5631428956985474
Blogs,1,68,results,results,has,attention,results has attention,0.4885852336883545
Blogs,2,11,ablation-analysis,new languages,reduces,effective per-task capacity,new languages reduces effective per-task capacity,0.6476808786392212
Blogs,2,11,ablation-analysis,effective per-task capacity,of,model,effective per-task capacity of model,0.5799109935760498
Blogs,2,11,ablation-analysis,ablation analysis,adding,new languages,ablation analysis adding new languages,0.7261276841163635
Blogs,2,21,baselines,baselines,has,baselines dedicated bilingual models ( variants of transformers ),baselines has baselines dedicated bilingual models ( variants of transformers ),0.5445412993431091
Blogs,2,28,baselines,all the datasets,as if,single dataset,all the datasets as if single dataset,0.5755420327186584
Blogs,2,12,experiments,multilingual translation,Maximize,number of languages,multilingual translation Maximize number of languages,0.6623554229736328
Blogs,2,17,experiments,in -house corpus,generated by,crawling,in -house corpus generated by crawling,0.6898666024208069
Blogs,2,17,experiments,in -house corpus,extracting,parallel sentences,in -house corpus extracting parallel sentences,0.6745682954788208
Blogs,2,17,experiments,parallel sentences,from,web,parallel sentences from web,0.6100704073905945
Blogs,2,18,experiments,102 languages,with,25 billion sentence pairs,102 languages with 25 billion sentence pairs,0.6109039783477783
Blogs,2,42,experiments,low resource languages,has,translating multiple languages to english,low resource languages has translating multiple languages to english,0.5649450421333313
Blogs,2,23,hyperparameters,medium and low resource languages,has,transformer base,medium and low resource languages has transformer base,0.5741174221038818
Blogs,2,23,hyperparameters,hyperparameters,For,medium and low resource languages,hyperparameters For medium and low resource languages,0.5846982598304749
Blogs,2,24,hyperparameters,batch size,of,1 m tokes per-batch,batch size of 1 m tokes per-batch,0.6676301956176758
Blogs,2,24,hyperparameters,hyperparameters,has,batch size,hyperparameters has batch size,0.5015887022018433
Blogs,2,32,hyperparameters,batch size,of,4 m tokens,batch size of 4 m tokens,0.6034111976623535
Blogs,2,49,hyperparameters,hyperparameters,has,temperature sampling,hyperparameters has temperature sampling,0.535940945148468
Blogs,2,5,model,model,build,universal neural machine translation system,model build universal neural machine translation system,0.6760125756263733
Blogs,2,30,model,target   index  ,prepended with,every input sentence,target   index   prepended with every input sentence,0.673026442527771
Blogs,2,30,model,every input sentence,to indicate,language,every input sentence to indicate language,0.639165997505188
Blogs,2,30,model,model,has,target   index  ,model has target   index  ,0.5447673201560974
Blogs,2,31,model,shared encoder and decoder,used across,all the language pairs,shared encoder and decoder used across all the language pairs,0.7564511895179749
Blogs,2,31,model,model,has,shared encoder and decoder,model has shared encoder and decoder,0.5717955231666565
Blogs,2,36,model,countering interference temperature based sampling strategy,to control,ratio of samples,countering interference temperature based sampling strategy to control ratio of samples,0.7332767844200134
Blogs,2,36,model,ratio of samples,from,different language pairs,ratio of samples from different language pairs,0.5722682476043701
Blogs,2,36,model,model,has,countering interference temperature based sampling strategy,model has countering interference temperature based sampling strategy,0.5567653179168701
Blogs,2,9,results,learning signal,from,one language,learning signal from one language,0.6143034100532532
Blogs,2,9,results,one language,should bene t,quality of other languages1,one language should bene t quality of other languages1,0.6983588337898254
Blogs,2,9,results,results,has,learning signal,results has learning signal,0.580795168876648
Blogs,2,10,results,positive transfer,evident for,low resource languages,positive transfer evident for low resource languages,0.6743696928024292
Blogs,2,10,results,positive transfer,tends to,hurt,positive transfer tends to hurt,0.7255526781082153
Blogs,2,10,results,performance,for,high resource languages,performance for high resource languages,0.5483009219169617
Blogs,2,10,results,hurt,has,performance,hurt has performance,0.6066785454750061
Blogs,2,10,results,results,has,positive transfer,results has positive transfer,0.581314742565155
Blogs,2,13,results,results,Maximize,positive transfer to low resource languages,results Maximize positive transfer to low resource languages,0.6921361684799194
Blogs,2,14,results,negative interference,to,high resource languages,negative interference to high resource languages,0.5509461164474487
Blogs,2,14,results,results,Minimize,negative interference,results Minimize negative interference,0.6660476922988892
Blogs,2,15,results,well,ion,"realistic , multi-domain settings","well ion realistic , multi-domain settings",0.5819395780563354
Blogs,2,15,results,results,Perform,well,results Perform well,0.6339512467384338
Blogs,2,19,results,existing datasets,is,much larger,existing datasets is much larger,0.5355120301246643
Blogs,2,19,results,much larger,spans,more domains,much larger spans more domains,0.6868695020675659
Blogs,2,19,results,good variation,in,amount of data,good variation in amount of data,0.51661217212677
Blogs,2,19,results,amount of data,available for,different language pairs,amount of data available for different language pairs,0.6257179975509644
Blogs,2,19,results,results,Compared with,existing datasets,results Compared with existing datasets,0.6791427135467529
Blogs,2,25,results,batch size,improves,model quality,batch size improves model quality,0.7382325530052185
Blogs,2,25,results,batch size,speeds up,convergence,batch size speeds up convergence,0.7891561388969421
Blogs,2,25,results,results,Increasing,batch size,results Increasing batch size,0.7211412787437439
Blogs,2,26,results,results,has,effect of transfer and interference,results has effect of transfer and interference,0.556171715259552
Blogs,2,34,results,all the languages,are,equally sampled,all the languages are equally sampled,0.5951716899871826
Blogs,2,34,results,performance,on,low resource languages,performance on low resource languages,0.49613523483276367
Blogs,2,34,results,performance,on,high resource languages,performance on high resource languages,0.49985426664352417
Blogs,2,34,results,performance,on,high resource languages,performance on high resource languages,0.49985426664352417
Blogs,2,34,results,low resource languages,cost of,performance,low resource languages cost of performance,0.6187626719474792
Blogs,2,34,results,performance,on,high resource languages,performance on high resource languages,0.49985426664352417
Blogs,2,34,results,all the languages,has,performance,all the languages has performance,0.5838314294815063
Blogs,2,35,results,all the data at once,reverse,trend,all the data at once reverse trend,0.7418773174285889
Blogs,2,35,results,results,Training over,all the data at once,results Training over all the data at once,0.6882694959640503
Blogs,2,37,results,balanced sampling strategy,improves,performance,balanced sampling strategy improves performance,0.7473190426826477
Blogs,2,37,results,performance,for,high resource languages,performance for high resource languages,0.5483009219169617
Blogs,2,37,results,high transfer performance,on,low resource languages,high transfer performance on low resource languages,0.5029382705688477
Blogs,2,37,results,results,has,balanced sampling strategy,results has balanced sampling strategy,0.5192322134971619
Blogs,2,38,results,lagging performance,compared to,bilingual baselines,lagging performance compared to bilingual baselines,0.6577686667442322
Blogs,2,38,results,lagging performance,capacity of,multilingual models,lagging performance capacity of multilingual models,0.7415095567703247
Blogs,2,38,results,results,reason behind,lagging performance,results reason behind lagging performance,0.7014743089675903
Blogs,2,46,results,nmt models,more amenable to,transfer,nmt models more amenable to transfer,0.7305302023887634
Blogs,2,46,results,nmt models,more amenable to,transfer,nmt models more amenable to transfer,0.7305302023887634
Blogs,2,46,results,transfer,across,multiple domains,transfer across multiple domains,0.7445250749588013
Blogs,2,46,results,transfer,than,transfer,transfer than transfer,0.6281726360321045
Blogs,2,46,results,transfer,across,tasks,transfer across tasks,0.6677939891815186
Blogs,2,46,results,transfer,across,tasks,transfer across tasks,0.6677939891815186
Blogs,2,46,results,results,has,nmt models,results has nmt models,0.48923376202583313
Blogs,2,47,results,performance,for,most language pairs,performance for most language pairs,0.5693984031677246
Blogs,2,47,results,most language pairs,increases as,number of languages,most language pairs increases as number of languages,0.5931782126426697
Blogs,2,47,results,change,from,10 to 102,change from 10 to 102,0.642787516117096
Blogs,2,47,results,zero-shot performance,has,performance,zero-shot performance has performance,0.5412192940711975
Blogs,2,47,results,number of languages,has,change,number of languages has change,0.5970616936683655
Blogs,2,47,results,results,In terms of,zero-shot performance,results In terms of zero-shot performance,0.6848838925361633
Blogs,2,48,results,results,has,effect of preprocessing and vocabulary sentence piece model ( spm ),results has effect of preprocessing and vocabulary sentence piece model ( spm ),0.528144121170044
Blogs,2,50,results,smaller vocabulary,perform,better,smaller vocabulary perform better,0.6157519817352295
Blogs,2,50,results,better,for,low resource languages,better for low resource languages,0.6477708220481873
Blogs,2,50,results,results,Using,smaller vocabulary,results Using smaller vocabulary,0.6768627166748047
Blogs,2,51,results,low and medium resource languages,tend to perform,better,low and medium resource languages tend to perform better,0.7529177069664001
Blogs,2,51,results,better,with,higher temperatures,better with higher temperatures,0.665896475315094
Blogs,2,51,results,results,has,low and medium resource languages,results has low and medium resource languages,0.5361381769180298
Blogs,2,52,results,effect of capacity,Using,deeper models,effect of capacity Using deeper models,0.7158061265945435
Blogs,2,52,results,performance,compared to,wider models,performance compared to wider models,0.6765374541282654
Blogs,2,52,results,wider models,on,most language pairs,wider models on most language pairs,0.5204942226409912
Blogs,2,52,results,results,has,effect of capacity,results has effect of capacity,0.5739011764526367
Blogs,3,71,ablation-analysis,our work,provides,theoretical explanation,our work provides theoretical explanation,0.6519317030906677
Blogs,3,71,ablation-analysis,ablation analysis,provides,theoretical explanation,ablation analysis provides theoretical explanation,0.6427978873252869
Blogs,3,71,ablation-analysis,ablation analysis,has,our work,ablation analysis has our work,0.5920498967170715
Blogs,3,18,experiments,perfect translator,in,two - to - one task,perfect translator in two - to - one task,0.5257994532585144
Blogs,3,23,experiments,multilingual system,train,decoder,multilingual system train decoder,0.7366184592247009
Blogs,3,23,experiments,decoder,takes,sentence representation,decoder takes sentence representation,0.6284964680671692
Blogs,3,12,hyperparameters,parallel sentences,for,each pair of languages,parallel sentences for each pair of languages,0.6067988276481628
Blogs,3,17,hyperparameters,each source language,assume,perfect translator,each source language assume perfect translator,0.6936457753181458
Blogs,3,17,hyperparameters,perfect translator,takes,"sentence ( or string , sequence )","perfect translator takes sentence ( or string , sequence )",0.618742823600769
Blogs,3,17,hyperparameters,perfect translator,outputs,corresponding translation,perfect translator outputs corresponding translation,0.7315378189086914
Blogs,3,17,hyperparameters,hyperparameters,for,each source language,hyperparameters for each source language,0.5263985991477966
Blogs,3,36,hyperparameters,hyperparameters,has,err l i ?l d i ( h,hyperparameters has err l i ?l d i ( h,0.6230158805847168
Blogs,3,50,hyperparameters,each encoder -decoder pair,consists of,deterministic mappings,each encoder -decoder pair consists of deterministic mappings,0.681641161441803
Blogs,3,5,model,shared   semantic space  ,between,multiple source and target languages,shared   semantic space   between multiple source and target languages,0.6412510275840759
Blogs,3,5,model,language - invariant structure,from,high- resource translation pairs,language - invariant structure from high- resource translation pairs,0.5469462275505066
Blogs,3,5,model,language - invariant structure,to transfer to,translation,language - invariant structure to transfer to translation,0.6649764180183411
Blogs,3,5,model,language - invariant structure,enable,zero-shot translation,language - invariant structure enable zero-shot translation,0.6829507946968079
Blogs,3,5,model,translation,between,lowresource language pairs,translation between lowresource language pairs,0.6677758097648621
Blogs,3,5,model,model,by learning,shared   semantic space  ,model by learning shared   semantic space  ,0.7009949088096619
Blogs,3,5,model,model,leverage,language - invariant structure,model leverage language - invariant structure,0.7315856218338013
Blogs,3,16,model,basic two - to - one setup,where,two source languages,basic two - to - one setup where two source languages,0.5999084711074829
Blogs,3,16,model,basic two - to - one setup,where,one target language,basic two - to - one setup where one target language,0.5922736525535583
Blogs,3,16,model,model,start with,basic two - to - one setup,model start with basic two - to - one setup,0.6921815872192383
Blogs,3,19,model,words,receiving,sentence,words receiving sentence,0.7381107211112976
Blogs,3,19,model,words,checks,source language,words checks source language,0.7000088095664978
Blogs,3,19,model,words,call,corresponding ground -truth translator,words call corresponding ground -truth translator,0.6004893183708191
Blogs,3,19,model,sentence,checks,source language,sentence checks source language,0.7087292671203613
Blogs,3,19,model,sentence,call,corresponding ground -truth translator,sentence call corresponding ground -truth translator,0.5869867205619812
Blogs,3,19,model,model,In,words,model In words,0.5487357378005981
Blogs,3,19,model,model,receiving,sentence,model receiving sentence,0.7546455264091492
Blogs,3,20,model,encoder,takes,sentence ( string ),encoder takes sentence ( string ),0.6443510055541992
Blogs,3,20,model,sentence ( string ),from,alphabet,sentence ( string ) from alphabet,0.5392113327980042
Blogs,3,20,model,sentence ( string ),to a representation,vector space,sentence ( string ) to a representation vector space,0.7285070419311523
Blogs,3,21,model,anuniversal language mapping,if,distributions,anuniversal language mapping if distributions,0.6038798093795776
Blogs,3,21,model,distributions,of,sentence representations,distributions of sentence representations,0.5746959447860718
Blogs,3,21,model,sentence representations,different languages and are,- close to each other,sentence representations different languages and are - close to each other,0.7024977207183838
Blogs,3,21,model,model,call,anuniversal language mapping,model call anuniversal language mapping,0.598953127861023
Blogs,3,22,model,induced distribution of sentence ( from ) representations,in,shared space,induced distribution of sentence ( from ) representations in shared space,0.5164206027984619
Blogs,3,22,model,model,for,divergence measure,model for divergence measure,0.6406885385513306
Blogs,3,24,model,language - invariant semantic information,about,input sentence,language - invariant semantic information about input sentence,0.541763424873352
Blogs,3,24,model,translate,to,target language,translate to target language,0.6355918049812317
Blogs,3,24,model,model,encodes,language - invariant semantic information,model encodes language - invariant semantic information,0.6888274550437927
Blogs,3,26,model,error term,measures,translation performance,error term measures translation performance,0.5446873307228088
Blogs,3,26,model,translation performance,given by,encoder-decoder pair,translation performance given by encoder-decoder pair,0.6447359323501587
Blogs,3,26,model,model,has,error term,model has error term,0.5017419457435608
Blogs,3,27,model,rst term,in,lower bound,rst term in lower bound,0.5234521627426147
Blogs,3,27,model,rst term,measures,difference of distributions,rst term measures difference of distributions,0.540123462677002
Blogs,3,27,model,lower bound,measures,difference of distributions,lower bound measures difference of distributions,0.49716874957084656
Blogs,3,27,model,difference of distributions,over,sentences,difference of distributions over sentences,0.6598880887031555
Blogs,3,27,model,sentences,from,target language,sentences from target language,0.5200644135475159
Blogs,3,27,model,model,has,rst term,model has rst term,0.516323447227478
Blogs,3,31,model,our lower bound,is,algorithm - independent,our lower bound is algorithm - independent,0.559824526309967
Blogs,3,31,model,our lower bound,holds,unbounded computation and data,our lower bound holds unbounded computation and data,0.6518471240997314
Blogs,3,31,model,model,worth pointing out,our lower bound,model worth pointing out our lower bound,0.6602360010147095
Blogs,3,32,model,smaller,larger,lower bound,smaller larger lower bound,0.7387474179267883
Blogs,3,32,model,xed distributions,has,smaller,xed distributions has smaller,0.6300016641616821
Blogs,3,32,model,model,for,xed distributions,model for xed distributions,0.6574989557266235
Blogs,3,35,model,theorem,in,more general many - to -many translation setting,theorem in more general many - to -many translation setting,0.5423939228057861
Blogs,3,37,model,decoder,to have access to,input sentences,decoder to have access to input sentences,0.6317488551139832
Blogs,3,37,model,input sentences,during,decoding process,input sentences during decoding process,0.6361117362976074
Blogs,3,37,model,model,allow,decoder,model allow decoder,0.766578197479248
Blogs,3,38,model,information ow,from,input sentences,information ow from input sentences,0.5424003601074219
Blogs,3,38,model,information ow,would break,markov structure,information ow would break markov structure,0.6370901465415955
Blogs,3,38,model,input sentences,during,decoding,input sentences during decoding,0.6778591871261597
Blogs,3,38,model,input sentences,would break,markov structure,input sentences would break markov structure,0.6821573972702026
Blogs,3,38,model,decoding,would break,markov structure,decoding would break markov structure,0.6919243335723877
Blogs,3,38,model,markov structure,of,input-representation - output,markov structure of input-representation - output,0.6124167442321777
Blogs,3,39,model,language - invariant ( hence languageindependent ) and language - dependent information,would be,used,language - invariant ( hence languageindependent ) and language - dependent information would be used,0.6314399838447571
Blogs,3,40,model,extra structure,on,distributions of our corpora,extra structure on distributions of our corpora,0.5261682271957397
Blogs,3,40,model,natural generative process,capturing,distribution,natural generative process capturing distribution,0.7145680785179138
Blogs,3,40,model,distribution,of,parallel corpora,distribution of parallel corpora,0.6220324635505676
Blogs,3,40,model,parallel corpora,used for,training,parallel corpora used for training,0.5903730988502502
Blogs,3,40,model,model,assume,extra structure,model assume extra structure,0.7481487989425659
Blogs,3,42,model,learnable,using,corpora,learnable using corpora,0.6492749452590942
Blogs,3,42,model,corpora,from,very small ( linear ) number of pairs,corpora from very small ( linear ) number of pairs,0.5560973882675171
Blogs,3,42,model,very small ( linear ) number of pairs,has,of language,very small ( linear ) number of pairs has of language,0.6013435125350952
Blogs,3,42,model,model,under,suitable generative model,model under suitable generative model,0.6200382113456726
Blogs,3,44,model,learnable,using,corpora,learnable using corpora,0.6492749452590942
Blogs,3,44,model,corpora,from,very small ( linear ) number of pairs of language,corpora from very small ( linear ) number of pairs of language,0.549989640712738
Blogs,3,44,model,model,discuss,generative model,model discuss generative model,0.5933947563171387
Blogs,3,46,model,what kind of generative model,suitable for,task of umt,what kind of generative model suitable for task of umt,0.7102017402648926
Blogs,3,46,model,what kind of generative model,to have,feature space,what kind of generative model to have feature space,0.6282382607460022
Blogs,3,46,model,feature space,where,vectors,feature space where vectors,0.6724320650100708
Blogs,3,46,model,vectors,correspond to,semantic encoding,vectors correspond to semantic encoding,0.663845956325531
Blogs,3,46,model,semantic encoding,of,sentences,semantic encoding of sentences,0.5766301155090332
Blogs,3,46,model,sentences,from,different languages,sentences from different languages,0.5434473156929016
Blogs,3,46,model,model,to have,feature space,model to have feature space,0.5875665545463562
Blogs,3,48,model,languagedependent decoders,take,semantic vectors,languagedependent decoders take semantic vectors,0.5675137042999268
Blogs,3,48,model,languagedependent decoders,decode them as,observable sentences,languagedependent decoders decode them as observable sentences,0.566925585269928
Blogs,3,48,model,semantic vectors,decode them as,observable sentences,semantic vectors decode them as observable sentences,0.520857036113739
Blogs,3,48,model,model,has,languagedependent decoders,model has languagedependent decoders,0.5598857998847961
Blogs,3,49,model,generative process,assume,common distribution,generative process assume common distribution,0.6774731874465942
Blogs,3,49,model,common distribution,over,feature space,common distribution over feature space,0.6898978352546692
Blogs,3,49,model,feature space,from which,parallel sentences,feature space from which parallel sentences,0.5978883504867554
Blogs,3,49,model,parallel sentences,are,sampled and generated,parallel sentences are sampled and generated,0.6063607335090637
Blogs,3,53,model,complexity measure,of,class,complexity measure of class,0.5890491604804993
Blogs,3,56,model,star graph,where,central ( high - resource ) language,star graph where central ( high - resource ) language,0.6410307288169861
Blogs,3,56,model,central ( high - resource ) language,acts as,pivot node,central ( high - resource ) language acts as pivot node,0.6715970635414124
Blogs,3,56,model,model,could be,star graph,model could be star graph,0.6722646951675415
Blogs,3,57,model,epsilon-net argument,to show,learned encoders / decoders,epsilon-net argument to show learned encoders / decoders,0.6556297540664673
Blogs,3,57,model,learned encoders / decoders,using,connectivity,learned encoders / decoders using connectivity,0.7117626070976257
Blogs,3,57,model,generalize,on,pair of language,generalize on pair of language,0.5961078405380249
Blogs,3,57,model,pair of language,appears in,training corpora,pair of language appears in training corpora,0.6254323124885559
Blogs,3,57,model,connectivity,of,graph,connectivity of graph,0.5887885093688965
Blogs,3,57,model,connectivity,apply,chain of trianglelike inequalities,connectivity apply chain of trianglelike inequalities,0.5639869570732117
Blogs,3,57,model,chain of trianglelike inequalities,to bound,error,chain of trianglelike inequalities to bound error,0.788141667842865
Blogs,3,57,model,error,along,path,error along path,0.7464859485626221
Blogs,3,57,model,learned encoders / decoders,has,generalize,learned encoders / decoders has generalize,0.580045759677887
Blogs,3,57,model,model,use,epsilon-net argument,model use epsilon-net argument,0.656154990196228
Blogs,3,57,model,model,using,connectivity,model using connectivity,0.7157583832740784
Blogs,3,57,model,model,apply,chain of trianglelike inequalities,model apply chain of trianglelike inequalities,0.6080732345581055
Blogs,3,61,model,some common knowledge,in,translation,some common knowledge in translation,0.5347239375114441
Blogs,3,61,model,translation,from,high- resource languages,translation from high- resource languages,0.5724966526031494
Blogs,3,61,model,high- resource languages,to,low-resource ones,high- resource languages to low-resource ones,0.5551877021789551
Blogs,3,61,model,transfer,has,some common knowledge,transfer has some common knowledge,0.5866192579269409
Blogs,3,63,model,simple setup,allows for,zeroshot translation,simple setup allows for zeroshot translation,0.7052805423736572
Blogs,3,63,model,model,proposes,simple setup,model proposes simple setup,0.7174550890922546
Blogs,3,66,model,generative modeling,of,sentences,generative modeling of sentences,0.5906497240066528
Blogs,3,66,model,more hierarchical structures,in,semantic space,more hierarchical structures in semantic space,0.515567421913147
Blogs,3,67,model,global distribution,over,representation space,global distribution over representation space,0.6933153867721558
Blogs,3,67,model,representation space,from which,sentences of language,representation space from which sentences of language,0.5948994159698486
Blogs,3,67,model,sentences of language,generated,decoder,sentences of language generated decoder,0.6585246920585632
Blogs,3,68,model,sentences,encoded,to,sentences encoded to,0.7680169343948364
Blogs,3,68,model,model,has,sentences,model has sentences,0.609123945236206
Blogs,3,9,results,translation quality,over,high- resource language pairs,translation quality over high- resource language pairs,0.6430347561836243
Blogs,3,9,results,translation quality,worse than,corresponding bilingual baselines,translation quality worse than corresponding bilingual baselines,0.6985393762588501
Blogs,3,9,results,high- resource language pairs,by using,single umt system,high- resource language pairs by using single umt system,0.6530569195747375
Blogs,3,9,results,single umt system,worse than,corresponding bilingual baselines,single umt system worse than corresponding bilingual baselines,0.6835355162620544
Blogs,3,9,results,results,observe that,translation quality,results observe that translation quality,0.5073474645614624
Blogs,3,10,results,well,on,any unseen language pair,well on any unseen language pair,0.5960777997970581
Blogs,3,10,results,results,perform,well,results perform well,0.6339512467384338
Blogs,3,13,results,single model,performs,well,single model performs well,0.6884297728538513
Blogs,3,13,results,well,on,all pairs of translation tasks,well on all pairs of translation tasks,0.5271298289299011
Blogs,3,13,results,all pairs of translation tasks,based on,common representation space,all pairs of translation tasks based on common representation space,0.6399898529052734
Blogs,3,13,results,results,train,single model,results train single model,0.6220058798789978
Blogs,3,14,results,general dataprocessing principle,if,representation,general dataprocessing principle if representation,0.6097530126571655
Blogs,3,14,results,representation,invariant to,multiple source languages,representation invariant to multiple source languages,0.6854922771453857
Blogs,3,14,results,any decoder,have to generate,same language model,any decoder have to generate same language model,0.7045940160751343
Blogs,3,14,results,same language model,on,target language,same language model on target language,0.5623134970664978
Blogs,3,14,results,results,due to,general dataprocessing principle,results due to general dataprocessing principle,0.6971725821495056
Blogs,3,15,results,parallel corpora,to train,system,parallel corpora to train system,0.6754946708679199
Blogs,3,15,results,system,could have,drastically different sentence distributions,system could have drastically different sentence distributions,0.5980557799339294
Blogs,3,15,results,drastically different sentence distributions,on,target language,drastically different sentence distributions on target language,0.5234315991401672
Blogs,3,15,results,drastically different sentence distributions,leading to,discrepancy ( error ),drastically different sentence distributions leading to discrepancy ( error ),0.6970969438552856
Blogs,3,15,results,discrepancy ( error ),between,generated sentence distribution,discrepancy ( error ) between generated sentence distribution,0.6285990476608276
Blogs,3,25,results,perfect translator,by learning,"common , shared representation","perfect translator by learning common , shared representation",0.6843472123146057
Blogs,3,25,results,results,recover,perfect translator,results recover perfect translator,0.6373741030693054
Blogs,3,29,results,corresponding sentence distributions,from,english,corresponding sentence distributions from english,0.5816279649734497
Blogs,3,29,results,english,are,quite different,english are quite different,0.6018775105476379
Blogs,3,29,results,quite different,between,different corpora,quite different between different corpora,0.6492763161659241
Blogs,3,29,results,quite different,leading to,large lower bound,quite different leading to large lower bound,0.7071731090545654
Blogs,3,29,results,different corpora,leading to,large lower bound,different corpora leading to large lower bound,0.6653251051902771
Blogs,3,30,results,our theorem,interpreted as,uncertainty principle,our theorem interpreted as uncertainty principle,0.5886032581329346
Blogs,3,30,results,uncertainty principle,in,umt,uncertainty principle in umt,0.5643632411956787
Blogs,3,30,results,large error,on,at least one of the translation pairs,large error on at least one of the translation pairs,0.5061614513397217
Blogs,3,30,results,results,has,our theorem,results has our theorem,0.620058000087738
Blogs,3,45,results,quadratic number of translation pairs,in,our universe,quadratic number of translation pairs in our universe,0.4932307004928589
Blogs,3,45,results,generative model,has,zero-shot translation,generative model has zero-shot translation,0.5272420048713684
Blogs,3,45,results,zero-shot translation,has,is actually possible,zero-shot translation has is actually possible,0.6216161847114563
Blogs,3,51,results,generative model assumption,circumvent,previous lower bound,generative model assumption circumvent previous lower bound,0.7008321285247803
Blogs,3,51,results,rst term,gracefully reduces to,0,rst term gracefully reduces to 0,0.7338134050369263
Blogs,3,51,results,encoder-decoder generative assumption,has,rst term,encoder-decoder generative assumption has rst term,0.5284778475761414
Blogs,3,51,results,results,under,encoder-decoder generative assumption,results under encoder-decoder generative assumption,0.6671162843704224
Blogs,3,52,results,class of encoders and decoders,by using,traditional empirical risk minimization ( erm ) framework,class of encoders and decoders by using traditional empirical risk minimization ( erm ) framework,0.6246401071548462
Blogs,3,52,results,traditional empirical risk minimization ( erm ) framework,to learn,language - dependent encoders and decoders,traditional empirical risk minimization ( erm ) framework to learn language - dependent encoders and decoders,0.5762615203857422
Blogs,3,52,results,language - dependent encoders and decoders,on,small number of language pairs,language - dependent encoders and decoders on small number of language pairs,0.5147038698196411
Blogs,3,54,results,path length,diameter of,graph,path length diameter of graph,0.7307369112968445
Blogs,3,55,results,graphs,do not have,long paths,graphs do not have long paths,0.6108039021492004
Blogs,3,55,results,long paths,are,preferable,long paths are preferable,0.6289129257202148
Blogs,3,64,results,still some gaps,between,theory and practice,still some gaps between theory and practice,0.6746702790260315
Blogs,3,69,results,translation quality,by using,single massively multilingual model,translation quality by using single massively multilingual model,0.6186296343803406
Blogs,3,69,results,single massively multilingual model,against,bilingual baselines,single massively multilingual model against bilingual baselines,0.6773343086242676
Blogs,3,70,results,translation performances,over,low resource languages,translation performances over low resource languages,0.600532054901123
Blogs,3,70,results,translation performances,over,high resource languages,translation performances over high resource languages,0.6119286417961121
Blogs,3,70,results,translation performances,over,high resource languages,translation performances over high resource languages,0.6119286417961121
Blogs,3,70,results,performances,over,high resource languages,performances over high resource languages,0.6183320879936218
Blogs,4,24,ablation-analysis,values,marked in,bold,values marked in bold,0.7388184666633606
Blogs,4,24,ablation-analysis,bold,as,best ones,bold as best ones,0.5673330426216125
Blogs,4,24,ablation-analysis,best ones,in,row,best ones in row,0.5944523215293884
Blogs,4,24,ablation-analysis,best ones,do not appear to be,so,best ones do not appear to be so,0.6569792628288269
Blogs,4,42,ablation-analysis,final mbart25 checkpoint,with,25 additional languages,final mbart25 checkpoint with 25 additional languages,0.6212371587753296
Blogs,4,42,ablation-analysis,ablation analysis,extend,final mbart25 checkpoint,ablation analysis extend final mbart25 checkpoint,0.7234264016151428
Blogs,4,64,ablation-analysis,interesting question,reason of,high fluctuations,interesting question reason of high fluctuations,0.7083738446235657
Blogs,4,64,ablation-analysis,more languages,on,positive side,more languages on positive side,0.5842710733413696
Blogs,4,64,ablation-analysis,move,has,more languages,move has more languages,0.6307321190834045
Blogs,4,65,ablation-analysis,strong multilingual models,for,very low-resourced langauge pairs,strong multilingual models for very low-resourced langauge pairs,0.5971938371658325
Blogs,4,68,ablation-analysis,mbart50,seems that,degradation,mbart50 seems that degradation,0.69204181432724
Blogs,4,68,ablation-analysis,degradation,has,does not occur anymore,degradation has does not occur anymore,0.643578290939331
Blogs,4,68,ablation-analysis,ablation analysis,With,mbart50,ablation analysis With mbart50,0.6450226902961731
Blogs,4,5,baselines,mbart,to,50 languages,mbart to 50 languages,0.6440814733505249
Blogs,4,5,baselines,mbart,with no loss,accuracy,mbart with no loss accuracy,0.7947306036949158
Blogs,4,5,baselines,50 languages,with no loss,accuracy,50 languages with no loss accuracy,0.8112354278564453
Blogs,4,5,baselines,accuracy,on,bilingual fine-tuning,accuracy on bilingual fine-tuning,0.5707936882972717
Blogs,4,6,baselines,fine-tuning,for,"many- to- english , english - to - many and many - to - many","fine-tuning for many- to- english , english - to - many and many - to - many",0.6006225943565369
Blogs,4,15,baselines,last one,combining,data,last one combining data,0.7399538159370422
Blogs,4,15,baselines,baselines,has,last one,baselines has last one,0.5884065628051758
Blogs,4,35,baselines,mbart50,has,extending a pretrained model,mbart50 has extending a pretrained model,0.6116924285888672
Blogs,4,35,baselines,baselines,has,mbart50,baselines has mbart50,0.5992350578308105
Blogs,4,14,experiments,multilingual fine-tuning,in,three different settings,multilingual fine-tuning in three different settings,0.4808228611946106
Blogs,4,17,experiments,25 languages,to train,mbart,25 languages to train mbart,0.6565479636192322
Blogs,4,31,experiments,good conditional probabilities,for,all the language directions,good conditional probabilities for all the language directions,0.5939210653305054
Blogs,4,34,experiments,more experiments,if,mbart,more experiments if mbart,0.6498460173606873
Blogs,4,34,experiments,mbart,solves,problem,mbart solves problem,0.708459734916687
Blogs,4,46,experiments,same sentencepiece model,for,word segmentation,same sentencepiece model for word segmentation,0.6104790568351746
Blogs,4,53,experiments,multilingual fine-tuning,of,mbart50,multilingual fine-tuning of mbart50,0.5938650369644165
Blogs,4,57,experiments,translation quality,on,downstream task,translation quality on downstream task,0.5411384701728821
Blogs,4,57,experiments,any quality metrics,of,pretrained model,any quality metrics of pretrained model,0.5389686822891235
Blogs,4,58,experiments,experiments,has,is too expensive,experiments has is too expensive,0.5947850942611694
Blogs,4,10,hyperparameters,data,for,all the language directions s->t,data for all the language directions s->t,0.6522729396820068
Blogs,4,10,hyperparameters,all the language directions s->t,collected in,single training set,all the language directions s->t collected in single training set,0.6623467803001404
Blogs,4,10,hyperparameters,hyperparameters,has,data,hyperparameters has data,0.5382587313652039
Blogs,4,12,hyperparameters,data,for,each batch,data for each batch,0.6627724766731262
Blogs,4,12,hyperparameters,each batch,chosen from,different sets,each batch chosen from different sets,0.650580883026123
Blogs,4,12,hyperparameters,different sets,using,temperature sampling,different sets using temperature sampling,0.7114787697792053
Blogs,4,12,hyperparameters,temperature sampling,on,language directions,temperature sampling on language directions,0.557826817035675
Blogs,4,12,hyperparameters,hyperparameters,has,data,hyperparameters has data,0.5382587313652039
Blogs,4,44,hyperparameters,25 randomly - initialized language tokens,in,embedding layers,25 randomly - initialized language tokens in embedding layers,0.48056843876838684
Blogs,4,3,model,pretrained mbart,to train,strong bilingual machine translation models,pretrained mbart to train strong bilingual machine translation models,0.6337023377418518
Blogs,4,13,model,temperature sampling,produces,more balanced distribution,temperature sampling produces more balanced distribution,0.6185231804847717
Blogs,4,13,model,more balanced distribution,so that,low-resourced language pairs,more balanced distribution so that low-resourced language pairs,0.6304807066917419
Blogs,4,13,model,low-resourced language pairs,chosen with,higher probability,low-resourced language pairs chosen with higher probability,0.6985710859298706
Blogs,4,39,model,pretraining languages,constrain,possible downstream tasks,pretraining languages constrain possible downstream tasks,0.7270784974098206
Blogs,4,39,model,possible downstream tasks,to,languages,possible downstream tasks to languages,0.5902543067932129
Blogs,4,39,model,model,has,pretraining languages,model has pretraining languages,0.5303300023078918
Blogs,4,40,model,mbart,with,new languages,mbart with new languages,0.685225248336792
Blogs,4,40,model,mbart,important to have,more monolingual data,mbart important to have more monolingual data,0.6432942152023315
Blogs,4,40,model,effectively,with,new languages,effectively with new languages,0.6735326647758484
Blogs,4,40,model,more monolingual data,be added to,pretrained model,more monolingual data be added to pretrained model,0.6044198870658875
Blogs,4,40,model,mbart,has,effectively,mbart has effectively,0.6302640438079834
Blogs,4,40,model,model,to use,mbart,model to use mbart,0.7562999725341797
Blogs,4,45,model,monolingual training data,of,original 25 languages,monolingual training data of original 25 languages,0.5311100482940674
Blogs,4,45,model,monolingual training data,data of,25 new languages,monolingual training data data of 25 new languages,0.634168803691864
Blogs,4,45,model,monolingual training data,use,resulting training set,monolingual training data use resulting training set,0.5915929079055786
Blogs,4,45,model,resulting training set,to continue,training,resulting training set to continue training,0.7163649201393127
Blogs,4,45,model,training,of,saved checkpoint,training of saved checkpoint,0.6164856553077698
Blogs,4,45,model,model,concatenate,monolingual training data,model concatenate monolingual training data,0.6868217587471008
Blogs,4,47,model,some data filtering,filtering out,sentences,some data filtering filtering out sentences,0.805326521396637
Blogs,4,47,model,preventing any overlap,between,train and dev/test sets,preventing any overlap between train and dev/test sets,0.682903528213501
Blogs,4,47,model,sentences,not recognized as in,correct language,sentences not recognized as in correct language,0.6870444416999817
Blogs,4,47,model,correct language,by,fasttext,correct language by fasttext,0.5839088559150696
Blogs,4,47,model,model,perform,some data filtering,model perform some data filtering,0.6734548211097717
Blogs,4,47,model,model,filtering out,sentences,model filtering out sentences,0.771816611289978
Blogs,4,55,model,old and new data,merged into,new training set,old and new data merged into new training set,0.7473958134651184
Blogs,4,55,model,new training set,to obtain,new model,new training set to obtain new model,0.6184636950492859
Blogs,4,55,model,new model,does n't,degrade,new model does n't degrade,0.6606228947639465
Blogs,4,55,model,degrade,on,old languages,degrade on old languages,0.5833361744880676
Blogs,4,55,model,model,has,old and new data,model has old and new data,0.5836465954780579
Blogs,4,71,model,wave 2ve c2.0,to cope with,speech input,wave 2ve c2.0 to cope with speech input,0.5651401281356812
Blogs,4,71,model,mbart,for,translation side,mbart for translation side,0.6218831539154053
Blogs,4,71,model,model,powered by,wave 2ve c2.0,model powered by wave 2ve c2.0,0.5565928220748901
Blogs,4,7,results,particularly large,for,many- to-english direction,particularly large for many- to-english direction,0.6585219502449036
Blogs,4,7,results,many- to-english direction,Provide,ml50 benchmark,many- to-english direction Provide ml50 benchmark,0.5867437124252319
Blogs,4,7,results,ml50 benchmark,with,standardized train / dev/ test split,ml50 benchmark with standardized train / dev/ test split,0.6388564109802246
Blogs,4,7,results,standardized train / dev/ test split,covers,"low- , mid- , and high- resourced languages","standardized train / dev/ test split covers low- , mid- , and high- resourced languages",0.7559597492218018
Blogs,4,7,results,"low- , mid- , and high- resourced languages",total of,230 m parallel sentences,"low- , mid- , and high- resourced languages total of 230 m parallel sentences",0.6370880007743835
Blogs,4,7,results,results,Provide,ml50 benchmark,results Provide ml50 benchmark,0.5512425303459167
Blogs,4,7,results,results,has,gains,results has gains,0.5357105135917664
Blogs,4,11,results,amount of text,available for,each language direction,amount of text available for each language direction,0.612785279750824
Blogs,4,11,results,each language direction,is,extremely diverse,each language direction is extremely diverse,0.5825856328010559
Blogs,4,11,results,extremely diverse,covering,full spectrum,extremely diverse covering full spectrum,0.8262941241264343
Blogs,4,11,results,full spectrum,has,from 4 k to more than 10 m sentence pairs per language pair,full spectrum has from 4 k to more than 10 m sentence pairs per language pair,0.584214448928833
Blogs,4,11,results,results,has,amount of text,results has amount of text,0.541439950466156
Blogs,4,18,results,many- to - english direction,has,multilingual models,many- to - english direction has multilingual models,0.5676255822181702
Blogs,4,18,results,multilingual models,has,clearly outperform,multilingual models has clearly outperform,0.5905066728591919
Blogs,4,18,results,clearly outperform,has,all the bilingual models,clearly outperform has all the bilingual models,0.5949442982673645
Blogs,4,18,results,results,on,many- to - english direction,results on many- to - english direction,0.5168336629867554
Blogs,4,19,results,multilingual fine-tuning,is,significantly better,multilingual fine-tuning is significantly better,0.5497313737869263
Blogs,4,19,results,significantly better,than,multilingual from scratch,significantly better than multilingual from scratch,0.6018980145454407
Blogs,4,19,results,multilingual from scratch,except for,language directions,multilingual from scratch except for language directions,0.6053822040557861
Blogs,4,19,results,language directions,with,more than 1 m sentence pairs,language directions with more than 1 m sentence pairs,0.6364822387695312
Blogs,4,19,results,results,has,multilingual fine-tuning,results has multilingual fine-tuning,0.5445360541343689
Blogs,4,20,results,less-resourced languages ( 7k - 30 k sentence pairs ),advantage of,multilingual fine-tuning,less-resourced languages ( 7k - 30 k sentence pairs ) advantage of multilingual fine-tuning,0.6785184144973755
Blogs,4,20,results,multilingual fine-tuning,over,bilingual from scratch,multilingual fine-tuning over bilingual from scratch,0.6234106421470642
Blogs,4,20,results,multilingual fine-tuning,is,outstanding 18.03 average bleu points,multilingual fine-tuning is outstanding 18.03 average bleu points,0.5138528347015381
Blogs,4,20,results,bilingual from scratch,is,outstanding 18.03 average bleu points,bilingual from scratch is outstanding 18.03 average bleu points,0.5051928758621216
Blogs,4,20,results,results,for,less-resourced languages ( 7k - 30 k sentence pairs ),results for less-resourced languages ( 7k - 30 k sentence pairs ),0.5780728459358215
Blogs,4,21,results,second best model,is,multilingual from scratch,second best model is multilingual from scratch,0.5293899178504944
Blogs,4,21,results,multilingual from scratch,with,improvement,multilingual from scratch with improvement,0.6483651995658875
Blogs,4,21,results,improvement,of,14.63,improvement of 14.63,0.5616143941879272
Blogs,4,21,results,improvement,over,bilingual from scratch baseline,improvement over bilingual from scratch baseline,0.6841922998428345
Blogs,4,21,results,stops,after,10.80 bleu points of improvement,stops after 10.80 bleu points of improvement,0.6286526918411255
Blogs,4,21,results,10.80 bleu points of improvement,over,bilingual from scratch baseline,10.80 bleu points of improvement over bilingual from scratch baseline,0.6150636076927185
Blogs,4,21,results,bilingual fine-tuning,has,stops,bilingual fine-tuning has stops,0.5853663086891174
Blogs,4,23,results,results,do not fully match,table,results do not fully match table,0.6898517608642578
Blogs,4,23,results,table,received,less care,table received less care,0.5040717720985413
Blogs,4,23,results,less care,than,previous ones,less care than previous ones,0.6050628423690796
Blogs,4,23,results,results,do not fully match,table,results do not fully match table,0.6898517608642578
Blogs,4,25,results,multilingual fine-tuning,achieves,best result,multilingual fine-tuning achieves best result,0.6536696553230286
Blogs,4,25,results,best result,by,strict margin,best result by strict margin,0.5724344253540039
Blogs,4,25,results,best result,only on,>10 m sentence pairs group,best result only on >10 m sentence pairs group,0.6208361387252808
Blogs,4,25,results,other groups,multilingual from scratch,english - to - many,other groups multilingual from scratch english - to - many,0.6369182467460632
Blogs,4,27,results,many - to - many model,with,multilingual fine- tuning,many - to - many model with multilingual fine- tuning,0.6101033091545105
Blogs,4,27,results,many - to - many model,is,0.90 better,many - to - many model is 0.90 better,0.5413343906402588
Blogs,4,27,results,many - to - many model,only,0.90 better,many - to - many model only 0.90 better,0.689270555973053
Blogs,4,27,results,0.90 better,than,bilingual from scratch baseline,0.90 better than bilingual from scratch baseline,0.5452805757522583
Blogs,4,27,results,advantage,of,7.9 bleu points,advantage of 7.9 bleu points,0.5660103559494019
Blogs,4,27,results,7.9 bleu points,over,baseline,7.9 bleu points over baseline,0.6351359486579895
Blogs,4,27,results,least-resourced group,has,many - to - many model,least-resourced group has many - to - many model,0.5851006507873535
Blogs,4,27,results,multilingual from scratch,has,advantage,multilingual from scratch has advantage,0.6129812598228455
Blogs,4,27,results,results,on,least-resourced group,results on least-resourced group,0.47787564992904663
Blogs,4,28,results,improvement,for,multilingual fine-tuning model,improvement for multilingual fine-tuning model,0.5543463826179504
Blogs,4,28,results,multilingual fine-tuning model,is,much better,multilingual fine-tuning model is much better,0.507372260093689
Blogs,4,28,results,multilingual fine-tuning model,is,still slightly worse,multilingual fine-tuning model is still slightly worse,0.5163258910179138
Blogs,4,28,results,much better,on,one - to - many scenario,much better on one - to - many scenario,0.5692911744117737
Blogs,4,28,results,still slightly worse,than,counterpart,still slightly worse than counterpart,0.5946731567382812
Blogs,4,28,results,counterpart,has,trained from scratch,counterpart has trained from scratch,0.5910282135009766
Blogs,4,28,results,trained from scratch,has,7.6 vs 8.1,trained from scratch has 7.6 vs 8.1,0.5371496081352234
Blogs,4,28,results,results,has,improvement,results has improvement,0.6248279809951782
Blogs,4,29,results,results,on,one-to - many,results on one-to - many,0.5298190116882324
Blogs,4,29,results,one-to - many,where,training from scratch,one-to - many where training from scratch,0.5673898458480835
Blogs,4,29,results,training from scratch,better than,fine-tuning,training from scratch better than fine-tuning,0.7184285521507263
Blogs,4,29,results,results,on,one-to - many,results on one-to - many,0.5298190116882324
Blogs,4,30,results,many- to- english direction,is,so strong,many- to- english direction is so strong,0.5685216784477234
Blogs,4,30,results,results,has,many- to- english direction,results has many- to- english direction,0.5360203981399536
Blogs,4,32,results,multilingual systems,show,improvements,multilingual systems show improvements,0.631823718547821
Blogs,4,32,results,improvements,over,bilingual baselines,improvements over bilingual baselines,0.7121249437332153
Blogs,4,32,results,improvements,in,whole data size spectrum,improvements in whole data size spectrum,0.5271788239479065
Blogs,4,32,results,results,interesting that,multilingual systems,results interesting that multilingual systems,0.6840654611587524
Blogs,4,33,results,bilingual models,has,for high- resourced languages,bilingual models has for high- resourced languages,0.4934079647064209
Blogs,4,33,results,results,has,multilingual,results has multilingual,0.5869664549827576
Blogs,4,36,results,mbart,to be,fine-tuned,mbart to be fine-tuned,0.5691395998001099
Blogs,4,36,results,fine-tuned,on,language pairs,fine-tuned on language pairs,0.5217973589897156
Blogs,4,36,results,language pairs,with,at least one unseen language,language pairs with at least one unseen language,0.6385374069213867
Blogs,4,37,results,results,not as good as for,pretraining languages,results not as good as for pretraining languages,0.635654091835022
Blogs,4,37,results,results,has,results,results has results,0.48582205176353455
Blogs,4,38,results,particularly bad,when,unseen language,particularly bad when unseen language,0.6987404823303223
Blogs,4,38,results,unseen language,is,on the source side,unseen language is on the source side,0.5872467160224915
Blogs,4,38,results,results,has,degradation,results has degradation,0.5088335871696472
Blogs,4,43,results,procedure,is,conceptually simple but effective,procedure is conceptually simple but effective,0.4871668815612793
Blogs,4,43,results,results,has,procedure,results has procedure,0.44952884316444397
Blogs,4,48,results,multilingual fine- tuning,of,new pretrained model,multilingual fine- tuning of new pretrained model,0.49643567204475403
Blogs,4,48,results,new pretrained model,leads to,huge improvements,new pretrained model leads to huge improvements,0.6514220237731934
Blogs,4,48,results,huge improvements,particularly for,low-resourced languages,huge improvements particularly for low-resourced languages,0.6089451909065247
Blogs,4,48,results,results,has,multilingual fine- tuning,results has multilingual fine- tuning,0.5445360541343689
Blogs,4,49,results,improvements,in,many - to - one direction,improvements in many - to - one direction,0.5659635066986084
Blogs,4,49,results,not as performant,as,biligual fine-tuning,not as performant as biligual fine-tuning,0.5908287167549133
Blogs,4,49,results,not as performant,except for,very low-resourced language pairs,not as performant except for very low-resourced language pairs,0.6667435169219971
Blogs,4,49,results,results,has,improvements,results has improvements,0.615561842918396
Blogs,4,50,results,bilingual fine-tuning,of,mbart50,bilingual fine-tuning of mbart50,0.6311668753623962
Blogs,4,50,results,mbart50,on,original 25 languages,mbart50 on original 25 languages,0.5710871815681458
Blogs,4,50,results,bilingual fine-tuning,has,results,bilingual fine-tuning has results,0.5488776564598083
Blogs,4,50,results,mbart50,has,results,mbart50 has results,0.5326101183891296
Blogs,4,50,results,results,when performing,bilingual fine-tuning,results when performing bilingual fine-tuning,0.6914582252502441
Blogs,4,51,results,number of languages,lead to,performance degradation,number of languages lead to performance degradation,0.5984036922454834
Blogs,4,51,results,performance degradation,on,any language pair,performance degradation on any language pair,0.5585984587669373
Blogs,4,51,results,results,higher,number of languages,results higher number of languages,0.7117339968681335
Blogs,4,56,results,similar results,do n't have access to,original data,similar results do n't have access to original data,0.6481143832206726
Blogs,4,56,results,mbart25 and mbart50,obtain,same results,mbart25 and mbart50 obtain same results,0.6196328401565552
Blogs,4,56,results,same results,on,bilingual fine-tuning,same results on bilingual fine-tuning,0.5678694844245911
Blogs,4,56,results,bilingual fine-tuning,for,original 25 languages,bilingual fine-tuning for original 25 languages,0.5588754415512085
Blogs,4,56,results,model quality,does n't,degrade,model quality does n't degrade,0.5930757522583008
Blogs,4,56,results,results,achieve,similar results,results achieve similar results,0.5849912166595459
Blogs,4,59,results,averaged,among,many languages,averaged among many languages,0.6560547351837158
Blogs,4,59,results,number of languages,has,results,number of languages has results,0.5680010318756104
Blogs,4,59,results,results,given,number of languages,results given number of languages,0.663888156414032
Blogs,4,60,results,averages,see,large improvements,averages see large improvements,0.6300891041755676
Blogs,4,60,results,averages,see,slight degradation,averages see slight degradation,0.6357818841934204
Blogs,4,60,results,large improvements,for,many - to - 1,large improvements for many - to - 1,0.6683967709541321
Blogs,4,60,results,slight degradation,for,1 - to - many,slight degradation for 1 - to - many,0.6893665194511414
Blogs,4,60,results,results,From,averages,results From averages,0.5573199391365051
Blogs,4,61,results,individual languages,show,complex picture,individual languages show complex picture,0.6358967423439026
Blogs,4,61,results,results,on,individual languages,results on individual languages,0.4628223478794098
Blogs,4,62,results,improvements,are,significant,improvements are significant,0.6254523992538452
Blogs,4,62,results,significant,for,basically all languages,significant for basically all languages,0.6057637333869934
Blogs,4,62,results,basically all languages,with,many - to - 1 model,basically all languages with many - to - 1 model,0.6387140154838562
Blogs,4,62,results,slightly - below - zero average degradation,for,1 - to - many model,slightly - below - zero average degradation for 1 - to - many model,0.6713650822639465
Blogs,4,62,results,results,has,improvements,results has improvements,0.615561842918396
Blogs,4,63,results,difference,with,baseline,difference with baseline,0.6552605032920837
Blogs,4,63,results,difference,as high as,+- 4 bleu points,difference as high as +- 4 bleu points,0.5020025372505188
Blogs,4,63,results,single languages,has,difference,single languages has difference,0.6375792622566223
Blogs,4,63,results,results,For,single languages,results For single languages,0.578787088394165
Blogs,4,66,results,multilingual models,offer,maintainance advantages,multilingual models offer maintainance advantages,0.6936686038970947
Blogs,4,66,results,maintainance advantages,over,plethora of models,maintainance advantages over plethora of models,0.6869356036186218
Blogs,4,66,results,plethora of models,has,in production,plethora of models has in production,0.5733722448348999
Blogs,4,66,results,results,has,multilingual models,results has multilingual models,0.5098084211349487
Blogs,4,67,results,traded - off,with,lower quality,traded - off with lower quality,0.6965615153312683
Blogs,4,67,results,lower quality,for,highest - resourced languages,lower quality for highest - resourced languages,0.6076067686080933
Blogs,4,69,results,experiments,run only on,single test,experiments run only on single test,0.718134880065918
Blogs,4,69,results,experiments,would,great,experiments would great,0.695106029510498
Blogs,4,69,results,experiments,be,great,experiments be great,0.6175203919410706
Blogs,4,69,results,single test,for,each language direction,single test for each language direction,0.630020260810852
Blogs,4,70,results,outperformed,has,significantly,outperformed has significantly,0.6861791014671326
Blogs,4,70,results,significantly,has,other participants,significantly has other participants,0.5517651438713074
Blogs,4,70,results,results,analyze,facebook 's submission,results analyze facebook 's submission,0.5838387608528137
Blogs,5,57,ablation-analysis,context vector c ( t ),calculated differently,local and global attention,context vector c ( t ) calculated differently local and global attention,0.6999114155769348
Blogs,5,57,ablation-analysis,ablation analysis,has,context vector c ( t ),ablation analysis has context vector c ( t ),0.5542873740196228
Blogs,5,17,baselines,hidden state h( t ),at,top layer,hidden state h( t ) at top layer,0.5344380140304565
Blogs,5,17,baselines,top layer,of,stacking lstm,top layer of stacking lstm,0.5457148551940918
Blogs,5,19,baselines,h_bar ( t ),from,simple concatenation of h( t ) and c ( t ),h_bar ( t ) from simple concatenation of h( t ) and c ( t ),0.5873817801475525
Blogs,5,30,baselines,"alignment vector ( a( t , s ) )",defined as,score,"alignment vector ( a( t , s ) ) defined as score",0.5949994921684265
Blogs,5,30,baselines,score,is,content - based function,score is content - based function,0.5824162364006042
Blogs,5,30,baselines,baselines,has,"alignment vector ( a( t , s ) )","baselines has alignment vector ( a( t , s ) )",0.5619006752967834
Blogs,5,62,baselines,monotonic alignment ( local -m ),Set,p ( t ) =t,monotonic alignment ( local -m ) Set p ( t ) =t,0.6642934083938599
Blogs,5,62,baselines,baselines,has,monotonic alignment ( local -m ),baselines has monotonic alignment ( local -m ),0.5677368640899658
Blogs,5,69,baselines,local attention,has,2 flavors,local attention has 2 flavors,0.5888307094573975
Blogs,5,69,baselines,2 flavors,has,local-m,2 flavors has local-m,0.5687676668167114
Blogs,5,69,baselines,baselines,has,local attention,baselines has local attention,0.5594788193702698
Blogs,5,35,experiments,a( t ),dependent on,h( t ),a( t ) dependent on h( t ),0.7141172289848328
Blogs,5,35,experiments,a( t ),dependent on,h_bar ( s ),a( t ) dependent on h_bar ( s ),0.7306976318359375
Blogs,5,35,experiments,h_bar ( s ),dependent on,a ( t ),h_bar ( s ) dependent on a ( t ),0.7286525964736938
Blogs,5,35,experiments,c( t ),dependent on,a ( t ),c( t ) dependent on a ( t ),0.7198190093040466
Blogs,5,35,experiments,c( t ),dependent on,h_bar ( s ),c( t ) dependent on h_bar ( s ),0.7345002889633179
Blogs,5,66,experiments,s,is,source sentence length,s is source sentence length,0.5126394033432007
Blogs,5,26,hyperparameters,c,compute,a ( t ),c compute a ( t ),0.7593626976013184
Blogs,5,26,hyperparameters,c,compute,variable length alignment vector,c compute variable length alignment vector,0.7856162786483765
Blogs,5,26,hyperparameters,a ( t ),is,variable length alignment vector,a ( t ) is variable length alignment vector,0.5469849109649658
Blogs,5,26,hyperparameters,hyperparameters,to calculate,c,hyperparameters to calculate c,0.6874492168426514
Blogs,5,29,hyperparameters,hyperparameters,has,alignment vector ( a ( t ) ),hyperparameters has alignment vector ( a ( t ) ),0.531619131565094
Blogs,5,41,hyperparameters,context vector ( c( t ) ),derived as,weighted average,context vector ( c( t ) ) derived as weighted average,0.6196111440658569
Blogs,5,41,hyperparameters,weighted average,over,set of source hidden states,weighted average over set of source hidden states,0.6716903448104858
Blogs,5,41,hyperparameters,set of source hidden states,within,"window [ p ( t ) - d , p ( t ) + d ]","set of source hidden states within window [ p ( t ) - d , p ( t ) + d ]",0.6281588673591614
Blogs,5,41,hyperparameters,hyperparameters,has,context vector ( c( t ) ),hyperparameters has context vector ( c( t ) ),0.5337958931922913
Blogs,5,42,hyperparameters,local alignment vector a ( t ),is,fixed dimensional,local alignment vector a ( t ) is fixed dimensional,0.5582692623138428
Blogs,5,42,hyperparameters,global alignment vector,has,local alignment vector a ( t ),global alignment vector has local alignment vector a ( t ),0.5555947422981262
Blogs,5,3,model,nmt,directly models,conditional probability p( y /x ),nmt directly models conditional probability p( y /x ),0.7503217458724976
Blogs,5,3,model,conditional probability p( y /x ),of,translating,conditional probability p( y /x ) of translating,0.6016875505447388
Blogs,5,3,model,"source ( x1 , x2 ?. xn ) sentence",into,"target sentence ( y1,y2 ?.yn )","source ( x1 , x2 ?. xn ) sentence into target sentence ( y1,y2 ?.yn )",0.4817042946815491
Blogs,5,3,model,translating,has,"source ( x1 , x2 ?. xn ) sentence","translating has source ( x1 , x2 ?. xn ) sentence",0.5916137099266052
Blogs,5,3,model,model,has,nmt,model has nmt,0.5953153967857361
Blogs,5,4,model,nmt,consist of,two components,nmt consist of two components,0.7521587014198303
Blogs,5,4,model,encoder,computes,representation s,encoder computes representation s,0.7671980261802673
Blogs,5,4,model,representation s,for,each source sentence,representation s for each source sentence,0.6036840081214905
Blogs,5,4,model,two components,has,encoder,two components has encoder,0.5971474647521973
Blogs,5,4,model,model,has,nmt,model has nmt,0.5953153967857361
Blogs,5,5,model,decoder,generates,translation,decoder generates translation,0.7212231159210205
Blogs,5,5,model,decoder,decomposes,conditional probability,decoder decomposes conditional probability,0.7391023635864258
Blogs,5,5,model,conditional probability,as,probability of translation y,conditional probability as probability of translation y,0.5507924556732178
Blogs,5,5,model,probability of translation y,given,source sentence x,probability of translation y given source sentence x,0.6508291363716125
Blogs,5,5,model,translation,has,one word,translation has one word,0.6207319498062134
Blogs,5,5,model,model,has,decoder,model has decoder,0.6226420402526855
Blogs,5,6,model,model,parametrize,probability of decoding each word y( j ),model parametrize probability of decoding each word y( j ),0.6947495341300964
Blogs,5,7,model,transformative function,outputs,vocabulary size vector h,transformative function outputs vocabulary size vector h,0.7358862161636353
Blogs,5,7,model,rnn hidden unit f,computes,current hidden state,rnn hidden unit f computes current hidden state,0.6615462899208069
Blogs,5,7,model,current hidden state,given,previously hidden state,current hidden state given previously hidden state,0.691978394985199
Blogs,5,7,model,g,has,transformative function,g has transformative function,0.6086116433143616
Blogs,5,7,model,model,has,rnn hidden unit definition,model has rnn hidden unit definition,0.49403247237205505
Blogs,5,8,model,training objective,for,translation process,training objective for translation process,0.6013693809509277
Blogs,5,8,model,model,has,training objective,model has training objective,0.5122246146202087
Blogs,5,15,model,non-attention based rnn architecture source representation,has,s,non-attention based rnn architecture source representation has s,0.5773845314979553
Blogs,5,15,model,model,In,non-attention based rnn architecture source representation,model In non-attention based rnn architecture source representation,0.4922749698162079
Blogs,5,15,model,model,most of,non-attention based rnn architecture source representation,model most of non-attention based rnn architecture source representation,0.5731858015060425
Blogs,5,16,model,based methods,have,common steps,based methods have common steps,0.5965701937675476
Blogs,5,16,model,model,has,based methods,model has based methods,0.5958007574081421
Blogs,5,18,model,derive c ( t ),to capture,relevant source side information,derive c ( t ) to capture relevant source side information,0.7321374416351318
Blogs,5,18,model,model,has,derive c ( t ),model has derive c ( t ),0.5879361033439636
Blogs,5,20,model,h_bar,has access to,all the states of hidden states,h_bar has access to all the states of hidden states,0.7147743105888367
Blogs,5,20,model,all the states of hidden states,of the encoder,informative view,all the states of hidden states of the encoder informative view,0.7538895606994629
Blogs,5,20,model,all the states of hidden states,provides,informative view,all the states of hidden states provides informative view,0.5969834923744202
Blogs,5,20,model,informative view,of,source sentence,informative view of source sentence,0.5556767582893372
Blogs,5,20,model,h_bar,has,),h_bar has ),0.6413361430168152
Blogs,5,21,model,transformed,using,softmax layer,transformed using softmax layer,0.6715162992477417
Blogs,5,21,model,softmax layer,to produce,predictive distribution,softmax layer to produce predictive distribution,0.7035722136497498
Blogs,5,21,model,model,has,attentional vector,model has attentional vector,0.5404672622680664
Blogs,5,22,model,softmax layer,to found,most probable word,softmax layer to found most probable word,0.5669714212417603
Blogs,5,22,model,most probable word,from,all the available words,most probable word from all the available words,0.5051923394203186
Blogs,5,22,model,all the available words,in,our vocabulary,all the available words in our vocabulary,0.4845462143421173
Blogs,5,23,model,barebone architecture,of,attention based networks,barebone architecture of attention based networks,0.5701953768730164
Blogs,5,23,model,model,explains,barebone architecture,model explains barebone architecture,0.6819313168525696
Blogs,5,25,model,attention,takes into consideration,all encoder hidden states,attention takes into consideration all encoder hidden states,0.647234320640564
Blogs,5,25,model,all encoder hidden states,to derive,context vector ( c ( t ) ),all encoder hidden states to derive context vector ( c ( t ) ),0.6822857856750488
Blogs,5,25,model,global attention,has,attention,global attention has attention,0.5185985565185547
Blogs,5,25,model,model,has,global attention,model has global attention,0.5330514311790466
Blogs,5,27,model,alignment vector,computing,similarity measure,alignment vector computing similarity measure,0.7128800749778748
Blogs,5,27,model,similarity measure,between,h( t ) and h_bar ( s ),similarity measure between h( t ) and h_bar ( s ),0.6370457410812378
Blogs,5,27,model,h( t ) and h_bar ( s ),h(,target hidden state,h( t ) and h_bar ( s ) h( target hidden state,0.6894990801811218
Blogs,5,27,model,h( t ) and h_bar ( s ),),target hidden state,h( t ) and h_bar ( s ) ) target hidden state,0.6179388761520386
Blogs,5,27,model,model,has,alignment vector,model has alignment vector,0.525394856929779
Blogs,5,28,model,similar states,in,encoder and decoder,similar states in encoder and decoder,0.5441575050354004
Blogs,5,28,model,model,has,similar states,model has similar states,0.5911271572113037
Blogs,5,32,model,score function,trying to calculate,similarity,score function trying to calculate similarity,0.6877830028533936
Blogs,5,32,model,similarity,between,hidden states,similarity between hidden states,0.6855096817016602
Blogs,5,32,model,hidden states,of,target and the source,hidden states of target and the source,0.5941856503486633
Blogs,5,32,model,model,Through,score function,model Through score function,0.6557745337486267
Blogs,5,33,model,similar states,in,hidden and source,similar states in hidden and source,0.5608550310134888
Blogs,5,33,model,hidden and source,refer to,same meaning,hidden and source refer to same meaning,0.6450116038322449
Blogs,5,33,model,model,Intuitively,similar states,model Intuitively similar states,0.7876474857330322
Blogs,5,33,model,model,has,similar states,model has similar states,0.5911271572113037
Blogs,5,34,model,connecting lines,represent,interdependent variables,connecting lines represent interdependent variables,0.5971152782440186
Blogs,5,34,model,model,has,connecting lines,model has connecting lines,0.5747602581977844
Blogs,5,36,model,h_bar ( t ),dependent on,c,h_bar ( t ) dependent on c,0.7363860607147217
Blogs,5,36,model,model,has,h_bar ( t ),model has h_bar ( t ),0.6010932922363281
Blogs,5,37,model,local attention,chooses to,focus,local attention chooses to focus,0.7221112847328186
Blogs,5,37,model,focus,only on,small subset,focus only on small subset,0.6835828423500061
Blogs,5,37,model,small subset,of,hidden states,small subset of hidden states,0.6195012331008911
Blogs,5,37,model,hidden states,of,encoder,hidden states of encoder,0.6068733930587769
Blogs,5,37,model,encoder,per,target word,encoder per target word,0.603441059589386
Blogs,5,37,model,deficiency,has,local attention,deficiency has local attention,0.5874651074409485
Blogs,5,38,model,aligned position p( t ),for,each target word,aligned position p( t ) for each target word,0.5739067196846008
Blogs,5,38,model,model,generates,aligned position p( t ),model generates aligned position p( t ),0.6372299790382385
Blogs,5,38,model,model,has,local attention,model has local attention,0.5568013191223145
Blogs,5,39,model,aligned positions,in,local attention,aligned positions in local attention,0.5160099267959595
Blogs,5,39,model,model,learn,aligned positions,model learn aligned positions,0.6385102272033691
Blogs,5,43,model,both the translated and source sentence,are,monotonically aligned,both the translated and source sentence are monotonically aligned,0.5794391632080078
Blogs,5,43,model,model,assumed,both the translated and source sentence,model assumed both the translated and source sentence,0.7168758511543274
Blogs,5,44,model,further differentiation,of,local attention,further differentiation of local attention,0.5716249942779541
Blogs,5,44,model,local attention,To favor,alignment position p ( t ),local attention To favor alignment position p ( t ),0.6595706343650818
Blogs,5,44,model,alignment position p ( t ),place,gaussian distribution,alignment position p ( t ) place gaussian distribution,0.7365525960922241
Blogs,5,44,model,gaussian distribution,centered around,p ( t ),gaussian distribution centered around p ( t ),0.7427942156791687
Blogs,5,45,model,more weight,to,position p ( t ),more weight to position p ( t ),0.6117271184921265
Blogs,5,45,model,model,gives,more weight,model gives more weight,0.6832191348075867
Blogs,5,46,model,model,modify,alignment weights,model modify alignment weights,0.674902617931366
Blogs,5,46,model,model,modify,local -p,model modify local -p,0.7712520956993103
Blogs,5,48,model,attention decisions,made,independently,attention decisions made independently,0.6148106455802917
Blogs,5,48,model,proposed attention mechanisms,has,attention decisions,proposed attention mechanisms has attention decisions,0.5040717124938965
Blogs,5,48,model,model,In,proposed attention mechanisms,model In proposed attention mechanisms,0.5022481083869934
Blogs,5,49,model,future alignment decisions,take into consideration,past alignment information h_bar ( t ),future alignment decisions take into consideration past alignment information h_bar ( t ),0.6471182703971863
Blogs,5,49,model,past alignment information h_bar ( t ),concatenated with,inputs,past alignment information h_bar ( t ) concatenated with inputs,0.744777262210846
Blogs,5,49,model,inputs,at,next time steps,inputs at next time steps,0.5642210245132446
Blogs,5,49,model,model,to make sure that,future alignment decisions,model to make sure that future alignment decisions,0.6695102453231812
Blogs,5,51,model,decoder,access only to,last layer of the encoder,decoder access only to last layer of the encoder,0.4989372789859772
Blogs,5,51,model,model,has,decoder,model has decoder,0.6226420402526855
Blogs,5,52,model,attention - based networks,refer to,set of source hidden states s,attention - based networks refer to set of source hidden states s,0.5707763433456421
Blogs,5,52,model,set of source hidden states s,throughout,translation process,set of source hidden states s throughout translation process,0.5656850934028625
Blogs,5,52,model,model,has,attention - based networks,model has attention - based networks,0.5251829028129578
Blogs,5,53,model,hidden states,of,encoder,hidden states of encoder,0.6068733930587769
Blogs,5,60,model,variable - length alignment weight vector a ( t ),based on,current target state h( t ),variable - length alignment weight vector a ( t ) based on current target state h( t ),0.6251875758171082
Blogs,5,60,model,variable - length alignment weight vector a ( t ),based on,all source states h_bar ( s ),variable - length alignment weight vector a ( t ) based on all source states h_bar ( s ),0.6142584085464478
Blogs,5,60,model,model,At,each time step t,model At each time step t,0.5409538149833679
Blogs,5,61,model,global context vector,computed asthe,weighted average,global context vector computed asthe weighted average,0.6406487226486206
Blogs,5,61,model,c( t ),computed asthe,weighted average,c( t ) computed asthe weighted average,0.7371717095375061
Blogs,5,61,model,weighted average,over,all the source states,weighted average over all the source states,0.704413890838623
Blogs,5,61,model,global context vector,has,c( t ),global context vector has c( t ),0.560232400894165
Blogs,5,61,model,model,has,global context vector,model has global context vector,0.5321965217590332
Blogs,5,65,model,model,predicts,aligned position,model predicts aligned position,0.7540652751922607
Blogs,5,65,model,model,has,predictive alignment ( local - p ),model has predictive alignment ( local - p ),0.5879771113395691
Blogs,5,67,model,alignment vector,for,local -p model,alignment vector for local -p model,0.6134248375892639
Blogs,5,67,model,model,has,alignment vector,model has alignment vector,0.525394856929779
Blogs,5,11,results,nmt,requires,minimal domain knowledge,nmt requires minimal domain knowledge,0.6379072070121765
Blogs,5,11,results,results,has,nmt,results has nmt,0.5392657518386841
Blogs,5,12,results,small memory footprint,does not store,gigantic phase tables,small memory footprint does not store gigantic phase tables,0.7374337911605835
Blogs,5,12,results,small memory footprint,does not store,language models,small memory footprint does not store language models,0.6848982572555542
Blogs,5,12,results,nmt,has,small memory footprint,nmt has small memory footprint,0.5365957021713257
Blogs,5,12,results,results,has,nmt,results has nmt,0.5392657518386841
Blogs,5,13,results,ability to generalize well,to,very long word sentences,ability to generalize well to very long word sentences,0.5125621557235718
Blogs,5,13,results,nmt,has,ability to generalize well,nmt has ability to generalize well,0.5524942278862
Blogs,5,13,results,results,has,nmt,results has nmt,0.5392657518386841
Blogs,5,14,results,results,has,difference between attention and non-attention based networks,results has difference between attention and non-attention based networks,0.5816661715507507
Blogs,5,56,results,hidden state,of,nmt architecture,hidden state of nmt architecture,0.558566153049469
Blogs,5,64,results,local alignment,same as,global alignment,local alignment same as global alignment,0.5874093174934387
Blogs,5,64,results,results,has,local alignment,results has local alignment,0.5152263641357422
Blogs,5,68,results,global attention,is,computationally more expensive,global attention is computationally more expensive,0.5547664165496826
Blogs,5,68,results,global attention,useless for,long sentences,global attention useless for long sentences,0.6774474382400513
Blogs,5,68,results,local attention,focusses on,d hidden states,local attention focusses on d hidden states,0.6441439986228943
Blogs,6,38,ablation-analysis,8 - 12 hypotheses,using,fewer ( 4 or 2 ),8 - 12 hypotheses using fewer ( 4 or 2 ),0.6615094542503357
Blogs,6,38,ablation-analysis,only slight negative effects,on,bleu scores,only slight negative effects on bleu scores,0.5441398024559021
Blogs,6,38,ablation-analysis,beam search,has,8 - 12 hypotheses,beam search has 8 - 12 hypotheses,0.5848055481910706
Blogs,6,38,ablation-analysis,fewer ( 4 or 2 ),has,only slight negative effects,fewer ( 4 or 2 ) has only slight negative effects,0.5588602423667908
Blogs,6,38,ablation-analysis,ablation analysis,During,beam search,ablation analysis During beam search,0.7119215726852417
Blogs,6,47,ablation-analysis,character - based model,predicts,character,character - based model predicts character,0.6703926920890808
Blogs,6,47,ablation-analysis,character,by,character,character by character,0.6144445538520813
Blogs,6,47,ablation-analysis,character - based model,has,meaning of words,character - based model has meaning of words,0.5564272999763489
Blogs,6,47,ablation-analysis,character,has,meaning of words,character has meaning of words,0.5912572145462036
Blogs,6,47,ablation-analysis,character,has,meaning of words,character has meaning of words,0.5912572145462036
Blogs,6,47,ablation-analysis,ablation analysis,has,character - based model,ablation analysis has character - based model,0.5133659839630127
Blogs,6,62,ablation-analysis,tpu,makes,inference,tpu makes inference,0.6758365631103516
Blogs,6,62,ablation-analysis,inference,has,much faster,inference has much faster,0.5595691204071045
Blogs,6,62,ablation-analysis,ablation analysis,has,tpu,ablation analysis has tpu,0.5174136757850647
Blogs,6,11,baselines,x=m +x,before going into,next lstm,x=m +x before going into next lstm,0.6312330961227417
Blogs,6,29,experiments,best vocabulary size,for,mixed word- character model,best vocabulary size for mixed word- character model,0.6001957058906555
Blogs,6,29,experiments,mixed word- character model,is,32k,mixed word- character model is 32k,0.5690224170684814
Blogs,6,33,experiments,translation production corpora,are,two to three decimal orders of magnitudes,translation production corpora are two to three decimal orders of magnitudes,0.5883291959762573
Blogs,6,33,experiments,bigger,than,wmt corpora,bigger than wmt corpora,0.5888170003890991
Blogs,6,33,experiments,two to three decimal orders of magnitudes,has,bigger,two to three decimal orders of magnitudes has bigger,0.5622355937957764
Blogs,6,14,hyperparameters,hyperparameters,has,n,hyperparameters has n,0.5270147323608398
Blogs,6,15,hyperparameters,mini-batch,of,m sentence pairs at a time,mini-batch of m sentence pairs at a time,0.5423755645751953
Blogs,6,24,hyperparameters,weights,of,fixed - point integer operations,weights of fixed - point integer operations,0.5735472440719604
Blogs,6,24,hyperparameters,fixed - point integer operations,replaced with,8 - bit,fixed - point integer operations replaced with 8 - bit,0.7164372801780701
Blogs,6,24,hyperparameters,fixed - point integer operations,replaced with,16 - bit resolution,fixed - point integer operations replaced with 16 - bit resolution,0.7254533767700195
Blogs,6,24,hyperparameters,hyperparameters,has,weights,hyperparameters has weights,0.5201958417892456
Blogs,6,52,hyperparameters,hyperparameters,has,standard maximum-likelihood ( ml ) training objective,hyperparameters has standard maximum-likelihood ( ml ) training objective,0.49748072028160095
Blogs,6,57,hyperparameters,training,of,model,training of model,0.6175848245620728
Blogs,6,57,hyperparameters,training,has,full- precision,training has full- precision,0.6080747842788696
Blogs,6,57,hyperparameters,model,has,full- precision,model has full- precision,0.5801940560340881
Blogs,6,57,hyperparameters,hyperparameters,During,training,hyperparameters During training,0.6737716197967529
Blogs,6,4,model,structure of bi-directional connections,first layer of,encoder,structure of bi-directional connections first layer of encoder,0.7741469144821167
Blogs,6,4,model,model,has,structure of bi-directional connections,model has structure of bi-directional connections,0.5495616793632507
Blogs,6,5,model,bottom encoder layer,is,bi-directional,bottom encoder layer is bi-directional,0.5617024898529053
Blogs,6,5,model,pink nodes,gather,information,pink nodes gather information,0.6870977282524109
Blogs,6,5,model,pink nodes,gather,information,pink nodes gather information,0.6870977282524109
Blogs,6,5,model,information,from,left to right,information from left to right,0.6147531270980835
Blogs,6,5,model,information,from,right to left,information from right to left,0.6032461524009705
Blogs,6,5,model,information,from,right to left,information from right to left,0.6032461524009705
Blogs,6,5,model,green nodes,gather,information,green nodes gather information,0.6559333205223083
Blogs,6,5,model,information,from,right to left,information from right to left,0.6032461524009705
Blogs,6,5,model,bi-directional,has,pink nodes,bi-directional has pink nodes,0.6046797037124634
Blogs,6,5,model,model,has,bottom encoder layer,model has bottom encoder layer,0.5364133715629578
Blogs,6,6,model,encoder,are,uni-directional,encoder are uni-directional,0.6058215498924255
Blogs,6,8,model,model,has,attention module,model has attention module,0.5680129528045654
Blogs,6,9,model,yi -1,be,decoder - rnn output,yi -1 be decoder - rnn output,0.5058038234710693
Blogs,6,9,model,decoder - rnn output,from,past decoding time step,decoder - rnn output from past decoding time step,0.5836111307144165
Blogs,6,10,model,attention context ai,for,current time step,attention context ai for current time step,0.5533052682876587
Blogs,6,10,model,attention context ai,computed,attentionfunction,attention context ai computed attentionfunction,0.5798328518867493
Blogs,6,10,model,feed forward network,with,one hidden layer,feed forward network with one hidden layer,0.6444839835166931
Blogs,6,10,model,model,has,attention context ai,model has attention context ai,0.5309295058250427
Blogs,6,13,model,n replicas,share,one copy,n replicas share one copy,0.7133943438529968
Blogs,6,13,model,one copy,of,model parameters,one copy of model parameters,0.5955327153205872
Blogs,6,13,model,model,has,n replicas,model has n replicas,0.5712073445320129
Blogs,6,16,model,encoder and decoder networks,partitioned along,depth dimension,encoder and decoder networks partitioned along depth dimension,0.7435616254806519
Blogs,6,16,model,encoder and decoder networks,placed on,multiple gpus,encoder and decoder networks placed on multiple gpus,0.6107276678085327
Blogs,6,16,model,model,has,encoder and decoder networks,model has encoder and decoder networks,0.5734876394271851
Blogs,6,17,model,layer,are,uni-directional,layer are uni-directional,0.6008860468864441
Blogs,6,17,model,layer,is,fully finished,layer is fully finished,0.6021632552146912
Blogs,6,17,model,layer i+ 1,start,computation,layer i+ 1 start computation,0.700630784034729
Blogs,6,17,model,computation,before,layer i,computation before layer i,0.633632481098175
Blogs,6,17,model,layer i,is,fully finished,layer i is fully finished,0.6110692024230957
Blogs,6,18,model,each partition,responsible for,subset of symbols,each partition responsible for subset of symbols,0.678257405757904
Blogs,6,18,model,model,has,softmax layer,model has softmax layer,0.5296618938446045
Blogs,6,22,model,accumulators,to be within,"[-? , ?]","accumulators to be within [-? , ?]",0.6413258910179138
Blogs,6,22,model,accumulators,to be within,certain range,accumulators to be within certain range,0.688320517539978
Blogs,6,22,model,accumulators,to be within,quantization,accumulators to be within quantization,0.7025812268257141
Blogs,6,22,model,certain range,for,quantization,certain range for quantization,0.6600023508071899
Blogs,6,22,model,quantized inference,has,gnmt,quantized inference has gnmt,0.6018183827400208
Blogs,6,22,model,model,For,quantized inference,model For quantized inference,0.6330820322036743
Blogs,6,23,model,forward computation,of,lstm stack,forward computation of lstm stack,0.5241934657096863
Blogs,6,23,model,lstm stack,with,residual connections,lstm stack with residual connections,0.6299107670783997
Blogs,6,23,model,model,has,forward computation,model has forward computation,0.5559741854667664
Blogs,6,36,model,decoder 8 lstm layers,used for,decoder,decoder 8 lstm layers used for decoder,0.5826642513275146
Blogs,6,36,model,model,has,decoder 8 lstm layers,model has decoder 8 lstm layers,0.5012826919555664
Blogs,6,37,model,beam search,used during,decoding,beam search used during decoding,0.6665971279144287
Blogs,6,37,model,decoding,to find,sequence y,decoding to find sequence y,0.6314157843589783
Blogs,6,37,model,sequence y,maximizes,"score function s( y , x )","sequence y maximizes score function s( y , x )",0.7770382165908813
Blogs,6,37,model,"score function s( y , x )",given,trained model,"score function s( y , x ) given trained model",0.6823448538780212
Blogs,6,37,model,model,has,beam search,model has beam search,0.5889679789543152
Blogs,6,39,model,two important refinements,used to,pure max-probability based beam search algorithm,two important refinements used to pure max-probability based beam search algorithm,0.5729231238365173
Blogs,6,39,model,pure max-probability based beam search algorithm,has,coverage penalty,pure max-probability based beam search algorithm has coverage penalty,0.5401090383529663
Blogs,6,39,model,coverage penalty,has,length normalization,coverage penalty has length normalization,0.5189170837402344
Blogs,6,39,model,model,has,two important refinements,model has two important refinements,0.5785545706748962
Blogs,6,43,model,residual connections,greatly improve,gradient flow,residual connections greatly improve gradient flow,0.6655757427215576
Blogs,6,43,model,gradient flow,in,backward pass,gradient flow in backward pass,0.5330121517181396
Blogs,6,43,model,layers,used for,encoder,layers used for encoder,0.6260147094726562
Blogs,6,43,model,model,has,residual connections,model has residual connections,0.5542914271354675
Blogs,6,45,model,gmnt,proposes,wordpiece model,gmnt proposes wordpiece model,0.684784471988678
Blogs,6,46,model,word - based model,predicts,word,word - based model predicts word,0.6905473470687866
Blogs,6,46,model,word - based model,predicts,rare words,word - based model predicts rare words,0.7291207313537598
Blogs,6,46,model,word,by,word,word by word,0.5946435332298279
Blogs,6,46,model,brief,has,word - based model,brief has word - based model,0.5431740880012512
Blogs,6,46,model,model,To be,brief,model To be brief,0.6079942584037781
Blogs,6,46,model,model,has,word - based model,model has word - based model,0.5620104074478149
Blogs,6,54,model,model refinement,done using,expected reward objective,model refinement done using expected reward objective,0.5798174142837524
Blogs,6,54,model,expected reward objective,by,reinforcement learning ( rl ),expected reward objective by reinforcement learning ( rl ),0.5658866167068481
Blogs,6,54,model,output sentences y,up to,certain length,output sentences y up to certain length,0.6168460249900818
Blogs,6,54,model,model,has,model refinement,model has model refinement,0.5459527969360352
Blogs,6,55,model,linear combination,of,ml and rl objectives,linear combination of ml and rl objectives,0.5604337453842163
Blogs,6,55,model,training,has,linear combination,training has linear combination,0.5755388140678406
Blogs,6,55,model,model,To further stabilize,training,model To further stabilize training,0.7435207366943359
Blogs,6,58,model,constraints,added to,model,constraints added to model,0.6563568711280823
Blogs,6,58,model,model,during,training,model during training,0.714866042137146
Blogs,6,58,model,model,are,clipping,model are clipping,0.614153265953064
Blogs,6,58,model,model,during,training,model during training,0.714866042137146
Blogs,6,58,model,model,has,constraints,model has constraints,0.5573897361755371
Blogs,6,28,results,results,on,wmt en > fr ( newstest 2014 ),results on wmt en > fr ( newstest 2014 ),0.5811606049537659
Blogs,6,28,results,results,has,ml training models,results has ml training models,0.5122057795524597
Blogs,6,30,results,best model wpm - 32 k,achieves,bleu score,best model wpm - 32 k achieves bleu score,0.6308415532112122
Blogs,6,30,results,bleu score,of,38.95,bleu score of 38.95,0.5218891501426697
Blogs,6,30,results,results,has,best model wpm - 32 k,results has best model wpm - 32 k,0.5729861855506897
Blogs,6,31,results,bleu score,represents,averaged score,bleu score represents averaged score,0.5652758479118347
Blogs,6,31,results,averaged score,of,8 models,averaged score of 8 models,0.6123903393745422
Blogs,6,32,results,maximum bleu score,of,8 models,maximum bleu score of 8 models,0.540902853012085
Blogs,6,32,results,8 models,higher at,39.37,8 models higher at 39.37,0.6839296221733093
Blogs,6,32,results,results,has,maximum bleu score,results has maximum bleu score,0.5491376519203186
Blogs,6,34,results,gmnt,reduces,translation errors,gmnt reduces translation errors,0.6661392450332642
Blogs,6,34,results,translation errors,by,more than 60 %,translation errors by more than 60 %,0.5651779174804688
Blogs,6,34,results,more than 60 %,compared to,pbmt model,more than 60 % compared to pbmt model,0.7145662903785706
Blogs,6,34,results,pbmt model,on,major pairs of languages,pbmt model on major pairs of languages,0.5536229610443115
Blogs,6,34,results,results,has,gmnt,results has gmnt,0.546523928642273
Blogs,6,40,results,search,by,30 % - 40 %,search by 30 % - 40 %,0.615501880645752
Blogs,6,40,results,30 % - 40 %,run on,cpus,30 % - 40 % run on cpus,0.7553584575653076
Blogs,6,42,results,simple stacked lstm layers,work well,4 layers,simple stacked lstm layers work well 4 layers,0.6152048110961914
Blogs,6,42,results,simple stacked lstm layers,work well,very poorly,simple stacked lstm layers work well very poorly,0.6987689733505249
Blogs,6,42,results,4 layers,barely with,6 layers,4 layers barely with 6 layers,0.7171202898025513
Blogs,6,42,results,very poorly,beyond,8 layers,very poorly beyond 8 layers,0.6791470050811768
Blogs,6,42,results,results,found that,simple stacked lstm layers,results found that simple stacked lstm layers,0.6150294542312622
Blogs,6,53,results,directly improve,has,bleu scores,directly improve has bleu scores,0.5305098295211792
Blogs,6,61,results,model inference,on,"cpu , gpu","model inference on cpu , gpu",0.5267466306686401
Blogs,6,61,results,model inference,on,tpu inference,model inference on tpu inference,0.5742476582527161
Blogs,6,61,results,model inference,using,cpu,model inference using cpu,0.7494479417800903
Blogs,6,61,results,tpu inference,using,cpu,tpu inference using cpu,0.6749801635742188
Blogs,6,61,results,tpu inference,is,faster,tpu inference is faster,0.5847843289375305
Blogs,6,61,results,cpu,is,faster,cpu is faster,0.5689741373062134
Blogs,6,61,results,faster,than,gpu,faster than gpu,0.6156558394432068
Blogs,6,61,results,faster,due to,data transfer,faster due to data transfer,0.6719797849655151
Blogs,6,61,results,gpu,due to,data transfer,gpu due to data transfer,0.6676622629165649
Blogs,6,61,results,results,has,model inference,results has model inference,0.515622079372406
Blogs,6,64,results,production datahistogram,of,side- by-side scores,production datahistogram of side- by-side scores,0.559992253780365
Blogs,6,64,results,production datahistogram,of,side- by- side scores,production datahistogram of side- by- side scores,0.559992253780365
Blogs,6,64,results,side- by-side scores,on,500 sampled sentences,side- by-side scores on 500 sampled sentences,0.5089737176895142
Blogs,6,64,results,500 sampled sentences,from,wikipedia and news websites,500 sampled sentences from wikipedia and news websites,0.5687479972839355
Blogs,6,64,results,side- by- side scores,on,production data,side- by- side scores on production data,0.542797863483429
Blogs,7,27,baselines,model,to translate,english phrase,model to translate english phrase,0.7027758359909058
Blogs,7,27,baselines,english phrase,to,french phrase,english phrase to french phrase,0.5849820971488953
Blogs,7,29,baselines,cslm ( continuous space language model ),compare,phrase scores,cslm ( continuous space language model ) compare phrase scores,0.6681463122367859
Blogs,7,29,baselines,phrase scores,from,trained model,phrase scores from trained model,0.5071567296981812
Blogs,7,23,hyperparameters,activation function,in,proposed hidden unit - hyperbolic tangent function,activation function in proposed hidden unit - hyperbolic tangent function,0.523809015750885
Blogs,7,23,hyperparameters,activation function,initialized by,sampling,activation function initialized by sampling,0.7060586810112
Blogs,7,23,hyperparameters,non-recurrent weights,initialized by,sampling,non-recurrent weights initialized by sampling,0.6977572441101074
Blogs,7,23,hyperparameters,sampling,from,isotropic gaussian distribution,sampling from isotropic gaussian distribution,0.6288421750068665
Blogs,7,23,hyperparameters,proposed hidden unit - hyperbolic tangent function,has,non-recurrent weights,proposed hidden unit - hyperbolic tangent function has non-recurrent weights,0.5686112642288208
Blogs,7,23,hyperparameters,isotropic gaussian distribution,has,"mean = 0 , sd = 0.01 )","isotropic gaussian distribution has mean = 0 , sd = 0.01 )",0.5327516794204712
Blogs,7,23,hyperparameters,hyperparameters,has,activation function,hyperparameters has activation function,0.4852520525455475
Blogs,7,24,hyperparameters,recurrent weights,initialized by,sampling,recurrent weights initialized by sampling,0.7072075605392456
Blogs,7,24,hyperparameters,recurrent weights,using,left singular vectors,recurrent weights using left singular vectors,0.6660788655281067
Blogs,7,24,hyperparameters,sampling,from,white gaussian distribution,sampling from white gaussian distribution,0.612477719783783
Blogs,7,24,hyperparameters,hyperparameters,has,recurrent weights,hyperparameters has recurrent weights,0.5275602340698242
Blogs,7,32,hyperparameters,input sequence,can be,variable length sequence of words,input sequence can be variable length sequence of words,0.6607486009597778
Blogs,7,32,hyperparameters,variable length sequence of words,input to,encoder,variable length sequence of words input to encoder,0.7567571997642517
Blogs,7,32,hyperparameters,encoder,to output,fixed - length vector representation,encoder to output fixed - length vector representation,0.7287341952323914
Blogs,7,32,hyperparameters,correctly,has,input sequence,correctly has input sequence,0.6193941831588745
Blogs,7,5,model,performance,of,statistical machine translation ( smt ) systems,performance of statistical machine translation ( smt ) systems,0.551935076713562
Blogs,7,8,model,model,consists of,two rnns,model consists of two rnns,0.6554335951805115
Blogs,7,9,model,variable - length input sequence,into,fixed - length vector representation,variable - length input sequence into fixed - length vector representation,0.5862240195274353
Blogs,7,10,model,given fixed - length vector representation,into,variable - length target sequence,given fixed - length vector representation into variable - length target sequence,0.5711116194725037
Blogs,7,11,model,two networks,trained,jointly,two networks trained jointly,0.677335262298584
Blogs,7,11,model,jointly,to maximise,conditional probability,jointly to maximise conditional probability,0.6949442625045776
Blogs,7,11,model,conditional probability,of,target sequence,conditional probability of target sequence,0.6095684170722961
Blogs,7,11,model,target sequence,given,input source sequence,target sequence given input source sequence,0.6927077770233154
Blogs,7,11,model,model,has,two networks,model has two networks,0.5891023278236389
Blogs,7,12,model,target sequence,given,input sequence,target sequence given input sequence,0.7053357362747192
Blogs,7,12,model,model,generate,target sequence,model generate target sequence,0.6809362173080444
Blogs,7,15,model,model,has,unit,model has unit,0.5322180390357971
Blogs,7,16,model,hidden unit,updated to have,reset gate,hidden unit updated to have reset gate,0.6433786749839783
Blogs,7,16,model,hidden state information,finds,irrelevant,hidden state information finds irrelevant,0.6508662700653076
Blogs,7,16,model,model,has,hidden unit,model has hidden unit,0.5714491009712219
Blogs,7,17,model,update gate,controls,how much information,update gate controls how much information,0.7049928903579712
Blogs,7,17,model,how much information,from,previous state,how much information from previous state,0.5239746570587158
Blogs,7,17,model,model,has,update gate,model has update gate,0.5733370184898376
Blogs,7,18,model,separate reset and update gates,improve,memory capacity,separate reset and update gates improve memory capacity,0.6653947830200195
Blogs,7,18,model,each hidden unit,has,separate reset and update gates,each hidden unit has separate reset and update gates,0.55290287733078
Blogs,7,18,model,model,has,each hidden unit,model has each hidden unit,0.5602502226829529
Blogs,7,20,model,translation model,factorised into,translation probabilities,translation model factorised into translation probabilities,0.6722947955131531
Blogs,7,20,model,translation probabilities,of,matching phrases,translation probabilities of matching phrases,0.558692216873169
Blogs,7,20,model,matching phrases,in,source and target sentences,matching phrases in source and target sentences,0.49963510036468506
Blogs,7,20,model,phrase - based smt framework,has,translation model,phrase - based smt framework has translation model,0.5103518962860107
Blogs,7,20,model,model,In,phrase - based smt framework,model In phrase - based smt framework,0.4878830015659332
Blogs,7,21,model,rnn encoder - decoder,to,rescore,rnn encoder - decoder to rescore,0.5566708445549011
Blogs,7,21,model,phrase pairs,in,phrase table,phrase pairs in phrase table,0.5101677179336548
Blogs,7,21,model,rescore,has,phrase pairs,rescore has phrase pairs,0.6001051664352417
Blogs,7,21,model,model,has,rnn encoder - decoder,model has rnn encoder - decoder,0.5510506629943848
Blogs,7,31,model,rnn encoder - decoder,learns,continuous space representation,rnn encoder - decoder learns continuous space representation,0.6924660801887512
Blogs,7,31,model,continuous space representation,for,phrases,continuous space representation for phrases,0.6496912240982056
Blogs,7,31,model,continuous space representation,preserves,semantic and syntactic structure,continuous space representation preserves semantic and syntactic structure,0.6721739172935486
Blogs,7,31,model,model,has,rnn encoder - decoder,model has rnn encoder - decoder,0.5510506629943848
Blogs,7,13,results,given pair,of,input and output sequences,given pair of input and output sequences,0.6059871912002563
Blogs,7,28,results,model,to score,phrase pairs,model to score phrase pairs,0.6434388160705566
Blogs,7,28,results,phrase pairs,in,standard phrase - based smt system,phrase pairs in standard phrase - based smt system,0.4851996898651123
Blogs,7,28,results,phrase pairs,the,translation performance,phrase pairs the translation performance,0.6181303262710571
Blogs,7,28,results,standard phrase - based smt system,has,translation performance,standard phrase - based smt system has translation performance,0.5199286341667175
Blogs,7,28,results,results,Using,model,results Using model,0.6410605907440186
Blogs,7,30,results,rnn encoder - decoder,better at capturing,linguistic regularities,rnn encoder - decoder better at capturing linguistic regularities,0.6552951335906982
Blogs,7,30,results,linguistic regularities,in,phrase table,linguistic regularities in phrase table,0.48365136981010437
Blogs,7,30,results,results,has,rnn encoder - decoder,results has rnn encoder - decoder,0.5302605628967285
Blogs,7,33,results,results,Thanks for,gist,results Thanks for gist,0.4877535402774811
Blogs,8,9,baselines,baselines,has,rnn encoder-decoder architecture,baselines has rnn encoder-decoder architecture,0.5473923683166504
Blogs,8,11,baselines,attention,modeled in,unit of words,attention modeled in unit of words,0.6834111213684082
Blogs,8,11,baselines,attention,use of,phrases ( instead of words ),attention use of phrases ( instead of words ),0.6531180739402771
Blogs,8,11,baselines,nmt models,has,attention,nmt models has attention,0.5671972036361694
Blogs,8,16,experiments,relation network neural network,desgined for,relational reasoning,relation network neural network desgined for relational reasoning,0.6231850385665894
Blogs,8,25,experiments,44 k sentences,from,tourism and travel domain,44 k sentences from tourism and travel domain,0.5865859985351562
Blogs,8,25,experiments,iwslt data,has,44 k sentences,iwslt data has 44 k sentences,0.6004759669303894
Blogs,8,5,model,relation network ( rn ),re nes,encoding representation,relation network ( rn ) re nes encoding representation,0.596691370010376
Blogs,8,5,model,model,introduces,relation network ( rn ),model introduces relation network ( rn ),0.6273514032363892
Blogs,8,13,model,relationship,between,source words,relationship between source words,0.5960155129432678
Blogs,8,13,model,source words,using,context ( neighboring words ),source words using context ( neighboring words ),0.6467005014419556
Blogs,8,13,model,model,Learn,relationship,model Learn relationship,0.6577098369598389
Blogs,8,14,model,relation networks ( rns ),build,pairwise relations,relation networks ( rns ) build pairwise relations,0.6710970997810364
Blogs,8,14,model,pairwise relations,between,source words,pairwise relations between source words,0.6078760623931885
Blogs,8,14,model,pairwise relations,using,representations,pairwise relations using representations,0.7022631168365479
Blogs,8,14,model,representations,generated by,rnns,representations generated by rnns,0.6469944715499878
Blogs,8,14,model,model,has,relation networks ( rns ),model has relation networks ( rns ),0.6041374206542969
Blogs,8,15,model,rn,sit between,encoder and the attention layer,rn sit between encoder and the attention layer,0.6993916034698486
Blogs,8,15,model,encoder and the attention layer,of,encoderdecoder framework,encoder and the attention layer of encoderdecoder framework,0.5600264072418213
Blogs,8,15,model,encoder and the attention layer,keeping,main architecture unaffected,encoder and the attention layer keeping main architecture unaffected,0.6082975268363953
Blogs,8,15,model,model,has,rn,model has rn,0.6751679182052612
Blogs,8,17,model,cnn layer,Extract,information,cnn layer Extract information,0.7444040179252625
Blogs,8,17,model,information,from,words,information from words,0.5653496384620667
Blogs,8,17,model,words,surrounding,given word ( context ),words surrounding given word ( context ),0.6522448062896729
Blogs,8,17,model,components,has,cnn layer,components has cnn layer,0.5781932473182678
Blogs,8,17,model,model,has,components,model has components,0.5117928981781006
Blogs,8,17,model,model,has,cnn layer,model has cnn layer,0.5405237078666687
Blogs,8,18,model,sequence of vectors,for,different kernel width,sequence of vectors for different kernel width,0.6203288435935974
Blogs,8,18,model,model,has,nal output,model has nal output,0.5405837893486023
Blogs,8,19,model,graph propagation ( gp ) layer,Connect,all the words,graph propagation ( gp ) layer Connect all the words,0.6647365093231201
Blogs,8,19,model,all the words,with,each other,all the words with each other,0.6527844667434692
Blogs,8,19,model,each other,in the form of,graph,each other in the form of graph,0.6763638854026794
Blogs,8,19,model,model,has,graph propagation ( gp ) layer,model has graph propagation ( gp ) layer,0.5788654685020447
Blogs,8,20,model,each output vector,from,cnn,each output vector from cnn,0.5933693647384644
Blogs,8,20,model,each output vector,corresponds to,node,each output vector corresponds to node,0.7664893865585327
Blogs,8,20,model,node,in,graph,node in graph,0.5677454471588135
Blogs,8,20,model,model,has,each output vector,model has each output vector,0.5616634488105774
Blogs,8,21,model,ows,between,nodes,ows between nodes,0.719825029373169
Blogs,8,21,model,ows,in,message passing sort of fashion,ows in message passing sort of fashion,0.5465070009231567
Blogs,8,21,model,nodes,of,graph,nodes of graph,0.5964953303337097
Blogs,8,21,model,message passing sort of fashion,to obtain,new set of vectors,message passing sort of fashion to obtain new set of vectors,0.6381367444992065
Blogs,8,21,model,new set of vectors,for,each node,new set of vectors for each node,0.6305999159812927
Blogs,8,21,model,information,has,ows,information has ows,0.6169261932373047
Blogs,8,21,model,model,has,information,model has information,0.5741810202598572
Blogs,8,22,model,model,has,multi-layer perceptron ( mlp ) layer,model has multi-layer perceptron ( mlp ) layer,0.5866236090660095
Blogs,8,23,model,representation,from,gp layer,representation from gp layer,0.527270495891571
Blogs,8,23,model,gp layer,fed to,mlp layer,gp layer fed to mlp layer,0.6938340067863464
Blogs,8,23,model,model,has,representation,model has representation,0.5781156420707703
Blogs,8,24,model,layer,uses,residual connections,layer uses residual connections,0.5463148355484009
Blogs,8,24,model,residual connections,from,previous layers,residual connections from previous layers,0.4670170247554779
Blogs,8,24,model,residual connections,in form of,concatenation,residual connections in form of concatenation,0.6648218035697937
Blogs,8,24,model,model,has,layer,model has layer,0.587888777256012
Blogs,8,31,model,related posts,has,rn,related posts has rn,0.616948664188385
Blogs,8,31,model,model,has,related posts,model has related posts,0.6020361185073853
Blogs,8,8,results,results,has,limitations of existing nmt models,results has limitations of existing nmt models,0.5089868307113647
Blogs,8,10,results,rnns,prone to,forgetting old information,rnns prone to forgetting old information,0.7000986933708191
Blogs,8,10,results,results,has,rnns,results has rnns,0.5510128736495972
Blogs,8,27,results,sentences,become,larger ( more than 50 words ),sentences become larger ( more than 50 words ),0.5968152284622192
Blogs,8,27,results,sentences,has,rnmt,sentences has rnmt,0.6134009957313538
Blogs,8,27,results,rnmt,has,clearly outperforms,rnmt has clearly outperforms,0.627593457698822
Blogs,8,27,results,clearly outperforms,has,other baselines,clearly outperforms has other baselines,0.5969277620315552
Blogs,8,27,results,results,has,sentences,results has sentences,0.4975784122943878
Blogs,8,28,results,qualitative evaluation,shows,rnmt + model,qualitative evaluation shows rnmt + model,0.6648744940757751
Blogs,8,28,results,rnmt + model,captures,word alignment,rnmt + model captures word alignment,0.7271223068237305
Blogs,8,28,results,word alignment,better than,nmt + models,word alignment better than nmt + models,0.6830807328224182
Blogs,8,28,results,results,has,qualitative evaluation,results has qualitative evaluation,0.5084247589111328
Blogs,8,29,results,nmt + system,tends to miss,some information,nmt + system tends to miss some information,0.7628198266029358
Blogs,8,29,results,some information,from,source sentence,some information from source sentence,0.49676603078842163
Blogs,8,29,results,results,has,nmt + system,results has nmt + system,0.5838443040847778
Blogs,8,30,results,weak,at capturing,long-term dependency,weak at capturing long-term dependency,0.6751275658607483
Blogs,8,30,results,weak,using,relation layer,weak using relation layer,0.706656813621521
Blogs,8,30,results,results,has,both cnns and rnns,results has both cnns and rnns,0.5543655753135681
Blogs,8,33,results,privacy policy,has,1,privacy policy has 1,0.5681158304214478
Blogs,8,33,results,privacy policy,has,login t tweet,privacy policy has login t tweet,0.5388237237930298
Blogs,8,33,results,1,has,login t tweet,1 has login t tweet,0.5877498984336853
Blogs,9,19,ablation-analysis,deep lstm ( s ),of,4 layers,deep lstm ( s ) of 4 layers,0.5315704941749573
Blogs,9,19,ablation-analysis,deep lstm ( s ),perform,better,deep lstm ( s ) perform better,0.6221280694007874
Blogs,9,19,ablation-analysis,ablation analysis,mentions,deep lstm ( s ),ablation analysis mentions deep lstm ( s ),0.6558586359024048
Blogs,9,21,ablation-analysis,trick,of inverting,input sequence,trick of inverting input sequence,0.7279757857322693
Blogs,9,21,ablation-analysis,input sequence,when mapping it to,output sequence,input sequence when mapping it to output sequence,0.6950981020927429
Blogs,9,21,ablation-analysis,ablation analysis,highlights,trick,ablation analysis highlights trick,0.6209011077880859
Blogs,9,23,ablation-analysis,minimal time lag,where,distance,minimal time lag where distance,0.6717451214790344
Blogs,9,23,ablation-analysis,distance,between,generated and source words,distance between generated and source words,0.6705597639083862
Blogs,9,23,ablation-analysis,minimized,by,reversing,minimized by reversing,0.6090772151947021
Blogs,9,23,ablation-analysis,reversing,has,order,reversing has order,0.6482312083244324
Blogs,9,3,baselines,baselines,has,seq2seq,baselines has seq2seq,0.5571820139884949
Blogs,9,7,baselines,limitation,for,nlp,limitation for nlp,0.5291300415992737
Blogs,9,16,baselines,baselines,has,two lstm ( s ) ( encoder- decoder ),baselines has two lstm ( s ) ( encoder- decoder ),0.5478962063789368
Blogs,9,18,baselines,baselines,has,deep lst m ( s ),baselines has deep lst m ( s ),0.5367038249969482
Blogs,9,27,baselines,translations,produced using,most probable outcomes,translations produced using most probable outcomes,0.6721923351287842
Blogs,9,27,baselines,baselines,has,translations,baselines has translations,0.6204454898834229
Blogs,9,38,baselines,2 lstm ( s ),to map,varying length input,2 lstm ( s ) to map varying length input,0.6652224063873291
Blogs,9,38,baselines,varying length input,to,fixed length vector,varying length input to fixed length vector,0.5799786448478699
Blogs,9,38,baselines,baselines,use of,2 lstm ( s ),baselines use of 2 lstm ( s ),0.612206757068634
Blogs,9,37,experiments,dnn,show,much promise,dnn show much promise,0.681401252746582
Blogs,9,37,experiments,lstm ( s ),show,much promise,lstm ( s ) show much promise,0.635789155960083
Blogs,9,37,experiments,much promise,for,seq2seq learning,much promise for seq2seq learning,0.5695828199386597
Blogs,9,24,hyperparameters,"160,000",of,most frequent words,"160,000 of most frequent words",0.5583845376968384
Blogs,9,24,hyperparameters,"160,000",of,most frequent words,"160,000 of most frequent words",0.5583845376968384
Blogs,9,24,hyperparameters,most frequent words,for,source language,most frequent words for source language,0.5318706035614014
Blogs,9,24,hyperparameters,"80,000",of,most frequent words,"80,000 of most frequent words",0.5614057183265686
Blogs,9,24,hyperparameters,most frequent words,for,target language,most frequent words for target language,0.5551711916923523
Blogs,9,24,hyperparameters,training details,has,"160,000","training details has 160,000",0.5903161764144897
Blogs,9,24,hyperparameters,training details,has,"80,000","training details has 80,000",0.5908273458480835
Blogs,9,24,hyperparameters,hyperparameters,has,training details,hyperparameters has training details,0.47968241572380066
Blogs,9,25,hyperparameters,out - of- vocabulary word,replaced with,special   unk   token,out - of- vocabulary word replaced with special   unk   token,0.7183122634887695
Blogs,9,25,hyperparameters,hyperparameters,has,out - of- vocabulary word,hyperparameters has out - of- vocabulary word,0.5101280212402344
Blogs,9,26,hyperparameters,training,maximize,log probability,training maximize log probability,0.7794596552848816
Blogs,9,26,hyperparameters,hyperparameters,has,training,hyperparameters has training,0.519983172416687
Blogs,9,29,hyperparameters,hypothesis,are,pairs of sentences,hypothesis are pairs of sentences,0.5915805101394653
Blogs,9,29,hyperparameters,pairs of sentences,generated,lstm,pairs of sentences generated lstm,0.668443500995636
Blogs,9,29,hyperparameters,uniform distribution,between,- 0.08 and 0.08,uniform distribution between - 0.08 and 0.08,0.6448935866355896
Blogs,9,29,hyperparameters,hyperparameters,has,hypothesis,hyperparameters has hypothesis,0.5745670795440674
Blogs,9,10,model,your input,maps it into,fixed dimension vector,your input maps it into fixed dimension vector,0.6908197999000549
Blogs,9,10,model,model,Takes,your input,model Takes your input,0.7040124535560608
Blogs,9,11,model,second,acts as,decoder,second acts as decoder,0.7256639003753662
Blogs,9,11,model,model,has,second,model has second,0.6080706119537354
Blogs,9,12,model,fixed vector,maps it to,output sequence,fixed vector maps it to output sequence,0.6789382696151733
Blogs,9,12,model,model,Takes,fixed vector,model Takes fixed vector,0.639050304889679
Blogs,9,14,model,lstm,tasked to,predict,lstm tasked to predict,0.672393262386322
Blogs,9,14,model,conditional probability,of,target sequence,conditional probability of target sequence,0.6095684170722961
Blogs,9,14,model,target sequence,given,input sequence,target sequence given input sequence,0.7053357362747192
Blogs,9,14,model,input sequence,generated from,last layer,input sequence generated from last layer,0.6178571581840515
Blogs,9,14,model,predict,has,conditional probability,predict has conditional probability,0.6266911625862122
Blogs,9,14,model,model,has,lstm,model has lstm,0.5196293592453003
Blogs,9,15,model,generated sequence,length different from,source text,generated sequence length different from source text,0.7207528352737427
Blogs,9,15,model,model,has,generated sequence,model has generated sequence,0.6522485017776489
Blogs,9,17,model,lstm,on,multiple language pairs simultaneously,lstm on multiple language pairs simultaneously,0.56537264585495
Blogs,9,17,model,model,training,lstm,model training lstm,0.6250404715538025
Blogs,9,20,model,model,has,reversing the order of input,model has reversing the order of input,0.5835338234901428
Blogs,9,28,model,hypothesis,by doing,beam search,hypothesis by doing beam search,0.6068265438079834
Blogs,9,28,model,created,by doing,beam search,created by doing beam search,0.6068382859230042
Blogs,9,28,model,stopped,has,when,stopped has when,0.6555553078651428
Blogs,9,28,model,when,has,reaches,when has reaches,0.66739422082901
Blogs,9,28,model,reaches,has,< eos >   ( end of string character ),reaches has < eos >   ( end of string character ),0.6083897948265076
Blogs,9,28,model,model,has,hypothesis,model has hypothesis,0.5652445554733276
Blogs,9,30,model,exploding gradients,scale,gradients,exploding gradients scale gradients,0.7238034009933472
Blogs,9,30,model,gradients,for,each batch,gradients for each batch,0.6921407580375671
Blogs,9,30,model,model,To deal with,exploding gradients,model To deal with exploding gradients,0.7037570476531982
Blogs,9,30,model,model,scale,gradients,model scale gradients,0.7761628031730652
Blogs,9,22,results,both short and long term predictions,of,lstm,both short and long term predictions of lstm,0.5622470378875732
Blogs,9,22,results,results,enhances,both short and long term predictions,results enhances both short and long term predictions,0.6045445203781128
Blogs,9,32,results,best results,obtained with,ensemble of lstms,best results obtained with ensemble of lstms,0.6267778873443604
Blogs,9,32,results,ensemble of lstms,differ in,random initializations,ensemble of lstms differ in random initializations,0.6352350115776062
Blogs,9,32,results,ensemble of lstms,differ in,random order of mini-batches,ensemble of lstms differ in random order of mini-batches,0.6778847575187683
Blogs,9,32,results,results,has,best results,results has best results,0.542218804359436
Blogs,9,34,results,translation,showed,some surprising long length results,translation showed some surprising long length results,0.670941174030304
Blogs,9,34,results,results,has,translation,results has translation,0.5237377285957336
Blogs,9,35,results,representations,sensitive to,order of words,representations sensitive to order of words,0.7073675394058228
Blogs,9,35,results,fairly insensitive,to,replacement,fairly insensitive to replacement,0.6213282346725464
Blogs,9,35,results,replacement,of,active voice,replacement of active voice,0.6376225352287292
Blogs,9,35,results,active voice,with,passive voice,active voice with passive voice,0.6754485368728638
Blogs,9,39,results,lstm ( s ),shown to be,surprisingly good,lstm ( s ) shown to be surprisingly good,0.6075382828712463
Blogs,9,39,results,surprisingly good,on,long sentences,surprisingly good on long sentences,0.5183000564575195
Blogs,9,39,results,results,has,lstm ( s ),results has lstm ( s ),0.530881941318512
Blogs,10,42,ablation-analysis,character - level models,used as,replacement,character - level models used as replacement,0.6404147148132324
Blogs,10,42,ablation-analysis,character - level models,yields,improvement,character - level models yields improvement,0.7080071568489075
Blogs,10,42,ablation-analysis,replacement,for,standard unk replacement technique,replacement for standard unk replacement technique,0.6460970044136047
Blogs,10,42,ablation-analysis,replacement,yields,improvement,replacement yields improvement,0.7111728191375732
Blogs,10,42,ablation-analysis,standard unk replacement technique,in,nmt,standard unk replacement technique in nmt,0.538557231426239
Blogs,10,42,ablation-analysis,improvement,up to,+ 7.9 bleu points,improvement up to + 7.9 bleu points,0.5887666344642639
Blogs,10,42,ablation-analysis,ablation analysis,has,character - level models,ablation analysis has character - level models,0.5349379181861877
Blogs,10,43,ablation-analysis,attention,important for,character - based models,attention important for character - based models,0.6833028793334961
Blogs,10,43,ablation-analysis,ablation analysis,has,attention,ablation analysis has attention,0.5569024085998535
Blogs,10,10,baselines,baselines,has,unk replacement technique,baselines has unk replacement technique,0.569451630115509
Blogs,10,13,baselines,words,as,independent entities,words as independent entities,0.5151323676109314
Blogs,10,26,baselines,hidden state,of,first layer,hidden state of first layer,0.5968194007873535
Blogs,10,26,baselines,same - path target generation approach,Use,context vector,same - path target generation approach Use context vector,0.634121835231781
Blogs,10,26,baselines,context vector,just before,softmax,context vector just before softmax,0.6230598092079163
Blogs,10,26,baselines,softmax,of,word- level nmt ),softmax of word- level nmt ),0.5875101089477539
Blogs,10,39,baselines,baselines,has,purely character based hybrid,baselines has purely character based hybrid,0.5620855689048767
Blogs,10,35,experiments,wmt '15 translation,from,english into czech,wmt '15 translation from english into czech,0.6215435862541199
Blogs,10,35,experiments,wmt '15 translation,as,test set,wmt '15 translation as test set,0.4879237711429596
Blogs,10,35,experiments,wmt '15 translation,as,test set,wmt '15 translation as test set,0.4879237711429596
Blogs,10,35,experiments,newstest 2013 ( 3000 sentences ),as,dev set,newstest 2013 ( 3000 sentences ) as dev set,0.49998152256011963
Blogs,10,35,experiments,newstest 2015 ( 2656 sentences ),as,test set,newstest 2015 ( 2656 sentences ) as test set,0.5135103464126587
Blogs,10,25,hyperparameters,source representation,layers of,lstm,source representation layers of lstm,0.6478192806243896
Blogs,10,25,hyperparameters,lstm,initialized with,zero hidden states and cell values,lstm initialized with zero hidden states and cell values,0.7868528366088867
Blogs,10,25,hyperparameters,hyperparameters,For,source representation,hyperparameters For source representation,0.5470916628837585
Blogs,10,28,hyperparameters,hyperparameters,has,training objective j = j + ?j,hyperparameters has training objective j = j + ?j,0.547439694404602
Blogs,10,11,model,post-processing step,replaces,unk tokens,post-processing step replaces unk tokens,0.7465131282806396
Blogs,10,11,model,unk tokens,with,actual words,unk tokens with actual words,0.6157563924789429
Blogs,10,11,model,model,has,post-processing step,model has post-processing step,0.5583290457725525
Blogs,10,16,model,model,has,word- level nmt deep lstm encoder-decoder,model has word- level nmt deep lstm encoder-decoder,0.517128050327301
Blogs,10,17,model,model,has,global attention mechanism,model has global attention mechanism,0.5221193432807922
Blogs,10,20,model,deep lstm model,to generate,on - the-fly representation of rare words,deep lstm model to generate on - the-fly representation of rare words,0.6540850400924683
Blogs,10,20,model,model,has,deep lstm model,model has deep lstm model,0.4884766638278961
Blogs,10,23,model,model,trained,easily,model trained easily,0.7323066592216492
Blogs,10,23,model,easily,in,end-to - end fashion,easily in end-to - end fashion,0.585517406463623
Blogs,10,23,model,model,trained,easily,model trained easily,0.7323066592216492
Blogs,10,23,model,model,has,model,model has model,0.5623406171798706
Blogs,10,27,model,target generation approach,Learn,new weight matrix w,target generation approach Learn new weight matrix w,0.6493123173713684
Blogs,10,27,model,new weight matrix w,to generate,context vector,new weight matrix w to generate context vector,0.6765083074569702
Blogs,10,27,model,model,has,target generation approach,model has target generation approach,0.5756717920303345
Blogs,10,31,model,final hidden state,from,character - level decoder,final hidden state from character - level decoder,0.560063362121582
Blogs,10,31,model,final hidden state,interpreted as,representation of unk token,final hidden state interpreted as representation of unk token,0.5998888611793518
Blogs,10,31,model,character - level decoder,interpreted as,representation of unk token,character - level decoder interpreted as representation of unk token,0.6087866425514221
Blogs,10,31,model,model,has,final hidden state,model has final hidden state,0.5575794577598572
Blogs,10,32,model,unk,fed to,word - level decoder,unk fed to word - level decoder,0.6943899989128113
Blogs,10,32,model,word - level decoder,to decouple,execution,word - level decoder to decouple execution,0.717235803604126
Blogs,10,32,model,execution,for,character - level model,execution for character - level model,0.6114242672920227
Blogs,10,32,model,model,has,unk,model has unk,0.6739746332168579
Blogs,10,33,model,beam search decoder,run at,word level,beam search decoder run at word level,0.7035579681396484
Blogs,10,33,model,word level,to find,best translation,word level to find best translation,0.6044549345970154
Blogs,10,33,model,best translation,using,word nmt alone,best translation using word nmt alone,0.6564974784851074
Blogs,10,33,model,testing,has,beam search decoder,testing has beam search decoder,0.5733430981636047
Blogs,10,33,model,model,During,testing,model During testing,0.7070921659469604
Blogs,10,34,model,character - level encoder,to generate,words,character - level encoder to generate words,0.6964805722236633
Blogs,10,34,model,words,in place of,unk,words in place of unk,0.7246081829071045
Blogs,10,34,model,unk,to minimise,combined loss,unk to minimise combined loss,0.6067226529121399
Blogs,10,34,model,model,has,character - level encoder,model has character - level encoder,0.5174731612205505
Blogs,10,7,results,faster and easier to train,compared to,character models,faster and easier to train compared to character models,0.6170259118080139
Blogs,10,7,results,advantages,has,faster and easier to train,advantages has faster and easier to train,0.5172276496887207
Blogs,10,7,results,results,has,advantages,results has advantages,0.5523803234100342
Blogs,10,8,results,unknown words,in,translations,unknown words in translations,0.539714515209198
Blogs,10,8,results,need to be removed,using,unk replacement techniques,need to be removed using unk replacement techniques,0.7387961149215698
Blogs,10,21,results,advantages,has,simplified architecture,advantages has simplified architecture,0.5309112071990967
Blogs,10,21,results,results,has,advantages,results has advantages,0.5523803234100342
Blogs,10,22,results,efficiency,through,precomputation,efficiency through precomputation,0.7037955522537231
Blogs,10,22,results,representations,for,rare sources words,representations for rare sources words,0.606683611869812
Blogs,10,22,results,representations,computed at once before,each mini-batch,representations computed at once before each mini-batch,0.7395538091659546
Blogs,10,22,results,precomputation,has,representations,precomputation has representations,0.5873926877975464
Blogs,10,22,results,results,has,efficiency,results has efficiency,0.5496513247489929
Blogs,10,29,results,- total loss j - loss,in,regular word- level nmt,- total loss j - loss in regular word- level nmt,0.49176689982414246
Blogs,10,29,results,j,has,- total loss j - loss,j has - total loss j - loss,0.5806998610496521
Blogs,10,29,results,results,has,j,results has j,0.5446270704269409
Blogs,10,29,results,results,has,- total loss j - loss,results has - total loss j - loss,0.5576586723327637
Blogs,10,36,results,results,Metrics,case-sensitive nist bleu,results Metrics case-sensitive nist bleu,0.687124490737915
Blogs,10,41,results,new state - of - the - art result,for,english - czech translation,new state - of - the - art result for english - czech translation,0.5809069275856018
Blogs,10,41,results,new state - of - the - art result,with,19.9 bleu,new state - of - the - art result with 19.9 bleu,0.6004471778869629
Blogs,10,44,results,character models,with,shorter time-step backpropagation,character models with shorter time-step backpropagation,0.5875398516654968
Blogs,10,44,results,shorter time-step backpropagation,perform,inferior,shorter time-step backpropagation perform inferior,0.6614951491355896
Blogs,10,44,results,results,has,character models,results has character models,0.5129165649414062
Blogs,10,45,results,separate - path strategy,has,outperforms,separate - path strategy has outperforms,0.6170004606246948
Blogs,10,45,results,outperforms,has,same - path strategy,outperforms has same - path strategy,0.6295241713523865
Blogs,10,45,results,results,has,separate - path strategy,results has separate - path strategy,0.5336342453956604
Blogs,10,46,results,results,has,rare word embeddings,results has rare word embeddings,0.5168473124504089
Blogs,10,47,results,spearman correlation,between,similarity scores,spearman correlation between similarity scores,0.6506116390228271
Blogs,10,47,results,similarity scores,assigned by,humans,similarity scores assigned by humans,0.667601466178894
Blogs,10,47,results,results,Compare,spearman correlation,results Compare spearman correlation,0.6512306332588196
Blogs,10,48,results,recursive neural network model,uses,morphological analyser,recursive neural network model uses morphological analyser,0.5566344261169434
Blogs,10,48,results,outperforms,has,recursive neural network model,outperforms has recursive neural network model,0.6113017201423645
Blogs,10,48,results,results,has,outperforms,results has outperforms,0.6657275557518005
Blogs,11,18,baselines,translation,yields,unknown target - language - word,translation yields unknown target - language - word,0.682989239692688
Blogs,11,18,baselines,unknown target - language - word,replace it with,respective ( untranslated ) word,unknown target - language - word replace it with respective ( untranslated ) word,0.6155814528465271
Blogs,11,18,baselines,respective ( untranslated ) word,from,source text,respective ( untranslated ) word from source text,0.5328945517539978
Blogs,11,11,experimental-setup,convolutions,of size,620xw,convolutions of size 620xw,0.7271462082862854
Blogs,11,11,experimental-setup,620xw,to,tensor,620xw to tensor,0.6196730136871338
Blogs,11,11,experimental-setup,experimental setup,Apply,convolutions,experimental setup Apply convolutions,0.6153701543807983
Blogs,11,13,experimental-setup,max-over -time,results of,convolutions,max-over -time results of convolutions,0.6889854669570923
Blogs,11,13,experimental-setup,experimental setup,Apply,max-over -time,experimental setup Apply max-over -time,0.62562096118927
Blogs,11,14,experimental-setup,experimental setup,has,reshape to 1d- vector,experimental setup has reshape to 1d- vector,0.55172199010849
Blogs,11,16,experimental-setup,1024 - dimensional vectors,has,one per word ),1024 - dimensional vectors has one per word ),0.6214354634284973
Blogs,11,16,experimental-setup,experimental setup,get,1024 - dimensional vectors,experimental setup get 1024 - dimensional vectors,0.5397477149963379
Blogs,11,2,experiments,actions projects wiki,has,security insights,actions projects wiki has security insights,0.5820913910865784
Blogs,11,4,model,vector,of,each word,vector of each word,0.6156964302062988
Blogs,11,4,model,each word,on,character -level,each word on character -level,0.5455950498580933
Blogs,11,5,model,character -,treat them in,similar way,character - treat them in similar way,0.6550830602645874
Blogs,11,5,model,similarities,between,words,similarities between words,0.71851646900177
Blogs,11,5,model,similarities,treat them in,similar way,similarities treat them in similar way,0.6480716466903687
Blogs,11,5,model,character -,has,similarities,character - has similarities,0.5699284076690674
Blogs,11,5,model,model,spot,character -,model spot character -,0.6793636083602905
Blogs,11,8,model,each word,of,source text,each word of source text,0.5850286483764648
Blogs,11,8,model,model,treat,each word,model treat each word,0.6031465530395508
Blogs,11,9,model,each word,apply,model,each word apply model,0.6143839359283447
Blogs,11,9,model,model,from,character - aware neural language models,model from character - aware neural language models,0.5574986934661865
Blogs,11,9,model,model,Embed,each character,model Embed each character,0.7229964733123779
Blogs,11,9,model,each character,into,620 - dimensional space,each character into 620 - dimensional space,0.6021554470062256
Blogs,11,9,model,model,To,each word,model To each word,0.5587555766105652
Blogs,11,10,model,vectors,next to,each other,vectors next to each other,0.7147405743598938
Blogs,11,10,model,vectors,resulting in,2d - tensor,vectors resulting in 2d - tensor,0.6656720638275146
Blogs,11,10,model,each other,resulting in,2d - tensor,each other resulting in 2d - tensor,0.6605316400527954
Blogs,11,10,model,2d - tensor,in which,each column,2d - tensor in which each column,0.5879557132720947
Blogs,11,12,model,tanh,after,convolutions,tanh after convolutions,0.692857027053833
Blogs,11,12,model,model,Apply,tanh,model Apply tanh,0.6333914995193481
Blogs,11,15,model,model,Apply,two highway -layers,model Apply two highway -layers,0.6479944586753845
Blogs,11,17,model,align and translate,to,vectors,align and translate to vectors,0.6176713705062866
Blogs,11,17,model,align and translate,yielding,translation,align and translate yielding translation,0.71258544921875
Blogs,11,17,model,translation,to,target language,translation to target language,0.5147458910942078
Blogs,11,6,results,only,for,source language,only for source language,0.5779962539672852
Blogs,11,6,results,only,for,target language,only for target language,0.5838176012039185
Blogs,11,6,results,only,not for,target language,only not for target language,0.6250386834144592
Blogs,11,6,results,source language,not for,target language,source language not for target language,0.6437147259712219
Blogs,11,6,results,results,do,only,results do only,0.5556902885437012
