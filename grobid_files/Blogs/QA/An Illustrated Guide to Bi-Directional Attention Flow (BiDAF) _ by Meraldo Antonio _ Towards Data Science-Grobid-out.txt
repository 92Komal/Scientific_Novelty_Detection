title
An Illustrated Guide to Bi-Directional Attention Flow (BiDAF)

abstract
This article illustrates the workings of BiDAF, an NLP model that has pushed the envelope in the Question and Answer domain. he year 2016 saw the publication of BiDAF by a team at University of Washington. BiDAF handily beat the best Q&A models at that time and for several weeks topped the leaderboard of the Stanford Question and Answering Dataset (SQuAD), arguably the most well-known Q&A dataset. Although BiDAF's performance has since been surpassed, the model remains influential in the Q&A domain. The technical innovation of BiDAF inspired the subsequent development of competing models such as ELMo and BERT, by which BiDAF was eventually dethroned. When I first read the original BiDAF paper, I was rather overwhelmed by how seemingly complex it was.

BiDAF exhibits a modular architecture -think of it as a composite structure made out of lego blocks with the blocks being "standard" NLP elements such as GloVe, CNN, LSTM and attention. The problem with understanding BiDAF is that there are just so many of these blocks to learn about and the ways they are combined can seem rather "hacky" at times. This complexity, coupled with the rather convoluted notations used in the original paper, serves as a barrier to understanding the model. In this article series, I will deconstruct how BiDAF is assembled and describe each component of BiDAF in (hopefully) an easy-todigest manner. Copious amount of pictures and diagrams will be provided to illustrate how these components fit together. Ability to answer non-factoid queries. Factoid Queries are questions whose answers are short factual statements. Most Queries that begin with "who", "where" and "when" are factoid because they expect concise facts as answers. Non-factoid Queries, simply put, are all questions that are not factoids. The non-factoid camp is very broad and includes questions that require logics and reasoning (e.g. most "why" and "how" questions) and those that involve mathematical calculations, ranking, sorting, etc. Another quick note: as you may have noticed, I have been capitalizing the words "Context", "Query" and "Answer". This is intentional. These terms have both technical and non-technical meaning and the capitalization is my way of indicating that I am using these words in their specialized technical capacities. With this knowledge at hand, we're now ready to explore how BiDAF is structured. Let's dive in! 

 Overview of BiDAF Structure BiDAF's ability to pinpoint the location of the Answer within a Context stems from its layered design. Each of these layers can be thought of as a transformation engine that transforms the vector representation of words; each transformation is accompanied by the inclusion of additional information. The BiDAF paper describes the model as having 6 layers, but I'd like to think of BiDAF as having 3 parts instead. These 3 parts along with their functions are briefly described below. 

 Embedding Layers BiDAF has 3 embedding layers whose function is to change the representation of words in the Query and the Context from strings into vectors of numbers. 

 Attention and Modeling Layers These Query and Context representations then enter the attention and modeling layers. These layers use several matrix operations to fuse the information contained in the Query and the Context. The output of these steps is another representation of the Context that contains information from the Query. This output is referred to in the paper as the "Query-aware Context representation." 

 Output Layer The Query-aware Context representation is then passed into the output layer, which will transform it to a bunch of probability values. These probability values will be used to determine where the Answer starts and ends. A simplified diagram that depicts the BiDAF architecture is provided below: Upgrade Open in app Here is the plan: Part 1 (this article) will provide an overview of BiDAF. Part 2 will talk about the embedding layers T Upgrade Open in app 21/02/2022, 21:17 An Illustrated Guide to Bi-Directional Attention Flow (BiDAF) | by Meraldo Antonio | Towards Data Science https://towardsdatascience.com/the-definitive-guide-to-bi-directional-attention-flow-d0e96e9e666b 2/5 Part 3 will talk about the attention layers Part 4 will talk about the modeling and output layers. It will also include a recap of the whole BiDAF architecture presented in a very easy language. If you aren't technically inclined, I recommend you to simply jump to part 4. BiDAF vis-?-vis Other Q&A Models Before delving deeper into BiDAF, let's first position it within the broader landscape of Q&A models. There are several ways with which a Q&A model can be logically classified. Here are some of them: Open-domain vs closed-domain. An open-domain model has access to a knowledge repository which it will tap on when answering an incoming Query. The famous IBM-Watson is one example. On the other hand, a closed-form model doesn't rely on pre-existing knowledge; rather, such a model requires a Context to answer a Query. A quick note on terminology here -a "Context" is an accompanying text that contains the information needed to answer the Query, while "Query" is just the formal technical word for question. Abstractive vs extractive. An extractive model answers a Query by returning the substring of the Context that is most relevant to the Query. In other words, the answer returned by the model can always be found verbatim within the Context. An abstractive model, on the other hand, goes a step further: it paraphrases this substring to a more human-readable form before returning it as the answer to the Query. 

 So where does BiDAF fit in within these classification schemes? BiDAF is a closed-domain, extractive Q&A model that can only answer factoid questions. These characteristics imply that BiDAF requires a Context to answer a Query. The Answer that BiDAF returns is always a substring of the provided Context. An example of An example of Context Context, , Query Query and and Answer. Answer. Notice how the Answer can be found verbatim in the Context. Notice how the Answer can be found verbatim in the Context. 

 Upgrade Open in app 21/02/2022, 21:17 An Illustrated Guide to Bi-Directional Attention Flow (BiDAF) | by Meraldo Antonio | Towards Data Science https://towardsdatascience.com/the-definitive-guide-to-bi-directional-attention-flow-d0e96e9e666b 3/5
