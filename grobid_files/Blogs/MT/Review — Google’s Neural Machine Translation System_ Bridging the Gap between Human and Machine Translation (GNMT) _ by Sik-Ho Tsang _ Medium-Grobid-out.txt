title


abstract


07/02/2022, 17:31 Review -Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Tr? https://sh-tsang.medium.com/review-googles-neural-machine-translation-system-bridging-the-gap-between-human-and-machin? 3/8 The structure of bi-directional connections in the first layer of the encoder The bottom encoder layer is bi-directional: the pink nodes gather information from left to right while the green nodes gather information from right to left. The other layers of the encoder are uni-directional. 

 Attention The attention module is similar to the one in Attention Decoder. More specifically, let yi-1 be the decoder-RNN output from the past decoding time step. Attention context ai for the current time step is computed: where AttentionFunction in our implementation is a feed forward network with one hidden layer.  where x=m+x before going into next LSTM. 

 Model Parallelism The n replicas all share one copy of model parameters. n is often around 10. Each replica works on a mini-batch of m sentence pairs at a time, which is often 128 in the experiments. The encoder and decoder networks are partitioned along the depth dimension and are placed on multiple GPUs, effectively running each layer on a different GPU, as shown in the figure of GNMT network architecture. Since all but the first encoder layer are uni-directional, layer i+1 can start its computation before layer i is fully finished. The softmax layer is also partitioned, with each partition responsible for a subset of symbols. (Please feel free to read the paper for more details.) 

 Wordpiece Model or Mixed Word/Character Model 

 Quantizable Model and Quantized Inference Neural machine translation is computationally intensive at inference, making low latency translation difficult, and high volume deployment computationally expensive. For quantized inference, GNMT explicitly constrains the values of these accumulators to be within  [-?, ?]  to guarantee a certain range that can be used for quantization later. The forward computation of an LSTM stack with residual connections is as follows: The weights of fixed-point integer operations are replaced with either 8-bit or 16-bit resolution. There is also softmax clipping ?: (Please feel free to read the paper for more details.) 

 Experimental Results 

 ML Training Models Single model results on  WMT En > Fr (newstest2014)  The best vocabulary size for the mixed word-character model is 32K. The best model WPM-32K, achieves a BLEU score of 38.95. Note that this BLEU score represents the averaged score of 8 models. The maximum BLEU score of the 8 models is higher at 39.37. Google's translation production corpora are two to three decimal orders of magnitudes bigger than the WMT corpora. GMNT reduces translation errors by more than 60% compared to the PBMT model on these major pairs of languages. 1.3. Decoder 8 LSTM layers are used for the decoder. Beam search is used during decoding to find the sequence Y that maximizes a score function s(Y, X) given a trained model. During beam search, 8-12 hypotheses are typically kept but it is found that using fewer (4 or 2) has only slight negative effects on BLEU scores. Two important refinements are used to the pure max-probability based beam search algorithm: a coverage penalty [42] and length normalization. 

 This speeds up search by 30%-40% when run on CPUs.(Please feel free to read the paper for more details.)1.4. Residual Connections Open in app 07/02/2022, 17:31 Review -Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Tr? https://sh-tsang.medium.com/review-googles-neural-machine-translation-system-bridging-the-gap-between-human-and-machin? 4/8 Left: Normal Stacked LSTM, Right: Stacked LSTM with Residual Connections Left: Deep normal stacked LSTMs are difficult to train due to exploding and vanishing gradient problems: It is found that simple stacked LSTM layers work well up to 4 layers, barely with 6 layers, and very poorly beyond 8 layers. Right: Residual connections greatly improve the gradient flow in the backward pass, 8 LSTM layers are used for the encoder and decoder: 

 There are two categories: Word-based and character-based models. In this work, GMNT proposes wordpiece model. To be brief, word-based model predicts word by word, and rare words are difficult to handle. Character-based model predicts character by character, the meaning of words is somehow lost. An example of turning words into wordpieces: Word: Jet makers feud over seat width with big orders at stake Open in app 07/02/2022, 17:31 Review -Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Tr? https://sh-tsang.medium.com/review-googles-neural-machine-translation-system-bridging-the-gap-between-human-and-machin? 5/8 Wordpieces: _J et _makers _fe ud _over _seat _width _with _big _orders _at _stake Wordpieces achieve a balance between the flexibility of characters and efficiency of words. (Please feel free to read the paper for more details.) 4. Model Training The standard maximum-likelihood (ML) training objective is used: But this does not directly improve the BLEU scores. In brief, model refinement is done using the expected reward objective by reinforcement learning (RL): where r(Y, Y *(i)) denotes the per-sentence score, and we are computing an expectation over all of the output sentences Y, up to a certain length. To further stabilize training, a linear combination of ML and RL objectives is optimized as: Due to the disadvantage of BLEU score, GLEU score is proposed. 

 's Neural Machine Translation System: Bridging the Gap between Human and Machine Tr? https://sh-tsang.medium.com/review-googles-neural-machine-translation-system-bridging-the-gap-between-human-and-machin? 6/8 During training of the model, full-precision is used. The only constraints added to the model during training are the clipping. Log perplexity vs. steps And it is shown that it does not affect the training at all. Model inference on CPU, GPU and TPU Inference using CPU is faster than the one in GPU due to data transfer.TPU is optimized which makes the inference much faster. 

 's Neural Machine Translation System:  Bridging the Gap between Human and Machine Tr? https://sh-tsang.medium.com/review-googles-neural-machine-translation-system-bridging-the-gap-between-human-and-machin? 8/86.4. Results on Production DataHistogram of side-by-side scores on 500 sampled sentences from Wikipedia and news websites for a typical language pair, here English > Spanish (PBMT blue, GNMT red, Human orange)Mean of side-by-side scores on production data
