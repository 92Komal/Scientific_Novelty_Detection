title


abstract


WHAT IS COVERAGE LOSS? The coverage loss involves creating a coverage vector that's the sum of attention over all previous time steps. With the coverage vector, coverage loss is compute by minimising between the attention vector and the coverage vector. 

 PG NETWORK TRANSFORMER With PG Network, our transformer has the ability to copy or generate words at each given time step. The generation probability is computed using the hidden state, context vector, and decoder input.  on the transformer model and assess the impact it has on abstractive summarisation. The mechanisms include n-gram blocking, coverage loss, and pointer-generator (PG) network. The mechanisms attempt to alleviate the repetition and factual inconsistent problem. Results show that ROUGE scores improved as a result of these mechanisms. ? Natural Language Processing 365 ? ? ? ? ? ? ? 21/02/2022, 21:34 Day 206: NLP Papers Summary -Transformers and Pointer-Generator Networks for Abstractive Summarizat? https://ryanong.co.uk/2020/07/24/day-206-nlp-papers-summary-transformers-and-pointer-generator-networks-for-abstractive-s? 2blocking is added to our decoder. Decoder uses beam search to construct summaries and as it is selecting the next word, n-gram blocking would eliminates those words that would lead to an n-gram that already exists within the beam. 

 NLP Papers Summary -Transformers and Pointer-Generator Networks for Abstractive Summarizat? https://ryanong.co.uk/2020/07/24/day-206-nlp-papers-summary-transformers-and-pointer-generator-networks-for-abstractive-s? 3Papers Summary -Transformers and Pointer-Generator Networks for Abstractive Summarizat? https://ryanong.co.uk/2020/07/24/day-206-nlp-papers-summary-transformers-and-pointer-generator-networks-for-abstractive-s? 4/7 RESULTS The results are displayed below. Note that the results are generated from only 24 hours of training and this is signi cant less than PG network, which took 4+ days and Sanjabi's transformer, which took 2+ days. Comparing the transformer baseline with our PG-transformer, we can see that PG network was able to improve the performance of our model. We further improved the performance of our PG-transformer by adding coverage and n-gram blocking mechanism. We found that n-gram block yielded a much larger performance increase than coverage loss. Baseline transformer generated summaries with lots of repetition and unable to handle OOV words as shown in the qualitative analysis below. Our PG-transformer was able to reduce OOV mishandling but still suffer from factual inconsistency. With the added coverage, we see that the summaries don't have repetition problem anymore, however, repetition starts to appear in ideas and phrases level. The n-gram blocking eliminates all repetitions. Papers Summary -Transformers and Pointer-Generator Networks for Abstractive Summarizat? https://ryanong.co.uk/2020/07/24/day-206-nlp-papers-summary-transformers-and-pointer-generator-networks-for-abstractive-s? 5Papers Summary -Transformers and Pointer-Generator Networks for Abstractive Summarizat? https://ryanong.co.uk/2020/07/24/day-206-nlp-papers-summary-transformers-and-pointer-generator-networks-for-abstractive-s? 6
