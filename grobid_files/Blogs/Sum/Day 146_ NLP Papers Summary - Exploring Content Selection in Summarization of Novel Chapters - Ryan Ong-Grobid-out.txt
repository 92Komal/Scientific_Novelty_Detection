title
Day 146: NLP Papers Summary -Exploring Content Selection In Summarization Of Novel Chapters Objective and Contribution

abstract
Proposed a new summarisation task of summarising novel chapters from online study guides. This is much more challenging than the news summarisation due to the length of the source document and the higher level of paraphrasing. The contributions of the paper is as follow: 1. Proposed a new summarisation task of summarising novel chapters ? Natural Language Processing 365 ? ? ? ? ? ? ?

Experiments and Results For evaluation, we have three extractive models: 1. Hierarchical CNN-LSTM (CB) 2. Seq2seq with attention (K) 

 RNN (N) We experiment with alignment methods applied at both the word and constituent level since our data analysis shows that summary sentences are often selected from different chapters. Our evaluation metrics is ROUGE-1, 2, L, and METEOR. Each chapter has 2 -5 reference summaries we evaluate our generated summaries against all of them. 

 RESULTS The results above compared the performance of three different extractive models as well as the performance difference of using different alignment methods. We can see that our proposed alignment method outperformed the baseline method in all three extractive models. All three models seem to perform similarly using our extractive targets, suggesting the importance of selecting the appropriate method to generate extractive targets. Given the unreliability of ROUGE, we perform human evaluation and compute the pyramid score of each alignment methods on our best performing model (CB). The crowd workers are asked to identify which generated summary best convey the sampled reference summary content. The results are displayed below. 

 Conclusion and Future Work We have shown that sentence-level, stable-matched alignment method with R-wtd similarity metric performed better than previous method of computing gold extractive summaries. However, there seem to be a contradictory in automatic and human evaluation on whether extraction is better at the sentence or constituent level. We speculate that this might be because we didn't include the additional context when scoring the summaries of extracted constituents and so the irrelevant context didn't go against the system whereas in the human evaluation, we do include sentence context and so fewer constituents are included in the generated summary. In future work, we plan on examining how we can combine constituents to make uent sentences without including irrelevant context. We would also like to explore abstractive summarisation, to examine if language models would be effective in our domain. This could be challenging as language models typically has a limit of 512 tokens. The truncation of our documents might hurt the performance of our novel chapter summarisation model.  
