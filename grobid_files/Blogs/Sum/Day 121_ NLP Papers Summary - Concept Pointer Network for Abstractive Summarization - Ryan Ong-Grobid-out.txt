title
Data Science Natural Language Processing NLP Papers Summary

abstract
Objective and Contribution Proposed Concept Pointer Network for abstractive summarisation, which uses knowledgebased and context-aware conceptualisations to derive a set of candidate concepts. The model would then choose between the concept set and the original source text when generating abstractive summaries. Both automatic and human evaluation were conducted on generated summaries.

This context vector is feed to our decoder, where it will use the context vector to determine the probability to generating new words (p_gen) from our vocabulary distribution. 

 CONCEPT POINTER GENERATOR Firstly, we use the Microsoft Concept Graph to map a word to its related concepts. This knowledge base covers a huge concept space and the relationships between concepts and entities are probabilistic depending on how strongly related they are. Essentially, the concept graph will take in the word and estimates the probability that this word belongs to a particular concept, p(c|x). With probabilities, this means that given each word, the concept graph will have a set of concept candidates (with different con dence level) that it believes the word belongs to. In order for our model to select the right concept candidate, for example, distinguishing between fruit and company concept for the word "apple", we will use the context vector from the encoder-decoder framework.  

 Experimental Setup and Results There are two evaluation datasets: Gigaword and DUC-2004. The evaluation metric is the ROUGE score. 

 MODELS COMPARISON There are 8 baseline models: 1  

 RESULTS In table 1, our concept pointer outperformed all the baseline models on all metrics except RG-2 on Gigaword (CGU scored the highest). In table 2, we show that the summaries generated by concept pointer has the lowest percentage of UNK words, alleviated the OOV problem. In table  3 , we showcase the abstractiveness of our generated summaries. We show that the summaries ?  

 CONTEXT-AWARE CONCEPTUALISATION We want to measure the impact of concept update strategy and so we have experimented with different number of concept candidates. The results are as shown below. There are only small variation in ROUGE scores between different number of concept candidates. 

 HUMAN EVALUATIONS We conducted human evaluations where each volunteer has to answer the following questions: 1. Abstraction -How appropriate are the abstract concepts in the summary?  The proposed Concept Pointer Network doesn't simply copy text from the source document, it would also generate new abstract concepts from human knowledge as shown below: On top of our novel model architecture, we also proposed a distant supervised learning technique to allow our model to adapt to different datasets. Both automatic and human evaluation shown strong improvement over SOTA baselines. NLP Papers Summary -Concept Pointer Network for Abstractive Summarization -Ryan Ong https://ryanong.co.uk/2020/04/30/day-121-nlp-papers-summary-concept-pointer-network-for-abstractive-summarization/ 3/10 ENCODER-DECODER The encoder-decoder framework consists of a two-layer bidirectional LSTM-RNN encoder and an one-layer LSTM-RNN decoder with attention mechanism. Each word in the input sequence is represented by the concatenation of the forward and backward hidden states. The context vector is computed by applying the attention mechanism over the hidden state representations. 

 Papers Summary -Concept Pointer Network for Abstractive Summarization -Ryan Ong https://ryanong.co.uk/2020/04/30/day-121-nlp-papers-summary-concept-pointer-network-for-abstractive-summarization/ 4/10 We will use the context vector to update the concept distribution. We compute the updated weights by feeding the current hidden state, the context vector, and the current concept candidate into a softmax classi er. This updated weight is then added to the existing concept probability to factor in the context of the input sequence, allowing us to derive the contextaware concept probability. Our concept pointer network, consists of the normal pointer to the source document as well as a concept pointer to relevant concepts given the source document. The concept pointer is scaled element-wise by the attention distribution and are added to the normal pointer (attention distribution). This would be the copy distribution where the model copies from and it includes concept distribution on top of the usual text distribution over original source document. DISTANT SUPERVISION FOR MODEL ADAPTION If the summary-document pairs of our training set are different than to the testing set, our model would perform poorly. To counter this, we would want to retrain our model to lower this dissimilarity in our nal loss. To do so, we need labels to indicate how close our training set is to our test set. In order to create these labels, we use KL divergence between each training reference summary and a set of documents from the test set. In other words, the training pairs are distantly-labelled. The representations of both reference summaries and documents are computed by summing the constituent word embeddings. This KL divergence loss function is included in the training process and it measures the overall distance between the test set and each of our reference summary-document pairs. This allows us to determine whether our training set is relevant or irrelevant for model adaption. 

 Papers Summary -Concept Pointer Network for Abstractive Summarization -Ryan Ong https://ryanong.co.uk/2020/04/30/day-121-nlp-papers-summary-concept-pointer-network-for-abstractive-summarization 

 temporal attention on decoder to reduce repetition in summary 6. SEASS. Uses selective gate to control information owing from encoder to decoder 7 21/02/2022, 21:41 Day 121: NLP Papers Summary -Concept Pointer Network for Abstractive Summarization -Ryan Ong ? ? . ABS+. Abstractive summarisation model 2. Luong-NMT. LSTM encoder-decoder https://ryanong.co.uk/2020/04/30/day-121-nlp-papers-summary-concept-pointer-network-for-abstractive-summarization/ 5/10 3. RAS-Elman. CNN for encoder and RNN with attention for decoder 4. Seq2seq+att. BiLSTM encoder and LSTM with attention decoder 5. Lvt5k-lsent. Uses . Pointer-generator. Normal PG 8. CGU. Uses 

 convolutional gated unit and self-attention for encoding 

 How readable, relevant, and informative is the summary? We randomly selected 20 examples, each with three different summaries (from three models) and score how often does each type of summary gets pick. The results are shown below, which showcase the concept pointer network outperformed both the seq2seq model and pointer generator. The generated summaries seems to be uent and informative, however, it's still not as abstractive as human reference summaries. 21/02/2022, 21:41 Day 121: NLP Papers Summary -Concept Pointer Network for Abstractive Summarization -Ryan Ong 2. Overall Quality -Conclusion and Future Work ? Ryan Data Scientist ? https://ryanong.co.uk/2020/04/30/day-121-nlp-papers-summary-concept-pointer-network-for-abstractive-summarization/ 7/10 

 On top of our novel model architecture, we also proposed a distant supervised learning technique to allow our model to adapt to different datasets. Both automatic and human evaluation shown strong improvement over SOTA baselines. Source: https://arxiv.org/pdf/1910.08486.pdf Day 121: NLP Papers Summary -Concept Pointer Network for Abstractive Summarization -Ryan Ong 21/02/2022, 21:41 Day 120: NLP Papers Summary -A Simple Theoretical Model of Importance for Summarization Day 122: NLP Papers Summary -Applying BERT to Document Retrieval with Birch ? Ryan Previous Post ? 30th December 2020 Next Post https://ryanong.co.uk/2020/04/30/day-121-nlp-papers-summary-concept-pointer-network-for-abstractive-summarization/ 8/10You May Also Like Day 365: NLP Papers Summary -A Survey on Knowledge Graph Embedding
