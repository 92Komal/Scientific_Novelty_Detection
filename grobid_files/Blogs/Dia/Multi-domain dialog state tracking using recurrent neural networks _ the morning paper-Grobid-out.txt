title


abstract
Multi-domain dialog state tracking using recurrent neural networks | the morning paper https://blog.acolyer.org/2016/07/07/multi-domain-dialog-state-tracking-using-recurrent-neural-networks/ 1/5 a random walk through Computer Science research, by Adrian Colyer Made delightfully fast by Multi-domain dialog state tracking using recurrent neural networks JULY 7, 2016 ~ ADRIAN COLYER Multi-domain dialog state tracking using recurrent neural networks a Mrksic et al. 2015 Suppose you want to build a chatbot for a domain in which you don't have much data yet. What can you do to bootstrap your dialog state tracking system? In today's paper the authors show how to transfer knowledge across domains, and through a series of experiments demonstrate that this can be very effective at quickly improving model performance. Consider a model trained on out of domain data -in this case for answering questions about restaurants, hotels, and tourism, that is then used initialise a model for answering questions about laptops (the in domain). The chart below shows how goal accuracy improves as more in-domain training dialogs are made available. Note that the multi-domain model always does better in this case, and the difference is very signi cant with low amounts of in-domain data. MENU ? the morning paper 13/03/2022, 15:18 Multi-domain dialog state tracking using recurrent neural networks | the morning paper https://blog.acolyer.org/2016/07/07/multi-domain-dialog-state-tracking-using-recurrent-neural-networks/ 2/5

In fact, it turns out that true out-of-domain training data may be even more bene cial than training data from similar domains: Let's take a step back and see how the multi-domain dialog state tracking (DST) training process works. (See yesterday's write-up for a general overview of the dialog state tracking problem). RNN's are well-suited to DST since they can capture contextual information and hence model and label complex sequences. "The approach is particulary well-suited to our goal of building open-domain dialog systems, as it does not require handcrafted domainspeci c resources for semantic interpretation." The high-level approach is easy to understand. First all of the data available (across multiple domains) is used to train a very general belief tracking model. The starting point for this in Henderson et al.'s RNN framework for belief tracking: a single hidden layer RNN that outputs a distribution over all goal slot-value pairs for each user utterance in a dialog. At each turn the input consists of the ASR (automatic speech recognition) hypothesis, the last system action, the current memory vector, and the previous belief state. n-grams are extracted from the input, and to aid generalisation they are delexicalised. In Henderson's model, the delexicalised features are used in addition to the lexicalised (i.e. regular) ones (not as a replacement). This allows generalisation across slots within the same domain. For the multi-domain application studied by authors, all of the There are two factors at play here: exposing the training procedure to substantially different out-of-domain dialogs allows it to learn delexicalised features not present in the in-domain training data. These features are applicable to the Laptops domain, as evidenced by the very strong starting performance. As additional in-domain dialogs are introduced, the delexicalised features not present in the out-of-domain data are learned as well, leading to consistent improvements in belief tracking performance. 

 ? In the context of these results, it is clear that the out-of-domain training data has the potential to be even more bene cial to tracking performance than data from relatively similar domains. This is especially the case when the available in-domain training datasets are too small to allow the procedure to learn appropriate delexicalised features. 

 ? This model learns the most frequent and general dialog features present across the various domains. What difference does the out-of-domain pre-training make? The authors work with six dialog datasets: three based on restaurant bookings (in Cambridge, SF, and Michigan respectively); a tourist information dataset containing dialogs about hotels, restaurants, pubs, and coffee shops; a hotels dataset; and a laptops dataset. Using these they train three different general models:  

 ? To ensure that the model learns the relative importance of different features for each of the slots, we train slot speci c models for each slot across all the available domains. To train these slot-specialised models, the shared RNN's parameters are replicated for each slot and specialised further by performing additional runs of stochastic gradient descent using only the slot-speci c (delexicalised) training data. The all restaurants model uses the three restaurant datasets The R+T+H model trains on the restaurant, tourist, and hotel datasets The R+T+H+L model trains on all of the datasets. As a baseline, we can ask how well these models perform in each of the domains (i.e., with no domain-speci c training at all). Adding in the slot-specialisation phase to the general model further improves performance in the vast majority of experiments. In the table below, moving down through a column The parameters of the three multi-domain models are not slot or even domain speci c. Nonetheless, all of them improve over the domain-speci c model for all but one of their constituent domains? The R+T+H+L model is much better at balancing its performance across all six domains, achieving the highest geometric mean (for accuracy) and still improving over all but one of the domain-specifc models.   dialog state tracking using recurrent neural networks | the morning paper https://blog.acolyer.org/2016/07/07/multi-domain-dialog-state-tracking-using-recurrent-neural-networks/ 3/5 training data from the different (out of domain) domains available is delexicalised, and the combined slot-agnostic delexicalised dialogs are used to train the parameters of a shared RNN model. I.e, the delexicalised features are used instead of of the regular ones, not in addition to. We're not quite done building the shared model at this point. The delexicalised training phase does a good job of extracting general dialog dynamics. But it misses out on the af nity of certain features for certain slots. As an example 'near tagged-slot-value' is likely to be a feature relating to location. This completes the training of the shared general model. The general model can then be specialised for the target domain which enables it to learn domain-speci c behaviour while retaining the previously learned cross-domain dialog patterns. 

 dialog state tracking using recurrent neural networks | the morning paper https://blog.acolyer.org/2016/07/07/multi-domain-dialog-state-tracking-using-recurrent-neural-networks/ 4/5 

 corresponds to adding more out-of-domain training data, and moving right corresponds to slot-specialising the shared model for each slot in the current domain. The really interesting question, and the one we opened with, is what happens when the outof-domain shared model is used to initialise a model for a new domain. We've already seen the results for training a laptop dialog model starting with a shared model built from all the other domains. Here's what happens when the R+T+H datasets (minus Michigan) are used to pretrain a shared model for the Michigan restaurant dialogs: 

 ? https://blog.acolyer.org/2016/07/07/multi-domain-dialog-state-tracking-using-recurrent-neural-networks 

 ? Looks like those available corpora for building dialog systems might come in handy! POSTED IN UNCATEGORIZED MACHINE LEARNING In both experiments, the use of out-of-domain data helps to initialise the model to a much better starting point when the in-domain training dataset is small. The out-of-domain initialisation consistently improves performance: the joint goal accuracy is improved even when the entire in-domain dataset becomes available to the training procedure.? < PREVIOUS Machine learning for dialog state tracking: a review NEXT > Ten challenges in highly-interactive dialog systems
