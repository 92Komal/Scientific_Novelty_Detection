title


abstract
Learning sentence embeddings by Natural Language Inference Unsupervised learning approach seems like a normal way to build word, sentence or document embeddings because it is more generalized such that pre-trained embeddings result can be transfer to other NLP downstream problems. For example, skip-gram in word embeddings and skip-though in sentence embeddings and distributed bag-of-words in paragraph embeddings. "closeup photo of person carrying professional video recorder" by Laura Lee Moreau on Unsplash Conneau et al. noted that supervised learning in ImageNet (Image Classification) doing good job in transferring result to downstream problems. Some features can be transferred to downstream somehow. Therefore, Conneau et al. used textual entailment data to train a sentence embeddings layer which calls InferSent.

which are entailment, contradiction, and neutral. Here is the very sample example: "two apples and walnuts on white towel" by Alex Kotomanov on Unsplash 1. I eat fruit. 

 I eat apple. Intuitively, the relationship is entailment. Authors believe that NLI is a suitable task to understand semantic relationships within sentences such that it helps to build a good embeddings for sentence embeddings for downstream NLP problems. 

 Architecture The overall idea is that two sentences (premise input and hypothesis input) will be transformed by sentence encoder (same weights). After that leveraging 3 matching methods to recognize relations between premise input and hypothesis input. Before conclude the best approach first, we may believe that Attention with BiLSTM should be the best approach as attention mechanism helps to identify important weight. Actually, it may harm when using it in transfer learning. On the other hand, BiLSTM with mean polling perform not very good may due to unable to locate the important part. Hierarchical convolutional networks  (Conneau et al., 2017)  From the experiment result, the best approach is Bi-directional LSTM with max polling.  

 Train embeddings Another approach is training the embeddings by your self. You may either using your own data or using original data set. Since this InferSent uses supervised learning approach to generate sentence embeddings, you need to have a annotated (labeled) data first. Here is the step of going first approach. Clone InferSent original repo to local. Then Execute "get_data.bash" in console such that SNLI ( Stanford Natural Language Inference) and MultiNLI )MultiGenre NLI) corpus will be downloaded and processed. Make sure that you have to execute the following shell script in current folder but not other relative path  

 Take Away To access all code, you can visit my github repo. Compare to other embedding approaches, InferSent uses a supervised learning to compute word vectors. InferSent leverages word embeddings (GloVe/ fastText) to build sentence embeddings.  21/02/2022, 21:50 Learning sentence embeddings by Natural Language Inference | by Edward Ma | Towards Data Science https://towardsdatascience.com/learning-sentence-embeddings-by-natural-language-inference-a50b4661a0b8 2/6 

 by Natural Language Inference | by Edward Ma | Towards Data Science https://towardsdatascience.com/learning-sentence-embeddings-by-natural-language-inference-a50b4661a0b8 3/6 1. Concatenation of two vectors 2. Element-wise product two vectors 3. Absolute element-wise difference of two vectors After the overview, may jump into the architecture of sentence encoders. Conneau et al. evaluated 7 different architectures: of last hidden states of forward and backward GRU 4. Bi-directional LSTM with mean polling 5. Bi-directional LSTM with max polling 6. Self-attentive Network (Attention with BiLSTM) 7. Hierarchical convolutional networks 

 # 5 5 Bi-directional LSTM with max polling(Conneau et al., 2017)    #6 Self-attentive Network Architecture(Conneau et al., 2017)    UpgradeOpen in app21/02/2022, 21:50 Learning sentence embeddings by Natural Language Inference | by Edward Ma | Towards Data Science https://towardsdatascience.com/learning-sentence-embeddings-by-natural-language-inference-a50b4661a0b8 4/6 

 to use InferSent. First of is using a pre-trained embeddings layer in your NLP problems. Another one is building InferSent by your self. Load pre-trained Embeddings Facebook research team provide 2 pre-trained models which are version 1 (based on GloVe) and version 2 (based on fastText). Loading both InferSent pre-trained model and GloVe (or fastText) model than you can encode sentence to vectors. # Init InferSent Model infer_sent_model = InferSent() infer_sent_model.load_state_dict(torch.load(dest_dir + dest_file)) by Natural Language Inference | by Edward Ma | Towards Data Science https://towardsdatascience.com/learning-sentence-embeddings-by-natural-language-inference-a50b4661a0b8 5/6 # Build Vocab for InferSent model model.build_vocab(sentences, tokenize=True) # Encode sentence to vectors model.encode(sentences, tokenize=True) 

 Pretrained model supports both GloVe (version 1) and fasttext (version 2) About Me I am Data Scientist in Bay Area. Focusing on state-of-the-art in Data Science, Artificial Intelligence , especially in NLP and platform related. You can reach me from Medium Blog, LinkedIn or Github. 

 Downloading InferSent pre-trained model. Version 1 is trained by using GloVe while Version 2 leveraged fastText. ./get_data.bash After that, downloading GloVe (and/or fastText) mkdir dataset/GloVe curl -Lo dataset/GloVe/glove.840B.300d.zip http://nlp.stanford.edu/data/glove.840B.300d.zip unzip dataset/GloVe/glove.840B.300d.zip -d dataset/GloVe/ mkdir dataset/fastText curl -Lo dataset/fastText/crawl-300d-2M.vec.zip https://s3-us-west-1.amazonaws.com/fasttext-vectors/crawl- 300d-2M.vec.zip unzip dataset/fastText/crawl-300d-2M.vec.zip -d dataset/fastText/ curl -Lo encoder/infersent1.pkl https://s3.amazonaws.com/senteval/infersent/infersent1.pkl curl -Lo encoder/infersent2.pkl https://s3.amazonaws.com/senteval/infersent/infersent2.pkl Finally, you can execute the following command to train the embeddings layers.python train_nli.py --word_emb_path ./glove.42B.300d.txt For my single GPU VM, it takes about 1 day to finish the training.
