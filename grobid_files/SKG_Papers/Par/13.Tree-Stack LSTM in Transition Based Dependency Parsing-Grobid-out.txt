title
Tree-stack LSTM in Transition Based Dependency Parsing

abstract
We introduce tree-stack LSTM to model state of a transition based parser with recurrent neural networks. Tree-stack LSTM does not use any parse tree based or hand-crafted features, yet performs better than models with these features. We also develop new set of embeddings from raw features to enhance the performance. There are 4 main components of this model: stack's ?-LSTM, buffer's ?-LSTM, actions' LSTM and tree-RNN. All LSTMs use continuous dense feature vectors (embeddings) as an input. Tree-RNN updates these embeddings based on transitions. We show that our model improves performance with low resource languages compared with its predecessors. We participate in CoNLL 2018 UD Shared Task as the "KParse" team and ranked 16th in LAS, 15th in BLAS and BLEX metrics, of 27 participants parsing 82 test sets from 57 languages.

Introduction Recent studies in neural dependency parsing creates an opportunity to learn feature conjunctions only from primitive features.  (Chen and Manning, 2014)  A designer only needs to extract primitive features which may be useful to take parsing actions. However, extracting primitive features from state of a parser still remains critical. On the other hand, representational power of recurrent neural networks should allow a model both to summarize every action taken from the beginning to the current state and tree-fragments obtained until a current state. We propose a method to concretely summarize previous actions and tree fragments within current word embeddings. We employ word and context embeddings from  (K?rnap et al., 2017)  as an initial representer. Our model modifies these embeddings based on parsing actions. These embeddings are able to summarize, children-parent relationship. Finally, we test our system in CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Rest of the paper is organized as follows: Section 2 summarizes related work done in neural transition based dependency parsing. Section 3 describes the models that we implement for tagging, lemmatization and dependency parsing. Section 4 discusses our results and section 5 presents our contributions. 

 Related Work In this section we describe the related work done in neural transition based dependency parsing and morphological analysis. 

 Morphological Analysis and Tagging Finite-state transducers (FST) have an important role in previous morphological analyzers.  (Koskenniemi, 1983)  Unlike modern neural systems, these type of analyzers are language dependent rule based systems. Morphological tagging, on the other hand, tries to solve tagging and analysis problem at the same stage. Koskenniemi proposed conditional random fields (CRFs) based model and Heigold et al. proposed neural network architectures to solve tagging and analysis problem immediately. Modern systems heavily based on word and context based features that we explain in the following paragraph. 

 Embedding Features Chen and Manning, Kiperwasser and Goldberg, use pre-trained word and random part-of-speech (POS) embeddings.  

 Decision Module We name a part of our model, which provides transitions from features, as decision module. Decision module is a neural architecture designed to find best feature conjunctions.  

 Model In this section, we describe MorphNet  (Dayan?k et al., 2018)  used for tagging and lemmatization; and Tree-stack LSTM used for dependency parsing. We train these models separately. MorphNet employs UDPipe  (Straka et al., 2016)  for tokenization to generate conll-u formatted file with missing head and dependency relation columns. Treestack LSTM takes that for dependency parsing. We detail these models in the remaining part of this section. 

 Lemmatization and Part of Speech Tagging We implement MorphNet  (Dayan?k et al., 2018)  for lemmatization and Part of Speech tagging. It is trained on  (Nivre et al., 2018) . MorphNet is a sequence-to-sequence recurrent neural network model used to produce a morphological analysis for each word in the input sentence. The model operates with a unidirectional Long Short Term Memory (LSTM)  (Hochreiter and Schmidhuber, 1997)  encoder to create a character-based word embeddings and a bidirectional LSTM encoder to obtain context embeddings. The decoder consists of two layers LSTM. The input to the MorphNet consists of an N word sentence S = [w 1 , . . . , w N ], where w i is the i'th word in the sentence. Each word is input as a sequence of characters w i = [w i1 , . . . , w iL i ], w ij ? A where A is the set of alphanumeric characters and L i is the number of characters in word w i . The output for each word consists of a stem, a part-of-speech tag and a set of morphological features, e.g. "earn+Upos=verb+Mood=indicative+Tense=past" for "earned". The stem is produced one character at a time, and the morphological information is produced one feature at a time. A sample output for a word looks like [s i1 , . . . , s iR i , f i1 , . . . , f iM i ] where s ij ? A is an alphanumeric character in the stem, R i is the length of the stem, M i is the number of features, f ij ? T is a morphological feature from a feature set such as T = {Verb,Adjective,Mood=Imperative,Tense=Past, . . .}. In Word Encoder we map each character w ij to an A dimensional character embedding vector a ij ? R A .The word encoder takes each word and processes the character embeddings from left to right producing hidden states [h i1 , . . . , h iL i ] where h ij ? R H . The final hidden state e i = h iL i is used as the word embedding for word w i . h ij = LSTM(a ij , h ij?1 ) (1) h i0 = 0 (2) e i = h iL i (3) We model context encoder by using a bidirectional LSTM. The inputs are the word embeddings e 1 , ? ? ? , e N produced by the word encoder. The context encoder processes them in both directions and constructs a unique context embedding for each target word in the sentence. For a word w i I define its corresponding context embedding c i ? R 2H as the concatenation of the forward ? ? c i ? R H and the backward ? ? c i ? R H hidden states that are produced after the forward and backward LSTMs process the word embedding e i . Figure illustrates the creation of the context vector for the target word earned. ? ? c i = LSTM f (e i , ? ? c i?1 ) (4) ? ? c i = LSTM b (e i , ? ? c i+1 ) (5) ? ? c 0 = ? ? c N +1 = 0 (6) c i = [ ? ? c i ; ? ? c i ] (7) The decoder is implemented as a 2-Layer LSTM network that outputs the correct tag for a single target word. By conditioning on the input embeddings and its own hidden state, the decoder learns to generate y i = [y i1 , . . . , y iK i ] where y i is the correct tag of the target word w i in sentence S, y ij ? A ? T represents both stem characters and morphological feature tokens, and K i is the total number of output tokens (stem + features) for word w i . The first layer of the decoder is initialized with the context embedding c i and the second layer is initialized with the word embedding e i . d 1 i0 = relu(W d ? c i ? W db ) (8) d 2 i0 = e i (9) (10) We parameterize the distribution over possible morphological features and characters at each time step as p(y ij |d 2 ij ) = softmax(W s ? d 2 ij ? W sb ) (11) where W s ? R |Y|?H and W sb ? R |Y| where Y = A ? T is the set of characters and morphological features in output vocabulary. 

 Word and Context Embeddings We benefit pre-trained word embeddings from  (K?rnap et al., 2017)  in our parser. Both word and context embeddings are extracted from the language model described in section 3.1 of  (K?rnap et al., 2017) . 

 Features We use limited number of continuous embeddings in parser model.  

 Morphological Feature Embeddings We introduce morphological feature embeddings, which differs from , as an additional input to our model. Each feature is represented with 128 dimensional continuous vector. We experiment that vector sizes lower than 128 reduces the performance of a parser, and higher than 128 does not bring further enhancements. We formulate morphological feature embeddings by adding feature vectors of a word. For example, suppose we are given a word it with following morphological features: Case=Nom and Gen-der=Neut and Number=Sing and Person=3 and PronType=Prs. We basically sum corresponding 5 unique vectors to provide morphological feature embedding. However, our experiments suggest that not all languages benefit from morphological feature embeddings. (See section 4 for details) 

 Dependency Label Embeddings Each distinct dependency label defined in CoNLL 2018 UD Shared Task represented with a 128 dimensional continuous vector. These vectors combined to construct hidden states in tree-RNN part of our model. We randomly initialize these vectors and learned during training. 

 ArcHybrid Transition System We implement the ArcHybrid Transition System which has three components, namely a stack of tree fragments ?, a buffer of unused words ? and a set A of dependency arcs, c = (?, ?, A). Stack is empty, there is no any arcs and, all the words of a sentence are in buffer initially. This system has 3 type of transitions: ? shift(?, b|?, A) = (?|b, ?, A) ? left d (?|s, b|?, A) = (?, b|?, A ? {(b, d, s)}) ? right d (?|s|t, ?, A) = (?|s, ?, A ? {(s, d, t)}) where | denotes concatenation and (b, d, s) is a dependency arc between b (head) and s (modifier) with label d. The system terminates parsing when the buffer is empty and the stack has only one word assumed to be the root. 

 Tree-stack LSTM Tree-stack LSTM has 4 main components: buffer's ?-LSTM, stack's ?-LSTM, actions'-LSTM and tree's tree-RNN or t-RNN in short. We aim to represent each component of the transition system, c = (?, ?, A), with a distinct LSTM similar to . Initial inputs to these LSTMs are embeddings obtained by concatenating the features explained in section 3.3. Buffer's ?-LSTM is initialized with zero hidden state, and fed with input features from last word to the beginning. Similarly, stack's ?-LSTM is also initialized with zero hidden state and fed with input features from the beginning word to the last word of a stack. Actions' LSTM is also started with zero hidden state, and updated after each action. Inputs to ?-LSTM and ?-LSTM are updated via tree-RNN. We update either buffer's or stack's input embeddings based on parsing actions. For instance, suppose we are given ? i a top word in buffer and ? i a final word in stack. The lef t d transition taken in current state. tree-RNN uses concatenation of previous embedding, ? i , and dependency relation embedding (explained in 3.5) as a hidden h t?1 .Input of a tree-RNN is a previous word em- There are 73 distinct actions for shift, labeled left and labeled right actions. We randomly initialize 128 dimensional vector for each labeled action and shift. These vectors become an input for action-LSTM shown in Figure  6 . Concatenation of stack's LSTM, buffer's LSTM and actions' LSTM's final hidden layer becomes an input to MLP which outputs the probabilities for each transition in the next step. 

 Training Our training strategy varies based on training data sizes. We divide datasets into 4 parts: 100k tokens or more, tokens in between 50k and 100k, and more than 20k less than 50k tokens. For languages having more than 50k tokens in training data, we employ morphological featureembeddings as an additional input dimension (see Figure  2 ). However, for languages having tokens less than 50k we do not use this feature dimension. Finally we realize that the languages with more than 100k tokens, using morphological feature embeddings does not improve parsing performance but we use that additional feature dimension. We use 5-fold cross validation for languages without development data. We do not change the LSTMs' hidden dimensions, but record the number of epochs took for convergence. The average of these epochs is used to train a model with whole training set. 

 Optimization and Hyper-Parameters We conduct experiments to find best set of hyperparameters. We start with a dimension of 32 and increase the dimension by powers of two until 512 for LSTM hiddens, 1024 for LM matrix (explained in below). We report the best hyperparameters in this paper. Although the performance does not decrease after the best setting, we choose the minimum-best size not to sacrifice from training speed. All the LSTMs and tree-RNN have hidden dimension of 256. The vectors extracted from LM having dimension of 950, but we reduce that to 512 by a matrix-vector multiplication. This matrix is also learned. We use Adam optimizer with default parameters.  (Kingma and Ba, 2014) .Training is terminated if the performance does not improve for 9 epochs. 

 Results In this section we inspect our best/worst results and the conclusions we obtain during CoNLL 2018 UD Shared Task experiments. We submit our system to CoNLL 2018 UD Shared Task as "KParse" team. Our scoring is provided under the official CoNLL 2018 UD Shared Task website. 1 as well as in Table  4 .1. All experiments are done with UD version 2.2 datasets  (Nivre et al., 2018)  and  (Nivre et al., 2017)  for training and testing respectively. The model improves performance by reducing hand-crafted feature selection. In order to analyze our tree-stack LSTM, we compare that model with K?rnap et al. sharing similar feature interests and transition system with our model. The difference between these two models is that K?rnap et al. based on handcrafted feature selection from state, e.g. number of left children of buffer's first word. However, treestack LSTM only needs raw features and previous parsing actions. Our model comparatively performs better with languages less than 50k training tokens, e.g. sv lines and hu szeged and tr imst. However, when the number of training examples increases the performance improve slightly saturates, e.g. ar padt, en ewt. This may be due to convergence problems of our model. This conclusion 129  We next analyze the performance gain by including morphological features with languages training token in between 50k and 100k. As we deduce from Table  3 , tree-stack LSTM benefits from morphological information with mid-resource languages. However, we could not gain the similar  Figure 1 :Figure 2 : 12 Figure 1: MorphNet illustration for the sentence "Bush earned 340 points in 1969" and target word "earned". 

 Figure 3 :Figure 4 : 34 Figure 3: ? ? LST M processing a sentence. It starts to read from right to left. Each vector ((w i ) represents the concatenation of POS, language and morph-feat embeddings. 

 Figure 5 : 5 Figure 5: Buffer word's embedding update based on left move. Inputs are old embeddings obtained from Table 3.3 . 

 Figure 6 : 6 Figure 6: End-to-end tree-stack model composed of 4 main components, namely, ?-LSTM, ?-LSTM and actions' LSTMs and the tree-RNN. 

 Ballesteros et al.  use   character-based word representation for the stack-LSTM parser. In Alberti et al., end-to-end approach is taken for both word and POS embeddings. In other words, one component of their model has responsibility to generate POS embeddings and the other to generate word embeddings. 

 Chen and Manning uses MLP, Dozat et al. applies BiLSTM stacked with MLP as a decision module. We inspire from Dyer et al.'s stack-LSTM which basically represents each component of a state (buffer, stack and actions) with an LSTM. We found new inputs to tree-RNN, and modify this model to obtain better results. 

 Table 1 : 1 These are POS, word, context, and morphological feature embeddings. Word and context embeddings are pre-trained and not finetuned during training. POS and morphological feature embeddings are randomly initialized and learned during training. Possible features for each word Abbrev Feature c context embedding v word embedding p universal POS tag f morphological features 

 Table 5 : 5 If there is no data from the same language, we pick a parent language from the same family. If there are more than one parent for a language, we select a parent with more training data.We list our selections in Table4. Our official results in CoNLL 2018 UD Shared Task, ranks are given in LAS-MLAS-BLEX order 132 Milan Straka, Jan Haji?, and Jana Strakov?. 2016. UD-Pipe: trainable pipeline for processing CoNLL-U files performing tokenization, morphological analysis, POS tagging and parsing. In Proceedings of the 10th International Conference on Language Resources and Evaluation (LREC 2016). European Language Resources Association, Portoro, Slovenia. Lang code Morp-Feats no Morp-Feats ko gsd 73.74 72.54 got proiel 54.33 53.24 id gsd 75.76 73.97 Table 3: Morphological feature embeddings in some languages having tokens more than 50k and less than 100k in training data performance enhancement with languages more than 100k training tokens. 4.1 Languages without Training Data We have three criteria to choose a trained model for languages without training data. If there is a training corpus with the same language we use that as a parent. 

			 http://universaldependencies.org/conll18/ results.html
