title
Improving Compositional Generalization in Semantic Parsing

abstract
Generalization of models to out-ofdistribution (OOD) data has captured tremendous attention recently. Specifically, compositional generalization, i.e., whether a model generalizes to new structures built of components observed during training, has sparked substantial interest. In this work, we investigate compositional generalization in semantic parsing, a natural test-bed for compositional generalization, as output programs are constructed from sub-components. We analyze a wide variety of models and propose multiple extensions to the attention module of the semantic parser, aiming to improve compositional generalization. We find that the following factors improve compositional generalization: (a) using contextual representations, such as ELMO and BERT, (b) informing the decoder what input tokens have previously been attended to, (c) training the decoder attention to agree with pre-computed token alignments, and (d) downsampling examples corresponding to frequent program templates. While we substantially reduce the gap between in-distribution and OOD generalization, performance on OOD compositions is still substantially lower.

Introduction Neural models trained on large datasets have recently shown great performance on data sampled from the training distribution. However, generalization to out-of-distribution (OOD) scenarios has been dramatically lower  (Sagawa et al., 2019; Kaushik et al., 2020) . A particularly interesting case of OOD generalization is compositional generalization, the ability to systematically generalize to test examples composed of components seen during training. For example, we expect a model that observed the questions "What is the capital of France?" and "What is the * The authors contributed equally. population of Spain?" at training time to generalize to questions such as "What is the population of the capital of Spain?". While humans generalize systematically to such compositions  (Fodor et al., 1988) , models often fail to capture the structure underlying the problem, and thus miserably fail  (Atzmon et al., 2016; Loula et al., 2018; Bahdanau et al., 2019b; Ruis et al., 2020) . Semantic parsing, mapping natural language utterances to structured programs, is a task where compositional generalization is expected, as substructures in the input utterance and output program often align. For example, in "What is the capital of the largest US state?", the span "largest US state" might correspond to an argmax clause in the output program. Nevertheless, prior work  (Finegan-Dollak et al., 2018; Herzig and Berant, 2019; Keysers et al., 2020)  has shown that data splits that require generalizing to new program templates result in drastic loss of performance. However, past work did not investigate how different modeling choices interact with compositional generalization. In this paper, we thoroughly analyze the impact of different modeling choices on compositional generalization in 5 semantic parsing datasets-four that are text-to-SQL datasets, and DROP, a dataset for executing programs over text paragraphs. Following  Finegan-Dollak et al. (2018) , we examine performance on a compositional split, where target programs are partitioned into "program templates", and templates appearing at test time are unobserved at training time. We examine the effect of standard practices, such as contextualized representations ( ?3.1) and grammar-based decoding ( ?3.2). Moreover, we propose novel extensions to decoder attention ( ?3.3), the component responsible for aligning sub-structures in the question and program: (a) supervising attention based on precomputed token alignments, (b) attending over constituency spans, and (c) encouraging the decoder attention to cover the entire input utterance. Lastly, we also propose downsampling examples from frequent templates to reduce dataset bias ( ?3.4). Our main findings are that (i) contextualized representations, (ii) supervising the decoder attention, (iii) informing the decoder on coverage of the input by the attention mechanism, and (iv) downsampling frequent program templates, all reduce the gap in generalization when comparing standard iid splits to compositional splits. For SQL, the gap in exact match accuracy between in-distribution and OOD is reduced from 84.6 ? 62.2 and for DROP from 96.4 ? 77.1. While this is a substantial improvement, the gap between in-distribution and OOD generalization is still significant. All our code and data are publicly available at http://github.com/ inbaroren/improving-compgen-in-semparse. 

 Compositional Generalization Natural language is compositional in a sense that complex utterances are interpreted by understanding the structure of the utterance and the meaning of its parts  (Montague, 1973) . For example, the meaning of "a person below the tree" can be composed from the meaning of "a person", "below" and "tree". By virtue of compositionality, an agent can derive the meaning of new utterances, even at first encounter. Thus, we expect our systems to model this compositional nature of language and generalize to new utterances, generated from subparts observed during training but composed in novel ways. This sort of model generalization is often called compositional generalization. Recent work has proposed various benchmarks to measure different aspects of compositional generalization, showing that current models struggle in this setup. Lake and Baroni (2018) introduce a benchmark called SCAN for mapping a command to actions in a synthetic language, and proposed a data split that requires generalizing to commands that map to a longer sequence of actions than observed during training.  Bahdanau et al. (2019a)  study the impact of modularity in neural models on the ability to answer visual questions about pairs of objects that were not observed during training.  Bahdanau et al. (2019b)  assess the ability of models trained on CLEVR  (Johnson et al., 2017)  to interpret new referring expressions composed of parts observed at training time.  Keysers et al. (2020)  develop a benchmark of Freebase questions and pro- pose a data split such that the test set contains new combinations of knowledge-base constants (entities and relations) that were not seen during training.  Ruis et al. (2020)  proposed gSCAN, which focuses on compositional generalization when mapping commands to actions in a situated environment. In this work, we focus on a specific kind of compositional data split, proposed by  Finegan-Dollak et al. (2018) , that despite its simplicity leads to large drops in performance.  Finegan-Dollak et al. (2018)  propose to split semantic parsing data such that a model cannot memorize a mapping from question templates to programs. To achieve this, they take question-program pairs, and anonymize the entities in the question-program pair with typed variables. Thus, questions that require the same abstract reasoning structure now get mapped to the same anonymized program, referred to as program template. For example, in the top two rows of Figure  1 , after anonymizing the name of a river to the typed variable river name0, two lexicallydifferent questions map to the same program template. Similarly, in the bottom two rows we see two different questions that map to the same program even before anonymization. The data is then split in a manner such that a program template and all its accompanying questions belong to the same set, called the program split. This ensures that all test-set program templates are unobserved during training. For example, in a iid split of the data, it is possible that the question "what is the capital of France?" will appear in the training set, and the question "Name Spain's capital." will appear in the test set. Thus, the model only needs to memorize a mapping from question templates to program templates. However, in the program split, each program template is in either the training set or test set, and thus a model must generalize at test time to new combinations of predicates and entities (see Figure  1  -Program split). We perform the compositional split proposed by  Finegan-Dollak et al. (2018)  on four text-to-SQL datasets from  Finegan-Dollak et al. (2018)  and one dataset for mapping questions to QDMR programs  (Wolfson et al., 2020)  on DROP  (Dua et al., 2019) . Exact experimental details are in ?4. 

 Model Finegan-Dollak et al. (  2018 ) convincingly showed that a program split leads to low semantic parsing performance. However, they examined only a simple baseline parser, disregarding many standard variations that have been shown to improve indistribution generalization, and might affect OOD generalization as well. In this section, we describe variants to both the model and training, and evaluate their effect on generalization in ?5. We examine well-known choices, such as the effect of contextualized representations ( ?3.1) and grammar-based decoding ( ?3.2), as well as several novel extensions to the decoder attention ( ?3.3), which include (a) eliciting supervision (automatically) for the decoder attention distribution, (b) allowing attention over question spans, and (c) encouraging attention to cover all of the question tokens. For DROP, where the distribution over program templates is skewed, we also examine the effect of reducing this bias by downsampling frequent program templates ( ?3.4). Baseline Semantic Parser A semantic parser maps an input question x into a program z, and in the supervised setup is trained from (x, z) pairs. Similar to  Finegan-Dollak et al. (2018) , our baseline semantic parser is a standard sequence-tosequence model  (Dong and Lapata, 2016 ) that encodes the question x with a BiLSTM encoder  (Hochreiter and Schmidhuber, 1997)  over GloVe embeddings  (Pennington et al., 2014) , and decodes the program z token-by-token from left to right with an attention-based LSTM decoder  (Bahdanau et al., 2015) . 

 Contextualized Representations Pre-trained contextualized representations revolutionized natural language processing in recent years, and semantic parsing has been no exception  (Guo et al., 2019; . We hypothesize that better representations for question tokens should improve compositional generalization, because they reduce language variability and thus may help improve the mapping from input to output tokens. We evaluate the effect of using ELMO  and BERT  (Devlin et al., 2019)  to represent question tokens. 1 

 Grammar-Based Decoding A unique property of semantic parsing, compared to other generation tasks, is that programs have a clear hierarchical structure that is based on the target formal language. Decoding the output program token-by-token from left to right  (Dong and Lapata, 2016; Jia and Liang, 2016)  can thus generate programs that are not syntactically valid, and the model must effectively learn the syntax of the target language at training time. Grammar-based decoding resolves this issue and has been shown to consistently improve in-distribution performance  (Rabinovich et al., 2017; Yin and Neubig, 2017) . In grammar-based decoding, the decoder outputs the abstract syntax tree of the program based on a formal grammar of the target language. At each step, a production rule from the grammar is chosen, eventually outputting a topdown left-to-right linearization of the program tree. Because decoding is constrained by the grammar, the model outputs only valid programs. We refer the reader to the aforementioned papers for details on grammar-based decoding. Compositional generalization involves combining known sub-structures in novel ways. In grammar-based decoding, the structure of the output program is explicitly generated, and this could potentially help compositional generalization. We discuss the grammars used in this work in ?4. 

 Decoder Attention Semantic parsers use attention-based decoding: at every decoding step, the model computes a distribution (p 1 . . . p n ) over the question tokens x = (x 1 , . . . , x n ) and the decoder computes its next prediction based on the weighted average n i=1 p i ? h i , where h i is the encoder representation of x i . Attention has been shown to both improve in-distribution performance  (Dong and Lapata, 2016)  and also lead to better compositional How many yards longer was L. 's pass to E. than V. Y. 's shortest pass ? ARITHMETIC_diff( SELCT_num( SELECT ) SELECT_num( ARGMIN( SELECT ))) Figure  2 : Example of an alignment between question and program tokens in DROP as predicted by FastAlign. "Lossman", "Evan", "Vince", and "Young" are abbreviated to "L.", "E.", "V.", and "Y." for brevity. generalization  (Finegan-Dollak et al., 2018) , by learning a soft alignment between question and program tokens. Since attention is the component in a sequence-to-sequence model that aligns parts of the input to parts of the output, we propose new extensions to the attention mechanism, and examine their effect on compositional generalization. (a) Attention Supervision Intuitively, learning good alignments between question and program tokens should improve compositional generalization: a model that correctly aligns the token largest to the predicate max should output this predicate when encountering largest in novel contexts. To encourage learning better alignments, we supervise the attention distribution computed by the decoder to attend to specific question tokens at each time-step  (Liu et al., 2016) . We use an offthe-shelf word aligner to produce a "gold" alignment between question and program tokens (where program tokens correspond to grammar rules in grammar-based decoding) for all training set examples. Then, at every decoding step where the next prediction symbol's "gold" alignment is to question tokens at indices I, we add the term ? log i?I p i to the objective, pushing the model to put attention probability mass on the aligned tokens. We use the FastAlign word alignment package  (Dyer et al., 2013) , based on IBM model 2, which is a generative model that allows to extract word alignments from parallel corpus without any annotated data. Figure  2  shows an example question-program pair and the alignments induced by FastAlign. (b) Attention over Spans Question spans can align to subtrees in the corresponding program. For example, in Fig.  1 , largest state aligns to state.area = (select max . . . from state). Similarly, in a question such as "What does Lionel Messi do for a living?", the multiword phrase "do for a living" aligns to the KB relation Profession. Allowing the model to directly attend to multi-token phrases could induce more meaningful alignments that improve compo-sitional generalization. Here, rather than computing an attention distribution over input tokens (x 1 , . . . x n ), we compute a distribution over the set of spans corresponding to all constituents (including all tokens) as predicted by an off-the-shelf constituency parser  (Joshi et al., 2018) . Spans are represented using a self-attention mechanism over the hidden representations of the tokens in the span, as in  Lee et al. (2017) . (c) Coverage Questions at test time are sometimes similar to training questions, but include new information expressed by a few tokens. A model that memorizes a mapping from question templates to programs can ignore this new information, hampering compositional generalization. To encourage models to attend to the entire question, we add the attention-coverage mechanism from  See et al. (2017)  to our model. Specifically, at each decoding step the decoder holds a coverage vector c = (c 1 , . . . , c n ), where c i corresponds to the sum of attention probabilities over x i in all previous time steps. The coverage vector is given as another input to the decoder, and a loss term is added that penalizes attending to tokens with high coverage:   (Sakaguchi et al., 2020; , an algorithmic approach to bias reduction in datasets. AFLite is applied when bias is hard to define; as we have direct access to a skewed program distribution, we can take a much simpler approach for reducing bias. n i=1 min(c i , p i ), 

 Datasets We create iid and program splits for five datasets according to the procedure of  Finegan-Dollak et al. (2018)  as described in ?2: 2 Four text-to-SQL datasets from Finegan-Dollak et al. (  2018 ) and one dataset for mapping questions to QDMR programs  (Wolfson et al., 2020)  in DROP  (Dua et al., 2019) . Similar to prior work  (Finegan-Dollak et al., 2018) , we train and test models on anonymized programs, that is, entities are replaced with typed variables ( ?2). Table  1  gives an example question and program for each of these datasets. ? ATIS: questions for a flight-booking task  (Price, 1990; Dahl et al., 1994) . ? GEOQUERY: questions about US geography (Zelle and Mooney, 1996). ? ADVISING: questions about academic course information.  (Finegan-Dollak et al., 2018) . ? SCHOLAR: questions about academic publications  (Iyer et al., 2017) . ? DROP: questions on history and football games described in text paragraphs. We use annotated QDMR programs from  Wolfson et al. (2020) . 

 SQL Grammar: We adapt the SQL grammar developed for ATIS  (Lin et al., 2019)  to cover the four SQL datasets. To achieve that, additional data normalization steps were taken (see appendix), such as rewriting programs to have a consistent SQL style. The grammar uses the DB schema to produce domain-specific production rules, e.g., in ATIS table name ? FLIGHTSalias0, column name ? FLIGHTSalias0.MEAL DESCRIPTION, and value ? class type0. At inference time, we enforce context-sensitive constraints that eliminate production rules that are invalid given the previous context. For example, in the WHERE clause, the set of column name rules is limited to columns that are part of previously mentioned tables. These constraints reduce the number of syntactically invalid programs, but do not eliminate them completely. 

 DROP Grammar: We manually develop a grammar over QDMR programs to perform grammar-based decoding for DROP, similar to . This grammar contains typed operations required for answering questions, such as, ARITHMETIC diff(NUM, NUM) ? NUM, SELECT num(PassageSpan) ? NUM, and SELECT ? PassageSpan. Because QDMR programs are executed over text paragraphs (rather than a KB), QDMR operators receive string arguments as inputs (analogous to KB constants), which we remove for anonymization (Table  1 ). This results in program templates that include only the logical operations required for finding the answer. While such programs cannot be executed asis on a database, they are sufficient for the purpose of testing compositional generalization in semantic parsing, and can be used as "layouts" in a neural module network approach . 

 Experiments We now present our empirical evaluation of compositional generalization. 

 Experimental Setup We create training/development/test splits using both an iid split and a program split, such that the number of examples is similar across splits. Table  2  presents exact statistics on the number of unique examples and program templates for all datasets. There are much fewer new templates in the development and test sets for the iid split than for the program split, thus the iid split requires less compositional generalization. In DROP, we report results for the downsampled dataset ( ?3.4), and analyze downsampling below. Evaluation Metric We evaluate models using exact match (EM), that is, whether the predicted program is identical to the gold program. In addition, we report relative gap, defined as 1 ? EMprogram EM iid , where EM program and EM iid are the EM on the program and iid splits, respectively. This metric measures the gap between in-distribution generalization and OOD generalization, and our goal is to minimize it (while additionally maximizing EM iid ). We select hyper-parameters by tuning the learning rate, batch size, dropout, hidden dimension, and use early-stopping w.r.t. development set EM (specific values are in the appendix). The results reported are averaged over 5 different random seeds. Evaluated Models Our goal is to measure the impact of various modeling choices on compo-Dataset: GEOQUERY x: how many states border the state with the largest population? z: select count( border info.border ) from border info as border info where border info.state name in ( select state.state name from state as state where state.population = ( select max( state.population ) from state as state ) ) 

 Dataset: ATIS x: what is the distance from airport code0 airport to city name0 ? z: select distinct airport service.miles distant from airport as airport , airport service as airport service , city as city where airport.airport code = "airport code0" and airport.airport code = airport service.airport code and city.city code = airport service.city code and city.city name = "city name0" Dataset: SCHOLAR x: What papers has authorname0 written? z: select distinct paper.paperid from author as author , paper as paper , writes as writes where author.authorname = "authorname0" and writes.authorid = author.authorid and writes.paperid = paper.paperid  sitional generalization. We term our baseline sequence-to-sequence semantic parser SEQ2SEQ, and denote the parser that uses grammar-based decoding by GRAMMAR ( ?3.2). Use of contextualized representations in these parsers is denoted by +ELMO and +BERT ( ?3.1). We also experiment with the proposed additions to the decoder attention ( ?3.3). In a parser, use of (a) auxiliary attention supervision obtained from FastAlign is denoted by +ATTNSUP, (b) use of attention over constituent spans by +ATTNSPAN, and (c) use of attention-coverage mechanism by +COVERAGE. 

 Main Results Below we present the performance of our various models on the test set, and discuss the impact of these modeling choices. For SQL, we present results averaged across the four datasets, and report the exact numbers for each dataset in Table  9 . Baseline Performance The top-row in Table  3  shows the performance of our baseline SEQ2SEQ model using GloVe representations. In SQL, it achieves 74.9 EM on the iid split and 10. As ELMO performs slightly better than BERT, we present results only for ELMO in some of the subsequent experiments, and report results for BERT in Table  9 . Grammar-based Decoding Table  3  shows that grammar-based decoding both increases accuracy and reduces the relative gap on DROP in all cases. In SQL, grammar-based decoding consistently decreases the absolute performance compared to SEQ2SEQ. We conjecture this is because our SQL grammar contains a large set of rules meant to support the normalized SQL structure of Finegan-Dollak et al. (  2018 ), which makes decoding this structure challenging. We provide further in-depth comparison of performance in ?5.3. Attention Supervision Table  4  shows that attention supervision has a substantial positive effect on compositional generalization, especially in SQL. In SQL, adding auxiliary attention supervision to a SEQ2SEQ model improves the program split EM from 10.8 ? 18.5, and combining with ELMO leads to an EM of 20.3. Overall, using ELMO and ATTNSUP reduces the relative gap from 84.6 ? 70.6 compared to SEQ2SEQ. In DROP, attention supervision improves iid performance and reduces the relative gap for GRAMMAR using GloVe representations, but does not lead to additional improvements when combined with ELMO. Attention-coverage Table  5  shows that attention-coverage improves absolute performance and compositional generalization in all cases. Interestingly, in SQL, best results are obtained without the attention coverage loss term, but still providing the coverage vector as additional input to the decoder. In SQL, adding attention-coverage improves program split EM from 10.8 ? 17. Combining coverage with ELMO and ATTNSUP leads to our best results, where program split EM reaches 25.4, and the relative gap drops from 84.6 ? 62.2 (with a slight drop in iid split EM). In DROP, using attention-coverage mechanism with auxiliary coverage loss improves iid performance from 53.2 ? 64.6 and reduces the relative gap from 96 ? 93.1. Attention over Spans Table  6  shows that, without ELMO, attention over spans improves iid and program split EM in both SQL and DROP, but when combined with ELMO differences are small and inconsistent. SQL datasets and label each example with one of three categories (Table  8 ): Seen programs are errors resulting from outputting program templates that appear in the training set, while new programs are wrong programs that were not observed in the training set. Invalid syntax errors are outputs that are syntactically invalid programs. Table  8  shows that for SEQ2SEQ models, those that improve compositional generalization also increase the frequency of new programs and invalid syntax errors. Grammarbased models output significantly more new programs than SEQ2SEQ models, and less invalid syntax errors.  3  Overall, the correlation between successful compositional generalization and the rate of new programs is inconsistent. We further inspect 30 random predictions of multiple models on both the program split and the iid split (Table  10 ). Semantically equivalent errors are predictions that are equivalent to the target programs. Semantically similar is a relaxation of the former category (e.g., an output that represents "flights that depart at time0", where the gold program represents "flights that depart after time0"). Limited divergence or significant divergence corresponds to invalid programs that slightly or significantly diverge from the target output, respectively. 

 Downsampling Frequent Templates Table  10  shows that adding attentionsupervision, attention-coverage, and attention over spans increases the number of predictions that are semantically close to the target programs. We also find that the frequency of correct typed variables in predictions is significantly higher when using attention-supervision and attention-coverage compared to the baseline model (p < 0.05). In addition, the errors of the GRAMMAR model tend to be closer to the target program compared to SEQ2SEQ. Compositional Generalization in DROP Our results show that compositional generalization in DROP is harder than in the SQL datasets. We hypothesized that this could be due to the existence of KB relations in SQL programs after program anonymization, while QDMR programs do not contain any arguments. To assess that, we further anonymize the predicates in all SQL programs in all four datasets, such that the SQL programs do not contain any KB constants at all (similar to DROP). We split the data based on this anonymization, and term it the KB-free split. On the development set, when moving from a program split to a KB-free split, the average accuracy drops from 14.5 ? 9.8. This demonstrates that indeed a KB-free split is harder than the program split from Finegan-Dollak et al. (  2018 ), partially explaining the difference between SQL and DROP. 

 Conclusion We presented a comprehensive evaluation of compositional generalization in semantic parsers by analyzing the performance of a wide variety of models across 5 different datasets. We experimented with well-known extensions to sequenceto-sequence models and also proposed novel extensions to the decoder's attention mechanism. Moreover, we proposed reducing dataset bias towards a heavily skewed program template distribution by downsampling examples from frequent templates. We find that our proposed techniques improve generalization to OOD examples. However, the generalization gap between in-distribution and OOD data remains high. This suggests that future research in semantic parsing should consider more drastic changes to the prevailing encoder-decoder approach to address compositional generalization. 
